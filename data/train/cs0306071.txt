{
  "article_text": [
    "the basic grid services in new lhc era experiments @xcite are distributed data storage and computing for high performance access and parallel processing of experimental data .",
    "two fundaments have to be provided in this environment :    * a global grid file system * a global queue system and/or a global analysis framework    in the following sections we will discuss the layout of a global grid file system in general and the concrete implementation in the alien grid .",
    "the global grid file system has to glue multiple different types of storage systems and data transport protocols into one virtual file system .",
    "a common abstract user access interface has to be provided while concrete implementations can be hidden . + it should support access privileges and ownership of directories and files for individual users ( user ) , group of users ( group ) or all members in the community ( others ) .",
    "moreover meta data should be tagged to grid files for indexing . in the context of a conventual file system ,",
    "meta data plays a minor role .",
    "+ the file system representation needs :    * shell(-like ) commands ( ls , mkdir , cp , rm ... ) * application file access    a unix - like command line interface are nowadays standard requirements , which are important for interactive sessions .",
    "+ on the other hand application file access plays a more crucial role in the context of henp applications and will be discussed in the following sections .",
    "we can distinguish three spheres for the implementation of a grid file system :    * hierarchical virtual directory structure * data storage elements * data transfer layer      the user interface of distributed file storages is based on a virtual directory structure , which connects virtual ( in the following referred to as logical ) file names to physical locations and their access methods .",
    "these lfns ( logical file names ) connect to pfns ( physical file names ) in a common url format :    @xmath1    logical file names are constructed in a file system - like notation : @xmath2/[subdir]/ .. /[filename]$ ] .",
    "directories and subdirectories are connected like inodes in a conventional file system .",
    "+      data storage elements ( se ) implement the functionality of persistent data storage .",
    "a se provides the namespace and storage access for pfns . at the backend ses",
    "use conventional disk file systems or mass storage systems like castor @xcite , dcache @xcite , hpss @xcite etc .",
    "+ physical locations and names of files are given by the configuration of a se and have no connection to lfn names in the virtual directory structure .",
    "the data transfer layer can be divided into a local and a global component ( see figure [ transferlayer ] ) .",
    "the local data transfer layer describes the data access of ses to the local used storage system via local access protocols like posix i / o @xcite , root i / o @xcite , rfio @xcite , dcache @xcite etc .",
    "the global data transfer component specifies data transfer between different ses or between ses and remote users / applications .",
    "moreover one can distinguish scheduled and interactive access , while in the context of a file system we will consider mostly interactive ( immediate ) file access          mainly three different approaches for a grid file system implementation exist :    * a virtual file system as a kernel module in the operation system * a bypass library which overloads standard posix i / o commands * a grid application interface library which has to be linked into grid connected applications      the implementation of a virtual file system module , as it is common in operating systems like linux , is the preferred solution for interactive user access @xcite .",
    "the user can access the grid file system in a familiar way like every standard file system ( ext3/*afs * etc . ) with shell commands , posix i / o etc . +",
    "a difficulty with the implementation of such modules is the implementation of grid connectivity in a kernel environment .",
    "a solution to this problem will be discussed under [ lufssection ] .",
    "since a kernel module needs a bigger installation effort and requires high stability and availability , the preferred method for production farm computers is a bypass or an api library .",
    "while a bypass library may dedicate a computer completely to one grid implementation , a connection of grid functionality to programs by use of an api library allows the most flexible and stable use of grid file system functionality .",
    "alien ( alice _ environment _ ) is an implementation of distributed computing infrastructure ( grid framework ) designed to simulate , reconstruct and analyze data of the alice experiment .",
    "it is built on top of internet standards for information exchange and authentication ( soap @xcite , pki ) and common open source components .",
    "it provides a file catalogue and services for authentication , job execution , file transport , performance monitoring and event logging .",
    "the alien core services are implemented using perl .",
    "further reading can be done under @xcite and @xcite .",
    "the virtual directory structure for the alien grid file system is provided by the alien file catalogue ( see figure [ catalogue ] ) .",
    "it is implemented using a database independent interface which connects at the moment to mysql databases @xcite . in terms of database labeling ,",
    "virtual directories are represented by tables in the database .",
    "subdirectories are connected to directories through sub - table entries . for performance gain",
    "different directory branches can be assigned to different database servers .",
    "+        the base names of lfns are represented as entries in directory tables .",
    "these entries contain beside the lfn itself the _ master _ pfn with information about the physical location and the access protocol of the specific files",
    ". locations of replicated files are not stored in the lfn entries itself but in a dedicated table containing lfns and replica locations .",
    "+ for fast response an additional database table contains all lfns and the corresponding sub - table references , where a specific lfn can be found .",
    "each lfn entry contains moreover size , owner , group and access privilege information .",
    "+ additional information about the virtual file catalogue can be found under @xcite and @xcite .        the basic storage element implements the storage , access and pfn labeling functionality for files .",
    "it deals only with complete files and does not support partial file access .",
    "it provides a plug - in architecture to support any kind of storage systems .",
    "therefore standard directory and file manipulation functions are implemented in plug - in modules ( see table [ seplugins ] ) .",
    ".plug - in functions of the basic storage element .",
    "[ cols= \" < \" , ]     [ semodules ]    files are stored in a local disk cache for performance improvement in case of repetitive file access .",
    "the logical volume manager allows to merge several storage system volumes of the same kind into one logical storage volume .",
    "this is especially useful to cluster a set of raid disk systems or hard disks into one logical volume . for each volume a mount point , disk space and a file lifetime can be defined .",
    "the logical volume manager keeps track of used and available disk space and blocks additional storage save operations , if the se size is exceeded .",
    "expired files can be erased . using the _ lslist _ function of the se plug - in modules , it allows ( re-)synchronisation of existing data in the storage volumes .      to extend the functionality of the basic storage element , which supports only access of complete files ,",
    "i / o daemons can be started permanently or on demand to support partial file access .",
    "the possible protocols are described in the following subsection .",
    "alien uses at the moment three mechanism for file transfer .",
    "small files can be accessed using the soap @xcite protocol , if supported by the se .",
    "otherwise a transfer broker exists to move or replicate files , which have been entered into the transfer queue .",
    "the transfer is executed by file transfer daemons , which reside on ses using a bbftp @xcite server and client .",
    "since the transfers are scheduled from a queue , this has to be considered as an asynchronous mechanism .",
    "+ for interactive and partial file access , alien uses i / o servers , which can be started on demand or permanently at ses .",
    "possible servers could be f.e .",
    "gridftp @xcite , rootd @xcite , sshd @xcite , rfiod @xcite , a self - developed server etc .",
    "the best choice is still in the evaluation phase ( see section [ future ] ) .",
    "the alien se supports two operations for file access :    * @xmath3 ( offset , size ) * @xmath4 ( streamed )    while the @xmath3 operation allows to read files at arbitrary positions , the @xmath5 operation allows only streaming without offset change between concurrent @xmath5 operations .",
    "an @xmath6 for @xmath5 can be done only on not existent files , what is often refered to as _",
    "write once _ modus .",
    "this guarantees the consistency of replicated or cached data .",
    "+ for the @xmath3 operation two access strategies are supported :    * partial file transfer via remote i / o server according to @xmath3 requests * local download of the complete file and partial file access from the local disk    accordingly the strategies for @xmath5 operations are :    * partial file transfers to remote i / o server according to @xmath5 requests * @xmath5 to local disk cache and complete file transfer after end of @xmath5 operations ( @xmath7 ) .",
    "evidently the application has to select the appropriate access strategy .",
    "+      the flow diagram for file access is shown in figure [ ioflow ] .",
    "the operation sequence for a @xmath3 operation starts with the lfn resolving .",
    "a dedicated alien service resolves all locations of a file and selects the _ best _ access location and protocol . for the time being this",
    "is done with a trivial matching of the closest location . for the @xmath6 operation , a connection to the remote i / o server is established and possibly ( depending on the access strategy ) downloaded as a complete file . with the @xmath6 command",
    "the user permissions are checked on client and server side .",
    "the @xmath7 command closes a remote connection or the local file .",
    "the operation sequence for a @xmath5 operation starts on @xmath6 with a privilege check for the creation of a new lfn entry in the given directory .",
    "if this is passed , the se is contacted to provide a new pfn",
    ". it also examines , if there is enough space to store the file . at the end of the @xmath6 sequence",
    ", a connection is opened to the remote i / o server . with the @xmath7 operation ,",
    "the file size in the se is validated and the lfn / pfn pair inserted in the file catalogue . +",
    "the client has to authenticate in both cases to the i / o server using the appropriate authentication method for the used protocol .",
    "the above described file access methods are implemented in the alien c++ api library .",
    "moreover the api library contains all needed methods for authentication and exchange with grid services .",
    "+ since all physical directory creation and deletion operations are up to the se , the api provides directory creation , browsing , deletion and privilege / owner manipulation of files and directories only for lfns .",
    "physical files can be created or accessed through so called _ generic _ i / o commands :   + the api itself keeps track after an @xmath6 of the needed remote connections or local file descriptors to proceed with the generic @xmath3 or @xmath5 operations .",
    "these informations are stored in a generic list of open files .",
    "each @xmath6 assigns a file handle ( fdtype ) to be used in consecutive i / o operations to associate with entries in the generic list .",
    "[ htbp ]       as already mentioned , the problematic of a virtual file system implementation under linux is based on the need of high level libraries like the alien c++ api @xcite . in a vfs module in the kernel environment",
    "only kernel functions are known and shared libraries can not be linked .",
    "+ these technical problem can be solved by splitting the vfs module into two parts : one in kernel space and one in user space .",
    "the user space part can be linked to any external library .",
    "all kernel requests have to be redirected from the core vfs kernel module to the user space module .",
    "the open source project lufs ( linux userland file system ) @xcite offers a modular file system plug - in architecture with a generic lufs vfs kernel module , which communicates with various user space file system modules ( see figure [ lufs ] ) .",
    "+        the recent version supports linux kernel v2.4 and v2.5 with large file support .",
    "+ a couple of file system modules come with lufs like @xmath8 , @xmath9 , @xmath10 et al .",
    "@xmath8 e.g. utilises the @xmath11 client protocol to mount a file system from an @xmath11 server . in almost the same manner the alien grid file system module @xmath0 is implemented .",
    "it uses the alien api library for directory and file handling .",
    "+ the @xmath12 command has to be executed by each individual user . during the @xmath12 execution",
    "the grid authentication is done only once for the user space thread . in case of a broken connection , the connection to the grid is automatically rebuilt .",
    "therefore it is recommended to choose a key- or proxy - certificate - based authentication method .",
    "+ some extensions to the @xmath13 library calls have to be made , since lufs uses in @xmath3 and @xmath5 operations always the lfn as a file descriptor and the offset position is always given as an argument in the i / o call .",
    "+ the implementation of the kernel module is done in such a way that it redirects vfs calls via unix domain sockets to a lufs daemon which runs in user space . this daemon loads according to the given mount option a shared library module for the requested file system module .",
    "the lufs daemon provides also directory caching with configurable lifetime for performance increase in repetitive @xmath14 calls .",
    "+ the user / group translation from the alien file catalogue uses the local @xmath15 and @xmath16 files for ownership translation .",
    "+ in case of a write to the grid file system , the assigned default se is used for file storage .",
    "the user can select an alternative se by appending ",
    "@se - name  to the destination file .",
    "+ in the future file meta data can be made visible in the file system as virtual _ extra _ files , which return in case of a read operation the meta data tag list .",
    "+ lufs also supports @xmath17 , which is a desirable functionality for a grid file system .",
    "to provide a more flexible framework , a more general _ gridfs _",
    "module has been developed .",
    "this module allows dynamic loading of grid api libraries and enables to use the same lufs module for various grid platforms ( see figure [ lufs2 ] ) .",
    "+        to introduce a new grid file system , one has to implement a template class , which is the base for a shared library . by executing the mount command ,",
    "the library name of the plug - in class has to be passed as an option to the lufs _ gridfs _ module .",
    "additional needed grid api libraries can also be given as a mount option and will be loaded dynamically from the plug - in module .",
    "the alien plug - in is at present in the evaluation and debugging phase .",
    "the bypass library mechanism has not yet been tested since there are some obvious objections .",
    "a bypass library redirects system calls to the grid api library .",
    "this has some performance implications for non - grid applications .",
    "+ another difficult item is , how to avoid repeated authentication of individual users with every system call . +",
    "therefore the bypass library is at present not considered to be an applicable solution for the alien grid file system .",
    "the crucial point of a grid file system resides not in the end points like the user access through the vfs or the se itself but in the @xmath18 transport layer .",
    "some effort has been spent in the recent time to establish a new efficient transport layer .",
    "+ requirements for a flexible , efficient and safe transport architecture are :    * secure grid user authentication * * a*ccess * c*ontrol * l*ists * optional data encryption * optimization for high- and low - bandwidth transfers * network weather service : * * dynamic bandwidth regulation per connection * * transfer ( re-)routing through cache servers and high - speed connections * load balancing * * on - site distributed cache server * * on - site distributed i / o server * safe operation modus : * * @xmath3 according to catalogue permissions * * @xmath5-@xmath19 according to catalogue permissions * * no directory manipulation allowed    after evaluation of existing ( grid ) protocols and i / o servers like in section [ protocols ] a conclusion was that none of these solutions can satisfy approximately the above requirements .",
    "+ motivated by the given requirements a new architecture for the grid file system transport layer has been developed : the @xmath20 .",
    "since grid files are shared between a large community of users , it is reasonable to move from a point - to - point connection scheme between ses and off - site applications to connections of a cache - gateway type : @xmath21 .",
    "+ in this scheme file accesses are routed through distributed cache site - servers to allow efficient usage of cache functionality ( see figure [ clc ] ) .",
    "a grid service will provide for each file access information about the most efficient file access route .",
    "moreover this service can route file @xmath5 operations through caches to perform automatic file replication .",
    "+ to allow asynchronous caching , each file has to be labeled with a globally unique identifier guid @xcite . the _ write - once _ strategy combined with guid labeling guarantees then the identity of files with the same guid label in different caches . +      at present a new i / o server @xmath22 is under development to allow the cache - and - forward mechanism ( see figure [ aiod ] ) .",
    "it consists of two daemons , one handling the client @xmath23 requests and one pre - loading the clients data from other @xmath22 servers or local storage systems .",
    "the transfers are split into cache pages depending on the total file size and client request ( f.e .",
    "random access requires preferably small cache pages ) .",
    "+     cache - and - forward server.,width=245 ]    the authentication is done with certificates , which reside in the client directory inside the alien file catalogue as virtual db files .",
    "each cache page can be encrypted using the ssl envelope mechanism @xcite for efficient performance . for the transport between servers",
    "the quanta extended parallel tcp class has been modified to allow transfer speed regulation @xcite . a c++ client class and an @xmath24 and @xmath25 command has been developed for file access from applications and shells .",
    "if a file is requested by a client , the client has to provide the default routing information ( cache - forward addresses like @xmath26 ) , grid username , certificate file locations , lfn , pfn , guid , encryption type to the @xmath22 server .",
    "this information is validated by the @xmath22 server before the access is performed .",
    "all transfered cache pages are written with guid , offset and size information into the cache directory . in concurrent read requests , data can be taken out of the cache directory instead of downloading from remote servers through the network .      if an @xmath22 server is set up as a i / o gate keeper , it can re - route a connection to a slave server ( see figure [ gatekeeper ] ) .",
    "the total i / o bandwidth and status of all configured slave servers are reported through a monitoring daemon to the gate keeper , which chooses the server with the lowest load .",
    "+     gate keeper redirects incoming i / o requests to slave servers.,width=245 ]      the same @xmath22 server can act as a cache gate keeper . if a client asks for a special guid , all slave cache servers are queried by the gate keeper for this guid . in case of a match the client",
    "is redirected to the associated slave cache .",
    "+      ideally every site should be configured with cache gate keepers , which present the access point for all off - site and on - site i / o requests .",
    "these gate keepers redirect to slave caches . if files are not cached the slave cache forwards from the i / o gate keeper , which redirects to a slave i / o server with a low load .",
    "data is then read by a slave i / o server through a slave i / o cache server .",
    "ideally the slave cache itself acts also as a slave server and the cache gate keeper as the i / o gate keeper .",
    "a combination of the concepts introduced in the previous sections points out a possible scenario for the use of grid file systems in multi - user environments .",
    "interactive work and applications use the grid file system in a slightly different way +      the needs of interactive work are fully satisfied by the vfs implementation .",
    "users can read , edit and write files of the grid file system like in every conventional file system interactively .",
    "+        a special setup can allow to cross - mount the grid file system from a lufs server via nfs or lufs modules to linux nodes or via samba to windows computers ( see figure [ lufsexport ] ) .",
    "the lufs server can act in this scenario moreover as a user - shared file cache .",
    "the export of the grid file system with user specific permissions will require no additional kernel modules on client nodes and simplifies the installation of large computing farms .",
    "+ each user has to setup once the authentication for the lufs server to allow auto - mounting of the grid file catalogue with specific user permissions .",
    "the security of the export from the lufs server to any file system client depends then on the used protocol ( samba / nfs / etc . ) .",
    "+ the unix - like command line interface allows all standard file system operations .",
    "the syntax + `` _ @xmath27command@xmath28 @xmath27file1@xmath28@@xmath27se1@xmath28 @xmath27file2@xmath28@@xmath27se2@xmath28 _ '' allows to implement all additional file moving , linking and replication commands of a grid file system . + all transfers from the grid file system to any other local file system or the other way around have to be executed synchronously . on the other hand replication requests of large files ( f.e . _",
    "cp @xmath27lfn@xmath28@@xmath27se1@xmath28 @xmath27lfn@xmath28@@xmath27se2@xmath28 _ ) between storage elements can be executed asynchronously .",
    "these requests can be queued in a file transfer queue .",
    "when the transfer is scheduled and executed , the new replica location will be added to the lfn .",
    "+          applications use the alien api library for file access .",
    "the node setup requires only the installation of the alien api libraries .",
    "+ access should be routed through regional and main cache servers to minimise data movements .",
    "regional or nationwide cache servers are connected through high - bandwidth networks like shown in figure [ wcache ] .",
    "+      file replication is done in an active and passive way .",
    "files are replicated by explicit replication requests using the file transfer queue . on the other hand",
    "repetitive file access triggers an automatic file replication into a nearby storage element . in this case replication needs no additional long - distance transfers anymore , since accessed files are already cached in regional cache servers .",
    "the implementation of a global grid file system has importancy for any grid platform .",
    "the alien ambition is to provide a self - made server for the global transport layer and to use existing protocols for local transfers .",
    "the user can use the file system through a vfs and a _ gridfs _ module like a conventual file system , while applications will make use of the alien api .",
    "alien will ideally also support other grid storage solutions like the edg module , to glue most existing solutions in one .",
    "+ the crosslink - cache architecture will allow cached file access and implements the idea of efficient resource sharing and resource control .",
    "+ the future use in the alice experiment will assert or disprove the conceptual design very soon .",
    "+    9 http://public.web.cern.ch/public/about/future/ +  lhcexperiments / lhcexperiments.html http://castor.web.cern.ch/castor/welcome.html http://www-dcache.desy.de/ http://www.sdsc.edu/storage/hsi/ http://www4.clearlake.ibm.com/hpss/index.jsp ieee std 1003.1 - 2001,the open group base specifications issue 6 http://root.cern.ch http://www.cse.unsw.edu.au/eilb/oss/linux-commentary/vfs.html p. saiz , l. aphecetche , p. buncic , r. piskac , j. -e .",
    "revsbech and v. sego , alien - alice environment on the grid , nuclear instruments and methods in physics research section a : accelerators , spectrometers , detectors and associated equipment , volume 502 , issues 2 - 3 , 21 april 2003 , pages 437 - 440 p.buncic , a.j .",
    "peters , p. saiz - the alien system , status and perspectives , these proceedings , moat004    p. saiz , p. buncic and andreas j. peters , alien resource brokers , these proceedings , tuap002 http://www.soap.org http://www.mysql.org http://www.ibm.com/storage/adsm http://alien.cern.ch http://hpcf.nersc.gov/storage/cray/dmf.html http://eu-datagrid.web.cern.ch/eu-datagrid/ http://doc.in2p3.fr/bbftp/ http://www.openssh.com http://www.globus.org/datagrid/gridftp.html http://alien.cern.ch/alien/ +  main?task = doc&section = api http://lufs.sourceforge.net/lufs/intro.html http://www.opengroup.org/onlinepubs/ +  9629399/apdxa.htm http://www.openssl.org http://www.evl.uic.edu/cavern/quanta/"
  ],
  "abstract_text": [
    "<S> among the services offered by the alien ( alice environment http://alien.cern.ch ) grid framework there is a virtual file catalogue to allow transparent access to distributed data - sets using various file transfer protocols . </S>",
    "<S> @xmath0 ( alien file system ) integrates the alien file catalogue as a new file system type into the linux kernel using lufs , a hybrid user space file system framework ( open source http://lufs.sourceforge.net ) . </S>",
    "<S> lufs uses a special kernel interface level called vfs ( virtual file system switch ) to communicate via a generalised file system interface to the alien file system daemon . </S>",
    "<S> the alien framework is used for authentication , catalogue browsing , file registration and read / write transfer operations </S>",
    "<S> . a c++ api implements the generic file system operations . </S>",
    "<S> the goal of alienfs is to allow users easy interactive access to a worldwide distributed virtual file system using familiar shell commands ( f.e . </S>",
    "<S> cp , ls , rm ... ) + the paper discusses general aspects of grid file systems , the alien implementation and present and future developments for the alien grid file system . </S>"
  ]
}