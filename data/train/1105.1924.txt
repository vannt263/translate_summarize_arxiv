{
  "article_text": [
    "consider the regression model for @xmath2 and @xmath3 @xmath4 where @xmath5 and @xmath6 is a mean 0 random variable . in this paper , we study the situation where @xmath7 is subject to a convexity constraint .",
    "that is , @xmath8for every @xmath9 and @xmath10 .",
    "given the observations @xmath11 , we would like to estimate @xmath7 subject to the convexity constraint ; this is called the convex regression problem .",
    "note that convex regression is easily extended to concave regression since a concave function is the negative of a convex function .",
    "convex regression problems occur in a variety of settings .",
    "economic theory dictates that demand  @xcite , production  @xcite and consumer preference  @xcite functions are often concave . in financial engineering ,",
    "stock option prices usually have convexity restrictions  @xcite .",
    "stochastic optimization problems , studied in operations research and reinforcement learning , have response surfaces  @xcite or value - to - go functions that exhibit concavity in many settings , like resource allocation  @xcite or stochastic control  @xcite . similarly , efficient frontier methods like data envelopment analysis  @xcite include convexity constraints . in statistics , shape restrictions like",
    "log - concavity are useful in density estimation  @xcite .",
    "finally , in optimization , convex approximations to posynomial constraints are valuable for geometric programming  @xcite .",
    "although convex regression has been well - explored in the univariate setting , the literature remains underdeveloped in the multivariate setting .",
    "existing methods do not scale well to more than a few thousand observations or more than a handful of dimensions .    in this paper",
    ", we introduce the first computationally efficient , theoretically sound multivariate convex regression method , called convex adaptive partitioning ( cap ) .",
    "it relies on an alternate definition of convexity , @xmath12 for every @xmath9 , where @xmath13 is a subgradient of @xmath7 at @xmath14 .",
    "equation ( [ eq : convexity ] ) states that a convex function lies above all of its supporting hyperplanes , or subgradients tangent to @xmath7 .",
    "moreover , with enough supporting hyperplanes , @xmath7 can be approximately reconstructed by taking the maximum over those hyperplanes .",
    "the cap estimator is formed by adaptively partitioning a set of observations . within each subset of the partition",
    ", we fit a linear model to approximate the subgradient of @xmath7 within that subset . given a partition with @xmath15 subsets and linear models , @xmath16 , a continuous , convex ( concave ) function",
    "is then generated by taking the maximum ( minimum ) over the hyperplanes by @xmath17 the partition is refined by a twofold strategy .",
    "first , one of the subsets is split along a cardinal direction ( say , @xmath18 or @xmath19 ) to grow @xmath15 .",
    "then , the hyperplanes themselves are used to refit the subsets . a piecewise linear function like @xmath20 induces a partition ; a subset is defined as the region where a particular hyperplane is dominant .",
    "the refitting step places the hyperplanes in closer alignment with the observations that generated them .",
    "this procedure is repeated until all subsets have a minimal number of observations .",
    "the cap estimator is then created by selecting the value of @xmath15 that balances fit with complexity using a generalized cross validation method  @xcite .",
    "cap has strong theoretical properties , both in terms of computational complexity and asymptotic properties .",
    "we show that cap is consistent with respect to the @xmath21 metric and has a computational complexity of @xmath22 flops .",
    "the most widely implemented convex regression method , the least squares estimator , has only recently been shown to be consistent  @xcite and has a computational complexity of @xmath23 flops . despite a difference of almost @xmath24 runtime",
    ", the cap estimator usually has better predictive error as well .",
    "because of its dramatic reduction in runtime , cap opens a new class of problems for study , namely moderate to large problems with convexity or concavity constraints .",
    "the rest of this paper is organized as follows . in section [ sec : litreview ]",
    ", we review the literature on convex regression . in section [ sec : cap ] , we present the cap algorithm . in section [ sec : theory ] , we give computational complexity results and conditions for consistency . in section [ sec : implementation ] , we derive a generalized cross - validation method and give a fast approximation for the full cap algorithm . in section [ sec : numbers ] , we empirically test cap on convex regression problems , including value function estimation for pricing american basket options . in section [ sec : conclusions ] , we discuss our results and give directions for future work .",
    "the literature for nonparametric convex regression is dispersed over a variety of fields , including statistics , operations research , economics , numerical analysis and electrical engineering .",
    "there seems to be little communication between the fields , leading to the independent discovery of similar techniques .    in the univariate setting , there are many computationally efficient algorithms for convex regression .",
    "these methods rely on the ordering implicit to the real line . setting @xmath25 for @xmath26",
    ", @xmath27 is a sufficient constraint for pointwise convexity .",
    "when @xmath7 is differentiable , equation ( [ eq : ordering ] ) is equivalent to an increasing derivative function .",
    "various methods have been used to solve the univariate convex regression problem .",
    "the least squares estimator ( lse ) is the oldest and simplest method .",
    "it produces a piecewise linear estimator by solving a quadratic program with @xmath28 linear constraints  @xcite .",
    "although the lse is completely free of tunable parameters , the estimator is not smooth and can overfit , particularly in the multivariate setting .",
    "consistency , rate of convergence , and asymptotic distribution of the lse were shown by @xcite , @xcite and @xcite , respectively .",
    "algorithmic methods for solving the quadratic program were given in @xcite and @xcite .",
    "spline methods have also been popular . @xcite and @xcite used convex - restricted splines with positive parameters in frequentist and bayesian settings , respectively . @xcite and @xcite used unrestricted splines with restricted parameters , likewise , in frequentist and bayesian settings .",
    "in other methods , @xcite used convexity constrained kernel regression ; @xcite used a random bernstein polynomial prior with constrained parameters ; and @xcite transformed the ordering problem into a combinatorial optimization problem which they solved with dynamic programming .    due to the constraint on the derivative of @xmath7 ,",
    "univariate convex regression is quite similar to univariate isotonic regression .",
    "the latter has been studied extensively with many approaches ; for examples , see @xcite and @xcite .    unlike the univariate setting , convex functions in multiple dimensions can not be represented by a simple set of first order conditions and projection onto the set of convex functions becomes computationally intensive .",
    "as in the univariate case , the earliest and most popular regression method is the lse , which directly projects a least squares estimator onto the cone of convex functions .",
    "it was introduced by @xcite and @xcite .",
    "the estimator is found by solving the quadratic program , @xmath29here , @xmath30 and @xmath31 are the estimated values of @xmath32 and the subgradient of @xmath7 at @xmath33 , respectively .",
    "the estimator @xmath34 is piecewise linear , @xmath35the characterization  @xcite and consistency  @xcite of the least squares problem have only recently been studied .",
    "the lse quickly becomes impractical due to its size : equation ( [ eq : lse ] ) has @xmath36 constraints .",
    "this results in a computational complexity of @xmath37 , which becomes impractical after one to two thousand observations .",
    "while the lse is widely studied across all fields , the remaining literature on multivariate convex regression is sparser and more dispersed than the univariate literature .",
    "one approach is to place a positive semi - definite restriction on the hessian of the estimator . in the economics literature ,",
    "@xcite used kernel smoothing with a restricted hessian and found a solution with sequential quadratic programming . in electrical engineering , @xcite , and in a variational",
    "setting , @xcite and @xcite , used semi - definite programming to search the space of functions with positive semi - definite local hessians . although consistent in some cases  @xcite , hessian methods are computationally intensive and can be poorly conditioned in boundary regions . in another approach ,",
    "@xcite proposed a method based on reformulating the maximum likelihood problem as one minimizing entropic distance , which can be solved as a linear program .",
    "however , like the original maximum likelihood problem , the transformed problem still has @xmath38 constraints and does not scale to more than a few thousand observations .",
    "recently , multivariate convex regression methods have been proposed with a more traditional statistics approach .",
    "@xcite proposed a two step smoothing and fitting process .",
    "first , the data were smoothed and functional estimates were generated over an @xmath6-net over the domain .",
    "then the convex hull of the smoothed estimate was used as a convex estimator . again , although this method is consistent , it is sensitive to the choice of smoothing parameter and does not scale to more than a few dimensions .",
    "@xcite proposed a bayesian model that placed a prior over the set of all piecewise linear models .",
    "they were able to show adaptive rates of convergence , but the inference algorithm did not scale to more than a few thousand observations .    in a more computational approach , @xcite use an iterative fitting scheme . in this method ,",
    "the data were divided into @xmath15 random subsets and a linear model was fit within each subset ; a convex function was generated by taking the maximum over these hyperplanes .",
    "the hyperplanes induce a new partition , which is then used to refit the function .",
    "this sequence was repeated until convergence . despite relatively strong empirical performance , this method is sensitive to the initial partition and the choice of @xmath15 . moreover ,",
    "it is not consistent and there are cases when the algorithm does not even converge .",
    "as seen in much of the literature , a natural way to model a convex function @xmath7 is through the maximum of a set of hyperplanes .",
    "one example of this method is the least squares estimator , which fits every observation with its own hyperplane .",
    "this is computationally expensive and can result in overfitting , as shown in figure [ fig : capvlse ] .",
    "instead , we wish to model @xmath7 through only @xmath15 hyperplanes .",
    "we do this by partitioning the covariate space and approximating the gradients within each region by hyperplanes generated by the least squares estimator .",
    "the covariate space partition and @xmath15 are chosen through adaptive partitioning .",
    "given a partition @xmath39 of @xmath40 , an estimate of the gradient for each subset can be created by taking the least squares linear estimate based on all of the observations within that region , @xmath41 a convex function @xmath42 can be created by taking the maximum over @xmath43 , @xmath44    models adaptive partitioning models with linear leaves have been proposed before ; see @xcite and @xcite for examples . in most of these cases ,",
    "the partition is created by adaptively refining an existing partition by dyadic splitting of one subset .",
    "the split is chosen in a way that minimizes local error within the subset .",
    "there are two problems with these partitioning methods that arise when a piecewise linear summation function , @xmath45 is changed into a piecewise linear maximization function , like @xmath42 .",
    "first , a split that minimizes local error does not necessarily minimize global error for @xmath42 .",
    "this is fairly easy to remedy by considering splits based on minimizing global error .",
    "the second problem is more difficult : the gradients often act in areas over which they were not estimated .",
    "a piecewise linear maximization function , @xmath42 , generates a new partition , @xmath46 , by @xmath47 the partition @xmath39 is not necessarily the same as @xmath48 .",
    "we can use this new partition to refit the hyperplanes and produce a significantly better estimate .",
    "refitting hyperplanes in this manner can be viewed as a gauss - newton method for the non - linear least squares problem  @xcite , @xmath49 similar methods for refitting hyperplanes have been proposed in @xcite and @xcite .",
    "however , repeated refitting may not converge to a stationary partition and is sensitive to the initial partition .",
    "convex adaptive partitioning ( cap ) uses adaptive partitioning with linear leaves to fit a convex function that is defined as the maximum over the set of leaves .",
    "the adaptive partitioning itself differs from previous methods in order to fit piecewise linear maximization functions .",
    "partitions are refined in two steps .",
    "first , candidate splits are generated through dyadic splits of existing partitions .",
    "these are evaluated and the one that minimizes global error is greedily selected .",
    "second , the new partition is then refit .",
    "although simple , these rules , and refitting in particular , produce large gains over naive adaptive partitioning methods ; empirical results are discussed in section [ sec : numbers ] .",
    "most other adaptive partitioning methods use backfitting or pruning to select the tree or partition size . due to the construction of the cap estimator",
    ", we can not locally prune and so instead we rely on model selection criteria .",
    "we derive a generalized cross - validation method for this setting that is used to select @xmath15 .",
    "this is discussed in section [ sec : gcv ] .    since cap shares many feature with existing adaptive partitioning algorithms",
    ", we are able to use many adaptive partitioning results to study cap practically and theoretically .",
    "this is in stark contrast to the least squares estimator . despite being introduced by @xcite ,",
    "it has not been implemented in many practical settings  @xcite and has only very recently been shown to be consistent  @xcite .",
    "we now introduce some notation required for convex adaptive partitioning .",
    "when presented with data , a partition can be defined over the covariate space ( denoted by @xmath39 , with @xmath50 ) or over the observation space ( denoted by @xmath51 , with @xmath52 ) .",
    "the observation partition is defined from the covariate partition , @xmath53 cap proposes and searches over a set of models , @xmath54 .",
    "a model @xmath55 is defined by : 1 ) the covariate partition @xmath39 , 2 ) the corresponding observation partition , @xmath56 , and 3 ) the hyperplanes @xmath57 fit to those partitions .",
    "the cap algorithm progressively refines the partition until each subset can not be split without one subset having fewer than a minimal number of observations , @xmath58 , where @xmath59 here @xmath60 is a log scaling factor , which acts to change the base of the log operator .",
    "this minimal number is chosen so that 1 ) there are enough observations to accurately fit a hyperplane , and 2 ) there is a lower bound on the growth rate for the number of observations in each subset  and an upper bound on the number of subsets .",
    "this is used to show consistency .",
    "we briefly outline the cap algorithm below .",
    "details are given in the following subsections .",
    "[ [ convex - adaptive - partitioning - cap ] ] convex adaptive partitioning ( cap ) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    1 .",
    "* initialize . *",
    "set @xmath61 ; place all observations into a single observation subset , @xmath62 ; @xmath63 ; this defines model @xmath64 .",
    "* split.*[item : split ] refine partition by splitting a subset .",
    "a.   _ generate candidate splits .",
    "_ generate candidate model @xmath65 by 1 ) fixing a subset @xmath66 , 2 ) fixing a dimension @xmath67 , 3 ) dyadically dividing the data in subset @xmath66 and dimensions @xmath67 according to knot @xmath68 .",
    "this is done for @xmath69 knots , all @xmath70 dimensions and @xmath15 subsets .",
    "b.   _ select split .",
    "_ choose the model @xmath71 from the candidates that minimizes global mean squared error on the training set and satisfies @xmath72 .",
    "set @xmath73 .",
    "3 .   * refit .",
    "* use the partition induced by the hyperplanes to generate model @xmath74 .",
    "set @xmath75 if for every subset @xmath76 in @xmath74 , @xmath77 .",
    "4 .   * stopping conditions . * if for every subset @xmath78 in @xmath55 , @xmath79 , stop fitting and proceed to step [ item : modelsize ] .",
    "otherwise , go to step [ item : split ] .",
    "[ item : modelsize ] * select model size .",
    "* each model @xmath55 creates an estimator , @xmath80 use generalized cross - validation on the estimators to select final model @xmath81 from @xmath82 .",
    "to split , we create a collection of candidate models by dyadically splitting a single subset .",
    "since the best way to do this is not apparent , we create models for every subset and search along every cardinal direction by splitting the data along that direction .",
    "we create model @xmath65 by 1 ) fixing subset @xmath83 , and 2 ) fixing dimension @xmath84 .",
    "let @xmath85 be the minimum value and @xmath86 be the maximum value of the covariates in this subset and dimension , @xmath87let @xmath88 be a set of evenly spaced knots that represent the proportion between @xmath85 and @xmath86 .",
    "use the weighted average @xmath89 to split @xmath78 and @xmath90 in dimension @xmath67 .",
    "set @xmath91these define new subset and covariate partitions , @xmath92 and @xmath93 where @xmath94 and @xmath94 for @xmath95 .",
    "fit hyperplanes @xmath96 in each of the subsets .",
    "the triplet of observation partition @xmath92 , covariate partition , @xmath93 , and set of hyperplanes @xmath96 defines the model @xmath65 .",
    "this is done for @xmath97 , @xmath98 and @xmath99 .",
    "after all models are generated , set @xmath73 .",
    "we note that any models where @xmath100 are discarded .",
    "if all models are discarded in one subset / dimension pair , we produce a model by splitting on the subset median in that dimension .",
    "we select the model @xmath65 that gives the smallest _ global _ error .",
    "let @xmath101 be the hyperplanes associated with @xmath65 and let @xmath102 be its estimator .",
    "we set the model @xmath103 to be the one that minimizes global mean squared error , @xmath104set @xmath105 to be the minimal estimator .",
    "we refit by using the partition induced by the hyperplanes .",
    "let @xmath106 be the hyperplanes associated with @xmath103 .",
    "refit the partitions by @xmath107 for @xmath97 .",
    "the covariate partition , @xmath108 is defined in a similar manner .",
    "fit hyperplanes in each of those subsets .",
    "let @xmath109 be the model generated by the partition @xmath110 .",
    "set @xmath75 if @xmath77 for all @xmath66 .",
    "cap has two tunable parameters , @xmath69 and @xmath60 .",
    "@xmath69 specifies the number of knots used when generating candidate models for a split .",
    "its value is tied to the smoothness of @xmath7 and after a certain value , usually 5 to 10 for most functions , higher values of @xmath69 offer little fitting gain .",
    "the parameter @xmath60 is used to specify a minimum subset size , @xmath111 . here",
    "@xmath60 transforms the base of the logarithm from @xmath112 into @xmath113 .",
    "we have found that @xmath114 ( implying base @xmath115 ) is a good choice for most problems .",
    "increases in either of these parameters increase the computational time .",
    "sensitivity to these parameters , both in terms of predictive error and computational time , is empirically examined in section [ sec : sensitivity ] .",
    "in this section , we give the computational complexity for the cap algorithm and conditions for consistency . since cap is similar to existing adaptive partitioning methods , we can leverage existing results to show consistency .",
    "computational complexity describes the number of bit operations a computer must do to perform a routine , such as cap .",
    "it is useful to determine small sample runtimes and how well routines will scale to larger problems .",
    "the computational complexity of the least squares estimator is unworkably high at @xmath37 flops to solve a problem with @xmath116 observations in @xmath70 dimensions  @xcite .",
    "the worst case computational complexity of cap is much lower , at @xmath117 flops when implemented as in section [ sec : cap ] .",
    "the most demanding part of the cap algorithm is the linear regression ; each one has complexity @xmath118 . for iteration @xmath66 of the algorithm ,",
    "@xmath119 linear regressions are fit .",
    "this is done for @xmath120 , where @xmath15 is bounded by @xmath121 . putting this together we obtain the above complexity .    to demonstrate how much these factors matter in practice , we empirically compare cap , fast cap and lse on a small problem , @xmath122 where @xmath123 , @xmath124 and @xmath125 . the runtimes and mean absolute errors of each method are shown in figure [ fig : complexity ] .",
    "we now show consistency for the cap algorithm .",
    "consistency is shown in a similar manner to consistency for other adaptive partitioning models , like cart  @xcite , treed linear models  @xcite and other variants  @xcite .",
    "we take a two - step approach , first showing consistency for the mean function and first derivatives of a more traditional treed linear model based on cap under the @xmath21 metric and then we use that to show consistency for the cap estimator itself .    letting @xmath126 be the model for the cap estimate after @xmath116 observations ,",
    "define the discontinuous piecewise linear estimate based on @xmath126 , @xmath127where @xmath128 is the partition size , @xmath129 are the covariate partitions and @xmath130 are the hyperplanes associated with @xmath126 .",
    "likewise , let @xmath131 be the cap estimator based on @xmath126 , @xmath132each subset @xmath90 has an associated diameter , @xmath133 , where @xmath134 define the empirical covariate mean for subset @xmath66 as @xmath135 for @xmath136 define @xmath137 \\\\",
    "d_{nk}^{-1 } \\left ( { \\mathbf{x}}_i - \\bar{{\\mathbf{x}}}_k\\right )    \\end{array}\\right ] , & g_k & = \\sum_{i \\in c_k } \\gamma_i \\gamma_i^t.\\end{aligned}\\]]note that @xmath138 whenever @xmath139 is nonsingular .",
    "let @xmath140 be i.i.d .",
    "random variables .",
    "we make the following assumptions :    1 .",
    "@xmath40 is compact and @xmath7 is lipschitz continuous and continuously differentiable on @xmath40 with lipschitz parameter @xmath141 .",
    "there is an @xmath142 such that @xmath143 $ ] is bounded on @xmath40 .",
    "3 .   let @xmath144 be the smallest eigenvalue of @xmath145 and @xmath146 .",
    "then @xmath147 remains bounded away from 0 in probability as @xmath148 .",
    "the diameter of the partition @xmath149 in probability as @xmath148 .",
    "assumptions * a1 . * and * a2 . * place regularity conditions on @xmath7 and the noise distribution , respectively .",
    "assumption * a3 . * is a regularity condition on the covariate distribution to ensure the uniqueness of the linear estimates .",
    "assumption * a4 .",
    "* is a condition that can be included in the algorithm and checked along with the subset cardinality , @xmath150 .",
    "if @xmath40 is given , it can be computed directly , otherwise it can be approximated using @xmath151 . in some cases , such as when @xmath7 is strongly convex , * a4 .",
    "* will be satisfied without enforcement due to problem structure .    to show consistency of @xmath20 under the @xmath21 metric",
    ", we first show consistency of @xmath152 and its derivatives under the @xmath21 metric in theorem [ thm : fk ] .",
    "this is very close to theorem 1 of @xcite for treed linear models , although we need to modify it to allow partitions with an arbitrarily large number of faces .",
    "[ thm : fk ] suppose that assumptions * a1 .",
    "* through * a4 .",
    "then , @xmath153 in probability as @xmath154 .",
    "the cap algorithm is similar to the support algorithm of @xcite , except the refitting step of cap allows partition subsets to be polyhedra with up to @xmath121 faces .",
    "theorem [ thm : fk ] is analogous to theorem 1 of @xcite ; to prove our theorem , we modify parts of the proof in @xcite that rely on a fixed number of polyhedral faces . as such , we first need to modify lemma 12.27 of @xcite .",
    "[ lem:12.27 ] suppose that * a2 .",
    "* holds and that there exists a @xmath155 where @xmath156 .",
    "then , for every compact set @xmath157 in @xmath40 and every @xmath158 and @xmath159 , @xmath160    to prove this lemma , we only need to lift the restriction on the number of faces of the polyhedron @xmath90 from being bounded by a fixed @xmath161 to @xmath162 .",
    "first , we note that @xmath163 implies that @xmath164 following the proof in @xcite , we note that @xmath165 for a fixed constant @xmath166 depending on assumption 6 . since @xmath167 , the conclusion holds .    with lemma [ lem:12.27 ] ,",
    "the proof of theorem [ thm : fk ] follows directly from the arguments of @xcite .",
    "using the results from theorem [ thm : fk ] , extension to consistency for @xmath20 under the @xmath21 metric is fairly simple ; this is given in theorem [ thm : consistency ] .",
    "[ thm : consistency ] suppose that assumptions * a1 .",
    "* through * a4 .",
    ". then , @xmath168 in probability as @xmath154 .",
    "fix @xmath169 ; let @xmath170 be the diameter of @xmath40 . choose @xmath116 such that @xmath171 fix a @xmath172 net over @xmath40 such that at least one point of the net sits in @xmath90 for each @xmath173",
    "let @xmath174 be the number of points in the net and let @xmath175 be a point .",
    "then , @xmath176",
    "the terminal model produced by the cap algorithm often overfits the data and is computationally more intensive than necessary . in this section ,",
    "we derive a generalized cross - validation method to select the best model from all of those produced by cap , @xmath54 .",
    "we then propose an approximate algorithm , fast cap , that requires substantially less computation than the original algorithm .",
    "cross - validation is a method to assess the predictive performance of statistical models and is routinely used to choose tunable parameters . in this case , we would like to choose the cardinality of the partition , @xmath15 . as a fast approximation to leave - one - out cross - validation , we use generalized cross - validation  @xcite . in a linear regression",
    "setting , @xmath177where @xmath178 is the @xmath179 diagonal element of the hat matrix , @xmath180 , @xmath181 is the estimator conditioned on all of the data minus element @xmath182 , and @xmath183 is the degrees of freedom .    a given model @xmath103 is generated by a collection of linear models .",
    "a similar type approximation to leave - one - out cross - validation can be used to select the model size .",
    "the model @xmath103 is defined by @xmath184 , the partition , and the hyperplanes @xmath43 , which were generated by the partition .",
    "let @xmath185 be the collection of hyperplanes generated when observation @xmath182 is removed ; notice that if @xmath186 , only @xmath187 changes .",
    "let @xmath188 be the estimator for model @xmath103 with observation @xmath182 removed . using the derivation in equation ( [ eq : lineargcv ] ) , @xmath189where , in a slight abuse of notation",
    ", @xmath190 is the diagonal entry of the hat matrix for subset @xmath66 corresponding to element @xmath182 , and @xmath191to select @xmath15 , we find the @xmath15 that minimizes the right hand side of equation ( [ eq : gcv ] ) .",
    "although more computationally intensive than traditional generalized cross - validation , the computational complexity for cap generalized cross - validation is similar to that of the cap split selection step .",
    "the cap algorithm offers two main computational bottlenecks .",
    "first , it searches over all cardinal directions , and only cardinal directions , to produce candidate models .",
    "second , it keeps generating models until no subsets can be split without one having less than the minimum number of observations . in most cases ,",
    "the optimal number of components is much lower than the terminal number of components .    to alleviate the first problem",
    ", we suggest using @xmath192 random projections as a basis for search . using ideas similar to compressive",
    "sensing , each projection @xmath193 for @xmath194 .",
    "then we search along the direction @xmath195 rather than @xmath196 .",
    "when we expect the true function to live in a lower dimensional space , as is the case with superfluous covariates , we can set @xmath197 .",
    "we solve the second problem by modifying the stopping rule .",
    "instead of fulling growing the tree until each subset has less than @xmath198 observations , we use generalized cross - validation .",
    "we grow the tree until the generalized cross - validation value has increased in two consecutive iterations or each subset has less than @xmath198 observations .",
    "as the generalized cross - validation error is usually concave in @xmath15 , this heuristic often offers a good fit at a fraction of the computational expense of the full cap algorithm .",
    "the fast cap algorithm has the potential to substantially reduce the @xmath199 factor by halting the model generation long before @xmath15 reaches @xmath121 .",
    "since every feasible partition is searched for splitting , the computational complexity grows as @xmath66 gets larger .",
    "the fast cap algorithm is summarized as follows .",
    "[ [ fast - convex - adaptive - partitioning - fast - cap ] ] fast convex adaptive partitioning ( fast cap ) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    1 .",
    "* initialize . * as in cap .",
    "* split . * 1 .",
    "_ generate candidate splits .",
    "_ generate candidate model @xmath65 by 1 ) fixing a subset @xmath66 , 2 ) generating a random direction @xmath67 with @xmath200 , and 3 ) dyadically dividing the data as follows : * set @xmath201 , @xmath202 and @xmath89 * set @xmath203 + then new hyperplanes are fit to each of the new subsets .",
    "this is done for @xmath69 knots , @xmath192 dimensions and @xmath15 subsets .",
    "2 .   _ select split . _ as in cap .",
    "3 .   * refit . * as in cap .",
    "4 .   * stopping conditions . *",
    "let @xmath204 be the generalized cross - validation error for model @xmath103 .",
    "stop if @xmath205 and @xmath206 . then select final model as in cap .",
    "in this section , we empirically analyze the performance of cap .",
    "there are no benchmark problems for multivariate convex regression , so we analyze the predictive performance , runtime , sensitivity to tunable parameters and rates of convergence on a set of synthetic problems .",
    "we then apply cap to value function approximation for pricing american basket options .",
    "we apply cap to two synthetic regression problems to demonstrate predictive performance and analyze sensitivity to tunable parameters .",
    "the first problem has a non - additive structure , high levels of covariate interaction and moderate noise , while the second has a simple univariate structure embedded in a higher dimensional space and low noise .",
    "low noise or noise free problems often occur when a highly complicated convex function needs to be approximated by a simpler one  @xcite .",
    "[ [ problem-1 ] ] problem 1 + + + + + + + + +    here @xmath207 .",
    "set @xmath208 where @xmath125 .",
    "the covariates are drawn from a 5 dimensional standard gaussian distribution , @xmath209 .",
    "[ [ problem-2 ] ] problem 2 + + + + + + + + +    here @xmath210 .",
    "set @xmath211 where @xmath212 was randomly drawn from a dirichlet(1,@xmath213,1 ) distribution , @xmath214 we set @xmath215 .",
    "the covariates are drawn from a 10 dimensional standard gaussian distribution , @xmath216 .",
    "we compared the performance of cap and fast cap to other regression methods on problems 1 and 2 .",
    "the only other convex regression method included was least squares regression ( lse ) ; it was implemented with the cvx convex optimization solver .",
    "the general methods included gaussian processes  @xcite , a widely implemented bayesian nonparametric method , and two adaptive methods : tree regression with constant values in the leaves and multivariate adaptive regression splines ( mars )  @xcite .",
    "tree regression was run through the matlab function classregtree .",
    "mars was run through the matlab package areslab .",
    "gaussian processes were run with the matlab package gpml .",
    "parameters for cap and fast cap were set as follows .",
    "the log scale parameter set as @xmath114 and the number of knots was set as @xmath217 for both . in fast cap ,",
    "the number of random search directions was set to be @xmath70 .",
    "all methods were given a maximum runtime of 90 minutes , after which the results were discarded .",
    "methods were run on 10 random training sets and tested on the same testing set .",
    "average runtimes and predictive performance are given in table [ tab : synthetic ] .",
    "l | r@.l | r@.l | r@.l | r@.l | r@.l | r@.l | r@.l   +   + method & & & & & & & + cap & * 1 * & * 5884 * & * 0 * & * 6827 * & * 0 * & * 2740 * & 0 & 1644 & * 0 * & * 0927 * & * 0 * & * 0629 * & * 0 * & * 0450 * + fast cap & 1 & 8661 & 0 & 7471 & 0 & 3197 & * 0 * & * 1526 * & 0 & 1356 & 0 & 0724 & 0 & 0566 + lse & 15 & 8340 & 9 & 5970 & 18 & 0701 & 9,862 & 4602 & & & + tree & 12 & 2794 & 9 & 8356 & 6 & 7606 & 5 & 3478 & 4 & 1230 & 2 & 9173 & 2 & 3152 + gp & 8 & 5056 & 13 & 5495 & 6 & 8472 & 3 & 7610 & 2 & 2928 & 1 & 2058 & + mars & 8 & 3517 & 8 & 0031 & 6 & 8813 & 6 & 2618 & 5 & 9809 & 5 & 8558 & 5 & 8234 +   + method & & & & & & & + cap & 0 & 0159 & 0 & 0138 & 0 & 0110 & * 0 * & * 0018 * & 0 & 0012 & * 0 * & * 0007 * & * 0 * & * 0003 * + fast cap & 0 & 0159 & 0 & 0138 & 0 & 0090 & * 0 * & * 0018 * & * 0 * & * 0011 * & * 0 * & * 0007 * & * 0 * & * 0003 * + lse & 0 & 6286 & 0 & 2935 & 31 & 2426 & & & & + tree & 0 & 1372 & 0 & 1129 & 0 & 0928 & 0 & 0797 & 0 & 0670 & 0 & 0552 & 0 & 0495 + gp & * 0 * & * 0109 * & * 0 * & * 0063 * & * 0 * & * 0039 * & 0 & 0027 & 0 & 0047 & 0 & 0076 & + mars & 0 & 0205 & 0 & 0140 & 0 & 0120 & 0 & 0110 & 0 & 0105 & 0 & 0102 & 0 & 0100 +   +   +   + method & & & & & & & + cap & 0 & 15 sec & 0 & 24 sec & 0 & 78 sec & 1 & 34 sec & 2 & 18 sec & 4 & 33 sec & 9 & 31 sec + fast cap & 0 & 04 sec & 0 & 07 sec & 0 & 15 sec & 0 & 30 sec & 0 & 57 sec & 1 & 14 sec & 2 & 06 sec + lse & 1 & 56 sec & 10 & 17 sec & 226 & 20 sec & 43 & 37 min & & & + tree & 0 & 06 sec & 0 & 02 sec & 0 & 04 sec & 0 & 09 sec & 0 & 19 sec & 0 & 49 sec & 1 & 15 sec + gp & 0 & 22 sec & 0 & 35 sec & 1 & 35 sec & 5 & 07 sec & 22 & 03 sec & 248 & 72 sec & + mars & 0 & 22 sec & 0 & 34 sec & 0 & 76 sec & 1 & 81 sec & 3 & 95 sec & 16 & 65 sec & 56 & 19 sec +   + method & & & & & & & + cap & 0 & 05 sec & 0 & 25 sec & 2 & 15 sec & 6 & 35 sec & 10 & 06 sec & 21 & 06 sec & 46 & 50 sec + fast cap & 0 & 02 sec & 0 & 03 sec & 0 & 08 sec & 0 & 13 sec & 0 & 25 sec & 0 & 89 sec & 2 & 03 sec + lse & 1 & 86 sec & 15 & 13 sec & 339 & 16 sec & & & & + tree & 0 & 02 sec & 0 & 03 sec & 0 & 07 sec & 0 & 14 sec & 0 & 27 sec & 0 & 71 sec & 1 & 53 sec + gp & 0 & 15 sec & 0 & 34 sec & 1 & 46 sec & 4 & 93 sec & 23 & 13 sec & 264 & 77 sec & + mars & 0 & 72 sec & 0 & 48 sec & 1 & 38 sec & 3 & 43 sec & 8 & 01 sec & 33 & 29 sec & 98 & 75 sec +    unsurprisingly , the non - convex regression methods did poorly compared to cap and fast cap , particularly in the higher noise setting .",
    "gaussian processes offered the best performance of that group , but their computational complexity scales like @xmath0 ; this computational times of more than 90 minutes for @xmath218 .",
    "more surprisingly , however , the lse did extremely poorly .",
    "this can be attributed to overfitting , particularly in the boundary regions ; this phenomenon can be seen in figure [ fig : capvlse ] as well . while the natural response to overfitting is to apply a regularization penalty to the hyperplane parameters , implementation in this setting is not straightforward .",
    "we have tried implementing @xmath219 penalties on the hyperplane coefficients , but tuning the parameters quickly became computationally infeasible due to runtime issues with the lse .",
    "although cap and fast cap had similar predictive performance , their runtimes often differed by an order of magnitude with the largest differences on the biggest problem sizes . based on this performance , we would suggest using fast cap on larger problems rather than the full cap algorithm .",
    "treed linear models are a popular method for regression and classification .",
    "they can be easily modified to produce a convex regression estimator by taking the maximum over all of the linear models .",
    "cap differs from existing treed linear models in how the partition is refined .",
    "first , subset splits are selected based on global reduction of error .",
    "second , the partition is refit after a split is made . to investigate the contributions of each step , we compare to treed linear models generated by : 1 ) local error reduction as an objective for split selection and no refitting , 2 ) global error reduction as an objective function for split selection and no refitting , and 3 ) local error reduction as an objective for split selection along with refitting . all estimators based on treed linear models are generated by taking the maximum over the set of linear models in the leaves .",
    "we wanted to determine which properties led to a low variance estimator with low predictive error . by low variance ,",
    "we mean that changes in the training set do not lead to large changes in predictive error . to do this , we compared the performance of these methods on problems 1 and 2 over 10 different training sets and a single testing set .",
    "all treed linear model parameters were the same as those for cap .",
    "we viewed a model with local subset split selection and no refitting as a baseline .",
    "we compared both the average squared predictive error and the variance of that error between training sets .",
    "percentages of average error and variance reduction are displayed in table [ tab : treedlinearmodel ] .",
    "average predictive error is displayed in figure [ fig : treedlinearmodel ] .",
    "l | r@.l | r@.l | r@.l | r@.l | r@.l | r@.l | r@.l   +   + method & & & & & & & + refitting & 48 & 65% &  58 & 95% & 32 & 62% &  61 & 76% &  73 & 04% &  74 & 77% &  70 & 01 % + global selection & 24 & 67% & 34 & 85% & 21 & 32% & 23 & 46% & 29 & 40% & 30 & 48% & 19 & 23% + cap & 68 & 25% & 69 & 81% & 74 & 74% & 76 & 97% & 80 & 18% & 81 & 40% & 81 & 04% +   + method & & & & & & & + refitting & 0 & 0% & 0 & 0% & -17 & 73% & 71 & 48% & 78 & 36% & 79 & 67% & 77 & 05% + global selection & 0 & 0% & 0 & 0% & -4 & 36% & 17 & 69% & 15 & 22% & 25 & 04% & 9 & 74% + cap & 0 & 0% & 0 & 0% & -17 & 10% & 71 & 70% & 75 & 60% & 81 & 66% & 86 & 21% +   +   +   + method & & & & & & & + refitting & 19 & 16% & 65 & 00% & -243 & 33% & -4 & 03% & -163 & 40% & 64 & 86% & -18 & 88% + global selection & 38 & 41% & 68 & 78% & -17 & 84% & 61 & 34% & 24 & 51% & 91 & 44% & 75 & 97% + cap & 96 & 89% & 92 & 72% & 68 & 74% & 97 & 05% & 74 & 85% & 95 & 29% & 63 & 17% +   + method & & & & & & & + refitting & 0 & 0% & 0 & 0% & -61 & 34% & 44 & 75% & 94 & 16% & 73 & 93% & 75 & 42% + global selection & 0 & 0% & 0 & 0% & -19 & 84% & -223 & 58% & -209 & 92% & -8 & 29% & -7 & 17% + cap & 0 & 0% & 0 & 0% & -76 & 78% & 52 & 44% & 89 & 30% & 30 & 18% & 15 & 16% +        table [ tab : treedlinearmodel ] shows that global split selection and refitting are both beneficial , but in different ways .",
    "refitting dramatically reduces predictive error , but can variance to the estimator in noisy settings .",
    "global split selection modestly reduces predictive error but can reduce variance in noisy settings , like problem 1 .",
    "the combination of the two produces cap , which has both low variance and high predictive accuracy .      in this subsection",
    ", we empirically examine the effects of the two tunable parameters , the log factor , @xmath60 , and the number of knots , @xmath69 .",
    "the log factor controls the minimal number of elements in each subset by setting @xmath220 , and hence it controls the number of subsets , @xmath15 , at least for large enough @xmath116 .",
    "increasing @xmath60 allows the potential accuracy of the estimator to increase , but at the cost of greater computational time due to the increase in possible values for @xmath15 and the larger number of possibly admissible sets generated in the splitting step of cap .",
    "we compared values for @xmath60 ranging from @xmath221 to @xmath222 on problems 1 and 2 with sample sizes of @xmath223 and @xmath224 .",
    "results are displayed in figure [ fig : dcompare ] .",
    "note that error may not be strictly decreasing with @xmath60 because different subsets are proposed under each value . additionally , fast cap is a randomized algorithm so variance in error rate and runtime is to be expected .",
    "empirically , once @xmath225 , there was little substantive error reduction in the models , but the runtime increased as @xmath226 for the full cap algorithm .",
    "since @xmath60 controls the maximum partition size , @xmath227 , and a linear regression is fit @xmath228 times , the expected increase in the runtime should only be @xmath229 .",
    "we believe that the extra empirical growth comes from an increased number of feasible candidate splits . in the fast cap algorithm , which terminates after generalized cross - validation gains cease to be made , we see runtimes leveling off with higher values of @xmath60 . based on these results , we believe that setting @xmath230 offers a good balance between fit and computational expense .",
    "the number of knots , @xmath69 , determines how many possible subsets will be examined during the splitting step . like @xmath60 , an increase in @xmath69",
    "offers a better fit at the expense of increased computation .",
    "we compared values for @xmath60 ranging from @xmath231 to @xmath232 on problems 1 and 2 with sample sizes of @xmath223 and @xmath224 .",
    "results are displayed in figure [ fig : lcompare ] .",
    "the changes in fit and runtime are less dramatic with @xmath69 than they are with @xmath60 .",
    "after @xmath233 , the predictive error rates almost completely stabilized .",
    "runtime increased as @xmath234 as expected . due to the minimal increase in computation",
    ", we feel that @xmath217 is a good choice for most settings .",
    "although theoretical rates of convergence are not yet available for cap , we are able to empirically examine them . rates of convergence for multivariate convex regression have only been studied in two articles of which we are aware .",
    "first , @xcite studied rates of convergence for an estimator that is created by first smoothing the data , then evaluating the smoothed data over an @xmath6-net , and finally convexifying the net of smoothed data by taking the convex hull .",
    "they showed that the convexify step preserved the rates of the smoothing step . for most smoothing algorithms ,",
    "these are minimax nonparametric rates , @xmath235 with respect to the empirical @xmath219 norm . in the second article",
    ", @xcite showed adaptive rates for a bayesian model that places a prior over the set of all piecewise linear functions .",
    "specifically , they showed that if the true mean function @xmath7 actually maps a @xmath236-dimensional linear subspace of @xmath40 to @xmath237 , that is @xmath238then their model achieves rates of @xmath239 with respect to the empirical @xmath219 norm .",
    "empirically , we see these types of adaptive rates with cap .    .slopes for linear models fit to @xmath240 vs. @xmath241 in figure [ fig : rates ] .",
    "expected slopes are given when : 1 ) rates are with respect to full dimensionality , @xmath70 , and 2 ) rates are with respect to dimensionality of linear subspace , @xmath236 .",
    "empirical slopes are fit to mean squared error generated by cap and fast cap .",
    "note that all empirical slopes are closest to those for linear subspace rates rather than those for full dimensionality rates . [ cols=\"<,>,<,>,<\",options=\"header \" , ]     results are displayed in table [ tab : options ] .",
    "we found that cap and fast cap gave state of the art performance without the difficulties associated with linear functions , such as choosing basis functions and regularization parameters .",
    "we observed a decline in the performance of least squares as the number of assets grew due to overfitting .",
    "ridge regularization greatly improved the least squares performance as the number of assets grew .",
    "tree regression did poorly in all settings , likely due to overfitting in the presence of the non - symmetric error distribution generated by the geometric brownian motion .",
    "these results suggest that cap is robust even in less than ideal conditions , such as when data have heteroscedastic , non - symmetric error distributions .",
    "again , we noticed that while the performances of cap and fast cap were comparable , the runtimes were about an order of magnitude different . on the larger problems , runtimes for fast cap were similar to those for unregularized least squares .",
    "this is likely because the number of covariates in the least squares regression grew like @xmath242 , while all linear regressions in cap only had @xmath243 covariates .",
    "in this article , we presented convex adaptive partitioning ( cap ) , a computationally efficient , theoretically sound and empirically robust method for regression subject to a convexity constraint .",
    "cap is the first convex regression method to scale to large problems , both in terms of dimensions and number of observations . as such",
    ", we believe that it can allow the study of problems that were once thought to be computationally intractable .",
    "these include econometrics problems , like estimating consumer preference or production functions in multiple dimensions , approximating complex constraint functions for convex optimization , or creating convex value - to - go functions or response surfaces that can be easily searched in stochastic optimization .",
    "our preliminary results are encouraging , but some important questions remain unanswered .    1 .",
    "what are the convergence rates for cap ?",
    "are they adaptive , as they empirically seem to be ? 2 .",
    "the current splitting proposal is effective but cumbersome .",
    "are there less computationally intensive ways to refine the current partition ? 3 .",
    "the modified stopping in fast cap provides substantially reduced runtimes with little performance degradation compared to cap .",
    "can this rule or a similarly efficient one be theoretically justified ?",
    "allon , g. , beenstock , m. , hackman , s. , passy , u.  shapiro , a. 2007 , ` nonparametric estimation of concave production technologies by entropic methods ' , _ journal of applied econometrics _ * 22*(4 ) ,  795816 .",
    "chang , i .-",
    "s . , chien , l .- c . ,",
    "hsiung , c.  a. , wen , c .- c .",
    "wu , y .- j .",
    "2007 , ` shape restricted regression with random bernstein polynomials ' , _ lecture notes - monograph series : complex datasets and inverse problems : tomography , networks and beyond _ * 54 * ,  187202 .",
    "dobra , a.  gehrke , j. 2002 , secret : a scalable linear regression tree algorithm , _ in _ ` proceedings of the eighth acm sigkdd international conference on knowledge discovery and data mining ' , acm , pp .",
    "481487 .",
    "fraser , d. a.  s.  massam , h. 1989 , ` a mixed primal - dual bases algorithm for regression under inequality constraints .",
    "application to concave regression ' , _ scandinavian journal of statistics _ * 16*(1 ) ,  6574 .",
    "henderson , d.  j.  parmeter , c.  f. 2009 , imposing economic constraints in nonparametric regression : survey , implementation and extension , _ in _",
    "q.  li  j.  s. racine , eds , ` nonparametric econometric methods ( advances in econometrics ) ' , vol .  25 , emerald publishing group limited , pp .",
    "433469 .",
    "kim , j. , lee , j. , vandenberghe , l.  yang , c. 2004 , techniques for improving the accuracy of geometric - programming based analog circuit design optimization , _ in _ ` proceedings of the ieee international conference on computer aided design ' , pp .",
    "863870 .",
    "lim , e. 2010 , response surface computation via simulation in the presence of convexity constraints , _ in _",
    "b.  johansson , s.  jain , j.  montoya - torres , j.  hugan e.  ycesan , eds , ` proceedings of the 2010 winter simulation conference ' , pp .",
    "12461254 .",
    "meyer , m.  c. , hackstadt , a.  j.  hoeting , j.  a. 2011 , ` bayesian estimation and inference for generalised partial linear models using shape - restricted splines ' , _ journal of nonparametric statistics _",
    "p.  to appear .",
    "roy , s. , chen , w. , chen , c. c .- p .",
    "hu , y.  h. 2007 , ` numerically convex forms and their application in gate sizing ' , _ ieee transactions on computer - aided design of integrated circuits and systems _ * 26*(9 ) ,  16371647 .",
    "tsitsiklis , j.  n.  van  roy , b. 1999 , ` optimal stopping of markov processes : hilbert space theory , approximation algorithms , and an application to pricing high - dimensional financial derivatives ' , _ ieee transactions on automatic control _ * 44*(10 ) ,  18401851 ."
  ],
  "abstract_text": [
    "<S> we propose a new , nonparametric method for multivariate regression subject to convexity or concavity constraints on the response function . </S>",
    "<S> convexity constraints are common in economics , statistics , operations research , financial engineering and optimization , but there is currently no multivariate method that is computationally feasible for more than a few hundred observations . </S>",
    "<S> we introduce convex adaptive partitioning ( cap ) , which creates a globally convex regression model from locally linear estimates fit on adaptively selected covariate partitions . </S>",
    "<S> cap is computationally efficient , in stark contrast to current methods . </S>",
    "<S> the most popular method , the least squares estimator , has a computational complexity of @xmath0 . </S>",
    "<S> we show that cap has a computational complexity of @xmath1 and also give consistency results . </S>",
    "<S> cap is applied to value function approximation for pricing american basket options with a large number of underlying assets .    </S>",
    "<S> regression , shape constraint , convex regression , treed linear model , adaptive partitioning </S>"
  ]
}