{
  "article_text": [
    "the study of the dynamics or statistical properties of a system usually consists in making predictions of its behavior based on assumed microscopic laws such as , for example , using knowledge about its hamiltonian . however , ill posed and inverse problems can be found in a vast array of areas . typically , in these cases , the problem is not to find the behavior , but rather to obtain the microscopic laws that gave rise to it . among the many interesting questions that can be asked , we point out those about the structure of the law , its uniqueness and how well it can be determined based on partial information .    inverse problems of different degrees of difficulty that have been subject of recent intense research activity include rule extraction and learning in artificial systems ; pattern recognition , clustering and categorization problems ; to find out the sequence of amino acids that leads to a predetermined chemical activity ; obtaining the parameters of a dynamical system from the time series it generates ; obtaining renormalized hamiltonians from monte carlo renormalization group data etc .",
    "in dealing with these problems , techniques from statistics , combinatorial optimization , statistical mechanics , dynamical theory and other areas have been found useful to different degrees . in this paper",
    ", we deal with the problem of determining the hamiltonian of a spin glass from data about its metastable states ( ms ) , that is , _ learning _ a spin glass .",
    "related issues has been recently addressed by kanter and gotesdyner @xcite . in particular , they `` show(ed ) that static properties determine the dynamics for a large class of systems '' and asked whether `` classical spin systems with the same ms have the same hamiltonians '' .",
    "the affirmative answer , for a large class of systems , immediately calls for the following question  how hard is it to determine the hamiltonian from partial information about the ms ? in trying to answer this , we use ideas from learning in neural networks @xcite .",
    "a set of ms is used as a learning set in the _ student - teacher _ scenario . the teacher being the original classical fully connected spin system of hamiltonian @xmath2 .",
    "the student , another system of a similar structure @xmath3 , is our approximation for the teacher , being constructed from the ms data .    here",
    "we specialize to the case of an ising spin glass teacher whose couplings are drawn independently from the distribution @xmath4 .",
    "the self - couplings @xmath5 are set to zero .",
    "the training set is generated by letting a randomly chosen initial teacher configuration @xmath6 relax to the nearest local minimum @xmath7 according to a zero temperature aligning - field dynamics , @xmath8    both teacher and student spin systems are equivalent to @xmath0 fully connected perceptrons .",
    "the @xmath9-th student perceptron learns from a set of @xmath10 examples , @xmath11 .",
    "the input vector @xmath12 for site @xmath9 is obtained from the metastable configuration by setting to zero the @xmath9-th component ; that component is the desired output @xmath13 .",
    "the task of the learning process is to build a student with the same energy landscape of the teacher system or , equivalently , to estimate the hamiltonian parameters @xmath14 .",
    "we show that it is possible using only information contained in a small set of ms ( @xmath15 ) in comparison to the exponential number of spin - glass local minima ( @xmath16 @xcite ) .    in the online learning @xcite strategy ,",
    "each example is presented only once inducing a change in the synaptic weights in the following way , @xmath17 the function @xmath18 modulates the hebb term @xmath19 and characterizes different learning algorithms .",
    "the self - couplings are always set to @xmath20 .    the macroscopic description of the learning process involves the quantities @xmath21 ( the squared norm of the @xmath9-th student perceptron ) , @xmath22 ( the corresponding teacher squared norm ) .",
    "we also define the normalized teacher - student overlap @xmath23 which will be our performance measure as a function of the number of examples @xmath24 .",
    "the normalized teacher and student local fields at site @xmath9 are @xmath25 and @xmath26 .    in the simpler case of feed - forward networks , the update rule eq .",
    "( [ update ] ) leads , in the thermodynamic limit , to a set of coupled differential equations describing the order parameters learning dynamics @xcite . in order to proceed and obtain similar equations we need to make several approximations which will eventually be checked by simulations .",
    "first , we ignore the correlation among the minima and thereby do not take into account the effects that different choices of the particular sequence of ms will have .",
    "second , we assume self - averaging of the order parameters and site symmetric evolution ( @xmath27 ) . finally , we assume that the relevant features of @xmath28 can be incorporated into an uncorrelated teacher local field distribution @xmath29 .",
    "this obviously is not the case in the spin - glass problem .",
    "even if the true teacher local fields distribution @xmath30 was known , the distribution of local minima @xmath31 has special directions related in a complicated way to the vectors @xmath32 .",
    "it will be interesting , however , to compare the performance in the spin glass problem with the theoretical results for a simple approximation for the distribution of local fields suggested by palmer and pond @xcite : @xmath33 .",
    "although this has been proposed for the fields of global minima states , it is also a good approximation for local minima fields @xcite .",
    "the parameter @xmath34 is adjustable , and in our simulations we obtained @xmath35 .    within these approximations , instead of",
    "@xmath36 learning equations ( two for each site ) , we need only two : @xmath37 where @xmath38 .",
    "the joint density @xmath39 can be written as @xmath40 , with @xmath41^{-1/2 } \\exp\\left[- \\frac{1}{2}(h-\\rho b)^2/(1-\\rho^2 ) \\right]$ ] and @xmath30 being the particular teacher fields distribution to be studied .",
    "the asymptotic @xmath42 behavior of the ` order ' parameter @xmath43 is a quality measure for comparing the algorithms . in this limit",
    "we write @xmath44 where @xmath45 is the usual _ learning exponent _ considered in the literature .",
    "the internal fields in the spin glass examples are correlated in some unknown way .",
    "it is interesting to observe the effect of these correlations by comparing with the case where the teacher local fields are assumed to be independently distributed ( @xmath46 ) according to @xmath47 parametrized by @xmath48 and @xmath34 ; @xmath49 ensures normalization .",
    "the palmer ",
    "pond distribution is achieved by setting @xmath50 .",
    "the learning equations ( [ dyn ] ) are _ exact _ in the thermodynamic limit if the learning sets @xmath51 were generated by choosing random independent identically distributed examples whose teacher fields obey the above palmer  pond - like distribution ( this case will be denoted iid pp ) .",
    "standard analytical calculations @xcite in the @xmath52 limit , using the distribution ( [ ppond ] ) lead us to the following results :    * simple hebb rule * , @xmath53 : we obtained @xmath54 , independent of @xmath48 ; the prefactors as functions of @xmath48 and @xmath34 are given by @xmath55 ^ 2 \\frac{1}{4 s^2 } \\ : ;   \\end{aligned}\\ ] ]    * rosemblatt perceptron algorithm * , @xmath56 : the learning exponent is @xmath57 , with the following prefactors : @xmath58^{2/(r+3)}\\:;\\end{aligned}\\ ] ] with @xmath59 and @xmath60 ; @xmath61 and @xmath62 is the gamma function .",
    "all the @xmath63 integrals can be found by using @xmath64 starting from @xmath65 and @xmath66 .",
    "it is worth to note that , due to the behavior @xmath57 , the perceptron algorithm shows only partial learning in the limit @xmath67 ( as indicated by @xmath68 ) , that is , when @xmath30 goes to zero exponentially as @xmath69 , ( say , as @xmath70 ) .",
    "this condition relates to , but is weaker than , the case of distributions with a gap around @xmath71 discussed by reimann and van den broeck @xcite .",
    "another interesting point is that , since in principle @xmath48 may be any real non - negative number , the learning exponent @xmath45 can assume real values which are not simple fractions .",
    "the previous ubiquity of these fractions found in the literature reflects simply the particular choices for the small @xmath72 behavior of the distributions @xmath73 studied so far .",
    "we compare these analytical results with simulations for examples generated by the iid pp case with @xmath50 and @xmath74 , and also with simulation results for the spin - glass teacher , see fig.[fig1 ] .",
    "since the spin glass local minima distribution has structure and special directions ( related in an unknown way with the matrix @xmath14 ) , we expect that the results are only approximate .",
    "= 0.8    indeed , the simulations for examples distributed exactly according to ( [ ppond ] ) are in excellent agreement with the theoretical predictions , see table 1 and fig .",
    "[ fig1 ] ( insert ) .",
    "but in the simulations with the spin glass teacher , simple hebb learning stops at @xmath75 ( _ partial learning _ ) .",
    "analogously to the results of riegler _ et al . _",
    "@xcite , this can be attributed to the presence of special directions in @xmath76 not aligned with @xmath77 .",
    "this lack of robustness is a feature only of the simple hebb rule .",
    "the perceptron learning algorithm is robust to the correlations in the examples provided by the spin glass teacher : perfect learning is achieved in the @xmath78 limit .",
    "the learning exponent @xmath45 , however , seems to be @xmath79 instead of the theoretically expected value @xmath80 .",
    "this is a finite size effect which can not be eliminated by using larger systems due to the following non - uniform convergence phenomena . in the palmer ",
    "pond distribution , @xmath81 as @xmath82 , but in the sg simulations , @xmath30 assumes a finite value @xmath83 of order @xmath84 at this point due to finite size effects found in the replica symmetric calculation of roberts  @xcite , which is presumably wrong due to broken replica symmetry . ]",
    "a better description of the local field distribution is obtained by the form @xmath85 repeating the calculation with the above distribution we found that any finite parameter @xmath86 changes the large @xmath87 behavior , leading to @xmath88 , the same value found for the @xmath89 distribution .",
    "thus , learning the spin - glass hamiltonian is easier for finite @xmath0 .    .learning exponent @xmath45 .",
    "the first two columns were obtained by numerical simulations for increasing @xmath0 and making an extrapolation to @xmath90 ; the last two , by analytical calculations using the distribution eq .",
    "( [ ppond ] ) . [ cols=\"^,^,^,^,^\",options=\"header \" , ]",
    "the optimal performance for on - line learning in the class of distributions examined here is given by the prescription @xcite @xmath91 where @xmath92 , @xmath93 and @xmath94 .",
    "the distribution @xmath95 is obtained from the joint distribution @xmath96 by integration over @xmath97 , and the distribution @xmath98 is equal to @xmath99 .",
    "since optimal algorithms are obtained always for specific distributions , they could suffer from lack of robustness .",
    "this possibility seems not to be a serious problem in the absence of specific knowledge of , e.g. , @xmath48 and @xmath34 .",
    "we have done simulations with @xmath100 where @xmath101 and @xmath102 , which is the optimal algorithm for gaussian teacher local fields @xmath103 with unit variance @xcite .",
    "the examples , however , are generated with the palmer ",
    "pond distribution with parameters @xmath104 and with the spin glass teacher .",
    "although not optimal , the performance of the @xmath105 is better than standard algorithms , see fig .",
    "[ fig1 ] .",
    "thus , although derived for specific distributions of examples , optimal algorithms can be used successfully for other distributions .",
    "the robustness of the optimal algorithm arises as a very welcome property , since in real world problems the examples may be nontrivially distributed in an unknown manner .",
    "we studied the learning process in neural networks in a scenario at the midway between the simple distributions of examples studied so far in the literature @xcite and real world problems .",
    "the true distribution @xmath106 of ` examples ' ( local minima ) generated by the spin - glass is _ unknown _ , but the teacher system is yet _ realizable _ by the student network .",
    "we have compared the performance of standard algorithms in this spin - glass problem with theoretical and simulation results for examples with a palmer  pond distribution @xmath107 for the local fields .",
    "various extensions on this scenario can be devised : we may study teachers with more structured distribution of local minima such as hopfield networks .",
    "we expect that these hamiltonians are harder to learn since they have less local minima .",
    "for example , a ferromagnetic hamiltonian with only two global minima is unlearnable because many @xmath108 vectors are compatible with these two ` examples ' @xcite .",
    "another interesting extension is to learn from a teacher which generates examples at a non zero temperature .",
    "this corresponds to learning from ` noisy examples ' with a noise level depending in a non trivial way on the temperature @xmath109 .",
    "since it has been demonstrated that it is possible to learn perfectly from noisy examples @xcite , we expect that this task is also learnable .",
    "finally , we think that our work opens an unexplored learning scenario where the distribution of examples is generated dynamically by the teacher system but the teacher architecture is yet _ realizable _ by the student .",
    "another example could be the learning from examples generated from the attractor time - series of a recurrent perceptron @xcite .",
    "it is worthwhile to study these _ realizable _ cases since they define _ upper bounds _ for the performance achievable by neural networks .",
    "the approach of determining theoretical upper bounds for the efficiency of simple ( thermal or computational ) machines follows a long tradition in thermodynamics and statistical physics .",
    "p. riegler , m. biehl , s. solla and c. marangi , in : _ neural nets wirn vietrie-95 , proceedings of the 7th italian workshop on neural nets _ , m. marinaro and r. tagliaferri , eds . ,",
    "world scientific , singapore , 1996 ."
  ],
  "abstract_text": [
    "<S> we study the problem of determining the hamiltonian of a fully connected ising spin glass of @xmath0 units from a set of measurements , whose sizes needs to be @xmath1 bits . </S>",
    "<S> the student - teacher scenario , used to study learning in feed - forward neural networks , is here extended to spin systems with arbitrary couplings . </S>",
    "<S> the set of measurements consists of data about the local minima of the rugged energy landscape . </S>",
    "<S> we compare simulations and analytical approximations for the resulting learning curves obtained by using different algorithms .    ,    and    neural networks , generalization , spin glasses , inverse problems , on - line learning .    _ pacs number _ : 07.05.mh , 84.35.+i , 87.10+e , 02.50-r , 05.90+m . </S>"
  ]
}