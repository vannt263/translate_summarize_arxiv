{
  "article_text": [
    "estimates of the data gathered by a self - driving car start from at least 750 mb / s . with proper annotation or through an unsupervised learning scheme ,",
    "all of this data can become useful for training the object detection system or grid - occupancy system of a self - driving car .",
    "the resulting training set can lead to weeks or more of training time on a single cpu / gpu system .",
    "therefore , for such applications training time defines the most time consuming element of the workflow , and reduced training time is highly desirable .    to achieve significant reductions in training time ,",
    "the training must be distributed across multiple cpus / gpus with the goal of strong scaling : as more nodes are thrown at the problem , the training time should ideally decrease proportionally .",
    "there are two primary approaches to distributed stochastic gradient descent ( sgd ) for training deep neural networks : ( i ) synchronous all - reduce sgd based on a fast all - reduce collective communication operation @xcite , and ( ii ) asynchronous sgd using a parameter server @xcite .    both approaches ( i ) and ( ii ) have weaknesses at scale .",
    "synchronous sgd is penalized by straggling processors , underutilizes compute resources , and is not robust in the face of failing processors or nodes . on the other hand ,",
    "asynchronous approaches using parameter servers create a communication bottleneck and underutilize the available network resources , slowing convergence .",
    "individual researchers also have different numbers of nodes at their disposal , including compute and network resources .",
    "so determining the best approach for a given number of nodes , as well as the approach that scales to the most number of nodes , is of interest to practitioners with finite resources .",
    "we are concerned with the following questions :    1 .",
    "how fast do asynchronous and synchronous sgd algorithms converge at both the beginning of training ( large step sizes ) and at the end of training ( large step sizes ) ? 2 .",
    "how does the convergence of async .  and sync .",
    "sgd vary with the number of nodes ?    to compare the strengths and weaknesses of asynchronous and synchronous sgd algorithms , we train a modern resnet convolutional network @xcite on the imagenet dataset @xcite using various distributed sgd methods .",
    "we primarily compare synchronous all - reduce sgd , the recently proposed asynchronous elastic averaging sgd @xcite , as well as our own method , asynchronous gossiping sgd , based on an algorithm originally developed in a different problem setting @xcite .",
    "gossiping sgd is an asynchronous method that does not use a centralized parameter server , and in a sense , gossiping is a decentralized version of elastic averaging . we find that asynchronous sgd , including both elastic averaging and gossiping , exhibits the best scaling at larger step sizes and , perhaps counterintuitively , at smaller scales ( up to around 32 distributed nodes ) . for smaller step sizes and at larger scales ,",
    "all - reduce consistently converges to the most accurate solution faster than the asynchronous methods .",
    "in this section , we will describe the baseline synchronous and asynchronous sgd methods , as well as a recently proposed asynchronous method that is more scalable than its predecessor .",
    "we will use the following naming convention for sgd : @xmath0 are the parameters over which the objective is minimized , @xmath1 is the center parameter ( if applicable ) , @xmath2 is the step size , @xmath3 is the momentum , subscript @xmath4 refers to the @xmath4-th node out of @xmath5 total nodes , and subscript @xmath6 refers to the @xmath6-th ( minibatch ) iteration .",
    "additionally , @xmath7 will refer to the _ per - node _ minibatch size , whereas @xmath8 will refer to the _ aggregate _ minibatch size summed across all nodes .      in traditional synchronous all - reduce sgd",
    ", there are two alternating phases proceeding in lock - step : ( 1 ) each node computes its local parameter gradients , and ( 2 ) all nodes collectively communicate all - to - all to compute an aggregate gradient , as if they all formed a large distributed minibatch .",
    "the second phase of exchanging gradients forms a barrier and is the communication - intensive phase , usually implemented by an eponymous all - reduce operation .",
    "the time complexity of an all - reduction can be decomposed into latency - bound and bandwidth - bound terms .",
    "although the latency term scales with @xmath9 , there are fast ring algorithms which have bandwidth term independent of @xmath5 @xcite . with modern networks capable of handling bandwidth on the order of 110 gb / s combined with neural network parameter sizes on the order of 10100 mb",
    ", the communication of gradients or parameters between nodes across a network can be very fast .",
    "instead , the communication overhead of all - reduce results from its use of a synchronization barrier , where all nodes must wait for all other nodes until the all - reduce is complete before proceeding to the next stochastic gradient iteration .",
    "this directly leads to a straggler effect where the slowest nodes will prevent the rest of the nodes from making progress .",
    "examples of large - scale synchronous data parallel sgd for distributed deep learning are given in @xcite , @xcite , @xcite , and @xcite .",
    "we provide pseudocode for synchronous data - parallel sgd in algorithm [ alg : sync - sgd ] .",
    "initialize @xmath10 @xmath11 @xmath12 @xmath13      a different approach to sgd consists of each node asynchronously performing its own gradient updates and occasionally synchronizing its parameters with a central parameter store",
    ". this form of asynchronous sgd was popularized by `` hogwild '' sgd @xcite , which considered solving sparse problems on single machine shared memory systems . `` downpour '' sgd @xcite then generalized the approach to distributed sgd where nodes communicate their gradients with a central _",
    "parameter server_. the main weakness of the asynchronous parameter - server approach to sgd is that the workers communicate all - to - one with a central server , and the communication throughput is limited by the finite link reception bandwidth at the server .",
    "one approach for alleviating the communication bottleneck is introducing a delay between rounds of communication , but increasing the delay greatly decreases the rate of convergence @xcite .",
    "large scale asynchronous sgd for deep learning was first implemented in google distbelief @xcite and has also been implemented in @xcite ; large scale parameter server systems in the non - deep learning setting have also been demonstrated in @xcite and @xcite .",
    "elastic averaging sgd @xcite is a new algorithm belonging to the family of asynchronous parameter - server methods which introduces a modification to the usual stochastic gradient objective to achieve faster convergence .",
    "elastic averaging seeks to maximize the consensus between the center parameter @xmath1 and the local parameters @xmath14 in addition to the loss : @xmath15.\\end{aligned}\\ ] ] the elastic averaging algorithm is given in algorithm [ alg : elastic - sgd ] .",
    "the consensus objective of elastic averaging is closely related to the augmented lagrangian of admm , and the gradient update derived from the consensus objective was shown by @xcite to converge significantly faster than vanilla async sgd . however , as elastic averaging is a member of the family of asynchronous parameter - server approaches , it is still subject to a communication bottleneck between the central server and the client workers .    because recent published results indicate that elastic averaging dominates previous asynchronous parameter - server methods @xcite , we will only consider elastic averaging from this point on .",
    "# client code initialize @xmath10 @xmath16 @xmath17 @xmath18 @xmath19 @xmath20    # server code initialize @xmath21 @xmath22 @xmath23 @xmath24",
    "in a nutshell , the synchronous all - reduce algorithm consists of two repeating phases : ( 1 ) calculation of the local gradients at each node , and ( 2 ) exact aggregation of the local gradients via all - reduce . to derive gossiping sgd",
    ", we would like to replace the synchronous all - reduce operation with a more asynchronous - friendly communication pattern .",
    "the fundamental building block we use is a _",
    "gossip aggregation _",
    "algorithm @xcite , which combined with sgd leads to the gossiping sgd algorithm .",
    "asynchronous gossiping sgd was introduced in @xcite for the general case of a sparse communication graph between nodes ( e.g.  wireless sensor networks ) .",
    "the original problem setting of gossiping also typically involved synchronous rounds of communication , whereas we are most interested in asynchronous gossip .",
    "the mathematical formulation of the gossiping sgd update can also be derived by conceptually linking gossiping to elastic averaging .",
    "introduce a distributed version of the global consensus objective , in which the center parameter is replaced with the average of the local parameters :    @xmath25.\\end{aligned}\\ ] ]    the corresponding gradient steps look like the following : @xmath26 if we replace the distributed mean @xmath27 with the unbiased one - node estimator @xmath28 , such that @xmath29 and @xmath30=\\frac{1}{p}\\sum_{j=1}^p\\theta_{t , j}$ ] , then we derive the gossiping sgd update : @xmath31 to make this more intuitive , we describe a quantity related to the distributed consensus , the _ diffusion potential_. fix @xmath32 , and consider the synchronous gossip setting of purely calculating an average of the parameters and where there are no gradient steps . then the updated parameter",
    "after @xmath33 repeated gossip rounds , @xmath34 , can be represented as a weighted average of the initial parameter values @xmath35 . denoting the weighted contribution of @xmath35 toward @xmath34 by @xmath36 ,",
    "the diffusion potential is : @xmath37 it can be shown that repeated rounds of a form of gossiping reduces the diffusion potential by a fixed rate per round @xcite .    if @xmath38 is chosen uniformly as above , then the algorithm is equivalent to `` pull - gossip , '' i.e.  each node pulls or receives @xmath39 from one and only one other random node per iteration . on the other hand ,",
    "if we replace the `` one - node estimator '' with querying @xmath39 from multiple nodes , with the constraint that each @xmath40 is represented only once per iteration , then the algorithm becomes `` push - gossip , '' i.e.  each node pushes or sends its own @xmath14 to one and only one other random node , while receiving from between zero and multiple other nodes .",
    "push - gossiping sgd can be interpreted as an interleaving of a gradient step and a simplified push - sum gossip step @xcite .",
    "algorithms [ alg : pull - gossip - sgd ] and [ alg : push - gossip - sgd ] describe pull - gossiping and push - gossiping sgd respectively .",
    "initialize @xmath10 set @xmath41 choose a target @xmath40 @xmath42 average of @xmath43 @xmath19 @xmath20    initialize @xmath10 set @xmath41 choose a target @xmath40 send @xmath44 to @xmath4 ( ourselves ) and to @xmath40 @xmath42 average of received @xmath45 s",
    "@xmath19 @xmath20      our analysis of gossiping sgd is based on the analyses in @xcite .",
    "we assume that all processors are able to communicate with all other processors at each step .",
    "the main convergence result is the following :    let @xmath46 be a @xmath8-strongly convex function with @xmath47-lipschitz gradients .",
    "assume that we can sample gradients @xmath48 with additive noise with zero mean @xmath49 = 0 $ ] and bounded variance @xmath50 \\leq \\sigma^2 $ ] .",
    "then , running the asynchronous pull - gossip algorithm , with constant step size @xmath51 , the expected sum of squares convergence of the local parameters to the optimal @xmath52 is bounded by @xmath53 \\leq \\left ( 1 - \\frac{2\\alpha}{p } \\frac{ml}{m+l } \\right)^t \\vert \\theta_0 - \\theta_\\ast \\mathbf{1 } \\vert^2 + p \\alpha \\sigma^2 \\frac{m+l}{2ml}\\end{aligned}\\ ] ] furthermore , with the additional assumption that the gradients are uniformly bounded as @xmath54 , the expected sum of squares convergence of the local parameters to the mean @xmath55 is bounded by @xmath56 & \\leq \\left ( \\lambda \\left ( 1- \\frac{\\alpha \\ , m}{p } \\right ) \\right)^t \\vert \\theta_0 - \\bar{\\theta}_0 \\mathbf{1 } \\vert^2   + \\frac{\\lambda \\alpha^2 ( c^2+\\sigma^2)}{1 - \\lambda \\left ( 1- \\frac{\\alpha \\ , m}{p } \\right ) } \\\\",
    "\\text { where } \\lambda & = 1 - \\frac{2\\beta(1-\\beta)}{p } - \\frac{2 \\beta^2}{p}\\end{aligned}\\ ] ]    for the proofs , please see subsection ( 6.1 ) in the supplementary material .",
    "0.32   nodes with _ per - node _ minibatch size @xmath57 , and ( right ) @xmath58 nodes with _ per - node _ minibatch size @xmath59 , , title=\"fig : \" ]    0.32   nodes with _ per - node _ minibatch size @xmath57 , and ( right ) @xmath58 nodes with _ per - node _ minibatch size @xmath59 , , title=\"fig : \" ]     +    0.32   nodes with _ per - node _ minibatch size @xmath57 , and ( right ) @xmath58 nodes with _ per - node _ minibatch size @xmath59 , , title=\"fig : \" ]    0.32   nodes with _ per - node _ minibatch size @xmath57 , and ( right ) @xmath58 nodes with _ per - node _ minibatch size @xmath59 , , title=\"fig : \" ]    0.32   and _ per - node _ minibatch size @xmath59 . shown",
    "are : ( left ) @xmath60 nodes , ( middle ) @xmath61 nodes , and ( right ) @xmath62 nodes .",
    ", title=\"fig : \" ]    0.32   and _ per - node _ minibatch size @xmath59 . shown",
    "are : ( left ) @xmath60 nodes , ( middle ) @xmath61 nodes , and ( right ) @xmath62 nodes .",
    ", title=\"fig : \" ]    0.32   and _ per - node _ minibatch size @xmath59 . shown",
    "are : ( left ) @xmath60 nodes , ( middle ) @xmath61 nodes , and ( right ) @xmath62 nodes .",
    ", title=\"fig : \" ]     +    0.32   and _ per - node _ minibatch size @xmath59 . shown",
    "are : ( left ) @xmath60 nodes , ( middle ) @xmath61 nodes , and ( right ) @xmath62 nodes .",
    ", title=\"fig : \" ]    0.32   and _ per - node _ minibatch size @xmath59 . shown",
    "are : ( left ) @xmath60 nodes , ( middle ) @xmath61 nodes , and ( right ) @xmath62 nodes .",
    ", title=\"fig : \" ]    0.32   and _ per - node _ minibatch size @xmath59 . shown",
    "are : ( left ) @xmath60 nodes , ( middle ) @xmath61 nodes , and ( right ) @xmath62 nodes .",
    ", title=\"fig : \" ]      we implement the communication systems of gossiping sgd and other algorithms using message passing interface ( mpi ) @xcite .",
    "because we wanted to run our code in cluster computing environments with infiniband or more specialized interconnects , then targeting mpi was the easiest solution .",
    "we targeted our code to run on gpus , using the nvidia cuda 7.0 driver and using the cublas and cudnnv4 @xcite libraries for the core computational kernels .    for our experiments up to @xmath58 nodes , we use a local cluster of 16 machines , each one consisting of an nvidia kepler k80 dual gpu , an 8-core intel haswell e5 - 1680v2 cpu , and a mellanox connectx-3 fdr @xmath63 infiniband ( 56 gb / s ) nic .",
    "we utilize only one gpu per k80 .    for our larger scale experiments up to @xmath62 nodes , we used a gpu supercomputer with over 10,000 total nodes .",
    "nodes consist of an nvidia kepler k20x gpu and an 8-core amd bulldozer opteron 6274 cpu , and are connected by a cray gemini interconnect in a 3d torus configuration .",
    "we chose resnets @xcite for our neural network architecture ; specifically , we trained resnet-18 , which is small enough to train rapidly for experimentation , but also possesses features relevant to modern networks , including depth , residual layers , and batch normalization @xcite .",
    "we ran on the image classification problem of imagenet consisting of 1.28 million training images and 50,000 validation images divided into 1000 classes @xcite .",
    "our data augmentation is as follows : we performed multi - scale training by scaling the shortest dimension of images to between @xmath64 and @xmath65 pixels @xcite , we took random @xmath66 crops and horizontal flips , and we added pixelwise color noise @xcite .",
    "we evaluate validation loss and top-1 error on center crops of the validation set images with the shortest dimension scaled to @xmath64 pixels .    unless otherwise noted , we initialized the learning rate to @xmath67 , then we annealed it twice by a factor of @xmath68 . for our experiments with aggregate minibatch size @xmath69 , we annealed at exactly 150k and 300k iterations into training . for our experiments with larger aggregate minibatch sizes",
    ", we decreased the number of iterations at which the step size was annealed .",
    "we used nesterov momentum of @xmath70 and weight decay of @xmath71 . for elastic averaging , we set @xmath72 . for all - reduce and gossiping , we used a communication interval of @xmath73 , i.e.  communication occurred every iteration . for gossiping , we used both @xmath73 and @xmath74 ( the latter is recommended in @xcite ) .",
    "our first set of experiments compare all - reduce , elastic averaging , and push - gossiping at @xmath75 and @xmath58 with an aggregate minibatch size @xmath69 .",
    "the results are in figure [ fig : valid_p8_p16 ] .",
    "for @xmath75 , elastic averaging with a communication delay @xmath74 performs ahead of the other methods , interestingly , all - reduce has practically no synchronization overhead on the system at @xmath75 and is as fast as gossiping .",
    "all methods converge to roughly the same minimum loss value .    for @xmath58",
    ", gossiping converges faster than elastic averaging with @xmath74 , and both come ahead of all - reduce .",
    "additionally , elastic averaging with both @xmath73 and @xmath74 has trouble converging to the same validation loss as the other methods once the step size has been annealed to a small value ( @xmath76 in this case ) .",
    "we also perform larger scale experiments at @xmath60 nodes , @xmath61 nodes , and @xmath62 nodes in the gpgpu supercomputing environment . in this environment ,",
    "elastic averaging did not perform well so we do not show those results here ; pull - gossiping also performed better than push - gossiping , so we only show results for pull - gossiping .",
    "the results are in figure [ fig : valid_p32_p64 ] . at this scale ,",
    "we begin to see the scaling advantage of synchronous all - reduce sgd .",
    "one iteration of gossiping sgd is still faster than one iteration of all - reduce sgd , and gossiping works quickly at the initial step size . but",
    "gossiping sgd begins to converge much slower after the step size has annealed .",
    "we note that the training time of sgd can be thought of as the product @xmath77 .",
    "one observation we made consistent with @xcite was the following : letting synchronous all - reduce sgd run for many epochs , it will typically converge to a lower optimal validation loss ( or higher validation accuracy ) than either elastic averaging or gossiping sgd . we found that letting all - reduce sgd run for over 1 million iterations with a minibatch size of 256 led to a peak top-1 validation accuracy of @xmath78 .",
    "however , elastic averaging often had trouble breaking @xmath79 , as did gossiping when the number of nodes was greater than @xmath60 .",
    "in other words , at larger scales the asynchronous methods require more iterations to convergence despite lower wall - clock time per iteration .",
    "revisiting the questions we asked in the beginning :    1 .   _",
    "how fast do asynchronous and synchronous sgd algorithms converge at both the beginning of training ( large step sizes ) and at the end of training ( large step sizes ) ? _ + up to around 32 nodes , asynchronous sgd can converge faster than all - reduce sgd when the step size is large .",
    "when the step size is small ( roughly @xmath80 or less ) , gossiping can converge faster than elastic averaging , but all - reduce sgd converges most consistently .",
    "_ how does the convergence behavior of asynchronous and synchronous distributed sgd vary with the number of nodes ? _ + both elastic averaging and gossiping seem to converge faster than synchronous all - reduce sgd with fewer nodes ( up to 1632 nodes ) . with more nodes ( up to a scale of 100 nodes ) , all - reduce sgd can consistently converge to a high - accuracy solution , whereas asynchronous methods seem to plateau at lower accuracy . in particular , the fact that gossiping sgd does not scale as well as does synchronous sgd with more nodes suggests that the asynchrony and the pattern of communication , rather than the amount of communication ( both methods have low amounts of communication ) , are responsible for the difference in convergence .    in this work , we focused on comparing the scaling of synchronous and asynchronous sgd methods on a supervised learning problem on two platforms : a local gpu cluster and a gpu supercomputer .",
    "however , there are other platforms that are relevant for researchers , depending on what resources they have available .",
    "these other platforms include multicore cpus , multi - gpu single servers , local cpu clusters , and cloud cpu / gpu instances , and we would expect to observe somewhat different results compared to the platforms tested in this work .",
    "while our experiments were all in the setting of supervised learning ( imagenet image classification ) , the comparison between synchronous and asynchronous parallelization of learning may differ in other settings ; c.f .  recent results on asynchronous methods in deep reinforcement learning @xcite .",
    "additionally , we specifically used _ convolutional _ neural networks in our supervised learning experiments , which because of their high arithmetic intensity ( high ratio of floating point operations to memory footprint ) have a different profile from networks with many fully connected operations , including most recurrent networks .    finally , we exclusively looked at sgd with nesterov momentum as the underlying algorithm to be parallelized / distributed .",
    "adaptive versions of sgd such as rmsprop @xcite and adam @xcite are also widely used in deep learning , and their corresponding distributed versions may have additional considerations ( e.g.  sharing the squared gradients in @xcite ) .",
    "we thank kostadin ilov , ed f.  dazevedo , and chris fuson for helping make this work possible , as well as josh tobin for insightful discussions .",
    "we would also like to thank the anonymous reviewers for their constructive feedback .",
    "this research is partially funded by darpa award number hr0011 - 12 - 2 - 0016 and by aspire lab industrial sponsors and affiliates intel , google , hewlett - packard , huawei , lge , nvidia , oracle , and samsung .",
    "this research used resources of the oak ridge leadership computing facility at the oak ridge national laboratory , which is supported by the office of science of the u.s .",
    "department of energy under contract no .",
    "de - ac05 - 00or22725 .",
    "the analysis below loosely follow the arguments presented in , and use a combination of techniques appearing from [ 17,16,10,18,9 ] . + for ease of exposition and notation , we focus our attention on the case of univariate strongly convex function @xmath46 with lipschitz gradients .",
    "( since the sum of squares errors are additive in vector components , the arguments below generalize to the case of multivariate functions . )",
    "we assume that the gradients of the function @xmath46 can be sampled by all processors independently up to additive noise with zero mean and bounded variance .",
    "( gaussian noise satisfy these assumptions , for example . )",
    "we also assume a fully connected network where all processors are able to communicate with all other processors .",
    "additional assumptions are introduced below as needed .",
    "+ we denote by @xmath81 the vector containing all local parameter values at time step @xmath6 , we denote by @xmath52 the optimal value of the objective function @xmath46 , and we denote by @xmath82 the spatial average of the local parameters taken at time step @xmath6 .",
    "+ we derive bounds for the following two quantities :      where the expectation is taken with respect to both the `` pull '' parameter choice and the gradient noise term . in the literature [ 18 ] , the latter is usually referred to as `` agent agreement '' or `` agent consensus '' .",
    "we begin by analyzing the synchronous version of the pull - gossip algorithm described in algorithm 3 . for each processor @xmath4 , let @xmath86 denote the processor , chosen uniformly randomly from @xmath87 from which processor @xmath4 `` pulls '' parameter values .",
    "the update for each @xmath88 is given by        let @xmath46 be a @xmath8-strongly convex function with @xmath47-lipschitz gradients .",
    "assume that we can sample gradients @xmath48 with additive noise with zero mean @xmath49 = 0 $ ] and bounded variance @xmath50 \\leq \\sigma^2 $ ] .",
    "then , running the synchronous pull - gossip algorithm as outlined above with step size @xmath51 , the expected sum of squares convergence of the local parameters to the optimal @xmath52 is bounded by      note that this bound is characteristic of sgd bounds , where the iterates converge to within some ball around the optimal solution .",
    "it can be shown by induction that a decreasing step size schedule of @xmath91 can be used to achieve a convergence rate of @xmath92 .    for notational simplicity ,",
    "we denote @xmath93 , and drop the @xmath94 in the gradient term .",
    "we tackle the first quantity by conditioning on the previous parameter values and expanding as @xmath95 & = \\mathbb{e}\\left [ \\sum_{i=1}^p \\left ( \\theta_{t , i , j_i } - \\theta_\\ast \\right)^t \\left ( \\theta_{t , i , j_i } - \\theta_\\ast \\right ) \\ , \\big\\vert \\ , \\theta_{t } \\right ] \\label{eq : firstterm } \\\\ & - 2 \\alpha_{t } \\mathbb{e}\\left [ \\sum_{i=1}^p \\nabla f(\\theta_{t , i , j_i})^t ( \\theta_{t , i , j_i } - \\theta_\\ast ) \\ , \\big\\vert \\ , \\theta_{t } \\right ]",
    "\\label{eq : term2 } \\\\ & - 2 \\alpha_{t } \\mathbb{e}\\left [ \\sum_{i=1}^p \\xi_{t , i}^t \\left ( \\theta_{t , i , j_i } - \\theta_\\ast \\right ) \\ , \\big\\vert \\ , \\theta_{t } \\right ]   \\\\ & + \\alpha_{t}^2 \\left [ \\sum_{i=1}^p \\left ( \\nabla f(\\theta_{t , i , j_i } ) + \\xi_{t , i } \\right)^t \\left ( \\nabla f(\\theta_{t , i , j_i } ) + \\xi_{t , i } \\right ) \\ ,",
    "\\big\\vert \\ , \\theta_{t } \\right ]",
    "\\label{eq : lastterm}\\end{aligned}\\ ] ]    recalling that strongly convex functions satisfy [ 17 ] , @xmath96 , @xmath97 we can use this inequality , with @xmath98 and @xmath99 to bound the term in : @xmath100 \\\\ \\nonumber & \\leq - \\frac{2 \\alpha_{t } ml}{m+l } \\mathbb{e}\\left [ \\sum_{i=1}^p ( \\theta_{t , i , j_i } - \\theta_\\ast)^t ( \\theta_{t , i , j_i } - \\theta_\\ast ) \\ , \\big\\vert \\ ,",
    "\\theta_{t } \\right ] - \\frac{2 \\alpha_{t } } { m+l } \\mathbb{e}\\left [ \\sum_{i=1}^p \\nabla f(\\theta_{t , i ,",
    "j_i})^t \\nabla f(\\theta_{t , i , j_i } ) \\",
    ", \\big\\vert \\ , \\theta_{t } \\label{eq : stronglyconvex } \\right]\\end{aligned}\\ ] ]    using , and regrouping terms in - , we obtain @xmath95 & \\leq \\left ( 1 - 2 \\alpha_{t } \\frac{ml}{m+l } \\right ) \\mathbb{e}\\left [ \\sum_{i=1}^p \\left ( \\theta_{t , i , j_i } - \\theta_\\ast \\right)^t \\left ( \\theta_{t , i , j_i } - \\theta_\\ast \\right ) \\ , \\big\\vert \\ , \\theta_{t } \\right ] \\\\ & + \\left ( \\alpha_{t}^2 - 2 \\alpha_{t } \\frac{1}{m+l } \\right ) \\mathbb{e}\\left [ \\sum_{i=1}^p \\nabla f(\\theta_{t ,",
    "i , j_i})^t \\nabla f(\\theta_{t , i , j_i } ) \\ , \\big\\vert \\ , \\theta_{t } \\right ] \\label{eq : dropifalphasmall } \\\\ & + \\alpha^2_{t } \\mathbb{e}\\left [ \\sum_{i=1}^p \\xi_{t , i}^t \\xi_{t , i } \\ , \\big\\vert \\ , \\theta_{t } \\right ] \\label{eq : noiseterm}\\end{aligned}\\ ] ] in the above expression , we have dropped the terms linear in @xmath101 , using the assumption that these noise terms vanish in expectation .",
    "in addition , if the step size parameter @xmath2 is chosen sufficiently small , @xmath102 , then the second term in can also be dropped .",
    "the expression we must contend with is @xmath95 & \\leq \\left ( 1 - 2 \\alpha_{t } \\frac{ml}{m+l } \\right ) \\mathbb{e}\\left [ \\sum_{i=1}^p \\left ( \\theta_{t , i , j_i } - \\theta_\\ast \\right)^t \\left ( \\theta_{t , i , j_i } - \\theta_\\ast \\right ) \\ , \\big\\vert \\ , \\theta_{t } \\right ] + \\alpha^2_t p \\sigma^2 \\label{eq : recursionsync}\\end{aligned}\\ ] ]    using the definition of @xmath103 , we can verify that the following matrix relation holds .",
    "@xmath104 where the random matrix @xmath105 depends on the random index set @xmath106 of uniformly randomly drawn indices .",
    "@xmath105 has two entries in each row , one on the diagonal , and one appearing in the @xmath86th column , and is a right stochastic matrix but need not be doubly stochastic .",
    "+ we can express this matrix as @xmath107 and compute its second moment @xmath108 & = \\frac{1}{4 } \\ , \\mathbb{e}\\left [ \\left",
    "( i + \\sum_{i=1}^p e_i e_{j_i}^t \\right)^t \\left ( i + \\sum_{i=1}^p e_i e_{j_i}^t \\right ) \\right ] \\\\ & = \\frac{1}{2 } \\left ( i + \\frac{1}{p } \\mathbf{1 } \\mathbf{1}^t",
    "\\right ) \\\\ & = q \\begin{bmatrix } 1 & & & \\\\ &",
    "\\tfrac{1}{2 } & & \\\\ & & \\ddots & \\\\ & & & \\tfrac{1}{2 } \\end{bmatrix } q^t \\label{eq : eigendecomp}\\end{aligned}\\ ] ] where in the last line , the orthogonal diagonalization reveals that the eigenvalues of this matrix are bounded by @xmath109 .",
    "+ using , we can further simply to @xmath95 & \\leq \\left ( 1 - 2 \\alpha_{t } \\frac{ml}{m+l } \\right ) ( \\theta_t - \\theta_\\ast \\mathbf{1})^t \\mathbb{e}\\left [ m_i^t m_i \\right ] ( \\theta_t - \\theta_\\ast \\mathbf{1 } ) + \\alpha^2_t p \\sigma^2 \\\\ & \\leq \\left ( 1 - 2 \\alpha_{t } \\frac{ml}{m+l } \\right ) \\vert \\theta_t - \\theta_\\ast \\mathbf{1 } \\vert^2 + \\alpha^2_t p \\sigma^2\\end{aligned}\\ ] ] assuming a constant step size @xmath110 , the above recursion can be unrolled to derive the bound @xmath90 & \\leq \\left ( 1 - 2 \\alpha \\frac{ml}{m+l } \\right)^t \\vert \\theta_0 - \\theta_\\ast \\mathbf{1 } \\vert^2 + p \\alpha \\sigma^2\\frac{m+l}{2ml}\\end{aligned}\\ ] ] we note that the above bound is characteristic of sgd bounds , where the iterates converge to a ball around the optimal solution , whose radius now depends on the number of processors @xmath5 , in addition to the step size @xmath2 and the variance of the gradient noise @xmath111 .",
    "+      we provide similar analysis for the asynchronous version of the pull - gossip algorithm .",
    "as is frequently done in the literature , we model the time steps as the ticking of local clocks governed by poisson processes .",
    "more precisely , we assume that each processor has a clock which ticks with a rate @xmath112 poisson process . a master clock which ticks whenever a local processor clock ticks is then governed by a rate @xmath5 poisson process , and a time step in the algorithm is defined as whenever the master clock ticks . since each master clock tick corresponds to the tick of some local clock on processor @xmath4 , this in turn marks the time step at which processor @xmath4 `` pulls '' the parameter values from the uniformly randomly chosen processor @xmath86 . modeling",
    "the time steps by poisson processes provide nice theoretical properties , i.e. the inter - tick time intervals are i.i.d .",
    "exponential variables of rate @xmath5 , and the local clock @xmath4 which causes each master clock tick is i.i.d . drawn from @xmath87 , to name a few . for an in depth analysis of this model , and results that relate the master clock ticks to absolute time , please see [ 16 ] .",
    "+ the main variant implemented is the pull - gossip with fresh parameters in subsection 6.2.2 .",
    "the update at each time step is given by @xmath113 note in the implementation we use @xmath114 , however in the analysis we retain the @xmath115 parameter for generality .",
    "+ we have the following convergence result .",
    "let @xmath46 be a @xmath8-strongly convex function with @xmath47-lipschitz gradients .",
    "assume that we can sample gradients @xmath48 with additive noise with zero mean @xmath49 = 0 $ ] and bounded variance @xmath50 \\leq \\sigma^2 $ ] .",
    "then , running the asynchronous pull - gossip algorithm with the time model as described , with constant step size @xmath51 , the expected sum of squares convergence of the local parameters to the optimal @xmath52 is bounded by @xmath53 \\leq \\left ( 1 - \\frac{2\\alpha}{p } \\frac{ml}{m+l } \\right)^t \\vert \\theta_0 - \\theta_\\ast \\mathbf{1 } \\vert^2 + p \\alpha \\sigma^2 \\frac{m+l}{2ml}\\end{aligned}\\ ] ] furthermore , with the additional assumption that the gradients are uniformly bounded as @xmath54 , the expected sum of squares convergence of the local parameters to the mean @xmath55 is bounded by @xmath56 & \\leq \\left ( \\lambda \\left ( 1- \\frac{\\alpha \\ , m}{p } \\right ) \\right)^t \\vert \\theta_0 - \\bar{\\theta}_0 \\mathbf{1 } \\vert^2   + \\frac{\\lambda \\alpha^2 ( c^2+\\sigma^2)}{1 - \\lambda \\left ( 1- \\frac{\\alpha \\ , m}{p } \\right ) } \\\\",
    "\\text { where } \\lambda & = 1 - \\frac{2\\beta(1-\\beta)}{p } - \\frac{2 \\beta^2}{p}\\end{aligned}\\ ] ]      for simplicity of notation , we denote @xmath116 .",
    "we can write the asynchronous iteration step in matrix form as @xmath117 the random matrix @xmath118 depends on the indices @xmath4 and @xmath86 , both of which are uniformly randomly drawn from @xmath87 .",
    "for notational convenience we will drop the subscripts , but we keep in mind that the expectation below is taken with respect to the randomly chosen indices .",
    "we can express the matrix as @xmath119 and compute its second moment as @xmath120 & = \\left ( 1 - \\frac{2 \\beta ( 1-\\beta)}{p } \\right ) \\mathbf{i } + \\frac{2 \\beta ( 1-\\beta)}{p^2 } \\mathbf{1 } \\mathbf{1}^t \\\\ & = q \\begin{bmatrix } 1 & & & \\\\ & 1-\\frac{2\\beta(1-\\beta)}{p } & & \\\\ & & \\ddots & \\\\ & & & 1-\\frac{2\\beta(1-\\beta)}{p } \\end{bmatrix } q^t \\label{eq : asyncdiag}\\end{aligned}\\ ] ] the orthogonal diagonalization reveals that the eigenvalues of this matrix are bounded by @xmath121 .",
    "+ using , we can expand and bound the expected sum of squares deviation by @xmath122 \\nonumber \\\\ & = \\mathbb{e } \\left [ ( ( \\theta_t - \\theta_\\ast \\mathbf{1 } ) -",
    "\\alpha_t g_{t})^t d^td ( ( \\theta_t - \\theta_\\ast \\mathbf{1 } ) - \\alpha_t g_{t } ) \\right ] \\\\ & \\leq \\mathbb{e } \\left [ \\vert ( \\theta_t - \\theta_\\ast \\mathbf{1 } ) - \\alpha_t g_{t } \\vert^2 \\right ] \\\\",
    "& = \\vert \\theta_t - \\theta_\\ast \\mathbf{1 } \\vert^2 - \\frac{2 \\alpha_t}{p } \\sum_{i=1}^p \\nabla f(\\theta_{t , i})^t ( \\theta_{t , i } - \\theta_\\ast ) + \\frac{\\alpha_t^2}{p } \\sum_{i=1}^p \\nabla f(\\theta_{t , i})^t \\nabla f(\\theta_{t , i } ) + \\frac{\\alpha_t^2}{p } \\sum_{i=1}^p \\xi_{t , i}^t \\xi_{t , i } \\label{eq : asyncrecursion}\\end{aligned}\\ ] ] where in the last line , we have dropped terms that are linear in @xmath101 by using the zero mean assumption . making use of the strong convexity inequality , we can bound the second term in the above sum by @xmath123 and rearrange the terms in to derive @xmath124 & \\leq \\left(1 - \\frac{2 \\alpha_t}{p } \\frac{ml}{m+l } \\right ) \\vert \\theta_t - \\theta_\\ast \\mathbf{1 } \\vert^2 \\\\ & + \\left ( \\frac{\\alpha_t^2}{p } - \\frac{2 \\alpha_t}{p } \\frac{1}{m+l } \\right ) \\sum_{i=1}^p \\nabla f(\\theta_{t , i})^t \\nabla f(\\theta_{t , i } ) \\label{eq : asyncdisappear } \\\\ & + \\alpha_t^2 \\sigma^2 \\end{aligned}\\ ] ] the term in can be dropped if @xmath125 is chosen sufficiently small , with the now familiar requirement that @xmath126 .",
    "assuming this is the case , we have @xmath124 & \\leq \\left(1 - \\frac{2 \\alpha_t}{p } \\frac{ml}{m+l } \\right ) \\vert \\theta_t - \\theta_\\ast \\mathbf{1 } \\vert^2 + \\alpha_t^2 \\sigma^2   \\label{eq : asyncrecurse } \\end{aligned}\\ ] ]    assuming a constant step size @xmath110 , we can use the law of iterated expectations to unroll the recursion , giving @xmath53 \\leq \\left ( 1 - \\frac{2\\alpha}{p } \\frac{ml}{m+l } \\right)^t \\vert \\theta_0 - \\theta_\\ast \\mathbf{1 } \\vert^2 + p \\alpha \\sigma^2 \\frac{m+l}{2ml}\\end{aligned}\\ ] ] note this is the same convergence rate guarantee as stochastic gradient descent with an extra factor of @xmath5 , albeit the coordination amongst processors required to adjust the step size is unrealistic in practice .",
    "+ next , we prove the bound on the processors local parameters convergence to each other / to the mean , @xmath127 $ ] .",
    "+ first , we write the spatial parameter average in vector form as @xmath128    with @xmath129 as previously defined in , we can write @xmath130 and so the term we wish to bound can be expanded as @xmath131 in addition to the diagonalization we previous calculated in ( reproduced below for convenience ) , we calculate some other useful diagonalizations . @xmath120 & = \\left ( 1 - \\frac{2 \\beta ( 1-\\beta)}{p } \\right ) \\mathbf{i } + \\frac{2 \\beta ( 1-\\beta)}{p^2 } \\mathbf{1 } \\mathbf{1}^t \\\\ & = q \\begin{bmatrix } 1 & & & \\\\ & 1-\\frac{2\\beta(1-\\beta)}{p } & & \\\\ & & \\ddots & \\\\ & & & 1-\\frac{2\\beta(1-\\beta)}{p } \\end{bmatrix } q^t \\\\ \\mathbb{e}[d^t \\mathbf{1 } \\mathbf{1}^t d ] & = \\frac{2 \\beta^2}{p } \\mathbf{i } + \\left ( 1 - \\frac{2 \\beta^2}{p^2 } \\right ) \\mathbf{1 } \\mathbf{1}^t \\\\ & = q \\begin{bmatrix } p & & & \\\\ & \\frac{2\\beta^2}{p } & & \\\\ & & \\ddots & \\\\ & & & \\frac{2 \\beta^2}{p } \\end{bmatrix } q^t \\\\   \\mathbb{e}\\left [ d^t d - \\frac{1}{p } d^t \\mathbf{1 } \\mathbf{1}^t d \\right ] & = \\left ( 1 - \\frac{2 \\beta ( 1-\\beta)}{p } - \\frac{2 \\beta^2}{p^2 } \\right ) \\mathbf{i } + \\left ( \\frac{2 \\beta ( 1-\\beta)}{p^2 } - \\frac{1}{p } \\left ( 1 - \\frac{2 \\beta^2}{p^2 } \\right )",
    "\\right ) \\mathbf{1 } \\mathbf{1}^t \\\\ & = q \\begin{bmatrix } 0 & & & \\\\ & 1 - \\frac{2 \\beta(1-\\beta)}{p } - \\frac{2\\beta^2}{p^2 } & & \\\\ & & \\ddots & \\\\ & & & 1 - \\frac{2 \\beta(1-\\beta)}{p } - \\frac{2\\beta^2}{p^2 } \\end{bmatrix } q^t \\label{eq : asyncdiagtwo}\\end{aligned}\\ ] ] we note that all three matrices are diagonalized by the same orthogonal matrix @xmath132 .",
    "furthermore , the first eigenvector , corresponding to the eigenvalues @xmath112 , @xmath5 , @xmath133 respectively , is @xmath134 .",
    "+ continuing from , we take the expectation and use to further bound the expression . in the computation below we use @xmath135 as a notational shorthand . @xmath136",
    "\\\\ & =   \\mathbb{e } \\left [ ( \\theta_t - \\alpha_t g_t)^t \\left ( d^t d - \\frac{1}{p } d^t \\mathbf{1 } \\mathbf{1}^t d \\right ) ( \\theta_t - \\alpha_t g_t ) \\right ] \\\\ & \\leq \\mathbb{e}\\left [ \\lambda \\vert ( \\theta_t - \\bar{\\theta}_t \\mathbf{1 } ) - \\alpha_t g_t \\vert^2 \\right ] \\\\ & = \\lambda \\left ( \\vert \\theta_t - \\bar{\\theta}_t \\mathbf{1 } \\vert^2 - \\frac{2 \\alpha_t}{p } \\sum_{i=1}^p \\nabla f(\\theta_{t , i})^t ( \\theta_{t , i } - \\bar{\\theta}_t ) + \\frac{\\alpha_t^2}{p } \\sum_{i=1}^p \\nabla f(\\theta_{t , i})^t \\nabla",
    "f(\\theta_{t , i } ) + \\frac{\\alpha_t^2}{p } \\sum_{i=1}^p \\xi_{t , i}^t \\xi_{t , i } \\right ) \\label{eq : strongconvexity } \\\\ & \\leq \\lambda \\left ( \\vert \\theta_t - \\bar{\\theta}_t \\mathbf{1 } \\vert^2 + \\frac{2 \\alpha_t}{p } \\sum_{i=1}^p \\left ( f(\\bar{\\theta}_t ) - f(\\theta_{t , i } ) \\right ) - \\frac{\\alpha_t m}{p } \\sum_{i=1}^p ( \\theta_{i , t } - \\bar{\\theta}_t)^2 + \\frac{\\alpha_t^2}{p } \\sum_{i=1}^p ( c^2+\\sigma^2 ) \\right )   \\label{eq : convexity } \\\\ & \\leq \\lambda \\left ( 1- \\frac{\\alpha_t \\ , m}{p } \\right ) \\vert \\theta_t - \\bar{\\theta}_t \\mathbf{1 } \\vert^2 + \\lambda \\alpha_t^2 ( c^2+\\sigma^2 ) \\end{aligned}\\ ] ] in and , we have used the definition of strong convexity and convexity respectively .",
    "+ finally , taking constant step size @xmath110 , and using the law of iterated expectations to unroll the recursion , we have @xmath56 & \\leq \\left ( \\lambda \\left ( 1- \\frac{\\alpha \\ , m}{p } \\right ) \\right)^t \\vert \\theta_0 - \\bar{\\theta}_0 \\mathbf{1 } \\vert^2   + \\frac{\\lambda \\alpha^2 ( c^2+\\sigma^2)}{1 - \\lambda \\left ( 1- \\frac{\\alpha \\ , m}{p } \\right)}\\end{aligned}\\ ] ] when the step size @xmath2 is small and the number of processors @xmath5 is large , the quantity @xmath137 is well approximated by @xmath138 , which makes clear the dependence of the rate on the parameter @xmath5 .      [",
    "[ gossip - with - stale - parameters - gradient - step - and - gossip - at - the - same - time ] ] gossip with stale parameters ( gradient step and gossip at the same time ) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    consider a one - step distributed consensus gradient update : @xmath139 if we replace the distributed mean @xmath140 with an unbiased one - sample estimator @xmath141 , such that @xmath142 and @xmath143=\\frac{1}{p}\\sum_{j=1}^p\\theta_{j , t}$ ] , then we derive the gossiping sgd update : @xmath144      consider a two - step distributed consensus gradient update : @xmath145 if we replace the distributed mean @xmath146 with an unbiased one - sample estimator @xmath147 , such that @xmath142 and @xmath148=\\frac{1}{p}\\sum_{j=1}^p\\theta_{j , t}'$ ] , then we derive the gossiping sgd update : @xmath149          we found that multi - scale training could be a significant performance bottleneck due to the computational overhead of resizing images , even when using multiple threads and asynchronous data loading . to remedy this , we used fast cuda implementations of linear and cubic interpolation filters to perform image scaling during training on the gpu .",
    "we also preprocessed imagenet images such that their largest dimension was no larger than the maximum scale ( in our case , 480 pixels ) .",
    "we implemented resnet-18 using stacked residual convolutional layers with @xmath150 projection shortcuts .",
    "we used the convolution and batch normalization kernels from cudnnv4 .",
    "the highest imagenet validation set accuracy ( center crop , top-1 ) our implementation of resnets achieved was about 68.7% with the aforementioned multi - scale data augmentation ; we note that researchers at facebook independently reproduced resnet with a more sophisticated data augmentation scheme and achieved 69.6% accuracy using the same evaluation methodology on their version of resnet-18 ."
  ],
  "abstract_text": [
    "<S> training time on large datasets for deep neural networks is the principal workflow bottleneck in a number of important applications of deep learning , such as object classification and detection in automatic driver assistance systems ( adas ) . to minimize training time </S>",
    "<S> , the training of a deep neural network must be scaled beyond a single machine to as many machines as possible by distributing the optimization method used for training . </S>",
    "<S> while a number of approaches have been proposed for distributed stochastic gradient descent ( sgd ) , at the current time synchronous approaches to distributed sgd appear to be showing the greatest performance at large scale . </S>",
    "<S> synchronous scaling of sgd suffers from the need to synchronize all processors on each gradient step and is not resilient in the face of failing or lagging processors . in asynchronous approaches using parameter servers , </S>",
    "<S> training is slowed by contention to the parameter server . in this paper </S>",
    "<S> we compare the convergence of synchronous and asynchronous sgd for training a modern resnet network architecture on the imagenet classification problem . </S>",
    "<S> we also propose an asynchronous method , gossiping sgd , that aims to retain the positive features of both systems by replacing the all - reduce collective operation of synchronous training with a gossip aggregation algorithm . </S>",
    "<S> we find , perhaps counterintuitively , that asynchronous sgd , including both elastic averaging and gossiping , converges faster at fewer nodes ( up to about 32 nodes ) , whereas synchronous sgd scales better to more nodes ( up to about 100 nodes ) . </S>"
  ]
}