{
  "article_text": [
    "the hidden markov model ( hmm ) is an important statistical model used in many fields including bioinformatics ( e.g.  @xcite ) , econometrics ( e.g.  @xcite ) and population genetics ( e.g.  @xcite ) ; see @xcite for a recent overview .",
    "a hmm is comprised of a latent process @xmath1 and an observed process @xmath2 .",
    "the latent process is a markov chain with an initial density @xmath3 and the transition density @xmath4 , i.e.@xmath5 it is assumed that @xmath6 and @xmath7 are densities on @xmath8 with respect to a dominating measure denoted generically as @xmath9 .",
    "the observation at time @xmath10 is conditionally independent of all other random variables given @xmath11 and its conditional observation density is @xmath12 on @xmath13 with respect to the dominating measure @xmath14 , i.e.@xmath15 the law of the hmm is parametrised by a vector @xmath16 taking values in some compact subset @xmath17 of the euclidean space @xmath18 .",
    "in this paper we focus on hmms where the probability density @xmath19 of the observations is",
    "_ intractable_. by intractable we mean that @xmath19 can not be evaluated ( or it is computationally prohibitive to calculate ) .",
    "however , we are able to generate samples from @xmath20 despite its intractability",
    ".    we will denote the actual observed random variables of the hmm as @xmath21 and assume that they are generated by some unknown @xmath22 which is to be estimated .",
    "the maximum likelihood estimate of @xmath23 given @xmath24 is @xmath25 where @xmath26 is the probability density , or the _ likelihood _ , of the observations @xmath24 , and from - , is given by @xmath27 dx_{1:n}.\\ ] ] even when @xmath8 is a finite set , @xmath26 can not be evaluated because @xmath19 is intractable .",
    "there is a sizeable literature on the use of sequential monte carlo ( smc ) methods , also known as particle filters , to evaluate the gradient of @xmath26 with respect to @xmath16 , which is subsequently used to compute its maximiser , see e.g.  the review in @xcite .",
    "however , these methods require a _ tractable _ @xmath19 and they are not directly applicable when this density is intractable .",
    "we thus propose new smc based maximum likelihood estimation ( mle ) algorithms to fill this void .",
    "we handle the intractable @xmath19 by drawing on ideas from approximate bayesian computation ( abc ) , an inference technique initially developed for bayesian models with an intractable likelihood , see @xcite for a recent review .",
    "our static parameter estimation algorithms are gradient based and cover both offline ( or batch ) and online estimation .",
    "recently @xcite have proposed a gradient based mle algorithm for hmms with an intractable observation density @xmath19 .",
    "the authors estimate the gradient of the likelihood in using a finite difference approximation which ultimately relies on estimates of @xmath26 only , which itself is calculated using smc .",
    "the major advantage of our method over that of @xcite is that we characterise the gradient of the log likelihood directly , by using available information on how the intractable @xmath19 is simulated from , and subsequently approximate it using smc , thus avoiding the added error of a finite difference approximation .",
    "our online mle algorithm is asymptotically unbiased ( as our numerical results indicate ) as the number of particles increases whereas the same can not be said for @xcite due to the finite difference approximation ; their numerical results indicate a bias that does not diminish with increasing data , even when @xmath26 can be calculated exactly as they illustrate for a linear gaussian state - space model ( see ( * ? ? ?",
    "* figure 2 ) ) .",
    "also , as observed from the results in @xcite , the variance of the parameter estimates of their recursive mle algorithm does not diminish with more data while ours does ( see the discussion in section [ sec : gradient ascent ] ) .",
    "the remainder of this paper is organised as follows .",
    "the theory that underpins our mle methodology is detailed in section [ sec : the abc mle approach for parameter estimation ] and in section [ sec : implementing abc mle with smc ] we describe its smc implementation .",
    "numerical examples using both simulated and real data sets are given in section [ sec : numerical examples ] .",
    "the numerical work covers three intractable models , namely the @xmath0-stable distribution , _ g - and - k _ distribution , and the stochastic volatility model with @xmath0-stable returns .",
    "finally , section [ sec : discussion ] provides a discussion of other possible methods for parameter estimation in hmms when both state and observation densities intractable .",
    "the particle filter sequentially approximates the sequence of posterior densities @xmath28 of the hmm @xmath29 using a weighted discrete distribution with @xmath30 support points for @xmath31 which are called particles . at each time",
    "@xmath10 , the particles are resampled according to their current weights , and then the resampled particles are propagated independently of each other using a proposal transition density @xmath32 .",
    "the particles are then reweighed to correct for the discrepancy between @xmath33 and the law of the proposed particles which is @xmath34 .",
    "this is standard importance sampling and the assumption in the weight correction step is that the law of each resampled particle at time @xmath10 is @xmath35 , which is an erroneous but progressively correct as @xmath30 is increased @xcite . in the implementation of the particle filter",
    "the normalising constants of the sequence of target posteriors are not needed but calculating the new weights requires @xmath36 to be tractable .",
    "it was shown by @xcite that the weights of the particle approximation of @xmath28 can be used to obtain an unbiased estimate of the likelihoods @xmath37 .",
    "see the appendix for an example code for a particle filter .",
    "@xcite consider the problem of constructing an smc approximation of the _ filter _",
    "@xmath38 , which is the marginal of the particle approximation for @xmath35 , for a hmm with an intractable observation density @xmath19 .",
    "since it is not possible to calculate the weights of the particle filter for such an hmm where @xmath19 is intractable , they propose a particle filter approximation for the extended hmm @xmath39 where the joint process @xmath29 , which is now the latent process of the extended hmm , is defined by and and the new sequence @xmath40 is @xmath41 where @xmath42 denotes the ball of radius @xmath43 centred at @xmath44 and @xmath45 is the uniform distribution over the set @xmath46 .",
    "then , the density @xmath47 of the extended hmm is regarded as an approximation for @xmath48 where @xmath49 reflects the error of the approximation and this error diminishes as @xmath50 ; see also @xcite and @xcite for theoretical results on this approximation .",
    "note that @xmath51 does not coincide with @xmath48 because @xmath52 obeys the law - and not .",
    "@xcite remark that @xmath51 is the abc approximation for the filter of a hmm .",
    "furthermore , they show it is straightforward to approximate @xmath51 with a bootstrap particle filter .",
    "consider now the extended hmm @xmath39 specified by , and and let @xmath53 denote the probability density ( or likelihood function ) of the process @xmath54 evaluated at some @xmath55 .",
    "( see for the precise expression of this density . )",
    "@xcite study the theoretical properties of the following maximum likelihood estimate of @xmath23 : @xmath56 they call this procedure abc mle .",
    "( note that despite the word ` bayesian ' in abc , the procedure is not bayesian . )",
    "the bootstrap particle filter of @xcite provides an unbiased smc approximation of the likelihood @xmath57 and this likelihood may be maximised by evaluating the approximation over a grid of values for @xmath16 .",
    "this , however , is clearly not practical as the dimension of @xmath16 increases , has no straightforward extension for recursive estimation and is not an accurate convergent method .",
    "it was shown in @xcite that the abc mle leads to a biased estimate of the parameter vector @xmath23 in the sense that as @xmath58 , @xmath59 will converge to some point @xmath60 , and that this bias can be made arbitrarily small by choosing a sufficiently small value of @xmath61 , i.e.  @xmath62 as @xmath50 .",
    "the bias of abc mle is due to the fact the observed sequence @xmath63 is the outcome of the law for @xmath64 and not . @xcite",
    "suggest removing the bias of @xmath59 in by adding noise to the real data and then computing the maximum likelihood estimate , i.e. let @xmath65 be a realisation of i.i.d .",
    "samples from @xmath66 and let @xmath67 note that the noisy data @xmath68 now obeys the law of @xmath54 when @xmath64 . therefore , the procedure @xmath69 can now produce a consistent estimator of the parameter vector @xmath23 as @xmath70 . this result proved by @xcite can be interpreted as the frequentist equivalence of wilkinson s observation that the abc posterior distribution is exact under the assumption of model error @xcite .",
    "finally , @xcite also remark that the use of other types of noise in is possible without compromising the asymptotics of noisy abc mle , i.e. @xmath71 where @xmath72 is a smooth centred density .",
    "( accordingly , noisy abc mle is performed with the noise corrupted observations where now @xmath73 are realisations of i.i.d .",
    "samples from @xmath74 ) as we show , a continuously differentiable @xmath72 is important for the development of practical gradient based mle techniques . in this work",
    "we choose @xmath72 to be the probability density of zero - mean unit - variance gaussian random variable .",
    "other choices are possible ( but not investigated ) and our framework would still be applicable .",
    "we remark that although the theoretical basis for abc mle was established in @xcite , the authors do not propose a practical methodology for implementing abc mle in their work ; this is indeed an important void to be filled . in this paper",
    "we demonstrate how , by using ideas from @xcite , both batch and online versions of noisy abc mle can be implemented with smc .",
    "we assume that for all @xmath75 there exist a distribution on some auxiliary space @xmath76 with a tractable density @xmath77 with respect to @xmath78 and a function @xmath79 such that one can sample from @xmath20 by first sampling @xmath80 from @xmath77 and then applying the transformation @xmath81 ; i.e.  the law of @xmath82 is @xmath20 . from this",
    "it follows that the process @xmath83 in can be equivalently generated as @xmath84 where @xmath85 is the hidden state of the original hmm given by and @xmath86 for all @xmath10 .",
    "we will implement smc based mle for the following hmm : let @xmath87 be the latent process and @xmath88 in be the observation process .",
    "the initial and transition densities for @xmath89 ( with respect to the dominating measure @xmath90 ) and the observation density of @xmath40 ( with respect to the lebesgue measure on @xmath91 ) are @xmath92 where @xmath93 and @xmath94 .",
    "the density of the observed process @xmath95 of this hmm evaluated at some @xmath96 is @xmath97 d z_{1:n}.\\end{aligned}\\ ] ] where @xmath98 .",
    "note that @xmath99 in is indeed the likelihood function @xmath100 to be maximised with respect to @xmath16 in abc mle in section [ sec : the abc mle approach for parameter estimation ] ; see and .",
    "moreover all the densities declared in are tractable and differentiable functions of @xmath16 ( provided that @xmath4 , @xmath101 , and @xmath102 are differentiable with respect to @xmath16 ) .    henceforth , we will work exclusively with the hmm @xmath103 defined in .",
    "as discussed before , we corrupt the real measurements @xmath104 with a single realisation of independent samples @xmath105 from a @xmath16-independent probability density @xmath72 , i.e.@xmath106 to obtain a realisation of the observed process of the hmm @xmath107 .",
    "one well known mle algorithm is the following iterative gradient ascent method which updates the parameter estimate @xmath108 using the rule @xmath109 where @xmath110 is an arbitrary initial estimate . here",
    "@xmath111 is a sequence of step - sizes satisfying the constraints @xmath112 and @xmath113 so as to ensure that the algorithm converges to a local maximum of @xmath114 .",
    "the term @xmath115 is shorthand for the @xmath18-valued vector @xmath116 which is also called the score vector , and is given by fisher s identity ( e.g.  see @xcite ) @xmath117   p_{\\theta}(z_{1:n } | y^{\\epsilon}_{1:n } ) d z_{1:n } \\label{eq : score}\\end{aligned}\\ ] ] with the convention that @xmath118 .",
    "note that the method in uses the whole data set @xmath119 at every parameter update step , which makes it a batch method .",
    "an alternative to it is the following online gradient ascent method which updates the parameter estimate every time a new data point is received @xmath120 where @xmath121 while the subscript @xmath122 indicates that @xmath123 is evaluated at @xmath124 , a necessary requirement for a truly online implementation is that the previous values of @xmath16 estimates ( i.e.  other than @xmath122 ) are also used in the evaluation of @xmath125 @xcite .",
    "it is important to note that , for both batch and online method , we require that the transition density of @xmath89 be tractable and differentiable with respect to @xmath16 , which is precisely why we propose to work with @xmath126 rather than @xmath39 whose state transition density contains the intractable @xmath127 .",
    "( we discuss suitable alternatives when the state transition density is intractable in section [ sec : other mle methods for hmms with an intractable density ] . )",
    "it is apparent from and that an smc implementation of these mle algorithms hinges on the availability of a particle approximation of the score in .",
    "@xcite discuss two methods to estimate the score using the smc approximation of the full posterior @xmath128 .",
    "one method is nothing more than the substitution of the law @xmath128 in with its particle approximation and has a cost , like the particle filter itself , which is @xmath129",
    ". we will refer to this estimate of the gradient as the @xmath129 method ( * ? ? ?",
    "* algorithm 1 ) . due to",
    "resampling step of the particle filter there is a lack of unique samples in the particle approximation of @xmath130 for @xmath131 much smaller than @xmath132 , which is called particle degeneracy in the literature .",
    "@xcite shows that the variance of this @xmath129 score estimate , where the variance is computed with respect to the particles being sampled while the observation sequence is held fixed , grows quadratically with time . while this may not be an issue for the batch method in , it is not suitable for online estimation since the variance of the resulting estimate of @xmath125 grows linearly with time @xmath132 .    as an alternative to this standard @xmath129 score estimate , @xcite",
    "propose an @xmath133 estimate of the score computed using the same particle approximation to @xmath128 which aims to avoid the particle degeneracy problem mentioned .",
    "we will refer to this as the @xmath133 method ( * ? ? ?",
    "* algorithm 2 ) .",
    "the authors experimentally show that the variance of the score estimate now grows linearly in time @xmath132 while the variance of the resulting estimate of @xmath125 is time - uniformly bounded ( i.e.  does not grow ) ; a proof of the latter fact can be found in @xcite .",
    "therefore , the smc implementation of @xmath123 we adopt for online estimation is the @xmath134 method .    finally , we mention that the score can also be estimated using a fixed - lag method which would have a computational cost which is @xmath129 and a variance which grows linearly in time",
    ". however there is the added error introduced by not smoothing beyond a certain lag ; see @xcite for a review of static parameter estimation techniques .",
    "if the monte carlo estimates of the gradient terms have high or infinite variances , we expect failure of the gradient ascent methods . we can stabilise the variance by transforming the observed data , but without compromising the identifiability of the model , and then add noise as discussed in noisy abc .",
    "this approach to stabilising the variance is novel as the issue of infinite variance has not been reported before in the smc literature .",
    "this issue of the potential for infinite variance ( prior to stabilising by adopting a specific transformation ) can be perfectly exemplified by the problem of learning the parameters of a distribution from a sequence of i.i.d .",
    "random variables which we now discuss .",
    "let @xmath2 be an i.i.d .",
    "sequence with an intractable probability density @xmath135 on @xmath13 . for",
    "any @xmath16 , assume @xmath136 can be sampled from @xmath127 by first generating @xmath137 from the density @xmath138 and then followed by the application a certain transformation function @xmath139 , i.e.  the law of @xmath140 is @xmath127 .",
    "( the @xmath0-stable process is generated precisely in this way ; see example [ ex : stability alpha stable ] below . )",
    "we are given a realisation @xmath141 from @xmath23 and the latter is to be estimated .",
    "let @xmath142 be the noise corrupted observed sequence as in . in the context of the discussion in section [ sec : implementing abc mle with smc ] , the aim is to maximise the likelihood of the noisy observations @xmath68 ( generated from the true model @xmath23 ) using the parametric family of hmms @xmath143 . since @xmath144 are i.i.d .",
    "the batch and online update rules become , respectively , @xmath145 @xmath146 in becomes @xmath147 and @xmath148 p_{\\theta}(u | y^{\\epsilon } )   du,\\end{aligned}\\ ] ] where @xmath149 therefore @xmath150 can be estimated using an @xmath30-sample monte carlo approximation to @xmath151 , e.g.  with either mcmc or importance sampling .",
    "one important point to note about this i.i.d .",
    "case is that the @xmath134 method becomes @xmath129 .",
    "we now calculate the variance of the monte carlo estimate of at @xmath64 given @xmath30 i.i.d .",
    "samples from @xmath152 ( note that in the numerical examples we actually use importance sampling to sample from @xmath151 but the following calculation is done assuming i.i.d .",
    "samples are available for illustrative purposes . ) dropping the index @xmath10 , given a noise corrupted measurement @xmath153 generated from the true model @xmath23 , and i.i.d .",
    "samples @xmath154 , an estimate of @xmath155 is @xmath156 + \\nabla \\log \\nu_{\\theta^\\ast}(u_{i}).\\ ] ] we are interested in the variance of this quantity with respect to the law of @xmath157 .",
    "we consider the case where @xmath158 has a finite second moment , e.g.  see the example to follow .",
    "then , the sum above has a finite second moment if and only if @xmath159 $ ] has a finite second moment with respect to the joint law of @xmath160 .",
    "one can show that @xmath161 \\right\\}^{2 } \\right ] = \\epsilon^{2 } \\mathbb{e}_{\\theta^{\\ast } } \\left [ \\left\\ { \\nabla \\tau_{\\theta^{\\ast}}(u_{i } ) \\right\\}^{2 } \\right ] \\label{eq : infvarexample}\\end{aligned}\\ ] ] if the second moment of @xmath162 is infinite ( or very high ) , we may circumvent this instability problem by transforming the _ actual _ observed process from @xmath23 using a suitable one - to - one function @xmath163 prior to adding noise .",
    "that is , we replace with the following transformed noise corrupted process @xmath164 the conditional density @xmath165 becomes @xmath166}{\\epsilon } \\right)\\ ] ] and the right hand side of now is @xmath167 $ ] . in this paper",
    "we use @xmath168 throughout , and in the following example we show how is infinite but subsequently stabilised with this transformation .",
    "[ ex : stability alpha stable ] * ( the @xmath0-stable distribution . ) * let @xmath169 denote the @xmath0-stable distribution . the parameters of the distribution , @xmath170 \\times [ -1 , 1 ] \\times \\mathbb{r } \\times [ 0 , \\infty),\\ ] ] represent the shape , skewness , location , and scale respectively .",
    "one can generate a random sample from @xmath169 by generating @xmath171 , where @xmath172 and @xmath173 are independent , and setting @xmath174 the mapping @xmath175 is defined as @xcite @xmath176}{\\left [ \\cos(u_{1 } ) \\right]^{1/\\alpha } } \\left(\\frac{\\cos \\left [ u_{1 } - \\alpha ( u_{1 } + b_{\\alpha , \\beta } ) \\right]}{u_{2 } } \\right)^{(1-\\alpha)/\\alpha } , & \\alpha \\neq 1\\\\      x = \\frac{2}{\\pi } \\left [ \\left ( \\frac{\\pi}{2 } + \\beta u_{1 } \\right ) \\tan u_{1 } - \\beta \\log\\left ( \\frac{u_{2 } \\cos u_{1}}{\\frac{\\pi}{2 } + \\beta u_{1 } } \\right ) \\right ] ,    & \\alpha = 1 .",
    "\\end{cases } \\nonumber\\ ] ] where @xmath177 although it is hard to show for @xmath0 and @xmath178 , we can show that @xmath179 = \\mathbb{e}_{\\theta } \\left [ \\left\\ { \\tau_{\\alpha , \\beta}(u ) \\right\\}^{2 } \\right ] =   \\infty\\ ] ] unless @xmath180 .",
    "therefore , it is not desirable to run the gradient ascent method for the process @xmath54 with @xmath181 since the variance of the gradient estimate will be infinite",
    ". instead , we use the transformation @xmath182 , i.e.  @xmath183 to make the gradient ascent method stable .",
    "one can indeed check that for the parameter @xmath184",
    "@xmath185 \\right\\}^{2 } \\right ] = \\mathbb{e}_{\\theta } \\left [ \\left\\ { \\frac{\\tau_{\\alpha , \\beta}(u)}{1 + \\tau_{\\theta}(u)^{2 } } \\right\\}^{2 } \\right ] < \\infty\\ ] ] we also verify numerically in section [ sec : numerical examples ] that the gradients with respect to the other parameters @xmath0 , @xmath178 are stabilised with @xmath182 ( while we can show that @xmath186 = 1 $ ] ) .",
    "in this section we demonstrate the performance of the gradient ascent methods described in section [ sec : implementing abc mle with smc ] on the i.i.d .",
    "@xmath0-stable and _ g - and - k _ models as well as the stochastic volatility model with @xmath0-stable returns .",
    "we first consider the problem of estimating the parameters of an @xmath0-stable distribution @xmath169 ( developed in example [ ex : stability alpha stable ] ) from a sequence of i.i.d .  samples .",
    "several methods for estimating parameter values for stable distributions have been proposed , including a bayesian approach based on abc , see @xcite . in this example",
    "we consider estimating these parameters using the online gradient ascent method to implement noisy abc mle .",
    "since the only discontinuity in the transformation function @xmath102 for generating an @xmath0-stable random variable is at @xmath187 , we can safely use the gradient ascent method for estimating @xmath23 with @xmath188 being not in the close vicinity of @xmath189 .    as recommended in example",
    "[ ex : stability alpha stable ] , we transform the observations using @xmath182 for stability . in order to check , numerically , whether the transformation in with @xmath182 stabilises the gradients , we can look at the empirical distribution of the monte carlo estimates of @xmath190 after transforming the observations @xmath191 . for this purpose",
    ", we generate @xmath192 samples @xmath193 from @xmath194 and @xmath73 from @xmath72 for @xmath195 , and for each sample we estimate @xmath196 , where @xmath197 , with @xmath198 , using self - normalised importance sampling with @xmath199 samples generated from @xmath101 .",
    "figure [ fig : histogram of gradients with arctan ] shows the histograms of the monte carlo estimates of @xmath196 which confirms that the transformation does stabilise the gradients .",
    "the outcome of online gradient ascent method to implement noisy abc mle for the same data set is shown in figure [ fig : estimation of alpha stable parameters ] . a trace plot of the sequence of gradient estimates ( as @xmath16 is adjusted )",
    "is also shown as further confirmation of the stability of the estimated gradients .",
    "the next experiment contrasts the abc mle and noisy abc mle solutions for the same data set .",
    "the results in figure [ fig : comparison of noisy abc mle and noisy abc mle for alpha - stable ] compare the online @xmath23 estimates averaged over 50 independent runs for both algorithms .",
    "each run used the same data set but a new realisation of particles .",
    "the outcome of this comparison is that abc mle yields biased estimates for the shape and skewness parameters @xmath0 and @xmath178 whereas the bias is not present in noisy abc mle .",
    "the _ g - and - k _ distribution is defined by the following parameterised quantile ( or inverse distribution ) function @xmath200 @xmath201 \\left ( 1 + \\phi(u)^{2 } \\right)^{k } \\phi(u ) , \\quad u \\in ( 0 , 1)\\ ] ] where @xmath202 is the @xmath203th standard normal quantile .",
    "the parameters @xmath204 are the skewness , kurtosis , location and scale , and @xmath205 is usually fixed to @xmath206 .",
    "therefore one can generate from the _ g - and - k _",
    "distribution by first sampling @xmath207 and then returning @xmath208 @xcite .",
    "bayesian parameter estimation for the _ g - and - k _ distribution using abc was recently proposed in @xcite .",
    "we consider online mle for @xmath16 using the noisy abc likelihood .",
    "note that @xmath200 in is differentiable with respect to @xmath16 and so the gradient ascent method is applicable . to avoid gradients with very high variances resulting from the factor @xmath209 in @xmath200 , similar to the case of @xmath0-stable distribution",
    ", we transform the actual observations using @xmath182 and add noise with @xmath198 . in our experiments it was noticed that our method performs better when the location parameter @xmath210 is closer to @xmath211 , which must be a result of the non - linear behaviour of the transformation function @xmath212 . therefore , whenever possible , it is suggested to estimate @xmath210 using some ( possibly heuristic ) method ( such as using the mean of the first few samples ) as a preprocessing step , subtract the heuristically estimated value @xmath213 of @xmath210 from the samples , perform mle on the ( approximately ) centred data , and then add back @xmath213 to the estimated location obtained by the mle algorithm .",
    "figure [ fig : online estimation of g - and - k distribution parameters ] shows the results of online gradient ascent method to implement noisy abc mle for estimating @xmath214 . in the figure we observe the mean and log - variance of 50 runs on the _ same _ noisy transformed data sequence .",
    "( therefore , the accuracy and the variance of the estimates correspond to the performance of the monte carlo approximation of the gradients @xmath196 . )",
    "self - normalised importance sampling is used with @xmath199 samples generated from @xmath101 . from the results in figure [ fig : online estimation of g - and - k distribution parameters ]",
    ", we can see that the bias introduced by the finite number of particles is negligible for @xmath199 and that the variance of the algorithm reduces in time suggesting the convergence of the estimates in each run to essentially the true parameter values .",
    "the next experiment shows how the noisy abc mle can be implemented with the batch gradient ascent method when the data set is too small for the online method to converge . a detailed study of mle for _ g - and - k _ distribution can be found in @xcite where mle methods based on numerical approximation of the likelihood itself are investigated .",
    "we generated 500 data sets of size @xmath215 from the same _ g - and - k _ distribution with @xmath214 and executed the batch gradient ascent method with @xmath198 on each data set .",
    "again , self - normalised importance sampling is used with @xmath199 samples .",
    "the upper half of figure [ fig : batch estimation of g - and - k distribution parameters ] shows the estimation results with noisy abc mle versus number of iterations for a single data set .",
    "note that for short data sets , @xmath23 is usually not the true maximum likelihood solution .",
    "the lower half of figure [ fig : batch estimation of g - and - k distribution parameters ] shows the distributions ( histograms over 20 bins ) of the converged maximum likelihood solution for @xmath23 .",
    "the mean and variance of the estimates for @xmath216 are @xmath217 and @xmath218 respectively .",
    "comparable values for these moments at this particular @xmath23 and data size @xmath132 were also obtained in ( * ? ? ?",
    "* table 3 ) .",
    "the stochastic volatility model with @xmath0-stable returns ( sv@xmath0r ) is a financial data model @xcite .",
    "the hidden process @xmath1 represents the log - volatility in time whereas the observation process @xmath2 is the log return values . the model for @xmath29 with parameters",
    "@xmath219 is : @xmath220 this model is an alternative to the stochastic volatility model with gaussian returns to account for an observed series which is heavy - tailed and displays outliers . for more discussion on the model as well as a review of methods for estimating the static parameters of such models , see @xcite and the references therein .",
    "these existing methods for parameter estimation in sv@xmath0r are batch and suitable for only short data sequences .",
    "we simulated a scenario where a very long data sequence generated from this model with @xmath221 is being received sequentially .",
    "we used online gradient ascent method to find the noisy abc mle solution for this data sequence , where the @xmath134 method ( * ? ? ?",
    "* algorithm 2 ) with @xmath222 particles was used to estimate .",
    "again , we transform the actual observations with the function @xmath182 and then add noise .",
    "figure [ fig : estimation of stochastic volatility model parameters with gradient ascent ] shows the online estimates of @xmath23 for @xmath223 data samples .",
    "the estimates seem to converge after around @xmath224 samples and are accurate .",
    "we now consider a real data experiment , where the data are the daily gbp - dem exchange rates between @xmath225 to @xmath226 containing @xmath227 samples @xmath228 ; these data are considered in @xcite .",
    "log - returns @xmath229 are obtained by @xmath230 , @xmath231 .",
    "the observations , @xmath232 , are the residuals of the ar(1 ) process that is fitted to @xmath229 .",
    "( we used the same model and data set as @xcite in order to compare our results with theirs ) .",
    "the sv@xmath0r model above is assumed for @xmath24 , where the hidden process has an extra parameter @xmath233 : @xmath234 hence @xmath235 .",
    "we implemented noisy abc mle using batch gradient ascent with the @xmath129 method ( * ? ? ? * algorithm 1 ) with @xmath236 particles to approximate . to measure the variability of the estimates as a function of the realisation of added noise and the @xmath61 value",
    ", we repeated the estimation with @xmath237 , @xmath198 , and @xmath238 , separately , where for each @xmath61 we ran the method with @xmath239 different added noise realisations . for all runs , we terminated the batch gradient ascent algorithm after @xmath240 iterations .",
    "@xmath236 particles were used to evaluate the gradients at each iteration .",
    "figure [ fig : batch mle trajectories and box - plots for real data ] ( top ) shows the estimates versus number of iterations , where the trajectories for different noisy data sets for the same value of @xmath61 are superimposed .",
    "also , the bottom part of figure [ fig : batch mle trajectories and box - plots for real data ] shows the box - plots of the estimates of @xmath23 for different @xmath61 values , where the box - plots for each @xmath61 were created from the converged estimates of @xmath23 ( the average of the estimates at the last 1000 iterations ) obtained from 10 different noisy data sets generated using that value of @xmath61 . for the ease of explanation ,",
    "we will denote them as @xmath241 where @xmath242 is the converged estimate obtained from the @xmath243th noisy data set that was generated using @xmath61 .",
    "figures [ fig : batch mle trajectories and box - plots for real data ] suggests a trade off between accuracy in the estimates and computational efficiency in the following sense . a smaller value of @xmath61 is expected yield less biased estimates ( with respect to the maximiser of the true likelihood of the real data ) with less variance ( with respect to the added noise ) provided that the maximisation @xmath244 is performed exactly , that is with infinitely many @xmath30 and infinitely many number of parameter updates . on the other hand , smaller @xmath61 results in the decrease of the effective sample size in the smc algorithm and hence increases the variance of the smc estimate of the gradient of the log likelihood .",
    "the effect of this on our results is the larger variance in the estimates obtained with @xmath237 compared to those obtained with @xmath198 ( which would eventually be smaller if the maximisation were performed exactly ) . in conclusion , for a fixed batch data size and a given amount of computational resource , one must optimise the trade off between the ( average ) accuracy and the variability in the estimates , for which the effective sample size of the particles could be used as a rule of thumb .",
    "@xcite fitted the same model to the same data set using the indirect estimation method and their estimates of @xmath23 was @xmath245 , which is slightly different to our results .",
    "both ours and their method aim for the maximum likelihood solution , which suggests that it would be sensible to compare the likelihood of the true data sequence for the estimates of @xmath23 obtained from both methods .",
    "however , this is not possible since neither @xmath246 nor an unbiased monte carlo estimator of it is available .",
    "instead , we compared the unbiased smc estimates of the abc likelihoods @xmath57 using an @xmath61 small enough to make the effect of model mismatch negligible ( see the discussion of model mismatch error in section 2 ) for comparison and @xmath30 large enough to ensure that the variability of the smc estimate of the likelihood across the particle realisations is not too much ; for these reasons we chose @xmath247 and @xmath248 .",
    "( see appendix for the details of the implementation . )",
    "the left hand side of figure [ fig : comparison of smc likelihood estimates for real data ] shows the logarithms of the 10 independent smc estimates of @xmath57 calculated at the value of each estimate in . for comparison ,",
    "the results are shown with 10 independent smc estimates of @xmath57 at @xmath249 .",
    "the figure shows that noisy abc mle has improved the results of @xcite for all values of @xmath61 that we used , in the sense that almost all the estimates resulting from the abc mle method yields a higher likelihood of the data set to which the model is fitted .",
    "finally , we perform a simple model check for by considering the conditional cumulative distribution functions @xmath250 at the values of @xmath23 estimated using noisy abc mle and the indirect estimation method in @xcite . since @xmath251 are i.i.d .",
    "uniform random variables on @xmath252 $ ] @xcite , we expect the probability plot ( for the uniform distribution ) of the population @xmath253 to approximate the @xmath254 line under the hypothesis that @xmath24 is generated from the sv@xmath0r model @xmath29 . however , we are unable to perform these calculations for the original hmm due to the intractability of @xmath255",
    ". instead , we use the modified hmm @xmath39 but with @xmath61 small enough for one to neglect the difference between the two models ( as in the previous experiment ) .",
    "the probability plots at the right hand side of figure [ fig : comparison of smc likelihood estimates for real data ] were generated from the smc estimates of @xmath256 ( see appendix for the details ) , with @xmath247 and @xmath248 , for four different values of @xmath16 : the first three are the means of @xmath257 for @xmath237 , @xmath198 and @xmath238 , respectively , and the fourth one is @xmath258 .",
    "the probability plots are all close to the @xmath254 line which justifies the sv@xmath0r model ; they also indicate that there is more agreement between the sv@xmath0r model and the data when @xmath16 is the noisy abc mle solution than when it is the maximum likelihood solution of the indirect estimation method .",
    "in this paper , we have presented smc implementations of mle for hmms with an intractable observation density .",
    "we showed how smc versions of both batch and online gradient ascent algorithms can be used to implement noisy abc mle and how a further transformation of the data can stabilise the variance of the smc gradient estimate .",
    "we have shown that smc implementations of the methodology in @xcite is practical and yields convergent and accurate estimates of @xmath23 even when the exact procedures in @xcite are replaced by their smc counterparts .",
    "although not as general as the gradient ascent mle approach , the expectation - maximisation ( em ) algorithm may be available for some models , at least for a part of the parameters in @xmath16 , if the joint density @xmath259 belongs to an exponential family . both @xmath129 and @xmath134 batch and online em algorithms can be devised using smc ; details of such algorithms can be found in @xcite and @xcite .",
    "there are other gradient mle methods in the literature that are available for implementing noisy abc mle and we have discussed the technique of @xcite in the introduction .",
    "one advantage of their finite difference method is that it is essentially a gradient free technique as it bypasses having to calculate the derivatives with respect to @xmath16 of the state transition and observation densities of the hmm and thus can cope , without modification , with an intractable state transition density .",
    "another gradient based method that uses smc to approximate the gradient of the log - likelihood without the need to calculate the derivatives of the hmms densities is the iterated filtering algorithm of @xcite . in particular , one can use iterated filtering for @xmath39 or @xmath260 in order to estimate @xmath115 .",
    "however , the method does not have an extension to online estimation .",
    "another downside is that the algorithm requires an increasing number of particles versus iteration for convergence .",
    "@xcite study a hmm with a tractable observation density @xmath19 but an intractable state transition density @xmath261 .",
    "assume one can generate from @xmath262 by sampling @xmath263 from @xmath264 and using a differentiable function @xmath265 such that @xmath266 .",
    "the gradient of the log likelihood in such hmms can be estimated using the infinitesimal perturbation analysis ( ipa ) approach proposed in @xcite , provided that @xmath264 , @xmath267 , and @xmath19 are differentiable with respect to @xmath16 as well as the state variable @xmath268 .",
    "we can straightforwardly adopt the ipa approach with our noisy abc mle to deal with a fully intractable model , where both the state transition and the observation densities are intractable .",
    "however , ipa is a path space method and suffers from particle degeneracy .",
    "this will lead to the variance of the estimate of the score in increasing quadratically in time like the @xmath129 method in @xcite .",
    "as the authors mention , fixed - lag smoothing could be use to control this variance growth but at the cost of a small bias .",
    "static parameter estimation for hmms with intractable state and observation densities have been addressed in a bayesian context by @xcite .",
    "@xcite utilise the so called convolution particle filter , which uses ideas from kernel density estimation to replace the intractable densities needed for the weight evaluation in the particle filter with their kernel estimates , to sequentially estimate the posterior distribution of @xmath23 .",
    "while an smc based bayesian approach can potentially produce good estimates of @xmath23 for short data lengths , at least for tractable models where standard particle methods apply , particle degeneracy does bias the estimation results for long data sets @xcite . in contrast our methods do give rise to practically consistent estimators as our numerical results indicate .    finally , we remark that mle using abc is studied in the recent work @xcite , but in a non - hmm setting where the likelihood of data @xmath269 given @xmath16 is intractable .",
    "the authors form a kernel density estimate of the likelihood from @xmath16 samples drawn from the abc posterior distribution .",
    "they propose maximising the kernel density estimate as an approximation to mle .",
    "unlike @xcite , we consider the hmm setting and our methods do not need samples of @xmath16 .",
    "s.s .  singh and t.",
    "dean s research was funded by the engineering and physical sciences research council ( ep / g037590/1 ) whose support is gratefully acknowledged .",
    "a.  jasra was supported by an moe singapore grant and is also affiliated with the risk management institute at the national university of singapore .",
    "* smc for estimating @xmath270 and @xmath271 * + begin with @xmath272 . for @xmath273 ,    * _ prediction : _ for @xmath274 , sample @xmath275 as follows : * * if @xmath276 , sample @xmath277 , @xmath278 * * if @xmath279 , sample @xmath280 , @xmath281 . * _ weighting : _ for @xmath274 , calculate the unnormalised weights @xmath282 * _ likelihood estimate : _ update the likelihood estimate by @xmath283 . * _ conditional cumulative distribution function : _ calculate @xmath284 * _ resampling : _",
    "sample @xmath285 from @xmath286 using the weights @xmath287 ."
  ],
  "abstract_text": [
    "<S> we propose sequential monte carlo based algorithms for maximum likelihood estimation of the static parameters in hidden markov models with an intractable likelihood using ideas from approximate bayesian computation . </S>",
    "<S> the static parameter estimation algorithms are gradient based and cover both offline and online estimation . </S>",
    "<S> we demonstrate their performance by estimating the parameters of three intractable models , namely the @xmath0-stable distribution , _ g - and - k _ distribution , and the stochastic volatility model with @xmath0-stable returns , using both real and synthetic data .    </S>",
    "<S> * key words : * hidden markov models , maximum likelihood estimation , approximate bayesian computation , intractable likelihood , sequential monte carlo . </S>"
  ]
}