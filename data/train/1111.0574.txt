{
  "article_text": [
    "we call @xmath0 a pseudo - boolean function . the present work discusses approaches to obtain heuristics for the program @xmath1 \\text{subject to } & $ \\v x\\in\\b^d$ \\end{tabular}\\ ] ] using sequential monte carlo techniques . in the sequel",
    ", we refer to @xmath2 as the _ objective function_. for an excellent overview of applications of binary programming and equivalent problems we refer to the survey paper by and references therein .",
    "the idea to use particle filters for global optimization is not new [ , section 2.3.1.c ] , but novel sequential monte carlo methodology making use of suitable parametric families on binary spaces @xcite may allow to construct more efficient samplers for the special case of pseudo - boolean optimization .",
    "we particularly discuss how this methodology connects with the cross - entropy method @xcite which is another particle driven optimization algorithm based on parametric families .    the sequential monte carlo algorithm as developed by",
    "is rather complex compared to local search algorithms such as simulated annealing @xcite or @xmath3-opt local search @xcite which can be implemented in a few lines .",
    "the aim of this paper is to motivate the use of advanced particle methods and sophisticated parametric families in the context of pseudo - boolean optimization and to provide conclusive numerical evidence that these complicated algorithms can indeed outperform simple heuristics if the objective function has poorly connected strong local maxima .",
    "this is not at all clear , since , in terms of computational time , multiple randomized restarts of fast local search heuristics might very well be more efficient than comparatively complex particle approaches .",
    "the article is structured as follows .",
    "we first introduce some notation and review how to model an optimization problem as a filtering problem on an auxiliary sequence of probability distributions .",
    "section [ sec : smc ] describes a sequential monte carlo sampler @xcite designed for global optimization on binary spaces @xcite .",
    "section [ sec : pfbs ] reviews three parametric families for sampling multivariate binary data which can be incorporated in the proposed class of particle algorithms .",
    "section [ sec : algorithms ] discusses how the cross - entropy method @xcite and simulated annealing @xcite can be interpreted as special cases of the sequential monte carlo sampler . in section [ sec : applications ] we carry out numerical experiments on instances of the unconstrained quadratic binary optimization problem .",
    "first , we investigate the performance of the proposed parametric families in particle - driven optimization algorithms .",
    "secondly , we compare variants of the sequential monte carlo algorithm , the cross - entropy method , simulated annealing and simple multiple - restart local search to analyze their respective efficiency in the presence or absence of strong local maxima .",
    "we briefly introduce some notation that might be non - standard .",
    "we denote scalars in italic type , vectors in italic bold type and matrices in straight bold type .",
    "given a set @xmath4 , we write @xmath5 for the number of its elements and @xmath6 for its indicator function . for @xmath7",
    "we denote by @xmath8 the discrete interval from @xmath9 to @xmath10 . given a vector @xmath11 and an index set @xmath12 , we write @xmath13 for the sub - vector indexed by @xmath14 and @xmath15 for its complement .",
    "we occasionally use the norms @xmath16 and @xmath17 .        for particle optimization",
    ", the common approach is defining a family of probability measures @xmath18 associated to the optimization problem @xmath19 in the sense that @xmath20 where @xmath21 denotes the uniform distribution on the set @xmath22 and @xmath23 the set of maximizers .",
    "the idea behind this approach is to first sample from a simple distribution , potentially learn about the characteristics of the associated family and smoothly move towards distributions with more mass concentrated in the maxima .",
    "we review two well - known techniques to explicitly construct such a family @xmath24 .",
    "we call @xmath18 a tempered family , if it has probability mass functions of the form @xmath25 where @xmath26 .",
    "as @xmath27 increases , the modes of @xmath28 become more accentuated until , in the limit , all mass is concentrated on the set of maximizers .",
    "the name reflects the physical interpretation of @xmath29 as the probability of a configuration @xmath11 for an inverse temperature @xmath27 and energy function @xmath30 .",
    "this is the sequence used in simulated annealing @xcite .",
    "we call @xmath18 a level set family , if it has probability mass functions of the form @xmath31 where @xmath32\\leq 1}$ ] for @xmath33 .",
    "indeed , @xmath34 is the super - level set of @xmath2 with respect to the level @xmath35 , for @xmath36 , and @xmath37 is the uniform distribution on @xmath34 . as @xmath27 increases , the support of @xmath28 becomes restricted to the points that have an objective value sufficiently close to the maximum of the @xmath2 . in the limit",
    ", the support is reduced to the set of global maximizers .",
    "the particle - driven optimization algorithms are computationally more involved than local search heuristics since we need to construct a sequence of distributions instead of a sequence of states .",
    "we shall see that this effort pays off in strongly multi - modal scenarios , where even sophisticated local search heuristics can get trapped in a subset of the state space .",
    "while the tempered sequence is based on a physical intuition , the level set sequence has an immediate interpretation as a sequence of rare events since , as @xmath38 increases , the super - level set becomes a ` rare event ' with respect to the uniform measure .",
    "rare event simulation and global optimization are therefore closely related concepts and methods for rare event estimation can often be adapted to serve as optimization algorithms .",
    "particle algorithms for rare event simulation include the cross - entropy method @xcite and the sequential monte carlo sampler @xcite .",
    "the former uses the level set sequence , the latter uses a _ logistic potential family _ @xmath39),\\end{aligned}\\ ] ] where @xmath40)$ ] and @xmath41^{-1}$ ] denotes the logistic function . did not specifically design their algorithm for optimization but their approach to static rare event simulation is closely related to the particle optimization framework .",
    "we discuss a static sequential monte carlo sampler that uses a transition kernel with independent proposals in the move step based on suitable parametric families . this methodology has been demonstrated to reliably estimate the mean of the posterior distribution in bayesian variable selection problems for linear normal models @xcite . in this section",
    ", we provide a self - contained description of this framework . for a more general overview of sequential monte carlo methods we refer to .",
    "for convenience of notation , we index the sequence of distributions @xmath42 directly by @xmath43 rather that by the parameter of the family @xmath44 .",
    "we refer to @xmath45 and @xmath46^n$ ] with @xmath47 as a _ particle system _ with @xmath48 particles .",
    "we say the particle system @xmath49 _ targets _ the probability distribution @xmath50 if the empirical distribution @xmath51 converges to @xmath50 for @xmath52 .",
    "we sample @xmath53 with @xmath54 and set @xmath55 to initialize the system .",
    "suppose we are given a particle approximation @xmath56 of @xmath57 and want to target the subsequent distribution @xmath58 . for all @xmath59 and @xmath60",
    ", we update the weights to @xmath61 and @xmath62 denotes the unnormalized version of @xmath28 .",
    "the normalizing constants @xmath63 and @xmath64 defined in equations and are unknown but the algorithm only requires ratios of unnormalized probability mass functions .",
    "we refer to @xmath65 as the _ step length _ at time @xmath43 . after updating , the particle system targets the distribution @xmath66",
    "as we choose @xmath65 larger , that is @xmath67 further from @xmath68 , the weights become more uneven and the accuracy of the importance approximation deteriorates .",
    "if we repeat the weighting step , we just increase @xmath65 and finally obtain an importance sampling estimate of @xmath69 with instrumental distribution @xmath70 .",
    "this yields a poor estimator since the probability to hit the set @xmath71 with @xmath48 uniform draws is @xmath72 and decreases rapidly as the dimension @xmath73 grows .",
    "the pivotal idea behind sequential monte carlo is to alternate moderate updates of the importance weights and improvements of the particle system via resampling and markov transitions .",
    "the importance weight degeneracy is often measured through the _ effective sample size _",
    "criterion @xcite defined as @xmath74.\\ ] ] the effective sample size is @xmath75 if the weights are uniform , that is equal to @xmath76 ; the effective sample size is @xmath76 if all mass is concentrated in a single particle .",
    "given any increasing sequence @xmath42 , we could repeatedly reweight and monitor whether the effective sample size falls below a critical threshold .",
    "for the special case of annealing via sequential monte carlo , however , the effective sample size after weighting @xmath77 is merely a function of @xmath65 . for a particle system @xmath78 at time @xmath43",
    ", we pick a step length @xmath65 such that @xmath79 that is we lower the effective sample with respect to the current particle approximation by some fixed ratio @xmath80 [ , ] .",
    "this ensures a ` smooth ' transition between two auxiliary distributions , in the sense that consecutive distributions are close enough to approximate each other reasonably well using importance weights ; in our numerical experiments , we took @xmath81 .",
    "we obtain the associated sequence @xmath82 by setting @xmath83 where @xmath84 is a unique solution of .",
    "since @xmath77 is continuous and monotonously decreasing in @xmath65 we can use bi - sectional search to solve .      in this section ,",
    "we discuss how to condition the weighted system and improve its quality before we proceed with the next weighting step .",
    "we replace the system @xmath85 targeting @xmath58 by a selection of particles @xmath86 drawn from the current particle reservoir @xmath87 such that @xmath88 where @xmath89 denotes the number of particles identical with @xmath90 .",
    "thus , in the resampled system particles with small weights have vanished while particles with large weights have been multiplied . for the implementation of the resampling step ,",
    "there exist several recipes .",
    "we could apply a multinomial resampling @xcite which is straightforward .",
    "there are , however , more efficient ways like residual @xcite , stratified @xcite and systematic resampling @xcite .",
    "we use the latest in our simulations , see procedure [ algo : resample ] .",
    "@xmath91 + * sample * @xmath92}$ ] + @xmath93      if we repeated the weighting and resampling steps several times , we would rapidly reduce the number of different particles to a very few .",
    "the key to fighting the depletion of the particle reservoir is moving the particles according to a markov transition kernel @xmath94 with invariant measure @xmath58 .",
    "the particle @xmath95 is by construction approximately distributed according to @xmath58 , and a draw @xmath96 is therefore again approximately distributed according to @xmath58 . the last sample of the generated markov chain @xmath97 is , for sufficiently many move steps @xmath98 , almost exactly distributed according to the invariant measure @xmath58 and independent of its starting point .",
    "while we could always apply a fixed number of move steps , we rather use an adaptive stopping criterion based on the number of distinct particles .",
    "we define the _ particle diversity _ as @xmath99.\\ ] ]    ideally , the sample diversity @xmath100 should correspond to the expected diversity @xmath101 where @xmath102 is the smallest value that solves @xmath103 .",
    "this is the particle diversity we would expect if we had an independent sample from @xmath104 .",
    "therefore , if @xmath94 is fast - mixing , we want to move the system until @xmath105 since the quantity on the right hand side is unknown , we stop moving the system as soon as the particle diversity reaches a steady state we can not push it beyond @xcite .",
    "more precisely , we stop if the absolute diversity is above a certain threshold @xmath106 or the last improvement of the diversity is below a certain threshold @xmath107 .",
    "we always stop after a finite number of steps but the thresholds @xmath108 and @xmath109 need to be calibrated to the efficiency of the transition kernel . for slow - mixing kernels ,",
    "we recommend to perform batches of consecutive move steps instead of single move steps .",
    "if the average acceptance rate @xmath110 of the kernel ( see section [ sec : kernels ] ) is smaller than @xmath109 , it is likely that the algorithm stops after the first iteration although further moves would have been necessary .",
    "we could adaptively adjust the threshold @xmath109 to be proportional to an estimate of the average acceptance rate ; for our numerical experiments , however , we kept it fixed to @xmath111 .",
    "@xmath112 + @xmath113      most transition kernels in monte carlo simulations are some variant of the metropolis - hastings kernel ( see e.g. ) , @xmath114 where we sample from the kernel by proposing a new state @xmath115 and accepting the proposal with probability @xmath116 or returning @xmath90 otherwise .",
    "again , we denote by @xmath117 the unnormalized version of @xmath58 since the kernel only requires the ratio of the unnormalized probability mass functions .    on binary spaces ,",
    "a common choice for the proposal distribution is @xmath118 with weight vector @xmath119^d$ ] normalized such that @xmath120 .    with probability @xmath121 ,",
    "the kernel proposes a uniform draw from the @xmath3-neighborhood of @xmath90 , @xmath122 we refer to this type of kernel as _ symmetric kernel _ since @xmath123 and equation simplifies .",
    "this class of kernels provide a higher mutation rate than the random - scan gibbs kernel ( see for adiscussion ) .",
    "locally operating transition kernels of the symmetric type are known to be slowly mixing .",
    "if we put most weight on small values of @xmath3 , the kernel only changes one or a few entries in each step .",
    "if we put more weight on larger values of @xmath3 , the proposals will hardly ever be accepted if the invariant distribution @xmath50 is multi - modal .",
    "ideally , we want the particles sampled from the transition kernel to be nearly independent after a few move steps which is often hard to achieve using local transition kernels .    for the sequential monte carlo algorithm",
    ", we use _ adaptive independent kernels _ which have proposal distributions of the kind @xmath124 which do not depend on the current state @xmath90 but have a parameter @xmath125 which we adapt during the course of the algorithm .",
    "the adaptive independent kernel is rapidly mixing if we can fit the _",
    "parametric family _",
    "@xmath126 such that the proposal distribution @xmath127 is sufficiently close to the target distribution @xmath58 , yielding thus , on average , high acceptance rates @xmath128 . the general idea behind this approach",
    "is to take the information gathered in the current particle approximation into account ( see e.g. ) .",
    "the usefulness of this strategy for sampling on binary spaces has been illustrated by .",
    "we fit a parameter @xmath129 to the particle approximation of @xmath58 according to some suitable criterion .",
    "precisely , @xmath129 is taken to be the maximum likelihood or method of moments estimator applied to the weighted sample @xmath130 .",
    "the choice of the parametric family @xmath126 is crucial to the implementation of a sequential monte carlo sampler with adaptive independent kernel .",
    "we discuss this issue in detail in section [ sec : pfbs ] .",
    "adaptation could , to a certain extent , also be done for local transition kernels . propose an adaptive kernel which replaces the full conditional distribution of the gibbs sampler by an easy to compute linear approximation which is estimated from the sampled particles .",
    "this method accelerates gibbs sampling if the target distribution @xmath50 is hard to evaluate but does not provide fast mixing like the adaptive independent kernel ( see for a comparison ) .",
    "still , the use of local kernels in the context of the proposed sequential monte carlo algorithm might be favorable if , for instance , the structure of the problem allows to rapidly compute the acceptance probabilities of local moves .",
    "further , batches of local moves can be alternated with independent proposals to ensure that the algorithm explores the neighborhood of local modes sufficiently well .",
    "since the sample space @xmath131 is discrete , a given particle is not necessarily unique .",
    "this raises the question whether it is sensible to store multiple copies of the same weighted particle in our system . in the sequel",
    ", we discuss some more details concerning this issue which has only been touched upon briefly by .",
    "let @xmath89 denote the number of copies of the particle @xmath90 in the system @xmath49 .",
    "indeed , for parsimonious reasons , we could just keep a single representative of @xmath90 and aggregate the associated weights to @xmath132 .      shifting weights between identical particles",
    "does not affect the nature of the particle approximation but it obviously changes the effective sample size @xmath133 which is undesirable since we introduced the effective sample size as a criterion to measure the goodness of a particle approximation . from an aggregated particle system , we can not distinguish the weight disparity induced by reweighting according to the importance function and the weight disparity induced by multiple sampling of the same states which occurs if the mass of the target distribution is concentrated .",
    "more precisely , we can not tell whether the effective sample size is actually due to the gap between @xmath134 and @xmath58 or the presence of particle copies due to the mass of @xmath134 concentrating on a small proportion of the state space which occurs by construction of the auxiliary distribution in section [ sec : stat model ] .",
    "aggregating the weights means that the number of particles is not fixed at runtime . in this case , the straightforward way to implement the move step presented in section [ sec : move ] is breaking up the particles into multiple copies corresponding to their weights and moving them separately .",
    "but instead of permanently splitting and pooling the weights it seems more efficient to just keep the multiple copies .",
    "we could , however , design a different kind of resample - move algorithm which first augments the number of particles in the move step and then resamples exactly @xmath48 weighted particles from this extended system using a variant of the resampling procedure proposed by . a simple way to augment the number of particles is sampling and reweighting via @xmath135 where @xmath136 denotes the acceptance probability of the metropolis - hastings kernel .",
    "we tested this variant but could not see any advantage over the standard sampler presented in the preceding sections .",
    "for the augment - resample type algorithm the implementation is more involved and the computational burden significantly higher . in particular , the rao - blackwellization effect one might achieve when replacing",
    "the accept - reject steps of the transition kernel by a single resampling step does not seem to justify the extra computational effort .    indeed ,",
    "aggregating the weights does not only prevent us from using the effective sample size criterion , but also requires extra computational time of @xmath137 in each iteration of the move step since pooling the weights is as complex as sorting . with our application in mind ,",
    "however , computational time is more critical than memory , and we therefore recommend to refrain from aggregating the weights .",
    "[ sec : pfbs ] we review three parametric families on @xmath131 . in contrast to the similar discussion in @xcite , we also consider a parametric family which can not be used in sequential monte carlo samplers but in the context of the cross - entropy method . for more details on parametric families on binary spaces",
    "we refer to .",
    "we frame some properties making a parametric family suitable as proposal distribution in sequential monte carlo algorithms .",
    "a.   for reasons of parsimony , we prefer a family of distributions with at most @xmath138 parameters like the multivariate normal .",
    "b.   given a sample @xmath139 from the target distribution @xmath50 , we need to estimate @xmath140 in a reasonable amount of computational time .",
    "c.   we need to generate samples @xmath141 from the family @xmath126 .",
    "we need the rows of @xmath142 to be independent .",
    "d.   for the sequential monte carlo algorithm , we need to evaluate @xmath143 point - wise .",
    "however , the cross - entropy method still works without this requirement .",
    "e.   we want the calibrated family @xmath144 to reproduce e.g. the marginals and covariance structure of @xmath50 to ensure that the parametric family @xmath144 is sufficiently close to @xmath50 .      the simplest non - trivial distributions on @xmath131",
    "are certainly those having independent components .    for a vector @xmath145 of marginal probabilities",
    ", we define the _ product family _",
    "we check the requirement list from section [ sec : properties ] : ( a ) the product family is parsimonious with @xmath147 .",
    "( b ) the maximum likelihood estimator @xmath148 is the weighted sample mean .",
    "( c ) we can easily sample @xmath149 .",
    "( d ) we can easily evaluate the mass function @xmath150 .",
    "( e ) however , the product family does not reproduce any dependencies we might observe in @xmath49 .    the last point is the crucial weakness which makes the product family impractical for particle optimization algorithms on strongly multi - modal problems .",
    "consequently , the rest of this section deals with ideas on how to sample binary vectors with a given dependence structure .",
    "there are , to our knowledge , two major strategies to this end .    1 .   we construct a generalized linear model which permits to compute the conditional distributions .",
    "we apply the chain rule and write @xmath126 as @xmath151 which allows to sample the entries of a random vector component - wise .",
    "we sample from an auxiliary distribution @xmath152 and map the samples into @xmath131 .",
    "we call @xmath153 a copula family , although we refrain from working with explicit uniform marginals .    we first present a generalized linear model and then review a copula approach .      even for rather simple non - linear models",
    "we usually can not derive closed - form expressions for the marginal probabilities required for sampling according to .",
    "therefore , we might directly construct a parametric family from its conditional probabilities .",
    "we define , for a lower triangular matrix @xmath154 , the _",
    "logistic conditionals family _ as @xmath155^{1-\\gamma_i}\\end{aligned}\\ ] ] where @xmath41^{-1}$ ] is the logistic function .",
    "we readily identify the product family @xmath156 as the special case @xmath157 .",
    "the virtue of the logistic conditionals family is that , by construction , we can sample a random vector component - wise while the full probability @xmath158 of the sample @xmath159 is computed as a by - product of procedure [ algo : sampling ] .",
    "we refer to the online supplement for instructions on how to fit the parameter @xmath160 .",
    "@xmath161 + @xmath162      we check the requirement list from section [ sec : properties ] : ( a ) the logistic conditionals family is sufficiently parsimonious with @xmath163 .",
    "( b ) we can fit the parameter @xmath160 via likelihood maximization .",
    "the fitting is computationally intensive but feasible .",
    "( c ) we can sample @xmath164 using the chain rule factorization .",
    "( d ) we can exactly evaluate @xmath165 .",
    "( e ) the family @xmath166 reproduces the dependency structure of the data @xmath167 although we can not explicitly compute the marginal probabilities .",
    "let @xmath152 be a family of multivariate auxiliary distributions on @xmath168 and @xmath169 a mapping into the binary space .",
    "we can sample from the copula family by setting @xmath170 for a draw @xmath171 from the auxiliary distribution .",
    "most multivariate parametric families with at most @xmath138 parameters appear to either have a rather limited dependency range or they do not scale to higher dimensions @xcite .",
    "therefore , the natural and seemingly only viable option for @xmath152 is the multivariate normal distribution @xcite .    for a vector @xmath172 and a correlation matrix @xmath173",
    ", we introduce the mapping @xmath174}(v_1),\\dots,\\ind_{(-\\infty , a_d]}(v_d)),\\ ] ] and define the _ gaussian copula family _ as @xmath175    for index sets @xmath176 , the cross - moments @xmath177 are equal the cumulative distribution function of the multivariate normal with respect to the entries indexed by @xmath14 ( see for a more detailed discussion ) .",
    "in particular , the first and second moments are @xmath178 where @xmath179 and @xmath180 denote the cumulative distribution functions of the univariate and bivariate normal distributions with zero mean , unit variance and correlation coefficient @xmath181 $ ] .",
    "we refer to the online supplement for instructions on how to fit the parameters @xmath182 and @xmath183 .",
    "we check the requirement list from section [ sec : properties ] : ( a ) the gaussian copula family is sufficiently parsimonious with @xmath163 .",
    "( b ) we can fit the parameters @xmath182 and @xmath184 via method of moments . however , the parameter @xmath184 is not always positive definite .",
    "( c ) we can sample @xmath185 using @xmath186 with @xmath187 .",
    "( d ) we can not easily evaluate @xmath188 since this requires computing high - dimensional integral expressions which is a computationally challenging problem in itself ( see e.g. ) .",
    "the gaussian copula family is therefore less useful for sequential monte carlo samplers but can be incorporated into the cross - entropy method reviewed in section [ sec : cross entropy ] .",
    "( e ) the family @xmath189 reproduces the exact mean and , possibly scaled , correlation structure .",
    "we briefly discuss a toy example to illustrate the usefulness of the parametric families . for the quadratic function @xmath190 the associated probability mass function @xmath191 has a correlation matrix @xmath192 which indicates that this distribution has considerable dependencies and its mass function is therefore strongly multi - modal .",
    "we generate pseudo - random data from @xmath50 , adjust the parametric families to the data and plot the mass functions of the fitted parametric families .",
    "figure [ fig : toy exa ] shows how the three parametric families cope with reproducing the true mass function .",
    "clearly , the product family is not close enough to the true mass function to yield a suitable instrumental distribution while the logistic conditional family almost copies the characteristics of @xmath50 and the gaussian copula family allows for an intermediate goodness of fit .",
    "in this section , we provide a synopsis of all steps involved in the sequential monte carlo algorithm and connect this framework to the cross - entropy method and simulated annealing . in table",
    "[ tab : seq ] , we state the necessary formulas for the tempered and the level set sequence introduced in section [ sec : stat model ] .      for convenience , we summarize the complete sequential monte carlo sampler in algorithm [ algo : smc ] . note that , in practice , the sequence @xmath68 is not indexed by @xmath43 but rather by @xmath44 , which means that the counter @xmath43 is only given implicitly .",
    "the algorithm terminates if the particle diversity sharply drops below some threshold @xmath193 which indicates that the mass has concentrated in a single mode .",
    "if we use a kernel with proposals from a parametric family @xmath194 , we might already stop if the family degenerates in the sense that only a few components of @xmath194 , say less than @xmath195 , are random while the others are constant ones or zeros .",
    "in this situation , additional moves using a parametric family are a pointless effort . we either return the maximizer within the particle system or we solve the subproblem of dimension @xmath196 by brute force enumeration .",
    "we might also perform some final local moves in order to further explore the regions of the state space the particles concentrated on .",
    "* sample * @xmath197 * for all * @xmath59 .",
    "+ @xmath198 , @xmath199 + @xmath200      for the level set sequence , the effective sample size is the fraction of the particles which have an objective function value greater than @xmath201 ; see table [ tab : seq ] and equation .",
    "the remaining particles are discarded since their weights equal zero .",
    "consequently , there is no need to explicitly compute @xmath84 as a solution of .",
    "we simply order the particles @xmath202 according to their objective values @xmath203 and only keep the @xmath204 particles with the highest objective values .",
    "rubinstein @xcite , who popularizes the use of level set sequences in the context of the cross - entropy method , refers to @xmath204 as the size of the _ elite sample_. the cross - entropy method has been applied successfully to a variety of combinatorial optimization problems , some of which are equivalent to pseudo - boolean optimization @xcite , and is closely related to the proposed sequential monte carlo framework .    however , the central difference between the cross - entropy method and the sequential monte carlo algorithm outlined above is the use of the invariant transition kernel in the latter .",
    "we obtain the cross - entropy method as a special case if we replace the kernel @xmath205 by its proposal distribution @xmath194 .",
    "the sequential monte carlo approach uses a smooth family of distributions @xmath18 and explicitly schedules the evolution @xmath68 which in turn leads to the proposal distributions @xmath194 .",
    "the cross - entropy method , in contrast , defines the subsequent proposal distribution @xmath206 without any reference sequence @xmath134 to balance the speed of the particle evolution .    in order to decelerate the advancement of the cross - entropy method",
    ", we introduce a lag parameter @xmath207 and use a convex combination of the previous parameter @xmath208 and the parameter @xmath209 fit to the current particle system , setting @xmath210 however , there are no guidelines on how to adjust the lag parameter during the run of the algorithm .",
    "therefore , the sequential monte carlo algorithm is easier to calibrate since the reference sequence @xmath134 controls the stride and automatically prevents the system from overshooting .    on the upside",
    ", the cross - entropy method allows for a broader class of auxiliary distributions @xmath194 since we do not need to evaluate @xmath211 point - wise which is necessary in the computation of the acceptance probability of the hastings kernel ; see section [ sec : gaussian copula ] .",
    ".formulas for optimization sequences [ cols=\"<,^,^ \" , ]      a well - studied approach to pseudo - boolean optimization is simulated annealing @xcite . while the name stems from the analogy to the annealing process in metallurgy , there is a pure statistical meaning to this setup .",
    "we can picture simulated annealing as approximating the mode of a tempered sequence using a single particle .",
    "since a single observation does not allow for fitting a parametric family , we have to rely on symmetric transition kernels in the move step .",
    "a crucial choice is the sequence @xmath44 which in this context is often referred to as the _ cooling schedule_. there is a vast literature advising on how to calibrate @xmath44 where a typical guideline is the expected acceptance rate of the hastings kernel .",
    "we calibrate @xmath44 such that the empirical acceptance rate @xmath212 follows approximately @xmath213 for @xmath214 $ ] .",
    "there are variants of simulated annealing which use more complex cooling schedules , tabu lists and multiple restarts , but we stick to this simple version for the sake of simplicity .",
    "algorithm [ algo : sim ann ] describes the version we use in our numerical experiments in section [ sec : perf algo ] .",
    "@xmath215 ( time elapsed ) + @xmath216      we describe a greedy local search algorithm which works on any state space that allows for defining a neighborhood structure .",
    "the typical neighborhood on binary spaces is the @xmath3-neighborhood defined in .",
    "a greedy local search algorithm computes the objective value of all states in the current neighborhood and moves to the best state found until a local optimum is reached .",
    "the local search algorithm is called @xmath3-opt if it searches the neighborhood @xmath217 ( see e.g. for a discussion )",
    ".    the algorithm can be randomized by repeatedly restarting the procedure from randomly drawn starting points .",
    "there are more sophisticated versions of local search algorithms exploit the properties of the objective function but even a simple local search procedure can produce good results @xcite . algorithm [ algo : local search ] describes the @xmath75-opt local search procedure we use in our numerical experiments in section [ sec : perf algo ] .",
    "@xmath218 ( time elapsed ) + @xmath216",
    "it is well - known that any pseudo - boolean function @xmath219 can be written as a multi - linear function @xmath220 where @xmath221 are real - valued coefficients .",
    "we say the function @xmath2 is of order @xmath3 if the coefficients",
    "@xmath222 are zero for all @xmath176 with @xmath223 .",
    "while optimizing a first order function is trivial , optimizing a non - convex second order function is already an np - hard problem @xcite .    in the sequel , we focus on optimization of second order pseudo - boolean functions to exemplify the stochastic optimization schemes discussed in the preceding sections . if @xmath2 is a second order function , we restate program as @xmath224 \\text{subject to } & $ \\v x\\in\\b^d$ , \\end{tabular}\\ ] ] where @xmath225 is a symmetric matrix .",
    "we call an unconstrained quadratic binary optimization problem ( uqbo ) ; we refer to for a list of applications and equivalent problems . in the literature it is also referred to as unconstrained quadratic boolean or bivalent or zero - one programming @xcite .",
    "meta - heuristics are a class of algorithms that optimize a problem by improving a set of candidate solutions without systematically enumerating the state space ; typically they deliver solutions in polynomial time while an exact solution has exponential worst case running time .",
    "the outcome is neither guaranteed to be optimal nor deterministic since most meta - heuristics are randomized algorithms .",
    "we briefly discuss the connection to particle optimization against the backdrop of the unconstrained quadratic binary optimization problem where we roughly separate them into two classes : local search algorithms and particle - driven meta - heuristics .",
    "local search algorithms iteratively improve the current candidate solution through local search heuristics and judicious exploration of the current neighborhood ; examples are local search @xcite , tabu search @xcite , simulated annealing @xcite .",
    "particle driven meta - heuristics propagate a set of candidate solutions and improve it through recombination and local moves of the particles ; examples are genetic algorithms @xcite , memetic algorithms @xcite , scatter search @xcite . for comparisons of these methods",
    "we refer to or .",
    "the sequential monte carlo algorithm and the cross - entropy method are clearly in the latter class of particle - driven meta - heuristics .",
    "the idea behind sequential monte carlo is closely related to the intuition behind population ( or swarm ) optimization and genetic ( or evolutionary ) algorithms .",
    "however , the mathematical framework used in sequential monte carlo allows for a general formulation of the statistical properties of the particle evolution while genetic algorithms are often problem - specific and empirically motivated .      if we can explicitly derive the multi - linear representation of the objective function , there are techniques to turn program into a linear program . for the uqbo it reads @xmath226 \\text{subject to } & $ \\v",
    "x\\in\\b^{d(d+1)/2}$ \\\\            & $ \\left .",
    "\\begin{array}{l }                    \\hspace{-1ex}x_{ij}\\leq x_{ii } \\\\",
    "\\hspace{-1ex}x_{ij}\\leq x_{jj } \\\\",
    "\\hspace{-1ex}x_{ij}\\geq x_{ii } + x_{jj } -1 \\\\",
    "\\end{array }              \\right\\}\\text { for all}\\ i , j\\in \\dset{1,d}$. \\end{tabular}\\ ] ] note , however , that there are more parsimonious linearization strategies than this straightforward approach [ , ] .",
    "the transformed problem allows to access the tool box of linear integer programming which consist of branch - and - bound algorithms that are combined with rounding heuristics , various relaxations techniques and cutting plane methods [ , ] .",
    "naturally , the question arises whether particle - driven meta - heuristics can be incorporated into exact solvers to improve branch - and - bound algorithms . indeed ,",
    "stochastic meta - heuristics deliver lower bounds for maximization problems , but particle - driven algorithms are computationally somewhat expensive for this purpose unless the objective function is strongly multi - modal and other heuristics fail to provide good results ; see the discussion in section [ sec : extreme ]",
    ".    however , the sequential monte carlo approach in combination with the level set sequence might also be useful to determine a global branching strategy , since the algorithm provides an estimator for @xmath227 which is the average of the super - level set @xmath228 .",
    "these estimates given for a sequence of levels @xmath229 might provide branching strategies than are superior to local heuristics or branching rules based on fractional solutions .",
    "a further discussion of this topic is beyond the scope of this paper but it certainly merits consideration .",
    "the meta - heuristics we want to compare do not exploit the quadratic structure of the objective function and might therefore be applied to any binary optimization program . if the objective function can be written in multi - linear form like there are efficient local search algorithms @xcite which exploit special properties of the target function and easily beat particle methods in terms of computational time .",
    "therefore , the use of particle methods is particularly interesting if the objective function is expensive to compute or even a black box .",
    "the posterior distribution in bayesian variable selection for linear normal models is an example of such an objective function ( see and references therein ) .",
    "we stick to the uqbo for our numerical comparison since problem instances of varying difficulty are easy to generate and interpret while the results carry over to general binary optimization .    in the vast literature on uqbo",
    ", authors typically compare the performance of meta - heuristics on a suite of randomly generated problems with certain properties .",
    "proposes standardized performance tests on symmetric matrices @xmath230 with entries @xmath231 drawn from the uniform",
    "@xmath232 the test suites generated by ( * ? ?",
    "? * http://people.brunel.ac.uk/~mastjjb/jeb/orlib/bqpinfo.html[or-library ] ) and follow this approach have been widely used as benchmark problems in the uqbo literature ( see for an overview ) . in the sequel",
    "we discuss the impact of diagonal dominance , shifts , the density and extreme values of @xmath233 on the expected difficulty of the corresponding uqbo problem .      generally , stronger _ diagonal dominance _ in @xmath233 corresponds to easier uqbo problems @xcite .",
    "consequently , the original problem generator presented by is designed to draw the off - diagonal elements from a uniform on a different support @xmath234 with @xmath235 .    in this context",
    ", we point out that the impact of diagonal dominance carries over to the statistical properties of the tempered distributions we defined in the introductory section [ sec : stat model ] . indeed ,",
    "stronger diagonal dominance in @xmath233 corresponds to exponential quadratic distributions @xmath236 having lower dependencies between the components of @xmath237 .",
    "we can analytically derive a parameter @xmath154 for a logistic conditionals family @xmath166 that approximates @xmath238 where the quality of the approximation increases as the diagonal of @xmath233 becomes more dominant @xcite .",
    "we can accelerate the sequential monte carlo algorithm by initializing the system from @xmath166 instead of @xmath239 .",
    "however , we did not exploit this option to keep the present work more concise .    for positive definite @xmath240 ,",
    "the optimization problem is convex and can be solved in polynomial time @xcite ; in exact optimization , this fact is exploited to construct upper bounds for maximization problems @xcite .",
    "we observe a corresponding complexity reduction in statistical modeling . for @xmath240 ,",
    "the auxiliary distribution @xmath241 is a feasible mass function , and we can derive analytical expressions concerning all cross - moments and marginal distributions @xcite which allows to largely analyze the properties of @xmath238 without enumerating the state space .",
    "the global optimum of the uqbo problem is more difficult to detect as we shift the entries of the matrix @xmath233 but the relative gap between the optimum and any heuristic value diminishes .",
    "if we sample @xmath242 from a uniform on the _ shifted _ support @xmath243 we obtain an objective function @xmath244 where @xmath245 means equality in distribution .",
    "hence , with growing @xmath246 the optimum depends less on @xmath233 and the relative gap between the optimum and a solution provided by any meta - heuristic vanishes .",
    "define a related criterion @xmath247\\ ] ] and report a significant impact of @xmath248 on the solution quality of their local search algorithms which is not surprising .",
    "the difficulty of the optimization problem is related to the number of interactions , that is the number of non - zero elements of @xmath233 .",
    "we call the proportion of non - zeros the _ density _ of @xmath233 . drawing @xmath231 from the mixture @xmath249\\ ] ] we adjust the difficulty of the problem to a given expected density @xmath250 .",
    "note that not all algorithms are equally sensitive to the density of @xmath233 . using the basic linearization",
    ", each non - zero off - diagonal element requires the introduction of an auxiliary variable and three constraints .",
    "thus , the expected total number of variables and the expected total number of constraints , which largely determine the complexity of the optimization problem , are proportional to the density @xmath250 .    on the other hand , many randomized approaches , including the particle algorithms discussed in section [ sec : smc ] , are less sensitive to the density of the problem in the sense that replacing zero elements by small values has a minor impact on the performance of these algorithms . rather than the zero / non - zero duality",
    ", we suggest that the presence of extreme values determines the difficulty of providing heuristic solutions .      the uniform sampling approach advocated by is widely used in the literature for comparing meta - heuristics .",
    "certainly , particle - driven methods are computationally too expensive to outperform local search heuristics on test problems with uniformly drawn entries ; @xcite confirms this intuition with respect to genetic algorithms versus tabu search and simulated annealing .",
    "however , the uniform distribution does not produce _ extreme values _ and it is vital to keep in mind that these have an enormous impact on the performance of local search algorithms .",
    "extreme values in @xmath233 lead to the existence of distinct local maxima @xmath251 of @xmath2 in the sense that there is no better candidate solution than @xmath252 in the neighborhood @xmath253 even for relatively large @xmath3 .",
    "further , extreme local minima might completely prevent a local search heuristic from traversing the state space in certain directions .",
    "consequently , local search algorithms , as discussed in section [ sec : meta ] , depend more heavily on their starting value , and their performance deteriorates with respect to particle - driven algorithms .",
    "we propose to draw the matrix entries @xmath231 from a discretized cauchy distribution @xmath254 that has heavy tails which cause extreme values to be frequently sampled .",
    "figure [ fig : problem distr ] shows the distribution of a cauchy and a uniform to illustrate the difference .",
    "the resulting uqbo problems have quite distinct local maxima ; in that case we also say that the function @xmath255 is _ strongly multi - modal_.     and a uniform @xmath256 distribution .",
    "]      in this section , we provide numerical comparisons based on instances of the uqbo problem .",
    "we generated two random test suites of dimension @xmath257 , each having @xmath258 instances .",
    "for the first suite , we sampled the matrix entries from a uniform distribution @xmath259 on @xmath260 ; for the second , we sampled from a cauchy distribution @xmath261 as defined in .",
    "for performance evaluation , we run a specified algorithm @xmath262 times on the same problem and denote the outcome by @xmath263 .      since the absolute values are not meaningful , we report the relative ratios @xmath264,\\ ] ] where the best known solution is the highest objective value ever found for that instance and the worst solution is the lowest objective value among the @xmath262 outcomes .",
    "we summarize the results in a histogram .",
    "the first @xmath48 bins are singletons @xmath265 for the highest values @xmath266 ; the following @xmath48 bins are equidistant intervals @xmath267 .",
    "the graphs show the bins @xmath268 in descending order from left to right on the @xmath269-axis .",
    "the interval bins are marked with a sign `` @xmath270 '' and the lower bound .",
    "the @xmath271-axis represents the counts .    for comparison ,",
    "we draw the outcome of several algorithms into the same histogram , where the worst solution found is the lowest overall objective value among the outcomes . for each algorithm ,",
    "the counts are depicted in a different color and , for better readability , with diagonal stripes in a different angle . to put it",
    "plainly , an algorithm performs well if its boxes are on the left of the graph since this implies that the outcomes where often close to the best known solution .",
    "we study how the choice of the binary parametric family affects the quality of the delivered solutions .",
    "the focus is on the cross - entropy method , since we can not easily use the gaussian copula family in the context of sequential monte carlo .",
    "we use @xmath272 particles , set the speed parameter to @xmath273 ( or the elite fraction to @xmath274 ) and the lag parameter to @xmath275 .",
    "the numerical comparisons , given in figures [ fig : uni mc ] and [ fig : cauchy mc ] , clearly suggest that using more advanced binary parametric families allows the cross - entropy method to detect local maxima that are superior to those detected using the product family .",
    "hence , the numerical experiments confirm the intuition of our toy example in figure [ fig : toy exa ] .    on the strongly multi - modal instance [ fig : cauchy mc ]",
    "the numerical evidence for this conjecture is stunningly clear - cut ; on the weakly multi - modal problem [ fig : uni mc ] its validity is still unquestionable .",
    "this result seems natural since reproducing the dependencies induced by the objective function is more relevant in the former case than in the latter .",
    "we compare a sequential monte carlo sampler with parametric family , a sequential monte carlo sampler with a single - flip symmetric kernel , the cross - entropy method , simulated annealing and @xmath75-opt local search as described in section [ sec : algorithms ] .    for the cross entropy method , we use the same parameters as in the preceding section . for the sequential monte carlo algorithm",
    ", we use @xmath276 particles and set the speed parameter to @xmath277 ; we target a tempered auxiliary sequence . for both algorithms",
    "we use the logistic conditionals family as sampling distribution . with these configurations ,",
    "the algorithms converge in roughly @xmath278 minutes .",
    "we calibrate the sequential monte carlo sampler with local moves to have the same average run time by processing batches of @xmath258 local moves before checking the particle diversity criterion .",
    "the simulated annealing and @xmath75-opt local search algorithms run for exactly @xmath278 minutes .",
    "the results shown in figures [ fig : uni ac ] and [ fig : cauchy ac ] assert the intuition that particle methods perform significantly better on strongly multi - modal problems .",
    "however , on the easy test problems , the particle methods tend to persistently converge to the same sub - optimal local modes .",
    "this effect is probably due to their poor local exploration properties . since particle methods perform significantly less evaluations of the objective function , they are less likely to discover the highest peak in a region of rather flat local modes .",
    "the use of parametric families aggravates this effect , and it seems advisable to alternate global and local moves to make a particle algorithm more robust against this kind of behavior .",
    "further numerical results are shown in figure [ fig : r250c ] and figure [ fig : r250u ] .",
    "the numerical experiments carried out on different parametric families revealed that the use of the advanced families proposed in this paper significantly improves the performance of the particle algorithms , especially on the strongly multi - modal problems .",
    "the experiments demonstrate that local search algorithms , like simulated annealing and randomized @xmath75-opt local search , indeed outperform particle methods on weakly multi - modal problems but deliver inferior results on strongly multi - modal problems .",
    "using tabu lists , adaptive restarts and rounding heuristics , we can certainly design local search algorithms that perform better than simulated annealing and @xmath75-opt local search .",
    "still , the structural problem of strong multi - modality persists for path - based algorithms . on the other hand , cleverly designed local search heuristics will clearly beat sequential monte carlo methods on easy to moderately difficult problems .",
    "the results encourage the use of particle methods if the objective function is known to be potentially multi - modal and hard to analyze analytically .",
    "we have to keep in mind that multiple restarts of rather simple local search heuristics can be very efficient if they make use of the structure of the objective function . for @xmath278 minutes of randomized restarts , the heuristic proposed by , which exploits the fact that the partial derivatives of a multi - linear function are constant",
    ", practically always returns the best known solution on all test problems treated to create figures [ fig : r250c ] and [ fig : r250u ] .",
    "the numerical work was completely done in http://www.python.org/[python 2.6 ] using http://www.scipy.org[scipy ] packages and run on a cluster with @xmath279 ghz processors .",
    "the sources used in this work and the problems processed in this paper can be found at http://code.google.com/p/smcdss .",
    "this work is part of the author s ph.d .",
    "thesis at crest under supervision of nicolas chopin whom i would like to thank for the numerous discussions on particle algorithms .",
    "i thank the editor and two anonymous referees for their detailed comments which helped to significantly improve this paper .",
    "we briefly summarize how the parameters of the logistic conditionals family and the gaussian copula family can be assessed for a given particle system @xmath139 with weights @xmath280 .",
    "we denote by @xmath281 the weighted first and second sample moments .",
    "the log - likelihood function of the weighted logistic regression of @xmath282 on @xmath283 is @xmath284+(1-y^{(i)}_k)\\log[1-\\logistic(\\v z_{k \\bullet}^{(i)}\\v a)]\\right ] \\\\ & = \\sum_{k=1}^n w_k\\left[y^{(i)}_k\\v z_{k \\bullet}^{(i)}\\v a-\\log[1+\\exp(\\v z_{k \\bullet}^{(i)}\\v a)]\\right],\\end{aligned}\\ ] ] where we used that @xmath285=-\\log[1+\\exp(\\v x\\t\\v a)]=-\\v x\\t\\v a+\\log[\\logistic(\\v x\\t\\v a)]$ ] . since @xmath286/\\partial\\v a=\\logistic(\\v x\\t\\v a)\\v x$ ] , the gradient of the log - likelihood is @xmath287 = ( \\m z^{(i)})\\t \\diag(\\v w)[\\v y^{(i ) } - \\v p^{(i)}_{\\v a}],\\end{aligned}\\ ] ] where @xmath288 . since @xmath289\\v x$ ] ,",
    "the hessian matrix of the log - likelihood is @xmath290\\right]\\v",
    "z_{k \\bullet}^{(i)}(\\v z_{k \\bullet}^{(i)})\\t = -(\\m z^{(i)})\\t \\diag(\\v w ) \\diag(\\v q^{(i)}_{\\v a } ) \\m z^{(i)},\\end{aligned}\\ ] ] where @xmath291 $ ] .",
    "the data might suffer from complete or quasi - complete separation @xcite which causes the likelihood function @xmath292 to be monotonic . in that case",
    "there is no maximizer .",
    "we can avoid the monotonicity by assigning a suitable prior distribution to the parameter @xmath182 . recommends the jeffreys prior for its bias reduction which can conveniently be implemented via data adjustment @xcite .    for the sake of simplicity",
    ", however , we only assign a simple gaussian prior with variance @xmath293 such that , up to a constant , the log - posterior distribution is the log - likelihood function plus a quadratic penalty term and therefore always convex . the score function and its jacobian",
    "matrix become @xmath294 - \\varepsilon\\v a,\\quad s'(\\v a)=-(\\m z^{(i)})\\t \\diag(\\v w ) \\diag(\\v q^{(i)}_{\\v a } ) \\m z^{(i)}-\\varepsilon \\m i.\\end{aligned}\\ ] ] the bias - reduced estimators are known to shrink towards @xmath295 which is an undesired property when fitting a parametric family .",
    "therefore , we attempt to keep the shrinkage parameter @xmath296 as small as possible .      the first order condition @xmath297 is solved iteratively @xmath298^{-1}s(\\v a^{(t ) } ) = \\v a^{(t)}+\\v x^{(t)}\\end{aligned}\\ ] ] where @xmath299 is the vector that solves",
    "@xmath300\\v x^{(t)}= \\left[(\\m z^{(i)})\\t \\diag(\\v w ) [ \\v y^{(i ) } - \\v p^{(i)}_{\\v a^{(t)}}]-\\varepsilon\\v a^{(t)}\\right].\\end{aligned}\\ ] ] if the newton iteration at the @xmath301th component fails to converge , we can either augment the penalty term @xmath296 which leads to stronger shrinkage of the mean @xmath302 towards @xmath295 or we can drop some covariates @xmath303 for @xmath304 from the iteration to improve the numerical condition of the procedure .    in practice , we drop the predictors from the regression model which are only weakly correlated with the explained variable .",
    "this step is important to speed up the algorithm and improve its numerical properties . for a proposal distribution",
    ", it is particularly important to take the strong dependencies into account but it is often sufficient to work with very sparse logistic conditionals families .    in particularly difficult cases",
    ", we might prefer to set @xmath305 and @xmath306 , where @xmath307 is defined in , which guarantees that at least the mean is correct .",
    "this is an important issue since misspecification of the mean of @xmath308 also affects the distribution of the components @xmath303 for @xmath309 which are sampled conditional on @xmath308 .",
    "we adjust the gaussian copula family @xmath189 by method of moments .",
    "we need to solve the non - linear equations @xmath310 where @xmath311 and @xmath312 are defined in while @xmath313 and @xmath314 denote the cumulative distribution functions of the univariate and bivariate normal distributions with zero mean , unit variance and correlation coefficient @xmath181 $ ] .",
    "while the parameter @xmath315 is easy to assess , the challenging task is to compute the bivariate variances @xmath316 for all @xmath317 .",
    "recall the standard result [ , p.255 ] @xmath318 where @xmath319 denotes the density of the bivariate normal distribution with correlation coefficient @xmath320 .",
    "we obtain the following newton - raphson iteration @xmath321 starting at some initial value @xmath322 ; see procedure [ algo : fit gaussian ] . in the sequential monte carlo context",
    ", good initial values are obtained from the parameters of the previous auxiliary distributions .",
    "we use a fast series approximation @xcite to evaluate @xmath323 .",
    "these approximations are critical when @xmath316 comes very close to either boundary of @xmath324 $ ] .",
    "the newton iteration might repeatedly fail when restarted at the corresponding boundary @xmath325 . in any event",
    ", @xmath326 is strictly monotonic in @xmath320 since its derivative is positive , and we can switch to bi - sectional search if necessary .",
    "the locally fitted correlation matrices @xmath184 might not be positive definite for @xmath333 because the gaussian copula is not flexible enough to model the full range of cross - moments binary distributions might have ( see for an extended discussion ) .",
    "we obtain a feasible parameter replacing @xmath184 by @xmath334 where @xmath335 is smaller than all eigenvalues of the locally fitted matrix @xmath184 .",
    "this approach evenly lowers the local correlations to a feasible level and is easy to implement on standard software . in practice",
    ", we also prefer to work with a sparse version of the gaussian copula family that concentrates on strong dependencies and sets minor correlations to zero ."
  ],
  "abstract_text": [
    "<S> we discuss a unified approach to stochastic optimization of pseudo - boolean objective functions based on particle methods , including the cross - entropy method and simulated annealing as special cases . </S>",
    "<S> we point out the need for auxiliary sampling distributions , that is parametric families on binary spaces , which are able to reproduce complex dependency structures , and illustrate their usefulness in our numerical experiments . </S>",
    "<S> we provide numerical evidence that particle - driven optimization algorithms based on parametric families yield superior results on strongly multi - modal optimization problems while local search heuristics outperform them on easier problems . </S>"
  ]
}