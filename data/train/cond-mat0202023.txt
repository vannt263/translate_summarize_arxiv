{
  "article_text": [
    "the capacity with which a population of neurons , or neuron - like units , can represent a set of elements in the outside world is an important issue both for experimental studies of neural coding and for theoretical analysis of neural network models . in the former situation , typically a discrete set of @xmath0 stimuli",
    "is presented to a subject while the activity of a population of @xmath1 neurons is recorded .",
    "the measured response can be represented as an @xmath1 dimensional vector @xmath2 , whose components stand for the activity of individual neurons , computed over a predefined time window .",
    "this activity is expected to be selective , at least to some degree , to each one of the stimuli .",
    "the degree of selectivity can be quantified by the mutual information between the set of stimuli and the responses ( shannon 1948 ) @xmath3 , \\label{inf}\\ ] ] where @xmath4 is the probability of showing stimulus @xmath5 , @xmath6 is the conditional probability of observing response @xmath2 when the stimulus @xmath5 is presented and @xmath7 the mutual information @xmath8 characterizes the mapping between the @xmath0 stimuli and the response space , and represents the amount of information conveyed by @xmath2 about which of the @xmath0 elements was shown .",
    "if each of the stimuli evokes a unique set of responses , i.e. the responses are different for different stimuli , then eq . ( [ inf ] ) reduces to the entropy of the set of stimuli @xmath9 . when the stimuli are equiprobable , the subject is exposed to the most variable presentation of the @xmath0 elements , and the entropy of the stimuli reaches its maximum value , that is , @xmath10 . for other choices of @xmath4 ,",
    "the mutual information is still bounded by @xmath11 , which in turn , is less than @xmath12 .    on the other hand ,",
    "if a response @xmath2 may be evoked by more than one stimulus , the mutual information is less than @xmath11 . in the extreme case where the responses are independent of the stimuli , @xmath13 . in short",
    ", the mutual information quantifies how precisely can any single stimulus be identified out of the @xmath0 possible ones , by measuring the response of the @xmath1 units .    here",
    ", we are interested in describing the dependence of the mutual information with the number of neurons sampled .",
    "such a dependence is crucial in quantifying the redundancy between the messages carried by different cells .",
    "since information is an additive quantity , if different neurons provide independent information , we expect @xmath14 to grow linearly with @xmath1 .",
    "in fact , any departure of linearity is a sign of either a redundant or a synergetic coding ( depending on whether the behaviour is sub or supra - linear ) .",
    "however , before drawing conclusions about the way information is shared among the neurons , it is important to understand to what extent the experimental design , more precisely , the set of stimuli chosen , determines by itself the shape of @xmath14 . as stated above , the maximum information that can be extracted from the neural responses",
    "is @xmath11 .",
    "it is clear that if we have a set of neurons that already provides an information very near to this maximum , by adding one more unit we will gain no more than redundant information . in other words , we have reached a regime where the neural responses correctly identify all of the stimuli .",
    "but we can not deduce from this that the representational capacity of the responses remains unchanged when the number of neurons increases .",
    "one should rather realize that the task itself is no longer appropriate to test the way additional neurons contribute in the encoding of the stimuli .",
    "gawne and richmond ( 1993 ) have considered this issue quantitatively , when recording from pairs of neurons .",
    "they presented a simple model which yields an analytical expression for @xmath14 under the assumption that each neuron provides a fixed amount of information @xmath15 , and that a fixed fraction @xmath16 of such an amount is redundant with the information conveyed by any other neuron .",
    "in figure [ f1 ]    we reproduce their scheme , where all pairs of neurons are supposed to share the same redundancy @xmath17 .",
    "their model yields @xmath18 . \\label{gr}\\ ] ] here , @xmath15 and @xmath16 are considered independent quantities .",
    "gawne and richmond ( 1993 ) measured both of them , using a single electrode to record the activity of pairs of nearby neurons .",
    "they got an average value for @xmath19 bits , and their mean redundancy @xmath20/i(1 ) =   0.2 $ ] . according to equation ( [ gr ] ) , as @xmath1 increases ,",
    "@xmath14 approaches a limiting value equal to @xmath21 . in their formulation",
    ", no attention is payed to the fact that @xmath22 should approach @xmath11 , at least if the subject is behaviourally able to identify every single stimulus . according to the measured values of @xmath16 and @xmath15 , gawne and richmond calculated @xmath23 . since in their case @xmath24",
    ", they concluded that the assumption that all pairs of neurons share the same amount of redundant information , as measured by nearby neurons , was wrong .    what appeared important , then , was to go beyond what could be extrapolated from the shared information between pairs of cells , and measure directly the information that could be extracted from large populations . from the experimental point of view ,",
    "this meant to measure the activity of a large number of units , ideally , with simultaneous recordings . rolls ( 1997 ) have performed the experiment measuring the responses from cells in the inferior temporal cortex of a macaque , when exposed to @xmath0 visual stimuli .",
    "they recorded one neuron at a time , and therefore , were not able to capture the correlations in the neural responses present in every single trial .",
    "one should bear in mind , however , that such non - simultaneous recordings correctly describe the information carried by the firing rates of the neurons , and are exact in the limit of short time windows ( panzeri , 1999a ) .",
    "in figure [ f2 ]    we show their estimation of @xmath14 , after a decoding procedure . in their experiment , stimuli were equiprobable , so @xmath25 .",
    "diamonds correspond to @xmath26 , squares to @xmath27 and triangles to @xmath28 .",
    "the graph is plotted as a function of the number of neurons sampled .",
    "each line is an average of all the possible curves that could be made , depending on the order in which the neurons are selected .",
    "we see that initially the information grows linearly , and later descelerates . for @xmath28",
    "there is a clear saturation at @xmath29 bits . for @xmath27 and @xmath26",
    "the behaviour of the curves is compatible with the predicted asimptotes , at @xmath30 bits and @xmath31 bits .",
    "this experiment shows that the pool of neurons sampled appears to reach the entropy of the set of stimuli , showing that the activity of a sufficiently large number of cells carries the information needed to correctly identify every single element .    in order to explain their results ,",
    "rolls have considered a more constrained model than the one of gawne and richmond , that further assumes that @xmath32 .",
    "this means that the only parameter to be measured is now @xmath15 .",
    "their approach also leads to equation ( [ gr ] ) , but now @xmath33 .",
    "a fit of their expression is shown by the full line in figure [ f2 ] , for the case of 20 stimuli .",
    "although their choice for the overlap @xmath16 might seem arbitrary , it is the only one that leads to the correct asymptotic behaviour . in this paper",
    "we show that such a choice is also the average redundancy if the information provided by a pair of neurons has a random overlap .    in the following section we set the problem in a probabilistic framework ,",
    "namely we calculate the probability distribution that @xmath1 neurons provide an amount @xmath8 of information .",
    "we also show that equation ( [ gr ] ) is the mean value of such a distribution , and we obtain its standard deviation .",
    "next , in section 3 we extend our model to a more complex case , and we end in section 4 with some concluding remarks .",
    "our aim is to derive the probability @xmath34 that @xmath1 neurons provide an amount @xmath8 of information when firing in response to a set of @xmath0 stimuli .",
    "we do so in terms of a single parameter , @xmath15 defined as the mean single - neuron information .",
    "we assume that the information extracted from a given neuron has a uniform probability of representing any portion of the maximum information .",
    "figure [ f3 ] shows a pictorial representation similar to the one introduced by gawne and richmond , with the difference that now the information provided by every single cell may fall anywhere in the striped area , and with a uniform probability distribution .    for the moment",
    ", we assume that the information provided by neuron @xmath35 is either entirely redundant with the one carried by the previous @xmath1 , or the whole of it is new . in other words , neurons provide information in indivisible blocks of size @xmath15 .",
    "later we allow for the possiblility of partial overlap . at the present stage ,",
    "however , @xmath8 can only increase in steps of a fixed size , namely of @xmath15 .    with these assumptions , the probability that the information extracted from the response of the neuron @xmath35 is redundant with the information provided by neurons @xmath36 is equal to the ratio @xmath37 .",
    "accordingly , the probability that neuron @xmath35 gives new information is @xmath38 / h\\{s\\}$ ] .",
    "therefore , the probability that @xmath35 neurons provide an information @xmath39 has two different contributions . on one hand",
    ", it may happen that the first @xmath1 neurons had already provided an information @xmath39 , and the information of neuron @xmath35 comes to be redundant to the previous information .",
    "alternatively , it may be that the response of the first @xmath1 neurons gave an information equal to @xmath8 , and the new neuron provides the extra @xmath15 .",
    "we therefore write a recurrence equation for the distribution @xmath40 , @xmath41 as a first step , and in order to make the notation simpler , we choose to measure the information in units of @xmath15 . in these units ,",
    "the maximum information reads @xmath42 .",
    "thus , eq . ( [ recurr ] ) reads @xmath43 this recurrence equation has to be solved with the initial conditions @xmath44 where @xmath45 if @xmath46 , and @xmath47 otherwise .",
    "the first condition states that no neurons can only give no information , while the second indicates that if there is no information , then , for sure , there are no neurons .",
    "we define a generating function @xmath48 clearly , @xmath49 therefore , if equation ( [ recurr ] ) is transformed into an equation for @xmath50 , and this last equation is solved , the probability @xmath40 can be readily calculated , by derivation . moreover ,",
    "if we define the mean values @xmath51 we observe that the first two moments can be obtained with the equalities @xmath52 \\ x^n , \\label{medios}\\end{aligned}\\ ] ] namely , making a series expansion of derivatives of @xmath50 .",
    "it can be shown that the recurrence relation ( [ recu1 ] ) with its initial conditions ( [ borde ] ) are equivalent to a differential equation for @xmath50 , @xmath53 to be solved with the border conditions @xmath54 morover , since @xmath40 is a normalized probability distribution , @xmath55 the condition @xmath56 implies that @xmath50 is a polynomial in @xmath16 .",
    "the differential equation ( [ part ] ) for @xmath50 does not involve derivatives in @xmath57 .",
    "therefore , for each value of @xmath57 one has an ordinary first order differential equation in @xmath16 , which can be readily integrated .",
    "the result is @xmath58   \\nonumber \\\\ & & + \\frac{i_\\infty(1 - y)}{x + i_\\infty(1 - x ) } \\ _ 2f_1 \\left[1 , 1 - i_\\infty ; 2 + i_\\infty \\left(\\frac{1 - x}{x } \\right ) ; 1 - y \\right ] , \\label{g}\\end{aligned}\\ ] ] where @xmath59 is gauss hypergeometric function ( abramowitz , 1972 ) .",
    "it can be easily seen that @xmath50 fulfills both the border conditions in ( [ part1 ] ) and the normalization constraint ( [ norm ] ) .",
    "morover , if @xmath60 is a positive integer , then the hypergeometric functions in ( [ g ] ) become polynomials in @xmath16",
    ". it may seem , in a first thought , that the requirement of @xmath60 being an integer is too restrictive .",
    "however , if the whole of the striped rectangle in figure [ f3 ] is supposed to be filled by blocks of area @xmath15 , it is in fact necessary to have a total area ( @xmath11 ) that is an integer multiple of @xmath15 .    by simple derivation",
    ", we get @xmath61}.\\ ] ] this expression involves a linear term in @xmath57 and two factors that can be written as geometric series .",
    "it is therefore easily expanded in powers of @xmath57 . using equation ( [ medios ] ) we obtain @xmath62 .",
    "\\label{huk}\\ ] ] if instead of writing @xmath8 in units of @xmath15 we turn back to bits , equation ( [ huk ] ) coincides with ( [ gr ] ) .",
    "in order to state the equivalence of the two , we have to set @xmath63 as was done by rolls ( 1997 ) .    in figure [ f4 ]",
    "we show @xmath64 as a function of @xmath1 , for different values of @xmath60 .",
    "the crosses represent the case of @xmath65 , where a single neuron provides the maximum information .",
    "obviously , additional cells produce no variation in the mutual information .",
    "the squares and the circles represent @xmath66 and @xmath67 , respectively .",
    "the dashed line shows the behaviour in the limit of @xmath68 .",
    "all the curves saturate at @xmath60 .",
    "equation ( [ huk ] ) implies that when @xmath69 ( or equivalently @xmath70 ) , the mutual information rises almost linearly with @xmath1 .",
    "in fact , expressing @xmath14 in bits , @xmath71 therefore , we conclude that for @xmath70 initially different neurons provide independent information . in the schematic view of figure [ f3 ] ,",
    "when the number of neurons is small , they will probably occupy different areas of the striped region .",
    "the slope of the initial rise is a measure of the mean information provided by a single unit .",
    "the deviation from a strictly linear behaviour is given by the mean pairwise overlap @xmath20 /i(1)$ ] .",
    "in all cases , as the number of units is increased , almost the whole of the rectangle is covered , and therefore , the mean information approaches its ceiling .",
    "in fact , as @xmath72 increases , the initial linear rise is no longer observed , as shown by the squares and in the limit case of the crosses of figure [ f4 ] .",
    "we now turn to the second moment of the distribution @xmath40 .",
    "we define the dispersion @xmath73 thus , @xmath74 quantifies how much variability one expects to find in different trials of the random process depicted in figure [ f3 ] . or , from the point of view of the experimentalist , the changes that may be expected when @xmath14 is measured using different sets of @xmath0 stimuli  keeping them all within the broad class of stimuli to which the cells are supposed do be responsive .",
    "making use of equation ( [ medios ] ) , we get @xmath75 \\left[1 - \\left(\\frac{i_\\infty - 1}{i_\\infty}\\right)^{n - 1 } \\right ] \\right\\ } , \\nonumber\\end{aligned}\\ ] ] where , for simplicity , @xmath76 is written in units of @xmath77 ^ 2 $ ] .",
    "as expected , @xmath78 vanishes for @xmath79 . in figure    [ f5 ]",
    "we plot the relative dispersion @xmath80 as a function of @xmath1 .",
    "we observe that for @xmath65 the standard dispersion vanishes , for all @xmath1 . as @xmath60 grows , @xmath81 rises .",
    "its maximum value is reached for @xmath82 .",
    "for very large @xmath60 , the curve flatterns again .",
    "let us now re - formulate the model to allow for the possibility of partial overlap between the information conveyed by any two neurons .",
    "here we present one possible way to do so . instead of thinking that when one more neuron is considered a whole block of information @xmath15 is added , we imagine that the same amout @xmath15 appears in @xmath83 pieces of size @xmath84 .",
    "in other words , the information provided by @xmath1 neurons is the result of sampling at random the whole of the available information a number @xmath85 of times , each sample of size @xmath86 .",
    "ultimately , we shall put @xmath87 .",
    "we coin @xmath88 for the probability distribution that after @xmath89 steps , an amount @xmath8 of information is extracted . clearly , and by analogy with equation ( [ recurr ] ) , @xmath90 obeys the recurrence relation @xmath91 since we are interested in the information provided by @xmath1 neurons  and not by @xmath89 steps  we relate the two probabilities stating @xmath92 this subdivision of the process has two main consequences . first , and as desired , the overlap between the information conveyed by any two neurons in a particular realization is no longer either zero or @xmath15 .",
    "now , the overlap can be any @xmath93 , for integer @xmath94 varying in @xmath95 $ ] .",
    "but in addition , now single neurons do not provide a fixed amount of information .",
    "the @xmath83 blocks of information conveyed by a single neuron may overlap with one another .",
    "therefore , the information provided by just one neuron obeys a distribution ranging from @xmath86 ( when all the blocks overlap ) up to @xmath15 ( when there is no overlap at all ) .    in order to solve ( [ recu2 ] ) we choose to measure the information in units of @xmath86 .",
    "we define @xmath96 if the recurrence relation for @xmath90 is written as a function of @xmath89 and @xmath97 , equation ( [ recu1 ] ) is obtained , with the only difference that now all variables appear primed .",
    "this means that the whole procedure of the previous section can be applied . using equation ( [ junta ] ) to go back to @xmath40 and taking the limit @xmath87",
    ", we get @xmath98 as stated above , in this case , @xmath99 is not necesarily equal to @xmath15 .",
    "moreover , the whole curve @xmath100 differs , in general , from ( [ huk ] ) .",
    "more precisely , the factor @xmath101^n$ ] has now been replaced by @xmath102 .",
    "we observe , however , that as @xmath60 increases , the two results coincide .",
    "it can be also shown that in the limit @xmath103 the dispersion @xmath74 vanishes .",
    "the procedure we have devised in order to include the possibility of partial overlaps is equivalent to a re - scaling of the variables @xmath1 and @xmath72 .",
    "it may be seen that as @xmath83 grows , the distribution @xmath34 becomes more and more concentrated in its mean value @xmath104 .",
    "in fact , had we attempted to solve equation ( [ recurr ] ) by taking the continuous limit in the size of the increments of both @xmath1 and @xmath8 , we would have obtained a solution that is a delta function vanishing for all @xmath8 and @xmath1 , except for those that obey the equation @xmath105 $ ] , which is exactly the mean value predicted by ( [ blip ] ) .",
    "we have calculated the probability distribution that @xmath1 neuron provide an amount @xmath8 of information .",
    "we have seen that the mean value of such a distribution is given by equation ( [ gr ] ) already proposed by gawne and richmond ( 1993 ) , and further constrained by rolls ( 1997 ) .",
    "the remarkably good accuracy with which equation ( [ gr ] ) can fit the real data ( see figure [ f2 ] ) suggests that the present approach , although phenomenological , captures the relevant aspects of the way the information about the identity of the stimulus shown is shared among neurons .",
    "the probability distribution characterizes an ensamble of equivalent processes , all of which fulfill two conditions . in the first place ,",
    "every unit is supposed to provide a fixed amount of information @xmath15 .",
    "rigourously speaking , this assumption is not true , since there is no doubt that some neurons are more informative than others .",
    "our main interest , however , is to derive how the information scales with the number of neurons .",
    "we aim at predicting the type of behaviour shown in figure [ f2 ] . in order to obtain a somewhat universal trend that does not depend on the particular identity and order in which neurons are chosen ,",
    "it is useful to work with the average single - unit information . also in the plot of figure [ f2 ]",
    ", each point represents an average over all the possible selections of @xmath1 neurons ( @xmath1 ranging from one to fourteen ) , out of the fourteen sampled ones .",
    "of course , the suposition that every unit provides @xmath15 bits of information does not mean that there is no trial to trial variability .",
    "whatever the size of the variability  in other words , whatever the distribution @xmath106one calculates , using equation ( [ inf ] ) , the information provided by unit @xmath107 . by averaging this quantity on all @xmath107 , @xmath15 is obtained .",
    "the second main assumption is that any one neuron provides a random portion of the total available information .",
    "when many trials of such a stochastic process are averaged together , one deduces that any two neurons share , on average , a fixed fraction @xmath16 of @xmath15 . moreover , when a third neuron is added , the portion of the information that is shared between the three of them is the same fraction @xmath16 of the mean redundancy between two neurons . in principle , the two fractions need not be the same . in this sense ,",
    "the present approach is the simplest choice that could be made .",
    "what does this random overlap hipothesis mean , from the point of view of neural operation ?",
    "if two neurons are strongly correlated ( for expample , if they share an appreciable fraction of their imputs ) they often provide the same information  more often than by chance .",
    "if , on the contrary , the set of neurons is precisely designed so that each unit represents a different feature of the stimuli then , if the stimuli evenly cover the feature space , the information provided by different neurons will seldom overlap  this means , less frequently than by chance .",
    "the random overlap hipothesis is , in some way , a null hipothesis .",
    "no special organization is assumed , neither in making the neurons particularly redundant nor synergetic .",
    "this phenomenological level of description should be contrasted to others approaches , where the correlations among units are described in more detail ( oram 1998 , abott and dayan 1999 , panzeri 1999b , karbowski 2000 ) .",
    "as limited as it might seem , our approach has the appealing property of being analyticaly tractable .",
    "moreover , the resulting @xmath14 depends on a single parameter @xmath60 .",
    "an alternative to the assumptions made in the present paper would be to model explicitely the conditional probabilities @xmath108 and calculate @xmath14 using equation ( [ inf ] ) , as done in samengo and treves ( 2000 ) .",
    "this is a much more detailed level of description , and allows the exploration of different coding strateggies , for example , a localized scheme ( sometimes called a grandmother - cell encoding ) shows a particular behaviour of @xmath14 , that differs from the one observed in a distributed code .",
    "however , such an approach is also much more arbitrary , since a precise model of the unknown parameters shaping the neural responses is needed .",
    "the formalism presented here predicts an average redundancy @xmath109 entirely determined by ceiling effects . in other words , if the average information provided by each neuron is @xmath15 , then just because the maximum information is finite , there will be a pairwise overlap of @xmath17 .",
    "therefore , if in an actual experiment the mean redundancy measured is similar to @xmath72 , this means that there is not much redundancy or synergy in the coding scheme .",
    "such a value stems from the null hypothesis , namely , that neurons share the information at random",
    ". it would be therefore interesting to find an experimental overlap that differs signifficantly from @xmath72 . or , as suggested by gawne and richmond , that the overlaps bear a spatial dependence .",
    "that is to say , that even though the overall mean redundancy might be close to @xmath110 , neurons sitting close to one another could be more redundant than pairs standing further apart .",
    "we would finally like to stress that the saturation of @xmath14 is only determined by the set of stimuli .",
    "one should not deduce that when reaching the ceiling , the pool of neurons has reached its maximum representational capacity .",
    "rather , the stimuli are no longer adequate to explore the coding capabilities of the cells .",
    "if an experimentalist is interested in measuring the representational capacity of the system , he or she should choose a set of stimuli whose entropy is large enough so that he can determine precisely the slope of the initial linear rise , that is , @xmath15 . if the aim is to characterize how redundant the messages conveyed by different neurons are , then a measure of the departure from the linear rise is needed .",
    "therefore , @xmath11 should not be extremely large , as compared to @xmath15 .",
    "we suggest that a comparison between the measured value of @xmath16 with @xmath72 can indicate if the ceiling effects are only because the set of stimuli is finite , or whether they arise from some special organization in the way different neurons represent information ."
  ],
  "abstract_text": [
    "<S> we study the capacity with which a system of independent neuron - like units represents a given set of stimuli . </S>",
    "<S> we assume that each neuron provides a fixed amount of information , and that the information provided by different neurons has a random overlap . </S>",
    "<S> we derive analytically the dependence of the mutual information between the set of stimuli and the neural responses on the number of units sampled . for a large set of stimuli , </S>",
    "<S> the mutual information rises linearly with the number of neurons , and later saturates exponentially at its maximum value . </S>"
  ]
}