{
  "article_text": [
    "lattice qcd directly performs the path integral for the qcd lagrangian by monte - carlo integration on a computer .",
    "space - time is discretised on a 4-torus , and a large number of snapshots of typical vacuum configurations is used to evaluate hadronic correlation functions non - perturbatively .",
    "the numerical integration scheme introduces finite - volume , discretisation , and statistical errors that can be removed with sufficient compute power : lattice qcd is systematically improvable .    with improved actions and extrapolation in the lattice spacing , fairly modest lattice sizes , such as @xmath0 ,",
    "are believed to be adequate for controlling the discretisation and finite volume effects on most hadronic observables .",
    "algorithms for including the effects of quark loops in the selection of typical vacuum configurations are numerically very expensive .",
    "the expense is believed to grow with a large power of the inverse quark mass for current best algorithms , and has thus far not been paid in full by lattice simulations , either choosing to ignore such effects ( quenching ) or simulating with artificially large quark masses .",
    "the goal for lattice qcd is to simulate in a region where the dynamical quark mass at least reliably connects to chiral perturbation theory , if not to the physical masses , and requires at least many tens of teraflop years of computer power .",
    "the notable characteristic of this problem is the need to focus ever more compute power on reducing the quark mass for a fixed problem size , rather than scaling up the problem size as more compute power becomes available . to this end qcdoc",
    "has been designed to allow efficient distribution of a single lattice qcd simulation over a very large multi - dimensional grid of a few tens of thousands of compute nodes .    at such extreme scalability",
    "the sparse matrix inversions involved require both global summation and nearest neighbour communication at a much shorter timescale than on smaller mpps . consequently both global summation time and nearest neighbour latency bite much harder , and both the qcdoc hardware and software",
    "have been designed to address these issues very effectively , with an order of magnitude improvement over traditional cluster technology on these key operations .",
    "continuing advances in the microelectronics industry have made it possible to integrate almost all components that make up a computer system on a single chip .",
    "this is known as system - on - a - chip technology . using this technology , the individual processing nodes in a massively parallel computer",
    "can be greatly simplified .",
    "this is the idea behind the design of the qcdoc supercomputer : the processing elements consist of a single application - specific integrated circuit ( asic ) and an industry - standard ddr memory module .",
    "large machines can then be build by simply adding many such processing elements .",
    "the main ingredients of the qcdoc asic are    * 500 mhz , 32-bit powerpc 440 processor core * 64-bit , 1 gflops floating - point unit * 4 mbytes of on - chip memory ( embedded dram ) * controllers for embedded and external memory * nearest - neighbor serial communications unit with aggregate bandwidth of 12 gbit / s in 12 independent directions * other components such as ethernet controller , interrupt controller , etc .    in the following subsections , the qcdoc hardware is described in more detail ( see also ref .",
    "the main advantages of the qcdoc design are    * high scalability : 50% sustained performance for typical applications on machines with several 10,000 nodes * low price - performance ratio of $ 1 per sustained mflops * low power consumption * high reliability      ibm is the leading vendor in the system - on - a - chip industry . the qcdoc asic was designed in close collaboration with ibm research ( yorktown heights ) and is shown schematically in fig .  [",
    "fig : asic ] .",
    "it contains standard cores from the ibm library as well as custom - designed components for faster memory access and high - speed off - chip communication .",
    "the on - chip components are linked by three busses from ibm s coreconnect@xmath1 technology @xcite :    * processor local bus ( plb ) .",
    "this is a 128-bit wide , fully synchronous bus running at 1/3 of the cpu frequency .",
    "it contains independent read and write data busses .",
    "the plb protocol ( implemented by the plb arbiter ) is rather sophisticated , allowing for pipelining , split transactions , burst transfers ( fixed and variable length , early termination possible ) , dma transfers , programmable arbitration , and other features such as timeout , abort , etc . * on - chip peripheral bus ( opb ) .",
    "this is a 32-bit wide , fully synchronous bus running at 1/6 of the cpu frequency .",
    "its basic purpose is to off - load slower devices from the plb bus .",
    "* device control register bus ( dcr ) .",
    "this is a very simple bus which is used to read and write various control registers in the on - chip devices .",
    "the 440 is the only master on this bus .",
    "the ibm library components in the asic are    * the powerpc - based 440 cpu core @xcite with attached 64-bit ieee floating point unit @xcite . the 440 is a book e compliant 32-bit processor with a 32 kbyte prefetching instruction cache and a 32 kbyte data cache with flexible cache control options ( 64-way associative , partitionable , lockable ) .",
    "the processor includes memory management with a 64-entry translation - lookaside - buffer which supports variable page sizes from 1 kbyte to 256 mbyte .",
    "it also features dynamic branch prediction and a 7-stage , dual issue pipeline .",
    "the target frequency of the 440 is 500 mhz , i.e. the peak performance is 1 gflops .",
    "+ the 440 also features a jtag interface .",
    "jtag ( joint test action group ) is an industry - standard protocol that allows an external device to take complete control of the processor .",
    "this functionality will be used for booting and debugging , see below . *",
    "4 mbytes of embedded dram ( or edram ) which is accessed with low latency and high bandwidth through a custom - designed controller , the pec , see below .",
    "* the plb arbiter provides programmable arbitration for the up to eight allowed masters that can control plb transfers .",
    "we are using six masters : the 440 instruction read , data read , and data write interfaces ( the last two are channeled through the pec ) , the edram dma , the scu dma , and the mal dma used by the ethernet controller . * the plb - opb bridge is used to transfer data between the two busses . it is the only master on the opb and a slave on the plb . *",
    "the universal interrupt controller ( uic ) processes the interrupts that are generated on- and off - chip and provides critical and non - critical interrupt signals to the 440 . *",
    "the ddr controller is a slave on the plb , capable of transferring data to / from external ddr ( double data rate ) sdram at a peak bandwidth of 2.6 gbytes / s .",
    "it supports an address space of 2 gbytes and provides error detection , error correction , and refresh of the off - chip sdram .",
    "* the ethernet media access controller ( emac ) provides a 100 mbit / s ethernet interface ( it also supports gbit - ethernet , but we are not making use of this capability ) .",
    "the media - independent interface ( mii ) signals at the asic boundary are connected to a physical layer chip on the daughterboard .",
    "the emac is a slave on the opb and has sideband signals to the mcmal on the plb . * the dma - capable memory access layer ( mcmal ) loads / unloads the emac through the sideband signals .",
    "it is a master on the plb . * the inter - integrated circuit ( i@xmath2c ) controller is a slave on the opb .",
    "it is used to communicate with off - chip devices supporting the i@xmath2c protocol , such as the serial presence detect eprom on the ddr dimm or voltage and temperature sensors on the motherboard . *",
    "the general purpose i / o ( gpio ) unit is another slave on the opb whose 32-bit wide data bus is taken out to the asic boundary .",
    "it is used , e.g. , to drive leds , to control the global interrupt tree , and to receive interrupts from off - chip devices . *",
    "the high - speed serial links ( hssl ) used by the scu each contain eight independent ports ( four each for send / receive ) through which bits are clocked into / out of the asic at 500 mbit / s per port .",
    "the bits are converted to bytes ( or vice versa ) in the hssl .",
    "the hssl input clocks are phase - aligned by another ibm macro , the phase - locked loop ( pll ) .",
    "in addition to the components provided by ibm , the qcdoc asic contains custom - designed components .",
    "most importantly , these components provide essential support for the high - speed communications required in a massively parallel machine .        *",
    "serial communications unit ( scu ) .",
    "the task of the scu is to reliably manage the exchange of data between neighboring nodes with minimum latency and maximum bandwidth .",
    "the design takes into account the particular communication requirements of lattice qcd simulations . a schematic picture of the scu is shown in fig .",
    "[ fig : scu ] .",
    "+ the custom protocol governing the data transfers defines packets that contain a 64-bit data word and an 8-bit header containing control and parity bits . when the receive unit receives a packet , it first interprets the header , buffers the bytes from the hssl , and assembles the 64-bit word . it then transfers the word to the receive register or passes it on to the send unit .",
    "the receive buffer can store three 64-bit words so that the send unit ( in a neighboring node ) can send three words before an acknowledgment is received .",
    "the functionality of the send unit is essentially the inverse of that of the receive unit .",
    "send and receive operations can proceed simultaneously .",
    "each send or receive unit is controlled by a dma engine which then transfers the data between memory and a send / receive register .",
    "each dma engine is controlled by block - strided - move instructions stored in sram in the scu itself .",
    "+ a low - latency passthrough mode is provided for global operations . because of the latencies associated with the hssl , the most efficient method to perform global sums is `` shift - and - add '' , using a store - and - forward capability built into the scu .",
    "the main advantage of this scheme is that the software latency of about 300 ns is paid only once per dimension , rather than for each node in this dimension .",
    "+ the total end - to - end latency is estimated to be about 350 ns for supervisor transfers and about 550 ns for normal transfers .",
    "this is at least an order of magnitude lower than the latency associated with myrinet . since a write instruction from the 440",
    "can initiate many independent transfers on any subset of the 24 send or receive channels , the latencies associated with multiple transfers can be overlapped to some degree .",
    "the total off - chip bandwidth using all 24 hssl ports is 12 gbit / s . in a 4-dimensional physics application only 16 of the 24 hssl ports",
    "will be used , resulting in a total bandwidth of 8 gbit / s .",
    "this provides a good match for the communications requirements of typical applications .",
    "concrete performance figures are given in sec .",
    "[ peterperformance ] below .",
    "* prefetching edram controller ( pec ) .",
    "the pec is designed to provide the 440 with high - bandwidth access to the edram .",
    "it interfaces to the 440 data read and data write busses via a fast version of the plb that runs at the cpu frequency and that we call processor direct bus ( pdb ) .",
    "the pec also contains a plb slave interface to allow for read and write operations from / to any master on the plb as well as a dma engine to transfer data between edram and the external ddr memory .",
    "+ the access to edram ( which is memory - mapped ) proceeds at 8 gbytes / s .",
    "ecc is built into the pec , with 1-bit error correct and 2-bit error detect functionality .",
    "the pec also refreshes the edram .",
    "the latency of the pdb itself is 1 - 2 cpu cycles .",
    "this very low latency eliminates the need for pipelining .",
    "the maximum pdb bandwidth is 8 gbytes / s for read and write .",
    "however , due to internal latencies in the 440 , the maximum sustained bandwidth is 3.2 gbytes / s . + the read data prefetch from edram occurs in two 1024-bit lines .",
    "three read ports ( pdb , plb slave , dma ) arbitrate for the common edram . the coherency between pdb , plb slave , and dma is maintained within the pec .",
    "each read port has four 1024-bit registers that are paired in two sets to allow for ping - ponging between different memory locations .",
    "there are also two 1024-bit write buffer registers each for the pdb / plb slave / dma write interfaces .",
    "* ethernet - jtag interface . as mentioned above , the 440 core has a jtag interface over which one can take complete control of the processor . in particular , this interface can be used to load boot code into the instruction cache and start execution .",
    "this completely eliminates the need for boot rom .",
    "the question is how the jtag instructions should be loaded into the 440 .",
    "( there are special tools that use the jtag interface , but it would be impractical to connect one tool per asic for booting . )",
    "a solution to this question has already been developed at ibm research , implemented using a field - programmable gate array ( fpga ) , that converts special ethernet packets to jtag commands and vice versa .",
    "this logic is now part of the qcdoc asic and will be used not only for booting but also to access the cpu for diagnostics / debugging at run time .",
    "the unique mac address of each asic is provided to the ethernet - jtag component by location pins on the asic , and the ip address is then derived from the mac address .",
    "the qcdoc asic is manufactured using cmos 7sf technology ( 0.18 @xmath3 m lithography process ) .",
    "the mechanical design is indicated in figs .",
    "[ fig : dbd][fig : backplane ] .",
    "two qcdoc asics will be mounted on a daughterboard , together with two industry - standard ddr sdram modules ( one per asic ) whose capacity will be determined by the price of memory when the machine is assembled .",
    "a maximum of 2 gbytes per asic are supported .",
    "the daughterboard also contains four physical layer chips ( two per asic for their ethernet and ethernet - jtag interfaces ) and a 4:1 ethernet hub so that a single 100 mbit / s ethernet signal is taken off the daughterboard .",
    "32 daughterboards are mounted on a motherboard .",
    "the motherboard also contains eight 4:1 ethernet hubs so that the total ethernet bandwidth off a motherboard is 800 mbit / s .",
    "furthermore , the motherboard contains power transformers as well as temperature and voltage sensors .",
    "8 motherboards are mounted in a crate with a single backplane .",
    "the final machine then consists of a certain number of such crates connected by cables .",
    "there are three separate networks : the high - speed physics network , an ethernet - based auxiliary network , and a global interrupt tree .",
    "the physics network consists of high - speed serial links between nearest neighbors with a bandwidth of 2@xmath4500 mbits / s per link .",
    "transfers across these links are managed by the serial communications unit in the qcdoc asic as described above .",
    "the nodes are arranged in a 6-dimensional torus which allows for an efficient partitioning of the machine in software , described in more detail below . on a motherboard ,",
    "the node topology is @xmath5 , with three dimensions open and three dimensions closed on the motherboard ( one of which is closed on the daughterboard ) .",
    "the auxiliary network is used for booting , diagnostics , and i / o over ethernet , with an ethernet controller integrated on the asic . the ethernet traffic to and from the asic",
    "will run at 100 mbit / s . as mentioned above",
    ", hubs on the daughter- and motherboards provide a total bandwidth of 800 mbit / s off a motherboard to commercial gbit - ethernet switches , a parallel disk system , and the host workstation ( a standard unix smp with multiple gbit - ethernet cards ) .",
    "the global interrupt tree consists of three separate interrupt lines that are visible across all partitions .",
    "an interrupt asserted by any asic is first transmitted to the top of the tree and then propagated down the tree to all other asics in the full machine .",
    "it will remain asserted until cleared by the asic from which it originated .",
    "the qcdoc system software is minimally required to boot and manage the machine , load and run application code , and service application , communication and i / o requests .",
    "the system can be thought of as composed of three major parts .    *",
    "the front - end operating system , known as the qdaemon . * the node operating system , known as the run - kernel . * the run - time support libraries used by applications .    in this section",
    "we discuss the qdaemon and run - kernel and defer discussion of the run - time support libraries till a later sec .",
    "[ chulwooscidacandscu ] .",
    "the qdaemon runs like a normal unix daemon and is responsible for booting , managing and partitioning the `` back - end '' grid of qcdoc nodes .",
    "the qdaemon is the sole point of access to the back - end for users and communicates with many nodes concurrently via rpc over the multiple gigabit ethernet links .",
    "the qdaemon is contacted either via a pbs based queuing system , or directly via a client program called the `` qcsh '' .",
    "the qcsh is a modified shell , with additional built - in commands for sending requests to the qdaemon to perform operations on the qcdoc , such as running code on a partition .",
    "the qdaemon is aggressively multi - threaded and supports many partitions and connected users simultaneously .",
    "the node operating system is a simple ( non - preemptive ) kernel .",
    "the overall design goal for the run - kernel is to run one compute process , and run it well",
    ".    thus the kernel uses the 440 s mmu for memory protection , but not translation .",
    "this allows for protection of the o / s from errant user code , and for zero - copy communication with simple ( i.e. non - virtual memory aware ) hardware .",
    "further , the entire memory map can be covered by the 64 entry tlb , so that tlb misses ( which are a source of significant performance loss in many hpc machines ) can not occur on qcdoc .",
    "the kernel does not implement scheduling so that the application is guaranteed 100% of the cpu .",
    "this is very important since a more traditional kernel on such a large and _ very _ tightly coupled machine ( will self - synchronise every 22 microseconds in some codes ) would be impacted by a few nodes running their scheduler during any given dslash application .",
    "the run kernel does , however , service both hardware and software interrupts .",
    "the features of the standard powerpc architecture allow user code errors to be trapped cleanly and robustly from software interrupts .",
    "the kernel also services system call requests from user code , both to access the communication hardware through a very lean software layer , and to implement the standard c run - time environment ( cygnus newlib ) .",
    "the kernel includes an ethernet driver allowing for host - node communication , and an nfs client has been implemented allowing for file i / o from the nodes , both to the front end and to a parallel disk system composed from standard network attached storage .",
    "both run - kernel and application circular print buffers are maintained on the nodes .",
    "this allows for post - mortem readout of each node s output , or optionally any subset of nodes can be configured at run - time to output directly to the console .",
    "as discussed , qcdoc is based on a six - dimensional toroidal grid of nodes .",
    "a partition is a rectangular subvolume of the total machine and can be defined by the 6-coordinates of two nodes in the machine grid , namely the bottom - left , upper - right pair in six dimensions .",
    "each node has its own mapping from application directions to machine directions . by changing this mapping with node coordinate",
    ", we can change the topology and dimensionality seen by application code .",
    "this is done by successively folding machine axes together into a single application axis .",
    "a simple 2d example of mapping an in principle non - periodic @xmath6 square into a 1d - torus of length 16 is shown in fig .",
    "[ fig4x4 ] .     folding two machine axes into one periodic application axis , width=226 ]     folding three pairs of machine axes into three periodic application axes .",
    "this example corresponds to remapping a single qcdoc motherboard.,width=226 ]    this process may be repeated , and an example of remapping a @xmath5 single motherboard is shown in fig .",
    "[ fig4x4x4 ] . for any given machine",
    "it is possible to configure for any application dimensionality from 1 to 6 . from the point of view of the communications software ,",
    "all partitions are six dimensional , but some variable number of these dimensions may be trivial .",
    "calls in trivial application dimensions implement local copies from a node to itself , while calls in non - trivial application dimensions are mapped to the appropriate machine directions .      in this section",
    "we present the performance of optimised code on cycle accurate simulation of qcdoc .",
    "table  [ tabfpu ] shows the performance of key single - node assembler kernels .",
    "most of these assembler kernels were in fact generated via a c++ program which will also output alpha and sparc assembler .",
    "this allows a very fair comparison with other contemporary risc chips .    .",
    "[ tabfpu ] double precision floating point performance on optimised variants of common ( single - node ) qcd kernels on the qcdoc simulator . here",
    "we assume a 500mhz nominal clock , and very high fractions of the 1 gflop / s peak can be obtained in the dominant kernels such as su3 - 2spinor .",
    "[ cols=\"^,^\",options=\"header \" , ]      the asqtad action @xcite is one of the improved discretizations of continuum qcd fermion actions which exhibits smaller lattice spacing errors and flavor mixing .",
    "it uses many different paths connecting @xmath7 and @xmath8 or @xmath7 and @xmath9 in contrast to the wilson or staggered actions where only one gauge link @xmath10 is used for each pair of neighbors .",
    "a diagram of the paths in the asqtad action is shown in fig .",
    "[ fig : asqtad ] .",
    "diagram of paths used in the asqtad action.,width=302 ]    calculation of the force term for the gauge link @xmath11 , @xmath12 , mostly consists of parallel transport @xmath13 and su(3 ) vector outer products @xmath14 .",
    "because of the complexity of the gauge paths used in the action , the number of floating point operations for the asqtad force term is much larger than for the application of the dirac operator and the force term for simpler actions .",
    "the number of floating point operations per site is @xmath15 250,000 . considering that the number of flops for the asqtad dirac operator is only 1146 per site , and the typical number of conjugate gradient iterations is about 1000 for the mass used in typical lattice simulations , the force term calculation is a significant portion of the hybrid monte carlo simulation , where a dirac matrix inversion is alternated with a force term calculation , and thus should be optimized as well as the dirac operator .",
    "we found the performance of the asqtad force term was initially rather low , only 3% of peak for @xmath16 and 6% for @xmath17 local lattice volumes . upon examining the source code",
    ", we found that the communication channels are being created and destroyed for each parallel transport .",
    "this amounts to 30@xmath1560% of the total number of cycles for parallel transport . after modifying the parallel transport to reuse communication channels and combining small routines used in the outer product routine to eliminate function call overhead ,",
    "the overall performance increased significantly to 12% for a @xmath16 and 13% for a @xmath17 local lattice .",
    "further optimization was done by eliminating function calls for computation routines defined for each lattice site within parallel transport and outer product routines . together with loop unrolling",
    ", this made it possible to preload data into the l1 cache and registers to avoid cache misses .",
    "this improved the performance of computation routines by a factor of 1.5@xmath151.7 .",
    "the overall performance increased to 14% for @xmath16 and 20% for @xmath17 local volume , a 300@xmath15400% increase over the original performance and quite acceptable for a c routine .",
    "qcdoc is a massively parallel computing architecture optimized for lattice",
    "qcd . the node has been designed to balance the floating point performance , memory bandwidth , and communication performance such that for qcd no single subsystem limits the performance .",
    "the design improves the network and memory subsystem performance relative to the floating point peak when compared with current commercial mpps .",
    "simulation measurements of the nearest neighbour latency and global summation suggests at least an order of magnitude improvement over current commercial machines . this has enabled us to demonstrate very efficient use of the floating point unit that is maintained on remarkably small local volumes , such as @xmath16 , and estimate scalability on typical lattices to machines as large as several tens of thousands of nodes .",
    "the six dimensional mesh network is dealt with transparently by the operating system , and the machine can be dynamically partitioned to run multiple applications of any dimension from one through six .",
    "this should enable applications with similar characteristics to qcd ( local communication on a regular multi - dimensional mesh ) to make efficient use of the machine .",
    "a run - time environment compliant with the scidac software effort will be available .",
    "the message passing interface defined by the scidac effort , qmp , provides portable , efficient communication routines for lattice",
    "qcd . the qmp implementation on qcdoc is complete except for non - nearest neighbor communication , which is in the process of being implemented .",
    "performance numbers for lattice qcd routines from milc code with qmp were presented . to take full advantage of the hardware capabilities of provided via qmp ,",
    "the user programs should be written in a way that minimizes repetitive overheads , as shown by the performance numbers for the asqtad force term .",
    "the first batch of qcdoc asics has been manufactured and is being assembled for initial testing . while we thank the editors for their patience , we regret having been unable to further delay the submission of these proceedings until after the hardware went online .",
    "performance figures obtained from real hardware will be available in the near future ."
  ],
  "abstract_text": [
    "<S> an overview is given of the qcdoc architecture , a massively parallel and highly scalable computer optimized for lattice qcd using system - on - a - chip technology . </S>",
    "<S> the heart of a single node is the powerpc - based qcdoc asic , developed in collaboration with ibm research , with a peak speed of 1 gflop / s . </S>",
    "<S> the nodes communicate via high - speed serial links in a 6-dimensional mesh with nearest - neighbor connections . </S>",
    "<S> we find that highly optimized four - dimensional qcd code obtains over 50% efficiency in cycle accurate simulations of qcdoc , even for problems of fixed computational difficulty run on tens of thousands of nodes . </S>",
    "<S> we also provide an overview of the qcdoc operating system , which manages and runs qcdoc applications on partitions of variable dimensionality . </S>",
    "<S> finally , the scidac activity for qcdoc and the message - passing interface qmp specified as a part of the scidac effort are discussed for qcdoc . </S>",
    "<S> we explain how to make optimal use of qmp routines on qcdoc in conjunction with existing c and c++ lattice qcd codes , including the publicly available milc codes . </S>"
  ]
}