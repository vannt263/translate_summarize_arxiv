{
  "article_text": [
    "reconstruction of the plasma equilibrium shape is a key requirement for the operation of current and forthcoming tokamak experiments ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "the most commonly employed method numerically reconstructs the plasma equilibrium by iteratively solving the two - dimensional grad - shafranov equation @xcite for the poloidal flux function ( @xmath2 ) , and using diagnostic signals from the experiment as constraints @xcite . in short , the source term of the grad - shafranov equation",
    "is expanded into a linear combination of basis functions which allows formulating a linear regression problem for the expansion coefficients , using the actual diagnostic signals and their forward - modelling based on the numerical solution for @xmath2 .",
    "the problem can be tackled numerically by solving a number of independent grad - shafranov - type equations  one for each basis function of the expansion  and employing a picard - iteration procedure that allows evaluating the source term using the solution from the previous iteration step .",
    "a variety of equilibrium - reconstruction codes based on this strategy @xcite or similar approaches @xcite exist and have been routinely used for diagnostics and data analysis at the various tokamak experiments .",
    "the codes efit @xcite and cliste @xcite , for example , have been employed at diii - d , east and asdex upgrade ( aug ) , respectively , a class of medium - sized tokamaks which are characterized by a control cycle on the order of a millisecond . until recently , however , computing times in the millisecond range were inaccessible to such `` first - principles '' codes , and hence their applicability for real - time diagnostic analysis and plasma - control is very limited , unless the numerical resolution or the number of iteration steps is drastically reduced or other simplifying assumptions like function parametrization @xcite are adopted ( e.g. * ? ? ?",
    "lately , three codes based on fast grad - shafranov solvers have been able to demonstrate real - time capability : p - efit @xcite , a gpu - based variant of the efit equilibrium - reconstruction code @xcite , is able to compute one iteration within 0.22  ms on a @xmath3 grid with 3 basis functions . with a similar resolution",
    "the janet software @xcite has reached runtimes of about 0.5  ms per iteration on a few cpu cores within the labview - based system at aug . using a single cpu core overclocked at 5 ghz ,",
    "the liuqe code @xcite which is deployed at the swiss tcv experiment , achieves a cycle time of about 0.2  ms with a spatial resolution of @xmath4 points .",
    "in all these cases , however , only a single iteration is performed and only a very small number of basis functions can be afforded for the parametrization of the source term of the grad - shafranov equation . while such restrictions may well be justifiable under specific circumstances @xcite the motivation for our work is to substantially push the limits of numerical accuracy in terms of spatial resolution , number of basis functions , and number of iterations and thus the quality of the equilibrium solution .",
    "this is expected to aid the generality and robustness of the application , in particular with respect to variations in the plasma conditions .",
    "in addition to the reliability and accuracy of real - time equilibrium reconstructions , the sustainability and maintainability of the codes is considered an important aspect",
    ". typical implementations of analysis tools in fusion science have a rather long life time which often exceeds life times of commercial hardware and software product cycles .",
    "the long usability required for analysis tools poses special requirements on the software chosen and the maintenance capabilities necessary to adapt to changing environments .",
    "open - source software together with off - the - shelf computer hardware is thought of being perfectly suited for these demands @xcite .",
    "this in particular applies to all non - standardized components like , e.g. , the implementation of complex , tailored , and evolving numerical algorithms for equilibrium reconstruction .",
    "commercial , closed - source software ( like , e.g. , compilers or numerical libraries ) and hardware components may well comply with such demands , provided that they can be modularly interchanged using standardized interfaces .    to this end",
    "we have implemented a new , parallel equilibrium - reconstruction code , gpec ( garching parallel equilibrium code ) , which builds on the fast , shared - memory - parallel grad - shafranov solver we have developed previously @xcite and a two - level hybrid parallelization scheme which was pointed out in the same paper .",
    "the basic numerical model and functionality of the new code originate from the well - established and validated algorithms implemented by the equilibrium codes cliste @xcite and specifically its descendant ide @xcite , both of which are being used for comprehensive offline diagnostics and data analysis at the asdex upgrade experiment .",
    "gpec is based entirely on open - source software components , runs on standard server hardware and uses the same code base and computer architecture as employed for performing offline analyses with the ide code .",
    "such a strategy is considered a major advantage , in particular concerning verification and validation of the code @xcite and also its evolution : on the one hand , algorithmic and functional innovations in the offline physics modeling can gradually be taken over by the real - time version . on the other hand ,",
    "high - resolution offline analysis can directly take advantage of optimizations achieved for the real - time variant .",
    "the new code enables computing the equilibrium flux distribution and the derived diagnostics and control parameters within 1  ms of runtime , given a grid size of @xmath5 zones with up to 15 spline basis functions for the discretization , and about 90 diagnostic signals . with a runtime of less than 0.2  ms for a single equilibrium iteration and about 0.2  ms required for computing a variety of more than 20 relevant control parameters",
    ", one can afford 4 iterations for converging the equilibrium solution .",
    "the latter was checked to be sufficient for reaching accuracies of better than a percent for the relevant quantities .",
    "as a proof of principle we shall demonstrate the real time capability of the new code by performing an offline analysis using data from an aug discharge . to our knowledge",
    "the new gpec code to date is one of the fastest ( at a given numerical accuracy ) and most accurate ( at a given runtime constraint ) of its kind .",
    "the paper is organized as follows : in section  [ sect : algorithm ] we recall the basic equations and describe our new , hybrid - parallel implementation of the equilibrium solver and its verification and validation .",
    "its computational performance and in particular real - time capability is demonstrated in section  [ sect : application ] on an example application using real aug data .",
    "section  [ sect : conclusions ] provides a summary and conclusions .",
    "[ [ basic - equations ] ] basic equations + + + + + + + + + + + + + + +    the grad - shafranov equation @xcite describing ideal magneto - hydrodynamic equilibrium in two - dimensional tokamak geometry reads @xmath6 with cylindrical coordinates ( @xmath7 , @xmath8 ) , the elliptic differential operator @xmath9 and the poloidal flux function ( in units of vs ) , @xmath10 . the toroidal current density profile , @xmath11 consists of two terms , where @xmath12 is the plasma pressure ( isotropic case ) and @xmath13 is proportional to the total poloidal current , @xmath14 .    [",
    "[ numerical - solution ] ] numerical solution + + + + + + + + + + + + + + + + + +    the classical strategy @xcite for numerically solving eq .",
    "( [ gs ] ) is based on an expansion of the current density profile @xmath15 on the right - hand - side into a linear combination of a number @xmath16 of basis functions , @xmath17 in each cycle of a picard - iteration scheme , a number of @xmath18 poisson - type equations , @xmath19 and @xmath20 are solved individually , where the solution @xmath2 from the last iteration step is used for evaluating the right - hand side . the updated flux distribution is then given by @xmath21 which follows from eqs .",
    "( [ gs],[rhsdecomp ] ) and the linearity of the operator @xmath22 .",
    "the unknown coefficients @xmath23 are determined by experimental data : using the @xmath18 distributions @xmath24 obtained from eqs .",
    "( [ gsphi1],[gsphi2 ] ) the so - called response matrix @xmath25 , consisting of predictions for a set of @xmath26 diagnostic signals @xmath27 is calculated .",
    "the prediction @xmath28 is some ( linear ) function employing the flux distribution @xmath2 to produce the forward - modelled signal .",
    "all measurements are located within the poloidal flux grid such that the poloidal flux outside the grid is not needed .",
    "linear regression of the response matrix with the measured diagnostic signals finally yields the coefficients @xmath23 for eq .",
    "( [ bfdecomp ] ) .",
    "this procedure is iterated until certain convergence criteria are fulfilled .",
    "the private flux region close to the divertor is treated differently , although the same poloidal flux as inside the plasma occurs .",
    "since there are no measurements of the current distribution in the private flux region and the chosen functional form for the current decay was proven to be of minor importance for the equilibrium reconstruction , the current is chosen to decay approximately exponentially with a decay length of about 5 mm , starting at the last closed flux surface .",
    "the algorithm can be summarized as follows :    1 .",
    "evaluate right - hand sides @xmath29 and @xmath30 of eqs .",
    "( [ gsphi1],[gsphi2 ] ) , using the solution , @xmath31 , from the previous iteration step ( or initialization ) .",
    "2 .   solve the poisson - type equations @xmath32 individually for each @xmath33 .",
    "an additional poisson problem arises from a convergence - stabilization procedure which introduces the vertical plasma position as an auxiliary free parameter @xcite .",
    "3 .   evaluate @xmath34 to construct the response matrix @xmath35 .",
    "additional columns @xmath36 and @xmath37 arise from the abovementioned convergence - stabilization procedure and from deviations in the currents measured at a number of @xmath38 external field coils to account for wall shielding and plasma - induced wall currents , respectively .",
    "4 .   perform linear regression on @xmath39 to obtain the set of coefficients @xmath40 .",
    "evaluate new solution @xmath41 , adding contributions from the measured and fitted deviating currents in external coils , @xmath42 .",
    "go back to step 1 .",
    "until convergence is reached , e.g.  in terms of the maximum norm evaluated over all grid points , @xmath43 .",
    "the response matrix @xmath35 is already prepared for additional measurements of the motional stark effect ( mse ) , the faraday rotation , the pressure profile ( electron , ion and fast particle pressure ) , the q - profile ( e.g. from mhd modes ) , the divertor tile currents constraining @xmath44 on open flux surfaces , the measurements of loop voltage and of iso - flux constraints @xcite .",
    "these additional measurements are used in the ide code in the off - line mode , but are not subject of the present work due to the lack of reliable on - line availability .",
    "the relatively large number of basis functions ( and hence fit parameters ) , which is motivated by the need to allow for equilibria sufficiently flexible to address all occurring plasma scenarios , requires application of regularization constraints . while in the ide code additional smoothness ( curvature and amplitude ) requirements",
    "are applied to the source profiles @xmath45 and @xmath46 ( which effectively adds additional columns to the response matrix ) , gpec currently adopts a simpler ridge - regression procedure@xcite .    [",
    "[ implementation - and - parallelization ] ] implementation and parallelization + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the implementation of gpec starts out from the serial offline - analysis code ide @xcite , thus closely following the concepts of the well - known cliste code @xcite .",
    "specifically , for each of the two sets of basis functions , @xmath47 and @xmath48 , gpec employs a basis of cubic spline polynomials which are defined by a number of @xmath49 and @xmath50 knots at positions @xmath51 , respectively , with @xmath52 , @xmath53 and natural boundary conditions .",
    "bicubic hermite interpolation is employed for evaluating the flux distribution @xmath24 and its gradients ( which are required for computing the magnetic field ) at the positions of the magnetic probes and the flux loops in order to calculate the corresponding forward - modelled signals for the response matrix .",
    "thus , besides achieving higher - order interpolation accuracy , smoothness of the magnetic field is guaranteed by construction @xcite .",
    "gpec utilizes the fast , thread - parallel poisson solver we have developed previously @xcite .",
    "it is based on the two - step scheme for solving the poisson equation with dirichlet boundary conditions @xcite and replaces the cyclic - reduction scheme which is traditionally employed in the codes gec and cliste @xcite by a parallelizable fourier - analysis method for decoupling the linear system into tridiagonal blocks @xcite . for details on the implementation and computational performance",
    "we refer to refs .",
    "@xcite .    as briefly sketched in ref .",
    "@xcite the main idea of the new code is to exploit an additional level of parallelism by distributing the individual poisson problems for determining each of the @xmath24 to different mpi - processes .",
    "the algorithm , together with notes on the parallelization ( relevant mpi communication routines are noted in brackets ) is summarized as follows .",
    "each process @xmath54 is assigned to a single basis function poisson problems to a number of @xmath55 processes , provided @xmath55 divides @xmath56 . ] , and process @xmath57 is dedicated to the additional poisson problem that arises from the introduction of the vertical shift parameter @xcite .",
    "each process @xmath58 :    1 .",
    "computes @xmath59 using the solution @xmath10 from the previous iteration step ( or initialization ) + _ thread - parallelization over @xmath60-grid _ 2 .",
    "employs fast poisson solver @xcite to solve @xmath32 + _ thread - parallelization _",
    "@xcite 3 .",
    "computes column @xmath61 of the response matrix + _ thread - parallelization over @xmath60-grid _ 4 .",
    "gathers columns @xmath62 computed by the other processes @xmath63 and assembles response matrix @xmath35 + _ collective communication _",
    "( mpi_allgather ) 5",
    ".   performs linear regression on @xmath64 + _ thread - parallelization of linear algebra routines _ 6 .",
    "gathers all @xmath65 computed by the other processes @xmath63 and computes @xmath66 + _ collective communication _ ( mpi_allreduce )    the distribution of the poisson problems to different processes comes at the price of collective communication ( steps 4 and 6 ) , in which all processes combine their data with all others , using mpi routines mpi_allgather and mpi_allreduce , respectively .",
    "given the solution for the poloidal flux function @xmath10 , gpec computes more than 20 derived quantities which are relevant for plasma control .",
    "two of them , the @xmath60-coordinates of the magnetic axis , @xmath67 and @xmath68 can be obtained immediately by searching the maximum of the equilibrium flux . for the rest of the quantities four major computational tasks ( labelled a  d in the following compilation ) have to be carried out .",
    "their results allow evaluating the individual control parameters ( summarized under the bullet points ) in a numerically straightforward and inexpensive way",
    ". for details on the physical definition and derivation of these quantities , see , e.g.  ref .",
    "implementation details are given further below .    1",
    ".   determination of grid points which are enclosed by the separatrix * coordinates @xmath69 of the geometric axis , defined as the area - weighted integral within the separatrix @xmath70 . *",
    "the horizontal and vertical minor plasma radius is given by @xmath71 and @xmath72 , respectively , which determines the elongation @xmath73 .",
    "2 .   identification of contour lines @xmath74 with @xmath75 for a number of particular values of the normalized flux @xmath76 * from the separatrix curve ( with @xmath76 taken as the value at the innermost x - point which defines also the last - closed flux surface lcfs ) a number of geometric properties can be derived straightforwardly such as the ( @xmath77)-coordinates of the uppermost and lowermost point ( in @xmath8-direction ) on the plasma surface , or @xmath78 ( @xmath79 ) as the @xmath80-coordinate of the innermost ( outermost ) point on the plasma surface . * the length of the poloidal perimeter @xmath81 of the plasma is computed by integration over the lcfs . *",
    "the safety factor @xmath1 is computed by integration over contours defined by levels @xmath82 $ ] .",
    "computation of the toroidal plasma current @xmath83 ( eq .  [ gs_current ] ) * the indicators for the current - center are given by the following current - weighted integrals of the current density @xmath84 , @xmath85 .",
    "computation of the pressure distribution @xmath86 and the poloidal magnetic field * the total energy content of the plasma is given by @xmath87 .",
    "* the same integration can be used for computing the poloidal beta parameter , @xmath88 , employing the normalization constant @xmath89 ( as used in cliste ) with @xmath81 and @xmath90 as specified under ( b ) and ( c ) , respectively . * the plasma self - inductivity @xmath91 is given by an integral over the squared poloidal magnetic field @xmath92 .",
    "* @xmath93 , the difference of the poloidal flux at the two x - points divided by ( @xmath94 ) and by the poloidal magnetic field at ( @xmath95 ) : @xmath96 .    [",
    "[ implementation - and - parallelization-1 ] ] implementation and parallelization + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the algorithms for computing the plasma - control parameters and their basic implementation are taken directly from the ide code .",
    "we emphasize that unlike other real - time codes @xcite gpec makes no algorithmic simplifications compared with the offline variant of the code .",
    "in particular , gpec uses the same spatial grid resolution as adopted for equilibrium reconstruction and employs the high - order interpolation schemes ( cubic splines , bicubic hermite interpolation ) of ide . in the following we",
    "briefly summarize the main concepts adopted for the computationally most expensive routines :    1 .   for performing the integrations @xmath97 , grid points located outside the separatrix",
    "are masked out according to a comparison with the value of @xmath2 at the x - point .",
    "a custom contour - finding algorithm is used which employs quadratic interpolation to obtain the values at the intersections of the spatial grid with the contour curve .",
    "quantities on the lcfs , like @xmath98 , are determined by quadratic extrapolation from three equidistant contour levels close to the plasma surface .",
    "3 .   for the evaluation of the toroidal plasma",
    "current @xmath99 the current density @xmath100 is computed according to eq .",
    "( [ rhsdecomp ] ) .",
    "the summands @xmath101 and @xmath102 in principle would be available from the last equilibrium iteration but are distributed among processors ( cf .",
    "[ sect : algorithm_equil ] ) .",
    "it turns out that locally recomputing @xmath103 and @xmath104 by using the converged equilibrium solution @xmath2 ( a copy of which is available on each processor ) is faster than collecting the individual summands by mpi communication .",
    "the computation of the pressure distribution @xmath86 is based on the identity @xmath105 ( cf .",
    "eq .  [ rhsdecomp ] ) . for fast evaluation of @xmath86 on the poloidal mesh , an equidistant grid of @xmath106 points",
    "is constructed . for each @xmath107 the corresponding pressure contribution @xmath108 is computed by numerical integration starting at the plasma boundary .",
    "a cubic spline interpolation is used to evaluate @xmath109 on this grid .",
    "the gradients for computing the poloidal magnetic field are obtained using centered finite differences of the neighboring grid points ( consistent with the derivatives used to define the bicubic hermite interpolation polynomials for intra - grid interpolation , cf .",
    "[ sect : algorithm_equil ] ) .    as shall be shown in sect .",
    "[ sect : performance ] the computing time is dominated by finding and integrating along contours as well as by evaluating the pressure distribution .",
    "accordingly , the computations are grouped into the following mutually independent tasks which can be assigned to different mpi tasks , with only a few scalar quantities to be communicated for evaluating the final results :    * computation of the pressure distribution @xmath86 and the poloidal magnetic field * handling of 3 contour levels for extrapolation to the last closed flux surface * handling of 4 contour levels corresponding to the set of safety factors @xmath110 * determination of the x - point and handling of 1 contour level corresponding to the separatrix curve    within each mpi task , openmp threads are used , e.g.  for parallelizing the computation of the pressure over the individual points of the @xmath60-grid or for parallelizing over different contour levels .      the codes cliste and ide have been extensively validated and verified by application to asdex upgrade data and by means of code comparisons .",
    "thus , only those parts of the new gpec code need to be verified which are treated differently from the ide code .",
    "the procedure is greatly facilitated by the fact that both codes implement the same algorithms and share the same code base .",
    "the only differences in the gpec implementation with respect to ide are 1 ) the prescription of a fixed number of equilibrium iterations and 2 ) the use of only 12 instead of 30 integration points for computing the pressure distribution @xmath86 from its gradient @xmath111 ( see item d above ) on every grid point @xmath60 .    .",
    "for each quantity the relative deviation from its converged value is shown as a function of the number of iterations .",
    "note that the deviations for the geometric quantities , @xmath112 , @xmath113 , and the total volume of the plasma , @xmath114 are multiplied by a factor of 10 .",
    "data was taken from aug shot # 23221 at time @xmath115  ms . ]",
    "figure  [ fig : sens_iter ] shows the sensitivity of a number of selected parameters to the number of equilibrium iterations for an arbitrarily chosen time point ( @xmath115  ms ) of aug discharge # 23221 . by using the solution @xmath2 from the last time point as an initial value , the plasma geometry ( parameters @xmath112 , @xmath113 , and the total volume of the plasma , @xmath114 ) is already very well determined . within only a few iteration steps accuracies of better than a percent compared with the converged solution ( obtained with 200 iterations )",
    "are obtained also for the quantities which depend on the pressure distribution ( @xmath116 , @xmath117 ) , or on the toroidal current ( @xmath90 ) . for the real - time demonstration presented in the subsequent section we shall fix the number of iterations to four and show that the accuracy level of 1  % is maintained over a time sequence of 1  s with 1000 time points .    figure",
    "[ fig : sens_npsi ] shows that a number of 12 integration points for the numerical integration of @xmath111 is more than sufficient in order to reach sub - percent accuracies for the control parameters @xmath116 and @xmath117 , both of which are proportional to @xmath118 ( see item d above ) .",
    "the time for computing @xmath86 is proportional to the number of integration points and scales almost ideally with the number of threads which motivates our specific choice of 12 points as a multiple of 6 threads that shall be employed for the majority of computations ( cf .",
    "[ sect : performance ] ) .     and of the total energy content @xmath117 to the number of integration points , @xmath119 , spent for computing the pressure from its gradient on every grid point @xmath60 .",
    "the relative deviation from the converged value is shown as a function of the number of integration points , @xmath120 .",
    "data was taken from aug shot # 23221 at time @xmath115  ms . ]",
    "and @xmath121 . the time interval @xmath122 $ ] which is chosen for performance analysis is indicated by horizontal lines .",
    "the individual plots show the coordinates @xmath123 and @xmath124 of the magnetic axis , and of the x - point , respectively , the horizontal and vertical minor plasma radius , @xmath125 and @xmath126 , the plasma self - inductivity , @xmath127 , the poloidal beta parameter , @xmath128 , the total energy content of the plasma , @xmath129 , and the safety factor @xmath130 ( see sect .  [ sect : postprocessing ] for the definition of the quantities ) . the solid , black lines correspond to a `` real - time '' application which is restricted to 4 equilibrium iterations . for comparison ,",
    "values derived from converged equilibrium solutions ( convergence criterion @xmath131 ) are shown as dashed , blue lines . a standard spatial resolution of @xmath132 grid points and 7 basis functions were employed . ]",
    "$ ] ) of the quantities which were computed after 4 equilibrium iterations , from the values computed with converged equilibrium solutions ( convergence criterion @xmath133 ) during the time interval chosen for performance analysis ( as indicated by horizontal lines in fig .",
    "[ fig : shot_23221alltime ] ) . a standard spatial resolution of @xmath132 grid points and 7 basis functions were employed . ]    in order to demonstrate the real - time capability of the new code a post - processing run is performed using data from a typical aug discharge .",
    "we chose aug shot number # 23221 , and focus on a time window between @xmath134 and @xmath135 with 1000 time points , corresponding to a time resolution of exactly 1  ms .",
    "as shall be shown below this simulation of 1000 time points can be performed on standard server hardware within less than a second of computing time .",
    "the gpec code computes a fixed number of four equilibrium iteration steps for every time point and uses the equilibrium solution @xmath136 at time point @xmath137 as the initial value for the next time point @xmath138 .",
    "a standard spatial resolution of @xmath139 grid points is employed , and @xmath140 basis functions are used .",
    "the response matrix @xmath35 contains signals from 61 magnetic probes and 18 flux loops and in addition takes into account 10 external coils .",
    "figure  [ fig : shot_23221alltime ] provides an overview of the time evolution of a number of characteristic parameters , such as selected plasma - shape parameters , energy content and safety factor @xmath130 ( black , solid lines ) . for reference",
    ", the figure shows an extended time window between @xmath141 and @xmath121 , which includes the dynamic start - up phase of the discharge with significant variations in the plasma parameters , and also shows results from a run which converges the solution @xmath2 until the maximum changes on the grid , @xmath43 , are below @xmath142 ( blue , dashed lines ) .",
    "compared with the converged reference run one notices good overall agreement of the `` real - time '' simulation with a fixed number of 4 equilibrium iterations throughout the extended time interval , even during the initial start - up phase which lasts until approx .",
    "1.5  s. it is hence justified to confine the analysis on the  arbitrarily chosen  time interval , @xmath143 $ ] in the `` plateau '' phase of the discharge .",
    "focusing on this time window , figure  [ fig : shot_23221-accuracy ] shows that the relative errors for most quantities are at smaller than one percent .",
    "the comparably large variations of @xmath144 are due to its small absolute value .",
    "the absolute scatter of @xmath144 is in the order of mm .",
    "note that the cycle time of @xmath145  ms is much shorter than the timescale of the changes of plasma conditions ( cf .",
    "the increase of @xmath129 , @xmath128 at @xmath146  s ) .",
    "thus the equilibrium solution , even if not fully converged , can easily follow such secular trends , which is reflected by the fact that there is no visible change in the magnitudes of the relative error at @xmath146  s.    the employed spatial resolution of @xmath139 grid points is a common choice for real - time analysis in medium - sized tokamaks @xcite . for the chosen aug",
    "discharge a comparison with a run using a twofold finer spatial grid spacing , @xmath147 , shows good agreement for the majority of quantities , with @xmath130 and @xmath148 exhibiting the largest sensitivity to the resolution .    we conclude that by performing four iterations per time point and using @xmath139 spatial grid points , sufficiently accurate equilibrium solutions for deriving real - time control parameters are obtained , at least in the sense of a proof - of - principle using the chosen example data from aug shot # 23221 .",
    "a systematic analysis of the accuracy and a comprehensive validation of the code under various tokamak operational scenarios is beyond the scope of this paper .",
    "[ [ overview ] ] overview + + + + + + + +    the computational performance of gpec is assessed on a standard compute cluster with x86_64 cpus and infiniband ( fdr  14 ) interconnect .",
    "the individual compute nodes are equipped with two intel xeon e5 - 2680v3 `` haswell '' cpus ( 2.5 ghz , 2x12 cores ) .",
    "we use a standard intel software stack ( fortran compiler v14.0 , mpi v5.0 ) on top of the linux operating system ( sles11 sp3 ) . for the required linear algebra functionality the openblas @xcite library ( version 0.2.13 )",
    "is employed using the de - facto - standard interfaces from blas @xcite and lapack @xcite , and the fftw @xcite library ( version 3.3.4 ) is used for the discrete sine transforms in the poisson solver .",
    "both libraries are open - source software released under the bsd or the gpl license , respectively .",
    "if desired , open - source alternatives to the intel compilers ( e.g. gcc @xcite ) and mpi libraries ( e.g. openmpi @xcite , mvapich @xcite ) could be readily utilized , albeit possibly with a certain performance impact .",
    ".overview of the total runtime @xmath149 per time point ( @xmath150 column ) for different combinations of the numerical resolution ( @xmath151 column ) and compute resources .",
    "the latter are given in terms of the total number of cores ( nodes ) ( @xmath152 column ) , of mpi tasks ( @xmath153 column ) and of openmp threads per mpi task ( @xmath154 column ) .",
    "@xmath149 is the sum of the runtime @xmath155 ( @xmath156 column ) for computing the equilibrium distribution @xmath2 with 4 iteration steps ( cf .",
    "[ sect : algorithm_equil ] ) , and @xmath157 ( @xmath158 column ) for deriving plasma - control parameters ( cf .",
    "[ sect : postprocessing ] ) .",
    "the benchmarks were performed on xeon e5 - 2680v3 `` haswell '' cpus ( 2.5 ghz , 24 cores per node ) and used data from aug shot # 23221 . [ cols= \" < , <",
    ", > , > , > , > , > , > \" , ]     [ [ equilibrium - reconstruction ] ] equilibrium reconstruction + + + + + + + + + + + + + + + + + + + + + + + + + +    table  [ tab : psi_timing ] further shows that the runtime for the equilibrium iterations is dominated by computing the distributed sum , @xmath159 , which is also an implicit synchronization point for all processes , and by the poisson solver . due to the use of high - order interpolation schemes ( cf .",
    "sect.[sect : algorithm_equil ] ) a significant fraction of about 25% is spent on the evaluation of the right - hand sides of eqs .",
    "( [ gsphi1],[gsphi2 ] ) ( `` rhs update '' ) and of the response matrix @xmath35 ( `` rm evaluation '' ) .",
    "in general , the chosen products of number of mpi tasks times openmp threads per task turned out as the most efficient ones for this hardware platform with 24 cores per node : using more than 6 threads ( or 8 threads in the case of @xmath147 ) results only in a modest speedup of the poisson solver @xcite but doubles the required number of cores and network resources .",
    "keeping those constant and reducing the number of mpi tasks instead by a factor of two would require each mpi task to handle two basis functions .",
    "although the collective mpi communications would be somewhat faster in this case the resultant doubling of the effective runtime for the poisson solver can not be compensated for .",
    "[ [ plasma - control - parameters ] ] plasma - control parameters + + + + + + + + + + + + + + + + + + + + + + + + +    ) and their assignment to four mpi tasks which execute in parallel . within each task",
    "the individual parts of the algorithm ( indicated by different colours ) execute in the order from bottom to top . accordingly",
    ", the dashed horizontal line marks the effective runtime in the application .",
    "the data corresponds to aug shot # 23221 computed with a resolution of @xmath160 using in total 8 mpi tasks , each with 6 openmp threads ( cf .",
    "table  [ tab : pide_timing ] ) . ]    for the example of @xmath161 computed with 48 cpu cores , figure  [ fig : pp_mpi ] shows how the runtime of @xmath162  ms ( cf .",
    "table  [ tab : pide_timing ] ) is composed and illustrates the benefits of exploiting task parallelism . due to the final mpi_allreduce operation in the equilibrium construction algorithm , which comes at little extra effective runtime cost as compared to a single mpi_reduce , all mpi tasks hold a copy of the equilibrium flux distribution @xmath10 .",
    "hence , independent parts of the time - consuming computations for the plasma - control parameters like , e.g.  evaluating the pressure distribution or determining the contours for the safety factor @xmath1 , can be handled in parallel by different mpi tasks and only scalar quantities need to be communicated ( within a few microseconds ) between the mpi tasks at the end . if necessary",
    ", the remaining load - imbalance indicated by figure  [ fig : pp_mpi ] , namely task 0 and task 3 becoming idle after about 0.1  ms , and tasks @xmath163 not computing anything at all at this stage , could be exploited for computing additional quantities and/or for further reducing the effective runtime . without task - parallelization , on the contrary",
    ", the runtimes would add up to more than 0.4  ms despite the thorough openmp parallelization ( e.g.  across different contour levels ) within the task .",
    "with the motivation of achieving sub - millisecond runtimes a new , parallel equilibrium - reconstruction code , gpec , was presented which is suitable for real - time applications in medium - sized tokamaks like asdex upgrade ( aug ) .",
    "gpec implements the classical concept of iteratively solving the grad - shafranov equation and feeding in diagnostic signals from the experiment @xcite .",
    "specifically , gpec is implemented as a variant of the ide code @xcite , which is a descendant of cliste @xcite .",
    "compared with these well - established and validated offline - analysis codes no algorithmic simplifications are necessary for achieving the desired cycle times of less than a millisecond , besides limiting the number of equilibrium iterations to four .",
    "in addition to real - time applications the new code enables fast and highly accurate offline analyses soon after the tokamak discharge : here , tolerable runtimes are in the range of 0.11  s per cycle which allows computing fully converged equilibria employing highly resolved spatial and basis functions grids .",
    "the parallelization of the new code builds on a fast shared - memory - parallel grad - shafranov solver @xcite together with the mpi - distributed solution of the individual poisson - type problems and a thorough parallelization of the post - processing algorithms for computing the relevant plasma - control parameters from the equilibrium flux distribution .",
    "using data from a typical aug discharge the real - time capability of the new code was demonstrated by the offline computation of a sequence of 1000 time points within less than a second of runtime .",
    "the relative accuracy was ascertained by comparing the relevant plasma parameters with a converged run . by allowing four iterations for computing the equilibrium solution",
    "the majority of control parameters can be computed with an accuracy of a percent or better .    the adopted two - level , hybrid parallelization scheme allow efficient utilization available compute resources ( in terms of the numbers of nodes , of cpu sockets per node and of cores per socket ) for a given numerical resolution ( in terms of the spatial grid and number of basis functions ) .",
    "moreover , foreseeable advances in computer technology , in particular the increasing number of cpu cores per compute node , are expected to push the limits of real - time applications with gpec towards even higher numerical accuracies ( in terms of affordable resolution and number of equilibrium iterations ) .",
    "we note that the benchmarks figures reported in this work can be considered rather conservative as the computations were performed on a standard compute cluster which comprises hundreds of nodes . for energy - budget reasons such clusters",
    "are typically configured not with cpus of the highest clock - frequency . in our case ,",
    "for example , cpus with 2.5 ghz and only 24 cores per node were available .",
    "for deployment in the control system of a tokamak experiment such as aug , by contrast , we envision dedicated server hardware with higher clock frequencies and at least four cpu sockets , both of which is expected to further boost the computational performance of the real - time application . thus it should be rather straightforward to save enough time for the communication of the code with the control system , which , depending of the specifics of the system , requires another few hundred microseconds per cycle .",
    "finally , it is worth mentioning that gpec is based entirely on open - source software components , relies on established industry ( fortran , mpi , openmp ) or de - facto software standards ( blas , lapack , fftw ) , runs on standard server hardware and software environments and hence can be released to the community and utilized without legal or commercial restrictions .",
    "we acknowledge stimulating discussions with k.  lackner and w.  treutterer .",
    "thanks to p.  martin who has provided a subversion of the cliste code with which this study started and to s.  gori who has developed an initial fortran  90-version .",
    "+ this work has been carried out within the framework of the eurofusion consortium and has received funding from the euratom research and training programme 2014 - 2018 under grant agreement no 633053 . the views and opinions expressed herein do not necessarily reflect those of the european commission ."
  ],
  "abstract_text": [
    "<S> * a new parallel equilibrium reconstruction code for tokamak plasmas is presented . </S>",
    "<S> gpec allows to compute equilibrium flux distributions sufficiently accurate to derive parameters for plasma control within 1  ms of runtime which enables real - time applications at the asdex upgrade experiment ( aug ) and other machines with a control cycle of at least this size . </S>",
    "<S> the underlying algorithms are based on the well - established offline - analysis code cliste , following the classical concept of iteratively solving the grad - shafranov equation and feeding in diagnostic signals from the experiment . </S>",
    "<S> the new code adopts a hybrid parallelization scheme for computing the equilibrium flux distribution and extends the fast , shared - memory - parallel poisson solver which we have described previously by a distributed computation of the individual poisson problems corresponding to different basis functions . </S>",
    "<S> the code is based entirely on open - source software components and runs on standard server hardware and software environments . </S>",
    "<S> the real - time capability of gpec is demonstrated by performing an offline - computation of a sequence of 1000 flux distributions which are taken from one second of operation of a typical aug discharge and deriving the relevant control parameters with a time resolution of a millisecond . on current server hardware </S>",
    "<S> the new code allows employing a grid size of @xmath0 zones for the spatial discretization and up to 15 basis functions . </S>",
    "<S> it takes into account about 90 diagnostic signals while using up to 4 equilibrium iterations and computing more than 20 plasma - control parameters , including the computationally expensive safety - factor @xmath1 on at least 4 different levels of the normalized flux . * </S>"
  ]
}