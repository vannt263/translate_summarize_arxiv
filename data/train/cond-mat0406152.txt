{
  "article_text": [
    "scale - free networks with heavy - tailed and power - law ( pl ) degree distributions have been observed in several different fields and scenarios ( see , e.g. , @xcite and references therein ) . in a pl degree distribution ,",
    "the probability that a randomly chosen node has degree @xmath9 is given by @xmath10 , where @xmath11 is referred to as the exponent of the distribution .",
    "for @xmath12 a network with @xmath0 nodes has constant or at most @xmath2 average degree , but the variance of the degree distribution is unbounded .",
    "it is in this regime of @xmath13 that the pl networks display many of the advantageous properties , such as small diameter @xcite , tolerance to random node deletions @xcite , and a natural hierarchy , where there are sufficiently many nodes of high degree .    the searching problem in random power - law networks can be stated as follows@xcite : starting from a randomly selected node , _ the source _ , find another randomly selected node , _ the destination _",
    ", through only _",
    "local communications_. equivalently , this can be cast into a messaging problem , where it is desirable to transfer a message from an arbitrary node to another randomly chosen node through local ( i.e. , first neighbor ) communications . since a searcher has no idea about the location of the destination node in the network ( unless , each node somehow has path information for all other nodes cached in it ) , the problem is indeed that of transferring a message from a node to _ all _ other nodes in the network .    another equivalent version of this problem appears in",
    "_ unstructured p2p networks _ , such as gnutella@xcite , limewire@xcite , kazaa@xcite , morpheus@xcite , and imesh @xcite , where the data objects _ do not have global unique ids _ , and _ queries are done via a set of key words_. the reasons that search in pl networks is important for such unstructured p2p networks , include : ( i ) a number of recent studies have shown that the structure of these existing networks has complex network characteristics @xcite , including approximate power law degree distributions .",
    "thus pl networks , or at least networks with heavy - tailed degree distributions , seem to naturally emerge in the existing services .",
    "( ii ) systematic p2p protocols that will lead to the emergence of pl networks with tunable exponents , even when nodes are deleted randomly , have been proposed recently @xcite .",
    "this makes it possible to systematically design robust and random p2p networks that admit pl degree distributions , and that can exploit several properties of pl graphs that are extremely useful for networking services , e.g. , low diameter , which allows fast searches , a randomized hierarchy , which allows optimal usage of heterogeneous computing and networking resources without the intervention of a global manager , and extreme tolerance to random deletions of nodes , which provides robustness .    in a straightforward parallel search approach in p2p networks ,",
    "each query is given a unique i d , and then each node on receiving the query message sends it out to all of its neighbors , unless the node has already processed the query , which the node can identify by checking the i d s of the queries it has already processed .",
    "this leads to @xmath14 total queries in the network for every single query , and results in significant scaling problems .",
    "for example , ripeanu et.al.@xcite estimated that in december of 2000 gnutella traffic accounted for @xmath15 of internet backbone traffic .    as reviewed later in section [ p2p - sec ] ,",
    "a number of ad hoc measures , ranging from forcing an ultra - peer structure on the network , to a random - walk based approach , where it is assumed that a constant fraction of the nodes in the network caches each content s location , have been proposed in the p2p community . but",
    "none of these measures provides a provably scalable and decentralized solution , where any content , even if it is located in only one node , is guaranteed to be found .",
    "the only systematic work on searches in random pl networks reported so far @xcite , employs a _ serial search technique _ based on random walks and caching of content - lists of every node on all its neighbors ( or on all its first and second neighbors ) , and is reviewed in greater detail in section [ prior - work ] .",
    "we present a parallel , but scalable , search algorithm that exploits the structure of pl networks judiciously , and provides precise scaling laws that can be verified via extensive large - scale simulations ( section [ iii ] ) .",
    "the key steps in our search algorithm are : ( i ) _ caching or content implantation : _ each node executes a short random walk and caches its content list or directory on the visited nodes . for example , for @xmath16 , this one - time - only random walk is of length @xmath2 , and thus the average cache size per node is @xmath2 .",
    "( ii ) _ query implantation _ : when a node wants to make a query , it first executes a short random walk and implants its query request on the nodes visited .",
    "( iii ) _ bond percolation : _ all the implanted query requests are propagated independently through the network _ in parallel _ using a probabilistic broadcast scheme . in this scheme ,",
    "a node on receiving a query message for the first time , relays the message on each of its edges with a certain probability @xmath17 , which is vanishingly greater than the percolation threshold , @xmath18 , of the underlying pl network ( see @xcite for a review of bond percolation in pl graphs ) .    the physics of how and why percolation search algorithm works efficiently , can be described as follows .",
    "the bond percolation step , executed just above the percolation threshold , guarantees that a query message is received by all nodes in a giant connected component of diameter @xmath2 and consisting of high - degree nodes .",
    "the content and query implantation steps ensure that the content list of every node is cached on at least one of the nodes in this giant component with probability approaching one , and that one of the nodes in the giant connected component receives a query implantation with probability approaching one .",
    "thus with @xmath19 traffic ( which scales sublinearly for pl graphs , as shown in section [ iii ] and @xcite ) , any content ( _ even if it is owned by a single node in the network _ ) can be located with _",
    "probability approaching one _ in time @xmath2 .",
    "an interesting outcome pertaining to the physics of networking is that the accessible contents / nodes _ exhibit a first - order phase transition _ as a function of the broadcast or percolation probability @xmath17 , showing a sharp rise as soon as @xmath17 exceeds the percolation threshold @xmath18 .",
    "in contrast to the accessible contents , the _ number of nodes and edges _ in the giant connected component _ exhibits only a second order phase transition_. one of the _ primary appeals of the percolation search algorithm _ is that by combining serial random walks ( i.e. , content and query implantations ) with bond percolation _ it engineers a second - order phase transition into a first - order _ , allowing query - hits approaching 100% , even when @xmath20 .",
    "while the proof that the percolation search algorithm leads to scalable traffic and low latency is based on fairly involved concepts , the _ algorithm itself can be easily implemented _ and _ directly adapted to solve the scaling problem plaguing unstructured p2p networks_. in section [ p2p - sec ] , we discuss such applications , and present simulation results which show that even on sample large - scale subnetworks of existing p2p services , the overall traffic can be reduced by almost two - orders of magnitude , without any significant loss in search performance , by a direct implementation of percolation search .",
    "we also consider _ heterogeneous networks _ in section [ hetero - sec ] , where the degree distribution is a mixture of heavy - tailed and light - tailed pl distributions .",
    "such mixture distributions can model networks , such as the popular p2p services , where nodes belong to only few types , and each type has its own capability and hence , its own degree distribution .",
    "we provide both simulation and analytical studies of the improvements to be accrued from the percolation search algorithms when implemented on random heterogeneous networks ( section [ hetero - sec ] ) .",
    "the search algorithm by adamic et .",
    "@xcite can be described as follows : to convey a message from node @xmath21 to @xmath22 , @xmath21 sends a message that goes on a random walk through the network .",
    "when arriving at a new node , the message requests it to scan all its neighbors for the destination node @xmath22 .",
    "if @xmath22 is not found among the neighbors of the current node , then the message is sent to one of the neighbors of the current node picked randomly .",
    "this algorithm exploits the skewed degree distribution of the nodes in pl networks : the random walk naturally gravitates towards nodes with higher degree , and therefore , by scanning the neighbors of these high degree nodes , the random walker is expected to soon be able to scan a large fraction of the network .",
    "one could also _ scan both the first and the second neighbors _ of a node visited through the random walk ( rather than just scanning its first neighbors ) , in order to find the destination node @xmath22 .",
    "estimates for both search time and the number of messages created ( i.e. , traffic ) per query can be obtained as follows : for a power - law random graph with exponent @xmath13 , the expected degree of a node arrived at via a random link is @xmath23 , assuming that @xmath24 .",
    "also , the expected number of the second neighbors of a node randomly arrived at by following a link is around @xmath25 .",
    "therefore , assuming that nodes are not scanned multiple times during the random walk , the whole network is expected to be scanned after around : @xmath26 hops if only the first neighbors are scanned , and @xmath27 if the second neighbors are scanned as well . for @xmath28 , and",
    "the case where _ both first and second neighbors _ of a node are scanned , the predicted scaling is poly - logarithmic in the size of the network .",
    "while this technique is an _ important first step towards exploiting the hierarchical structure of pl networks _ and provides a sublinear scaling of traffic , there are several drawbacks that need to be addressed :    * the actual performance of the algorithm _ is far worse than the theoretically predicted scaling laws_. the primary reason for this discrepancy is that the estimates in eqs .",
    "( [ slaws ] )  and  ( [ slaws2 ] ) are based on the assumption that the nodes scanned during a walk are unique , i.e. , no node is scanned more than once . as pointed out by the authors in @xcite ,",
    "while this is a good approximation at the start of the walk , it quickly becomes invalid when a good fraction of the nodes have been scanned .",
    "extensive simulations in @xcite show that actual scaling is significantly worse than the predicted values : for example , for @xmath29 , eq .  ( [ slaws ] ) predicts a scaling of @xmath30 , but the actual scaling observed is more than a power of @xmath31 worse ( i.e. , @xmath32 ) . the same is true for ( [ slaws2 ] ) , where a scaling of @xmath33 is predicted for @xmath29 while @xmath34 is observed . * _ the random - walk search is serial _ in operation , and even assuming that the predicted scalings are accurate , the search time for finding any node or its content in the network is polynomially long in @xmath0 . as an example",
    ", for @xmath35 , a value observed in early gnutella networks , the predicted search time scalings are : @xmath36 or @xmath37 .",
    "+ however , as mentioned before , these scalings are going to be significantly worse and we know that they will be at least larger than @xmath34 . * in order to obtain the best traffic scalings , one needs to scale cache ( storage ) size _ per node _ polynomially ; e.g. , for @xmath38 , the cache size per node should increase as @xmath39 .",
    "recall that the search strategy requires every node to answer if the node / content satisfying the query message is in any of its first neighbors or in any of its first and second neighbors .",
    "this scanning can be performed in two ways : ( 1 ) _ without caching : _ for each query message , the node queries all its first ( or first and second ) neighbors .",
    "_ this strategy is then at least as bad as flooding _ , since for each independent search , all the links have to be queried at least once which results in a traffic per search of at least @xmath14 . ( 2 ) _ with caching : _ each node caches its content - list on all its neighbors , or on all of its first and second neighbors , as required by the protocol . through the random",
    "walk , the walker can scan the contents of the neighbors ( or both first and second neighbors ) by observing the content lists in the current node without having to query the neighbors .",
    "the total cache size required per node in the case of the first - neighbor - only caching scheme is exactly the average degree of nodes ( i.e. , @xmath2 for @xmath28 ) , and @xmath40 ( i.e. , @xmath39 for @xmath28 ) when scanning of both the first and second neighbors are required .",
    "thus the least traffic and equivalently , shortest search times , are obtained at the expense of an increased cache size requirements per node .    as noted in the introduction and elaborated in the later sections , we build on the basic ideas in @xcite , and exploit the hierarchical structure of pl networks more efficiently to successfully resolve many of the above - mentioned issues . in particular ,",
    "our results have the following distinctive features : ( 1 ) the actual performance of the algorithm matches the theoretical predictions . ( 2 ) the algorithm takes @xmath2 time and is parallel in nature . (",
    "3 ) the average cache size increases with the exponent @xmath13 , and is minimum for @xmath28 , when the traffic scaling is the most favorable .",
    "for example , for a random pl network with exponent , @xmath41 , and maximum degree @xmath5 , we show that any content in the network can be found with probability approaching one in time @xmath2 , while generating only @xmath42 traffic per query",
    ". moreover , the content and query implantation random walks are @xmath2 in size , _ leading to the average cache size of @xmath2_. thus , if @xmath43 ( as is the case for a randomly generated pl network with no a priori upper bound on @xmath5 ) then the overall traffic scales as @xmath4 per query , and if @xmath44 ( as is the usual practice in the literature ) then the overall traffic scales as @xmath45 ( for any @xmath46 ) per query .",
    "the percolation search algorithm can be described as follows : + ( i ) _ content list implantation : _ each node in a network of size @xmath0 duplicates its content list ( or directory ) through a random walk of size @xmath47 starting from itself .",
    "the exact form of @xmath47 depends on the topology of the network ( i.e. , @xmath13 for pl networks ) , and is in general a sub - linear function of @xmath0 .",
    "thus the total amount of directory storage space required in the network is @xmath48 , and the average cache size is @xmath47 .",
    "note that , borrowing a terminology from the gnutella protocol , _ the length of these implantation random walks will be also referred to as the ttl ( time to live)_. + ( ii ) _ query implantation : _ to start a query , a query request is _ implanted _ through a random walk of size @xmath47 starting from the requester . + ( iii ) _ bond percolation : _ when the search begins , each node with a query implantation starts a _",
    "probabilistic broadcast search _",
    ", where it sends a query to each of its neighbors with probability @xmath17 , with @xmath49 where @xmath18 is the percolation threshold@xcite .",
    "we next derive scaling and performance measures of the above algorithm .",
    "our derivations will follow the following steps :    * first we define high degree nodes and compute the number of high degree nodes in a given network . *",
    "second , we show that after the probabilistic broadcast step ( i.e. , after performing a bond percolation in the query routing step ) , a query is received by all members of connected component to which an implant of that query belongs .",
    "we also see that the diameter of all connected components is @xmath2 , and thus the query propagates through it quickly .",
    "* third , we show that _ a random walk of length @xmath47 _ starting from any node will _ pass through a highly connected node , with probability approaching one_. this will ensure that ( i ) a pointer to any content is owned by at least one highly connected node , and ( ii ) at least one implant of any query is at one of the high degree nodes . *",
    "finally , we examine the scaling of the maximum degree of the network @xmath5 and give the scaling of query costs and cache sizes in terms of the size of the entire network @xmath0 .",
    "we show that both cache size and query cost scale sublinearly for all @xmath50 , and indeed can be made to scale @xmath4 with the proper choice of @xmath13 and @xmath5 .      in this section",
    "we define the notion of a high degree node . for any node with degree @xmath9",
    ", we say it is a high degree node if @xmath51 .",
    "we assume that we deal with random power - law graphs which have a degree distribution : @xmath52 and @xmath53 is the riemann zeta function .",
    "@xmath21 approaches the approximate value quickly as @xmath5 gets large , and thus can be considered constant .",
    "thus the number of high degree nodes , @xmath54 is given by : @xmath55 since for all decreasing , positive,@xmath56 we have @xmath57 and @xmath58 , we can bound @xmath54 from above and below : @xmath59 for @xmath60 we have that @xmath61 thus : @xmath62 we have shown that @xmath63 . as we discuss in section [ ssec : on_maximum ] , there are two choices for scaling of @xmath5 .",
    "if we put no prior limit on @xmath5 it will scale like @xmath64 . as we will discuss",
    ", we may also consider @xmath65 .",
    "we should note that the first scaling law gives @xmath66 , or a constant number of high degree nodes as the system scales .",
    "the second gives @xmath67 .",
    "for all @xmath68 , we have @xmath54 scaling sublinearly in @xmath0 .    in the next sections",
    "we will show that without explicitly identifying or arranging the high degree nodes in the network , we can still access them and make use of their resources to make the network efficiently searchable .      in conventional percolation studies ,",
    "one is guaranteed that as long as @xmath69 , where @xmath70 is a constant independent of the size of the network , then there will be a giant connected component in the percolated graph . however , in our case , i.e. , pl networks with @xmath71 , @xmath72 ( for example , @xmath73 for a pl network with exponent @xmath28 @xcite ) , and since the traffic ( i.e. , the number of edges traversed ) scales as @xmath74 , we can not afford to have a constant @xmath7 such that @xmath75 : the traffic will then scale linearly .",
    "the conventional analysis can be extended to show that even if @xmath49 for a constant @xmath76 ( thus , @xmath77 ) one is still guaranteed to have a giant connected component in the percolated graph@xcite .",
    "we want to make our communications cost increase as slowly as possible .",
    "if we percolation to query nodes , then the communication cost must increase at least as fast as the percolation threshold .",
    "hence , we will percolate not at a constant above the threshold , but at a multiple above the threshold : @xmath49 .",
    "we consider this problem in detail in a separate work@xcite .",
    "the result is that if we _",
    "follow a random edge in the graph , the probability it reaches an infinite component is @xmath78 for a constant @xmath79 which depends only on @xmath13 and @xmath76 , but not @xmath5_.    thus , since each high degree node has at least @xmath80 degree , the average number of edges of a high degree node that connect to the infinite component ( @xmath81 ) is at least : @xmath82 the probability that a high degree node has at least one link to the infinite component is at least : @xmath83 thus both the average number of degrees that a high degree node has to the giant component , and the probability that a high degree node has at least one edge to the giant component are independent of @xmath5 .",
    "so as we scale up @xmath5 , we can expect that the high degree nodes stay connected to the giant component .",
    "we can make @xmath79 larger by decreasing @xmath76 , particularly , if @xmath84 we have @xmath85 @xcite .",
    "it remains to be shown that the diameter of the connected component is on the order of @xmath2 . to see this",
    ", we use the approximate formula @xmath86 @xcite of the diameter of a random graph with size @xmath87 and average degree @xmath88 . we know that the size of the percolated graph is @xmath89 and that the average degree is approximately 2@xcite .",
    "thus the diameter of the giant component is : @xmath90    at this point we have presented the main result . if we can cache content on high degree nodes , and query by percolation _ starting from _ a high degree node , we will always find the content we are looking for .",
    "we have not yet addressed how each node can find a high degree node . in the next section",
    "we show that by taking a short random walk through the network we will reach a high degree node with high probability , and this gives us the final piece we need to make the network searchable by all nodes .",
    "consider a random pl network of size @xmath0 and with maximum node degree @xmath5 .",
    "we want to compute the probability that following a randomly chosen link one arrives at a high degree node . to find this probability , consider the generating function @xmath91@xcite of the degree of the nodes arrived at by following a random link :    @xmath92    where @xmath93 .",
    "this results in the probability of arriving at a node with degree greater than @xmath94 to be : @xmath95 since the degrees of the nodes in the network are independent , each step of the random walk is an independent sample of the same trial .",
    "the probability of reaching a high degree node within @xmath96 steps is : @xmath97 therefore , after @xmath98 steps , a high degree node will be encountered in the random walk path with high ( constant ) probability .",
    "now we need to compute @xmath99 for @xmath28 and @xmath100 .",
    "since for all decreasing , positive , @xmath56 we have @xmath101 and @xmath102 , we can bound the following sums .",
    "if @xmath28 , we have the probability of arriving at a node with degree greater than @xmath94 is : @xmath103 and @xmath104 .",
    "we finally get : @xmath105 for @xmath28 , then in @xmath106 steps we have reached a high degree node .",
    "if @xmath107 , we have the probability of arriving at a node with degree greater than @xmath94 is : @xmath108 and @xmath109 .",
    "we finally get : @xmath110 for @xmath111 , then in @xmath112 steps we have reached a high degree node , which is polynomially large in @xmath5 rather than logarithmically large , as in the case of @xmath28 .    a sequential random walk requires @xmath113 time steps to traverse @xmath113 edges , and hence , the query implantation time will dominate the search time , making the whole search time scale faster than @xmath2 . recall that the percolation search step will only require @xmath2 time , irrespective of the value of @xmath13 .",
    "a simple parallel query implantation process can solve the problem .",
    "to implement @xmath114 query seeds for example , a random walker with time to live ( ttl ) of @xmath115 will initiate a walk from the node in question and at each step of the walk it implants a query seed , and also initiates a second random walker with time to live @xmath116 .",
    "this process will continue recursively until the time to live of all walkers are exhausted .",
    "the number of links traversed by all the walkers is easily seen to be : @xmath117 figure [ paral - vs - seq - fig ] gives simulation results to show that the parallel walk is effective , and thus search time scales as @xmath2 for all @xmath50 . in practice , for values of @xmath13 close to two , the quality of search is fairly insensitive to how the number of query implants are scaled .",
    "each time we want to cache a content , we send it on a random walk across @xmath118 edges .",
    "every time we query we send a message on a random walk across @xmath98 edges , and from each of the nodes it reaches it could at most reach the infinite component which is of size @xmath119 @xcite .",
    "thus the total communication cost of a query scales as @xmath120 . for @xmath121 , @xmath122 and @xmath123 when we make a query ,",
    "if we reach the giant component , each edge passes it with probability @xmath17 ( if we do nt reach a giant component only a constant number of edges pass the query ) .",
    "thus , the total communications traffic scales as @xmath124 .",
    "since @xmath125 we have @xmath126 . for all @xmath50 , @xmath127 . for @xmath121 , @xmath122 which gives @xmath128 for @xmath107 , @xmath129 is constant which gives @xmath130 in section [ ssec : high_degree_nodes ] , we showed that the number of high degree nodes @xmath131 .",
    "we also know that @xmath132 and @xmath133 and @xmath134 .",
    "thus we can rewrite the communication scaling in terms of the high degree nodes , @xmath135 .",
    "so we see that communication costs scales linearly in @xmath54 , but as the square of the length of the walk to the high degree nodes .",
    "this meets with our intuition since the high degree nodes are the nodes that store the cache and answer the queries .    in the next section we discuss explicit scaling of @xmath5 to get communication cost scaling as a function of @xmath0 .",
    "tables [ tb1 ] and [ tb2 ] show the scaling of the cache and communication cost in @xmath0 .",
    "we see that for all @xmath136 , we have sublinear communication cost scaling in @xmath0 .",
    "there are two ways to generate a random pl network : + ( i ) fix a @xmath5 and normalize the distribution , i.e. , @xmath137    to construct the random pl graphs , @xmath0 samples are then drawn from this distribution . for several reasons ,",
    "the choice @xmath65 is recommended in the literature @xcite , and in our scaling calculations ( e.g. , table [ tb1 ] ) we follow this upper bound . + ( ii ) no a priori bound on the maximum is placed , and @xmath0 samples are drawn from the distribution @xmath138 , where @xmath139 .",
    "it is quite straightforward to show that almost surely , @xmath140 .",
    "thus , when @xmath28 , @xmath141 ( @xmath142 ) in this method of generating a random pl graphs .    a potential problem with using the larger values of @xmath5 , as given by method ( ii )",
    ", is that the assumption that the links are chosen independently might be violated .",
    "random graph assumptions can be shown to still hold when the maximum degree of a power - law random graph is @xmath65 @xcite .",
    "this however does not necessarily mean , that the scaling calculations presented in the previous section do not hold for @xmath143 .",
    "in fact , extensive large - scale simulations ( see section [ simul - sec ] ) suggest that one can indeed get close to poly - logarithmic scaling of traffic ( i.e. , @xmath144 ) , as predicted by the scaling calculations in this section .",
    "there are several practical reasons for bounding @xmath5 , as well .",
    "first , in most grown random graphs , @xmath5 scales as @xmath145 . while grown random graphs display inherent correlations , we would like to compare our scaling predictions with performance of the search algorithm when implemented on grown graphs . hence , the scaling laws that would be relevant for such p2p systems correspond to the case of bounded @xmath5 .",
    "second , since the high degree nodes end up handling the bulk of the query traffic , it might be preferable to keep the maximum degree low .",
    "for example , for @xmath28 , the traffic generated is of the same order as the maximum degree , when @xmath146 , thus providing a balance between the overall traffic and the traffic handled by the high degree nodes individually .",
    ".the scaling properties of the proposed algorithm when @xmath147 . [ cols=\"^,^,^,^,^\",options=\"header \" , ]     [ tb10 ]",
    "we have presented a scalable search algorithm that uses random - walks and bond percolation on random graphs with heavy - tailed degree distributions to provide access to any content on any node with probability one .",
    "_ while the concepts involved in the design of our search algorithm have deep theoretical underpinnings , any implementation of it is very straightforward . _",
    "our extensive simulation results using both random pl networks and gnutella crawl networks show that unstructured p2p networks can indeed be made scalable .",
    "moreover , our studies show that even in networks with different categories of nodes ( i.e. , graphs where the degree distribution is a mixture of heavy - tailed and light - tailed distributions ) the search algorithm exhibits the favorable scaling features , while shielding the nodes with light - tailed degree distribution from the query - generated traffic .",
    "our recent results @xcite indicate that it is indeed possible to have local rules , that will enforce a desired category of the nodes in the network to have either a heavy or light tailed degree distribution .",
    "one can thus make sure that the subgraph consisting of the nodes with low capacity has a light tail , and is thus exempted from the search traffic with high probability . on the other hand ,",
    "the high capability nodes evolve into a subgraph with a heavy tail degree distribution and hence will carry the majority of the search load .    together with the new algorithms for building heavy - tailed growing graphs , even in the presence of extreme unreliability of the nodes , and a heterogeneous sets of nodes ( in terms of connectivity and bandwidth capacities ) , the percolation search algorithm can provide an end - to - end solution for constructing a large scale , highly scalable , and fault tolerant distributed p2p networking system .",
    "qin lv , pei cao , edith cohen , kai li , and scott shenker , _ search and replication in unstructured peer - to - peer networks _ , proceedings of the 16th international conference on supercomputing , acm press , 2002 , pp .",
    "8495 .",
    "matei ripeanu , ian foster , and adriana iamnitchi , _ mapping the gnutella network : properties of large - scale peer - to - peer systems and implications for system design _ , ieee internet computing journal * 6 * ( 2002 ) , no .  1 ."
  ],
  "abstract_text": [
    "<S> we introduce a scalable searching algorithm for finding nodes and contents in random networks with power - law ( pl ) and heavy - tailed degree distributions . </S>",
    "<S> the network is searched using a probabilistic broadcast algorithm , where a query message is relayed on each edge with probability just above the bond percolation threshold of the network . </S>",
    "<S> we show that if each node caches its directory via a short random walk , then the total number of _ accessible contents exhibits a first - order phase transition _ , ensuring very high hit rates just above the percolation threshold . in any random pl network of size , @xmath0 , and exponent , @xmath1 , the total traffic per query scales sub - linearly , while the search time scales as @xmath2 . in a pl network with exponent , @xmath3 , </S>",
    "<S> _ any content or node _ can be located in the network with _ probability approaching one _ in time @xmath2 , while generating traffic that scales as @xmath4 , if the maximum degree , @xmath5 , is unconstrained , and as @xmath6 ( for any @xmath7 ) if @xmath8 . </S>",
    "<S> extensive large - scale simulations show these scaling laws to be precise . </S>",
    "<S> we discuss how this percolation search algorithm can be directly adapted to solve the well - known scaling problem in unstructured peer - to - peer ( p2p ) networks . </S>",
    "<S> simulations of the protocol on sample large - scale subnetworks of existing p2p services show that overall traffic can be reduced by almost two - orders of magnitude , without any significant loss in search performance . </S>"
  ]
}