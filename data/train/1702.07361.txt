{
  "article_text": [
    "supermassive black holes ( smbhs ) reside in the majority of the galactic nuclei in the universe . in a subset of such galaxies ,",
    "accretion drives a highly energetic active galactic nucleus ( agn ) often associated with powerful jets .",
    "understanding the nature of these systems has been a central quest in astronomy and astrophysics .",
    "the smbhs at the centers of our galaxy ( sgr a * ) and the giant elliptical galaxy m87 provide unprecedented opportunities to directly image the innermost regions close to the central black hole , since the angular size of the event horizon is the largest among known black holes .",
    "the angular size of the schwarzschild radius ( @xmath3 ) is @xmath4  @xmath5as for sgr a * for a distance of 8.3 kpc and a mass of @xmath6  @xmath7 ( e.g. * ? ? ?",
    "* ) , and @xmath8 @xmath5as for m87 with a distance for 16.7  mpc @xcite and a mass of @xmath9  @xmath10 ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "the apparent diameter of the dark silhouette of the black hole is @xmath11  @xmath3 for the non - rotating black hole .",
    "it corresponds to @xmath12 @xmath5as for sgr a * and @xmath13 @xmath5as for m87 , which changes by only 4% with the black - hole spin and viewing orientation @xcite .",
    "very long baseline interferometric ( vlbi ) observations at short / sub - millimeter wavelengths ( @xmath14  mm , @xmath15  ghz ) can achieve a spatial resolution of a few tens of microarcseconds and therefore are expected to resolve event - horizon - scale structures , including the shadow of smbhs . indeed , recent significant progress on 1.3  mm vlbi observations with the event horizon telescope ( eht ; * ? ? ?",
    "* ) has succeeded in resolving compact structures of a few @xmath3 in the vicinity of the smbhs in both sgr a * and m87 ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "direct imaging of these scales will be accessible in the next few years with technical developments and the addition of new ( sub)millimeter telescopes such as the atacama large submillimeter / millimeter array ( alma ) to the eht ( e.g. * ? ? ? * ) .",
    "regardless of the observing wavelength , angular resolution ( often referred to as `` beam size '' in radio astronomy and `` diffraction limit '' in optical astronomy ) is simply given by @xmath16 , where @xmath17 and @xmath18 are the observed wavelength and the diameter of the telescope ( or the longest baseline length for the radio interferometer ) , respectively .",
    "a practical limit for a ground - based , 1.3  mm vlbi array like the eht is @xmath19 @xmath5as ( @xmath20  mm/@xmath21  km ) , which is comparable to the radius of the black hole shadow in m87 and sgr a*. hence , imaging techniques with good imaging fidelity at a spatial resolution smaller than @xmath22 would be desirable , particularly for future eht observations of m87 and sgr a*.    the imaging problem of interferometry is formulated as an under - determined linear problem when reconstructing the image from full - complex visibilities that are fourier components of the source image . in the context of compressed sensing ( also known as `` compressive sensing '' ) , it has been revealed that an ill - posed linear problem may be solved accurately if the underling solution vector is sparse @xcite . since then , many imaging methods have been applied to radio interferometry ( see * ? ? ? * and references therein ) .",
    "we call these approaches `` sparse modeling '' since they utilize the sparsity of the ground truth .    in @xcite",
    ", we applied lasso ( least absolute shrinkage and selection operator ; * ? ? ?",
    "* ) , a technique of sparse modeling , to interferometric imaging .",
    "lasso solves under - determined ill - posed problems by utilizing the @xmath0-norm ( see  [ sec:2.2 ] for details ) .",
    "minimizing the @xmath0-norm of the solution reduces the number of non - zero parameters in the solution , equivalent to choosing a sparse solution .",
    "the philosophy of lasso is therefore similar to that of the traditional clean technique @xcite , which favors sparsity in the reconstructed image and has been independently developed as matching pursuit @xcite in statistical mathematics for sparse reconstruction . in @xcite , we found that lasso can potentially reconstruct structure @xmath23 times finer than @xmath2 .",
    "indeed , it works well for imaging the black hole shadow for m87 with the eht in simulations .",
    "our previous work @xcite has three relevant issues .",
    "the first issue is reconstructing the image only from the visibility amplitudes and closure phases ( see  [ sec:2.1 ] ) , which have been the standard data products of eht observations @xcite and optical / infrared interferometry .",
    "the algorithm of @xcite is applicable only for full - complex visibilities , which are the usual data products from longer - wavelength radio interferometers .",
    "we have recently developed a fast and computationally cheap method to retrieve the visibility phases from closure phases ( precl ; * ? ? ?",
    "* ) , which can reconstruct the black hole shadow of m87 combined with lasso in simulated eht observations . however , since the phase reconstruction in precl adopts a different prior on visibilities than lasso , the resultant image may not be optimized well in terms of @xmath0-norm minimization and sparse image reconstruction .",
    "another potential approach is to solve for the image and visibility phases with @xmath0-norm regularization simultaneously , enabling us to reconstruct the image with full advantage of the regularization function .",
    "the second issue is that the @xmath0-norm regularization might not provide a unique solution and/or could reconstruct an image that is too sparse image if the number of pixels with non - zero brightness is not small enough compared to the number of pixels .",
    "this violates a critical assumption in techniques with @xmath0-norm regularization that the solution ( i.e. the true image ) should be sparse .",
    "such a situation may occur for an extended source or also even for a compact source if the imaging pixel size is set to be much smaller than the size of the emission structure .",
    "pioneering work has made use of transforms to wavelet or curvelet bases , in which the image can be represented sparsely ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "as another strategy to resolve this potential issue , in @xcite , we proposed to add another regularization , total variation ( tv ; e.g. * ? ? ?",
    "* ) , which is another popular regularization in sparse modeling .",
    "tv is a good indicator for sparsity of the image in its gradient domain instead of the image domain ( see  [ sec:2.2 ] for details ) and it has been applied to astronomical imaging ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) that includes optical interferometric imaging without the visibility phases ( e.g. mira ; @xcite ; and also see @xcite for a review ) .",
    "tv regularization generally favors a smooth image ( i.e. with larger effective resolution ) but with a sharp edge , in contrast with maximum entropy methods ( mem ; e.g. * ? ? ?",
    "* ) , which favor a smooth edge ( see e.g. * ? ? ?",
    "* for comparison between tv and mem ) .",
    "inclusion of tv regularization enables reconstruction of an extended image while preserving sharp emission features preferred by @xmath0-norm regularization , thereby extending the class of objects where sparse modeling is applicable .",
    "indeed , regularization with both the @xmath0-norm and tv has been shown to be effective for imaging polarization with full complex visibilities in our recent work @xcite .",
    "an important detail is the determination of regularization parameters ( e.g. weights on regularization functions ) , which is common in the vast majority of existing techniques . since one can not know the true image of the source a priori",
    ", one should evaluate goodness - of - fitting and select appropriate regularization parameters from the data themselves . in well - posed problems , one can use statistical quantities considering residuals between data and models as well as model complexity to avoid over - fitting , such as reduced @xmath24 , the akaike information criterion ( aic ) and the bayesian information criterion ( bic ) using the degrees of freedom to constrain model complexity .",
    "however , for ill - posed problems like interferometric imaging , degrees of freedom can not be rigorously defined , preventing the use of such statistical quantities .    in this paper ,",
    "we propose a new technique to reconstruct images from interferometric data using sparse modeling .",
    "the proposed technique directly solves the image from visibility amplitudes and closure phases .",
    "in addition to the @xmath0-norm ( lasso ) , we also utilize another new regularization term , tv , so that a high - fidelity image will be obtained even with a small pixel size and/or for extended sources .",
    "furthermore , we propose a method to determine optimal regularization parameters with cross validation ( cv ; see  [ sec:2.3 ] ) , which can be applied to many existing imaging techniques . as an example , in this paper , we applied our new technique to data obtained from simulated observations of m87 with the array of the eht expected in spring 2017 .",
    "a goal of radio and optical / infrared interferometry is to obtain the brightness distribution @xmath25 of a target source at a wavelength @xmath17 or a frequency @xmath26 , where @xmath27 is a sky coordinate relative to a reference position so called the phase - tracking center .",
    "the observed quantity is a complex function called visibility @xmath28 , which is related to @xmath25 by two - dimensional fourier transform given by @xmath29 here , the spatial frequency @xmath30 corresponds to the baseline vector ( in units of the observing wavelength @xmath17 ) between two antennas projected to the tangent plane of the celestial sphere at the phase - tracking center .",
    "observed visibilities are discrete quantities , and the sky image can be approximated by a pixellated version where the pixel size is much smaller than the nominal resolution of the interferometer .",
    "the image can therefore be represented as a discrete vector @xmath31 , related to the stokes visibilities @xmath32 by a discrete fourier transform @xmath33 : @xmath34 the sampling of visibilities is almost always incomplete . since the number of visibility samples @xmath32 is smaller than the number of pixels in the image , solving the above equation for the image @xmath31 is an ill - posed problem .    here , we consider that the complex visibility @xmath35 is obtained from observation(s ) with multiple antennas .",
    "let us define its phase and amplitude as @xmath36 and @xmath37 , respectively , denoted as follows @xmath38 where @xmath39 is the index of the measurement .",
    "each measurement corresponds to a point @xmath40 in @xmath30-plane and recorded at time @xmath41 . in actual observations , some instrumental effects and the atmospheric turbulence primary from the troposphere",
    "induce the antenna - based errors in the visibility phase , leading that the observed phase @xmath42 is offset from the true phase @xmath36 of the true image . in particular , this is a serious problem in vlbi observations performed at different sites ( see * ? ? ?",
    "however , the robust interferometric phase information can be obtained through the measurements of the _ closure phase _ , defined as a combination of triple phases on a closed triangle of baselines recorded at the same time .",
    "it is known that the closure phase is free from antenna - based phase errors @xcite , which can be seen from the following definition of the closure phase , @xmath43 where @xmath44 is the index of the closure phase , and upper numbers ( @xmath45 ) mean the index of stations involved in the closure phase or the visibility phase .",
    "the closure phase is also known as a phase term of the triple product of visibilities on closed baselines recorded at the same time , @xmath46 , known as the _ bi - spectrum_. closure phases have been used to calibrate visibility phases in vlbi observations ( e.g. * ? ? ? * ) .    in short / sub - millimeter vlbi or optical / infrared interferometry ,",
    "the stochastic atmospheric turbulence in the troposphere over each station induces a rapid phase rotation in the visibility , making it difficult to calibrate or even measure the visibility phase reliably ( e.g. , see * ? ? ?",
    "* ; * ? ? ?",
    "thus , image reconstruction using more robust closure phases , free from station - based phase errors , is useful for interferometric imaging with such interferometers .      in this paper",
    ", we propose a method to solve the two - dimensional image @xmath47 by the following equations : @xmath48 the cost function @xmath49 is defined as @xmath50 where @xmath51 is @xmath52-norm of the vector @xmath31 given by @xmath53 and @xmath54 indicates an operator of tv .    the first term of eq.([eq : cost_func ] )",
    "is the traditional @xmath55 term representing the deviation between the reconstructed image and observational data ( i.e. the visibility amplitude @xmath56 and closure phase @xmath57 ) , defined by @xmath58 where @xmath59 and @xmath60 indicate operators to calculate the visibility amplitude and closure phase , respectively .",
    "deviations between the model and observational data are normalized with the errors .",
    "this form of the residual sum of squares ( rss ) is originally proposed in the bi - spectrum maximum entropy method ( bsmem ; * ? ? ?",
    "* ) and also for modeling eht data ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) . note that it could be replaced to a rss term for bi - spectra ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "the second term represents lasso - like regularization using the @xmath0-norm . under the non - negative condition ,",
    "@xmath0-norm is equivalent to the total flux .",
    "@xmath61 is the regularization parameter for lasso , adjusting the degree of sparsity by changing the weight of the @xmath0-norm penalty . in general",
    ", a large @xmath61 prefers a solution with very few non - zero components , while @xmath62 introduces no sparsity . in this paper",
    ", we use the normalized regularization parameter @xmath63 defined by @xmath64 which is less affected by the number of visibility amplitude and closure phase data points , @xmath65 and @xmath66 , respectively , and also by the total flux density of the target source .",
    "the third term is the tv regularization , defined by the sum of all differences of the brightness between adjacent image pixels . in this paper",
    ", we adopt a typical form for two - dimensional images @xcite that has been used in astronomical imaging , ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , defined as @xmath67 adjusts the effective spatial resolution of the reconstructed image . in general , a larger ( smaller ) @xmath68 prefers smoother ( finer ) distribution of power with less ( higher ) discreteness , leading to larger ( smaller ) angular resolution . in the present work",
    ", we use the normalized regularization parameter @xmath69 defined by @xmath70 similar to the lasso term . a factor of 4 is based on a property of tv that takes a difference in the brightness to all four directions at each pixel . note that a major difference to maximum entropy methods , which also favor smooth images , is that tv regularization has a strong advantage in edge - preserving ; strong tv regularization favors a piecewise smooth structure , but with clear and often sharp boundaries between non - emitting and emitting regions .",
    "the problem described in eq.([eq : equation ] , [ eq : cost_func ] ) is non - linear minimum optimization . in this work",
    ", we adopt a non - linear programming algorithm l - bfgs - b @xcite that is an iterative method for solving bound - constrained nonlinear optimization problems .",
    "l - bfgs - b is one of the quasi - newton methods that approximates the broyden - fletcher - goldfarb - shanno ( bfgs ) algorithm using a limited amount of computer memory . in l - bfgs - b ,",
    "the cost function and its gradient are used to determine the next model parameters at each iterative process .",
    "we approximately set partial derivatives to 0 at non - differentiable points for both the @xmath0-norm and tv .",
    "the partial derivatives of @xmath71 are calculated numerically with central differences .",
    "we use the latest fortran implementation of l - bfgs - b ( l - bfgs - b v3.0 ; * ? ? ?",
    "we note that the problem is non - convex as other imaging techniques using closure quantities ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , and a global solution is generally not guaranteed .      in the proposed method ,",
    "the most important tuning parameters are the regularization parameters for the @xmath0-norm ( @xmath63 ) and tv ( @xmath72 ) , which determine the sparseness and effective spatial resolution of the image , respectively .",
    "smaller regularization parameters generally favor images with larger numbers of non - zero image pixels and more complex image structure , which could give better @xmath55 values by over - fitting . on the other hand ,",
    "large regularization parameters provide images that are too simple and that do not fit the data well . to determine optimal parameters , we need to evaluate the goodness - of - fit using occam s razor to prevent over - fitting .",
    "in this work , we adopt cross validation ( cv ) to evaluate goodness - of - fit .",
    "cv is a measure of the relative quality of the models for a given set of data .",
    "cv checks how the model will generalize to an independent data set by using separate datasets for fitting the model and for testing the fitted model .",
    "cv consists of three steps : ( 1 ) randomly partitioning a sample of data into complementary subsets , ( 2 ) performing the model fitting on one subset ( called the _ training set _ ) , and ( 3 ) validating the analysis on the other subset ( called the _ validation set _ ) . to reduce variability , multiple rounds of cross - validation",
    "are performed using different partitions , and the validation results are averaged over the rounds",
    ". if the regularization parameters are too small , the established model from the training set would be over - fitted and too complicated , resulting in a large deviation in the validation set .",
    "on the other hand , if the regularization parameters are too large , the established model would be too simple and not well - fitted to the training set , also resulting in a large deviation in the validation set .",
    "thus , reasonable parameters can be estimated by finding a parameter set that minimizes deviations ( e.g. @xmath55 ) of the validation set .    in this work",
    ", we adopted 10-fold cv for evaluating the goodness - of - fit .",
    "the original data were randomly partitioned into 10 equal - sized subsamples .",
    "9 subsamples were used in the image reconstruction as the training set , and the remaining single subsample was used as the validation set for testing the model using @xmath55 .",
    "we repeated the procedure by changing the subsample for validation data 10 times , until all subsamples were used for both training and validation .",
    "the @xmath55 values of the validation data were averaged and then used to determine optimal tuning parameters .",
    "an important advantage of this method compared with previously proposed methods is that it is applicable to any type of regularization functions and also imaging with multiple regularization functions .",
    "for instance , @xcite and subsequent work solve images by utilizing the @xmath0-norm on wavelet - transformed image or tv regularization alone . in this case , the parameter can be uniquely determined from the @xmath73-norm of the estimated uncertainties on observational data ( see * ? ? ? * for details ) . however , it is not straightforward to extend the idea for the problems with multiple regularization functions . for another example",
    ", @xcite proposes another heuristic method to determine the regularization parameters on @xmath0-regularization on the wavelet / curvelet - transformed image by estimating its noise level on each scale , which is successful . however",
    ", the method would not work for all types of regularization functions . on the other hand",
    ", cv is a general technique that can be applied to imaging with any other regularization functions or any combination of them in principle , which include mem ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) and patch priors ( e.g. * ? ? ?",
    "this advantage is particularly important for sgr a * , which needs to involve a regularization function to mitigate the interstellar scattering effects ( scattring optics ; * ? ? ? * ) in addition to the general regularization function(s ) for imaging .",
    "a relevant disadvantage of this method is its computational cost , since @xmath74-fold cv requires to reconstruct @xmath75 images for each set of regularization parameters .",
    "recently , an accurate approximation of cv , which can be derived from a single imaging on full data set for each parameter set , has been proposed for imaging from full complex visibilities with @xmath0+tv regularizations @xcite .",
    "future development of such heuristic approximations for other types of data and regularization functions could overcome this issue .",
    "in this paper , we adopt four physical models previously proposed for 1.3  mm emission on event - horizon scales .",
    "the first model is a simple , but qualitatively correct , force - free jet model ( hereafter bl09 ) in the magnetically dominated regime presented in @xcite and @xcite .",
    "we adopted a model image presented in @xcite , which is based on the model parameters fitted to the results of 1.3  mm observations with the eht in @xcite and the sed of m87 ( broderick et al . in preparation ) .",
    "the approaching jet is predominant for this model ( see * ? ? ? * for more details ) .",
    "the second and third models are based on results of grmhd simulations presented in @xcite .",
    "we used the representative models dj1 and j2 , which are based on the same grmhd simulation but with different energy and spatial distributions for radio - emitting leptons .",
    "the dominant emission region is the accretion flow in dj1 and the counter jet in j2 illuminating the last photon orbit in j2 .",
    "we adopt model images in @xcite , where the position angle of the large - scale jet for models is adjusted to @xmath76 inferred for m87 ( e.g. * ? ? ?",
    "the last model is based on results of grmhd simulations presented in @xcite , which models m87 core emission as radiation produced by the jet sheath .",
    "we use the image averaged for @xmath77 3  months for our simulation ( hereafter m16 ) .",
    "the image has its dominant contribution from the counter jet illuminating the last photon orbit similar to j2 of @xcite , but the m16 model assumes energy distributions of leptons quite different from j2 .",
    "we rotate the original model image of @xcite to adjust the position angle of the large - scale jet to @xmath76 .",
    "we simulate observations with the eht at 1.3  mm ( 230  ghz ) using the maps ( mit array performance simulator ) package based on the above models .",
    "the simulated observations are performed for the array expected to comprise in spring 2017 .",
    "we assume an array consisting of stations at 6 different sites : a phased array of the submillimeter array ( sma ) antennas and the james clerk maxwell telescope ( jcmt ) on mauna kea in hawaii ; the arizona radio observatory s submillimeter telescope ( aro / smt ) on mt .",
    "graham in arizona ; the large millimeter telescope ( lmt ) on sierra negra , mexico ; a phased array of the atacama large millimeter / submillimeter array ( alma ) in the atacama desert , chile ; the institut de radioastronomie millimtrique ( iram ) 30 m telescope on pico veleta , spain ; and a single dish telescope of the northern extended millimeter array ( noema ) in france .",
    "we adopt the system equivalent flux density ( sefd ) of each station shown in table [ tab : sefd ] based on the proposer s guide of 1-mm vlbi observations in alma cycle 4 .",
    ".stations used in simulated observations [ cols=\"<,^\",options=\"header \" , ]     the simulations assume a bandwidth of 3.5  ghz for stokes @xmath78 , which is half of the standard setting in alma cycle 4 .",
    "we assume a correlation efficiency of 0.7 , including a quantization efficiency of 0.88 for 2-bit sampling and other potential losses such as bandpass effects and pointing errors .",
    "-coverage of the simulated observations with the eht array expected in spring 2017 .",
    "each baseline is split in two colors to show involving stations . ]",
    "we simulate observations as a series of 5-minutes scans with a cadence of 20 minutes over a gst range of 13 - 0 hour , corresponding to the timerange when m87 can be observed by alma or lmt at an elevation greater than 20@xmath79 .",
    "alma and lmt are sensitive stations near the middle of the east - west extent of the array , and they may be important anchor stations for fringe detection .",
    "this provides an observational efficiency of 25% in time , expected for vlbi observations with alma in 2017 .",
    "data are integrated for the duration of each scan ( i.e. 5 min ) following previous eht observations ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "figure  [ fig : uv - coverage ] shows the resultant @xmath80-coverage of simulated observations .",
    "the maximum baseline length of observations is 7.2  g@xmath17 , corresponding to @xmath81  @xmath82as .",
    "we note that the conditions of our simulation are much worse than previous simulations in @xcite in terms of the baseline sensitivity , @xmath80-coverage , angular resolution ( i.e. the maximum baseline length ) and the exposure time of observations . nevertheless , our simulated conditions are much closer to the observational conditions in spring 2017 .",
    "we reconstruct images from simulated data - sets based on the method described in  [ sec:2.1 ] .",
    "we adopt a field of view ( fov ) of 200  @xmath5as gridded by 100  pixels in both the ra and dec directions for all models , giving a pixel size of @xmath771.6  @xmath83as corresponding to a physical scale of @xmath84  @xmath3 .",
    "images are reconstructed at 4 regularization parameters for both @xmath63 and @xmath85 , ranging as @xmath86 . as a result",
    ", we obtain @xmath87  images for each model .    since the problem described in ",
    "[ sec:2.1 ] is non - convex , our algorithm may be trapped in a local minimum and therefore may end up at an initial - condition dependent solution , similar to other algorithms using techniques minimizing non - convex functions with gradient descent methods .",
    "to avoid this , we start reconstructing images at @xmath88 , which is expected to derive the simplest image among parameters we adopt , assuming a point source as initial images .",
    "this is then used as the initial image at other values of the regularization parameters .    at each parameter",
    ", we do iterations until achieving convergence or 1000 iterations , and then filter the output image with a hard thresholding defined by @xmath89 where @xmath90 is a threshold .",
    "we repeat this process until the normalized root mean square error ( nrmse ) between the previous and latest filtered images becomes smaller than @xmath91 .",
    "nrmse is defined by ( e.g. * ? ? ?",
    "* ) @xmath92 where @xmath31 and @xmath93 are the image to be evaluated and the reference image , respectively .",
    "we adopt the latest non - filtered image as the final product .",
    "although this procedure makes the computational time longer , we found this works very well for avoiding convergence at some local minima . in this paper , we set 10% of the peak flux as a threshold @xmath90 . for the simulated observational data in this paper",
    ", it takes typically about several to ten minutes on a standard desktop computer with six intel core - i7 cpu cores to reconstruct an image at each set of two regularization parameters .",
    "we evaluate the goodness - of - fit for each image and then selected the best - fit images with 10-fold cv as described in  [ sec:2.3 ] .",
    "the quality of the reconstructed images is evaluated with the nrmse . since the all model images have finer resolutions with narrower fovs than the reconstructed images , we calculated these metrics as follows .",
    "first , we adjusted the pixel size of the reconstructed image to that of the model image with bi - cubic spline interpolation .",
    "second , we adjusted the position offsets between these two images so that the positions of their centers of mass coincide , because absolute positions can not be defined from visibility amplitudes and closure phases alone . finally , the metrics were evaluated .",
    "in addition to nrmse , we also measure structural dissimilarity @xcite between the model and reconstructed images using the dssim metric adopted work by @xcite and @xcite .",
    "since both metrics show similar trends , we show only the behavior of the nrmse in the figures that follow .      for evaluating the performance of our techniques",
    ", we also reconstructed images with the most widely - used cotton - schwab clean ( henceforth cs - clean ; * ? ? ?",
    "* ) implemented in the common astronomy software applications ( casa ) package with uniform weighting . since clean requires complex visibilities , we adopted the simulated complex visibilities with thermal noises .",
    "we set a gain of 0.1 and a threshold of 0.08  mjy  beam@xmath94 , comparable to the image sensitivity of simulated observations .",
    "since the fast fourier transform is often used in clean , a very small fov can require a grid size in @xmath80-plane that is too large , which could cause additional deconvolution errors .",
    "hence , we set 1024 pixels with the same pixel size in each axis for the entire map , and put a clean box in the central 100@xmath95100 pixels to put clean components in the same region as other techniques .",
    "we use the model image instead of the clean map for calculating metrics , since the residual map , which is generally added to the clean map , can not be calculated for the proposed method .",
    "we show the best - fit images selected with cv in figure  [ fig : best - fit - images ] . a clear shadow feature is well reproduced for the counter - jet- and accretion - flow - dominated models ( j2 , m16 and dj1 ) .",
    "this demonstrates that the eht will achieve effective sufficient spatial resolution to image the black hole shadow of m87 if the mass - loading radius of the jet is not too large @xcite .",
    "figure  [ fig : fidelity - resolution ] shows the nrmse metric for reconstructed images . the black curve labeled `` model '' shows the nrmse calculated when the model image is convolved with a circular gaussian beam with a full width at half maximum ( fwhm ) as shown on the abscissa and compared against the original ( unconvolved ) model image .",
    "the model curve effectively quantifies the best - case scenario in which the differences from the original input are due solely to a loss of resolution , not to errors in reconstructing the image .",
    "figure  [ fig : fidelity - resolution ] also show the nrmse of each of the reconstructed images convolved with circular gaussian beams .",
    "figure  [ fig : fidelity - resolution ] clearly shows that closure - phase imaging with @xmath0+tv regularization works well compared to cs - clean in particular at finer resolutions , despite the fact that cs - clean uses full complex visibilities with more information and higher snrs than closure phases . both cs - clean and @xmath0+tv images achieve similar nrmses on scales comparable to or greater than the diffraction limit . on the other hand ,",
    "the nrmses of the reconstructed images start to deviate from the model images in the super - resolution regime ",
    "namely on scales smaller than the diffraction limit . in this regime ,",
    "the nrmses differ by technique .",
    "cs - clean has a common trend for all four models , which is broadly consistent with results of @xcite .",
    "they achieve minimum errors at a resolution of @xmath96% of the diffraction limit and then show a rapid increase in errors at smaller scales .",
    "in contrast , closure - phase imaging with @xmath0+tv regularizations show much more modest variations in the super - resolution regime .",
    "they achieve minimum errors at a resolution of @xmath1% of the diffraction limit , smaller than cs - clean , and show only a slight increase at smaller scales .",
    "@xmath0+tv reconstructions produce images that have a smooth distribution similar to the model images , resulting in smaller errors than cs - clean , even if the @xmath0+tv reconstructions are not convolved with a restoring beam .     + ( a ) filtered with @xmath97 +   + ( b ) filtered with @xmath98 +   + ( c ) filtered with @xmath99 +   + ( d ) original images ( no filtering ) +    in super - resolution regimes , the errors in @xmath0+tv images mostly arise from the presence of tiny substructures in the image .",
    "for instance , we show model , reconstructed and residual images filtered with different baseline lengths for dj1 in figure  [ fig : residuals ] . as shown in figure  [",
    "fig : residuals ] , residuals are small when filtering baseline lengths are shorter than the maximum baseline length . on the other hand , for longer filtering baseline lengths , systematic residuals due to tiny substructures much smaller than the diffraction limit start to appear , which can be traced only with baselines longer than the simulated observations .",
    "although nrmses are better than cs - clean at finer resolutions , @xmath0+tv images have broader emission region sizes regardless of models .",
    "this is due to a typical feature of images reconstructed with the isotropic tv , which prefers flat images with sharp edges .",
    "since the simulated data do not have visibilities at baseline lengths long enough to resolve the width of ring- or crescent - like features , tv enlarges their widths until images start to deviate from observed visibilities .",
    "this property of the isotropic tv regularization would be useful to constrain the upper - limit size of the emission regions and black - hole shadow ( see  [ sec:5.1 ] for further discussions ) .",
    "on the other hand , in terms of the image fidelity , these results suggest that regularizations preferring much smoother edges are preferable ; smoother gradients in the image lead to images with higher contrast ( i.e. brighter / fainter pixels become even brighter / fainter , respectively ) to conserve the total flux , which often makes the effective emission region size smaller . in  [ sec:5.2 ] , we discuss alternative regularizations of sparse imaging reconstruction for improving the image fidelity in super - resolution regime .           + ( a ) cross validation ( @xmath100 )",
    "+ ( b ) nrmse ( % )    reconstructed images for all 16 sets of the regularization parameters are shown in figure  [ fig : all - images ] for the accretion - flow - dominated model .",
    "as shown in figure  [ fig : all - images ] , the reconstructed images with @xmath101 have noisy artifacts .",
    "this is true for all four models .",
    "such artifacts may appear because the images are poorly constrained by both regularization functions at lower regularization parameters .",
    "cv works as an occam s razor and prevents over - fitting . in figure",
    "[ fig : param - dependence ] ( a ) , we show the residual @xmath24 between the validation set and the model image reconstructed from the training set , which is averaged for all 10 trials ( henceforth the cv value ) . as expected in ",
    "[ sec:2.2 ] , the cv value tends to be large for large regularization parameters , since the regularization functions too strongly constrain the image so that the model image is inconsistent with observational data .",
    "once the cv value achieves its minimum value ( at a parameter set marked with stars in figure  [ fig : param - dependence ] ( a ) ) , it starts to increase again for lower regularization parameters , since the model image is over - fitted to the training set and then shows larger deviations from the validation set .",
    "since the groundtruth images are known in this work , one can compare cv - selected images with images on other parameter sets , which can not be done for real observational data with unknown ground truth images .",
    "although cv selects the optimal parameter based on the noise level of the data , it does not guarantee that the selected parameter achieves the best imaging fidelity among all parameter sets examined . in fig",
    "[ fig : param - dependence ] ( b ) , we show the nrmse of non - gaussian - convolved images for all parameter sets .",
    "although metrics for the image fidelity of best - fit images are slightly larger than the minimum values for the three of four models ( j2 , m16 and dj1 ) , they are consistent to within a few percents .",
    "these slight differences between the best - fit and best - fidelity parameters would not produce substantial differences in the image , and the resulting images are good enough .",
    "the above results clearly demonstrate that cv is a useful technique to determine the regularization parameters so that the reconstructed image does not overfit noises in the data .",
    "we emphasize that cv is a general technique and can be applied to imaging regardless of the specific data products used ( e.g. full complex visibilities , visibility amplitudes and/or closure quantities ) or chosen regularization ( for instance , sparse modeling ; mem :  e.g. ,  @xcite ,  @xcite ; patch priors :  e.g. ,  @xcite ; scattering optics :  @xcite ) .",
    "the proposed method successfully reproduces a clear signature for the black hole shadow for the models j2 , m16 and dj1 .",
    "this is as expected from the visibility distribution of these models , which have null amplitude regions , created by the shadow feature , at intermediate baseline lengths of @xmath102  g@xmath17 @xcite .",
    "mem also succeeds in reproducing the black hole shadow in these models @xcite .",
    "presence of a clear shadow feature is tightly connected to where the dominant emission originates , since the silhouette of the black hole is created by the photons produced a few @xmath3 from the black hole , illuminating the last photon orbit ( see discussion in * ? ? ?",
    "as demonstrated in previous imaging simulations @xcite , future eht observations can constrain the loading radius of the high - energy leptons producing synchrotron emission at 1.3  mm via the appearance of the black hole .",
    "the most important implication of this work is that our regularization function tv and parameter selection with cv will enlarge the ring- or crescent - like emission illuminating the last - photon orbit of the black hole as much as possible , within the range that the model image neither over - fits nor deviates too much from the observed visibilities .",
    "hence , the obtained width of the surrounding emission in the reconstructed image is close to an upper - limit on the width of the emission region , and simultaneously , the obtained diameter of the black hole shadow should be interpreted as a reasonable lower limit for it .",
    "the clear shadow features in the reconstructed images for models j2 , m16 and dj1 , therefore , strongly indicate that the eht can sample a large enough range of visibilities with appropriately low noise levels to image the black hole shadow .",
    "in addition , the raw reconstructed image with the proposed method can be used to constrain the lower - limit size of the shadow .",
    "this would be useful to constrain the mass of m87 , which has an uncertainty of an factor of about two between the stellar - dynamical ( e.g. * ? ? ? * ) and gas - dynamical ( e.g. * ? ? ?",
    "* ) modeling , and therefore be informative to clarify that which of modeling methods is desirable to measure the mass of super - massive black holes .",
    "another important implication is that , at least for the black hole images , post - processing gaussian convolution would not be required with the @xmath103 regularization , although the clean techniques do require it to reduce many compact artifacts in the image .",
    "as shown in figure [ fig : fidelity - resolution ] , the nrmse curves for the @xmath103 regularization are shallow for smaller convolving sizes , and applying a circular gaussian beam therefore makes only small improvements of a few percent in the nrmse regardless of the input model images .",
    "similar results are also shown in recent work with the mem @xcite .",
    "our results support that application of the beam is not required for the recent state - of - art imaging methods utilizing multi - resolution regularization functions for imaging the @xmath3-scale structure of m87 and sgr a * with the eht .        in this paper ,",
    "we adopt tv regularization , which favors a smooth image , so that images can be reconstructed with smaller pixel sizes and/or for more extended sources . as shown in @xmath104[sec:4 ] , the reconstructed images have good image fidelity .",
    "in particular , it is noteworthy that cv selected high values of @xmath105 for all models ( see figure  [ fig : param - dependence ] ( a ) ) , suggesting that the solutions would be over - fitted without tv regularization .",
    "these results demonstrate that inclusion of the tv regularization can extend the range of objects where sparse modeling is applicable .",
    "however , as described in  [ sec:4.1 ] , the reconstructed images have larger emission regions than the original model , since the isotropic tv @xcite adopted in this work prefers a flat brightness distribution in super - resolution regimes where the image can not be constrained well . hence",
    ", a regularization preferring more smoothed edges is required to improve the fidelity for the black - hole imaging with the eht .    in the context of sparse reconstruction ,",
    "there are several candidates for improving the image fidelity as a natural extension of this work .",
    "first of all , there are other forms of regularization functions , which prefer sparse images in the gradient domain with smoother edges . for instance , an alternative form , given by , latexmath:[\\[\\begin{aligned }    is also convex like the isotropic tv term adopted in this work , and prefers images with smoother edges .",
    "furthermore , previous studies of sparse image reconstruction techniques have shown that regularization with @xmath0+wavelet / curvelet transformation is also a promising approach ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "we will test these sparse regularizations in a forthcoming paper .      in vlbi",
    ", the visibility phase is initially calibrated with fringe fitting ( also called as fringe search ) , which is a self - calibration technique using phase closure ( see * ? ? ?",
    "the fringe fitting can mitigate most station - based errors due to atmospheric and instrumental effects , although errors may remain if an incorrect source model is assumed .",
    "traditionally , self - calibration with hybrid / differential mapping ( e.g. * ? ? ?",
    "* ) has been employed to solve for residual structural phase errors and images simultaneously , which has been successful for vlbi imaging .",
    "this work and previous works on closure - phase imaging techniques using other regularizations such as mems @xcite and patch priors ( chirp ; * ? ? ?",
    "* ) demonstrate that an image can be reconstructed with high fidelity even from closure quantities .",
    "however , since closure phases and other closure quantities have less information about the source structure and also larger thermal noises than full complex visibilities , imaging with closure quantities can limit the dynamic range , image sensitivity and optimal spatial resolution .",
    "a promising approach to improve the dynamic range is to use a reconstructed image from closure imaging techniques as an initial image for hybrid / differential mapping .",
    "it can also be used as a model for fringe fitting , as self - calibrated images are often applied to detect more fringes on faint sources ( e.g. * ? ? ?",
    "in a forthcoming paper , we will evaluate the performance of such a hybrid - mapping technique including closure - phase / full - closure imaging priors .",
    "we emphasize that , for this purpose , one does not need to reconstruct the image with pixels much smaller than scales where the brightness distribution can not be constrained by data and therefore will not affect results of self - calibration .",
    "we have presented a new imaging technique reconstructing images from visibility amplitudes and closure phases by utilizing two regularizations of sparse modeling : the @xmath0-norm and total variation ( tv ) .",
    "furthermore , we also propose a method to select optimal regularization parameters with cross validation ( cv ) , which can be applied to most existing imaging algorithms . as an example",
    ", we applied our technique to simulated observations of m87 with the eht at 1.3  mm . here , we summarize our conclusions .    1 .",
    "we find that @xmath0+tv regularization can achieve an optimal resolution of @xmath1% of the diffraction limit @xmath2 , which is the nominal spatial resolution of a radio interferometer .",
    "this optimal resolution is better than that of the most - widely used cotton - schwab clean , which uses full complex visibilities .",
    "we confirm that cross validation ( cv ) works as an occam s razor and prevents over - fitting when selecting the optimal regularization parameters .",
    "cv is a general method that can be applied to interferometric imaging more generally , such as imaging with full - complex visibilities and/or using other regularizations .",
    "3 .   using @xmath107tv regularization , the reconstructed image maximizes the width of the emission region within the range that it neither over - fits nor deviates too strongly from the data .",
    "hence , the clear reproduction of the black hole shadow in the reconstructed image suggests that future eht observations will have the @xmath80-coverage and sensitivity sufficient for imaging it .",
    "in addition , the reconstructed image will be able to constrain the sizes of the black hole shadow and surrounding emission region .",
    "finally , we remark that all of above results demonstrate the clear promise of the eht for providing an unprecedented view of the event - horizon - scale structure of the super - massive black hole in m87 and also the galactic center sgr a*.    we thank the anonymous referee for his / her useful and constructive suggestions to improve the paper . k.a",
    "thanks members dr .",
    "michael  d.  johnson and dr .",
    "lindy  blackburn for many useful suggestions on this work .",
    "k.a . and this work are financially supported by the program of postdoctoral fellowships for research abroad at the japan society for the promotion of science . m.m .",
    "acknowledges support from the erc synergy grant ( grant 610058 ) .",
    "event horizon telescope work at mit haystack observatory and the harvard smithsonian center for astrophysics is supported by grants from the national science foundation ( nsf ; ast-1440254 , ast-1614868 ) and through an award from the gordon and betty moore foundation ( gmbf-3561 ) .",
    "work on sparse modeling and event horizon telescope at the mizusawa vlbi observatory is financially supported by the mext / jsps kakenhi grant numbers 24540242 , 25120007 and 25120008 ."
  ],
  "abstract_text": [
    "<S> we propose a new imaging technique for radio and optical / infrared interferometry . </S>",
    "<S> the proposed technique reconstructs the image from the visibility amplitude and closure phase , which are standard data products of short - millimeter very long baseline interferometers such as the event horizon telescope ( eht ) and optical / infrared interferometers , by utilizing two regularization functions : the @xmath0-norm and total variation ( tv ) of the brightness distribution . in the proposed method , </S>",
    "<S> optimal regularization parameters , which represent the sparseness and effective spatial resolution of the image , are derived from data themselves using cross validation ( cv ) . as an application of this technique </S>",
    "<S> , we present simulated observations of m87 with the eht based on four physically motivated models . </S>",
    "<S> we confirm that @xmath0+tv regularization can achieve an optimal resolution of @xmath1% of the diffraction limit @xmath2 , which is the nominal spatial resolution of a radio interferometer . </S>",
    "<S> with the proposed technique , the eht can robustly and reasonably achieve super - resolution sufficient to clearly resolve the black hole shadow . </S>",
    "<S> these results make it promising for the eht to provide an unprecedented view of the event - horizon - scale structure in the vicinity of the super - massive black hole in m87 and also the galactic center sgr a*. </S>"
  ]
}