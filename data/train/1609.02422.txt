{
  "article_text": [
    "there is now a widespread view that information is fundamentally about differences , distinguishability , and distinctions . as charles h. bennett ,",
    "one of the founders of quantum information theory , put it :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ so information really is a very useful abstraction .",
    "it is the notion of distinguishability abstracted away from what we are distinguishing , or from the carrier of information .",
    "@xcite _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    this view even has an interesting history . in james",
    "gleick s book , _ the information : a history , a theory , a flood _ , he noted the focus on differences in the seventeenth century polymath , john wilkins , who was a founder of the royal society . in 1641 ,",
    "the year before newton was born , wilkins published one of the earliest books on cryptography , _ mercury or the secret and swift messenger _ , which not only pointed out the fundamental role of differences but noted that any ( finite ) set of different things could be encoded by words in a binary code .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ for in the general we must note , that whatever is capable of a competent difference , perceptible to any sense , may be a sufficient means whereby to express the cogitations .",
    "it is more convenient , indeed , that these differences should be of as great variety as the letters of the alphabet ; but it is sufficient if they be but twofold , because two alone may , with somewhat more labour and time , be well enough contrived to express all the rest .",
    "xvii , p. 69 ) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    wilkins explains that a five letter binary code would be sufficient to code the letters of the alphabet since @xmath0 .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ thus any two letters or numbers , suppose a.b .",
    "being transposed through five places , will yield thirty two differences , and so consequently will superabundantly serve for the four and twenty letters ... .",
    "xvii , p. 69 ) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    as gleick noted :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ any difference meant a binary choice .",
    "any binary choice began the expressing of cogitations .",
    "here , in this arcane and anonymous treatise of 1641 , the essential idea of information theory poked to the surface of human thought , saw its shadow , and disappeared again for [ three ] hundred years . @xcite _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    thus _ counting distinctions _",
    "@xcite would seem the right way to measure information , and that is the measure that emerges naturally out of partition logic  just as finite logical probability emerges naturally as the measure of _ counting elements _ in boole s subset logic .    although usually named after the special case of ` propositional ' logic , the general case is boole s logic of subsets of a universe @xmath1 ( the special case of @xmath2 allows the propositional interpretation since the only subsets are @xmath3 and @xmath4 standing for truth and falsity ) .",
    "category theory shows that is a duality between sub - sets and quotient - sets ( or partitions and equivalence relations ) , and that allowed the recent development of the dual logic of partitions ( @xcite , @xcite ) . as indicated in the title of his book , _ an investigation of the laws of thought",
    "on which are founded the mathematical theories of logic and probabilities _ @xcite , boole also developed the normalized counting measure on subsets of a finite universe @xmath1 which was finite logical probability theory .",
    "when the same mathematical notion of the normalized counting measure is applied to the partitions on a finite universe set @xmath1 ( when the partition is represented as the complement of the corresponding equivalence relation on @xmath5 ) then the result is the formula for logical entropy .",
    "in addition to the philosophy of information literature @xcite , there is a whole sub - industry in mathematics concerned with different notions of ` entropy ' or ` information ' ( @xcite ; see @xcite for a recent ` extensive ' analysis ) that is long on formulas and ` axioms ' but short on interpretations . out of that plethora of definitions ,",
    "logical entropy is the measure of information that arises out of partition logic just as logical probability theory arises out of subset logic .",
    "logical entropy is to the logic of partitions as logical probability is to the boolean logic of subsets .",
    "hence we will start with a brief review of the relationship between these two dual forms of mathematical logic .",
    "modern category theory shows that the concept of a subset dualizes to the concept of a quotient set , equivalence relation , or partition .",
    "f. william lawvere called a subset or , in general , a subobject a `` part '' and then noted : the dual notion ( obtained by reversing the arrows ) of ` part ' is the notion of _",
    "partition_.  @xcite that suggests that the boolean logic of subsets ( usually named after the special case of propositions as ` propositional ' logic ) should have a dual logic of partitions ( @xcite , @xcite ) .",
    "a _ partition _ @xmath6 on @xmath1 is a set of subsets , called cells or blocks , @xmath7 that are mutually disjoint and jointly exhaustive ( @xmath8 ) . in the duality between subset logic and partition logic ,",
    "the dual to the notion of an ` element ' of a subset is the notion of a ` distinction ' of a partition , where @xmath9 is a _ distinction _ or _ dit _ of @xmath10 if the two elements are in different blocks , i.e. , the ` dits ' of a partition are dual to the ` its ' ( or elements ) of a subset .",
    "let @xmath11 be the set of distinctions or _ ditset _ of @xmath10 .",
    "similarly an _",
    "indistinction _ or _ indit _ of @xmath10 is a pair @xmath12 in the same block of @xmath10 .",
    "let @xmath13 be the set of indistinctions or _ inditset _ of @xmath10 .",
    "then @xmath14 is the equivalence relation associated with @xmath10 and @xmath15 is the complementary binary relation that might be called a _ partition relation _ or an _ apartness relation_.",
    "the algebra associated with the subsets @xmath16 is , of course , the boolean algebra @xmath17 of subsets of @xmath1 with the partial order as the inclusion of elements . the corresponding algebra of partitions @xmath10 on @xmath1 is the _ partition algebra _",
    "@xmath18 defined as follows :    * the _ partial order _ @xmath19 of partitions @xmath20 and @xmath21 holds when @xmath10 _ refines _",
    "@xmath22 in the sense that for every block @xmath23 there is a block @xmath24 such that @xmath25 , or , equivalently , using the element - distinction pairing , the partial order is the inclusion of distinctions : @xmath19 if and only if ( iff ) @xmath26 ; * the minimum or bottom partition is the _ indiscrete partition _ ( or blob ) @xmath27 with one block consisting of all of @xmath1 ; * the maximum or top partition is the _ discrete partition _",
    "@xmath28 consisting of singleton blocks ; * the _ join _ @xmath29 is the partition whose blocks are the non - empty intersections @xmath30 of blocks of @xmath10 and blocks of @xmath22 , or , equivalently , using the element - distinction pairing , @xmath31 ; * the _ meet _ @xmath32 is the partition whose blocks are the equivalence classes for the equivalence relation generated by : @xmath33 if @xmath34 , @xmath35 , and @xmath36 ; and * @xmath37 is the _ implication partition _ whose blocks are : ( 1 ) the singletons @xmath38 for @xmath39 if there is a @xmath24 such that @xmath40 , or ( 2 ) just @xmath23 if there is no @xmath24 with @xmath40 , so that trivially : @xmath41 iff @xmath19 .",
    "parallel to the familiar powerset boolean algebra @xmath17 . ]",
    "the logical partition operations can also be defined in terms of the corresponding logical operations on subsets .",
    "a ditset @xmath42 of a partition on @xmath1 is a subset of @xmath5 of a particular kind , namely the complement of an equivalence relation .",
    "an _ equivalence relation _ is reflexive , symmetric , and transitive .",
    "hence the complement , i.e. , a partition relation ( or apartness relation ) , is a subset @xmath43 that is :    1 .",
    "irreflexive ( or anti - reflexive ) , @xmath44 ( where @xmath45 is the _ diagonal _ ) ; 2 .",
    "symmetric , @xmath46 implies @xmath47 ; and 3 .",
    "anti - transitive ( or co - transitive ) , if @xmath48 then for any @xmath49 , @xmath50 or @xmath51 .    given any subset @xmath52 ,",
    "the _ reflexive - symmetric - transitive ( rst ) closure _ @xmath53 of the complement @xmath54 is the smallest equivalence relation containing @xmath54 , so its complement is the largest partition relation contained in @xmath55 , which is called the _",
    "@xmath56 of @xmath55 .",
    "this usage is consistent with calling the subsets that equal their rst - closures _ closed subsets _ of @xmath5 ( so closed subsets = equivalence relations )",
    "so the complements are the _ open subsets _ (= partition relations ) .",
    "however it should be noted that the rst - closure is not a topological closure since the closure of a union is not necessarily the union of the closures , so the ` open ' subsets do not form a topology on @xmath5 . it will be shown below that any two nonempty open sets have a nonempty intersection .    the interior operation @xmath57 provides a universal way to define operations on partitions from the corresponding subset operations :    apply the subset operation to the ditsets and",
    "then , if necessary ,    take the interior to obtain the ditset of the partition operation .",
    "since the same operations can be defined for subsets and partitions , one can interpret a formula @xmath58 either way as a subset or a partition .",
    "given either subsets on or partitions of @xmath1 substituted for the variables @xmath10 , @xmath22 , ... , one can apply , respectively , subset or partition operations to evaluate the whole formula . since @xmath58 is either a subset or a partition ,",
    "the corresponding proposition is `` @xmath59 is an element of @xmath60 '' or `` @xmath61 is a distinction of @xmath58 '' .",
    "and then the definitions of a valid formula are also parallel , namely , no matter what is substituted for the variables , the whole formula evaluates to the top of the algebra . in that case , the subset @xmath58 contains all elements of @xmath1 , i.e. , @xmath62 , or the partition @xmath58 distinguishes all pairs @xmath63 for distinct elements of @xmath1 , i.e. , @xmath64 .",
    "the parallelism between the dual logics is summarized in the following table 1 .",
    "[ c]|c||c|c|table 1 & subset logic & partition logic + ` elements ' ( its or dits ) & elements @xmath59 of @xmath55 & dits @xmath65 of @xmath10 + inclusion of ` elements ' & inclusion @xmath66 & refinement : @xmath26 + top of order = all ` elements ' & @xmath1 all elements & @xmath67 , all dits + bottom of order = no ` elements ' & @xmath4 no elements & @xmath68 , no dits + variables in formulas & subsets @xmath55 of @xmath1 & partitions @xmath10 on @xmath1 + operations : @xmath69 & subset ops . &",
    "partition ops .",
    "+ formula @xmath70 holds & @xmath59 element of @xmath71 & @xmath63 dit of @xmath72 + valid formula & @xmath73 , @xmath74 & @xmath75 , @xmath76 +    table 1 : duality between subset logic and partition logic",
    "george boole @xcite extended his logic of subsets to classical finite probability theory where , in the equiprobable case , the probability of a subset @xmath55 ( event ) of a finite universe set ( outcome set or sample space ) @xmath77 was the number of elements in @xmath55 over the total number of elements : @xmath78 .",
    "laplace s classical finite probability theory @xcite also dealt with the case where the outcomes were assigned real point probabilities @xmath79 ( where @xmath80 and @xmath81 ) so rather than summing the equal probabilities @xmath82 , the point probabilities of the elements were summed : @xmath83where the equiprobable formula is for @xmath84 for @xmath85 .",
    "the conditional probability of an event @xmath86 given an event @xmath55 is @xmath87 .",
    "given a real - valued random variable @xmath88 on the outcome set @xmath1 , the possible values of @xmath89 are @xmath90 and the probability of getting a certain value given @xmath55 is : @xmath91 .    then we may mimic boole s move going from the logic of subsets to the finite logical probabilities of subsets by starting with the logic of partitions and using the dual relation between elements and distinctions .",
    "the dual notion to probability turns out to be ` information content ' or ` entropy ' so we define the _ logical entropy _ of @xmath10 , denoted @xmath92 , as the size of the ditset @xmath93 normalized by the size of @xmath5 :    @xmath94    logical entropy of @xmath10 ( equiprobable case ) .",
    "the inditset of @xmath10 is @xmath95 so where @xmath96 in the equiprobable case , we have :    @xmath97 .",
    "this definition corresponds to the equiprobable case @xmath98 of the normalized number of elements rather than normalized number of distinctions .    the corresponding definition for the case of point probabilities @xmath99 is to just add up the probabilities of getting a particular distinction :    @xmath100    logical entropy of @xmath10 with point probabilities @xmath101 .",
    "this suggest that in the case of point probabilities , we should take @xmath102 and have @xmath103 .",
    "this is confirmed with a little calculation using that definition of @xmath104 :    @xmath105   \\left [ p\\left (   b_{1}\\right )   + ... + p\\left (   b_{m}\\right )   \\right ]   = \\sum_{i=1}^{m}p\\left (   b_{i}\\right )   ^{2}+\\sum_{i\\neq i^{\\prime}}p\\left (   b_{i}\\right ) p\\left (   b_{i^{\\prime}}\\right )   $ ]    so that :    @xmath106 . of distinct indices satisfies",
    "@xmath107 both ways so @xmath108 . ]",
    "moreover , we have :    @xmath109    so that :    @xmath110    since :    @xmath111    @xmath112 .    thus the logical entropy with point probabilities is ( using the point probability definition of @xmath104 ) :    @xmath113 .",
    "one other version of the classical logical entropy might be mentioned . instead of being given a partition @xmath6 on @xmath1 with point probabilities @xmath114 defining the finite probability distribution of block probabilities @xmath115",
    ", one might be given only a finite probability distribution @xmath116 .",
    "the substituting @xmath117 for @xmath118 gives the :    @xmath119    logical entropy of a finite probability distribution .",
    "since @xmath120 , we again have the logical entropy @xmath121 as the probability @xmath122 of drawing a distinction in two independent samplings of the probability distribution @xmath101 .",
    "this is also clear from defining the product measure on the subsets @xmath52 :    @xmath123    _ product measure _ on @xmath5    then the logical entropy @xmath124 is just the product measure of the@xmath125ditset of the discrete partition @xmath126 on @xmath1 .",
    "there is also the obvious generalization to consider any partition @xmath10 on @xmath1 and where for each block @xmath23 , @xmath127 .",
    "then the logical entropy @xmath128 is the product measure of the ditset of @xmath10 ( so it is still interpreted as the probability of drawing a distinction of @xmath10 ) and that is equivalent to @xmath129 .",
    "there are also parallel element @xmath130 distinction interpretations :    * @xmath131 is the probability that a single draw , sample , or experiment with @xmath1 gives a element @xmath132of @xmath55 , and * @xmath133 is the probability that two independent ( with replacement ) draws , samples , or experiments with @xmath1 gives a distinction @xmath134 of @xmath10 , or if we interpret the independent experiments as sampling from the set of blocks @xmath135 , then it is the probability of getting distinct blocks .    in probability theory ,",
    "when a random draw gives an outcome @xmath136 in the subset or event @xmath55 , we say the event @xmath55 _ occurs _ , and in logical information theory , when the random draw of a pair @xmath137 gives a distinction of @xmath10 , we say the partition @xmath10 _",
    "distinguishes_.    the parallelism or duality between logical probabilities and logical entropies based on the parallel roles of _ its _ ( elements of subsets ) and _ dits _ ( distinctions of partitions ) is summarized in table 2 .",
    "[ c]|c||c|c|table 2 & logical probability theory & logical information theory + ` outcomes ' & elements @xmath138 finite & dits @xmath139 finite + ` events ' & subsets @xmath16 & ditsets @xmath11 + equiprobable points & @xmath140 & @xmath141 + point probabilities & @xmath142 & @xmath143 + interpretation & @xmath144 @xmath3-draw prob . of @xmath55-element & @xmath145 @xmath146-draw prob . of @xmath10-distinction +",
    "table 2 : classical logical probability theory and classical logical information theory    this concludes the argument that logical information theory arises out of partition logic just as logical probability theory arises out of subset logic .",
    "now we turn to the formulas of logical information theory and the comparison to the formulas of shannon information theory .",
    "the formula for logical entropy is not new . given a finite probability distribution @xmath147 , the formula @xmath148 was used by gini in 1912 ( @xcite reprinted in @xcite ) as a measure of mutability  or diversity .",
    "what is new here is not the formula , but the derivation from ( partition ) logic .    as befits the logical origin of the formula",
    ", it occurs in a variety of fields .",
    "the formula in the complementary form , @xmath149 , was developed early in the 20@xmath150 century in cryptography .",
    "the american cryptologist , william f. friedman , devoted a 1922 book ( @xcite ) to the index of coincidence ( i.e. , @xmath151 ) .",
    "solomon kullback worked as an assistant to friedman and wrote a book on cryptology which used the index .",
    "@xcite during world war ii , alan m. turing worked for a time in the government code and cypher school at the bletchley park facility in england .",
    "probably unaware of the earlier work , turing used @xmath152 in his cryptoanalysis work and called it the _ repeat rate _ since it is the probability of a repeat in a pair of independent draws from a population with those probabilities .",
    "after the war , edward h. simpson , a british statistician , proposed @xmath153 as a measure of species concentration ( the opposite of diversity ) where @xmath154 is the partition of animals or plants according to species and where each animal or plant is considered as equiprobable so @xmath155 .",
    "and simpson gave the interpretation of this homogeneity measure as `` the probability that two individuals chosen at random and independently from the population will be found to belong to the same group.''@xcite hence @xmath156 is the probability that a random ordered pair will belong to different species , i.e. , will be distinguished by the species partition .",
    "in the biodiversity literature @xcite , the formula @xmath156 is known as _",
    "simpson s index of diversity _  or sometimes , the _ gini - simpson index _ @xcite .",
    "however , simpson along with i. j. good worked at bletchley park during wwii , and , according to good , `` e. h. simpson and i both obtained the notion [ the repeat rate ] from turing . ''",
    "@xcite when simpson published the index in 1948 , he ( again , according to good ) did not acknowledge turing `` fearing that to acknowledge him would be regarded as a breach of security . ''",
    "@xcite since for many purposes logical entropy offers an alternative to shannon entropy ( @xcite , @xcite ) in classical information theory , and the quantum version of logical entropy offers an alternative to von neumann entropy @xcite in quantum information theory , it might be useful to call it ` turing entropy ' to have a competitive ` famous name ' label .",
    "but even before the logical derivation of the formula , i. j. good pointed out a certain naturalness :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ if @xmath157 are the probabilities of @xmath158 mutually exclusive and exhaustive events , any statistician of this century who wanted a measure of homogeneity would have take about two seconds to suggest @xmath151 which i shall call @xmath159 .",
    "@xcite _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in view of the frequent and independent discovery and rediscovery of the formula @xmath152 or its complement @xmath160 by gini , friedman , turing , and many others [ e.g. , the hirschman - herfindahl index of industrial concentration in economics ( @xcite , @xcite ) ] , i. j. good wisely advises that `` it is unjust to associate @xmath159 with any one person . ''",
    "for a partition @xmath6 with block probabilities @xmath104 ( obtained using equiprobable points or with point probabilities ) , the _",
    "shannon entropy of the partition _",
    "( using natural logs ) is :    @xmath161 .    or",
    "if given a finite probability distribution @xmath116 , the _ shannon entropy of the probability distribution _ is :    @xmath162 .",
    "shannon entropy is often taken as a ` measure ' of information .",
    "the formulas for mutual information , joint entropy , and conditional entropy are defined so these shannon entropies satisfy venn diagram formulas ( abramson : it ; @xcite ) that would follow automatically if shannon entropy was a measure in the technical sense . as lorne campbell put it :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ certain analogies between entropy and measure have been noted by various authors .",
    "these analogies provide a convenient mnemonic for the various relations between entropy , conditional entropy , joint entropy , and mutual information .",
    "it is interesting to speculate whether these analogies have a deeper foundation .",
    "it would seem to be quite significant if entropy did admit an interpretation as the measure of some set .",
    "@xcite _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    for any finite set @xmath163 , a ( finite ) _ measure _",
    "@xmath164 is a function @xmath165 such that :    1 .",
    "@xmath166 , 2 .   for any @xmath167 , @xmath168 , and 3 .   for any disjoint subsets @xmath169 and @xmath170 , @xmath171 .",
    "considerable effort has been expended to try to find a framework in which shannon entropy would be a measure in this technical sense and thus would satisfy the desiderata :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ that @xmath172 and @xmath173 are measures of sets , that @xmath174 is the measure of their union , that @xmath175 is the measure of their intersection , and that @xmath176 is the measure of their difference .",
    "the possibility that @xmath177 is the entropy of the `` intersection '' of two partitions is particularly interesting .",
    "this `` intersection , '' if it existed , would presumably contain the information common to the partitions @xmath178 and @xmath179.@xcite _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    but these efforts have not been successful beyond special cases such as @xmath180 equiprobable elements where , as campbell notes , the shannon entropy is just the counting measure @xmath181 of the minimum binary partitions it takes to distinguish all the elements .",
    "in contrast , it is `` quite significant '' that logical entropy is defined as a measure , the normalized counting measure on the ditset @xmath182 representation of a partition @xmath10 as a subset of the set @xmath5 .",
    "thus all of campbell s desiderata are true when :    * `` sets '' = ditsets , the set of distinctions of partitions , and * `` entropies '' = normalized counting measure of the ditsets , i.e. , the logical entropies .",
    "the logical entropy formulas for various compound notions ( e.g. , conditional entropy , mutual information , and joint entropy ) stand in certain venn diagram relationships because logical entropy is a measure .",
    "the shannon entropy formulas for these compound notions also satisfy the venn diagram relationships even though shannon entropy is not a measure .",
    "how can that be ? perhaps there is some connection to explain why the shannon formulas still satisfy those venn diagram relationships ?",
    "indeed , there is such a connection , the dit - bit connection .",
    "the basic datum is the set @xmath183 of @xmath181 elements with the equal probabilities @xmath184 . in that basic case of an equiprobable set",
    ", we can derive the dit - bit connection , and then by using a probabilistic average , we can develop the shannon entropy , expressed in terms of bits , from the logical entropy , expressed in terms of ( normalized ) dits , or vice - versa .",
    "given @xmath183 with @xmath181 equiprobable elements , the number of dits ( of the discrete partition on @xmath183 ) is @xmath185 so the normalized dit count is :    @xmath186 normalized dits .",
    "that is the dit - count or logical measure of the information in a set of @xmath181 distinct elements ( think of it as the logical entropy of the discrete partition on @xmath183 with equiprobable elements ) .",
    "but we can also measure the information in the set by the number of binary partitions it takes ( on average ) to distinguish the elements , and that bit - count is @xcite :    @xmath187 bits .    _",
    "shannon - hartley entropy for an equiprobable set _ @xmath1 of @xmath181 elements    the _ dit - bit connection _ is that the terms @xmath188 will play the same role in the shannon formulas that @xmath189 plays in the logical entropy formulas  when both are formulated as probabilitistic averages .",
    "the common thing being measured is an equiprobable @xmath183 where @xmath190 .",
    "the dit - count for @xmath183 is @xmath189 and the bit - count for @xmath1 is @xmath191 , and the dit - bit transform converts one count into the other . )",
    "is different from using @xmath192 as an approximation since it is the linear term in the newton - mercator series for @xmath193 [ see later section on ` linear entropy ' ] . ]",
    "using this dit - bit transform between the two different ways to quantify the ` information ' in @xmath183 , each entropy can be developed from the other .",
    "nevertheless , this dit - bit connection should not be interpreted as if there is one thing ` information ' that can be measured on different scales  like measuring a length using inches or centimeters .",
    "this is demonstrated below by the analysis of mutual information ; there is no bit - count mutual information between independent partitions but there is always dit - count information even between two ( non - trivial ) independent partitions .",
    "we start with the logical entropy of a probability distribution @xmath194 : @xmath195 .",
    "it is expressed as the probabilistic average of the dit - counts or logical entropies of the sets @xmath196 with @xmath197 equiprobable elements .",
    "but if we switch to the binary - partition bit - counts of the information content of those same sets @xmath196 of @xmath197 equiprobable elements , then the bit - counts are @xmath198 and the probabilistic average is the shannon entropy : @xmath199 .",
    "both entropies have the mathematical form :    @xmath200    and differ by using either the dit - count or bit - count conception of information in @xmath196 .",
    "naturally , formulas have to be put into the form of the probabilistic average of the appropriate count in order to transform the formula to the other count .",
    "moreover the dit - bit connection carries over to the compound notions of entropy so that the shannon notions of conditional entropy , mutual information , cross - entropy , and divergence can be developed from the corresponding notions for logical entropy . since the logical notions are the values of a probability measure , the compound notions of logical entropy have the usual venn diagram relationships .",
    "and then by the dit - bit transform , those venn diagram relationships carry over to the compound shannon formulas . _ that",
    "_ is why the shannon formulas satisfy the venn diagram relationships even though shannon entropy is not a measure .",
    "note that while the logical entropy formula @xmath201 ( and the corresponding compound formulas ) are put into that form of an average to apply the dit - bit transform , logical entropy is the exact measure of the subset @xmath202 for the product probability measure @xmath203   $ ] where for @xmath204 , @xmath205 .",
    "given two partitions @xmath206 and @xmath207 on a finite set @xmath1 , how might one measure the new information that is provided by @xmath10 that was not already in @xmath22 ?",
    "campbell suggests associating sets with partitions so the conditional entropy would be the measure of the difference between the sets .",
    "taking the information as distinctions , we take the difference between the@xmath125ditsets , i.e. , @xmath208 , and then take the normalized counting measure of that subset of @xmath209 :    @xmath210    _ logical conditional entropy of _ @xmath10 _ _  given _",
    "_ @xmath22 .    when the two partitions @xmath10 and @xmath22 are joined together in the join @xmath29 , whose blocks are the non - empty intersections @xmath30 , their information as distinctions is also joined together as sets , @xmath211 ( the `` union '' mentioned by campbell ) , which has the normalized counting measure of :    @xmath212   $ ]    _ logical entropy of a partition join _ @xmath29 .",
    "this logical entropy is interpreted as the probability that a pair of random draws from @xmath1 will yield a @xmath10-distinction _ or _ a @xmath213-distinction ( where `` or '' includes both )",
    ".    then the relationships between the logical entropy concepts can be read off from the field of subsets on which the normalized counting measure of distinctions is defined :    @xmath214    so that    @xmath215 .",
    "figure1.eps    figure 1 : venn diagram for subsets of @xmath5    the shaded area in the venn diagram of figure 1 has the dit - count measure :    @xmath216    so normalizing by @xmath217 yields :    @xmath218 .",
    "for the corresponding definitions for random variables and their probability distributions , consider a random variable @xmath219 taking values in the product @xmath220 of finite sets with the joint probability distribution @xmath221 , and thus with the marginal distributions @xmath222 and @xmath223 where @xmath224 and @xmath225 .",
    "for notational simplicity , the entropies can be considered as functions of the random variables or of their probability distributions , e.g. , @xmath226 , @xmath227 , and @xmath228 . for the joint distribution",
    ", we have the :    @xmath229   $ ]    _ logical entropy of the joint distribution _    which is the probability that two samplings of the joint distribution will yield a pair of _ distinct _ ordered pairs @xmath230 , @xmath231 , i.e. , with an @xmath163-distinction @xmath232 _ or _ a @xmath233-distinction @xmath234 .    for the definition of the conditional entropy @xmath235",
    ", we simply take the product measure of the set of pairs @xmath219 and @xmath236 that give an @xmath163-distinction but not a @xmath233-distinction .",
    "thus given the first draw @xmath219 , we can again use a venn diagram to compute the probability that the second draw @xmath236 will have @xmath237 but @xmath238 .    to illustrate this using venn diagram reasoning , consider the probability measure defined by @xmath239   $ ] on the subsets of @xmath220 .",
    "given the first draw @xmath219 , the probability of getting an @xmath219-distinction on the second draw is @xmath240 and the probability of getting a @xmath241-distinction is @xmath242 . a draw that is a @xmath241-distinction is , a fortiori , an @xmath219-distinction so the area @xmath242 is contained in the area @xmath240 .",
    "then the probability of getting an @xmath219-distinction that is not a @xmath241-distinction on the second draw is the difference : @xmath243as pictured in figure 2 .",
    "figure2.eps    figure 2 : @xmath244    = probability of an @xmath245-distinction but not a @xmath241-distinction on @xmath220 .    since the first draw @xmath219 was with probability @xmath246",
    ", we have the following as the product probability measure of the subset of @xmath247 of pairs @xmath248 that are @xmath163-distinctions but not @xmath233-distinctions :    @xmath249   $ ]    _ logical conditional entropy of _ @xmath245 _ _  given _",
    "_ @xmath241 .",
    "then a little algebra quickly yields :    @xmath249   $ ]    @xmath250   -\\left [   1-\\sum _ { y}p\\left (   y\\right )   ^{2}\\right ]   = h\\left (   x , y\\right )   -h\\left (   y\\right ) $ ] .",
    "the summation over @xmath246 recasts the venn diagram to the set @xmath247 where the product probability measure ( for the two independent draws ) gives the logical entropies as in figure 3 .",
    "figure3.eps    figure 3 : @xmath251 .",
    "it might be noted that the logical conditional entropy , like the other logical entropies , is not just an average ( unlike the corresponding shannon formula ) ; the conditional entropy is the product probability measure of the subset :    @xmath252 .",
    "the shannon conditional entropy for partitions @xmath10 and @xmath22 is based on subset reasoning which is then averaged over a partition .",
    "given a subset @xmath253 @xmath254 , a partition @xmath255 induces a partition of @xmath253 with the blocks @xmath256 . then @xmath257 is the probability distribution associated with that partition so it has a shannon entropy which we denote : @xmath258 .",
    "the shannon conditional entropy is then obtained by _ averaging _ over the blocks of @xmath22 :    @xmath259    _ shannon conditional entropy of _ @xmath10 _ _  given _ _",
    "@xmath22 .    since the join @xmath29 is the partition whose blocks are the non - empty intersections @xmath30 ,    @xmath260 .",
    "developing the formula gives :    @xmath261   = h\\left (   \\pi \\vee\\sigma\\right )   -h\\left (   \\sigma\\right )   $ ] .",
    "thus the conditional entropy @xmath262 is interpreted as the shannon - information contained in the join @xmath29 that is not contained in @xmath22 ( see figure 4 ) .",
    "figure4.eps    figure 4 : @xmath263    venn diagram picture for shannon conditional entropy of partitions    given the joint distribution @xmath221 on @xmath220 , the conditional probability distribution for a specific @xmath264 is @xmath265 which has the shannon entropy : @xmath266 .",
    "then the conditional entropy is the _ average _ of these entropies :    @xmath267    _ shannon conditional entropy of _ @xmath245 _ _  given _ _",
    "@xmath241 .    expanding",
    "as before gives @xmath268 with a similar venn diagram picture ( see figure 5 ) .",
    "figure5.eps    figure 5 : @xmath269 .",
    "now we can develop the shannon conditional entropy _ from _ the logical conditional entropy using the dit - bit transform and thereby _ explain _ the figure 5 venn diagram relationship .",
    "first we put the logical conditional entropy in the proper form as the average of dit - counts :    @xmath249   $ ]    where @xmath240 is the normalized dit count for the discrete partition on a set @xmath270 with @xmath271 equiprobable elements .",
    "hence that same equiprobable set requires the bit - count of @xmath272 binary partitions to distinguish its elements .",
    "similarly @xmath242 is the normalized dit count for ( the discrete partition on ) a set @xmath273 with @xmath274 equiprobable elements , so it requires @xmath275 binary partitions to make those distinctions .",
    "those binary partitions are included in the @xmath276 binary partitions ( since a @xmath241-distinction is automatically a @xmath219-distinction ) and we do nt want the @xmath241-distinctions so they are subtracted off to get : @xmath277 bits . taking the same probabilistic average , the average number of binary partitions needed to make the @xmath245-distinctions but not the @xmath241-distinctions is :    @xmath278   = \\sum_{x , y}p\\left (   x , y\\right )   \\log\\left (   \\frac{p\\left (   y\\right ) } { p\\left (   x , y\\right )   } \\right )   = h\\left (   x|y\\right )   .$ ]    thus transforming the dit - counts into the bit - counts for the equiprobable sets , and taking the probabilistic average gives the same venn diagram picture for the shannon entropies .",
    "if the ` atom '  of information is the distinction or dit , then the atomic information in a partition @xmath10 is  its ditset , @xmath182 . following again campbell s dictum about the mutual information , the information common to two partitions @xmath10 and @xmath22 would naturally be the intersection of their@xmath125ditsets :    @xmath279    _ mutual information set_.    it is an interesting and not completely trivial fact that as long as neither @xmath10 nor @xmath22 is the indiscrete partition @xmath280 ( where @xmath281 ) , then @xmath10 and @xmath22 have a distinction in common . in terms of ditsets as open sets , any two nonempty open sets intersect .",
    "[ nonempty ditsets",
    "intersect]given two partitions @xmath10 and @xmath22 on @xmath1 which contain any information ( i.e. , have nonempty ditsets ) , then they have common information , i.e. , @xmath282 . , if every pair of elements @xmath283 is equated by one or the other of the relations , i.e. , @xmath284 , then either @xmath285 or @xmath286 . ]",
    "proof : since @xmath10 is not the indiscrete partition ( which contains no information ) , consider two elements @xmath59 and @xmath287 distinguished by @xmath10 but identified by @xmath22 [ otherwise @xmath288 . since @xmath22 is also not the indiscrete partition , there must be a third element @xmath289 not in the same block of @xmath22 as @xmath59 and @xmath287 .",
    "but since @xmath59 and @xmath287 are in different blocks of @xmath10 , the third element @xmath289 must be distinguished from one or the other or both in @xmath10 .",
    "hence @xmath290 or @xmath291 must be distinguished by both partitions and thus must be in their mutual information set @xmath292.@xmath293    since the only partition with a logical entropy of @xmath294 is the indiscrete partition or blob @xmath280 , we also have the :    if @xmath295 , then @xmath296.@xmath293    the ditsets @xmath42 and their complementary inditsets (= equivalence relations ) @xmath297 are easily characterized as : @xmath298    the mutual information set can also be characterized in this manner .",
    "[ structure of mutual information sets]given partitions @xmath10 and @xmath22 with blocks @xmath299 and @xmath300 , then    @xmath301 .",
    "proof : the union ( which is a disjoint union ) will include the pairs @xmath61 where for some @xmath23 and @xmath24 , @xmath302 and @xmath303 . since @xmath287 is in @xmath253 but not in the intersection @xmath30 , it must be in a different block of @xmath10 than @xmath304 so @xmath305 .",
    "symmetrically , @xmath306 so @xmath307 . conversely if @xmath308 then take the @xmath304 containing @xmath59 and the @xmath253 containing @xmath287",
    ". since @xmath61 is distinguished by both partitions , @xmath309 and @xmath310 so that @xmath311.@xmath293    the probability that a pair randomly chosen from @xmath5 would be distinguished by @xmath10 _ and _ @xmath22 would be given by the normalized counting measure of the mutual information set which is the :    @xmath312 @xmath313 probability that @xmath10 and @xmath22 distinguishes    _ logical mutual information of _ @xmath10 _ _  and _ _ @xmath22 .    by the above corollary , if @xmath295 , then @xmath314 . and by the inclusion - exclusion principle that holds for measures like the cardinality of subsets :    @xmath315 .",
    "normalizing , the probability that a random pair is distinguished by both partitions is given by the inclusion - exclusion principle : @xmath316    inclusion - exclusion principle for logical entropies of partitions    this can be extended after the fashion of the inclusion - exclusion principle to any number of partitions .",
    "it was previously noted that the intersection of two ditsets is not necessarily the ditset of a partition , but the interior of the intersection is the ditset @xmath317 of the partition meet @xmath32 .",
    "hence we also have the :    @xmath318    submodular inequality for logical entropies .",
    "consider again a joint distribution @xmath221 over @xmath220 for finite @xmath163 and @xmath233 .",
    "if information is based on distinctions , then the information in a joint distribution @xmath319 on @xmath220 is analyzed in terms of the distinctions between ordered pairs @xmath320 where two ordered pairs @xmath219 and @xmath321 could be distinguished by either coordinate . intuitively , the mutual logical information @xmath322 in the joint distribution@xmath221 would be the probability that a sampled pair of pairs @xmath219 and @xmath323 would be distinguished in both coordinates , i.e. , a distinction @xmath232 of @xmath324 _ and _ a distinction @xmath234 of @xmath325 .",
    "that means for each probability @xmath246 , it must be multiplied by the probability of not drawing the same @xmath245 _ and _ not drawing the same @xmath241 ( e.g. , in a second independent drawing from @xmath220 ) . in the venn diagram ,",
    "the area or probability of the drawing that @xmath245 or that @xmath241 is @xmath326 ( correcting for adding the overlap twice ) so the probability of getting neither that @xmath245 nor that @xmath241 is the complement @xmath327   + \\left [   1-p\\left (   y\\right )   \\right ] -\\left [   1-p\\left (   x , y\\right )   \\right ]   $ ] as shown in figure 6 .",
    "figure6.eps    figure 6",
    ": @xmath328   + \\left [   1-p\\left (   y\\right ) \\right ]   -\\left [   1-p\\left (   x , y\\right )   \\right ]   $ ]    = shaded area in venn diagram for @xmath220    hence we have :",
    "@xmath329   + \\left [   1-p\\left (   y\\right )   \\right ]   -\\left [ 1-p\\left (   x , y\\right )   \\right ]   \\right ]   $ ]    _ logical mutual information in a joint probability distribution_.    the probability of two independent draws differing in _ either _ the @xmath245 _ or _ the @xmath241 is just the logical entropy of the joint distribution :    @xmath330   = 1-\\sum_{x , y}p\\left (   x , y\\right )   ^{2}$ ] .",
    "using a little algebra to expand the logical mutual information : @xmath331   + \\left [   1-{\\textstyle\\sum\\nolimits_{x , y } } p\\left (   x , y\\right )   p\\left (   y\\right )   \\right ]   -\\left [   1-{\\textstyle\\sum\\nolimits_{x , y } } p\\left (   x , y\\right )   ^{2}\\right ] \\\\ &   = h\\left (   x\\right )   + h\\left (   y\\right )   -h\\left (   x , y\\right)\\end{aligned}\\ ] ]    inclusion - exclusion principle for logical entropies of a joint distribution .",
    "figure7.eps    figure 7 : @xmath332    = shaded area in venn diagram for @xmath247 .",
    "it might be noted that the logical mutual information , like the other logical entropies , is not just an average ( like the shannon formula ) ; the mutual information is the product probability measure of the subset :    @xmath333 .",
    "the fact that nonempty ditsets of partitions always intersect was perhaps surprising and it helps to sharpen the differences between logical and shannon entropies in the matter of ` intuitions ' about independence ( see below ) .",
    "hence it may be helpful to prove the corresponding result for any joint probability distributions @xmath221 on the finite set @xmath220 .",
    "the corresponding ` ditsets ' are :    @xmath334    @xmath335 .    for the product probability measure @xmath164 on @xmath247 ,    @xmath336    @xmath337    @xmath338 ,    and similarly @xmath339 .",
    "then @xmath340 iff @xmath341 iff there is an @xmath342 such that @xmath343 , and similarly for @xmath344 .",
    "[ nonempty ditsets still intersect]if @xmath345 , then @xmath346 .",
    "proof : since @xmath347 is nonempty , there are two pairs @xmath219 and @xmath236 such that @xmath232 and @xmath348 .",
    "if @xmath234 then @xmath349 as well and we are finished .",
    "hence assume @xmath350 .",
    "since @xmath351 is also nonempty and thus @xmath352 , there is another @xmath353 such that for some @xmath354 , @xmath355 . since @xmath354 ca nt be equal to both @xmath245 and @xmath356 , at least one of the pairs @xmath357 or @xmath358 is in both @xmath347 and @xmath351 , and thus the product measure on @xmath359 is positive ,",
    "i.e. , @xmath346.@xmath293      the usual heuristic motivation for shannon s mutual information is much like its dit - bit development from the logical mutual information so we will take that approach at the outset . the logical mutual information for partitions can be expressed in the form :    @xmath360   $ ]    so if we substitute the bit - counts for the dit - counts as before , we get :    @xmath361   = \\sum_{b , c}p_{b\\cap c}\\log\\left ( \\frac{p_{b\\cap c}}{p_{b}p_{c}}\\right )   $ ]    _ shannon s mutual information for partitions_.    keeping the log s separate gives the venn diagram picture for the shannon entropies : @xmath362 \\\\ &   = h\\left (   \\pi\\right )   + h\\left (   \\sigma\\right )   -h\\left (   \\pi\\vee \\sigma\\right)\\end{aligned}\\ ] ]    inclusion - exclusion analogy for shannon entropies of partitions .      to move from partitions to probability distributions ,",
    "consider again the joint distribution @xmath246 on @xmath220 .",
    "then developing the shannon mutual information from the logical mutual information amounts to replacing the block probabilities @xmath363 in the join @xmath29 by the joint probabilities @xmath246 and the probabilities in the separate partitions by the marginals ( since @xmath364 and @xmath365 ) , to obtain :    @xmath366    _ shannon mutual information in a joint probability distribution_.    then the same proof carries over to give the :    @xmath367    figure8.eps    figure 8 : venn diagram relation for shannon entropies of probability distributions .",
    "the logical mutual information formula :    @xmath329   + \\left [   1-p\\left (   y\\right )   \\right ]   -\\left [ 1-p\\left (   x , y\\right )   \\right ]   \\right ]   $ ]    develops via the dit - count to bit - count transformation to :    @xmath368   = \\sum _ { x , y}p\\left (   x , y\\right )   \\log\\left (   \\frac{p\\left (   x , y\\right )   } { p\\left ( x\\right )   p\\left (   y\\right )   } \\right )   = i\\left (   x , y\\right )   $ ] .",
    "thus the genuine venn diagram relationships for the product probability measure that gives the logical entropies carry over , via the dit - count to bit - count transformation , to give a similar venn diagram picture for the shannon entropies .",
    "two partitions @xmath10 and @xmath22 are said to be ( stochastically ) _ independent _ if for all @xmath23 and @xmath24 , @xmath369 . if @xmath10 and @xmath22 are independent , then :    @xmath370 ,    so that :    @xmath371    shannon entropy for partitions additive under independence .    in ordinary probability theory , two events @xmath372 for a sample space @xmath1 are said to be _ independent _ if @xmath373 .",
    "we have used the motivation of thinking of a partition - as - dit - set @xmath42 as an event  in a sample space @xmath5 with the probability of that event being @xmath92 , the logical entropy of the partition .",
    "the following proposition shows that this motivation extends to the notion of independence .",
    "[ independent partitions have independent dit sets]if @xmath10 and @xmath22 are ( stochastically ) independent partitions , then their ditsets @xmath42 and @xmath374 are independent as events in the sample space @xmath5 ( with equiprobable points ) .    for independent partitions @xmath10 and @xmath22 , we need to show that the probability @xmath375 of the event @xmath376 is equal to the product of the probabilities @xmath92 and @xmath377 of the events @xmath42 and @xmath378 in the sample space @xmath5 . by the assumption of stochastic independence , we have @xmath379 so that @xmath380 . by the previous structure theorem for the mutual information set : @xmath381 , where the union is disjoint so that :    @xmath382    so that :    @xmath383.@xmath293    hence the mutual logical information behaves like probabilities under independence ; the probability that @xmath10 _ and _ @xmath22 distinguishes , i.e. , @xmath384 , is equal to the probability @xmath92 that @xmath10 distinguishes times the probability @xmath377 that @xmath22 distinguishes :    @xmath385    logical mutual information multiplicative under independence .",
    "the somewhat surprising nonempty - ditsets - intersect proposition shows that unless @xmath386 or @xmath387 ( i.e. , one of them is the indiscrete partition or blob @xmath280 ) then @xmath388 _ even _ when the partitions are independent .",
    "two non - blob partitions always have mutual information when information is measured by ( normalized ) distinctions .",
    "this is a striking difference with shannon mutual information where @xmath389 for independent partitions .",
    "this shows that in spite of the many connections ( e.g. , the dit - bit connection ) between the logical and shannon entropies , they should not be thought as measuring the same thing ( ` information ' ) just by different units like centimeters and inches .",
    "this difference also comes out in the fact that logical entropy always has a direct probability interpretation whereas shannon entropy requires an averaging process involving the law of large numbers ( see below ) to be interpreted .",
    "it is also useful to compare the formulas for conditional entropies :    @xmath390    @xmath391    conditional entropies in general .    when @xmath10 and @xmath22 are independent , then @xmath392 so that we have the ` intuitive ' results that @xmath393 ( like probabilities ) and @xmath394(unlike probabilities ) .",
    "the logical conditional entropy @xmath395 can always be interpreted as the probability that a random pair drawn from @xmath1 is a @xmath10-distinction and not a @xmath22-distinction .",
    "but under independence , @xmath396 so @xmath397 , and thus the conditional entropy @xmath398 can be computed as the probability @xmath92 that a random pair is a @xmath10-distinction ( which is defined independently of @xmath22 ) times the probability @xmath399 that a random pair is not a @xmath213-distinction ( which is defined independently of @xmath10 ) .",
    "it is sometimes convenient to think in the complementary terms of an equivalence relation equating or identifying  rather than a partition distinguishing .",
    "since @xmath92 can be interpreted as the probability that a random pair of elements from @xmath1 are distinguished by @xmath400 , i.e. , as a distinction probability , its complement @xmath401 can be interpreted as an _ identification _ or _ indistinction probability _ ,",
    "i.e. , the probability that a random pair is equated by @xmath10 ( thinking of @xmath10 as an equivalence relation on @xmath1 ) . in general",
    ",    @xmath402   \\left [   1-h\\left (   \\sigma\\right ) \\right ]   = 1-h\\left (   \\pi\\right )   -h\\left (   \\sigma\\right )   + h\\left ( \\pi\\right )   h\\left (   \\sigma\\right )   = \\left [   1-h\\left (   \\pi\\vee\\sigma\\right ) \\right ]   + \\left [   h\\left (   \\pi\\right )   h\\left (   \\sigma\\right )   -m(\\pi , \\sigma\\right ]   $ ]    which could also be rewritten as :    @xmath403   -\\left [   1-h\\left ( \\pi\\right )   \\right ]   \\left [   1-h\\left (   \\sigma\\right )   \\right ]   = m(\\pi , \\sigma)-h\\left (   \\pi\\right )   h\\left (   \\sigma\\right )   $ ]",
    ".    thus if @xmath10 and @xmath22 are independent , then the probability that the join partition @xmath29 identifies is the probability that @xmath10 identifies times the probability that @xmath22 identifies :    @xmath402   \\left [   1-h\\left (   \\sigma\\right ) \\right ]   = \\left [   1-h\\left (   \\pi\\vee\\sigma\\right )   \\right ]   $ ]    multiplicative indistinction probabilities under independence .",
    "a joint probability distribution @xmath221 on @xmath220 is _ independent _ if each value is the product of the marginals : @xmath404 .    for an independent distribution , the shannon mutual information    @xmath405",
    "is immediately seen to be zero so we have :    @xmath406    shannon entropies for independent @xmath221 .    for the logical mutual information @xmath407 ,",
    "independence gives : @xmath408 \\\\ &   = { \\textstyle\\sum\\nolimits_{x , y } } p\\left (   x\\right )   p\\left (   y\\right )   \\left [   1-p\\left (   x\\right )   -p\\left ( y\\right )   + p\\left (   x\\right )   p\\left (   y\\right )   \\right ] \\\\ &   = { \\textstyle\\sum\\nolimits_{x } } p\\left (   x\\right )   \\left [   1-p\\left (   x\\right )   \\right ] { \\textstyle\\sum\\nolimits_{y } } p\\left (   y\\right )   \\left [   1-p\\left (   y\\right )   \\right ] \\\\ &   = h\\left (   x\\right )   h\\left (   y\\right)\\end{aligned}\\ ] ]    logical entropies for independent @xmath221 .",
    "as in the case of partitions , the logical conditional entropy @xmath409 is the probability that a random pair of pairs @xmath219 and @xmath236 is a distinction @xmath232 for @xmath222 but not a distinction @xmath234 of @xmath410 . under independence ,",
    "that logical conditional entropy is @xmath411 which is the probability of randomly drawing a distinction from the marginal distribution @xmath222 times the probability of randomly drawing an indistinction from the other marginal distribution @xmath412 .",
    "the nonempty - ditsets - still - intersect proposition shows that we get the same type of result : @xmath413 implies @xmath414 , for joint probability distributions as for partitions , and thus that logical mutual information @xmath322 is still positive for independent distributions when @xmath413 , in which case @xmath415 .",
    "this is a striking difference between the average bit - count shannon entropy and the dit - count logical entropy . aside from the waste case",
    "where @xmath416 , there are always positive probability mutual distinctions for @xmath163 and @xmath233 , and that dit - count information is not recognized by the average bit - count shannon entropy .",
    "this independence condition @xmath417 plus the inclusion - exclusion principle @xmath418 also implies that : @xmath419   \\left [   1-h\\left (   y\\right )   \\right ] &   = 1-h\\left (   x\\right )   -h\\left (   y\\right )   + h\\left (   x\\right )   h\\left ( y\\right ) \\\\ &   = 1-h\\left (   x\\right )   -h\\left (   y\\right )   + m\\left (   x , y\\right ) \\\\ &   = 1-h\\left (   x , y\\right )   \\text{.}\\ ] ]    hence under independence , the probability of drawing the same pair @xmath219 in two independent draws is equal to the probability of drawing the same @xmath245 twice times the probability of drawing the same @xmath241 twice .",
    "given two probability distributions @xmath147 and @xmath420 on the same sample space @xmath421 , we can again consider the drawing of a pair of points but where the first drawing is according to @xmath101 and the second drawing according to @xmath422 .",
    "the probability that the points are distinct would be a natural and more general notion of logical entropy that would be the :    @xmath423    _ logical _ _ cross entropy of _",
    "@xmath101 _ and _ @xmath422    which is symmetric",
    ". the logical cross entropy is the same as the logical entropy when the distributions are the same , i.e. , if @xmath424 , then @xmath425 .",
    "the notion of _ cross entropy _ in shannon entropy can be developed by applying dit - bit connection to the logical cross entropy @xmath426 to obtain :    @xmath427    which is not symmetrical due to the asymmetric role of the logarithm , although if @xmath424 , then @xmath428 .",
    "since the logical cross entropy is symmetrical , it could also be expressed as @xmath429 which develops to the shannon cross entropy @xmath430 so it might be more reasonable to use a _ symmetrized shannon cross entropy _ :    @xmath431   $ ] .    the _ kullback - leibler divergence _ ( or _ relative entropy _ )",
    "@xmath432 is defined as a measure of the distance or divergence between the two distributions where @xmath433 .",
    "a basic result is the :    @xmath434 with equality if and",
    "only if @xmath424    _ information inequality _ @xcite .    given two partitions @xmath10 and @xmath22 , the inequality @xmath435 is obtained by applying the information inequality to the two distributions @xmath436 and @xmath437 on the sample space @xmath438 :    @xmath439    with equality iff independence .    in the same manner , we have for the joint distribution @xmath440 :    @xmath441    with equality iff independence .",
    "the _ symmetrized kullback - leibler divergence _ is :    @xmath442   = h_{s}\\left (   p||q\\right )   -\\left [   \\frac{h\\left (   p\\right ) + h\\left (   q\\right )   } { 2}\\right ]   $ ] .",
    "but starting afresh , one might ask : what is the natural measure of the difference or distance between two probability distributions @xmath147 and @xmath420 that would always be non - negative , and would be zero if and only if they are equal ?  the ( euclidean ) distance between the two points in @xmath443 would seem to be the logical  answer ",
    "so we take that distance ( squared with a scale factor ) as the definition of the :    @xmath444 @xmath445    _ logical divergence _ ( or _ logical _ _ relative entropy _ ) . ]    which is symmetric and we trivially have :    @xmath446 with equality iff @xmath424    logical information inequality .",
    "we have component - wise :    @xmath447   -\\left [   \\frac{1}{n}-p_{i}^{2}\\right ]   -\\left [   \\frac{1}{n}-q_{i}^{2}\\right ]   $ ]    so that taking the sum for @xmath448 gives :    @xmath449   -\\frac{1}{2}\\left [   \\left (   1-{\\textstyle\\sum\\nolimits_{i } } p_{i}^{2}\\right )   + \\left (   1-{\\textstyle\\sum\\nolimits_{i } } q_{i}^{2}\\right )   \\right ] \\\\ &   = h\\left (   p\\vert q\\right )   -\\frac{h\\left (   p\\right )   + h\\left (   q\\right ) } { 2}\\text{.}\\ ] ]    logical divergence = _ jensen difference _",
    "@xcite between probability distributions .",
    "then the information inequality implies that the logical cross - entropy is greater than or equal to the average of the logical entropies :    @xmath450 with equality iff @xmath424 .    the half - and - half probability distribution @xmath451 that mixes @xmath101 and @xmath422 has the logical entropy of    @xmath452   $ ]    so that :    @xmath453 with equality iff @xmath424 .    mixing different @xmath101 and @xmath422 increases logical entropy .",
    "the logical divergence can be expressed in the proper form to apply the dit - bit transform :    @xmath454   -\\frac{1}{2}\\left [   \\left (   \\sum_{i}p_{i}\\left (   1-p_{i}\\right )   \\right )   + \\left ( \\sum_{i}q_{i}\\left (   1-q_{i}\\right )   \\right )   \\right ]   $ ]    so the dit - bit transform is :    @xmath455   $ ]    @xmath456   = \\frac{1}{2}\\left [   d\\left (   p||q\\right )   + d\\left (   q||p\\right )   \\right ]   $ ]    @xmath457 .",
    "thus the logical divergence @xmath458 ( which is symmetrical ) develops via the dit - bit transform to the symmetrized version of the kullback - leibler divergence .",
    "the logical cross - entropies and divergences can also be defined for partitions using the device of a density matrix .",
    "a binary relation @xmath459 on @xmath77 can be represented by an @xmath460 _ incidence matrix _",
    "@xmath461 where    @xmath462{c}1\\text { if } \\left (   u_{i},u_{j}\\right )   \\in r\\\\ 0\\text { if } \\left (   u_{i},u_{j}\\right )   \\notin r\\text{.}\\end{array } \\right .",
    "$ ]    taking @xmath463 as the equivalence relation @xmath464 associated with a partition @xmath10 , the _ density matrix _ @xmath465 _ of the partition _",
    "@xmath10 is just the incidence matrix @xmath466 rescaled to be of trace @xmath3 ( i.e. , sum of diagonal entries is @xmath3 ) :    @xmath467 .    from coding theory @xcite",
    ", we have the notion of the _ hamming distance between two _ @xmath468 _ vectors or matrices _ ( of the same dimensions ) which is the number of places where they differ .",
    "the powerset @xmath469 can be viewed as a vector space over @xmath470 where the sum of two binary relations @xmath471 , symbolized @xmath472 , is the set of elements ( i.e. , ordered pairs @xmath473 ) that are in one set or the other but not both .",
    "thus the hamming distance @xmath474 between the incidence matrices of two binary relations is just the cardinality of their symmetric difference : @xmath475 .",
    "moreover , the size of the symmetric difference does not change if the binary relations are replaced by their complements : @xmath476 .",
    "we previously defined the logical divergence between two probability distributions as ( one - half ) the euclidean distance squared : @xmath477 .",
    "it would seem equally logical to define the logical divergence @xmath478 between two partitions as ( one - half ) the normalized hamming distance between the incidence matrices of their indit sets :    @xmath479    @xmath480    .    since the number of places the two @xmath468 matrices can differ is always non - negative and is @xmath294 if and only if they are the same matrices ( and that is unchanged by the scaling factor @xmath481 and normalization by @xmath217 ) , we immediately have :    @xmath482 with equality iff @xmath483 .",
    "inequality for hamming distance between partitions .",
    "formulas involving probability distributions @xmath147 and summations can be transformed into formulas using density matrices by substituting a density matrix @xmath159 for the probability distribution @xmath101 and substituting the trace operation ( sum of diagonals ) for the summation .",
    "for instance if @xmath101 was replaced by the @xmath460 density matrix @xmath484 that is diagonal with entries @xmath117 along the diagonal , then @xmath485 so @xmath486   $ ] .    at first the definition of the logical cross - entropy @xmath487",
    "does not seem suitable to apply to partitions .",
    "but when we represent the partition @xmath10 by its density matrix @xmath488 and switch to the density matrix treatment , then the definition of the logical cross - entropy of two partitions that suggests itself is :    @xmath489   $ ] .",
    "then we can calculate the trace as :    @xmath490   = \\frac { 1}{\\left\\vert u\\times u\\right\\vert } \\sum_{i}\\left [   i\\left ( \\operatorname*{indit}\\left (   \\pi\\right )   \\right )   i\\left ( \\operatorname*{indit}\\left (   \\sigma\\right )   \\right )   \\right ]   _ { ii}$ ]    @xmath491    @xmath492    so the logical cross - entropy of two partitions is just the logical entropy of their join :    @xmath489   = 1-\\left [   1-h\\left (   \\pi",
    "\\vee\\sigma\\right )   \\right ]   = h\\left (   \\pi\\vee\\sigma\\right )   $ ] .",
    "following again the methodology of replacing probability distributions by density matrices , we apply the notion of logical divergence previously defined as half the euclidean distance squared to density matrices as follows :    @xmath493 = h\\left (   \\pi||\\sigma\\right )   -\\frac{h\\left (   \\rho\\left (   \\pi\\right )   \\right ) + h\\left (   \\rho\\left (   \\sigma\\right )   \\right )   } { 2}$ ] .",
    "but @xmath494   $ ] and by the previous calculation :    @xmath495 = \\operatorname*{tr}\\left [   \\rho\\left (   \\pi\\right )   \\rho\\left (   \\pi\\right ) \\right ]   = 1-h\\left (   \\pi\\vee\\pi\\right )   = 1-h\\left (   \\pi\\right )   $ ]    so @xmath496 and then :    @xmath497 .",
    "thus the euclidean distance definition of the logical divergence ( or distance ) between two partitions is the same as the hamming distance definition .",
    "moreover the two natural notions of distance have clean expressions in terms of logical entropy .",
    "two partitions are said to be _ compatible _ if they are related in the refinement partial order of the lattice of partitions on @xmath1 . for two compatible partitions ,",
    "say , @xmath19 , we have @xmath498 so the distance between compatible partitions is just ( the scale factor times ) the distance between their logical entropies : @xmath499   $ ] .",
    "these results show that the unified way to treat probability distributions and partitions is the density matrix  which also extends to the corresponding quantum notions .",
    "the following table 3 summarizes the concepts for the shannon and logical entropies .",
    "we use the case of probability distributions rather than partitions , and we use the abbreviations @xmath500 , @xmath501 , and @xmath502 .    [ c]l|c|c|table 3 & @xmath503 & @xmath504 + & @xmath505 & @xmath506 + & @xmath507 & @xmath508 + &  @xmath509 & @xmath510 + & @xmath511 & @xmath415 + & @xmath512 & @xmath513   + & @xmath514 & @xmath515   { \\small = } \\left [   1-h\\left (   x\\right )   \\right ]   \\left [ 1-h\\left (   y\\right )   \\right ]   $ ] + & @xmath516 & @xmath517 + & @xmath518 &  @xmath519 + & @xmath520 & @xmath521   /2 $ ] + & @xmath522 & @xmath523 +    table 3 : comparisons between shannon and logical entropy formulas    the following table 4 summarizes the dit - bit transforms .",
    "[ c]c|c|table 4 & the dit - bit transform : @xmath524 + & @xmath525 + & @xmath526 + & @xmath527   $ ] + & @xmath528   $ ] + & @xmath529   + \\left [   1-p\\left ( y\\right )   \\right ]   -\\left [   1-p\\left (   x , y\\right )   \\right ]   \\right ]   $ ] + & @xmath530   $ ] + & @xmath531   $ ] + & @xmath532   $ ] + & @xmath533   $ ] + & @xmath534 $ ] +    table 4 : the logical entropy to shannon entropy dit - bit transform",
    "the taylor series for @xmath535 around @xmath536 is :    @xmath537    so substituting @xmath538 ( with @xmath539 ) gives a version of the newton - mercator series :    @xmath540    then multiplying by @xmath117 and summing yields :    @xmath541    @xmath542 .",
    "a similar relationship holds in the quantum case between the von neumann entropy @xmath543   $ ] and the _ quantum logical entropy _ @xmath544   = 1-\\operatorname*{tr}\\left [   \\rho^{2}\\right ]   $ ] which is defined by having a density matrix @xmath159 replace the probability distribution @xmath101 and the trace replace the sum .",
    "quantum logical entropy is beyond the scope of this paper but it might be noted that some quantum information theorists have been using that concept to rederive results previously derived using the von neumann entropy such as the klein inequality , concavity , and a holevo - type bound for hilbert - schmidt distance ( @xcite , @xcite ) .",
    "there are many older results derived under the misnomer `` linear entropy '' or derived for the quadratic special case of the tsallis - havrda - charvat entropy ( @xcite ; @xcite , @xcite ) .",
    "moreover the logical derivation of the logical entropy formulas using the notion of distinctions gives a certain naturalness to the notion of quantum logical entropy .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ we find this framework of partitions and distinction most suitable ( at least conceptually ) for describing the problems of quantum state discrimination , quantum cryptography and in general , for discussing quantum channel capacity . in these problems , we are basically interested in a distance measure between such sets of states , and this is exactly the kind of knowledge provided by logical entropy ( @xcite ) .",
    "boaz - cohen : logicalentropy _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the relationship between the shannon / von neumann entropies and the logical entropies in the classical and quantum cases is responsible for presenting the logical entropy as a ` linear ' approximation to the shannon or von neumann entropies since @xmath545 is the linear term in the series for @xmath546 [ _ before _ the multiplication by @xmath117 to make the term quadratic ! ] . and",
    "@xmath547 or it quantum counterpart @xmath548   $ ] are even called `` linear entropy '' ( e.g. , @xcite or @xcite ) even though the formulas are obviously quadratic.[multiblock footnote omitted ] another name for the quantum logical entropy found in the literature is `` mixedness '' @xcite which at least does nt call a quadratic formula ` linear . '",
    "it is even called `` impurity '' since the complement @xmath549   $ ] ( i.e. , the quantum version of turing s repeat rate @xmath550 ) is called the `` purity . '' and as noted above , the formula for logical entropy occurs as the quadratic special case of the tsallis - havrda - charvat entropy .",
    "those parameterized families of entropy formulas are sometimes criticized for lacking a convincing interpretation , but we have seen that the quadratic case is based on partition logic dual to boole s subset logic . in terms of the duality between elements of a subset ( its ) and distinctions of a partition ( dits ) , it is based on the normalized counting measures of its and dits ) .    in accordance with its quadratic nature , logical entropy is the logical special case of c. r. rao s quadratic entropy @xcite .",
    "two elements from @xmath77 are either identical or distinct .",
    "gini @xcite introduced @xmath551 as the distance  between the @xmath552 and @xmath553 elements where @xmath554 for @xmath555 and @xmath556which might be considered the logical distance function @xmath557 , the complement of the kronecker delta . since @xmath558 , the logical entropy ,",
    "i.e. , gini s index of mutability , @xmath559 , is the average logical distance between distinct elements .",
    "but one might generalize by allowing other distances @xmath560 for @xmath555 ( but always @xmath556 ) so that @xmath561 would be the average distance between distinct elements from @xmath1 .",
    "in 1982 , c. r. rao introduced this concept as _ quadratic entropy",
    "_ @xcite .",
    "rao s treatment also includes ( and generalizes ) the natural extension of logical entropy to continuous ( square - integrable ) probability density functions @xmath562 for a random variable @xmath163 : @xmath563 .",
    "it might be noted that the natural extension of shannon entropy to continuous probability density functions @xmath564 through the limit of discrete approximations contains terms @xmath565 that blow up as the mesh size @xmath566 goes to zero .",
    "hence the definition of shannon entropy in the continuous case is defined not by the limit of the discrete formula but by the _ analogous _",
    "formula @xmath567 which , as mceliece points out , `` is not in any sense a measure of the randomness of @xmath163 . ''",
    "mceliece : info .",
    "lacking an immediate and convincing interpretation for an entropy formula , one might produce a number of axioms about a ` measure of information ' where each axiom is more or less intuitive .",
    "one supposed intuition about ` information ' is that the information in independent random variables should be additive ( unlike probabilities ) or that the information in one variable conditional on a second variable should be the same as the information in the first variable alone when the variables are independent ( just like conditional probabilities ) .",
    "another intuition is that the information gathered from the occurrence of an event is inversely related to the probability of the event .",
    "for instance , if the probability of an outcome is @xmath117 , then @xmath197 is a good indicator of the surprise - value information gained by the occurrence of the event .",
    "very well ; let us follow out that intuition to construct a measure of surprise - value entropy .",
    "we need to average the surprise - values across the probability distribution @xmath568   = \\left (   p_{1}, ... ,p_{n}\\right )   $ ] , and since the surprise - value is the multiplicative inverse of the @xmath117 , the natural notion of average is the multiplicative ( or geometric ) average :    @xmath569 .",
    "surprise - value entropy of a probability distribution @xmath568 = \\left (   p_{1}, ... ,p_{n}\\right )   $ ] .",
    "it might be noted that the surprise - value entropy is also independent of any choice of base for logarithms .",
    "how do the surprise - value intuitions square with intuitions about additive information content for independent events ? given a joint probability distribution @xmath570 on @xmath220 , the two marginal distributions @xmath571 and @xmath572 .",
    "then we showed previously that if the joint distribution was independent , i.e. , @xmath573 , then the shannon entropies were additive :    @xmath406    shannon entropies under independence .",
    "this is in accordance with the ` intuition ' about independence .",
    "but the surprise - value entropy is also based on intuitions so we need to check if it is also additive for an independent joint distribution so that the intuitions would be consistent .",
    "the surprise - value entropy of the independent joint distribution is :    @xmath574   \\right )   = \\prod_{x , y}\\left (   \\frac{1}{p_{xy}}\\right )   ^{p_{xy}}=\\prod_{x , y}\\left (   \\frac{1}{p_{x}p_{y}}\\right ) ^{p_{x}p_{y}}=\\prod_{x}\\prod_{y}\\left (   \\frac{1}{p_{x}}\\right )   ^{p_{x}p_{y}}\\left (   \\frac{1}{p_{y}}\\right )   ^{p_{x}p_{y}}$ ]    @xmath575   \\left [   \\prod_{y}\\prod_{x}\\left (   \\frac{1}{p_{y}}\\right ) ^{p_{x}p_{y}}\\right ]   $ ]    @xmath576   \\left [ \\prod_{y}\\left (   \\frac{1}{p_{y}}\\right )   ^{p_{y}}\\right ]   $ ]    @xmath577   \\right )   e\\left (   \\left [   p_{y}\\right ] \\right )   $ ]    so the surprise - value of an independent joint distribution is the _ product _ of the surprise - value entropies of the marginal distributions .",
    "the derivation used the fact that the multiplicative average of a constant is , of course , that constant , e.g. , @xmath578 .    since the two intuitions give conflicting results , which , if either , is ` correct ' ? at this point , it is helpful to step back and note that in statistics , for example , any product of random variables @xmath579 can sometimes , with advantage , be analyzed using the sum of log - variables , @xmath580 .",
    "is it a question of right or wrong ? which are the ` true ' variables ?    in the case at hand , the notion of surprise - value entropy which is multiplicative for independent distributions can trivially be turned into an expression that is additive for independent distributions by taking logarithms to some base :    @xmath581   \\right )   = \\log e\\left (   \\left [ p_{x}\\right ]   \\right )   + \\log e\\left (   \\left [   p_{y}\\right ]   \\right )   $ ] .",
    "is the original surprise - value formula @xmath582 or the log - of - surprise - value formula @xmath583 the ` true ' measure ? and , in the case at hand , the point is that the log - of - surprise - value formula _ is _ the shannon entropy :    @xmath584 or @xmath585 .",
    "some authors have suggested that the surprise - value formula is more intuitive than the log - formula . to understand this intuition , we need to develop another interpretation of the surprise - value formula .",
    "when an event or outcome has a probability @xmath117 , it is intuitive to think of it as being drawn from a set of @xmath197 equiprobable elements ( particularly when @xmath197 is an integer ) so @xmath197 is called the _ numbers - equivalent _ @xcite of the probability @xmath117 .",
    "hence the multiplicative average of the numbers - equivalents for a probability distribution @xmath147 is @xmath582 , which thus could also be called the _ numbers - equivalent entropy_. this approach also supplies an interpretation : sampling a probability distribution @xmath101 is like , on average , sampling from a distribution with @xmath586 equiprobable outcomes .    in the biodiversity literature",
    ", the situation is that each animal ( in a certain territory ) is considered to be equiprobable to be sampled and the partition of the animals is by species .",
    "taking @xmath147 as the probability distribution of the @xmath181 species , the numbers - equivalent entropy @xmath582 is the measure of biodiversity that says sampling the population is like sampling a population of @xmath582 equally common species .",
    "the mathematical biologist robert h. macarthur finds this much more intuitive than shannon entropy .    _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ returning to the example of a census of 99 individuals of one species and 1 of a second , we calculate h = ... = 0.0560 [ as the shannon entropy using natural logs ] . for a census of fifty individuals of each of the two species we would get h = ... = 0.693 . to convert these back to ` equally common species",
    "' , we take e0.0560 = 1.057 for the first census and e0.693 = 2.000 for the second . these numbers , 1.057 and 2 , accord much more closely with our intuition of how diverse the areas actually are , ... . @xcite",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    macarthur s interpretation is `` that [ @xmath582 ] equally common species would have the same diversity as the [ @xmath181 ] unequally common species in our census . ''",
    "@xcite    the point is that ` intuitions ' differ even between shannon entropy @xmath589 and its anti - log @xmath582 , not to mention between other approaches to entropy .",
    "incidentally , in the biodiversity literature , the logical entropy @xmath590 is usually called the _ gini - simpson index of biodiversity _ and it has the immediate interpretation as the probability of getting animals of different species in two independent samples .",
    "shannon entropy is sometimes referred to as `` boltzmann - shannon entropy '' or `` boltzmann - gibbs - shannon entropy '' since the shannon formula supposedly has the same functional form as boltzmann entropy which even motivated the name `` entropy . ''",
    "the name `` entropy '' is here to stay but the justification of the formula by reference to statistical mechanics is not quite correct .",
    "the connection between entropy in statistical mechanics and shannon s entropy is only via a numerical approximation , the stirling approximation , where if the first two terms in the stirling approximation are used , then the shannon formula is obtained .",
    "the first two terms in the stirling approximation for @xmath591 are : @xmath592 .",
    "the first three terms in the stirling approximation are : @xmath593 .",
    "if we consider a partition on a finite @xmath1 with @xmath594 , with @xmath181 blocks of size @xmath595 , then the number of ways of distributing the individuals in these @xmath181 boxes with those numbers @xmath596 in the @xmath552 box is : @xmath597 . the normalized natural log of @xmath598",
    ", @xmath599 is one form of entropy in statistical mechanics .",
    "indeed , the formula `` @xmath600 '' is engraved on boltzmann s tombstone .",
    "the entropy formula :    @xmath601   $ ]    can then be developed using the first two terms in the stirling approximation    @xmath602   -\\sum_{i}n_{i}\\left [   \\ln\\left (   n_{i}\\right )   -1\\right ] \\right ]   $ ]    @xmath603   = \\frac{1}{n}\\left [ \\sum n_{i}\\ln\\left (   n\\right )   -\\sum n_{i}\\ln\\left (   n_{i}\\right )   \\right ]   $ ]    @xmath604    where @xmath605 ( and where the formula with logs to the base @xmath606 only differs from the usual base @xmath146 formula by a scaling factor ) .",
    "entropy @xmath607 is in fact an excellent numerical approximation to boltzmann entropy @xmath608 for large @xmath609 ( e.g. , in statistical mechanics ) .",
    "but that does not justify using expressions like `` boltzmann - shannon entropy '' as if the log of the combinatorial formula @xmath598 involving factorials was the same as the two - term stirling approximation .    the common claim that shannon s entropy has the _ same functional form _ as entropy in statistical mechanics is simply false .",
    "if we use a three - term stirling approximation , then we obtain an _ even better _ numerical approximation : , mackay @xcite also uses the next term in the stirling s approximation to give a `` more accurate approximation '' to the entropy of statistical mechanics than the shannon entropy ( the two - term approximation ) . ]    @xmath610    but no one would suggest using that `` more accurate '' entropy formula in information theory or dream of calling it the `` boltzmann - shannon entropy . ''",
    "shannon s formula should be justified and understood on its own terms , and not by over - interpreting the numerically approximate relationship with entropy in statistical mechanics .",
    "shannon , like ralph hartley @xcite before him , starts with the question of how much ` information ' is required to single out a designated element from a set @xmath1 of equiprobable elements .",
    "renyi formulated this in terms of the search @xcite for a hidden element like the answer in a twenty questions game or the sent message in a communication . but being able to always find the designated element is equivalent to being able to distinguish all elements from one another .",
    "one might measure ` information ' as the minimum number of yes - or - no questions in a game of twenty questions that it would take in general to _ distinguish _ all the possible `` answers '' ( or `` messages '' in the context of communications ) .",
    "this is readily seen in the simple case where @xmath611 , i.e. , the size of the set of equiprobable elements is a power of @xmath146 . then following the lead of wilkins over three centuries earlier , the @xmath612 elements could be encoded using words of length @xmath613 in a binary code such as the digits @xmath614 of binary arithmetic ( or @xmath615 in the case of wilkins ) .",
    "then an efficient or minimum set of yes - or - no questions needed to single out the hidden element is the set of @xmath613 questions :    `` is the @xmath553 digit in the binary code for the hidden element a @xmath3 ? ''    for @xmath616 .",
    "each element is distinguished from any other element by their binary codes differing in at least one digit .",
    "the information gained in finding the outcome of an equiprobable binary trial , like flipping a fair coin , is what shannon calls a _ bit _ ( derived from `` binary digit '' ) .",
    "hence the information gained in distinguishing all the elements out of @xmath612 equiprobable elements is :    @xmath617 bits    where @xmath618 is the probability of any given element ( henceforth all logs to base @xmath146 ) .    in the more general case where @xmath619 is not a power of @xmath146 , shannon and hartley",
    "extrapolate to the definition of @xmath620 where @xmath621 as ( in the dit - bit connection previously discussed ) :    @xmath622    shannon - hartley entropy for an equiprobable set @xmath1 of @xmath181 elements .",
    "the shannon formula then extrapolates further to the case of different probabilities @xmath147 by taking the average :    @xmath623 .",
    "shannon entropy for a probability distribution @xmath147    how can that extrapolation and averaging be made rigorous to offer a more convincing interpretation ?",
    "shannon uses the law of large numbers .",
    "suppose that we have a three - letter alphabet @xmath624 where each letter was equiprobable , @xmath625 , in a multi - letter message .",
    "then a one - letter or two - letter message can not be exactly coded with a binary @xmath468 code with equiprobable @xmath294 s and @xmath3 s . but any probability can be better and better approximated by longer and longer representations in the binary number system . hence we can consider longer and longer messages of @xmath609 letters along with better and better approximations with binary codes .",
    "the long run behavior of messages @xmath626 where @xmath627 is modeled by the law of large numbers so that the letter @xmath628 on average occur @xmath629 times and similarly for @xmath630 and @xmath631 .",
    "such a message is called _",
    "typical_.    the probability of any one of those typical messages is :    @xmath632   ^{n}$ ]    or , in this case ,    @xmath633   ^{n}=\\left (   \\frac{1}{3}\\right )   ^{n}$ ] .",
    "hence the number of such typical messages is @xmath634 .",
    "if each message was assigned a unique binary code , then the number of @xmath468 s in the code would have to be @xmath163 where @xmath635 or @xmath636 .",
    "hence the number of equiprobable binary questions or bits needed per letter of the messages is :    @xmath637 .",
    "this example shows the general pattern .",
    "in the general case , let @xmath147 be the probabilities over a @xmath181-letter alphabet @xmath638 .",
    "in an @xmath609-letter message , the probability of a particular message @xmath626 is @xmath639 where @xmath640 could be any of the symbols in the alphabet so if @xmath641 then @xmath642 .    in a _",
    "typical _ message , the @xmath552 symbol will occur @xmath643 times ( law of large numbers ) so the probability of a typical message is ( note change of indices to the letters of the alphabet ) :    @xmath644   ^{n}$ ] .    since the probability of a typical message is @xmath645 with each letter in a typical message being equiprobable with probability @xmath646 .",
    "hence it is as if each letter in a typical message is being draw from an alphabet with @xmath647 equiprobable letters ( i.e. , the numbers - equivalent interpretation of the probability @xmath648 ) .",
    "hence the number of @xmath609-letter messages from the equiprobable alphabet is then @xmath649   ^{n}$ ] .",
    "so far , there is no choice to determine the base of the logs in the shannon formula .",
    "the choice of @xmath146 means assigning a unique binary code to each typical message requires @xmath163 bits where @xmath650   ^{n}$ ] where :    @xmath651   ^{n}\\right\\ } = n\\log\\left [   \\pi_{k=1}^{n}p_{k}^{-p_{k}}\\right ]   $ ]    @xmath652    @xmath653 .",
    "dividing by the number @xmath609 of letters gives the average bit - count interpretation of the shannon entropy ; @xmath654 is the _ average number of bits necessary per letter in a typical message_.    it should be noted that the quantity that emerges in the proof before the choice of base is precisely the base - free notion of surprise - value or numbers - equivalent entropy @xmath655 which has the numbers - equivalent interpretation that each letter in typical message is , in effect , being drawn from an alphabet with @xmath582 equiprobable letters .",
    "the answer to the title question is that partition logic gives a derivation of the ( old ) formula @xmath656 for partitions as the normalized counting measure on the distinctions ( dits ) of a partition @xmath657 that is the analogue of the boolean subset logic derivation of logical probability as the normalized counting measure on the elements ( its ) of a subset .",
    "thus partition logic contributes a _",
    "logical _ notion of entropy to information theory that is parallel to the logical notion of probability contributed to probability theory by subset logic . since classical information theory has heretofore been focused on the original notion of shannon entropy ( and quantum information theory on the corresponding notion of von neumann entropy ) , much of the paper has compared logical entropy to shannon entropy .",
    "logical entropy , like probability , is a measure , while shannon entropy is not .",
    "the compound shannon entropy concepts nevertheless satisfy many of the venn diagram relationships that are automatically satisfied by a measure . in this case ,",
    "that was explained by the dit - bit transform so that by putting a logical entropy notion into the proper form as an average of dit counts , one can replace a dit count by a bit count and obtain the corresponding shannon entropy notion  which explains why the latter concepts satisfy the same venn diagram relationships .",
    "the intersecting - ditsets propositions showed that ( aside from a waste case ) , there is always some mutual dit - count information in two partitions or two random variables even when they are independent .",
    "other comparisons were made in terms of the ` intuitions ' expressed in axioms , on the alleged identity in functional form between shannon entropy and entropy in statistical mechanics , and on the statistical interpretation of shannon entropy and its antilog @xmath658 .",
    "the basic idea of information is distinctions , and distinctions have a precise definition ( dits ) in partition logic .",
    "logical entropy thus directly measures information by taking the normalized counting measure of the dits in a partition .",
    "shannon entropy takes a more indirect approach by counting the minimum number of binary partitions ( bits ) that are required on average to make ( by the join operation on partitions ) all the distinctions of a partition .",
    "ricotta , carlo and laszlo szeidl 2006 . towards a unifying approach to diversity measures : bridging the gap between the shannon entropy and rao s quadratic index .",
    "_ theoretical population biology_. 70 : 237 - 43 ."
  ],
  "abstract_text": [
    "<S> logical probability theory was developed as a quantitative measure based on boole s logic of subsets . </S>",
    "<S> but information theory was developed into a mature theory by claude shannon with no such connection to logic . </S>",
    "<S> but a recent development in logic changes this situation . in category theory , the notion of a subset is dual to the notion of a quotient set or partition , and recently the logic of partitions has been developed in a parallel relationship to the boolean logic of subsets ( subset logic is usually mis - specified as the special case of propositional logic ) . </S>",
    "<S> what then is the quantitative measure based on partition logic in the same sense that logical probability theory is based on subset logic ? </S>",
    "<S> it is a measure of information that is named `` logical entropy '' in view of that logical basis . </S>",
    "<S> this paper develops the notion of logical entropy and the basic notions of the resulting logical information theory . </S>",
    "<S> then an extensive comparison is made with the corresponding notions based on shannon entropy . </S>"
  ]
}