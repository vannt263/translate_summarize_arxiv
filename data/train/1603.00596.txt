{
  "article_text": [
    "for given random variables @xmath0 the distribution of the stochastic linear combination @xmath1 is used for the problems in lifetime , stochastic matrices , neural networks and other applications in sociology and biology .",
    "let @xmath2 ( @xmath3 ) be the lifetime measured in a lab and @xmath4 be the random effect of the environment on it ; so @xmath5 and thus @xmath6 is the average lifetime in the environment ( see homei  ( 2015 ) ) . recently",
    ", several authors have focused on computing the lifetime of systems in the real conditions .",
    "indeed , the randomly linear combination of random vectors have many applications including traditional portfolio selection models , relationship between attitudes and behavior , number of cancer cells in tumor biology , stream flow in hydrology ( nadarajah & kotz  ( 2005 ) ) , branching processes , infinite particle systems and probabilistic algorithms , and vehicle speed and lifetime ( cf .",
    "homei  ( 2015 ) , rezapour & alamatsaz  ( 2014 ) and the references therein ) so finding their distributions has attracted the attentions of numerous researchers .    in this paper ,",
    "considering the dependent components and the real environmental conditions , the distribution of lifetimes ( in case of the dirichlet distributions ) is calculated showing that the main result obtained here covers several previous results and provides a simpler proof ( cf .",
    "e.g. johnson & kotz  ( 1990a ) , sethuraman  ( 1994 ) , van assche  ( 1987 ) , volodin & kotz & johnson  ( 1993 ) ) .",
    "the inner product of two random vectors was introduced in homei  ( 2014 ) and the exact distribution of this product was investigated for some random vectors with beta and dirichlet distributions . in this paper a new generalization for the inner product of two random vectors is introduced . for a random vector @xmath7 and a vector @xmath8 of random vectors ( each @xmath9 being @xmath10-dimensional ) the inner product of @xmath11 and @xmath12 is essentially the linear transformation of @xmath12 under the @xmath13 matrix @xmath14 which is @xmath15 .",
    "we assume that @xmath12 is independent from @xmath16 and has dirichlet distribution ; also @xmath9 s have dirichlet distributions . identifying the distribution of @xmath17",
    "usually requires    * either some long computations with combinatorial identities ( see e.g. volodin & kotz & johnson  ( 1993 ) , johnson & kotz  ( 1990 ) , johnson & kotz  ( 1990a ) , sethuraman  ( 1994 ) , homei  ( 2014 ) ) ; * or advanced techniques requiring performing certain transformations and solving differential equations ( see e.g. homei  ( 2012 ) , soltani & homei ( 2009 ) , van assche  ( 1987 ) or homei  ( 2015 ) and the references therein ) .    in this paper a new way is introduced to identify the distribution of randomly linear combinations ( of dirichlet distributions ) by which a class of stochastic differential equations can be solved ; this new method is much simpler than the existing ones .",
    "our main result identifies the distribution of the randomly linear combination when the coefficients come from a dirichlet distribution .",
    "[ main ] if @xmath18 are independent @xmath10-variate random vectors with respectively @xmath19 distributions , for some @xmath10-dimensional vectors @xmath20 @xmath21 , and the random vector @xmath22 is independent from @xmath18 and has the distribution @xmath23 then the randomly linear combination @xmath24 has the distribution @xmath25    let @xmath26 ( @xmath21 ) be independent random variables independent from @xmath27 that have the distribution @xmath28 , respectively . thus , @xmath29 the @xmath10-dimensional vectors @xmath30 have independent components and @xmath31 distribution .",
    "thus , we have @xmath32 now , @xmath33 has dirichlet@xmath34 distribution , so @xmath35 has the same distribution .",
    "[ cor ] let the @xmath10-variate random variables @xmath18 be independent and with common distribution . if @xmath22 is independent from @xmath18 having @xmath36 distribution , then the randomly linear combination @xmath24 has the @xmath37 distribution if and only if @xmath9 s ( @xmath38 ) have @xmath39 distributions .    the  if \" part follows from theorem  [ main ] and the  only if \" part follows from the theorem of volodin & kotz & johnson  ( 1993 ) .",
    "some similar results ( to corollary  [ cor ] ) can be found in e.g. alamatsaz  ( 1993 ) .",
    "the following results are obtained as special cases of theorem  [ main ] and corollary  [ cor ] :    @xmath40 lemma  1 of sethuraman  ( 1994 ) ( for @xmath41 ) , and    @xmath40 theorem of volodin & kotz & johnson  ( 1993 ) ( when @xmath42 and @xmath43 for @xmath44 , @xmath45 ) ,    which discusses the multivariate case , or the references below which discuss the single - variable case ( note that for @xmath46 the dirichlet distribution leads to the beta distribution ) :    @xmath40 theorem of van assche  ( 1987 ) ( for @xmath47 ) ,    @xmath40 theorem  2 of johnson & kotz  ( 1990a ) ( for @xmath48 ) ,    @xmath40 theorem  2.4 of homei  ( 2014 ) ( for @xmath46 and @xmath49 for @xmath21 ) ,    @xmath40 theorem  1 of homei  ( 2013 ) ( for @xmath46 ) ;    and others ; see the references in homei  ( 2015 ) .",
    "the generating moment function of @xmath50 s in the first proof are @xmath51 by @xcite we have @xmath52 so , the components of the vector @xmath50 are independent and have gamma distributions , which proves the theorem .      we can write ( for @xmath21 ) @xmath53 and @xmath54 so , @xmath55 let us recall that @xmath56 s ( for @xmath57 and @xmath21 ) are independent random variables with gamma distributions , which implies the independence of the components of @xmath12 from @xmath58 s ( basu s theorem ) .",
    "for @xmath59 the theorem follows from ( * ? ? ?",
    "* lemma  1 ) .",
    "suppose the theorem holds for @xmath60 ( the induction hypothesis ) .",
    "we prove it for @xmath61 ( the induction conclusion ) : by dividing the both sides of @xmath62 by @xmath63 and using the induction hypothesis we have @xmath64 in which the right hand side holds by ( * ? ? ?",
    "* lemma  1 ) ( for @xmath59 ) .",
    "the general moments @xmath65 of @xmath17 are as follows : @xmath66 where @xmath67 denotes summation over all non - negative integers @xmath68 this equation can be rearranged as @xmath69 @xmath70 ( where @xmath71 ) and also @xmath72 by the dirichlet distribution we have @xmath73 and also @xmath74 and again by the dirichlet distribution @xmath75 so , by using @xmath76 @xmath77 @xmath78 @xmath79 @xmath80 by considering the fact that the sum of the dirichlet - multimonial distributions on their support equals to one , we have @xmath81 which is the general moment of the k - variate @xmath82 distribution , and since @xmath17 is a bounded random variable , its distribution is uniquely determined by its moments .",
    "thus the proof is complete .",
    "the distribution of the randomly linear combination @xmath24 is @xmath83 where @xmath18 are two - dimensional independent multivariate random vectors with @xmath84 distributions and the random vector @xmath22 is independent from @xmath27 and has the distribution @xmath85    let @xmath26 ( @xmath21 ) be independent random variables independent from @xmath27 that have the distribution @xmath86 , respectively .",
    "it can be seen , by some classic ways ( e.g. @xmath87^{\\sum_j\\alpha_j}$ ] from kubo & kuo & namli  ( 2013 ) , table  2 ) , that the distribution of @xmath88 is the same distribution of @xmath50 with the parameter @xmath89 .",
    "we can also write @xmath90 as @xmath91 and so we have @xmath92 in which @xmath93 has the gamma distribution with the parameter @xmath89 , and @xmath90 has the @xmath94 distribution with the parameter @xmath89 , and @xmath93 and @xmath17 are independent from each other .",
    "of course , one can define @xmath95 in such a way that @xmath96 , @xmath97 and @xmath98 .",
    "one can conclude that @xmath17 and @xmath99 have identical distributions by calculating the general moments @xmath100 of @xmath90 and @xmath101 , i.e. , @xmath102 .",
    "actually , the above proof also shows that :    let @xmath26 @xmath21 be independent random variables independent from @xmath27 that have the distribution @xmath86 , respectively , where @xmath9 s are independent from each other and have dirichlet distributions .",
    "if @xmath11 has a bounded support and the independent random variable @xmath93 has @xmath103 distribution such that @xmath104 , then @xmath11 and @xmath105 have identical distributions , where @xmath22 is independent from @xmath9 s and has dirichlet@xmath106 distribution .",
    "in this section , using theorem  [ main ] and corollary  [ cor ] , we prove some interesting mathematical facts . as an example consider the following differential equation for each @xmath60 ( cf .",
    "homei  ( 2014 ) ) : @xmath107 which could be of interest for some authors , who first guess the solution and then , by using techniques like leibniz differentiations or change of variables or integration by part , prove that it satisfies the equation ( by some long inductive arguments ) .",
    "let us recall that theorem  1 of homei  ( 2015 ) identifies the distribution of ( the @xmath108-dimensional ) @xmath109 from the distributions of @xmath110 s by means of the differential equation : @xmath111 where @xmath112 denotes the cumulative distribution function of a random variable @xmath93 and @xmath113 is defined by @xmath114 for @xmath115 in which @xmath116 stands for the support of @xmath112 .",
    "let the distribution of @xmath9 ( @xmath38 ) be arcsin , that is @xmath117 .",
    "also , let @xmath118 ( @xmath38 ) and @xmath119 .",
    "then from the equation  ( 2 ) we will have @xmath120 the solution of the equation  ( 3 ) identifies the stieltjes transformation of the distribution of @xmath109 .",
    "alternatively , from theorem  [ main ] ( or corollary  [ cor ] ) the distribution of @xmath121 is power semicircle ( see homei  ( 2014 ) for more details ) . since the stieltjes transformation of the power semicircle distribution is @xmath122 ( see e.g. arizmendi & perez - abreu  ( 2010 ) ) then by substituting it in the equation  ( 2 ) we will get the equation  ( 1 ) immediately .    as another example",
    "consider the moment generating function on the vector @xmath50 : @xmath123 using the double conditional expectation and the fact that the components of @xmath50 are independent with gamma distributions we have @xmath124 which proves proposition  4.4 of kerov & tsilevich  ( 2004 ) ( cf . also karlin & micchelli & rinott  ( 1986 ) , page  77 ) .",
    "some sporadic works of other authors have been unified here ; the main result ( theorem  [ main ] ) has several other different proofs ( available upon request ) each of which can have various applications ( the five proofs presented here are dedicated to all the family members of professor a.r .",
    "soltani ) .",
    "our method reveals the advantage of the method of stieltjes transforms for identifying the distribution of stochastic linear combinations , first used by van assche  ( 1987 ) and later generalized by others ."
  ],
  "abstract_text": [
    "<S> stochastic linear combinations of some random vectors are studied where the distribution of the random vectors and the joint distribution of their coefficients are dirichlet . </S>",
    "<S> a method is provided for calculating the distribution of these combinations which has been studied before by some authors . </S>",
    "<S> our main result is a generalization of some existing results with a simpler proof .     </S>",
    "<S> multivariate randomly weighted average ; multivariate stieltjes transform ; dirichlet distributions ; multivariate stable distributions ; randomly linear transformations ; dependent components ; lifetime . </S>"
  ]
}