{
  "article_text": [
    "one of the exciting developments in statistics during the last decade has been the development of the theory and methodologies for dealing with high - dimensional data . the term _ high dimension _ is primarily interpreted as meaning that the dimensionality of the observed multivariate data is comparable to the available number of replicates or subjects on which the measurements on the different variables are taken .",
    "this is often expressed in the asymptotic framework as @xmath3 , where @xmath0 denotes the dimension of the observation vectors ( forming a triangular array ) and @xmath2 the sample size .",
    "much of this development centered on understanding the behavior of the sample covariance matrix and especially its eigenvalues and eigenvectors , due to their role in dimension reduction , in estimation of population covariances and as building block of numerous inferential procedures for multivariate data .",
    "comprehensive reviews of this topic can be found in johnstone  @xcite and paul and aue  @xcite .",
    "one most notable high - dimensional phenomena associated with sample covariance matrices is that the sample eigenvalues do not converge to their population counterparts if dimension and sample sizes remain comparable even as the sample size increases . a formal way to express",
    "this phenomenon is through the use of the _ empirical spectral distribution _ ( esd ) , that is , the empirical distribution of the eigenvalues of the sample covariance matrix . the celebrated work of marenko and pastur @xcite shows that if one studies a triangular array of random vectors @xmath4 , whose components form independent , identically distributed ( i.i.d . ) random variables with zero mean , unit variance and finite fourth moment , then as @xmath5 such that @xmath6 , the esd of @xmath7 converges almost surely to a nonrandom probability distribution known as the marenko  pastur distribution . since this highly influential discovery",
    "a large body of literature under the banner of random matrix theory ( rmt ) has been developed to explore the properties of the eigenvalues and eigenvectors of large random matrices .",
    "one may refer to anderson et  al .",
    "@xcite , bai and silverstein  @xcite and tao  @xcite to study various aspects of this literature .",
    "many important classes of high - dimensional data , particularly those arising in signal processing , economics and finance , have the feature that in addition to the dimensional correlation , the observations are correlated in time .",
    "classical models for time series often assume a stationary correlation structure and use spectral analysis methods or methods built on the behavior of the sample autocovariance matrices for inference and prediction purposes . in spite of this , to our knowledge , no work exists that analyzes the behavior of the sample autocovariance matrices of a time series from a random matrix perspective , even though jin et  al .",
    "@xcite have dealt recently covered autocovariance matrices in the independent case .",
    "a  striking observation is that , in the high - dimensional scenario , the distribution of the eigenvalues of the symmetrized sample autocovariance of a given lag order tends to stabilize to a nondegenerate distribution even in the setting where the observations are i.i.d .",
    "this raises questions about the applicability of sample autocovariance matrices as diagnostic tools for determining the nature of temporal dependence in high - dimensional settings .",
    "thus a detailed study of the phenomena associated with the behavior of the esd of the sample autocovariance matrices when the observations have both dimensional and temporal correlation is of importance to gain a better understanding of the ways in which the dimensionality affects the inference for high - dimensional time series .",
    "all the existing work on high - dimensional time series dealing with the limiting behavior of the esd focuses on the sample covariance matrix of the data when @xmath4 are @xmath0-dimensional observations recorded in time and @xmath5 such that @xmath8 .",
    "this includes the works of jin et  al .",
    "@xcite , who assume the process @xmath9 has i.i.d .",
    "rows with each row following a causal arma process .",
    "pfaffel and schlemm  @xcite and yao  @xcite extend this framework to the setting where the rows are arbitrary i.i.d .",
    "stationary processes with short - range dependence .",
    "zhang  @xcite , paul and silverstein  @xcite and el karoui  @xcite , under slightly different assumptions , consider the limiting behavior of the esd of the sample covariance when the data matrices are of the form @xmath10 where @xmath11 and @xmath12 are positive semidefinite matrices , and @xmath13 has i.i.d .",
    "entries with zero mean , unit variance and finite fourth moment .",
    "this model is known as the separable covariance model , since the covariance of the data matrix is the kronecker product of @xmath11  and  @xmath12 .",
    "if the rows indicate spatial coordinates and columns indicate time instances , then this model implies that spatial ( dimensional ) and temporal dependencies in the data are independent of each other .",
    "the work of this paper is also partly related to the results of hachem et  al .",
    "@xcite , who prove the existence of the limiting esd for sample covariance of data matrices that are rectangular slices from a bistationary gaussian process on @xmath14 .    in this paper",
    ", the focus is on a class of time series known as linear processes [ or @xmath15 processes ] .",
    "the assumptions to be imposed in section  [ secmain ] imply that , up to an unknown rotation , the coordinates of the linear process , say @xmath9 , are uncorrelated stationary linear processes with short range dependence . extending the work of jin et  al .",
    "@xcite to the time series case , the goal is to relate the behavior of the esd of the lag-@xmath16 symmetrized sample autocovariances , defined as @xmath17 , with @xmath18 denoting complex conjugation , to that of the spectra of the coefficient matrices of the linear process when @xmath5 such that @xmath8 .",
    "this requires assuming certain stability conditions on the joint distribution of the eigenvalues of the coefficient matrices which are described later .",
    "the class of models under study here includes the class of causal autoregressive moving average ( arma ) processes of finite orders satisfying the requirement that the coefficient matrices are simultaneously diagonalizable and the joint empirical distribution of their eigenvalues ( when diagonalized in the common orthogonal or unitary basis ) , converges to a finite - dimensional distribution .",
    "the results are expressed in terms of the _ stieltjes transform _ of the esd of the sample autocovariances .",
    "specifically , it is shown that the esd of the symmetrized sample autocovariance matrix of any lag order converges to a nonrandom probability distribution on the real line whose stieltjes transform can be expressed in terms a unique _ stieltjes kernel_. the definition of the stieltjes kernel involves integration with respect to the limiting joint empirical distribution of the eigenvalues of the coefficient matrices as well as the spectral density functions of the one - dimensional processes that correspond to the coordinates of the process @xmath9 , after rotation in the common unitary or orthogonal matrix that simultaneously diagonalizes the coefficient matrices .",
    "thus this result neatly ties the dimensional correlation , captured by the eigenvalues of the coefficient matrices , with the temporal correlation , captured by the spectral density of the coordinate processes .",
    "the main contributions of this paper are the following : ( i ) a framework is provided for analyzing the behavior of symmetrized autocovariance matrices of linear processes ; ( ii ) for linear processes satisfying appropriate regularity conditions , a  concrete description of the limiting stieltjes transform is given in terms of the limiting joint esd of the coefficient matrices and the spectral density of the coordinate processes after a rotation of the coordinates of the observation .",
    "extensions to these main results are ( iii ) the characterization of the behavior of the esd of autocovariances of linear filters applied to the observed process ; ( iv ) the description of the esds of a class of tapered estimates of the spectral density operator of the observed process that can be used to analyze the long - run variance and spectral coherence of the process .",
    "these contributions surpass the work in the existing literature dealing with high - dimensionality effects for time series in two different ways .",
    "first , the class of time series models that are analyzed in detail encompasses the setting of stationary i.i.d .",
    "rows studied by jin et  al .",
    "@xcite , pfaffel and schlemm  @xcite and yao  @xcite , as well as the setting of separable covariance structure studied by zhang  @xcite , paul and silverstein  @xcite and el karoui  @xcite .",
    "the proofs of the main results also require more involved arguments .",
    "they are partly related to the constructions in hachem et  al .",
    "@xcite , but additional technical arguments are needed to go beyond gaussanity .",
    "the results are also related to the work of hachem et  al .",
    "@xcite , who studies limiting spectral distributions of covariance matrices for data with a given variance profile .",
    "the connection is through the fact that after an approximation of lag operators by circulant shift matrices , and appropriate row and column rotations , the data matrix in our setting can be equivalently expressed as a matrix with independent entries and with a variance profile related to the spectral densities of the different coordinates of the time series .",
    "second , the framework allows for a unified analysis of the esd of symmetrized autocovariance matrices of all lag orders as well as that of the tapered spectral density operator .",
    "none of the existing works deals with the behavior of autocovariances for time series ( note again that jin et  al .",
    "@xcite treat the i.i.d .",
    "case ) , and this analysis requires a nontrivial variation of the arguments used for dealing with the stieltjes transform of the sample covariance matrix .",
    "moreover , even though we stick to the setting where the coefficient matrices are hermitian and simultaneously diagonalizable , the main steps in the derivation , especially the construction of a `` deterministic equivalent '' of the resolvent of the symmetrized autocovariance matrix , is very general and can be applied to linear processes with structures that go beyond the settings studied in this paper , for example , when the simultaneous diagonalizability of the coefficient matrices is replaced by a form of simultaneous block diagonalizability , even though the latter is not pursued in this paper due to lack of clear statistical motivation .",
    "the existence and uniqueness of the limits of the resulting equations and their solutions is the key to establishing the existence of liming esds of the autocovariances .",
    "this step requires certain regularity conditions on the coefficient matrices and is not pursued beyond the setting described in section  [ secmain ] . a number of potential applications , for example",
    ", to problems in signal processing , and dynamic and static factor models , are discussed in section  [ secexandapp ] .",
    "the remaining sections of the paper are organized as follows .",
    "extensions of the main results in section  [ secmain ] are discussed in section  [ secex ] .",
    "the outcomes of a small simulation study are reported in section  [ secsimulations ] , while the proofs of the main results are provided in sections  [ secstructure][secpreal ] .",
    "several technical lemmas are collected in the online supplemental material ( sm ) @xcite .",
    "let @xmath19 denote the set of integers .",
    "a sequence of random vectors @xmath9 with values in @xmath20 is called a linear process or moving average process of order infinity , abbreviated by the acronym ma(@xmath21 ) , if it has the representation @xmath22 where @xmath23 denotes a sequence of independent , identically distributed dimensional random vectors whose entries are independent and satisfy @xmath24=0 $ ] , @xmath25=1 $ ] and @xmath26<\\infty$ ] , where @xmath27 denotes the @xmath28th coordinate of @xmath29 . in the complex - valued case",
    "this is meant as @xmath30=\\mathbb{e}[\\operatorname { im}(z_{jt})^2]=1/2 $ ] .",
    "it is also assumed that real and imaginary parts are independent .",
    "let further @xmath31 , the identity matrix . to ensure finite fourth moments for @xmath32 and a sufficiently fast decaying weak dependence structure , assumption  [ asmainfty ] below lists several additional conditions imposed on the coefficient matrices @xmath33 .",
    "the results presented in this paper are concerned with the behavior of the symmetrized lag-@xmath16 sample autocovariances @xmath34 assuming observations for @xmath4 are available . for @xmath35 ,",
    "this definition gives the covariance matrix @xmath36 discussed in the .",
    "note that in order to make predictions in the linear process setting , it is imperative to understand the second - order dynamics which are captured in the population autocovariance matrices @xmath37 $ ] , @xmath38 , as all of the popular prediction algorithms such as the durbin  levinson and innovations algorithms are starting from there ; see , for example , ltkepohl @xcite .",
    "the set - up in ( [ eqmainfty ] ) provides a ( strictly ) stationary process and consequently the definition of @xmath39 does not depend on the value of @xmath40 .",
    "the main goal of this paper is to analyze the behavior of the matrices @xmath41 , which can be viewed as a special sample counterpart to the corresponding @xmath42 , in the high - dimensional setting for which @xmath43 is a function of the sample size such that @xmath44 thereby extending the above mentioned marenko  pastur - type results to more general time series models and to autocovariance matrices",
    ". we can weaken requirement ( [ eqpn ] ) to `` @xmath45 bounded away from zero and infinity , '' in which case , the asymptotic results hold for subsequences @xmath46 satisfying @xmath47 converging to a positive constant @xmath48 , provided that the structural assumptions on the model continue to hold .",
    "let then @xmath49 denote the empirical spectral distribution ( esd ) of @xmath50 given by @xmath51 where @xmath52 are the eigenvalues of @xmath41 .",
    "the proof techniques for establishing large - sample results about @xmath53 are based on exploiting convergence properties of stieltjes transforms , which continue to play an important role in verifying theoretical results in rmt ; see , for example , paul and aue @xcite for a recent summary .",
    "the stieltjes transform of a distribution function @xmath54 on the real line is the function @xmath55 where @xmath56 denotes the upper complex half plane .",
    "it can be shown that @xmath57 is analytic on @xmath58 and that the distribution function @xmath54 can be reconstructed from @xmath57 using an inversion formula ; see @xcite . in order to make statements about @xmath49",
    ", the following additional assumptions on the coefficient matrices @xmath33 are needed .",
    "let @xmath59 and @xmath60 denote the positive and nonnegative integers , respectively .",
    "[ asmainfty ] ( a ) the matrices @xmath61 are simultaneously diagonalizable random hermitian matrices , independent of @xmath23 and satisfying @xmath62 for all @xmath63 and large @xmath0 with @xmath64 note that one can set @xmath65 .",
    "\\(b ) there are continuous functions @xmath66 , @xmath63 , such that , for every  @xmath0 , there is a set of points @xmath67 , not necessarily distinct , and a unitary @xmath68 matrix @xmath69 such that @xmath70 and @xmath71 .",
    "[ note that the functions @xmath72 are allowed to depend on @xmath43 as long as they converge to continuous functions as @xmath73 uniformly . ]",
    "\\(c ) with probability one , @xmath74 , the esd of @xmath75 , converges weakly to a nonrandom probability distribution function @xmath76 on @xmath77 as @xmath78 .",
    "let @xmath79 $ ] denote the matrix collecting the coefficient matrices of the linear process @xmath9 .",
    "define the transfer functions @xmath80 as well as the power transfer functions @xmath81 note that the contribution of the temporal dependence of the underlying time series on the asymptotic behavior of @xmath53 is quantified through @xmath82 . specifically ,",
    "@xmath83 with @xmath84 as in part ( b ) of assumption  [ asmainfty ] is ( up to normalization ) the spectral density of the @xmath28th coordinate of the process rotated with the help of the unitary matrix @xmath69 . with these definitions ,",
    "the main results of this paper can be stated as follows .",
    "[ thesdmainfty ] if a complex - valued linear process @xmath32 with independent , identically distributed @xmath27 , @xmath85=0 $ ] , @xmath30=\\mathbb { e}[\\operatorname{im}(z_{jt})^2]=1/2 $ ] , @xmath86 and @xmath87 are independent , and @xmath88<\\infty$ ] , satisfies assumption  [ asmainfty ] , then , with probability one and in the high - dimensional setting ( [ eqpn ] ) , @xmath53  converges to a nonrandom probability distribution @xmath89 with stieltjes transform @xmath90 determined by the equation @xmath91^{-1}\\,df^{\\mathbf{a}}(\\lambda),\\ ] ] where @xmath92\\to\\mathbb{c}^+$ ] is a stieltjes kernel ; that is , @xmath93 is the stieltjes transform of a measure with total mass @xmath94 for every fixed @xmath95 $ ] , whenever @xmath96 .",
    "moreover , @xmath97 is the unique solution of @xmath98\\\\[-12pt]\\nonumber & & \\qquad = \\int\\biggl[\\frac{1}{2\\pi}\\int _ 0^{2\\pi } \\frac{\\cos(\\tau\\nu^\\prime)h(\\lambda,\\nu^\\prime)}{1+c\\cos(\\tau \\nu^\\prime)k_\\tau(z,\\nu^\\prime)}\\,d\\nu^\\prime - z \\biggr]^{-1}h(\\lambda,\\nu ) \\,df^{\\mathbf{a}}(\\lambda),\\hspace*{-20pt}\\end{aligned}\\ ] ] subject to the restriction that @xmath97 is a stieltjes kernel . otherwise , if @xmath99 , then @xmath100 is identically zero on @xmath58 and so still satisfies ( [ eqktau ] ) .",
    "[ coesdmainfty ] if a real - valued linear process @xmath32 with independent , identically distributed real - valued @xmath27 , @xmath24=0 $ ] , @xmath101=1 $ ] and @xmath102<\\infty$ ] , satisfies assumption  [ asmainfty ] with real symmetric coefficient matrices @xmath61 , then the result of theorem  [ thesdmainfty ] is retained .    when each coefficient matrix @xmath33 is a multiple of the identity matrix ,",
    "that is , @xmath103 where @xmath104 is a sequence of real numbers satisfying @xmath105 , the result of theorem  [ thesdmainfty ] reduces to the results obtained in pfaffel and schlemm  @xcite and yao  @xcite .",
    "one can relax the assumption of simultaneous diagonalizability of the coefficient matrices of the linear process to certain forms of near - simultaneous diagonalizability , so that the conclusions of theorem  [ thesdmainfty ] continue to hold for linear processes where the ma coefficients are toeplitz matrices whose entries decay away from the diagonal at an appropriate rate .",
    "specifically , if @xmath106 is the toeplitz matrix with @xmath28th row equaling @xmath107 , for the bi - infinite sequence @xmath108 satisfying the condition @xmath109 for some @xmath110 , which in particular implies assumption  [ asmainfty](a ) by the gershgorin theorem , then the existence of the limiting esd of symmetrized autocovariance matrices can be proved . for brevity , instead of giving a thorough technical argument",
    ", we only provide the main idea of proof .",
    "first , the @xmath15 series is approximated by an @xmath111 series with @xmath112 , using arguments along the line of section  [ secintlin ] .",
    "second , banding with bandwidth @xmath113 is applied to the coefficient matrices @xmath106 .",
    "it can be shown through an application of norm inequality , that the limiting spectral behavior is unchanged under the banding so long as @xmath114 under  ( [ eqpn ] ) .",
    "third , circulant matrices are constructed from the banded toeplitz matrices by periodization .",
    "the resulting matrices are therefore simultaneously diagonalizable , and the eigenvalues of the @xmath115th approximate coefficient matrix approximate the transfer function of the sequence @xmath116 .",
    "the limiting spectral behavior is seen to be unchanged after the use of the rank inequality so long as @xmath117 under ( [ eqpn ] ) .",
    "the rest of the derivations follow the arguments in the proof of theorem  [ thesdmainfty ] .",
    "while this particular result is related to the work of hachem et  al .",
    "@xcite , who study the convergence of the empirical distribution of the sample covariance matrix of rectangular slices of bistationary gaussian random fields , @xcite does not cover the transition to non - gaussian processes or the spectral behavior of sample autocovariance matrices .",
    "several extensions of theorem  [ thesdmainfty ] are discussed in section  [ secex ] below .",
    "the proof steps needed in order to verify the main result are outlined in section  [ secstructure ] , and the details are in sections  [ secgauss][secpreal ] .",
    "the online sm @xcite contains additional technical lemmas .",
    "in this section , let @xmath9 be the causal arma(@xmath119 ) process given by the stochastic difference equations @xmath120 where @xmath121 and @xmath122 are , respectively , the matrix - valued autoregressive and moving average polynomials in the lag operator @xmath123 for which it is assumed that @xmath124 and @xmath125 .",
    "moreover , @xmath126 with entries possessing finite fourth moments . under these conditions",
    "@xmath9 admits the ma(@xmath21 ) representation @xmath127 with @xmath128 .",
    "assume now further that @xmath129 and @xmath130 are simultaneously diagonalizable",
    ". then @xmath131 and @xmath132 , where @xmath133 and @xmath134 such that @xmath135 and @xmath136 .",
    "with regard to assumption  [ asmainfty ] , let @xmath137 .",
    "part ( c ) of the assumption then requires almost sure weak convergence of the esd of @xmath138 to a nonrandom probability distribution function on @xmath139 .",
    "moreover , using that for each coordinate , @xmath140 it follows that @xmath141 with @xmath142 and @xmath143 for @xmath144 .",
    "this illustrates part ( b ) of assumption  [ asmainfty ] .",
    "the summability conditions stated in part ( a ) are clearly satisfied .",
    "generalization to arbitrary causal arma models follows in a similar fashion .      in this section ,",
    "the situation of time series with independent rows is considered .",
    "our results describe the limiting esd of the symmetrized sample autocovariances in the setting where the @xmath28th row of the time series , denoted @xmath145 , is given by @xmath146 where : ( i ) the @xmath27 s are independent , identically distributed real- or complex - valued random variables with mean zero , unit variance and finite fourth moments ; ( ii ) the @xmath72 s are continuous functions from @xmath147 satisfying @xmath148 and the summability condition @xmath149 ; ( iii ) the @xmath150 s are i.i.d .",
    "realizations from an @xmath151-dimensional probability distribution denoted by @xmath152 . if the supremum in condition ( ii ) is taken over @xmath77 , condition ( iii ) can be weakened to require that the empirical distribution of the @xmath150 s converges almost surely to a nonrandom distribution @xmath153 .",
    "let @xmath154 .",
    "then the empirical distributions of the eigenvalues of the lag-@xmath16 symmetrized autocovariance matrices converge almost surely to a nonrandom probability distribution @xmath89 with stieltjes transform @xmath90 determined by equations ( [ eqstau ] ) and ( [ eqktau ] ) , where @xmath100 is as in theorem  [ thesdmainfty ] .",
    "this is in the spirit of the works of jin et  al .",
    "@xcite , pfaffel and schlemm @xcite and yao @xcite , who studied the sample covariance case with @xmath155 for all @xmath156 , hachem et  al .",
    "@xcite , who considered the sample covariance case for stationary gaussian fields and jin et  al .",
    "@xcite , who studied the symmetrized sample autocovariance case with @xmath157 and @xmath158 for @xmath159 ( i.e. , when the @xmath160 s are i.i.d . with zero mean and unit variance ) .",
    "the results derived here can be useful in dealing with a number of important statistical questions .",
    "signal detection in a noisy background is one of the most important problems in signal processing and communications theory .",
    "often the observations are taken in time , and the standard assumption is that the noise is i.i.d . in time , referred to as white noise . however , in spatio - temporal signal processing , it is quite apt to formulate the noise as `` colored '' or correlated in time , as well as in the spatial dimension . the proposed model for the time series is a good prototype for such a noise structure",
    ". thus the problem of detecting a low - dimensional signal embedded in dimensional noise , for example , through a factor model framework , can be effectively addressed by making use of the behavior of the esds of autocovariances of the noise .",
    "another potential application of the results is in building diagnostic tools for high - dimensional time series . by focusing on the esds of the autocovariances for various lag orders , or that of a tapered estimate of the spectral density operator",
    ", one can infer about the nature of dependence , provided the model assumptions hold .",
    "the proposed model also provides a broad class of alternatives for the hypothesis of independence of observations in settings where those observations are measured in time .",
    "finally , in practical applications , it is of interest whether the spectrum of the coefficient matrices of the linear process can be estimated from the data .",
    "the equations for the limiting stieltjes kernel and its relation to the stieltjes transform of the autocovariance matrices provide a tool for attacking this problem .",
    "this aspect has been explored in the ph.d .",
    "thesis of the first author @xcite and the methodology will be reported elsewhere .",
    "forni and lippi @xcite describe a class of time series models that captures the subject specific variations of microeconomic activities .",
    "this class of models , referred to as dynamic factor models ( dfm ) , has proved immensely popular in the econometrics community and beyond .",
    "dfms have , for example , been used for describing the stock returns in @xcite , forecasting national accounts in @xcite , modeling portfolio allocation in @xcite and modeling psychological development in @xcite , as well as in many other applications .",
    "important theoretical and inferential questions regarding dfms have been investigated in a series of papers by forni and lippi @xcite , forni et  al .",
    "@xcite and stock and watson @xcite , to name a few .",
    "dfms have also shown early promise for applications to other interesting multivariate time series problems such as the study of fmri data .",
    "a dfm can be described as follows . as in @xcite , let @xmath161 be the response corresponding to the @xmath28th individual / agent at time @xmath40 , modeled as @xmath162 the model specifies that @xmath161 is determined by a small , fixed number of underlying common factors @xmath163 and their lags , determined by the polynomials @xmath164 in the lag - operator @xmath123 , plus an idiosyncratic component @xmath165 assumed independent across individuals .",
    "typically , @xmath166 is taken to be a stationary linear processes , independent across @xmath28 .",
    "one of the key questions pertaining to dfm is the determination of the number of dynamic factors .",
    "this question has been investigated by bai and ng @xcite , stock and watson @xcite and hallin and lika @xcite . unlike in pca , here",
    "one has to deal with the additional problem of detecting the lag orders of the dynamic factors .",
    "this can be approached through the study of the behavior of the extreme eigenvalues of the sample autocovariance matrices as in jin et  al . @xcite .",
    "the issue becomes even more challenging when the dimensionality of the problem increases .",
    "in such settings , one expects that a form of phase transition phenomenon , well known in the context of a high - dimensional static factor model ( or spiked covariance model ) with i.i.d .",
    "observations ( see , e.g. , baik and silverstein @xcite ) , will set in .",
    "in particular , as jin et  al .",
    "@xcite argue , a dynamic factor will be detectable from the data only if the corresponding total signal intensity , as measured , for example , by the sum of the variances of the factor loadings , is above a threshold . moreover , the number of eigenvalues that lie outside the bulk of the eigenvalues of the symmetrized sample autocovariance of a certain lag order provide information about the lag order of the dfm . driven by the analogy with the static factor model with i.i.d .",
    "observations , it is expected that the detection thresholds will depend on the dimension - to - sample size ratio , as well as the behavior of the bulk spectrum of the autocovariances of the idiosyncratic terms at specific lag orders , including the support of the limiting esd . equation ( [ eqidiot ] ) in section  [ secind ] constitutes the `` null '' model for the dfm in which the dynamic common factors are absent . therefore follow - up studies on the different aspects of the esd of the symmetrized sample autocovariances of such processes will be helpful in determining the detection thresholds and estimation characteristics of high - dimensional dfms .",
    "onatski @xcite describes a model for production @xmath167 , at time @xmath40 , involving @xmath0 different industries in an economy that is given by the equations @xmath168 model  ( [ eqproductionmodel ] ) is a static factor model in which @xmath169 vectors @xmath170 denote the ( unobserved ) common static factors , @xmath171 denote the ( unobserved ) factor scores consisting of independent time series corresponding to different factors @xmath172 and @xmath173 denote the @xmath169 vectors of idiosyncratic components .",
    "the entries of the matrix @xmath174 indicate the interactions among the different industries . in the following an enhanced version of the model",
    "is considered where the economy is thought to be divided into a finite number of distinct sectors for which the interaction across the sectors is assumed `` weak '' in a suitable sense to be described .",
    "in addition , the assumption of separable covariance structure of the @xmath173 made in @xcite is relaxed by requiring instead that the temporal variation in @xmath173 for all the industrial units within a sector is the same and is stationary in time .",
    "this assumption means that the component of the vector @xmath173 corresponding to a particular sector has a separable covariance structure with stationary time variation , and the components corresponding to different sectors are independent .",
    "specifically , if there are @xmath175 sectors , we can divide @xmath174 into @xmath176 block matrices @xmath177 = \\widetilde{\\mathbf{w } } + \\delta,\\ ] ] where @xmath178 . if the sectors have no interaction at all , that is , if @xmath179 for all @xmath180 , then the corresponding data model is an instance of a blockwise separable covariance model .",
    "`` weak interaction '' means that the norms of the off - diagonal blocks in the matrix @xmath174 are small .",
    "more precisely , if @xmath181 as @xmath182 , then the limiting esd of the symmetrized autocovariances for the data matrix @xmath183 is the same as that of @xmath184 obtained by replacing @xmath174 by @xmath185 in ( [ eqproductionmodel ] ) . under the assumption of a linear process structure on the different components of @xmath173 , and a natural requirement on the stability of the singular values of  @xmath185 , the existence and characterization of the limiting esds of the symmetrized autocovariances of @xmath186 can be dealt within the framework studied in section  [ secmain ] .",
    "these limiting esds will help in determining the detection thresholds for the static factors , or even dynamic factors , if the model were to be enhanced further .",
    "this section discusses three different extensions of the main result .",
    "the arguments for the proof are similar to that of the proof of theorem  [ thesdmainfty ] and hence only a brief outline is provided .",
    "moreover , the results stated here apply to both real- and complex - valued cases , the only difference being that , in the former case , the relevant matrices are real symmetric while , in the latter case , they are hermitian .",
    "the first extension involves a rescaling of the process defined in ( [ eqmainfty ] ) .",
    "thus it is assumed that @xmath187 where the processes @xmath23 and matrices @xmath188 satisfy assumption  [ asmainfty ] , and the matrix @xmath189 is the square root of a @xmath68 positive semidefinite hermitian ( or symmetric ) matrix @xmath12 satisfying the following assumption .",
    "[ asscaling ] let @xmath190 be as in assumption  [ asmainfty ] .",
    "then @xmath191 where @xmath192 is continuous and bounded on @xmath77 , and @xmath193 is nonempty .",
    "as before , @xmath53 is defined to be the esd of the symmetrized autocovariance @xmath194 of lag order @xmath16 . if the linear process @xmath9 defined through ( [ eqxtscaled ] ) satisfies assumptions  [ asmainfty ] and  [ asscaling ] , then the statement of theorem  [ thesdmainfty ] holds with the function @xmath195 replaced by @xmath196 .",
    "the second extension is about the existence and description of the limiting esd of the autocovariances of linear filters of the process @xmath9 defined through  ( [ eqxtscaled ] ) .",
    "a linear filter of this process is of the form @xmath197 where @xmath198 is a sequence of real numbers for which the following summability condition is needed .",
    "the sequence @xmath199 satisfies @xmath200 .",
    "if the linear process @xmath9 defined through ( [ eqxtscaled ] ) satisfies assumptions  [ asmainfty ] and [ asscaling ] , then the statement of theorem  [ thesdmainfty ] holds with the function @xmath82 replaced by @xmath201 , where @xmath202 , @xmath95 $ ] .",
    "this result follows using the properties of convolution and fourier transform .",
    "the third extension is about estimation of the spectral density operator @xmath203 , \\qquad\\eta\\in[0,2\\pi].\\ ] ] it is well known from classical multivariate time series analysis ( see , e.g. , chapter  10 of hamilton  @xcite ) that the `` natural '' estimator that replaces the population autocovariance @xmath204 $ ] by the corresponding sample autocovariance may not be positive definite . in order to obtain positive definite estimators and a better bias - variance trade - off , it is therefore standard in the literature to consider certain tapered estimators with standard choices given , for example , by the bartlett and parzen kernels as described in @xcite . in the following , the behavior of a class of tapered estimators of @xmath205 , which are given by @xmath206,\\ ] ]",
    "is studied , where @xmath207 is a sequence of even functions and the quantities @xmath208 are known as tapering weights for which the following restriction is imposed .    [ astapering ] ( i ) the even functions @xmath207 are such that @xmath209 for @xmath210 ; ( ii ) there exists an even function @xmath211 such that @xmath212 as @xmath213 and @xmath214 for some @xmath215 , for all @xmath216 ; ( iii ) @xmath217 .",
    "an implication of this assumption is that the function @xmath218 defined by @xmath219 is well defined and is uniformly lipschitz , and @xmath220 converges to @xmath221 uniformly in @xmath222 .",
    "examples of kernels @xmath223 are @xmath224 for @xmath225 and @xmath226 for @xmath227 .",
    "it can be seen from assumption  [ astapering ] that in the high - dimensional setting under consideration here , standard choices for tapering weights , such as those given by the bartlett and parzen kernels , are ruled out .",
    "now the following generalization of theorem  [ thesdmainfty ] is obtained .",
    "[ thesdmainftyspectraldensity ] suppose that the linear process @xmath9 defined through ( [ eqxtscaled ] ) satisfies assumptions  [ asmainfty ] and  [ asscaling ] , and that the estimated spectral density operators @xmath228)$ ] are defined by ( [ eqtaperedspectraldensity ] ) with tapering weights satisfying assumption  [ astapering ] .",
    "then , with probability one and in the high - dimensional setting  ( [ eqpn ] ) , for every @xmath229 $ ] , the esd of @xmath230 converges weakly to a probability distribution @xmath231 with stieltjes transform @xmath232 determined by the equation @xmath233^{-1 } \\,df^{\\mathbf{a}}(\\lambda),\\ ] ] where @xmath234\\to\\mathbb { c}^+$ ] is a stieltjes kernel ; that is , @xmath235 is the stieltjes transform of a measure with total mass @xmath236 for every fixed @xmath95 $ ] , whenever @xmath96 .",
    "moreover , @xmath237 is the unique solution to @xmath238^{-1 } \\\\ & & \\quad\\qquad{}\\hspace*{7pt}\\times g_{\\mathbf{b}}(\\lambda)h(\\lambda,\\nu ) \\,df^{\\mathbf{a}}(\\lambda),\\nonumber\\end{aligned}\\ ] ] subject to the restriction that @xmath237 is a stieltjes kernel .",
    "else , if @xmath99 , @xmath239 is identically zero on @xmath58 and so still satisfies the latter equation .",
    "section  [ secspecdens ] outlines the main argument needed to prove theorem  [ thesdmainftyspectraldensity ] .",
    "observations . left panel : @xmath240 ; right panel : @xmath241 .",
    "top panel : @xmath242 ; middle panel : @xmath243 ; bottom panel : @xmath244 .",
    "red curve : lsd for i.i.d . ; black curve : lsd for @xmath245 .",
    "esds are corresponding to @xmath246 [ deep blue for i.i.d . , light green for @xmath245 ] and @xmath248 [ light blue for i.i.d .",
    ", orange for @xmath245 ] . ]    in this section , a small simulation study is conducted to illustrate the behavior of the lsd ( limiting esd ) of the symmetrized sample autocovariances @xmath41 for different lag orders @xmath16 when the observations are i.i.d .",
    "( @xmath249 ) versus when they come from an @xmath245 process ( @xmath250 ) with a symmetric coefficient matrix @xmath251 . in this study ,",
    "two different sets of @xmath252 are chosen such that @xmath253 and @xmath241 , respectively , for @xmath246 and 50 .",
    "also , for the @xmath245 case , the esd of @xmath254 is chosen to be @xmath255 , where @xmath256 denotes the degenerate distribution at @xmath257 .",
    "the distribution of the @xmath27 s is chosen to be i.i.d .",
    "@xmath258 . in figure",
    "[ figsimul2050 ] , the esd of @xmath41 is plotted for the lags @xmath259 , for one random realization corresponding to each setting . the theoretical limits ( c.d.f . ) for all cases are plotted as solid smooth curves [ red for i.i.d . and",
    "black for @xmath245 ] .",
    "these c.d.f.s are obtained through numerically inverting the stieltjes transform @xmath90 of the lsd of @xmath41 using the inversion formula for stieltjes transforms ; cf .",
    "@xcite , @xcite . here",
    "@xmath90 is obtained from equations ( [ eqstau ] ) and ( [ eqktau ] ) where @xmath260 and @xmath261 .",
    "given @xmath262 , the stieltjes kernel @xmath263 for the @xmath245 case is solved numerically by using calculus of residues .",
    "the computational algorithm can be found in the first author s ph.d .",
    "thesis @xcite .",
    "the graphs clearly show the distinction in behavior of esd of symmetrized sample autocovariances between the i.i.d . and the @xmath245 processes .",
    "the lsds of @xmath41 for the i.i.d .",
    "case for @xmath243 and 2 are the same , which follows immediately from equations ( [ eqstau ] ) and ( [ eqktau ] ) that reduce to a single equation since the i.i.d .",
    "case corresponds to @xmath264 .",
    "the behavior of the lsds in the @xmath245 case is distinctly different .",
    "it can be shown that lsds of @xmath41 in the @xmath245 case converge to the lsd for @xmath265 in the i.i.d .",
    "case as @xmath16 becomes larger . owing to space constraints ,",
    "the graphical displays for higher lags are omitted .",
    "another important feature is that the lsds approximate the esds quite well even for @xmath0 as small as 20 , indicating a fast convergence .",
    "in this section , the overall proof strategy is briefly outlined , and the intuition behind the individual steps is developed for simpler first - order moving average , @xmath245 , time series .",
    "the key ideas in the proof of theorem  [ thesdmainfty ] consist of showing that :    * there is a unique stieltjes kernel solution @xmath97 to equation ( [ eqktau ] ) ; * almost surely , the stieltjes transform of @xmath53 , say , @xmath266 converges pointwise to a stieltjes transform @xmath90 which will be identified with @xmath89 ; * @xmath53 is tight .    to achieve the second item",
    ", one can argue as follows .",
    "first , replace the original linear process observations @xmath4 with transformed vectors @xmath267 that are serially independent .",
    "second , replace the symmetrized lag-@xmath16 autocovariance matrix @xmath41 by a transformed version @xmath268 built from @xmath267 . a heuristic formulation for the simpler gaussian @xmath245 case is given below in some detail .",
    "once these two steps have been achieved , the proof proceeds by verifying some technical conditions with the help of classical rmt results , available , for example , in the monograph bai and silverstein  @xcite . in the following ,",
    "let @xmath32 denote the @xmath0-dimensional @xmath245 process given by the equations @xmath269 where @xmath23 are assumed complex gaussian in addition to the requirements of section  [ secmain ] .",
    "for this time series , the conditions imposed through assumption  [ asmainfty ] simplify considerably with part ( a ) reducing to the condition that the eigenvalues of @xmath270 be uniformly bounded and part ( b ) being satisfied by choosing @xmath271 as the identity . moreover , @xmath272 $ ] , @xmath273 , @xmath274 , @xmath275 and @xmath276 , implying for each @xmath277 , @xmath82 is the spectral density ( up to normalization ) of a univariate @xmath245 process with parameter @xmath277 .",
    "the transformation to serial independence requires two steps , the first consisting of an approximation of the lag operator by a circulant matrix and the second of a rotation using the complex fourier basis to achieve independence .",
    "accordingly , let @xmath278 \\quad\\mbox{and}\\quad{\\tilde{\\mathbf{l}}}=[e_n\\dvtx e_1\\dvtx\\cdots\\dvtx e_{n-1}]\\ ] ] be the @xmath279 lag operator and its approximating circulant matrix , respectively , where @xmath280 denotes the @xmath2-dimensional zero vector and @xmath281 the @xmath28th canonical unit vector in @xmath282 taking the value 1 in the @xmath28th component and 0 elsewhere . since @xmath283 is a circulant matrix , its spectral decomposition is @xmath284 , @xmath285 , where @xmath286 , @xmath287 and @xmath288 the vector whose @xmath28th entry is @xmath289 .",
    "it follows that @xmath283 diagonalizes in the complex fourier basis with the usual fourier frequencies .",
    "let @xmath290 and @xmath291 $ ] denote the corresponding eigenvalue and eigenvector matrices , so that @xmath292 .",
    "using @xmath293 $ ] and @xmath294 $ ] , the @xmath245 process ( [ eqma1 ] ) can be transformed into @xmath295 where @xmath296 constitutes a redefinition of @xmath297 such that the first column is changed to @xmath298 , while all other columns are as in the original data matrix @xmath297 .",
    "rotating in the complex fourier basis , the observations are transformed again into the vectors @xmath267 given by @xmath299={\\mathbf{x}}_1 { \\mathbf{u}}_{{\\tilde{\\mathbf{l}}}}. \\ ] ] observe that @xmath300 has independent columns . to see this ,",
    "note first that @xmath301 possesses the same distribution as @xmath302 , since @xmath302 has ( complex ) gaussian entries and @xmath303 is a unitary matrix .",
    "write then @xmath304,\\ ] ] where @xmath305 $ ] , and independence of the columns ( and thus serial independence ) follows .",
    "note also that @xmath306 and consequently @xmath307 , using the transfer function @xmath308 defined in ( [ eqtransferpsi ] ) .",
    "the vectors @xmath267 give rise to approximations @xmath268 to the lag-@xmath16 symmetrized autocovariance matrices @xmath41 ; in particular @xmath268 and @xmath41 will be shown to have the same large - sample spectral behavior , irrespective of the distribution of the entries .",
    "so let @xmath309^\\tau\\bigr){\\mathbf{x}}_1^ * = \\frac{1}n \\sum_{t=1}^n\\cos(\\tau \\nu_t){\\tilde{x}}_t{\\tilde{x}}_t^*,\\ ] ] where the latter equality follows from several small computations using the quantities introduced in the preceding paragraph .",
    "now , @xmath310^\\tau\\bigr){\\mathbf{x}}_1^ * \\biggr ) \\\\ & & { } + \\biggl(\\frac { 1}{2n}{\\mathbf{x}}_1 \\bigl({\\mathbf{l}}^\\tau+\\bigl [ { \\mathbf{l}}^*\\bigr]^\\tau\\bigr){\\mathbf{x}}_1^*-\\frac { 1}{2n } { \\mathbf{x}}_1 \\bigl({\\tilde{\\mathbf{l}}}^\\tau+\\bigl[{\\tilde{\\mathbf{l}}}^*\\bigr]^\\tau \\bigr){\\mathbf{x}}_1^ * \\biggr).\\end{aligned}\\ ] ] the rank of the first difference on the right - hand side of the last display is at most  @xmath311 , since @xmath297 and @xmath296 differ only in the first column .",
    "the rank of the second difference is at most @xmath312 .",
    "the rank of @xmath313 is therefore at most @xmath314 . defining the resolvents",
    "@xmath315 the stieltjes transforms corresponding to the esds of @xmath41 and @xmath268 are , respectively , given by @xmath316 \\quad\\mbox{and}\\quad\\tilde s_{\\tau , p}(z)=\\frac{1}p \\operatorname{tr}\\bigl[{\\tilde{\\mathbf{r}}}_\\tau(z)\\bigr].\\ ] ] it follows from lemma  s.2 that with probability one , the esds of @xmath41 and @xmath268 converge to the same limit , provided the limits exist .",
    "we conclude from lemma  s.3 that the esd of @xmath268 converges a.s . to a nonrandom distribution by showing that @xmath317 converge pointwise a.s . to the stieltjes transform of a probability measure , thus establishing a.s .",
    "convergence of the esd of @xmath41 .    in order to derive limiting equation ( [ eqktau ] ) , a finite sample counterpart is needed .",
    "this can be derived as follows .",
    "the transformed data @xmath300 gives rise to the transformed stieltjes kernel @xmath318\\to\\mathbb{c}^+$ ] given by @xmath319.\\ ] ] following arguments typically used to establish the deterministic equivalent of a resolvent matrix , @xmath68 matrix - valued function solutions @xmath320 are needed such that , for sufficiently large @xmath0 , @xmath321\\approx0\\ ] ] for all @xmath322 and all @xmath68 hermitian matrix sequences @xmath323 with uniformly bounded norms @xmath324 . if one uses @xmath325 and the definition of @xmath326 , the latter approximate equation becomes @xmath327.\\ ] ] section  [ secgauss ] below is devoted to making precise the use of @xmath328 in the above equations and to showing that choosing @xmath329 with @xmath330 and @xmath331 , is appropriate .      in order to verify the statement of theorem  [ thesdmainfty ] for non - gaussian innovations",
    "@xmath23 , two key ideas are invoked , namely showing that :    * for any @xmath322 , the stieltjes transform @xmath332 concentrates around @xmath333 $ ] regardless of the underlying distributional assumption ; * the difference between the expectations @xmath333 $ ] under the gaussian model and the non - gaussian model is asymptotically negligible .    to establish the concentration property of the first item ,",
    "mcdiarmid s inequality is used to bound probabilities of the type @xmath334{\\vert}\\geq\\varepsilon)$ ] for arbitrary .",
    "these probabilities are then shown to converge to zero exponentially fast under ( [ eqpn ] ) . to establish the second item ,",
    "the generalized lindeberg principle of chatterjee  @xcite is applied . to this end , the argument @xmath335 is viewed as a parameter and @xmath332 as a function of the real parts @xmath336 and the imaginary parts @xmath337 of the innovation entries @xmath27 , for @xmath338 and @xmath285 .",
    "the difference between gaussian @xmath339 and non - gaussian @xmath339 can then be analyzed by consecutively changing one pair @xmath340 from gaussian to non - gaussian , thereby expressing the respective differences in the expected stieltjes transforms as a sum of these entrywise changes .",
    "these differences will be evaluated through a taylor series expansion , bounding certain third - order partial derivatives of @xmath341 .",
    "details are given in section  [ secpnon - gauss ] .",
    "while the arguments established so far work in the same fashion also for ma(@xmath342 ) processes , certain difficulties arise when making the transition to the ma(@xmath21 ) or linear process case .",
    "first , if one constructs the data matrix @xmath297 not from @xmath245 observations as above but from the linear process @xmath343 , then every column of @xmath297 is different from the corresponding column in the transformed matrix @xmath344 and not only the first column [ or the first @xmath342 columns for the ma(@xmath342 ) case ] .",
    "second , for the @xmath245 case one can write the stieltjes transform @xmath332 as a function of @xmath345 variables @xmath336 and @xmath337 [ or @xmath346 variables for the ma(@xmath342 ) case ] , but for linear processes , even for finite @xmath0 , @xmath332 is a function of infinitely many @xmath336 and @xmath337 .",
    "this makes their study substantially harder .",
    "linear processes are thus , for the purposes of this paper , approached through truncation , that is , by approximation through finite - order ma processes @xmath347 whose order @xmath348 is a function of the dimension @xmath0 and therefore grows with the sample size under ( [ eqpn ] ) .",
    "obviously @xmath349 is a necessary condition to make this approximation work .",
    "however , @xmath348 can not grow too fast ( leading to the same difficulties in transitioning from the gaussian to the non - gaussian case as for the linear process itself ) or too slow ( showing that the lsds of the linear process and its truncated version are identical becomes an issue ) .",
    "it turns out in section  [ secplin ] that @xmath350 , with @xmath351 denoting the ceiling function , is an appropriate choice .      to address the statements of theorem  [ coesdmainfty ] , the arguments presented thus far have to be adjusted for real - valued innovations @xmath352 .",
    "this is done using the eigen - decomposition of the coefficient matrices in the real fourier basis , after which arguments already developed for the complex case apply .",
    "detailed steps are given in section  [ secpreal ] .",
    "the key step toward proving theorem  [ thesdmainftyspectraldensity ] is to express @xmath353 as @xmath354 and then noticing that the matrix @xmath355 diagonalizes in the ( real or complex ) fourier basis with eigenvalues @xmath356 , for @xmath285 , so that the esd of @xmath230 can be approximated by the esd of the matrix @xmath357 , where @xmath358 is the @xmath40th column of the matrix @xmath359 , and @xmath360 denotes the @xmath361 fourier rotation matrix .",
    "we give the main steps of the arguments leading to this result .",
    "first , suppose that @xmath362 such that @xmath363 as @xmath73",
    ". then we can write @xmath364 where @xmath365 with @xmath366 , @xmath367 and @xmath368 now , the following facts together with representation ( [ eqhatgammanetarepresentation ] ) and theorem  a.43 of bai and silverstein @xcite ( rank inequality ) and lemma  s.1 ( norm inequality ) prove the assertion :    @xmath369}\\max\\{{\\vert}\\mathfrak{f}_{t_n , m_n}(\\theta ) - \\mathfrak{f}_{t_n}(\\theta){\\vert},{\\vert}\\mathfrak{f}_{t_n}(\\theta ) -\\mathfrak { f}_t(\\theta){\\vert}\\ } \\to0 $ ] , as @xmath73 ;    rank@xmath370 ;    @xmath371 and @xmath372",
    "throughout , @xmath374 is treated as a sequence of nonrandom matrices , and all the arguments are valid conditionally on this sequence . in this section ,",
    "the result of theorem  [ thesdmainfty ] is first verified for the ma(@xmath342 ) process @xmath375 when @xmath27 s are i.i.d .",
    "standard complex gaussian ; that is , real and imaginary parts of @xmath27 are independent normals with mean zero and variance one half . following the outline in section  [ subsecintu ] , the data matrix @xmath376",
    "$ ] is transformed into the matrix @xmath377 $ ] , with each column satisfying @xmath378 .",
    "then , since the rank of @xmath379 is at most @xmath342 , by lemma  s.2 , it follows that the esds of @xmath194 and @xmath380 have the same limit , if the latter exists . for simplicity ,",
    "let @xmath381 . to keep notation more compact , the extra subscripts @xmath0 and  @xmath16 ( indicating the lag of the autocovariance matrix under consideration )",
    "are often suppressed when no confusion can arise .",
    "for example , in ( [ eqapprox ] ) the notation @xmath382 will be preferred over the more complex @xmath383 . the proof is given in several steps .",
    "first , a bound on the approximation error is derived if the stieltjes kernel @xmath175 in  ( [ eqktau ] ) is replaced with its finite sample counterpart @xmath384 .",
    "second , existence , convergence and continuity of the solution to ( [ eqapprox ] ) are verified .",
    "third , tightness of the esds @xmath385 and convergence of the corresponding stieltjes transforms @xmath341 is shown .",
    "the goal is to provide a rigorous formulation of ( [ eqapprox ] ) and a bound on the resulting approximation error .",
    "the first step consists of giving a heuristic argument for the definition of @xmath382 in ( [ eqhtaup ] ) . to this end , note that @xmath386 it follows that , to achieve ( [ eqapprox ] ) , it is sufficient to solve @xmath387 .",
    "( the use of @xmath328 will be clarified below . )",
    "let @xmath388 and define the rank - one perturbation and its corresponding resolvent , respectively , given by @xmath389 using @xmath390+\\frac{1}n\\gamma_{\\tau , t}x_tx_t^*)^{-1}$ ] and defining @xmath391 the sherman  morrison formula and some matrix algebra lead to @xmath392 } \\nonumber \\\\ & = & { \\tilde{\\mathbf{r}}}(z)\\frac{1}n\\sum_{t=1}^n \\frac{\\gamma_{\\tau , t}{\\mathcal{h}}_t}{1+c_n\\gamma_{\\tau , t}{\\tilde{k}}(z,\\nu_t ) } , \\nonumber\\end{aligned}\\ ] ] thus validating the choice of @xmath382 as given in ( [ eqhtaup ] ) .",
    "the @xmath328 sign is due to substituting @xmath393 , @xmath394 , @xmath395 with @xmath396 , @xmath397 , @xmath398 $ ] , respectively .",
    "the arguments are made precise in the following theorem .",
    "[ thapperr ] let @xmath399 be a hermitian matrix such that @xmath400 and @xmath322 .",
    "if the assumptions of theorem  [ thesdmainfty ] are satisfied , then @xmath401 \\to0 \\qquad\\mbox{a.s.}\\ ] ] under ( [ eqpn ] ) , where @xmath402 is defined in ( [ eqhtaup ] ) , and @xmath403 is the resolvent of the symmetrized autocvariance matrix @xmath404 with @xmath405 .    observe that using ( [ eq * ] ) , ( [ eq * * ] ) and the definition of @xmath402 in ( [ eqhtaup ] ) , the a.s",
    "convergence in ( [ eq * * * ] ) is shown to be equivalent to @xmath406 where @xmath407,\\ ] ] with @xmath408 , @xmath409\\\\[-8pt]\\nonumber \\beta_{\\tau , t } & = & \\biggl(1+\\frac{1}n\\gamma_{\\tau , t}\\operatorname{tr}\\bigl[{\\tilde{\\mathbf{r}}}(z ) { \\mathcal{h}}_t\\bigr ] \\biggr)^{-1}.\\end{aligned}\\ ] ] decomposing further , write next @xmath410 , where @xmath411 , \\\\ d_{\\tau , t}^{(2 ) } & = & \\frac{1}{zp}\\operatorname{tr } \\bigl [ \\gamma_{\\tau , t}\\tilde{\\beta}_{\\tau , t}{\\tilde{\\mathbf{r}}}_t(z ) ( { \\tilde{{\\mathcal{h}}}}_t-{\\mathcal{h}}_t ) \\bigl({\\mathbf{i}}+{\\mathbf{h}}_t(z ) \\bigr)^{-1}{\\mathbf{d}}\\bigr ] , \\\\ d_{\\tau , t}^{(3 ) } & = & \\frac{1}{zp}\\operatorname{tr } \\bigl [ \\gamma_{\\tau , t}\\tilde{\\beta}_{\\tau , t } { \\tilde{\\mathbf{r}}}_t(z ) { \\mathcal{h}}_t \\bigl ( \\bigl({\\mathbf{i}}+{\\mathbf{h}}_t(z ) \\bigr)^{-1}- \\bigl({\\mathbf{i}}+{\\mathbf{h}}(z ) \\bigr)^{-1 } \\bigr){\\mathbf{d}}\\bigr ] , \\\\ d_{\\tau , t}^{(4 ) } & = & \\frac{1}{zp}\\operatorname{tr } \\bigl [ \\gamma_{\\tau , t}\\tilde\\beta_{\\tau , t } \\bigl({\\tilde{\\mathbf{r}}}_t(z)- { \\tilde{\\mathbf{r}}}(z ) \\bigr){\\mathcal{h}}_t \\bigl({\\mathbf{i}}+{\\mathbf{h}}(z ) \\bigr)^{-1}{\\mathbf{d}}\\bigr ] , \\\\ d_{\\tau , t}^{(5 ) } & = & \\frac{1}{zp}\\operatorname{tr } \\bigl [ \\gamma_{\\tau , t } ( \\tilde\\beta_{\\tau , t}-\\beta_{\\tau , t } ) { \\tilde{\\mathbf{r}}}(z){\\mathcal{h}}_t \\bigl({\\mathbf{i}}+{\\mathbf{h}}(z ) \\bigr)^{-1}{\\mathbf{d}}\\bigr],\\end{aligned}\\ ] ] with @xmath412 \\biggr)^{-1},\\ ] ] thus exhibiting the various approximations being made in the proof .",
    "the borel ",
    "cantelli lemma provides that @xmath413 almost surely is implied if @xmath414 faster than @xmath415 for all @xmath416 . since @xmath417 , in order to verify ( [ eq * * * ] ) , it is sufficient to show that @xmath418 goes to zero faster than @xmath415 for all @xmath416 and @xmath419 .",
    "the corresponding arguments are detailed in section s1.1 of the online sm .      in this section ,",
    "the proof of theorem  [ thesdmainfty ] is completed for the complex gaussian innovation model . in",
    "what follows , @xmath420 is without loss of generality assumed to be nonrandom , thereby restricting randomness to the innovations @xmath294 $ ] . for a fixed @xmath421 in the underlying sample space @xmath422 ,",
    "notation such as @xmath423 will be utilized to indicate realizations of the respective random quantities .",
    "noticing first that theorem  [ thesdmainfty ] makes an almost sure convergence statement , a  suitable subset @xmath424 with @xmath425 is determined .",
    "this subset is used for all subsequent arguments . to this end , observe that since the matrix @xmath305 $ ] has i.i.d .",
    "entries with zero mean and unit variance the norm of @xmath426 converges almost surely to a number not exceeding @xmath427 .",
    "let @xmath428 denote the esd of @xmath426 , and define @xmath429 with suitably chosen @xmath430",
    ". then @xmath431 .",
    "define next @xmath432 , \\\\",
    "\\bar{d}_{\\mathbf{i}}(z,\\omega ) & = & \\frac{1}{zp}\\operatorname{tr } \\bigl [ \\bigl ( { \\mathbf{i}}+{\\mathbf{h}}(z ) \\bigr)^{-1}+z{\\tilde{\\mathbf{r}}}(z,\\omega ) \\bigr].\\end{aligned}\\ ] ] let @xmath433 denote the set of complex numbers with rational real part and positive rational imaginary part and @xmath434_\\mathbb{q}=[0,2\\pi]\\cap\\mathbb{q}$ ] .",
    "define the set @xmath435_\\mathbb{q } \\bigr\\}.\\ ] ] in view of theorem  [ thapperr ] , it follows that @xmath436 a.s . and @xmath437 a.s . for all fixed @xmath438 and @xmath95_\\mathbb{q}$ ] .",
    "thus @xmath439 .",
    "henceforth only @xmath440 , so that @xmath441 , are considered .",
    "recall that @xmath442 $ ] .",
    "the following theorem establishes existence of a stieltjes kernel solution to the equations in ( [ eqktau ] ) along a subsequence .",
    "[ thexist ] suppose that the assumptions of theorem  [ thesdmainfty ] are satisfied :    for all @xmath443 and for all subsequences of @xmath444 , there exists another subsequence @xmath445 along which @xmath446 converges pointwise in @xmath322 and uniformly in @xmath95 $ ] to a limit @xmath447 analytic in @xmath335 and continuous in @xmath448 .    for every subsequence",
    "@xmath445 satisfying , @xmath449 satisfies ( [ eqktau ] ) for any @xmath322 .",
    "moreover , @xmath447 is the stieltjes transform of a measure with mass @xmath94 , provided that @xmath96 .",
    "\\(a ) let @xmath450 .",
    "then for a compact subset @xmath451 , @xmath452 for all @xmath95 $ ] and @xmath453 .",
    "enumerate @xmath434_\\mathbb{q}=\\ { \\nu_\\ell\\dvtx \\ell\\in\\mathbb{n}\\}$ ] .",
    "let @xmath454 mean that @xmath455 is a further subsequence of @xmath456 , and let @xmath444 denote the original sequence .",
    "an application of lemma  3 in geronimo and hill @xcite yields that , for any fixed @xmath443 , there is a sequence of subsequences @xmath457 so that @xmath458 converges to an analytic function of @xmath335 on @xmath459 .",
    "a standard diagonal argument implies that @xmath460 converges to an analytic function of @xmath335 on @xmath434_\\mathbb{q}$ ] . to simplify notation , write @xmath461 in place of @xmath460 .",
    "observe that the thus obtained limit , which will be denoted by @xmath447 , is so far defined only on @xmath462_\\mathbb{q}$ ] .",
    "it remains to obtain the extension of the limit to @xmath462 $ ] .",
    "note that lemma  s.10 implies that , for any @xmath322 , @xmath463 are equicontinuous in @xmath448 and converge pointwise to @xmath464 on the dense subset @xmath434_\\mathbb{q}$ ] of @xmath434 $ ] . by the arzela ",
    "ascoli theorem , @xmath465 converges therefore uniformly to a continuous function of @xmath448 on @xmath462 $ ] .",
    "this limit , denoted again by @xmath447 , is also analytic on @xmath466 . to see this , pick @xmath467\\setminus\\mathbb{q}$ ] and a sequence @xmath468_\\mathbb{q}$ ] such that @xmath469 .",
    "then @xmath470 satisfies the conditions of lemma  3 in @xcite , and consequently there is a subsequence of @xmath470 that converges to an analytic function .",
    "the limit of this subsequence has to coincide with @xmath471 by continuity on @xmath434 $ ] .",
    "it follows that @xmath447 analytic .",
    "\\(b ) by lemma  s.11 and the definition of @xmath472 , for all @xmath443 , @xmath447 satisfies  ( [ eqktau ] ) for all @xmath473 .",
    "so , by analyticity in @xmath335 , it holds for all @xmath466 .",
    "suppose first that @xmath99",
    ". then @xmath474 .",
    "thus @xmath475 for all @xmath322 , and the claim is verified .    for the remainder ,",
    "suppose that @xmath96 .",
    "showing that @xmath447 is a stieltjes transform of a measure with mass @xmath94 is equivalent to showing that @xmath476 is stieltjes transform of a borel probability measure .",
    "let @xmath477 and @xmath478 denote the eigenvalue and eigenvector matrices of @xmath268 .",
    "then @xmath479 $ ] is the stieltjes transform of a measure with mass @xmath480 $ ] . by the weak convergence of @xmath481 to @xmath76 , @xmath482 as @xmath483",
    "this shows that @xmath484 for all @xmath485 .",
    "since the diagonal entries of @xmath486 are bounded from above by @xmath487 , it follows that @xmath488 is the stieltjes transform of a measure @xmath489 , say , such that , for all real @xmath216 , @xmath490 , where @xmath491 denotes the esd of @xmath268 .",
    "it follows from lemma  s.12 that @xmath492 is a tight sequence .",
    "therefore @xmath493 are the stieltjes transforms of a tight sequence of borel measures .",
    "an application of lemma  s.13 yields that @xmath494 is the stieltjes transform of a measure with mass @xmath495 , completing the proof .    [ thunique ] if the assumptions of theorem  [ thesdmainfty ] are satisfied , then there is a unique solution @xmath100 to ( [ eqktau ] ) that is analytic in @xmath322 and continuous in @xmath95 $ ] with @xmath263 being a stieltjes transform of a measure with mass @xmath496 .",
    "suppose there are two solutions @xmath497 and @xmath498 to ( [ eqktau ] ) .",
    "let @xmath499 and @xmath500 .",
    "define then @xmath501 , @xmath502 .",
    "note that@xmath503 and @xmath504 have nonpositive imaginary parts .",
    "now @xmath505 and thus @xmath506 using the fact that @xmath507 is a stieltjes transform with mass bounded from above by @xmath487 , it follows that @xmath508 is bounded by @xmath509 .",
    "thus @xmath510 if @xmath511 , then @xmath512 and thus @xmath513 which by continuity in @xmath448 implies that @xmath514 for any fixed @xmath322 .",
    "since both solutions are analytic , the equality holds indeed for all @xmath322 .",
    "this proves uniqueness .    in the remainder of this section , the proof of theorem  [ thesdmainfty ]",
    "is completed for the gaussian ma(@xmath342 ) case .",
    "this is done by establishing that ( a ) the convergence along subsequences as stated in theorem  [ thexist ] holds indeed for the whole sequence and ( b ) the relevant esds converge .    toward ( a )",
    ", it is necessary to prove that , for every @xmath443 , @xmath515 converges to @xmath100 pointwise in @xmath322 and uniformly for @xmath95 $ ] under  ( [ eqpn ] ) .",
    "assume the contrary , and suppose that there are @xmath516 , @xmath467 $ ] and @xmath517 such that @xmath518 does not converge to @xmath519 .",
    "by boundedness of @xmath518 , there is a subsequence @xmath445 along which @xmath518 converges to a limit different from @xmath520 . invoking theorems  [ thexist ] and [ thunique ]",
    ", there is a further subsequence @xmath521 of @xmath445 along which @xmath522 converges to @xmath523 uniformly in @xmath95 $ ] .",
    "this is a contradiction .",
    "it follows that for every @xmath443 , @xmath515 converges to @xmath100 pointwise in @xmath322 and @xmath524 $ ] .",
    "an application of theorem  [ thexist ] and the arzela ",
    "ascoli theorem shows that the convergence is uniform on @xmath434 $ ] .",
    "note that , for any @xmath322 , @xmath525 $ ] converges to @xmath100 uniformly on @xmath434 $ ] .",
    "since we have @xmath526 , assertion ( a ) follows .    toward ( b ) , let @xmath443 .",
    "it needs to be shown that @xmath527 on  @xmath466 . by arguments as in the proof of lemma  s.11",
    ", it is already established that @xmath528 on @xmath529 .",
    "now , for any compact @xmath451 and @xmath530 , @xmath531\\bigr{\\vert}\\\\ & = & \\frac{1}{p}\\bigl{\\vert}\\operatorname{tr } \\bigl[(z_1-z_2 ) { \\tilde{\\mathbf{r}}}(z_1,\\omega){\\tilde{\\mathbf{r}}}(z_2,\\omega ) \\bigr]\\bigr{\\vert}\\\\ & \\leq & { \\vert}z_1-z_2{\\vert}\\bigl(\\min _ { z\\in s}v \\bigr)^2.\\end{aligned}\\ ] ] thus @xmath532 are equicontinuous in @xmath335 ( with @xmath0 and @xmath421 as parameters ) on @xmath533 . by arzela ",
    "ascoli , @xmath534 thus converges uniformly to @xmath535 on @xmath533 .",
    "consequently @xmath528 on @xmath466 .",
    "since @xmath536 , the esd of @xmath268 , is tight ( by lemma  s.12 ) , it follows from lemmas s.13 and s.3 that @xmath535 is a stieltjes transform of a ( nonrandom ) probability measure , and @xmath537 converges a.s . to the distribution",
    "whose stieltjes transform is given by @xmath90 .",
    "since , by lemma s.2 , @xmath538 a.s .",
    ", it follows that @xmath49 converges a.s . to the same limit , and",
    "hence @xmath539 converges a.s . to @xmath535 .",
    "the proof for the gaussian ma(@xmath342 ) case is complete . it can be checked that all the statements remain valid even if @xmath540 sufficiently slowly under  ( [ eqpn ] ) , for example , if @xmath541 .",
    "the extension of the result to non - gaussian innovations requires in its first step , a truncation argument , followed by a centering and rescaling of the innovations .",
    "this section justifies that the symmetrized autocovariance matrices obtained from gaussian innovations and from their truncated , centered and rescaled counterparts have the same lsd .",
    "the extension to the non - gaussian case is then completed in section  [ secpnon - gauss ] .",
    "since the underlying process is an ma(@xmath342 ) series , the observations @xmath4 are functions of the innovations @xmath542 . for @xmath338 and @xmath543 , define the quantities @xmath544 , \\\\",
    "\\bar{z}_{jt}^i&=&z_{jt}^i\\mathbb{i } \\bigl(\\bigl{\\vert}z_{jt}^i\\bigr{\\vert}<\\sqrt{p}\\bigr ) , \\qquad\\check{z}_{jt}^i=\\bar z_{jt}^r- \\mathbb{e}\\bigl[\\bar z_{jt}^i\\bigr],\\end{aligned}\\ ] ] where @xmath545 and @xmath546 the indicator function .",
    "correspondingly define @xmath547 and @xmath548 to be the autocovariance matrices obtained from @xmath549 and @xmath550 , @xmath551 , respectively .",
    "if the assumptions of theorem  [ thesdmainfty ] are satisfied , then a.s . under ( [ eqpn ] ) ,",
    "@xmath552 where @xmath553 , @xmath554 and @xmath555 denote the esds of @xmath41 , @xmath547 and @xmath556 , respectively .",
    "let @xmath557 and @xmath558 .",
    "let further @xmath559 and @xmath560 , and note that these quantities are independent due to the assumed i.i.d .",
    "structure on @xmath27 .",
    "since the fourth moments of the latter random variables are assumed finite , it also follows that @xmath561<\\infty$ ] and @xmath562<\\infty$ ] .",
    "observe next that the rank of a matrix does not exceed the number of its nonzero columns and that each nonzero @xmath27 causes at most @xmath563 nonzero columns .",
    "recalling that @xmath564 , theorem  a.44 of @xcite ( using @xmath565 and @xmath566 ) implies that , for any @xmath416 , @xmath567 let @xmath568^{-1}$ ] .",
    "for @xmath0 large enough so that @xmath569 , hoeffding s inequality yields @xmath570 as well as @xmath571 the borel ",
    "cantelli lemma now implies that @xmath572 a.s .",
    ", which is the first claim of the proposition .    to verify the second , note that the equality @xmath573+i\\mathbb{e}\\bigl[\\bar{z}_{11}^i\\bigr ] \\bigr ) \\mathbf{1},\\ ] ] with @xmath574 being the vector with all entries equal to 1 , shows that @xmath575 is independent of @xmath40 .",
    "thus an application of lemma s.1 leads to @xmath576 which converges to 0 a.s . under ( [ eqpn ] ) .",
    "this is the second assertion .",
    "after truncation and centering , it does not necessarily follow that @xmath577=\\mathbb{e}[{\\vert}\\check { z}_{11}^r+i\\check{z}_{11}^i{\\vert}^2]$ ] is equal to 1 . however , rescaling @xmath578 by dividing with @xmath579)^{1/2}$ ] ( in order to obtain unit variance ) does not affect the lsd because @xmath577\\to1 $ ] under ( [ eqpn ] ) .",
    "the detailed arguments follow as in section  3.1.1 of bai and silverstein  @xcite .",
    "this shows that the symmetrized autocovariances from  @xmath27 and their truncated , centered and rescaled counterparts have the same lsd .",
    "thus to simplify the argument , it can be assumed that the recentered process has variance one .",
    "in this section , the results for the gaussian ma(@xmath342 ) case are extended to general innovation sequences @xmath580 satisfying the same moment conditions as their gaussian counterparts @xmath581 .",
    "the processes of interest are then the two ma(@xmath342 ) processes @xmath582 define the symmetrized autocovariance matrix @xmath583 , the resolvent @xmath584 and the stieltjes transform @xmath585 $ ] . in the following , it will be shown that the esds of @xmath41 and @xmath586 converge to the same limit .",
    "this is done via verifying that , for all @xmath322 , @xmath587 and @xmath332 converge to the same limit @xmath588 under ( [ eqpn ] ) , which in turn requires us to show that :    @xmath589\\to0 $ ] under ( [ eqpn ] ) for all @xmath322 ;    @xmath590{\\vert}\\geq \\varepsilon]\\to0 $ ] under ( [ eqpn ] ) for all @xmath322 and @xmath416 .",
    "part ( a ) requires the use of the lindeberg principle , and part ( b ) is achieved via an application of mcdiarmid s inequality .      for the use in this section , redefine @xmath591 $ ] , define @xmath592 $ ] and let @xmath593 , @xmath594 and @xmath595 , @xmath596 be the corresponding matrices of real and imaginary parts .",
    "claim ( a ) will be verified via the lindeberg principle developed in chatterjee @xcite .",
    "this involves successive replacements of gaussian variables with non - gaussian counterparts in a telescoping sum . to this end , define an order on the index set @xmath597 by letting @xmath598 if either ( 1 ) @xmath599 or ( 2 ) @xmath600 and @xmath601 , so that one successively moves columnwise through the entries of a matrix .",
    "moreover , let @xmath602 let @xmath603 denote the @xmath604 matrix given by the entries @xmath605 further let the @xmath604 matrix @xmath606 be equal to @xmath607 for all entries but the @xmath608th one , which is set to equal 0 , and define analogously the matrices @xmath609 and @xmath610 . these matrices determine how many of the original gaussian @xmath27 s have been replaced by the non - gaussian @xmath611 . in the following",
    ", @xmath332 will be viewed as a function of @xmath593 and @xmath595 , fixing @xmath335 and @xmath0 as parameters , that is , @xmath612 .",
    "similarly , let @xmath613 .",
    "utilizing this notation , the quantity to be bounded in expectation can be written as @xmath614 \\\\ & & \\quad\\qquad { } + \\sum_{(j , t)=(1,1-q)}^{(p , n ) } \\bigl[s_p^z\\bigl({\\mathbf{v}}_{0,-q}^r,{\\mathbf{v}}_{jt}^i\\bigr)-s_p^z\\bigl ( { \\mathbf{v}}_{0,-q}^r,{\\mathbf{v}}_{(j , t)-1}^i\\bigr ) \\bigr ] \\\\ & & \\qquad = \\delta_1+\\delta_2,\\end{aligned}\\ ] ] where @xmath615 and @xmath616 are the real and imaginary part of the difference with @xmath617 and @xmath618 . in the following only the telescoping real parts will be discussed , as the imaginary parts can be estimated along the same lines . inserting @xmath606 , one obtains @xmath619 + \\bigl[s_p^z \\bigl({\\bar{{\\mathbf{v}}}}_{j ,",
    "t}^r,{\\mathbf{v}}_{p , n}^i \\bigr)-s_p^z\\bigl({\\mathbf{v}}_{(j , t)-1}^r,{\\mathbf{v}}_{p , n}^i\\bigr ) \\bigr ] \\\\ & = & \\delta_{j , t}^{(1,1)}+\\delta_{j , t}^{(1,2)}.\\end{aligned}\\ ] ] let @xmath620 denote the @xmath172th - order partial derivative of @xmath621 with respect to @xmath336 .",
    "a  taylor series expansion gives @xmath622}\\bigl{\\vert}\\partial_{j , t,1}^{(3)}s_p^z \\bigl(\\alpha{\\mathbf{v}}_{jt}^r+(1-\\alpha){\\bar{{\\mathbf{v}}}}_{jt}^r , { \\mathbf{v}}_{pn}^i\\bigr)\\bigr{\\vert}\\end{aligned}\\ ] ] and @xmath623}\\bigl{\\vert}\\partial_{j , t,1}^{(3)}s_p^z \\bigl(\\alpha{\\mathbf{v}}_{(j , t)-1}^r+(1-\\alpha){\\bar{{\\mathbf{v}}}}_{jt}^r , { \\mathbf{v}}_{pn}^i\\bigr)\\bigr{\\vert}.\\end{aligned}\\ ] ] the entries of the matrices @xmath593 , @xmath595 , @xmath594 and @xmath596 are all independent of each other and the first and second moments of the various real parts ( and imaginary parts ) coincide , so that the bound in the last two inequalities also hold for higher - order terms ( hot )",
    ". this leads to @xmath624\\bigr { \\vert}&=&\\biggl{\\vert}\\mathbb{e } \\biggl [ \\bigl(z_{jt}^r - w_{jt}^r \\bigr)\\partial_{j , t,1}^{(1)}s_p^z \\bigl({\\bar{{\\mathbf{v}}}}^r_{jt},{\\mathbf{v}}_{pn}^i\\bigr ) \\nonumber \\\\ & & \\hspace*{15pt}{}+ \\frac{1}2 \\bigl [ \\bigl(z_{jt}^r \\bigr)^2-\\bigl(w_{jt}^r \\bigr)^2 \\bigr]\\partial_{j , t,1}^{(2)}s_p^z\\bigl ( { \\bar{{\\mathbf{v}}}}^r_{jt},{\\mathbf{v}}_{pn}^i\\bigr)\\pm \\mathrm{hot } \\biggr]\\biggr{\\vert}\\nonumber\\\\[-8pt]\\\\[-8pt]\\nonumber & \\leq&\\frac{1}6\\mathbb{e } \\bigl[\\bigl{\\vert}z_{jt}^r \\bigr{\\vert}^3 \\max_{\\alpha\\in[0,1]}\\bigl{\\vert}\\partial_{j , t,1}^{(3)}s_p^z\\bigl(\\alpha { \\mathbf{v}}_{jt}^r+(1-\\alpha){\\bar{{\\mathbf{v}}}}_{jt}^r , { \\mathbf{v}}_{pn}^i\\bigr)\\bigr{\\vert}\\bigr ] \\\\ & & { } + \\frac{1}6\\mathbb{e } \\bigl[\\bigl{\\vert}w_{jt}^r\\bigr{\\vert}^3 \\max _ { \\alpha\\in[0,1]}\\bigl{\\vert}\\partial_{j , t,1}^{(3)}s_p^z \\bigl(\\alpha{\\mathbf{v}}_{(j , t)-1}^r+(1-\\alpha){\\bar{{\\mathbf{v}}}}_{jt}^r , { \\mathbf{v}}_{pn}^i\\bigr)\\bigr{\\vert}\\bigr].\\nonumber\\hspace*{-25pt}\\end{aligned}\\ ] ] dealing with the right - hand side of ( [ eqtaylor ] ) requires the computation and estimation of the third - order derivatives @xmath625 $ ] . focusing only on the first term of the right - hand side of ( [ eqtaylor ] ) ( the second can be handled similarly ) , lemma s.14 shows that this term converges to zero under ( [ eqpn ] ) if , almost surely under ( [ eqpn ] ) , @xmath626 } \\biggl(\\sum _ { \\ell\\in\\mathcal{i}_+(t)}{\\vert}{\\mathbf{a}}_\\ell{\\vert}\\bigl{\\vert}{\\tilde{x}}_{t+\\tau + \\ell}^\\alpha\\bigr{\\vert}\\nonumber\\\\[-8pt ] \\label{eqremainder1 } \\\\[-8pt ] \\nonumber & & \\hspace*{165pt } { } + \\sum_{\\ell\\in\\mathcal{i}_-(t)}\\bigl{\\vert}{\\tilde{x}}_{t-\\tau+\\ell}^\\alpha \\bigr{\\vert}{\\vert}{\\mathbf{a}}_\\ell{\\vert}\\biggr)^3 \\biggr]\\to 0,\\hspace*{-20pt } \\nonumber \\\\ & & \\frac{q+1}{n^2p}\\sum_{(j , t)=(1,1-q)}^{(p , n ) } \\mathbb{e } \\biggl[\\bigl{\\vert}z_{t , j}^r\\bigr{\\vert}^3\\max_{\\alpha\\in[0,1 ] } \\biggl(\\sum _ { \\ell\\in\\mathcal{i}_+(t)}{\\vert}{\\mathbf{a}}_\\ell{\\vert}\\bigl{\\vert}{\\tilde{x}}_{t+\\tau+\\ell}^\\alpha\\bigr{\\vert}\\nonumber\\\\[-8pt ] \\label{eqremainder2 } \\\\[-8pt]\\nonumber & & \\hspace*{165pt } { } + \\sum_{\\ell\\in\\mathcal{i}_-(t)}\\bigl{\\vert}{\\tilde{x}}_{t-\\tau+\\ell}^\\alpha",
    "\\bigr{\\vert}{\\vert}{\\mathbf{a}}_\\ell{\\vert}\\biggr ) \\biggr]\\to0,\\hspace*{-20pt } \\nonumber\\end{aligned}\\ ] ] where @xmath627 , @xmath628 , @xmath629=\\sum_{\\ell = 0}^q{\\mathbf{a}}_\\ell{\\mathbf{v}}^\\alpha{\\mathbf{l}}^\\ell{\\mathbf{f}}$ ] , @xmath630 and @xmath631^\\prime$ ] being an @xmath632 matrix .",
    "the choice of @xmath633 $ ] only affects the value of the @xmath28th entry of @xmath634 . for @xmath635",
    ", the notation @xmath636 is therefore preferred . by definition",
    ", @xmath637 is the vector whose @xmath28th entry has a real part of zero .",
    "let @xmath638 .",
    "then @xmath639}\\bigl{\\vert}{\\tilde{x}}^\\alpha_{t\\pm\\tau+\\ell } \\bigr{\\vert}\\\\ & & \\qquad \\leq\\sum_{\\ell^\\prime\\in\\mathcal{j}_\\pm(t,\\ell)}\\bar { \\lambda } _ { { \\mathbf{a}}_{\\ell^\\prime}}{\\vert}z_{t\\pm\\tau+\\ell-\\ell^\\prime}{\\vert}+\\bar { \\lambda}_{{\\mathbf{a}}_{\\ell\\pm\\tau } } \\bigl(\\bigl{\\vert}v_t^0\\bigr{\\vert}+\\bigl{\\vert}z_{j , t}^r\\bigr{\\vert}\\bigr).\\end{aligned}\\ ] ] hence , setting @xmath640 , @xmath639}\\sum_{\\ell\\in\\mathcal{j}_\\pm(t ) } { \\vert}{\\mathbf{a}}_\\ell{\\vert}\\bigl{\\vert}{\\tilde{x}}^\\alpha_{t\\pm\\tau+\\ell}\\bigr{\\vert}\\\\ & & \\qquad \\leq\\bar{\\lambda}_{\\mathbf{a}}\\sum_{\\ell =- q,\\ell\\neq\\mp\\tau}^q \\bar{\\lambda}_\\ell^{\\mathrm{rem}}{\\vert}z_{t\\pm\\tau+\\ell}{\\vert}+ \\bar{\\lambda}_{\\mathbf{a}}\\bar{\\lambda}_{\\pm\\tau}^{\\mathrm{rem } } \\bigl ( \\bigl{\\vert}v_t^0\\bigr{\\vert}+\\bigl{\\vert}z_{j , t}^r\\bigr{\\vert}\\bigr).\\end{aligned}\\ ] ] using this , it follows from lemma s.15 that the left - hand sides of ( [ eqremainder1 ] ) and ( [ eqremainder2 ] ) converge to zero a.s .",
    ", thus establishing that @xmath641\\to0 $ ] .      for a fixed @xmath335 ,",
    "@xmath587 is a function of the @xmath642 vectors @xmath643 .",
    "letting @xmath644 , these are now segmented into the groups @xmath645 , @xmath646 , possibly adding additional vectors to the last group to ensure all groups have the same length [ even though the value @xmath587 does not depend on the additions ] . to satisfy the conditions needed in order to apply mcdiarmid s inequality ,",
    "note that a change of the values @xmath647 in one group to , say , @xmath648 , causes the values of @xmath649 to change to , say , @xmath650 . in the following ,",
    "the focus is on changes applied to the first group of innovations @xmath651 .",
    "consider the case @xmath652 and let @xmath653 @xmath654 and @xmath655 $ ] .",
    "the goal is now to represent @xmath656 as a finite rank perturbation of @xmath547 in the form @xmath657 with appropriate @xmath658 and  @xmath659 .",
    "write @xmath660.\\end{aligned}\\ ] ] choosing @xmath661 and repeatedly utilizing ( s.3 ) with @xmath662 replaced by @xmath663 , it follows that @xmath664 for some appropriately chosen constant @xmath665 .",
    "this bound holds for any of the @xmath151 groups of innovations .",
    "mcdiarmid s inequality consequently implies , for any @xmath416 and a suitable constant @xmath666 , @xmath667\\bigr{\\vert}\\geq\\varepsilon\\bigr ) \\leq4\\exp \\biggl(-c_2\\frac{\\varepsilon^2v^2p^2}{mq^2 } \\biggr ) \\sim4\\exp\\biggl(-c_2 \\frac{\\varepsilon^2v^2c}{q}p \\biggr).\\ ] ] the right - hand side converges to zero at a rate faster than @xmath668 and concentration of the stieltjes transform around its mean is established , since the case @xmath669 can be handled in a similar fashion . note that the last argument remains valid if @xmath670 as @xmath78 at a sufficiently slow rate , for example , if @xmath671",
    "let @xmath9 now denote a linear process . to complete the proof of theorem  [ thesdmainfty ] , a truncation argument is invoked .",
    "let @xmath672 denote the truncated process given by @xmath673 , @xmath674 , where @xmath348 depends on the dimension .",
    "let @xmath675 be the process given by @xmath676 , @xmath677 , and denote by @xmath678 the lvy distance between distribution functions @xmath54 and @xmath679 .",
    "[ lemlinproclevy ] if the assumptions of theorem  [ thesdmainfty ] are satisfied and if @xmath680 , then @xmath681 under ( [ eqpn ] ) , where @xmath682 .    by lemma s.1",
    ", it suffices to show that @xmath683 \\to0 $ ] a.s .",
    "write @xmath684 and @xmath685",
    ". then @xmath686 say . from repeated applications of the cauchy ",
    "schwarz inequality , we have @xmath687 & = & 2\\operatorname{tr}\\bigl({\\mathbf{p}}^2\\bigr ) + \\operatorname{tr } \\bigl({\\mathbf{q}}^2\\bigr ) + 2 \\operatorname{tr}\\bigl({\\mathbf{p}}{\\mathbf{p}}^*\\bigr ) + 4 \\operatorname{tr}({\\mathbf{p}}{\\mathbf{q } } ) \\\\ & \\leq & 4 \\operatorname{tr}\\bigl({\\mathbf{p}}{\\mathbf{p}}^*\\bigr ) + \\operatorname{tr}\\bigl({\\mathbf{q}}^2\\bigr ) + 4 \\sqrt{\\operatorname{tr}\\bigl({\\mathbf{p}}{\\mathbf{p}}^*\\bigr ) } \\sqrt{\\operatorname{tr}\\bigl({\\mathbf{q}}^2 \\bigr)}.\\end{aligned}\\ ] ] since @xmath688 , @xmath689 $ ] , and by another application of cauchy  schwarz , @xmath690 \\biggr)^{1/2 } \\biggl(\\frac{1}{n^2}\\operatorname{tr}\\bigl[\\bigl(\\check{\\mathbf{x}}\\check{\\mathbf{x}}^*\\bigr)^2\\bigr ] \\biggr)^{1/2}.\\ ] ] since it is easy to see that @xmath691 $ ] is stochastically bounded ( e.g. , by showing that the expectation is finite ) , it is enough to show that @xmath692 \\to0 $ ] a.s .",
    "this is established by showing that the sum @xmath693 ] < \\infty$ ] , and then applying the borel ",
    "cantelli lemma .",
    "to this end , note that @xmath694 \\bigr ] \\\\ & & \\qquad = \\sum_{t=1}^n \\sum _ { s=1}^n \\mathbb{e } \\bigl[\\bigl{\\vert}\\check x_t^ * \\check x_s\\bigr{\\vert}^2 \\bigr ] \\\\ & & \\qquad   = \\sum_{t=1}^n \\sum _ { s=1}^n \\sum_{\\ell = q+1}^\\infty \\sum_{\\ell'=q+1}^\\infty\\sum _ { m = q+1}^\\infty\\sum_{m'=q+1}^\\infty \\mathbb{e } \\bigl[\\operatorname{tr}\\bigl(z_{t-\\ell'}z_{t-\\ell}^ * { \\mathbf{a}}_\\ell{\\mathbf{a}}_m \\\\ & & \\hspace*{204pt}{}\\times z_{s - m}z_{s - m ' } { \\mathbf{a}}_{\\ell ' } { \\mathbf{a}}_{m'}\\bigr ) \\bigr].\\end{aligned}\\ ] ] it is clear from the independence of @xmath29 s that the summands are nonzero only if the indices of @xmath695 s pair up .",
    "direct calculation shows that the total contribution of all four types of pairings : ( i ) @xmath696 ; ( ii ) @xmath697 ; ( iii ) @xmath698 and ( iv ) @xmath699 can be bounded by @xmath700 for some @xmath215 , using the fact that @xmath0 and @xmath2 are of the same order .",
    "thus since @xmath701 , @xmath702 this proves the result .",
    "using gaussian innovations @xmath23 , let @xmath703=\\break   \\sum_{\\ell=0}^q{\\mathbf{a}}_\\ell{\\mathbf{z}}{\\tilde{\\mathbf{l}}}^\\ell{\\mathbf{u}}_{{\\tilde{\\mathbf{l}}}}$ ] be the corresponding transformed data matrix .",
    "then define @xmath704 , @xmath705 , @xmath706,@xmath707 , @xmath708 $ ] , @xmath709 $ ] , @xmath710 and @xmath711 .",
    "then one verifies in a similar vein , as in the proofs of theorems  [ thexist ] and  [ thunique ] , that for all @xmath443 with @xmath472 defined in the beginning of section  [ secgaussexconcon ] , under ( [ eqpn ] ) , @xmath712^{-1}h^{\\mathrm{tr}}(\\lambda,\\nu)\\,df_{p_\\ell}^{\\mathbf{a } } ( \\lambda ) \\to0,\\ ] ] where @xmath713^{-1}h^{\\mathrm{tr}}(\\lambda,\\nu _ { tj})$ ] .",
    "this is done by exploiting the convergence @xmath714 , which is uniform in @xmath448 and @xmath277 , and @xmath715 , which is uniform in @xmath277 .",
    "therein , @xmath716^{-1}$ ] . from these facts",
    "it follows that the limiting version of the truncated version satisfies the defining equations for the stieltjes kernel ( [ eqktau ] ) .",
    "therefore the results for the complex gaussian innovation model with fixed order @xmath342 are , subject to minor modifications , still applicable when orders @xmath348 grow at a suitable rate .",
    "the idea of the proof of theorem  [ coesdmainfty ] is motivated by focusing on the @xmath245 case .",
    "the derivation for the @xmath373 and finally @xmath15 cases follows the from the corresponding transformations and subsequent constructions analogous to the complex case .",
    "so , let @xmath717 be the data matrix obtained from a gaussian @xmath245 time series , and suppose that @xmath270 possesses an eigendecomposition @xmath718 with @xmath719 orthogonal .",
    "let @xmath69 denote the real fourier basis ( see , e.g. , chapter  10 of @xcite ) , and let for the real case @xmath720 .",
    "since @xmath721 and @xmath722 have independent columns , it follows that @xmath300 has independent columns .",
    "moreover , @xmath300 has also independent rows . to see this , note that the transpose of the @xmath28th row of @xmath300 is @xmath723 where @xmath724 is the @xmath28th column of @xmath725 and @xmath84 the @xmath28th eigenvalue of @xmath270 .",
    "the covariance of the @xmath28th column is @xmath726 = { \\mathbf{u}}^t \\bigl({\\mathbf{i}}+\\lambda_j \\bigl({\\tilde{\\mathbf{l}}}+{\\tilde{\\mathbf{l}}}^t\\bigr)+\\lambda_j^2{\\tilde{\\mathbf{l}}}{\\tilde{\\mathbf{l}}}^t \\bigr){\\mathbf{u}}.\\ ] ] since @xmath727 is a symmetric circulant matrix , it diagonalizes in the real fourier basis , and hence the covariance matrix of the last display is diagonal . from the same display it follows also that the variance of the @xmath608th entry is @xmath728 , so that the rest of the proof follows as in the complex case ( theorem  [ thesdmainfty ] ) ."
  ],
  "abstract_text": [
    "<S> this paper is concerned with extensions of the classical marenko  pastur law to time series . </S>",
    "<S> specifically , @xmath0-dimensional linear processes are considered which are built from innovation vectors with independent , identically distributed ( real- or complex - valued ) entries possessing zero mean , unit variance and finite fourth moments . </S>",
    "<S> the coefficient matrices of the linear process are assumed to be simultaneously diagonalizable . in this </S>",
    "<S> setting , the limiting behavior of the empirical spectral distribution of both sample covariance and symmetrized sample autocovariance matrices is determined in the high - dimensional setting @xmath1 for which dimension @xmath0 and sample size @xmath2 diverge to infinity at the same rate . </S>",
    "<S> the results extend existing contributions available in the literature for the covariance case and are one of the first of their kind for the autocovariance case .    , </S>"
  ]
}