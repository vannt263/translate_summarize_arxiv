{
  "article_text": [
    "the problem of early word - learning has been subject of philosophical controversy for centuries @xcite .",
    "the always visionary augustine argued that the child makes the connections between words and their referents by understanding the referential intentions of others , thus anticipating the modern theory of mind in about fifteen centuries @xcite . in the 17th century , locke s empiricism supported the associationist viewpoint , which contends that the mechanism of word learning is sensitivity to covariation , i.e. , if two events occur at the same time , they become associated .",
    "here we examine a radical offshoot of the associationist approach to lexicon acquisition termed cross - situational or observational learning @xcite , which asserts that the meaning of a word can be determined by looking for something in common across all observed uses of that word @xcite . in other words , learning takes place through the statistical sampling of the contexts in which a word appears .    a scenario to describe",
    "the lexicon acquisition process should take into account the inherent ambiguity of the learning task ( i.e. , many distinct objects may be associated to the same word ) as well as the noisy effect of out - of - context words ( i.e. , the uttered word may not refer to any object in the context ) . whereas the noiseless scenario has been explored in great detail in the literature @xcite , where it was shown that the learning error decreases exponentially with the number of learning trials , a systematic study of the effect of noise is lacking .    to remedy this deficiency ,",
    "we modify the minimal model of noiseless cross - situational learning @xcite so as to include the effect of noise produced by out - of - context words . using monte carlo simulations and finite - size scaling we identify and characterize a critical phenomenon that separates the asymptotic regime where the lexicon can be acquired without errors from the regime where learning is impossible . at the critical noise level",
    ", we find that the duration of the periods with zero error is distributed by a power - law distribution .",
    "we assume that there are @xmath4 objects , @xmath4 words and a one - to - one mapping between words and objects . at each learning event ,",
    "@xmath5 objects are chosen at random without replacement from the fixed list of @xmath4 objects and one of these objects is named according to the word - object mapping .",
    "the @xmath5 objects form the context which determines the interpretation of the uttered word and the learner s task is to guess which of the @xmath5 objects that word refers to .",
    "this is then an ambiguous word - learning scenario in which there are multiple object candidates for any word .",
    "the parameter @xmath5 is a measure of the ambiguity of the learning task . in particular , in the case @xmath6 the word - object mapping is not learnable within a cross - situational scenario .",
    "a learning episode comprises a context and a single target word . in an uncorrupted learning episode",
    ", the context must exhibit the correct object ( i.e. , the object named by the target word according to the object - word mapping ) plus @xmath7 distinct mismatching objects .",
    "noise is added to the learning scenario by removing the correct object from the context , which will then exhibit @xmath5 mismatching objects .",
    "such corrupted and misguiding learning episodes occur with probability @xmath8 $ ] .",
    "this type of noise is an integrant part of any realistic learning situation , arising usually from the unwarranted narrowing of the context by the learner .    to represent the one - to - one object - word mapping we use the index @xmath9 to label the distinct objects and @xmath10 to label the distinct words",
    "then , without lack of generality , the correct mapping is defined by assigning object @xmath11 to word @xmath12 , object @xmath13 to word @xmath14 and so on .",
    "the problem faced by the learner is to determine the correct mapping given a sequence of learning episodes .",
    "next we will describe a simple ( perhaps , the simplest ) procedure to accomplish this learning task .",
    "we assume that learning is a change in the confidence with which the learner associates the target word @xmath15 to a given object @xmath16 and represent this confidence by a non - negative integer @xmath17 .",
    "our associative accumulator learning procedure is described as follows . before learning all confidences",
    "are set to zero , i.e. , @xmath18 for @xmath19 , and whenever object @xmath20 appears in a context with target word @xmath21 the confidence @xmath22 increases by one unit @xcite .",
    "hence , exactly @xmath5 confidence values are updated at each learning trial .    to determine which object corresponds to word @xmath15",
    "the learner simply chooses the object index @xmath16 for which @xmath17 is maximum . in the case of ties ,",
    "the learner selects one object at random among those that maximize the confidence . from the definition of the correct word - object mapping",
    ", our learning algorithm achieves a perfect performance when @xmath23 for all @xmath15 and @xmath24 .",
    "a critical feature of the accumulator model is that words are learned independently .",
    "this fact alone allows us to split the analysis of the vocabulary learning task in two parts .",
    "the first and most important part is the problem of learning the meaning ( or the referent ) of a _ single _ word .",
    "once this is done , we can easily solve the problem of learning the @xmath4 words given their sampling frequencies @xcite .",
    "hence , in this work we will focus on the single - word learning problem only .",
    "accordingly , we consider the learning of a single word , say word @xmath15 , which is then uttered at all learning trials @xmath2 . we define the single - word learning error @xmath25 for @xmath26 as follows . if @xmath27 for any @xmath24 then @xmath28 , otherwise if @xmath29 for @xmath30 values of @xmath24 then @xmath31 with @xmath32 . at @xmath33",
    "all confidences are set to zero and so @xmath34 .    in the noiseless case ( @xmath35 ) we have @xmath36 for all @xmath24 since object @xmath37 is always part of the context . so errors are due to ties @xmath38 only .",
    "in fact , it can be shown analytically that in this case the average learning error vanishes like @xmath39^\\tau $ ] for large @xmath2 @xcite .",
    "as expected , for @xmath40 we have @xmath41 at the first learning trial @xmath42 already , but more interestingly is that learning becomes faster with increasing @xmath4 .",
    "this apparently counterintuitive result has a simple explanation : a large list of objects to select from actually decreases the odds of choosing the same confounding object during the learning trials , thus reducing the number of ties .",
    "however , this decrease is overcompensated by the sampling effect when we consider the problem of learning the entire vocabulary and then learning slows down as @xmath4 increases , as expected @xcite .     for a single sample of the learning process using the accumulator learning model .",
    "the parameters are @xmath43 , @xmath44 and @xmath45 .",
    "the lines are guides to the eye . ]    in the case the contexts are corrupted by noise with a probability @xmath46 an analytical approach is not possible and we have to resort to simulations to study the stochastic learning process . figure [ fig:1 ] shows a typical evolution of the learning error at the critical noise level .",
    "although this figure reveals a rich stochastic dynamics , it is rather uninformative from the learning perspective . in that sense , the behavior of the average learning error @xmath47 , shown in fig .",
    "[ fig:2 ] , is more relevant . for a fixed @xmath2 , this average",
    "is calculated using typically @xmath48 to @xmath49 realizations of the learning process .",
    "as function of the number of learning trials for @xmath50 , @xmath51 and ( bottom to top ) @xmath52 .",
    "the critical value of the noise parameter is @xmath53 at which @xmath54 .",
    "the symbols are the simulation results and the lines are guides to the eyes . ]",
    "figure [ fig:2 ] reveals that learning is possible provided that the noise parameter does not exceed a certain threshold @xmath0 .",
    "more pointedly , in the asymptotic regime @xmath55 we find that @xmath56 for @xmath57 and that @xmath58 for @xmath59 .",
    "the surprising finding is that at @xmath60 , the average learning error becomes independent of @xmath61 .",
    "there is a simple reasoning to determine @xmath0 as well as the error @xmath62 at this critical noise parameter .",
    "first , we note that the borderline between learning and non - learning occurs when all @xmath4 objects are equally likely of being selected to compose the contexts .",
    "we recall that this is exactly the situation prior to learning and so we expect that @xmath63 accordingly , @xmath0 is determined by equating the probability of selecting the correct object with the probability of selecting any given incorrect object to compose the context in a learning episode , i.e. , @xmath64 from which we get @xmath65 these neat expressions for @xmath66 and @xmath0 proved correct for a vast selection of values of @xmath4 and @xmath5 , but we have no mathematical proof of their validity , besides the argument presented above .",
    "however , we can perform a simple consistency check on these expressions as follows .",
    "the average learning error at the first trial is given by @xmath67 and by setting @xmath60 we recover eq .",
    "( [ e2c ] ) as it should be since @xmath68 is independent of @xmath2 ( see fig .",
    "[ fig:2 ] ) .",
    "considering the ` size ' of the system as the number of learning trials @xmath2 we proceed now to examine the sharpness of the phase transition at @xmath0 using finite - size scaling @xcite .",
    "this threshold phenomenon is best appreciated in fig .",
    "[ fig:3 ] , which exhibits the dependence of the average learning error on the distance to the critical parameter for different values of @xmath2 . as the number of trials @xmath2 increases ,",
    "the difference between the regimes @xmath69 and @xmath70 becomes evident .",
    "all curves intersect at @xmath60 for which the average error is a constant given by eq .",
    "( [ e2c ] ) .     and @xmath51 .",
    "the symbols are the simulation results for ( top to bottom in the positive ordinate region ) @xmath71 and @xmath72 .",
    "the lines are guides to the eyes . ]",
    "the key insight is obtained when one considers the average learning error as a function of the reduced variable @xmath73 , as exhibited in fig .",
    "[ fig:4 ] .",
    "use of this reduced variable produces the collapse of the data for different @xmath2 into a single scaling function , which depends on the values of @xmath4 and @xmath5 only .",
    "as illustrated in the figure , the data is fitted very well by the functional form @xmath74,\\ ] ] which has a single fitting parameter , @xmath75 .",
    "the parameter @xmath76 is obtained by setting @xmath77 and then using the expression of @xmath62 , given by eq .",
    "( [ e2c ] ) .",
    "the final result is @xmath78,\\ ] ] where @xmath79 stands for the inverse complementary error function .",
    "we note that @xmath80 and @xmath81 for @xmath82 .     for @xmath83 and @xmath84 and @xmath85 .",
    "the symbols are the simulation results and the lines are given by the scaling function ( [ ansatz ] ) with the parameter @xmath86 obtained from the fitting of the data . ]    we can get some insight on the fitting parameter @xmath75 by calculating explicitly the average learning error for @xmath87 and @xmath40 . in the limit @xmath88 and @xmath89 such that @xmath90 is finite ,",
    "we find to the leading order @xmath91^{1/2 } }   \\right ] .\\ ] ] hence we assume that @xmath92 and plot this fitting parameter in fig .",
    "[ fig:5 ] for a large selection of values of @xmath4 and @xmath5 .",
    "more pointedly , for each value of @xmath4 ( represented by different symbols in the figure ) we vary @xmath5 from @xmath93 to @xmath94 to obtain scaling functions as those shown in fig .",
    "[ fig:4 ] .",
    "then these functions are fitted using eq .",
    "( [ ansatz ] ) in order to determine the fitting parameter @xmath86 .",
    "for @xmath95 the data is fitted very well by the function @xmath96^{1/2}}\\ ] ] with @xmath97 .",
    "note that for @xmath87 we have @xmath98 .     on the ratio @xmath99 for @xmath100 , @xmath101 , @xmath102 , @xmath103 and @xmath104 .",
    "the solid line is given by eq .",
    "( [ b ] ) . ]",
    "figure [ fig:5 ] reveals a most interesting symmetry : for fixed @xmath4 the average learning error when plotted against the reduced variable @xmath105 is invariant to the change @xmath106 which implies @xmath107 . in particular , in fig .",
    "[ fig:4 ] the results for @xmath108 are identical to those displayed for @xmath40 , the results for @xmath109 to those for @xmath51 and so on .",
    "however , we must note that this symmetry is exact only in the limits @xmath110 and @xmath111 .    for an infinitely large lexicon , @xmath112",
    ", we have @xmath113 and so @xmath114 if the context size @xmath5 grows linearly with @xmath4 ( i.e. , @xmath0 is nonzero ) , but @xmath115 if @xmath5 remains finite since in this case @xmath116 diverges faster than @xmath117 .",
    "a distinctive feature of the learning process revealed by fig .  [ fig:1 ] is the existence of long periods when the learning error stands at zero value , i.e. , @xmath23 for all objects @xmath24 .",
    "these periods or stases are characterized by repeated additions of credence units to the confidence values @xmath17 and they end when one ( or more ) of the @xmath94 confidences @xmath118 equals @xmath119 .",
    "we begin the analysis of the distribution @xmath120 of the durations @xmath121 of the stases at the critical parameter @xmath0 by showing in fig .",
    "[ fig:6 ] how the total number of learning trials @xmath122 ( basically a cutoff time ) affects this distribution .",
    "the rescaling @xmath123 makes the results essentially independent of the cutoff parameter @xmath122 provided @xmath124 is not too small ( data not shown ) .",
    "the curves exhibit a clear power law behavior with exponent @xmath3 , which is the mean - field exponent for the size of avalanches in self - organized critical models @xcite .",
    "in addition , we find that away from the critical point the distribution @xmath125 is exponential and that the average duration of the stases diverges like @xmath126 as @xmath111 .    as expected , these mean - field critical exponents are robust to changes in the model parameters @xmath4 and @xmath5 .",
    "in fact , for @xmath87 and @xmath40 the distribution @xmath127 can be easily calculated analytically for any value of @xmath46 since this is the classical ruin problem in which a gambler with initial capital @xmath128 plays against an infinitely rich adversary .",
    "the results for the duration of the game @xmath129 are simply @xmath130 and @xmath131 ( see chapter xiv of @xcite ) .",
    "changes in the number of objects @xmath4 have no significant influence on @xmath132 whereas changes in the context size @xmath5 produce a shift on the distribution , without affecting the power - law exponent , as illustrated in fig .",
    "[ fig:7 ] .",
    "in fact , increase of @xmath5 increases the frequency of short stases and , consequently , reduces the frequency of long ones .",
    "this is expected since the larger the context size , the greater the number of mismatching objects that have their confidences updated , and so the greater the odds of occurrence of the jump condition @xmath133 for some object @xmath134 .    , @xmath44 , @xmath135 , and ( bottom to top ) @xmath136 and @xmath137 .",
    "the slope of the straight line is @xmath3 . ]    , @xmath138 and ( bottom to top at @xmath139 ) @xmath140 .",
    "the slope of the straight line is @xmath3 . ]",
    "finally , we note that although we have focused on the periods of the learning process when the error learning is @xmath141 , the very same conclusions hold for the periods when the learning error is @xmath93 .",
    "the view of language as a collective phenomenon arising out of local social interactions has prompted its modeling and investigation through statistical physics concepts and tools@xcite .",
    "words have been likened to genes and their evolution studied within a population genetics framework @xcite , whereas the competition between whole languages has been considered using population dynamics models @xcite .",
    "the study of the bootstrap of a common lexicon among a large population of individuals has revealed a sharp phase transition towards shared conventions @xcite as well as an unexpected connection with random occupancy problems in the case only two individuals interact but the lexicon size is very large@xcite .",
    "the problem of acquiring , rather than bootstrapping , a fixed lexicon from observational learning is relevant to developmental psychology since it allows a quantitative appraisal of the associationist hypothesis on early - word learning @xcite .",
    "in particular , we show that the utterance of out - of - context words may result in severe limitations to learning , depending on the ratio @xmath142 between the number of objects presented to the learner at a learning trial and the total number of objects .",
    "if this ratio is small ( i.e. , @xmath0 is close to 1 ) then this noisy effect is largely irrelevant and the lexicon can quickly be learned to perfection .",
    "however , for large values of this ratio ( i.e. , @xmath0 is close to 0 ) learning becomes impossible regardless of the number of trials @xmath2 .",
    "finite - size scaling shows that the threshold phenomenon persists across a region of size @xmath1 around @xmath0 and offers the explicit functional form of the learning error in this region .",
    "the simplicity of our associative learning algorithm allowed us to consider the learning of the distinct words as independent stochastic processes .",
    "interactions between words , such as the mutual exclusivity constraint that instructs children to associate novel words to unnamed objects @xcite , are well - established in developmental psychology and it would be interesting to see whether and how they alter the characteristics of the critical phenomenon reported here .",
    "this research was supported by the southern office of aerospace research and development ( soard ) , grant no .",
    "fa9550 - 10 - 1 - 0006 , and conselho nacional de desenvolvimento cientfico e tecnolgico ( cnpq ) .",
    "p.f.c.t . was supported by fundao de amparo  pesquisa do estado de so paulo ( fapesp ) ."
  ],
  "abstract_text": [
    "<S> the associationist account for early word - learning is based on the co - occurrence between objects and words . here </S>",
    "<S> we examine the performance of a simple associative learning algorithm for acquiring the referents of words in a cross - situational scenario affected by noise produced by out - of - context words . we find a critical value of the noise parameter @xmath0 above which learning is impossible . </S>",
    "<S> we use finite - size scaling to show that the sharpness of the transition persists across a region of order @xmath1 about @xmath0 , where @xmath2 is the number of learning trials , as well as to obtain the learning error ( scaling function ) in the critical region . </S>",
    "<S> in addition , we show that the distribution of durations of periods when the learning error is zero is a power law with exponent @xmath3 at the critical point . </S>"
  ]
}