{
  "article_text": [
    "nonnegative matrix factorization ( nmf ) @xcite has attracted much attention from both research and engineering communities . given a data matrix with all its elements nonnegative , nmf tries to decompose it as a product of two nonnegative low - rank matrices .",
    "one matrix can be regarded as a basic matrix with its columns as basic vectors , and the the other one as linear combination coefficient matrix so that the original data columns in the original matrix could be represented as the linear contamination of the basic vectors .",
    "because of the nonnegative constrains on both the factorization metrics , it only allow the additive linear combination , and thus a parts - based representation could be archived @xcite . since the original nmf approach was proposed by seung and lee @xcite , due to its ability to learn the parts of the data set @xcite , it has been used as a effective data representation method in various problems , such as pattern recognition @xcite , computer vision @xcite , bioinformatics @xcite , etc .",
    "the most popular application of nmf as a data representation problem is pattern recognition , where the nonnegative feature vectors of the data samples are organized as a nonnegative matrix , and the columns of the coefficient matrix are used as the new low - dimension representations .    among the pattern recognition problems ,",
    "when nmf is performed to the data matrix , it is usually assumed that the class labels of the data samples are not available , making it an unsupervised problem .",
    "such typical application is clustering of images and documents @xcite .",
    "however , in real world supervised or semi - supervised classification applications , the class labels of the training data samples are usually available , which is ignored by most existed nmf methods .",
    "if the class label information could be unutilized during the representation procedure , the discriminative ability of the representation could be improved significantly . to this end , some supervised and semi - supervised nmf are proposed .",
    "for example , wang and jia @xcite proposed the fisher nonnegative matrix factorization ( fnmf ) method to encode discrimination information for a classification problem by imposing fisher constraints on the nmf algorithm .",
    "lee et al .",
    "@xcite proposed the semi - supervised nonnegative matrix factorization ( ssnmf ) by jointly incorporating the data matrix and the ( partial ) class label matrix into nmf .",
    "most recently , liu et al .",
    "@xcite the constrained nonnegative matrix factorization ( cnmf ) by incorporateing the label information as additional constraints .    in this paper , we propose to a novel supervised nmf method , by exploring the class label information and using it to constrain the coefficient vectors of the data samples .",
    "we consider the data sample pairs , and the class labels of the samples allow us to separate the pairs to two types  the within - class pair and the between - class pair .",
    "to improve the discriminate ability of the coefficient vectors of the samples , we consider the distance between the coefficient vectors of each sample pairs , and try to minimize that of the the within - class pairs , while maximize the that of the between - class pairs . in this way ,",
    "the coefficient vectors of data samples of the same class can be gathered , while that of different classes can be separated .",
    "one problem is how to assign different weights to differen pairs in the objective function . to avoid this problem",
    ", we apply a strategy similar to max - min distance analysis @xcite .",
    "the * maximum * within - class pair coefficient vector distance is minimized , so that all the within - class pair coefficient vector distance can be minimized also . meanwhile the * minimum * between - class pair coefficient vector distance is maximized , so that all the between - class pair coefficient vector distance can be maximized also .",
    "we construct a novel objective function for nmf to imposing both the maximum within - class pair distance minimization and the minimum between - class pair distance maximization problems . by optimizing it with an alternative strategy",
    ", we develop an iterative algorithm .",
    "the proposed method is called max - min distance nmf ( mmdnmf ) .",
    "the remaining parts of this paper is organized as follows : in section [ sec : met ] , we introduce the novel nmf method . in section [ sec : experiment ] , the experimental results are given to verify the effectiveness of the problem method . the paper is concluded in section [ sec : conclusion ] .",
    "in this section , we first formulate the problem with and objective function , and then optimize it to obtain the iterative algorithm .",
    "suppose we have @xmath0 data samples in the training set @xmath1 , where @xmath2 is the @xmath3-dimensional nonnegative feature vector of the @xmath4-th sample , we organize the samples as a nonnegative matrix @xmath5\\in \\mathbb{r}^{d\\times n}_+$ ] .",
    "the @xmath4-th column of the matrix @xmath6 is the feature vector of the @xmath4-th sample .",
    "their corresponding class label set is denoted as @xmath7 , where @xmath8 is the class label of the @xmath4-th sample , and @xmath9 is the class label space .",
    "nmf aims to find two low rank nonnegative matrices @xmath10 and @xmath11 , where @xmath12 , so that the product of them , @xmath13 , could approximate the original matrix , @xmath6 , as accurate as possible    @xmath14    the @xmath15 columns of matrix @xmath16 could be regarded as @xmath15 basic vectors , and each sample @xmath17 could be represented as the nonnegative linear combination of these basic vectors .",
    "the linear combination coefficient vector of @xmath17 is the @xmath4-th column vector @xmath18 of @xmath19 .",
    "we can also regard @xmath20 as the new low - dimensional presentation vector of @xmath17 with regard to the basic matrix @xmath16 . to seek the optimal matrices @xmath16 and @xmath19 , we consider the following problems to construct our objective function :    * to reduce the approximation error between @xmath6 and @xmath13 , the squared @xmath21 distance between them is usually minimized with regard to @xmath16 and @xmath19 as follows , + @xmath22 * we consider the training sample pairs in the training set , and separate them to two pair sets  the within - class pair set @xmath23 , and the between - class pair set @xmath24 .",
    "the within - class pair set is defined as the set of sample pair belonging to the same class , i.e. , @xmath25 .",
    "the between - class pair set is defined as the set of sample pairs belonging to different classes , i.e. , @xmath26 .",
    "to the two samples of the @xmath27-th pair in the new coefficient vector space , we use the squared @xmath21 norm distance between their coefficient vectors , @xmath28 .",
    "apparently , to improve the discriminate ability of the new nmf presentation , the coefficient vector distance of within - class pairs should be minimized while that of the between - class pairs should be maximized .",
    "instead of considering all the pairs , we directly minimize the maximum coefficient vector distance of within - class pairs , and thus we duly considers the aggregation of all within - class pairs , as follows , @xmath29 meanwhile , we also maximize the minimum coefficient vector distance of between - class pairs , and thus we consider the separation of all between - class pairs , as follows , + @xmath30 + in this way , the maximum within - class pair distance is minimized , so that all the within - class pair distances are also minimized .",
    "similarly , the minimum between - class pair distance is maximized , so that all the between - class pair distances are also minimized .    to formulate our problem ,",
    "we combine the problems in ( [ equ : nmf ] ) , ( [ equ : within ] ) and ( [ equ : between ] ) , and propose the novel optimization problem for nmf as    @xmath31    where @xmath32 and @xmath33 are the trade - off parameters .",
    "it should be noted that in ( [ equ : objective ] ) , the maximization and minimization problem are coupled , making it difficult to optimize . to solve this problem",
    ", we introduce to nonnegative slake variables @xmath34 and @xmath35 to represent the maximum coefficient vector distance between all within - class pairs , and the minimum coefficient vector distance between all between - class pairs . in this way , ( [ equ : objective ] )",
    "could be rewritten as    @xmath36    in this problem , the two slake variables are also optimized with the basic matrix @xmath16 and the coefficient matrix @xmath19 .      to solve the problem introduce in ( [ equ : objective1 ] ) , we come up with the lagrange function as follows ,    @xmath37    where @xmath38 is the lagrange multiplier for the constrain @xmath39 , @xmath40 is the lagrange multiplier for the constrain @xmath41 , @xmath42 is the lagrange multiplier matrix for @xmath43 , @xmath44 is the lagrange multiplier matrix for @xmath45 , @xmath46 is the lagrange multiplier for @xmath47 , and @xmath48 is the lagrange multiplier for @xmath35 .",
    "according to the duality theory of optimization @xcite , the optimal solution could be achieved by solving the following problem ,    @xmath49    by substituting ( [ equ : lagrange ] ) to ( [ equ : optim ] ) , we obtain the following problem ,    @xmath50    this problem is difficult to optimize directly . instead of solving it with regard to all the variables simultaneously , we adopt an alternate optimization strategy @xcite .",
    "the nmf factorization matrices @xmath16 and @xmath19 , slack variables @xmath51 and @xmath52 , and the lagrange multipliers @xmath53 and @xmath54 are updated alternatively in an iterative algorithm .",
    "when one variable is optimized , other variables are fixed .      by fixing other variables and removing the terms irrelevant to @xmath16 or @xmath19 , the optimization problem in ( [ equ : objective2 ] )",
    "is reduced to    @xmath55    where @xmath56 and @xmath57 with    @xmath58    @xmath59 is a diagonal matrix whose entries are column sums of @xmath60 , @xmath61 , and @xmath62 is a diagonal matrix whose entries are column sums of @xmath63 , @xmath64 . to solve this problem , we set the partial derivatives of the objective function in ( [ equ : objectiveuv ] ) with respect to @xmath16 and @xmath19 to zero , and we have    @xmath65    using the kkt conditions @xmath66 \\circ [ u ] = $ ] and @xmath67 \\circ [ v ] = 0 $ ] @xcite , where @xmath68\\circ [ ~]$ ] denotes the element - wise product between two matrices , we get the following equations for @xmath16 and @xmath19 :    @xmath69 \\circ [ u ] + [ uv v^\\top ] \\circ [ u ] = 0\\\\ & -   [ u^\\top x ] \\circ [ v ]   +   [ u^\\top uv ] \\circ [ v ] + [   v ( d-\\lambda)]\\circ [ v ] - [   v ( e-\\xi ) ] \\circ [ v ]   = 0 \\end{aligned}\\ ] ]    which lead to the following updating rules :    @xmath70}{[uv v^\\top]}\\circ [ u]\\\\ & v   \\leftarrow    \\frac{[u^\\top x +    v \\lambda + v e ] } { [ u^\\top uv + v d + v \\xi]}\\circ [ v ] \\end{aligned}\\ ] ]    where @xmath71}{[~]}$ ] is the element - wise matrix division operator .      by removing terms irrelevant to @xmath72 and @xmath73 and fixing all other variables , we have the following optimization problem with regard to only @xmath72 and @xmath73 :    @xmath74    by setting the partial derivatives of the objective function in ( [ equ : var ] ) with respect to @xmath72 and @xmath73 to zero , we have    @xmath75    using the kkt conditions @xmath76 and",
    "@xmath77 , we get the following equations for @xmath72 and @xmath73 :    @xmath78    which lead to the following updating rules :    @xmath79      based on ( [ equ : constrain ] ) , we have the following constrains for @xmath53 and @xmath54 ,    @xmath80    by considering these constrains , fixing other variables and removing terms irrelevant to @xmath53 and @xmath54 from ( [ equ : optim ] ) , we have the following problem with regard to @xmath53 and @xmath54 ,    @xmath81    this problem can be solved as a linear programming ( lp ) problem .",
    "in this paper , we investigate how to use the class labels of the data samples to improve the discriminative ability of their nmf representations . to explore the class label information of the data samples , we consider the within - class sample pairs with the same class labels , and also the between - class sample pairs with different class labels . apparently , in the nmf representation space",
    ", we need to minimize the distances between the within - class pairs , and also maximize the distances between the between - class pairs .",
    "inspired by the max - min distance analysis @xcite , we also consider the extreme situation : we pick up the maximum within - class distance and then try to minimize it , so that all the within - class distances are also minimized , and we pick up the minimum between - class distance and then maximize it , so that all the between - class distances are maximized . differently from the max - min distance analysis , which only pick up",
    "the maximize the between - class distance and minimize it , we consider the between - class and within class distances dually .",
    "s.  z. li , x.  w. hou , h.  j. zhang , q.  s. cheng , learning spatially localized , parts - based representation , in : computer vision and pattern recognition , 2001 .",
    "cvpr 2001 .",
    "proceedings of the 2001 ieee computer society conference on , vol .  1 , ieee , 2001 , pp . i207",
    ".      a.  shashua , t.  hazan , non - negative tensor factorization with applications to statistics and computer vision , in : proceedings of the 22nd international conference on machine learning , acm , 2005 , pp . 792799 .",
    "h.  liu , z.  wu , x.  li , d.  cai , t.  s. huang , constrained nonnegative matrix factorization for image representation , pattern analysis and machine intelligence , ieee transactions on 34  ( 7 ) ( 2012 ) 12991311 .              f.  r. bach , g.  r. lanckriet , m.  i. jordan , multiple kernel learning , conic duality , and the smo algorithm , in : proceedings of the twenty - first international conference on machine learning , acm , 2004 , p.  6 ."
  ],
  "abstract_text": [
    "<S> nonnegative matrix factorization ( nmf ) has been a popular representation method for pattern classification problem . </S>",
    "<S> it tries to decompose a nonnegative matrix of data samples as the product of a nonnegative basic matrix and a nonnegative coefficient matrix , and the coefficient matrix is used as the new representation . </S>",
    "<S> however , traditional nmf methods ignore the class labels of the data samples . in this paper , we proposed a supervised novel nmf algorithm to improve the discriminative ability of the new representation . using the class labels , we separate all the data sample pairs into within - class pairs and between - class pairs . to improve the discriminate ability of the new nmf representations , we hope that the maximum distance of the within - class pairs pairs in the new nmf space could be minimized , while the minimum distance of the between - class pairs pairs could be maximized . </S>",
    "<S> with this criteria , we construct an objective function and optimize it with regard to basic and coefficient matrices , and slack variables alternatively , resulting in a iterative algorithm .    </S>",
    "<S> nonnegative matrix factorization . </S>",
    "<S> , max - min distance analysis </S>"
  ]
}