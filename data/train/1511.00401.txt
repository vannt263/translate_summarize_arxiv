{
  "article_text": [
    "in this paper we present part of our on - going efforts towards prototyping ` data - driven processing ' for the square kilometre array ( ska ) science data processor ( sdp ) .",
    "the ska will require advances of several orders of magnitude in the processing of data , pushing radio astronomy into the forefront of the ` big - data challenge ' @xcite .",
    "the ska phase 1 data - rates out of the correlator will approach a terabyte / sec for ska - mid and ska - low @xcite combined .",
    "the final stage , with the complete collecting area , will be an order of magnitude higher .",
    "the calculation of the required data processing rates for the sdp is highly dependent on the science case , but will be several hundred petaflops in the most extreme cases . to deliver such performance , highly parallel computer processing solutions are required and in this paper we set out to explore some of the options .    in this investigation",
    "we are testing pipeline solutions for the calibrated and flagged datasets from the karl g. jansky very large array ( vla ) deep hi survey chiles , the cosmos hi large extragalactic survey @xcite .",
    "this survey aims to study the neutral atomic hydrogen ( hi ) content of galaxies over 4 billion years of cosmic time , approximately 1/3 the history of the universe and twice the lookback time of any previous emission - line survey .",
    "hi is a crucial ingredient to study for understanding galaxy evolution as it is the dominant baryonic fuel out of which stars and galaxies are ultimately made , as well as being an important tracer of galaxy kinematics .",
    "such surveys have previously been too expensive to carry out due to limitations in telescope technology and back - end processing resources .",
    "the chiles  survey will be one of the prime pathfinders for the data processing on ska scales .",
    "( although not alone in this ; the lofar project @xcite also has similar challenges . )",
    "the data volumes and processing requirements mean this project will stretch the bounds of current computing capability .",
    "the chiles  project therefore is performing a crucial role in our prototyping investigations for the key sdp concepts and approaches .",
    "chiles  is running at the vla , which is a 27 antenna array . the new , upgraded front - end ( wide - band l - band receivers ) and back - end",
    "( the widar correlator ) @xcite , provided through the expanded very large array project @xcite , can now provide instantaneous coverage for spectral line observing between @xmath0940 and @xmath01430-mhz on the sky ( 15 spectral windows of 32mhz , each giving a total of 480mhz in each session ) .",
    "the observations are dithered in frequency to smooth out the edges of the spectral windows .",
    "the antennas ( being 25 m in diameter ) see about 0.5 degrees ( @xmath02000 arcsecond ) across the sky at the pointing centre .",
    "the array configuration is vla - b , which has 11 km baselines and a typical beam size of @xmath05x7 arcsec at 1.4 ghz , assuming natural weighting .",
    "we are currently oversampling this with 2048 pixels of 1 arcsecond in size during this development phase .",
    "this data is in 15.625khz channels ( representing about 3km / s at the rest frequency of the hi line being observed ) .",
    "therefore there are 351 baselines , a little less than 31,000 channels per polarization product to be processed , and a full field of view of 2048x2048 pixels in the image plane .",
    "these datasets are therefore much larger than those normally analysed and therein lies the challenge .",
    "we expect about five epochs of observing ( the epochs are defined by when the vla is in the correct configuration for the science , approximately every 15 months ) .",
    "the first epoch was of 178 hours in total , broken into 42 days worth of observing , with each day s observation between 1 and 6 hours long .",
    "the mean size of the flagged and calibrated dataset from a days observations in the first epoch are 330 gb , with the maximum size being 803 gb and the minimum being 45 gb . the second epoch of 213 hours has been observed but not yet calibrated .",
    "one of the main challenges for the project will be to produce the image cubes from the observations , post - flagging and calibrating .",
    "the processing steps required to complete this analysis will be presented in a forthcoming paper ; here we limit ourselves to the investigations on the computing resources required to undertake the analysis .",
    "to investigate the application and total costs of operation for the workflow in a range of indicative environments we have repeated the same data - reduction pipeline on three very different computing infrastructures : a moderate sized cluster ( _ pleiades _ ) , such as a group like icrar could ( and does ) host and control ; a high performance computing cluster ( _ magnus _ ) that would be provided by a national facility such as the pawsey centre @xcite and a cloud computing environment , such as provided by the amazon web service ( _ aws _ ) .",
    "this allowed us to explore three very different approaches , all of which would be of the scale accessible to groups such as ours via in - house capital expenditure , via competitive applications for resources on national infrastructure or via cumulative operational expenditure , respectively .",
    "it should be noted that the ska infrastructure is not necessarily limited to the above three candidate environments , so these may not represent the final choice of ska architecture .",
    "we have found that the copying stage is an important work item , which is not normally considered part of the data reduction pipeline . given the size of the input and output data items",
    ", we tried to keep the data movement to a minimum .",
    "the data was moved from the chiles project s data repository at nrao ( socorro ) to perth for every individual observation , after the flagging and calibration steps had been completed .",
    "once the data was in perth it was stored on a large dedicated storage pod and then copied to the target test environments of _ aws _  and _ magnus_. the storage pod is connected via a 10 g ethernet connector .    the copying stage , as it is immediately followed with the splitting , could be combined into a single process or be staged via temporary short term storage .",
    "the data - reduction tools we use are exclusively from casapy  @xcite version 4.3 and therefore the issues that arose all revolve around limitations in the casa performance , as discussed in the following sections .",
    "note that ` operations ' are printed in small capitals ; ` tasks ' used to perform the operations are printed in small capitals with brackets appended .",
    "the data provided by nrao has been calibrated and the target field selected , therefore the remaining operations to be performed in the hi - data reduction are : to copy the data to an accessible point , to split the data into manageable sizes whilst correcting for the station doppler shifts of that day and selecting specific frequency ranges and to fourier invert the observed data ( taken in the reciprocal of the image domain ) into a 3-d image cube ( these dimensions being right ascension , declination and velocity ) . additionally one should deconvolve the image to correct the initial ` dirty ' image for the spatially extended point spread function ( psf ) , which is the fourier transform of the points in reciprocal space where the antenna pairs measured the correlated signal . after the deconvolution",
    "the psf is replaced with a compact gaussian representing the maximum resolution to form the ` clean ' image .",
    "see @xcite for a full discussion of these concepts .",
    "traditionally one would read all the data files simultaneously and invert to produce an image cube , but this is not possible as the task clean ( ) @xcite fails due to the extreme size of the input datasets .",
    "this is why we must split the input data into smaller frequency ranges , or sub - bands , and perform invert  on all of the days simultaneously , but with fewer channels .",
    "the copying does not involve data reduction , so is discussed in detail later .",
    "once the data is accessible we need to pre - select the data from the input measurementsets(henceforth referred to as the operation split ) . for this",
    "we have trialed the tasks split ( ) , cvel ( ) and finally mstransform ( ) and have settled on the latter , as that was the fastest .",
    "we note that the doppler shift correction process involves ffting the entire selected frequency range ( or spectral window ) before applying the doppler shift , then selecting only those channels which fall in the requested frequency range .",
    "this process has a strong potential for improved efficiency .",
    "the invert  operation takes the frequency split data and combines the many days into images for that frequency range . in these investigations",
    "we have not deconvolved the images , except to investigate the residual noise as discussed in section [ sec : resid ] .",
    "we found that with the limited frequency ranges we were using , the noise levels per channel were very sensitive to the weighting scheme .",
    "for example uniform weighting ( or briggs weight -2 ) causes large increases of noise at the edges of the sub - cube .",
    "this is due to the implementation of the briggs scheme in casapy , which depends on the total data selected not just the data channel being imaged . obviously in this case , where we have a limited input frequency span of data points from which to derive the weights , the edge data displays enhanced noise .",
    "however with natural weighting ( equivalent to briggs + 2 ) , or any briggs weight greater than @xmath01 this does not occur .",
    "alternative weighting schemes are being discussed , both in the casapy  team and within the literature ( see @xcite ) so this should not be an issue for the sdp continuum imaging pipeline architectures .",
    "the final operation is to combine the individual small image cubes into the required full sized cube .",
    "the final cube is 2048@xmath1 pixels by 30,720 channels , single stokes , resulting in a @xmath0500 gb cube when using 4 bytes per voxel . in these tests",
    "we have not explored the range of options for deconvolution and continuum subtraction , as the detailed plan is still being developed . this latter step could be post - clean ( distributed but frequency independent ) or post - combination . in the latter case the parallelisation could come from subtraction along individual pixels of the signal averaged ( perhaps with a spectral dependence ) along frequency .",
    "the obvious issue for massive datasets is how to process them in a parallel fashion .",
    "it is an often - stated fact that `` astronomy data is embarrassingly parallel '' ; image cubes can be formed per frequency channel and calibration can be performed per solution interval .",
    "these are to a large extent completely independent of each other .",
    "however when working with the standard data formats , be they fits or measurementsets , the combination of the visibility data into a single file or file structure limits the immediate implementation of the natural parallelism .",
    "therefore we have derived a workflow , see figure [ fig : workflow_image2 ] , which would divide the data between work - units in a usable fashion .",
    "the input data was per day , and this was used as the division in the first stages ( copy and split ) .",
    "after splitting the day s data into multiple frequency sub - bands the images can be formed in parallel by combining all the days across the different frequency sub - bands simultaneously .",
    "in this section , we discuss the workflow for the analysis pipeline and how we assessed it .",
    "the detailed workflow model we are following is shown in figure [ fig : workflow_image2 ] , which represents the data distribution breakdown . for each day",
    "( that is n@xmath2 parallel processes ) there is a copy process ( or the ingest stage ) , followed by a split process to separate each frequency .",
    "the ingested measurementsets  could be retained in a temporary archive .",
    "the split includes the data re - ordering , as the outputs are files that are frequency sorted as well as divided by day .",
    "the invert  process ( that is n@xmath3 parallel processes ) takes the frequency split data and combines the many days into images for that frequency range .",
    "the final process is the combination of the frequency ranges into a concatenated cube .",
    "represents the number of visibilities collected per time step , ranging from @xmath04,000 to @xmath06,000 .",
    "the number of sub - bands @xmath4 is determined by the instantaneous bandwidth ( i.e. 480 mhz ) and each subband s frequency width @xmath5 ( e.g. 4mhz).,scaledwidth=90.0% ]      the work flow is controlled by a set of python and shell scripts .",
    "the shell scripts contain the setup information ( and are provided to the queue ) and the python scripts extract that setup information to drive the process .",
    "the pipeline scripts for the three environments are extremely similar , but have to be independent because of the different processing environments .",
    "we use two different methods to collect performance metrics for both compute ( e.g. cpu and memory usage ) and i / o ( e.g. i / o operations , throughput , inter - arrival time , etc . ) . in the first method ,",
    "we periodically measure a list of process - specific kernel counters available in the linux proc  file system @xcite while the processing tasks were running .",
    "the sample interval is currently one measurement per second .",
    "while this method provides useful measurements on cpu and memory usage , some detailed i / o metrics can not be directly derived from the proc  file system .",
    "therefore , we used a second method  the strace  @xcite linux system tool  to access more advanced i / o performance indicators , such as whether the i / o requests were sequential or random and the size of each i / o requests issued to the underlying file system .",
    "strace  is able to capture all system calls and signals .",
    "however , since we are only interested in i / o requests made by the application as system calls , we instruct strace  to only measure four types of system calls  file descriptor related , process management - related ( in order to track sub - processes ) , socket - related and those which take a file name as an argument such as open , close , read , write , etc .",
    "one issue of strace  is the tracing overhead associated with frequent context switching , which can vary between less than 10% ( for hundreds of system calls per second ) and over 100% ( for tens of thousands of system calls per second ) .",
    "this overhead in turn substantially prolongs the application completion time .",
    "however , this is not an issue for profiling the i / o access patterns ( random and sequential ) , which are basically time invariant .",
    "the proc  and strace  measurements are both provided in the additional data products available in the online version of this paper and only the results from proc  are plotted in the printed version .",
    "three test environments were selected to represent three different approaches to the data reduction . our goal is to present options for deciding which model for sourcing computational resources would best match both the specific case we address ( i.e. chiles ) and guide the resourcing decisions that will be raised for other computing problems .      _",
    "pleiades _  was specifically designed and built to provide a development platform for icrar s hpc projects .",
    "it is a six - compute node hpc cluster ( + 1 head node ) located at icrar .",
    "each of the compute nodes currently contains a dual intel xeon x5650 2.66ghz cpu , 64 - 192 gb of ram , one tesla or two k10 or k20 gpus , and a mellanox mt26428 qdr ( 40gbps ) infiniband interconnect .",
    "in addition , a dedicated storage node provides persistent data across the qdr infiniband fabric .",
    "the chiles  dataset is provided on a triple raid-6 striped volume that is 147 tb in size .      the _ magnus _",
    "hpc cluster is provided by the pawsey centre , which plays a key role in the australian government s strategy to provide high level scientific computing resources for the australian research community .",
    "it is sited in perth , owned by csiro and managed by the pawsey supercomputing centre .    _",
    "_  comprises 1536 nodes in 384 blades .",
    "each compute node hosts two 12-core , intel xeon e5 - 2690v3 `` haswell '' processors running at 2.6 ghz , for a total of 35,712 cores , delivering in excess of 1 petaflop of computing power .",
    "each node hosts 64 gb of ram .",
    "the nodes communicate amongst themselves over cray s high - speed , low - latency aries interconnect . global storage",
    "( also known as the scratch file system ) is provided by a three - cabinet cray sonexion 1600 lustre appliance , with a usable capacity of 3pb and a sustained read / write performance of 70 gb / sec . in the november 2014 top500 list , _ magnus",
    "_  debuted at # 41 , achieving 1,097 teraflops ( 1 petaflop+ ) . at the time of writing , this makes _",
    "magnus _  the most advanced scientific supercomputer in the southern hemisphere . for our investigations we requested 44-nodes of _ magnus",
    "_ , about 3% of the total computing power .",
    "this ratio is carried forward for the calculation of the fractional capital expenditure .",
    "the cloud environment allows for considerable flexibility , which is discussed below ; the main constraint is how much one is willing to pay for the performance .",
    "the code to run the pipeline on _ aws _  is written in python using the boto package  @xcite .",
    "this allows us to start many servers with different configurations on demand when we need them .",
    "our python scripts will always look for the cheapest spot price in the regions specified .",
    "disk storage is provided by amazon elastic block store ( ebs ) .",
    "amazon ebs volumes are network - attached , and persist independently from the life of an amazon elastic compute cloud ( ec2 ) instance .",
    "amazon provides three volume types : general purpose ( ssd ) , provisioned iops ( ssd ) , and magnetic .",
    "general purpose ( ssd ) is the ssd - backed general purpose ebs volume type .",
    "io rates are primarily controlled by the instance types generic network capacity .",
    "provisioned iops ( ssd ) volumes offer storage with consistent and low - latency performance .",
    "these were used for all ebs instances to improve the iops required by casapy .",
    "initial experiments showed the general purpose ssd gave about 20 - 30 iops with casapy .",
    "provisioned iops improved this to between 90 - 100 iops .",
    "magnetic storage was not used .",
    "many amazon ec2 instance types can also access disk storage located on ssd disks that are physically attached to the host computer and do not persist .",
    "this disk storage is referred to as instance store or ephemeral storage .",
    "this was used for scratch storage for some processing tasks and when data was to be written to long term storage in the amazon simple storage service ( s3 ) .",
    "amazon s3 provides access to a reliable data storage infrastructure .",
    "s3 stores data as objects within resources called `` buckets '' .",
    "one can store as many objects as required within a bucket , and write , read , and delete objects in the bucket .",
    "objects can be up to 5 tb in size .",
    "s3 is designed for 99.999999999% durability and 99.99% availability of objects over a given year .",
    "there is also a low - cost reduced redundancy storage option for less critical data , and amazon glacier for long term storage where access time is not important .",
    "all our work used the reduced redundancy s3 storage .",
    "aws has two relevant pricing models .",
    "a third option exists call reserved instances , but that requires the purchase of 1 or 3 year contracts and was not used for these tests .    on - demand instances : : :    these provide the purchase of compute capacity by the hour with no    long - term commitments or upfront payments .",
    "one can increase or    decrease the compute capacity depending on the demands of the    application and only pay the specified hourly rate for the instances    used .",
    "spot instances : : :    these provide the ability to purchase compute capacity at hourly    rates , usually at lower cost than the on - demand rate .",
    "spot instances    allow us to specify the maximum hourly price that we are willing to    pay to run a particular instance type .",
    "ec2 sets a spot price for each    instance type in each availability zone , which is the price all    customers will pay to run a spot instance for that given period .",
    "the    spot price fluctuates based on supply and demand for instances , but    customers will never pay more than the maximum price they have    specified .",
    "if the spot price moves higher than a customer s maximum    price , the customer s instance will be shut down after a two minute    warning .",
    "table [ tab : price_comparison ] shows the difference in cost    between on demand and spot prices at the aws sydney data centre during    the test runs . for the final processing only spot instances ,",
    "being    significantly cheaper , were used .",
    ".a table showing the typical difference in cost between on demand and spot prices on the aws cloud .",
    "these numbers are for the sydney data centre on 6 mar 2015 [ cols=\"<,^,^\",options=\"header \" , ]      we are using these studies to refine and develop our operating infrastructure .",
    "we list here the improvements we are making for processing the second epoch of chiles  data , as informed by the performance measurements made .",
    "these are improvements which are probably of interest to all facility managers .",
    "* ssd for local high speed scratch space .",
    "we are installing local ssd disks on all nodes of _ pleiades _  as that will allow a high - speed random access on locally - hosted data files . with this",
    "we maybe able to complete the processing on a moderate sized cluster .",
    "* improved i / o performance .",
    "conversations with aws are underway to improve the i / o limitations we have been struggling with .",
    "* trialling the next generation archive system ngas @xcite for the transfer of the data from nrao to the _ aws _  infrastructure .",
    "we will attempt to perform the entire data reduction chain , including flagging and calibration , on the cloud computing platform . * developing a data - driven workflow for the chiles  project , which will be able to prototype many of the sdp concepts and pipeline designs .",
    "* a new task is being developed in casapy , uvgridder ( ) can cumulatively grid all days onto one uv - grid , which may prove to be the best approach @xcite .",
    "the karl g. jansky very large array and the national radio astronomy observatory is a facility of the national science foundation operated under cooperative agreement with associated universities inc .. we wish to thank the chiles team for flagging and calibrating the data used in these tests .",
    "this work was supported by grants from amazon web services , the astrocompute project and by resources provided by the pawsey supercomputing centre with funding from the australian government and the government of western australia .",
    "the _ magnus _  supercomputer @xcite is managed by the pawsey supercomputing centre . the chiles project is supported by a collaborative research grant from nsf .",
    "24 natexlab#1#1url # 1`#1`urlprefix[2][]#2 [ 2]#2 , . .",
    ". , . . , . , , , ,",
    ", , , , , , . .",
    ". , . . , , , , , , , .",
    ", , , , , , , , , , et  al . , . .",
    ", . , , , , . , in : , p. . ,",
    ", , , , , . , in : , p. .",
    ", , , , . . ,",
    ", , , , , , . .",
    ". , . . , , , .",
    ". , . . , , , , , , , , , , et  al . ,",
    ". . , . , ,",
    ". . , . , , , ,"
  ],
  "abstract_text": [
    "<S> we present the results of our investigations into options for the computing platform for the imaging pipeline in the chiles  project , an ultra - deep hi pathfinder for the era of the square kilometre array . </S>",
    "<S> chiles  pushes the current computing infrastructure to its limits and understanding how to deliver the images from this project is clarifying the science data processing requirements for the ska . </S>",
    "<S> we have tested three platforms : a moderately sized cluster , a massive high performance computing ( hpc ) system , and the amazon web services ( aws ) cloud computing platform . </S>",
    "<S> we have used well - established tools for data reduction and performance measurement to investigate the behaviour of these platforms for the complicated access patterns of real - life radio astronomy data reduction . </S>",
    "<S> all of these platforms have strengths and weaknesses and the system tools allow us to identify and evaluate them in a quantitative manner . with the insights from these tests we are able to complete the imaging pipeline processing on both the hpc platform and also on the cloud computing platform , which paves the way for meeting big data challenges in the era of ska in the field of radio astronomy . </S>",
    "<S> we discuss the implications that all similar projects will have to consider , in both performance and costs , to make recommendations for the planning of radio astronomy imaging workflows .    </S>",
    "<S> methods : data analysis , parallel architectures : multicore architectures , distributed architectures : cloud computing , chiles </S>"
  ]
}