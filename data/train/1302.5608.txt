{
  "article_text": [
    "since the pioneering work of joachims @xcite and platt  @xcite , support vector machine ( svm ) training is dominated by decomposition algorithms solving the dual problem .",
    "this approach is at the core of the extremely popular software libsvm , although a number of important algorithmic improvements have been added over the years  @xcite .    for linear svm training",
    ", the situation is different .",
    "it has been observed that a direct representation of the primal weight vector is computationally advantageous over optimization of dual variables , since the dimensionality of the optimization problem is independent of the number of training patterns . at first glance",
    "this hints at solving the primal problem directly , despite the non - differential nature of the hinge loss .",
    "influential examples of this research direction are the cutting plane approach @xcite and the stochastic gradient descent `` pegasos '' algorithm @xcite .",
    "hsieh et al .",
    "@xcite were the first to notice that it is possible to solve the dual problem with a decomposition algorithm similar to those used for non - linear svm training while profiting from the fixed dimensionality of the weight vector .",
    "this method combines fast ( linear ) convergence of its kernelized counterpart and a direct representation of the weight vector , which allows to perform update steps in time independent of the size of the training set .",
    "the method has been demonstrated to outperform several algorithms for direct optimization of the primal problem @xcite .",
    "we refer the reader to the excellent review @xcite and references therein for a detailed discussion of the differences , as well as for the relation of the method to non - linear ( kernelized ) svm training .",
    "our study builds upon this work .",
    "we present an algorithmic improvement of the dual method @xcite .",
    "this new method differs from the existing algorithm in two aspects , namely the selection of the currently active sub - problem and the shrinking heuristic .",
    "the selection of the working set , defining the sub - problem in the decomposition algorithm , has been subject to extensive research @xcite . however , these elaborate methods are not affordable in the algorithm @xcite , and they are replaced with systematic sweeps over all variables .",
    "we propose a more elaborate method that takes recent experience into account and adapts selection frequencies of individual variables accordingly . as a side effect",
    ", this algorithm replaces the existing shrinking heuristic @xcite .",
    "our experimental evaluation shows that this new method can achieve considerable speed - ups of more than an order of magnitude .",
    "the remainder of this paper is organized as follows . in the next section we review the dual training algorithm by @xcite and",
    "introduce our notation .",
    "then we present our modifications in section  [ sec : working - set - selection ] and an extensive experimental evaluation thereof in section  [ sec : experiments ] .",
    "we discuss the results ( section  [ sec : discussion ] ) and close with our conclusions ( section  [ sec : conclusion ] ) .",
    "in this section we describe the dual algorithm for linear svm training @xcite , as implemented in the software liblinear  @xcite . it should be noted that the liblinear software supports a large number of training methods for linear models , such as binary and multi - category classification and regression , as well as different regularizers and loss functions . in this study",
    "we restrict ourselves to the most basic case , which is the `` standard '' svm with hinge loss and two - norm regularizer .",
    "however , our proceeding is general in nature and therefore applicable to many of the above cases .    given a binary classification problem described by training data @xmath0 svm training corresponds to finding the solution of the ( primal ) optimization problem @xmath1 where @xmath2 is the hinge loss and @xmath3 controls the solution complexity @xcite .",
    "the prediction of the machine is of the form @xmath4 , @xmath5 , based on the linear decision function @xmath6 . in this formulation",
    "we have dropped the constant offset @xmath7 that is often added to the decision function , since it turns out to be of minor importance in high dimensional feature spaces , and dropping the term results in attractive algorithmic simplifications ( see , e.g,@xcite ) .",
    "the corresponding dual optimization problem is the box constrained quadratic program @xmath8 it holds @xmath9 .",
    "since the dual training method is rooted in non - linear svm training , we want to mention that in general all inner products @xmath10 between training examples are replaced with a usually non - linear mercer kernel function  @xmath11 .    a standard method for support vector machine training",
    "is to solve the dual problem with a decomposition algorithm @xcite .",
    "the algorithm decomposes the full quadratic program into a sequence of sub - problems restricted to few variables .",
    "the sub - problems are solved iteratively until an overall solution of sufficient accuracy is found .",
    "sequential minimal optimization ( smo ,  @xcite ) refers to the important special case of choosing the number of variables in each sub - problem minimal . for the above dual",
    "this minimal number is one , so that the algorithm essentially performs coordinate ascent .",
    "the skeleton of this method is shown in algorithm  [ algo : smo ] .",
    "select active variable @xmath12 solve sub - problem restricted to variable @xmath12 update @xmath13 and further state variables    the training algorithm for linear svms by @xcite is an adaptation of this technique .",
    "its crucial algorithmic improvement over standard smo is to reduce the complexity of each iteration from @xmath14 to only @xmath15 .",
    "is further reduced to @xmath16 , where @xmath17 is the number of non - zero components of the currently selected training example  @xmath18 .",
    "] the key trick is to keep track of the primal vector @xmath19 during the optimization .",
    "this allows to rewrite the derivative of the dual objective function as @xmath20 which can be computed in @xmath15 operations .",
    "the requirement to perform all steps inside the smo loop in @xmath15 operations makes some changes necessary , as compared to standard smo .",
    "for instance , the flat smo loop is split into an outer and an inner loop .",
    "the full algorithm is provided in detail as algorithm  [ algo : liblinear ] .",
    "@xmath21 ; @xmath22 ; @xmath23 @xmath24 ; @xmath25 @xmath26 @xmath27 @xmath27 * if * ( @xmath28 and @xmath29 ) * then * @xmath30 * if * ( @xmath31 and @xmath32 ) * then * @xmath33 @xmath34_{-\\alpha_i}^{c-\\alpha_i}$ ] @xmath35 @xmath36 * if * ( @xmath37 ) * then * * break * @xmath21 ; @xmath22 ; @xmath23 * if * @xmath38 * then * @xmath39 * else * @xmath22 * if * @xmath40 * then * @xmath41 * else * @xmath23    most prominently , the selection of the active variable @xmath42 ( defining the sub - problem to be solved in the current iteration ) can not be done with elaborate heuristics that are key to fast training of non - linear svms @xcite . instead , the algorithm performs systematic sweeps over all variables",
    ". the order of variables is randomized .",
    "the ( amortized ) complexity of selecting the active variable is @xmath43 .",
    "the solution of the sub - problem amounts to @xmath44_0^c          = \\left [ \\alpha_i + \\frac{1 - y_i \\langle x_i , w \\rangle}{\\|x_i\\|^2 } \\right]_0^c      \\enspace,\\end{aligned}\\ ] ] where @xmath45_a^b = \\max\\{a , \\min\\{b , x\\}\\}$ ] denotes clipping to the interval @xmath46 $ ] .",
    "this operation , requiring two inner products , is done in @xmath15 operations ( in the implementation the squared norm is precomputed ) .    the usual smo proceeding to keep track of the dual gradient @xmath47 is not possible within the tight budget of @xmath15 operations .",
    "instead the weight vector is updated .",
    "let @xmath48 denote the step performed for the solution of the sub - problem , then the weight update reads @xmath36 , which is an @xmath15 operation .",
    "the usual stopping criterion is to check the maximal violation of the karush - kuhn - tucker ( kkt ) optimality conditions . for the dual problem",
    "the kkt violation can be expressed independently for each variable .",
    "let @xmath49 denote the derivative of the dual objective .",
    "then the violation is @xmath50 if @xmath51 , @xmath52 for @xmath53 , and @xmath54 for @xmath55 .",
    "the algorithm is stopped as soon as this violations drops below some threshold  @xmath56 ( e.g. , @xmath57 ) .    in the original smo algorithm",
    "this check is a cheap by - product of the selection of the index  @xmath12 . since keeping track of the dual gradient",
    "is impossible , the exact check becomes an @xmath14 operation .",
    "this is the complexity of a whole sweep over the data . in algorithm  [ algo : liblinear ] the exact check",
    "is therefore replaced with an approximate check , where each variable is checked at the time it is active during the sweep .",
    "thus , all variables are checked , but not exactly at the time of stopping .",
    "the algorithm keeps track of @xmath58 and @xmath59 , and check for @xmath60 at the end of the sweep .    to exploit the sparsity of the svm solution",
    ", the algorithm is equipped with a shrinking heuristic .",
    "this heuristic removes a variable from the set @xmath61 of active variables if it is at the bounds and the gradient of the dual objective function indicates that it will stay there . after a while , this heuristic can remove most variables from the problem , making sweeps over the variables much faster .",
    "the drawback of this heuristic is that it can fail .",
    "therefore , at the end of the optimization run , the algorithm needs to check optimality of the deactivated variables .",
    "the detection of a mistake results in continuation of the loop , which can be costly .",
    "therefore , the decision to remove a variable needs to be conservative .",
    "the algorithm removes a variable only if it is at a bound and its gradient @xmath62 pushes against the bound with a strength that exceeds the maximal kkt violation of active variables during the previous sweep .",
    "this amounts to the condition @xmath63 for @xmath53 and to @xmath64 for @xmath55 .",
    "in this section we introduce our algorithmic improvement to the above described linear svm training algorithm .",
    "our modification targets two weaknesses of the algorithm at once .",
    "* algorithm  [ algo : liblinear ] executes uniform sweeps over all active variables .",
    "in contrast to the smo algorithm for non - linear svm training , the selection is not based on a promise of the progress due to this choice . although the computational restriction of @xmath15 operations does not allow for a search for the best a - priori guarantee of some sort ( such as the largest kkt violation ) , we can still learn from the observed progress after a step has been executed .",
    "* shrinking of variables is inevitably a heuristic .",
    "algorithm  [ algo : liblinear ] makes `` hard '' shrinking decisions by removing variables based on adaptive thresholds on the strength with which they press against their active constraints .",
    "it is problematic that even a single wrong decision to remove a variable early on can invalidate a large share of the algorithm s ( fine tuning ) efforts later on .",
    "therefore we replace this mechanism with what we think of as `` soft '' shrinking , which amounts to the reduction of the selection frequency of a variable , down to a predefined minimum .    in algorithm",
    "[ algo : liblinear ] there are only two possible frequencies with which variables are selected .",
    "all inactive variables are selected with frequency zero , and all active variables are selected with the same frequency @xmath65 .",
    "this scheme is most probably not optimal ; it is instead expected that some variables should be selected much more frequently than others .",
    "established working set selection heuristics aim to pick the best ( in some sense ) variable for the very next step , and therefore automatically adapt relative frequencies of variable selection over time .",
    "this is not possible within the given framework .",
    "however , we can still use the information of whether a step has made good progress or not to adapt selection frequencies for the future .",
    "this adaptation process is similar to so - called self - adaptation heuristics found in modern direct search methods , see e.g.  @xcite . to summarize , although we are unable to determine the best variable for the present step , we can still use current progress as an indicator for future utility .    for turning this insight into an algorithm",
    "we introduce adaptive variable selection frequencies based on preference values @xmath66 .",
    "the relative frequency of variable @xmath13 is defined as @xmath67 in each iteration of the outer loop the algorithm composes a schedule ( a list of @xmath68 variables indices ) that reflects these relative frequencies .",
    "this task is performed by algorithm  [ algo : schedule ] . with @xmath69 operations it is about as cheap as the randomization of the order of variables .",
    "we call this novel variable selection scheme _ adaptive variable selection frequencies _ ( avsf )",
    ".    the crucial question is : how to update the preferences @xmath70 over the course of the optimization run ? for this purpose the gain @xmath71 of an iteration with active variable @xmath13 is compared to the average ( reference ) gain @xmath72 . since the average gain decreases over time",
    ", this value is estimated as a fading average .",
    "the preference is changed by the rule @xmath73_{p_{\\min}}^{p_{\\max } }      \\enspace.\\end{aligned}\\ ] ] in our experiments we set the constants to @xmath74 , @xmath75 , and @xmath76 .",
    "the bounds @xmath77 ensure that the linear convergence guarantee established by theorem  1 in @xcite directly carries over to our modified version . the adaptation of preference values is taken care of by algorithm  [ algo : preferences ] .",
    "the added complexity per iteration is only @xmath43 .",
    "the dual objective gain @xmath78 is used in modern second order working set selection algorithms for non - linear svm training @xcite .",
    "our method resembles this highly efficient approach ; it can be understood as a time averaged variant .",
    "it is important to note that the above scheme does not only increase the preferences and therefore the relative frequencies of variables that have performed above average in the past .",
    "it also penalizes variables that do not move at all , typically because they are at the bounds and should be removed from the active set : such steps give the worst possible gain of zero .",
    "thus , the algorithm quickly drives their preferences to the minimum .",
    "however , they are not removed completely from the active set . checking these variables from time to time",
    "is a good thing , because it is cheap compared to uniform sweeps on the one hand and at the same time avoids that early mistakes are discovered only very much later .",
    "another difference to the original algorithm is that shrinking decisions are not based on kkt violations , but instead on relative progress in terms of the dual objective function .",
    "we are not aware of an existing approach of this type .",
    "algorithm  [ algo : avsf ] incorporates our modifications into the liblinear algorithm .",
    "the new algorithm is no more complex than algorithm  [ algo : liblinear ] , and it requires only a hand full of changes to the existing liblinear implementation .",
    "@xmath79 ; @xmath80 @xmath81 canstop @xmath82 true @xmath83 define schedule @xmath84 ( algorithm  [ algo : schedule ] ) @xmath26 * if * ( @xmath28 and @xmath85 ) * then * @xmath86 * if * ( @xmath31 and @xmath87 ) * then * @xmath88 @xmath34_{-\\alpha_i}^{c-\\alpha_i}$ ] @xmath35 @xmath36 update preferences ( algorithm  [ algo : preferences ] ) * if * canstop * then * * break * @xmath79 ; @xmath80 canstop @xmath82 true canstop @xmath82 false    @xmath89 @xmath90 @xmath91 @xmath92 with probability @xmath93 : @xmath94 @xmath95 @xmath96 @xmath97    @xmath98 @xmath99 @xmath100 @xmath101_{p_{\\min}}^{p_{\\max}}$ ] @xmath102 @xmath103 @xmath104",
    "we compare our adaptive variable frequency selection algorithm  [ algo : avsf ] ( avsf ) to the baseline algorithm  [ algo : liblinear ] in an empirical study . for a fair comparison",
    "we have implemented our modifications directly into the latest version of liblinear ( version 1.92 at the time of writing ) .",
    "the aim of the experiments is to demonstrate the superior training speed of algorithm  [ algo : avsf ] over a wide range of problems and experimental settings .",
    "therefore we have added time measurement and a step counter to both algorithms .",
    "the liblinear software comes with a hard - coded limit of @xmath105 outer loop iterations .",
    "we have removed this `` feature '' for the sake of comparison .",
    "instead we stop only based on the heuristic stopping criterion described in section  [ sec : liblinear ] , which is the exact same for both algorithms .",
    "we use the liblinear default of @xmath106 as well as the libsvm default of @xmath57 in all experiments .",
    "we ran both algorithms on a number of benchmark problems . in our comparison",
    "we rely on medium to extremely large binary classification problems , downloadable from the libsvm data website :    http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/    table  [ tab : datasets ] lists descriptive statistics of the data sets .    . [",
    "tab : datasets ] number of training examples and number of features of the data sets used in our comparison . [ cols=\"<,>,>\",options=\"header \" , ]     [ c][c]@xmath107 [ c][c]@xmath108 [ c][c]@xmath109 [ c][c]@xmath110 [ c][c]@xmath111 [ c][c]@xmath112 [ c][c]@xmath113 [ r][l]@xmath107 [ r][l]@xmath108 [ r][l]@xmath109 [ r][l]@xmath114 [ r][l]@xmath111 [ r][l]@xmath112 [ r][l]@xmath113 [ r][l]@xmath115 [ r][l]@xmath116 [ r][l]@xmath117 [ r][l]@xmath118 [ l][l]@xmath119 [ c][c]seconds [ c][c]*cover type * [ c][c]*kdd - a * [ c][c]*kdd - b * [ c][c]*news-20 * [ c][c]*rcv1 * [ c][c]*url *     +     +",
    "the typical behavior of the performance timing curves in figure  [ fig : results ] is that the original algorithm is superior for small values of @xmath119 , and that our new algorithm is a lot faster for large values of @xmath119 : often by an order of magnitude , sometimes even more .",
    "the differences are most pronounced for large data sets . in the following",
    "we will discuss this behavior .    for small values of @xmath119",
    "many examples tend to become support vectors .",
    "many dual variables end up at the maximum value .",
    "this relatively simple solution structure can be achieved efficiently with uniform sweeps through the data . moreover",
    ", the algorithm performs over few outer loop iterations . in this case",
    "our soft shrinking method is too slow to be effective and the original hard shrinking heuristic has an advantage . at the same time",
    "the problem structure is `` simple enough '' , so that falsely shrinking variables out of the problem is improbable .",
    "on the other hand , for large values of @xmath119 the range of values is much larger and the values of variables corresponding to points close to or exactly on the target margin are tedious to adjust to the demanded accuracy . in this case",
    "shrinking is important , and second order working set selection is known to work best . also , in this situation shrinking is most endangered to make wrong decisions , so soft shrinking has an advantage .",
    "only the magnitude of the speed - up is really surprising .",
    "the forest cover data is an exception to the above scheme . here",
    "the original algorithm is superior for all tested values of @xmath119 , although the difference diminishes for large @xmath119 .",
    "also , it seems odd that the training times for the different target accuracies are nearly identical , despite the fact that they can make huge differences for other data sets .",
    "the reason for this effect is most probably the rather low number of features  @xmath120 : once the right weight vector is found , is it easily tuned to nearly arbitrary precision .",
    "also , since the data is distributed in a rather low dimensional space there are many functionally similar instances , which is why adaptation of frequencies of individual variables is less meaningful than for the other problems .",
    "training times increase drastically for increasing values of @xmath119 ( note the logarithmic scale in the plots ) .",
    "therefore we argue that improved training speed is most crucial for large values of  @xmath119 .",
    "doubling the training time for small values of @xmath119 does not pose a serious problem , since these times are anyway short , while speeding up training for large values of @xmath119 by more than an order of magnitude can make machine training feasible in the first place .",
    "we observe this difference directly for problems kkd - a and kkd - b .",
    "this argument is most striking when doing model selection for @xmath119 .",
    "minimization of the cross validation error is a standard method .",
    "the parameter @xmath119 is usually varied on a grid on logarithmic scale .",
    "this procedure is often a lot more compute intensive than the final machine training with the best value of @xmath119 , and its cost is independent of the resulting choice of @xmath119 .",
    "its time complexity is proportional to the row - wise sum of the training times in the tables , i.e. , over all values of @xmath119 .",
    "this cost is clearly dominated by the largest tested value ( here @xmath121 ) , which is where savings due to variable selection frequencies are most pronounced .",
    "we have replaced uniform variable selection in sweeps over the data for linear svm training with an adaptive approach .",
    "the algorithm extrapolates past performance into the future and turns this information into an algorithm for adapting variable selection frequencies . at the same time",
    "the reduction of frequencies of variables at the bounds effectively acts as a soft shrinking technique , making explicit shrinking heuristics superfluous . to the best of our knowledge",
    "this is the first approach of this type for svm training .",
    "our experimental results demonstrate striking success of the new method in particular for costly cases . for most problems we achieve speed - ups of up to an order of magnitude or even more .",
    "this is a substantial performance gain .",
    "the speed - ups are largest when needed most , i.e. , for large training data sets and large values of the regularization constant  @xmath119 .",
    "t.  joachims . making large - scale svm learning practical . in b.",
    "schlkopf , c.  burges , and a.  smola , editors , _ advances in kernel methods  support vector learning _ , chapter  11 , pages 169184 . mit press , 1998 ."
  ],
  "abstract_text": [
    "<S> support vector machine ( svm ) training is an active research area since the dawn of the method . in recent years </S>",
    "<S> there has been increasing interest in specialized solvers for the important case of linear models . the algorithm presented by hsieh et al . </S>",
    "<S> , probably best known under the name of the `` liblinear '' implementation , marks a major breakthrough . </S>",
    "<S> the method is analog to established dual decomposition algorithms for training of non - linear svms , but with greatly reduced computational complexity per update step . </S>",
    "<S> this comes at the cost of not keeping track of the gradient of the objective any more , which excludes the application of highly developed working set selection algorithms . </S>",
    "<S> we present an algorithmic improvement to this method . </S>",
    "<S> we replace uniform working set selection with an online adaptation of selection frequencies . </S>",
    "<S> the adaptation criterion is inspired by modern second order working set selection methods . </S>",
    "<S> the same mechanism replaces the shrinking heuristic . </S>",
    "<S> this novel technique speeds up training in some cases by more than an order of magnitude . </S>"
  ]
}