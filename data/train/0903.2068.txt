{
  "article_text": [
    "astronomers usually face , in their daily work , the need of determining the boundary of some data sets .",
    "common examples are the computation of frontiers segregating regions in diagrams ( e.g.  colour  colour plots ) , or the estimation of reasonable pseudo - continua of spectra .",
    "using for illustration the latter example , several strategies are initially feasible in order to get an analytical determination of that boundary .",
    "one can , for example , fit a simple polynomial to the general trend of the considered spectrum , masking previously disturbing spectroscopic features , such as important emission lines or deep absorption characteristics . since this fit _ traverses _ the data , it must be shifted upwards a reasonable amount in order to be placed on top of the spectrum .",
    "however , since there is no reason to expect the pseudo - continuum following exactly the same functional form as the polynomial fitted through the spectrum , that shift does not necessarily provides the expected answer . as an alternative , one can also force the polynomial to pass over some special data points , which are selected to guide ( actually to force ) the fit through the apparent upper envelope of the spectrum . with this last method",
    "the result can be too much dependent on the subjectively selected points . in any case",
    ", the technique requires the additional effort of determining those special points .    with the aim of obtaining an objective determination of the boundaries , an automatic approach , based on a generalisation of the popular least - squares method ,",
    "is presented in this work .",
    "section  [ section : the_method ] describes the procedure in the general case . as an example , the boundary fitting using simple polynomials is included in this section . considering that these simple polynomials are not always flexible enough , section  [ section : adaptive_splines ] presents the use of _ adaptive splines _ , a variation of the typical fit to splines that allows the determination of a boundary that smoothly adapts to the data in an iterative way .",
    "section  [ section : application ] shows two practical uses of this technique : the computation of spectra pseudo - continuum and the determination of data ranges . since the scatter of the data due to the presence of data uncertainties tends to bias the boundary determinations , section  [ section : uncertainties ] analyses the problem and presents a modification of the method that allows to confront this situation .",
    "finally section  [ section : conclusions ] summarises the main conclusions .",
    "in addition , appendix  [ appendix : constraints ] discusses the inclusion of constraints in the fits , whilst appendix  [ appendix : normalization ] describes how the normalisation of the data ranges prior to the data fitting can help to reduce the impact of numerical errors in some circumstances .",
    "the method described in this work has been implemented into the program boundfit , a fortran code written by the author and available ( under the gnu general public license , version 3 ) at the following url + http://www.ucm.es/info/astrof/software/boundfit + all the fits presented in this paper have been computed with this program .",
    "the basic idea behind the method that follows is to introduce , in the fitting procedure , an asymmetric role for the data at both sides of a given fit , so the points located outside relative to that fit pull stronger toward themselves than the points at the opposite side .",
    "this idea is graphically illustrated in fig .",
    "[ figure : cartoon ] .",
    "as it is going to be shown , the problem is numerically treatable . in order to use the data asymmetrically ,",
    "it is necessary to start with some initial guess fit , that in practice can be obtained employing the traditional least - squares method ( with a symmetric data treatment ) . once this initial fit is available , it is straightforward to continue using the data asymmetrically and , in an iterative process , determine the sought boundary .",
    "let s consider the case of a two - dimensional data set consisting in @xmath0 points of coordinates @xmath1 , where @xmath2 is an independent variable , and @xmath3 a dependent variable , which value has an associated and known uncertainty @xmath4 .",
    "an ordinary error - weighted least - squares fit is obtained by minimising the _ cost function _",
    "@xmath5 ( also called _ objective function _ in the literature concerning optimisation strategies ) , defined as @xmath6 where @xmath7 is the _ fitted function _ evaluated at , and @xmath8 are the unknown @xmath9 parameters that define such function . actually , one should write the fitted function as .    in order to introduce the asymmetric weighting scheme ,",
    "the cost function can be generalised introducing some new coefficients , @xmath10 where @xmath11 is now a variable exponent ( in normal least squares ) .",
    "for that reason the distance between the fitted function @xmath7 and the dependent variable @xmath3 is considered in absolute value .",
    "the new overall weighting factors @xmath12 are defined differently depending on whether one is fitting the upper or the lower boundary .",
    "more precisely @xmath13 being @xmath14 the exponent that determines how error weighting is incorporated into the fit ( to ignore errors , in normal error - weighted least squares ) , and @xmath15 is defined as an _ asymmetry coefficient_. obviously , for and , eq .  ( [ equation : gls ] ) simplifies to eq .",
    "( [ equation : ols ] ) .",
    "as it is going to be shown later , the asymmetry coefficient must satisfy for the method to provide the required boundary fit .",
    "leaving apart the particular weighting effect of the data uncertainties  @xmath4 , the net outcome of introducing the factors  @xmath12 is that the points that are classified as being outside from a given frontier simply have a higher weight that the points located at the inner side ( see fig .  [ figure : cartoon ] ) , and this difference scales with the particular value of the asymmetry coefficient  @xmath15 .",
    "thus , the boundary fitting problem reduces to finding the parameters that minimise eq .  ( [ equation : gls ] ) , subject to the weighting scheme defined in eq .",
    "( [ equation : asymmetry ] ) . in the next sections",
    "several examples are provided , in which the functional form of @xmath16 is considered to be simple polynomials and splines .",
    "the method just described is , as defined , very sensitive to extreme data points .",
    "this fact , that at first sight may be seen as a serious problem , it is not necessarily so .",
    "for example , one may be interested in constraining the scatter exhibited by some measurements due to the presence error sources . in this case",
    "a good option would be to derive the upper and lower frontiers that surround the data , and in this scenario there is no need to employ an error - weighting scheme ( i.e.   would be the appropriate choice ) . on the other hand , there are situations in which the data sample contains some points that have larger uncertainties than others , and one wants those points to be ignored during the boundary estimation . under this circumstance",
    "the role of the @xmath14 parameter in eq .",
    "( [ equation : asymmetry ] ) is important .",
    "given the relevance of all these issues concerning the impact of data uncertainties in the boundary computation , this topic is intentionally delayed until section  [ section : uncertainties ] . at this point",
    "it is better to keep the problem in a more simplified version , which facilitates the examination of the basic properties of the proposed fitting procedure .",
    "an interesting generalisation of the boundary fitting method described above consists in the incorporation of additional constraints during the minimisation procedure , like forcing the fit to pass through some predefined fixed points , or imposing the derivatives to have some useful values at particular points .",
    "a discussion about this topic has been included in appendix  [ appendix : constraints ] .",
    "another issue of great relevance is the appearance of numerical errors during the minimisation procedure .",
    "the use of data sets exhibiting values with different orders of magnitude , or with a very high number of data points , can be responsible for preventing numerical methods to provide the expected answers . in some cases a simple solution to these problems consists in normalising the data ranges prior to the numerical minimisation .",
    "a detailed description of this approach is presented in appendix  [ appendix : normalization ] .",
    "returning to eq .",
    "( [ equation : gls ] ) , let s consider now the particular case in which the functional form of the fitted boundary @xmath16 is assumed to be a simple polynomial of degree @xmath17 , i.e. @xmath18 in this case , the function to be minimized , , is also a simple function of the coefficients",
    ". in ordinary least squares one simply takes the partial derivatives of the cost function @xmath5 with respect to each of these coefficients , obtaining a set of equations with unknowns , which can be easily solved , as far as the number of independent points @xmath0 is large enough , i.e.  @xmath19 .",
    "however , considering the special definition of the weighting coefficients @xmath12 given in eq .",
    "( [ equation : asymmetry ] ) , it is clear that in the general case an analytical solution can not be derived without any kind of iterative approach , since during the computation of the considered boundary ( either upper or lower ) , the classification of a particular data point as being inside or outside relative to a given fit explicitly depends on the function @xmath16 that one is trying to derive .",
    "fortunately numerical minimisation procedures can provide the sought answer in an easy way .",
    "for this purpose , the downhill simplex method @xcite is an excellent option .",
    "this numerical procedure performs the minimisation of a function in a multi - dimensional space . for this method to be applied , an initial guess for the solution must be available .",
    "this initial solution , together with a characteristic length - scale for each parameter to be fitted , is employed to define a simplex ( i.e. , a multi - dimensional analogue of a triangle ) in the solution space . the algorithm works using only function evaluations ( i.e.  not requiring the computation of derivatives ) , and in each iteration the method improves the previously computed solution by modifying one of the vertices of the simplex .",
    "the simplex adapts itself to the local landscape , and contracts on to the final minimum .",
    "the numerical procedure is halted once a pre - fixed numerical precision in the sought coefficients is reached , or when the number of iterations exceeds a pre - defined maximum value @xmath20 .",
    "a well - known implementation of the downhill simplex method is provided by @xcite .",
    "for the particular case of minimising eq .",
    "( [ equation : gls ] ) while fitting a simple polynomial , a reasonable guess for the initial solution is supplied by the coefficients of an ordinary least - squares fit to a simple polynomial derived by minimising eq .",
    "( [ equation : ols ] ) .",
    "it is important to highlight that whatever the numerical method employed to perform the numerical minimisation , the considered cost function will probably exhibit a parameter - space landscape with many peaks and valleys .",
    "the finding of a solution is never a guarantee of having found the right answer , unless one has the resources to employ brute force to perform a really exhaustive search at sufficiently fine sampling of the cost function to find the global minimum . in situations where this problem can be serious , more robust methods , like those provided by genetic algorithms ,",
    "must be considered ( see e.g. * ? ? ?",
    "fortunately , for the particular problems treated in this paper , the simpler downhill method is a good alternative , considering that the ordinary least - squares method will likely give a good initial guess for the expected solution in most of the cases .",
    "the dashed blue line is the ordinary least - squares fit to that data , used as the initial guess for the numerical determination of the boundary .",
    "since all the points have the same uncertainty , there is no need for an error - weighted procedure .",
    "for that reason has been used in eq .",
    "( [ equation : asymmetry ] ) .",
    "in addition and an asymmetry coefficient were employed .",
    "the grey lines indicate the boundary fits obtained for @xmath20 in the range from 5 to 2000  iterations , at arbitrary steps .",
    "the inset displays a zoomed plot region where some particular values of @xmath21 are annotated over the corresponding fits .",
    "the continuous red line is the final boundary determination obtained using .",
    "_ panel  ( b ) _ : effect of employing different asymmetry coefficients @xmath15 for the upper boundary fit shown in panel  ( a ) . in the four cases",
    "the same maximum number of iterations has been employed , with .",
    "_ panel  ( c ) _ : effect of using different values of the power @xmath11 , with and  .",
    "see discussion in section  [ subsection : simple_polynomial ] . ]    for illustration , fig .",
    "[ figure : oneoverx_pol]a displays an example of upper boundary fitting to a given data set , using a simple 5th order polynomial . as initial guess for the numerical minimisation ,",
    "the ordinary least - squares fit for the data ( shown with a dashed blue line ) has been employed .",
    "the grey lines represent the corresponding boundary fits obtained using the downhill method previously described .",
    "each line corresponds to a pre - defined maximum number of iterations @xmath21 in downhill , as labelled over the lines in the plot inset . in this particular example",
    "the fitting procedure has been carried out without weighting with errors ( i.e. , assuming ) , and using a power and an asymmetry coefficient .",
    "it is clear that after a few iterations the intermediate fits move upwards from the initial guess ( dashed blue line ) , until reaching the location marked with @xmath22 . beyond this number of iterations ,",
    "the fits move downwards slightly , rapidly converging into the final fit displayed with the continuous red line .",
    "[ figure : oneoverx_pol]b displays the effect of modifying the asymmetry coefficient @xmath15 .",
    "the ordinary least - squares fit corresponds to ( dashed blue line ) .",
    "the asymmetric fits are obtained for @xmath23 .",
    "the figure illustrates how for and  100 the resulting upper boundaries do still leave points in the _ wrong _ side of the boundary . only when ( continuous red line ) is the boundary fit appropriate .",
    "thus , a proper boundary fitting requires the asymmetry coefficient to be large enough to compensate for the pulling effect of the points that are in the inner side of the boundary .",
    "on the other hand , fig .",
    "[ figure : oneoverx_pol]c shows the impact of changing the power @xmath11 in eq .",
    "( [ equation : gls ] ) . for the lowest value , ( dotted blue line ) ,",
    "the fit is practically identical to the one obtained with ( continuous red line ) . for the largest values , ( dotted green and dashed orange lines ) , the boundaries are below the expected location , leaving some points outside ( above ) the fits . in these last cases the power @xmath11 is too high and , for that reason ,",
    "the distance from the boundary to the more distant points in the inner side have a too high effect in the cost function given by eq .",
    "( [ equation : gls ] ) .",
    "a. each panel represents the coefficient value at a given iteration ( @xmath24 , with , from bottom to top ) divided by @xmath25 , the final value derived after iterations . the same @xmath26-axis range is employed in all the plots .",
    "red lines correspond to an asymmetry coefficient , whereas the blue and green grey lines indicate the coefficients obtained with and , respectively ( in all the cases and have been employed ) .",
    "note that the plot @xmath27-scale is in logarithmic units . ]",
    "another important aspect to take into account when using a numerical method is the convergence of the fitted coefficients .",
    "[ figure : coeff_pol ] displays , for the same example just described in fig .  [ figure : oneoverx_pol]b , the values of the 6  fitted polynomial coefficients as a function of the maximum number of iterations allowed .",
    "the figure includes the results for , 100 and  1000 ( using and in the three cases ) . in overall",
    ", the convergence is reached faster when @xmath28 .",
    "[ figure : oneoverx_pol]a already showed that for this particular value of the asymmetry coefficient a quite reasonable fit is already achieved when . beyond this maximum number of iterations",
    "the coefficients only change slightly , until they definitely settle around .",
    "although simple polynomials can be excellent functional forms for a boundary determination ( as shown in the previous example ) , when the data to be fitted exhibit rapidly changing values , a single polynomial is not always able to reproduce the observed trend . a powerful alternative in these situations",
    "consists in the use of splines .",
    "the next section presents an improved method that using classic cubic splines , but introducing additional degrees of freedom , offers a much larger flexibility for boundary fitting .",
    "splines are commonly employed for interpolation and modelling of arbitrary functions .",
    "many times they are preferred to simple polynomials due to their flexibility . a spline is a piecewise polynomial function that is locally very simple , typically third - order polynomials ( the so called cubic splines ) .",
    "these local polynomials are forced to pass through a prefixed number of points , @xmath29 , which we will refer as knots . in this way",
    ", the functional form of a fit to splines can be expressed as @xmath30 ^ 3 +            s_2(k ) [ x - x_{\\mbox{\\scriptsize knot}}(k)]^2 + \\nonumber \\\\        + & s_1(k ) [ x - x_{\\mbox{\\scriptsize knot}}(k ) ] + s_0(k ) , \\end{array}\\ ] ] where ( @xmath31 ) are",
    "the @xmath32 coordinates of the @xmath33  knot , and @xmath34 , @xmath35 , @xmath36 , and @xmath37 are the corresponding spline coefficients for , with",
    ". these coefficients are easily computable by imposing the set of splines to define a continuous function and that , in addition , not only the function , but also the first and second derivatives match at the knots ( two additional conditions are required ; typically they are provided by assuming the second derivatives at the two endpoints to be zero , leading to what are normally referred as _ natural splines _ ) .",
    "the computation of splines is widely described in the literature ( see e.g. @xcite ) .",
    "the final result of a fit to splines will strongly depend on both , the number and the precise location of the knots . with the aim of having more flexibility in the fits",
    ", @xcite explored the possibility of setting the location of the knots as free parameters , in order to determine the optimal coordinates of these knots that improve the overall fit of the data .",
    "the solution to the problem can be derived numerically using any minimisation algorithm , as the downhill simplex method previously described . in this way",
    "the set of splines smoothly adapts to the data .",
    "the same approach can be applied to the data boundary fitting , using as functional form for the function @xmath16 in eq .",
    "( [ equation : gls ] ) the _ adaptive splines _ just described .",
    "it is important to highlight that in this case the optimal boundary fit requires not only to find the appropriate coefficients of the splines , but also the optimal location of the knots .      in order to carry out the double optimisation process ( for the coefficients and the knots location ) required to compute a boundary fit using adaptive splines ,",
    "the following steps can be followed :    1 .   _",
    "fix the initial number of knots to be employed _",
    ", @xmath29 .",
    "using a large value provides more flexibility , although the number of parameters to be determined logically scales with this number , and the numerical optimisation demands a larger computational effort .",
    "2 .   _ obtain an initial solution with fixed knot locations_. for this purpose it is sufficient , for example , to start by dividing the full @xmath27-range to be fitted by .",
    "this leads to a regular distribution of equidistant knots .",
    "the initial fit is then derived by minimising the cost function given in eq .",
    "( [ equation : gls ] ) , leaving as free parameters the @xmath26-coordinates of all the knots simultaneously , while keeping fixed the corresponding @xmath27-coordinates .",
    "this numerical fit also requires a preliminary guess solution , than can be easily obtained through independent ordinary least - squares fit of the data placed between each consecutive pair of knots , using for this purpose simple polynomials of degree  1 or  2 . in this guess solution the @xmath26-coordinate for each knot is then evaluated as the average value for the two neighbouring preliminary polynomial fits ( only one for the knots at the borders of the @xmath27-range ) .",
    "obviously , if there is additional information concerning a more suitable knot arrangement than the equidistant pattern , it must be used to start the process with an even better initial solution which will facilitate a faster convergence to the final solution .",
    "3 .   _ refine the fit_. once some initial spline coefficients have been determined , the fit is refined by setting as free parameters the location of all the `` inner '' knots , both in the @xmath27- and @xmath26-directions .",
    "the outer knots ( the first and last in the ordered sequence ) are only allowed to be refined in the @xmath26-axis direction with the aim of preserving the initial @xmath27-range coverage .",
    "the simultaneous minimisation of the @xmath27 and @xmath26 coordinates of all the knots at once will imply finding the minimum of a multidimensional function with too many variables .",
    "this is normally something very difficult , with no guarantee of a fast convergence .",
    "the problem reveals to be treatable just by solving for the optimised coordinates of every single knot separately . in practice ,",
    "a _ refinement _ can be defined as the process of refining the location of all the @xmath29 knots , one at a time , where the order in which a given knot is optimised is randomly determined .",
    "each knot optimisation requires , in turn , a value for the maximum number of iterations allowed @xmath21 .",
    "thus , at the end of every single refinement process all the knots have been refined once .",
    "an extra penalisation can be introduced in the cost function with the idea of avoiding that knots exchange their order in the list of ordered sequence of knots .",
    "this inclusion typically implies that , if @xmath29 is large , several knots end up colliding and having the same coordinates.the whole process can be repeated by indicating the total number of refinement processes , @xmath38 .",
    "4 .   _ optimise the number of knots_. if after @xmath39 refinement processes several knots have collided and exhibit the same coordinates , this is an evidence that @xmath29 was probably too large . in this case , those colliding knots can be merged and the effective number of knots be accordingly reduced . if , on the contrary , the knots being used do not collide , it is interesting to check whether a higher @xmath29 can be employed . with the new @xmath29 , step  ( iii )",
    "is repeated again .",
    "although at first sight it may seem excessive to use a large number of knots when some of them are going to end up colliding , these collisions will typically take place at optimised locations for the considered fit .",
    "as far as the minimisation algorithm is able to handle such large @xmath40 , it is not such a bad idea to start using an overestimated number and merge the colliding knots as the refinement processes take place .",
    "the fitting algorithm can be halted once a satisfactory fit is found at the end of step  ( iii ) . by satisfactory one can accept a fit which coefficients do not significantly change by increasing neither @xmath38 nor @xmath21 , and in which there are no colliding knots .      .",
    "in this case has been employed .",
    "_ panel  ( a ) _ : the preliminary fit ( dotted blue line ) shows the initial guess determined from independent ordinary least - squares fit of the data , as explained in section  [ subsection : example_splines ] . by imposing the fit improves , although in most cases the effective @xmath21 is much lower since the algorithm computes spline coefficients that have converged before the number of iterations reaches that maximum value .",
    "the dashed green line shows the first fit obtained with still the knots at their initial equidistant locations .",
    "successive refinements ( light grey ) allow the knots to change their positions , which leads to the final boundary determination ( continuous red line , corresponding to ) . in all these fits , and have been employed .",
    "_ panel  ( b ) _ : effect of using different asymmetry coefficients @xmath15 for the upper boundary fit shown in the previous panel . in the four cases , , and   were used .",
    "_ panel  ( c ) _ : effect of employing different values of the power @xmath11 , with , and  .",
    "see discussion in section  [ subsection : example_splines ] . ]",
    "a. before introducing any refinement , the 15 knots were regularly placed , as shown with the open blue circles . in each refinement process",
    "the inner knots are allowed to modify its location , one at a time .",
    "the first and last knots are fixed in order to preserve the fitted .",
    "the final knot locations after are shown with the filled red triangles . ]    to illustrate the flexibility of adaptive splines , fig .",
    "[ figure : oneoverx_spl]a displays the corresponding upper boundary fit employing the same example data displayed in fig .",
    "[ figure : oneoverx_pol ] , for the case .",
    "the preliminary fit ( shown as a dotted blue line ) was computed by placing the @xmath29 equidistantly spread in the @xmath27-axis range exhibited by the data , and performing independent ordinary least - squares fit of the data placed between each consecutive pair of knots , using 2nd order polynomials , as explained in step  ( ii ) .",
    "although unavoidably this preliminary fit is far from the final result ( due to the fact that this is just the merging of several independent ordinary fits _ through _ data exhibiting large scatter and that the @xmath27-range between adjacent knots is not large ) , after @xmath20 iterations without any refinement ( i.e. , without modifying the initial equidistant knot pattern ) the algorithm provides the fit shown as the dashed green line .",
    "the light grey lines display the resulting fits obtained by allowing the knot locations to vary , and after 40 refinements one gets the boundary fit represented by the continuous red line .",
    "since the knot location has a large influence in the quality of the boundary determination , very high values for @xmath21 are not required ( typically values for the number of iterations needed to obtain refined knot coordinates are  @xmath41 ) .",
    "analogously to what was done with the simple polynomial fit , in fig .",
    "[ figure : oneoverx_spl]b and  [ figure : oneoverx_spl]c the effects of varying the asymmetry coefficient @xmath15 and the power @xmath11 are also examined . in the case of @xmath15 , it is again clear that the highest value leads to a tighter fit .",
    "concerning the power @xmath11 , the best result is obtained when distances are considered quadratically , i.e. . for the largest values , and  5 , the resulting boundaries leave points above the fits .",
    "the case is not very different to the quadratic fit , although in some regions ( e.g. ) the boundary is probably too high . in addition ,",
    "[ figure : knots ] displays the variation in the location of the knots as @xmath38 increases , for the final fit displayed in fig .",
    "[ figure : oneoverx_spl]a .",
    "the initial equidistant pattern ( open blue circles ; corresponding to ) is modified as each individual knot is allowed to change its coordinates .",
    "it is clear that some of the knots approximate and could be , in principle , merged into single knots , revealing that the initial number of knots was overestimated .",
    "finally fig .",
    "[ figure : example ] presents , for the same sample data employed in figs .",
    "[ figure : oneoverx_pol ] and  [ figure : oneoverx_spl ] , the comparison between the boundary fits to simple polynomials ( continuous blue lines ) and to adaptive splines ( dotted red lines ) .",
    "the shaded area corresponds to the diagram region comprised between the two adaptive splines boundaries . in this figure",
    "both the upper and the lower boundary limits , computed as described previously , are represented .",
    "it is clear from this graphical comparison that the larger number of degrees of freedom introduced with adaptive splines allows a much tighter boundary determination .",
    "the answer to the immediate question of which fit ( simple polynomials or splines ) is more appropriate will obviously depend on the nature of the considered problem .",
    "as mention in section  [ section : introduction ] , a typical situation in which the computation of a boundary can be useful is in the estimation of spectra pseudo - continuum .",
    "the strengths of spectral features have been measured in different ways so far . however , although with slight differences among them , most authors have employed line - strength indices with definitions close to the classical expression for an equivalent width @xmath42 where @xmath43 is the observed spectrum and @xmath44 is the local continuum , usually obtained by interpolation of @xmath43 between two adjacent spectral regions ( e.g. * ? ? ? * ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) . in practice , as pointed out by @xcite ( see also * ? ? ?",
    "* ) , at low and intermediate spectral resolution the local continuum is unavoidably lost , and a pseudo - continuum is measured instead of a true continuum . the upper boundary fitting , either by using simple polynomials or adaptive splines , constitutes an excellent option for the estimation of that pseudo - continuum . to illustrate this statement ,",
    "several examples are presented and discussed in this section . in all these examples ,",
    "the boundary fits have been computed ignoring data uncertainties , i.e. , assuming in eq .",
    "( [ equation : asymmetry ] ) .",
    "the impact of errors is this type of application is discussed later , in section  [ section : uncertainties ] .",
    "[ figure : pol_spl ] displays upper boundary fits for the particular stellar spectrum of hd003651 belonging to the miles library @xcite .",
    "the results using simple polynomials and adaptive splines with different tunable parameters are shown .",
    "panels  [ figure : pol_spl]a and  [ figure : pol_spl]b show the results derived using simple 5th - order polynomials , whereas panels  [ figure : pol_spl]c and  [ figure : pol_spl]d display the fits obtained employing adaptive splines with .",
    "the impact of modifying the asymmetry coefficient @xmath15 is explored in panels  [ figure : pol_spl]a and  [ figure : pol_spl]c ( in these fits , and have been used ; the adaptive splines fits were refined times ) .",
    "the dashed blue lines indicate the ordinary least - squares fits , i.e. , those obtained when there is no effective asymmetry , which in each case was used as the initial guess fit in the numerical minimisation process .",
    "for relatively low values of the asymmetry coefficient ( or  100 ) the fits are not as good as when using the largest value .",
    "this is easy to understand , since the relatively large number of points to be fitted in this example , requires that the points that still fall in the outer side of the boundary during the numerical minimisation of eq .",
    "( [ equation : gls ] ) overcome the pulling effect of the points in the inner side of the boundary .",
    "on the other hand , panels  [ figure : pol_spl]b and  [ figure : pol_spl]d display the effect of changing the power @xmath11 in the fits .",
    "again , the dashed blue lines correspond to the ordinary least - squares fits ( in the rest of the cases and have been used ; the adaptive splines fits were refined times ) . in these cases ,",
    "the best boundary fits are obtained for , whereas for the larger values the fits depart from the expected result .",
    "the above example illustrates that the optimal asymmetry coefficient @xmath15 and power @xmath11 during the boundary procedure can ( and must ) be tuned for the particular problem under study .",
    "not surprisingly , this fact also concerns the number of knots when using adaptive splines .",
    "[ figure : spl_knots ] shows the different results obtained when estimating the pseudo - continuum in the same stellar spectrum previously considered , employing different values of .",
    "as expected , the fit adapts to the irregularities exhibited by the spectrum as the number of knots increases .",
    "this is something that for some purposes may not be desired .",
    "for instance , the fits obtained with , and more notably with , detect the absorption around the mg  i feature at @xmath45   , and for this reason these fits underestimate the total absorption produced at this wavelength region . in situations like",
    "this the boundary obtained with a lower number of knots may be more suitable .",
    "obviously there is no general rule to define the right @xmath29 , since the most convenient value will depend on the nature of the problem under study .    in order to obtain a quantitative determination of the impact of using the upper boundary fit instead in the estimation of local pseudo - continuum , fig .",
    "[ figure : balmer_lines ] compares the actual line - strength indices derived for three balmer lines ( h@xmath14 , h@xmath46 and h@xmath47 , from right to left ) using three different strategies . for this particular example",
    "the same stellar spectrum displayed in fig .",
    "[ figure : pol_spl ] has been used .",
    "overplotted on each spectrum are the bandpasses typically used for the measurement of these spectroscopic features .",
    "in particular , de bandpasses limits for h@xmath14 are the revised values given by @xcite , whereas for h@xmath46 and h@xmath47 the limits correspond to h@xmath48 and h@xmath49 , as defined by @xcite . for each feature ,",
    "the corresponding line - strength has been computed by determining the pseudo - continuum using : i ) the straight line joining the mean fluxes in the blue and red bandpasses ( top panels ) which is the traditional method ; ii ) the straight line joining the values of the upper boundary fits evaluated at the centres of the same bandpasses ( central panels ) ; and iii ) the upper boundary fits themselves ( bottom panels ) . for the cases",
    "ii ) and iii ) the upper boundary fits have been derived using a second order polynomial fitted to the three bandpasses . the resulting line - strength indices , numerically displayed above each spectrum ,",
    "have been computed as the area comprised between the adopted pseudo - continuum fit and the stellar spectrum within the central bandpass .",
    "for the three balmer lines it is clear that the use of the boundary fit provides larger indices .",
    "the traditional method provides very bad values for h@xmath46 and h@xmath47 ( which are even negative ! ) , given that the pseudo - continuum is very seriously affected by the absorption features in the continuum bandpasses .",
    "this is a well - known problem that has led many authors to seek for alternative bandpass definitions ( see e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) which , on the other hand , are not immune to other problems related to their sensitivity to spectral resolution and their high signal - to - noise requirements .",
    "these are very important issues that deserve a much careful analysis , that is beyond the aim of this paper , and they are going to be studied in a forthcoming work ( cardiel 2009 , in preparation ) .    the results of fig .",
    "[ figure : pol_spl ] reveal that , for the wavelength interval considered in that example , the boundary determinations obtained by using polynomials and adaptive splines are not very different .",
    "however , it is expected that as the wavelength range increases and the expected pseudo - continuum becomes more complex , the larger flexibility of adaptive splines in comparison with simple polynomials should provide better fits . to explore this flexibility in more detail , fig .",
    "[ figure : miles ] shows the result of using adaptive splines to estimate the pseudo - continuum of 12 different spectra corresponding to stars exhibiting a wide range of spectral types ( from b5v to m5v ) , selected from the empirical stellar library miles @xcite previously mentioned .",
    "although in all the cases the fits have been computed blindly without considering the use of an initial knot arrangement appropriate for the particularities of each spectral type , it is clear from the figure that adaptive splines are flexible enough to give reasonable fits independently of the considered star .",
    "more refined fits can be obtained using an initial knot pattern more adjusted to the curvature of the pseudo - continuum exhibit by the stellar spectra .    ) using exclusively these selected points is displayed with the dotted green line . a more suitable fit ( continuous red line )",
    "is obtained employing @xmath50 , in which case the fit is performed over the whole spectrum .",
    "the two fits have been carried out with , , , and .",
    "_ panel  ( b ) _ : ratio between the two fits displayed in the previous panel . ]    a good estimation of spectra pseudo - continuum is very useful , for example , when correcting spectroscopic data from telluric absorptions using featureless ( or almost featureless ) calibration spectra .",
    "this is a common strategy when performing observations in the near - infrared windows .",
    "[ figure : telluric]a illustrates a typical example , in which the observation of the hot star v986  oph ( hd165174 , spectral type b0iii ) is employed to determine the correction .",
    "this star was observed in the _ j _  band as part of the calibration work of the observations presented in @xcite .",
    "the stellar spectrum is shown in light grey , whereas the blue points indicate a manual selection of spectrum regions employed to estimate the overall pseudo - continuum .",
    "the dotted green line corresponds to the ordinary least - squares fit of these points , whereas the red continuous line is the upper boundary obtained with adaptive splines using with an asymmetry coefficient . in fig .",
    "[ figure : telluric]b the ratio between both fits in represented , showing that there are differences up to a few percent between these fits .",
    "two kind of errors are present here . in",
    "overall the ordinary least - squares fit underestimates the pseudo - continuum level , which introduces a systematic bias on the resulting depth of the telluric features ( the whole curve displayed in fig .",
    "[ figure : telluric]b is above  1.0 ) .",
    "in addition , since the selected blue points do include real ( although small ) spectroscopic features , there are variations as a function of wavelength of the above discrepancy .",
    "these differences can be important when trying to perform a high - quality spectrophotometric calibration .",
    "it is important to highlight that an important additional advantage of the boundary fitting is that this method does not require the masking of any region of the problem spectrum , which avoids the effort ( and the subjectivity ) of selecting special points to guide the fit .",
    "another important aspect concerning the use of boundary fits for the determination of the pseudo - continuum of spectra is that this method can provide an alternative approach for the estimation of the pseudo - continuum flux when measuring line - strength indices . instead of using the average fluxes in bandpasses located nearby the (",
    "typically central ) bandpass covering the relevant spectroscopic feature , the mean flux on the upper boundary can be employed . in this case",
    "it is important to take into account that flux uncertainties will bias the fits towards higher values . under these situations",
    "the approach described later in section  [ section : uncertainties ] can be employed . concerning this problem is worth mentioning here the method presented by @xcite , who employ a boosted median continuum to derive equivalent widths more robustly than using the classic side - band procedure .      , [ figure : oneoverx_spl ] and  [ figure : example ] , and computed using simple 5th order polynomials , it is trivial to subdivide the range spanned by the data in the by creating a regular grid ( i.e.  contant @xmath51 at a fixed @xmath27 ) between both boundary limits . in this example",
    "the region has been subdivided in ten intervals .",
    "_ panel  ( b ) _ : 30000 points randomly drawn from the functional form @xmath52 , with @xmath53 for all the points . splitting the @xmath27-range in 100  intervals , sorting the data within each interval and keeping track of the subsets containing 68.27% ( @xmath54 ; blue points ) and 95.44% ( @xmath55 ; green points ) of the data points around the median , it is possible to compute the upper and lower boundaries for those two subsets ( continuous red and orange lines , respectively ) .",
    "the boundaries in this example have been determined using adaptive splines with , , , , and . ]",
    "a quite trivial but useful application of the boundary fits is the empirical determination of data ranges .",
    "one can consider scenarios in which it is needed to subdivide the region spanned by the data in a particular grid .",
    "[ figure : levels]a illustrates this situation , making use of the 5th order polynomial boundaries corresponding to the data previously used in figs .  [ figure : oneoverx_pol ] , [ figure : oneoverx_spl ] , and  [ figure : example ] . once the lower and the upper boundaries are available , it is trivial to generate a grid of lines dividing the region comprised between the boundaries as needed .",
    "a more complex scenario is that in which the data exhibit a clear scatter around some tendency , and one needs to determine regions including a given fraction of the points .",
    "a frequent case appears when one needs to remove outliers , and then it is necessary to obtain an estimation of the regions containing some relevant percentages of the data . in fig .",
    "[ figure : levels]b this situation is exemplified with the use of a simulated data set consisting in 30000 points , for which the regions that include 68.27% and 95.44% of the centred data points , corresponding to @xmath56 and @xmath57 in a normal distribution , have been determined by first selecting those data subsets , and then fitting their corresponding boundaries using adaptive splines , as explained with more detail in the figure caption .",
    "although the method described in section  [ section : the_method ] already takes into account data uncertainties through their inclusion as a weighting parameter ( governed by the exponent @xmath14 ) , it is important to highlight that this weighting scheme does not prevent the boundary fits to be highly biased due to the presence of such uncertainties .",
    "for example , in the determination of the pseudo - continuum of a given spectrum , even considering the same error bars for the fluxes at all wavelengths , the presence of noise unavoidably produces some scatter around the real data . when fitting the upper boundary to a noisy spectrum the fit will be dominated by the points that randomly exhibit the largest positive departures . under these circumstances ,",
    "two different alternatives can be devised :     for the boundary fitting with data uncertainties .",
    "_ panel  ( a ) _ : original spectrum of hd003651 without noise ( blue spectrum ) , spectrum with artificially added noise ( green spectrum ) and noisy spectrum after a gaussian filtering ( red spectrum ) .",
    "note that the original ( blue ) and the filtered noisy ( red ) spectra are almost coincident .",
    "the upper boundary displayed with a dashed green line is the fit to the noisy spectrum using adaptive splines , whereas the upper boundaries plotted with continuous orange and cyan lines are the fits to the filtered noisy spectrum and to the original spectrum , respectively .",
    "_ panel  ( b ) _ : original and noisy spectra are plotted with blue and green lines , respectively ( the filtered spectrum is not plotted here ) .",
    "the cyan line is again the fit to the original spectrum .",
    "the rest of the boundary lines indicate the fits to the noisy spectrum using different values of the cut - off parameter ( red , orange , and green ) . in all the fits , , , , , and have been employed .",
    "see discussion in section  [ section : uncertainties ] . ]",
    "_ to perform a previous rebinning or filtering of the data _ prior to the boundary fitting , in order to eliminate , or at least minimize , the impact of data uncertainties .",
    "after the filtering one assumes that these uncertainties are not seriously biasing the boundary fit . in this way one can employ the same technique described in section  [ section : the_method ]",
    "this approach is illustrated in fig .",
    "[ figure : spl_errors1]a . in this case",
    "the original spectrum of hd00365 ( also employed in figs .",
    "[ figure : pol_spl ] and  [ figure : spl_knots ] ) , as extracted from the miles library @xcite , is considered as a noise - free spectrum ( plotted in blue ) .",
    "its corresponding upper boundary fit using adaptive splines with is shown as the cyan line .",
    "this original spectrum has been artificially degraded by considering an arbitrary signal - to - noise ratio per pixel ( displayed in green ) , and the resulting upper boundary fit is shown with a dashed green line .",
    "it is obvious that this last fit is highly biased , being dominated by the points with higher fluxes .",
    "finally , the noisy spectrum has been filtered by convolving it with a gaussian kernel ( of standard deviation 100  km / s ) , with the result being over - plotted in red .",
    "note that this filtered spectrum overlaps almost exactly with the original spectrum .",
    "the boundary fit plotted with the continuous orange line is the upper boundary for that filtered spectrum .",
    "although the result is not the same as the one derived with the original spectrum , it is much better than the one directly obtained over the noisy spectrum .",
    "2 .   _ to allow a loose boundary fitting . _",
    "another possibility consists in trying to leave a fraction of the points with extreme values to fall outside ( i.e. , in the wrong side ) of the boundary , specially those with higher uncertainties .",
    "this option is easy to parametrize by introducing a cut - off parameter @xmath58 into the overall weighting factors given in eq .",
    "( [ equation : asymmetry ] ) .",
    "the new factors can then be computed as @xmath59 where @xmath4 is the uncertainty associated to the dependent variable @xmath3 .",
    "the cut - off parameter assigns to a point that falls outside of the boundary by distance that is less than or equal to @xmath60 the same low weight during the fitting procedure than the weight that receive the inner points .",
    "in other words , points like that do not receive the extra weighting factor provided by the asymmetry coefficient @xmath15 , even though they are outside of the boundary .",
    "note that simplifies the algorithm to the one described in section  [ section : the_method ] .",
    "[ figure : spl_errors1]b illustrates the use of the cut - off parameter @xmath58 in the upper boundary fitting of the spectrum of hd003651 .",
    "the cyan boundary is again the upper boundary determination using adaptive splines with the original spectrum .",
    "the rest of the boundary fits correspond to the use of the weighting scheme given in eq .",
    "( [ equation : asymmetry_errors ] ) for different values of @xmath58 , as indicated in the legend .",
    "as @xmath58 increases , a larger number of points are left outside of the boundary during the minimisation procedure . in the example , the value seems to give a reasonable fit in the redder part of the spectrum , although in the bluer region the corresponding fit is too low .",
    "it is clear from this example that to define a _",
    "correct _ value of @xmath58 is not a trivial issue .",
    "most of the times the most suited @xmath58 will be a compromise between a high value ( in order to avoid the bias introduced by highly deviant points ) and a low value ( in order to avoid leaving outside of the boundary right data points ) .",
    ", as indicated in the legend , are employed .",
    "note that the unweighted fit (; dashed green line ) is highly biased .",
    "_ panel  ( b ) _ : the same fits of the previous panel are repeated here but using . in all the fits , , , , and have been employed .",
    "see discussion in section  [ section : uncertainties ] . ]",
    "an additional complication arises when one combines in the same data set points with different uncertainties .",
    "it is in these situations when the role of the power @xmath14 in eq .",
    "( [ equation : gls ] ) becomes important .",
    "to illustrate the situation , fig .",
    "[ figure : spl_errors2 ] shows the different pseudo - continuum estimations obtained again for the star hd003651 , but now considering that the spectrum is much noisier below 4200    than above this wavelength . in panel",
    "[ figure : spl_errors2]a the fits are derived ignoring the cut - off parameter previously discussed ( i.e.  assuming ) , but with different values of @xmath14 . in the unweighted case ( , dashed green line ) the resulting upper boundary is dramatically biased for @xmath61    due to the presence of highly deviant fluxes .",
    "the use of non - null ( and positive ) values of @xmath14 induces the fit to be less dependent on the noisier values , being necessary a value as high as @xmath62 to obtain a fit similar to the one obtained in absence of noise ( cyan line ) .",
    "however , since the fitted spectrum ( green ) do still have noise for @xmath63   , all the fits in that region are still biased compared to the fit for the original spectrum ( cyan ) . in order to deal not only with the variable noise , but with the noise itself independently of its absolute value , it is possible to combine the effect of a tuned @xmath14 value with the introduction of a cut - off parameter @xmath58 .",
    "[ figure : spl_errors2]b shows the results derived employing a fixed value with the same variable values of @xmath14 used in the previous panel . in this case , the boundary corresponding to @xmath64 ( magenta ) exhibits an excellent agreement with the fit for the original spectrum ( cyan ) at all wavelengths .",
    "thus , the combined effect of an error - weighted fit and the use of a cut - off parameter is providing a reasonable boundary determination , even under the presence of wavelength dependent noise .",
    "this work has confronted the problem of obtaining analytical expressions for the upper and lower boundaries of a given data set .",
    "the task reveals treatable using a generalised version of the very well - known ordinary least - squares fit method .",
    "the key ideas behind the proposed method can be summarised as follows :    * the sought boundary is iteratively determined starting from an initial guess fit .",
    "for the analysed cases an ordinary least - squares fit provides a suitable starting point . at every iteration in the procedure",
    "a particular fit is always available . * in each iteration the data to be fitted are segregated in two subgroups depending on their position relative to the particular fit at that iteration . in this sense , points are classified as being inside or outside of the boundary .",
    "* points located outside of the boundary are given an extra weight in the cost function to be minimized .",
    "this weight is parametrized through the _ asymmetry coefficient _ @xmath15 .",
    "the net effect of this coefficient is to generate a stronger pulling effect of the outer points over the fit , which in this way shifts towards the frontier delineated by the outer points as the iterations proceed . *",
    "the distance from the points to a given fit are introduced in the cost function with a variable power @xmath11 , not necessarily in the traditional squared way .",
    "this supplies an additional parameter to play with when performing the boundary determination .",
    "* since data uncertainties are responsible for the existence of highly deviant points in the considered data sets , their incorporation in the boundary determination has been considered in two different and complementary ways .",
    "errors can readily be incorporated into the cost function as weighting factors with a variable power @xmath14 ( which does not have to be necessarily two ) .",
    "in addition , a cutt - off parameter @xmath58 can also be tuned to exclude outer points from receiving the extra factor given by the asymmetry coefficient depending on the absolute value of their error bar .",
    "the use of both parameters ( @xmath14 and @xmath58 ) provides enough flexibility to handle the role of the data uncertainties in different ways depending on the nature of the considered boundary problem . *",
    "the minimisation of the cost function can be easily carried out using the popular downhill simplex method .",
    "this allows the use of any computable function as the analytical expression for the boundary fits .",
    "the described fitting method has been illustrated with the use of simple polynomials , which probably are enough for most common situations .",
    "for those scenarios where the data exhibit rapidly changing values , a more powerful approach , using _ adaptive splines _ , has also been described .",
    "examples using both simple polynomials and adaptive splines have been presented , showing that they are good alternatives to estimate the pseudo - continuum of spectra and to segregate data in ranges .",
    "the analysed examples have shown that there is no magic rule to _ a priori _ establish the most suitable values for the tunable parameters ( @xmath15 , @xmath11 , @xmath14 , @xmath58 , @xmath21 , @xmath40 ) .",
    "the most appropriate choices must be accordingly tuned for the particular problem under study . in any case ,",
    "typical values for some of these parameters in the considered examples are @xmath65 $ ] and @xmath66 $ ] .",
    "unweighted fits require @xmath67 . to take into account data uncertainties",
    "one must play around with the @xmath14 and @xmath58 parameters ( which typical values range from  0 to  3 ) .",
    "a new program called boundfit ( and available at the url given in section  [ section : introduction ] ) has been written by the author to help any person interested in playing with the method described in this paper .",
    "it is important to note that for some problems it is advisable to normalise the data ranges prior to the fitting computation in order to prevent ( or at least reduce ) numerical errors .",
    "boundfit incorporates this option , and the users should verify the benefit of applying such normalisation for their particular needs .",
    "valuable discussions with guillermo barro , juan carlos muoz and javier cenarro are gratefully acknowledged .",
    "the author is also grateful to the referee , charles jenkins , for his useful comments .",
    "this work was supported by the spanish programa nacional de astronoma y astrofsica under grant .",
    "99 bazaraa m.s . ,",
    "sherali h.d . ,",
    "shetty c.m . , 1993 ,",
    "nonlinear programming : theory and algorithms , john wiley & sons , 2nd edition cardiel n. , 1999 , phd thesis , universidad complutense de madrid cardiel n. , elbaz d. , schiavon r.p .",
    ", willmer c.n.a .",
    ", koo d.c , .",
    "phillips a.c . , gallego j. , 2003 , apj , 584 , 76 faber s.m .",
    ", 1973 , apj , 179 , 731 faber s.m . , burstein d. , dressler a. , 1977 , aj , 82 , 941 179 , 731 fletcher r. , 2007 , practical methods of optimization , john wiley & sons , 2nd edition geisler d. , 1984 , pasp , 96 , 723 gerald c.f . , wheatley p.o",
    ". , 1989 , applied numerical analysis , addison - wesley , 4th edition gill p.e .",
    ", murray w. , wright m.h . , 1989 , practical optimization , academic press haupt r.l . , haupt s.e . , 2004 , practical genetic algorithms , wiley - interscience , 2nd edition nelder j.a . , mead r. , 1965 , computer journal , 7 , 308 nocedal j. , wright s.j .",
    ", 2006 , numerical optimization , springer verlag , 2nd edition press w.h .",
    ", teukolsky s.a . ,",
    "vetterling w.t .",
    ", flannery b.p . , 2002 , numerical recipes in c++ , cambridge university press , 2nd edition rao s.s . , 1978 ,",
    "optimization : theory and applications , wiley eastern limited rich r.m .",
    ", 1988 , aj , 95 , 828 rogers b. , ferreras i. , peletier r. , silk j. , 2008 , mnras , in press ( astro - ph/0812.2029 ) snchez - bzquez p. , peletier r.f .",
    ", jimnez - vicente j. , cardiel n. , cenarro a.j . ,",
    "falcn - barroso j. , gorgas j. , selam s. , vazdekis a. , mnras , 371 , 703 rose j.a .",
    ", 1994 , aj , 107 , 206 trager s.c . , 1997 , ph.d .",
    "thesis , university of california , santa cruz vazdekis a. , arimoto n. , 1999 , apj , 525 , 144 whitford a.e .",
    ", rich r.m . , 1983 , apj , 274 , 723 whorthey g. , ottaviani d.l . , 1997 , apjs , 111 , 377",
    "sometimes it is not only necessary to obtain a given functional fit to a data set , but to do so while imposing restrictions on some of the fitted parameters .",
    "this can be done by introducing either equality or inequality constraints , or both .",
    "these constraints are normally expressed as @xmath68 being @xmath69 and @xmath70 the number of equality and inequality constraints , respectively . in the case of some boundary determinations it may be useful to incorporate these type of constraints , for example when one needs the boundary fit to pass through some pre - defined fixed points , and/or to have definite derivatives at some points ( allowing for a smooth connection between functions ) .    many techniques that allow to minimize cost functions while taking into account supplementary constraints are described in the literature ( see e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , and to explore them here in detail are beyond the aim of this work .",
    "however this appendix outlines two basic approaches that can be useful for some particular situations .      before facing the minimisation of a constrained fit , it is advisable to check whether some simple transformations can help to convert the constrained optimisation problem into an unconstrained one by making change of variables .",
    "@xcite presents some useful examples .",
    "for instance , a frequently encountered constraint is that in which a given parameter @xmath71 is restricted to lie within a given range , e.g. . in this case",
    "the simple transformation @xmath72 provides a new variable @xmath73 which can take any value . if the original parameter is restricted to satisfy @xmath74 , the trivial transformations @xmath75 , @xmath76 , or @xmath77 can be useful .",
    "unfortunately , when the constraints are not simple functions , it is not easy to find the required transformations . as highlighted by @xcite ,",
    "the transformation procedure is not always free of risk , and in the case where it is not possible to eliminate all the constraints by making change of variable , it is better to avoid partial transformation @xcite .",
    "an additional strategy that can be employed when handling equality constraints is trying to use the equations to eliminate some of the variables .",
    "for example , if for a given equality constraint @xmath78 is possible to rearrange the expression to solve for one of the variables @xmath79 then the cost function simplifies from a function in @xmath9 variables into a function in @xmath17 variables @xmath80 since the dependence on @xmath81 is removed .",
    "when the considered problem only has equality constraints and , in addition , for all of them it is possible to apply the above elimination , the fitting procedure transforms into a simpler unconstrained problem .      the weighting scheme underlying the minimisation of eq .",
    "( [ equation : gls ] ) is actually an optimisation process based on the penalisation in the cost function of the data points that falls in the _ wrong _ side ( i.e.  outside ) of the boundary to be fitted .",
    "for this reason it seems appropriate to employ additional penalty functions ( see e.g. * ? ? ?",
    "* ) to incorporate constraints into the fits .    in the case of constraining the range of some of the parameters to be fitted , , it is trivial to adjust the value of the cost function by introducing a large factor @xmath82 that clearly penalises parameters beyond the required limits . in this sense , eq .",
    "( [ equation : gls ] ) can be rewritten as @xmath83 where @xmath84 is a function that is null when the required parameters are within the requested ranges ( i.e. , the fit is performed in an unconstrained way ) , and some positive large value for the contrary situation .",
    "for the particular case of equality constraints of the form given in eq .",
    "( [ equation : equality_constraint ] ) , it is possible to directly incorporate these constraints into the cost functions as @xmath85 in this situation , for the constraints to have an impact in the cost function , the value of the penalisation factor @xmath82 must be large enough to guarantee that the first summation in eq .",
    "( [ equation : penalized_cost ] ) dominates over the second summation when a temporary solution implies a large value for any @xmath86 .",
    ", [ figure : oneoverx_spl ] and  [ figure : example ] .",
    "the boundary ( red line ) has been forced to pass through the points marked with open circles ( green ) , namely ( 0.05,100 ) and ( 0.20,100 ) . to give an important weight to the two constraints in eq .",
    "( [ equation : penalized_cost ] ) , the value of the penalisation factor has been set to .",
    "the dotted blue line is the same fit , but introducing two new additional constraints , in particular forcing the derivatives to be zero at the same fixed points . ]    as an example , fig .",
    "[ figure : constraints ] displays the upper boundary limit computed using adaptive splines for the same data previously employed in figs .",
    "[ figure : oneoverx_pol ] , [ figure : oneoverx_spl ] and  [ figure : example ] , but arbitrarily forcing the fit to pass through the two fixed points ( 0.05,100 ) and ( 0.20,100 ) , marked in the figure with the green open circles .",
    "the constrained fit ( thick continuos red line ) has been determined by introducing the two equality constraints @xmath87 the displayed fit was computed using a penalisation factor @xmath88 , with an asymmetry coefficient , , iterations , processes , , and . for comparison ,",
    "another fit ( dotted blue line ) has also been computed by introducing two more constraints , namely forcing the derivatives to be zero at the same points , i.e. , and .",
    "the resulting fit is clearly different , highlighting the importance of the introduction of the constraints .",
    "the appearance of numerical errors is one of the most important sources of problems when fitting functions , in particular polynomials , to any data set making use of a piece of software .",
    "the problems can be specially serious when handling large data sets , using high polynomial degrees , and employing different and large data ranges .",
    "since the size of the data set is usually something that one does not want to modify , and the polynomial degree is also fixed by the nature of the data being modelled ( furthermore in the case of cubic splines , where the polynomial degree is fixed ) , the easier way to reduce the impact of numerical errors is to normalise the data ranges prior to the fitting procedure . however , although this normalisation is a straightforward operation , the fitted coefficients can not be directly employed to evaluate the sought function in the original data ranges .",
    "previously it is necessary to properly transform those coefficients .",
    "this appendix provides the corresponding coefficient transformations for the case of the fitting to simple one - dimensional polynomials and to cubic splines .",
    "simple polynomials are typically expressed as @xmath89 let s consider that the ranges exhibited by the data in the corresponding coordinate axes are given by the intervals @xmath90 $ ] and @xmath91 $ ] , and assume that one wants to normalise the data within these intervals into new ones given by @xmath92 $ ] and @xmath93 $ ] , through a point - to - point mapping from the original intervals into the new ones , @xmath94   & \\longrightarrow & \\left[\\tilde{x}_{\\mbox{\\scriptsize min } } ,        \\tilde{x}_{\\mbox{\\scriptsize max}}\\right ] , \\quad \\mbox{and } \\\\",
    "\\left[y_{\\mbox{\\scriptsize min}},y_{\\mbox{\\scriptsize max}}\\right ]   & \\longrightarrow & \\left[\\tilde{y}_{\\mbox{\\scriptsize min } } ,        \\tilde{y}_{\\mbox{\\scriptsize max}}\\right]\\end{aligned}\\ ] ] for this purpose , linear transformations of the form @xmath95 are appropriate , where @xmath96 and @xmath97 are constants ( @xmath98 and @xmath99 are scaling factors , and @xmath100 and @xmath101 represent origin offsets in the normalised data ranges ) . the inverse transformations will be given by @xmath102 assuming that the original and final intervals are not null ( i.e. , , , and ) , it is trivial to show that the transformation constants are given by @xmath103 and the analogue expressions for the coefficients of the @xmath26-axis transformation .",
    "for example , to perform all the arithmetical manipulations with small numbers , it is useful to choose and , which leads to @xmath104 and the analogue expressions for @xmath99 and @xmath101",
    ".    once the data have been properly normalised in both axes following the transformations given in eq .",
    "( [ equation_linear_transformations ] ) , it is possible to carry out the fitting procedure , which provides the resulting polynomial expressed in terms of the transformed data ranges as @xmath105 at this point , the relevant question is how to transform the fitted coefficients into the coefficients corresponding to the same polynomial defined over the original data ranges . by substituting the relations given in eq .",
    "( [ equation_linear_transformations ] ) in the previous expression one directly obtains @xmath106 remembering that @xmath107 with the binomial coefficient computed as @xmath108 and comparing the substitution of eq .",
    "( [ equation_m_power ] ) and eq .",
    "( [ equation_binomial_coefficient ] ) into eq .",
    "( [ equation_replacing_transformations ] ) with the expression given in eq .",
    "( [ equation_fitted_polynomial ] ) , it is not difficult to show that if one defines @xmath109 the sought coefficients will be given by @xmath110 in the particular case in which @xmath111 , the above expressions simplify to @xmath112    a. this plot is the same than fig .",
    "[ figure : coeff_pol ] , but in this case analysing the impact of the normalisation of the data ranges prior to the boundary determination .",
    "each panel represents the coefficient value at a given iteration ( @xmath24 , with , from bottom to top ) divided by @xmath25 , the final value derived after iterations .",
    "the same range is employed in all the plots .",
    "the red line shows the results when applying the normalisation , and the blue line indicates the coefficient variations when this normalisation is not applied . in both cases , and were used .",
    "note that the plot is in logarithmic units . ]",
    "the normalisation of the data ranges has several advantages .",
    "[ figure : coeffnorm ] ( similar to fig .",
    "[ figure : coeff_pol ] ) shows the impact of data normalisation on the convergence properties of the fitted coefficients , as a function of the number of iterations , for the upper boundary fit ( 5th order polynomial ) shown in fig .",
    "[ figure : oneoverx_pol]a .",
    "the red line , corresponding to the results when the normalisation is applied prior to the boundary fitting , indicates that after , the coefficients have converged .",
    "the situation is much worse when the normalisation is not applied , as illustrated by the blue line . in this case",
    "the convergence is only reached after iterations , ten times more than when using the normalisation .",
    "in addition , the ranges spanned by the coefficient values along the minimisation procedure are narrower when the data ranges have been previously normalised .",
    "coordinates were transformed using and in order to artificially enlarge the data ranges . _ panel  ( a ) _",
    ": bootstrapped data and fitted boundaries . _",
    "panel  ( b ) _ : residuals relative to the original sinusoidal function . in both panels",
    "the lines indicate the resulting fits for different polynomial degrees and normalisation strategies ( in all the cases , and were employed ) .",
    "the continuous red lines are the boundaries obtained using polynomials of degree 10 and normalising the data ranges prior to the fitting procedure .",
    "the green and blue lines correspond to the fits obtained by fitting polynomials of degrees 9 and 8 , respectively , without normalising the data ranges .",
    "using the original data ranges the boundary fits start to depart from the expected location due to numerical errors for polynomials of degree 9 .",
    "however polynomials of degree 10 are still an option when the data ranges are previously normalised . ]",
    "[ figure : numerr ] exemplifies the appearance of numerical errors that takes place when increasing the polynomial degree during the fitting of a reasonably large data set . in this case 10000",
    "points are fitted employing upper and lower boundaries with simple polynomials of degree 10 ( red lines ) after normalising the data ranges using the coefficients given in eqs .",
    "( [ equation_normalization_c_uno ] ) and  ( [ equation_normalization_c_dos ] ) ( with the analogue expressions for the @xmath26-axis coefficients ) prior to the numerical minimisation .",
    "when the data ranges are not normalised , the fitting to polynomials of degree 10 gives non - sense results .",
    "only polynomials of degree less or equal than 9 are computable . and for the case of degree 9 the results are unsatisfactory ( green lines ) , being the polynomials of degree 8 ( blue lines ) the first reasonable boundaries while fitting the data preserving their original ranges .",
    "thus in this particular example the normalisation of the data ranges allows to extend the fitted polynomial degree in two units .",
    "normalisation of the data ranges is also important for the computation of cubic splines , in particular for the boundary fitting to adaptive splines described in section  [ section : adaptive_splines ] . in that section",
    "the functional form of a fit to set of @xmath29 was expressed as @xmath113 ^ 3 +            s_2(k ) [ x - x_{\\mbox{\\scriptsize knot}}(k)]^2 + \\nonumber \\\\        + & s_1(k ) [ x - x_{\\mbox{\\scriptsize knot}}(k ) ] + s_0(k ) , \\end{array } \\label{equation : splines_original}\\ ] ] where ( @xmath31 ) are the @xmath32 coordinates of the @xmath33  knot , and @xmath34 , @xmath35 , @xmath36 , and @xmath37 are the corresponding spline coefficients for , with .",
    "using the same nomenclature previously employed for the case of simple polynomials , the result of a fit to cubic splines performed over normalised data ranges should be written as @xmath114 ^ 3 +        \\tilde{s}_2(k ) [ \\tilde{x}-\\tilde{x}_{\\mbox{\\scriptsize knot}}(k)]^2 +         \\nonumber \\\\        + & \\tilde{s}_1(k ) [ \\tilde{x}-\\tilde{x}_{\\mbox{\\scriptsize knot}}(k ) ] +         \\tilde{s}_0(k ) .",
    "\\end{array } \\label{equation : splines_normalized}\\ ] ] following a similar reasoning to that used previously , it is straightforward to see that the sought transformations are @xmath115 where . note that these transformations are identical to eq .",
    "( [ equation : trans_pol_simple ] ) .",
    "this is not surprising considering that splines are polynomials and that the adopted functional form given in eq .  ( [ equation : splines_original ] ) is actually providing the @xmath16 coordinate as a function of the distance between the considered @xmath27 and corresponding value @xmath116 for the nearest knot placed at the left side of @xmath27 .",
    "thus , the @xmath100 coefficient is not relevant here .      although the method described in this appendix can help in some circumstances to perform fits with larger data sets or higher polynomial degrees than without any normalisation of the data ranges , it is important to keep in mind that such normalisation does not always produce the expected results and that numerical errors appear in any case sooner or later if one tries to use excessively large data sets or very high values for the polynomial degrees .",
    "anyhow , the fact that the normalisation of the data ranges can facilitate the boundary determination of large data sets or to use higher polynomial degrees justifies the effort of checking whether such normalisation is of any help . sometimes , to extend the polynomial degrees by even just a few units can be enough to solve the particular problem one is dealing with .",
    "the program boundfit incorporates the normalisation of the data prior to the boundary fitting as an option ."
  ],
  "abstract_text": [
    "<S> in many astronomical problems one often needs to determine the upper and/or lower boundary of a given data set . </S>",
    "<S> an automatic and objective approach consists in fitting the data using a generalised least - squares method , where the function to be minimized is defined to handle _ asymmetrically _ the data at both sides of the boundary . in order to minimise the cost function , a numerical approach , based on the popular downhill simplex method , </S>",
    "<S> is employed . </S>",
    "<S> the procedure is valid for any numerically computable function . </S>",
    "<S> simple polynomials provide good boundaries in common situations . for data exhibiting a complex behaviour </S>",
    "<S> , the use of _ adaptive splines _ gives excellent results . </S>",
    "<S> since the described method is sensitive to extreme data points , the simultaneous introduction of error weighting and the flexibility of allowing some points to fall outside of the fitted frontier , supplies the parameters that help to tune the boundary fitting depending on the nature of the considered problem . </S>",
    "<S> two simple examples are presented , namely the estimation of spectra pseudo - continuum and the segregation of scattered data into ranges . </S>",
    "<S> the normalisation of the data ranges prior to the fitting computation typically reduces both the numerical errors and the number of iterations required during the iterative minimisation procedure .    </S>",
    "<S> [ firstpage ]    methods : data analysis  methods : numerical . </S>"
  ]
}