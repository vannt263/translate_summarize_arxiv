{
  "article_text": [
    "support vector machines ( svm ) are a group of popular classification methods in machine learning .",
    "their input is a set of data points @xmath4 , each equipped with a label @xmath5 , which assigns each of the data points to one of two groups .",
    "svm aims for binary linear classification based on separating hyperplane between the two groups of training data , choosing a hyperplane with separating gap as large as possible .    since their introduction by vapnik and chervonenkis @xcite , the subject of svm was studied intensively .",
    "we will concentrate on the so - called soft margin svm @xcite , which allow also for misclassification of the training data are the most used version of svm nowadays .    in its most common form",
    "( and neglecting the bias term ) , the soft - margin svm is a convex optimization program @xmath6 for some tradeoff parameter @xmath7 and so called slack variables @xmath8",
    ". it will be more convenient for us to work with the following equivalent reformulation of @xmath9_+\\quad\\text{subject to}\\quad\\|w\\|_2\\leq r,\\end{aligned}\\ ] ] where @xmath10 gives the restriction on the size of @xmath11 .",
    "we refer to monographs @xcite and references therein for more details on svm and to ( * ? ? ?",
    "* chapter b.5 ) and ( * ? ? ?",
    "* chapter 9 ) for a detailed discussion on dual formulations .      as the classical svm and",
    "do not use any pre - knowledge about @xmath11 , one typically needs to have more training data than the underlying dimension of the problem , i.e. @xmath12 especially in analysis of high - dimensional data , this is usually not realistic and we typically deal with much less training data , i.e. with @xmath13 on the other hand , we can often assume some structural assumptions on @xmath11 , in the most simple case that it is _ sparse _ ,",
    "i.e. that most of its coordinates are zero",
    ". motivated by the success of lasso @xcite in sparse linear regression , it was proposed in @xcite that replacing the @xmath14-norm @xmath15 in by its @xmath0-norm @xmath16 leads to sparse classifiers @xmath17 .",
    "this method was further popularized in @xcite by zhu , rosset , hastie , and tibshirani , who developed an algorithm that efficiently computes the whole solution path ( i.e. the solutions of for a wide range of parameters @xmath10 ) .",
    "we refer also to @xcite and @xcite for other generalizations of the concept of svm .    using the ideas of concentration of measure @xcite and random constructions in banach spaces @xcite",
    ", the performance of lasso was analyzed in the recent area of compressed sensing @xcite .",
    "@xmath0-svm ( and its variants ) found numerous applications in high - dimensional data analysis , most notably in bioinformatics for gene selection and microarray classification @xcite .",
    "finally , @xmath0-svm s are closely related to other popular methods of data analysis , like elastic nets @xcite or sparse principal components analysis @xcite .",
    "the main aim of this paper is to analyze the performance of @xmath0-svm in the non - asymptotic regime . to be more specific ,",
    "let us assume that the data points @xmath18 can be separated by a hyperplane according to the given labels @xmath19 , and that this hyperplane is normal to a @xmath2-sparse vector @xmath20 .",
    "hence , @xmath21 if @xmath22 and @xmath23 if @xmath24 we then obtain @xmath25 as the minimizer of the @xmath0-svm .",
    "the first main result of this paper ( theorem [ theo : main_theorem ] ) then shows that @xmath26 is a good approximation of @xmath27 , if the data points are i.i.d .",
    "gaussian vectors and the number of measurements scales linearly in @xmath2 and logarithmically in @xmath1 .    later on , we introduce a modification of @xmath0-svm by adding an additional @xmath14-constraint . it will be shown in theorem [ theo : l2_main_theorem ] , that it still approximates the sparse classifiers with the number of measurements @xmath28 growing linearly in @xmath2 and logarithmically in @xmath1 , but the dependence on other parameters improves . in this sense , this modification outperforms the classical @xmath0-svm .",
    "the paper is organized as follows .",
    "section [ sec:2 ] recalls the concept of @xmath29-support vector machines of @xcite .",
    "it includes the main result , namely theorem [ theo : main_theorem ] .",
    "it shows that the @xmath29-svm allows to approximate sparse classifier @xmath27 , where the number of measurements only increases logarithmically in the dimension @xmath1 as it is typical for several reconstruction algorithms from the field of compressed sensing .",
    "the two most important ingredients of its proof , theorems [ theo : concentration_estimate ] and [ theo : psi - psi ] , are also discussed in this part .",
    "the proof techniques used are based on the recent work of plan and vershynin @xcite , which in turn makes heavy use of classical ideas from the areas of concentration of measure and probability estimates in banach spaces @xcite .",
    "section [ sec:3 ] gives the proofs of theorems [ theo : concentration_estimate ] and [ theo : psi - psi ] . in section [ sec:4 ]",
    "we discuss several extensions of our work , including a modification of @xmath0-svm , which combines the @xmath0 and @xmath14 penalty .",
    "finally , in section [ sec:5 ] we show numerical tests to demonstrate the convergence results of section [ sec:2 ] .",
    "in particular , we compare different versions of svm and 1-bit compressed sensing , which was first introduced by boufounos and baraniuk in @xcite and then discussed and continued in @xcite and others .",
    "we denote by @xmath30_+:=\\max(\\lambda,0)$ ] the positive part of a real number @xmath31 by @xmath32 and @xmath33 we denote the @xmath0 , @xmath14 and @xmath34 norm of @xmath17 , respectively .",
    "we denote by @xmath35 the normal ( gaussian ) distribution with mean @xmath36 and variance @xmath37 .",
    "when @xmath38 and @xmath39 are random variables , we write @xmath40 if they are equidistributed .",
    "multivariate normal distribution is denoted by @xmath41 , where @xmath42 is its mean and @xmath43 is its covariance matrix . by @xmath44",
    "we denote the natural logarithm of @xmath45 with basis @xmath46 .",
    "further notation will be fixed in section [ sec:2 ] under the name of `` standing assumptions '' , once we fix the setting of our paper .",
    "in this section we give the setting of our study and the main results .",
    "let us assume that the data points @xmath18 are equipped with labels @xmath5 in such a way that the groups @xmath47 and @xmath48 can indeed be separated by a sparse classifier @xmath27 , i.e. that @xmath49 and @xmath50 as the classifier is usually not unique , we can not identify @xmath27 exactly by any method whatsoever .",
    "hence we are interested in a good approximation of @xmath27 obtained by @xmath29-norm svm from a minimal number of training data . to achieve this goal",
    ", we will assume that the training points @xmath51 are i.i.d .",
    "measurement vectors for some constant @xmath52 .    to allow for more generality ,",
    "we replace by @xmath53 let us observe , that @xmath54 and @xmath55 implies also @xmath56 , i.e. with @xmath57    furthermore , we denote by @xmath25 the minimizer of @xmath58_+\\quad\\text{subject to}\\quad\\|w\\|_1\\leq r.\\end{aligned}\\ ] ]    let us summarize the setting of our work , which we will later on refer to as `` standing assumptions '' and which we will keep for the rest of this paper .. 2 cm    -.3cm.2 cm    in order to estimate the difference between @xmath27 and @xmath25 we adapt the ideas of @xcite .",
    "first we observe @xmath59 i.e. @xmath60 hence , it remains    * to bound the right hand side of from above and * to estimate the left hand side in by the distance between @xmath27 and @xmath25 from below .",
    "we obtain the following two theorems , whose proofs are given in section [ sec:3 ] .",
    "[ theo : concentration_estimate ] let @xmath61 . under the `` standing assumptions '' it holds @xmath62 with probability at least @xmath63    [ theo : psi - psi ]",
    "let the `` standing assumptions '' be fulfilled and let @xmath64 .",
    "put @xmath65 and assume that @xmath66 . if furthermore @xmath67 , then @xmath68 can be estimated from below by @xmath69",
    "if @xmath70 , then @xmath68 can be estimated from below by @xmath71    combining theorems [ theo : concentration_estimate ] and [ theo : psi - psi ] with we obtain our main result .",
    "[ theo : main_theorem ] let @xmath72 , @xmath73 , @xmath74 and @xmath75 for some constant @xmath76 . under the `` standing assumptions '' it holds @xmath77 with probability at least @xmath78 for some positive constants @xmath79 .    1 .   if the classifier @xmath20 with @xmath54 is @xmath2-sparse , we always have @xmath80 and we can choose @xmath81 in theorem [ theo : main_theorem ] .",
    "the dependence of @xmath28 , the number of samples needed , is then linear in @xmath2 and logarithmic in @xmath1 . intuitively , this is the best what we can hope for . on the other hand ,",
    "we leave it open , if the dependence on @xmath82 and @xmath83 is optimal in theorem [ theo : main_theorem ] .",
    "2 .   theorem [ theo : main_theorem ] uses the constants @xmath76 , @xmath84 and @xmath85 only for simplicity .",
    "more explicitly we show that taking @xmath86 we get the estimate @xmath87 with probability at least @xmath88 3 .",
    "if we introduce an additional parameter @xmath89 and choose @xmath90 , nothing but the probability changes to @xmath91 hence , by fixing @xmath92 large , we can increase the value of @xmath85 and speed up the convergence of to 1 .    to apply theorem [ theo : concentration_estimate ] we choose @xmath93 and",
    "we obtain the estimate @xmath94 with probability at least @xmath95 using this already implies @xmath96 with at least the same probability .",
    "now we want to apply theorem [ theo : psi - psi ] with @xmath97 to estimate the left hand side of this inequality",
    ". therefore we first have to deal with the case @xmath98 , which only holds if @xmath99 for some @xmath100 . if @xmath7 , then @xmath101 and the statement of the theorem holds trivially .",
    "if @xmath102 , then the condition @xmath103 can be rewritten as @xmath104_+\\le \\sum_{i=1}^m[1-|\\langle x_i , a \\rangle|]_+.\\end{aligned}\\ ] ] this inequality holds if , and only if , @xmath105 for all @xmath106 - and this in turn happens only with probability zero .",
    "we may therefore assume that @xmath107 holds almost surely and we can apply theorem [ theo : psi - psi ] .",
    "here we distinguish the three cases @xmath108 , @xmath109 and @xmath110 .",
    "first , we will show that the two cases @xmath111 and @xmath112 lead to a contradiction and then , for the case @xmath113 , we will prove our claim .",
    "case @xmath114 : _ using theorem [ theo : psi - psi ] we get the estimate @xmath115 and gives ( with our choices for @xmath83 and @xmath82 ) the contradiction @xmath116 _ 2 .",
    "case @xmath109 : _ as in the first case we use theorem [ theo : psi - psi ] in order to show a contradiction .",
    "first we get the estimate @xmath117 now we consider the function @xmath118 it holds @xmath119 and @xmath120 so @xmath121 is monotonic decreasing . with @xmath122",
    "this yields @xmath123 again , now gives the contradiction @xmath124 we conclude that it must hold @xmath66 and @xmath113 almost surely .",
    "case @xmath110 : _ in this case we get the estimate @xmath125 where we used @xmath126 for the last inequality . further we get @xmath127 finally , combining , and , we arrive at @xmath128 which finishes the proof of the theorem .",
    "the main aim of this section is to prove theorems [ theo : concentration_estimate ] and [ theo : psi - psi ] .",
    "before we come to that , we shall give a number of helpful lemmas .",
    "in this subsection we want to show that @xmath129 does not deviate uniformly far from its expected value @xmath130 , i.e. we want to show that @xmath131 is small with high probability . therefore we will first estimate its mean @xmath132 and then use a concentration inequality to prove theorem [ theo : concentration_estimate ] .",
    "the proof relies on standard techniques from @xcite and @xcite and is inspired by the analysis of 1-bit compressed sensing given in @xcite .    for @xmath133 let @xmath134 be i.i.d .",
    "bernoulli variables with @xmath135 let us put @xmath136_+,\\quad      { \\mathcal{a}}(w)=[1-y{\\langle x , w \\rangle}]_+,\\end{aligned}\\ ] ] where @xmath137 is an independent copy of any of the @xmath138 and @xmath139 .",
    "further , we will make use of the following lemmas .",
    "[ lemma : bernoulli ] for @xmath140 , i.i.d .",
    "bernoulli variables @xmath141 according to and any scalars @xmath142 it holds @xmath143_+\\geq t\\biggr )          \\leq 2{\\mathbb{p}}\\biggl(\\sum\\limits_{i=1}^m{\\varepsilon}_i \\lambda_i\\geq t\\biggr ) .",
    "\\end{aligned}\\ ] ]    first we observe @xmath144_+\\geq t\\biggr )          = { \\mathbb{p}}\\biggl(\\sum\\limits_{\\lambda_i\\geq0}{\\varepsilon}_i\\lambda_i\\geq t\\biggr)\\\\          & \\qquad={\\mathbb{p}}\\biggl(\\sum\\limits_{\\lambda_i\\geq0}{\\varepsilon}_i\\lambda_i\\geq t\\text { and }              \\sum\\limits_{\\lambda_i<0}{\\varepsilon}_i\\lambda_i\\geq 0\\biggr)\\\\              & \\quad\\qquad+{\\mathbb{p}}\\biggl(\\sum\\limits_{\\lambda_i\\geq0}{\\varepsilon}_i\\lambda_i\\geq t\\text { and }              \\sum\\limits_{\\lambda_i<0}{\\varepsilon}_i\\lambda_i < 0\\biggr ) .",
    "\\end{aligned}\\ ] ] now we can estimate the second of these two probabilities by the first one and we arrive at @xmath145_+\\geq t\\biggr )          & \\leq 2{\\mathbb{p}}\\biggl(\\sum\\limits_{\\lambda_i\\geq0}{\\varepsilon}_i\\lambda_i\\geq t\\text { and }              \\sum\\limits_{\\lambda_i<0}{\\varepsilon}_i\\lambda_i\\geq 0\\biggr)\\\\          & \\leq 2{\\mathbb{p}}\\biggl(\\sum\\limits_{i=1}^m{\\varepsilon}_i\\lambda_i\\geq t\\biggr ) .",
    "\\end{aligned}\\ ] ]    [ lemma : concentration_bernoulli_gaussian ]    1 .   for gaussian random variables @xmath4 according to it holds @xmath146 2 .",
    "let the i.i.d .",
    "bernoulli variables @xmath141 be according to and let @xmath61 .",
    "then it holds @xmath147 3 .   for @xmath4 and @xmath148 according to and we denote @xmath149 then it holds @xmath150    1 .",
    "the statement follows from @xmath151 with @xmath152 and proposition 8.1 of @xcite : @xmath153 2 .",
    "the estimate follows as a consequence of hoeffding s inequality @xcite .",
    "3 .   theorem 5.2 of @xcite gives the estimate @xmath154 with @xmath155 since the @xmath156 are independent we get @xmath157 and we end up with @xmath158      to estimate the mean @xmath36 , we first derive the following symmetrization inequality , cf .",
    "* chapter 6 ) and ( * ? ? ?",
    "* lemma 5.1 ) .",
    "[ lemma : symmetrization ] let @xmath141 be i.i.d .",
    "bernoulli variables according to . under the `` standing assumptions '' it holds for @xmath36 defined by @xmath159_+\\biggr\\vert .",
    "\\end{aligned}\\ ] ]    let @xmath160 and @xmath161 be according to .",
    "let @xmath162 and @xmath163 be independent copies of @xmath138 and @xmath137 . then @xmath164 and @xmath165 , generated in the same way with @xmath162 and @xmath163 instead of @xmath138 and @xmath137 , are independent copies of @xmath160 and @xmath161 .",
    "we denote by @xmath166 the mean value with respect to @xmath162 and @xmath163 . using @xmath167 ,",
    "we get @xmath168 applying jensen s inequality we further get @xmath169_+\\biggr\\vert\\end{aligned}\\ ] ] as claimed .",
    "equipped with this tool , we deduce the following estimate for @xmath36 .    [ lemma : estimate_mu ] under the `` standing assumptions '' we have @xmath170    using lemma [ lemma : symmetrization ] we obtain @xmath171_+\\biggr\\vert\\\\          & = 2\\int_0^\\infty{\\mathbb{p}}\\biggl(\\sup\\limits_{w\\in k}\\biggl\\vert              \\frac{1}{m}\\sum\\limits_{i=1}^m{\\varepsilon}_i[1-y_i{\\langle x_i , w \\rangle}]_+\\biggr\\vert \\ge t\\biggr)\\,dt .",
    "\\end{aligned}\\ ] ] now we can apply lemma [ lemma : bernoulli ] to get @xmath172 using the second part of lemma [ lemma : concentration_bernoulli_gaussian ] we can further estimate @xmath173 using the duality @xmath174 and the first part of lemma [ lemma : concentration_bernoulli_gaussian ] we get @xmath175      in this subsection we will estimate the probability that @xmath129 deviates anywhere on @xmath176 far from its mean , i.e. the probability @xmath177 for some @xmath89 . first we obtain the following modified version of the second part of lemma 5.1 of @xcite , cf . also ( * ? ? ? * chapter 6.1 ) .    [ lemma : deviation_inequality ] let @xmath141 be i.i.d .",
    "bernoulli variables according to and let the `` standing assumptions '' be fulfilled .",
    "then , for @xmath178 according to and any @xmath89 , it holds @xmath179_+\\biggr\\vert              \\geq t/2\\biggr ) .",
    "\\end{aligned}\\ ] ]    using markov s inequality let us first note @xmath180 using this inequality we get @xmath181 let @xmath182 and @xmath183 be again defined by , and let @xmath184 be independent copies of @xmath182 . we further get @xmath185 which yields the claim .    combining the lemmas [ lemma : bernoulli ] and [ lemma : deviation_inequality ] we deduce the following result .",
    "[ lemma : final_uniform_concentration ] under the `` standing assumptions '' it holds for @xmath36 and @xmath186 according to and and any @xmath61 @xmath187    applying lemma [ lemma : deviation_inequality ] and lemma [ lemma : bernoulli ] we get @xmath188_+\\biggr\\vert\\geq \\tilde\\mu+u/2\\biggr)\\\\ & \\leq 8{\\mathbb{p}}\\biggl(\\sup\\limits_{w\\in k}\\biggl\\vert\\frac{1}{m}\\sum\\limits_{i=1}^m{\\varepsilon}_i ( 1-y_i{\\langle x_i , w \\rangle})\\biggr\\vert\\geq \\tilde\\mu+u/2\\biggr)\\\\ & \\leq 8{\\mathbb{p}}\\biggl(\\biggl\\vert\\frac{1}{m}\\sum\\limits_{i=1}^m { \\varepsilon}_i\\biggr\\vert\\geq u/4\\biggr)\\\\ & \\qquad+8{\\mathbb{p}}\\biggl(\\sup\\limits_{w\\in k}\\biggl\\vert\\biggl\\langle\\frac{1}{m}\\sum\\limits_{i=1}^m x_i , w \\biggr\\rangle\\biggr\\vert\\geq \\tilde\\mu+u/4\\biggr).\\end{aligned}\\ ] ] finally , applying the second and third part of lemma [ lemma : concentration_bernoulli_gaussian ] this can be further estimated from above by @xmath189 which finishes the proof .",
    "using the two lemmas [ lemma : estimate_mu ] and [ lemma : final_uniform_concentration ] we can now prove theorem [ theo : concentration_estimate ] .",
    "lemma [ lemma : final_uniform_concentration ] yields @xmath190 using lemma [ lemma : estimate_mu ] we further get @xmath191 invoking the duality @xmath174 and the first part of lemma [ lemma : concentration_bernoulli_gaussian ] we can further estimate @xmath192 by @xmath193 hence , with probability at least @xmath194 we have @xmath195 as claimed .",
    "in this subsection we will estimate @xmath196_+-{\\mathbb{e}}[1-y{\\langle x , a \\rangle}]_+\\end{aligned}\\ ] ] for some @xmath197 with @xmath198 .",
    "we will first calculate both expected values separately and later estimate their difference .",
    "we will make use of the following statements from probability theory .",
    "[ lemma : covariance ] let @xmath199 be according to , and let @xmath197",
    ". then it holds    1 .",
    "@xmath200 , 2 .   @xmath201 .    the first statement is well known in probability theory as the 2-stability of normal distribution . for the second statement we get @xmath202 as claimed .",
    "it is very well known , cf .",
    "* corollary 5.2 ) , that projections of a gaussian random vector onto two orthogonal directions are mutually independent .",
    "[ lemma : independent ] let @xmath203 and let @xmath204 with @xmath205 then @xmath206 and @xmath207 are independent random variables .    applying these two lemmas to our case we end up with the following lemma .",
    "[ lemma : covariance_decomposition ] for @xmath20 according to , @xmath208 and @xmath17 we have @xmath209 for some @xmath210 independent of @xmath206 and @xmath211    note that @xmath212 is well defined , since @xmath213 .",
    "if @xmath214 , the statement holds trivially .",
    "if @xmath215 , we set @xmath216 hence , @xmath217 is indeed normally distributed with @xmath218 and @xmath219 .",
    "it remains to show that @xmath217 and @xmath206 are independent .",
    "we observe that @xmath220 and , finally , lemma [ lemma : independent ] yields the claim .",
    "[ lemma : psi_aa ] let @xmath20 and @xmath221 be according to ,",
    ". then it holds    1 .",
    "@xmath222_+ e^{\\frac{-t^2}{2}}\\,dt , $ ] 2 .",
    "@xmath223_+e^{\\frac{-t_1 ^ 2-t_2 ^ 2}{2}}\\,dt_1\\,dt_2 $ ] , where @xmath224 and @xmath212 are defined by .    1 .",
    "let @xmath225 and use the first part of lemma [ lemma : covariance ] to obtain @xmath226_+={\\mathbb{e}}[1-r\\vert\\omega\\vert]_+\\\\ & = \\frac{1}{\\sqrt{2\\pi}}\\int_{\\mathbb{r}}\\big[1-r\\vert t\\vert\\big]_+ e^{\\frac{-t^2}{2}}\\,dt.\\end{aligned}\\ ] ] 2 .   using the notation of lemma [ lemma : covariance_decomposition ]",
    "we get @xmath227_+\\\\ & = { \\mathbb{e}}[1-{\\mathrm{sign}}({\\langle x , a \\rangle})(c{\\langle x , a \\rangle}+c'z)]_+\\\\ & = { \\mathbb{e}}[1-c\\vert{\\langle x , a \\rangle}\\vert - c'{\\mathrm{sign}}({\\langle x , a \\rangle})z]_+\\\\ & = { \\mathbb{e}}[1-c\\vert{\\langle x , a \\rangle}\\vert - c'z]_+\\\\ & = \\frac{1}{2\\pi}\\int_{{\\mathbb{r}}^2}[1-cr\\vert t_1\\vert - c'rt_2]_+e^{\\frac{-t_1 ^ 2-t_2 ^ 2}{2}}\\,dt_1\\,dt_2.\\end{aligned}\\ ] ]    using this result we now can prove theorem [ theo : psi - psi ] .",
    "using lemma [ lemma : psi_aa ] we first observe @xmath228_+e^{\\frac{-t^2}{2}}\\,dt\\\\ & = -\\sqrt{2\\pi}\\int_0^{\\frac{1}{r}}\\big(1-rt\\big)e^{\\frac{-t^2}{2}}\\,dt\\notag\\\\ & \\ge-\\sqrt{2\\pi}\\int_0^{\\frac{1}{r}}e^{\\frac{-t^2}{2}}\\,dt \\notag\\geq-\\frac{\\sqrt{2\\pi}}{r}.\\end{aligned}\\ ] ] to estimate the expected value of @xmath129 we now distinguish the two cases @xmath67 and @xmath70 .",
    "+ _ 1 . case : @xmath111 : _ in that case we get @xmath229_+e^{\\frac{-t_1 ^ 2-t_2 ^ 2}{2}}\\,dt_1\\,dt_2 .",
    "\\end{aligned}\\ ] ] since @xmath230 for @xmath231 we can further estimate @xmath232_+e^{\\frac{-t_1 ^ 2-t_2",
    "^ 2}{2}}\\,dt_1\\,dt_2\\\\          & \\geq\\int_{-\\infty}^0\\int_0^\\infty(1-c'rt_2)e^{\\frac{-t_1 ^ 2-t_2 ^ 2}{2}}\\,dt_1\\,dt_2\\\\          & = \\int_{-\\infty}^0\\int_0^\\infty e^{\\frac{-t_1 ^ 2-t_2 ^ 2}{2}}\\,dt_1\\,dt_2              + c'r\\int_0^\\infty\\int_0^\\infty t_2e^{\\frac{-t_1 ^",
    "2-t_2 ^ 2}{2}}\\,dt_1\\,dt_2\\\\          & = \\frac{\\pi}{2}+c'r\\frac{\\sqrt{\\pi}}{\\sqrt{2}}.      \\end{aligned}\\ ] ] as claimed , putting both terms together , we arrive at @xmath233 _ 2 .",
    "case : @xmath70 : _ first let us observe that @xmath234 on @xmath235\\times(-\\infty,0]\\subset{\\mathbb{r}}^2 $ ] . hence ,",
    "we get @xmath236_+e^{\\frac{-t_1 ^ 2-t_2 ^ 2}{2}}\\,dt_2\\,dt_1\\\\      & \\geq\\int_0^{\\frac{1}{cr}}\\int_{-\\infty}^0(1-crt_1-c'rt_2)e^{\\frac{-t_1 ^ 2-t_2 ^ 2}{2}}\\,dt_2\\,dt_1\\\\      & = \\frac{\\sqrt{\\pi}}{\\sqrt{2}}\\int_0^{\\frac{1}{cr}}(1-crt)e^{\\frac{-t^2}{2}}\\,dt      + c'r\\int_0^{\\frac{1}{cr}}e^{\\frac{-t^2}{2}}\\,dt\\\\      & \\geq \\frac{\\sqrt{\\pi}}{\\sqrt{2}}\\int_0^{\\frac{1}{cr}}(1-crt)e^{\\frac{-t^2}{2}}\\,dt      + \\frac{c'}{c}\\exp\\left(\\frac{-1}{2c^2r^2}\\right).\\end{aligned}\\ ] ] combining this estimate with we arrive at @xmath237",
    "a detailed inspection of the analysis done so far shows that it would be convenient if the convex body @xmath176 would not include vectors with large @xmath14-norm .",
    "for example , in we needed to calculate @xmath239 , although the measure of the set of vectors in @xmath176 with @xmath14-norm close to @xmath240 is extremely small .",
    "therefore , we will modify the @xmath29-svm by adding an additional @xmath238-constraint , that is instead of we consider the optimization problem @xmath241_+\\ \\text{s .",
    "t.}\\ \\|w\\|_1\\leq r~\\text{and}~\\|w\\|_2\\leq 1.\\end{aligned}\\ ] ] the combination of @xmath0 and @xmath14 constraints is by no means new - for example , it plays a crucial role in the theory of elastic nets @xcite .",
    "furthermore , let us remark that the set @xmath242 appears also in @xcite .",
    "we get @xmath243 with @xmath176 according to . hence , theorem [ theo : concentration_estimate ] and still remain true if we replace @xmath176 by @xmath244 and we obtain @xmath245 with high probability and @xmath246 where @xmath25 is now the minimizer of .",
    "it remains to estimate the expected value @xmath247 in order to obtain an analogue of theorem [ theo : main_theorem ] for , which reads as follows .",
    "[ theo : l2_main_theorem ] let @xmath72 , @xmath248 , @xmath249 , @xmath20 according to , @xmath75 for some constant @xmath76 , @xmath4 according to and @xmath250 a minimizer of",
    ". then it holds @xmath251 with probability at least @xmath252 for some positive constants @xmath79 .    1 .",
    "as for theorem [ theo : main_theorem ] we can write down the expressions explicitly , i.e. without the constants @xmath253 and @xmath85 . that is , taking @xmath254 for some @xmath89 , we get @xmath255 with probability at least @xmath256 2 .",
    "the main advantage of theorem [ theo : l2_main_theorem ] compared to theorem [ theo : main_theorem ] is that the parameter @xmath83 does not need to grow to infinity .",
    "actually , is clearly not optimal for large @xmath83 . indeed , if ( say ) @xmath257 , we can take @xmath258 , and obtain @xmath259 for @xmath260 with high probability .    as in the proof of theorem [ theo : main_theorem ]",
    "we first obtain @xmath261 and @xmath262 . using lemma [ lemma : psi_aa ]",
    "we get @xmath263 with @xmath264 the claim now follows from and .",
    "we performed several numerical tests to exhibit different aspects of the algorithms discussed above . in the first two parts of this section we fixed @xmath265 and set @xmath266 with 5 nonzero entries @xmath267 , @xmath268 , @xmath269 , @xmath270 , @xmath271 , afterwards we normalized @xmath272 and set @xmath273 and @xmath274 .",
    "we run the @xmath0-svm with @xmath275 and @xmath276 for different values of @xmath83 between zero and 1.5 .",
    "the same was done for the @xmath0-svm with the additional @xmath14-constraint , which is called @xmath277-svm in the legend of the figure .",
    "the average error of @xmath278 trials between @xmath27 and @xmath26 is plotted against @xmath83 .",
    "we observe that especially for small @xmath83 s the @xmath0-svm with @xmath14-constraint performs much better than classical @xmath0-svm .",
    ", width=302 ]      in the second experiment , we run @xmath0-svm with and without the extra @xmath14-constraint for two different values of @xmath83 , namely for @xmath279 and for @xmath83 depending on @xmath28 as @xmath280 .",
    "we plotted the average error of @xmath281 trials for each value .",
    "the last method used is 1-bit compressed sensing @xcite , which is given as the maximizer of @xmath282 note that maximizer of is independent of @xmath83 , since it is linear in @xmath138 .",
    "-svm with 1-bit cs.,width=302 ]    first , one observes that the error of @xmath0-svm does not converge to zero if the value of @xmath279 is fixed .",
    "this is in a good agreement with theorem [ theo : main_theorem ] and the error estimate .",
    "this drawback disappears when @xmath280 grows with @xmath28 , but @xmath0-svm still performs quite badly .",
    "the two versions of @xmath277-svm perform essentially better than @xmath0-svm , and slightly better than 1-bit compressed sensing .      in figure",
    "[ fig : dep_d ] we investigated the dependency of the error of @xmath29-svm on the dimension @xmath1 .",
    "we fixed the sparsity level @xmath283 and for each @xmath1 between @xmath284 and @xmath285 we draw an @xmath2-sparse signal @xmath27 and measurement vectors @xmath138 at random .",
    "afterwards we run the @xmath29-svm with the three different values @xmath286 with @xmath287 , @xmath288 and @xmath289 .",
    "we plotted the average errors between @xmath27 and @xmath26 for @xmath290 trials .",
    ".,width=302 ]    we indeed see that to achieve the same error , the number of measurements only needs to grow logarithmically in @xmath1 , explaining once again the success of @xmath0-svm for high - dimensional classification problems .",
    "in this paper we have analyzed the performance of @xmath0-svm in recovering sparse classifiers .",
    "theorem [ theo : main_theorem ] shows , that a good approximation of such a sparse classifier can be achieved with small number of learning points @xmath28 if the data is well spread .",
    "the geometric properties of well distributed learning points are modelled by independent gaussian vectors with growing variance @xmath83 and it would be interesting to know , how @xmath0-svm performs on points chosen independently from other distributions .",
    "the number of learning points needs to grow logarithmically with the underlying dimension @xmath1 and linearly with the sparsity of the classifier . on the other hand ,",
    "the optimality of the dependence of @xmath28 on @xmath291 and @xmath83 remains open .",
    "another important question left open is the behavior of @xmath0-svm in the presence of missclasifications , i.e. when there is a ( small ) probability that the signs @xmath5 do not coincide with @xmath292 .",
    "finally , we proposed a modification of @xmath0-svm by incorporating an additional @xmath14-constraint .",
    "we would like to thank a. hinrichs , m. omelka , and r. vershynin for valuable discussions .",
    "a. ai , a. lapanowski , y. plan , and r. vershynin , `` one - bit compressed sensing with non - gaussian measurements '' , _ linear algebra appl .",
    "441 , pp . 222239 , 2014 .",
    "bennett and o.l .",
    "mangasarian , `` robust linear programming discrimination of two linearly independent inseparable sets '' , _ optimization methods and software _ , pp . 2334 , 1992 . h. boche , r. calderbank , g. kutyniok , and j. vybral , `` a survey of compressed sensing '' , applied and numerical harmonic analysis , birkhuser , boston , 2015 .",
    "boufounos and r.g .",
    "baraniuk , `` 1-bit compressive sensing '' , in 42nd annual conference on information sciences and systems , 2008 .",
    "bradley and o.l .",
    "mangasarian , `` feature selection via mathematical programming '' , _ informs j. comput . _ ,",
    "10 , pp . 209217 , 1998 .",
    "bradley and o.l .",
    "mangasarian , `` feature selection via concave minimization and support vector machines '' , in proceedings of the 13th international conference on machine learning , pp . 8290 , 1998 .",
    "e. cands , j. romberg , and t. tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information '' , _ ieee trans .",
    "inform . theory _",
    "489509 , 2006 . c. cortes and v. vapnik , `` support - vector networks '' , _ machine learning _ ,",
    "20 , no.3 , pp .",
    "273297 , 1995 . f. cucker and d. x. zhou , _ learning theory : an approximation theory viewpoint _ , cambridge university press , 2007 .",
    "davenport , m.f .",
    "duarte , y.c .",
    "eldar , and g. kutyniok , _ introduction to compressed sensing _ , in compressed",
    "sensing , cambridge univ .",
    "press , cambridge , pp . 164 , 2012 .",
    "donoho , `` compressed sensing '' , _ ieee trans .",
    "inform . theory _",
    "12891306 , 2006 . m. fornasier and h. rauhut , _ compressive sensing _ , in : _ handbook of mathematical methods in imaging _ , springer , pp .",
    "187228 , 2011 . s. foucart and h. rauhut , _ a mathematical introduction to compressive sensing _",
    ", applied and numerical harmonic analysis , birkhuser , boston , 2013 .",
    "w. hrdle and l. simar , _ applied multivariate statistical analysis _ , springer , berlin , 2003 .",
    "m. hilario and a. kalousis , `` approaches to dimensionality reduction in proteomic biomarker studies '' , _ brief bioinform _",
    ", vol . 9 , no .",
    "2 , pp . 102118 , 2008 . w. hoeffding , `` probability inequalities for sums of bounded random variables '' , _",
    "58 , pp . 1330 , 1963",
    ". k. knudson , r. saab , and r. ward , _ one - bit compressive sensing with norm estimation _ ,",
    "preprint , available at http://arxiv.org/abs/1404.6853 .",
    "mangasarian , `` arbitrary - norm separating plane '' , _ oper .",
    "1523 , 1999 .",
    "mangasarian , `` support vector machine classification via parameterless robust linear programming '' , _ optim .",
    "methods softw .",
    "20 , pp . 115125 , 2005 . m. ledoux , _ the concentration of measure phenomenon _ , am .",
    "m. ledoux and m. talagrand , _ probability in banach spaces : isoperimetry and processes _ , springer , berlin , 1991 .",
    "y. plan , r. vershynin , and e. yudovina , `` high - dimensional estimation with geometric constraints '' , available at http://arxiv.org/abs/1404.3749 .",
    "y. plan and r. vershynin , `` one - bit compressed sensing by linear programming '' , _ comm .",
    "pure appl .",
    "66 , pp . 12751297 , 2013 . y. plan and r. vershynin , `` robust 1-bit compressed sensing and sparse logistic regression : a convex programming approach '' , _ ieee trans .",
    "inform . theory _",
    "59 , pp . 482494 , 2013 . i. steinwart and a. christmann , _ support vector machines _ , springer , berlin , 2008 .",
    "r. tibshirani , `` regression shrinkage and selection via the lasso '' , _ j. royal stat",
    "b _ , vol . 58 , no .",
    "1 , pp . 267288 , 1996 . v. vapnik and a. chervonenkis , `` a note on one class of perceptrons '' , _ automation and remote control _",
    "25 , no . 1 , 1964 . v. vapnik , _ the nature of statistical learning theory _ , springer , berlin , 1995 . v. vapnik , _ statistical learning theory _ , wiley , chichester , 1998 . l. wang , j. zhu , and h. zou , `` hybrid huberized support vector machines for microarray classification and gene selection '' , _ bioinformatics _ , vol .",
    "3 , pp . 412419 , 2008 . h. h. zhang , j. ahn , x. lin , and ch .",
    "park , `` gene selection using support vector machines with non - convex penalty '' , _ bioinformatics _ , vol .",
    "1 , pp . 8895 , 2006 . h. zou and t. hastie , `` regularization and variable selection via the elastic net '' , _ j. r. stat",
    "301320 , 2005 .",
    "h. zou , t. hastie , and r. tibshirani , `` sparse principal component analysis '' , _ j. comput . graph .",
    "_ , vol . 15 , no .",
    "2 , pp . 265286 , 2006 .",
    "j. zhu , s. rosset , t. hastie , and r. tibshirani , `` 1-norm support vector machines '' , in proc .",
    "advances in neural information processing systems , vol .",
    "16 , pp . 4956 , 2004 .",
    "anton kolleck received his m.s . in mathematics at technical university berlin , germany in 2013 , where he now continues as ph.d .",
    "his research concentrates on sparse recovery and compressed sensing and their applications in approximation theory .",
    "jan vybral received his m.s . in mathematics at charles",
    "univeristy , prague , czech republic in 2002 .",
    "he earned the dr .",
    "degree in mathematics at friedrich - schiller university , jena , germany in 2005 .",
    "he had postdoc positions in jena , austrian academy of sciences , austria , and technical university berlin , germany .",
    "he is currently an assistant professor of mathematics at charles university .",
    "his core interests are in functional analysis with applications to sparse recovery and compressed sensing ."
  ],
  "abstract_text": [
    "<S> support vector machines ( svm ) with @xmath0 penalty became a standard tool in analysis of highdimensional classification problems with sparsity constraints in many applications including bioinformatics and signal processing . </S>",
    "<S> although svm have been studied intensively in the literature , this paper has to our knowledge first non - asymptotic results on the performance of @xmath0-svm in identification of sparse classifiers . </S>",
    "<S> we show that a @xmath1-dimensional @xmath2-sparse classification vector can be ( with high probability ) well approximated from only @xmath3 gaussian trials . </S>",
    "<S> the methods used in the proof include concentration of measure and probability in banach spaces .    </S>",
    "<S> support vector machines , compressed sensing , machine learning , regression analysis , signal reconstruction , classification algorithms , functional analysis , random variables </S>"
  ]
}