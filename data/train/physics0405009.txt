{
  "article_text": [
    "identifying causal relations among simultaneously acquired signals is an important problem in computational time series analysis and has applications in economy [ 1 - 2 ] , eeg analysis @xcite , human cardiorespiratory system @xcite , interaction between heart rate and systolic arterial pressure @xcite , and many others .",
    "several papers dealt with this problem relating it to the identification of interdependence in nonlinear dynamical systems @xcite , or to estimates of information rates @xcite .",
    "some approaches modelled data by oscillators and concentrated on the phases of the signals @xcite . one major approach to analyze causality between two time series is to examine if the prediction of one series could be improved by incorporating information of the other , as proposed by granger @xcite in the context of linear regression models of stochastic processes . in particular ,",
    "if the prediction error of the first time series is reduced by including measurements from the second time series in the linear regression model , then the second time series is said to have a causal influence on the first time series . by exchanging roles of the two time series",
    ", one can address the question of causal influence in the opposite direction .",
    "it is worth stressing that , within this definition of causality , flow of time plays a major role in making inference , from time series data , depending on direction .",
    "since granger causality was formulated for linear models , its application to nonlinear systems may not be appropriate .",
    "the question we address in this paper is : how is it possible to extend granger causality definition to nonlinear problems ?    in the next section we review the original approach by granger while describing our point of view about its nonlinear extension ; we also propose a method , exploiting radial basis functions , which fulfills the requirements a prediction scheme should satisfy to analyze causality . in section ( [ exp ] ) we show application of the proposed method to simulated and real examples .",
    "some conclusions are drawn in section ( [ conc ] ) .",
    "we briefly recall the vector autoregressive ( var ) model which is used to define linear granger causality @xcite .",
    "let @xmath0 and @xmath1 be two time series of @xmath2 simultaneously measured quantities . in the following",
    "we will assume that time series are stationary .",
    "for @xmath3 to @xmath4 ( where @xmath5 , @xmath6 being the order of the model ) , we denote @xmath7 , @xmath8 , @xmath9 , @xmath10 and we treat these quantities as @xmath4 realizations of the stochastic variables ( @xmath11 , @xmath12 , @xmath13 , @xmath14 ) . the following model is then considered @xcite : @xmath15 @xmath16 being four @xmath6-dimensional real vectors to be estimated from data .",
    "application of least squares techniques yields the solutions : @xmath17 and @xmath18 where @xmath19 matrices and @xmath20 vectors are the estimates , based on the data set at hand , of the following average values : @xmath21_{\\alpha\\beta}=\\langle x_\\alpha x_\\beta\\rangle & = { 1\\over m}\\sum_{k=1}^m x_\\alpha^k x_\\beta^k & \\alpha,\\beta=1, ...",
    ",m\\\\ \\left[\\sigma_{xy}\\right]_{\\alpha\\beta}=\\langle x_\\alpha y_\\beta\\rangle&={1\\over m}\\sum_{k=1}^m x_\\alpha^k y_\\beta^k & \\alpha,\\beta=1, ... ,m\\\\ \\left[\\sigma_{yx}\\right]_{\\alpha\\beta}=\\langle y_\\alpha x_\\beta\\rangle&={1\\over m}\\sum_{k=1}^m y_\\alpha^k x_\\beta^k & \\alpha,\\beta=1, ...",
    ",m\\\\ \\left[\\sigma_{yy}\\right]_{\\alpha\\beta}=\\langle y_\\alpha y_\\beta\\rangle&={1\\over m}\\sum_{k=1}^m   y_\\alpha^k y_\\beta^k & \\alpha,\\beta=1, ... ,m\\\\ \\left[{\\bf t_{11}}\\right]_{\\alpha}=\\langle x x_\\alpha\\rangle&={1\\over m}\\sum_{k=1}^m   x^k x_\\alpha^k &   \\alpha=1, ...",
    ",m\\\\ \\left[{\\bf t_{12}}\\right]_{\\alpha}=\\langle x y_\\alpha\\rangle&={1\\over m}\\sum_{k=1}^m   x^k y_\\alpha^k & \\alpha=1, ...",
    ",m\\\\ \\left[{\\bf t_{21}}\\right]_{\\alpha}=\\langle y x_\\alpha\\rangle&={1\\over m}\\sum_{k=1}^m   y^k x_\\alpha^k & \\alpha=1, ... ,m\\\\ \\left[{\\bf t_{22}}\\right]_{\\alpha}=\\langle y y_\\alpha\\rangle&={1\\over m}\\sum_{k=1}^m   y^k y_\\alpha^k & \\alpha=1, ...",
    ",m    \\end{array}\\end{aligned}\\ ] ] let us call @xmath22 and @xmath23 the prediction errors of this model , defined as the estimated variances of @xmath24 and @xmath25 respectively .",
    "in particular @xmath26 we also consider autoregressive ( ar ) predictions of the two time series , i.e. the model @xmath27 in this case the least squares approach provides @xmath28 and @xmath29 .",
    "the estimate of the variance of @xmath30 is called @xmath31 ( the prediction error when @xmath11 is predicted solely on the basis of the knowledge of its past values ) ; similarly @xmath32 is the variance of @xmath33 . if the prediction of @xmath11 improves by incorporating the past values of @xmath34 , i.e. @xmath22 is smaller than @xmath31 , then @xmath12 has a causal influence on @xmath11 .",
    "analogously , if @xmath23 is smaller than @xmath32 , then @xmath11 has a causal influence on @xmath12 .",
    "calling @xmath35 and @xmath36 , a directionality index can be introduced : @xmath37 the index @xmath38 varies from @xmath39 in the case of unidirectional influence ( @xmath40 ) to @xmath41 in the opposite case ( @xmath42 ) , with intermediate values corresponding to bidirectional influence . according to this definition of causality , the following property holds for @xmath2 sufficiently large : _",
    "if @xmath43 is uncorrelated with @xmath13 and @xmath11 , then @xmath44_. indeed in this case @xmath45 and @xmath46 , therefore @xmath47 .",
    "this means that var and ar modelling of the @xmath48 time series coincide .",
    "if @xmath13 is uncorrelated with @xmath14 and @xmath12 , then @xmath49_. it is clear that these properties are fundamental and make the linear prediction approach suitable to evaluate causality . on the other hand , for nonlinear systems higher order correlations may be relevant .",
    "therefore , we propose that any prediction scheme providing a nonlinear extension of granger causality should satisfy the following property : ( p1 ) _ if @xmath43 is statistically independent of @xmath13 and @xmath11 , then @xmath44 _ ; _ if @xmath13 is statistically independent of @xmath43 and @xmath12 , then @xmath49_. in a recent paper @xcite , use of a locally linear prediction scheme @xcite has been proposed to evaluate nonlinear causality . in this scheme , the joint dynamics of the two time series",
    "is reconstructed by delay vectors embedded in an euclidean space ; in the delay embedding space a locally linear model is fitted to data .",
    "the approach described in @xcite satisfies property p1 only if the number of points in the neighborhood of each reference point , where linear fit is done , is sufficiently high to establish good statistics ; however linearization is valid only for small neighborhoods .",
    "it follows that this approach to nonlinear causality requires very long time series to satisfy p1 . in order to construct methods working effectively with moderately long time series ,",
    "in the next subsection we will characterize the problem of extending granger causality as the one of finding classes of nonlinear models satisfying property p1 .",
    "what is the most general class of nonlinear models which satisfy p1 ?",
    "the complete answer to this question is matter for further study .",
    "here we only give a partial answer , i.e. the following family of models : @xmath50 where @xmath51 are four @xmath52-dimensional real vectors , @xmath53 are @xmath52 given nonlinear real functions of @xmath6 variables , and @xmath54 are @xmath52 other real functions of @xmath6 variables . given @xmath55 and @xmath56 , model ( [ mod - non ] ) is a linear function in the space of features @xmath57 and @xmath58 ; it depends on @xmath59 variables , the vectors @xmath51 , which must be fixed to minimize the prediction errors @xmath60    we also consider the model : @xmath61 and the corresponding prediction errors @xmath31 and @xmath32 .",
    "now we prove that model ( [ mod - non ] ) satisfies p1 .",
    "let us suppose that @xmath43 is statistically independent of @xmath13 and @xmath11 .",
    "then , for each @xmath62 and for each @xmath63 : @xmath64 is uncorrelated with @xmath11 and with @xmath65 .",
    "it follows that @xmath66=\\mbox{variance}\\left[x-{\\bf w_{11}}\\cdot { \\bf \\phi}\\left({\\bf x}\\right)\\right]+\\mbox{variance}\\left[{\\bf w_{12}}\\cdot { \\bf \\psi}\\left({\\bf y}\\right)\\right].\\ ] ] as a consequence , for large @xmath2 , at the minimum of @xmath22 one has @xmath67 .",
    "the same argument may be used exchanging x and y. this proves that p1 holds .    the solution of least squares fitting of model ( [ mod - non ] ) to data",
    "may be written in the following form : @xmath68    @xmath69    where @xmath70 denotes the pseudo - inverse matrix @xcite ; @xmath71 matrices and @xmath72 vectors are given by : @xmath73_{k \\rho}=\\varphi_\\rho\\left({\\bf",
    "x}^k\\right ) & k=1, ...",
    ",m , \\rho=1, ... ,n\\\\ \\left[{\\bf s_{2}}\\right]_{k \\rho}=\\psi_\\rho({\\bf y}^k ) & k=1, ... ,m , \\rho=1, ...",
    ",n\\\\ \\left[{\\bf t_{1}}\\right]_{k}=x^k & k=1, ... ,m\\\\ \\left[{\\bf t_{2}}\\right]_{k}=y^k & k=1, ... ,m \\end{array}\\end{aligned}\\ ] ] solution of model ( [ mmod ] ) is given by @xmath74 and @xmath75 .",
    "radial basis functions ( rbf ) methods were initially proposed to perform exact interpolation of a set of data points in a multidimensional space ( see , e.g. , @xcite ) ; subsequently an alternative motivation for rbf methods was found within regularization theory @xcite .",
    "rbf models have been used to model financial time series @xcite .    in this subsection",
    "we propose a strategy to choose the functions @xmath55 and @xmath76 , in model ( [ mod - non ] ) , in the frame of rbf methods . fixed @xmath77 , @xmath52 centers @xmath78 , in the space of @xmath13 vectors , are determined by a clustering procedure applied to data @xmath79 .",
    "analogously @xmath52 centers @xmath80 , in the space of @xmath43 vectors , are determined by a clustering procedure applied to data @xmath81 .",
    "we then make the following choice : @xmath82 @xmath83 being a fixed parameter , whose order of magnitude is the average spacing between the centers .",
    "centers @xmath84 are the prototypes of @xmath13 variables , hence @xmath57 functions measure the similarity to these typical patterns .",
    "analogously , @xmath58 functions measure the similarity to typical patterns of @xmath43 .",
    "many clustering algorithm may be applied to find prototypes , for example in our experiments we use fuzzy c - means @xcite .",
    "some remarks are in order .",
    "first , we observe that the models described above may trivially be adapted to handle the case of reconstruction embedding of the two time series in a delay coordinate space , as described in @xcite .",
    "second , we stress that in ( [ mod - non ] ) @xmath11 and @xmath12 are modelled as the sum of two contributions , one depending solely on @xmath13 and the other dependent on @xmath43",
    ". obviously better prediction models for @xmath11 and @xmath12 exists , but they would not be useful to evaluate causality unless they would satisfy p1 .",
    "this requirement poses a limit to the level of detail at which the two time series may be described , if one is looking at causality relationships .",
    "the justification of the model we propose here , based on regularization theory , is sketched in the appendix .      in the previous subsections",
    "the prediction error has been identified as the empirical risk , although there is a difference between these two quantities as statistical learning theory ( slt ) @xcite shows .",
    "the deep connection between empirical risk and generalization error deserves a comment here .",
    "first of all we want to point out that the ultimate goal of a predictor and in general of any supervised machine @xmath85 @xcite is _ to generalize _ , that is to correctly predict the output values @xmath11 corresponding to never seen before input patterns @xmath13 ( for definiteness we consider the case of predicting @xmath11 on the basis of the knowledge of @xmath13 ) . a measure of the generalization error of such a machine @xmath86 is the _ risk _ @xmath87 $ ] defined as the expected value of the loss function @xmath88 : @xmath89 = \\int dx\\ ; d{\\bf x}\\;\\;v\\left ( x , f({\\bf x})\\right ) p(x,{\\bf x } ) , \\label{er1}\\ ] ] where @xmath90 is the probability density function underlying the data . a typical example of loss function is @xmath91 and in this case the function minimizing @xmath87 $ ] is called the _ regression function_. in general @xmath92 is unknown and so we can not minimize the risk .",
    "the only data we have are @xmath4 observations ( examples ) @xmath93 of the random variables @xmath11 and @xmath13 drawn according to @xmath90 .",
    "statistical learning theory @xcite as well as regularization theory @xcite provide upper bounds of the generalization error of a learning machine @xmath86 .",
    "inequalities of the following type may be proven : @xmath89 \\leq   \\epsilon_{x } + { \\cal c } , \\label{ineq}\\ ] ] where @xmath94 is the _ empirical risk _ , that measures the error on the training data .",
    "@xmath95 is a measure of the _ complexity _ of machine @xmath86 and it is related to the so - called vapnik - chervonenkis ( vc ) dimension .",
    "predictors with low complexity guarantee low generalization error because they avoid overfitting to occur . when the complexity of the functional space where our predictor lives is _ small _ , then the empirical risk is a good approximation of the generalization error .",
    "the models we deal with in this work verify such constraint .",
    "in fact , linear predictors have a finite vc - dimension , equal to the size of the space where the input patterns live , and predictors expressed as linear combinations of radial basis functions are smooth . in conclusion empirical risk is a good measure of the generalization error for the predictors we are considering here and so it can be used to construct measures of causality between time series @xcite .",
    "in order to demonstrate the use of the proposed approach , in this section we study two examples , a pair of unidirectionally coupled chaotic maps and a bivariate physiological time series .",
    "let us consider the following pair of noisy logistic maps : @xmath96 @xmath97 and @xmath98 are unit variance gaussianly distributed noise terms ; parameter @xmath99 determines their relevance .",
    "we fix @xmath100 , and @xmath101 $ ] represents the coupling @xmath102 . in the noise - free case ( @xmath103 ) ,",
    "a transition to synchronization ( @xmath104 ) occurs at @xmath105 .",
    "we evaluate the lyapunov exponents by the method described in @xcite : the first exponent is @xmath106 , the second exponent depends on @xmath107 and is depicted in fig . 1 for @xmath108 ( it becomes negative for @xmath109 ) . for several values of @xmath107 , we have considered runs of @xmath110 iterations , after @xmath110 transient , and evaluated the prediction errors by ( [ mod - non ] ) and ( [ mmod ] ) , with @xmath111 , @xmath112 and @xmath113 . in fig .",
    "2a we depict , in the noise free case , the curves representing @xmath114 and @xmath115 versus coupling @xmath107 . in figures 2b , 2c and 2d we depict the directionality index @xmath38 versus @xmath107 , in the noise free case and for @xmath116 and @xmath117 respectively . in the noise free case we find @xmath118 ,",
    "i.e. our method revealed unidirectional influence .",
    "as the noise increases , also the minimum value of @xmath107 , which renders unidirectional coupling detectable , increases .      as a real example , we consider time series of heart rate and breath rate of a sleeping human suffering from sleep apnea ( ten minutes from data set b of the santa fe institute time series contest held in 1991 , available in the physionet data bank @xcite ) .",
    "there is a growing evidence that suggests a causal link between sleep apnea and cardiovascular disease @xcite , although the exact mechanisms that underlie this relationship remain unresolved @xcite .",
    "figure 3 clearly shows that bursts of the patient breath and cyclical fluctuations of heart rate are interdependent .",
    "we fix @xmath111 and @xmath119 ; varying @xmath83 we find that both @xmath120 ( @xmath11 representing heart rate ) and @xmath121 ( @xmath12 representing breath ) have a minimum at @xmath83 close to @xmath122 . in fig .",
    "4 we depict the directionality index @xmath38 vs @xmath83 , around @xmath123 . since we find @xmath38 positive , we may conclude that the causal influence of heart rate on breath is stronger than the reverse @xcite .",
    "this data have been already analyzed in @xcite , measuring the rate of information flow ( transfer entropy ) , and a stronger flow of information from the heart rate to the breath rate was found . in this example ,",
    "the rate of information flow entropy and granger nonlinear causality give consistent results : both these quantities , in the end , measure the departure from the generalized markov property @xcite @xmath124",
    "the components of complex systems in nature rarely display a linear interdependence of their parts : identification of their causal relationships provides important insights on the underlying mechanisms . among the variety of methods which have been proposed to handle this important task ,",
    "a major approach was proposed by granger @xcite .",
    "it is based on the improvement of predictability of one time series due to the knowledge of the second time series : it is appealing for its general applicability , but is restricted to linear models . while extending granger approach to the nonlinear case , on one hand one would like to have the most accurate modelling of the bivariate time series , on the other hand the goal is to quantify how much the knowledge of the other time series counts to reach this accuracy .",
    "our analysis is rooted on the fact that any nonlinear modelling of data , suitable to study causality , should satisfy the property p1 , described in section ( [ granger ] ) .",
    "it is clear that this property sets a limit on the accuracy of the model ; we have proposed a class of nonlinear models which satisfy p1 and constructed an rbf like approach to nonlinear granger causality .",
    "its performances , in a simulated case and a real physiological application , have been presented .",
    "we conclude remarking that use of this definition of nonlinear causality may lead to discover genuine causal structures via data analysis , but to validate the results the analysis has to be accompanied by substantive theory .    0.4 cm    * acknoledgements . *",
    "the authors thank giuseppe nardulli and mario pellicoro for useful discussions about causality .",
    "we show how the choice of functions ( [ eq - rbf ] ) arise in the frame of regularization theory .",
    "let @xmath125 be a function of @xmath13 and @xmath43 .",
    "we assume that @xmath125 is the sum of a term depending solely on @xmath13 and one depending on @xmath43 : @xmath126 .",
    "we also assume the knowledge of the values of @xmath86 and @xmath127 at points @xmath128 : @xmath129 let us denote @xmath130 the fourier transform of @xmath131 .",
    "the following functional is a measure of the smoothness of z(*x*,*y * ) : @xmath132=\\int d\\vec{\\omega}\\;\\;{|\\hat{f}(\\vec{\\omega})|^2+|\\hat{g}(\\vec{\\omega})|^2\\over \\hat{k } ( \\vec{\\omega})}. \\label{app2}\\ ] ] indeed it penalizes functions with relevant contributions from high frequency modes .",
    "variational calculus shows that the function that minimize @xmath133 under the constraints ( [ app1 ] ) is given by : @xmath134 where @xmath135 and @xmath136 are tunable lagrange multipliers to solve ( [ app1 ] ) . hence model ( [ mod - non])-([eq - rbf ] )",
    "corresponds to the class of the smoothest functions , sum of a term depending on @xmath13 and a term depending on @xmath43 , with assigned values on a set of @xmath52 points .",
    "granger , econometrica * 37 * , 424 ( 1969 ) .",
    "ting , physica a * 324 * , 285 ( 2003 ) .",
    "p. tass et al . , phys . rev .",
    "lett . * 81 * , 3291 ( 1998 ) ; m. le van quyen et al .",
    ", brain res . *",
    "792 * , 24 ( 1998 ) ; e. rodriguez et al . ,",
    "nature ( london ) * 397 * , 430 ( 1999 ) . c. ludwig , arch .",
    "* 13 * , 242 ( 1847 ) ; c. schafer et al . , phys",
    "e * 60 * , 857 ( 1999 ) ; m. g. rosemblum et al .",
    "e * 65 * , 41909 ( 2002 ) .",
    "s. akselrod et al . , am .",
    "j. physiol .",
    ". physiol . * 249 * , h867 ( 1985 ) ; g. nollo et al . ,",
    "j. physiol .",
    "physiol . * 283 * , h1200 ( 2002 ) .",
    "s. j. schiff et al . , phys . rev .",
    "e * 56 * , 6708 ( 1996 ) ; j. arnhold et al .",
    ", physica d * 134 * , 419 ( 1999 ) ; r. quian quiroga et al .",
    "e * 61 * , 5142 ( 2000 ) ; r. quian quiroga et al .",
    "e * 65 * , 41903 ( 2002 ) .",
    "t. schreiber , phys .",
    "lett . * 85 * , 461 ( 2000 ) .",
    "m. palus et al . , phys .",
    "e * 63 * , 46211 ( 2001 ) .",
    "f. r. drepper , phys .",
    "e * 62 * , 6376 ( 2000 ) ; m. g. rosemblum et al .",
    "e * 64 * , 45202r ( 2001 ) .",
    "usually both times series are normalized in the preprocessing stage , i.e. they are linearly transformed to have zero mean and unit variance .",
    "y. chen et al .",
    "a * 324 * , 26 ( 2004 ) .",
    "j.d . farmer and j.j .",
    "sidorowich , phys .",
    "* 59 * , 845 ( 1987 ) . c.  r. rao and s.k .",
    "mitra , _ generalized inverse of matrices and its applications _ ( john wiley , new york , 1971 ) . c.  m. bishop ,",
    "_ neural networks for pattern recognition _",
    "( oxford university press , new york , 1995 ) .",
    "t. poggio , f. girosi , science * 247 * , 978 ( 1990 ) . j. hutchinson , _ a radial basis function approach to financial time series analysis _ ,",
    "thesis , massachusetts institute of technology , department of electrical engineering and computer science ( 1994 ) .",
    "j. c. bezdek , _ pattern recognition with fuzzy objective function algorithms _",
    "( plenum press , new york , 1981 ) . v.  vapnik , _ statistical learning theory _",
    "( john wiley & sons , inc . , 1998 ) .",
    "machine means _algorithm which learns from data_ in the machine learning community . in the general case , leave - one - out ( loo ) error",
    "@xmath137 $ ] provides a better estimate of the generalization error of @xmath86 ( luntz and brailovsky theorem ) than the empirical risk , given a finite number of training data .",
    "loo - error is defined as the error variance when the prediction for the k - th pattern is made using the model trained on the m-1 other patterns ; it needs @xmath4 predictors to be trained , where @xmath4 is the cardinality of the data set .",
    "hence , loo - error estimation is unfeasible to compute for large training sets . for linear predictors , like the ones we consider in this paper , the empirical risk is already a good estimate , due to the low complexity of these machines .",
    "h. f. von bremen et al .",
    ", physica d * 101 * , 1 ( 1997 ) .",
    "f. roux et al . , am .",
    "* 108 * , 396 ( 2000 ) .",
    "h. w. duchna et al . , somnologie * 7 * , 101 ( 2003 ) .",
    "these results may also be due to coupling of the two signals to a common external driver ."
  ],
  "abstract_text": [
    "<S> we consider extension of granger causality to nonlinear bivariate time series . in this frame , if the prediction error of the first time series is reduced by including measurements from the second time series , then the second time series is said to have a causal influence on the first one . </S>",
    "<S> not all the nonlinear prediction schemes are suitable to evaluate causality , indeed not all of them allow to quantify how much the knowledge of the other time series counts to improve prediction error . </S>",
    "<S> we present a novel approach with bivariate time series modelled by a generalization of radial basis functions and show its application to a pair of unidirectionally coupled chaotic maps and to a physiological example . </S>"
  ]
}