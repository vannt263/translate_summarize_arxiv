{
  "article_text": [
    "a new technique called ` polarization ' has recently been introduced in @xcite to develop efficient channel coding schemes .",
    "the codes resulting from this technique , called polar codes , have several nice attributes : ( 1 ) they are linear codes generated by a low - complexity deterministic matrix ( 2 ) they can be analyzed mathematically and bounds on the error probability ( exponential in the square root of the block length ) can be _ proved _ ( 3 ) they have a low encoding and decoding complexity ( 4 ) they allow to reach the shannon capacity on any discrete memoryless channels ( dmc ) .",
    "these codes are indeed the first codes with low decoding complexity that are provably capacity achieving on any dmc .",
    "the key result in the development of polar code is the so - called ` polarization phenomenon ' , initially shown in the channel setting in @xcite .",
    "the same phenomenon admits a source setting formulation , as follows .",
    "[ thmari][@xcite ] let @xmath0 $ ] be i.i.d .",
    "bernoulli(@xmath1 ) , @xmath2 be a power of 2 , and @xmath3 , where @xmath4^{\\otimes \\log_2(n)}$ ] . then , for any @xmath5 , @xmath6 : h(y_j | y^{j-1 } ) \\geq 1-{\\varepsilon}\\}|   \\stackrel{n \\to \\infty}{\\longrightarrow }   h(p ) , \\label{polar}\\end{aligned}\\ ] ] where @xmath7 is the entropy of a bernoulli(@xmath1 ) distribution .",
    "note that implies that the proportion of components @xmath8 for which @xmath9 tends to 0 .",
    "hence most of the randomness has been extracted in about @xmath10 components having conditional entropy close to 1 and indexed by @xmath11 : h(y_j |y^{j-1 } ) \\geq 1 -{\\varepsilon}\\ } \\label{defr}\\end{aligned}\\ ] ] and besides @xmath12 fluctuating components , the remaining @xmath13 components have conditional entropy below @xmath14 .",
    "this theorem is extended in @xcite to @xmath0 $ ] being i.i.d .  from an arbitrary distribution @xmath15 on @xmath16 , where @xmath17 is a prime , replacing @xmath7 by @xmath18 ( and using the logarithm in base @xmath17 ) .",
    "it is however mentioned that the theorem may fail when @xmath17 is not a prime but a power of a prime , with a counter - example provided for @xmath19 . in section [ galois ] of this paper ,",
    "we show a generalized version of the polarization phenomenon , i.e. , of theorem [ thmari ] , for powers of primes ( we show it explicitly for powers of 2 , but the same holds for arbitrary primes ) .",
    "also , the formulation of theorem [ thmari ] is slightly more general in @xcite , it includes an auxiliary random variable @xmath20 ( side - information ) , which is a random variable correlated with @xmath21 but not intended to be compressed , and which is introduced in the conditioning of each entropy term .",
    "although this formulation is mathematically close to theorem [ thmari ] , it is more suitable for an application to the slepian - wolf coding problem ( distributed data compression ) , by reducing the problem to single - user source coding problems .",
    "a direct approach for this problem using polar codes is left open for future work in @xcite ; we investigate this here in section [ sw ] .",
    "finally , we also generalize theorem [ thmari ] to a setting allowing dependencies within the source ( non i.i.d .  setting . )",
    "this paper provides a unified treatment of the three problems mentioned above , namely , the compression of multiple correlated sources , non i.i.d .  sources and non binary sources .",
    "the main result of this paper is theorem [ main ] , where a `` matrix polarization '' shows how not only randomness but also dependencies can be extracted using @xmath22 .",
    "some results presented in this paper can be viewed as counter - parts of the results in @xcite for a source rather than channel setting .",
    "reciprocally , some results presented here in the source setting can be extended to a channel setting ( such as channels with memory , or non - prime input alphabets ) .",
    "finally , connections with extractors in computer science and the matrix completion problem in machine learning are discussed in sections [ pexts ] and [ discussion ] .",
    "* @xmath23=\\{1,2,\\dots , n\\}$ ] * for @xmath24 and @xmath25 $ ] , @xmath26=[x_i : i \\in s]$ ] * for @xmath24 , @xmath27 $ ] * @xmath28 \\cup [ 1-{\\varepsilon},1+{\\varepsilon } ] \\cup \\dots",
    "\\cup [ m-{\\varepsilon},m+{\\varepsilon}]$ ] * @xmath29 * for a matrix @xmath30 , the matrix @xmath31 is obtained by taking @xmath32 kronecker products of @xmath30 with itself .",
    "a random variable @xmath33 over @xmath34 is @xmath14-uniform if @xmath35 , and it is @xmath14-deterministic if @xmath36 .",
    "we also say that @xmath33 is @xmath14-deterministic given @xmath37 if @xmath38 .",
    "[ main ] ( 1 ) let @xmath2 be a power of 2 and @xmath21 be an @xmath39 random matrix with i.i.d",
    ".  columns of arbitrary distribution @xmath15 on @xmath40 .",
    "let @xmath3 where @xmath41^{\\otimes \\log_2(n)}$ ] .",
    "then , for any @xmath42 , there exist two disjoint subsets of indices @xmath43 \\times [ n]$ ] with @xmath44 \\times [ n ] \\setminus ( r_{\\varepsilon}\\cup d_{\\varepsilon})|=o(n)$ ] such that the subset of entries @xmath45 $ ] is @xmath14-uniform and @xmath46 $ ] is @xmath14-deterministic given @xmath47 $ ] .",
    "( hence @xmath48 , @xmath49 . )",
    "\\(2 ) moreover , the computation of @xmath20 as well as the reconstruction of @xmath21 from the non - deterministic entries of @xmath20 can be done in @xmath50 , with an error probability of @xmath51 , @xmath52 , using the algorithm ` polar - matrix - dec ` .",
    "* remarks .",
    "*    * the multiplication @xmath53 is over @xmath54 * the sets @xmath55 depend on the distribution @xmath15 ( and on the dimensions @xmath56 and @xmath2 ) , but not on the realization of @xmath20 .",
    "these sets can be accurately computed in linear time ( cf .",
    "section [ discussion ] ) . * to achieve an error probability of @xmath51 , one picks @xmath57 , for @xmath58 .    the following lemma provides a characterization of the dependencies in the columns of @xmath20 , it is proved in section [ proofs1 ] .",
    "recall that @xmath59 denotes the @xmath8-th column of @xmath20 , @xmath60 the @xmath61-entry of @xmath20 , @xmath62=[y_j(i ) : i \\in s]$ ] and @xmath63 $ ] .",
    "[ mainlemma ] for any @xmath42 , we have , @xmath64 :   h(y_j[s]|y^{j-1 } ) \\in \\{0,1,\\dots,|s|\\ } \\pm { \\varepsilon } , \\forall s \\subseteq [ m ] \\}| \\\\ \\to 1 \\end{aligned}\\ ] ]    this lemma implies the first part of theorem [ main ] , as shown in next section .",
    "the second part of the theorem is proved in section [ proofs2 ] , together with the following result , which further characterizes the dependency structure of @xmath20 .",
    "[ null ] for any @xmath42 and @xmath65 $ ] , let @xmath66 denote the binary matrix of maximal rank such that @xmath67 note that @xmath66 can have zero rank , i.e. , @xmath66 can be a matrix filled with zeros .",
    "we then have , @xmath68 moreover , the result still holds when @xmath69 , for @xmath58 .",
    "note that , if @xmath70 , @xmath71 is @xmath14-deterministic given @xmath72 , and if @xmath66 has rank @xmath73 , by freezing @xmath74 components in @xmath59 appropriately , say on @xmath75 , we have that @xmath71 can be reduced to a full rank matrix multiplication @xmath76 $ ] , and hence @xmath77 $ ] is @xmath14-deterministic given @xmath72 and @xmath78 $ ] . hence the number of bits to freeze , is exactly @xmath79 , and as stated in the lemma , this corresponds to the total entropy of y ( up to a @xmath12 ) .",
    "let @xmath42 and let @xmath82 be the set of indices @xmath83 $ ] for which @xmath84|y^{j-1 } ) \\in \\{0,1,\\dots,|s|\\ } \\pm { \\varepsilon}$ ] , for any @xmath85.$ ] from lemma [ mainlemma ] , @xmath86 .",
    "note that for @xmath87 , there exists a minimal set ( not necessarily unique ) @xmath88 such that @xmath89|y^{j-1 } ) \\geq h(y_j |y^{j-1 } ) - { \\varepsilon}\\label{max}\\end{aligned}\\ ] ] which also implies @xmath89|y^{j-1 } ) \\geq |t_j| - { \\varepsilon}\\label{max2},\\end{aligned}\\ ] ] and , by the chain rule and defining @xmath90 , @xmath91|y^{j-1 } y_j[s_j^c ] ) \\leq { \\varepsilon}. \\label{corr}\\end{aligned}\\ ] ] ( note that if @xmath92 , we define @xmath93 so that @xmath94 $ ] . ) we then have @xmath95| ( \\cup_{j \\in e_n}y_j[s_j])^c ) \\\\",
    "& \\leq   \\sum_{j \\in e_n } h(y_j[s_j]| y^{j-1}y_j[s_j^c ]   ) \\leq   { \\varepsilon}n\\end{aligned}\\ ] ] and @xmath96 $ ] is @xmath14-deterministic given @xmath97)^c$ ] , so that @xmath98 .",
    "moreover , we have @xmath99 )   \\geq \\sum_{j \\in e_n } h(y_j[t_j]|y^{j-1 } ] ) \\notag \\\\ & \\geq \\sum_{j \\in e_n } h(y_j |y^{j-1 } ) -{\\varepsilon}n \\notag \\\\ & \\geq \\sum_{j=1}^n h(y_j |y^{j-1 } ) -{\\varepsilon}n - o(n)\\notag \\\\ & = h(y ) -{\\varepsilon}n - o(n ) , \\label{same}\\end{aligned}\\ ] ] where the third inequality uses , and from , @xmath100 ) & \\geq \\sum_{j \\in e_n } |t_j| -{\\varepsilon}n.\\end{aligned}\\ ] ] since @xmath101 , we have @xmath102 and @xmath103 $ ] is @xmath104-uniform , so that @xmath105 .      `",
    "polar - matrix - dec ` + inputs : @xmath106 \\times [ n]$ ] , @xmath107 \\in { \\mathbb{f}}_2^{|d^c|}$ ] .",
    "+ output : @xmath108 .",
    "+ algorithm : + 0 .",
    "let @xmath109 ; + 1 . find the smallest @xmath8 such that @xmath110 is not empty ;",
    "compute @xmath111=\\arg\\max_{u \\in { \\mathbb{f}}_2^{|s_j| } } { \\mathbb{p}}\\ { y[s_j]= u | y^{j-1}=y^{j-1 } , y[s_j^c]=y[s_j^c ] \\};\\ ] ] 2 .",
    "update @xmath112 , @xmath113=y[m ] \\cup \\hat{y}[s_j]$ ] ; + 3 . if @xmath114 is empty output @xmath115 , otherwise go back to 1 .",
    "note that , using for the definition of @xmath116 ( and the corresponding @xmath81 ) , the realizations of @xmath72 and @xmath117 $ ] are known , and with high probability one guesses @xmath118 $ ] correctly in step 1 , because of .",
    "moreover , due to the kronecker structure of @xmath22 , and similarly to @xcite , step 1 .  and",
    "the entire algorithm require only @xmath50 computations .",
    "finally , from the proof of theorem [ main ] part ( 2 ) , it results that step 1 .",
    "can also be performed slightly differently , by finding sequentially the inputs @xmath119 $ ] for @xmath120 , reducing an optimization over all possible @xmath121 , where @xmath122 can be as large as @xmath56 , to only @xmath56 optimizations over @xmath54 ( which may be useful for large @xmath56 ) .",
    "we present now three direct applications of theorem [ main ] :    * distributed data compression , i.e. , slepian - wolf coding * compression of sources on arbitrary finite fields * compression of non i.i.d .",
    "sources      in @xcite , the two - user slepian - wolf coding problem is approached via polar codes by reducing the problem to single - user source coding problems .",
    "a direct approach is left open for future work ; we investigated this here , for arbitrary many users .",
    "consider @xmath56 binary sources which are correlated with an arbitrary distribution @xmath15 .",
    "we are interested in compressing an i.i.d .",
    "output of these sources .",
    "that is , let @xmath123 be i.i.d .  under @xmath15 on @xmath40 ,",
    "i.e. , @xmath124 is an @xmath56 dimensional binary random vector and , for example , @xmath125,\\dots , x_n[i]$ ] is the sources output for user @xmath126 .",
    "if we are encoding these sources together , a rate @xmath18 is sufficient ( and it is the lowest achievable rate ) . in @xcite , slepian and wolf showed that",
    ", even if the encoders are not able to cooperate , lossless compression can still be achieved at rate @xmath18 .",
    "we now present how to use theorem [ main ] to achieve this rate with a polar coding scheme .",
    "_ polar codes for distributed data compression : _ + 1 . for a given @xmath2 and @xmath14 ( which sets the error probability ) , since each user knows the joint distribution @xmath15 , each user can compute the `` chart '' of the deterministic indices , i.e. , the set @xmath127 \\times [ n]$ ] and identify its own chart @xmath128 .",
    "each user computes @xmath129 and stores @xmath130 $ ] , so that the joint decoder is in possession of @xmath131 $ ] , and can run ` polar - dec - matrix ` with @xmath131 $ ] as input to get @xmath20 , with an error probability at most @xmath132 . since @xmath22 is invertible , indeed @xmath133 , one can then find @xmath134 .    from theorem [ main ]",
    ", we have the following result .",
    "[ swc][distributed polar compression ] for @xmath56 correlated sources of joint distribution @xmath15 , previously described scheme allows to perform lossless and distributed compression of the sources at sum - rate @xmath18 , with an error probability of @xmath135 , @xmath52 , and an encoding and decoding complexity of @xmath50 .    note",
    "that this result allows to achieve the sum - rate of the slepian - wolf region , i.e. , a rate belonging to the dominant face of the slepian - wolf achievable rate region , it does not say that any rate in that region can be reached with the proposed scheme .      in @xcite ,",
    "the source polarization result is stated for sources that are i.i.d .  and @xmath17-ary , where @xmath17 is prime .",
    "it is also mentioned that if @xmath17 is not prime , the theorem may fail .",
    "in particular , an example for @xmath19 is provided where the conclusion of theorem [ thmari ] does not hold .",
    "it is also mentioned that if additional randomness is introduced in the construction of the polar transformation ( leading no longer to a deterministic matrix @xmath22 ) , the result holds for arbitrary powers of primes .",
    "we show here that a generalized polarization phenomenon still holds for arbitrary powers of primes ( we formally show it for powers of 2 only but any prime would work ) even for the deterministic polar transform @xmath22 .",
    "[ galoisc][polarization for finite fields ] let @xmath0 $ ] be i.i.d .  under @xmath15 on @xmath16 where @xmath136 , and let @xmath3 ( computed over @xmath16 ) .",
    "then , although @xmath20 may no polarize over @xmath137 , it polarizes over @xmath40 in the sense of theorem [ main ] , more precisely : define by @xmath138 a @xmath40 representation of @xmath137 , @xmath139 the distribution on @xmath40 induced by @xmath15 on @xmath137 , and set @xmath140 ( organized as an @xmath39 matrix ) .",
    "then the conclusions of theorem [ main ] hold for @xmath141 .",
    "note : this theorem still holds when @xmath17 is a power of any prime , by combining it with the result in @xcite for prime alphabets .",
    "the case where @xmath136 is particularly interesting for complexity considerations ( cf .",
    "section [ discussion ] ) .",
    "_ interpretation of corollary [ galoisc ] : _ when @xmath17 is a prime , @xmath142 , which means that @xmath59 is either roughly uniform and independent of the past or roughly a deterministic function of the past .",
    "however , for @xmath17 being a power of 2 ( or a power of a prime ) , we only get that @xmath143 , and previous conclusion can not be drawn , stressing indeed a different polarization phenomenon .",
    "however , corollary [ galoisc ] says that if we work with the vector representation of the elements in @xmath144 , we still have a ` polarization phenomenon ' in the sense of theorem [ main ] , i.e. , for almost all @xmath65 $ ] , a subset of the components of the @xmath145 are either roughly uniform and independent or deterministic functions of the past and the complementary components .",
    "_ compression of @xmath146-ary i.i.d .",
    "sources : _ for a given @xmath0 $ ] , compute @xmath3 and transform @xmath20 into @xmath141 based on the representation of @xmath137 by @xmath40 .",
    "organize @xmath141 to be an @xmath39 matrix .",
    "note one can equivalently map @xmath21 into @xmath147 and then take @xmath22 to get @xmath141 .",
    "this is due to the fact that the @xmath137 addition corresponds to the pointwise addition in @xmath40 .",
    "finally , store @xmath141 on @xmath148 , and run ` polar - matrix - dec ` to recover @xmath141 , hence @xmath20 and @xmath21 .",
    "let a binary source consist of i.i.d .",
    "blocks of length @xmath56 , each block having an arbitrary distribution @xmath15 .",
    "we can then compress the source as follows . from @xmath2 blocks",
    "@xmath123 each of length @xmath56 , i.e. , @xmath149 outputs of the source , create the matrix @xmath150 $ ] and apply the polar transform to get @xmath3 . then store the components of @xmath20 which belong to @xmath151 .",
    "to reconstruct @xmath21 , reconstruct @xmath20 from @xmath152 $ ] using ` polar - matrix - dec ` and find @xmath134 .",
    "if the source is not exactly block i.i.d .",
    "but is mixing , i.e. , if @xmath153 , for any @xmath154 , we can open windows of length @xmath155 between the blocks and store without compression these @xmath155 inter - block bits , which does not increase the compression rate .",
    "we are then left with a source formed by blocks which are ` almost ' i.i.d .  and",
    "a similar procedure can be used .    from theorem [ main ] , we have the following .",
    "[ memoryc]for a binary source consisting of i.i.d .",
    "blocks of length @xmath56 , each block having distribution @xmath15 , the polar coding scheme described previously allows to compress losslessly the source at rate @xmath18 , with an error probability of @xmath51 , @xmath52 , and an encoding and decoding complexity of @xmath50 .",
    "as discussed previously , a similar result holds for source which are mixing .",
    "we have discussed in this paper a procedure to extract randomness , i.e. , uniform bits , from non uniform bits .",
    "the applications we considered are in compression and coding , but there are also numerous applications of randomness extraction problems in computer science . in particular , there is a notion of `` extractor '' in theoretical computer science , which aims at extracting uniform bits from sources having much more general assumptions than the one considered here .    phrased in our terminology , an extractor is roughly a map that extracts @xmath56 bits that are @xmath14-uniform from @xmath2 bits that have a total entropy at least @xmath32 , with the help of a seed of @xmath156 uniform bits . for more details and a survey on extractors",
    "see for example @xcite .",
    "the notion of @xmath14-uniform , or @xmath14-close to uniform , used in computer science is usually measured by the @xmath157-norm , rather than the entropy as used in this paper .",
    "nevertheless , these two notions can be related and this is a minor distinction . also , the entropy used in the computer science literature is the min - entropy rather than the shannon - entropy , which is a stronger assumption , since the shannon - entropy is an upper bound to the min - entropy . on the other hand , the source for the extractor definition",
    "is only assumed to have min - entropy @xmath32 , and no further assumptions are made on the distribution of @xmath123 , whereas in our setting , we consider sources that are at least ergodic and with a known distribution .",
    "one should also stress that we did not make use of any seed in our problems .    in order to establish a more concrete connection between polar coding and formal extractors , we present here a result which takes into account one of the two caveat just mentioned : we only assume that the source has entropy at least @xmath32 , without requiring the exact knowledge of the distribution , but we keep an i.i.d .  setting . using section [ memory ]",
    ", one can generalize this result to a setting where the source is mixing , but in even then we do not make use of any seed .",
    "in particular , if one could use a seed , ideally of small size , e.g. , @xmath158 , to turn an arbitrary source of lower - bounded entropy , into a mixing source of comparable entropy , one could use the following result to construct real extractors ( work in progress ) .",
    "let @xmath159-@xmath160 be the matrix obtained by deleting the columns of @xmath22 that are not in @xmath161 , where @xmath162 is one of the two binary distribution having entropy @xmath163 ( and @xmath164 as defined in ) .",
    "note that @xmath165 benefits from the low encoding complexity of @xmath22 , namely @xmath50 .",
    "[ pext ] let @xmath2 be a power of two and @xmath0 $ ] be i.i.d .",
    "bernoulli such that @xmath166 ( where @xmath167 denotes the shannon or min - entropy ) . for any @xmath5 ,",
    "@xmath168 is @xmath14-uniform ( in the @xmath169 or entropy sense ) and @xmath170    this result is proved in section [ proofext ] , and using section [ memory ] it can be extended to a setting where the source is mixing .",
    "note that even in a mixing setting , the source entropy is @xmath171 , which is indeed a regime where good extractors are known @xcite .",
    "we have treated in this paper three problems , namely , compression of correlated sources , sources with memory and sources on finite fields , with a unified approach using a matrix polarization ( theorem [ main ] ) , and we provided polar coding schemes for each of these problems .",
    "the advantage of using polar coding schemes is that these schemes have low encoding and decoding complexity , and achieve the optimal performance ( shannon limit ) meanwhile affording mathematical guarantees on the performance , as described in corollaries [ swc ] , [ galoisc ] and [ memoryc ] .",
    "one can now also combine these different problems .",
    "namely , for multiple sources that are define on some finite fields , with some well - behaved correlations between and within themselves , one can , using the interleaving trick and the vector representation described respectively in sections [ memory ] and [ galois ] , organize the sources outputs in a matrix form so as to meet the hypotheses of theorem [ main ] , and hence have a polar compression scheme requiring the minimal compression rate .",
    "one can also translate the results in this paper to a channel setting , such as @xmath56-user multiple access channels ( already treated in @xcite ) , channels with memory or channels with non binary fields inputs , by using duality arguments .",
    "although the results in this paper are expected to hold when @xmath172 , one has to be careful with the complexity scaling when @xmath56 gets large . in that regard",
    ", an advantage of using finite fields of cardinality @xmath136 rather than modular fields of prime cardinality , is that some operations required in the polar decoding algorithm are convolution - like operations over the underlying field , and as the fft algorithm allows to reduce the computational cost of a convolution from @xmath173 to @xmath174 when @xmath17 is a power of 2 , one can benefit from this fact .",
    "we have assumed in this paper that the sets @xmath175 and @xmath176 can be computed , without discussing how .",
    "the first reason why we do not stress this aspect here , as in other papers in polar coding , is that these sets do not depend on the realization of the source(s ) .",
    "namely , if one is able to compute these sets once for several values of interest of @xmath14 and of the dimensions , one can then use the same sets for any outputs realizations .",
    "this is fundamentally different than the decoding algorithm which takes the source realization as an input .",
    "yet , it is still crucial to be able to compute these sets once , for the parameters of interests . in order to do so",
    ", there are at least two possible approaches .",
    "the first one is via simulations , and is discussed in @xcite : using the kronecker structure of @xmath22 , it is possible to run simulations and get accurate estimate of the conditional entropies @xmath177 , in particular ( from section [ proof1 ] ) of the sets @xmath175 and @xmath176 .",
    "another option is to use algorithms to approach the exact values of @xmath177 within a given precision , in linear time ; this has been proposed in particular in @xcite .",
    "it would also be interesting to have mathematical characterizations of these sets . at the moment , this is an open problem , even for the simplest settings ( single binary i.i.d .",
    "source , or in the channel setting , the binary erasure channel ) .",
    "finally , this work could also apply to the matrix completion setting .",
    "for example , if @xmath21 is an @xmath39 matrix where column @xmath178 contains the ratings of @xmath56 movies by user @xmath8 , we can use theorem [ main ] to show that by applying the matrix that are not in @xmath81 ] @xmath180 to @xmath21 , we are left with fewer entries ( the more correlations between the movie ratings the fewer entries ) that yet allow to recover the initial matrix . hence ,",
    "if we are given only a smaller set of appropriate entries ( and which sets can be characterized using section [ proof1 ] ) , we can reconstruct the initial matrix using ` polar - matrix - dec ` .",
    "for a random vector @xmath138 distributed over @xmath40 , define @xmath181 and @xmath182 , where @xmath183 is an i.i.d .",
    "copy of @xmath138 .",
    "let @xmath184 be i.i.d .  binary random variables in @xmath185 with uniform probability distribution , and",
    "let @xmath186=h(v^{b_1 \\dots b_k}[s]| v^{c_1 \\dots c_k } , \\forall ( c_1 \\dots c_k ) < ( b_1 \\dots b_k))\\end{aligned}\\ ] ] for @xmath85 $ ] , where the order between @xmath187-sequences is the lexicographic order ( with @xmath188 ) .            for @xmath197 , we have @xmath198   ) & =   h(x_1 [ s ] x_2[s ] ] ) \\notag \\\\ & = h(y_1 [ s ] y_2[s ] ) \\notag \\\\ & = h(y_1 [ s ]   ) + h(y_2 [ s ]   | y_1[s ]   ) \\notag \\\\ & \\geq h(y_1 [ s ] ) + h(y_2 [ s ]   | y_1 ) \\label{last }   \\ ] ] with equality in the if @xmath196 $ ] . for @xmath199 ,",
    "the same expansion holds including in the conditioning the appropriate `` past '' random variables .    note that because @xmath191 $ ] is a martingale for @xmath196 $ ] , the sum - rate @xmath18 is conserved through the polarization process .",
    "now , using previous lemma and the fact that @xmath191 \\in [ 0,|s|]$ ] for any @xmath200 , the martingale convergence theorem implies the following .",
    "[ invar ] for any @xmath42 , @xmath21 valued in @xmath201 , @xmath33 arbitrary , @xmath202 an i.i.d .",
    "copy of @xmath203 , @xmath85 $ ] , there exists @xmath204 such that @xmath205 | z ' ) - h(x ' [ s]| z , z',x[s]+x'[s ] ) \\leq \\delta$ ] implies @xmath206 | z')-h(x'[s \\setminus i ] | z ' )   \\in \\{0,1 \\ }",
    "\\pm { \\varepsilon}$ ] for any @xmath207 .",
    "we have @xmath208 | z ' ) - h(x ' [ s]| z , z',x[s]+x'[s ] ) \\notag \\\\ & = i(x'[s ] ; x[s]+x'[s]| z , z ' ) \\notag \\\\ & \\geq i(x'[s ] ; x[i]+x'[i]| z , z ' ) \\notag \\\\ & \\geq i(x'[i ] ; x[i]+x'[i]| z , z ' , x'[s \\setminus i ] ) \\notag \\\\ & = h(x'[i]| z ' , x'[s \\setminus i ] ) - h(x[i]+x'[i]| z , z ' , x'[s \\setminus i ] ) .",
    "\\label{squiz}\\end{aligned}\\ ] ] it is shown in @xcite that if @xmath209 are binary random variables and @xmath210 are arbitrary such that @xmath211 , for some conditional probability @xmath212 , then , for any @xmath213 , there exists @xmath214 such that @xmath215 implies @xmath216 . using this result",
    ", we can pick @xmath217 small enough to lower bound and show that @xmath218| z ' , x'[s \\setminus i ] ) \\in \\{0,1\\ } \\pm { \\varepsilon}$ ] . from the chain rule , we conclude that @xmath219| z ' ) \\in \\{0,1\\ } \\pm { \\varepsilon}$ ] .            in order to prove theorem [ main ] part ( 2 ) ,",
    "we basically need to show that part ( 1 ) still holds when taking @xmath14 scaling like @xmath221 for @xmath222 , as in @xcite .",
    "we did not find a direct way to show that when @xmath191 $ ] converges to @xmath223 , it must do it that fast ( the sub - martingale characterization is too week to apply results of @xcite directly ) .",
    "this is why we looked into lemma [ null ] . by developing a correspondence between previous results and analogue results dealing with linear forms of the @xmath224",
    "$ ] s , we are able to use the speed convergence results shown for the single - user setting and conclude .",
    "this approach was developed in @xcite for the multiple access channel , below is the counter - part for our source setting .",
    "[ tech1 ] for a random vector @xmath20 valued in @xmath40 , and an arbitrary random vector @xmath33 , if @xmath225| z ) \\in \\{0,1,\\dots,|s|\\ } \\pm { \\varepsilon}\\ ] ] for any @xmath85 $ ] , we have @xmath226|z ) \\in \\{0,1\\ } \\pm \\delta({\\varepsilon}),\\ ] ] with @xmath227",
    ".    this lemma is proved in @xcite .",
    "using this result , we have that for @xmath228 , there exists a matrix @xmath66 of rank @xmath229 , such that @xmath230 this implies the first part of lemma [ null ] , and we now show how we can use this other characterization of the dependencies in @xmath20 to conclude a speed convergence result .",
    "we first need the following `` single - user '' result .",
    "we define the auxiliary family of random processes @xmath234 $ ] , for @xmath85 $ ] , by @xmath235=z(\\sum_{i \\in s}v^{b_1 \\dots b_k}[i]| v^{c_1 \\dots c_k } , \\forall ( c_1 \\dots c_k ) < ( b_1 \\dots b_k))\\end{aligned}\\ ] ] where , for a binary uniform random variable @xmath30 and an arbitrary random variable @xmath236 , @xmath237 is the bhattacharyya parameter .",
    "note that @xmath238 ( this also follows from proposition 2 in @xcite . ) we then have , using the chain rule and source polarization inequalities on the bhattacharyya parameter , namely proposition 1 in @xcite , that @xmath239 \\leq \\zeta_{k}[s]^2 \\text { if } b_{k+1}=1,\\\\ & \\zeta_{k+1}[s ] \\leq 2 \\zeta_k[s ]   \\text { if } b_{k+1}=0 , \\ ] ] and using theorem 3 of @xcite , we conclude that for any @xmath58 @xmath240 finally , we conclude using .",
    "we then use lemma [ tech1 ] and [ tech2 ] to conclude that @xmath233 :   & h(y_j [ s]| y^{j-1 } ) \\in \\{0,1,\\dots,|s|\\ }",
    "\\pm { \\varepsilon } , \\forall s \\subseteq [ m ] , \\notag \\\\ & \\exists a_j \\text { with } { \\mathrm{rank}}(a_j)= \\mathrm{int } ( m - h(y_j | y^{j-1})),\\notag \\\\ & h(a_jy_j |y^{j-1 } )",
    "< { \\varepsilon}_n   \\ } | \\to 1 , \\label{set}\\end{aligned}\\ ] ] which implies lemma [ null ] . to conclude the proof of theorem [ main ] part ( 2 ) , let @xmath221 and @xmath241 be the set defined through ( which , in view of previous results , is equivalent to the definition given in section [ proof1 ] ) .",
    "we then have for @xmath228 that the components @xmath116 to be decoded in @xmath59 are not correctly decoded with probability @xmath242 and the block error probability is bounded as @xmath243 so that taking @xmath222 large enough , we can reach a block error probability of @xmath244 for any @xmath52 .      for @xmath245 , @xmath246 where @xmath247 and @xmath248 where @xmath249 is i.i.d .",
    "under @xmath162 . moreover , for any distribution @xmath1 on @xmath54 such that @xmath250 , there exists a distribution @xmath251 on @xmath54 such that @xmath252 , where @xmath253 denotes the circular convolution .",
    "equivalently , there exists @xmath254 independent of @xmath255 , such that @xmath256 .",
    "define @xmath257 , @xmath258 and @xmath259 , hence @xmath260 .",
    "we have @xmath261 where the last equality follows from the fact that @xmath262 is independent of @xmath37 since @xmath263 is independent of @xmath33 .",
    "therefore , for any @xmath263 i.i.d .  such that @xmath264 and for any @xmath265 , we have @xmath266 and @xmath267 )   & \\geq \\sum_{j \\in r_{\\tilde{{\\varepsilon}}}(p(k ) ) } h(y_j(p)|y^{j-1}(p ) ] ) \\notag \\\\ & \\geq   |r_{\\tilde{{\\varepsilon}}}(p(k))| ( 1-\\tilde{{\\varepsilon } } ) .\\notag \\end{aligned}\\ ] ] hence , defining by @xmath268 the distribution of @xmath269 $ ] and @xmath270 the uniform distribution on @xmath271 , we have @xmath272 using pinsker inequality and , we obtain @xmath273 finally , we have from theorem [ thmari ] latexmath:[\\[\\begin{aligned }"
  ],
  "abstract_text": [
    "<S> the basic polarization phenomenon for i.i.d .  </S>",
    "<S> sources is extended to a framework allowing dependencies within and between multiple sources . </S>",
    "<S> in particular , it is shown that taking the polar transform of a random matrix with i.i.d .  </S>",
    "<S> columns of arbitrary ( correlated ) distribution allows to extract the randomness and dependencies . </S>",
    "<S> this result is the used to develop polar coding schemes ( having low complexity ) for : ( 1 ) distributed data compression , i.e. , slepian - wolf coding ( without decomposing the problem into single - user problems ) , ( 2 ) compression of sources with memory , ( 3 ) compression of sources on finite fields , extending the polarization phenomenon for alphabets of prime cardinality to powers of primes . </S>"
  ]
}