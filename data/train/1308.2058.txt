{
  "article_text": [
    "ad - hoc analytics of genomic data is popular in domains such as computational biology and bioinformatics .",
    "typically , an analytical job comprises software scripts written by biologists or bioinformaticists in high - level programming languages , such as r @xcite , along with large amounts of data that needs to be processed .",
    "r - based analytics in computational biology or bioinformatics is gaining popularity and is supported through 671 software packages provided by bioconductor @xcite .",
    "analytical jobs which may require a few hours or perhaps even a few days may ingest large amounts of data and subsequently also produce data in large volumes .",
    "not only is analytics inherently computationally intensive , but also data intensive .",
    "high - performance computing systems have therefore become attractive for executing large - scale analytical jobs @xcite .",
    "traditional high - performance computing systems such as clusters and supercomputers offer a good platform to perform large - scale analytics .",
    "however , it is required of the computational biologist and bioinformaticist , who has excellent programming and statistical skills , to also have extensive knowledge of the high - performance computing hardware . moreover , the costs required for investing in large - scale systems and their maintenance is high .",
    "the cloud has become an appealing high - performance computing platform for ad - hoc analytics since it offers on - demand computing and storage resources , along with scalability and low maintenance costs @xcite .",
    "this has led to a variety of research for supporting computational biology and bioinformatics related jobs on the cloud ( for example , genome sequencing @xcite , genome informatics @xcite , comparative genomics @xcite , proteomics @xcite and biomedical computing @xcite ) .",
    "software projects such as elasticr @xcite , dare @xcite and azureblast @xcite support applications on the cloud , all of which require the user to have extensive knowledge of the cloud dashboard to be able to port an existing analytical workload onto the cloud .",
    "the options provided by such projects for a fully configurable cloud cluster can fit well with the skill set of a cloud developer , thereby narrowing their wide usage . the major challenge in the research of developing software similar to the ones above for computational biology and bioinformatics ( for example , cloudblast @xcite , galaxycloudman @xcite , simplex @xcite and crossbow @xcite ) is to seamlessly execute an analytical job on the cloud in a manner similar to how the job would be executed on the personal workstation .",
    "however , the use of such software adds an additional layer of complexity for managing the software on top of executing the job .",
    "further , adapting the above projects to execute workloads developed using the r programming language is cumbersome , specific adaptations being required in many cases .",
    "a similar challenge exists for executing the increasing number of analytical workloads that are developed using the r with bioconductor packages @xcite .",
    "the current bioconductor based solution @xcite is based on manually configuring the cloud dashboard for every job that needs to be executed .",
    "software developed to support r and bioconductor , for example , myrna @xcite , contrail @xcite and @xcite are restricted to specific applications in computational biology and bioinformatics .",
    "these challenges can be overcome by the development of a generic framework that can support r - based jobs supported by bioconductor packages , and their execution and management on the cloud .",
    "the research reported in this paper aims to address the above challenges .",
    "a light - weight framework , ` rbiocloud ' , for supporting r - based analytical applications which use bioconductor software packages and need to be executed on the cloud is presented .",
    "domain scientists have to often spend a lot of time dealing with the complex details of configuring the cloud . using rbiocloud ,",
    "an analytical job can be executed on the cloud with minimal effort using a set of five commands from a personal workstation .",
    "the need for any extensive knowledge of the cloud dashboard is minimised .",
    "the contributions of rbiocloud research is a framework ( i ) for handling a diverse range of bioconductor based analytical jobs on the cloud , ( ii ) for abstracting the complexities of cloud set up and configuration , ( iii ) for computational biologist and bioinformaticists to easily access and use the cloud , thereby saving time , and ( iv ) with seemingly minimal difference between a domain scientists workstation though remote resources are accessed .",
    "the feasibility of rbiocloud is validated by using three test cases employing bioconductor packages for executing r - based scripts on the cloud . in the first test case",
    ", genome searching is performed on a single cloud instance , in the second test case , differentially expressed genes are detected on a single cloud instance , and in the third test case , normalisation of microrna ( mirna ) microarray data is performed on a cluster in the cloud .    the remainder of this paper is organised as follows .",
    "section [ design ] considers the design of the rbiocloud framework .",
    "section [ tools ] describes the command line tools offered by rbiocloud for managing and executing an analytical job .",
    "section [ feasibilitystudy ] presents three test cases to validate the feasibility of rbiocloud .",
    "section [ conclusions ] concludes this paper by considering future work .",
    "figure [ figure1 ] , shows the design of the rbiocloud framework which is located on a host site for accessing and managing cloud resources . the host site represents the workstation of a computational biologist or a bioinformaticist who makes use of the cloud infrastructure to execute a job .",
    "the amazon cloud infrastructure is employed in this research .",
    "rbiocloud is designed so that the job can be executed from the host site using the following five step sequence ( refer figure [ figure2 ] ) :    * _ step 1 : _ gather resources - initialise cloud compute and storage resources from the host . *",
    "_ step 2 : _ submit job - send the analytical job from the host onto cloud resources . * _ step 3 : _ execute job - execute the scripts within the job on the resources . * _ step 4 : _ retrieve results - get results generated on the cloud resources onto host . * _ step 5 : _ terminate resources - release all resources which were initialised on the cloud .",
    "rbiocloud is developed using the python programming language and is supported by a number of interfaces .",
    "the compute and storage resources are provided by the amazon web services ( aws ) .",
    "all resources are available on - demand and are paid for on the basis of their usage .",
    "the computational resources are offered through elastic compute cloud ( ec2 ) and are available as instances .",
    "the storage resources are referred to as the elastic block storage ( ebs ) provide persistent data storage .",
    "two python interfaces , namely boto provides the interface to access the resources provided by aws and fabric facilitates remote administration of the cloud resources .",
    "amazon instances are initialized using amazon machine images ( ami ) .",
    "the rbiocloud framework is built on the bioconductor cloud ami @xcite and supports the r programming language along with bioconductor packages .",
    "the cloud is attractive for large analytical jobs as parallel computations incorporated within jobs can be exploited on the cloud .",
    "the simple network of workstations ( snow ) ] interface is employed for parallel execution of jobs on the cloud .",
    "the five command line tools offered by rbiocloud to support gathering of cloud resources , to submit and execute a job , retrieve results from the cloud and terminate resources are presented in this section .      `",
    "rbc_gatherresource ` provisions configuring an instance or multiple instances and a cluster on the cloud .",
    "the syntax of the command is    1 .",
    "` rbc_gatherresource [ -h ] [ -v ] [ -rname resource_name ] [ -rsize resource_size ] [ -ebsvol ebs_volume | -snap ebs_snap ] [ -type instance_type ] [ -desc resource_description ] `    the optional arguments are : ( a ) ` rname ` to name a resource ( instance or cluster ) that is created , ( b ) ` rsize ` , to specify the size of the resource ( if size is one , then only one instance is created , else if size is greater than one , then the instances are configured as a cluster ) , ( c ) ` ebsvol ` and ` snap ` , which are not specified at the same time . ` ebsvol ` specifies the ebs volume i d when an ebs volume is created . ` snap ` specifies the ebs snapshot i d from which an ebs volume can be created . `",
    "ebsvol ` can be specified when an ebs volume is available , however , if ` snap ` is specified then a new ebs volume is created from the snapshot specified .",
    "if both arguments are not provided , then a default snapshot from a configuration file is used , ( d ) ` type ` , which defines the amazon ec2 instance type based on the computational requirements of the task , and ( e ) ` desc ` , which can be used to provide a description for a resource .",
    "a job comprises the script that needs to be executed and the data required by the script both of which need to be submitted to the cloud .",
    "the ` rsync ' protocol is used to submit the job .",
    "one advantage of using rsync is that subsequent data transfers are quickly synchronised between the host and the cloud .",
    "the submission of a job is facilitated using ` rbc_submitjob ` and the syntax is    1 .   `",
    "rbc_submitjob [ -h ] [ -v ] [ -rname resource_name [ -toallnodes | -tomaster ] ] [ -jobdir job_directory][-data ] `    the optional arguments are : ( a ) ` rname ` to specify the resource to which the job needs to be submitted .",
    "if a resource is not specified then the default resource from rbiocloud s configuration file is employed , ( b ) ` jobdir ` to specify the job directory at the host . if the job directory is not specified then the current working directory at the host site is considered to be the source job directory .",
    "the destination job directory is not provided since in the current setup the host job directory is synchronised to the home directory of the root user on the cloud .",
    "the job directory comprises a set of r scripts , a set of data files required by the scripts and a sub - directory that will contain results after the execution of the script .",
    "the optional switch ` -tomaster ` ( default ) submits the job to the master node of a cluster , while ` -toallnodes ` submits the job to all nodes of a cluster .",
    "the ` -data ` switch synchronises any folder not adhering to the structure of the job directory on to the resource .      `",
    "rbc_executejob ` executes a job on the cloud resource .",
    "this command locks the resource onto the job and is only available for any additional use after the job has completed .",
    "the syntax of the command is    1 .   `",
    "rbc_executejob [ -h ] [ -v ] [ -rname resource_name ] [ -jobdir job_directory ] [ -rscript r_script ] [ -runname run_name ] `    the optional arguments of are : ( a ) ` rname ` to specify the resource on which the job needs to be executed , ( b ) ` jobdir ` indicates the job directory at the host site ; the job with the same name from the corresponding job directory on the cloud is executed , and ( d ) ` rscript ` to indicate the r script to be executed . if ` rscript ` is not provided then the user is prompted to select from a list of r scripts that are available in the job directory .",
    "the mandatory argument ` runname ` specifies the name of a run to distinguish multiple executions of a particular job .      `",
    "rbc_getresults ` retrieves results from the cloud resource onto the host and the syntax is    1 .",
    "` rbc_getresults [ -h ] [ -v ] [ -rname resource_name [ -frommaster | -fromall ] ] [ -jobdir job_directory ] [ -runname run_name ] `    the optional arguments are : ( a ) ` rname ` to specify the resource from where the results need to be retrieved , ( b ) ` jobdir ` to indicate the location of the source job directory at the host site ; the results are fetched from the corresponding job directory on the cloud . if no job directory is specified then the current working directory at the host site is used .",
    "the mandatory argument ` runname ` indicates the name of a run that was specified during execution and whose results need to be gathered .",
    "this argument can be used when the same r script has been executed a number of times and each execution had to be differentiated . within the job directory",
    "the results are generated in a sub - directory .",
    "there are two scenarios of generating results on a cluster . in the first scenario ,",
    "the master instance aggregates results from all worker instances and stores them on the master instance , and retrieval from the master instance is possible using ` -frommaster ` . in the second scenario ,",
    "the results are generated on all instances , and retrieving results is possible using ` -fromall ` .",
    "after the completion of a job , the resources on the cloud need to be safely released to avoid billing of unused resources . `",
    "rbc_terminateresource ` facilitates this and the syntax is    1 .   `",
    "rbc_terminateresource [ -h ] [ -v ] [ -rname resource_name ] [ -deletevol ] `    the optional arguments are : ( a ) ` rname ` to specify the resource that needs to be terminated .",
    "the optional switch ` -deletevol ` deletes the ebs volume attached to the resource being terminated .",
    "all the above commands can be used with two switches ; firstly , ` -h ` to provide a description of the use and arguments of the command , and secondly , ` -v ` to provide provides the version of the installation .",
    "popular biological jobs include searching , analysing and normalising data @xcite . in this section three test cases that represent such biological jobs",
    "are selected to demonstrate the feasibility of rbiocloud for bioconductor and r based jobs .",
    "firstly , genome searching , secondly , detecting differential expression of genes and thirdly , normalisation of microrna ( mirna ) microarray data are presented . in the first and second test cases a single amazon ec2 instance is used while in the third test case a cluster of amazon ec2 instances are employed .",
    "the first test case is based on the ` bsgenome ` software package @xcite available from bioconductor , and the script executed is ` genomesearching.r ` which performs efficient genome searching with biostrings and bsgenome data packages .",
    "the r script loads ` bsgenome.celegans.ucsc.ce2 ` , which is the ce2 genome for chromosome i of caenorhabditis elegans @xcite .",
    "the script finds an arbitrary nucleotide pattern in a chromosome and in an entire genome . for executing the script using rbiocloud , the job is organised into one directory , for example ` bsgenome ` , which contains the ` genomesearching.r ` script and all associated data . `",
    "bsgenome ` also needs to contain two additional directories ` results ` and ` runresults ` ( a similar directory structure needs to be followed for executing any job using rbiocloud ) . all the results that need to be generated by the script need to be directed to ` results ` . `",
    "runresults ` is not submitted onto the cloud but remains on the host site to retrieve and store results of each individual run .",
    "the following sequence of five commands will execute ` genomesearching.r ` on the cloud and fetch the results onto the host site :    * ` rbc_gatherresource -rname bsgenome_instance -rsize 1 -desc ",
    "for_genome _ searching ` * ` rbc_submitjob -rname bsgenome_instance ` * ` rbc_executejob -rname bsgenome_instance -rscript genomesearching.r -runname run1_on_bsgenome_instance ` * ` rbc_getresults -rname bsgenome_instance -runname run1_on_bsgenome_instance ` * ` rbc_terminateresource -rname bsgenome_instance -deletevol `    when the first command of the sequence is executed one ec2 instance is initialised using the bioconductor ami , and tagged as ` bsgenome_instance ` .",
    "if optional arguments such as type of instance and ebs volume are not provided then the default values which are defined in the rbiocloud configuration file are chosen ; the default values can be edited .",
    "the ` bsgenome ` folder is synchronised with ` bsgenome_instance ` when the second command is executed ; bsgenome is the current working directory from which the ` rbc_submitjob ` is executed .",
    "the script , ` genomesearching.r ` from ` bsgenome ` directory is executed on ` bsgenome_instance ` with a run name , ` run1_on_bsgenome_instance ` , when the third command is executed .",
    "the results from ` run1_on_bsgenome_instance ` are retrieved on to the host ` results ` directory when the fourth command is executed .",
    "the amazon resource ` bsgenome_instance ` is terminated using the fifth command .",
    "the multiple execution of the ` rbc_gatherresource ` command facilitates the creation of multiple instances , and multiple instances can not have the same name .",
    "the job is to find nucleotide patterns in an entire genome using two methods and produce their result in two seperate files .",
    "the input is a dictionary , containing 50 patterns , each of which is a short nucleotide sequence of 15 to 25 bases . in the first method ,",
    "the forward and reverse strands of seven caenorhabditis elegans chromosomes named as chri , chrii , chriii , chriv , chrv , chrx , chrm are the target .",
    "the result obtained is in a tabulated form in ` ce2dict0_ana1.txt ` providing the name of the chromosome where the hit occurs , two integers giving the starting and ending positions of the hit , an indication of the hit either in the forward or reverse strand , and unique identification for every pattern in the dictionary .",
    "a sample of the output in ` ce2dict0_ana1.txt ` is shown in figure [ figure3 ] ( left ) .    in the second method , a function",
    "which is approximately one hundred times faster is employed .",
    "one limitation of the function is that it works only when all dna patterns searched for have a constant number of nucleotide bases .",
    "therefore , the nucleotide patterns in the dictionary are truncated to a constant length of 15 .",
    "the output of this method is also tabulated in the second result file ` ce2dict0cw15_ana2.txt ` in a similar way to the first method .",
    "a sample of the output is shown in figure [ figure3 ] ( right ) .    2    ....          seqname start     end       strand   patternid          chri     5942496   5942511   -        pattern17          chri     6298363   6298377   +        pattern19          chri     12760564 12760587 -        pattern21          chri     3953136   3953150   +        pattern23          chri     11568996 11569018 -        pattern27          chri     753618    753641    +        pattern37          ...            seqname start    end      strand   patternid          ...          chri     13745040         13745054         +   pattern04          chri     14075187         14075201         +   pattern04          chri     11745177         11745191         +   pattern08          chri     8981081 8981095 +        pattern11                chri     12188778         12188792         +   pattern16          chri     12233665         12233679         +   pattern16          ...       ....      the second test case is based on the ` logitt ` software package @xcite available from bioconductor .",
    "the script executed is ` logitt.r ` which is a statistical method based on the logit - t algorithm for identifying differentially expressed genes using probe - level data .",
    "the input to the script is the ` spikein95 ` data set of the ` spikeinsubset ` library @xcite .",
    "this data set is a subset of the human genome u95 data set containing a series of genes spiked - in at known concentrations and arrayed in a latin square format .",
    "the logit - t algorithm requires limited pre - processing before the actual statistical analysis and produces better results @xcite compared to competing approaches such the regression modelling approach @xcite , the mixture model approach @xcite and the significance analysis of microarrays ( sam ) @xcite .    for executing the script using rbiocloud , the job is organised into one directory , for example ` logitt ` , which contains the ` logitt.r ` script , all associated data and the ` results ` and ` runresults ` directories",
    "the following sequence of five commands will execute ` logitt.r ` on the cloud and fetch the results onto the host site :    * ` rbc_gatherresource -rname logitt_instance -rsize 1 -desc  for_detecting_differentially_express - ed_genes ` * ` rbc_submitjob -rname logitt_instance ` * ` rbc_executejob -rname logitt_instance -rscript logitt.r -runname run1_on_logitt_instance ` * ` rbc_getresults -rname logitt_instance -runname run1_on_logitt_instance ` * ` rbc_terminateresource -rname logitt_instance -deletevol `    when the ` logitt.r ` script is executed on the ` logitt_instance ` , firstly , probe level intensities are normalised using the logit - log transformation",
    ". then the normalised probe level intensities are standardised using z - transformation .",
    "student s t - tests are then performed for every perfect match ( pm ) probe in a probe set .",
    "the median t - statistic for the probe set defines logit - t .",
    "the p - values of all the probe sets are calculated and probe sets with p - values less than 0.01 marks the detection of differentially expressed genes .",
    "the output of the algorithm is as follows :    ....      \" 1024_at \"   \" 1708_at \"   \" 32660_at \" \" 36202_at \"       \" 36311_at \" \" 38734_at \" ....      the third workflow is based on the ` lvsmirna ` software package @xcite available from bioconductor .",
    "the script executed is ` lvsmirna.r ` which normalises microrna ( mirna ) microarray data .",
    "the least - variant set ( lvs ) normalisation method @xcite is employed in the package and the input is the mirna expression data @xcite provided as ` comparison_ar - ray.txt ` .",
    "the script then identifies a subset of mirnas with the smallest array - to - array variation , using the ` estvc ` function .",
    "the first result obtained from the script is an ra - plot , which is a scatter plot ( refer figure [ figure5 - 1 ] ) with logarithmic scales showing the array effect versus standard deviation .",
    "the second result obtained from the script is a box plot ( refer figure [ figure5 - 2 ] ) of data after normalisation .",
    "the ` estvc ` function can benefit from using parallel computation for achieving higher speed up over sequential computation , and can take a cluster object as an argument . here amazon clusters can come to play , and will need to be manually configured using the amazon dashboard as shown in @xcite and @xcite .",
    "employing rbiocloud will be easier as the user can configure this as a single parameter in the ` rbc_gatherresource ` command .    to execute the ` lvsmirna.r ` script on an amazon cluster ,",
    "the script and the input data needs to be provided in a directory , for example ` lvsmirna ` , and the directory also needs to contain two additional sub - directories ` results ` and ` runresults ` .",
    "the two graphs generated by the script needs to be directed to ` results ` . `",
    "runresults ` is not submitted onto the cloud but remains on the host site to store results of every individual run .",
    "the following sequence of five commands will execute ` lvsmirna.r ` on a cloud cluster and fetch the results onto the host site :    * ` rbc_gatherresource -rname lvsmirna_cluster -rsize 8 -desc ",
    "for_lvs_mirna ` * ` rbc_submitjob -rname lvsmirna_cluster ` * ` rbc_executejob -rname lvsmirna_cluster -rscript lvsmirna.r -runname run2_on_lvsmirna_cluster ` * ` rbc_getresults -rname lvsmirna_cluster -runname run2_on_lvsmirna_cluster -frommaster ` * ` rbc_terminateresource -rname lvsmirna_cluster -deletevol `    a cluster with eight ec2 instances is initialised using the bioconductor ami , and tagged as ` lvsmirna_cluster ` when the first command is executed .",
    "should the optional arguments such as type of instance and ebs volume be not provided then the default values which are defined in a configuration file are chosen .",
    "the ` lvsmirna ` folder is synchronised on ` lvsmirna_cluster ` when the second command is executed ; ` lvsmirna ` is the current working directory .",
    "the script , ` lvsmirna.r ` from ` lvsmirna ` is executed on ` lvsmirna_cluster ` with a run name , ` run2_on_lvsmirna_cluster ` when the third command is executed .",
    "the resultant graphs from ` run2_on_lvsmirna_cluster ` run is retrieved on to the host ` results ` directory when the fourth command is executed .",
    "the amazon resource ` lvsmirna_cluster ` is terminated using the fifth command .",
    "figure [ figure6 ] shows a graph for the time taken to move data related to the job in and out of the cloud .",
    "the amazon resources used for test case 1 and test case 2 are one m1.xlarge instance and for test case 3 is a cluster of six m1.xlarge instances .",
    "there is an increase in the time taken for initialising and terminating the cluster over the time taken for initialising and terminating one instance .",
    "therefore , alternative techniques will need to be considered for initialising and terminating resources in parallel .",
    "this can contribute to the reduction of the overall time taken by rbiocloud .",
    "the time taken to submit the job is proportional to the size of the script and the input data being submitted .",
    "large data sets required by the three test cases are available on the amazon instances employed in this research ( a custom built amazon machine image ( ami ) based on the bioconductor ami is used in this research ) .",
    "genome searching takes 79 seconds and the detection of differential expression of genes takes 41 seconds to complete execution .",
    "the potential for parallelism in these jobs and scaling the job across multiple instances need to be explored to achieve speed up .",
    "the third test case exploits parallelism and executes on a cluster of six instances taking 18 seconds for completing the job .",
    "again the time for retrieving results is proportional to the size of the files produced as results .",
    "the second test case takes the least time for retrieval since it produces a small output .",
    "additional test cases to confirm the feasibility of rbiocloud were performed using a large number of scripts provided by over 150 bioconductor packages .",
    "for example , the following were performed using rbiocloud on a large - memory amazon ec2 instance :    * estimation of false discovery rate ( fdr ) using significance analysis of microarrays ( sam ) @xcite and the empirical bayes analyses of microarrays ( ebam ) @xcite provided as ` siggenes.r ` available from the ` siggenes ` package @xcite , * joint deregulation analysis ( joda ) for quantifying changes due to regulation of genes between two distinct cell populations provided as ` jodavignette.r ` available from the ` joda ` package @xcite , and * analysing chip - seq data including the detection of protein - bound genomic regions provided as ` csar.r ` available from the ` csar ` package @xcite .",
    "one observation from the test cases is that the full advantage of the cloud is exploited when jobs harness the potential of parallelism .",
    "the results obtained from the additional test cases are beyond the scope of this paper and will be reported elsewhere .",
    "gathering and managing vast cloud resources in the computational biology or bioinformatics setting for executing an analytical job can be cumbersome .",
    "this is not because cloud resources are nt readily accessible , but the pipeline for executing an analytical job on the cloud requires extensive knowledge of the cloud . while high - performance computer architects may be able to design and deploy such workflows for production based applications it may not be easily possible for biologists with limited high - performance computing skills to perform ad hoc analytics . to allow analytical jobs to fully benefit from the cloud there needs to be a framework that can seamlessly adapt analytical jobs located on a host site for execution on the cloud , provide minimal difference between a personal desktop and the cloud , and offer data and resource management easily on the cloud .    in this paper , such a framework , `",
    "rbiocloud ' , which is light - weight and easily deployable has been designed and developed to support analytical jobs comprising r scripts which employ bioconductor packages .",
    "the framework is deployed between a host site and the cloud , and a set of five command line tools are offered for analytical workflows to facilitate gathering resources , submitting a job , executing a job , retrieving results , and terminating resources .",
    "the research contributions of rbiocloud has been a framework to ( i ) seamlessly handle a diverse range of analytical job on the cloud , ( ii ) abstract the complexities of cloud set up and configuration , ( iii ) easily access and manage cloud resources , thereby saving time of domain scientists , and ( iv ) remotely access cloud resources from a workstation with seemingly minimal differences .",
    "test cases using bioconductor and r - based jobs demonstrate the feasibility of rbiocloud .",
    "three test cases have been employed to validate the feasibility of rbiocloud . in the first test case ,",
    "genome searching , and in the second test case , detection of differential expression of genes were both performed on a single amazon ec2 instance . in the second test case , normalisation of microrna ( mirna ) microarray data",
    "was performed using a cluster of amazon ec2 instances .",
    "the framework is available for download from ` http://www.rbiocloud.com ` .",
    "future efforts will be made towards extending rbiocloud for dynamic and automated management of compute and storage resources on the cloud and submission and execution of multiple jobs .",
    "on top of on - demand instances which are available for fixed price the cost effective solution of bidding for spare instances will be explored .",
    "r. gentleman , v. carey , d. bates , b. bolstad , m. dettling , s. dudoit , b. ellis , l. gautier , y. ge , j. gentry , k. hornik , t. hothorn , w. huber , s. iacus , r. irizarry , f. leisch , c. li , m. maechler , a. rossini , g. sawitzki , c. smith , g. smyth , l. tierney , j. yang and j. zhang , `` bioconductor : open software development for computational biology and bioinformatics , '' genome biology 5(10 ) : r80 , 2004 .",
    "a. rosenthal , p. mork , m. h. li , j. stanford , d. koester and p. reynolds , `` cloud computing : a new business paradigm for biomedical information sharing , '' journal of biomedical informatics 43 : 342353 , 2009 .",
    "b. d. halligan , j. f. geiger , a. k. vallejos , a. s. greene and s. n. twigger , `` low cost , scalable proteomics data analysis using amazon s cloud computing services and open source search algorithms , '' journal of proteome research 8(6 ) : 3148 - 3153 , 2009 .",
    "k. chine , `` scientific computing environments in the age of virtualization , toward a universal platform for the cloud , '' proceedings of the ieee international workshop on opensource software for scientific computation , 44 - 48 , 2009 .",
    "j. kim , s. maddineni and s. jha , `` building gateways for life - science applications using the dynamic application runtime environment ( dare ) framework , '' proceedings of the 2011 teragrid conference : extreme digital discovery , 2011 .",
    "w. lu , j. jackson and r. barga , `` azureblast : a case study of developing science applications on the cloud , '' proceedings of the 19th acm international symposium on high performance distributed computing , 413 - 420 , 2010 .",
    "a. matsunaga , m. tsugawa and j. fortes j , `` cloudblast : combining mapreduce and virtualization on distributed resources for bioinformatics applications , '' proceedings of the 4th ieee international conference on escience , 222 - 229 , 2008 .",
    "m. fischer , r. snajder , s. pabinger , a. dander , a. schossig , j. zschocke , z. tranjanoski and g. stocker , `` simplex : cloud - enable pipeline for the comprehensive analysis of exome sequencing data , '' plos one 7(8 ) , 2012 .",
    "jnomics website : http://sourceforge.net/apps/mediawiki/jnomics/index.\\php?title=jnomics .",
    "[ last accessed : 25 july 2013 ] r. stevens , c. goble , p. baker and a. brass , `` a classification of tasks in bioinformatics , '' bioinformatics 17(2 ) : 180 - 188 , 2001 .",
    "a. g. fraser , r. s. kamath , p. zipperlen , m. martinez - campos , m. sohrmann and j. ahringer , `` functional genomic analysis of c. elegans chromosome i by systematic rna interference , '' nature 408 : 325 - 330 , 2000 .",
    "j. g. thomas , j. m. olson , s. j. tapscott and l. p. zhao , `` an efficient and robust statistical modelling approach to discover differentially expressed genes using genomic expression profiles , '' genome research 11(7 ) : 1227 - 1236 , 2001 .",
    "h. willenbrock , j. salomon , r. sokilde , k. b. barken , t. n. hansen , f. c. nielsen , s. moller and t. litman , `` quanitative mirna expression analysis : comparing microarrays with next - generation sequencing , '' rna 15(11 ) : 2028 - 2034 , 2009 .",
    "j. m. muino , k. kaufmann , r. c. h. j. van ham , g. c. angenent and p. krajewski ,  chip - seq analysis in r ( csar ) : an r package for the statistical detection of protein - bound genomic regions , plant methods , 7:11 , 2011 ."
  ],
  "abstract_text": [
    "<S> large - scale ad hoc analytics of genomic data is popular using the r - programming language supported by 671 software packages provided by bioconductor . </S>",
    "<S> more recently , analytical jobs are benefitting from on - demand computing and storage , their scalability and their low maintenance cost , all of which are offered by the cloud . while biologists and bioinformaticists can take an analytical job and execute it on their personal workstations , it remains challenging to seamlessly execute the job on the cloud infrastructure without extensive knowledge of the cloud dashboard . </S>",
    "<S> how analytical jobs can not only with minimum effort be executed on the cloud , but also how both the resources and data required by the job can be managed is explored in this paper . </S>",
    "<S> an open - source light - weight framework for executing r - scripts using bioconductor packages , referred to as ` rbiocloud ' , is designed and developed . </S>",
    "<S> rbiocloud offers a set of simple command - line tools for managing the cloud resources , the data and the execution of the job . </S>",
    "<S> three biological test cases validate the feasibility of rbiocloud . </S>",
    "<S> the framework is publicly available from ` http://www.rbiocloud.com ` .    cloud computing , r programming , bioconductor , amazon web services , data analytics </S>"
  ]
}