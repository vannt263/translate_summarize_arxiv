{
  "article_text": [
    "this paper extends current functional data registration methods to encompass a broader family of registration models .",
    "traditional registration methods are designed to eliminate all phase variability in a set of functions so that amplitude variability in the registered functions can be described by one primary functional direction .",
    "a simple example can be found in figure [ fig : urex ] . here , each unregistered function , @xmath0 , in the center plot can be expressed as @xmath1 where @xmath2 is the primary direction of variation in the registered functions . after registration",
    ", these functions only exhibit amplitude variability in the direction of @xmath2 as seen in the illustration on the right . for a thorough discussion on the history of and current methods in functional registration , see @xcite .",
    "we build on this traditional concept of functional registration , by considering unregistered functions for which eliminating phase variability in these functions results in registered functions that vary in two primary functional directions which we will denote @xmath2 and @xmath3 . allowing two primary functional directions of variation in the registered functions",
    "extends the use of functional registration to functional data sets such as that found in figure [ fig : urex2 ] . here",
    "the composition of some of the registered functions includes a negative scaling of the second factor which confounds traditional approaches to registration . considering these factors separately in the registration process",
    "is essential to eliminate phase variability in these functions .",
    "the registration model presented here is an extension of our previous work in traditional functional registration in the framework of a bayesian hierarchical model , @xcite . in our previous work",
    ", we demonstrate this approach to functional registration not only allows for flexible modeling assumptions , but also results in estimates of registered functions that are similar to those determined by the best registration procedures currently available . here we will extend this model to not only register functions with multiple directions of variation after registration , but also to perform factor analysis . for these models ,",
    "approximate inference can be performed with an adapted variational bayes algorithm that significantly reduces the computational time needed for initial estimates .",
    "using these estimates are to initialize an mcmc sampling scheme eliminates the need for a burn - in period .",
    "appendix b provides the details of this algorithm for the registration and factor analysis model",
    ". a complete discussion of the adapted variational bayes ( avb ) algorithm can be found in @xcite where we also compare avb estimates to those obtained by mcmc sampling for several data sets .",
    "there is no previous work that combines registration and factor analysis ; however , in @xcite , the authors also consider registration where the aligned functions are assumed to contain variation in more than one functional direction . in their paper ,",
    "kneip and ramsay register functions using an iterative algorithm that updates the pca decomposition used to register functions in each iteration .",
    "this model can be seen as an extension of the procrustes method for traditional functional registration , @xcite and @xcite .",
    "@xcite demonstrates how our initial registration model improves upon the procrustes method .",
    "the basic organization of this paper is as follows .",
    "we present our model for registration and factor analysis in section [ sec : methfa ] . in section [ sec : comp ]",
    "we compare our model for functional alignment to one of the best traditional registration methods using two simulated data sets . in this section",
    ", we also show how functions can be grouped according to their estimated weights on each of the two factors . in section [ sec : jug ]",
    ", we apply this model to a juggling data set .",
    "finally , a discussion can be found in section [ sec : conc ] .    [ cols=\"^,^ \" , ]     the juggling data consist of three different functional data sets obtained by recording the finger position of dr .",
    "michael newton ( biostatistics , university of wisconsin ) as he juggles .",
    "these data were collected in collaboration with dr .",
    "james ramsay ( psychology , mcgill university ) , dr .",
    "david ostry ( psychology , mcgill university ) , and dr .",
    "paul gribble ( psychology , university of western ontario ) and can be downloaded from * http://mbi.osu.edu/programs/current_topic_workshop/ * under the link for the 2012 workshop , _ ctw : statistics of time warpings and phase variations_. as dr .",
    "newton juggled , the following were recorded : 1 ) the horizontal position of the right forefinger in the frontal plane , 2 ) the horizontal position of the right forefinger in the sagittal plane , and 3 ) the vertical position of the right forefinger .",
    "for this data analysis , the first functional data set of the horizontal position of the right forefinger in the frontal plane is used to demonstrate functional data registration and grouping using our gaussian process model .",
    "additional information on this data set can be found in @xcite .",
    "* description of the juggling data * for this analysis , our observations consist of individual cycles . in each cycle , the right hand cycles smoothly oscillates from left to right as the ball is caught and released .",
    "each functional observation begins at the apex of each cycle that corresponds to the x - coordinate of the juggler s right forefinger immediately after releasing the ball with his right hand . from here , each function takes a sharp dip as the juggler s hand moves to the left to catch the next ball .",
    "variation in the x - coordinate of these cycles correspond to the adjustments made by the juggler after the initial movement to the left to account for differences between where the ball actually descends and where the juggler anticipates it to be . of approximately 100 cycles available , we randomly selected 25 to use for this analysis .",
    "all cycles are considered over a common time domain ranging from 0 to 675 milliseconds where the original data are recorded in 5 millisecond intervals . thinning the data",
    "does not significantly alter its shape , and the final data contains 28 records per functional observation ( cycle ) taken every 25 milliseconds .    the goal of this analysis is two - fold . the first aim is to align the prominent features in these 25 cycles in conjunction with estimating the two primary factors of which these data are composed .",
    "secondly , using the estimated weights , @xmath4 and @xmath5 , classify these functions into groups of functions that share similar features .",
    "figure [ fig : jug ] contains plots of the unregistered functions , the functions registered by f - r , the functions registered by gp and the first two estimated primary directions of variation in these functions . here again , based on the _ sls _ criterion , the gp model provides a better function alignment than f - r .",
    "the estimated registered functions are split into four groups based on the estimated weights for each function in each of the primary directions of variation . since all estimated weights on the first factor were positive , we centered these weights to delineate functions with large weights on the first factor and those with smaller weights on the first factor .",
    "in contrast , the variation in the estimated weights on the second factor could be described by whether this weight was positive or negative .",
    "four groups were determined by the magnitude of the estimated weight on the first factor and the sign of the estimated weight on the second factor .",
    "this is equivalent to grouping by the quadrant in which the centered weight on the first factor and the unadjusted weight on the second factor , @xmath6 , lays .",
    "figure [ fig : clustjug ] contains the resulting four groups of functions . in cycles with large weights on the first factor , found in the top two plots , the peak found in these functions between 200 and 300 milliseconds",
    "corresponds to the juggler overcompensating for moving his hand too far to the left to catch the ball by making a sharp movement to the right .",
    "this is then followed by another adjustment to the left . a positive weight on the second factor in the first group corresponds to cycles where the juggler needs to make another significant adjustment in his hand position between 300 and 500 milliseconds .",
    "this not only results in another significant peak in these functions , but also corresponds to the juggler releasing the ball with his hand further to the left when compared to the previous cycle .",
    "this can be seen in these functions having a smaller x - coordinate at the end of the cycle than at the beginning of the cycle .",
    "differences in the x - coordinate when the ball is released between the previous and current cycle are much less prominent in group 2 .",
    "groups 3 and 4 contain cycles with smaller estimated weights on the first factor .",
    "this is reflected in more subtle adjustments in hand position in the horizontal plane between 200 and 300 milliseconds where these functions stay relatively flat .",
    "again , as seen in groups 1 and 2 , here we see that the sign of the weight on the second factor delineates between cycles where there is a distinct change in the x - coordinate at the time the ball is released between the previous and current cycle and when there was not .",
    "group 3 corresponds to cycles where the ball is released further to the left in comparison to the release point for the previous cycle while those in group 4 contain cycles with similar release points to that of the previous cycle .",
    "this example illustrates how our model for registration and factor analysis uncovers functional differences and similarities that can not be detected as effectively using traditional methods for registering functional data .",
    "furthermore , since the weights on each factor for each function are additional unknowns in our model , we can quantify how certain we are a function belongs to a particular group by looking at the variability in the posterior sample of these weights .",
    "in this paper , we have proposed a bayesian hierarchical model for functional registration and factor analysis .",
    "this model reduces phase variability in functions that when registered have significant variation in more than one primary functional direction .",
    "we have shown for these types of functions , our registration model outperforms one of the best registration algorithms available .",
    "furthermore , in addition to performing functional registration , with our model two primary directions of variation are estimated for the registered functions .",
    "each registered function is primarily composed of a weighted combination of these factors , and by classifying the estimated weights on these factors , functions can easily be grouped .    for this analysis , a metropolis within gibbs sampler",
    "is used to obtain mcmc samples from the joint posterior distribution of all parameters . in general ,",
    "mcmc sampling is inefficient for high - dimensional models .",
    "however , in this particular model , reasonable estimates can be obtained by utilizing an adapted form of variational bayes that significantly reduces computational costs .",
    "if mcmc sampling is preferred , more efficient sampling schemes are available for use . in particular",
    ", @xcite suggests that population mcmc can be employed to allow both global and local movement throughout the parameter space for a more efficient sampler and could be applied here .",
    "initial estimates for an mcmc sampler should be obtained using avb for optimal performance .",
    "this work was possible through the flexibility of prior assumptions in a bayesian hierarchical model .",
    "we have shown for functional data these models can encompass a multitude of inferential procedures including latent function estimation , functional linear regression , functional registration , and functional registration with factor analysis ( @xcite and @xcite ) .",
    "this list however is not exhaustive , and , in general , combinations of these inferential procedures can be encompassed within a single model .",
    "for instance , in @xcite we propose a model for registering latent functions .",
    "another example might be to encompass functional linear regression and registration within the same model .",
    "the advantage to these types of models are that common pre - processing steps such as smoothing , registration , and covariance estimation can be included within the model so that uncertainty in these steps can be encompassed in the final inferential procedure .",
    "future work will focus on exploring further extensions to these models and continuing to pursue greater computational efficiency for these models .",
    "below , in detail , are the specifications for the hierarchical bayesian registration and factor analysis model discussed in this paper .",
    "the first section includes the basic model for functional data registration and factor analysis also found in section [ sec : methfa ] .",
    "section a.2 describes the mcmc sampling scheme for this model .",
    "+ * *    as discussed in section [ sec : methfa ] , the initial assumption of this model is that we are interested in registering and possibly grouping functional data , @xmath7 . the registered functions , @xmath8 , @xmath9 , are assumed to be characterized almost completely by a linear combination of two factors , @xmath2 and @xmath3 .",
    "below are the data and prior distributions used for this model . @xmath10    @xmath11 is a fixed matrix designed to penalize variation in any direction from the corresponding mean of the distribution in which it is utilized .",
    "it is composed of two matrices , @xmath12 and @xmath13 , such that @xmath11 @xmath14 @xmath12 + @xmath13 .",
    "@xmath12 penalizes variation from the mean in constant and linear directions , and @xmath13 penalizes variation from the mean in directions of curvature .",
    "@xmath13 is also used to penalize curvature in the base functions and factors , @xmath2 and @xmath3 , with associated smoothing parameters @xmath15 and @xmath16 .",
    "further details of the construction of @xmath12 and @xmath13 are found in @xcite .      using these assumptions ,",
    "the following full conditional distributions are derived to run a mcmc sampler .",
    "note , this list will not include a full conditional for the base functions or registered functions as their priors are not conjugate . instead",
    ", the base and registered functions are sampled via a metropolis step .",
    "+ @xmath17\\nonumber\\\\ \\mathbf f_2 \\mid rest & \\sim & n_p(\\boldsymbol\\mu_{\\mathbf f_2\\mid rest},\\boldsymbol\\sigma_{\\mathbf f_2\\mid rest})\\nonumber\\\\ \\boldsymbol\\sigma_{\\mathbf f_2\\mid rest } & = & \\big(\\sum_{i=1}^{n}z_{2i}^2\\big(\\frac{\\gamma_2 ^ 2}{\\gamma_1+\\gamma_2}\\big)\\boldsymbol{\\sigma}^{-1 } + \\boldsymbol\\sigma_f^{-1}\\big)^{-1}\\nonumber\\\\ \\boldsymbol\\mu_{\\mathbf f_2 \\mid rest } & = & \\boldsymbol\\sigma_{\\mathbf f_2\\mid rest } \\big[\\gamma_2\\boldsymbol{\\sigma}^{-1}\\sum_{i=1}^{n } z_{2i}\\big(\\mathbf{x_i(h_i)}-(z_{0i}\\mathbf 1+z_{1i}\\mathbf f_1)\\big)\\big]\\nonumber\\\\ z_{0i}\\mid rest & \\sim & n(\\mu_{z_{0i}\\mid rest},\\sigma^2_{z_{0i}\\mid rest } ) \\nonumber\\\\ \\sigma^2_{z_{0i}\\mid rest } & = & ( \\sigma^{-2}_{z_{0 } } + 2*\\mathbf 1_p'(\\gamma_1+\\gamma_2)\\boldsymbol\\sigma^{-1}\\mathbf 1_p)^{-1}\\nonumber\\\\ \\mu_{z_{0i}\\mid rest } & = & \\sigma^2_{z_{0i}\\mid rest}\\big(\\mathbf{x_i(h_i)}-\\mathbf{x_n(h_n)}+(z_{1n}-z_{1i})\\mathbf f_1+\\big(\\frac{\\gamma_2}{\\gamma_1+\\gamma_2}\\big)(z_{2n}-z_{2i})\\mathbf f_2 - \\nonumber\\\\ & & \\textit { } \\sum_{j=1}^{n-1}z_{0j}\\mathbbm{1}\\ { j\\neq i\\}\\mathbf 1_p\\big)'(\\gamma_1+\\gamma_2)\\boldsymbol\\sigma^{-1}\\mathbf 1_p\\nonumber\\\\ \\sigma^2_{z_{0}}\\mid rest & \\sim & ig(a + ( n-1)/2 , b + 1/2 \\sum_{i=1}^{n-1}z_{0i}^2 ) \\nonumber\\\\ z_{1i}\\mid rest & \\sim & n(\\mu_{z_{1i}\\mid rest},\\sigma^2_{z_{1i}\\mid rest } ) \\nonumber\\\\ \\sigma^2_{z_{1i}\\mid rest } & = & ( \\sigma^{-2}_{z_{1 } } + \\mathbf f_2'(\\gamma_1+\\gamma_2)\\boldsymbol\\sigma^{-1}\\mathbf f_2)^{-1}\\nonumber\\\\ \\mu_{z_{2i}\\mid rest } & = & \\sigma^2_{z_{2i}\\mid rest}\\big(\\mathbf{x_i(h_i)}-(z_{0i}\\mathbf 1_p+\\frac{\\gamma_2}{\\gamma_1+\\gamma_2}z_{2i}\\mathbf f_2)\\big)'(\\gamma_1+\\gamma_2)\\boldsymbol\\sigma^{-1 } \\mathbf f_1\\nonumber\\\\ \\sigma^2_{z_{1}}\\mid rest & \\sim & ig(a + n/2 , b + 1/2 \\sum_{i=1}^{n}z_{1i}^2 ) \\nonumber\\\\ z_{2i}\\mid rest & \\sim & n(\\mu_{z_{2i}\\mid rest},\\sigma^2_{z_{2i}\\mid rest } ) \\nonumber\\\\ \\sigma^2_{z_{2i}\\mid rest } & = & ( \\sigma^{-2}_{z_{2 } } + \\mathbf f_2'\\frac{\\gamma_2 ^ 2}{\\gamma_1+\\gamma_2}\\boldsymbol\\sigma^{-1}\\mathbf f_2)^{-1}\\nonumber\\\\ \\mu_{z_{2i}\\mid rest } & = & \\sigma^2_{z_{2i}\\mid rest}\\gamma_2\\big(\\mathbf{x_i(h_i)}-(z_{0i}\\mathbf 1_p+z_{1i}\\mathbf f_1)\\big)'\\boldsymbol\\sigma^{-1 } \\mathbf f_2\\nonumber\\\\ \\sigma^2_{z_{2}}\\mid rest & \\sim & ig(a + n/2 , b + 1/2 \\sum_{i=1}^{n}z_{2i}^2 ) \\nonumber\\\\ \\eta_{f}\\mid rest & \\sim & g(c + 2 , d + \\frac{1}{2}tr\\big((\\mathbf f_1\\mathbf f_1 ' + \\mathbf f_2\\mathbf f_2')\\mathbf p_{1}^{-}\\big)\\big)\\nonumber\\\\ \\lambda_{f}\\mid rest & \\sim & g(c + ( p-2 ) , d + \\frac{1}{2}tr\\big((\\mathbf f_1\\mathbf f_1 ' + \\mathbf f_2\\mathbf f_2')\\mathbf p_{2}^{-}\\big)\\big)\\nonumber\\end{aligned}\\ ] ]        the variational bayes procedure described here is based on the variational methods proposed by @xcite and @xcite .",
    "their proposed method optimizes a lower bound of the marginal likelihood which results in finding an approximate joint posterior density that has the smallest kullback - leibler ( kl ) distance , @xcite , from the true joint posterior density .    in minimizing the kl distance between the approximate and true posterior distribution ,",
    "parameters are updated by an optimization method that requires an approximate posterior density that not only factors but factors into components of known parametric forms .",
    "suppose , @xmath18 is the approximated posterior joint distribution .",
    "then for some partition of @xmath19 , @xmath20 , where each distribution @xmath21 is of a known parametric form .    in our model , the gaussian process priors for the base functions , @xmath22 , @xmath23 , are not conditionally conjugate to the likelihood function .",
    "therefore , the traditional variational bayes optimization method does not apply directly since @xmath24 , @xmath23 are not known parametric distributions .",
    "thus , we propose an adapted variational bayes algorithm .",
    "after initializing all parameters , in each iteration , the adapted variational bayes algorithm performs two steps . in the first step ,",
    "the ` likelihood ' as a function of the base functions is maximized .",
    "for this ` likelihood ' , all other parameters are fixed at their current values .",
    "the second step uses a traditional variational bayes iterative scheme to update all other parameters .",
    "specifically , assuming @xmath25 , for @xmath26 , so that , @xmath27 , the adapted variational bayes algorithm is as follows :    1 .",
    "initialize @xmath28 2 .   for each iteration , @xmath29 , and each @xmath30 , @xmath31 , update the estimate for + @xmath32 so that @xmath33@xmath34 ) 3 .",
    "for each iteration , @xmath29 , and each @xmath30 , @xmath35 , update @xmath21 so that @xmath36 @xmath37 @xmath38 $ ] , where the expectation is taken with respect to the distributions @xmath39 , @xmath40 , @xmath41 4 .   repeat steps ( 2 ) and ( 3 ) until the desired convergence criterion is met    this algorithm",
    "is guaranteed to converge .",
    "however , convergence is not guaranteed to a global maximum , and in practice it is sometimes necessary to adjust the registration and warping penalties as the functions become registered .",
    "an unregistered function that requires a substantial amount of warping can cause convergence to a local maximum due to the small penalty on warping .",
    "the flexibility in warping allowed by this small penalty can cause the function to deform rather than register .",
    "this can be remedied in two ways .",
    "the first option might be to perform a simple initial warping for this function that prevents the optimization from falling into a local mode .",
    "the second option is to adjust the registration and warping parameters over time .",
    "initially a stronger warping penalty is employed to prevent function deformation .",
    "then , as the functions register , the warping penalty can be reduced to allow for a more complete registration . when initializing an mcmc sampler , the final penalties on warping and registration from the adapted variational bayes algorithm should be used . for further information on the convergence properties of the adapted variational bayes algorithm and an analysis of",
    "how well adapted variational bayes estimates correspond to mcmc estimates , see @xcite .    below are the approximate posterior distributions , @xmath42 , @xmath35 , for the adapted variational bayes estimation procedure for the registration and factor analysis model .",
    "note , the subscripts on the @xmath43 distributions has been omitted . for a more thorough discussion and illustration of how the optimal @xmath43 distributions are derived",
    "see @xcite .",
    "as the @xmath43 densities are all of known distributional forms , updating these densities is equivalent to updating their parameters . for each iteration ,",
    "the following parameters are updated for the @xmath43 densities found in .",
    "these updates are listed in an order that allows the convergence criterion to be calculated .",
    "further details on the convergence criterion can be found in appendix b.2 .    & _ q(f_1 ) = ^-1 & + & _ q(f_1 ) = _ q(f_1)(_1+_2)^-1 & + & _ q(f_2 ) = ^-1 & + & _ q(f_2 ) = _",
    "q(f_2)_2 ^ -1 & + & _ q(z_0i)^2 = ( _ q(^-2_z_0 ) + 1_p(_1+_2)^-11_p)^-1 & + & _ q(z_0i ) = _ q(z_0i)^2(x_i(h_i)-x_n(h_n)+(_q(z_1n)-_q(z_1i))_q(f_1)+(_q(z_2n)-_q(z_2i))_q(f_2))- & + & _ q(z_0i)^2(_j=1^n-1 _ q(z_0j)\\{ij}1_p ) & + & _ q(z_1i)^2 = ( _ q(^-2_z_1 ) + tr((_q(f_1 ) + _ q(f_1)_q(f_1))(_1 + _ 2)^-1))^-1 & + & _ q(z_1i ) = _",
    "q(z_1i)^2(_q(f_1)(_1+_2)^-1(x_i(h_i ) - ( _ q(z_0i)1_p+_q(z_2i)_q(f_2 ) ) ) ) & + & _ q(z_2i)^2 = ( _ q(^-2_z_2 ) + tr((_q(f_2 ) + _ q(f_2)_q(f_2))^-1))^-1 & + & _ q(z_2i ) = _",
    "q(z_2i)^2(_q(f_2)_2 ^ -1(x_i(h_i ) - ( _ q(z_0i)1_p+_q(z_1i)_q(f_1 ) ) ) ) & + & d_q(_f ) = d+1/2*tr(p_1 ^ -(_q(f_1)+_q(f_1)_q(f_1)+_q(f_2)+_q(f_2)_q(f_2) ) ) & + & d_q(_f ) = d+1/2*tr(p_2 ^ -(_q(f_1)+_q(f_1)_q(f_1)+_q(f_2)+_q(f_2)_q(f_2) ) ) & + & b_q(_z_0 ^ 2 ) = b+1/2_i=1^n-1 ( ^2_q(z_0i ) + _ q(z_0i)^2 ) & + & b_q(_z_1 ^ 2 ) = b+1/2_i=1^n ( ^2_q(z_1i ) + _ q(z_1i)^2 ) & + & b_q(_z_2 ^ 2 ) = b+1/2_i=1^n ( ^2_q(z_2i ) + _ q(z_2i)^2 ) &        @xmath47&=&e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } ( f(\\mathbf x , \\mathbf w\\mid \\boldsymbol\\theta_{-\\mathbf w})f ( \\boldsymbol\\theta_{-\\mathbf w } ) ) - \\textit{log } q(\\boldsymbol\\theta_{-\\mathbf w})\\big]\\nonumber\\\\ & = & e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } f(\\mathbf x , \\mathbf w\\mid \\boldsymbol\\theta_{-\\mathbf w } ) + \\textit{log } f ( \\boldsymbol\\theta_{-\\mathbf w } ) - \\textit{log } q(\\boldsymbol\\theta_{-\\mathbf w})\\big]\\nonumber\\\\ & = & e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } f(\\mathbf x , \\mathbf w\\mid \\boldsymbol\\theta_{-\\mathbf w})\\big ] \\nonumber \\\\ & &   + \\textit { } e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } f(\\mathbf f_1)-\\textit{log } q(\\mathbf f_1)\\big]\\nonumber \\\\ & &   + \\textit { } e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } f(\\mathbf f_2)-\\textit{log } q(\\mathbf f_2)\\big]\\nonumber \\\\ & & + \\sum_{i=1}^{(n-1)}e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } f(z_{0i})-\\textit{log } q(z_{0i})\\big]\\nonumber\\\\ & & + \\sum_{i=1}^{n}e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } f(z_{1i})-\\textit{log } q(z_{1i})\\big]\\nonumber\\\\ & & + \\sum_{i=1}^{n}e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } f(z_{2i})-\\textit{log } q(z_{2i})\\big]\\nonumber\\\\ & & + \\textit { } e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } f(\\sigma^2_{z_0})-\\textit{log } q(\\sigma^2_{z_0})\\big]\\nonumber\\\\ & & + \\textit { } e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } f(\\sigma^2_{z_1})-\\textit{log }",
    "q(\\sigma^2_{z_1})\\big]\\nonumber\\\\ & & + \\textit { } e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } f(\\sigma^2_{z_2})-\\textit{log } q(\\sigma^2_{z_2})\\big]\\nonumber\\\\ & & + \\textit { } e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } f(\\eta_f)-\\textit{log } q(\\eta_f)\\big]\\nonumber\\\\ & & + \\textit { } e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } f(\\lambda_f)-\\textit{log } q(\\lambda_f)\\big]\\nonumber\\end{aligned}\\ ] ]      @xmath48 $ ] @xmath49\\big)\\big]\\nonumber \\\\ & & + \\textit { } e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\sum_{i=1}^{n}-\\frac{1}{2}[(\\mathbf x_i(\\mathbf h_i)'(\\gamma_1+\\gamma_2)\\boldsymbol\\sigma^{-1}\\mathbf x_i(\\mathbf h_i)-2\\mathbf x_i(\\mathbf h_i)'(\\gamma_1+\\gamma_2)\\boldsymbol\\sigma^{-1}(z_{0i}\\mathbf 1_p+z_{1i}\\mathbf f_1+\\frac{\\gamma_2}{\\gamma_1+\\gamma_2}z_{2i}\\mathbf f_2 ) + \\nonumber\\\\ & & \\textit { } ( z_{0i}\\mathbf 1_p+z_{1i}\\mathbf f_1+\\frac{\\gamma_2}{\\gamma_1+\\gamma_2}z_{2i}\\mathbf f_2)'(\\gamma_1+\\gamma_2)\\boldsymbol\\sigma^{-1}(z_{0i}\\mathbf 1_p+z_{1i}\\mathbf f_1+\\frac{\\gamma_2}{\\gamma_1+\\gamma_2}z_{2i}\\mathbf f_2)]\\big]\\nonumber\\\\ & = & \\sum_{i=1}^{n}\\big(log[(2\\pi)^{-p/2}\\mid(\\gamma_1 + \\gamma_2)^{-1}\\boldsymbol\\sigma\\mid^{-1/2}]\\big)\\nonumber\\\\ & & + \\big[\\sum_{i=1}^{n } -\\frac{1}{2}\\big(\\mathbf x_i(\\mathbf h_i)'(\\gamma_1+\\gamma_2)\\boldsymbol\\sigma^{-1}\\mathbf x_i(\\mathbf h_i)-\\nonumber\\\\ & & \\textit {          } 2\\mathbf x_i(\\mathbf h_i)'(\\gamma_1+\\gamma_2)\\boldsymbol\\sigma^{-1}\\mu_{q(z_{0i})}\\mathbf 1_p-2\\mathbf x_i(\\mathbf h_i)'(\\gamma_1+\\gamma_2)\\boldsymbol\\sigma^{-1}\\mu_{q(z_{1i})}\\boldsymbol\\mu_{q(\\mathbf",
    "f_1 ) } -\\nonumber\\\\ & & \\textit {    } 2\\mathbf x_i(\\mathbf h_i)'\\gamma_2\\boldsymbol\\sigma^{-1}\\mu_{q(z_{2i})}\\boldsymbol\\mu_{q(\\mathbf f_2 ) } + \\nonumber\\\\ & & \\textit { } ( \\sigma^2_{q(z_{1i } ) } + \\mu_{q(z_{1i})}^2)tr((\\boldsymbol\\sigma_{q(\\mathbf f_1 ) } + \\boldsymbol\\mu_{q(\\mathbf f_1)}\\boldsymbol\\mu_{q(\\mathbf f_1)}')(\\gamma_1+\\gamma_2)\\boldsymbol\\sigma^{-1})+\\nonumber\\\\ & & \\textit { } ( \\sigma^2_{q(z_{2i } ) } + \\mu_{q(z_{2i})}^2)tr((\\boldsymbol\\sigma_{q(\\mathbf f_2 ) } + \\boldsymbol\\mu_{q(\\mathbf",
    "f_2)}\\boldsymbol\\mu_{q(\\mathbf f_2)}')\\frac{\\gamma_2 ^ 2}{(\\gamma_1+\\gamma_2)}\\boldsymbol\\sigma^{-1})+\\nonumber\\\\ & & \\textit { } 2\\mu_{q(z_{0i})}\\mu_{q(z_{1i})}\\mathbf 1_p'(\\gamma_1+\\gamma_2)\\boldsymbol\\sigma^{-1}\\boldsymbol\\mu_{q(\\mathbf f_1)}+ 2\\mu_{q(z_{1i})}\\mu_{q(z_{2i})}\\boldsymbol\\mu_{q(\\mathbf f_1)}'\\gamma_2\\boldsymbol\\sigma^{-1}\\boldsymbol\\mu_{q(\\mathbf f_2 ) } + \\nonumber\\\\ & & \\textit { } 2\\mu_{q(z_{0i})}\\mu_{q(z_{2i})}\\mathbf 1_p'\\gamma_2\\boldsymbol\\sigma^{-1}\\boldsymbol\\mu_{q(\\mathbf f_2 ) } \\big)\\big ] -\\nonumber\\\\ & & \\textit { } \\big[\\sum_{i=1}^{n-1 } ( \\sigma^2_{q(z_{0i } ) } + \\mu_{q(z_{0i})}^2 ) + \\frac{1}{2 } \\sum_{i=1}^{n-1}\\sum_{j=1}^{n-1}\\mu_{q(z_{0i})}\\mu_{q(z_{0j})}\\mathbbm{1}\\{j \\neq i\\}\\big]\\mathbf 1_p'(\\gamma_1+\\gamma_2)\\boldsymbol\\sigma^{-1}\\mathbf 1_p\\nonumber\\end{aligned}\\ ] ]    @xmath50 & = & e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[-\\frac{p}{2}\\textit{log } 2\\pi + \\frac{1}{2}\\textit{log } \\mid\\eta_f\\mathbf p_1^{- } + \\lambda_f\\mathbf p_2^{-}\\mid\\big ] - \\nonumber \\\\ & & \\textit { } e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\frac{1}{2}(tr[\\mathbf f_1\\mathbf f_1'(\\eta_f\\mathbf p_1^{-}+\\lambda_f\\mathbf p_2^{-})]\\big ] + \\nonumber\\\\ & & \\textit { }   e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\frac{p}{2}\\textit { log } 2\\pi + \\frac{1}{2 } \\textit{log } \\mid\\boldsymbol\\sigma_{q(\\mathbf f_1)}\\mid\\big ] + \\nonumber\\\\ & & \\textit { } e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\frac{1}{2}tr(\\mathbf f_1\\mathbf f_1 ' \\boldsymbol\\sigma_{q(\\mathbf f_1)}^{-1 } ) - \\mathbf f_1'\\boldsymbol\\sigma_{q(\\mathbf f_1)}^{-1}\\boldsymbol\\mu_{q(\\mathbf f_1)}\\big ] + \\nonumber\\\\ & & \\textit { } e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\frac{1}{2}\\boldsymbol\\mu_{q(\\mathbf f_1)}'\\boldsymbol\\sigma_{q(\\mathbf f_1)}^{-1}\\boldsymbol\\mu_{q(\\mathbf f_1)}\\big]\\nonumber\\\\ & = & c + \\frac{1}{2 } e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[2\\textit{log } \\eta_f\\big]+ \\frac{1}{2 } e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[(p-2)\\textit{log } \\lambda_f\\big ] - \\nonumber\\\\ & & \\textit { } \\frac{1}{2}tr\\big((\\boldsymbol\\sigma_{q(\\mathbf f_1 ) } + \\boldsymbol\\mu_{q(\\mathbf f_1)}\\boldsymbol\\mu_{q(\\mathbf f_1)}')(\\mu_{q(\\eta_f)}\\mathbf p_1^{-}+\\mu_{q(\\lambda_f)}\\mathbf p_2^{-})\\big)-\\nonumber\\\\ & & \\textit { } \\frac{1}{2}\\textit{log } \\mid\\boldsymbol\\sigma_{q(\\mathbf f_1)}^{-1}\\mid + \\frac{p}{2}\\nonumber\\end{aligned}\\ ] ]      @xmath52 & = & e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[-\\frac{p}{2}\\textit{log } 2\\pi + \\frac{1}{2}\\textit{log } \\mid\\eta_f\\mathbf p_1^{- } + \\lambda_f\\mathbf p_2^{-}\\mid\\big ] - \\nonumber \\\\ & & \\textit { } e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\frac{1}{2}(tr[\\mathbf f_2\\mathbf f_2'(\\eta_f\\mathbf p_1^{-}+\\lambda_f\\mathbf p_2^{-})]\\big ] + \\nonumber\\\\ & & \\textit { }   e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\frac{p}{2}\\textit { log } 2\\pi + \\frac{1}{2 } \\textit{log } \\mid\\boldsymbol\\sigma_{q(\\mathbf f_2)}\\mid\\big ] + \\nonumber\\\\ & & \\textit { } e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\frac{1}{2}tr(\\mathbf f_2\\mathbf f_2 ' \\boldsymbol\\sigma_{q(\\mathbf f_2)}^{-1 } ) - \\mathbf f_2'\\boldsymbol\\sigma_{q(\\mathbf f_2)}^{-1}\\boldsymbol\\mu_{q(\\mathbf f_2)}\\big ] + \\nonumber\\\\ & & \\textit { } e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\frac{1}{2}\\boldsymbol\\mu_{q(\\mathbf f_2)}'\\boldsymbol\\sigma_{q(\\mathbf f_2)}^{-1}\\boldsymbol\\mu_{q(\\mathbf f_2)}\\big]\\nonumber\\\\ & = & c + \\frac{1}{2 } e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[2\\textit{log } \\eta_f\\big]+ \\frac{1}{2 } e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[(p-2)\\textit{log } \\lambda_f\\big ] - \\nonumber\\\\ & & \\textit { } \\frac{1}{2}tr\\big((\\boldsymbol\\sigma_{q(\\mathbf f_2 ) } + \\boldsymbol\\mu_{q(\\mathbf f_2)}\\boldsymbol\\mu_{q(\\mathbf f_1)}')(\\mu_{q(\\eta_f)}\\mathbf p_1^{-}+\\mu_{q(\\lambda_f)}\\mathbf p_2^{-})\\big)-\\nonumber\\\\ & & \\textit { } \\frac{1}{2}\\textit{log } \\mid\\boldsymbol\\sigma_{q(\\mathbf f_1)}^{-1}\\mid + \\frac{p}{2}\\nonumber\\end{aligned}\\ ] ]      @xmath55 & = & e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[-\\frac{n-1}{2}\\textit{log } 2\\pi -\\frac{n-1}{2}\\textit{log } \\sigma^2_{z_0 } -\\sum_{i=1}^{n-1}-\\frac{1}{2\\sigma^2_{z_0}}z_{0i}^2   + \\nonumber\\\\ & & \\textit { } \\frac{n-1}{2}\\textit{log } 2\\pi+\\frac{n-1}{2}\\textit{log } \\sigma^2_{q({z_{0i}})}+\\nonumber \\\\ & & \\textit { } \\sum_{i=1}^{n-1}\\frac{1}{2\\sigma^2_{q(z_{0i})}}(z_{0i}-\\mu_{q(z_{0i})})^2\\big ] \\label{eq : z0}\\\\ & = & \\frac{n-1}{2}\\textit{log } \\sigma^2_{q({z_{0i } } ) } - e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\frac{n-1}{2}\\textit{log } \\sigma^2_{z_0}\\big]-\\label{eq : z02 } \\\\   & & \\textit { } \\frac{1}{2}\\mu_{q(\\frac{1}{\\sigma^2_{z_{0}}})}\\big(\\sum_{i=1}^{n-1}(\\sigma_{q(z_{0i})}^2+\\mu_{q(z_{0i})}^2)\\big ) + \\frac{n-1}{2}\\nonumber\\end{aligned}\\ ] ]      @xmath58 & = & e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[-\\frac{n}{2}\\textit{log } 2\\pi -\\frac{n}{2}\\textit{log } \\sigma^2_{z_1 } -\\sum_{i=1}^{n}-\\frac{1}{2\\sigma^2_{z_1}}z_{1i}^2   + \\nonumber\\\\ & & \\textit { } \\frac{n}{2}\\textit{log } 2\\pi+\\frac{n}{2}\\textit{log } \\sigma^2_{q({z_{1i}})}+\\sum_{i=1}^{n}\\frac{1}{2\\sigma^2_{q(z_{1i})}}(z_{1i}-\\mu_{q(z_{1i})})^2\\big ] \\nonumber\\\\ & = & \\frac{n}{2}\\textit{log } \\sigma^2_{q({z_{1i } } ) } - e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\frac{n}{2}\\textit{log } \\sigma^2_{z_1}\\big]-\\nonumber \\\\ & & \\textit { } \\frac{1}{2}\\mu_{q(\\frac{1}{\\sigma^2_{z_1}})}\\big(\\sum_{i=1}^{n}(\\sigma_{q(z_{1i})}^2+\\mu_{q(z_{1i})}^2)\\big ) + \\frac{n}{2}\\nonumber\\end{aligned}\\ ] ]      @xmath61 & = & e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[-\\frac{n}{2}\\textit{log } 2\\pi -\\frac{n}{2}\\textit{log } \\sigma^2_{z_2 } -\\sum_{i=1}^{n}-\\frac{1}{2\\sigma^2_{z_2}}z_{2i}^2   + \\nonumber\\\\ & & \\textit { } \\frac{n}{2}\\textit{log } 2\\pi+\\frac{n}{2}\\textit{log } \\sigma^2_{q({z_{2i}})}+\\sum_{i=1}^{n}\\frac{1}{2\\sigma^2_{q(z_{2i})}}(z_{2i}-\\mu_{q(z_{2i})})^2\\big ] \\nonumber\\\\ & = & \\frac{n}{2}\\textit{log } \\sigma^2_{q({z_{2i } } ) } - e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\frac{n}{2}\\textit{log } \\sigma^2_{z_2}\\big]-\\nonumber \\\\ & & \\textit { } \\frac{1}{2}\\mu_{q(\\frac{1}{\\sigma^2_{z_2}})}\\big(\\sum_{i=1}^{n}(\\sigma_{q(z_{2i})}^2+\\mu_{q(z_{2i})}^2)\\big ) + \\frac{n}{2}\\nonumber\\end{aligned}\\ ] ]      @xmath63 & = & e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } \\frac{b^a}{\\gamma(a)}-(a+1)\\textit{log } \\sigma_{z_0}^2-b\\frac{1}{\\sigma_{z_0}^2}-\\nonumber\\\\ & & \\textit{log   } \\frac{b_{q(\\sigma_{z_0}^2)}^{a_{q(\\sigma_{z_0}^2)}}}{\\gamma(a_{q(\\sigma_{z_0}^2)})}+(a_{q(\\sigma_{z_0}^2)}+1)\\textit{log } \\sigma_{z_0}^2+\\nonumber\\\\ & & \\textit { } b_{q(\\sigma_{z_0}^2)}\\frac{1}{\\sigma_{z_0}^2}\\big]\\nonumber\\\\ & = & e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[-(a+1)\\textit{log } \\sigma_{z_0}^2\\big ]   -b\\mu_{q(\\frac{1}{\\sigma_{z_0}^2 } ) } -\\textit{log } \\frac{b_{q(\\sigma_{z_0}^2)}^{a_{q(\\sigma_{z_0}^2)}}}{\\gamma(a_{q(\\sigma_{z_0}^2)})}+\\nonumber \\\\ & & \\textit { } \\textit{log } \\frac{b^a}{\\gamma(a)}+e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[(a_{q(\\sigma_{z_0}^2)}+1)\\textit{log } \\sigma_{z_0}^2\\big]+ b_{q(\\sigma_{z_0}^2)}\\mu_{q(\\frac{1}{\\sigma_{z_0}^2})}\\nonumber\\end{aligned}\\ ] ]      @xmath65 & = & e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } \\frac{b^a}{\\gamma(a)}-(a+1)\\textit{log } \\sigma_{z_1}^2-b\\frac{1}{\\sigma_{z_1}^2}-\\nonumber\\\\ & & \\textit{log   } \\frac{b_{q(\\sigma_{z_1}^2)}^{a_{q(\\sigma_{z_1}^2)}}}{\\gamma(a_{q(\\sigma_{z_1}^2)})}+(a_{q(\\sigma_{z_1}^2)}+1)\\textit{log } \\sigma_{z_1}^2+\\nonumber\\\\ & & \\textit { } b_{q(\\sigma_{z_1}^2)}\\frac{1}{\\sigma_{z_1}^2}\\big]\\nonumber\\\\ & = & e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[-(a+1)\\textit{log } \\sigma_{z_1}^2\\big ]   -b\\mu_{q(\\frac{1}{\\sigma_{z_1}^2 } ) } -\\textit{log } \\frac{b_{q(\\sigma_{z_1}^2)}^{a_{q(\\sigma_{z_1}^2)}}}{\\gamma(a_{q(\\sigma_{z_1}^2)})}+\\nonumber \\\\ & & \\textit { } \\textit{log } \\frac{b^a}{\\gamma(a)}+e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[(a_{q(\\sigma_{z_1}^2)}+1)\\textit{log } \\sigma_{z_1}^2\\big]+ b_{q(\\sigma_{z_1}^2)}\\mu_{q(\\frac{1}{\\sigma_{z_1}^2})}\\nonumber\\end{aligned}\\ ] ]      @xmath67 & = & e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } \\frac{b^a}{\\gamma(a)}-(a+1)\\textit{log } \\sigma_{z_2}^2-b\\frac{1}{\\sigma_{z_2}^2}-\\nonumber\\\\ & & \\textit{log   } \\frac{b_{q(\\sigma_{z_2}^2)}^{a_{q(\\sigma_{z_2}^2)}}}{\\gamma(a_{q(\\sigma_{z_2}^2)})}+(a_{q(\\sigma_{z_2}^2)}+1)\\textit{log } \\sigma_{z_2}^2+\\nonumber\\\\ & & \\textit { } b_{q(\\sigma_{z_2}^2)}\\frac{1}{\\sigma_{z_2}^2}\\big]\\nonumber\\\\ & = & e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[-(a+1)\\textit{log } \\sigma_{z_2}^2\\big ]   -b\\mu_{q(\\frac{1}{\\sigma_{z_2}^2 } ) } -\\textit{log } \\frac{b_{q(\\sigma_{z_2}^2)}^{a_{q(\\sigma_{z_2}^2)}}}{\\gamma(a_{q(\\sigma_{z_2}^2)})}+\\nonumber \\\\ & & \\textit { } \\textit{log } \\frac{b^a}{\\gamma(a)}+e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[(a_{q(\\sigma_{z_2}^2)}+1)\\textit{log } \\sigma_{z_2}^2\\big]+ b_{q(\\sigma_{z_2}^2)}\\mu_{q(\\frac{1}{\\sigma_{z_2}^2})}\\nonumber\\end{aligned}\\ ] ]      @xmath69 & = & e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } \\frac{d^c}{\\gamma(c)}+(c-1)\\textit{log } \\eta_f - d\\eta_f-\\nonumber\\\\ & & \\textit{log   } \\frac{d_{q(\\eta_f)}^{c_{q(\\eta_f)}}}{\\gamma(c_{q(\\eta_f)})}-c\\textit { log } \\eta_f + d_{q(\\eta_f)}{\\eta_f}\\big]\\nonumber\\\\ & = & \\textit{log } \\frac{d^c}{\\gamma(c ) } - \\textit{log   } \\frac{d_{q(\\eta_f)}^{c_{q(\\eta_x)}}}{\\gamma(c_{q(\\eta_f)})}-2e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } \\eta_f\\big ]   -d\\mu_{q(\\eta_f ) } + \\nonumber\\\\ & & d_{q(\\eta_f)}\\mu_{q(\\eta_f)}\\nonumber \\end{aligned}\\ ] ]      @xmath70 & = & e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } \\frac{d^c}{\\gamma(c)}+(c-1)\\textit{log } \\lambda_f - d\\lambda_f-\\nonumber\\\\ & & \\textit{log   } \\frac{d_{q(\\lambda_f)}^{c_{q(\\lambda_f)}}}{\\gamma(c_{q(\\lambda_f)})}-\\big(\\frac{p-2}{2}+c-1\\big)\\textit { log } \\lambda_f + d_{q(\\lambda_f)}{\\lambda_f}\\big]\\nonumber\\\\ & = & \\textit{log } \\frac{d^c}{\\gamma(c ) } - \\textit{log   } \\frac{d_{q(\\lambda_f)}^{c_{q(\\lambda_f)}}}{\\gamma(c_{q(\\lambda_f)})}-(p-2)e_{q(\\boldsymbol\\theta_{-\\mathbf w})}\\big[\\textit{log } \\lambda_f\\big ]   -d\\mu_{q(\\lambda_f ) } + \\nonumber\\\\ & & d_{q(\\lambda_f)}\\mu_{q(\\lambda_f)}\\nonumber \\end{aligned}\\ ] ]    the expression for @xmath46 $ ] can be simplified much further by combining terms that cancel out . however , in some cases the ability to cancel terms depends on the order of the updates .",
    "for instance , in the expression , @xmath71 $ ] , the terms @xmath72 and @xmath73 cancel with @xmath74 from @xmath75 $ ] as long as the parameters of @xmath76 are updated before @xmath77 . for convenience , we have taken account the ordering necessary to compute the convergence criterion in the updates given above .",
    "additionally , note all components in this expression that do not change from one iteration to the next can be ignored .",
    "calderhead , b. , girolami , m. , and lawrence , n.(2009 ) . accelerating bayesian inference over nonlinear differential equations with gaussian processes .",
    "_ advances in neural information processing systems _ * 21 * , 217 - 224 ."
  ],
  "abstract_text": [
    "<S> we extend the definition of functional data registration to encompass a larger class of registration models . </S>",
    "<S> in contrast to traditional registration models , we allow for registered functions that have more than one primary direction of variation . </S>",
    "<S> the proposed bayesian hierarchical model simultaneously registers the observed functions and estimates the two primary factors that characterize variation in the registered functions . </S>",
    "<S> each registered function is assumed to be predominantly composed of a linear combination of these two primary factors , and the function - specific weights for each observation are estimated within the registration model . </S>",
    "<S> we show how these estimated weights can easily be used to classify functions after registration using both simulated data and a juggling data set .    </S>",
    "<S> # 1    0    0    1    0    * title *    _ keywords : _ bayesian modeling , factor analysis , functional data , registration , variational bayes </S>"
  ]
}