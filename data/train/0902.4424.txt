{
  "article_text": [
    "there has been a vast amount of recent literature dedicated to algorithms for sparse recovery , both in the context of inverse imaging problems and of _ compressed sensing_. as an alternative to the usual quadratic penalties used in regularization theory for ill - posed or ill - conditioned inverse problems , the use of @xmath0-type penalties has been advocated in order to recover regularized solutions having sparse expansions on a given basis or frame , such as e.g. a wavelet system @xcite . denoting by @xmath1 the vector of coefficients describing the unknown object , by @xmath2 the vector of ( noisy ) data and by @xmath3 the linear operator ( @xmath4 matrix ) modelling the link between the two , the inverse problem amounts to finding a regularized solution of the equation @xmath5 . when it is known a priori that @xmath6 is a sparse vector",
    ", one can resort to the following penalized least - squares strategy @xcite , also referred to as the _ lasso _ after tibshirani @xcite : @xmath7 where @xmath8 is a positive regularization parameter regulating the balance between the penalty and the data misfit terms .",
    "the norm @xmath9 denotes the usual @xmath10 norm whereas @xmath11 is the @xmath0 norm of the vector @xmath6 .    in compressed",
    "sensing ( also called _ compressive sampling _ ) , the aim is to reconstruct a _",
    "sparse _ signal or object from a small number of linear measurements @xcite .",
    "the recovery of such an object can then be achieved by searching for the sparsest solution to the linear system @xmath12 representing the measurement process , or equivalently by looking for a solution with minimum `` @xmath13-norm '' . to avoid the combinatorial complexity of the latter problem",
    ", one can use as a proxy a convex @xmath0-norm minimization strategy .",
    "when the data @xmath14 are affected by measurement errors , the problem is reformulated as a penalized least - squares optimization analogous to ( [ minimizer ] ) .",
    "let us observe that problem ( [ minimizer ] ) is equivalent to the constrained minimization problem : @xmath15 for a certain @xmath16 .",
    "one can show that @xmath17 and @xmath18 are piecewise linear functions of @xmath8 and @xmath16 .",
    "one always has that @xmath19 for @xmath20 .",
    "the relationship between @xmath8 and @xmath16 is given by @xmath21 and @xmath22 @xcite .",
    "several iterative methods for solving the minimization problems ( [ minimizer ] ) or ( [ constrmin ] ) have been proposed in the literature . for the purpose of comparison with our new acceleration scheme",
    ", we will focus on the following algorithms :    1 .   the iterative soft - thresholding algorithm ( `` ista '' ) proposed in @xcite )",
    "goes as follows : @xmath23 $ ] where @xmath24 is the residual in step @xmath25 and the ( nonlinear ) soft - thresholding operator acts componentwise as @xmath26)_i = x_i - \\lambda\\ { \\mathrm{sgn}}(x_i)$ ] if @xmath27 and zero otherwise . for any initial vector @xmath28 and under the condition",
    "@xmath29 , this scheme has been shown to converge to the minimizer @xmath30 defined by ( [ minimizer ] ) @xcite . when reinterpreted as a forward - backward proximal scheme , convergence can be seen to hold also for @xmath31 @xcite .",
    "[ tlwalg ] 2 .",
    "the fast iterative soft - thresholding algorithm ( `` fista '' ) , proposed in @xcite , is a variation of ista .",
    "defining the operator @xmath32 by @xmath33 $ ] , the fista algorithm is : @xmath34 where @xmath35 , @xmath36 and @xmath37 .",
    "it has virtually the same complexity as the ista algorithm , but can be shown to have better convergence properties.[fistaalg ] 3 .   the gpsr algorithm proposed in @xcite .",
    "4 .   the sparsa algorithm proposed in @xcite . 5 .",
    "the projected steepest descent ( `` psd '' ) method proposed in @xcite : @xmath38 $ ] , with @xmath39 .",
    "@xmath40 denotes the projection onto the @xmath0-ball @xmath41 of radius @xmath16.[psdalg ]    the figures in section [ sec4 ] provide a visual way to compute the performance of these algorithms in two problem examples .",
    "note that these are the same as in @xcite , where the reader can find comparisons to yet other methods , including e.g. the @xmath0-ls method , an interior point algorithm proposed in @xcite .",
    "in this section we describe the acceleration scheme we propose for solving the optimization problem ( [ constrmin ] ) .",
    "this problem is a particular case of the general problem of minimizing a convex and continuously differentiable function @xmath42 over a closed convex set @xmath43 . here",
    "@xmath44 . a gradient projection method for solving this problem can be stated as in algorithm [ gpm ] .",
    "some comments about the main steps of algorithm gp are in order .",
    "+ first of all , it is worth to stress that any choice of the steplength @xmath45 in a closed interval is permitted .",
    "this is very important from a practical point of view since it allows to make the updating rule of @xmath45 problem - related and oriented at optimizing the performance .",
    "+ if the projection performed in step 2 returns a vector @xmath46 equal to @xmath47 , then @xmath47 is a stationary point and the algorithm stops . when @xmath48 , it is possible to prove that @xmath49 is a descent direction for @xmath50 in @xmath47 and the backtracking loop in step 5 terminates with a finite number of runs ; thus the algorithm is well defined @xcite .",
    "+ the nonmonotone line - search strategy implemented in step 5 ensures that @xmath51 is lower than the maximum of the objective function in the last @xmath52 iterations @xcite ; of course , if @xmath53 then the strategy reduces to the standard monotone armijo rule @xcite .",
    "concerning the convergence properties of the algorithm , the following result can be derived from the analysis carried out in @xcite for more general gradient projection schemes : if the level set @xmath54 is bounded , then every accumulation point of the sequence @xmath55 generated by the algorithm gp is a stationary point of @xmath42 in @xmath41 .",
    "we observe that the assumption is trivially satisfied for problem since in this case the feasible region @xmath41 is bounded .",
    "now , we may discuss the choice of the steplengths @xmath56 $ ] .",
    "steplength selection rules in gradient methods have received an increasing interest in the last years from both the theoretical and the practical point of view .",
    "on one hand , following the original ideas of barzilai and borwein ( bb ) @xcite , several steplength updating strategies have been devised to accelerate the slow convergence exhibited in most cases by standard gradient methods , and a lot of effort has been put into explaining the effects of these strategies @xcite . on the other hand , numerical experiments on randomly generated , library and real - life test problems have confirmed the remarkable convergence rate improvements involved by some bb - like steplength selections @xcite .",
    "thus , it seems natural to equip a gradient projection method with a steplength selection that takes into account the recent advances on the bb - like updating rules .",
    "choose the starting point @xmath57 , set the parameters @xmath58 , @xmath59 and fix a positive integer @xmath52 . + for @xmath60 do the following steps :    choose the parameter @xmath61 $ ] ;    projection : @xmath62 ; + if @xmath63 then stop , declaring that @xmath47 is a stationary point ;    descent direction : @xmath64 ;    set @xmath65 and @xmath66 ;    backtracking loop :    if @xmath67  then + go to step 6 ;    else + set @xmath68 and go to step 5 ;    endif    set @xmath69 .",
    "end    first of all we must recall the two bb rules usually exploited by the main steplength updating strategies . to this end , by denoting with @xmath70 the @xmath71 identity matrix , we can regard the matrix @xmath72 as an approximation of the hessian @xmath73 and derive two updating rules for @xmath45 by forcing quasi - newton properties on @xmath74 : @xmath75 where @xmath76 and @xmath77 . in this way",
    ", the steplengths @xmath78 are obtained .",
    "if  @xmath79  then + set @xmath80 $ ] , @xmath81 and a non - negative integer @xmath82 ; + else + if @xmath83   then + @xmath84 ; + else + @xmath85 ; + @xmath86 ; +   + if @xmath87  then + @xmath88 ; + @xmath89 ; + else + @xmath90 ; + @xmath91 ; + endif + endif + endif    at this point , inspired by the steplength alternations successfully implemented in recent gradient methods @xcite , we propose a steplength updating rule for gp which adaptively alternates the values provided by .",
    "the details of the gp steplength selection are given in algorithm [ ss ] .",
    "this rule decides the alternation between two different selection strategies by means of the variable threshold @xmath92 instead of a constant parameter as done in @xcite and @xcite .",
    "this trick makes the choice of @xmath93 less important for the gp performance and , in our experience , seems able to avoid the drawbacks due to the use of the same steplength rule in too many consecutive iterations . in the following we denote by gpss the algorithm [ gpm ] equipped with the steplength selection [ ss ] .",
    "+ we end this section by describing the setting for the gpss parameters used in the computational study of this work :    * _ line - search parameters _ : @xmath53 ( monotone line - search ) ,  @xmath94 ,  @xmath95 ; * _ steplength parameters _ : @xmath96 ,   @xmath97 , + @xmath98 ,  @xmath99 ,  @xmath100",
    ".    in our experience the above setting often provides satisfactory performance ; however , it can not be considered optimal for every application and a careful parameter tuning is always advisable .",
    "to assess the performances of our gpss algorithm and estimate the gain in speed it can provide with respect to the algorithms 1 to 5 , we perform some numerical tests . to this purpose",
    "we adopt the methodology proposed in @xcite and based on the notion of _ approximation isochrones_. it improves on the comparisons made for a single value of @xmath8 or @xmath16 , i.e. for a single level of sparsity of the recovered object .    for values of @xmath8 in a given interval @xmath101 ,",
    "one computes the minimizer @xmath17 of ( [ minimizer ] ) .",
    "when the number of nonzero components in @xmath17 is not too large , this can be done by means of the direct ( non - iterative ) _ homotopy _ method @xcite or lars algorithm @xcite .",
    "then , for a fixed and given computation time , one runs one of the algorithms for each value of @xmath8 ( or @xmath16 ) . the relative error @xmath102 reached at the end of the computation",
    "is plotted as a function of @xmath8 and hence this plot is just the approximation isochrone showing the degree of accuracy reached in the given amount of computing time for each value of @xmath8 .",
    "a set of such plots allow to quickly grasp the performances of a given algorithm in various parameter regimes and to easily compare it with other methods ; it reveals in one glance under which circumstances the algorithms do well or fail .",
    "the paper @xcite also demonstrates the fact that the relative performances of the algorithms may strongly depend on the specific application one considers , and in particular on the properties of the linear operator @xmath3 modelling the problem .",
    "we test the different algorithms on two different operators arising typically either from a compressed sensing or from an inverse problem . in both cases",
    "the matrix @xmath3 is of size 1848x8192 . in the first case ,",
    "the elements of @xmath3 are taken from a gaussian distribution with zero mean and variance such that @xmath103 .",
    "this matrix is rather well conditioned and can serve as a paradigm of compressed sensing applications .",
    "it is applied to a sparse vector and perturbed by additive gaussian noise ( about @xmath104 ) to yield the data @xmath14 .",
    "the second matrix models a severely ill - conditioned linear inverse problem that finds its origin in a problem of seismic tomography described in detail in @xcite .    for both operators , the minimizer @xmath17 is computed for 50 different values of @xmath8 ( or equivalently , 50 different values of @xmath16 ) .",
    "then , for each iterative algorithm , we make plots having the relative error @xmath105 on the vertical axis and @xmath106 on the bottom horizontal axis ( on the top horizontal axis the value of @xmath107 is also reported ) .",
    "the number of nonzero components @xmath108 in @xmath17 is indicated by vertical dashed lines . in each plot",
    "we report the isochrone lines that correspond to a given amount of computer time . in this way",
    "one can see how close , for the different values of @xmath8 , the iterates approach the minimizer after a given time .",
    "let us remark that although the reported computing times are of course specific to a given computer and implementation , the overall behavior of the isochrones should be fairly general .",
    "for example , the fact that they get very close to each other in some places can be interpreted as a bottleneck feature of the algorithm .    in figure [ gausspic ] ,",
    "we report the results for the ista , fista , gpsr , sparsa , psd and our new algorithm gpss for the case of the gaussian random matrix .",
    "the proposed gpss algorithm compares favorably with the other five , especially for small values of @xmath8 .",
    "experiments made by varying the parameter @xmath52 showing no significant difference , we report here only the results obtained with @xmath53 ( monotonic line search ) . however , the behavior for large penalties is not clearly visible on figure [ gausspic ] .",
    "it is better demonstrated when using a logarithmic scale for the relative error on the vertical axes as reported in figure [ gausspiclog ] .    in figures",
    "[ geopic ] and [ geopiclog ] , we report the results for the case of the ill - conditioned matrix arising from the seismic inverse problem . clearly , for this operator , ista , gpsr and psd have a lot of difficulty in approaching the minimizer for small values of @xmath8 ( lines not approaching @xmath109 ) .",
    "the fista algorithm appears to work best for small penalty parameters whereas gpss and sparsa compete for the second place in such instance . from figure [ geopiclog ]",
    ", we see that the gpss and sparsa algorithms are performing best for large values of @xmath8 .",
    "the reported encouraging numerical results call of course for further experiments , but we believe that they are sufficiently representative to allow honest extrapolation to reliable conclusions holding more generally . as seen , the proposed gpss algorithm performs well for the compressed sensing problem : for small values of @xmath8 , it clearly outperforms the other algorithms ( see figure [ gausspic ] ) whereas it is still competitive for larger values of @xmath8 . in the ill - conditioned inversion problem ,",
    "gpss an sparsa appear to perform better than all other tested algorithms for large values of @xmath8 , whereas they are challenged by the fista method for smaller values .",
    "i.l . and c.d.m .",
    "are supported by grant goa-062 of the vub .",
    "i.l . is supported by grant g.0564.09n of the fwo - vlaanderen .",
    "m.b . , r.z . and l.z .",
    "are partly supported by mur grant 2006018748 .",
    "figueiredo , r.d .",
    "nowak , s.j .",
    "wright , gradient projection for sparse reconstruction : application to compressed sensing and other inverse problems , ieee j. selected topics in signal process . 1 ( 2007 ) 586597 ."
  ],
  "abstract_text": [
    "<S> we propose a new gradient projection algorithm that compares favorably with the fastest algorithms available to date for @xmath0-constrained sparse recovery from noisy data , both in the compressed sensing and inverse problem frameworks . </S>",
    "<S> the method exploits a line - search along the feasible direction and an adaptive steplength selection based on recent strategies for the alternation of the well - known barzilai - borwein rules . the convergence of the proposed approach is discussed and a computational study on both well - conditioned and ill - conditioned problems is carried out for performance evaluations in comparison with five other algorithms proposed in the literature . </S>"
  ]
}