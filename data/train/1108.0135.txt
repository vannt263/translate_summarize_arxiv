{
  "article_text": [
    "for any positive integer x ,  mobius function ",
    "@xmath8 is defined as :    * 0 , if x is divisible by a square of any prime    * 1 , if the factorization of x contains an even number of distinct primes    * -1 , otherwise    mertens function of a positive integer x is defined as the sum of values of the mobius function for 1 to x :    @xmath9    this function scales roughly as @xmath10 over the interval where it s been studied . an old hypothesis , `` mertens conjecture '' ( stieltjes , 1885 ) proposed that @xmath11 for all n. this was disproved by odlyzko and te riele ( 1985 )  @xcite , though no explicit counterexample is known .    the largest known value of @xmath12 , 0.570591 at n=7766842813 , was found in  @xcite .",
    "values of m(n ) for all consecutive n up to @xmath13 were computed in  @xcite .",
    "no larger values were found .    a computer algorithm to calculate isolated values of m(n ) in @xmath14 time",
    "was described in  @xcite and used to calculate several values up to @xmath15 .",
    "the algorithm described next is , in a sense , a variation of the algorithm by  @xcite .",
    "the running time is still @xmath14 .",
    "a distinctive feature of this algorithm is that executing it at any value of n allows to simultaneously compute all values of @xmath16 for integer @xmath17 for all integer @xmath18 .",
    "the starting point for the algorithm is a standard identity :    @xmath19    or , equivalently ,    @xmath20    it s been noted in  @xcite that the sequence @xmath21 takes on only @xmath22 distinct values .",
    "we can rewrite the right - hand side as follows    @xmath23    where @xmath24 .",
    "now notice that the right hand side can be calculated recursively . in order to calculate m(n ) , we need to know m(n/2 ) , m(n/3 ) , m(n/4 ) , .... in turn , m(n/2 ) requires m(n/4 ) , m(n/6 ) , etc .",
    "suppose that we pick a value of @xmath25 and create an array of @xmath26 elements , which would end up holding values of @xmath27 for @xmath28 .",
    "we then calculate all @xmath16 for @xmath29 in order by direct sieving ( this would take @xmath30 time ) . for each @xmath31",
    ", we update the array to account for contributions of @xmath16 according to the formula above .",
    "it s not hard to see that updating takes @xmath32 time .",
    "the overall time for these two tasks is minimized at @xmath33 and it also scales as @xmath14 .    finally , when we are done sieving , it is a simple task to compute final values of m , and it takes negligible @xmath34 time .",
    "furthermore , it is easy to see that , if we want to compute @xmath5 for multiple close values of @xmath6 , we only need to do sieving once .",
    "adjusting @xmath35 will result in optimal running time of @xmath36 , where @xmath37 is the number of values @xmath6 for which we re trying to calculate @xmath5 .",
    "the structure of the algorithm , with many large elements in the array that can be computed in parallel and more or less independently from each other , suggests that the algorithm may benefit from optimization for a general - purpose gpu ( e.g. nvidia fermi ) .",
    "since u is much larger than the amount of computer memory ( for example , for @xmath38 , @xmath39 , but modern desktop computers rarely have more than @xmath40 bytes of memory ) , it is necessary to divide calculations into blocks of y and to handle each block in order .    for each block ,",
    "the first step is to compute consecutive values of @xmath16 throughout by sieving .",
    "duplicate operations are avoided , and optimal performance is achieved if the block is greater than @xmath41 . the nature of this computation is such that it involves a large number of random memory accesses across the entire block , and this is not a task that s particularly suitable for gpu optimization ( among other reasons , the cpu has much larger l1 and l2 caches ) .",
    "therefore , this task is done on the cpu .",
    "a naive algorithm that performs sieving in a block @xmath42 $ ] can be described as follows :    \\1 . create an array a of integers with @xmath43 elements , corresponding to @xmath44 , @xmath45 , ... , @xmath46 .",
    "set these all to 1 initially .",
    "every prime number @xmath47 between 2 and @xmath48 , multiply elements in the array corresponding to multiples of @xmath47 by @xmath49 .",
    "\\3 . for every such prime ,",
    "set all elements in the array corresponding to multiples of @xmath50 to 0 .",
    "go through the array .",
    "for every element @xmath31 , calculate the mobius function @xmath51 :    - if @xmath52 $ ] is 0 , then @xmath51 is 0    - otherwise , if @xmath53|$ ] is equal to @xmath31 , then @xmath51 is equal to the sign of @xmath52 $ ]    - otherwise , @xmath51 is equal to minus the sign of @xmath52 $ ]    \\5 .",
    "add up all values of @xmath51 to calculate @xmath16 .",
    "the number of operations , according to the prime number theorem , is @xmath54 .",
    "while there s not much that can be done about the overall growth rate ( obviously , the task ca nt be done faster than @xmath55 , and the log factor only plays a minor role , since , even for @xmath56 , @xmath57 ) , we can improve the constant factor .",
    "one significant improvement can be achieved by modifying the temporary array to store approximate logarithms of prime products instead of exact values .",
    "this serves two purposes : it replaces each integer multiplication with an integer addition ( which is often substantially faster ) ; and it reduces the memory throughput of the algorithm by a factor of 8 , because the array a in the range of values we re interested in has to contain 64-bit values , but base-2 logarithms can be stored in 8-bit variables .",
    "this is the modified algorithm :    \\0 .",
    "( preparation , has to be done once ) create an array of log - primes : for each prime @xmath58 , the corresponding log - prime @xmath59 ( here @xmath60 is binary `` or '' . )    \\1 .",
    "create an array a of 8-bit integers with @xmath43 elements , corresponding to @xmath44 , @xmath45 , ... , @xmath46 .",
    "set these all to 0 initially .",
    "every prime number @xmath47 between 2 and @xmath48 , add @xmath61 to all elements in the array corresponding to multiples of @xmath47 .",
    "\\3 . for every such prime , set the most significant bit ( by performing binary or with 0x80 ) in all elements corresponding to multiples of @xmath50 within the block .",
    "go through the array .",
    "for every element @xmath31 , calculate the mobius function @xmath51 :    - if the most significant bit of @xmath52 $ ] is set , then @xmath51 is 0 - otherwise , if @xmath52 $ ] is greater than @xmath62 , then @xmath51 is equal to @xmath63 \\operatorname{\\ & } 1)$ ] ( here `` & '' is binary `` and '' ) .    - otherwise , @xmath51 is equal @xmath64 \\operatorname{\\ & } 1)$ ]    \\5 .",
    "add up all values of @xmath51 to calculate @xmath16 .",
    "it can be proved that sieving according to the algorithm will result in correct results , as long as @xmath46 is less than approximately @xmath65 .",
    "further , we can precompute results of sieving with the first several primes and square primes , storing them in separate array , and copying this array instead of rerunning sieving . for example",
    ", the state of the temporary array after sieving with the first 4 primes and the first 2 square primes is periodic with the period of 2 * 2 * 3 * 3 * 5 * 7 * 11 = 13860 .",
    "finally , the whole process can be modified to make use of multiple cpu cores , through the use of openmp .    to perform the array updating in the minimal amount of time",
    ", we have to divide the ( x , y ) space into several distinct regions and process each region using a different procedure .    in regions 1 and 1",
    "( defined as @xmath66 for some @xmath67 ) , almost every ( x , y ) pair makes a nonzero contribution to one of the elements of the array .",
    "adequate performance can be achieved by creating one gpu thread to handle a region with dimensions ( 1,512 ) and to add up contributions consecutively from every pair .",
    "an important factor that slows down computations in these regions is the necessity to perform at least one integer 64-bit division for every ( x , y ) pair .",
    "integer division is an extremely slow operation that is not supported natively by the gpu and can take as much as a hundred clock cycles .",
    "( it s also quite slow on the cpu , but less so . )",
    "to improve performance , we can apply an algorithm such as  @xcite , which accelerates integer division by replacing it with integer multiplication , addition , and shift .",
    "it requires that we precompute several constants for each value of the divisor , but , as long as each divisor is used multiple times , this results in net reduction of computation time .",
    "we can precompute all constants in advance for divisors up to some constant c , where c is only limited by the amount of available memory .",
    "this will result in substantial speedup as long as @xmath68 .    in regions 2 and 2 ( defined as @xmath69 where @xmath70 )",
    ", there is a substantial number of ( x , y ) pairs which do nt contribute anything , but the memory access pattern is relatively dense .",
    "good performance is achieved when each gpu thread handles a region with dimensions ( 1,8192 ) .    in the region 3 ,",
    "nonzero contributing pairs are sparse and processing time is limited by random memory access bandwidth .    in the region 4 ( defined as @xmath71 , where @xmath72 ) , accesses are so sparse that it s not even justified to transfer results of the sieving onto the gpu any more : time spent transferring data over the pci - e bus is larger than time that would be spent processing the same data with the cpu . at this point",
    "we no longer need the table of divisors used to speed up regions 1 and 2 , so , if the amount of memory is a bottleneck , we can free that memory and increase the sieving block size up to several times @xmath41 ( this is the only region where the ratio of block size to @xmath41 significantly greater than 1 is warranted ) .",
    "a different algorithm that can be employed in this task is based on the following remarkable formula :    @xmath73 ( 1 )    or in a simple yet practical reformulation ,    @xmath74 ( 2 )    where    @xmath75 @xmath76 @xmath77    and @xmath78 is the ith zero of the riemann zeta function .",
    "somewhat more sophisticated reformulations , constructed by inserting a nontrivial kernel @xmath79 inside the sum , are mentioned in the literature , but the formula above is particularly easy to calculate , and differences between different kernels for large @xmath80 are minor .",
    "although this formula is only approximate , it s usefulness here stems from the fact that we can get a good approximation - for example , two significant digits of @xmath2 - with just 1000 terms , regardless of the magnitude of x.    calculating @xmath81 is also a task that nicely affords to gpu optimization . in this case",
    ", the implementation is relatively straightforward .",
    "the only nuance that needs addressing involves the use of single - precision vs. double - precision floating point .",
    "modern gpus are generally faster when operating in single precision than in double precision .",
    "this is particularly true with regard to the calculation of the cosine .",
    "all modern gpus have native hardware support of basic single - precision transcendental functions such as the cosine .",
    "gpus in nvidia fermi family , for example , have the throughput of 4 or 8 single - precision cosines per clock cycle per multiprocessor : the nvidia gtx 560 can , in theory , exceed @xmath82 cosines per second ( compare with modern desktop cpu s , which can only manage on the order of @xmath83 cosines / second ) . on the other hand , double - precision cosines are not natively supported by nvidia or amd and they can be a factor of 20 or so slower .",
    "but the algorithm we have here ca nt be implemented completely in single precision , unless both n and x are small .",
    "to test the algorithm described in this article , a four - core intel i5 - 2500 cpu running at 4.4 ghz with 16 gb of memory and a nvidia geforce gtx 560 video card ( 384 processing cores at 1.7 ghz ) with 1 gb of memory were used .",
    "several choices of operating systems are available in this hardware configuration . of these ,",
    "x64 linux ( e.g. ubuntu ) is notable , because it comes with a c / c++ compiler ( gcc / g++ ) that natively supports a 128-bit integer type .",
    "since we re trying to compute m(n ) for n above the 64-bit limit ( @xmath84 ) , native 128-bit support substantially simplifies the code .",
    "all the various constants and thresholds involved in this algorithm ( such as dimensions handled by each gpu thread , or boundaries between regions 1/1 and 2/2 ) were tuned for peak performance . to verify correctness , many small values in @xmath85 range were computed and results were compared with results obtained by naive o(n ) time sieving .",
    "values of mertens function for @xmath86 , and @xmath15 were compared against previously reported in the literature  @xcite .",
    "calculating @xmath87 takes 1 minute : a substantial improvement , compared to the previously reported 1996 result where calculation for the same n took two days .",
    "( it should be noted that only a small part of this improvement is due to an overall increase in clock frequencies : the 1996 analysis employed a cpu running at 0.3 ghz , only one order of magnitude slower than hardware used here .",
    "most of it is due to substantial parallelization and larger cache sizes . )    for higher n , direct verification is problematic , but results can be compared with approximate values . in order to make use of the approximate algorithm , the list of the first 2,000,000 zeros of riemann zeta was downloaded from  @xcite .",
    "zeros were then refined to 30 digits and corresponding values of @xmath88 were computed using mathematica . to ensure correctness ,",
    "these values for the first 200,000 zeros were independently computed to 30 digits using the arbitrary - precision floating point arithmetic library `` mpmath '' ver .",
    "0.17  @xcite , and results were checked against each other to confirm the lack of major discrepancies .",
    "approximations of m(n ) for many large values of n were calculated .",
    "these turned out to be in agreement with exact calculations : the residual error between the exact and the approximate values of @xmath89 for @xmath90 was approximately normally distributed with the standard deviation of @xmath91 .",
    "finally , the code was used to compute values of m(n ) for powers of 10 from @xmath92 to @xmath1 . as expected ,",
    "execution time scaled roughly as @xmath93 .",
    "the computation for @xmath1 took 10 days .",
    "as described in the beginning of the article , computing @xmath94 simultaneously yields values of @xmath16 for integer @xmath95 for all integer c , including values such as @xmath96 and @xmath97 ; these agreed with their direct computations .",
    "results are listed in table 1 ."
  ],
  "abstract_text": [
    "<S> a gpu implementation of an algorithm to compute the mertens function in @xmath0 time is discussed . </S>",
    "<S> results for x up to @xmath1 , and a new extreme value for @xmath2 , -0.585768 ( @xmath3 at @xmath4 ) , are reported.an approximate algorithm is used to examine values of @xmath5 for @xmath6 up to @xmath7 .    </S>",
    "<S> 2.0 cm    * computing the mertens function on a gpu *    eugene kuznetsov </S>"
  ]
}