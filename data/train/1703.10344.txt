{
  "article_text": [
    "wikipedia is the largest source of open and collaboratively curated knowledge in the world",
    ". introduced in 2001 , it has evolved into a reference work with around 5 m pages for the english wikipedia alone .",
    "in addition , entities and event pages are updated quickly via collaborative editing and all edits are encouraged to include source citations , creating a knowledge base which aims at being both timely as well as authoritative . as a result , it has become the preferred source of information consumption about entities and events .",
    "moreso , this knowledge is harvested and utilized in building knowledge bases like yago  @xcite and dbpedia  @xcite , and used in applications like text categorization  @xcite , entity disambiguation  @xcite , entity ranking  @xcite and distant supervision  @xcite .",
    "however , not all wikipedia pages referring to entities ( entity pages ) are comprehensive : relevant information can either be _ missing _ or added with a _",
    "delay_. consider the city of _ new orleans _ and the state of _ odisha _ which were severely affected by cyclones _",
    "hurricane katrina _ and _ odisha cyclone _ , respectively .",
    "katrina _ finds extensive mention in the entity page for _ new orleans _ , _",
    "odisha cyclone _ which has 5 times more human casualties ( cf",
    ". figure  [ fig : cyclone ] ) is not mentioned in the page for _",
    "odisha_. arguably _ katrina _ and _",
    "new orleans _ are more popular entities , but _ odisha cyclone _ was also reported extensively in national and international news outlets .",
    "this highlights the lack of important facts in trunk and long - tail entity pages , even in the presence of relevant sources .",
    "in addition , previous studies have shown that there is an inherent delay or lag when facts are added to entity pages  @xcite .    to remedy these problems ,",
    "it is important to identify information sources that contain novel and salient facts to a given entity page .",
    "however , not all information sources are equal .",
    "the online presence of major news outlets is an authoritative source due to active editorial control and their articles are also a timely container of facts . in addition , their use is in line with current wikipedia editing practice , as is shown in  @xcite that almost 20% of current citations in all entity pages are news articles .",
    "we therefore propose _ news suggestion _ as a novel task that enhances entity pages and reduces delay while keeping its pages authoritative .",
    "existing efforts to populate wikipedia  @xcite start from an entity page and then generate candidate documents about this entity using an external search engine ( and then post - process them ) .",
    "however , such an approach lacks in ( a ) reproducibility since rankings vary with time with obvious bias to recent news ( b ) maintainability since document acquisition for each entity has to be periodically performed . to this effect",
    ", our news suggestion considers a news article as input , and determines if it is valuable for wikipedia .",
    "specifically , given an input news article @xmath0 and a state of wikipedia , the news suggestion problem identifies the entities mentioned in @xmath0 whose entity pages can improve upon suggesting @xmath0 .",
    "most of the works on knowledge base acceleration  @xcite , or wikipedia page generation  @xcite rely on high quality input sources which are then utilized to extract textual facts for wikipedia page population . in this work ,",
    "we do not suggest snippets or paraphrases but rather entire articles which have a high potential importance for entity pages . these suggested news articles could be consequently used for extraction , summarization or population either manually or automatically  all of which rely on high quality and relevant input sources .",
    "we identify four properties of good news recommendations : _ salience _ , _ relative authority _ , _ novelty _ and",
    "_ placement_. first , we need to identify the most salient entities in a news article .",
    "this is done to avoid pollution of entity pages with only marginally related news .",
    "second , we need to determine whether the news is important to the entity as only the most relevant news should be added to a precise reference work . to do this",
    ", we compute the _ relative authority _ of all entities in the news article : we call an entity more authoritative than another if it is more popular or noteworthy in the real world . entities with very high authority have many news items associated with them and only the most relevant of these should be included in wikipedia whereas for entities of lower authority the threshold for inclusion of a news article will be lower .",
    "third , a good recommendation should be able to identify _ novel _ news by minimizing redundancy coming from multiple news articles .",
    "finally , addition of facts is facilitated if the recommendations are fine - grained , i.e. , recommendations are made on the section level rather than the page level ( _ placement _ ) .    *",
    "approach and contributions .",
    "* we propose a two - stage news suggestion approach to entity pages . in the first stage , we determine whether a news article should be suggested for an entity , based on the entity s _ salience _ in the news article , its _ relative authority _ and the _ novelty _ of the article to the entity page .",
    "the second stage takes into account the class of the entity for which the news is suggested and constructs _ section templates _ from entities of the same class .",
    "the generation of such templates has the advantage of suggesting and expanding entity pages that do not have a complete section structure in wikipedia , explicitly addressing long - tail and trunk entities .",
    "afterwards , based on the constructed template our method determines the best fit for the news article with one of the sections .",
    "we evaluate the proposed approach on a news corpus consisting of 351,982 articles crawled from the _ news _ external references in wikipedia from 73,734 entity pages .",
    "given the wikipedia snapshot at a given year ( in our case [ 2009 - 2014 ] ) , we suggest news articles that might be cited in the coming years .",
    "the existing news references in the entity pages along with their reference date act as our ground - truth to evaluate our approach . in summary",
    ", we make the following contributions .",
    "* we propose a two - stage news suggestion approach for wikipedia entity pages .",
    "* we adopt and address the problem of determining whether a news article should be referenced to an entity considering the entity _ salience _ , _ relative authority _ and _ novelty _ of the article for the entity page .",
    "* we are able to place articles in a specific section of the entity page . through _",
    "section templates _",
    ", we address the problems of entities with a limited section structure by class - based generalization i.e. we can expand entity pages with sections that come from entities of a similar class . * an extensive evaluation on 351,982 news articles and 73,734 entity pages , using their state for the years [ 2009 - 2013 ] .",
    "as we suggest a new problem there is no current work addressing exactly the same task .",
    "however , our task has similarities to wikipedia page generation and knowledge base acceleration .",
    "in addition , we take inspiration from natural language processing ( nlp ) methods for salience detection .    * wikipedia page generation * is the problem of populating wikipedia pages with content coming from external sources .",
    "sauper and barzilay @xcite propose an approach for automatically generating whole entity pages for specific entity classes .",
    "the approach is trained on already - populated entity pages of a given class ( e.g. ` _ _ diseases _ _ ' ) by learning templates about the entity page structure ( e.g. diseases have a _ treatment _ section ) . for a new entity page , first , they extract documents via web search using the entity title and the section title as a query , for example ` _ _ lung cancer__'+`__treatment _ _ ' .",
    "as already discussed in the introduction , this has problems with reproducibility and maintainability .",
    "however , their main focus is on identifying the best paragraphs extracted from the collected documents .",
    "they rank the paragraphs via an optimized supervised _ perceptron model _ for finding the most representative paragraph that is the least similar to paragraphs in other sections .",
    "this paragraph is then included in the newly generated entity page .",
    "taneva and weikum  @xcite propose an approach that constructs short summaries for the long tail .",
    "the summaries are called ` _",
    "_ gems _ _ ' and the size of a ` _ _ gem _ _ ' can be user defined .",
    "they focus on generating summaries that are novel and diverse .",
    "however , they do not consider any structure of entities , which is present in wikipedia .",
    "in contrast to @xcite and @xcite , we actually focus on suggesting entire documents to wikipedia entity pages .",
    "these are authoritative documents ( news ) , which are highly relevant for the entity , novel for the entity and in which the entity is salient .",
    "whereas relevance in sauper and barzilay is implicitly computed by web page ranking we solve that problem by looking at relative authority and salience of an entity , using the news article and entity page only . as sauper and barzilay",
    "concentrate on empty entity pages , the problem of novelty of their content is not an issue in their work whereas it is in our case which focuses more on updating entities .",
    "updating entities will be more and more important the bigger an existing reference work is .",
    "both the approaches in  @xcite and @xcite ( finding paragraphs and summarization ) could then be used to process the documents we suggest further .",
    "our concentration on news is also novel",
    ".    * knowledge base acceleration . * in this task , given specific information extraction templates , a given corpus is analyzed in order to find worthwhile mentions of an entity or snippets that match the templates .",
    "balog @xcite recommend news citations for an entity .",
    "prior to that , the news articles are classified for their appropriateness for an entity , where as features for the classification task they use entity , document , entity - document and temporal features .",
    "the best performing features are those that measure similarity between an entity and the news document .",
    "west et al .",
    "@xcite consider the problem of knowledge base completion , through question answering and complete missing facts in freebase based on templates , i.e. _ frank_zappa _ ` bornin ` _ baltymore , maryland_.    in contrast , we do not extract facts for pre - defined templates but rather suggest news articles based on their relevance to an entity . in cases of long - tail entities ,",
    "we can suggest to add a novel section through our abstraction and generation of section templates at entity class level .",
    "* entity salience . *",
    "determining which entities are prominent or salient in a given text has a long history in nlp , sparked by the linguistic theory of centering  @xcite .",
    "salience has been used in pronoun and co - reference resolution @xcite , or to predict which entities will be included in an abstract of an article @xcite .",
    "frequent features to measure salience include the frequency of an entity in a document , positioning of an entity , grammatical function or internal entity structure ( pos tags , head nouns etc . ) .",
    "these approaches are not currently aimed at knowledge base generation or wikipedia coverage extension but we postulate that an entity s salience in a news article is a prerequisite to the news article being relevant enough to be included in an entity page .",
    "we therefore use the salience features in  @xcite as part of our model .",
    "however , these features are document - internal  we will show that they are not sufficient to predict news inclusion into an entity page and add features of entity authority , news authority and novelty that measure the relations between several entities , between entity and news article as well as between several competing news articles .",
    "we are interested in named entities mentioned in documents .",
    "an entity @xmath1 can be identified by a canonical name , and can be _ mentioned _ differently in text via different _",
    "surface forms_. we canonicalize these mentions to entity pages in wikipedia , a method typically known as _ entity linking_. we denote the set of canonicalized entities extracted and linked from a news article @xmath0 as @xmath2 .",
    "for example , in figure  [ fig : approach ] , entities are canonicalized into wikipedia entity pages ( e.g. _ odisha _ is canonicalized to the corresponding article ) . for a collection of news articles",
    "@xmath3 , we further denote the resulting set of entities by @xmath4 .",
    "information in an entity page is organized into sections and evolves with time as more content is added .",
    "we refer to the state of wikipedia at a time @xmath5 as @xmath6 and the set of sections for an entity page @xmath1 as its _ entity profile _ @xmath7 .",
    "unlike news articles , text in wikipedia could be explicitly linked to entity pages through anchors .",
    "the set of entities explicitly referred in text from section @xmath8 is defined as @xmath9 .",
    "furthermore , wikipedia induces a category structure over its entities , which is exploited by knowledge bases like yago ( e.g. _ barack_obama _ ` isa person ` ) .",
    "consequently , each entity page belongs to one or more entity categories or classes @xmath10 .",
    "now we can define our news suggestion problem below :    given a set of news articles @xmath11 and set of wikipedia entity pages @xmath12 ( from @xmath6 ) we intend to suggest a news article @xmath0 published at time @xmath13 to entity page @xmath1 and additionally to the most relevant section for the entity page @xmath8 .",
    "we approach the news suggestion problem by decomposing it into two tasks :    1 .",
    "_ aep _ : _ article  entity _ placement 2 .",
    "_ asp _ : _ article  section _ placement    in this first step , for a given entity - news pair @xmath14 , we determine whether the given news article @xmath15 should be suggested ( we will refer to this as _",
    "` relevant ' _ ) to entity @xmath16 . to generate such @xmath17 pairs ,",
    "we perform the _ entity linking _ process , @xmath2 , for @xmath0 .",
    "the _ article  entity _ placement task ( described in detail in section  [ subsec : article_linking ] ) for a pair @xmath17 outputs a binary label ( either _ ` non - relevant ' _ or _ ` relevant ' _ ) and is formalized in equation  [ eq : article_entity ] .",
    "@xmath18    in the second step , we take into account all _ ` relevant ' _ pairs @xmath17 and find the correct _ section _ for article @xmath0 in entity @xmath1 , respectively its profile @xmath7 ( see section  [ subsec : section_linking ] ) .",
    "the _ article  section _ placement task , determines the correct section for the triple @xmath19 , and is formalized in equation  [ eq : article_section ] .",
    "@xmath20    in the subsequent sections we describe in details how we approach the two tasks for suggesting news articles to entity pages .",
    "in this section , we provide an overview of the _ news suggestion _ approach to wikipedia entity pages ( see figure  [ fig : approach ] ) .",
    "the approach is split into two tasks : ( i ) _ article - entity _ ( _ aep _ ) and ( ii ) _ article - section _ ( _ asp _ ) placement . for a wikipedia snapshot @xmath6 and a news corpus @xmath3 ,",
    "we first determine which news articles should be suggested to an entity @xmath1 .",
    "we will denote our approach for _ aep _ by @xmath21 .",
    "finally , we determine the most appropriate section for the _ asp _ task and we denote our approach with @xmath22 .    in the following , we describe the process of learning the functions @xmath21 and @xmath22 .",
    "we introduce features for the learning process , which encode information regarding the entity _ salience _ , _ relative authority _ and _ novelty _ in the case of aep task . for the _ asp _ task , we measure the _ overall fit _ of an article to the entity sections , with the entity being an input from _ aep _ task . additionally , considering that the entity profiles @xmath7 are incomplete , in the case of a missing section we suggest and expand the entity profiles based on _ section templates _ generated from entities of the same class @xmath10 ( see section  [ subsubsec : as_sectiontemplates ] ) .      in this step",
    "we learn the function @xmath21 to correctly determine whether @xmath0 should be suggested for @xmath1 , basically a binary classification model ( 0=__`non - relevant ' _ _ and 1=__`relevant ' _ _ ) .",
    "note that we are mainly interested in finding the _ relevant _ pairs in this task . for every news article",
    ", the number of disambiguated entities is around 30 ( but @xmath0 is suggested for only two of them on average ) .",
    "therefore , the distribution of _ ` non - relevant ' _ and _ ` relevant ' _ pairs is skewed towards the earlier , and by simply choosing the _ ` non - relevant ' _ label we can achieve a high accuracy for @xmath21 . finding the relevant pairs is therefore a considerable challenge .",
    "an article @xmath0 is suggested to @xmath1 by our function @xmath21 if it fulfills the following properties .",
    "the entity @xmath1 is _ salient _ in @xmath0 ( a central concept ) , therefore ensuring that @xmath0 is about @xmath1 and that @xmath1 is important for @xmath0 .",
    "next , given the fact there might be many articles in which @xmath1 is _ salient _ , we also look at the reverse property , namely whether @xmath0 is important for @xmath1 .",
    "we do this by comparing the _ authority _ of @xmath1 ( which is a measure of popularity of an entity , such as its frequency of mention in a whole corpus ) with the authority of its co - occurring entities in @xmath2 , leading to a feature we call _ relative authority_. the intuition is that for an entity that has overall lower authority than its co - occurring entities , a news article is more easily of importance . finally , if the article we are about to suggest is already covered in the entity profile @xmath7 , we do not wish to suggest _ redundant _ information , hence the _ novelty_. therefore , the learning objective of @xmath21 should fulfill the following properties .",
    "table  [ tbl : importance_salience ] shows a summary of the computed features for @xmath21 .    1 .",
    "* salience : * entity @xmath1 should be a _",
    "salient _ entity in news article @xmath0 2 .",
    "* relative authority : * the set of entities @xmath23 with which @xmath1 co - occurs should have higher _ authority _ than @xmath1 , making @xmath0 important for @xmath1 3 .",
    "* novelty : * news article @xmath0 should provide _ novel _ information for entity @xmath1 taking into account its profile @xmath24      * baseline features . * as discussed in section  [ sec : related - work ] , a variety of features that measure salience of an entity in text are available from the nlp community .",
    "we reimplemented the ones in dunietz and gillick  @xcite .",
    "this includes a variety of features , e.g. positional features , occurrence frequency and the internal pos structure of the entity and the sentence it occurs in .",
    "table 2 in @xcite gives details .",
    "* relative entity frequency .",
    "* although frequency of mention and positional features play some role in baseline features , their interaction is not modeled by a single feature nor do the positional features encode more than sentence position .",
    "we therefore suggest a novel feature called _ relative entity frequency _ , @xmath25 , that has three properties .",
    ": ( i ) it rewards entities for occurring throughout the text instead of only in some parts of the text , measured by the number of paragraphs it occurs in ( ii ) it rewards entities that occur more frequently in the opening paragraphs of an article as we model @xmath25 as an _ exponential decay _ function .",
    "the decay corresponds to the positional index of the news paragraph .",
    "this is inspired by the news - specific discourse structure that tends to give short summaries of the most important facts and entities in the opening paragraphs .",
    "( iii ) it compares entity frequency to the frequency of its co - occurring mentions as the weight of an entity appearing in a specific paragraph , normalized by the sum of the frequencies of other entities in @xmath2 .",
    "@xmath26 where , @xmath27 represents a news paragraph from @xmath0 , and with @xmath28 we indicate the set of all paragraphs in @xmath0 . the frequency of @xmath1 in a paragraph @xmath27",
    "is denoted by @xmath29 . with @xmath30 and @xmath31",
    "we indicate the number of paragraphs in which entity @xmath1 occurs , and the total number of paragraphs , respectively",
    ".      * relative authority . * in this case , we consider the comparative relevance of the news article to the different entities occurring in it . as an example , let us consider the meeting of the sudanese bishop _",
    "_ elias taban _ _ with _ _ hillary clinton__. both entities are salient for the meeting .",
    "however , in taban s wikipedia page , this meeting is discussed prominently with a corresponding news reference , whereas in hillary clinton s wikipedia page it is not reported at all .",
    "we believe this is not just an omission in clinton s page but mirrors the fact that for the lesser known taban the meeting is big news whereas for the more famous clinton these kind of meetings are a regular occurrence , not all of which can be reported in what is supposed to be a selection of the most important events for her . therefore ,",
    "if two entities co - occur , the news is more relevant for the entity with the lower a priori authority .    the _ a priori authority _ of an entity ( denoted by @xmath32 ) can be measured in several ways .",
    "we opt for two approaches : ( i ) probability of entity @xmath1 occurring in the corpus @xmath3 , and ( ii ) authority assessed through centrality measures like pagerank  @xcite . for the second case",
    "we construct the graph @xmath33 consisting of entities in @xmath34 and news articles in @xmath3 as _ vertices_. the _ edges _ are established between @xmath0 and entities in @xmath2 , that is @xmath35 , and the out - links from @xmath1 , that is @xmath36 ( arrows present the _ edge _ direction ) .    starting from a priori authority ,",
    "we proceed to _ relative authority _ by comparing the a priori authority of co - occurring entities in @xmath2 .",
    "we define the _ relative authority _ of @xmath1 as the proportion of co - occurring entities @xmath37 that have a higher a priori authority than @xmath1 ( see equation  [ eq : avg_authority ] . @xmath38 as we might run the danger of not suggesting any news articles for entities with very high a priori authority ( such as clinton ) due to the strict inequality constraint , we can relax the constraint such that the authority of co - occurring entities is above a certain threshold .",
    "* news domain authority .",
    "* the news domain authority addresses two main aspects .",
    "firstly , if bundled together with the _ relative authority _ feature , we can ensure that dependent on the entity authority , we suggest news from authoritative sources , hence ensuring the quality of suggested articles .",
    "the second aspect is in a news streaming scenario where multiple news domains report the same event  ideally only articles coming from authoritative sources would fulfill the conditions for the news suggestion task .",
    "the _ news domain _ authority is computed based on the number of news references in wikipedia coming from a particular _ news domain _ @xmath39 .",
    "this represents a simple prior that a news article @xmath0 is from domain @xmath39 in corpus @xmath3 .",
    "we extract the domains by taking the base urls from the news article urls .",
    "an important feature when suggesting an article @xmath0 to an entity @xmath1 is the _ novelty _ of @xmath0 w.r.t the already existing entity profile @xmath24 .",
    "studies  @xcite have shown that on comparable collections to ours ( trec gov2 ) the number of duplicates can go up to @xmath40 .",
    "this figure is likely higher for major events concerning highly authoritative entities on which all news media will report .",
    "given an entity @xmath1 and the already added news references @xmath41 up to year @xmath42 , the _ novelty _ of @xmath43 at year @xmath5 is measured by the kl divergence between the language model of @xmath43 and articles in @xmath44 .",
    "we combine this measure with the _ entity _ overlap of @xmath43 and @xmath45 .",
    "the _ novelty _",
    "value of @xmath43 is given by the minimal divergence value .",
    "low scores indicate low novelty for the entity profile @xmath7 .    @xmath46    where @xmath47 is the kl divergence of the language models ( @xmath48 and @xmath49 ) , whereas @xmath50 is the mixing weight ( @xmath51 ) between the language models @xmath47 and the entity overlap in @xmath0 and @xmath52 .",
    "we model the _ asp _ placement task as a successor of the _ aep _ task .",
    "for all the _ ` relevant ' _ news entity pairs , the task is to determine the correct entity section .",
    "each section in a wikipedia entity page represents a different topic . for example , _ barack obama _ has the sections _ ` early life ' , ` presidency ' , ` family and personal life ' _ etc",
    ". however , many entity pages have an incomplete section structure . incomplete or missing sections are due to two wikipedia properties .",
    "first , long - tail entities miss information and sections due to their lack of popularity .",
    "second , for all entities whether popular or not , certain sections might occur for the first time due to real world developments . as an example , the entity _ germanwings _ did not have an _",
    "` accidents ' _ section before this year s disaster , which was the first in the history of the airline .",
    "even if sections are missing for certain entities , similar sections usually occur in other entities of the same class ( e.g. other airlines had disasters and therefore their pages have an accidents section ) .",
    "we exploit such homogeneity of section structure and construct templates that we use to expand entity profiles .",
    "the learning objective for @xmath22 takes into account the following properties :    1 .",
    "* section - templates : * account for incomplete section structure for an entity profile @xmath7 by constructing section templates @xmath53 from an entity class @xmath10 2 .   *",
    "overall fit : * measures the overall fit of a news article to sections in the section templates @xmath53      given the fact that _ entity profiles _ are often incomplete , we construct _ section templates _ for every _ entity class_. we group entities based on their class @xmath10 and construct _ section templates _ @xmath53 . for different entity classes , e.g. ` person ` and ` location ` , the section structure and the information represented in those section varies heavily .",
    "therefore , the section templates are with respect to the individual classes in our experimental setup ( see figure  [ fig : entity_distribution ] ) .",
    "@xmath54    generating _ section templates _ has two main advantages .",
    "firstly , by considering class - based profiles , we can overcome the problem of incomplete individual entity profiles and thereby are able to suggest news articles to sections that do not yet exist in a specific entity @xmath7 .",
    "the second advantage is that we are able to canonicalize the sections , i.e. _ ` early life ' _ and _ ` early life and childhood ' _ would be treated similarly .    to generate the section template @xmath53",
    ", we extract all sections from entities of a given type @xmath10 at year @xmath5 .",
    "next , we cluster the entity sections , based on an extended version of _ k  means _ clustering @xcite , namely _ x  means _ clustering introduced in pelleg et al . which estimates the number of clusters efficiently @xcite . as a similarity metric",
    "we use the cosine similarity computed based on the _ tf  idf _ models of the sections . using the _ x ",
    "algorithm we overcome the requirement to provide the number of clusters _ k _ beforehand .",
    "_ x  means _ extends the _ k  means _",
    "algorithm , such that a user only specifies a range [ @xmath55 , @xmath56 that the number of clusters may reasonably lie in .",
    "the learning objective of @xmath22 is to determine the overall fit of a news article @xmath0 to one of the sections in a given section template @xmath53 .",
    "the template is pre - determined by the class of the entity for which the news is suggested as relevant by @xmath21 . in all cases ,",
    "we measure how well @xmath0 fits each of the sections @xmath57 as well as the specific entity section @xmath58 .",
    "the section profiles in @xmath59 represent the aggregated entity profiles from all entities of class @xmath10 at year @xmath42 .    to learn @xmath22 we rely on a variety of features that consider several similarity aspects as shown in table  [ tbl : feature_list ] . for the sake of simplicity",
    "we do not make the distinction in table  [ tbl : feature_list ] between the individual entity section and class - based section similarities , @xmath60 and @xmath61 , respectively . bear in mind that an entity section @xmath62 might be present at year @xmath5 but not at year @xmath42 ( see for more details the discussion on entity profile expansion in section  [ subsubsec : profile_expansion ] ) .",
    "* we use topic similarities to ensure ( i ) that the content of @xmath0 fits topic - wise with a specific section text and ( ii ) that it has a similar topic to previously referred news articles in that section . in a pre - processing stage",
    "we compute the topic models for the news articles , entity sections @xmath24 and the aggregated class - based sections in @xmath63 .",
    "the topic models are computed using lda  @xcite .",
    "we only computed a single topic per article / section as we are only interested in topic term overlaps between article and sections .",
    "we distinguish two main features : the first feature measures the overlap of topic terms between @xmath0 and the entity section @xmath60 and @xmath64 , and the second feature measures the overlap of the topic model of @xmath0 against referred news articles in @xmath44 at time @xmath42 .",
    "* syntactic . *",
    "these features represent a mechanism for conveying the importance of a specific text snippet , solely based on the frequency of specific pos tags ( i.e. ` nnp , cd ` etc . ) , as commonly used in text summarization tasks .",
    "following the same intuition as in @xcite , we weigh the importance of articles by the count of specific pos tags .",
    "we expect that for different sections , the importance of pos tags will vary .",
    "we measure the similarity of pos tags in a news article against the section text . additionally , we consider _ bi - gram _ and _ tri - gram _ pos tag overlap .",
    "this exploits similarity in syntactical patterns between the news and section text .    *",
    "* as _ lexical _ features , we measure the similarity of @xmath0 against the entity section text @xmath60 and the aggregate section text @xmath61 .",
    "further , we distinguish between the overall similarity of @xmath0 and that of the different news paragraphs ( @xmath28 which denotes the paragraphs of @xmath0 up to the 5th paragraph ) . a higher similarity on the first paragraphs represents a more confident indicator that @xmath0 should be suggested to a specific section @xmath65 .",
    "we measure the similarity based on two metrics : ( i ) the kl - divergence between the computed _ language models _ and ( ii ) _ cosine _ similarity of the corresponding paragraph text @xmath28 and section text .",
    "* entity - based .",
    "* another feature set we consider is the overlap of _ named entities _ and their corresponding _",
    "entity classes_. for different entity sections , we expect to find a particular set of entity classes that will correlate with the section , e.g. ` _ _ early life _",
    "_ ' contains mostly entities related to family , school , universities etc",
    ".    * frequency . * finally , we gather statistics about the number of entities , paragraphs , news article length , top@xmath66 entities and entity classes , and the frequency of different pos tags . here",
    "we try to capture patterns of articles that are usually cited in specific sections .",
    "in this section we outline the evaluation plan to verify the effectiveness of our learning approaches . to evaluate the news suggestion problem we are faced with two challenges .    * _ what comprises the ground truth for such a task ? _ * _ how do we construct training and test splits given that entity pages consists of text added at different points in time ? _    consider the ground truth challenge . evaluating if an arbitrary news article should be included in wikipedia",
    "is both subjective and difficult for a human if she is not an expert . an invasive approach , which was proposed by barzilay and sauper  @xcite , adds content directly to wikipedia and expects the editors or other users to redact irrelevant content over a period of time .",
    "the limitations of such an evaluation technique is that content added to long - tail entities might not be evaluated by informed users or editors in the experiment time frame .",
    "it is hard to estimate how much time the added content should be left on the entity page .",
    "a more non - invasive approach could involve crowdsourcing of entity and news article pairs in an ir style relevance assessment setup .",
    "the problem of such an approach is again finding knowledgeable users or experts for long - tail entities . thus",
    "the notion of _ relevance _ of a news recommendation is challenging to evaluate in a crowd setup .    we take a slightly different approach by making an assumption that the news articles already present in wikipedia entity pages are relevant . to this extent ,",
    "we extract a dataset comprising of all news articles referenced in entity pages ( details in section  [ subsec : datasets ] ) . at the expense of not evaluating the space comprising of news articles absent in wikipedia",
    ", we succeed in ( i ) avoiding restrictive assumptions about the quality of human judgments , ( ii ) being invasive and polluting wikipedia , and ( iii ) deriving a reusable test bed for quicker experimentation .",
    "the second challenge of construction of training and test set separation is slightly easier and is addressed in section  [ subsec : test_train ] .",
    "the datasets we use for our experimental evaluation are directly extracted from the wikipedia entity pages and their revision history .",
    "the generated data represents one of the contributions of our paper .",
    "the datasets are the following :    * entity classes .",
    "* we focus on a manually predetermined set of _ entity classes _ for which we expect to have news coverage .",
    "the number of analyzed _ entity classes _ is @xmath67 , including @xmath68 entities with at least one news reference .",
    "the _ entity classes _ were selected from the dbpedia class ontology .",
    "figure  [ fig : entity_distribution ] shows the number of entities per class for the years ( 2009 - 2014 ) .",
    "* news articles . *",
    "we extract all news references from the collected wikipedia entity pages .",
    "the extracted news references are associated with the sections in which they appear . in total",
    "there were @xmath69 news references , and after crawling we end up with @xmath70 successfully crawled news articles .",
    "the details of the news article distribution , and the number of entities and sections from which they are referred are shown in table  [ tbl : news_dist ]",
    ".    * article - entity ground - truth . * the dataset",
    "comprises of the news and entity pairs @xmath71 .",
    "news - entity pairs are relevant if the news article is referenced in the entity page .",
    "non - relevant pairs ( i.e. negative training examples ) consist of news articles that contain an entity but are not referenced in that entity s page .",
    "if a news article @xmath0 is referred from @xmath1 at year @xmath5 , the features are computed taking into account the entity profiles at year @xmath72 .    * article - section ground - truth . *",
    "the dataset consists of the triple @xmath73 , where @xmath74 , where we assume that @xmath17 has already been determined as relevant .",
    "we therefore have a multi - class classification problem where we need to determine the section of @xmath1 where @xmath0 is cited .",
    "similar to the _ article - entity _ ground truth , here too the features compute the similarity between @xmath0 , @xmath72 and @xmath59 .",
    "we pos - tag the news articles and entity profiles @xmath7 with the stanford tagger  @xcite . for entity linking the news articles , we use tagme!@xcite with a confidence score of 0.3 . on a manual inspection of a random sample of 1000 disambiguated entities ,",
    "the accuracy is above 0.9 . on average ,",
    "the number of entities per news article is approximately 30 . for entity linking the entity profiles , we simply follow the _ anchor _ text that refers to wikipedia entities .",
    "we evaluate the generated supervised models for the two tasks , _ aep _ and _ asp _ , by splitting the train and testing instances .",
    "it is important to note that for the pairs @xmath17 and the triple @xmath75 , the news article @xmath0 is referenced at time @xmath5 by entity @xmath1 , while the features take into account the entity profile at time @xmath42 .",
    "this avoids any ` overlapping ' content between the news article and the entity page , which could affect the learning task of the functions @xmath21 and @xmath22 .",
    "table  [ tbl : train_test_instances ] shows the statistics of train and test instances .",
    "we learn the functions at year @xmath5 and test on instances for the years greater than @xmath5 .",
    "please note that we do not show the performance for year 2014 as we do not have data for 2015 for evaluation .",
    "here we introduce the evaluation setup and analyze the results for the _ article  entity ( aep ) _ placement task .",
    "we only report the evaluation metrics for the _ ` relevant ' _ news - entity pairs .",
    "a detailed explanation on why we focus on the _ ` relevant ' _ pairs is provided in section  [ subsec : article_linking ] .",
    "* baselines . *",
    "we consider the following baselines for this task .    *",
    "the first baseline uses only the salience - based features by dunietz and gillick  @xcite . *",
    "the second baseline assigns the value _ relevant _ to a pair @xmath17 , if and only if @xmath1 appears in the title of @xmath0 .",
    "* learning models .",
    "* we use _ random forests _ ( rf )  @xcite .",
    "we learn the rf on all computed features in table  [ tbl : importance_salience ] .",
    "the optimization on rf is done by splitting the feature space into multiple trees that are considered as ensemble classifiers . consequently , for each classifier it computes the margin function as a measure of the average count of predicting the correct class in contrast to any other class . the higher the margin score the more robust the model .    * metrics .",
    "* we compute _ precision _",
    "p , _ recall _ r and f1 score for the _ relevant _ class .",
    "for example , precision is the number of news - entity pairs we correctly labeled as relevant compared to our ground truth divided by the number of all news - entity pairs we labeled as relevant .",
    "the following results measure the effectiveness of our approach in three main aspects : ( i ) overall _ performance _ of @xmath21 and comparison to baselines , ( ii ) _ robustness _ across the years , and ( iii ) _ optimal _ model for the _ aep _ placement task",
    ".    * performance .",
    "* figure  [ fig : salience_pr_curve ] shows the results for the years 2009 and 2013 , where we optimized the learning objective with instances from year @xmath5 and evaluate on the years @xmath76 ( see section  [ subsec : test_train ] ) .",
    "the results show the _ precision  recall _ curve .",
    "the _ red _ curve shows baseline * b1 *  @xcite , and the _ blue _ one shows the performance of @xmath21",
    ". the curve shows for varying _ confidence scores _",
    "( high to low ) the precision on labeling the pair @xmath77 as _ ` relevant'_. in addition , at each _ confidence score _ we can compute the corresponding recall for the _ ` relevant ' _ label . for high confidence scores on labeling the news - entity pairs ,",
    "the baseline * b1 * achieves on average a precision score of p=0.50 , while @xmath21 has p=0.93 .",
    "we note that with the drop in the confidence score the corresponding precision and recall values drop too , and the overall f1 score for * b1 * is around f1=0.2 , in contrast we achieve an average score of f1=0.67 .",
    "it is evident from figure  [ fig : salience_pr_curve ] that for the years 2009 and 2013 , @xmath21 significantly outperforms the baseline * b1*. we measure the significance through the _ t - test _ statistic and get a _",
    "p - value _ of @xmath78 .",
    "the improvement we achieve over * b1 * in absolute numbers , @xmath79p=+0.5 in terms of precision for the years between 2009 and 2014 , and a similar improvement in terms of f1 score .",
    "the improvement for recall is @xmath79 r=+0.4 .",
    "the relative improvement over * b1 * for p and f1 is almost 1.8 times better , while for recall we are 3.5 times better . in table",
    "[ tbl : article_entity_results ] we show the overall scores for the evaluation metrics for * b1 * and @xmath21 .",
    "finally , for * b2 * we achieve much poorer performance , with average scores of p=0.21 , r=0.20 and f1=0.21 .    0.37     0.37     * robustness . * in table  [ tbl : article_entity_results ] , we show the overall performance for the years between 2009 and 2013 . an interesting observation",
    "we make is that we have a very robust performance and the results are stable across the years .",
    "if we consider the experimental setup , where for year @xmath80 we optimize the learning objective with only 74k training instances and evaluate on the rest of the instances , it achieves a very good performance .",
    "we predict with f1=0.68 the remaining 469k instances for the years @xmath81 $ ] .",
    "the results are particularly promising considering the fact that the distribution between our two classes is highly skewed . on average",
    "the number of _ ` relevant ' _ pairs account for only around @xmath82 of all pairs .",
    "a good indicator to support such a statement is the _ kappa _ ( denoted by @xmath83 ) statistic .",
    "@xmath83 measures agreement between the algorithm and the gold standard on both labels while correcting for chance agreement ( often expected due to extreme distributions ) .",
    "the @xmath83 scores for * b1 * across the years is on average @xmath84 , while for @xmath21 we achieve a score of @xmath85 ( the maximum score for @xmath83 is 1 ) .      in figure",
    "[ fig : salience_feature_pr_curve ] we show the impact of the individual feature groups that contribute to the superior performance in comparison to the baselines .",
    "_ relative entity frequency _ from the _ salience _ feature , models the entity salience as an exponentially decaying function based on the positional index of the paragraph where the entity appears .",
    "the performance of @xmath21 with _ relative entity frequency _ from the _ salience _ feature group is close to that of all the features combined .",
    "the _ authority _ and _ novelty _ features account to a further improvement in terms of precision , by adding roughly a 7%-10% increase . however ,",
    "if both feature groups are considered separately , they significantly outperform the baseline *",
    "b1*.        [ fig : salience_feature_pr_curve ]      here we show the evaluation setup for _ asp _ task and discuss the results with a focus on three main aspects , ( i ) the overall performance across the years , ( ii ) the _ entity class _ specific performance , and ( iii ) the impact on _ entity profile _ expansion by suggesting missing sections to entities based on the pre - computed templates .",
    "* baselines . * to the best of our knowledge , we are not aware of any comparable approach for this task . therefore , the baselines we consider are the following :    * * s1 * : pick the section from template @xmath53 with the highest lexical similarity to @xmath0 : * * s1**@xmath86 * * s2 * : place the news into the most frequent section in @xmath53    * learning models .",
    "* we use _ random forests _ ( rf )  @xcite and _ support vector machines _ ( svm )  @xcite .",
    "the models are optimized taking into account the features in table  [ tbl : feature_list ] .",
    "in contrast to the _ aep _ task , here the scale of the number of instances allows us to learn the svm models .",
    "the svm model is optimized using the @xmath87 _ loss _ function and uses the _",
    "gaussian _ kernels",
    ".    * metrics .",
    "* we compute _ precision _ p as the ratio of news for which we pick a section @xmath65 from @xmath53 and @xmath65 conforms to the one in our ground - truth ( see section  [ subsec : datasets ] ) .",
    "the definition of _ recall _ r and f1 score follows from that of precision .",
    "figure  [ fig : incremental_learning ] shows the overall performance and a comparison of our approach ( when @xmath22 is optimized using svm ) against the best performing baseline * s2*. with the increase in the number of training instances for the _ asp _ task the performance is a monotonically non - decreasing function . for the year 2009",
    ", we optimize the learning objective of @xmath22 with around 8% of the total instances , and evaluate on the rest .",
    "the performance on average is around p=0.66 across all classes .",
    "even though for many classes the performance is already stable ( as we will see in the next section ) , for some classes we improve further .",
    "if we take into account the years between 2010 and 2012 , we have an increase of @xmath79p=0.17 , with around 70% of instances used for training and the remainder for evaluation .",
    "for the remaining years the total improvement is @xmath79p=0.18 in contrast to the performance at year 2009 .",
    "on the other hand , the baseline * s1 * has an average precision of p=0.12 .",
    "the performance across the years varies slightly , with the year 2011 having the highest average precision of p=0.13 . always picking the most frequent section as in * s2 * , as shown in figure  [ fig : incremental_learning ] , results in an average precision of p=0.17 , with a uniform distribution across the years .          here",
    "we show the performance of @xmath22 decomposed for the different entity classes .",
    "specifically we analyze the 27 classes in figure  [ fig : entity_distribution ] . in table",
    "[ tbl : section_classifier ] , we show the results for a range of years ( we omit showing all years due to space constraints ) . for illustration purposes only , we group them into four main classes ( @xmath88 ` person , organization , location , event`@xmath89 ) and into the specific sub - classes shown in the second column in table  [ tbl : section_classifier ] .",
    "for instance , the entity classes ` officeholder ` and ` politician ` are aggregated into ` person``politics ` .",
    "it is evident that in the first year the performance is lower in contrast to the later years .",
    "this is due to the fact that as we proceed , we can better generalize and accurately determine the correct _ fit _ of an article @xmath0 into one of the sections from the pre - computed _ templates _ @xmath53 .",
    "the results are already stable for the year range @xmath90 $ ] .",
    "for a few ` person ` sub - classes , e.g. ` politics ` , ` entertainment ` , we achieve an f1 score above 0.9 .",
    "these additionally represent classes with a sufficient number of training instances for the years @xmath91 $ ] .",
    "the lowest f1 score is for the ` criminal ` and ` television ` classes .",
    "however , this is directly correlated with the insufficient number of instances .",
    "the baseline approaches for the _ asp _ task perform poorly .",
    "* s1 * , based on _ lexical similarity _ , has a varying performance for different entity classes .",
    "the best performance is achieved for the class ` person ",
    "politics ` , with p=0.43 .",
    "this highlights the importance of our feature choice and that the _ asp _ can not be considered as a _ linear function _",
    ", where the maximum similarity yields the best results . for different entity classes different features and combination of features",
    "is necessary . considering that * s2 *",
    "is the overall best performing baseline , through our approach @xmath22 we have a significant improvement of over @xmath79p=+0.64 .",
    "the models we learn are very robust and obtain high accuracy , fulfilling our pre - condition for accurate news suggestions into the entity sections .",
    "we measure the robustness of @xmath22 through the @xmath83 statistic . in this case",
    ", we have a model with roughly 10 labels ( corresponding to the number of sections in a template @xmath53 ) .",
    "the score we achieve shows that our model predicts with high confidence with @xmath92 .",
    "the last analysis is the impact we have on _ expanding _ entity profiles @xmath7 with new sections .",
    "figure  [ fig : missing_sections ] shows the ratio of sections for which we correctly suggest an article @xmath0 to the right section in the section template @xmath93 .",
    "the ratio here corresponds to sections that are not present in the entity profile at year @xmath42 , that is @xmath94 .",
    "however , given the generated templates @xmath59 , we can expand the entity profile @xmath72 with a new section at time @xmath5 . in details , in the absence of a section at time @xmath5 , our model trains well on similar sections from the section template @xmath59 , hence we can predict accurately the section and in this case suggest its addition to the entity profile . with time",
    ", it is obvious that the expansion rate decreases at later years as the entity profiles become more ` complete ' .",
    "this is particularly interesting for expanding the entity profiles of long - tail entities as well as updating entities with real - world emerging events that are added constantly . in many cases such missing sections are present at one of the entities of the respective entity class @xmath10 .",
    "an obvious case is the example taken in section  [ subsec : article_linking ] , where the _ ` accidents ' _ is rather common for entities of type ` airline ` . however , it is non - existent for some specific entity instances , i.e _ germanwings _ airline .    through our _ asp _ approach @xmath22 , we are able to expand both _ long - tail _ and _ trunk _ entities .",
    "we distinguish between the two types of entities by simply measuring their section text length .",
    "the real distribution in the ground truth ( see section  [ subsec : datasets ] ) is 27% and 73% are _ long - tail _ and _ trunk _ entities , respectively .",
    "we are able to expand the entity profiles for both cases and all entity classes without a significant difference , with the only exception being the class ` creative work ` , where we expand significantly more _ trunk _ entities .",
    "in this work , we have proposed an automated approach for the novel task of suggesting news articles to wikipedia entity pages to facilitate wikipedia updating .",
    "the process consists of two stages . in the first stage ,",
    "_ article  entity _ placement , we suggest news articles to entity pages by considering three main factors , such as _ entity salience _ in a news article , _ relative authority _ and _ novelty _ of news articles for an entity page . in the second stage ,",
    "_ article  section _ placement , we determine the best fitting section in an entity page . here",
    ", we remedy the problem of incomplete entity section profiles by constructing section templates for specific entity classes .",
    "this allows us to add missing sections to entity pages .",
    "we carry out an extensive experimental evaluation on 351,983 news articles and 73,734 entities coming from 27 distinct entity classes . for the first stage",
    ", we achieve an overall performance with p=0.93 , r=0.514 and f1=0.676 , outperforming our baseline competitors significantly . for the second stage , we show that we can learn incrementally to determine the correct section for a news article based on section templates .",
    "the overall performance across different classes is p=0.844 , r=0.885 and f1=0.860 .    in the future",
    ", we will enhance our work by extracting facts from the suggested news articles .",
    "results suggest that the news content cited in entity pages comes from the first paragraphs .",
    "however , challenging task such as the canonicalization and chronological ordering of facts , still remain .",
    "j.  hoffart , m.  a. yosef , i.  bordino , h.  frstenau , m.  pinkal , m.  spaniol , b.  taneva , s.  thater , and g.  weikum .",
    "robust disambiguation of named entities in text . in _ 2011 emnlp _ , stroudsburg , pa , usa , 2011 .",
    "m.  surdeanu , d.  mcclosky , j.  tibshirani , j.  bauer , a.  x. chang , v.  i. spitkovsky , and c.  d. manning . a simple distant supervision approach for the tac - kbp slot filling task . in _ text analysis conference 2010 workshop_."
  ],
  "abstract_text": [
    "<S> wikipedia entity pages are a valuable source of information for direct consumption and for knowledge - base construction , update and maintenance </S>",
    "<S> . facts in these entity pages are typically supported by references . </S>",
    "<S> recent studies show that as much as 20% of the references are from online news sources . however , many entity pages are incomplete even if relevant information is already available in existing news articles . even for the already present references , </S>",
    "<S> there is often a delay between the news article publication time and the reference time . in this work , </S>",
    "<S> we therefore look at wikipedia through the lens of news and propose a novel news - article suggestion task to improve news coverage in wikipedia , and reduce the lag of newsworthy references . </S>",
    "<S> our work finds direct application , as a precursor , to wikipedia page generation and knowledge - base acceleration tasks that rely on relevant and high quality input sources .    </S>",
    "<S> we propose a two - stage supervised approach for suggesting news articles to entity pages for a given state of wikipedia . </S>",
    "<S> first , we suggest news articles to wikipedia entities ( article - entity placement ) relying on a rich set of features which take into account the _ salience _ and _ relative authority _ of entities , and the _ novelty _ of news articles to entity pages . </S>",
    "<S> second , we determine the exact section in the entity page for the input article ( article - section placement ) guided by class - based section templates . </S>",
    "<S> we perform an extensive evaluation of our approach based on ground - truth data that is extracted from external references in wikipedia . </S>",
    "<S> we achieve a high precision value of up to 93% in the _ article - entity _ suggestion stage and upto 84% for the _ article - section placement_. finally , we compare our approach against competitive baselines and show significant improvements .    </S>",
    "<S> [ information search and retrieval ] </S>"
  ]
}