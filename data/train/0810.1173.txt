{
  "article_text": [
    "the paper deals with the estimation problem in the heteroscedastic nonparametic regression model @xmath0 where the design points @xmath1 , @xmath2 is an unknown function to be estimated , @xmath3 is a sequence of centered independent random variables with unit variance and @xmath4 are unknown scale functionals depending on the design points and the regression function @xmath5 .    typically , the notion of asymptotic optimality is associated with the optimal convergence rate of the minimax risk ( see e.g. , ibragimov , hasminskii,1981 ; stone,1982 ) . an important question in optimality results is to study the exact asymptotic behavior of the minimax risk .",
    "such results have been obtained only in a limited number of investigations .",
    "as to the nonparametric estimation problem for heteroscedastic regression models we should mention the papers by efromovich , 2007 , efromovich , pinsker , 1996 , and galtchouk , pergamenshchikov , 2005 , concerning the exact asymptotic behavior of the @xmath6-risk and the paper by brua , 2007 , devoted to the efficient pointwise estimation for heteroscedastic regressions .",
    "heteroscedastic regression models are largely used in financial mathematics , in particular , in problem of calibrating ( see e.g. , belomestny , reiss , 2006 ) .",
    "an example of heteroscedastic regression models is given by econometrics ( see , for example , goldfeld , quandt , 1972 , p. 83 ) , where for consumer budget problems one uses some parametric version of model with the scale coefficients defined as @xmath7 where @xmath8 , @xmath9 and @xmath10 are some unknown positive constants .",
    "the purpose of the article is to study asymptotic properties of the adaptive estimation procedure proposed in galtchouk , pergamenshchikov , 2007 , for which a non - asymptotic oracle inequality was proved for quadratic risks .",
    "we will prove that this oracle inequality is asymptotically sharp , i.e. the asymptotic quadratic risk is minimal .",
    "it means the adaptive estimation procedure is efficient under some the conditions on the scales @xmath4 which are satisfied in the case .",
    "note that in efromovich , 2007 , efromovich , pinsker , 1996 , an efficient adaptive procedure is constructed for heteroscedastic regression when the scale coefficient is independent of @xmath5 , i.e. @xmath11 . in galtchouk , pergamenshchikov , 2005 , for the model",
    "the asymptotic efficiency was proved under strong the conditions on the scales which are not satisfied in the case .",
    "moreover in the cited papers the efficiency was proved for the gaussian random variables @xmath3 that is very restrictive for applications of proposed methods to practical problems .    in the paper",
    "we modify the risk .",
    "we take a additional supremum over the family of unknown noise distributions like to galtchouk , pergamenshchikov , 2006 .",
    "this modification allows us to eliminate from the risk dependence on the noise distribution .",
    "moreover for this risk a efficient procedure is robust with respect to changing the noise distribution .",
    "it is well known to prove the asymptotic efficiency one has to show that the asymptotic quadratic risk coincides with the lower bound which is equal to the pinsker constant . in the paper",
    "two problems are resolved : in the first one a upper bound for the risk is obtained by making use of the non - asymptotic oracle inequality from galtchouk , pergamenshchikov , 2007 , in the second one we prove that this upper bound coincides with the pinsker constant .",
    "let us remember that the adaptive procedure proposed in galtchouk , pergamenshchikov , 2007 , is based on weighted least - squares estimates , where the weights are proper modifications of the pinsker weights for the homogeneous case ( when @xmath12 ) relative to a certain smoothness of the function @xmath5 and this procedure chooses a best estimator for the quadratic risk among these estimators . to obtain the pinsker constant for the model one has to prove a sharp asymptotic lower bound for the quadratic risk in the case when the noise variance depends on the unknown regression function . in this case , as usually",
    ", we minorize the minimax risk by a bayesian one for a respective parametric family . then for the bayesian risk we make use of a lower bound ( see theorem 6.1 ) which is a modification of the van trees inequality ( see , gill , levit , 1995 )",
    ".    the paper is organized as follows . in section",
    "[ sec : ad ] we construct an adaptive estimation procedure . in section  [ sec : co ] we formulate principal the conditions .",
    "the main results are presented in section  [ sec : ma ] .",
    "the upper bound for the quadratic risk is given in section  [ sec : up ] .",
    "in section  [ sec : lo ] we give all main steps of proving the lower bound .",
    "in subsection  [ subsec : tr ] we find the lower bound for the bayesian risk which minorizes the minimax risk . in subsection",
    "[ subsec : fa ] we study a special parametric functions family used to define the bayesian risk . in subsection  [ subsec : br ]",
    "we choose a prior distribution for bayesian risk to maximize the lower bound .",
    "section  [ sec : np ] is devoted to explain how to use the given procedure in the case when the unknown regression function is non periodic . in section  [ sec : cn ] we discuss the main results and their practical importance .",
    "the proofs are given in section  [ sec : pr ] .",
    "the appendix contains some technical results .",
    "in this section we describe the adaptive procedure proposed in galtchouk , pergamenshchikov , 2006 . we make use of the standard trigonometric basis @xmath13 in @xmath14 $ ] , i.e. @xmath15x)\\,,\\ j\\ge 2\\,,\\ ] ] where the function @xmath16 for even @xmath17 and @xmath18 for odd @xmath17 ; @xmath19 $ ] denotes the integer part of @xmath20 .    to evaluate the error of estimation in the model we will make use of the empiric norm in the hilbert space @xmath21 $ ] , generated by the design points @xmath22 of model . to this end , for any functions @xmath23 and @xmath24 from @xmath21 $ ] , we define the empiric inner product @xmath25 moreover , we will use this inner product for vectors in @xmath26 as well , i.e. if + @xmath27 and @xmath28 , then @xmath29 the prime denotes the transposition .",
    "notice that if @xmath30 is odd , then the functions @xmath31 are orthonormal with respect to this inner product , i.e. for any @xmath32 , @xmath33 where @xmath34 is kronecker s symbol , @xmath35 if @xmath36 and @xmath37 for @xmath38 .",
    "[ re.ad.1 ] note that in the case of even @xmath30 , the basis is orthogonal and it is orthonormal except the @xmath30th function for which the normalizing constant should be changed .",
    "the corresponding modifications of the formulas for even @xmath30 one can see in galtchouk , pergamenshchikov,2005 . to avoid these complications of formulas related to even @xmath30 , we suppose @xmath30 to be odd .",
    "thanks to this basis we pass to the discrete fourier transformation of model : @xmath39 where @xmath40 , @xmath41 , @xmath42 and @xmath43    we estimate the function @xmath5 by the weighted least squares estimator @xmath44 where the weight vector @xmath45 belongs to some finite set @xmath46 from @xmath47^n$ ] with @xmath48 .    here",
    "we make use of the weight family @xmath46 introduced in galtchouk , pergamenshchikov , 2008 , i.e. @xmath49 where @xmath50 and @xmath51 $ ] .",
    "we suppose that the parameters @xmath52 and @xmath53 are functions of @xmath30 , i.e. @xmath54 and @xmath55 , such that , @xmath56 & \\lim_{{\\mathchoice{n\\to\\infty}{n\\to\\infty}{\\lower.25ex\\hbox{$\\scriptstylen\\to\\infty$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylen\\to\\infty$}}}}\\,{\\varepsilon}_{{\\mathchoice{n}{n}{\\lower.25ex\\hbox{$\\scriptstylen$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylen$}}}}\\,=\\,0 \\quad\\mbox{and}\\quad \\lim_{{\\mathchoice{n\\to\\infty}{n\\to\\infty}{\\lower.25ex\\hbox{$\\scriptstylen\\to\\infty$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylen\\to\\infty$}}}}\\,n^{\\nu}\\,{\\varepsilon}_{{\\mathchoice{n}{n}{\\lower.25ex\\hbox{$\\scriptstylen$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylen$}}}}\\,=+\\infty\\ , , \\end{array } \\right.\\ ] ] for any @xmath57 .",
    "for example , one can take for @xmath48 @xmath58 where @xmath59 is any nonnegative constant .",
    "for each @xmath60 we define the weight vector @xmath61 as @xmath62 here @xmath63 $ ] with @xmath64 where @xmath65 is any nonnegative constant and @xmath66    [ re.ad.2 ] note that the weighted least squares estimators have been introduced by pinsker , 1981 , for continuous time optimal signal filtering in the gaussian noise .",
    "he proved that the mean - square asymptotic risk is minimized by weighted least squares estimators with weights of type .",
    "moreover he has found the sharp minimal value of the mean - square asymptotic risk , which was called later as the pinsker constant .",
    "nussbaum , 1985 , used the same method with proper modification for efficient estimation of the function @xmath5 of known smoothness in the homogeneous gaussian model , i.e. when @xmath12 and @xmath3 is i.i.d . @xmath67 sequence .    to choose weights from the set we minimize the special cost function introduced by galtchouk , pergamenshchikov , 2007",
    "this cost function is as follows @xmath68 where @xmath69 and @xmath70 $ ] .",
    "the penalty term we define as @xmath71 where @xmath72 is any slowly increasing sequence , i.e. @xmath73 for any @xmath57 .    finally , we set @xmath74    the goal of this paper is to study asymptotic ( as @xmath75 ) properties of this estimation procedure .",
    "[ re.ad.3 ] now we explain why does one choose the cost function in the form . developing the empiric quadratic risk for estimate , one obtains @xmath76 it s natural to choose the weight vector @xmath77 for which this function reaches the minimum .",
    "since the last term on the right - hand part is independent of @xmath77 , it can be droped and one has to minimize with respect to @xmath77 the function equals the difference of two first terms on the right - hand part .",
    "it s clear that the minimization problem cannt be solved directly because the fourier coefficients @xmath78 are unknown.to overcome this difficulty , we replace the product @xmath79 by its asymptotically unbiased estimator @xmath80 ( see , galtchouk , pergamenshchikov , 2007 , 2008 ) . moreover , to pay this substitution , we introduce into the cost function the penalty term @xmath81 with a small coefficient @xmath82 .",
    "the form of the penalty term is provided by the principal term of the quadratic risk for weighted least - squares estimator , see galtchouk , pergamenshchikov , 2007 , 2008.the coefficient @xmath82 means , that the penalty is small , because the estimator @xmath80 approximates in mean the quantity @xmath79 asymptotically , as @xmath75 .",
    "note that the principal difference between the procedure and the adaptive procedure proposed by golubev , nussbaum , 1993 , for a homogeneous gaussian regression , consists in presence of the penalty term in the cost function .",
    "[ re.ad.4 ] as it was noted at remark  [ re.ad.2 ] , nussbaum , 1985 , has shown that the weight coefficients of type provide the asymptotic minimum of the mean - squared risk at the regression function estimation problem for the homogeneous gaussian model , when the smoothness of the function @xmath5 is known .",
    "in fact , to obtain an efficient estimator one needs to take a weighted least squares estimator with the weight vector @xmath83 , where the index @xmath84 depends on smoothness of function @xmath5 and on coefficients @xmath4 , ( see below ) , which are unknown in our case .",
    "for this reason , galtchouk , pergamenshchikov , 2007 , 2008 , have proposed to make use of the family of coefficients , which contains the weight vector providing the minimum of the mean - squared risk .",
    "moreover , they proposed the adaptive procedure for which a non - asymptotic oracle inequality ( see , theorem  [ th.m.1 ] below ) was proved under some weak conditions on the coefficients @xmath4 .",
    "it is important to note that due the properties of the parametric family , the secondary term in the oracle inequality is slowly increasing ( slower than any degree of @xmath30 ) .",
    "first we impose some conditions on unknown function @xmath5 in the model .",
    "let @xmath85 be the set of @xmath86-periodic @xmath87 times differentiable @xmath88 functions .",
    "we assume that @xmath5 belongs to the following set @xmath89 where @xmath90 denotes the norm in @xmath21 $ ] , i.e. @xmath91 moreover , we suppose that @xmath92 and @xmath93 are unknown parameters .",
    "note that , we can represent the set @xmath94 as an ellipse in @xmath21 $ ] , i.e. @xmath95\\,:\\ , \\sum_{{\\mathchoice{j=1}{j=1}{\\lower.25ex\\hbox{$\\scriptstylej=1 $ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylej=1$}}}}^\\infty\\,a_{{\\mathchoice{j}{j}{\\lower.25ex\\hbox{$\\scriptstylej$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylej$}}}}\\theta^2_{{\\mathchoice{j}{j}{\\lower.25ex\\hbox{$\\scriptstylej$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylej$}}}}\\le r\\}\\,,\\ ] ] where @xmath96 and @xmath97)^{2i}\\,.\\ ] ] here @xmath13 is the trigonometric basis defined in .",
    "now we describe the conditions on the scale coefficients @xmath98 .    *   _",
    "@xmath99 for some unknown function @xmath100\\times { { \\cal l}}_{{\\mathchoice{1}{1}{\\lower.25ex\\hbox{$\\scriptstyle1 $ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyle1$}}}}[0,1 ] \\to \\bbr_+$ ] , which is square integrable with respect to @xmath20 such that @xmath101 where @xmath102 .",
    "moreover , @xmath103 and @xmath104 _ *   _ for any @xmath105 $ ] , the operator @xmath106\\to \\bbr$ ] is differentiable in the frchet sense for any fixed function @xmath107 from @xmath108 $ ] , i.e. for any @xmath109 from some vicinity of @xmath107 in @xmath108 $ ] , @xmath110 where the frchet derivative @xmath111\\to \\bbr$ ] is a bounded linear operator and the residual term @xmath112 , for each @xmath105 $ ] , satisfies the following property : @xmath113 where @xmath114 . _",
    "* _ there exists some positive constant @xmath115 such that for any function @xmath5 from @xmath108 $ ] the operator @xmath116 defined in the condition @xmath117 satisfies the following inequality for any function @xmath109 from @xmath108 $ ] : @xmath118 where @xmath119 .",
    "_ * _ the function @xmath120 corresponding to @xmath121 is continuous on the interval @xmath47 $ ] .",
    "moreover , @xmath122 _    [ re.co.1 ] let us explain the conditions @xmath123@xmath124 .",
    "in fact , this is the regularity conditions of the function @xmath125 generating the scale coefficients @xmath4 .",
    "condition @xmath123 means that the function @xmath126 should be uniformly integrable with respect to the first argument in the sens of convergence .",
    "moreover , this function should be separated from zero ( see inequality ) and bounded on the class ( see inequality ) .",
    "boundedness away from zero provides that the distribution of observations @xmath127 is nt degenerate in @xmath26 , and the boundedness means that the intensity of the noise vector should be finite , otherwise the estimation problem hasnt any sens .",
    "conditions @xmath117 and @xmath128 mean that the function @xmath129 is regular , at any fixed @xmath130 , with respect to @xmath5 in the sens , that it is differentiable in the frchet sens ( see e.g. , kolmogorov , fomin , 1989 ) and moreover the frchet derivative satisfies the growth condition given by the inequality which permits to consider the example .",
    "last the condition @xmath124 is the usual uniform continuity the condition of the function @xmath131 at the function @xmath132 .",
    "now we give some examples of functions satisfying the conditions @xmath123-@xmath124 .",
    "we set @xmath133 with some coefficients @xmath134 , @xmath135 , @xmath136 .    in this case @xmath137",
    "the frchet derivative is given by @xmath138 it is easy to see that the function satisfies the conditions @xmath123@xmath124 .",
    "moreover , the conditions @xmath123@xmath124 are satisfied by any function of type @xmath139 where the functions @xmath140 and @xmath141 satisfy the following the conditions :    * @xmath140 is a @xmath47\\times\\bbr\\to [ c_{{\\mathchoice{0}{0}{\\lower.25ex\\hbox{$\\scriptstyle0",
    "$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyle0$}}}}\\,,\\,+\\infty)$ ] function ( with @xmath134 ) such that @xmath142 * @xmath141 is a continuously differentiable @xmath143 function such that @xmath144 where @xmath145 is the derivative of @xmath141 .    in this case @xmath146 and @xmath147 where @xmath148 . now to estimate the last term in this inequality note that @xmath149 therefore , from the condition we get @xmath150 and through the bounyakovskii - cauchy - schwarz inequality , for any @xmath151 , @xmath152 now , the condition implies @xmath123",
    ".    moreover , the frchet derivative in this case is given by @xmath153 one can check directly that this operator satisfies the inequality with @xmath154 .",
    "denote by @xmath155 the family of distributions @xmath156 in @xmath26 of the vectors @xmath157 in the model such that the components @xmath158 are jointly independent , centered with unit variance and @xmath159 where @xmath160 is slowly increasing sequence , that is it satisfies the property .",
    "it is easy to see that , for any @xmath161 , the centered gaussian distribution in @xmath26 with unit covariation matrix belongs to the family @xmath155 .",
    "we will denote by @xmath162 this gaussian distribution .    for any estimator @xmath163",
    "we define the following quadratic risk @xmath164 where @xmath165 is the expectation with respect to the distribution @xmath166 of the observations @xmath167 with the fixed function @xmath5 and the fixed distribution @xmath168 of random variables @xmath3 in the model .",
    "moreover , to make the risk independent of the design points , in this paper we will make use of the risk with respect to the usual norm in @xmath21 $ ] also , i.e. @xmath169    if an estimator @xmath163 is defined only at the design points @xmath22 , then we extend it as step function onto the interval @xmath47 $ ] by setting @xmath170 , for all @xmath130 , where @xmath171}{[0,x_{1}]}{\\lower.25ex\\hbox{$\\scriptstyle[0,x_{1}]$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyle[0,x_{1}]$}}}}(x ) + \\sum_{k=2}^n\\,f(x_k)\\chi_{{\\mathchoice{(x_{k-1},x_k]}{(x_{k-1},x_k]}{\\lower.25ex\\hbox{$\\scriptstyle(x_{k-1},x_k]$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyle(x_{k-1},x_k]$}}}}(x)\\,.\\ ] ]    in galtchouk , pergamenshchikov , 2007 , 2008 the following non - asymptotic oracle inequality has been shown for the procedure .    [ th.m.1 ]",
    "assume that in the model the function @xmath5 belongs to @xmath172 .",
    "then , for any odd @xmath48 and @xmath92 , the estimator @xmath173 satisfies the following oracle inequality @xmath174 where the function @xmath175 is such that , for any @xmath57 , @xmath176    [ re.m.1 ] note that in galtchouk , pergamenshchikov , 2007 , 2008 , the oracle inequality is proved for the model , where the random variables @xmath3 are independent identically distributed .",
    "in fact , the result and the proof are true for independent random variables which are not identically distributed , i.e. for any distribution of the random vector @xmath157 from @xmath155 .",
    "now we formulate the main asymptotic results . to this end , for any function @xmath151 , we set @xmath177 where @xmath178 it is well known ( see e.g. , nussbaum , 1985 ) that the optimal rate of convergence is @xmath179 when the risk is taken uniformly over @xmath180 .",
    "[ th.m.2 ] assume that in the model the sequence @xmath181 fulfills the condition @xmath182 .",
    "then the estimator @xmath173 from satisfies the inequalities @xmath183 and @xmath184    the following result gives the sharp lower bound for risk and show that @xmath185 is the .",
    "[ th.m.3 ] assume that in the model the sequence @xmath181 satisfies the conditions @xmath186 @xmath187 .",
    "then the risks and admit the following asymptotic lower bounds @xmath188 and @xmath189    [ re.m.2 ] note that in galtchouk , pergamenshchikov , 2005 , an asymptotically efficient estimator has been constructed and results similar to theorems [ th.m.2 ] and [ th.m.3 ] were claimed for the model .",
    "in fact the upper bound is true there under some additional condition on the smoothness of the function @xmath5 , i.e. on the parameter @xmath87 . in the cited paper",
    "this additional condition is not formulated since erroneous inequality @xmath190 . to avoid using this inequality",
    "we modify the estimating procedure by introducing the penalty term @xmath191 in the cost function . by this way",
    "we remove all additional conditions on the smoothness parameter @xmath87 .",
    "[ re.m.3 ] in fact to obtain the non - asymptotic oracle inequality , it is nt necessary to make use of equidistant design points and the trigonometric basis .",
    "one may take any design points ( deterministic or random ) and any orthonormal basis satisfying .",
    "but to obtain the property one needs to impose some technical conditions ( see galtchouk , pergamenshchikov , 2008 ) .",
    "note that the results of theorem  [ th.m.2 ] and theorem  [ th.m.3 ] are based on equidistant design points and the trigonometric basis .",
    "in this section we prove theorem  [ th.m.2 ] . to this end",
    "we will make use of the oracle inequality .",
    "we have to find an estimator from the family - for which we can show the upper bound .",
    "we start with the construction of such an estimator .",
    "first we put @xmath192 where @xmath193 .",
    "then we choose an index from the set @xmath194 as @xmath195 where @xmath87 is the parameter of the set @xmath180 and @xmath196 .",
    "finally , we set @xmath197 now we show the upper bound for this estimator .",
    "[ th.u.1 ] assume that the condition @xmath182 holds .",
    "then @xmath198    [ re.u.1 ] note that the estimator @xmath199 belongs to the family - , but we ca nt use directly this estimator because the parameters @xmath87 , @xmath200 and @xmath201 are unknown .",
    "we can use this upper bound only through the oracle inequality proved for procedure .",
    "now theorem  [ th.m.1 ] and theorem  [ th.u.1 ] imply the upper bound . to obtain the upper bound we need the following auxiliary result .",
    "[ sec : le.u.1 ] for any @xmath202 and any estimate @xmath203 of @xmath151 , @xmath204 where the function @xmath205 is defined in .",
    "proof of this lemma is given in appendix  [ subsec : a.1 ] .    now inequality and this lemma",
    "imply the upper bound .",
    "hence theorem  [ th.m.2 ] .",
    "in this section we give the main steps of proving the lower bounds and . in common ,",
    "we follow the same scheme as nussbaum , 1985 . we begin with minorizing the minimax risk by a bayesian one constructed on a parametric functional family introduced in section  [ subsec : fa ] ( see ) and using the prior distribution . further",
    ", a special modification of the van trees inequality ( see , theorem  [ th.tr.1 ] ) yields a lower bound for the bayesian risk depending on the chosen prior distribution , of course . finally , in section  [ subsec : br ] , we choose parameters of the prior distribution ( see ) providing the maximal value of the lower bound for the bayesian risk .",
    "this value coincides with the pinsker constant as it is shown in section  [ subsec : th.m.3 ] .",
    "let @xmath206 be a statistical model relative to the observations @xmath127 governed by the regression equation @xmath207 where @xmath208 are i.i.d .",
    "@xmath67 random variables , @xmath209 is a unknown parameter vector , @xmath210 is a unknown ( or known ) function and @xmath211 , with the function @xmath125 defined in the condition @xmath123 .",
    "assume that a prior distribution @xmath212 of the parameter @xmath213 in @xmath214 is defined by the density @xmath215 of the following form @xmath216 where @xmath217 is a continuously differentiable bounded density on @xmath218 with @xmath219 let @xmath220 be a continuously differentiable @xmath221 function such that , for any @xmath222 , @xmath223 where @xmath224 let @xmath225 be an estimator of @xmath226 based on observations @xmath127 . for any @xmath227 - measurable integrable function @xmath228 , we set @xmath229 where @xmath230 is the expectation with respect to the distribution @xmath231 of the vector @xmath232 .",
    "note that in this case @xmath233 where @xmath234    we prove the following result .",
    "[ th.tr.1 ] assume that the conditions @xmath235 hold .",
    "moreover , assume that the function @xmath236 with @xmath237 is uniformly over @xmath130 differentiable with respect to @xmath238 , i.e. for any @xmath222 there exists a function @xmath239 $ ] such that @xmath240 where @xmath241 , all coordinates are @xmath242 , except the i - th equals to @xmath86 . then for any square integrable estimator @xmath225 of @xmath226 and any @xmath222 , @xmath243 where @xmath244 , @xmath245 and @xmath246 @xmath247 , the operator @xmath248 is defined in the condition @xmath186 .",
    "* proof * is given in appendix  [ subsec : a.2 ] .",
    "[ re.tr.1 ] note that the inequality is some modification of the van trees inequality ( see , gill , levit , 1995 ) adapted to the model .      in this section we define and study some special parametric family of kernel function which will be used to prove the sharp lower bound .",
    "let us begin by kernel functions .",
    "we fix @xmath249 and we set @xmath250 where @xmath251 is the indicator of a set @xmath252 , the kernel @xmath253 is such that @xmath254 it is easy to see that the function @xmath255 possesses the properties : @xmath256 moreover , for any @xmath257 and @xmath258 @xmath259 we divide the interval @xmath47 $ ] into @xmath260 equal subintervals of length @xmath261 and on each of them we construct a kernel - type function which equals to zero at the boundary of the subinterval together with all derivatives .",
    "it provides that the fourier partial sums with respect to the trigonometric basis in @xmath262 $ ] give a natural parametric approximation to the function on each subinterval .",
    "let @xmath263 be the trigonometric basis in @xmath264 $ ] , i.e. @xmath265 x\\right)\\,,\\ j\\ge 2\\,,\\ ] ] where the functions @xmath266 are defined in .",
    "now , for any array @xmath267 we define the following function @xmath268 where @xmath269 , @xmath270 - 1\\,.\\ ] ]    we assume that the sequences @xmath271 and @xmath272 , satisfy the following conditions .",
    "@xmath273 _ the sequence @xmath274 as @xmath75 and for any @xmath57 @xmath275 moreover , there exist @xmath276 and @xmath277 such that @xmath278 _ to define a prior distribution on the family of arrays , we choose the following random array @xmath279 with @xmath280 where @xmath281 are i.i.d . @xmath67",
    "random variables and @xmath282 are some nonrandom positive coefficients . we make use of gaussian variables since they possess the minimal fisher information and therefore maximize the lower bound .",
    "we set @xmath283 we assume that the coefficients @xmath284 satisfy the following conditions .",
    "@xmath285 _ there exists a sequence of positive numbers @xmath286 such that @xmath287 moreover , for any @xmath288 , @xmath289 _    @xmath290 _ for some @xmath291 @xmath292 _",
    "@xmath293 _ there exists @xmath294 such that @xmath295 _    [ sec : pr.fa.1 ] let the conditions @xmath273@xmath285 .",
    "then , for any @xmath57 and for any @xmath296 , @xmath297    [ sec : pr.fa.2 ] let the conditions @xmath273@xmath293 . then , for any @xmath57 , @xmath298    [ sec : pr.fa.3 ] let the conditions @xmath273@xmath293 . then , for any @xmath57 , @xmath299    [ sec : pr.fa.4 ] let the conditions @xmath273@xmath293 .",
    "then for any function @xmath300 satisfying the conditions and @xmath124 @xmath301    proofs of propositions  [ sec : pr.fa.1][sec : pr.fa.4 ] are given in appendix .",
    "now we will obtain the lower bound for the bayesian risk that yields the lower bound for the minimax risk .",
    "we make use of the sequence of random functions @xmath302 defined in - with the coefficients @xmath282 satisfying the conditions @xmath273@xmath293 which will be chosen later .    for any estimator @xmath203",
    "we introduce now the corresponding bayes risk @xmath303 where the kernel family @xmath304 is defined in , @xmath212 denotes the distribution of the random array @xmath213 defined by in @xmath214 with @xmath305 .",
    "we remember that @xmath162 is a centered gaussian distribution in @xmath26 with unit covariation matrix .",
    "first of all , we replace the functions @xmath203 and @xmath5 by their fourier series with respect to the basis @xmath306 by making use of this basis we can estimate the norm @xmath307 from below as @xmath308 where @xmath309 moreover , from the definition one gets @xmath310 it is easy to see that the functions @xmath311 satisfy the condition for gaussian prior densities . in this case",
    "( see the definition in ) we have @xmath312 where @xmath313 now to obtain a lower bound for the bayes risk @xmath314 we make use of theorem  [ th.tr.1 ] which implies that @xmath315 where @xmath316 and @xmath317 with @xmath318 . in the appendix",
    "we show that @xmath319 and @xmath320 this means that , for any @xmath57 and for sufficiently large @xmath30 , @xmath321 where @xmath322 is defined in . therefore ,",
    "if we denote in @xmath323 we obtain , for sufficiently large @xmath30 , @xmath324 in the appendix we show that @xmath325 where @xmath326 therefore we can write that , for sufficiently large @xmath30 , @xmath327    obviously , to obtain a `` good '' lower bound for the risk @xmath314 one needs to maximize the right - hand side of the inequality .",
    "hence we choose the coefficients @xmath328 by maximization the function @xmath329 , i.e. @xmath330 the parameter @xmath331 will be chosen later to satisfy the condition @xmath290 . by",
    "the lagrange multipliers method it is easy to find that the solution of this problem is given by @xmath332 with @xmath333    to obtain a positive solution in we need to impose the following condition @xmath334 moreover , from the condition @xmath290 we obtain that @xmath335 where @xmath336 note that by the condition @xmath124 the function @xmath120 is continuous on the interval @xmath47 $ ] , therefore @xmath337    now we have to choose the sequence @xmath338 . note that if we put in @xmath339 we can rewrite the inequality as @xmath340 where @xmath341 it is clear that @xmath342 therefore to obtain a positive finite asymptotic lower bound in we have to take the parameter @xmath343 as @xmath344 with some positive coefficient @xmath345 .",
    "moreover , the conditions - imply that , for sufficiently large @xmath30 , @xmath346 moreover , taking into account that for sufficiently large @xmath30 @xmath347 we obtain the following condition on @xmath345 @xmath348 where @xmath349 to maximize the function @xmath350 on the right - hand side of the inequality we take @xmath351 defined in . therefore we obtain that @xmath352 where @xmath353 furthermore , taking into account that @xmath354 we get @xmath355 where @xmath356    this means that to obtain in the maximal lower bound one has to take in @xmath357 it is important to note that if one defines the prior distribution @xmath212 in the bayesian risk by formulas , , and , then the bayesian risk would depend on a parameter @xmath358 , i.e. @xmath359 .    therefore , the inequality implies that , for any @xmath358 , @xmath360 where the function @xmath361 is defined in for @xmath121 .    now to end the definition of the sequence of the random functions @xmath362 defined by and one has to define the sequence @xmath363 .",
    "let us remember that we make use of the sequence @xmath362 with the coefficients @xmath282 constructed in for @xmath351 given in and for the sequence @xmath343 given by and for some fixed arbitrary @xmath358 .",
    "we will choose the sequence @xmath363 to satisfy the conditions @xmath273@xmath293 .",
    "one can take , for example , @xmath364 + 1 $ ] .",
    "then the condition @xmath273 is trivial .",
    "moreover , taking into account that in this case @xmath365 we find thanks to the convergence @xmath366 therefore , the solution , for sufficiently large @xmath30 , satisfies the following inequality @xmath367 now it is easy to see that the condition @xmath285 holds with @xmath368 and the condition @xmath293 holds for arbitrary @xmath369 . as to the condition @xmath290 , note that in view of the definition of @xmath370 in we get @xmath371 hence the condition @xmath290 .",
    "now we consider the estimation problem of the non periodic regression function @xmath5 in the model . in this case",
    "we will estimate the function @xmath5 on any interior interval @xmath372 $ ] of @xmath47 $ ] , i.e. for @xmath373 .",
    "it should be pointed out that at the boundary points @xmath374 and @xmath375 , one must to make use of kernel estimators ( see brua , 2007 ) .",
    "let now @xmath376 be a infinitely differentiable @xmath47\\to\\bbr_{{\\mathchoice{+}{+}{\\lower.25ex\\hbox{$\\scriptstyle+$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyle+$}}}}$ ] function such that @xmath377 for @xmath378 and @xmath379 for all @xmath380 , for example , @xmath381}{[a',b']}{\\lower.25ex\\hbox{$\\scriptstyle[a',b']$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyle[a',b']$}}}}(z)\\ , \\d z\\,,\\ ] ] where @xmath141 is some kernel function introduced in , @xmath382 multiplying the equation by the function @xmath383 and simulating the i.i.d . @xmath67",
    "sequence @xmath384 one comes to the estimation problem of the periodic regression function @xmath385 , i.e. @xmath386 where @xmath387 , @xmath388 and @xmath389 is some sufficiently small parameter .",
    "it is easy to see that if the sequence @xmath4 satisfies the conditions @xmath390 , then the sequence @xmath391 satisfies these conditions as well with @xmath392",
    "in conclusion , it should be noted that this paper completes the investigation of the estimation problem of the nonparametric regression function for the heteroscedastic regression model in the case of quadratic risk .",
    "it is proved that the adaptive procedure satisfies the non asymptotic oracle inequality and it is asymptotically efficient for estimating a periodic regression function . moreover , in section  [ sec : np ] we have explained how to apply the procedure to the case of non periodic function .",
    "as far as we know , the procedure is unique for estimating the regression function at the model .",
    "let us remember once more the main steps of this investigation .",
    "the procedure combines the both principal aspects of nonparametric estimation : non asymptotic and asymptotic .",
    "non - asymptotic aspect is based on the selection model procedure with penalization ( see e.g. , barron , birg and massart , 1999 , or fourdrinier , pergamenshchikov , 2007 ) .",
    "our selection model procedure differs from the commonly used one by a small coefficient in the penalty term going to zero that provides the sharp non - asymptotic oracle inequality . moreover , the commonly used selection model procedure is based on the least - squares estimators whereas our procedure uses weighted least - squares estimators with the weights minimizing the asymptotic quadratic risk that provides the asymptotic efficiency , as the final result . from practical point of view",
    ", the procedure gives an acceptable accuracy even for small samples as it is shown via simulations by galtchouk , pergamenshchikov , 2008 .",
    "to prove the theorem we will adapt to the heteroscedastic case the corresponding proof from nussbaum , 1985 .",
    "first , from we obtain that , for any @xmath168 , @xmath393 where @xmath394 setting now @xmath395 with the function @xmath396 defined in , the index @xmath397 defined in , @xmath398 $ ] , @xmath399 $ ] and @xmath400 we rewrite as follows @xmath401 with @xmath402 note that we have decomposed the first term on the right - hand of into the sum @xmath403 this decomposition allows us to show that @xmath404 is negligible and further to approximate the first term by a similar term in which the coefficients @xmath405 will be replaced by the fourier coefficients @xmath406 of the function @xmath5 .    taking into account the definition of @xmath396 in we can bound @xmath407 as @xmath408 therefore , by lemma  [ sec : le.a.1 ] we obtain @xmath409",
    "let us consider now the next term @xmath410 .",
    "we have @xmath411 now by lemma  [ sec : le.a.2 ] and the definition we obtain directly the same property for @xmath410 , i.e. @xmath412 setting @xmath413 and applying the well - known inequality @xmath414 to the first term on the right - hand side of the inequality we obtain that , for any @xmath296 and for any @xmath168 , @xmath415 \\label{sec : up.1 - 3 } & + { \\widetilde}{\\delta}_{{\\mathchoice{1,n}{1,n}{\\lower.25ex\\hbox{$\\scriptstyle1,n$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyle1,n$}}}}+ { \\widetilde}{\\delta}_{{\\mathchoice{2,n}{2,n}{\\lower.25ex\\hbox{$\\scriptstyle2,n$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyle2,n$}}}}+(1 + 1/\\delta)\\ , { \\widetilde}{\\delta}_{{\\mathchoice{3,n}{3,n}{\\lower.25ex\\hbox{$\\scriptstyle3,n$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyle3,n$}}}}\\,,\\end{aligned}\\ ] ] where @xmath416 taking into account that @xmath93 and that @xmath417 we can show through lemma  [ sec : le.a.3 ] that @xmath418 therefore , the inequality yields @xmath419 and to prove it suffices to show that @xmath420 first , it should be noted that the definition and the inequalities - imply directly @xmath421 moreover , by the definition of @xmath422 in , for sufficiently large @xmath30 , for which @xmath423 we find @xmath424 therefore , by the definition of the coefficients @xmath425 in @xmath426 furthermore , in view of the definition we calculate directly @xmath427 now , the definition of @xmath180 in and the condition imply the inequality .",
    "hence theorem  [ th.u.1 ] .      in this section",
    "we prove theorem  [ th.m.3 ] .",
    "lemma  [ sec : le.u.1 ] implies that to prove the lower bounds and , it suffices to show @xmath428 where @xmath429    for any estimator @xmath203 , we denote by @xmath430 its projection onto @xmath431 , i.e. + @xmath432 .",
    "since @xmath180 is a convex set , we get @xmath433 now we introduce the following set @xmath434 where @xmath281 are i.i.d . @xmath67 random variables from and",
    "the sequence @xmath286 is given in the condition @xmath285 .",
    "therefore , we can write that @xmath435 here the kernel function family @xmath304 is given in in which @xmath436 + 1 $ ] and the parameter @xmath437 is defined in and ; the measure @xmath212 is defined in .",
    "moreover , note that on the set @xmath438 the random function @xmath439 is uniformly bounded , i.e. @xmath440 where the coefficient @xmath441 is defined in .",
    "thus , we estimate the risk @xmath442 from below as @xmath443 with @xmath444    by making use of the bayes risk with the prior distribution given by formulae , , and for any fixed parameter @xmath358 we rewrite the lower bound for @xmath442 as @xmath445 with @xmath446    in section  [ subsec : br ] we proved that the parameters in chosen prior distribution satisfy the conditions @xmath273@xmath293 .",
    "therefore propositions  [ sec : pr.fa.2][sec : pr.fa.3 ] and the limit imply that , for any @xmath57 , @xmath447 moreover , by the condition @xmath124 the sequence @xmath448 goes to @xmath361 as @xmath75 . therefore , from this , and we get , for any @xmath358 , @xmath449 where @xmath450 is defined in .",
    "limiting here @xmath451 implies inequality . hence theorem  [ th.m.3 ] .",
    "for any @xmath202 , by making use of the elementary inequality @xmath456 one gets @xmath457 moreover , for any @xmath452 with @xmath93 , by the bounyakovskii - cauchy - schwarz inequality we obtain that @xmath458 hence lemma [ sec : le.u.1 ] .      for any",
    "@xmath459 we set @xmath460 note that due to the condition , the density is bounded , i.e. @xmath461 so through we obtain that @xmath462 therefore , integrating by parts yields @xmath463 now the bounyakovskii - cauchy - schwarz inequality gives the following lower bound @xmath464 to estimate the denominator in the last ratio , note that @xmath465 > from it follows that @xmath466 moreover , the conditions @xmath117 and imply @xmath467 from which it follows @xmath468 this implies inequality .",
    "hence theorem  [ th.tr.1 ] .",
    "first note that , for @xmath130 , we can represent the @xmath469th derivative as @xmath470 where @xmath471 therefore @xmath472 and by the bounyakovskii - cauchy - schwarz inequality we obtain that @xmath473 with @xmath474 and @xmath475 now we show that , for any @xmath476 and @xmath296 , @xmath477 to this end note that @xmath478 therefore , taking into account the definition of the set @xmath479 in , the functions @xmath480 with @xmath481 can be estimated on this set as @xmath482 and by we get , for any @xmath296 and sufficiently large @xmath30 , @xmath483 moreover , for sufficiently large @xmath30 , @xmath484 therefore , the condition @xmath273 implies @xmath485 for any @xmath57 . hence proposition  [ sec : pr.fa.1 ] .",
    "first of all we prove that for @xmath486 from the condition @xmath290 @xmath487 indeed , putting in @xmath488 we can represent the @xmath87th derivative of @xmath489 as follows @xmath490 with @xmath491 and @xmath492 first , note that , we can estimate the norm of @xmath493 by the same way as in the inequality , i.e. @xmath494 by making use of we obtain that , for any @xmath495 and for any @xmath296 , @xmath496    let us consider now the last term in . taking into account",
    "that @xmath497 we get @xmath498 therefore from the condition @xmath290 we get for sufficiently large @xmath30 @xmath499 with @xmath500 we show that for any @xmath57 and for any @xmath296 @xmath501 indeed , by the chebychev inequality for any @xmath502 @xmath503 note now that according to the burkholder - davis - gundy inequality for any @xmath504 there exists a constant @xmath505 such that @xmath506 moreover , by putting @xmath507 we can estimate the random variable @xmath508 as @xmath509 therefore , by the condition @xmath293 , for sufficiently large @xmath30 , @xmath510 where @xmath511 . now",
    "the condition @xmath273 implies , for sufficiently large @xmath30 , @xmath512 thus , choosing in @xmath513 we obtain the limiting equality which together with - implies .",
    "now it is easy to deduce that proposition  [ sec : pr.fa.1 ] yields proposition  [ sec : pr.fa.2 ] .",
    "first of all , we recall that , due to the condition @xmath285 , @xmath514 therefore , taking into account that @xmath515 we obtain , for sufficiently large @xmath30 , @xmath516 moreover , for any @xmath517 and @xmath518 , we estimate the last term as @xmath519 & + n\\,\\p(\\xi^c_{{\\mathchoice{n}{n}{\\lower.25ex\\hbox{$\\scriptstylen$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylen$ } } } } ) + 2\\e\\,\\zeta^2\\,\\chi_{{\\mathchoice{\\{\\zeta^2\\ge n\\}}{\\{\\zeta^2\\ge n\\}}{\\lower.25ex\\hbox{$\\scriptstyle\\{\\zeta^2\\ge n\\}$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyle\\{\\zeta^2\\ge n\\}$}}}}\\,,\\end{aligned}\\ ] ] where @xmath520 . by applying now proposition  [ sec : pr.fa.2 ] and the limit",
    "we come to proposition  [ sec : pr.fa.3 ] .",
    "[ sec : le.a.2 ] for any @xmath524 , @xmath525}n^{-m}\\left|\\sum_{l=2}^{n}\\,l^m \\",
    ", \\left ( \\phi^2_{{\\mathchoice{l}{l}{\\lower.25ex\\hbox{$\\scriptstylel$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylel$}}}}(x)-1 \\right)\\ , \\right|\\le 2^m\\,.\\ ] ]            first of all , note that proposition  [ sec : pr.fa.4 ] , the condition and the condition @xmath124 imply that @xmath532 let us show now that for any continuously differentiable function @xmath109 on @xmath533 $ ] @xmath534 indeed , setting @xmath535 we deduce @xmath536 where @xmath537 + 1 $ ] , @xmath538 $ ] , @xmath539 + 1-n{\\widetilde}{x}_{{\\mathchoice{m}{m}{\\lower.25ex\\hbox{$\\scriptstylem$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylem$}}}})/(nh ) \\quad \\mbox{and } \\quad v^*=([n{\\widetilde}{x}_{{\\mathchoice{m}{m}{\\lower.25ex\\hbox{$\\scriptstylem$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylem$}}}}+nh]-n{\\widetilde}{x}_{{\\mathchoice{m}{m}{\\lower.25ex\\hbox{$\\scriptstylem$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylem$}}}})/(nh)\\,.\\ ] ] therefore , taking into account that the derivative of the function @xmath109 is bounded on the interval @xmath533 $ ] we obtain that @xmath540 taking into account the conditions on the sequence @xmath272 given in @xmath273 we obtain limiting equality which together with implies .",
    "now we study the behavior of @xmath541 . due to the inequality we estimate the frchet derivative as @xmath542 consider now the fisrt term on the right - hand side of this inequality .",
    "we have @xmath543 we recall that the sequence @xmath441 is defined in .",
    "therefore , property implies @xmath544 as to the second term on the right - hand side of , we get @xmath545 similarly , @xmath546 and , by @xmath547 therefore , @xmath548 and the condition @xmath273 implies .      indeed , by the direct calculation it easy to see that , for any @xmath549 and for any vector @xmath550 , @xmath551 where the operator @xmath552 is defined in .",
    "moreover , we remember that @xmath553 . therefore , taking into account the property we obtain .",
    "brua , j .-",
    ". asymptotic efficient estimators for non - parametric heteroscedastic model .",
    "_ statistical metodologie _ , accepted to publication , available at _ http://hal.archives-ouvertes.fr/hal/-00178536/fr/ _",
    "galtchouk , l.,@xmath554 pergamenshchikov , s.(2005 ) .",
    "efficient adaptive nonparametric estimation in heteroscedastic regression models .",
    "preprint of the strasbourg louis pasteur university , irma , 2005/020 available at _ http://www.univ-rouen.fr/lmrs/persopage/pergamenchtchikov _",
    "galtchouk , l.,@xmath554 pergamenshchikov , s.(2007 ) .",
    "adaptive nonparametric estimation in heteroscedastic regression models .",
    "sharp non - asymptotic oracle inequalities .",
    "preprint of the strasbourg louis pasteur university , irma , 2007/09 , available at _ http://hal.archives-ouvertes.fr/hal/-00179856/fr/ _",
    "galtchouk , l.,@xmath554 pergamenshchikov , s.(2008 ) .",
    "sharp non - asymptotic oracle inequalities for nonparametric heteroscedastic regression models .",
    "_ journal of nonparametric statistics _",
    ", accepted to publication"
  ],
  "abstract_text": [
    "<S> the paper deals with asymptotic properties of the adaptive procedure proposed in the author paper , 2007 , for estimating a unknown nonparametric regression . we prove that this procedure is asymptotically efficient for a quadratic risk , i.e. the asymptotic quadratic risk for this procedure coincides with the pinsker constant which gives a sharp lower bound for the quadratic risk over all possible estimates . </S>"
  ]
}