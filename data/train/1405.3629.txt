{
  "article_text": [
    "consider the following markov chain @xmath3 where the random variable @xmath4 is the original message ( which is to be estimated on the basis of @xmath5 only ) , each @xmath6 is a standard vector - gaussian channel of dimension @xmath7 : @xmath8 and each input @xmath9 satisfies a power constraint : @xmath10 \\le d e\\,.\\ ] ] the goal is to design the transition kernels @xmath11 , which we refer to as processors or encoders , to facilitate the estimation of @xmath4 at the end of the chain .",
    "see fig .",
    "[ fig : control ] for an illustration .",
    "intuitively , at each stage some information about the original message @xmath4 is lost due to the external noise .",
    "furthermore , each processor can not de - noise completely due to the finite power constraint .",
    "therefore it is reasonable to expect that for very large @xmath0 we should have @xmath12 that is , @xmath4 and @xmath5 become almost independent .",
    "we quantify this intuition in terms of the total variation , kullback - leibler ( kl ) divergence and correlation , namely @xmath13 - q[e]| = { 1\\over 2 } \\int |{{\\rm d}}p - { { \\rm d}}q| , \\label{eq : tv}\\\\         d(p\\|q ) & \\eqdef \\int \\log { { { \\rm d}}p\\over { { \\rm d}}q}\\ , { { \\rm d}}p,\\\\         \\rho(a , b ) & \\eqdef { \\ee[ab ] - \\ee[a ] \\ee[b]\\over \\sqrt { \\var[a ] \\var[b]}},\\\\         i(a;b ) & \\eqdef d(p_{a , b } \\| p_a p_{b } ) .",
    "\\end{aligned}\\ ] ] our main result is the following theorem , which shows that the information about the original message is eventually lost in both an information - theoretic and an estimation - theoretic sense .    [",
    "th : main ] let @xmath14 for a markov chain as in  ( [ eq : mc1 ] ) ",
    "( [ eq : mc3 ] ) . then @xmath15 where @xmath16 are some universal constants . moreover , the right - hand side of  ( [ eq : main2 ] ) is @xmath17 if @xmath4 is finitely valued and @xmath18 if @xmath4 is sub - gaussian , respectively .    when @xmath4 is scalar gaussian , all estimates of the convergence rates in are sharp , in the sense that there exists a sequence of power - constrained relay functions such that @xmath19 , @xmath20 and @xmath21 .",
    "our interest in the problem has been mainly motivated by the fact that the moment constraint  ( [ eq : mc3 ] ) renders the standard tools for estimating convergence rates of information measures inapplicable .",
    "thus a few new ideas are developed in this paper . in order to explain this subtlety , it is perhaps easiest to contrast theorem  [ th : main ] and especially  ( [ eq : main3 ] ) with the recent results of subramanian et al .",
    "@xcite on cascades of awgn channels .",
    "other applications of our techniques are deferred till section  [ sec : appl ] .    in  @xcite an upper estimate on @xmath22",
    "is derived under extra constraints on relay functions . among these constraints ,",
    "the most important one is that the average constraint  ( [ eq : mc3 ] ) is replaced with a seemingly similar one : @xmath23 it turns out , however , that for the analysis of  ( [ eq : mc3sub ] ) the standard tools ( in particular the dobrushin contraction coefficient ) not only recover all the results of  @xcite but in fact simplify and strengthen them .",
    "thus , we start with describing those classical methods in the next section , and describe how to analyze  ( [ eq : mc3sub ] ) in section  [ sec : subram ] to follow .    _ added in print : _ a completely different method ( without recoursing to the total variation ) for showing  ( [ eq : main3 ] ) has been developed in  @xcite based on strong data processing inequalities for mutual information in gaussian noise .",
    "( f1 ) @xmath24 ; ( z0 ) [ left of = f1 , node distance=1.6 cm ] @xmath4 ; ( x0 ) [ left of = z0,node distance=1.5 cm , coordinate ] ; ( x1 ) [ right of = f1 , node distance=1.5 cm ] ; ( z0 ) edge node ( f1 ) ; ( f1 ) edge node @xmath25 ( x1 ) ;    ( z1 ) [ right of = f1,node distance=1.85 cm ] @xmath26 ; ( f2 ) [ right of = z1,node distance=1.85 cm ] @xmath27 ; ( z1 ) edge node @xmath28 ( f2 ) ; ( x2 ) [ right of = f2 , node distance=1.5 cm ] ; ( f2 ) edge node @xmath29 ( x2 ) ;    ( dots ) [ right of = x2,node distance=0.3 cm ] @xmath30 ; ( fn ) [ right of = dots , node distance=0.6 cm ] @xmath31 ; ( xn ) [ right of = fn , node distance=1.5 cm ] ; ( fn ) edge node @xmath32 ( xn ) ; ( zn ) [ right of = fn , node distance=1.85 cm ] @xmath26 ; ( yn ) [ right of = zn , node distance=1.6 cm ] @xmath5 ; ( zn ) edge node ( yn ) ;    to 40pt=.8tikz diagram      fix a transition probability kernel ( channel ) @xmath33 acting between two measurable spaces .",
    "we denote by @xmath34 the distribution on @xmath35 induced by the push - forward of the distribution @xmath36 , which is the distribution of the output @xmath37 when the input @xmath38 is distributed according to @xmath36 , and by @xmath39 the joint distribution @xmath40 if @xmath41 .",
    "we also denote by @xmath42 the serial composition of channels .",
    "let @xmath43 be a convex function with @xmath44 and let @xmath45 $ ] denote the corresponding @xmath46-divergence , cf .  @xcite . for example",
    "taking @xmath47 we obtain the @xmath48-divergence : @xmath49 for any @xmath50 that is not a point mass , define : @xmath51 for @xmath52 , @xmath47 and @xmath53 we will write @xmath54 and @xmath55 , respectively . in particular , @xmath56 is known as the _ dobrushin s coefficient _ of the kernel @xmath57 , which is one of the main tools for studying ergodicity property of markov chains as well as gibbs measures .",
    "[ [ general - alphabets ] ] general alphabets + + + + + + + + + + + + + + + + +    dobrushin  @xcite showed that supremum in the definition of @xmath56 can be restricted to single - point distributions @xmath36 and @xmath50 , thus providing a simple criterion for strong ergodicity of markov processes .",
    "it is well - known , e.g.  sarmanov @xcite , that @xmath58 is the squared _",
    "maximal correlation _ coefficient of the joint distribution @xmath59 : @xmath60 later  ( * ? ? ?",
    "* proposition ii.4.10 ) ( see also ( * ? ? ?",
    "* theorem 4.1 ) for finite alphabets ) demonstrated that all other contraction coefficients are upper - bounded by the dobrushin s coefficient @xmath56 : @xmath61 and this inequality is typically strict .",
    "we have @xmath62 . ] in the opposite direction it can be shown , cf .",
    "* proposition ii.6.15 ) , @xmath63 whenever @xmath46 is thrice differentiable with @xmath64 .",
    "moreover , holds with equality for all nonlinear and operator convex @xmath46 , e.g. , for kl divergence and for squared hellinger distance ; see  ( * ? ? ?",
    "* theorem 1 ) and ( * ? ? ?",
    "* proposition ii.6.13 and corollary ii.6.16 ) .",
    "in particular , @xmath65 which was first obtained in @xcite using different methods . rather naturally , we also have  ( * ? ? ?",
    "* proposition ii.4.12 ) : @xmath66 for any non - linear @xmath46 .",
    "the fixed - input contraction coefficient @xmath55 is closely related to the ( modified ) log - sobolev inequalities . indeed , when @xmath50 is invariant under @xmath57 ( i.e. @xmath67 ) any initial distribution @xmath36 converges to @xmath50 exponentially fast in terms of @xmath68 with exponent upper - bounded by @xmath55 , which in turn can be estimated from log - sobolev inequalities , e.g.  @xcite .",
    "when @xmath50 is not invariant , it was shown  @xcite that @xmath69 holds for some universal constant @xmath70 , where @xmath71 is a modified log - sobolev ( also known as @xmath72-log - sobolev ) constant : @xmath73 \\over \\ee[f^2(x ) \\log f^2(x ) ] } , \\qquad p_{x x ' } = q \\times     ( p_{x|y } \\circ p_{y|x } ) .\\ ] ]    [ [ finite - alphabets ] ] finite alphabets + + + + + + + + + + + + + + + +    ahlswede and gcs  @xcite have shown @xmath74 as a criterion for @xmath75 , this is an improvement of  ( [ eq : eta_ub ] ) only for channels with @xmath76 . furthermore ,  @xcite shows @xmath77 with inequality frequently being strict .",
    "we note that the main result of  @xcite characterizes @xmath55 as the maximal ratio of hyper - contractivity of the conditional expectation operator @xmath78 $ ] . for finite alphabets , can be strengthened to the following fixed - input version under the same conditions on @xmath46 ( c.f .",
    "* theorem 3.3 ) ) : @xmath79 for connections between @xmath80 and log - sobolev inequalities on finite alphabets see  @xcite .",
    "first , it can be shown that ( see for a proof in the general case . the finite alphabet case has been shown in @xcite ) @xmath82 where the supremum is taken over all markov chains @xmath83 with fixed @xmath40 such that @xmath84 .",
    "thus , for an arbitrary markov chain @xmath85 with equal channels @xmath86 for all @xmath87 , we have @xmath88 a similar argument leads to @xmath89 thus , in the simple case when @xmath81 we have from  ( [ eq : eta_ub ] ) that when @xmath90 , all three information quantities converge to zero exponentially as fast as @xmath91 .",
    "let us now consider the case of  @xcite , namely the awgn channel @xmath57 with maximal power constraint  ( [ eq : mc3sub ] ) .",
    "first recall that @xmath92 where @xmath93 is the gaussian complimentary cdf and @xmath94 denotes the euclidean norm .",
    "then by dobrushin s characterization of @xmath56 we get that for any @xmath95 satisfying  ( [ eq : mc3sub ] ) we have @xmath96 from  ( [ eq : expo1 ] ) this implies @xmath97    it turns out  ( [ eq : fuck_sub ] ) is stronger than the main result of  @xcite and independent of the cardinality of @xmath4 . indeed , although  @xcite did not point this out , the analysis there corresponds to the following upper - bound on @xmath56 @xmath98 ( here we assumed finite alphabet @xmath35 for simplicity ) .",
    "this bound is clearly tight for the case of @xmath99 but rather loose for larger @xmath100 .",
    "since we calculated @xmath56 exactly ,  ( [ eq : fuck_sub ] ) must yield a better bound than that of  @xcite .",
    "however , the estimate relies on the dobrushin coefficient , which , as will be shown below , breaks down if the power constraints is imposed on average instead of almost surely . to remedy this problem requires developing new tools to complement the dobrushin coefficient . for the generalization to average power constraint as well as",
    "discussions for multi - hop communication , see and in .",
    "the main part of this paper handles convergence of @xmath101 in the case  ( [ eq : mc3 ] ) , for which unfortunately @xmath102 . indeed , by taking @xmath103 and performing a straightforward calculation",
    ", we find @xmath104 therefore , even if one restricts the supremum in  ( [ eq : eta_f ] ) to @xmath36 and @xmath50 satisfying the moment constraint  ( [ eq : mc3 ] ) ( in fact , any constraint on the tails for that matter ) , choosing @xmath105 and @xmath106 accordingly drives the ratio in to one , thus proving @xmath76 .",
    "this example is instructive : the ratio  ( [ eq : rt1 ] ) approaches @xmath72 only when the @xmath107 .",
    "our idea is to get _ non - multiplicative _ contraction inequalities that still guarantee strict decrease of total variation after convolution .",
    "similarly , there is no moment condition which can guarantee the strict contraction of the kl divergence or mutual information .",
    "for example , it can be shown that @xmath108 where the supremum is over all markov chains @xmath109 with @xmath110 \\leq 1 $ ] .",
    "this suggests that the exponential decay of mutual information in obtained under peak power constraint might fail .",
    "indeed , we will show that under average power constraint , the decay speed of mutual information can be much slower than exponential ( see ) .",
    "the rest of the paper is organized as follows .",
    "section  [ sec : curve ] proves results on reduction of total variation over additive - noise channels ; we call the resulting relation the _ dobrushin curve _ of a channel",
    ". section  [ sec : mate ] shows how to convert knowledge about total variation to other @xmath46-divergences , extending  ( [ eq : eta_ub ] ) .",
    "section  [ sec : proof ] shows how to use dobrushin curve to prove theorem  [ th : main ] .",
    "finally , section  [ sec : appl ] concludes with applications ( other than theorem  [ th : main ] ) .    in particular , in",
    "we show that the optimal correlation achieved by non - linear control in the @xmath0-stage gaussian quadratic control problem studied by lipsa and martins @xcite is @xmath111 ; in contrast , the best linear controller only achieves exponentially small correlation .",
    "the inferiority of linear control can be explained from the viewpoint of dissipation of information and contraction of kl divergence . in",
    "we extend dobrushin s strategy for proving uniqueness of gibbs measures to unbounded systems with moment constraints on marginal distributions . and",
    "in we apply our technique to proving a lower bound on the probability of error in circuits of noisy gates .",
    "finally , in we show that in the question of broadcasting a single bit on a tree of gaussian channels there is no phase transition .",
    "namely , for arbitrarily low snr it is possible to build relays satisfying the average power constraint so that given the received values on all leaves at depth @xmath7 the probability of error of estimating the original bit is bounded away from @xmath112 .",
    "this is in contrast to the case of trees of binary symmetric channels , studied by evans - kenyon - peres - schulman  @xcite , who showed that there there is a phase transition in terms of the strength of the channel noise .",
    "let @xmath113 be a probability transition kernel .",
    "then , we define the _ dobrushin curve _ of @xmath57 as follows : @xmath114      \\label{eq : ft}\\ ] ] where @xmath115 is some ( convex ) set of pairs of probability measures .",
    "the curve @xmath116 defines the upper boundary of the region @xmath117 ^ 2 ,      \\label{eq : calf}\\ ] ] which is the joint range of the input and output total variations .",
    "we notice the following `` data - processing '' property of dobrushin curves : if @xmath118 and @xmath119 are the dobrushin curves of channels @xmath120 and @xmath121 ( and the respective feasible sets @xmath122 and @xmath123 ) , then for any @xmath124 that connects them : @xmath125 we naturally have for the combined channel @xmath126 ( the constraint set @xmath115 corresponding to @xmath127 is defined so that @xmath128 and @xmath129 )",
    ". this observation will be central for the analysis of the markov chain  ( [ eq : mc1 ] ) .",
    "we proceed to computing @xmath130 .",
    "for simplicity , in the sequel we focus our presentation on the following :    1 .",
    "consider @xmath131 with borel @xmath132-algebra and @xmath133 .",
    "2 .   there is a norm @xmath94 on @xmath134 .",
    "the constraint set @xmath115 is defined by some average cost constraint : @xmath135 + { \\mathbb{e}}_q[{{{\\mathsf{m}}}}(|x| ) ] \\leq 2 a \\ } ,      \\label{eq : ga}\\ ] ] where @xmath136 is a strictly increasing convex cost function with @xmath137 , @xmath138 and @xmath139 with @xmath140 , which we call @xmath141-moment , sub - exponential and sub - gaussian constraints , respectively . ] with @xmath142 and @xmath143 .",
    "4 .   the random transformation @xmath57 acts by convolution ( on @xmath134 ) with noise @xmath144 : @xmath145    [ rmk : ftvconcave ] for any point @xmath146 in the region @xmath147 and @xmath148 $ ]",
    ", we can achieve the point @xmath149 by setting @xmath150 and @xmath151 .",
    "this implies that @xmath152 is non - increasing .",
    "however , this does not imply that @xmath147 is convex or that @xmath130 is concave . shortly",
    ", we will demonstrate that for many noise distribution @xmath144 the dobrushin curve @xmath130 is in fact concave .",
    "expanding on the previous remark , we can further show relations between @xmath130 computed for different cost values of @xmath153 in  ( [ eq : ga ] ) .",
    "[ prop : yw ] let @xmath154 be the dobrushin curve for some channel @xmath57 and constraint  ( [ eq : ga ] ) , where @xmath142 .",
    "then for all @xmath155 such that @xmath156 we have @xmath157 in particular , @xmath158 , where @xmath159 and in the unconstrained case dobrushin curve is a straight line : @xmath160 .    without loss of generality , we may assume @xmath161 ( otherwise , apply to @xmath162 and @xmath163 )",
    ". for all @xmath164 $ ] we have two inequalities @xmath165 to show the first start with arbitrary @xmath166 such that @xmath167 and @xmath168",
    ". then we can construct distributions @xmath169 for which @xmath170 and thus  ( [ eq : yw1 ] ) follows after optimizing over @xmath171 .",
    "the second inequality follows by considering and @xmath172 denote the two pieces of jordan decomposition of measure @xmath173 . ]",
    "@xmath174 and a similar argument .",
    "finally ,  ( [ eq : yw0 ] ) follows from  ( [ eq : yw1 ] ) ( with @xmath175 ) and  ( [ eq : yw2 ] ) ( with @xmath176 ) .",
    "similar to how dobrushin s results  @xcite reduce the computation of @xmath56 to considering the two - point quantity @xmath178 , our main tool will be the following function @xmath179 $ ] defined by @xmath180 some simple properties of @xmath181 ( general case ) are as follows :    * @xmath182 , @xmath183 .",
    "* @xmath184 . *",
    "if @xmath144 is compactly supported then @xmath185 when @xmath186 is sufficiently large .",
    "* @xmath181 is lower - semicontinuous ( since total variation is weakly lower - semicontinuous ) . * if @xmath144 has a density @xmath187 , then @xmath188 and @xmath181 is continuous on @xmath189 , which follows from the denseness of compactly - supported continuous functions in @xmath190",
    ".    further properties of @xmath181 in dimension @xmath191 include :    * @xmath181 is continuous at 0 if and only if @xmath192 has a density with respect to the lebesgue measure . to see this , decompose @xmath193 into absolutely continuous and singular parts ( with respect to the lebesgue measure ) . by  (",
    "* theorem 10 ) , @xmath194 if and only if @xmath144 is absolutely continuous . by the previous remark we have @xmath195 *",
    "if @xmath144 has a non - increasing density supported on @xmath196 , then @xmath197 is a concave , non - decreasing function on @xmath196 given by @xmath198\\ , , \\qquad x\\ge0\\,.\\ ] ] * if @xmath144 has a symmetric density which is non - increasing on @xmath196 , then @xmath197 is a concave , non - decreasing function on @xmath196 given by @xmath199\\ , , \\qquad x \\ge 0\\ ] ] * in general , @xmath181 need not be monotonic on @xmath200 ( e.g. @xmath144 is discrete or has a multimodal density such as a gaussian mixture ) .",
    "the following result gives a necessary and sufficient condition for the total variation to strictly contract on an additive - noise channel , which essentially means that the noise distribution is almost mutually singular to a translate of itself .",
    "intuitively , it means that if the noise is too weak ( e.g. , when the noise has a compact support or has a singular distribution ) , then one can send one bit error - free if the signal magnitude is sufficiently large .",
    "[ th : ftv_contracts ] define @xmath201 the following are equivalent    1 .",
    "@xmath202 for some @xmath203 .",
    "2 .   @xmath204 in some neighborhood of @xmath205 .",
    "3 .   @xmath204 for some @xmath206 .",
    "it is possible to have @xmath202 with @xmath207 on @xmath208 $ ] .",
    "for example , let @xmath209 where @xmath210 denotes the uniform distribution on @xmath211 .",
    "the equivalence of @xmath212 and @xmath213 follows from remark  [ rmk : ftvconcave ] .",
    "for @xmath214 , choosing @xmath215 and @xmath216 , we have @xmath217 .",
    "optimizing over @xmath218 $ ] yields @xmath204 , provided that @xmath219 .    before proceeding further",
    ", we notice that for any channel @xmath57 with dobrushin coefficient @xmath56 and any measure @xmath220 on @xmath221 such that @xmath222 we have @xmath223 where here and below the total variation distance defined in naturally extended to non - probability measures as follows : @xmath224 next , by representing @xmath225 and playing with scaling @xmath226 or @xmath227 we get the result of  ( * ? ? ?",
    "* lemma 3.2 ) : @xmath228    now we prove @xmath229 .",
    "fix arbitrary @xmath230 and choose large @xmath203 .",
    "let @xmath231 be restrictions of @xmath36 and @xmath50 to the closed ball @xmath232 and @xmath233 . by  ( * ?",
    "* lemma 3.2 ) we have then : @xmath234 since @xmath230 , applying markov s inequality yields @xmath235 and thus @xmath236 also , since @xmath237 , we have @xmath238 putting it all together and using triangle inequality , we have @xmath239 where the equality step follows from the crucial fact that @xmath240 , due to the disjointedness of supports .    by the arbitrariness of @xmath171",
    ", we have shown that for every @xmath203 and @xmath241 , @xmath242 thus if @xmath204 for some @xmath206 , then @xmath243 for all @xmath203 .",
    "therefore we must have @xmath202 whenever @xmath244 .",
    "[ th : coupling ] define @xmath245 and let @xmath246 $ ] be the concave envelope ( i.e. , the smallest concave majorant ) of @xmath247 on @xmath196 , then @xmath248    note that for the upper bound  ( [ eq : coupling ] ) to be non - trivial , i.e. , better than @xmath249 , for all @xmath206 , it is necessary and sufficient to have @xmath250 for all @xmath251 .",
    "this is consistent with theorem  [ th : ftv_contracts ] .",
    "recall that @xmath252 , by definition of the function @xmath181 in .",
    "fix any @xmath230 .",
    "the map @xmath253 is convex ( as is any wasserstein distance ) , thus for any coupling @xmath254 with @xmath255 and @xmath256 we have @xmath257\\ ] ] furthermore , @xmath258 is necessarily continuous on @xmath259 , strictly increasing on @xmath260 and concave .",
    "thus , @xmath261 = & ~\\pp[a\\neq b ] \\ee[\\theta(a - b ) \\,|\\ , a\\neq b ] \\\\",
    "\\le & ~\\pp[a\\neq b ] \\ee[\\theta_c(|a - b| ) \\,|\\ , a\\neq b]\\\\                   \\le & ~\\pp[a\\neq b ] \\theta_c\\left(\\ee[|a - b| \\,|\\ , a\\neq b]\\right)\\label{eq : tc3 }               \\end{aligned}\\ ] ] where is by jensen s inequality and the concavity of @xmath258 . then @xmath262}{2 } \\right ) } \\leq & ~ \\ee{\\left [ { { { \\mathsf{m}}}}{\\left ( \\frac{|a - b|}{2 } \\right ) } \\big| a\\neq b \\right ] } \\label{eq : m1}\\\\ = & ~ \\frac{1}{{\\mathbb{p}\\left[a \\neq b\\right ] } } \\ee{\\left [ { { { \\mathsf{m}}}}{\\left ( \\frac{|a - b|}{2 } \\right ) } \\right ] }   \\label{eq : m2}\\\\",
    "\\leq & ~ \\frac{1}{{\\mathbb{p}\\left[a \\neq b\\right ] } } \\ee{\\left [ { { { \\mathsf{m}}}}{\\left ( \\frac{|a|+|b|}{2 } \\right ) } \\right ] }     \\label{eq : m3}\\\\ \\leq & ~ \\frac{\\ee[{{{\\mathsf{m}}}}(|a|)]+\\ee[{{{\\mathsf{m}}}}(|b|)]}{2{\\mathbb{p}\\left[a \\neq b\\right ] } } \\label{eq : m4}\\\\ \\leq & ~ \\frac{a}{{\\mathbb{p}\\left[a \\neq b\\right ] } } \\label{eq : m5}\\end{aligned}\\ ] ] where and are by jensen s inequality and the convexity of @xmath263 , is by @xmath142 , is by the monotonicity of @xmath263 , and is by the constraint @xmath264 .",
    "applying @xmath265 to both sides of and plugging into , we obtain @xmath266 \\le \\pp[a\\neq b ] \\theta_c\\left(2 { { { \\mathsf{m}}}}^{-1 } { \\left ( \\frac{a}{{\\mathbb{p}\\left[a \\neq b\\right ] } } \\right)}\\right ) .",
    "\\label{eq : tc2 }               \\end{aligned}\\ ] ] note that both @xmath265 and @xmath258 are increasing concave functions .",
    "thus their composition @xmath267 is concave and increasing too .",
    "furthermore it is easy to show that @xmath268 is increasing . hence the upper bound  ( [ eq : tc2 ] ) is tightest for the coupling minimizing @xmath269 $ ] . recall that by strassen s characterization  @xcite we have @xmath270 = { { \\sf tv}}(p , q),\\ ] ] where the infimum is over all couplings @xmath254 of @xmath36 and @xmath50 such that @xmath255 and @xmath256 .",
    "then  ( [ eq : tc1 ] ) and  ( [ eq : tc2 ] ) and the continuity of @xmath258 imply the upper bound in  ( [ eq : coupling ] ) .    for the lower bound",
    ", we choose @xmath271 with @xmath272 , which ensures that @xmath166 .",
    "it is straightforward to show that @xmath167 and @xmath273 .",
    "taking the supremum over @xmath251 yields the left inequality of .",
    "if the dimension @xmath191 and @xmath247 is concave on @xmath196 then @xmath274 [ cor : ftv - z ]    examples of the noise distributions satisfying assumptions of are given by  ( [ eq : conc0 ] ) and  ( [ eq : conc1 ] ) .",
    "note that from concavity of @xmath181 the map @xmath275 is also concave .",
    "therefore , the map @xmath276 is the _ perspective _ of the concave function  ( [ eq : mt1 ] ) , and hence is concave on @xmath277  @xcite .",
    "consequently , for fixed @xmath278 , @xmath130 is concave , which , as we mentioned , does not immediately follow from the definition of @xmath130 .",
    "[ rmk : tau ]    for the purpose of showing theorem  [ th : main ] we next point out the particularization of to the awgn channel . a representative plot of the @xmath130 for the awgn channel and average power constraint ( second - order moment ) is given in fig .",
    "[ fig : tvawgn ] , which turns out to be dimension - independent .",
    "let @xmath279 , @xmath280 and @xmath281 be the euclidean norm .",
    "then @xmath282 [ cor : ftv - g ]    from  ( [ eq : tvnorm ] ) we have that @xmath283 regardless of dimension and thus theorem  [ th : coupling ] yields  ( [ eq : ftv - g ] ) .",
    "in the gaussian case @xmath284 with @xmath285.,title=\"fig : \" ] ( -50,105 ) @xmath127   in the gaussian case @xmath284 with @xmath285.,title=\"fig : \" ]      recall the following 1-wasserstein distance between distributions with finite first moment : @xmath286 then the same coupling method in the proof of yields the following bound , which relates the total variation between convolutions to the @xmath287 distance .",
    "if @xmath144 has a symmetric density which is non - increasing on @xmath196 .",
    "then for any @xmath36 and @xmath50 , @xmath288}.      \\label{eq : tv - w1}\\ ] ] [ prop : tv - w1 ]    by , the function @xmath289 $ ] is concave and non - decreasing in @xmath251 . applying jensen s inequality to and optimizing over the coupling yields .",
    "it is worth mentioning that for gaussian smoothing , using similar coupling and convexity arguments , the following counterpart of for kl divergence has been proved in @xcite , which provides a simple proof of otto - villani s hwi inequality @xcite in the gaussian case : @xmath290 where the @xmath291 distance is analogously defined as with @xmath292-norm replacing @xmath293-norm .",
    "in particular , if @xmath144 has a bounded density near zero , then the right - hand side of is @xmath294 . as an application",
    ", we consider a central limit theorem setting and let @xmath295 where @xmath9 are iid , zero - mean and unit - variance .",
    "choosing @xmath296 and applying to @xmath297 and @xmath298 , we obtain @xmath299 \\over \\sqrt{2 \\pi \\sigma^2 n } }            \\label{eq : smooth - clt}\\ ] ] where the convergence rate in @xmath287 can be obtained from stein s method and the dual representation of @xmath300 ( see , e.g. , ( * ? ? ?",
    "* theorem 3.2 ) ) .",
    "in other words , smoothing the law of @xmath301 by convolving with a gaussian density ( or any other bounded density that satisfies the conditions of  ( [ eq : conc1 ] ) ) results in a distribution that is closer in total variation to the gaussian distribution . on the other hand , the law of @xmath301 might never converge to gaussian ( e.g. , for discrete @xmath25 ) .",
    "the non - asymptotic estimate should be contrasted with the sharp asymptotics of total variation in clt due to sirazhdinov and mamatov @xcite , which states that the left - hand side of is equal to @xmath302}{6\\sqrt{2 \\pi n ( 1+\\sigma^2)^3 } } ( 1+o(1))$ ] when @xmath90 and @xmath132 is fixed .",
    "the main apparatus for obtaining the dobrushin curve of total variation in is the infimum - representation via couplings , thanks to the special role of the total variation as a wasserstein distance .",
    "unfortunately such representation is not known for other divergences such as the hellinger distance or kl divergence . to extend the contraction property of total variation ,",
    "our strategy is as follows : we first study a special family of @xmath46-divergences @xmath303 , which enjoys the same contraction property as the total variation for any channel . then using an integral representation of general @xmath46-divergences @xcite in terms of @xmath304 , we extend the contraction results in for additive - noise channels to @xmath46-divergences , in particular , rnyi divergences .      for a pair of distributions @xmath305 , define the following family of @xmath46-divergences parameterized by @xmath306 : @xmath307 typical plots of @xmath308 are given in where @xmath36 and @xmath50 are gaussians or bernoullis .",
    "[ fig : mate - g ] .,title=\"fig : \" ] [ fig : mate - b ] .,title=\"fig : \" ] [ fig : mate - g ] .,title=\"fig : \" ] [ fig : mate - b ] .,title=\"fig : \" ]    some general properties of @xmath309 are as follows :    1 .",
    "2 .   @xmath311 .",
    "3 .   @xmath312 is convex , positive , increasing on @xmath313 $ ] , and decreasing on @xmath314 .",
    "4 .   reciprocity : @xmath315 5 .",
    "derivative of @xmath316 recovers @xmath317 $ ] : @xmath318 - \\frac{1}{2 } \\nonumber \\\\          & = { { \\mathbf{1}_{\\left\\{{\\gamma < 1}\\right\\ } } } } - q\\big[{\\frac{{{\\rm d}}p}{{{\\rm d}}q } } > \\gamma\\big ]   \\label{eq : dmate }      \\end{aligned}\\ ] ] 6 .",
    "@xmath319-contraction property : if @xmath320 are outputs of @xmath305 under some channel @xmath57 with known @xmath130 , then @xmath321 this follows from the more general result below , which shows that the divergence @xmath309 for general @xmath322 enjoys the same ( if not better ) contraction property as the total variation , i.e. , @xmath323 .",
    "assume that for each choice of @xmath278 in  ( [ eq : ga ] ) the corresponding @xmath130 curve is denoted by @xmath324 .",
    "then for any channel @xmath57 and any @xmath264 we have @xmath325 and , in particular ,  ( [ eq : mate_contracts ] ) holds .",
    "[ prop : mate ]    first notice that if @xmath220 is any signed measure on @xmath326 satisfying @xmath327 for some @xmath328 , then we have . ]",
    "@xmath329 indeed , let @xmath225 be the jordan decomposition of @xmath220 . then by the assumption we have that @xmath330 are mutually singular sub - probability measures .",
    "thus by introducing @xmath331 for some constant @xmath332 chosen so that @xmath36 and @xmath50 are probability measures , we get @xmath333 since @xmath334 . in turn ,  ( [ eq : rx2 ] ) is equivalent to  ( [ eq : rx1 ] ) .    now consider @xmath335 and a pair of probability measures @xmath166 .",
    "write @xmath336 and set @xmath337 since @xmath338 , which follows from the convexity of @xmath339 , we have @xmath340",
    ". then @xmath341 + c \\ , \\ee_p[{{{\\mathsf{m}}}}(|x| ) ] \\le 2\\gamma a\\,.\\ ] ] consequently @xmath220 satisfies condition with @xmath342 .",
    "furthermore , observe that for @xmath343 we have @xmath344 we have @xmath345 .",
    "thus from  ( [ eq : rx1 ] ) we get @xmath346 next from the representation @xmath347 and the triangle inequality we have @xmath348 in view of , it remains to notice that the last term in  ( [ eq : rx3 ] ) equals @xmath349 , from which  ( [ eq : mec ] ) follows via @xmath350    for @xmath351 the proof is entirely analogous , except that we set @xmath352 and the best bound we have on @xmath353 is @xmath354 , which follows from the fact that @xmath355 and hence @xmath356 .      for an @xmath46-divergence , analogous to the dobrushin curve",
    "we define @xmath357 note that the usual data processing inequality amounts to @xmath358 .",
    "we say the channel @xmath57 _ contracts _ the @xmath46-divergence @xmath359 if @xmath360 for all @xmath241 in a neighborhood near zero .",
    "we have already shown that the total variation is always contracted by additive noise satisfying the necessary and sufficient condition in . in view of",
    ", the formulas in corollaries [ cor : ftv - z ] and [ cor : ftv - g ] apply to @xmath309 as well .",
    "a natural question is in order : do other divergences , such as the kl divergence , also contract in additive noise ? to this end , we need the following integral representation of @xmath46-divergences in terms of the family of divergence @xmath309 : if @xmath361 , then ( see ( * ? ? ?",
    "* corollary 3.7 , p. 99 ) ) @xmath362 for instance , the area under the curve @xmath339 is half the @xmath48-divergence @xmath363 .    for conciseness , below we focus on the scalar awgn channel under the first moment constraint and the special case of rnyi divergence of order @xmath364 , which is a monotonic transformation of the @xmath365-divergence with @xmath366 note that the special case of @xmath367 corresponds to the kl divergence @xmath368 , the @xmath48-divergence @xmath369 , and half the squared hellinger distance @xmath370 , respectively .",
    "the following result shows that the awgn channel contracts rnyi divergence of order @xmath364 if and only if @xmath371 .",
    "consequently , the hellinger distance always contracts when passing through the awgn channel , but @xmath48 and kl divergences do not .",
    "consider the scalar awgn channel @xmath372 .",
    "let @xmath373 and @xmath278",
    ". then    1 .   for @xmath371 , for any @xmath374 ,",
    "@xmath375 2 .   for @xmath376 , @xmath377 which holds for all @xmath206 if @xmath378 and @xmath379 if @xmath380 , respectively .",
    "[ thm : falpha ]    @xmath381 fix @xmath371 and @xmath382 such that @xmath383 .",
    "let @xmath384 denote the standard normal distribution .",
    "fix @xmath385 . applying the integral representation to @xmath386",
    ", we have @xmath387 where follows from with @xmath388 , and follows from , and is due to @xmath389 . using , for all @xmath390",
    ", we have @xmath391 \\leq 1 $ ] . by the convexity of @xmath339 and @xmath392 , we have @xmath393 \\gamma \\leq \\gamma$ ] .",
    "therefore @xmath394 plugging into and by the arbitrariness of @xmath385 , we obtain @xmath395 which implies the desired upon choosing @xmath396 .",
    "@xmath397 turning to the case of @xmath376 , we construct examples where @xmath398 does not contract .",
    "fix @xmath399 and let @xmath400 be sufficiently small .",
    "let @xmath401 with @xmath402 and @xmath403 if @xmath404 and @xmath405 if @xmath378 .",
    "then it is clear that @xmath406 for all sufficiently small @xmath407 .",
    "furthermore , @xmath408 where @xmath409 if @xmath410 and @xmath411 if @xmath380 .    next , by applying the data - processing inequality to the transformation",
    "@xmath412 we get @xmath413 where @xmath414 and @xmath415 .",
    "this follows from the fact that @xmath416 , which is obvious for @xmath378 ; for @xmath404 , since we have assumed that @xmath417 , we have @xmath418 .",
    "consequently , @xmath419 as @xmath420 , which completes the proof of .",
    "extends in the following directions :    1",
    ".   for general additive noise @xmath192 , continues to hold with @xmath421 replaced by the concave envelope @xmath422 in .",
    "2 .   for the @xmath141-moment constraint with @xmath423 and @xmath424 , holds for all @xmath425 if @xmath378 . for kl divergence ( @xmath404 ) , however , it remains unclear whether holds in a neighborhood near zero since the above construction no longer applies .",
    "theorem  [ th : main ] follows from propositions  [ prop : tv - lost ] ,  [ prop : iconv ] and  [ prop : rconv ] given in sections  [ sec : t ] ,  [ sec : i ] and  [ sec : rho ] , respectively .",
    "the special case of finite - alphabet @xmath4 is much simpler and is treated by proposition  [ prop : itv ] ( section  [ sec : discrete ] ) .",
    "finally , shows that our converse bounds are optimal for total variation , mutual information and correlation in the scalar gaussian case .      the development in deals with comparing a pair of distributions and studies by how much their total variation shrinks due to smoothing by the additive noise .",
    "therefore these results are applicable to binary sources , i.e. , transmitting one bit .",
    "what if the sources takes more than two , or rather , a continuum of , values ? to this end , the data processing inequality for mutual information is relevant , which states that @xmath426 implies that @xmath427 . in other words , dependency decreases on markov chains . our goal next is to find a quantitative data pre - processing and post - processing inequalities as a counterpart of .",
    "since we know , in view of , that kl divergence does not contract , it is natural to turn to total variation and define the following _ @xmath428-information _ : @xmath429 which has been studied in , e.g. , @xcite .",
    "similar to mutual information , it is easy to see that the @xmath428-information satisfies the following properties :    1 .",
    "@xmath430={\\mathbb{e } } [ { { \\sf tv}}(p_{x|y } , p_x)]= t(y;x)$ ] .",
    "data - processing inequality : @xmath426 implies that @xmath431 .",
    "3 .   if @xmath432 is bern(@xmath433 ) , then @xmath434 4 .   if @xmath432 and @xmath435 are both binary , then , @xmath436 $ ] and @xmath437 $ ] . then @xmath438 $ ] . ]",
    "@xmath439},{\\mathbb{p}\\left[s=1\\right]}\\ } - { \\mathbb{p}[s \\neq \\hat s]}.      \\label{eq : t - binary}\\ ] ] 5 .",
    "pinsker s inequality : @xmath440    the next theorem gives a quantitative data processing theorem for the @xmath428-information with additive noise :    [ thm : tvproc ] let @xmath441 , where @xmath442 and @xmath443 \\leq a$ ] .",
    "let @xmath258 be as in theorem  [ th : coupling ]",
    ". then @xmath444    exactly the same inequality holds for the following functional of real - valued random variables @xmath445 \\le a } { { \\sf tv}}(p_{ab } , p_a q_b),\\ ] ] which is a natural extension of the @xmath446-information of sibson  @xcite and csiszr  @xcite and satisfies @xmath447 . optimizing over @xmath448 instead of taking @xmath449 may lead to more powerful converse bounds , see  @xcite for details .    by the definition of @xmath450 and the markov chain condition",
    ", we have @xmath451 then yields @xmath452 +      { 1\\over2}\\ee[{{{\\mathsf{m}}}}(|x| ) ] \\right)\\ , .",
    "\\label{eq : ttww}\\ ] ] in view of , the function @xmath46 defined in is jointly concave and non - decreasing in each argument .",
    "thus taking expectation over @xmath453 on the right - hand side of  ( [ eq : ttww ] ) and applying jensen s inequality , we complete the proof .",
    "as an application of , next we describe how the @xmath428-information decays on the markov chain .",
    "[ prop : tv - lost ] assume the markov chain , where @xmath454 are i.i.d . and @xmath455 \\leq a$ ] for all @xmath456 $ ] .",
    "then for all @xmath278 and @xmath457 , @xmath458 where @xmath459 .    in particular , if @xmath460 are i.i.d .",
    ", then @xmath461 where @xmath462 { { \\rm d}}\\tau$ ] , and @xmath70 is a positive constant only depending on the cost function @xmath263 .    particularizing the result of to the awgn channel and the following cost functions we obtain the corresponding convergence rates    a.   @xmath141-moment constraint : @xmath463 for some @xmath137 . then @xmath464 .",
    "in particular , for power constraint @xmath465 , holds .",
    "b.   sub - exponential : @xmath466 for some @xmath467 and @xmath468",
    ". then @xmath469 .",
    "c.   sub - gaussian : @xmath466 for some @xmath467 and @xmath468",
    ". then @xmath470",
    ".    intuitively , the faster the cost function grows , the closer we are to amplitude - constrained scenarios , where we know that information contracts linearly thanks to the dobrushin s coefficient being strictly less than one .",
    "hence we expect the convergence rate to be faster and closer to , but always strictly slower than , exponential decay . in view of , implies that transmitting one bit is impossible under any cost constraint , since the optimal type - i+ii error probability is given by @xmath471 ( see  ( * ? ? ?",
    "* theorem 13.1.1 ) ) and the total - variation vanishes as @xmath90 .",
    "the slow convergence rates obtained above for gaussian noise can be explained as follows : in view of , the @xmath428-information obeys the iteration @xmath472 . for instance , consider the dobrushin curve under unit power constraint is given by @xmath473 , which satisfies @xmath474 and all other derivatives vanish at zero .",
    "therefore @xmath130 is smooth but not real analytic at zero , and the rate of convergence of the iteration @xmath475 to the fixed point zero is very slow .",
    "see for an illustration .",
    "[ rmk : rate - awgn ]    by , we have @xmath476 where the first inequality follows from , and the second inequality follows from the data processing theorem for @xmath428 and the monotonicity of @xmath130 .",
    "applying , we have @xmath477 repeating the above argument leads to @xmath478 where the sequence @xmath479 is defined iteratively via @xmath480 with @xmath481 and @xmath482 . by , @xmath258 is strictly increasing .",
    "therefore @xmath483 is an increasing function . applying in",
    ", the convergence rate of the sequence satisfies @xmath484 where @xmath485 .    for the gaussian noise",
    ", we have @xmath486 ( see ) . in view of the bound @xmath487 for @xmath488 , where @xmath489 denote the standard normal density ,",
    "follows from upon changes of variables .",
    "a consequence of the total variation estimates in and is that for finitely - valued message @xmath4 they entail estimates on the mutual information and maximal correlation , as the next proposition shows . )",
    "is essentially  ( * ? ? ?",
    "* lemma 1 ) .",
    "the bound  ( [ eq : chi_tv ] ) was shown by f. p. calmon  ` < flavio@mit.edu > ` and included here with his permission . ]    [ prop : itv ] assume @xmath4 take values on a finite set @xmath490 and let @xmath491 denote the minimal non - zero mass of @xmath492 .",
    "then @xmath493 where @xmath494 and @xmath48 are defined in  ( [ eq : chi_maxcor0 ] ) and  ( [ eq : chi2def ] ) , respectively , and @xmath495 is the binary entropy function .    by coupling and fano s inequality , for any @xmath36 and @xmath50 on @xmath490 , we have @xmath496 then @xmath497 \\\\ \\leq & ~     \\log ( |{{\\mathcal{w}}}|-1 ) { { \\sf tv}}(p_wp_y , p_{wy } ) + h({{\\sf tv}}(p_wp_y , p_{wy } ) ) , \\ ] ] where the last step is due to the concavity of @xmath498 .",
    "the inequality  ( [ eq : chi_maxcor ] ) follows  @xcite by noticing that @xmath499 is the sum of squares of the singular values of @xmath500 $ ] minus 1 ( the largest one ) , while @xmath494 is the second largest singular value .",
    "bound  ( [ eq : chi_tv ] ) follows from the chain : @xmath501 - 1\\\\       & = \\ee_{p_{wy}}\\left[{p_{w|y}(w|y)\\over p_w(w ) } \\right ] -\\ee_{p_w p_y}\\left[{p_{w|y}(w|y)\\over p_w } \\right ] \\\\       & \\le \\esssup_{w , y } { p_{w|y}(w|y)\\over p_w(w ) } \\cdot { { \\sf tv}}(p_{wy } , p_w p_y)\\\\       & \\le { 1\\over p_{w,\\min } }   t(w;y)\\,,\\end{aligned}\\ ] ] where first step is by  ( [ eq : chi2def ] ) and the rest are self - evident .    combining propositions [ prop : tv - lost ] and [ prop : itv ] , we conclude that both @xmath502 and @xmath503 vanish for finitely - valued @xmath4 .",
    "in particular , for gaussian noise , by ( second moment constraint ) we have @xmath504",
    ". then the maximal correlation satisfies @xmath505 and the mutual information vanishes according to @xmath506      in this subsection we focus on the awgn channel and show that the convergence rate continues to hold for _ any _ random variable @xmath4 , which will be useful for applications in optimal stochastic control where @xmath4 is gaussian distributed .",
    "such that @xmath507 and @xmath508 for all @xmath509 and @xmath510\\le e$ ]",
    ". then   follows by applying   repeatedly and the behavior of @xmath511 curve near zero : @xmath512 . ] to deal with non - discrete @xmath4 , a natural idea to apply is _",
    "quantization_. by propositions  [ prop : tv - lost ] and  [ prop : itv ] , for any quantizer @xmath513 $ ] , we have @xmath514 for some universal constant @xmath70 .",
    "a natural conjecture is the following implication : for any sequence of channels @xmath515 we have : @xmath516 : i(q(w ) ; y_n ) \\to 0 \\quad \\implies \\quad i(w;y_n ) \\to0,\\ ] ] which would imply the desired conclusion that mutual information vanishes .",
    "somewhat counter - intuitively , this conjecture is generally false , as the following counterexample shows : consider @xmath517)$ ] and @xmath518 on one hand it is clear that @xmath519 . on the other hand , among all @xmath520-point quantizers @xmath407 ,",
    "it is clear that the optimal one is to quantize to some levels corresponding to the partition that @xmath5 incurs ( other quantizers are just equivalent to randomization ) .",
    "thus @xmath521\\to[m ] } i(q(x ) ; y_n ) = \\sup_{q:[2^n+1]\\to[m ] } h(q(y_n ) ) .\\ ] ] but the rhs tends to zero as @xmath522 for any fixed @xmath520 because the dominating atom shoots up to @xmath72 .",
    "the same example also shows that @xmath523 nevertheless , under additional constraints on kernels @xmath524 , we can prove that indeed holds and obtain the convergence rate .",
    "the main idea is to show that the set of distributions @xmath525 can be grouped into finitely many clusters , so that the diameter ( in kl divergence ) of each cluster is arbitrarily small .",
    "this can indeed be done in our setting since the channel @xmath524 is a stochastically degraded version of an awgn channel .",
    "[ prop : iconv ] let @xmath526 be as in theorem  [ th : main ] .",
    "if @xmath527 \\leq d e$ ] for all @xmath528 $ ] , then @xmath529 where @xmath70 is the absolute constant in . in particular , for fixed @xmath7 and @xmath530 , @xmath531    note that the upper bound  ( [ eq : tp ] ) deteriorates as the dimension @xmath7 grows , which is to be expected .",
    "indeed , for large @xmath7 one can employ very reliable error - correcting codes for the awgn channel with blocklength @xmath7 , that can tolerate a large number of hops over the awgn channels .",
    "if the blocklength @xmath532 grows with @xmath0 such that @xmath533 and the power per coordinate @xmath530 is fixed , then reduces to @xmath534 using fano s inequality , this implies that in order to reliably communicate over @xmath0 hops at some positive rate , thereby @xmath535 , it is necessary to have the blocklength @xmath536 grow at least as fast as @xmath537 this conclusion has been obtained in @xcite under the simplified assumption of almost sure power constraint of the codebook ( see ) . here extends it to power constraint in expectation .",
    "[ rmk : rate - pos ]    fix @xmath538 to be specified later .",
    "it is well - known that the @xmath539-ball in @xmath540 of radius @xmath541 can be covered by at most @xmath542 @xmath539-balls of radius @xmath543 , whose centers are denoted by @xmath544 .",
    "define @xmath545 $ ] by @xmath546 } \\|x_i - x\\| \\bigg)}{{\\mathbf{1}_{\\left\\{{\\|x\\|\\leq u}\\right\\}}}}+ ( m+1){{\\mathbf{1}_{\\left\\{{\\|x\\| > u}\\right\\}}}}.\\ ] ] then @xmath547 } \\leq \\epsilon^2 $ ] for any @xmath548 $ ] . hence @xmath549}{d } \\right)}\\label{eq : tt2}\\\\          & \\le \\frac{d}{2}\\log{\\left ( 1 + \\frac{\\epsilon^2}{d } \\right)}\\label{eq : tt3 } ,      \\end{aligned}\\ ] ] where in  ( [ eq : tt1 ] ) we used the markov relation @xmath550 , and follows from the vector awgn channel capacity : @xmath551 } i(x;x+z ) = \\frac{d}{2 } \\log{\\left ( 1+\\frac{p}{d } \\right ) } ,      \\label{eq : awgn - mi}\\ ] ] where @xmath552 is independent of @xmath38 .",
    "similarly , @xmath553}{d } \\right ) } \\nonumber\\\\          & \\le \\frac{d}{2}\\log{\\left ( 1 + \\frac{e}{{\\mathbb{p}\\left[\\|x_1\\| > u\\right ] } } \\right)}\\label{eq : tt4 } ,      \\end{aligned}\\ ] ] where follows from the fact that @xmath554 { \\mathbb{p}\\left[\\|x_1\\| > u\\right ] } \\leq { \\mathbb{e}}[\\|x_1\\|^2]$ ] .",
    "averaging and over @xmath555 $ ] , we obtain @xmath556 } \\log{\\left ( 1 + \\frac{e}{{\\mathbb{p}\\left[\\|x_1\\| > u\\right ] } } \\right ) }     \\nonumber \\\\ \\leq & ~ \\frac{d}{2}\\log{\\left ( 1 + \\frac{\\epsilon^2}{d } \\right ) } + \\frac{d^2e}{2u^2 } \\log{\\left ( 1 + \\frac{u^2}{d } \\right ) } ,                  \\label{eq : tt0x}\\end{aligned}\\ ] ] where follows from the fact that @xmath557 is increasing on @xmath558 . ] and the chebyshev s inequality : @xmath559 } \\leq \\frac{{\\mathbb{e}}[\\|x_1\\|^2]}{u^2 } \\leq \\frac{de}{u^2}.      \\label{eq : cheb}\\ ] ] applying , we have @xmath560 where @xmath561 in view of    combining and yields @xmath562 choosing @xmath563 and @xmath564 yields the desired .      given a pair of random variables @xmath565 , the conditional expectation of @xmath38 given @xmath37 has the maximal correlation with @xmath38 among all functions of @xmath37 , i.e.@xmath566 ) = \\frac{\\|{\\mathbb{e}}[x|y]-{\\mathbb{e}}[x]\\|_2}{\\sqrt{{\\mathsf{var}}(x)}}\\,,\\ ] ] which is a simple consequence of the cauchy - schwartz inequality .",
    "as the next result shows , vanishing mutual information provides a convenient sufficient condition for establishing vanishing correlation coefficients .",
    "[ prop : rconv ] assume that @xmath567 < \\infty$ ] . for any sequence of @xmath524 , @xmath568 ) = 0 .",
    "\\label{eq : irho}\\ ] ] moreover , if @xmath4 is gaussian , then @xmath569 ) \\leq 1 - \\exp(- 2 i(w ; y_n ) ) \\le 2 i(w ; y_n ) .",
    "\\label{eq : irho - g}\\ ] ]    for the gaussian case , follows from the inequality @xmath570 which is equivalent to the gaussian rate - distortion formula . to see the implication , first notice the equivalence @xmath571)^2 ]",
    "\\to { \\mathsf{var}}(w ) \\quad\\iff\\quad \\rho(w , { \\mathbb{e}}[w|y_n ] ) \\to 0.\\ ] ] from here proposition  [ prop : rconv ] follows from the next ( probably well - known ) lemma .",
    "assume that @xmath572   < \\infty$ ] .",
    "let @xmath573 .",
    "denote the rate - distortion function of @xmath38 with respect to the mean - square error by @xmath574 then @xmath575 [ lmm : rd ]    ( @xmath576 ) the rate - distortion function is dominated by that of the gaussian distribution @xcite : @xmath577 where @xmath578 .",
    "( @xmath579 ) note that @xmath580 is decreasing and concave on @xmath581 $ ] , hence continuous on the open interval @xmath582 .",
    "suppose there exists @xmath583 such that @xmath584 .",
    "then by definition of the rate - distortion function , there exists a sequence of @xmath585 such that @xmath586 .",
    "note that @xmath588 for all @xmath0 .",
    "therefore the sequence @xmath589 is tight . by prokhorov",
    "s theorem , there exists a subsequence @xmath590 which converges weakly to some @xmath591 . by the lower semicontinuity of the divergence and the second - order moment , @xmath592 and @xmath593 .",
    "hence @xmath594 , contradicting @xmath595 .    allows us to capitalize on the results on mutual information in to obtain correlation estimates for the markov chain .",
    "in particular , combining with yields . additionally , if @xmath4 is gaussian , then yields @xmath596 ) = o(\\sqrt{i(w ; y_n ) } ) = o{\\left ( \\sqrt{\\frac{\\log \\log n}{\\log n } } \\right)}.          \\label{eq : main3-rate}\\ ] ] these prove the correlation part of the main result .    however , the estimate is not entirely satisfactory in the sense that it highly depends on the gaussianity of @xmath4 ; if @xmath4 is not gaussian , the rate - distortion function of @xmath4 is not explicitly known and it is unclear whether still applies . how to obtain quantitative estimates on the correlation coefficient if we only have sub - gaussianity or moment constraints on @xmath4 ?",
    "it turns out that one can circumvent mutual information completely and directly obtain correlation estimate from the @xmath428-information , whose convergence rate has been found in .",
    "the key connection between total variation and correlation is the following simple observation :    [ prop : rconv - ng ] assume @xmath4 is zero - mean , unit - variance . for any @xmath597",
    "$ ] we have @xmath598 ) \\leq 4 t(w;y)^{1-\\frac{1}{q } } \\|w\\|_{2q}^2 .",
    "\\label{eq : rconv - moment}\\ ] ] if @xmath4 is sub - gaussian and @xmath599 , we have @xmath598 ) \\leq { 8\\over \\log e } \\|w\\|^2_{\\psi_2 } \\ , t(w;y ) \\log \\frac{1}{t(w;y ) } \\ , .",
    "\\label{eq : rconv - subg}\\ ] ] where @xmath600 \\leq 2\\}$ ] is an orlicz norm .",
    "is reminiscent of tao s inequality @xcite and ( * ? ? ?",
    "* theorem 10 ) , which use mutual information to produce correlation estimates for bounded random variables : if @xmath601 , then @xmath598 ) \\leq \\frac{2}{\\log { { \\rm e } } } \\|w\\|_\\infty^2 i(w;y).\\ ] ] in contrast , uses @xmath428-information in lieu of mutual information and allows more general tail condition .    combining with the convergence rate of the @xmath428-information in",
    ", we obtain the corresponding convergence rate of correlation under various cost constraints on the relays and tail conditions on the original message @xmath4 .",
    "for example , in view of , if the cost function is @xmath602 and @xmath4 is sub - gaussian , then @xmath596 ) = o{\\left ( \\frac{\\sqrt{\\log \\log n}}{(\\log n)^{p/4 } } \\right)}.        \\label{eq : corr - pmoment - subg}\\ ] ] in particular , for average power constraint ( @xmath603 ) , the convergence rate applies to all sub - gaussian @xmath4 .",
    "we will show in the next subsection that is in fact optimal for all @xmath604 when @xmath4 is gaussian .",
    "[ rmk : rcov - ng ]    since @xmath605 $ ] we may construct a probability space with three variables @xmath606 such that @xmath607 and furthermore @xmath608 = t(w ; y).\\ ] ] then , consider an arbitrary zero - mean @xmath609 and write @xmath610 = \\ee[w g(y ) ] - \\ee[w ' g(y ) ] & \\le \\ee[|g(y)| \\cdot |w - w'| { { \\mathbf{1}_{\\left\\{{w\\neq w'}\\right\\ } } } } ]              \\label{eq : rcm1a}\\\\              & \\le \\|g(y)\\|_2 \\|w - w'\\|_{2q } t(w ; y)^{1\\over 2q'}\\,,\\label{eq : rcm1 }      \\end{aligned}\\ ] ] where the last step is by hlder s inequality since @xmath611 and @xmath612 .",
    "since @xmath613 , normalizing both sides of  ( [ eq : rcm1 ] ) by @xmath614 and @xmath615 yields the desired .    for",
    "the second part of the proposition , consider arbitrary non - negative , convex @xmath616 with @xmath617 and define the following orlicz norm @xmath618 \\le 1\\}\\,.\\ ] ] if @xmath619 is the legendre dual of @xmath620 then from young s inequality we have for arbitrary @xmath565 : @xmath621 and , hence , @xmath622 \\le 2 \\|x\\|_{\\psi } \\|y\\|_{\\psi^*}\\ ] ] consider @xmath623 and notice an easy identity @xmath624 then , proceeding as above we only need to upper - bound @xmath625 $ ] in  ( [ eq : rcm1a ] ) . from inequality  ( [ eq : young_orlicz ] ) and  ( [ eq : rcm2 ] ) we get @xmath626 \\le 2 \\|w - w'\\|_{\\psi_2}^2 \\|{{\\mathbf{1}_{\\left\\{{w\\neq w'}\\right\\}}}}\\|_{\\psi_1^*}\\,.\\ ] ] for the first term we apply triangle inequality .",
    "the @xmath627-norm of the indicator is found as a unique solution of @xmath628 \\ln{2\\over ec}\\,,\\ ] ] with @xmath629 .",
    "it is easy to show that if @xmath630 < e^{-2/e}$ ] then @xmath631 \\ln { 1\\over \\pp[w\\neq w']}\\,,\\ ] ] from which the proposition follows .      for the scalar case we construct a relay scheme under which the @xmath428-information , mutual information and the correlation between the initial message @xmath632 and the final output @xmath5 achieve the lower bounds  up to constants .",
    "this scheme is also useful for the optimal control problem in . for simplicity",
    "we only consider the @xmath141 moment constraint @xmath463 and assume @xmath633 and @xmath634 for notational conciseness .",
    "[ [ binary - messaging - scheme ] ] binary - messaging scheme + + + + + + + + + + + + + + + + + + + + + + +    in view of the converse results in sections [ sec : t ]  [ sec : rho ] , the majority of the information will be inevitably lost regardless of the relay design .",
    "thus we only aim to transmit a small fraction of the original message , e.g. , a highly skewed quantized version , reliably . to this end , let @xmath635 let @xmath636 , which satisfies @xmath637 . at each stage",
    ", the relay decodes the previous message by @xmath638 .",
    "note that all @xmath639 s take values in @xmath640 .",
    "then @xmath641 } \\leq { \\mathbb{p}\\left[|z_k| \\geq \\mu/2\\right ] } = 2 { { \\mathsf{q}}}(\\mu/2)$ ] . for any @xmath642 $ ] , applying the union bound and the fact that @xmath643 , we obtain @xmath644 } \\leq 2 n { { \\mathsf{q}}}(\\mu/2 ) \\leq n^{-1 }     .",
    "\\label{eq : xkx1}\\ ] ] moreover , the moment constraint is satisfied since @xmath645 } \\leq \\mu^p ( { \\mathbb{p}\\left[x_1 \\neq 0\\right]}+{\\mathbb{p}\\left[x_k \\neq x_1\\right ] } ) \\leq 1 + \\frac{1}{n}(16\\log n)^{p/2 } \\leq 2\\ ] ] for all sufficiently large @xmath0 .",
    "[ [ total - variation - and - mutual - information ] ] total variation and mutual information + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we show that @xmath646 which matches the upper bound in and the upper bound ( for @xmath603 ) , respectively .",
    "since @xmath25 and @xmath647 are deterministic functions of @xmath4 and @xmath5 , respectively , we have @xmath648 and @xmath649 } = \\omega((\\log n)^{-p/2}),\\ ] ] where the first inequality follows from data processing , the second inequality follows from , and the last inequality is by . similarly , @xmath650    [ [ correlation ] ] correlation +",
    "+ + + + + + + + + +    denote @xmath651 and @xmath652=g(b)$ ] , where @xmath653 = -\\frac{\\varphi(a)}{\\phi(a ) } , \\quad g(1 ) = { \\mathbb{e}}[w|w > a ] = \\frac{\\varphi(a)}{{{\\mathsf{q}}}(a)}.      \\label{eq : g}\\ ] ] using the fact that @xmath654 as @xmath655 , we have @xmath656={\\mathbb{e}}[{\\hat{w}}^2]=\\frac{\\varphi^2(a)}{\\phi(a ) { { \\mathsf{q}}}(a ) } = { { \\mathsf{q}}}(a ) a^2 ( 1+o(1 ) ) = \\theta{\\left ( \\frac{\\log \\log n}{(\\log n)^{p/2 } } \\right ) } ,      \\label{eq : hwnorm}\\ ] ] where the last inequality follows from the choice of @xmath153 in .",
    "set @xmath657 and @xmath658 . by",
    ", we have @xmath659 \\leq \\frac{1}{n}$ ] .",
    "therefore @xmath660 = & ~ { \\mathbb{e}}[w{\\hat{w } } ] + { \\mathbb{e}}[w(w_n-{\\hat{w } } ) { { \\mathbf{1}_{\\left\\{{b \\neq b_n}\\right\\ } } } } ]       \\nonumber \\\\ \\geq & ~ { \\mathbb{e}}[w{\\hat{w } } ] - \\max\\{|g(0)|,|g(1)|\\ } { \\mathbb{e}}[|w| { { \\mathbf{1}_{\\left\\{{b \\neq b_n}\\right\\ } } } } ]   \\nonumber \\\\ \\geq & ~ { \\mathbb{e}}[w{\\hat{w } } ] - g(1 ) \\sqrt{{\\mathbb{p}}[b \\neq b_n ] } \\label{eq : cor1 } \\\\",
    "= & ~ \\theta{\\left ( \\frac{\\log \\log n}{(\\log n)^{p/2 } } \\right ) } , \\label{eq : cor2}\\end{aligned}\\ ] ] where is by cauchy - schwartz , is by and @xmath661 .",
    "similarly , @xmath662|",
    "\\leq g(1)^2 { \\mathbb{p}}[b \\neq b_n ] = o(\\log \\log n / n ) .",
    "\\label{eq : hv - norm}\\end{aligned}\\ ] ] therefore @xmath663 .",
    "consequently , the correlation satisfies @xmath596 ) = \\sup_{g\\in l_2(p_{y_n } ) } \\rho(w , g(y_n ) ) \\geq \\rho(w , w_n ) = \\frac{{\\mathbb{e}}[ww_n]}{\\|w_n\\|_2 } = \\omega{\\left ( \\frac{\\sqrt{\\log\\log n}}{(\\log n)^{p/4 } } \\right ) } ,         \\label{eq : rho - opt}\\ ] ] which meets the upper bound .",
    "the problem of optimal memoryless control in gaussian noise was investigated in @xcite .",
    "consider the @xmath0-stage stochastic control problem in in one dimension ( @xmath191 ) where the input @xmath664 with @xmath665 independent of @xmath666 .",
    "the additive noise @xmath667 are i.i.d.standard gaussian , and the relay function @xmath668 plays the role of a memoryless controller mapping the noisy observation @xmath669 into a control signal @xmath670 .",
    "let @xmath671 denote the final estimate .",
    "then we have the following markov chain which has two more stages than : @xmath672 the major difference is that , instead of requiring that each controller satisfies the same power constraint as in , here only a _ total",
    "_ power budget is imposed : @xmath673 \\le ne\\ , .",
    "\\label{eq : total - power}\\ ] ] the objective is to maximize the correlation between @xmath674 and @xmath647 .    the main results of @xcite show that although linear controllers are optimal for two stages ( @xmath675 ) ( * ? ? ?",
    "* proposition 7 ) , for multiple stages they can be strictly sub - optimal .",
    "specifically , subject to the constraint , the optimal squared correlation @xmath676 achieved by linear controllers is ( * ? ? ?",
    "* lemma 6 ) @xmath677 which vanishes exponentially as @xmath678 .",
    "* theorem 15 ) shows that can be improved by using binary quantizers in certain regimes , although the correlation still vanishes exponentially fast albeit with a better exponent .",
    "the optimal performance of non - linear controllers is left open in @xcite .",
    "capitalizing on the results developed in , next we show that the squared correlation achieved by the best non - linear controllers is @xmath679 , which is significantly better than the exponentially small correlation achieved by the best linear controllers .    * for any sequence @xmath680 satisfying the total power constraint , the correlation necessarily satisfies @xmath681 to see this , applying the data processing inequality and the @xmath130 curve in with @xmath373 , we have @xmath682 where @xmath683 and @xmath684 $ ] .",
    "since @xmath685 , we have @xmath686 .",
    "consequently , applies with @xmath0 replaced by @xmath687 and , by , we have @xmath688 for some constant @xmath70 only depending on @xmath530 . since @xmath674 is gaussian , applying yields the upper bound . *",
    "conversely , the binary - quantizer scheme described in ( with @xmath603 ) achieves @xmath689 set @xmath690 , where @xmath691 is defined in .",
    "since @xmath664 and @xmath692 are independent , we have @xmath693 = \\frac{\\sigma_0 ^ 2}{1+\\sigma_0 ^ 2 } { \\mathbb{e}}[ww_n]$ ] and the rest follows from .",
    "the fact that linear control only achieves exponentially decaying correlation can also be understood from the perspective of contraction coefficient of kl divergence .",
    "note that if all controllers are linear , then all input @xmath670 s to the awgn channel are gaussian .",
    "recall the distribution - dependent contraction coefficient @xmath55 defined in .",
    "for awgn channel with noise variance @xmath694 and gaussian input with variance @xmath36 , erkip and cover showed in ( * ? ? ?",
    "* theorem 7 ) that @xmath695 , which is strictly less than one .",
    "this results in exponentially small mutual information : @xmath696,{\\mathsf{var}}(x_i ) ) )    \\nonumber \\\\ \\leq & ~ \\frac{\\log(1+\\sigma_0 ^ 2)}{2 } \\prod_{i=2}^n \\frac{{\\mathsf{var}}(x_i)}{1+{\\mathsf{var}}(x_i ) } \\leq \\frac{\\log(1+\\sigma_0 ^ 2)}{2 } { \\left ( \\frac{e}{1+e } \\right)}^{n-1},\\end{aligned}\\ ] ] where the last step follows from and the concavity and monotonicity of @xmath697 . together with the gaussian rate - distortion function , this implies @xmath698 must vanish as @xmath699 which agrees with . therefore from a control - theoretic perspective , it is advantageous to design the controller to steer the output away from gaussian , which requires , of course , non - linear control .      in this section",
    "we rely on the notations and results from the theory of infinite - volume gibbs measures ; in particular we assume familiarity with  ( * ? ? ?",
    "* chapter 2 ) .",
    "consider a @xmath189-valued markov random field @xmath700 specified by pairwise potentials @xmath701 .",
    "we assume that for every @xmath702 and every @xmath703 we have @xmath704 this specification translates into requiring the conditional probabilities to be of the following form : @xmath705 and in particular @xmath32 form a doubly - infinite markov chain : @xmath706    one of the principal questions in gibbs theory is : do there exist none , one or many joint distributions satisfying conditional probabilities  ( [ eq : gibbs1 ] ) ? such a joint distribution is called a gibbs measure consistent with the specification .",
    "it is believed that the existence of multiple gibbs measures corresponds to the existence of second - order phase transitions in physics ( such as the curie temperature in ferromagnets ) .",
    "a typical method for proving non - existence of multiple phases is the application of dobrushin contraction , cf .",
    "next we extend this technique to cases where dobrushin contraction is not available ( @xmath707 ) by relying on the knowledge of the dobrushin curve @xmath130 .",
    "here is an illustration .",
    "suppose that potentials @xmath708 are such that each conditional distribution  ( [ eq : gibbs1 ] ) factors through the gaussian channel , i.e. for each @xmath709 there exists a representation @xmath710 with @xmath711 a two - dimensional gaussian channel  ( [ eq : mc2 ] ) .",
    "then there may exist at most one joint distribution of @xmath712 satisfying @xmath713   < \\infty .\\ ] ] [ thm : gibbs ]    assumptions of guarantee that `` strengths '' of all links in  ( [ eq : gibbs2 ] ) are uniformly upper - bounded .",
    "thus we can see that on @xmath714 the only possibilities for a phase transition are : 1 ) when the links become asymptotically noiseless , or 2 ) when the ( non shift - invariant ) solutions are allowed to grow unbounded .",
    "this is in accord with known examples of systems with non - unique gibbs measures : e.g. , the asymptotically noiseless example in  ( * ? ? ?",
    "* chapter 6 ) , or the non shift - invariant examples of spitzer - cox and kalikow in  ( * ? ? ? * chapter 11 ) .",
    "we recall the following idea due to dobrushin ( * ? ? ?",
    "* lemma 5 ) :    [ pr : dobr ] let @xmath715 be any coupling of @xmath254 to @xmath716 ( i.e. @xmath717 is @xmath254 or @xmath716 when restricted to first pair or second pair ) .",
    "assume also that for every @xmath153 and @xmath718 we have is a wasserstein distance with respect to the metric @xmath719 , analogously defined as in with the @xmath293 distance replaced by @xmath719 . ]",
    "@xmath720 then there exists a coupling @xmath721 between @xmath254 and @xmath716 such that @xmath722 and @xmath723 \\le \\ee_\\pi[r(a , a')].\\ ] ]    when @xmath724 and @xmath725 ( dobrushin contraction ) , we can progressively refine the coupling at various points between two distributions @xmath36 and @xmath50 and show that they must coincide .",
    "this is a brilliant idea of dobrushin  @xcite .",
    "we apply the same recursion here , except without relying on @xmath725 .",
    "suppose that there exist two distributions @xmath36 and @xmath50 of @xmath726 satisfying  ( [ eq : gibbs4 ] ) and  ( [ eq : gibbs3 ] ) .",
    "let @xmath727 denote the left - hand side of , i.e. , the common upper bound on the second moment of @xmath9 . given a coupling @xmath715 between @xmath36 and @xmath50 , that is @xmath728",
    "denote @xmath729 \\le 1,\\ ] ] where @xmath730 is large integer .",
    "denote @xmath731 and @xmath732 its euclidean norm .",
    "using the factorization condition  ( [ eq : gibbs4 ] ) and the data processing inequality for total variation , we have @xmath733 where @xmath734 , cf . corollary  [ cor : ftv - g ] . applying proposition  [ pr : dobr ] with @xmath735 and @xmath736 , we can produce a new coupling @xmath737 so that @xmath738 and @xmath739 \\le \\ee_\\pi[\\theta_c(|x_{\\pm n } - \\tilde x_{\\pm n}| ) ] .\\ ] ] in view of the moment constraint  ( [ eq : gibbs3 ] ) , we have @xmath740 =      \\ee_p[|x_{-n}|^2 + |x_n|^2 ] + \\ee_q[|x_{-n}|^2 + |x_n|^2 ] \\le 4e .\\ ] ] thus , as we noticed in the proof of theorem  [ th : coupling ] , the constraint  ( [ eq : gibbs5 ] ) leads to @xmath741      \\le f(\\pi[x_{\\pm",
    "n } \\neq \\tilde x_{\\pm n } ] ) \\leq f(\\epsilon_n),\\ ] ] where the concave non - decreasing function @xmath46 is @xmath742 therefore , starting from any coupling @xmath715 which achieves @xmath743 we produced a new coupling @xmath737 which achieves @xmath744 as we have seen in the proof of , lemma  [ lmm : rate ] shows that such iterations lead to @xmath743 decreasing to zero .",
    "hence for any @xmath0 , starting with sufficiently large @xmath745 , we have shown that @xmath746 is arbitrarily small , hence zero .",
    "in other words , distributions @xmath36 and @xmath50 have the same finite - dimensional marginals , and must therefore coincide .    as one can see our proof crucially relies on the fact that boundary of the interval @xmath747 $ ] on the chain graph  ( [ eq : gibbs2 ] ) always consists of two points @xmath748 ( see  ( [ eq : gibbs5 ] ) ) .",
    "this is why a similar argument is not applicable to markov random fields on @xmath749 , where the number of variables in the boundary of @xmath747 ^ 2 $ ] grows with @xmath750 .",
    "but in that case it is well - known that even for binary - valued @xmath38 there can exist multiple gibbs measures ( the two - dimensional ising model example ) .",
    "a circuit is a directed acyclic graph emanating from @xmath0 inputs @xmath751 , going through multiple intermediate nodes ( `` gates '' ) and terminating at a final node @xmath4 . each gate @xmath752 with inputs @xmath753 performs a simple operation @xmath754 and produces an output , which is then subjected to additive gaussian noise , so that the output value @xmath755 of the @xmath756 gate is given by @xmath757 the outputs of the @xmath756 gate are connected to the inputs @xmath758 of subsequent gates according to the graph .",
    "the value of @xmath4 is the output of the last gate .",
    "we say that the circuit computes the boolean function @xmath759 with probability of error @xmath543 if @xmath760 \\ge 1-\\epsilon\\,,\\ ] ] for some @xmath761 and all binary vectors @xmath762 .",
    "we assume that all gates have at most @xmath763 inputs .",
    "we say that the function @xmath319 depends essentially on input @xmath764 if there exist @xmath765 differing in the @xmath756 coordinate _ only _ , such that @xmath766    we show below that it is not possible to have small @xmath543 , complicated @xmath319 , large @xmath0 and small power consumed by outputs of each gate : @xmath767 \\le p\\,.\\ ] ] this is a natural extension of the well - studied model of binary symmetric noise ( bit flips ) @xcite .",
    "we note that even for the settings of binary symmetric channels ( bsc ) , quite a few open questions remain .",
    "for example , it is known that for each @xmath763 there exists a threshold of maximum tolerable noise beyond which arbitrarily complex circuits are not possible @xcite . however",
    ", this threshold is generally unknown and is sensitive to whether bscs have crossover probability exactly @xmath768 or @xmath769 , cf .",
    "@xcite , and whether the output of one gate is allowed to be used at one or multiple consequent gates , cf .",
    "@xcite .",
    "[ prop : noisy ] for any signal - to - noise ratio @xmath770 , any boolean function @xmath319 essentially depending on @xmath0 inputs , and any circuits of noisy @xmath763-input gates computing @xmath319 , the probability of error satisfies @xmath771 where @xmath772 and @xmath127 is given by  ( [ eq : ftv - g ] ) with @xmath773 .    for three - input gates ,",
    "the lower bound is evaluated in fig .",
    "[ fig : noisy ] as a function of @xmath36 .",
    ".,title=\"fig : \" ] .,title=\"fig : \" ]    we recall a combinatorial fact shown in the proof of  ( * ? ? ?",
    "* theorem 2 ) : for every boolean function @xmath319 essentially depending on @xmath0 inputs , and for every circuit that computes @xmath319 with probability of error strictly less than @xmath774 , there must exist at least one input , say @xmath25 , such that _ every _ path from @xmath25 to @xmath4 has length at least @xmath775 since @xmath319 essentially depends on @xmath25 , we can assume , without loss of generality , that @xmath776    note that the random variables in the circuit consist of the inputs @xmath777 , inputs @xmath778 and outputs @xmath779 of the gates , and the final output @xmath4 , which is equal to some @xmath755 . to simplify notation ,",
    "let @xmath780 .",
    "denote the neighbors of the gate @xmath752 by @xmath781 whose outputs serve as inputs to gate @xmath752",
    ". then @xmath782 by assumption .",
    "without loss of generality , we assume that all gates are numbered so that @xmath756 gate s inputs all come from gates with indices strictly less than @xmath752 . then @xmath783 by construction .",
    "consider now probability distributions @xmath36 and @xmath50 of all random variables in the circuit , such that under @xmath36 we have @xmath784 and under @xmath50 we have @xmath785 , while @xmath786 under both .",
    "the idea is to progressively build coupling between @xmath36 and @xmath50 to show that @xmath787 from which the desired lower bound follows .    to prove , suppose that there is a joint distribution @xmath715 such that @xmath788 i.e. @xmath715 is a coupling of @xmath36 to @xmath50 . consider an arbitrary gate @xmath752 with input @xmath789 and output @xmath755 . in view of the noise model ,",
    "the proof of theorem  [ thm : gibbs ] shows that the moment constraint  ( [ eq : nc_cost ] ) enables us to use to build another coupling @xmath790 , such that a ) @xmath791 have identical joint distribution under either @xmath715 or @xmath721 , and b ) at the @xmath756 gate we have @xmath792 \\le { { f_{{{\\sf",
    "tv}}}}}(\\pi[s_i \\neq s_i'])\\,.\\ ] ] recall that @xmath793 under @xmath715 .",
    "then @xmath789 is determined by the outputs of the neighboring gates and possibly @xmath794 , collectively denoted by @xmath795 . by",
    "the union bound , we have @xmath796 \\le k",
    "\\max_{j \\in n_i } \\pi[o_j \\neq o'_j]\\,.\\ ] ] so if we introduce the function @xmath797 then we can relax  ( [ eq : nc_a1 ] ) to @xmath798 \\le f_k\\big(\\max_{j \\in n_i } \\pi[o_j \\neq o'_j]\\big).\\ ] ]    now , let @xmath799 be the trivial ( independent ) coupling .",
    "since @xmath785 and @xmath800 under @xmath799 , we have @xmath801 = 1 \\triangleq t_0 $ ] .",
    "consider the first gate , whose inputs can be either @xmath25 or constants . applying the previous construction yields a coupling @xmath802 such that @xmath803 \\le f_k(1)\\,.\\ ] ] here @xmath804 measures the quality of coupling at the output of the first gate .",
    "next , suppose that all gates @xmath805 are similarly coupled by @xmath806 with respective @xmath807 .",
    "we refine the coupling at gate @xmath752 to get @xmath808 , so that a ) the joint distribution of @xmath809 and hence @xmath807 are unchanged , and b ) @xmath810 \\le f_k\\big(\\max_{j \\in n_i } t_j\\big)\\,,\\ ] ] which follows from",
    ". continuing similarly , we arrive at the last gate which outputs @xmath4 .",
    "now let us construct a path from @xmath4 back to @xmath811 as follows : starting from @xmath4 go back from gate @xmath752 to the neighboring gate @xmath812 that achieves @xmath813 .",
    "let @xmath520 be the length of this path and let the indices ( in increasing order ) be @xmath814 by  ( [ eq : pathlen ] ) we must have @xmath815 . by construction of the path , we have @xmath816 , @xmath817 , etc .",
    "so finally @xmath818 \\le t_{i_m } \\leq f_k\\big(\\max_{j \\in n_i } t_j\\big ) = f_k(t_{i_{m-1 } } ) \\leq \\ldots \\le \\underbrace{f_k \\circ f_k \\cdots",
    "\\circ f_k}_{m\\text{~times}}(1).\\ ] ] hence as @xmath90 this repeated composition of @xmath819 s must converge to a fixed point @xmath820 , thus proving  ( [ eq : nctv1 ] ) .",
    "consider the setting studied in  @xcite : the original bit @xmath821 is to be broadcasted along the binary tree of noisy channels :    @xmath822 where arrows @xmath823 represent independent noisy channels and @xmath824 are relays .",
    "the goal is to design the relay functions so that for some @xmath374 one can reconstruct @xmath4 with probability of error at most @xmath825 based on the values at the @xmath826 layer @xmath827 for all sufficiently large @xmath0 ; to wit , the total variation of the distributions conditioned on @xmath828 or @xmath829 is strictly bounded away from zero .",
    "one of the main results of  @xcite is that when all channels are bsc with flip probability @xmath768 such broadcasting is possible if and only if @xmath830 , thus establishing a certain `` phase transition '' in this problem .",
    "in fact , the impossibility part of the bsc result follows from a result of evans and schulman  @xcite : for a binary tree of discrete channels the probability of error tends to @xmath433 as the depth tends to infinity whenever @xmath831 . for gaussian channels",
    "we know that @xmath832 which suggests that such transition _ does not occur _ for a tree of gaussian channels . indeed ,",
    "in this section we demonstrate that it is possible to broadcast some information to arbitrarily deep layers regardless of how small the snr is .",
    "specifically , consider channels @xmath833 with cost constraint @xmath834 \\le e\\ , .",
    "\\qquad \\forall k , j\\ ] ] choose the initial ( randomized ) encoder as follows : @xmath835 = 1-\\pp[b=0 ] = 2p , b\\dperp w,\\ ] ] with parameters @xmath836 to be specified later .",
    "similar to the scheme in , choose relays as follows : @xmath837 where @xmath838 can be set arbitrarily . notice that if @xmath839 is selected so that @xmath840 then a simple computation shows that for all @xmath841 we have @xmath842 = \\pp[x_{k , j}=-\\mu ] = p\\ , .",
    "\\label{eq : xkj}\\ ] ] but from  ( [ eq : bt0 ] ) and the fact that @xmath843 for large @xmath839 we get @xmath844 in particular , regardless of how small @xmath530 in  ( [ eq : bt0a ] ) is and for any @xmath241 , there exists a sufficiently large @xmath839 such that the cost constraint is satisfied .",
    "another important parameter turns out to be @xmath845 again , taking @xmath839 large we may ensure @xmath846 thus we assume from now on that @xmath847 and @xmath241 are selected in such a way that both  ( [ eq : bt0a ] ) and  ( [ eq : bt0b ] ) are satisfied .    similarly to  @xcite we will employ the idea of t. kamae , see  ( * ? ? ?",
    "* remark on p. 342 ) , and consider the behavior of `` spin sums '' : @xmath848 where @xmath849 with @xmath850 , or equivalently , @xmath851 . to show that it is possible to test @xmath852 based on the statistic @xmath301 , we show that @xmath853 which is strictly positive .",
    "according to  ( * ? ? ?",
    "* lemma 4.2 ( i ) and ( iii ) ) we have : @xmath854 - \\ee[s_n|w=-1])^2\\over 4 \\ee[s_n^2]}.\\ ] ] so the estimate  ( [ eq : bt1 ] ) follows from two results : @xmath855   & = \\pm 2p(2\\theta)^{n-1 } \\label{eq : sn1 } , \\\\",
    "\\ee[s_n^2 ] & \\le 2^{n } p + 2 p \\frac{(2\\theta)^{2n}}{(2\\theta)^2 - 1 } \\label{eq : sn2}. \\ ] ] both of these are verified below : consider two arbitrary nodes @xmath856 and @xmath857 at the @xmath858 level and let @xmath859 be their common ancestor in the tree .",
    "denote the parent node of @xmath856 by @xmath860 .",
    "then @xmath861 = { \\mathbb{e}}[\\ee[\\sigma_{k , j } | \\sigma_{k-1,j '' } ] | \\sigma_{u , j ' } ] =   \\theta \\ , \\ee[\\sigma_{k-1,j '' } | \\sigma_{u , j ' } ] = \\ldots =   \\theta^{k - u } \\sigma_{u , i}.\\ ] ] furthermore , @xmath862 and @xmath863 are independent conditioned on @xmath864 .",
    "note that @xmath865 = \\pm { \\mathbb{p}}[b=1 ] = \\pm 2p$ ] , which yields .",
    "next , note that @xmath866 = \\sum_{j=1}^{2^{k-1 } } { \\mathbb{e}}[\\sigma^2_{n , j } ] + 2 \\sum_{j'<j } { \\mathbb{e}}[\\sigma_{n , j } \\sigma_{n , j'}]$ ] , where the first term is @xmath867 since @xmath868 in view of . to estimate the cross term , denote the depth of the common ancestor of @xmath869 and @xmath870 by @xmath871",
    ". then @xmath872 = & ~ \\sum_{u=1}^{n-1 } \\sum_{u(j',j)=u } { \\mathbb{e}}[\\sigma_{n , j } \\sigma_{n , j ' } ]     = \\sum_{u=1}^{n-1 } \\sum_{u(j',j)=u } \\theta^{2(n - u ) } 2 p     \\\\ = & ~ 2 p \\sum_{u=1}^{n-1 } \\theta^{2(n - u ) } \\binom{2^{n - u}}{2 }   \\leq p \\frac{(2\\theta)^{2n}}{(2\\theta)^2 - 1},\\end{aligned}\\ ] ] which yields .",
    "it is a pleasure to thank max raginsky ( uiuc ) for many helpful discussions and flavio du pin calmon ( mit ) for proposition  [ prop : itv ] .",
    "consider the following iteration @xmath873 where @xmath874 \\to [ 0,1]$ ] satisfies @xmath875 and @xmath876 for all @xmath877 . then @xmath878 $ ] a monotonically decreasing sequence converging to the unique fixed point zero as @xmath90 . under the monotonicity assumption of the function @xmath483 , the following result gives a non - asymptotic upper estimate of this sequence .",
    "define @xmath879 \\to { \\mathbb{r}}_+$ ] by @xmath880 .",
    "if @xmath483 is increasing , then for any @xmath881 , @xmath882 [ lmm : rate ]    by the positivity and monotonicity of @xmath483 , @xmath883 is a strictly decreasing and concave function .",
    "hence @xmath884 $ ] is well - defined .",
    "put @xmath885 .",
    "then @xmath886 hence @xmath887 since @xmath888 .",
    "we shall assume that @xmath889 is not a point mass , namely , there exists a measurable set @xmath530 such that @xmath890 .",
    "define @xmath891 where the supremum is over all @xmath892 such that @xmath893 .",
    "it is clear that such @xmath892 always exists ( e.g. , @xmath894 and @xmath895 ) .",
    "let @xmath896 where the supremum is over all markov chains @xmath83 with fixed @xmath40 such that @xmath84 .",
    "such markov chains always exist , e.g. , @xmath897 and then @xmath898 .",
    "the goal of this appendix is to prove  ( [ eq : etakl ] ) , namely @xmath899    the inequality @xmath900 follows trivially : @xmath901    for the other direction , fix @xmath892 such that @xmath893 .",
    "first , consider the case where @xmath902 is bounded , namely , @xmath903 for some @xmath278 @xmath892-a.s . for any @xmath904 ,",
    "let @xmath905 and define the probability measure @xmath906 .",
    "let @xmath907 and @xmath908 , which defines a markov chain @xmath909 such that @xmath565 is distributed as the desired @xmath40 .",
    "note that @xmath910 where @xmath911 .",
    "we claim that @xmath912 which , in view of the data processing inequality @xmath913 , implies @xmath914 as desired . to establish , define",
    "the function @xmath915 one easily notices that @xmath46 is continuous on @xmath916\\times[0,{1\\over 2a}]$ ] and thus bounded .",
    "so we get , by bounded convergence theorem , @xmath917 \\to   \\ee_{p_x}\\left[{\\frac{{{\\rm d}}q_x}{{{\\rm d}}p_x}}-1\\right ] \\log e = 0\\,.\\ ] ]    to drop the boundedness assumption on @xmath902 we simply consider the conditional distribution @xmath918 where @xmath919 and @xmath278 is sufficiently large so that @xmath920 . clearly , as @xmath921 , we have @xmath922 and @xmath923 pointwise , where @xmath924",
    ". hence the lower - semicontinuity of divergence yields @xmath925 furthermore , since @xmath926 , we have @xmath927.\\end{aligned}\\ ] ] since @xmath928 , by dominated convergence ( note : @xmath929 < \\infty$ ] ) we have @xmath930",
    ". therefore , @xmath931 completing the proof .",
    "answer : yes . indeed , according to  ( [ eq : ftv ] ) at each iteration the total variation contracts by at most @xmath940 ( i assume there is a scheme to achieve this ) .",
    "set @xmath941 then we have for all @xmath942 @xmath943 since @xmath944 whenever @xmath945 .",
    "furthermore , evidently the @xmath946 can be made arbitrarily small , implying a bit can be transferred to infinity with zero energy expended , provided there are relays along the path .",
    "* todo : * does nt it contradict minimum energy - per - bit concept ?      1 .",
    "strong conjecture : there is a positive function @xmath947 such that for all @xmath948 @xmath949 this conjecture is * false*. consider , @xmath950 and @xmath951 $ ] with @xmath952 .",
    "then @xmath953 on the other hand , it is easy to show that @xmath954 on the other hand , for the current example @xmath955 2 .",
    "find @xmath956 for binary @xmath957 the question boils down to contraction of some funny @xmath46-divergence and @xmath507 follows from integral representation via @xmath309 s . for general ,",
    "not clear what to do .",
    "another problem : if we are interconnected by awgns of infinite dimension ( and a bound on total energy ) then we _ do not know _ whether @xmath101 or not .",
    "this is because our upper bound  ( [ eq : main3 ] ) is of the form @xmath958 as opposed to @xmath959 .",
    "so our @xmath960 technique should be changed for infinite dimension .",
    "is this related to rakhlin s fetish with kolmogorov - packings of infinite dimensional spaces shit ?",
    "+ later : also note that @xmath958 corresponds to dimension of @xmath25 , not subsequent @xmath9 s .",
    "our result on circuits suggests there is no phase transition for awgns ( like for bsc : @xmath961 implies @xmath962 as @xmath522 ) .",
    "so we need to make von  neumann s construction for awgn noise and show that @xmath963 no matter how low snr is .",
    "here is a funny argument i noticed .",
    "let @xmath144 be noise on some abelian group and suppose that @xmath964 where @xmath965 is the haar measure on the group .",
    "then the channel @xmath966 is tv - contractive since @xmath967 thus by  @xcite we also have @xmath968 and by ahslwede - gcs the operator @xmath969 is hypercontractive @xmath970 for every @xmath889 !",
    "this may be quite tough to show in general , while condition @xmath971 is really nice ."
  ],
  "abstract_text": [
    "<S> one of the basic tenets in information theory , the data processing inequality states that output divergence does not exceed the input divergence for any channel . for channels without input constraints , various estimates on the amount of such contraction </S>",
    "<S> are known , dobrushin s coefficient for the total variation being perhaps the most well - known . </S>",
    "<S> this work investigates channels with average input cost constraint . </S>",
    "<S> it is found that while the contraction coefficient typically equals one ( no contraction ) , the information nevertheless dissipates . </S>",
    "<S> a certain non - linear function , the _ dobrushin curve _ of the channel , is proposed to quantify the amount of dissipation . tools for evaluating the dobrushin curve of additive - noise channels are developed based on coupling arguments . </S>",
    "<S> some basic applications in stochastic control , uniqueness of gibbs measures and fundamental limits of noisy circuits are discussed .    as an application </S>",
    "<S> , it shown that in the chain of @xmath0 power - constrained relays and gaussian channels the end - to - end mutual information and maximal squared correlation decay as @xmath1 , which is in stark contrast with the exponential decay in chains of discrete channels . </S>",
    "<S> similarly , the behavior of noisy circuits ( composed of gates with bounded fan - in ) and broadcasting of information on trees ( of bounded degree ) does not experience threshold behavior in the signal - to - noise ratio ( snr ) . </S>",
    "<S> namely , unlike the case of discrete channels , the probability of bit error stays bounded away from @xmath2 regardless of the snr . </S>"
  ]
}