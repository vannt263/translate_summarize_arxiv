{
  "article_text": [
    "an inequality is denoted mathematically by @xmath0 , where @xmath1 and @xmath2 denote respectively the _ left - hand - side _ and _ right - hand - side _ of the inequality .",
    "one can build _ dissimilarity _ measures from inequalities @xmath0 by measuring the _ inequality tightness _ : for example , we may quantify the tightness of an inequality by its _ difference gap _ : @xmath3    when @xmath4 , the inequality tightness can also be gauged by the _ log - ratio gap _ : @xmath5    we may further compose this inequality tightness value measuring non - negative gaps with a _ strictly monotonically increasing function _",
    "@xmath6 ( with @xmath7 ) .    a _ bi - parametric _",
    "inequality @xmath8 is called _ proper _ if it is strict for @xmath9 ( i.e. , @xmath10 ) and _ tight _ if and only if ( iff ) @xmath11",
    "( i.e. , @xmath12 ) .",
    "thus a proper bi - parametric inequality allows one to define dissimilarities such that @xmath13 iff @xmath11 .",
    "such a dissimilarity is called proper .",
    "otherwise , an inequality or dissimilarity is said _",
    "improper_. note that there are many equivalent words used in the literature instead of ( dis-)similarity : distance ( although often assumed to have metric properties ) , pseudo - distance , discrimination , proximity , information deviation , etc .",
    "a _ statistical dissimilarity _ between two discrete or continuous distributions @xmath14 and @xmath15 on a support @xmath16 can thus be defined from inequalities by summing up or taking the integral for the inequalities instantiated on the observation space @xmath16 : @xmath17 & \\mbox{discrete case},\\\\ \\int_{\\cal x } \\big [ \\rhs(p(x),q(x))-\\lhs(p(x),q(x ) ) \\big]\\dx & \\mbox{continuous case}. \\end{array } \\right.\\end{aligned}\\ ] ] in such a case , we get a _",
    "separable divergence_. some non - separable inequalities induce a _",
    "non - separable divergence_. for example , the renown cauchy - schwarz divergence  @xcite is not separable because in the inequality : @xmath18 the rhs is not separable .",
    "furthermore , a proper dissimilarity is called a _ divergence _ in information geometry  @xcite when it is @xmath19 ( i.e. , three times differentiable thus allowing to define a metric tensor  @xcite and a cubic tensor  @xcite ) .",
    "many familiar distances can be reinterpreted as inequality gaps in disguise .",
    "for example , bregman divergences  @xcite and jensen divergences  @xcite ( also called burbea - rao divergences  @xcite ) can be reinterpreted as inequality difference gaps , and the cauchy - schwarz distance  @xcite as an inequality log - ratio gap :    a proper score function  @xcite @xmath20 induces a gap divergence @xmath21 .",
    "a bregman divergence  @xcite @xmath22 for a strictly convex and differentiable real - valued generator @xmath23 is induced by the",
    "_ bregman score _ @xmath24 .",
    "let @xmath25 denote the bregman proper score minimized for @xmath11 .",
    "then the bregman divergence is a gap divergence : @xmath26 .",
    "when @xmath27 is strictly convex , the bregman score is proper , and the bregman divergence is proper .",
    "consider the cauchy - schwarz inequality @xmath28 .",
    "then the cauchy - schwarz distance  @xcite between two continuous distributions is defined by @xmath29 .",
    "note that we use the modern notation @xmath30 to emphasize that the divergence is potentially asymmetric : @xmath31 , see  @xcite .",
    "in information theory  @xcite , the older notation `` @xmath32 '' is often used instead of `` @xmath33 '' that is used in information geometry  @xcite .    to conclude this introduction ,",
    "let us finally introduce the notion of _ projective statistical distances_. a statistical distance @xmath30 is said projective when : @xmath34    the cauchy - schwarz distance is a projective divergence .",
    "another example of such a projective divergence is the parametric @xmath35-divergence  @xcite .",
    "the @xmath35-divergence  @xcite @xmath36 for @xmath37 is projective : @xmath38 the @xmath35-divergence is related to the proper pseudo - spherical score  @xcite .",
    "the @xmath35-divergences have been proven useful for robust statistical inference  @xcite in the presence of heavy outlier contamination .",
    "consider a broader class of _ statistical pseudo - divergences _ based on _ improper inequalities _ , where the tightness of @xmath8 does not imply that @xmath11 .",
    "this family of dissimilarity measures have interesting properties which have not been studied before .",
    "formally , statistical pseudo - divergences are defined with respect to density measures @xmath14 and @xmath15 with @xmath39 , where @xmath16 denotes the support . by definition ,",
    "pseudo - divergences satisfy the following three fundamental properties :    1 .",
    "non - negativeness : @xmath40 for any @xmath41 ; 2 .",
    "reachable indiscernability : * @xmath42 , there exists @xmath15 such that @xmath43 , * @xmath44 , there exists @xmath14 such that @xmath43 .",
    "positive correlation : if @xmath45 , then @xmath46 for any @xmath47 , @xmath48 .    as compared to",
    "_ statistical divergence _ measures such as the kullback - leibler ( kl ) divergence : @xmath49",
    "pseudo - divergences do not require @xmath50 . instead",
    ", any pair of distributions @xmath14 and @xmath15 with @xmath43 only has to be `` positively correlated '' such that @xmath51 implies @xmath52 , and vice versa .",
    "any divergence with @xmath53 ( law of indiscernibles ) automatically satisfies this weaker condition , and therefore any divergence belongs to the broader class of pseudo - divergences . indeed ,",
    "if @xmath54 then @xmath55",
    ". however the converse is not true .",
    "as we shall describe in the remainder , the family of pseudo - divergences is _ not _ limited to proper divergence measures . in the remainder , the term `` pseudo - divergence '' refers to such divergences that are _ not _ proper divergence measures .",
    "we study two novel statistical dissimilarity families : one family of statistical improper pseudo - divergences and one family of proper statistical divergences . within the class of pseudo - divergences ,",
    "this work concentrates on defining a one - parameter family of dissimilarities called hlder log - ratio gap divergence that we concisely abbreviate as hpd for `` hlder pseudo divergence '' in the remainder .",
    "we also study its proper divergence counterpart termed hd for `` hlder divergence . ''",
    "the term `` hlder divergence '' has first been coined in 2014 based on the definition of the _ hlder score _",
    "@xcite : the score - induced hlder divergence @xmath30 is a proper gap divergence that yields a scale - invariant divergence  @xcite . let @xmath56 for @xmath57 be a transformation . then a scale - invariant divergence  @xcite satisfies @xmath58 for a function @xmath59 .",
    "this gap divergence is proper since it is based on the so - called hlder score  @xcite but is _ not _ projective and does not include the cauchy - schwarz divergence . due to these differences",
    "the hlder log - ratio gap divergence introduced here shall not be confused with the hlder gap divergence induced by the _",
    "hlder score _",
    "@xcite that relies both on a scalar @xmath35 and a function @xmath60 .",
    "we shall introduce _ two _ novel families of log - ratio projective gap divergences based on hlder ordinary ( or forward ) and reverse inequalities that extend the cauchy - schwarz divergence , study their properties , and consider as an application clustering gaussian distributions : we experimentally show better clustering results when using symmetrized hlder divergences than using the cauchy - schwarz divergence . to contrast with",
    "the `` hlder composite score - induced divergences '' of  @xcite , our hlder divergences admit closed - form expressions between distributions belonging to the same exponential families  @xcite provided that the natural parameter space is a cone or affine .",
    "our main contributions are summarized as follows :    * define the uni - parametric family of hlder improper pseudo - divergences ( hpds ) in  [ sec : hpd ] and the bi - parametric family of hlder proper divergences in  [ sec : hd ] ( hds ) for positive and probability measures , and study their properties ( including their relationships with skewed bhattacharrya distances  @xcite via escort distributions ) ; * report closed - form expressions of those divergences for exponential families when the natural parameter space is a cone or affine ( include but not limited to the cases of categorical distributions and multivariate gaussian distributions ) in  [ sec : hcef ] ; * provide approximation techniques to compute those divergences between mixtures based on log - sum - exp inequalities in  [ sec : hmm ] ; * describe a variational center - based clustering technique based on the convex - concave procedure for computing hlder centroids , and report our experimental results in  [ sec : centroid ] .",
    "this paper is organized as follows :  [ sec : hpd ] introduces the definition and properties of hlder pseudo - divergences ( hpds ) .",
    "it is followed by ",
    "[ sec : hd ] that describes hlder proper divergences ( hds ) . in ",
    "[ sec : hcef ] , closed - form expressions for those novel families of divergences are reported for the categorical , multivariate gaussian , bernoulli , laplace and wishart distributions .",
    " [ sec : centroid ] defines hlder statistical centroids and presents a variational @xmath61-means clustering technique : we show experimentally that using hlder divergences improve over the cauchy - schwarz divergence .",
    "finally , [ sec : concl ] concludes this work and hints at further perspectives from the viewpoint of statistical estimation and manifold learning . in appendix",
    "[ sec : hdproof ] , we recall the proof of the ordinary and reverse hlder s inequalities .",
    "hlder s inequality ( see  @xcite and appendix [ sec : hdproof ] for a proof ) states for positive real - valued functions @xmath14 and @xmath15 defined on the support @xmath16 that : @xmath62 where exponents @xmath63 and @xmath64 satisfy @xmath65 as well as the _ exponent conjugacy _ condition : @xmath66 .",
    "we also write @xmath67 meaning that @xmath63 and @xmath64 are _ conjugate _ hlder exponents .",
    "we check that @xmath68 and @xmath69 .",
    "hlder inequality holds even if the lhs is infinite ( meaning that the integral diverges ) since the rhs is also infinite in that case .    the _ reverse hlder inequality",
    "_ holds for conjugate exponents @xmath66 with @xmath70 ( then @xmath71 and @xmath72 , or @xmath73 and @xmath74 ) :    @xmath75    both hlder s inequality and the reverse hlder inequality turn tight when @xmath76 ( see proof in appendix  [ sec : hdproof ] ) .",
    "let @xmath77 be a measurable space where @xmath78 is the lebesgue measure , and let @xmath79 denote the lebesgue space of functions that have their @xmath35-th power of absolute value lebesgue integrable , for any @xmath37 ( when @xmath80 , @xmath79 is a banach space ) .",
    "we define the following pseudo - divergence :    for conjugate exponents @xmath63 and @xmath64 with @xmath65 , the _ hlder pseudo - divergence _ ( hpd ) between two densities @xmath81 and @xmath82 of positive measures absolutely continuous with respect to ( wrt . ) @xmath78 is defined by the following log - ratio gap : @xmath83    when @xmath71 and @xmath84 , or @xmath73 and @xmath74 , the _ reverse hpd _ is defined by : @xmath85    by hlder s inequality and the reverse hlder inequality , @xmath86 with @xmath87 iff @xmath88 or equivalently @xmath89 .",
    "when @xmath68 , @xmath90 is monotonically increasing , and @xmath91 is indeed a pseudo - divergence .",
    "however , the reverse hpd is _ not _ a pseudo - divergence because @xmath90 will be monotonically decreasing if @xmath73 or @xmath71 .",
    "therefore we only consider hpd with @xmath68 in the remainder , and leave here the notion of reverse hlder divergence .    when @xmath92 , the hpd becomes the cauchy - schwarz divergence @xmath93  @xcite : @xmath94 which has been proved useful to get closed - form divergence formulas between mixtures of exponential families with conic or affine natural parameter spaces  @xcite .",
    "the cauchy - schwarz divergence is proper for probability densities since the cauchy - schwarz inequality becomes an equality iff @xmath95 implies that @xmath96 .",
    "it is however not proper for positive densities .",
    "the cauchy - schwarz divergence @xmath97 is proper for square - integrable probability densities @xmath98 but not proper for positive square - integrable densities .      in the general case , when @xmath99 , the divergence @xmath91 is not even proper for normalized ( probability ) densities , not to mention general unnormalized ( positive ) densities . indeed , when @xmath54 , we have : @xmath100 let us consider the general case . for unnormalized positive distributions",
    "@xmath101 and @xmath102 ( the tilde notation stems from the notation of homogeneous coordinates in projective geometry ) , the inequality becomes an equality when : @xmath103 , i.e. , @xmath88 , or @xmath104 .",
    "we can check that @xmath105 for any @xmath106 : @xmath107 since @xmath108 .    for @xmath109",
    ", we find indeed that @xmath110 for any @xmath111 .",
    "the hlder pseudo - divergences are improper statistical distances .",
    "in general , hlder divergences are asymmetric when @xmath112 ( @xmath113 ) but enjoy the following _ reference duality _",
    "@xcite : @xmath114    the hlder pseudo - divergences satisfy the reference duality @xmath115 : @xmath116 .    an arithmetic symmetrization of the hpd yields a symmetric hpd @xmath117 , given by : @xmath118      in the above definition , densities @xmath14 and @xmath15 can either be positive or normalized probability distributions .",
    "let @xmath101 and @xmath102 denote positive ( not necessarily normalized ) measures , and @xmath119 the _ overall mass _ so that @xmath120 is the corresponding normalized probability measure .",
    "then we check that hpd is a _ projective divergence",
    "_  @xcite since : @xmath121 or in general : @xmath122 for all prescribed constants @xmath123 .",
    "projective divergences may also be called `` _ angular divergences _ '' or `` _ cosine divergences _ '' since they do not depend on the total mass of the measure densities .",
    "the hlder pseudo - divergences are projective distances .",
    "let us define with respect to the probability measures @xmath124 and @xmath125 the following _ escort probability distributions _",
    "@xcite : @xmath126 and @xmath127    since hpd is a projective divergence , we compute with respect to the conjugate exponents @xmath63 and @xmath64 the _ hlder escort divergence _ ( he d ) : @xmath128 which turns out to be the familiar _ skew bhattacharyya divergence _",
    "@xmath129 , see  @xcite .",
    "the hlder escort divergence amounts to a skew bhattacharyya divergence : @xmath130 for any @xmath131 .",
    "in particular , the _ cauchy - schwarz escort divergence _",
    "@xmath132 amounts to the bhattacharyya distance  @xcite @xmath133 : @xmath134    observe that the cauchy - schwarz escort distributions are the square root density representations  @xcite of distributions .",
    "let @xmath14 and @xmath15 be positive measures in @xmath79 for a prescribed scalar value @xmath37 . plugging the positive measures @xmath135 and @xmath136 into the definition of hpd @xmath91 , we get the following definition :    for conjugate exponents @xmath137 and @xmath37 , the proper hlder divergence between two densities @xmath14 and @xmath15 is defined by : @xmath138    by definition , @xmath139 is a _ two - parameter family _ of dissimilarity statistical measures .",
    "following hlder s inequality , we can check that @xmath140 and @xmath141 iff @xmath142 , i.e. @xmath143 ( see appendix [ sec : hdproof ] ) .",
    "if @xmath14 and @xmath15 belong to the statistical probability manifold , then @xmath141 iff @xmath54 almost everywhere .",
    "this says that hd is a proper divergence for probability measures , and it becomes a pseudo - divergence for positive measures .",
    "note that we have abused the notation @xmath144 to denote both the hlder pseudo - divergence ( with one subscript ) and the hlder divergence ( with two subscripts ) .",
    "similar to hpd , hd is asymmetric when @xmath112 with the following reference duality : @xmath145    hd can be symmetrized as : @xmath146 furthermore , one can easily check that hd is a projective divergence .    for conjugate exponents",
    "@xmath147 and @xmath37 , we rewrite the definition of hd as : @xmath148 therefore hd can be reinterpreted as the skew bhattacharyya divergence  @xcite between the escort distributions . in particular ,",
    "when @xmath149 , we get : @xmath150    the two - parametric family of statistical hlder divergence @xmath151 passes through the one - parametric family of skew bhattacharyya divergences when @xmath149 .",
    "we consider the intersection of the uni - parametric class of hlder pseudo - divergences ( hpd ) with the bi - parametric class of proper hlder divergences ( hd ) : that is , the class of divergences which belong to both hpd and hd .",
    "then we must have @xmath152 .",
    "since @xmath153 , we get @xmath154 .",
    "therefore the cauchy - schwarz ( cs ) divergence is the _",
    "unique _ divergence belonging to both hpd and hd classes : @xmath155 in fact , the cs divergence is the intersection of the four classes hpd , symmetric hpd , hd , and symmetric hd .",
    "figure  [ fig : set ] displays a diagram of those divergence classes with their inclusion relationships .        as stated earlier , notice that the cauchy - schwarz inequality : @xmath156 is not proper as it is an equality when @xmath14 and @xmath15 are _ linearly dependent _",
    "( i.e. , @xmath157 for @xmath106 ) . the arguments of the cs divergence are square - integrable real - valued density functions @xmath14 and @xmath15 .",
    "thus the cauchy - schwarz divergence is not proper for positive measures but is proper for normalized probability distributions since @xmath158 implies that @xmath159 .",
    "let us define the inner product of unnormalized densities as : @xmath160 ( for @xmath161 integrable functions ) , and define the @xmath162 norm of densities as @xmath163 for @xmath164 .",
    "then the cs divergence can be concisely written as : @xmath165 and the hlder pseudo - divergence writes as : @xmath166    when @xmath167 , we have @xmath168 .",
    "then it comes that : @xmath169 when @xmath170 and @xmath171 , we have : @xmath172    now consider a pair of probability densities @xmath14 and @xmath15 .",
    "we have : @xmath173 in an estimation scenario , @xmath14 is fixed and @xmath174 is free along a parametric manifold @xmath175 , then minimizing hlder divergence reduces to : @xmath176 therefore when @xmath177 varies from 1 to @xmath178 , only the regularizer in the minimization problem changes . in any case , hlder divergence always has the term @xmath179 , which shares a similar form with the bhattacharyya distance  @xcite : @xmath180 hpd between @xmath101 and @xmath102 is also closely related to their cosine similarity @xmath181 . when @xmath109 , hd is exactly the cosine similarity after a non - linear transformation",
    "we report closed - form formulas for the hpd and hd between two distributions belonging to the same exponential family provided that the natural parameter space is a cone or affine .",
    "a cone @xmath182 is a convex domain such that for @xmath183 and any @xmath106 , we have @xmath184 . for example , the set of positive measures absolutely continuous with a base measure @xmath78 is a cone .",
    "recall that an exponential family  @xcite has a density function @xmath185 that we be written canonically as :    @xmath186    in this work , we consider the auxiliary carrier measure term @xmath187 .",
    "the base measure is either the lebesgue measure @xmath78 or the counting measure @xmath188 .",
    "a conic or affine exponential family ( caef ) is an exponential family with the natural parameter space @xmath189 a cone or affine .",
    "the log - normalizer @xmath190 is a strictly convex function also called cumulant generating function  @xcite .",
    "[ lemma : holderclosedform ] for distributions @xmath191 and @xmath192 belonging to the same exponential family with conic or affine natural parameter space  @xcite , both the hpd and hd are available in closed - form : @xmath193    consider @xmath187 and a conic or affine natural space @xmath189 ( see  @xcite ) , then for all @xmath194 , we have : @xmath195 since @xmath196 .",
    "indeed , we have : @xmath197    similarly , we have for all @xmath198 ( details omitted ) , @xmath199 since @xmath200 .",
    "therefore , we get : @xmath201    when @xmath202 , we have @xmath203 . to get similar results for the reverse hlder divergence , we need the natural parameter space to be affine ( eg . , isotropic gaussians or multinomials , see  @xcite ) .    in particular , if @xmath14 and @xmath15 belong to the same exponential family so that @xmath204 and @xmath205 , one can easily check that @xmath206 iff @xmath207 . for hd , we can check @xmath208 is proper since @xmath66 .",
    "the following result is straightforward from lemma  [ lemma : holderclosedform ] .    for distributions @xmath191 and @xmath192 belonging to the same exponential family with conic or affine natural parameter space  @xcite ,",
    "the symmetric hpd and hd are available in closed - form : @xmath209 -f(\\theta_p+\\theta_q ) ; \\\\",
    "s^\\holder_{\\alpha,\\gamma}(p(x):q(x ) ) & = \\frac{1}{2 } \\left [ f(\\gamma\\theta_p ) + f(\\gamma\\theta_q )   -f\\left(\\frac{\\gamma}{\\alpha}\\theta_p + \\frac{\\gamma}{\\beta}\\theta_q\\right ) -f\\left(\\frac{\\gamma}{\\beta}\\theta_p + \\frac{\\gamma}{\\alpha}\\theta_q\\right)\\right].\\end{aligned}\\ ] ]    by reference duality , @xmath210    note that the hlder score - induced divergence  @xcite does _ not _ admit in general closed - form formulas for exponential families since it relies on a _ function _",
    "@xmath60 ( see definition  4 of  @xcite ) .    note that caef convex log - normalizers satisfy : @xmath211 a necessary condition is that @xmath212 for @xmath106 ( take @xmath213 , @xmath214 and @xmath215 in the above equality ) .",
    "the escort distribution for an exponential family is given by : @xmath216    the hlder equality holds when @xmath217 or @xmath218 . for exponential families ,",
    "this condition is satisfied when @xmath219 .",
    "that is , we need to have :    @xmath220    thus we may choose small enough @xmath221 so that the condition is not satisfied for fixed @xmath222 and @xmath223 for many exponential distributions .",
    "since multinomials have affine natural space  @xcite , this condition is always met , but not for non - affine natural parameter spaces like normal distributions",
    ".    notice the following fact :    the density of exponential families with conic or affine natural parameter space belongs to @xmath79 for any @xmath37 .",
    "we have @xmath224 for any @xmath37 provided that @xmath225 belongs to the natural parameter space . when @xmath189 is a cone or affine , the condition is satisfied .",
    "let @xmath226 denote the unnormalized positive exponential family density and @xmath227 the normalized density with @xmath228 the partition function .",
    "although hd is a projective divergence since we have @xmath229 , observe that the hd value _ depends _ on the log - normalizer @xmath190 ( since the hd is an integral on the support , see also  @xcite for a similar argument with the @xmath35-divergence  @xcite ) .    in practice , even when the log - normalizer is computationally intractable , we may still estimate the hd by monte - carlo techniques : indeed , we can sample a distribution @xmath101 either by rejection sampling  @xcite or by the markov chain monte - carlo ( mcmc ) metropolis - hasting technique : it just requires to be able to sample a proposal distribution that has the same support .",
    "we shall now instantiate the hpd and hd formulas for several exponential families with conic or affine natural parameter spaces .",
    "let @xmath230 and @xmath231 be two categorical distributions in the @xmath232-dimensional probability simplex @xmath233 .",
    "we rewrite @xmath234 in the canonical form of exponential families  @xcite as : @xmath235 with the redundant parameter : @xmath236 from eq .",
    "[ eq : pcat ] , the convex cumulant generating function has the form @xmath237 . the inverse transformation from @xmath234 to @xmath177",
    "is therefore given by : @xmath238 the natural parameter space @xmath189 is affine ( hence conic ) , and by applying lemma  [ lemma : holderclosedform ] , we get the following closed - form formula : @xmath239     for @xmath240 , and @xmath241 for @xmath242 , compared to kl divergence . the reference distribution @xmath243 is presented as `` @xmath244 '' .",
    "the minimizer of @xmath245 , if different from @xmath246 , is presented as `` @xmath247 '' .",
    "notice that @xmath248.[fig : cat ] ]     for @xmath240 , and @xmath241 for @xmath242 , compared to kl divergence .",
    "the reference distribution @xmath243 is presented as `` @xmath244 '' .",
    "the minimizer of @xmath245 , if different from @xmath246 , is presented as `` @xmath247 '' . notice that @xmath248.[fig : cat ] ]    to get some intuitions , fig .",
    "[ fig : cat ] shows the hlder divergence from any categorical distribution @xmath249 to a given reference distribution @xmath243 in the 2d probability simplex @xmath250 .",
    "a main observation is that the kullback - leibler ( kl ) divergence exhibits a barrier near the boundary @xmath251 with large values .",
    "this is _ not _ the case for hlder divergences : @xmath252 does not have a sharp increase near the boundary ( although it still penalizes the corners of @xmath250 ) .",
    "for example , let @xmath253 , @xmath254 , then @xmath255 but @xmath256 .",
    "another observation is that the minimum @xmath257 can be reached at some point @xmath258 ( see for example @xmath259 in fig .",
    "[ subfig : cat2 ] , the bluest area corresponding to the minimum of @xmath257 is not in the same location as the reference point ) .    consider a hpd ball of center @xmath260 and prescribed radius @xmath261 wrt the hpd .",
    "since @xmath262 for @xmath263 does not belong to the probability manifold but to the positive measure manifold and since the distance is projective , we deduce that the displaced ball center @xmath264 of a ball @xmath260 lying the probability manifold can be computed as the intersection of the ray @xmath265 with the probability manifold .",
    "for the discrete probability simplex @xmath266 , since we have @xmath267 , we deduce that the displaced ball center is at :    @xmath268    this center is displayed as `` @xmath247 '' in figure  [ fig : cat ] .    in general ,",
    "the hpd bisector  @xcite between two distributions belonging to the same caef is defined by : @xmath269      bernoulli distribution is just a special case of the category distribution when the number of categories is @xmath270 ( i.e. , @xmath271 ) . to be consistent with the previous section ,",
    "we rewrite a bernoulli distribution @xmath272 in the canonical form : @xmath273 so that @xmath274 then the cumulant generating function becomes @xmath275 . by lemma  [ lemma : holderclosedform ] ,",
    "@xmath276      let us now instantiate the formulas for multivariate normals ( gaussian distributions ) .",
    "we have the log - normalizer @xmath190 expressed using the usual parameters as  @xcite : @xmath277 since @xmath278    it follows that :    @xmath279    therefore , we have :    @xmath280    , where @xmath243 is standard gaussian distribution , and @xmath281 as compared to the kl divergence .",
    "second row : @xmath241 for @xmath242 .",
    "notice that @xmath248 . ]",
    "we thus get the following closed - form formula for @xmath282 and @xmath283 :    @xmath284    figure [ fig : gauss ] shows hpd and hd for univariate gaussian distributions as compared to the kl divergence .",
    "the zero - centered laplace distribution is defined on the support @xmath285 with the pdf : @xmath286    we have @xmath287 , @xmath288 .",
    "therefore , it comes that : @xmath289 in this special case , @xmath290 does not vary with @xmath35 .",
    "the wishart distribution is defined on the @xmath291 positive definite cone with the density : @xmath292 where @xmath293 is the degree of freedom , and @xmath294 is a scale matrix .",
    "we rewrite it into the canonical form : @xmath295 we can see that @xmath296 , @xmath297 , @xmath298 , and @xmath299 the resulting @xmath300 and @xmath290 are straightforward from the above expression of @xmath190 and lemma  [ lemma : holderclosedform ] .",
    "we will omit these tedious expressions for brevity .      given two finite mixture models @xmath301 and @xmath302 , we derive analytic bounds of their hlder divergences .",
    "when approximation is only needed , one may compute hlder divergences based on monte - carlo stochastic sampling .",
    "let us assume that all mixture components are in an exponential family  @xcite so that @xmath303 and @xmath304 are densities ( wrt the lebesgue measure @xmath78 ) .",
    "we rewrite the hlder divergence into the form : @xmath305    to compute the first term , we observe that a product of mixtures is also a mixture : @xmath306 which can be computed in @xmath307 time .    the second and third terms in eq .",
    "[ eq : holdermixture ] are not straightforward to calculate and shall be bounded .",
    "based on computational geometry , we adopt the log - sum - exp bounding technique of  @xcite and divide the support @xmath16 into @xmath308 pieces of elementary intervals @xmath309 . in each interval @xmath310 , the indices : @xmath311 representing the unique dominating component and the dominated component .",
    "then we bound as follows : @xmath312    all terms on the lhs and rhs of eq .",
    "[ eq : lse ] can be computed exactly by noticing that :    @xmath313    when @xmath314 where @xmath189 denotes the natural parameter space , the integral @xmath315 converges , see  @xcite for further details .",
    "then the bounds of @xmath316 can be obtained by summing the bounds in eq .",
    "[ eq : lse ] over all elementary intervals .",
    "thus @xmath317 can be both lower and upper bounded .",
    "we study the application of hpd and hd for clustering distributions  @xcite , specially clustering gaussian distributions  @xcite , which have been used in sound processing  @xcite , sensor network  @xcite , statistical debugging  @xcite , and quadratic invariants of switched systems  @xcite , etc .",
    "other potential applications of hd may include nonnegative matrix factorization  @xcite , and clustering von mises - fisher  @xcite ( log - normalizer expressed using bessel functions ) .",
    "we study center - based clustering of a finite set of distributions : given a list of distributions belonging to the same conic exponential family with natural parameters @xmath318 and their associated positive weights @xmath319 with @xmath320 , consider their centroids based on hpd and hd as follows : @xmath321    by abuse of notation , @xmath322 denotes both the hpd centroid and hd centroid .",
    "when the context is clear , the parameters in parentheses can be omitted so that these centroids are simply denoted as @xmath323 and @xmath324 .",
    "both of them are defined as the right - sided centroids .",
    "the corresponding left - handed centroids are obtained according to reference duality , i.e. , @xmath325    by lemma  [ lemma : holderclosedform ] , these centroids can be obtained for distributions belonging to the same exponential family as follows : @xmath326,\\\\ c_{\\alpha,\\gamma } & = \\operatorname*{\\arg\\,\\min}_{c}\\left [ \\frac{1}{\\beta}f(\\gamma c ) - \\sum_{i=1}^n w_i f\\left(\\frac{\\gamma}{\\alpha}\\theta_i+\\frac{\\gamma}{\\beta}c\\right ) \\right].\\end{aligned}\\ ] ]    let @xmath327 , we get : @xmath328 = \\frac{\\beta}{\\alpha } c_\\alpha = \\frac{1}{\\alpha-1 } c_{\\alpha}(\\{\\theta_i , w_i\\}),\\ ] ] meaning that the hpd centroid is just a special case of hd centroid up to a _ scaling transformation _ in the natural parameters space",
    ". let @xmath329 , we get : @xmath330 = c_\\alpha\\left(\\left\\{\\frac{\\beta}{\\alpha}\\theta_i , w_i\\right\\}\\right ) = c_\\alpha\\left(\\left\\{\\frac{1}{\\alpha-1}\\theta_i , w_i\\right\\}\\right).\\ ] ]    let us consider the general hd centroid @xmath324 . since @xmath27 is convex , the minimization energy is the sum of a _ convex function _",
    "@xmath331 with a _ concave function _ @xmath332 . we can therefore use the concave - convex procedure ( cccp )  @xcite that optimizes difference of convex programs ( dcps ) : we start with @xmath333 ( the barycenter , belonging to @xmath189 ) and then update : @xmath334 for @xmath335 until convergence .",
    "this can be done by noting that @xmath336 are the dual parameters that are also known as the expectation parameters ( or moment parameters ) .",
    "therefore @xmath337 and @xmath338 can be computed through legendre transformations between the natural parameter space and the dual parameter space .",
    "this iterative optimization is guaranteed to converge to a _",
    "minimum , with a main advantage of bypassing the learning rate parameter of gradient descent algorithms .",
    "since @xmath27 is strictly convex , @xmath339 is monotonous , and the rhs expression can be interpreted as a multi - dimensional quasi - arithmetic mean .",
    "in fact , it is a barycenter on non - normalized weights scaled by @xmath340 .      in this case ,",
    "cccp update rule is not in closed form because we can not easily inverse the sum of gradients ( but when @xmath342 , the two terms collapse so cs centroid can be calculated using cccp ) .",
    "nevertheless , we can implement the reciprocal operation numerically .",
    "interestingly , the symmetric hd centroid can be solved by cccp ! it amounts to solve : @xmath343.\\ ] ]              given a set of fixed densities @xmath348 , we can perform _",
    "variational @xmath61-means _",
    "@xcite with respect to the hlder divergence to minimize the cost function : @xmath349 where @xmath350 are the cluster centers , and @xmath351 is the cluster label of @xmath352 .",
    "the algorithm is given by algorithm  [ algo : kmeans ] .",
    "notice that one does not need to wait for the cccp iterations to converge .",
    "it only has to improve the cost function @xmath353 before updating the assignment .",
    "we have implemented the algorithm based on the symmetric hd",
    ". one can easily modify it based on hpd and other variants ."
  ],
  "abstract_text": [
    "<S> we describe a framework to build distances by measuring the tightness of inequalities , and introduce the notion of proper statistical divergences and improper pseudo - divergences . </S>",
    "<S> we then consider the hlder ordinary and reverse inequalities , and present two novel classes of hlder divergences and pseudo - divergences that both encapsulate the special case of the cauchy - schwarz divergence . </S>",
    "<S> we report closed - form formulas for those statistical dissimilarities when considering distributions belonging to the same exponential family provided that the natural parameter space is a cone ( e.g. , multivariate gaussians ) , or affine ( e.g. , categorical distributions ) . </S>",
    "<S> those new classes of hlder distances are invariant to rescaling , and thus do not require distributions to be normalized . </S>",
    "<S> finally , we show how to compute statistical hlder centroids with respect to those divergences , and carry out center - based clustering toy experiments on a set of gaussian distributions that demonstrate empirically that symmetrized hlder divergences outperform the symmetric cauchy - schwarz divergence .    </S>",
    "<S> keywords : _ hlder inequalities ; hlder divergences ; projective divergences ; cauchy - schwarz divergence ; hlder escort divergences ; skew bhattacharyya divergences ; exponential families ; conic exponential families ; escort distribution ; clustering . _ </S>"
  ]
}