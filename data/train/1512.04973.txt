{
  "article_text": [
    "dictionary - based entity mention extraction has wide use in search and semantic web related work .",
    "for example , shopping portals annotate text documents with dictionary products to maximize product relevance to user queries .",
    "semantic search detects mentions of entities such as people , organizations and locations in text documents in order to facilitate entity - oriented search .",
    "determining wether a document mentions a product is a challenging task , particulary becuase entity mentions in web documents are noisy .",
    "such mentions are rarely exact matches of the dictionary entities , which can be too long for users to write in full everytime they refer to the entity . for example , in product reviews , it is common for reviewers to use short representations of the entities in product catalogs .",
    "a mention in a document may miss some of the words of the entity or it may have extra words not found in the entity name in the dictionary , therefore it is crucial for algorithms to detect _",
    "approximate mentions _ of entities in addition to exact mentions @xcite    finding entity mentions entails finding substrings in a document sequence such that the substring matches an entity in the dictionary .",
    "candidate substrings _ , are substrings whose words are a full or partial subset of the words of an entity in the dictionary . a naive method to generate _",
    "candidate substrings _ would be to scan a document , generating all substrings of up to size @xmath0 where @xmath0 is the longest dictionary entity and do a dictionary lookup for each of the substrings .",
    "efficient algorithms for this problem follow one of two approaches . the first approach is to build an _",
    "index _ on the entities and perform index lookups of document substrings @xcite .",
    "the second approach recognizes that the number of substrings generated from documents can explode to large numbers , to get around this , they use a _",
    "filter _ to prune many such substrings which do not match any dictionary entity @xcite @xcite and then only verify the remaining substrings if they are entity mentions of dictionary entities , by means of a text join . if the longest entity in the dictionary has _",
    "l _ words . given a document @xmath1 , all substrings with length up to _",
    "l _ are possible mention candidates .",
    "this produces @xmath2 substrings to be looked up .",
    "the filter serves to reduce the number of lookups as only substrings that pass the filter are verified if they in fact refer to a dictionary entity or not .",
    "the choice between the _ index - based _ approach and the _ filter & verification - based _ approach is a case - to - case decision as the best approach depends on the characteristics of the input entity dictionary , for example frequency of entity mentions . for non - distributed algorithms ,",
    "the main differentiating factor is that the filter is a more compact structure than the index , in most cases the filter fits in memory whereas the index does not .",
    "performace of the index - based approach is affected by the indexing scheme .",
    "indexing on single words has a different effect to , for example , indexing sets of words that frequently co - occur in dictionary entities .",
    "individual word index posting lists can grow too long which incur long times for merging posting lists .",
    "performance of the filter & verification - based approach is affected by the type of filter used to prune substrings , a basic filter like a prefix - based filter for pruning out prefixes not likely to match dictionary entities would perform differently from a probabilistic filter such a latent signature hash ( lsh ) filter .",
    "if we consider the indexing scheme to be a paremeter and the filter to be a paremeter , than the number of available approaches can be large and the choice becomes a challenge . in this paper",
    "we propose to add to this space of approximate entity mention extraction methods .",
    "our proposal is based on the fact that while performance of the input entity dictionary at hand , the dictionary can be heterogenous such that it consists of partitions that if each partition is processed seperately by the most suitable method , it can result in cheaper aggregate run times than any of the methods applied to the entire dictionary .",
    "this is the hybrid approach .",
    "given the choices of indexing schemes and filters , and the hybrid approach combining index - based and fiilter & verification based methods , the important problem we address is this paper is that of which of the indexing schemes to use , which of the filters to use and wether or not to use a hybrid approach , and if using a hybrid approach where should we partition the entity dictionary .",
    "this is an optimization problem over a large space of approaches .",
    "we solve this optimization problem over distributed algorithms , since we need to deal with large dictionaries and even larger large datasets , we develop and optimize over scalable algorithms in the form of mapreduce distributed processing . in a mapreduce setting , network time plays a significant role on job completion time , furthermore , additional mapreduce - specific coordination tasks such as disk - based sorting introduce costs that are not part of single machine algorithms .",
    "additional distributed setting variables make the choice between the _ index - based _ approach and the _ filter & verification - based _ approach more challenging . to differentiate between these parallel computation coordination tasks and actual processing time spent on the job",
    ", we make a distinction between _",
    "work done time _ and _ job completion time _ in the objective functions we define for optimization .    in this paper",
    ", we introduce an operator named the _ entity extraction join operator , ee - join operator _ for optimizing entity extraction in mapreduce .",
    "we define the space of approaches for approximate entity mention extraction , and propose a hybrid approach .",
    "we develop a cost model for estimating the costs of each of the approaches and efficiently search the space of available options .    in summary",
    "we make the following contributions :    * an operator , _ ee - join _ for highly scalable and optimized entity extraction in mapreduce , which works with different objective functions , we use two distinct objective functions , the _",
    "work done time _ and _ job completion time _ * a study the space of available approaches for approximate entity mention extraction , and propose an additional approach * a cost model for estimating execution time for the twor objective functions . * a means to gather data statistics needed leveraged by the cost model .",
    "* an efficient algorithm for searching the space of available approaches . * an experimental evaluation of our operator , with entity dictionaries consisting of entities that follow various mention distributions .",
    "the rest of this paper is organized as follows .",
    "we next give the semantics of dictionary entity extraction in section 2 .",
    "section 3 describes approximate entity mention algorithms and presents our adaptations of single machine algorithms to their mapreduce counterparts that form the building blocks of the entity extraction join operator .",
    "section 4 describes the costmodel .",
    "section 5 describes the optimization problem over the cost model as the objective function and presents our solution to the optimization problem .",
    "we report our experimental results in section 6 .",
    "section 7 is a review of related work on optimization in text analytics and joins in mapreduce .",
    "finally , we conclude in section 8 .",
    "in this section , we formally define dictionary - based entity extraction , our focus is on approximate mentions where substrings can be partial matches of the dictionary entitites .",
    "the similarity function used plays an important role in the semantics of approximate mentions , we therefore define and motivate similarity functions we use .",
    "approximate dictionary - based entity extraction takes as input a dictionary of entities , @xmath3 and a document collection @xmath4 to output all pairs @xmath5 , such that @xmath6 where @xmath7 , @xmath8 is a substring in @xmath1 , @xmath9 and @xmath10 is a given similarity threshold .    to compute pair - wise similarity @xmath11 ,",
    "different similarity functions can be used depending on the desired semantics .",
    "a commonly used similarity measure is _ jaccard similarity _",
    "@xcite , defined as : @xmath12 = @xmath13 .",
    "jaccard similarity is a symmetric measure ; where asymmetric semantics are desired a different measure is needed .",
    "for example , suppose the dictionary contains two entries , _",
    "e1:iphone charger _ and _ e2 : apple iphone 4 black or white 32 g at&t_. assuming a document contains a substring _",
    "s1 : iphone 4_. then @xmath14 = @xmath15 , but @xmath16 = @xmath17 , although _ e2 _ is semantically the better match .",
    "jaccard similarity gives a lower score to _ e2 _ because a large fraction of its words are missing in _ s1_. a measure that reflects the fact that _ s1 _",
    "is fully contained in _ e2 _ would solve this problem .",
    "prior work has defined an asymmetric measure _ jaccard containment _ as : @xmath18 = @xmath19 .",
    "+ thus the jaccard containment of _ iphone 4 _ in _ e1 _ , @xmath20 = @xmath15 , whereas @xmath21 = @xmath22 .",
    "we use jaccard containment as the similarity measure .",
    "we note that there are two important variations for the _ jaccard containment _ measure .",
    "the first jaccard containment variation tolerates missing words in the approximate mention , as in the example above .",
    "the second variation tolerates extra tokens in the approximate mention . clearly , the semantics fo the jaccard containment variations are different and one may be desirable in settings where the other is not . +",
    "* definition 1 : * given an entity @xmath23 and a document substring @xmath8 , the jaccard containment of @xmath8 in @xmath23 , allowing for _ missing _ words in @xmath8 , is : @xmath24 = @xmath25 . allowing _",
    "extra _ words in @xmath8 , @xmath26 = @xmath27 .",
    "+ using jaccard containment and its variations we can leverage an interesting property that enables efficient computation of jaccard variants . given a similarity threshold @xmath28 , we can compute all substrings @xmath29 such that the total weight of words in @xmath30 , @xmath31 is @xmath32 .",
    "all these substrings are approximate mentions according to jaccard containment , we refer to these as the the jaccard variants of a string . for example , consider the entity , _ apple iphone 4 32g_. suppose the token weights are as follows : _",
    "apple:\\{1 } , iphone:\\{8 } , 4:\\{2 } , 32g:\\{1}_. for @xmath33 , the jaccard variants is of the entity are : \\{apple iphone 4 } , \\{iphone 4 } , \\{iphone 4 32 g } , \\{apple iphone 4 32g}. if we store compute and store all jaccard variants of dictionary entities , we can perform exact match comparisons between the jaccard variants of the dictionary entities and the jaccard variants of potential mentions . + * definition 2 : * a subsequence @xmath34 is a jaccard variant of @xmath35 if @xmath36 . for settings where words are _ weighted _ , a weighted subsequence @xmath37 is a jaccard variant of @xmath38 whose weight is @xmath39 , if @xmath40 . + computing the jaccard variants for the dictionary entities is straightforward , however computing the the jaccard variants if done naively can explode since every substring in a document is potentially a jaccard variant of some dictionary entity .",
    "all these have to be queried against the variants of the dictionary entities .",
    "we avoid generating all possible jaccard variants as explained later .",
    "having defined the semantics , we can introduce algorithms for approximate entity mention extraction . for each algorithm",
    "we briefly describe the single machine version before explaining how adaption to mapreduce is realized through the use of mapreduce constructs .",
    "we then explain the impact of the mapreduce constructs on the performance for each of the algorithms .",
    "chaudhuri et al .",
    "@xcite introduced the notion of set similarity join ( ssjoin ) for identifying similar strings .",
    "they observe that typically efficient algorithms for similartity joins use a similarity function chosen to suit the domain and application .",
    "the premise of ssjoin is to decouple similarity functions from the implementation of similarity joins , instead they propose an operator as a foundation to implement similarity joins that can adaptively handle a variety of similarity functions .",
    "for example , depending on the size of the relations being joined and the availability of indexes , the ssjoin optimizer may choose either index - based plans or merge and hash joins in order to implement the ssjoin operator .",
    "the simplest implementation of the ssjoin on a single machine thus compares every substring @xmath35 to every entity @xmath7 .",
    "we adapt the ssjoin algorithm to mapreduce to create a baseline mapreduce algorithm .",
    "the mappers generate all substrings @xmath35 of length @xmath41 ( maximum entity length ) for every document @xmath9 .",
    "the mappers then generate one or more signatures for each of the substrings .",
    "the same mapper functions are applied to the entire dictionary such that for each @xmath7 , the mappes generate signatures , applying the same signature generating function .",
    "the signature generating function is constructed such that if a substring and a dictionary entity are similar , they will at least have one signature in common .",
    "thus , using signatures as the reduce key , substrings and entities with a signature in common are shuffled to at least one common reducer which computes similarities between substrings .",
    "one of the shortcomings of this algoirthm is that it requires a reduce function .",
    "this incurs a significant amount of data transfer time required to shuffle the substrings and entities to the reducers .",
    "the shuffling cost of this baseline algorithm is exacerbated by the fact that all possible substrings from all documents are generated .",
    "the mapreduce algorithm is outlined in figure [ fig : baselinessj ] .    ' '' ''    * function * map(@xmath42 , @xmath43 ) + * if * = @xmath42 is a document + list @xmath44 generate all substrings from document ( @xmath45 ) + * else if * @xmath42 is an entity + list @xmath44 @xmath42 + * for * @xmath46 @xmath47 @xmath48 * do * + generate signatures from @xmath35 + emit signatures +   + * function * reduce(@xmath49 $ ] ) + hashtable @xmath50 @xmath51 \\ { } + * if * = @xmath42 is an entity + add @xmath52 to hashtable @xmath50 + * else * + * for * @xmath53 * do * + * if * = @xmath54 + emit@xmath55 )    ' '' ''      the index on entities approach creates an index on words of the dictionary entities . it then generates all substrings and queries the index for similar entities .",
    "we adapt the index - based approach to entities as follows : first we generate the index on the entities as a seperate mapreduce job .",
    "the index is then broadcast to every mapper node .",
    "the dictionary is broadcast to every mapper node .",
    "the type of index used can vary from application domain . assuming a basic index with inverted lists per word , and then for each query substring @xmath56 retrieve all lists corresponding to words in @xmath56 .",
    "the union of the lists is the candidates entities that are mentioned by @xmath56 .",
    "each of the candidates entities are then verified to determine if they are true mentions .",
    "the mapreduce algorithm is outlined in figure [ fig : indexonentities ] .",
    "though the index is created in separate mapreduce it is not a significant portion of the execution time as the dictionary is typically much smaller than the document collection .",
    "one of the limiting factors of this algorithm is the fact that the index can be large , may not fit in memory .",
    "this means the index has to be partitioned into smaller indices which can fit in memory , and the entire corpus has to be processed serveral times , once for every index partition .",
    "the mapreduce algorithm is outlined in figure [ fig : indexonentities ] .",
    "the type of index used plays an important role in the performance of the algorithm .",
    "we studied three types of indices and their properties .    * * per word index * : this is the basic index , where an inverted list is generated for every word , storing all entities consisting of that word .",
    "while single word inverted index can be generated quickly , these lists can grow very large , making the task of list merging expensive . * * prefix - index * : a prefix index arranges the words of the entities according to a fixed order , for example , based on decreasing occurrence frequency . during similarity join , we have to generate the prefixes of the substrings and query them against the index to verify that the substring - entity match surpasses a user specified threshold . like the per word index , the prefix index is quick to generate",
    ", its advatange is that it reduces the problem of potentially long inverted lists . * * jaccard variant index * : the jaccard variant index is an index on all the jaccard variants of all the entities . during similarity",
    "join , we have to generate the jaccard variants of the the substrings .",
    "the advantage of the jaccard variant index is that it requires no verification , a substring with a jaccard variant with an inverted list @xmath57 is an approximate mention of all those entities with the jaccard variant as a substring .",
    "constructing a jaccard vairant index is slightly more expensive then other two .    ' '' ''    * function * map(@xmath58 , @xmath45 ) + index @xmath59 load index into memory + list @xmath44 generate all substrings from document ( @xmath45 ) + * for * @xmath46 @xmath47 @xmath48 * do * + lookup @xmath35 on the index + * if * = @xmath60 + emit@xmath61 )    ' '' ''      we have introduced the baseline ssjoin algorithm and the index - based algorithm , both generate all possible substrings and then computing a similarity join between a large set of substrings and the dictionary entities .",
    "perfoming a similarity join between all substrings is a large peformance bottleneck . to overcome this problem we first filter out all substrings that can not match with any dictionary entity , and only then , perform a set similarity join ( ssjoin ) .",
    "we use the the ishfilter introduced by chakrabarti et al .",
    "@xcite to prune a large number of substrings that are obvious non - mentions .",
    "the ssjoin algorithm is then applied to remaining substrings with the dictionary entities .",
    "the difference between baseline _ ssjoin _ and _ ishfilter & ssjoin _ is that shuffling cost is much lower as number of substrings is substantially reduced by the filter .",
    "the mapreduce algorithm is outlined in figure [ fig : filterssj ] .",
    "so far we have introduced the ssjoin as using a signature scheme to generate sinatures such that substrings and entities with a signature in common are shuffled to atleast one reducer . the signature used upon which the data is shuffled plays a significant role in performance .",
    "we studied three signature schemes .    * * single word signatures * : using each word as a signature has a lot of skew , becuase some words are very common .",
    "this results in high shuffling costs . since each substring and entity",
    "consists of many words , this type of signature results in duplicate work at the reducers . * * prefix signatures * : the prefix signature uses prefixes as signatures . while quick to generate , the prefix signatures are susceptible to skew , causing shuffling costs to be skewed to a few nodes .",
    "the prefix signature requires verification at the reducers . * * locality - sensitive hashing ( lsh ) signatures * : is an algorithm for solving approximate or exact similarity .",
    "it is probabilistic in the sense that it uses a hash on the the input so that similar items are mapped into the same group with high probability . like prefix signatures ,",
    "lsh signatures require verification . *",
    "* jaccard variant signatures * : jaccard variants as signatures has the advantage that it reduces data skew , while also not requiring verification at the reducers .    ' '' ''    * function * map(@xmath42 , @xmath43 ) + * if * = @xmath42 is a document + list @xmath62 generate all substrings from document ( @xmath45 ) + list @xmath44 apply _ ishfilter _ to all substrings @xmath63 + * else if * @xmath42 is an entity + list @xmath44 @xmath42 + * for * @xmath46 @xmath47 @xmath48 * do * + generate signatures from @xmath64 + emit signatures +   + * function * reduce(@xmath49 $ ] ) + hashtable @xmath50 @xmath51 \\ { } + * if * = @xmath42 is an entity + add @xmath52 to hashtable @xmath50 + * else * + * for * @xmath53 * do * + * if * = @xmath54 + emit@xmath55 )    ' '' ''      the fourth approach for approximate entity mention extraction on mapreduce is to create an index on the entire document collection .",
    "the input to the mappers are partitions of the document index , the dictionary of entities is broadcast to every mapper .",
    "each dictionary entity @xmath7 is treated as a query which is posed on the index .",
    "each mapper searches its part of the document index for all entity queries .",
    "the complete list of mentions is the union of mentions found by each mapper .",
    "when the dictionary of entities does not fit in memory , the algorithm makes multiple passes over the document index .",
    "constructing an index on the entire corpus is an expensive operation , this is unlike the approach that constructs the index on the dictionary of entities becuase the dictionary of entities is usually orders of maginitude smaller than the document collection . since the index on the document collection is usually not available upfront , and constructing it is expensive , we do not persue this approach further in the rest of the study .",
    "we have aldready eliminated the _ index on documents _ algorithm from the algorithms considered for the eejoin operator .",
    "we further eliminate the _ mapreduce ssjoin _",
    "algorithm due to its limitation with generating all substrings .",
    "instead we keep the optimized version of ssjoin , the _ mapreduce ishfilter & ssjoin_. we also keep the _ mapreduce index on entities _ but instead of generating all substrings from documents , we use the filter to effectively have the _ mapreduce ishfilter & index on entities _ approach .",
    "these two algorithms provide a rich set of options as both the allow tuning of the signatures used by the ssjoin and the type of index used by the entity indexing algorithm .",
    "having described the algorithms for the eejoin operator , we now present the cost - model used to estimate performance of the each of the algorithms . using a cost model ,",
    "we can automatically determine which of the algorithms has the best performance for a given input dictionary @xmath3 and document collection @xmath4 .",
    "we consider two objective functions , one for the total work done and another for the job completion time .",
    "the job completion time of the _ index on entities _ approach is made up of two main componets .",
    "the first is the substring lookup time denoted by @xmath65 in definition 3 .",
    "the total lookup time is equally distributed among the mappers due to the mapreduce load balancers for the mapper , thus total lookup time is @xmath66 .",
    "the second is the number of iterations made over all the substrings due to th entity dictionary not fitting in memory , denoted by @xmath67 .",
    "_ explain estimation of |c| and |m| from data statistics .",
    "_ + * definition 3 : * the cost of the index approach , for job completion time , is defined as follows : @xmath68 where @xmath69 is the number of candidate substrings from all documents in the dataset , @xmath70 is the number of mappers , @xmath71 is the dictionary size and @xmath72 is the memory budget for the index .",
    "thus @xmath73 is the total number of passes made over the data .",
    "+ the job completion time of the _ ishfilter & ssjoin _ approach consists of three main components .",
    "the first is the cost of generating signatures over @xmath74 this cost depends on the type of signature used .",
    "the second is the cost of shuffling the signatures over the network , @xmath75 .",
    "this cost depends on the number of signatures per candidate .",
    "the third is the cost of verifying the candidates of a signature , @xmath76 as shown in definition 4 . + * definition 4 : * the cost of the filter & ssjoin approach , for job completion time , is defined as : @xmath77 . where @xmath78 is the cost of generating signatures , @xmath79 is the cost of shuffling signatures and @xmath80 is the cost of verifying candidates of a signature .",
    "we optimize over a large plan space of algorithms .",
    "the plan space is made up of two core algorithms .",
    "however , each of the algorithms can be instantiated with several signature schemes . furthermore ,",
    "the dictionary is partitioned such that a fraction of the entities is processed by one of the core approaches and the rest are processed by another core approach .",
    "furthermore , any combinations of the different signature schemes form a possible hybrid approach , thus creating a large space of possible plans .",
    "the _ ee - operator _ optimizes entity extraction by partitioning entities into mention frequency categories .",
    "the intuition is that each of the algorithms performs better for entities of certain mention frequencies than the other approach .",
    "therefore , for a given input dictionary and text collection , a fraction of the entities is processed by the _ index - based _ approach , for some signature scheme , and the remaining fraction is processed by the _",
    "filter & verification - based _ approach , for some signature scheme , not necessarily the same as the one used by the _ index - based _ approach .",
    "therefore , a plan for the _ ee - operator _ is a combination of the _ index - based _ approach and _ filter & verification - based _ approach .",
    "thus the cost of a plan , where @xmath81 proportion of the entities are processed by the _ index - based _ approach and @xmath82 proportion are processed by the _",
    "filter & verification - based _ approach , is :    @xmath83    when only one of the approaches is used , the approach not used contributes a cost of @xmath84 to the plan .",
    "the signature schemes are denoted by _",
    "sigx _ and _ sigy _ for the _ index - based _ approach and the _ filter & verification - based _",
    "approach respectively .",
    "thus searching over the search space is an optimization problem to _ minimize _ the cost of the plan .      in this section",
    "we describe the algorithm for searching an optimal plan , @xmath85 , in a large space .",
    "the plan is a function of the signature scheme and the entity extraction algorithms.s suppose that we have three signature schemes _ prefix , jaccard variants , and lsh_. we pick any pair of approaches , for example , _ index - based _ approach using prefix signatures and the _ filter & verification - based _ using jaccard variant signatures and we then seek to determine how to partition the entities . if we do a naive enumeration of the costs at every possible partitioning point , for a dictionary size of @xmath86 , we do @xmath86 enumerations .",
    "the dictionary is typically large , in the order of millions , thus an exhaustive enumeration would not be practical .",
    "we reduce the number of enumerations by using an efficient search algorithm since the entities are already sorted by occurrence frequency .",
    "1 .   currentcheapestcost = @xmath87 2 .",
    "searchrange = 0 - @xmath86 3 .",
    "binarysearch ( searchrange , find new cheapest cost < current cheapest ) 4 .",
    "currentcheapestcost = newcheapestcost 5 .",
    "searchrange = area bounding newcheapestcost 6 .",
    "repeat steps 2 - 5 over an increasingly narrow search range 7 .   until new cheapest is = = current cheapest or @xmath88 is 0 .",
    "emit plan(signature , extraction method )    we repeat the procedure for all pairs of approaches .",
    "if we have three signature schemes we have maximum of nine pairs , which is a small constant . for each pair",
    "we do binary search over an increasingly small range space , @xmath89 times , which is another small constant .",
    "therefore the complexity of the search algorithm is @xmath90 .",
    "we prove that the algorithm works correctly , that is , its output is the partitioning with the cheapest cost within the joint space signature schemes and entity extraction algorithms . in order to prove the correctness of the algorithm",
    ", we need to show that both @xmath91 and @xmath92 are monotonically non - decreasing functions over the sorted space of entities , for any given signature scheme .",
    "since the entities are sorted based on occurrence frequency , based on the definition 3 , @xmath91 , the index cost is only affected by the memory size , how many times the entire collection must be searched is based on the memory capacity and the more the number of the entities the more times we have to search over the collection , thus index cost is monotonically non - decreasing . based on definition 4",
    ", the cost of the ssj is based on cost of shuffling , shuffling cost is highest for the most frequently occurring entities which appear at the beginning since the entities are sorted in descending order of occurrence , thus again ssj cost is non - decreasing .",
    "we thus have the following lemma .",
    "* lemma 1 : * given an ordered list of entities in decreasing order of given a collection of entities ordered in descending order of frequency of occurrence . show this with x , y , like math symbols .",
    "the cost of the filter & ssjoin approach is defined as : @xmath77 . where @xmath78 is the cost of generating signatures , @xmath79 is the cost of shuffling signatures and @xmath80 is the cost of verifying candidates of a signature .",
    "chaudhuri et al .",
    "@xcite eveloped an operator - centric approach for for set - similarity joins .",
    "the main difference in our approach is the target for mapreduce algorithms .",
    "sarawagi et al .",
    "@xcite uses an inverted - index approach to compute set overlap string similarities .",
    "the main difference with our approach is that we do not fix on the index - approach but instead allow the operator to make cost - based decisions in choosing the best implementation of approcximate entity mentions .    in terms of optimization for text - centric tasks",
    ", @xcite introduced an optimizer for choosing between query - based and crawl - based method for various text - analytics tasks in a cots - based way .",
    "the optimizer adaptively selects the best execution strategy .",
    "the ee operator is specifically targeted for mapreduce implementations .",
    "afrati and ullman @xcite investigated the problem of efficient joins in mapreduce .",
    "vernica et al .",
    "@xcite developed algorithms for set similarty joins in mapreduce .",
    "yang , et al .",
    "@xcite extended mapreduce to map - reduce - merge , in order to allow users to express different join types and algorithms .",
    "none of these join approaches propose a cost - model approach for finding the best approach for a given setting .",
    "a number of recent work have studied optimization for mapreduce tasks , though none investigate optimization for approximate mention extraction of entities .",
    "the manimal system @xcite analyses mapreduce programs to do general database - style optimizations .",
    "dittrich , et al @xcite .",
    "proposed the use of indices in order to improve mapreduce performance for certain tasks .",
    "hadoopdb @xcite combines relational and mapreduce qualities into one system .",
    "however , hadoopdb is designed to be a parallel relational database , it does not optimize mapreduce tasks .",
    "ndapandula nakashole , martin theobald , gerhard weikum scalable knowledge harvesting with high precision and high recall in _ proceedings of the forth international conference on web search and web data mining , wsdm , _ 2011 .",
    "n. nakashole , g. weikum real - time population of knowledge bases : opportunities and challenges . in _ proceedings of the joint workshop on automatic knowledge base construction and web - scale knowledge extraction _ , akbc , 2012    n. nakashole , m. sozio , f. m. suchanek , and m. theobald query - time reasoning in uncertain rdf knowledge bases with soft and hard rules . in _ proceedings of the second international workshop on searching and integrating new web data sources , vlds _ , 2012"
  ],
  "abstract_text": [
    "<S> dictionary - based entity extraction involves finding mentions of dictionary entities in text . </S>",
    "<S> text mentions are often noisy , containing spurious or missing words . </S>",
    "<S> efficient algorithms for detecting approximate entity mentions follow one of two general techniques . </S>",
    "<S> the first approach is to build an _ </S>",
    "<S> index _ on the entities and perform index lookups of document substrings . </S>",
    "<S> the second approach recognizes that the number of substrings generated from documents can explode to large numbers , to get around this , they use a _ filter _ to prune many such substrings which do not match any dictionary entity and then only verify the remaining substrings if they are entity mentions of dictionary entities , by means of a text join . </S>",
    "<S> the choice between the _ index - based _ approach and the _ filter & verification - based _ approach is a case - to - case decision as the best approach depends on the characteristics of the input entity dictionary , for example frequency of entity mentions . </S>",
    "<S> choosing the right approach for the setting can make a substantial difference in execution time . making </S>",
    "<S> this choice is however non - trivial as there are parameters within each of the approaches that make the space of possible approaches very large . in this paper , we present a cost - based operator for making the choice among execution plans for entity extraction . </S>",
    "<S> since we need to deal with large dictionaries and even larger large datasets , our operator is developed for implementations of mapreduce distributed algorithms .    </S>",
    "<S> = 10000 = 10000    [ general ] </S>"
  ]
}