{
  "article_text": [
    "the interactive nature of the mining process has been acknowledged from the start  @xcite .",
    "it motivated the idea of a `` data mining query language '' @xcite and was stressed again by ng , lakshmanan , han and pang  @xcite .",
    "a data mining query language allows the user to ask for specific subsets of association rules by specifying several constraints within each query .    in this paper , working in the concrete setting of association rule mining , we consider a class of conditions on associations to be generated which should be expressible in any reasonable data mining query language : boolean combinations of atomic conditions , where an atomic condition can either specify that a certain item occurs in the body of the rule or the head of the rule , or set a threshold on the support or on the confidence . a _ mining session",
    "_ then consists of a sequence of such boolean combinations ( henceforth referred to as _ queries _ ) . efficiently supporting data mining query language environments",
    "is a challenging task . towards this goal ,",
    "we present and compare three approaches . in the first extreme , the _ integrated querying _",
    "approach , every individual data mining query will be answered by running an adaptation of the mining algorithm in which the constraints on the rules and sets to be generated are directly incorporated .",
    "the second extreme , the _ post - processing _ approach , first mines as much associations as possible , by performing one major , global mining operation .",
    "after this relatively expensive operation , the actual data mining queries issued by the user then amount to standard lookups in the set of materialized associations",
    ". a third approach , the _ incremental querying _",
    "approach , combines the advantages of both previous approaches .",
    "we present the first algorithm to support interactive mining sessions efficiently .",
    "we measure efficiency in terms of the total number of itemsets that are generated , but do not satisfy the query , and the number of scans over the database that have to be performed .",
    "specifically , our results are the following :    1 .",
    "although our results show significant improvements of performance , we will also show that exploiting constraints is not always the best solution . more specifically ,",
    "if mining without constraints is feasible to begin with , then the presented post - processing approach will eventually outperform integrated querying .",
    "the querying achieved by exploiting the constraints is _ optimal _ , in the sense that it never generates an itemset that could give rise to a rule that does not satisfy the query , apart from the minimal support and confidence thresholds .",
    "therefore , the number of generated itemsets during the execution of a query , becomes proportional to the strength of the constraints in the query : the more specific the query , the faster its execution .",
    "3 .   not only is the number of passes trough the database reduced , but also the size of the database itself , again proportionally to the strength of the constraints in the query .",
    "a generated itemset will , within a session , never be regenerated as a candidate itemset : results of earlier queries are reused when answering a new query .",
    "this paper is further organized as follows .",
    "section  [ related ] gives an overview of related work on constrained mining . in section  [ queries ] , we present a way of incorporating query - constraints inside a frequent set mining algorithm . in section  [ interactive ] , we discuss ways of supporting interactive mining sessions .",
    "we conclude the paper in section  [ conclude ] .",
    "the idea that queries can be integrated in the mining algorithm was initially launched by srikant , vu , and agrawal  @xcite , who considered queries that are boolean expressions over the presence or absence of certain items in the rules .",
    "queries specifically as bodies or heads were not discussed .",
    "the authors considered three different approaches to the problem .",
    "the proposed algorithms are not optimal : they generate and test several itemsets that do not satisfy the query , and their optimizations also do not always become more efficient for more specific queries .",
    "also lakshmanan , ng , han and pang worked on the integration of constraints on itemsets in mining , considering conjunctions of conditions on itemsets such as those considered here , as well as others ( arbitrary boolean combinations were not discussed )  @xcite .",
    "of the various strategies for the so - called `` cap '' algorithm they present , the one that can handle the queries considered in the present paper is their `` strategy ii '' .",
    "again , this strategy generates and tests itemsets that do not satisfy the query .",
    "also , their algorithms implement a rule - query by separately mining for possible heads and for possible bodies , while we tightly couple the querying of rules with the querying of sets .",
    "this work has also been further studied by pei , han and lakshmanan  @xcite , and employed within the fpgrowth algorithm .",
    "still other work focused on other kinds of constraints over association rules and frequent sets , such as _ correlation _  @xcite , and _ improvement _  @xcite . these and other statistical measures of interestingness will not be discussed in this paper .",
    "all previously mentioned works do not discuss the reuse of results acquired from earlier queries within a session .",
    "nag et al .",
    "proposed the use of a knowledge cache for this purpose  @xcite .",
    "several caching strategies were studied for different cache sizes .",
    "however , their work only considers mining sessions of queries where only constraints on the support of the itemsets are allowed .",
    "no solutions were provided for other constraints like those studied in this paper . also jeudy and boulicaut",
    "have studied the use of a knowledge cache for finding a condensed representation of all itemsets , based on the concept of _ free sets _  @xcite .",
    "as introduced by agrawal et al .",
    "@xcite , the association rule mining problem can be described as follows : we are given a set of items @xmath0 and a database @xmath1 of subsets of @xmath0 called transactions .",
    "an association rule is an expression of the form @xmath2 , where @xmath3 and @xmath4 are sets of items ( itemsets ) .",
    "the _ support _ of an itemset @xmath5 is the number of transactions that include @xmath5 .",
    "an itemset is called _ frequent _ if its support is no less than a given minimal support threshold .",
    "an association rule is called frequent if @xmath6 is frequent and it is called _ confident _ if the support of @xmath7 divided by the support of @xmath3 exceeds a given minimal confidence threshold .",
    "the goal is now to find all association rules over @xmath1 that are frequent and confident .",
    "the standard association rule mining algorithm apriori  @xcite is divided in two phases : phase 1 generates all frequent itemsets with respect to the given minimal support threshold , and phase 2 generates all confident rules with respect to the given minimal confidence threshold .",
    "phase 1 is performed based on the observation ( also called the anti - monotonicity property ) that all supersets of an infrequent itemset are also infrequent .",
    "an itemset is thus potentially frequent , also called a _",
    "itemset , if its support is unknown and all of its subsets are frequent . in every step of the algorithm ,",
    "all candidate itemsets are generated and their supports are then counted by performing a complete scan of the transaction database .",
    "this is repeated until no new candidate itemsets can be generated .",
    "phase 2 generates for every frequent itemset a set of rules by dividing the itemset in potential bodies and heads .",
    "this can be done in a similar level - wise manner as in phase 1 , based on the observation that if a head - set represents a confident rule for that itemset , then all of its subsets also represent confident rules  @xcite .",
    "for example , if the itemset @xmath8 is a frequent set and @xmath9 is a confident rule , then @xmath10 and @xmath11 must also be confident . in every step within phase 2 ,",
    "all candidate head - sets are generated and their confidences are computed , until no new candidate head - sets can be generated . because we do not need to access the database , phase 2 is much faster in comparison with phase 1 .",
    "the performance of apriori - like algorithms is highly dependent of three factors :    1 .",
    "the number of candidate patterns increases exponentially with a decreasing minimal support threshold , 2 .",
    "the number of association rules can become very large for small confidence thresholds , and 3 .",
    "the size of the transaction database is typically very large , such that scanning the database becomes a costly operation .",
    "also the length of the transactions ( density of the database ) plays an important role , because large transactions can result in large frequent patterns , implying a lot of candidate patterns and a lot of scans through the database . since the introduction of the apriori algorithm",
    ", a lot of research has been done to improve its performance by improving on one or more of these factors .",
    "almost all improvements rely on its levelwise , bottom - up , breadth - first nature and on the anti - monotonicity property of the minimal support threshold .",
    "nevertheless , han et al . presented the fpgrowth algorithm  @xcite , which uses a depth - first strategy .",
    "although this algorithm has a very efficient counting mechanism , it suffers from two major deficiencies :    1 .",
    "it can not exploit the anti - monotonicity property , resulting in a lot more candidate patterns , and 2 .",
    "although the used trie data structure somewhat compresses the transaction database , the algorithm implicitly requires the database to reside in main - memory .",
    "although recent improvements have increased the performance of apriori tremendously , the support and confidence thresholds can always be set low enough , resulting in an exponential blowup of the number of patterns and rules .",
    "nevertheless , such low thresholds can still reveal interesting patterns and rules , but one is not interested in all of the discovered patterns and rules , but queries out the interesting ones according to some specified constraints . pushing these constraints as deep as possible into the mining algorithm , such that the amount of computation is proportional to what the user gets ,",
    "should improve its performance and allow lower thresholds .",
    "as already mentioned in the introduction , the constraints we consider in this paper are boolean combinations of atomic conditions .",
    "an atomic condition can either specify that a certain item @xmath12 occurs in the body of the rule or the head of the rule , denoted respectively by @xmath13 or @xmath14 , or set a threshold on the support or on the confidence .    in this section ,",
    "we explain how we can incorporate these constraints in the mining algorithm .",
    "we first consider the special case of constraints where only conjunctions of atomic conditions or their negations are allowed .",
    "let @xmath15 ,  , @xmath16 be the items that must be in the body by the constraint ; @xmath17 ,  , @xmath18 those that must not ; @xmath19 ,  , @xmath20 those that must be in the head ; and @xmath21 ,  , @xmath22 those that must not",
    "recall that an association rule @xmath23 is only generated if @xmath24 is a frequent set .",
    "hence , we only have to generate those frequent sets that contain every @xmath25 and @xmath26 , plus some of the subsets of these frequent sets that can serve as bodies or heads .",
    "therefore we will create a set - query corresponding to the rule - query , which is also a conjunctive expression , but now over the presence or absence of an item @xmath12 in a frequent set , denoted by @xmath27 and @xmath28 .",
    "we do this as follows :    1 .   for each positive literal @xmath13 or @xmath14 in the rule - query , add the literal @xmath27 in the set - query .",
    "2 .   if for an item @xmath12 both @xmath29 and @xmath30 are in the rule - query , add the negated literal @xmath28 to the set - query .",
    "3 .   add the minimal support threshold to the set - query .",
    "all other literals in the rule - query are ignored because they do not restrict the frequent sets that must be generated .",
    "+ formally , the following is readily verified :    [ lemma1 ] an itemset @xmath31 satisfies the set - query if and only if there exists itemsets @xmath32 and @xmath33 such that @xmath34 and the rule @xmath23 satisfies the rule - query , apart from the confidence threshold .",
    "so , once we have generated all sets @xmath31 satisfying the set - query , we can generate all rules satisfying the rule - query by splitting all these @xmath31 in all possible ways in a body @xmath32 and a head @xmath33 such that the rule - query is satisfied .",
    "lemma  [ lemma1 ] guarantees that this method is `` sound and complete '' .",
    "so , we need to explain two things :    1 .   finding all frequent @xmath31 satisfying the set - query .",
    "2 .   finding , for each such @xmath31 , the frequencies of all bodies and heads @xmath32 and @xmath33 such that @xmath34 and @xmath35 satisfies the rule - query .",
    "[ [ finding - the - frequent - sets - satisfying - the - set - query ] ] finding the frequent sets satisfying the set - query + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath36 in set - query@xmath37 and @xmath38 in set - query@xmath37 .",
    "note that @xmath39 .",
    "denote the dataset of transactions by @xmath40 .",
    "we define the following derived dataset @xmath41 : @xmath42 in other words , we ignore all transactions that are not supersets of @xmath43 and from all transactions that are not ignored , we remove all items in @xmath43 plus all items that are in @xmath44 .",
    "we observe :    [ lemma2 ] let @xmath45 be the support threshold defined in the query .",
    "let @xmath46 be the set of itemsets over the new dataset @xmath41 , without any further conditions , except that their support is at least @xmath45 .",
    "let @xmath47 be the set of itemsets over the original dataset @xmath1 that satisfy the set - query , and whose support is also at least @xmath45 . then @xmath48    to show the inclusion from left to right , consider @xmath49 .",
    "we show that @xmath50 is in @xmath51 .",
    "thereto , it suffices to establish an injection @xmath52 from the transactions @xmath53 in the support set of @xmath31 in @xmath1 ( i.e. , the set of all transactions in @xmath40 containing @xmath31 ) into the transactions @xmath54 in the support set of @xmath55 in @xmath41 .",
    "let @xmath53 be in @xmath40 and containing @xmath31 .",
    "since @xmath31 satisfies the set - query , @xmath31 contains @xmath43 , and hence @xmath53 contains @xmath43 as well .",
    "thus , @xmath56 is in @xmath41 . since @xmath57 ( again because @xmath31 satisfies the set - query ) , @xmath54 contains @xmath58 . hence , @xmath54 is in the support set of @xmath55 in @xmath41 , as desired .    to show the inclusion from right to left ,",
    "consider @xmath59 .",
    "we show that @xmath60 is in @xmath61 .",
    "thereto , it suffices to establish an injection @xmath62 from the transactions @xmath54 in the support set of @xmath55 in @xmath63 into the transactions @xmath53 in the support set of @xmath31 in @xmath64 .",
    "let @xmath54 be in @xmath41 and containing @xmath55 .",
    "obviously , a transaction @xmath65 exists , such that @xmath66 .",
    "since @xmath53 contains @xmath67 , it is in the support set of @xmath31 in @xmath40 , as desired .",
    "we can thus perform any frequent set generation algorithm , using only @xmath41 instead of @xmath40 .",
    "note that the number of transactions in @xmath41 is exactly the support of @xmath43 in @xmath40 .",
    "also , the search space of all itemsets is halved for every item in @xmath68 . in practice , the search space of all frequent itemsets is at least halved for every item in @xmath43 and at most halved for every item in @xmath69 .",
    "still put differently : we are mining in a world where itemsets that do not satisfy the query simply do not exist .",
    "the correctness and optimality of our method is thus automatically guaranteed .",
    "note however that now an itemset @xmath5 , actually represents the itemset @xmath70 !",
    "we thus head - start with a lead of @xmath71 , where @xmath71 is the cardinality of @xmath43 , in comparison with standard , non - constrained mining .",
    "[ [ finding - the - frequencies - of - bodies - and - heads ] ] finding the frequencies of bodies and heads + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we now have all frequent sets containing every @xmath25 and @xmath26 , from which rules that satisfy the rule - query can be generated .",
    "recall that in the standard association rule mining algorithm rules are generated by taking every item in a frequent set as a head and the others as body .",
    "all heads that result in a confident rule , with respect to the minimal confidence threshold , can then be combined to generate more general rules .",
    "but , because we now only want rules that satisfy the query , a head must always be a superset of @xmath72 and may not include any of the @xmath73 and @xmath25 ( the latter because bodies and heads of rules are disjoint ) . in this way , we head - start with a lead of @xmath74 .",
    "similarly , a body must always be a superset of @xmath75 and may not include any of the @xmath76 and @xmath26 .",
    "the following lemma ( which follows immediately from lemma  [ lemma2 ] ) tells us that these potential heads and bodies are already present , albeit implicitly , in @xmath51 :    [ lemma3 ] let @xmath51 be as in lemma  [ lemma2 ] .",
    "let @xmath77 ( @xmath78 ) be the set of bodies ( heads ) of those association rules over @xmath40 that satisfy the rule - query . then @xmath79 and @xmath80    so , for the potential bodies ( heads ) , we use , in @xmath51 , all sets that do not include any of the @xmath76 and @xmath26 ( @xmath73 and @xmath25 ) , and add all @xmath25 ( @xmath26 ) .",
    "hence , all we have to do is to determine the frequencies of these subsets by performing one additional scan trough the dataset .",
    "( we do not necessarily yet have these frequencies because these sets do not contain either items @xmath25 or @xmath26 , while we ignored transactions that did not contain all items @xmath25 and @xmath26 . )",
    "each generated itemset can thus have up to three different `` personalities : ''    1 .   a frequent set that satisfies the set - query ; 2 .",
    "a frequent set that can act as body of a rule that satisfies the rule - query ; 3 .   a frequent set that can act as head of a rule that satisfies the rule - query .    hence , we finally have at most three families of sets , i.e. , those sets from which rules must be generated , the _ rule - sets _ ( @xmath51 with all @xmath25 and @xmath26 added ) ; a family of possible bodies , the _ body - sets _ ( @xmath51 with all @xmath25 added , minus all those sets that include any of the @xmath76 and @xmath26 ) ; and yet another family of possible heads , the _ head - sets _ ( @xmath51 with all @xmath26 added , minus all those sets that include any of the @xmath73 and @xmath25 ) .",
    "note that the frequencies of the body - sets and head - sets need not necessarily to be recounted since their frequencies are equal to the frequencies of their corresponding sets in @xmath51 if the query consists of negated atoms only .",
    "we finally generate the desired association rules from the rule - sets , by looking for possible bodies and heads only within the body - sets and head - sets respectively , on condition that they have enough confidence .",
    "[ [ optimality ] ] optimality + + + + + + + + + +    note that every rule - set , body - set , and head - set is needed to construct the rules potentially satisfying the rule - query so that these can be tested for confidence , and moreover , no other sets are ever needed . in this precise sense ,",
    "our method is _",
    "optimal_.    we illustrate our method with an example .",
    "assume we are given the rule - query @xmath81 we begin by converting it to the set - query @xmath82 hence @xmath83 and @xmath84 .",
    "consider a database consisting of the three transactions @xmath85 , @xmath86 and @xmath87 .",
    "we ignore the first transaction because it is not a superset of @xmath43 .",
    "we remove items @xmath88 and @xmath89 from the second transaction because they are in @xmath43 , and we also remove @xmath90 because it is in @xmath44 .",
    "we only remove items @xmath88 and @xmath89 from the third transaction .",
    "table  [ vb1 ] shows the itemsets that result from the mining algorithm after reading , according to lemma  [ lemma1 ] and  [ lemma2 ] , the two resulting transactions .",
    "for example , the itemset @xmath91 actually represents the set @xmath87 .",
    "it also represents a potential body , namely @xmath92 , but it does not represent a head , because it includes item @xmath93 , which must not be in the head according to the given rule - query . as another example , the empty set now represents the set @xmath94 from which a rule can be generated .",
    "it also represents a potential body and a potential head .",
    ".an example of generated sets , which can represent a frequent set , as well as a body , as well as a head [ cols=\"^,^,^,^\",options=\"header \" , ]     for each data set , we generated @xmath95 random boolean queries consisting of at most three atomic conditions .",
    "figure  [ improvement ] shows the improvement on the performance of the algorithm exploiting the constraints .",
    "the y - axis shows the time needed for the algorithm exploiting our queries , relative to the time needed without exploiting the queries .",
    "the x - axis shows the number of patterns satisfying the given query , relative to the total number of patterns .        as can be seen ,",
    "the time needed to generate all frequent sets and association rules is proportional to the restrictiveness of the constraints .",
    "notice that the proportionality factor is 1 .",
    "in the previous section , we have seen a way to integrate constraints tightly into the mining of association rules .",
    "we call this _ integrated querying_. at the other end of the spectrum we have _ post - processing _ , where we perform standard , non - constrained mining , save the resulting itemsets and rules , and then query those results for the constraints .    integrated querying has the following two obvious advantages over post - processing :    1 .",
    "answering one single data mining query using integrated querying is much more efficient than answering it using post - processing .",
    "it is well known that , by setting parameters such as minimal support too low , or by the nature of the data , association rule mining can be infeasible simply because of a combinatorial explosion involved in the generation of rules or frequent itemsets . under such circumstances , of course , post - processing is infeasible as well ; yet , integrated querying can still be executed , if the query conditions can be effectively exploited to reduce the number of itemsets and rules from the outset .    however , as already mentioned in the introduction , data mining query language environments must support an interactive , iterative mining process , where a user repeatedly issues new queries based on what he found in the answers of his previous queries .",
    "now consider a situation where minimal support requirements and data set particulars are favorable enough so that post - processing is not infeasible to begin with",
    ". then the global , non - constrained mining operation , on the result of which the querying will be performed by post - processing , _ can be executed once and its result materialized for the remainder of the data mining session_.    in that case , if the session consists of , say , 20 data mining queries , these 20 queries amount to standard retrieval queries on the materialized mining results .",
    "in contrast , answering every single of the 20 queries by integrated querying will involve at least 20 , and often many more , passes over the data , as each query involves a separate mining operation .",
    "also , several queries could have a non - empty intersection , such that a lot of work is repeated several times .",
    "hence , the total time needed to answer the integrated queries is guaranteed to grow beyond the post - processing total time .",
    "the naively conceived advantages of integrated querying over post - processing become much less clear now .",
    "indeed , if the number of data mining queries issued by the user is large enough , then the post - processing approach clearly outperforms the integrated querying approach .",
    "we have performed several experiments on the data sets described in the previous section which all confirmed this predicted effect .",
    "however , for the post - processing approach , we only materialized all frequent itemsets since the time needed to generate all association rules that satisfy the query turned out to be as fast as finding all such rules from the materialized results . figure  [ cutoff ] shows the total time needed for answering up to @xmath96 different queries on the bms - webview-1 data set . since the time needed to generate all association rules is the same for both approaches , we only recorded the time to generate all itemsets that were needed to generate all association rules",
    ". the queries were randomly generated , only those queries with an empty output were replaced , but all used the same support threshold as was used for the initial mining operation of the post - processing approach . as can be seen , the cut - off point from where the post - processing approach outperforms the integrated querying approach occurs already after the eighth query .          from the above discussion it is clear that we should try to combine the advantages of integrated querying and post - processing .",
    "we now introduce such an approach , which we call _ incremental querying_.    in the incremental approach , all itemsets that result from every posed query , as well as all intermediate generated itemsets , are stored into a cache .",
    "initially , when the user issues his first query , nothing has been mined yet , and thus we answer it using integrated querying .",
    "every subsequent query is first converted to its corresponding rule- and set - query in disjoint dnf . for every disjunct in the set - query , the system adds all currently cached itemsets that satisfy the disjunct to the data structure holding itemsets , that is used for mining that disjunct , as well as all of its subsets that satisfy the disjunct ( note that these subsets may not all be cached ; if they are not , we have to count their supports during the first scan through the data set ) .",
    "we also immediately add all candidate itemsets .",
    "if no new candidate itemsets can be generated , which means that all necessary itemsets were already cached , we are done .",
    "however , if this is not the case , we can now begin our queried mining algorithm with the important generalization that in each iteration , candidate itemsets of different cardinalities are now generated . in order for this to work , candidate itemsets that turn out to be infrequent",
    "must be kept such that they are not regenerated in later iterations .",
    "this generalization was first used by toivonen in his sampling algorithm  @xcite .",
    "caching all generated itemsets gives us another advantage that can be exploited by the integrated querying algorithm .",
    "consider a set - query stating that items @xmath88 and @xmath97 must be in the itemsets .",
    "in the first iteration of the algorithm , all single itemsets are generated as candidate sets over the new data set @xmath41 ( cf .",
    "section  [ conjunct ] ) .",
    "we explained that these single itemsets actually represent supersets of @xmath98 .",
    "normally , before we generate a candidate itemset , we check if all of its subsets are frequent .",
    "of course , this is impossible if these subsets do not even exist in @xmath41 .",
    "now , however , we can check in the cache for a subset with too low support ; if we find this , we avoid generating the candidate .",
    "we thus obtain an algorithm which reuses previously generated itemsets as if they had been generated in previous iterations of the algorithm .",
    "we are optimal in the sense that we never generate and test itemsets that were generated before . for rule generation",
    ", we again did not cache the results , but in stead generated all association rules when needed for the same reasons as explained in the previous section .    in the worst case , the cached results do not contain anything that can be reused for answering a query , and hence the time needed to generate the itemsets and rules that satisfy the query is equal to the time needed when answering that query using the integrated querying approach . in the best case",
    ", all requested itemsets are already cached , and hence the time needed to find all itemsets and rules that satisfy the query is equal to the time needed for answering that query using post - processing . in the average case , part of the needed itemsets",
    "are cached and will then be used to speed up the integrated querying approach .",
    "if the time gained by this speedup is more than the time needed to find the reusable sets , then the incremental approach will always be faster than the integrated querying approach . in the limit",
    ", all itemsets will be materialized , and hence all subsequent queries will be answered using post - processing .",
    "could it be that the time gained by the speedup in the integrated querying approach is less than the time needed to find and reuse the reusable itemsets ?",
    "this could happen when a lot of itemsets are already cached , but almost none of them satisfy the constraints .",
    "it is also possible that the reusable itemsets give only a marginal improvement .",
    "we can however counter this phenomenon by estimating what is currently cached , as follows .",
    "we keep track of a set - query @xmath99 which describes the stored sets .",
    "this query is initially _",
    "false_. given a new query ( rule - query ) @xmath100 , the system now goes through the following steps : ( step  1 was described in section  [ conjunct ] )    1 .",
    "convert the rule - query @xmath100 to the set - query @xmath101 2 .",
    "@xmath102 3 .",
    "@xmath103    after this , we perform :    1 .",
    "[ prisets ] generate all frequent sets according to @xmath104 , using the basic incremental approach .",
    "[ postsets ] retrieve all cached sets satisfying @xmath105 .",
    "[ subsets ] add all needed subsets that can serve as bodies or heads .",
    "[ postrules ] generate all rules satisfying @xmath100 .",
    "note that the query @xmath104 is much more specific than the original query @xmath101 .",
    "we thus obtain a speedup , because we have shown in section  [ queries ] that the speed of integrated querying is proportional to the restrictiveness of the query .",
    "the improvement just described incurs a new problem .",
    "the formula @xmath106 becomes longer with the session .",
    "when , given the next query @xmath101 , we mine for @xmath107 , and convert this to disjoint dnf which could explode .    to avoid this , consider @xmath106 in dnf : @xmath108 . instead of the full query @xmath107",
    ", we are going to use a query @xmath109 , where @xmath110 is obtained from @xmath106 by keeping only the least restrictive disjuncts @xmath111 ( their negation will thus be most restrictive ) .",
    "in this way @xmath109 is kept short .",
    "but how do we measure restrictiveness of a @xmath111 ?",
    "several heuristics come to mind . a simple",
    "one is to keep for each @xmath111 the number of cached sets that satisfy it .",
    "these numbers can be maintained incrementally .",
    "for each data set described in section  [ queries ] we experimented with a session of @xmath95 queries using the integrated querying approach , the post - processing approach and the incremental approach .",
    "again , the queries used for the sessions where randomly generated .",
    "figure  [ fig : incremental ] shows the evolution of the sessions in time .",
    "+     +    for all four sessions , the cut - off point where the integrated querying approach loses against the post - processing approach is the same for the incremental querying approach since not enough itemsets could be reused before that . except for the mushroom data set ,",
    "the incremental approach starts paying off after the twentieth query .",
    "nevertheless , the reuse of previous results does not improve the performance enough for the incremental approach . indeed",
    ", the incremental approach will always need some time to fetch all pre - generated itemsets and it will try to generate some more .",
    "however , as can be seen , the incremental approach shows a significant improvement on the integrated querying approach . only for the mushroom data set ,",
    "the cut - off point occurs at the fifth query , and almost all itemsets have been generated after the eighteenth query . as can be seen",
    ", the performance of the post - processing approach is very good compared to the other approaches .",
    "nevertheless , if we still lowered the support thresholds , the post - processing approach becomes unfeasible to begin with , due to an overload of frequent itemsets . in that case , the integrated and incremental approach are still feasible and perform very similar as in the presented experiments .",
    "this study revealed several insights into the association rule mining problem .",
    "first , due to recent advances on association rule mining algorithms , the performance has been significantly improved , such that the advantages of integrating constraints into the mining algorithm suddenly become less clear .",
    "indeed , we showed that as long mining without any constraints is feasible , that is , if the number of frequent itemsets does not reach a huge amount , the total time spent to query the frequent itemsets and confident association rules becomes less after a certain amount of queries , compared to integrated querying , in which every query is pushed into the mining algorithm .",
    "the incremental approach still improves the integrated approach by reusing as much previously generated results as possible . if the cut - off point would lie beyond the number of queries in which the user is interested , the incremental approach is obviously the best choice to use .    of course",
    ", if the user is interested is some frequent itemsets and association rules which have very low frequencies , and hence mining without any constraints becomes infeasible , the incremental approach can still be performed .    also note , that if a user is still interested in all frequent sets and association rules , but mining without constraints",
    "is infeasible , our queries can be used to divide the task over several runs , without spending much more time .",
    "for example , one can ask different queries of which the disjunction still gives all sets and rules .",
    "essentially , this technique forms the basis of the well known eclat  @xcite and fp - growth algorithms  @xcite .",
    "we wish to thank blue martini software for contributing the kdd cup 2000 data , the machine learning repository librarians catherine blake and chris mertz for providing access to the mushroom data , and tom brijs for providing the belgian retail market basket data .",
    "r.  agrawal , t.  imielinski , and a.n .",
    "mining association rules between sets of items in large databases . in p.",
    "buneman and s.  jajodia , editors , _ proceedings of the 1993 acm sigmod international conference on management of data _ , volume 22:2 of _ sigmod record _ , pages 207216 .",
    "acm press , 1993 .",
    "g.  grahne , l.v.s .",
    "lakshmanan , and x.  wang .",
    "efficient mining of constrained correlated sets . in _ proceedings of the 16th international conference on data engineering _ , pages 512521 .",
    "ieee computer society , 2000 .",
    "j.  han , y.  fu , k.  koperski , w.  wang , and o.  zaiane . : a data mining query language for relational databases .",
    "presented at sigmod96 workshop on research issues on data mining and knowledge discovery , 1996 .",
    "j.  han , y.  fu , w.  wang , et  al . : a system for mining knowledge in large relational databases . in e.",
    "simoudis , j.  han , and u.  fayyad , editors , _ proceedings of the second international conference on knowledge discovery and data mining _ , pages 250255 .",
    "aaai press , 1996 .",
    "j.  han , j.  pei , and y.  yin .",
    "mining frequent patterns without candidate generation . in w.  chen , j.f .",
    "naughton , and p.a .",
    "bernstein , editors , _ proceedings of the 2000 acm sigmod international conference on management of data _ , volume 29:2 of _ sigmod record _ , pages 112 .",
    "acm press , 2000 .",
    "b.  jeudy and j - f .",
    "boulicaut . using condensed representations for interactive association rule mining . in t.",
    "elomaa , h.  mannila , and h.  toivonen , editors , _ proceedings of the 6th european conference on principles of data mining and knowledge discovery _ , volume 2431 of _ lecture notes in computer science _ , pages 225236 .",
    "springer , 2002 .",
    "bayardo jr . , r.  agrawal , and d.  gunopulos .",
    "constraint - based rule mining on large , dense data sets . in _ proceedings of the 15th international conference on data engineering _ , pages 188197 .",
    "ieee computer society , 1999 .",
    "lakshmanan , r.t .",
    "ng , j.  han , and a.  pang .",
    "optimization of constrained frequent set queries with 2-variable constraints . in a.",
    "delis , c.  faloutsos , and s.  ghandeharizadeh , editors , _ proceedings of the 1999 acm sigmod international conference on management of data _ , volume 28:2 of _ sigmod record _ , pages 157168 .",
    "acm press , 1999 .",
    "r.  meo , g.  psaila , and s.  ceri . a new sql - like operator for mining association rules . in t.m .",
    "vijayaraman , a.p .",
    "buchmann , c.  mohan , and n.l .",
    "sarda , editors , _",
    "proceedings 22nd international conference on very large data bases _ , pages 122133 .",
    "morgan kaufmann , 1996 .",
    "biswadeep nag , prasad deshpande , and david  j. dewitt . using a knowledge cache for interactive discovery of association rules . in u.",
    "fayyad , s.  chaudhuri , and d.  madigan , editors , _ proceedings of the fifth acm sigkdd international conference on knowledge discovery and data mining _ ,",
    "pages 244253 .",
    "acm press , 1999 .",
    "ng , l.v.s .",
    "lakshmanan , j.  han , and a.  pang .",
    "exploratory mining and pruning optimizations of constrained association rules . in l.m .",
    "haas and a.  tiwary , editors , _ proceedings of the 1998 acm sigmod international conference on management of data _ , volume 27:2 of _ sigmod record _ , pages 1324 .",
    "acm press , 1998 .",
    "j.  pei and j.  han .",
    "can we push more constraints into frequent pattern mining ? in r.  ramakrishnan , s.  stolfo , r.  bayardo , and i.  parsa , editors , _ proceedings of the sixth acm sigkdd international conference on knowledge discovery and data mining _ , pages 350354 .",
    "acm press , 2000 .",
    "j.  pei , j.  han , and l.v.s .",
    "lakshmanan . mining frequent itemsets with convertible constraints . in _ proceedings of the 17th international conference on data engineering",
    ", pages 433442 .",
    "ieee computer society , 2001 .",
    "r.  srikant and r.  agrawal . mining generalized association rules . in u.",
    "dayal , p.m.d .",
    "gray , and s.  nishio , editors , _ proceedings 21th international conference on very large data bases _ , pages 407419 .",
    "morgan kaufmann , 1995 .",
    "h.  toivonen . sampling large databases for association rules . in t.",
    "m. vijayaraman , alejandro  p. buchmann , c.  mohan , and nandlal  l. sarda , editors , _",
    "proceedings 22th international conference on very large data bases _ , pages 134145 .",
    "kaufmann , 1996 ."
  ],
  "abstract_text": [
    "<S> we investigate ways to support interactive mining sessions , in the setting of association rule mining . in such sessions , users specify conditions ( queries ) on the associations to be generated . </S>",
    "<S> our approach is a combination of the integration of querying conditions inside the mining phase , and the incremental querying of already generated associations . </S>",
    "<S> we present several concrete algorithms and compare their performance . </S>"
  ]
}