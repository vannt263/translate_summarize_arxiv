{
  "article_text": [
    "the numerical solution of an initial value problem given as a system of ordinary differential equations ( odes ) is often required in engineering and applied sciences , and is less common , but not unusual in pure sciences . for precisely estimating asymptotic properties of the solutions ,",
    "the global truncation errors must be kept lower than the desired tolerance during a very large number of iterations .",
    "this is usually achieved by using an adaptive algorithm for the estimation of the largest step for integration yielding a local truncation error below the tolerance .",
    "nevertheless , such a correction usually leads to a drastic increase of the computational time .",
    "on the other hand , the use of spectral methods to solve parabolic and hyperbolic partial differential equations ( pde ) is becoming more and more popular .",
    "the spectral methods reduce these pdes to a set of odes @xcite .",
    "the higher the desired precision for the numerical solution , the larger the resulting system of odes .",
    "very large systems arise also in simulations of multi  agent systems @xcite .",
    "it can take hours to integrate this kind of systems over a few steps .",
    "taking all the above into account , it can be concluded that devising improved algorithms to compute the numerical solution of ode systems is still a very important task .    with the steady development of cheap multi - processors technology",
    ", it is reasonable to consider using parallel computing for speeding up real - time computations . particularly interesting",
    "are the current options for small - scale parallelism with a dozen or so relatively powerful processors .",
    "several methods have been deviced with that aim ( see for instance refs .",
    "@xcite ) , many warranting a substantial reduction of the runtime . for instance , the authors in ref.@xcite claim that the performance of their parallel method is comparable to that of the serial method developed by dormand and prince @xcite and , in terms of the required number of evaluations of the right  hand side of the odes , demonstrates a superior behaviour .",
    "the aim of this work was twofold .",
    "firstly , we wished to test these claims by solving odes systems with different degree of complexity over different ranges of time ; secondly , we proposed and tested a new method which focuses on taking full advantage of parallel computing for estimating the optimal stepsize .",
    "all our codes were written in c and for parallalel programing we used the openmp resources .",
    "the programs were tested in a server supermicro a+ 1022gg - tf with 24 cpus and 32 gigabytes of operational memory .    in the next section ,",
    "we describe the numerical methods we used for our tests , the standard fourth order runge - kutta , a version of the dormand - prince method and the parallel iterated runge - kutta method proposed in ref.@xcite .",
    "these last two methods are widely regarded to be amongst the best options for serial and parallel numerical solution of odes .",
    "it is also briefly described how the optimal stepsize is estimated in each case . in section [ sec : systems ] the initial value problems used for testing the methods are described .",
    "next , in section [ sec : tresults ] we report the results of our comparison of the performance of these methods . in section [ sec : aspa ] we introduce an adaptive stepsize parallel algorithm coupled to the dormand - prince integrator , and report the results of the corresponding tests .",
    "finally , in section [ sec : concl ] we present our conclusions .",
    "let the initial value problem be specified as follows , @xmath0 here @xmath1 is the vector solution at time @xmath2 , dot stands for the derivative with respect to time and the right  hand side of the equation defines a vector field .",
    "our aim is to compare the performance of several methods for approximating the solution of this problem .",
    "all of them are members of the family of explicit runge  kutta methods and aproximate @xmath1 at @xmath3 as @xmath4 where @xmath5 and @xmath6 is known as the number of stages .",
    "therefore , a method with @xmath6 stages usually requires , at least , @xmath6 evaluations of the right  hand side of the system at each iteration",
    ".    a runge  kutta method can be especified by a butcher tableau like in table [ table : butcher ] .",
    "@xmath7    the order of a method is @xmath8 if the local truncation error is on the order of @xmath9 , while the total accumulated error is of order @xmath10 .",
    "the higher the order of the method , the lower the error of the approximation , nevertheless , constructing higher order runge ",
    "kutta formulas is not an easy task .",
    "to avoid increasing @xmath6 ( and , therefore , the number of evaluations of @xmath11 ) a common alternative is to develope methods with adaptive stepsize .    for any numerical method , an estimate for the local truncation error while integrating from @xmath12 to @xmath13",
    "is given by @xmath14 where @xmath15 stands for a given norm , and @xmath16 and @xmath17 are the results of different numerical approximations of @xmath18 .",
    "the stepsize yielding a local error below the tolerance ( @xmath19 ) is then given by @xmath20      the method given in table [ table : rk4 ] is the classical member of the family of runge  kutta methods .",
    "c|c c c c 0 & + 1/2 & 1/2 + 1/2 & 0 & 1/2 + 1 & 0 & 0 & 1 +    ' '' ''    & 1/6 & 2/6 & 2/6 & 1/6    we used rk4 without a stepsize control mechanism . hence , in all our tests we choose the stepsize in such a way that the global error had the same order of those obtained by the methods with adaptive stepsize .",
    "in this method @xcite , the last stage is evaluated at the same point as the first stage of the next step ( this is the so - called fsal property ) , so that the number of evaluations of @xmath11 is one less than the number of stages .",
    "here there is no easy way to present the butcher coefficients in a tableau , because it involves dealing with irrational quantities @xcite .",
    "the coefficients we use can be found in the code by e. hairer and g. wanner available in the site @xcite .",
    "the approximations @xmath16 and @xmath17 in equation ( [ eq : error ] ) correspond here to the results obtained using different orders , and the @xmath21 are determined by minimizing the error of the higher order result . as a matter of fact , in the version we use two comparisons are made , one between 8th and 5th orders , and the second one between 8th and 3th orders .",
    "then , the error is estimated using @xcite : @xmath22      let us consider a s - stage runge  kutta method given by the coefficients @xmath23 and let @xmath24 be defined as :    k_i^^(0)&=f(x__0,y__0 ) + k_i^^()&=f(x__0+c_ih , y__0 + h_j=1^s a_ijk_j^^(-1 ) )   = 1,  ,m[pirk ] + y_1&=y_0+h_i=1^s b_ik_i^^(m )    here @xmath25 is the number of iterations used to estimate @xmath26 .",
    "as it is shown in @xcite , provided that @xmath6 processors are available , this scheme represents an explicit runge  kutta method .",
    "furthermore , since each @xmath27 can be computed in parallel , we have the following theorem ,    let @xmath28 define an s - stage runge  kutta method of order @xmath29 .",
    "then the method defined by represents an @xmath30 explicit runge  kutta method of order @xmath8 , where @xmath31 [ thm : thm ]    one of the advantages of this method is that if we set @xmath32 , then the order of the method is equal to the number of stages , which results in less right  hand side evaluations ( sequentially ) . in general the number of stages of explicit runge  kutta methods is greater than the order of the method , therefore if an explicit method is used the required number of processors is greater as well .    along the lines in ref.@xcite and with the butcher coefficients in table [ table : rk10 ] in the appendix",
    ", we implemented a parallel iterated runge ",
    "kutta method of order @xmath33 .",
    "here , @xmath34 and @xmath35 in equation ( [ eq : error ] ) correspond to the results obtained using different number of iterations for approximating @xmath26 , @xmath36 and @xmath37",
    "next , we list and briefly describe the systems of odes that we have used to test the above numerical methods .      as a first initial value problem we chose : @xmath38    since this system is readily integrable , we used it to assess the quality of the numerical results by comparing with the analytical ones .",
    "this is a hamiltonian system which describes the nonlinear dynamics of a star around a galactic center when the motion is constrained to a plane @xcite : @xmath39 since the hamiltonian @xmath40 is a constant of motion , it can be used to assess the precision of the numerical solution .",
    "we choose initial conditions such that @xmath41 , yielding a chaotic solution .      to force the integrators to work a little bit harder we constructed a new system by replicating the hnon - heiles system @xmath42 times , resulting in a nonlinear system with @xmath43 equations : @xmath44 with @xmath45 .",
    "we also tested the methods by solving the system obtained from the einstein field equations for the gravitational collapse of a scalar field in anti de sitter spacetime @xcite . using the galerkin method @xcite the 10 coupled hyperbolic - elliptic nonlinear partial differential equations",
    "were converted to a set of @xmath46 nonlinear ordinary differential equations .",
    "the corresponding solutions were shown to be chaotic too @xcite .",
    "finally , the last system we used was obtained by reducing the previous one to ten equations .",
    "to test the methods we ask for the numerical solution of the corresponding problem starting from @xmath47 and up to a given @xmath48 , such that the straigthforward integration with step @xmath49 yields a result with an error above the desired tolerance .",
    "this implies that , typically , a number of intermediate integrations will be required .    in table",
    "[ table : smalltime ] is shown the order of the runtime in seconds taken for solving the ho and hh problems in the time interval @xmath50 using rk4 , dop853 and pirk10 . in the methods with an adaptive stepsize algorithm we have used a tolerance of @xmath51 , what corresponded to using a step @xmath52 in the rk4 .",
    "@xmath53\\hline      dop853{\\rule{0pt}{2.6ex } } & 10^{-2 } & 10^{-2}\\\\[.2 cm ]      pirk10 & 10^{-1 }   & 10^{-1}\\\\[.2 cm ]      rk4 & 10^{-1 }   & 10^{-1}\\\\[.2 cm ]      \\hline          \\end{array}\\ ] ]    in all the following tests the prik10 used its optimal number of 5 processors .",
    "as we can see very similar results were obtained with the three methods , and even though dop853 seems to be faster , the differences are very small .",
    "nevertheless , the serial methods can be considered to be better than pirk10 because they are easier to implement and require significantly less computational resources for execution .",
    "searching for a bigger runtime difference we tested the hh100 problem keeping the same tolerance for dop853 and pirk10 , but now in the time interval @xmath54 .",
    "this implied to use @xmath55 in the rk4 . in this case",
    "the rk4 and pirk10 recorded a runtime of @xmath56 seconds and @xmath57 seconds respectively , both greater than the @xmath58 seconds obtained with dop853 .    at this point",
    "we recall that , according with theorem [ thm : thm ] , by using 5 processors the prik10 method at each timestep does 9 evaluations of the right - hand - side of the corresponding problem .",
    "this is to be contrasted with the , at least , 11 evaluations done at each timestep by the dop853 .",
    "therefore , since according with the above results the serial method outperforms the parallel one , we conjecture that this due to a parallel overhead problem , i.e. , the amount of time required to coordinate parallel tasks is larger than the time required for evaluating the system right  hand side .    to verify this conjecture we tested the methods with the huge system of problem gc40 .",
    "in this case we integrated the system over the small time interval @xmath59 , with a tolerance of @xmath60 , what corresponded to using a step @xmath61 in the rk4 .",
    "the results are presented in table [ table : colapso1 ] .",
    "c c    ' '' ''    & @xmath62 + dop853    ' '' ''    & @xmath63 days + pirk10 & @xmath64 hrs + rk4 & @xmath65 days +    we can observe that the performance of pirk10 was way better than dop853 and rk4 , being dop853 unable to solve the system after six days .",
    "since parallelizing the integrator does not seems to be helpful , we opted for a different approach , that is , to parallelize the choice of an optimal integration step .",
    "let us consider an embedded runge  kutta method , which allows us to estimate the local error @xmath66 .",
    "given an initial step @xmath67 and a tolerance @xmath19 , for integrating from @xmath47 to @xmath48 with @xmath68 processors , the next step is determined as follows :    1 .",
    "each processor @xmath69 , with @xmath70 , integrates the system from @xmath12 to @xmath71 and estimates the local error @xmath72 .",
    "2 .   @xmath73 .",
    "3 .   @xmath74 .",
    "4 .   @xmath75 .",
    "all the above steps are repeated while @xmath76 .",
    "figure [ fig : aspa ] is an illustration of how the stepsize could change with respect to @xmath67 , depending on the number @xmath25 of processors yielding an acceptable result .",
    "the interval of @xmath68 black vertical bars is the amount of time probed by the integration using the initial step @xmath67 . in our computations @xmath67",
    "is assumed to be the total length of integration over the number of processors .",
    "a green horizontal line below a processor label indicates a successful integration , otherwise , a red line is used .      in a given iteration",
    "we define success as obtaining an integration result with a local error below the user defined tolerance .",
    "the aim is to maximize the probability of success in each iteration , i.e. , to determine the step @xmath77 such that it is obtained the biggest possible number of successful processors @xmath25 amongst the total number of available cpus ( @xmath68 ) .",
    "we define @xmath77 as a function of @xmath25 , keeping @xmath68 constant .",
    "so , if with a given integration step less than half of the processors are successful , then the next integration step needs to be smaller ( @xmath78 ) .",
    "otherwise , we increase the integration step .",
    "this way , each integration becomes more efficient , both in amount of time and precision .",
    "this idea is summarized with the following expresion : @xmath79h_n & \\text { if } m = n_{cpu}/2 + k,\\\\    \\left[(1-\\frac{2}{n_{cpu } } k)+\\epsilon\\right]h_n & \\text { if } m = n_{cpu}/2 - k,\\\\   \\end{cases}\\\\\\end{aligned}\\ ] ] for some integer @xmath80 $ ] .",
    "we need @xmath66 to keep @xmath81 finite , even when @xmath82 and it has to be less than one for the step to always decrease in this particular case .",
    "a reasonable proposal is then , to carry the integration in half the interval when @xmath82 , i.e. , @xmath83 on the other hand , if @xmath25 is large enough , the integration step will nearly doubles .",
    "if , for instance , this happens sequentially , for typical initial value problems there is a high probability that the next @xmath25 will be very small , making a poor use of the available cpus . to avoid this",
    ", we finally propose the following recurrence : @xmath84 h_n \\ , .",
    "\\label{eq : hn}\\ ] ]",
    "since @xmath25 is a function of @xmath85 , this is a first order nonlinear map .",
    "it yields , @xmath86 if @xmath87 and @xmath88 if @xmath89 moreover , @xmath82 implies @xmath90 , and when @xmath91 , then @xmath92 . in consequence",
    ", this expression has the desired properties ; a large integration step will ultimately lead to a low @xmath25 that , in turn , will decrease the stepsize and , then , increase @xmath25 .",
    "this way , we expect @xmath25 to converge to the optimal value @xmath93 .    nevertheless , it is not desirable that the stepsize occurs to be insensitive to the given integration interval .",
    "thus , the map @xmath94 was also designed to not have fixed points .",
    "note that requiring @xmath95 , implies @xmath96 , i.e. , there are not fixed points when using less than 3 cpus . for the remaining cases ,",
    "@xmath97 give us the condition for the map to have fixed points : @xmath98 let us prove that , whereas @xmath99 , the righ hand side of the above expresion is never an integer .",
    "suppose there is a @xmath100 such that @xmath101 and @xmath102 .",
    "stands for @xmath103 divides @xmath11 . ] since @xmath104 then ,",
    "so , by assumption @xmath106 $ ] , leading to @xmath107 .",
    "therefore , considering that @xmath105 , @xmath107 and @xmath108 , we conclude that @xmath109 . in turn",
    "this implies @xmath110 , and , this way , @xmath111 is not an integer .",
    "thus , @xmath112 is an integer if and only if , @xmath113 .",
    "but , recalling that @xmath96 , then @xmath114 and we get a contradiction because by definition @xmath115 . therefore @xmath116 has no fixed points .",
    "finally , note that while increasing the value of @xmath68 , @xmath67 becomes smaller , implying a big number of integration steps in the beginning of the process .",
    "however at a given time , since there are not fixed points , @xmath77 should show a bounded oscillatory behaviour around the optimal stepsize .",
    "it would imply that the proposed recurrence has an attractor , i.e. , asymptotically , the process of integration will settled down around an optimal stepsize independently of its initial value .",
    "indeed , this can be seen in figures [ fig : hnbeh ] where we show some numerical realizations of @xmath117 for different initial value problems and number of cpus .",
    "we tested the above described algorithm by coupling it to a version of the serial dop853 .",
    "we then compared the performances of the serial dop853 and the dop853 with aspa ( dop853-aspa ) . with this aim we calculated the difference of the number of stepsize corrections and the difference of runtime required to reach @xmath118 as function of the tolerance for a fixed number @xmath68 of processors .",
    "we also calculated the same differences but as function of the number of processors with the tolerance fixed to @xmath51 .",
    "the actual values of the runtime for each case are given in correponding tables in the appendix [ sec : runtimes ] . in figures [ fig : aspahh ] the results for the hh problem are presented .    here @xmath119 and @xmath120 . in the two top panels we can see that , even if the dop853 requires more stepsize corrections to reach the required tolerance",
    ", it does it in relatively less runtime . from the two bottom panels we draw the unexpected conclusion that the runtimes are comparable only when the number of stepsize corrections required by dop853-aspa is significantly more than that required by dop853 .",
    "this happens when using five or less processors .",
    "moreover , notice that in the bottom panel the differences are all calculated with respect of the fixed number obtained with dop853 ( where @xmath121 ) .",
    "it means that , as expected , increasing @xmath68 , the number of stepsize corrections in dop853-aspa decreases , nevertheless , the corresponding runtime increases .",
    "all these observations hint that , when more processors are used , at each iteration the parallel overhead is more important than the time required for integration .    to determine whether this is the case , we tested the hh100 problem .",
    "the results are presented in figures [ fig : aspahh100 ] .    here @xmath119 and @xmath120 . as in the case of hh ,",
    "here dop853 requires more stepsize corrections than dop853-aspa , nevertheless , for low tolerances the parallel algorithm performs slightly better than the serial .",
    "this could be due to the fact that for the hh100 problem the amount of time used for the evaluation of the rhs is comparable with the parallel overhead and that , for tolerances greater than @xmath122 , @xmath120 processors are good enough to probe the whole time interval up to @xmath119 in very few stages .",
    "trying further to make the number of evaluations of the rhs to have a larger weight in the runtime , we tested the problem of the gravitational collapse , but the reduced version gc10 , because the dop853 was able to integrate it in a reasonable runtime .",
    "in figures [ fig : aspagc10 ] we present the results of the comparison .    for these calculations we used @xmath123 and @xmath124 .",
    "now it is clear that the parallel algorithm typically performs better than the serial .",
    "from the top two panels we observe that increasing the tolerance induces a steadily increase of the difference in required stepsize corrections and this straightforwardly leads to a larger difference in runtime . the results in the bottom panels show that the dop853-aspa is efficient when @xmath125 . recalling that in the bottom panel the differences are all calculated with respect of the fixed number obtained with dop853",
    ", we see now that increasing @xmath68 leads to less stepsize corrections required by the dop853-aspa , but now this also corresponds to less runtime .",
    "all the above suggests that , indeed , for the gc10 system , the parallel overhead problem is solved .    to end the comparisons , note in table [ table : cpuvstime ]",
    "that for this last system , dop853-aspa with 5 processors lasted a little bit more than 7 minutes , while we verified that pirk10 took about an hour to solve the same problem .",
    "on the other hand , we also checked that for gc40 , dop853-aspa took about an hour to reach @xmath126 with a tolerance of @xmath60 , while as mentioned in section [ sec : tresults ] pirk10 needed about six times more ( see table [ table : colapso1 ] ) .",
    "we tested a parallel iterated runge ",
    "kutta method of order 10 ( pirk10 ) and an adaptive stepsize parallel algorithm ( aspa ) , introduced in this paper , which was coupled to a dormand  prince method of order 8 ( dop853-aspa ) .",
    "the results presented in this paper show that when the initial value problem to solve has a simple to evaluate right  hand side ( as is the case in more common dynamical systems ) , even in the best case scenarios for the parallel methods , their performances were only comparable to the corresponding performance of a serial dormand - prince method of order 8 ( dop853 ) .",
    "therefore , taking into account code and algorithmic efficiencies , parallel integration seems to not be a good practice .",
    "this negative result seems to be due to a parallel overhead problem , i.e. , the amount of time required to coordinate parallel tasks is larger than the time required for evaluating the system right  hand side .",
    "we verified that for very complex initial value problems or low tolerances the parallel methods can outperform dop853 . for instance , such systems arise while using galerkin projection to solve systems of partial differential equations or when simulating multi",
    " agent systems . in these cases",
    ", it seems to be more efficient to parallelize the search for an optimal stepsize for integration than to parallelize the integration scheme .",
    "indeed , our method , dop853-aspa , consistently outperformed pirk10 by almost an order of runtime . moreover , even in some cases where dop853 did a better job than pirk10 , our method was able to solve the corresponding initial value problem in less time than both these methods .",
    "a nice feature of aspa is that it does not relies on a given core integrator , it can be coupled to any method with a scheme to estimate the local integration error",
    ". it can even be another parallel method more efficient than the one tested here .",
    "this research was supported by the sistema nacional de investigadores ( mxico ) .",
    "the work of cat - e was also partially funded by fraba - ucol-14 - 2013 ( mxico ) .",
    "butcher tableau for an implicit runge ",
    "kutta method of order @xmath33 ref .",
    "@xcite .",
    "@xmath127           \\frac{1}{2}-\\omega_{2}^{\\prime}&\\omega_{1}-\\omega_{3}^{\\prime}+   \\omega_{4}&\\omega_{1}^{\\prime}&\\frac{32}{225}-\\omega_{5}^{\\prime}&\\omega_{1}^ { \\prime}-\\omega_{6}^{\\prime } & \\omega_{1}-\\omega_{3}^{\\prime}-\\omega_{4}\\\\[.25   cm ]            \\frac{1}{2}&\\omega_{1}+\\omega_{7}&\\omega_{1}^{\\prime}+   \\omega_{7}^{\\prime}&\\frac{32}{225}&\\omega_{1}^{\\prime}-\\omega_{7}^{\\prime } &   \\omega_{1}-\\omega_{7}\\\\[.25 cm ]            \\frac{1}{2}+\\omega_{2}^{\\prime}&\\omega_{1}+\\omega_{3}^{\\prime}+           \\omega_{4}&\\omega_{1}^{\\prime}+\\omega_{6}^{\\prime } &   \\frac{32}{225}+\\omega_{5}^{\\prime } & \\omega_{1}^{\\prime } &   \\omega_{1}+\\omega_{^3}^{\\prime}-\\omega_{4}\\\\[.25 cm ]           \\frac{1}{2}+\\omega_{2}&\\omega_{1}+\\omega_{6}&\\omega'_{1}+   \\omega_{3}+\\omega_{4}'&\\frac{32}{225}+\\omega_{5}&\\omega'_{1}+\\omega_{3}-\\omega ' _ { 4 } & \\omega_{1}\\\\[.25 cm ]           \\hline\\\\[-.25 cm ]            & 2\\omega_{1}&2\\omega_{1}'&\\frac{64}{225}&2\\omega_{1 } ' & 2\\omega_{1}\\\\            \\end{array}\\ ] ]    where the @xmath128 are given by , @xmath129",
    ".@xmath130 . hh system using 10 processor and final time 5000 . [ cols=\"^,^,^,^,^ \" , ]                dormand , j.r . ;",
    "prince , p.j . ,  a family of embedded runge  kutta formulae  , journal of computational and applied mathematics , volume 6 , issue 1 , march 1980 , pages 1926 http://dx.doi.org/10.1016/0771-050x(80)90013-3        de oliveira , h.  p. ; pando zayas , l.  a. ; terrero - escalante , c.  a. , `` turbulence and chaos in anti - de - sitter gravity , '' int .",
    "j.  mod .",
    "d * 21 * , 1242013 ( 2012 ) [ arxiv:1205.3232 [ hep - th ] ] .",
    "butcher , j.c . ,",
    " implicit runge  kutta processes \" , american mathematical society , january 1964 , pages 50 - 64 ."
  ],
  "abstract_text": [
    "<S> in this paper the performance of a parallel iterated runge - kutta method is compared versus those of the serial fouth order runge - kutta and dormand - prince methods . </S>",
    "<S> it was found that , typically , the runtime for the parallel method is comparable to that of the serial versions , thought it uses considerably more computational resources . a new algorithm is proposed where full parallelization is used to estimate the best stepsize for integration . </S>",
    "<S> it is shown that this new method outperforms the other , notably , in the integration of very large systems . </S>"
  ]
}