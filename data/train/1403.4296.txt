{
  "article_text": [
    "molecular technologies have reached a state where it is possible to quickly and cheaply measure the status of millions of nucleotides , mrnas , genes , proteins , or metabolites simultaneously and repeatedly over time .",
    "the availability of these massive data ideally allows for complex and detailed modeling of the underlying biological system but they also pose a serious potential multiple testing problem because the number of covariates are typically orders of magnitude larger than the number of observations .",
    "recently , investigators are starting to combine several of these high - dimensional dataset which has increased the demand for analysis methods that accommodates these vast data @xcite .    as data sets increase in size , methods that promote sparse results have become more important and popular . @xcite and @xcite provide an overview of the development of these methods of which the lasso ( least absolute shrinkage and selection operator ) is the most widely used @xcite .",
    "while all these estimators enforce sparsity ( since only a fraction of the variables are believed to be influential ) and perform variable selection , they lack the inference from traditional statistics , such as @xmath0-values and confidence intervals . as a consequence , these models will always identify a set of the most important predictors even if none or only some of them are significant .    to address this issue several recent papers propose techniques that help asses the importance of the selected predictors .",
    "@xcite developed a method that use a covariance test statistic approach to compute @xmath0-values for parameters obtained from a lasso regression . with this method @xmath0-values",
    "can be calculated for all predictors in a data set if there are more samples than predictors , @xmath1 . in the case of @xmath2",
    "it is not possible to use the covariance test without specifying an estimate of @xmath3 , the error standard deviation .",
    "@xcite presents another approach where the data is split into two groups .",
    "the lasso is applied to one group , after which the variables selected by the lasso are used as predictors to obtain @xmath0-values from an ordinary least squares ( ols ) regression on the other group . @xcite",
    "suggest to first use the lasso to select a set of relevant variables which are then fitted to a non - penalized model where from which the @xmath0-values can be calculated .    in applied sciences we find",
    "often that variable selection is used for screening to determine which predictors such as genes to focus on , in further ( possibly costly ) analyses .",
    "a useful tool in this setting would be a model that allowed us to perform variable selection and assign a @xmath0-value to evaluate the evidence of the most important predictors found from the variable selection procedure .",
    "we wish to propose an approach for variable selection and inference for the lasso that works directly when @xmath2 rather than choosing variables with lasso and transferring these to an ols domain for inference .",
    "our approach uses randomization / resampling to infer the significance of the @xmath4th chosen predictors from a lasso model .",
    "thus , our focus is on using the lasso to not only generate new ( biological ) hypotheses but also to evaluate the evidence for those hypotheses .",
    "the paper is structured as follows : in the next section we present a method to both identify features and for computing @xmath0-values of those features obtained from a lasso regression model .",
    "potential problems and possible extensions are also discussed .",
    "the simulation section presents results from a series of simulation studies that show the power of the proposed method under various conditions and for situations both with and without correlated predictors .",
    "in addition we show how well the method identifies the most important of the predictors and apply the proposed method to a dataset on prostate cancer .",
    "finally , the proposed method is discussed . note",
    "that while we use the lasso in the following the proposed procedure can essentially be applied in combination with any sparse feature selection approach ( e.g. , elastic net , ridge regression ) .",
    "we consider the situation where we have a quantitative response vector , @xmath5 and a set of @xmath0 quantitative predictors @xmath6 for each observation @xmath7",
    ". the number of predictors can be much larger than the number of observations , @xmath8 and we are interesting in identifying predictors that are associated with the response vector @xmath9 .",
    "suppose that @xmath10 where the noise is assumed to be gaussian , @xmath11 .",
    "the lasso is a penalized version of the multiple regression model where sparsity of the parameter vector @xmath12 is achieved by adding a penalty term to solve @xmath13 where @xmath14 is the regularization parameter @xcite .",
    "@xmath15 controls the amount of shrinkage of @xmath16 such that @xmath17 corresponds to setting all parameters to 0 , while @xmath18 corresponds to an ordinary least squares multiple regression model with no restrictions on the parameter vector .",
    "the choice of @xmath15 will be discussed below .",
    "the lasso can be used for variable selection for high - dimensional data and produces a list of selected non - zero predictor variables .",
    "@xcite show that while the lasso may not recover the full sparsity pattern when @xmath8 and when the irrepresentable condition is not fulfilled ( e.g. , if highly correlated predictor variables are present ) it still distinguishes between important predictors ( i.e. , those with sufficiently large coefficients ) and those which are zero with high probability under relaxed conditions .    in many high - dimensional molecular genetic datasets",
    "it is reasonable to assume that there is just a small number of genes that influence the outcome while the vast majority of genes are irrelevant for the outcome .",
    "this common setup is also relevant in other situations and it forms the basis of the following . when the relative number of truly non - zero predictors is small then the results from @xcite also show that the lasso selects the non - zero parameters as well as `` not too many additional zero entries of @xmath19 '' . here",
    ", we seek to identify which of the selected predictors that we believe to be non - zero .",
    "the predictors found by lasso are generally ranked by their importance , so that a high value for @xmath15 will include the most important predictors , decreasing @xmath15 will include other less important predictors @xcite .",
    "we wish to determine which of the features identified by the lasso that are indeed true positives .",
    "let @xmath20 be the ordered absolute coefficients of the @xmath4 non - zero predictors that are selected by the lasso . to ensure that the effects of the predictors are comparable across variables",
    "we assume that each predictor has been standardized by dividing by their standard deviation .",
    "this is not a restriction when determining the importance of selected variables as the internal structure of the predictors ( and their pairwise correlations ) is preserved @xcite .",
    "essentially , this approach mimics the adaptive lasso of @xcite and ensures that the coefficients are equally penalized by the @xmath21 penalty in .",
    "the variables can then be ordered according to their absolute size such that @xmath22 .",
    "we are interested in testing hypotheses of the form @xmath23 where @xmath24 is the @xmath4th most important feature .",
    "note that contrary to classical hypothesis testing we are not testing hypotheses about _ specific _ predictors but we are testing whether the @xmath4th identified parameter is equal to zero .    as a test statistic for the @xmath4th selected parameter we use the corresponding coefficient obtained from the lasso , @xmath25 . in order to derive",
    "the distribution of the @xmath4th identified predictor we permute response vector @xmath9 to remove any associations to the set of predictors while retaining the individual structure and correlations among the set of predictors @xcite .",
    "this randomization is undertaken a large number of times and for each randomization we run the lasso on the permuted data to obtain a distribution of coefficients for the @xmath4th identified predictor . note that this approach might result in different variables being selected for each permutation and this matches the null hypothesis where the focus is on finding out whether the @xmath4th identified predictor is significant .    the algorithm for testing the null hypothesis using a randomization approach can be summarized as follows :    1 .",
    "start by scaling the predictors so their effects are comparable , i.e. , @xmath26 where @xmath27 is the @xmath28th column of the design matrix @xmath29 .",
    "2 .   fit a lasso regression model to the original data and extract the coefficients for the selected predictors , @xmath30 .",
    "3 .   let @xmath31 be the ( large ) number of randomizations to perform to determine the distribution of the coefficients under the null hypothesis",
    ". for each @xmath32 do the following : 1 .",
    "permute the response vector , @xmath33 , and fit a new lasso model using @xmath33 as the response .",
    "2 .   extract the coefficients for the predictors for this model , @xmath34 .",
    "4 .   for each feature , @xmath4 , we can compare the size of the coefficient identified in the original dataset , @xmath35 to the distribution of the coefficients found for the @xmath4th feature for the permuted data , @xmath36 .",
    "the @xmath0-value for the @xmath4th feature is then the fraction of coeffients under the null that are larger than or equal to @xmath35 : @xmath37    note that by construction the @xmath0-value defined by controls the error rate since we are simulating the distribution on the null hypothesis .      in applications such as genetics ,",
    "the number of predictors , @xmath0 , is often several orders of magnitude larger than @xmath38 and we are typically more concerned with identifying _ some _ associations rather than finding _ all _ associations since any relationships found will often be the focus of subsequent specific experiments to verify the genetic function .",
    "thus , our interest is on hypothesis generation rather than testing a specific hypothesis : is it possible to identity one or a few potentially interesting or relevant predictors , in a scenario with little or no knowledge of the individual predictors .",
    "it is well - known that the lasso has a weakness when dealing with correlated predictors : it will only select one of a group of collinear predictors to represent their effect on the response @xcite and the coefficient for the selected variable will have high variance @xcite .",
    "collinearity poses a problem if we are intersted in making inferences about specific predictors because the corresponding standard error becomes unstable .",
    "however , in the present setup collinearity among predictors means that while we may not be able to identify all predictors that are associated with the outcome , the size of the coefficients for the variable that are selected essentially represent the maximum effect size of the cluster of correlated predictors .    in particular , if @xmath8 then we may be interested in just finding _ one _ association between a predictor and the response in which case our primary focus is on the null hypothesis @xmath39      the lasso involves a penalty parameter , @xmath15 , that determines the amount of shrinkage that is applied to the parameters . in our approach , we recommend using the same procedure for estimating @xmath15 , that the reseacher would typically use together with a lasso regularization model .",
    "typically , @xmath15 is determined through some form of @xmath40-fold cross - validation where @xmath15 is chosen such that it minimizes the cross - validation error .",
    "@xmath41    where @xmath42 is the set of covariates for individual @xmath43 , @xmath44 is the expected value from the lasso regression based on the folds / parts of the data that does not contain observation @xmath43 .",
    "there are two considerations to consider here : first , we know that we are in a situation where the majority of the variables are unrelated to the outcome",
    ". thus we want to ensure , that the majority ( or possibly all ) of the variables are shrunk to zero .",
    "this is particularly true under the null hypothesis where there is no association between the variables and the outcome due to randomization .",
    "secondly , we want to ensure that when there is indeed an association between some of the variables and the outcome then we want @xmath15 to be small enough that the effect of the associated variables are not shrunk too much .    in the following",
    "we have worked with a 10-fold cross - validation using the mean absolute error as loss function since our analyses have shown that this gives good stable results . using the mean absolute error instead of the traditional mean",
    "squared error makes the cross - validations less sensitive to spurious extreme fits which is often the case under the null , where none of the variables are associated with the outcome .",
    "however , other approaches would be directly applicable as well at the researchers discretion .",
    "one possible approach here , that would substantially reduce the computation time would be to use the same @xmath15 for the randomization samples as was used in the original dataset .",
    "that ensures that we only have to do cross - validation once ( for the original data ) and not for each randomization .",
    "also , the estimate of @xmath15 under the null hypohtesis ( where the randomization approach assumes that _ none _ of the predictors are associated with the outcome ) can be rather unstable since there really is no obvious minimum for the cross validation procedure to reach .      in some situations it is of interest to use the lasso to select predictors from among a subset of the predictors . for example",
    ", some predictors might be related to the design of the experiment ( and we wish to force them to be part of the modeling ) while the remaining predictors contains the variables we wish to identify / select from because we have no prior knowledge about them .",
    "thus , we consider two sets of predictors , @xmath45 and @xmath46 such that @xmath47 $ ] where @xmath45 are the predictors that we wish to force into the model while @xmath46 are the predictors from which we wish to make a selection .",
    "if we apply the lasso to @xmath29 we will select variables from the full set of predictors which means that some of the potential confounders that we wish to include are disregarded in the modeling .",
    "there are two obvious alternatives to just using the classical lasso in this situation .",
    "one is to make a two - step procedure where we first fit a model using only the predictors in @xmath45 , i.e. , @xmath48 from we extract the residuals , @xmath49 , and use the residuals as outcomes in combination with the procedure described above .",
    "this approach ensures that the ( marginal ) effects of the predictors in @xmath45 are removed and and features that are subsequently found are conditional on these marginal effects .",
    "the disadvantage is that the effects of the predictors in @xmath45 and @xmath46 are assumed to be working independently of each other , which may not be realistic .",
    "the adaptive ( or weighted ) lasso of @xcite uses a weighted penalty resembling @xmath50 where @xmath51 is a vector of non - negative weights for each predictor . typically , the individual weights are set to @xmath52 , where @xmath53 is the univariate regression coefficient estimate of parameter @xmath28 and @xmath54 .",
    "if a weight is set to zero then the corresponding predictor will not be penalized .",
    "thus if the weights of the parameters in @xmath45 are set to zero then the corresponding variables enter the model without being shrunk towards zero .",
    "the @xmath0-value obtained from controls the error rate by construction as mentioned above . however , the error rate control is on a per - hypothesis basis so if we are only aiming on making inference for the `` best '' selected predictor  corresponding to the hypothesis  then the proposed procedure yield the correct error level .",
    "however , in some situations it is of interest to extract inference on as many predictors as possible and simultaneously control the family - wise error rate . we can still use to obtain individual @xmath0-values for the first feature , the second feature , etc . , and",
    "holm s step - down procedure can control the family - wise error rate in those situations @xcite . in its original form ,",
    "holm s procedure orders the @xmath0-values and makes sequential comparisons from the smallest @xmath0-value to the largest until the first occurrence of a null hypothesis that fails to be rejected .",
    "all other subsequent hypotheses are also not rejected .    here",
    "we suggest a slightly different version of holm s procedure .",
    "the @xmath0-value of the @xmath4th selected feature is compared against @xmath55 where @xmath56 is the overall significance level desired and @xmath57 is the number of hypotheses tested .",
    "the significance level for each feature is identical to the level used by holm s procedure but unlike that we do not order our @xmath0-values according to size but test them in the order that they are selected by our procedure .",
    "this ensures that we still control the family - wise error rate ( by the exact same arguments as used by @xcite ) since it becomes more difficult to reject hypotheses by _ not _ ordering them according to the size of the @xmath0-value",
    ". also , since we keep the order of the features it prevents us from ending with a result where , say , the first and third selected features are significant but the second selected feature is not .",
    "to examine the validity of our procedure , we performed simulation studies to assess the performance of the sensitivity and specificity . for each setting",
    ", we simulated 100 data sets with only @xmath58 observations generated under the linear model , @xmath59 where @xmath60 and where @xmath61 . the unit variance is not really a restriction in this case as we start the analysis algorithm by standardizing each predictor ( as outlined in the methods section above ) .",
    "the number of predictors were varied from 1000 to 250000 to represent various datasets .",
    "initially we assume that there is just a single predictor that is associated with the response and we vary the corresponding regression coefficient , @xmath62 , from 0 to 1.5 .",
    "this corresponds to a partial correlation coefficient between the predictor and the outcome ranging from 0 to 0.83 . with these data we follow the procedure described above where each combination of predictors , @xmath0 , and associated coefficient , @xmath62 , is run 100 times and for each combination we use 100 permutations to determine the power of the feature identified as the most important corresponding to the hypothesis @xmath63 . in all computations a significance level of @xmath64 was used . in order to see how collinearity influences the power we run simulations with the same association between a single predictor and the outcome but we let that predictor be part of a cluster of ten correlated predictors such that @xmath65 . here , @xmath66 or @xmath67 to represent moderately and highly correlated predictors .",
    "we first focus on two primary abilities of the model : the power to identify the first selected predictor and the impact of correlated predictors in the model .",
    "the simulation results are shown in figure [ fig : pvals ] .",
    "the simulations show three clear results : 1 ) for moderate to high effect size ( i.e. , when @xmath62 is 11.5 ) we obtain a high power even when the number of predictors is large , 2 ) when the causal gene is part of a correlated cluster of predictors then the power is higher than if the gene is independent , and 3 ) generally there is a fall in power as the number of predictors increase although this drop appears to stabilize and level off as the set of predictors increases .",
    "the power is 100% for an effect size of 1 or 1.5 ( the lines overlay each other in figure  [ fig : pvals ] ) except for the independent predictors .",
    "that means that even when there is a ratio of predictors to observations , @xmath68 , of 5000 then we are still able to identify a signal in the data even with just 50 observations .",
    "an effect size of 1 corresponds to an effect of 1 standard deviation so we are even able to identify the presence of a gene with realistic biogical effects .    for correlated data we see that the power is _ larger _ than the power observed from independent predictors .",
    "this is caused by the fact that the causal predictor is part of a cluster ; with correlated predictors , then each of the predictors in the cluster has a probability of showing an improved association to the outcome just by chance .",
    "thus , we essentially improve our power to detect _",
    "one _ of the predictors in the cluster at the cost of perhaps not detecting the true one but at least one that is highly correlated to it .",
    "if the causal predictor was a singleton and not part of a cluster of correlated predictors then the power results resemble the results from the uncorrelated data ( results not shown ) .",
    "initially the power drops for all situations as the number of predictors increase ( making it harder to identify that there is in fact a true association among one of the predictors and the outcome ) but the overall power seems fairly constant once the number of predictors reach around 50000 ( of which just one is associated with the outcome ) .",
    "not surprisingly , this drop is more noticeable when the true association has a moderate effect than when the effect is smaller ( where the power is consistently low ) or larger ( where the power is consistently high ) .",
    "figure [ fig : pvals ] also shows that we are generally able to control the family - wise error rate at the desired significance level ( 0.05 ) .",
    "perhaps most surprisingly is the consistent high power for moderate to high effect sizes ( @xmath69 or @xmath70 ) considering this is based on just 50 observations .",
    "the black lines correspond to completely independent predictors , the red lines are when there is some collinearity between a group of predictors while the blue lines represent the situation with a group of highly correlated predictors . ]",
    "figure  [ fig : pvals ] focused on the evidence of the first / most important predictor .",
    "once that has been identified we would like to determine the power to detect the second most important predictor and its power . in our simulations we assumed that there was always one causal predictor with a corresponding parameter of @xmath71 and we varied the effect of another predictor , @xmath72 , with values ranging from 0 to 1.5 as previously . following the setup above",
    "we also allow for correlations and assume that the two predictors are actually part of the same cluster .",
    "the black lines correspond to completely independent predictors , the red lines are when there is some collinearity between a group of predictors while the blue lines represent the situation with a group of highly correlated predictors . ]",
    "figure  [ fig:2ndpvals ] shows the power to detect the _",
    "second most important _ variable when there are two potential causal variables from the same cluster .",
    "overall , we see the same trends as we saw for the most important variable in figure  [ fig : pvals ] except that the power is generally lower . looking at the solid line corresponding to the largest effect of the second most important variable we see a marked decline in power as the number of predictors increase if the predictors are all independent .",
    "the decline is not very surprising as both of the two causal predictors have the same effect size ( both @xmath73 ) so the second most important variable will by design be the smallest of those two ( after sampling error has been added ) .",
    "thus , due to random variation , the estimated effect of the second is likely to be slightly smaller than the true effect ( ie .",
    ", biased downward ) and contrary the effect of the most important variable will be slightly biased upward .",
    "however , the drop in power for the second most important variable is still substantial even for high effect sizes except when the predictors are correlated .",
    "figure  [ fig:2ndpvals ] also shows that the error rate is controlled around the 5% level for independent predictors when there is only one predictor that is truly associated to the response ( black dotted line ) . as for the results from the first identified predictor ,",
    "figure  [ fig : pvals ] , we get that correlated predictors increase the power to identify subsequent predictors .    as highlighted in the methods section",
    ", the proposed procedure computes the evidence for the identified features but does not directly provide significance inference for a particular predictor . in practice , however , it is also of interest how well the lasso approach identifies the predictors , that are truly associated with the response . while this has been investigated in several papers @xcite , we include results using the same simulation setup as described above for direct comparison with the simulation results shown in figure [ fig : pvals ] .",
    "the results are shown in figure [ fig : detecttrue ] which clearly shows that the precision increases with increasing signal strength and decreases when the true predictor is part of a cluster of predictors . in the latter case",
    ", it is generally one of the the predictors from the correlated cluster that is identified , so while we may not identify the specific variable we do identify the cluster ( results not shown ) .",
    "we apply our proposed method to the prostate cancer example from @xcite , a subset of 67 samples and 8 predictors from a larger dataset .",
    "the outcome is the logarithm of a prostate - specific antigen and we want to determine the association between the outcome and any predictors .",
    "data can be found at ` www-stat.stanford.edu/~tibs/elemstatlearn/ ` . the setup for this dataset is simpler than the situation mentioned in methods section since @xmath1 so it is possible to analyze all predictors simultaneously by using , say , a classical multiple regression model .",
    "we compare the results from our proposed method with three other analyses approaches : simple multiple regression , using the lasso in combination with the covariance test statistic of @xcite and the multi - split method of @xcite .",
    "the multi - split method essentially consists of variant of 2-fold cross - validation : first , lasso regression is applied to a part of the dataset to select a list of predictors , and secondly an ordinary multiple regression ( using the selected predictors ) is applied to the remaining data to obtain @xmath0-values for the selected predictors .    note that unlike our proposed approach , both the multiple regression , the lasso - covariance test , and the multi - split method are all designed to test hypotheses about each specific predictor . however , the multi - split method is vulnerable to the selection obtained from the first split which gives rise to rather substantial variance of the @xmath0-values in this dataset . splitting the sample",
    "repeatedly and getting a set of @xmath0-values has also been suggested by the authors , but `` it is not obvious , though , how to combine and aggregate the results . ''",
    "@xcite . here",
    ", we have just averaged the @xmath0-values obtained for each of the sets ( with non - selected predictors getting a @xmath0-value of 1 ) .",
    "rrrrrrc predictor & lr @xmath0 & ols @xmath74 & cov @xmath0 & split @xmath0 & rand @xmath0 & correlation with outcome + lcavol & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.73 + svi & 0.00 & 0.05 & 0.17 & 0.29 & 0.34 & 0.56 + lweight & 0.00 & 0.00 & 0.05 & 0.06 & 0.14 & 0.49",
    "+ lcp & 0.00 & 0.09&0.05 & 0.89 & 0.07 & 0.49 + pgg45 & 0.00 & 0.24 & 0.35 & 0.86 & 0.01 & 0.45 + lbph & 0.03 & 0.05&0.92 & 0.51 & 0.01 & 0.26 + age & 0.06 & 0.14 & 0.65 & 0.92 & 0.04 & 0.23 + gleason & 0.00 & 0.88 & 0.97 & 0.97 & 0.58 & 0.34 +    table  [ tab : pval ] shows that the three methods that accommodate multiple predictors and test hypotheses about each specific predictor ( ols , multi - split and the covariance test ) generally give the same results and all identify the same predictor , _ lcavol _ , as being the most important .",
    "the same result is obtained with our proposed method , where the first selected variable is highly significant ( and turns out to be _ lcavol _ ) .",
    "the simple marginal regression analyses show that virtually all predictors are associated with the response , but the other approaches reduce the number of significant variables partly due to correlation among the predictors .",
    "our proposed randomization test identifies slightly more variables than the covariance test ( and a few more than the multi - split procedure ) and seems to lie somewhere between the ordinary multiple regression model and the covariance test .",
    "it is only the covariance test that places any real emphasis on the _ lcp _ predictor in part because of a high degree of collinearity to _ lcavol _ ( the correlation coefficient between _ lcavol _ and _ lcp _ is @xmath75 , which is the second largest among the predictors ) .",
    "the largest pairwise correlation is between _ pgg45 _ and _ gleason _ and is 0.757 but the association to the outcome is less for those two variables so the impact of their collinearity on the @xmath0-values is less noticeable .",
    "we have proposed an algorithm to aid in making inference and subsequently relevant hypotheses concerning data with ( many ) more predictors than samples .",
    "it greatly extends the results from shrinkage regression methods to be able to assign a @xmath0-value to findings from regularized regression methods that combine variable selection and estimation .    when the number of predictors @xmath0 is larger than then sample size @xmath38 , regularized regression methods can be used to identify a sparse model and to provide stable parameter estimates .",
    "the standard lasso suffers from several problems in this regard , in particular that it is not consistent in cariable selection and that the limiting distribution of the estimates are non - standard and can not be directly derived . to overcome these shortcomings @xcite and",
    "@xcite have presented versions of regularized regression that not only have asymptotic oracle properties but also have consistency in variable selection and asymptotic normality of the estimators . however ,",
    "inference based on these improved regularization methods for _ finite samples _ generally perform poorly even when the effects are large @xcite .",
    "an obvious difference between the @xmath0-values obtained from ordinary least squares or from other regularized regression inference approaches is that we pursue investigating a null hypothesis that is not concerned with a specific ( set of ) predictor(s ) but is concerned solely with evaluating the evidence that the results obtained from regularized regression are stronger than what would be expected if none of the predictors were associated with the response .",
    "classical ols - style @xmath0-values are extremely useful but it can be argued that for the majority of problems in genomics  where there is often little or no prior information about _ any _ of the possible predictors  the focus is on hypothesis generation and variable selection and not on testing specific hypotheses .",
    "hence our focus ( and the proposed procedure ) is on hypothesis generation and not on inferences about specific variables .",
    "our simulations show that  even with just 50 observations  the proposed procedure has substantial power to identify at least one associated predictor among a set of 50250 thousand predictors without serious performance decline ( figure [ fig : pvals ] ) .",
    "this suggests that not only can we can identify relevant predictors within a large set of irrelevant predictors  we can also attach a level of significance so we can evaluate whether our findings are indeed likely to be true ( and relevant ) predictors , that are associated to the outcome .",
    "our results extend to the situation where there are multiple causal variables ( figure  [ fig:2ndpvals ] ) although the power declines slightly more rapidly with the number of predictors when testing the importance of subsequent features .",
    "the results regarding the second most important variable shown in figure  [ fig:2ndpvals ] was based on two causal variables in the same cluster .",
    "if we run the same analyzes where the two causal variables are in two different clusters we get essentially the same results as shown for the uncorrelated data in figure  [ fig:2ndpvals ] ( results not shown ) .",
    "we have run the same simulations with a reduced dataset of 30 observations instead of 50 observations and while the power obviously is lower we see the same overall trends as shown above . thus , even with fairly small datasets we have decent power to identify a variable that truly is associated to the outcome .",
    "it can be argued that for genetic data where many genes may follow a similar pattern of expression and thus be correlated , lasso is not an obvious choice for variable selection . if the purpose of the selection is indeed to identify a whole cluster of co - expressed genes , other shrinkage algorithms could be better suited . however",
    "if the genes selected by the lasso are seen as the most influential representative of a cluster there are numerous ways to identify others in the same cluster .",
    "since the whole nature of our approach is not to identify specific variables this is not a problem _ per se_. in fact , figures  [ fig : pvals ] and [ fig:2ndpvals ] show that the power to detect _ something _ from a clustered set of predictors rise dramatically so clustered predictors will just increase our power to identify at least one of the predictors from the cluster .",
    "when comparing methods applied to the prostate dataset , it was clear that all four methods ( the proposed approach , ordinary least squares , the covariance test statistic , and the split method ) are able to identify the variable lcavol as among the most important and a small @xmath0-value is assigned to this predictor . note that while the four methods differ somewhat in their results they also test different versions of the null hypothesis .",
    "the split method consistenly assigns a small @xmath0-value to the lcavol predictor but the remaining seven predictors obtain @xmath0-values estimated with very large uncertainties .",
    "the randomization approach generally yields lowest @xmath0-values which is partly due to the fact that the variables are indeed mostly correlated with the outcome , and partly due to that the null hypothesis is different ( in particular it assumes that _ none _ of the predictors are associated with the outcome ) .",
    "we believe the proposed method could also be applied in for instance genome - wide association studies ( gwas ) , where lasso is already widely used , but significance testing typically done outside the lasso domain , as done by @xcite .",
    "we have presented a method that can be used to make inference about variable selection results from regularized regression models such as the lasso .",
    "we show that it has high power to infer evidence that the selected features are not chance findings  even when the number of predictors is several orders of magnitude larger than the number of observations , i.e. , when @xmath2 .",
    "the method controls the family - wise error rate and can essentially be used with any regularization method .",
    "the proposed method is relevant for situations where regularized regression methods is used as part of the statistical modeling to identify features for subsequent analyses ."
  ],
  "abstract_text": [
    "<S> * motivation : * penalized regression models such as the lasso have proved useful for variable selection in many fields  especially for situations with high - dimensional data where the numbers of predictors far exceeds the number of observations . </S>",
    "<S> these methods identify and rank variables of importance but do not generally provide any inference of the selected variables . </S>",
    "<S> thus , the variables selected might be the `` most important '' but need not be significant . </S>",
    "<S> we propose a significance test for the selection found by the lasso .    </S>",
    "<S> * results : * we introduce a procedure that computes inference and @xmath0-values for features chosen by the lasso . </S>",
    "<S> this method rephrazes the null hypothesis and uses a randomization approach which ensures that the error rate is controlled even for small samples . </S>",
    "<S> we demonstrate the ability of the algorithm to compute @xmath0-values of the expected magnitude with simulated data using a multitude of scenarios that involve various effects strengths and correlation between predictors . the algorithm is also applied to a prostate cancer dataset that has been analyzed in recent papers on the subject . </S>",
    "<S> the proposed method is found to provide a powerful way to make inference for feature selection even for small samples and when the number of predictors are several orders of magnitude larger than the number of observations .    * </S>",
    "<S> availability : * the algorithm is implemented in the mess package in and is freely available .    * </S>",
    "<S> contact : * ekstrom@sund.ku.dk </S>"
  ]
}