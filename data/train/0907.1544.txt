{
  "article_text": [
    "one of the main issues in quantum information theory is the evaluation of the maximum rate , i.e.  the capacity , at which ( classical or quantum ) information can be reliably transmitted via a quantum communication channel . when studying models of noisy quantum communication , a common assumption is that the noise affecting the channel is identical and independent at each channel use . in mathematical terms , the completely positive trace - preserving ( cpt ) map @xmath0 describing @xmath1 uses of the quantum channel is the direct product of @xmath1 identical copies : @xmath2 where @xmath3 is the cpt map describing a single use of the quantum channel .",
    "a channel of this kind is called , as its classical counterpart , a memoryless quantum channel .",
    "coding theorems for memoryless quantum channels , allowing to write the channel capacities in terms of entropic quantities , are well established results in quantum information theory @xcite .",
    "however , the assumption of independent and identical noise can be rather artificial in several physical settings where memory effects may naturally appear , see e.g.  @xcite and references therein .",
    "this observation leads to consider quantum channels with a more general structure than the simple tensor - product structure of ( [ memory - less ] ) .",
    "every quantum channel such that @xmath4 is called a quantum channel with memory , or simply a _ memory channel_. for memory channels the noises affecting multiple channel uses are in general neither independent nor identical .",
    "the structure theorem for memory channels was provided in @xcite . under the assumptions of causality and invariance under time",
    "translation , a sequence of @xmath1 uses of a memory channel can be always decomposed as the @xmath1-fold concatenation @xmath5 of an elementary transformation @xmath6 .",
    "such decomposition requires the introduction of an ancillary system @xmath7 , called the _ memory kernel _ ( or simply the _ memory _ ) , which accounts for correlations .",
    "such elementary transformation has two input and two output systems . in fig .",
    "[ channel ] the horizontal lines indicates the sender ( @xmath8 ) and the receiver ( @xmath9 ) systems , the vertical line the input and output memory .",
    "multiple uses of the memory channel are hence obtained by concatenating the elementary transformation through the vertical line , as shown in the right hand side of fig .",
    "[ channel ] .     with two input systems",
    "@xmath8 and @xmath7 and two outputs @xmath9 and @xmath7 . on the right :",
    "@xmath1 uses of the memory channel are represented as the @xmath1-fold concatenation of the elementary transformation.,scaledwidth=40.0% ]    the performances of the memory channel are in general determined by the memory initialization .",
    "different initial states of the memory kernel can lead to different values of the channel capacities .",
    "this is not the case for `` forgetful '' channels , whose capacities are independent on the memory initialization .",
    "moreover , coding theorems for forgetful channels are straightforward extensions of their memoryless counterparts .",
    "the behavior of forgetful channels is asymptotically independent on the memory initialization , hence the memory system , after a sufficiently large number of channel uses , `` forgets '' what was its initial state .",
    "this property was put forward in @xcite .",
    "then , the notion of  forgetfulness in discrete quantum channels , i.e.  cpt map acting on finite dimensional hilbert spaces , has been formalized in @xcite .",
    "recently , in the framework of continuous memory channel , i.e.  cpt map acting on infinite dimensional hilbert spaces , it has been noticed that the extension of the notion of forgetfulness to this framework is highly nontrivial @xcite .",
    "below , this notion will be slightly weakened and extended to the case of continuous quantum channels by considering markovian noise . as an application we shall evaluate the classical capacity of a bosonic memory channel with additive noise .",
    "the paper develops along the following lines . in sec .",
    "[ markov ] the notion of forgetfulness will be considered in the context of quantum channels with markovian correlated noise and adapted to the continuous variable setting .",
    "we introduce a notion of `` weak - forgetfulness '' to be applied in the case of a markov process with continuous noise variable . in sec .",
    "[ model ] a model of quantum channel subjected to additive gaussian noise with markovian correlations will be proposed .",
    "suitable encoding and decoding unitary transformations allow us to unravel the effects of the memory , hence the channel capacities can be computed using known results from the memoryless setting .",
    "in this section we consider the notion of forgetfulness applied to the case of a class of quantum memory channels with markovian correlated noise .",
    "let us first recall the definition of forgetfulness as presented in @xcite .",
    "a memory channel is forgetful iff for any @xmath10 , there exists an integer @xmath11 such that for any @xmath12 @xmath13 - { { \\mathrm{tr}}}_\\mathcal{b}\\left[\\mathsf{s}^{(n)}(\\rho_{2,\\mathcal{a},\\mathcal{m}})\\right ] \\right\\|_1",
    "< \\epsilon \\ , , \\ ] ] for any @xmath14 , @xmath15 states of the @xmath1 inputs and the initial memory such that @xmath16    this definition of forgetfulness applies in the schroedinger picture description of the memory channel , an equivalent definition can be formulated in the heisenberg picture .",
    "let us briefly comment it .",
    "the density operators @xmath14 , @xmath15 describe two input states of the @xmath1-fold concatenation @xmath5 , including the initial state of the memory kernel @xmath7 and the state on the @xmath1 channel inputs belonging to the sender @xmath8 . equation ( [ mem_init ] ) states that @xmath14 and @xmath15 only differ for the reduced state of memory kernel , corresponding to two different memory initializations . in eq .",
    "( [ def1 ] ) , the partial traces @xmath17 $ ] , @xmath18 $ ] , over the @xmath1 output of the channel belonging to the receiver @xmath9 , indicate the final states of the memory kernel after @xmath1 channel uses .",
    "hence , after @xmath19 uses of a forgetful channel the final state of the memory kernel can be assumed to be independent on the memory initialization with an error smaller than @xmath20 , where @xmath11 is only determined by the error threshold @xmath20 , uniformly for all initial states of the memory kernel .",
    "the trace distance @xmath21 is used to quantify the distance between the final states of the memory kernel .",
    "if the memory channel is forgetful , one can adopt a double - block encoding . over @xmath22 channel uses",
    ", the first @xmath23 are not used to send information , but only to let the memory kernel forget its initial state with an error smaller than @xmath20 , then the remaining @xmath24 are used to send information to the channel .",
    "this double blocking procedure allows to prove the coding theorem for forgetful channels .",
    "quantum channels with markovian correlated noise were first considered in @xcite .",
    "here we are going to consider such channels characterized by a memory system represented by a classical random variable @xmath25 taking values @xmath26 in a measurable set @xmath27 .    at the @xmath28th use of the channel",
    "an input state @xmath29 maps to an output state @xmath30 where @xmath31 is the probability distribution of random variable @xmath25 at step @xmath28 and @xmath32 is a cpt map for any @xmath33 . the probability distribution of the noise variable changes according to the markov rule @xmath34 in which @xmath35 is the transition function determining the stationary markov process .",
    "the model studied in @xcite belongs to this class of memory channels .",
    "coding theorems for this class of memory channels were provided in @xcite in the case @xmath36 is a discrete variable .    recalling that in the forgetful channel",
    ", the final state of the memory is independent of the initial memory , one would say that a quantum memory channel with markovian correlated noise is forgetful if and only if the markov process of the environment has a unique stationary state .",
    "this is indeed the case for quantum channels acting on discrete variable quantum systems . from this intuition",
    "we are led to introduce the notion of `` weak - forgetfulness '' to include the case of infinite of infinite dimensional quantum memory channels . considering this we define weak - forgetful channels as follows :",
    "a memory channel with markovian correlated noise is `` weak - forgetful '' iff for any @xmath37 , and for any pair of initial probability distributions @xmath38",
    ", @xmath39 , there exists an integer @xmath11 such that for any @xmath19 @xmath40 where @xmath41 is the distance between the probability distributions at step @xmath1 with two different initial probability distributions @xmath38 and @xmath42 .",
    "@xmath43    hence we can say that , even in the case of continuous variables , a memory channel with markovian correlated noise is weak - forgetful iff the underlying markov process has unique stationary state .    to adopt a double block procedure one should wait for @xmath19 channel uses in order to let the noise process approaches the stationary state and then start encoding information .",
    "it is worth to mention that `` weak - forgetfulness '' differs from `` forgetfulness '' property in the sense that the noise probability distributions converge not uniformly with respect to the initial distributions @xmath44 , @xmath45 .",
    "some examples will be discussed in the next section . in conclusion , the notion of forgetfulness and weak - forgetfulness clearly coincide if the set @xmath27 in which the noise variable takes values is compact .",
    "this is the case of discrete random variable studied in @xcite .",
    "in this section we consider a model of bosonic memory channel with markovian correlated noise .",
    "the notion of weak - forgetfulness is applied to this model .",
    "the model under consideration is a bosonic channel with additive noise . a sequence of @xmath1 uses of the memory channel maps @xmath1 input bosonic modes , with ladder operators @xmath46 , onto @xmath1 output modes , described by the operators @xmath47 .",
    "in the heisenberg picture , the mode operators are transformed as follows : @xmath48 where @xmath49 is the value of the random variable @xmath25 at the @xmath28th step .    in the schroedinger picture ,",
    "a density operator @xmath50 describing the state of the @xmath1 input modes is subjected to a _ random displacement _ , i.e.@xmath51 p(z_1,z_2,\\dots z_n ) \\times \\nonumber\\\\ & \\times \\left [ \\bigotimes_{k=1}^n \\mathcal{d}_k(z_k)\\right ] \\rho^{(n)}_\\mathcal{a } \\left[\\bigotimes_{k=1}^n \\mathcal{d}_k(z_k)^\\dag\\right],\\end{aligned}\\ ] ] where @xmath52 is the displacement operator acting on the @xmath28th input mode , and @xmath53 is the joint probability distribution of the @xmath1 noise variables .    the quantum channel is gaussian if and only if the probability distribution of the noise is gaussian .",
    "our aim is to compute the capacity of the quantum channel . in order to avoid unphysical results ,",
    "we impose a constraint on the maximum energy at the input modes by the following condition : @xmath54    the memoryless limit is recovered iff the noise variables are mutually independent and identically distributed , i.e.  if and only if the joint probability distribution is the product of @xmath1 identical distributions : @xmath55    a remarkable case is obtained if the noise variables come from a time - independent markov process . in this case",
    "the quantum channel satisfies the conditions of causality and invariance under time translations and the structure theorem can be applied .",
    "the joint probability distribution reads @xmath56 where @xmath57 is the transition function determining the markov chain and @xmath58 is the initial probability distribution describing the noise variable at the first channel use .    in order to construct a gaussian channel",
    ", one has to consider a gaussian stochastic process .",
    "here we consider a gaussian transition function of the form : @xmath59 } \\ , .\\ ] ] here and in the following we omit writing the normalization factor in front of the probability density distributions .",
    "the memory channel is hence described by two parameters . the parameter @xmath60 $ ] accounts for the memory effects , and @xmath61 , as it will be made clear below , to the amount of noise in the channel .",
    "the memoryless limit is recovered for @xmath62 , in which case the joint probability distribution factorizes as in ( [ memoryless ] ) .",
    "the features of the memory channel depends on the underlying markov process . inserting ( [ trans ] ) into ( [ n - fold ] ) we obtain @xmath63 where @xmath64 . by considering the limit @xmath65",
    "we distinguish the following cases .",
    "for @xmath66 and @xmath67 there exists an unique stationary distribution @xmath68 hence , for these values of the parameters , the memory channel is weak - forgetful .",
    "notice that the parameter @xmath69 is the noise variance of the stationary distribution .",
    "considering the stationary state of the markov process is hence sufficient for computing the channel capacities . upon @xmath1",
    "channel uses the stationary process is described by a gaussian joint probability density distribution @xmath70 } \\",
    ", , \\ ] ] where @xmath71 is the @xmath72 tridiagonal matrix : @xmath73    for any @xmath1 , the quadratic form appearing in ( [ case_1 ] ) can be always put in a diagonal form @xmath74 in terms of the _ collective _ noise variables @xmath75 where @xmath76 is the @xmath72 orthogonal matrix diagonalizing @xmath71 : @xmath77    by applying a unitary encoding and decoding transformations , we can analogously define the _ collective _ input variables @xmath78 and output variables @xmath79 which transform according to @xmath80    hence , @xmath1 uses of the memory channel are unitary equivalent to the tensor product of @xmath1 additive noise channels , whose noise variables are mutually independent but not identically distributed . from ( [ diagonal ] ) , the collective noise variables are distributed according to the gaussian distributions @xmath81 where the noise variances are @xmath82 for any @xmath1 , the distribution of the noise variances can be computed from the eigenvalues of the matrix @xmath71 .",
    "notice that the energy constrain is preserved in terms of the collective input variables , i.e.@xmath83    in the limit of @xmath65 , the distribution of the eigenvalues of the matrix @xmath71 , arranged in nondecreasing order , tends to an asymptotic distribution , described by the function @xmath84 for @xmath85 $ ] , in the sense that @xcite : @xmath86 analogously , for @xmath87 , we have @xmath88 where the asymptotic distribution of the noise variances , arranged in nonincreasing order , is @xmath89    as consequence of ( [ as_equivalence ] ) , for any smooth function @xmath90 , the following equality holds true @xmath91      the additive noise channel has been widely studied in the memoryless , gaussian case .",
    "we recall the case of the memoryless broadband channel . at each use of the channel ,",
    "@xmath92 input modes @xmath93 are subject to independent , but not identically distributed , gaussian additive noises with variances @xmath94 .",
    "a lower bound on the classical capacity can be obtained optimizing over gaussian encoding . moreover , using the recently proven _ minimum output entropy conjecture _ , it is possible to show that the classical capacity of the broadband channel , per mode and expressed in bits , is @xmath95 where @xmath96 and @xmath97 where @xmath98 equals @xmath99 if @xmath100 and is zero otherwise .",
    "the value of the lagrange multiplier @xmath101 is the root of the integral equation @xmath102    using the result for the memoryless broadband channel we can now compute the classical capacity of the memory channel , in the region @xmath66 , @xmath103 , by following the same line of reasoning of @xcite .    for any @xmath1 , we can group the set of collective modes in @xmath92 blocks of length @xmath104 . at the boundaries of the @xmath105th",
    "block the maximum and minimum limits of the effective noise variances are @xmath106 recalling that the noise variances @xmath107 are in nonincreasing order , it follows that for arbitrary @xmath108 and for sufficiently large @xmath109 : @xmath110 for any @xmath105 and @xmath111 .    from the last equation it follows that the classical capacity of the memory channel is bounded from above and from below by the capacity of two memoryless broadband channels , respectively characterized by the set of @xmath92 noise variances @xmath112 and @xmath113 .",
    "now , keeping @xmath92 fixed and in the limit @xmath114 , we can write the following bounds for the classical capacity : @xmath115 where @xmath116 and the optimal distribution @xmath117 , @xmath118 are as in eq.s ( [ optimal ] ) , ( [ constraint ] ) .",
    "finally , in the limit @xmath119 the lower and upper bound coincide . using ( [ average ] )",
    "that yields the following formula for the classical capacity : @xmath120 - g[\\sigma(z ) ] \\ , , \\ ] ] where the function @xmath121 is determined according to the continuous limit of eq.s ( [ optimal ] ) , ( [ constraint ] ) , i.e.@xmath122    the formulas ( [ capacity_full ] ) , ( [ n_full ] ) , ( [ l_full ] ) can be used to numerically compute the classical capacity of the memory channel in the region @xmath66 and @xmath67 . the numerical results are plotted in fig .",
    "[ classical ] .",
    "we remark that , although we have assumed the noise process to be at the stationary state , since the memory channel is weak - forgetful , the obtained result is the classical capacity for all the initial states of the memory .     and @xmath123 .",
    "the maximum value of the number of excitation per mode is @xmath124 , corresponding to a noiseless channel classical capacity @xmath125.,scaledwidth=40.0% ]      some care is needed in dealing with the parameter regions defined by @xmath126 and @xmath66 , and defined by @xmath127 . for these values of the parameters",
    "the transition functions become singular .",
    "@xmath128    in the limit @xmath129 , an unique stationary state exists although singular , i.e.  @xmath130 .",
    "we can still say that the channel is weak - forgetful . by noticing that the stationary state of the markov process corresponds to a noiseless channel",
    ", we can say that for @xmath131 the classical capacity of the memory channel is given by the noiseless channel formula @xmath132 .    in the limit @xmath133 ,",
    "the dirac @xmath134-function in ( [ perfect ] ) implies that the noise acting at different channel uses are perfectly correlated .",
    "it is immediate to recognize that in this case the markov process has infinitely many stationary states .",
    "the channel has hence long - term memory and is not weak - forgetful .",
    "thus we can not say _ a priori _ that the channel capacity is independent on the memory initialization . however , we can still solve the channel by proceeding as follows . upon @xmath1",
    "channel uses the corresponding joint probability distribution of the noise variables reads as follows @xmath135 where @xmath58 is the initial noise distribution . for a generic initial noise distribution , even a nongaussian one",
    ", we can solve the problem of the channel capacity by introducing suitable encoding / decoding unitary transformations which allow to unravel the memory . for any @xmath1 , we can define the collective noise variable @xmath136 together with a set of @xmath137 variables @xmath138 in terms of these collective noise variables , the joint probability distribution ( [ ltm ] ) factorizes as follows : @xmath139 hence , introducing the collective input and output variables @xmath140 it follows that the collective mode @xmath141 is subject to the additive noise described by the initial noise probability @xmath142 , while the remaining @xmath137 collective modes experience a noiseless channel . in conclusion ,",
    "taking the limit @xmath65 and independently of the initial noise distribution , the classical capacity of the memory channel is given by the noiseless formula @xmath143 .",
    "it is worth noticing that the classical capacity at the singular region coincides with the analytical continuation of the expression in eq .",
    "( [ capacity_full ] ) .    to conclude this section we notice that the stationary gaussian process discussed in the previous subsection can be mapped into the gaussian model discussed in @xcite . in this mapping , the point @xmath127 , which gives rise to a channel with long - term memory , corresponds to the critical point of the gaussian model .",
    "in conclusion we have considered the notion of forgetfulness for memory channels with markovian correlated noise . for the case of a markov process with discrete noise variables forgetfulness",
    "is equivalent to the existence of unique stationary noise distribution . in the case of continuous variable markov process ,",
    "we have introduced a notion of weak - forgetfulness .",
    "a memory channel with continuous variable markovian correlated noise is weak - forgetful iff the noise process has unique stationary distribution .",
    "moreover the capacities are independent of the memory initialization .",
    "the notion of forgetfulness and weak - forgetfulness are equivalent in the discrete variables setting . as an application",
    ", we have proposed a model of bosonic gaussian channel with additive markovian correlated noise and computed the classical capacity .",
    "the channel is either weak - forgetful or has long - term memory . in all the cases the classical capacity has been",
    "be computed exactly ( fig .",
    "[ classical ] summarizes the obtained results ) .",
    "it is worth noticing that the capacity is reached without the use of entangled codewords .",
    "this can be easily proven by noticing that the encoding transformation in eq .",
    "( [ encoding ] ) transforms coherent states into coherent states , and recalling that coherent state encoding is optimal to reach the memoryless classical capacity in eq .",
    "( [ broadband ] ) .",
    "this is related to the fact that the considered channel model is covariant under gauge transformations @xmath144 .",
    "entangled codewords would be necessary if one consider a noise process which breaks this symmetry , see e.g.  @xcite .",
    "other kinds of capacities can be computed along the same line of reasoning for the considered model .",
    "furthermore , by exploiting the recently proven _ minimum output entropy conjecture _ @xcite , the same methods can be applied to determine the capacities of other bosonic channels , e.g.  attenuation and amplification channels , with markovian noise .",
    "the authors would like to thank v.  giovannetti for valuable comments .",
    "c.l .  and l.m .",
    "are grateful to j.  gtschow , d.  gross , and r.  f.  werner for the stimulating discussions .",
    "the research leading to these results has received funding from the european commission s seventh framework programme ( fp7/2007 - 2013 ) under grant agreement no .",
    "after completing the paper , we became aware of a related work on markovian memory channels @xcite ."
  ],
  "abstract_text": [
    "<S> the notion of forgetfulness , used in discrete quantum memory channels , is slightly weakened in order to be applied to the case of continuous channels . </S>",
    "<S> this is done in the context of quantum memory channels with markovian noise . as a case study , </S>",
    "<S> we apply the notion of weak - forgetfulness to a bosonic memory channel with additive noise . a suitable </S>",
    "<S> encoding and decoding unitary transformation allows us to unravel the effects of the memory , hence the channel capacities can be computed using known results from the memoryless setting . </S>"
  ]
}