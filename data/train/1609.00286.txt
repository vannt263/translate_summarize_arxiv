{
  "article_text": [
    "this paper studies a regression model where both predictor and response variables are random functions .",
    "let @xmath5 be @xmath6-valued random variables with @xmath7 $ ] , and consider a regression model of the form @xmath8(s ) = { \\mathrm{e}}[y(s ) ] + \\int_{i } b(s , t ) \\ { x(t ) - { \\mathrm{e}}[x(t ) ] \\ } dt .",
    "\\label{eq : model0}\\ ] ] see section [ sec : setup ] for the precise description of the setup .",
    "the focus of this paper is on estimation of the bivariate function @xmath0 , which is an ill - posed inverse problem ( see remark [ rem : ill - posed inverse problem ] in section [ sec : setup ] ) .",
    "data collected on dense grids can be typically regarded as realizations of a random function ( stochastic process ) , and such data are called _ functional data_. statistical methodology dealing with functional data is called _ functional data analysis _ and has a large number of fruitful applications ( cf . * ? ? ?",
    "for example , the functional linear model ( [ eq : model0 ] ) with functional predictor and response variables can be used to know how a complete daily temperature profile over one year influences a daily precipitation at each day ( see * ? ? ?",
    "* chapter 16 ) .    in this paper",
    ", we consider estimators for the function @xmath4 based on the functional principal component analysis ( pca ) , which is one of standard techniques used in functional data analysis . applying basis expansions of @xmath9 and @xmath4 using the eigenfunction system @xmath10 for the covariance operator of @xmath9",
    ", we can expand @xmath9 and @xmath4 as @xmath11 + \\sum_{k } \\xi_{k } \\phi_{k}(t)$ ] and @xmath12 , where we measure smoothness of @xmath4 via how fast @xmath13 decays as @xmath14 or @xmath15 .",
    "we consider two methods to estimate @xmath4 based on different characterizations of @xmath4 .",
    "the first method uses the fact that @xmath16 = { \\mathrm{e}}[\\xi_{k}^{2 } ] \\sum_{j } b_{j , k } \\phi_{j}(s)$ ] .",
    "this method is based on truncation of the series expansion @xmath17/{\\mathrm{e}}[\\xi_{k}^{2 } ] \\ } \\phi_{k}(t)$ ] by a finite series @xmath18 with @xmath19 as @xmath20 ( which we call single truncation in comparison with the second method below ) , and replace @xmath21 $ ] , @xmath22 $ ] , and @xmath23 by their estimators .",
    "this estimator was considered by @xcite .",
    "the second method uses the expansion of @xmath24 as @xmath25 .",
    "this alternative method is based on truncation of the double series expansion @xmath26/{\\mathrm{e}}[\\xi_{k}^{2 } ] \\ }   \\phi_{j}(s)\\phi_{k}(t)$ ] by a finite series @xmath27 with @xmath28 and @xmath29 as @xmath20 ( which we call double truncation ) , and replace @xmath30 , { \\mathrm{e}}[\\xi_{k}^{2}]$ ] , and @xmath31 by their estimators .",
    "@xcite consider our first estimator , but the focus in @xcite is on prediction , and not on estimation of the function @xmath4 _ per se_. these two problems are substantially different , and they do not derive sharp rates of convergence for their estimator of @xmath4 itself . @xcite and",
    "@xcite analyze the estimator of @xcite for @xmath4 with dependent functional data , but they only prove consistency of the estimator .",
    "@xcite consider a pca - based estimator similar to our second estimator , but do not explicitly derive rates of convergence for their estimator .",
    "the object of this paper is to study rates of convergence for estimation of @xmath4 .",
    "first , we show that under suitable regularity conditions , the estimator based on single truncation ( i.e. , the estimator of @xcite ) attains the convergence rate for the integrated squared error that is characterized by smoothness of the function @xmath3 in @xmath2 together with the the decay rate of the eigenvalues of the covariance operator , but the rate does not depend on smoothness of @xmath0 in @xmath1 .",
    "this rate is shown to be minimax optimal .",
    "this means that smoothness of @xmath0 in @xmath1 does not affect difficulty of estimating @xmath4 , which is in sharp contrast with nonparametric estimation of a bivariate regression function .",
    "next , we analyze the second estimator based on double truncation , and provide conditions under which it attains the optimal rate .",
    "we point out that some restrictions on smoothness levels for @xmath0 in @xmath1 and @xmath2 are required for the second estimator to achieve the optimal rate .",
    "we include the analysis of the second estimator since in applications , double truncation typically leads to an estimate more interpretable than single truncation , although from a theoretical point of view , single truncation is enough for the purpose of estimating @xmath4 ; see remark [ rem : double truncation ] ahead and the discussion in chapter 16 of @xcite .",
    "we also conduct simulations to verify the performance of the estimators in the finite sample .",
    "the literature on functional data analysis is now quite broad .",
    "we refer to @xcite , @xcite , and @xcite as general references on functional data analysis .",
    "one of the main focuses in the previous literature on functional data analysis is a functional linear model with a scalar response variable .",
    "see @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , and @xcite .",
    "in particular , @xcite consider a pca - based estimator and an estimator based on tikhonov regularization for the slope function , and provide conditions under which those estimators attain minimax rates of convergence for the integrated squared error .",
    "the analysis of functional responses was first considered by @xcite .",
    "@xcite consider a regression model where a predictor variable is finite - dimensional while a response variable is a random function .",
    "functional linear models with functional predictor and response variables are considered in @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , and @xcite .",
    "@xcite work with fixed designs , which is a different setting than ours , and prove consistency of a series estimator of the integral operator with kernel @xmath4 for the operator norm .",
    "we already referred to @xcite , @xcite , and @xcite . @xcite",
    "propose an estimator of @xmath4 based on the functional canonical correlation analysis , but do not study its asymptotic properties .",
    "@xcite considers prediction for functional linear regression with functional responses based on a reproducing kernel hilbert space approach , which is a topic substantially different from ours .",
    "the recent preprint by @xcite studies a tikhonov regularization estimation for @xmath4 and establishes rates of convergence for their estimator ; the estimator and the assumptions in @xcite are substantially different from ours and so their results are not directly comparable to ours .",
    "importantly , none of these papers derives optimal rates of convergence for estimation of @xmath4 ; the present paper fills this important void and thereby contributes to advancing the understanding of functional data analysis .",
    "we stress here that in applications , the function @xmath4 has its own interpretation , and therefore estimation of @xmath4 has its own importance .",
    "for example , in the canadian weather example discussed in ( * ? ? ?",
    "* chapter 16 ) , @xmath0 is interpreted as the relative weight placed on the temperature at day @xmath2 that is required to predict log precipitation on day @xmath1 .    from a technical point of view , the proofs of the main theorems ( theorems [ thm : main1 ] and [ thm : main2 ] ) build upon the techniques developed in @xcite .",
    "however , since we are estimating a bivariate function with two different levels of smoothness rather than a univariate function in the scalar response case , the proofs require a chain of delicate calculations .",
    "in this paper , we use basic results on functional analysis .",
    "we refer to @xcite as a general reference on functional analysis . @xcite and @xcite",
    "cover results on functional analysis useful for functional data analysis . for mathematical background on linear inverse problems ,",
    "we refer to @xcite .",
    "the rest of the paper is organized as follows . in section [ sec :",
    "setup ] , we formally describe the setup and estimators . in section [ sec : main results ] , we present the main results on rates of convergence of the pca - based estimators for the coefficient function . in section [ sec : simulation ] , we present simulation results to verify the performance of the pca - based estimate in the finite sample .",
    "section [ sec : proof ] contains all the proofs for the results stated in section [ sec : main results ] .",
    "we use the following notation . for any measurable functions @xmath32 ,",
    "let @xmath33 for any functions @xmath34 , define @xmath35 by @xmath36 let @xmath37 and define the equivalence relation @xmath38 for real - valued functions @xmath39 defined on @xmath40 by @xmath41 almost everywhere .",
    "define @xmath6 by the quotient space @xmath42 equipped with the inner product @xmath43 for @xmath44 where @xmath45 ; the space @xmath6 is a separable hilbert space , and as usual , we identify any element in @xmath46 as an element of @xmath6 .",
    "define @xmath47 analogously .",
    "we also identify any real - valued function @xmath48 defined almost everywhere on @xmath40 ( or @xmath49 ) as a function defined everywhere on @xmath40 ( or @xmath49 ) by setting @xmath50 for any point @xmath2 at which @xmath48 is not defined . for any positive sequences",
    "@xmath51 , we write @xmath52 if @xmath53 is bounded and bounded away from zero . in",
    "what follows , let @xmath54 denote an underlying probability space .",
    "suppose that we observe a pair of random functions @xmath55 with @xmath7 $ ] , where @xmath56 and @xmath57 are predictor and response variables , respectively . recall that a random function @xmath58 defined on the probability space @xmath59 is said to be measurable if the map @xmath60 is jointly measurable .",
    "define @xmath61 dt < \\infty \\big \\}.",
    "\\end{aligned}\\ ] ] note that if @xmath62 , then @xmath63 < \\infty$ ] for almost every @xmath64 , and any element in @xmath65 can be identified with an @xmath6-valued ( borel measurable ) random variable .",
    "recall that a measurable stochastic process with paths in @xmath6 almost surely can be identified with an @xmath6-valued random variable , and vice versa @xcite .",
    "we assume that @xmath66 , and consider a functional linear regression model @xmath67(s ) = { \\mathrm{e}}[y(s ) ]   + \\int_{i }   b(s , t ) \\ { x(t ) - { \\mathrm{e}}[x(t ) ] \\ }   dt , \\label{eq : model}\\ ] ] where @xmath68 $ ] is the conditional expectation of @xmath24 as an @xmath6-valued random variable conditionally on the @xmath69-field generated by @xmath9 ( which is well - defined since @xmath70 < \\infty$ ] , and @xmath68 $ ] itself is an @xmath6-valued random variable ; see chapter 5 in @xcite ) , and @xmath71 is the coefficient function assumed to be in @xmath47 , i.e. , @xmath72 the equality in ( [ eq : model ] ) should be understood as an equality as @xmath6-valued random variables , namely , @xmath73(s ) - { \\mathrm{e}}[y(s ) ]   - \\int_{i }   b(s , t ) \\ { x(t ) - { \\mathrm{e}}[x(t ) ] \\ } dt   \\right \\}^{2 } ds = 0\\ ] ] almost surely .",
    "the goal of this paper is estimation of the function @xmath71 , and to this end we shall employ the functional principal component analysis ( pca ) . consider the covariance function @xmath74 the assumption that @xmath75dt < \\infty$ ] ensures that @xmath76 .",
    "in addition , we assume that the integral operator from @xmath6 into itself with kernel @xmath77 , namely the covariance operator of @xmath9 , is injective ( which is equivalent to the condition that @xmath78 for all @xmath79 with @xmath80 ) .",
    "the covariance operator is self - adjoint and positive definite .",
    "the hilbert - schmidt theorem ( cf . * ? ? ?",
    "* theorem vi.16 ) then ensures that @xmath77 admits the spectral expansion @xmath81 in @xmath47 , where @xmath82 are a non - increasing sequence of eigenvalues tending to zero and @xmath10 is an orthonormal basis of @xmath6 consisting of eigenfunctions of the integral operator , namely , @xmath83 we will later assume that there are no ties in @xmath84 s , i.e. , @xmath85 . since @xmath10 is an orthonormal basis of @xmath6 , we have the following expansion in @xmath6 : @xmath86 + \\sum_{k=1}^{\\infty } \\xi_{k } \\phi_{k}(t),\\ ] ] where each @xmath87 is defined by @xmath88 \\ }",
    "\\phi_{k}(t ) dt.\\ ] ] note that by parseval s identity and fubini s theorem , @xmath89 = \\int_{i } \\operatorname{var}(x(t ) ) dt < \\infty,\\ ] ] and @xmath90 = \\iint_{i^{2 } } k(s , t ) \\phi_{k}(s ) \\phi_{\\ell}(t ) ds dt =   \\begin{cases } \\kappa_{k } & \\text{if } \\",
    "k=\\ell \\\\ 0 & \\text{if } \\ k",
    "\\neq \\ell \\end{cases } .",
    "\\label{eq : pc score}\\ ] ] furthermore , since @xmath91 is an orthonormal basis of @xmath47 , we have @xmath92 in @xmath47 , where each @xmath93 is defined by @xmath94 this yields that @xmath95 \\ } dt = \\sum_{j=1}^{\\infty } \\left ( \\sum_{k=1}^{\\infty } b_{j , k } \\xi_{k } \\right ) \\phi_{j}(s).\\ ] ]    now , because of ( [ eq : pc score ] ) and since the expansion of @xmath9 holds in @xmath96 too ( i.e. , @xmath97 - \\sum_{k=1}^{n } \\xi_{k } \\phi_{k}\\|^{2 } ] = \\sum_{k = n+1}^{\\infty } { \\mathrm{e}}[\\xi_{k}^{2 } ] \\to 0 $ ] as @xmath98 ) , we have that @xmath99 = \\kappa_{k } \\sum_{j=1}^{\\infty } b_{j , k } \\phi_{j}(s),\\ ] ] where the equality holds in @xmath6 , and therefore we obtain the following characterization of @xmath4 : @xmath100}{\\kappa_{k } } \\phi_{k}(t ) .",
    "\\label{eq : first characterization}\\ ] ] this characterization leads to a natural estimator .",
    "let @xmath101 be independent copies of @xmath102 as ( @xmath103)-valued random variables .",
    "we estimate @xmath77 by the empirical covariance function @xmath104 defined as @xmath105 where @xmath106 .",
    "let @xmath107 be the spectral expansion of @xmath104 in @xmath47 , where @xmath108 are a non - increasing sequence of eigenvalues tending to zero and @xmath109 is an orthonormal basis of @xmath6 consisting eigenfunctions of the integral operator with kernel @xmath104 , namely , @xmath110 the spectral expansion in ( [ eq : expansion of khat ] ) is possible since the integral operator with kernel @xmath104 is of finite rank ( at most @xmath111 ) , and so in addition to an orthonormal system of @xmath6 consisting of eigenfunctions corresponding to the positive eigenvalues , we can add functions so that the augmented system of functions @xmath109 becomes an orthonormal basis of @xmath6 .",
    "furthermore , let @xmath112 using the characterization in ( [ eq : first characterization ] ) , we consider the following estimator based on single truncation : @xmath113 where @xmath19 as @xmath20 .",
    "this estimator was considered in @xcite .",
    "we also consider an alternative estimator based on truncating the double series , namely , double truncation .",
    "let @xmath114 $ ] , and consider the expansions @xmath115 in @xmath6 .",
    "now , since for each @xmath116 , @xmath117 \\ } dt \\right \\ } \\phi_{j } ( s ) ds= \\sum_{k=1}^{\\infty } b_{j , k } \\xi_{k},\\ ] ] it holds that @xmath118 where @xmath119 = \\int_{i } { \\mathrm{e}}[y(s ) ] \\phi_{j}(s)ds$ ] for @xmath116 .",
    "therefore , @xmath120 = b_{j , k } { \\mathrm{e } } [ \\xi_{k}^{2 } ] = \\kappa_{k } b_{j , k},\\ ] ] namely , @xmath121}{\\kappa_{k}}. \\label{eq : representation of b}\\ ] ] based on this characterization , we consider the following alternative estimator : @xmath122 where @xmath28 and @xmath29 as @xmath20 , and each @xmath123 is given by @xmath124    in the next section , we will derive rates of convergence of the estimators @xmath125 and @xmath126 for the integrated squared error .",
    "[ rem : double truncation ] it will turn out in the next section that @xmath125 with properly chosen @xmath127 is rate optimal , and from a theoretical point of view , single truncation is enough for the purpose of estimating @xmath4 .",
    "however , in practice , double truncation would be a preferred option since , compared with single truncation , double truncation typically results in an estimate of @xmath0 more regular in @xmath1 and thereby yielding a more interpretable estimate .",
    "see the discussion in chapter 16 of @xcite .",
    "hence the analysis of our second estimator is of some importance .",
    "[ rem : ill - posed inverse problem ] in this remark , we shall point out that the problem of estimating @xmath4 can be regarded as a problem of estimating an unknown operator in the operator equation , and the latter is an ill - posed inverse problem . for any @xmath128 , let @xmath129 denote the integral operator with kernel @xmath130 , i.e , @xmath131 the adjoint operator @xmath132 of @xmath133 is also an integral operator and is of the form @xmath134 now , let @xmath135 .",
    "then , using the symmetry of @xmath77 , we have that for any @xmath136 , @xmath137 that is , @xmath138 .",
    "since we are assuming that @xmath139 is injective , we have that @xmath140 . both @xmath141 and @xmath77",
    "can be directly estimated from the data .",
    "however , since @xmath139 is a compact operator ( * ? ? ?",
    "* theorems vi.22 and vi.23 ) , @xmath142 is necessarily unbounded @xcite , and therefore the problem of recovering @xmath143 is ill - posed ( * ? ? ?",
    "* section 15.1 ) .",
    "in fact , consider @xmath144 , which converges to @xmath145 in @xmath47 as @xmath98 ( i.e. , @xmath146 converges to @xmath147 in the hilbert - schmidt norm ) .",
    "it is seen that @xmath148 satisfies @xmath149 , but @xmath150 .",
    "in this subsection , we derive rates of convergence of the estimators @xmath125 and @xmath126 defined in ( [ eq : first estimator ] ) and ( [ eq : second estimator ] ) , respectively . to this end",
    ", we make the following assumption .    [ as : 1 ] there exist constants @xmath151 , and @xmath152 such that @xmath153 < \\infty , \\ { \\mathrm{e}}[\\| y \\|^{2 } \\mid x]\\leq c_{1 } \\ \\text{a.s . } , \\label{eq : moment condition1 } \\\\ & { \\mathrm{e } } [ \\xi_{k}^{4 } ] \\leq c_{1 } \\kappa_{k}^{2 } , \\ \\forall k \\geq 1 , \\label{eq : moment condition2 } \\\\ & \\kappa_{k } \\leq c_{1 } k^{-\\alpha } , \\",
    "\\kappa_{k } - \\kappa_{k+1 } \\geq c_{1}^{-1 } k^{-\\alpha-1 } , \\ \\forall k",
    "\\geq 1 , \\label{eq : eigenvalue condition } \\\\ & | b_{j , k } | \\leq c_{1}j^{-\\gamma } k^{-\\beta } , \\",
    "\\forall j , k \\geq 1 , \\",
    "\\beta > \\frac{\\alpha}{2 } + 1 .",
    "\\label{eq : condition on b}\\end{aligned}\\ ] ]    some comments on assumption [ as : 1 ] are in order .",
    "the first row ( [ eq : moment condition1 ] ) is a standard moment condition .",
    "the second ( [ eq : moment condition2 ] ) and third rows ( [ eq : eigenvalue condition ] ) are adapted from @xcite .",
    "condition ( [ eq : moment condition2 ] ) is standard in the literature on functional linear models . in condition ( [ eq : eigenvalue condition ] ) , as in @xcite and @xcite , we require that the eigenvalues @xmath154 are `` well - separated '' , namely , @xmath155 for all @xmath156 . this condition is used to ensure sufficient estimation accuracy of the empirical eigenfunctions @xmath157 .",
    "note that this condition also ensures that , since @xmath158 as @xmath15 , @xmath159 so @xmath160 as @xmath15 .",
    "the value of @xmath161 measures `` ill - posedness '' of the estimation problem , so that the larger @xmath161 is , the more difficult estimation of @xmath4 will be .",
    "the last condition ( [ eq : condition on b ] ) is a smoothness condition on @xmath4 , where the smoothness is measured through the eigenfunction system @xmath10 , which is natural in our setting . since @xmath0 is a bivariate function , however , there are potentially a number of variations on how @xmath93 decays as @xmath14 or @xmath15 .",
    "we focus on a simple case where @xmath13 decays like @xmath162 as @xmath14 or @xmath15 , and @xmath163 measures smoothness of @xmath3 in @xmath1 while @xmath164 measures smoothness of @xmath0 in @xmath2 .",
    "we also require that @xmath165 for a technical reason ; see the discussion after theorem [ thm : main1 ] .",
    "the following theorem establishes rates of convergence for @xmath125 .",
    "[ thm : main1 ] consider the estimator @xmath125 defined in ( [ eq : first estimator ] ) .",
    "suppose that assumption [ as : 1 ] is satisfied .",
    "choose @xmath127 in such a way that @xmath19 and @xmath166 .",
    "then @xmath167 therefore , by choosing @xmath168 , we have @xmath169    it is not difficult to verify from the proof of theorem [ thm : main1 ] that the results of the theorem hold uniformly over a class of distributions @xmath170 of @xmath102 that verify ( [ eq : model ] ) and ( [ eq : moment condition1])([eq : condition on b ] ) for given constants @xmath171 , and @xmath172 . in particular , by choosing @xmath173 , we have @xmath174 where @xmath175 denotes the probability under @xmath176 .",
    "we will show in theorem [ thm : lower bound ] that the rate @xmath177 is minimax optimal .",
    "the requirement that @xmath166 comes from the following reason . in the proof of theorem [ thm : main1",
    "] , we require that there exists a sufficiently small constant @xmath178 such that , with probability approaching one , @xmath179 for all @xmath180 and @xmath181 .",
    "since @xmath182 and @xmath183 by lemma 4.2 in @xcite , it suffices to have that @xmath184 .",
    "now , for any @xmath180 and @xmath181 , @xmath185 , and to ensure that @xmath186 , we need that @xmath166 .",
    "in addition , in order that @xmath168 satisfies @xmath187 , we need that @xmath165 .",
    "the theorem shows that the value of @xmath163 does not affect rates of convergence of @xmath125 , which is perhaps not surprising in view of the definition of @xmath125 .",
    "what is interesting is the fact that @xmath125 with @xmath127 properly chosen is rate optimal , which means that smoothness of @xmath0 in @xmath1 does not affect difficulty of estimating @xmath4 .",
    "this is in sharp contrast with nonparametric estimation of a bivariate regression function .",
    "it should be noted that the results of theorem [ thm : main1 ] continue to hold even if the condition that @xmath188 for all @xmath189 is replaced by a weaker condition that @xmath190 for all @xmath189 for some ( given ) positive sequence @xmath191 such that @xmath192 .",
    "however , the value of @xmath163 does matter for the analysis of the second estimator @xmath126 .",
    "@xcite study prediction based on the estimator @xmath125 .",
    "they prove that , assuming @xmath193 = { \\mathrm{e}}[x(t ) ] = 0 $ ] for all @xmath64 , the estimator @xmath194 with an appropriate choice of the cut - off level @xmath127 attains the minimax rate for estimation of @xmath195 $ ] under the mean integrated squared error ( mise ) .",
    "importantly , the prediction problem considered in @xcite is related to but substantially different from the problem of estimating @xmath4 considered in the present paper ; the former is not an ill - posed inverse problem , and @xcite do not derive sharp rates of convergence for @xmath125 itself and hence do not cover theorem [ thm : main1 ] ( the proof of theorem 2 in @xcite does not lead to the results of our theorem [ thm : main1 ] since from the beginning their proof is bounding @xmath196 $ ] , and @xcite assume a stronger moment condition on @xmath87 ; see ( 6 ) in their paper ) . @xcite and",
    "@xcite analyze the estimator @xmath125 with dependent functional data , but they only prove consistency of @xmath125 and thus do not cover theorem [ thm : main1 ] . precisely speaking , they prove consistency of the integral operator with kernel @xmath125 for the operator norm .",
    "next , we derive rates of convergence for our second estimator .",
    "[ thm : main2 ] consider the estimator @xmath126 defined in ( [ eq : second estimator ] ) .",
    "suppose that assumption [ as : 1 ] is satisfied .",
    "furthermore , suppose that @xmath197 . then provided that @xmath198 , we have @xmath199 therefore , by choosing @xmath200 we have @xmath201    since the estimator @xmath202 depends on @xmath203 , accumulation of these estimation errors contributes to the term @xmath204 in the bound ( [ eq : bound1 ] ) , while the term @xmath205 comes from the bias . because of these terms , @xmath163 appears in the bound ( [ eq : bound2 ] ) , and in contrast to @xmath125 , the second estimator @xmath126 has suboptimal rates in some cases ( of course there could be a room to improve upon the bound ( [ eq : bound1 ] ) ) . still , the estimator @xmath126 is able to attain the optimal rate @xmath206 provided that @xmath207 which actually covers wide regions of @xmath208 .",
    "figure [ fig : parameters ] depicts regions of @xmath209 where @xmath126 attains the rate @xmath210 for different values of @xmath161 .",
    "the red region corresponds to the region @xmath211 , while the blue region corresponds to the region @xmath212 .     for different values of @xmath161 .",
    "when the parameters @xmath209 are contained in the red region @xmath213 or the blue region @xmath214 , the estimator @xmath126 attains the rate @xmath177 .",
    "the gray region corresponds to the region where @xmath215 or @xmath216 .",
    "[ fig : parameters ] ]    [ fig : one ]     for different values of @xmath161 .",
    "when the parameters @xmath209 are contained in the red region @xmath213 or the blue region @xmath214 , the estimator @xmath126 attains the rate @xmath177 .",
    "the gray region corresponds to the region where @xmath215 or @xmath216 .",
    "[ fig : parameters ] ]    [ fig : two ]     for different values of @xmath161 . when the parameters @xmath209 are contained in the red region @xmath213 or the blue region @xmath214 , the estimator @xmath126 attains the rate @xmath177 .",
    "the gray region corresponds to the region where @xmath215 or @xmath216 .",
    "[ fig : parameters ] ]    [ fig : three ]     for different values of @xmath161 . when the parameters @xmath209 are contained in the red region @xmath213 or the blue region @xmath214 , the estimator @xmath126 attains the rate @xmath177 .",
    "the gray region corresponds to the region where @xmath215 or @xmath216 .",
    "[ fig : parameters ] ]    [ fig : four ]     for different values of @xmath161 .",
    "when the parameters @xmath209 are contained in the red region @xmath213 or the blue region @xmath214 , the estimator @xmath126 attains the rate @xmath177 .",
    "the gray region corresponds to the region where @xmath215 or @xmath216 .",
    "[ fig : parameters ] ]    [ fig : five ]     for different values of @xmath161 .",
    "when the parameters @xmath209 are contained in the red region @xmath213 or the blue region @xmath214 , the estimator @xmath126 attains the rate @xmath177 .",
    "the gray region corresponds to the region where @xmath215 or @xmath216 .",
    "[ fig : parameters ] ]    [ fig : six ]    @xcite consider an estimator for @xmath4 that is related to but still different from our second estimator @xmath126 . their estimator is based on applying the functional pca to both @xmath9 and @xmath24 .",
    "let @xmath217 be the covariance function of @xmath24 , and let @xmath218 be the spectral expansion of @xmath219 where @xmath220 and @xmath221 is an orthonormal basis of @xmath6 ( we assume here that this expansion is possible )",
    ". then @xmath222 + \\sum_{j } \\zeta_{j}\\psi_{j}(s)$ ] where @xmath223 \\ } \\psi_{j}(s)dt$ ] , and observe that @xmath4 can be expanded in @xmath47 as @xmath224 the method of estimation of @xmath4 in @xcite is to approximate the infinite series @xmath225 by a finite series , and replace @xmath226 , and @xmath23 by their estimators .",
    "however , @xcite do not explicitly derive rates of convergence of this estimator , although it should be noted that @xcite assume that only discrete measurements with measurement errors for @xmath9 and @xmath24 are available .",
    "the analysis of the estimator of @xcite requires a substantially different set of assumptions than ours and thus is not pursued in the present paper .      in this subsection",
    ", we consider minimax lower bounds for estimation of @xmath4 . to this end",
    ", we narrow a class of distributions of @xmath102 and consider the following setting .",
    "let @xmath227 , and @xmath172 be given constants .",
    "let @xmath228 be an @xmath6-valued gaussian random variable such that @xmath229 = 0 $ ] and @xmath230 > 0 $ ] for all @xmath79 with @xmath231 ( recall that an @xmath6-valued random variable @xmath232 is said to be gaussian if @xmath233 is normally distributed for each @xmath79 ) .",
    "let @xmath234 $ ] be the covariance function of @xmath228 , and let @xmath235 be the spectral expansion of @xmath130 where @xmath236 and @xmath237 is an orthonormal basis of @xmath6 .",
    "now , let @xmath238 for @xmath239 being independent uniform random variables on @xmath240 $ ] independent from @xmath228 , and generate , as an @xmath6-valued random variable , @xmath241 where @xmath242 .",
    "since @xmath243 has mean zero and unit variance , we have @xmath244 , and so @xmath245 in addition , @xmath246 , and so @xmath247 = 9 k^{-\\alpha}/5 $ ] .",
    "define @xmath248 as a class of functions for @xmath4 .",
    "[ thm : lower bound ] work with the setting described as above .",
    "then there exists a constant @xmath249 such that @xmath250 where @xmath251 denotes the probability under @xmath4 , and @xmath252 is taken over all estimators @xmath253 of @xmath4 based on @xmath101 , independent copies of @xmath102 .",
    "this theorem shows that , under assumption [ as : 1 ] , the first pca - based estimator @xmath125 with @xmath127 properly chosen is minimax rate optimal , while the second pca - based estimator @xmath126 with @xmath254 and @xmath255 properly chosen is minimax rate optimal provided that the additional restriction ( [ eq : restriction ] ) is satisfied .",
    "in this section , we present simulation results to verify the performance of the estimators in the finite sample .",
    "we consider the following data generating process .",
    "let @xmath256 , and generate @xmath102 as follows : @xmath257 , \\\\ & b = \\sum_{j , k=1}^{50 } b_{j , k } \\phi_{j } \\otimes \\phi_{k } , \\",
    "b_{1,1 } = 0.3 , \\",
    "b_{j , k } = 4(-1)^{j+k } j^{-\\gamma } k^{-\\beta } \\ \\text{for } \\ ( j , k ) \\neq ( 1,1 ) , \\\\ & { \\mathcal{e}}= \\sum_{j=1}^{50 } j^{-1.1/2 } z_{j } \\phi_{j } , \\",
    "z_{j } \\sim n(0,1 ) , \\end{aligned}\\ ] ] where @xmath243 s and @xmath258 s are all independent , and the following sample sizes for @xmath259 are examined : @xmath260 .",
    "we consider the following configurations for @xmath261 : @xmath262 which verify the restriction ( [ eq : restriction ] ) .",
    "the number of repetitions for each simulation is @xmath263 .",
    "the numerical results obtained in this section were carried out by using the matrix language ox @xcite .    in this experiment , we simulate values of the mise of @xmath125 for @xmath264 and @xmath126 for @xmath265 in each case , and report the optimal mise .",
    "the selected values of @xmath266 and @xmath267 in each configuration are reported in figure [ fig : mset ] .",
    "it is observed that 1 ) the values of @xmath254 selected become smaller as @xmath163 increases , 2 ) the values of @xmath255 are less sensitive to @xmath259 than those of @xmath254 , and 3 ) the values of @xmath266 are close to those of @xmath255 .",
    "next , figure [ fig : logn ] plots the values of the log mise against @xmath268 .",
    "it is observed that 1 ) the values of the log mise of @xmath125 are almost identical for different values of @xmath163 ; 2 ) in contrast , the log mise of @xmath126 decreases as @xmath163 increases , but the slope is not sensitive to the value of @xmath163 , which indicates that the rate at which the mise of @xmath126 decreases is independent of @xmath163 , but the constant depends on @xmath163 and decreases as @xmath163 increases ; 3 ) all the slopes are close to @xmath269 , at least for large @xmath259 .",
    "these observations are consistent with our theoretical results . finally , in this limited experiment , the second estimator @xmath126 performs better than the first estimator @xmath125 , especially when @xmath270 ; the difference in the @xmath271 mise is roughly @xmath272 in that case , which means that the mise of @xmath125 is @xmath273 times that of @xmath126 .",
    "( top panel ) and @xmath274 ( middle and bottom panels ) minimizing mise of @xmath125 and @xmath126 for each parameter configuration : @xmath275 ( diamond ) and @xmath276 ( triangle ) , and @xmath277 ( red ) , @xmath278 ( green ) and @xmath279 ( blue ) .",
    "[ fig : mset ] ]     mise against @xmath268 for each parameter configuration : @xmath275 ( left panel ) and @xmath276 ( right panel ) , and @xmath277 ( red ) , @xmath278 ( green ) and @xmath279 ( blue ) .",
    "the circle corresponds to the @xmath271 mise of @xmath125 and the square corresponds to the @xmath271 mise of @xmath126 .",
    "the dashed line has slope @xmath280 .",
    "[ fig : logn ] ]",
    "in what follows , the notation @xmath281 signifies that the left hand side is bounded by the right hand side up to a constant that depends only on @xmath282 .",
    "we begin with some preliminary considerations .",
    "we first note that @xmath125 is invariant with respect to choices of signs of @xmath157 s , and so without loss of generality , we may assume that @xmath283 recall that @xmath284 .",
    "lemma 4.2 in @xcite yields that @xmath285 note that @xmath286 & = { \\mathrm{e}}\\left [ \\left ( \\sum_{k=1}^{\\infty } \\xi_{k}^{2 } \\right)^{2 } \\right ] = \\sum_{k,\\ell=1}^{\\infty } { \\mathrm{e}}[\\xi_{k}^{2}\\xi_{\\ell}^{2 } ] \\leq \\sum_{k,\\ell=1}^{\\infty}\\sqrt{{\\mathrm{e}}[\\xi_{k}^{4 } ] } \\sqrt{{\\mathrm{e}}[\\xi_{\\ell}^{4 } ] } \\\\ & \\lesssim \\left ( \\sum_{k=1}^{\\infty } \\kappa_{k } \\right)^{2 } \\lesssim 1,\\end{aligned}\\ ] ] which ensures that @xmath287 .",
    "define the event @xmath288 it is seen that , since @xmath289 whenever @xmath180 and @xmath181 , and since @xmath290 , we have @xmath291 furthermore , arguing as in @xcite , we have @xmath292 \\lesssim k^{2}/n \\label{eq : eigenfunction bound}\\ ] ] where @xmath293 is uniform in @xmath180 . in",
    "what follows , we will freely use the estimates in ( [ eq : eigenvalue bound ] ) and ( [ eq : eigenfunction bound ] ) . in particular ,",
    "since @xmath294 , we have @xmath295 in what follows , integrations such as @xmath296 and @xmath297 are abbreviated as @xmath298 and @xmath299 .",
    "let @xmath300 $ ] , and expand @xmath301 and @xmath302 as @xmath303 where @xmath304 and @xmath305 .",
    "observe that @xmath125 admits the following alternative representation : @xmath306 where @xmath307 the right hand side on ( [ eq : alternative ] ) converges in @xmath47 .",
    "since @xmath308 is an orthonormal basis of @xmath47 , expand @xmath4 as @xmath309 now , setting @xmath310 and @xmath311 , observe that @xmath312 plugging this expression into @xmath313 together with the facts that @xmath314 we have @xmath315 where the interchange of the orders of summation is guaranteed since @xmath316 converges absolutely with probability one ; in fact @xmath317 therefore , @xmath318 we divide the rest of the proof into three steps .",
    "it is seen that @xmath323 next , since @xmath324 we have on the event @xmath325 , @xmath326 in view of the assumption that @xmath327 , choose @xmath328 and @xmath329 large enough so that @xmath330 } \\leq 1/2 $ ] and @xmath331 + 1}/\\kappa_{k } \\leq 1/2 $ ] for all @xmath332 , where @xmath333 $ ] denotes the largest integer not exceeding @xmath334 .",
    "we may choose @xmath335 and @xmath336 in such a way that they depend only on @xmath161 and @xmath337 .",
    "now , partition the sum @xmath338 into @xmath339 } , \\",
    "\\sum_{\\ell=[k / c]+1,\\neq k}^{[ck ] } , \\ \\sum_{\\ell = [ ck]+1}^{\\infty}.\\ ] ] observe that @xmath340 } \\frac{\\ell^{-\\beta}}{(\\kappa_{\\ell}-\\kappa_{k } ) } & \\leq \\sum_{\\ell=1}^{[k / c ] } \\frac{\\ell^{-\\beta}}{\\kappa_{\\ell}(1-\\kappa_{k}/\\kappa_{[k / c ] } ) } \\lesssim \\sum_{\\ell=1}^{[k / c ] } \\ell^{-\\beta+\\alpha } \\\\ & \\lesssim   \\begin{cases } 1 & \\text{if } \\",
    "\\beta > \\alpha + 1 \\\\ \\log k & \\text{if } \\",
    "\\beta = \\alpha + 1\\\\ k^{\\alpha-\\beta+1 } & \\text{if } \\",
    "\\beta < \\alpha + 1 \\end{cases } , \\end{aligned}\\ ] ] and @xmath341 + 1}^{\\infty}\\frac{\\ell^{-\\beta}}{(\\kappa_{k}-\\kappa_{\\ell } ) } \\leq \\sum_{\\ell = [ ck]+1}^{\\infty } \\frac{\\ell^{-\\beta}}{\\kappa_{k } ( 1-\\kappa_{[ck]+1}/\\kappa_{k } ) } \\lesssim k^{\\alpha } \\sum_{\\ell=[ck]+1}^{\\infty } \\ell^{-\\beta } \\lesssim k^{\\alpha-\\beta+1}.\\ ] ] for @xmath342 < \\ell < k$ ] , observe that @xmath343 likewise , for @xmath344 $ ] , @xmath345 .",
    "hence @xmath346 + 1,\\neq k}^{[ck ] } \\frac{\\ell^{-\\beta}}{|\\kappa_{\\ell}-\\kappa_{k}| } \\lesssim k^{\\alpha+1 } \\sum_{\\ell=[k / c]+1,\\neq k}^{[ck ] } \\frac{\\ell^{-\\beta}}{|j-\\ell| } \\lesssim k^{\\alpha-\\beta+1 } \\log k.\\ ] ] this yields that @xmath347 and so on the event @xmath325 , @xmath348    turning to @xmath349 , observe that for each @xmath181 , @xmath350 which yields that @xmath351 $ ] is @xmath352    + { \\mathrm{e}}\\left [ \\overline{\\xi}_{k}^{2 } \\left ( \\sum_{\\ell : \\ell \\neq k }   \\frac{b_{j,\\ell}}{\\kappa_{k}-\\kappa_{\\ell}}\\overline{\\xi}_{\\ell } \\right ) ^{2 } \\right ] .",
    "\\label{eq : t1}\\ ] ] the first term on the right hand side is bounded by @xmath353 } \\left \\ { { \\mathrm{e}}\\left [ \\left ( \\sum_{\\ell : \\ell \\neq k } \\frac{b_{j,\\ell } \\xi_{\\ell } } { \\kappa_{k}-\\kappa_{\\ell}}\\right ) ^{4 } \\right ] \\right \\}^{1/2},\\ ] ] where @xmath354 } \\lesssim k^{-\\alpha}$ ] .",
    "now , observe that @xmath355 \\\\ & \\quad \\leq \\sum_{\\ell_{1 } : \\ell_{1 } \\neq k } \\cdots \\sum_{\\ell_{4 } : \\ell_{4}\\neq k } \\left | \\frac{b_{j,\\ell_{1 } } } { \\kappa_{k}-\\kappa_{\\ell_{1 } } } \\right | \\cdots \\left | \\frac{b_{j,\\ell_{4}}}{\\kappa_{k}-\\kappa_{\\ell_{4 } } } \\right | { \\mathrm{e } } [ | \\xi_{\\ell_{1 } } \\cdots",
    "\\xi_{\\ell_{4 } } | ] \\\\ & \\quad \\lesssim j^{-4\\gamma } \\sum_{\\ell_{1 } : \\ell_{1 } \\neq k } \\cdots \\sum_{\\ell_{4 } : \\ell_{4}\\neq k } \\frac{\\ell_{1}^{-\\beta}}{|\\kappa_{k}-\\kappa_{\\ell_{1}}| } \\cdots \\frac{\\ell_{4}^{-\\beta}}{|\\kappa_{k}-\\kappa_{\\ell_{4}}| } { \\mathrm{e } } [ | \\xi_{\\ell_{1 } } \\cdots \\xi_{\\ell_{4 } } | ] \\end{aligned}\\ ] ] and a repeated application of hlder s inequality yields that @xmath356 \\leq ( { \\mathrm{e } } [ \\xi_{\\ell_{1}}^{4}])^{1/4 } \\cdots   ( { \\mathrm{e } } [ \\xi_{\\ell_{4}}^{4}])^{1/4 } \\lesssim \\ell_{1}^{-\\alpha/2 } \\cdots \\ell_{4}^{-\\alpha/2}.\\ ] ] hence the first term on the right hand side of ( [ eq : t1 ] ) is @xmath357 where the last inequality follows from a similar estimate to ( [ eq : delicate bound ] ) together with the assumption that @xmath165 .",
    "using a similar argument to bound the second term on the right hand side of ( [ eq : t1 ] ) , we conclude that @xmath358 \\lesssim n^{-1 } j^{-2\\gamma } k^{-\\alpha}.\\ ] ]    finally , we shall bound @xmath359 . to this end , observe that , on the event @xmath325 , @xmath360 and @xmath361   & \\leq \\left ( \\sum_{\\ell : \\ell \\neq k } \\frac{\\ell^{-\\beta}}{|\\kappa_{k}-\\kappa_{\\ell}|^{2 } } \\sqrt{{\\mathrm{e}}[{\\widehat}{v}_{k,\\ell}^{2 } ] } \\right ) ^{2 } \\\\ & \\lesssim n^{-1 } k^{-\\alpha } \\left ( \\sum_{\\ell : \\ell \\neq k } \\frac{\\ell^{-\\beta-\\alpha/2}}{|\\kappa_{k}-\\kappa_{\\ell}|^{2 } }   \\right ) ^{2 } \\\\ & \\lesssim n^{-1 } ( k^{-\\alpha } + k^{2\\alpha - 2\\beta + 4}),\\end{aligned}\\ ] ] where we have used the following calculation : @xmath362 } + \\sum_{\\ell = [ k / c]+1 , \\neq k}^{[ck ] } + \\sum_{\\ell=[ck]+1}^{\\infty } \\right ) \\frac{\\ell^{-\\beta-\\alpha/2}}{|\\kappa_{k}-\\kappa_{\\ell}|^{2 } } \\\\ & \\quad \\lesssim \\sum_{\\ell=1}^{[k / c ] } \\ell^{3\\alpha/2 - \\gamma } + k^{2\\alpha+2 } \\sum_{\\ell = [ k / c]+1,\\neq k}^{[ck ] } \\frac{\\ell^{-\\beta-\\alpha/2}}{|k-\\ell|^{2 } } + k^{2\\alpha } \\sum_{\\ell=[ck]+1}^{\\infty } \\ell^{-\\beta-\\alpha/2 } \\\\",
    "& \\quad \\lesssim   1 + k^{3\\alpha/2 - \\gamma + 1 } \\log k +   k^{3\\alpha/2 - \\beta + 2 } + k^{3\\alpha/2-\\beta+1 } \\\\ & \\quad \\lesssim 1 + k^{3\\alpha/2 - \\gamma + 2}. \\end{aligned}\\ ] ]    summarizing , using ( [ eq : eigenvalue bound ] ) and ( [ eq : eigenfunction bound ] ) , we have @xmath363.\\end{aligned}\\ ] ] since @xmath166 , @xmath364 , so that the the last expression is @xmath365 .",
    "furthermore , @xmath366 since @xmath165 .",
    "hence we conclude that @xmath367      now , since conditionally on @xmath371 , @xmath372 are independent with mean zero , we have @xmath373 = \\frac{1}{n^{2 } } \\sum_{i=1}^{n } { \\mathrm{e}}[\\varepsilon_{i , j}^{2 } \\mid x_{1}^{n } ] { \\widehat}{\\xi}_{i , k}^{2}.\\ ] ] further , since by the monotone convergence theorem for conditional expectation and bessel s inequality , @xmath374 = { \\mathrm{e}}\\left [ \\sum_{j=1}^{\\infty } \\varepsilon_{i , j}^{2 } \\ \\bigg| \\",
    "x_{1}^{n } \\right ] \\leq { \\mathrm{e } } [ \\| { \\mathcal{e}}_{i } \\|^{2 } \\mid x_{1}^{n } ] = { \\mathrm{e}}\\| { \\mathcal{e}}_{i } \\|^{2 } \\mid x_{i } ] \\leq c_{1},\\ ] ] we have @xmath375   \\lesssim n^{-1 } \\sum_{k=1}^{m_{n } } { \\widehat}{\\kappa}_{k}^{-1 } \\\\ & \\quad \\leq n^{-1 } \\ { 1+o_{{\\mathrm{p}}}(1 ) \\ } \\sum_{k=1}^{m_{n } } \\kappa_{k}^{-1 } = o_{{\\mathrm{p } } } ( n^{-1 } m_{n}^{\\alpha+1 } ) .",
    "\\end{aligned}\\ ] ] this yields that @xmath376      * step 3*. conclusion .",
    "recall that @xmath378 , and observe that @xmath379 where @xmath380 .",
    "so @xmath381 now , observe that , using parseval s identity , @xmath382 this completes the proof for the first assertion .",
    "the second assertion follows directly from the first assertion .",
    "the proof is parallel to that of theorem [ thm : main1 ] .",
    "we freely use the results in the proof of theorem [ thm : main1 ] .",
    "since @xmath126 is invariant with respect to choices of signs of @xmath157 s , it is without loss of generality to assume ( [ eq : sign of phi ] ) .",
    "let @xmath383 , and define the event @xmath384 for which we have @xmath385 since @xmath386 .",
    "observe that @xmath395 step 1 in the proof of theorem [ thm : main1 ] shows that @xmath396 , and likewise we have @xmath397 .",
    "furthermore , using ( [ eq : difference ] ) , observe that @xmath398 where @xmath399 for each @xmath400 , on the event @xmath401 , @xmath402 which yields that on the event @xmath401 , @xmath403 therefore , we have @xmath404.\\end{aligned}\\ ] ] since @xmath165 and @xmath405 , the last expression is @xmath406 .",
    "so we conclude that @xmath407    next , since conditionally on @xmath371 , @xmath408 are independent with mean zero , we have @xmath409 = \\frac{1}{n^{2 } } \\sum_{i=1}^{n } { \\mathrm{e}}[{\\widehat}{\\varepsilon}_{i , j}^{2 } \\mid x_{1}^{n } ] { \\widehat}{\\xi}_{i , k}^{2}.\\ ] ] further , since by bessel s inequality , @xmath410 \\leq { \\mathrm{e}}\\left [ \\sum_{j=1}^{m_{n,1 } } { \\widehat}{\\varepsilon}_{i , j}^{2 } \\ \\bigg|",
    "\\ x_{1}^{n } \\right ] \\leq { \\mathrm{e } } [ \\| { \\mathcal{e}}_{i } \\|^{2 } \\mid x_{1}^{n } ] = { \\mathrm{e}}\\| { \\mathcal{e}}_{i } \\|^{2 } \\mid x_{i } ] \\leq c_{1},\\ ] ] we have , using the fact that @xmath411 , @xmath412   \\lesssim n^{-1 } \\sum_{k=1}^{m_{n,2 } } { \\widehat}{\\kappa}_{k}^{-1 } \\\\ & \\quad \\leq n^{-1 } \\ { 1+o_{{\\mathrm{p}}}(1 ) \\ } \\sum_{k=1}^{m_{n,2 } } \\kappa_{k}^{-1 } = o_{{\\mathrm{p } } } ( n^{-1 } m_{n,2}^{\\alpha+1 } ) .",
    "\\end{aligned}\\ ] ] this yields that @xmath413      recall that @xmath415 , and observe that @xmath416 where @xmath417 .",
    "so @xmath418 using the decomposition @xmath419 we have @xmath420 observe that @xmath421 likewise , we have @xmath422 finally , we have @xmath423          for any @xmath242 and @xmath425 , let @xmath426 denote the distribution of @xmath427 , and let @xmath428 denote the distribution of @xmath228 .",
    "those distributions are defined on the borel @xmath69-field of @xmath6 .",
    "associated to @xmath228 , the cameron - martin space is given by @xmath429 equipped with the inner product @xmath430 let @xmath431 and @xmath432 ; then @xmath426 is absolutely continuous with respect to @xmath428 if and only if @xmath433 and its radon - nikodym derivative is given by the cameron - martin formula @xmath434 where @xmath435 .",
    "see theorem 8.2.9 in @xcite .",
    "denote by @xmath436 the distribution of @xmath9 ; then the joint distribution of @xmath102 is given by @xmath437 .",
    "for any @xmath454 , let @xmath455 denote the hamming distance .",
    "then we have @xmath456 for any @xmath457 and any constant @xmath178 , where @xmath458 denotes the probability under @xmath459 .",
    "to lower bound the right hand side , we calculate the kullback - leibler divergence @xmath460 for any @xmath454 with @xmath461 .",
    "suppose that @xmath462 for some @xmath463 and @xmath464 for all @xmath181 .",
    "then a straightforward calculation shows that @xmath465 = \\frac{(\\nu_{n}+k)^{-\\alpha-2\\beta}}{2\\lambda_{1 } } \\leq \\frac{(\\nu_{n}+1)^{-\\alpha-2\\beta}}{2\\lambda_{1 } } \\leq \\frac{1}{2\\lambda_{1}n},\\ ] ] which yields that @xmath466 now , applying assouad s lemma and theorem 2.12 in @xcite , we have @xmath467 \\geq \\frac{\\nu_{n}}{4 } e^{-1/(2\\lambda_{1})},\\ ] ] where @xmath468 denotes the expectation under @xmath459 .",
    "choose @xmath457 at which the maximum on the left hand side is attained , and observe that @xmath469 .",
    "the paley - zygmund inequality then yields that @xmath470 \\right \\ } \\\\ & \\geq \\frac{1}{4 } \\frac{({\\mathrm{e}}_{\\theta}[\\rho ( \\overline{\\theta}^{n},\\theta)])^{2}}{{\\mathrm{e}}[\\rho ( \\overline{\\theta}^{n},\\theta)^{2 } ] } \\\\ & \\geq \\frac{1}{16 } e^{-1/(2\\lambda_{1})}.\\end{aligned}\\ ] ] therefore @xmath471 since @xmath472 , the proof is completed .",
    "99 benatia , d. , carrasco , m. , and florens , j .-",
    "functional linear regression with functional response .",
    "bosq , d. ( 2000 ) . _ linear processes in function spaces : theory and applications_. springer .",
    "cai , t.t . and hall , p. ( 2006 ) . prediction in functional linear regression .",
    "statist . _",
    "* 34 * 2159 - 2179 .",
    "cai , t.t . and yuan , m. ( 2012 ) .",
    "minimax and adaptive prediction for functional linear regression .",
    "_ j. amer .",
    "assoc . _ * 107 * 1201 - 1216 .",
    "cardot , h. and johannes , j. ( 2010 ) . thresholding projection estimators in functional linear models",
    ". _ j. multivariate anal . _",
    "* 101 * 395 - 408 .",
    "cardot , h. , ferraty , f. , and sarda , p. ( 1999 ) . functional linear model .",
    "_ statist .",
    "_ * 45 * 11 - 22 .",
    "cardot , h. , ferraty , f. , and sarda , p. ( 2003 ) .",
    "spline estimators for the functional linear models .",
    "_ statist .",
    "* 13 * 571 - 591 .",
    "cuevas , a. , febrero , m. , and fraiman , r. ( 2002 ) .",
    "linear functional regression : the case of fixed design and functional response . _",
    "canadian j. statist . _ * 30 * 285 - 300 .",
    "chiou , j.m . , mller , h.g . , and wang , j.l .",
    "( 2004 ) . functional response models . _ statist .",
    "* 14 * 675 - 693 .",
    "comte , f. and johannes , j. ( 2012 ) .",
    "adaptive functional linear regression .",
    "_ * 40 * 2765 - 2797 .",
    "crambes , c. , kneip , a. , and sarda , p. ( 2009 ) . smoothing splines estimators for functional linear regression .",
    "statist . _",
    "* 37 * 35 - 72 .",
    "crambes , c. and mas , a. ( 2013 ) .",
    "asymptotics of prediction in functional linear regression with functional outputs .",
    "_ bernoulli _ * 19 * 2627 - 2651 .",
    "delaigle , a. and hall , p. ( 2012 ) .",
    "methodology and theory for partial least squares applied to functional data .",
    "_ * 40 * 322 - 352 .",
    "doornik , j.a .",
    ". _ object - oriented matrix programming using ox ( 3rd edition ) . _ timberlake consultants press .",
    "hall , p. and horowitz , j.l .",
    "methodology and convergence rates for functional linear regression .",
    "statist . _",
    "* 35 * 70 - 91 .",
    "he , g. , mller , h .-",
    "g . , wang , j .- l . , and yang , w. ( 2010 ) .",
    "functional linear regression via canonical analysis .",
    "_ bernoulli _ * 16 * 705 - 729 .",
    "hrmann , s. and kidznski ,  .",
    "( 2015 ) . a note on estimation in hilbertian linear models .",
    ". j. statist . _ * 42 * 43 - 62 .",
    "hsing , t. , and eubank , r. ( 2015 ) .",
    "_ theoretical foundations of functional data analysis with an introduction to linear operators_. wiley .",
    "james , g.m . ,",
    "wang , j. , and zhu , j. ( 2009 ) .",
    "functional linear regression that s interpretable .",
    "statist . _",
    "* 37 * 2083 - 2108 .",
    "kress , r. ( 1999 ) .",
    "_ linear integral equations ( 2nd edition)_. springer .",
    "li , y. and hsing , t. ( 2007 ) . on rates of convergence in functional linear regression .",
    "_ j. multivariate anal . _ * 98 * 1782 - 1804 .",
    "lian , h. ( 2015 ) .",
    "minimax prediction for functional linear regression with functional responses in reproducing kernel hilbert spaces .",
    "_ j. multivariate anal . _ * 140 * 395 - 402 .",
    "meister , a. ( 2011 ) .",
    "asymptotic equivalence of functional linear regression with a white noise inverse problem .",
    "statist . _",
    "* 39 * 1471 - 1495 .",
    "park , j .- y . and qian , j. ( 2012 ) .",
    "functional regression of continuous state distributions",
    ". _ j. econometrics _ * 167 * 397 - 412 .",
    "rajput , b.s .",
    "gaussian measures on @xmath473 spaces , @xmath474 .",
    "_ j. multivariate anal . _",
    "* 2 * 382 - 403 .",
    "ramsay , j. o. and dalzell , c. j. ( 1991 ) . some tools for functional data analysis",
    ". _ j. r. stat .",
    "methodol . _",
    "* 53 * 539 - 572 .",
    "ramsay , j. o. and silverman , b. w. ( 2005 ) .",
    "_ functional data analysis_. 2nd edition .",
    "reed , m. and simon , b. ( 1980 ) .",
    "_ methods of modern mathematical physics i : functional analysis ( revised and enlarged edition)_. academic press .",
    "stroock , d.w .",
    "_ probability theory : an analytic view_. 2nd edition . cambridge university press .",
    "tsybakov , a.b .",
    "_ introduction to nonparametric estimation_. springer .",
    "yao , f. , mller , h .-",
    "g . , and wang , j .- l .",
    "( 2005 ) . functional linear regression analysis for longitudinal data .",
    "statist . _",
    "* 33 * 2873 - 2903 .",
    "yuan , m. and cai , t. ( 2010 ) . a reproducing kernel hilbert space approach to functional linear regression .",
    "statist . _",
    "* 38 * 3412 - 3444 ."
  ],
  "abstract_text": [
    "<S> this paper studies a regression model where both predictor and response variables are random functions . we consider a functional linear model where the conditional mean of the response variable at each time point is given by a linear functional of the predictor variable . </S>",
    "<S> the problem is then estimation of the integral kernel @xmath0 of the conditional expectation operator , where @xmath1 is an output variable while @xmath2 is a variable that interacts with the predictor variable . </S>",
    "<S> this problem is an ill - posed inverse problem , and we consider estimators based on the functional principal component analysis ( pca ) . </S>",
    "<S> we show that under suitable regularity conditions , an estimator based on single truncation attains the convergence rate for the integrated squared error that is characterized by smoothness of the function @xmath3 in @xmath2 together with the the decay rate of the eigenvalues of the covariance operator , but the rate does not depend on smoothness of @xmath0 in @xmath1 . </S>",
    "<S> this rate is shown to be minimax optimal , and consequently smoothness of @xmath0 in @xmath1 does not affect difficulty of estimating @xmath4 . </S>",
    "<S> we also consider an alternative estimator based on double truncation , and provide conditions under which the alternative estimator attains the optimal rate . </S>",
    "<S> we conduct simulations to verify the performance of pca - based estimators in the finite sample . </S>"
  ]
}