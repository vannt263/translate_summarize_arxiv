{
  "article_text": [
    "]    the goal of this work is modeling the statistics of images and associated textual data in large - scale internet photo collections in order to enable a variety of retrieval scenarios : similarity - based image search , keyword - based image search , and automatic image annotation ( figure [ fig : retrieval - scenarios ] ) .",
    "practical models for these tasks must meet several requirements .",
    "first , they must be _ accurate _ , which is a big challenge given that the imagery is extremely heterogeneous and user - provided annotations are noisy .",
    "second , they must be _ scalable _ to millions of images .",
    "third , they must be _ flexible _ , accommodating _ cross - modal _ retrieval tasks such as tag - to - image or image - to - tag search in the same framework , and enabling , for example , tag - based search of images without any tags .",
    "several promising recent approaches for modeling images and associated text  @xcite rely on _ canonical correlation analysis _ ( cca ) , a classic technique that maps two views , given by visual and and textual features , into a common latent space where the correlation between the two views is maximized  @xcite .",
    "this space is _ cross - modal _ , in the sense that embedded vectors representing visual and textual information are treated as the same class of citizens , and thus image - to - image , text - to - image , and image - to - text retrieval tasks can in principle all be handled in exactly the same way .",
    "while cca is very attractive in its simplicity and flexibility , existing cca - based approaches have several shortcomings .",
    "in particular , the works cited above use classic two - view cca , which only considers the direct correlations between images and corresponding textual feature vectors . however , as we will show in this paper , significant improvements can be obtained by considering a third view with which the first two are correlated  that of the underlying semantics of the image .    in this work ,",
    "we use the term `` semantics '' to refer to high - level labels or topics that characterize the content of an image for the sake of a given application . for concrete examples , consider figure [ data ] , which illustrates the three - view datasets used in our experiments . in these datasets ,",
    "the semantic view of an image consists of one or more ground - truth _ keywords_. even though our approach does not rely on a probabilistic generative model , we can think of the other two views , i.e. , _ visual features _ and _ tags / text _ , as being stochastically generated based on the keywords . in particular , the tags tend to come from a larger vocabulary than the keywords and they tend to be noisier . as in figure [ data ] ( a ) , the semantics of an image may be given by a single object category ( `` deer '' ) , while the user - provided tags may include a number of additional terms correlated with that category ( `` keywest , florida , tropics , fauna , wedding '' etc . ) .",
    "alternatively , the semantics might be given by multiple keywords corresponding to objects , scene types , or attributes .",
    "thus , as in figure [ data ] ( b ) , an image may be annotated by multiple ground - truth keywords `` buildings , sky , tower , water '' and tags `` sunset , night , colors , pink , tower , shot , kuwait , stunning . '' or , as in figure [ data ] ( c ) , the semantics may be given by the name of a logo or landmark , and the text may be taken from the surrounding webpage , and may or may not explicitly mention the ground - truth keyword .    in this paper , we present a three - view cca model that explicitly incorporates the high - level semantic information as a third view . the difference between the standard two - view cca and our proposed three - view embedding is visualized in figure [ 2d ] .",
    "in the two - view embedding space ( figure [ 2d ] ( a ) ) , which is produced by maximizing the correlations between visual features and the corresponding tag features , images from different classes are very mixed . on the other hand ,",
    "the three - view embedding ( figure [ 2d ] ( b ) ) provides a much better separation between the classes .",
    "as our experiments will confirm , a third semantic view  which may be derived from a variety of sources  is capable of considerably increasing the accuracy of retrieval on very diverse datasets .    in all the examples of figure [ data ] ,",
    "the ground - truth semantic keywords are defined ahead of time and accurately annotated for the express purpose of training recognition algorithms .",
    "however , in most realistic situations , it is easy to gather noisy text and tags , but not so easy to get at the underlying semantics .",
    "fortunately , we will show that even in cases when clean ground - truth annotation for the third view is unavailable , it is still possible to learn a better embedding for the photo collection by representing the semantics explicitly . in some cases , we can get an informative additional signal from search keywords .",
    "for example , if we retrieve a number of images together with their tags from flickr using a search for `` frog , '' then knowing the original search keyword gives us additional modeling power even if many of these images do not actually depict frogs .",
    "furthermore , if ground truth category or search keyword information is absent completely , we will demonstrate that an effective third view can be derived in an unsupervised way by clustering the noisy tag vectors constituting the second view .",
    "this approach is inspired by cluster - based information retrieval  @xcite and the `` cluster assumption '' in semi - supervised learning @xcite . in effect",
    ", the tag clustering can be thought of as `` reconstructing '' or `` recovering '' the absent topics or distinct types of image content .    to obtain high retrieval accuracy ,",
    "most modern methods have found it necessary to combine multiple high - dimensional visual features , each of which may come with a different similarity or kernel function .",
    "retrieval approaches of  @xcite accomplish this combination using nonlinear kernel cca ( kcca )  @xcite , but the standard kcca formulation scales _ cubically _ in the number of images in the dataset . instead of kcca",
    ", we use a scalable approximation scheme based on efficient explicit kernel mapping followed by linear dimensionality reduction and linear cca .",
    "finally , we specifically design a similarity function suitable for our learned latent embedding , and show that it achieves significant improvement over the euclidean distance .",
    "experiments on the three large - scale datasets of figure [ data ] show the promise of the proposed approach .",
    "the following is a preview of the structure and main contributions of this paper :    * a novel _ three - view _ cca framework that explicitly incorporates the dependence of visual features and text on the underlying image semantics ( section [ sec : three_view_cca ] ) . * a similarity function specially adapted to cca that improves the accuracy of retrieval in the embedded space ( section [ metric ] ) . *",
    "scalable yet discriminative representations for the visual and textual views based on multiple feature combination , explicit kernel mappings , and linear dimensionality reduction ( sections [ sec : visual_kernel ] and [ sec : tag_approximation ] ) . * two methods for instantiating the third semantic view : _ supervised _ , or derived from ground - truth annotations by unsupervised clustering ; and _ unsupervised _ , or derived by clustering the tag vectors from the second ( textual ) view . in both cases ,",
    "our experiments confirm that adding the third view helps to improve retrieval accuracy .",
    "for the unsupervised case , we perform a comparative evaluation of several tag clustering methods from the literature ( section [ topics ] ) .",
    "* extensive evaluation of the proposed three - view models on three tasks  image - to - image , tag - to - image , and image - to - tag search .",
    "section [ dataset ] will give an overview of our experimental protocol , and sections [ sec : evaluation]-[sec : evaluation3 ] will present results on the three large - scale datasets introduced in figure [ data ] .",
    "in the vision and multimedia communities , jointly modeling images and text has been an active research area .",
    "this section gives a non - exhaustive survey of several important lines of research related to our work .",
    "some of the earliest research on images and text  @xcite has focused on learning the co - occurrences between image regions and tags using a generative model .",
    "since most datasets used for training such models lack image annotation at the region level , learning to associate tags with image regions is a very challenging problem , especially for contaminated internet photo collections with very large tag vocabularies .",
    "moreover , image tags frequently refer to global properties or characteristics that can not be easily localized .",
    "therefore , we focus on establishing relationships between whole images and words .    conceptually , our three - view formulation may be compared to the generative model that attempts to capture the relationships between the image class , annotation tags , and image features .",
    "one example of such a model in the literature is @xcite . unlike  @xcite ,",
    "though , we do not concern ourselves with the exact generative nature of the dependencies between the three views , but simply assign symmetric roles to them and model the pairwise correlations between them .",
    "also , while  @xcite tie annotation tags to image regions following  @xcite , we treat both the image appearance and all the tags assigned to the image as global feature vectors .",
    "this allows for much more scalable learning and inference ( the approach of  @xcite is only tested on datasets of under 2,000 images and eight classes each ) .",
    "the major goal of our work is learning a joint latent space for images and tags , in which corresponding images and tags are mapped to nearby locations , so that simple nearest - neighbor methods can be used to perform cross - modal tasks , including image - to - image , tag - to - image , and image - to - tag search . a number of successful recent approaches to learning such an embedding rely on canonical correlation analysis ( cca )  @xcite .",
    "@xcite and @xcite have applied cca to map images and text to the same space for cross - modal retrieval tasks . @xcite",
    "have presented a cross - modal retrieval approach that models the relative importance of words based on the order in which they appear in user - provided annotations .",
    "@xcite have used kcca to develop a cross - view spectral clustering approach that can be applied to images and associated text .",
    "cca embeddings have also been used in other domains , such as cross - language retrieval  @xcite .",
    "unlike all the other cca - based image retrieval and annotation approaches , ours adds a third view that explicitly represents the latent image semantics .",
    "our approach also has connections to supervised _ multi - view learning _ , in which images are characterized by visual and textual views , both of which are linked to the underlying semantic labels .",
    "the literature contains a number of sophisticated methods for multi - view learning , including generalizations of cca / kcca  @xcite , metric learning  @xcite and large - margin formulations  @xcite .",
    "fortunately , we have found that our basic cca formulation already gives very promising results without having to pay the price of increased complexity for learning and inference .",
    "since learning a projection for the data is equivalent to learning a mahalanobis metric in the original feature space , our work is related to _",
    "metric learning _  @xcite .",
    "for example , the large - margin nearest neighbor ( lmnn ) approach @xcite learns a distance metric that is optimized for nearest neighbor classification , and neighborhood component analysis ( nca )  @xcite optimizes leave - one - out loss for nearest neighbor classification .",
    "metric learning has been used for image classification and annotation  @xcite .",
    "however , all of these approaches learn an embedding or a metric for visual features only , so they can not be used to perform cross - modal retrieval .",
    "the two main tasks we use for evaluating our system are image - to - image search , which has been traditionally studied as _ content - based image retrieval _",
    "@xcite , and tag - to - image search , or image retrieval using text - based queries  @xcite . a task related to tag - to - image search , though one we do not consider directly , is re - ranking of contaminated image search results for the purpose of dataset collection  @xcite .",
    "the third task we are interested in evaluating is image - to - tag search or automatic image annotation  @xcite .",
    "this task has traditionally been addressed with the help of sophisticated generative models such as  @xcite .",
    "more recently , a number of publications have reported better results with simple data - driven schemes based on retrieving database images similar to a query and transferring the annotations from those images  @xcite .",
    "we will adopt this strategy in our experiments and demonstrate that retrieving similar images in our embedded latent space can improve the accuracy of tag transfer .",
    "the data - driven image annotation approaches of  @xcite use discriminative learning to obtain a metric or a weighting of different features to improve the relevance of database images retrieved for a query .",
    "unfortunately , the learning stage is very computationally expensive  for example , in the tagprop method of  @xcite , it scales quadratically with the number of images . in fact , the standard datasets used for image annotation by  @xcite consist of 5k-20k images and have 260 - 290 tags each.by contrast , our datasets ( shown in figure [ data ] ) range in size from 71k to 270k and have tag vocabularies of size 1k-20k .",
    "while it is possible to develop scalable metric learning algorithms using stochastic gradient descent ( e.g. ,  @xcite ) , our work shows that learning a linear embedding using cca can serve as a simpler attractive alternative .",
    "perhaps the largest - scale image annotation system in the literature is the _ wsabie _ ( web scale annotation by image embedding ) system by @xcite .",
    "it uses stochastic gradient descent to optimize a ranking objective function and is evaluated on datasets with ten million training examples . like our approach , wsabie learns a common embedding for visual and tag features .",
    "unlike ours , however , it has only a two - view model and thus does not explicitly represent the distinction between the tags used to describe the image and the underlying image content .",
    "also , wsabie is not explicitly designed for multi - label annotation , and evaluated on datasets whose images come with single labels ( or single paths in a label hierarchy ) .",
    "one of the shortcomings of data - driven annotation approaches  @xcite as well as wsabie is that they not account for co - occurrence and mutual exclusion constraints between different tags for the same image .",
    "if the retrieved nearest neighbors of an image belong to incompatible semantic categories ( e.g. , `` bird '' and `` plane '' ) , then the tags transferred from them to the query may be incoherent as well ( see figure [ tagging ] ( a ) for an example ) . to better exploit constraints between multiple tags , it is possible to treat image annotation as a multi - label classification problem  @xcite . in the present work ,",
    "we limit ourselves to learning the joint visual - textual embedding .",
    "it would be interesting to impose multi - label prediction constraints in the joint latent space  in fact , @xcite have recently proposed an approach combining cca with multi - label decoding  but doing so is outside the scope of our paper .",
    "finally , our work has connections to approaches that use internet images and accompanying text as auxiliary training data to improve performance on tasks such as image classification , for which cleanly labeled training data may be scarce  @xcite .",
    "in particular , @xcite use the multi - task learning framework of @xcite to learn a discriminative latent space from web images and associated captions .",
    "we will use this embedding method as one of our baselines , though , unlike our approach , it can only be applied to images , not to tag vectors . apart from multi - task learning , another popular way to obtain an intermediate embedding space for images is by mapping them to outputs of a bank of concept or attribute classifiers  @xcite . once again , unlike our method , this produces an embedding for images only ; also , training of a large number of concept classifiers tends to require more supervision and be more computationally intensive than training of a cca model .",
    "in this section , we introduce a three - view kernel cca formulation for learning a joint space for visual , textual , and semantic information . then we show how to obtain a scalable approximation using explicit kernel embeddings and linear cca .",
    "we assume we have @xmath0 training images each of which is associated with a @xmath1-dimensional visual feature vector and a @xmath2-dimensional tag feature vector ( our specific feature representations for both views will be discussed in section [ approximation ] ) .",
    "the respective vectors are stacked as rows in matrices @xmath3 and @xmath4 .",
    "in addition , each training image is also associated with semantic class or topic information , which is encoded in a matrix @xmath5 , where @xmath6 is the number of classes or topics .",
    "each image may be labeled with exactly one of the @xmath6 classes ( in which case only one entry in each row of @xmath7 is 1 and the rest are 0 ) ; alternatively , each image may be described by several of the @xmath6 keywords ( in which case , multiple entries in each row of @xmath7 may be 1 ) .",
    "another possibility is that @xmath7 is a soft indication matrix , where the @xmath8th entry indicates the degree ( or posterior probability ) with which image @xmath9 belongs to the @xmath10th class or topic . in the supervised learning scenario , @xmath7",
    "is obtained from ( possibly noisy ) annotations that come with the training data . in the unsupervised scenario ( where only images and tags are initially given ) ,",
    "@xmath7 is `` latent '' and must be obtained by clustering the tags , as will be discussed in section [ topics ] . to simplify the notation in the following , we will also use @xmath11 to denote @xmath12 respectively .",
    "let @xmath13 denote two points from the @xmath9th view .",
    "the similarity between these points is defined by a kernel function @xmath14 such that @xmath15 , where @xmath16 is a function embedding the original feature vector into a nonlinear kernel space .",
    "practical kernel - based learning schemes do not work in the embedded space directly , relying on the kernel function instead .",
    "however , we will formulate kcca as solving for a linear projection from the kernel space , because this leads directly to our scalable approximation scheme based on explicit embeddings .    in kcca",
    ", we want to find matrices @xmath17 that project the embedded vectors @xmath18 from each view into a low - dimen - sional common space such that the distances in the resulting space between each pair of views for the same data item are minimized .",
    "the objective function for this formulation is given by @xmath19 where @xmath20 is the covariance matrix between @xmath21 and @xmath22 , and @xmath23 is the @xmath24th column of @xmath17 ( the number of columns in each @xmath17 is equal to the dimensionality of the resulting common space ) . to better understand this objective function ,",
    "let us consider its three terms : @xmath25 the first term tries to align corresponding images and tags , and it is the sole term in the standard two - view cca objective  @xcite .",
    "the remaining two terms , which are introduced in our three - view model , try to align images ( resp .",
    "tags ) with their semantic topic .",
    "figure [ toy ] illustrates the difference between the two- and three - view formulations graphically",
    ".    in the standard kcca formulation , instead of directly solving for linear projections of data explicitly mapped into the kernel space by @xmath26 , one applies the `` kernel trick '' and expresses the coordinates of a data point in the cca space as linear combinations of kernel values of that point and several training points .    to find the weights in this combination",
    ", one must solve a @xmath27 generalized eigenvalue problem ( see @xcite for details ) , which is infeasible for large - scale data .    to handle large numbers of images and high - dimensional features ,",
    "we propose a scalable approach based on the idea of approximate kernel maps  @xcite .",
    "let @xmath28 denote an approximate kernel mapping such that @xmath29 .",
    "the dimensionality of @xmath28 needs to be much lower than @xmath0 to reduce the complexity of the problem .",
    "the specific kernel mappings used in our implementation will be described in section [ sec : visual_kernel ] .",
    "then , instead of using the kernel trick , we can directly substitute @xmath28 into the linear cca objective function ( [ eq : cca ] ) .",
    "the solution is given by the following generalized eigenvalue problem : @xmath30 where @xmath31 is the covariance matrix between the @xmath9th and @xmath10th views , and @xmath32 is a column of @xmath17 .",
    "the size of this problem is @xmath33 , where the @xmath34 are the dimensionalities of the respective explicit mappings @xmath35 .",
    "this is independent of training set size , and much smaller than @xmath27 . to regularize the problem",
    ", we add a small constant ( @xmath36 in the experiments ) to the diagonal of the covariance matrix .    in order to obtain a @xmath37-dimensional embedding for different views , we form projection matrices @xmath38 from the top @xmath37 eigenvectors corresponding to each @xmath32",
    ". then the projection of a data point @xmath39 from the @xmath9th view into the latent cca space is given by @xmath40 .",
    "note that once they are learned , the respective projection matrices are applied to each view individually , which means that at test time , we can compute the embedding for data for which one or two views are missing ( e.g. , an image without tags or ground - truth semantic labels ) . in the latent cca space , points from different views are directly comparable , so we can do image - to - image , image - to - tag , and tag - to - image retrieval by nearest neighbor search .    in the implementation , we select the embedding dimensionality @xmath37 by measuring the retrieval accuracy in embedded spaces with different values of @xmath37 on validation images set aside from each of our datasets ( details will be given in sections 6 - 8 ) .",
    "we search a range from 16 to 1,024 , doubling the dimensionality each time , and the resulting values typically fall around 128 - 256 on all our datasets .      in the cca - projected latent space ,",
    "the function used to measure the similarity between data points is important .",
    "an obvious choice is the euclidean distance between embedded data points , as used in @xcite .",
    "however , for our learned embedding , we were able to find a similarity function that produces better empirical results .",
    "in particular , we scale the dimensions in the common latent space by the magnitude of the corresponding eigenvalues  @xcite , and then compute normalized correlation between projected vectors .",
    "indeed , the cca objective can be reformulated as maximizing the normalized correlation between different views @xcite .",
    "let @xmath39 and @xmath41 be points from the @xmath9th and @xmath10th views , respectively ( we can have @xmath42 ) .",
    "then we define the similarity function between @xmath39 and @xmath41 as @xmath43 where @xmath17 and @xmath44 are the cca projections for data points @xmath39 and @xmath41 , and @xmath45 and @xmath46 are diagonal matrices whose diagonal elements are given by the @xmath47-th power of the corresponding eigenvalues  @xcite .",
    "we fix @xmath48 in all our experiments as we have found this leads to the best performance .",
    "section [ distancemetric ] will experimentally confirm that the above similarity measure leads to much higher retrieval accuracy than euclidean distance .",
    "in sections [ sec : visual_kernel ] and [ sec : tag_approximation ] , we will present our visual and text features with their respective kernel mappings . next , in section [ topics ] , we will discuss different text clustering approaches that can be used to extract semantic topics in the unsupervised scenario , where the third view is not given for the training data .",
    "we represent image appearance using a combination of nine different visual cues :    * gist * @xcite : we resize each image to @xmath49 and use three different scales @xmath50 $ ] to filter each rgb channel , resulting in 960-dimensional ( @xmath51 ) gist feature vectors .",
    "* sift * : we extract six different texture features based on two different patch sampling schemes : dense sampling and harris corner detection .",
    "for each local patch , we extract * sift * @xcite , * csift * @xcite , and * rgbsift * @xcite . for each feature , we form a codebook of size 1,000 using k - means clustering and build a two - level spatial pyramid @xcite , resulting in a 5000-dimensional vector .",
    "we will refer to these six features as d - sift , d - csift , d - rgbsift , h - sift , h - csift , and h - rgbsift .",
    "* hog * @xcite : to represent texture and edge information on a larger scale , we use @xmath52 overlapping hog as described in @xcite .",
    "we quantize the hog features to a codebook of size 1,000 and use the same spatial pyramid scheme as above , once again resulting in 5,000-dimensional feature vectors .    * color * : we use a joint rgb color histogram of 8 bins per dimension , for a 512-dimensional feature .",
    "recall from section [ sec : three_view_cca ] that we transform all the features by nonlinear kernel maps @xmath28 and then apply linear cca to the result .",
    "we discuss the specific feature maps we use here . for gist features",
    ", we use the random fourier feature mapping  @xcite that approximates the gaussian kernel .",
    "we compute this mapping with 3,000 dimensions and set its standard deviation equal to the average distance to the 50th nearest neighbor in each dataset .",
    "all the other descriptors above are histograms , and for them we adopt the exact bhattacharyya kernel mapping given by term - wise square root  @xcite . to combine different features",
    ", we simply average the respective kernels , which has been proven to be quite effective in @xcite .",
    "this corresponds to concatenating all the different visual features after putting them through their respective explicit kernel mappings .",
    "however , the resulting concatenated feature has 38,512 dimensions , necessitating additional dimensionality reduction . to do this",
    ", we perform pca on top of each kernel - mapped feature @xmath35 .",
    "this is essentially using the low - rank approximation of kernel pca ( kpca )  @xcite to approximate the combined multiple feature kernel matrix .    in our experiments",
    ", we reduce each kernel - mapped feature to @xmath53 dimensions and the final concatenated feature is a @xmath54-dimensional vector . as validated in section [ kccamodel ] , this dimensionality achieves good balance between efficiency and accuracy .",
    "note that for multiple feature combination , we have found it important to center all feature dimensions at the origin .      for the tags associated with the images ,",
    "we construct a dictionary consisting of @xmath2 most frequent tags ( the vocabulary sizes used for the different datasets are summarized in figure [ data ] and will be further detailed in section [ dataset ] ) and manually remove a small set of stop words .",
    "these include camera brands ( e.g. , `` canon , '' `` nikon , '' etc . ) , lens characteristics ( e.g. , `` eos , '' `` 70 - 200 mm , '' etc . ) , and words like `` geo . ''",
    "the tag feature matrix @xmath55 is binary : @xmath56 if image @xmath9 is tagged with tag @xmath10 and 0 otherwise .",
    "note that even though the dimensionality of the tag feature may be high ( our vocabularies range from 1,000 to over 20,000 on the different datasets ) , this representation is highly sparse .    like  @xcite",
    ", we use the linear kernel for @xmath55 , which corresponds to counting the number of common tags between two images . however , because of the high dimensionality of the tag features , additional compression is required , just as with the concatenated visual features .",
    "we apply sparse svd  @xcite to the tag feature @xmath55 to obtain a low - rank decomposition as @xmath57 .",
    "it is easy to show that @xmath58 is actually the pca embedding for @xmath55 , but directly applying sparse svd to @xmath55 is more efficient . in our implementation",
    ", the compressed representation of @xmath55 is given by the top 500 columns of @xmath58 .",
    "we have also investigated more sophisticated tag features such as the ranking - based representation of  @xcite , which is based on the idea that tags listed earlier by users are more salient to the image content .",
    "however , we have found almost no improvement from our chosen representation .      as initially discussed in section [ cca ] , the third view of our cca model",
    "is given by the class or topic indicator matrix @xmath59 for @xmath0 images and @xmath6 topics .",
    "in the supervised training scenario , @xmath7 is straightforwardly given by ground - truth annotations or noisy search keywords used to download the data . in the more interesting unsupervised scenario ,",
    "training images come with noisy text or tags , but no additional semantic annotations . in this case , we choose to obtain @xmath7 by clustering the tags .",
    "given the raw tag feature @xmath55 ( _ prior _ to the application of sparse svd ) , our goal is to find @xmath6 semantic clusters . for this purpose ,",
    "we investigate several models that have proven successful for text clustering .",
    "we briefly describe these models below ; quantitative and qualitative evaluation results will be presented in section [ comparetopicmodel ] .",
    "* k - means clustering * : the simplest baseline approach is k - means clustering on raw tag feature @xmath55 using @xmath6 centers .",
    "the resulting matrix @xmath7 has a 1 in the @xmath8th position if the @xmath9th tag feature vector belongs to the @xmath10th cluster .",
    "* normalized cut ( nc ) *  @xcite : for text clustering , the normalized cut model is usually formulated as computing the eigenvectors of @xmath60 in which @xmath61 .",
    "this is equivalent to computing the first @xmath6 singular vectors of the sparse matrix @xmath62 .",
    "following @xcite , we normalize each row of the matrix of top @xmath6 eigenvectors to have unit norm and perform k - means clustering of rows of the resulting matrix @xmath63 . directly using @xmath63 as @xmath7 would represent a `` soft '' version of nc , but we have found that the `` hard '' version obtained by k - means produces better results .    * nonnegative matrix factorization ( nmf ) *  @xcite : the data matrix is normalized as @xmath62 , where @xmath64 is defined the same way as for nc , and then factorized into two nonnegative matrices @xmath65 and",
    "@xmath66 such that @xmath67 ( if @xmath55 is @xmath68 , then @xmath65 is @xmath69 and @xmath66 is @xmath70 ) . then , as in  @xcite , we obtain a normalized matrix @xmath63 with entries @xmath71 . finally , we do hard cluster assignment based on the highest value of @xmath63 in each row . just as with nc , this produces better results than using @xmath63 as a `` soft '' cluster indicator matrix directly .    * probabilistic latent semantic analysis ( plsa ) *  @xcite : this approach models each document ( vector of tags for an image ) as a mixture of topics .",
    "the output of plsa is the posterior probability of each topic given each document . directly using this matrix of posterior probabilities as @xmath7 leads to `` soft '' plsa clustering .",
    "however , once again , we get better performance with `` hard '' plsa where we map each document to the cluster index with the highest posterior probability .",
    "we have also investigated latent dirichlet allocation ( lda )  @xcite and found the performance to be similar to plsa , so we omit it .",
    "because the number of topics used in this work is not very high ( from 10 to 100 ) , we simply use a linear kernel on @xmath7 with no further dimensionality reduction .",
    "this section will present the components of our experimental evaluation , including datasets , retrieval tasks , multi - view models being compared , and baselines .",
    "subsequently , sections [ sec : evaluation]-[sec : evaluation3 ] will present results on our three datasets .",
    "our selection of datasets is motivated by two considerations .",
    "first , we want datasets that are as large as possible , both in the number of images and in the number of tags .",
    "second , we want datasets that have the right kind of annotations for evaluating our method  specifically , images that are accompanied both by noisy text or tags , and ground - truth labels .    we have considered a number of datasets used in recent related papers , but unfortunately , most of them are unsuitable for our goals .",
    "in particular , standard image annotation datasets used by @xcite  namely , corel5k @xcite , esp game @xcite , and iapr - tc @xcite  have only two views and are rather small - scale ( 5k-20k images and 260 - 290 tags ) .",
    "@xcite , who have first proposed a two - view cca model for cross - modal retrieval of internet images , perform experiments on a wikipedia dataset that has rich textual views as well as ground - truth labels , but it consists of only 2,866 documents .",
    "@xcite evaluate their wsabie annotation system on millions of images .",
    "however , one of their datasets is drawn from imagenet  @xcite , which is more appropriate for image classification , and the other one is proprietary ; neither has the three - view structure we are looking for .",
    "* flickr - cifar dataset * : we have downloaded 230,173 images from flickr by running queries for categories from the cifar10 dataset  @xcite : airplane , automobile , bird , cat , deer , dog , frog , horse , ship , and truck . we keep tags that appear at least 150 times , resulting in a tag dictionary with dimensionality 2,494 . on average",
    ", there are 6.84 tags per image .",
    "the flickr images come with search keywords and user - provided tags , but no ground - truth labels .",
    "to quantitatively evaluate retrieval performance we need another set of cleanly labeled test images .",
    "we get this set by collecting the same ten categories from imagenet  @xcite , resulting in 15,167 test images with no tags but ground - truth class labels .",
    "* nus - wide dataset * : this dataset  @xcite was collected at the national university of singapore .",
    "it also originates from flickr , and contains 269,648 images .",
    "the dataset is manually annotated with 81 ground truth concept labels , e.g. , animal , snow , dog , reflection , city , storm , fog , etc .",
    "one important difference between nus - wide and other datasets is that nus - wide images may be associated with multiple ground truth labels . for the tags",
    ", we use the list of 1,000 words provided by  @xcite ; on average , each image has 5.78 tags and 1.86 ground truth annotations .",
    "each ground truth concept is also in the tag dictionary .    * inria - websearch dataset * : finally , we use the inria web query dataset @xcite , which contains 71,478 web images and 353 different concepts or categories , which include famous landmarks , actors , films , logos , etc .",
    "each concept comes with a number of images retrieved via internet search , and each image is marked as either relevant or irrelevant to its query concept .",
    "this dataset is especially challenging in that it contains a very large number of concepts relative to the total number of images .",
    "the second view for this dataset consists of text surrounding images on web pages , not tags .",
    "we keep words that appear more than 20 times and remove stop words using a standard list for text document analysis , which gives us a tag dictionary of size 20,602 . on this dataset",
    ", we also apply _ tf - idf _ weighting to the tag feature .",
    "the above three datasets have different characteristics and present different challenges for our method .",
    "flickr - cifar has the fewest classes but the largest number of images per class .",
    "it is also the only dataset whose training images have no ground - truth semantic annotation , and whose test images come from a different distribution than the training images .",
    "we use this dataset for detailed comparative validation of the different implementation choices of our method ( section [ sec : evaluation ] ) .",
    "nus - wide images are fully manually annotated and come with multiple ground truth concepts per image .",
    "inria - websearch is the only one not collected from flickr , and its images are the most inconsistent in quality .",
    "it has the largest number of classes but the smallest number of images per class .",
    "it also has by far the largest vocabulary for the second view and the noisiest statistics for this view .",
    "* image - to - image search ( i2i ) * : given a query image , project its visual feature vector into the cca space , and use it to retrieve the most similar visual features from the database .",
    "recall that our similarity function in the cca space is given by eq .",
    "( [ distancefunction ] ) .    * tag - to - image search ( t2i ) * : given a search tag or combination of tags , project the corresponding feature vector into the cca space and retrieve the most similar database images .",
    "this is a cross - modal task , in that the cca - embedded tag query is used to directly search cca - embedded visual features .",
    "note that with our method , we can use tags to search database images that do not initially come with any tags . in scenarios where ground - truth labels or keywords are available for the database images ,",
    "we also consider a variant of this task where we use the keywords as queries , which we refer to as * keyword - to - image search ( k2i)*.    * image - to - tag search ( i2 t ) : * given an image , retrieve a set of tags that accurately describe it .",
    "this task is more challenging than the other two because going from a feature vector in cca space to a coherent set of tags requires a sophisticated reconstruction or decoding algorithm ( see , e.g. , @xcite ) .",
    "the design of such an algorithm is beyond the scope of our present work , but to get a preliminary idea of the promise of our latent space representation for this task , we evaluate a simple data - driven scheme similar to that of @xcite .",
    "namely , given a query image , we first find the fifty nearest neighbor tag vectors in cca space , and then return the five tags with the highest frequencies in the corresponding database images .",
    "note that @xcite return tags according to their global frequencies , while for our larger and more diverse datasets , we have found that local frequency works better . because our method for i2 t is somewhat preliminary and because proper evaluation of this task requires human annotators ( see section [ sec : tagging ] ) , our experiments on this task will be smaller - scale and more limited than on the other two .        in the subsequent presentation , we will denote visual features as * v * , tag features as * t * , the keyword or ground truth annotations as * k * , and the automatically computed topics as * c*. * cca ( v+t ) * will refer to the two - view baseline model based on visual and tag features ; * cca ( v+t+k ) * to the three - view model based on visual features , tags , and supervised semantic information ( ground truth labels or search keywords ) ; and * cca ( v+t+c ) * to the three - view model with the unsupervised third view ( automatically computed tag clusters ) .",
    "all of these models will be evaluated for both i2i and t2i retrieval . for completeness",
    ", we will also evaluate the two - view * cca ( v+c ) * and * cca ( v+k ) * models for i2i retrieval .",
    "however , because these models do not give an embedding for the tags , they can not be used for cross - modal retrieval ( i.e. , t2i ) .",
    "in addition , we will evaluate k2i and i2 t on subsets of models as appropriate ( see sections [ sec : evaluation]-[sec : evaluation3 ] for details ) .",
    "it is important to evaluate how cca compares to alternative methods for obtaining intermediate embeddings for visual features supervised by tag or keyword information . for this , we have implemented two embedding methods from the recent literature , as described below .    *",
    "structural learning . * our first baseline is given by the structural or multi - task learning method of  @xcite . in their formulation ,",
    "the tag matrix @xmath55 is treated as supervisory information for the visual features @xmath66 and a matrix of image - to - tag predictors @xmath72 is obtained by ridge regression : @xmath73 .",
    "next , since the tasks of predicting multiple tags are assumed to be correlated , we look for low - rank structure in @xmath72 by computing its svd . if @xmath74 , then we use @xmath75 ( or more precisely , its top @xmath37 columns ) as the embedding matrix for the visual features : @xmath76 .",
    "we select @xmath37 by validation just as with our cca - based methods .",
    "note that the structural learning method does not produce an embedding for tags , so unlike cca ( v+t ) and our three - view models , it is not suitable for cross - modal retrieval .",
    "the quantitative and qualitative results presented in this paper demonstrate that our proposed multi - view embedding space , together with the similarity function specially designed for it , successfully captures visual and semantic consistency in diverse , large - scale datasets .",
    "this space can form a good basis for a scalable and flexible retrieval system capable of simultaneously accommodating multiple usage scenarios .",
    "the visual and semantic clusters discovered by tag clustering and subsequent cca projection can be used to summarize and browse the content of internet photo collections @xcite .",
    "figure [ fig : nus_semantic ] has shown an example of what such a summary could look like .",
    "furthermore , users can search with images for similar images , or retrieve images based on queries consisting of multiple tags or keywords . as illustrated in figure [ weight ] , they can also manually adjust weights corresponding to different keywords according to the importance of those keywords .",
    "finally , our embedding space can also serve as a basis for an automatic image annotation system .",
    "however , as discussed in section [ sec : evaluation2 ] , in order to achieve satisfactory results on this task , we need to develop more sophisticated decoding methods incorporating multi - label consistency constraints .    besides the application scenarios named above , we are also interested in using our learned latent space as an intermediate representation for recognition tasks . one of these is nonparametric image parsing @xcite where , given a query image , a small number of similar training images is retrieved and labels are transferred from these images to the query . with a better embedding for images and tags",
    ", this retrieval step may be able to return training images more consistent with the query and lead to improved accuracy for image parsing .",
    "another problem of interest to us is describing images with sentences @xcite . once again , with a good intermediate embedding space linking images and tags , the subsequent step of sentence generation may become easier .",
    "we would like to thank the anonymous reviewers for their constructive comments ; jason weston for advice on implementing the wsabie method ; albert gordo and florent perronnin for useful discussions ; and joseph tighe , hongtao huang , juan caicedo , and mariyam khalid for helping with manual evaluation of the auto - tagging experiments .",
    "gong and lazebnik were supported by nsf grant iis 1228082 , darpa computer science study group ( d12ap00305 ) , and microsoft research faculty fellowship .",
    "chua , t .- s . , tang , j. , hong , r. , li , h. , luo , z. , and zheng , y .- t .",
    "( 2009 ) . :",
    "a real - world web image database from national university of singapore . in _ proc . of acm conf . on image and video retrieval ( civr09 )",
    "_ , santorini , greece .",
    "grubinger , m. , clough , p.  d. , mller , h. , and deselaers , t. ( 2006 ) .",
    "the iapr tc-12 benchmark - a new evaluation resource for visual information systems . in _ proceedings of the international workshop ontoimage2006 language resources for content - based image retrieval _ , pages 13  23 ."
  ],
  "abstract_text": [
    "<S> this paper investigates the problem of modeling internet images and associated text or tags for tasks such as image - to - image search , tag - to - image search , and image - to - tag search ( image annotation ) . </S>",
    "<S> we start with _ canonical correlation analysis ( cca ) _ , a popular and successful approach for mapping visual and textual features to the same latent space , and incorporate a third view capturing high - level image semantics , represented either by a single category or multiple non - mutually - exclusive concepts . </S>",
    "<S> we present two ways to train the three - view embedding : supervised , with the third view coming from ground - truth labels or search keywords ; and unsupervised , with semantic themes automatically obtained by clustering the tags . to ensure high accuracy for retrieval tasks </S>",
    "<S> while keeping the learning process scalable , we combine multiple strong visual features and use explicit nonlinear kernel mappings to efficiently approximate kernel cca . to perform retrieval , we use a specially designed similarity function in the embedded space , which substantially outperforms the euclidean distance . </S>",
    "<S> the resulting system produces compelling qualitative results and outperforms a number of two - view baselines on retrieval tasks on three large - scale internet image datasets . </S>"
  ]
}