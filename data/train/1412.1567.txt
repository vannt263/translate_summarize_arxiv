{
  "article_text": [
    "usually , when we talk about unbiased estimation of a parameter vector @xmath0 out of a measurement vector @xmath1 , then the estimation problem is treated in the classical framework @xcite , @xcite .",
    "letting @xmath2 be an estimator of @xmath3 , then the classical unbiased constraint asserts that @xmath4 = \\int{{\\mathbf}{g}({\\mathbf}{y } ) p({\\mathbf}{y};{\\mathbf}{x } ) d{\\mathbf}{y } = { \\mathbf}{x } } \\hspace{0.4 cm } \\text{for all possible } { \\mathbf}{x } , \\label{equ : culmmse045}\\ ] ] where @xmath5 is the probability density function ( pdf ) of vector @xmath6 parametrized by the unknown parameter vector @xmath3 .",
    "the index of the expectation operator shall indicate the pdf over which the averaging is performed .",
    "eq .   can also be formulated in the bayesian framework , where the parameter vector @xmath3 is treated as random . here",
    ", the corresponding problem arises by demanding global conditional unbiasedness , i.e. @xmath7 = \\int{{\\mathbf}{g}({\\mathbf}{y } ) p({\\mathbf}{y}|{\\mathbf}{x } ) d{\\mathbf}{y } } = { \\mathbf}{x }                                       \\hspace{0.4 cm } \\text{for all possible } { \\mathbf}{x}.\\label{equ : culmmse046}\\ ] ] the attribute _ global _ indicates that the condition is made on the whole parameter vector @xmath3 .",
    "however , the constricting requirement in prevents the exploitation of prior knowledge about the parameters , and hence leads to a significant reduction in the benefits brought about by the bayesian framework .    in component - wise conditionally unbiased ( cwcu ) bayesian parameter estimation @xcite-@xcite , instead of constraining the estimator to be globally unbiased , we aim for achieving conditional unbiasedness on one parameter component at a time .",
    "let @xmath8 be the @xmath9 element of @xmath3 , and @xmath10 be the estimator of @xmath8 .",
    "then the cwcu constraints are @xmath11 =                                       \\int{g_i({\\mathbf}{y } ) p({\\mathbf}{y}|x_i ) } d{\\mathbf}{y } = x_i , \\label{equ : culmmse047}\\ ] ] for all possible @xmath8 ( @xmath12 ) .",
    "the cwcu constraints are less stringent than the global conditional unbiasedness condition in , and it will turn out that a cwcu estimator in many cases allows the incorporation of prior knowledge about the statistical properties of the parameter vector .    the paper is organized as follows : in section [ sec : gaussianassumption ] we discuss the prerequisites and the solution of the cwcu linear minimum mean square error ( lmmse ) estimator under the jointly gaussian assumption of @xmath3 and @xmath6 . in section [ sec : linearmodel ]",
    "we discuss the cwcu lmmse estimator under the linear model assumption , and we extend the findings of @xcite . here",
    "we particularly distinguish between jointly gaussian , and mutually independent ( and otherwise arbitrarily distributed ) parameters .",
    "finally , in section [ sec : ieee_cir_model ] the cwcu lmmse estimator is compared against the best linear unbiased estimator ( blue ) and the lmmse estimator in a well - known channel estimation example .",
    "we assume that a vector parameter @xmath0 is to be estimated based on a measurement vector @xmath1 . as in lmmse estimation",
    "we constrain the estimator to be linear ( or actually affine ) , such that @xmath13 note that in lmmse estimation no assumptions on the specific form of the joint pdf @xmath14 have to be made . however , the situation is different in cwcu lmmse estimation .",
    "let us consider the @xmath9 component of the estimator @xmath15 where @xmath16 denotes the @xmath9 column of the estimator matrix @xmath17 .",
    "the conditional mean of @xmath18 can be written as @xmath11 =                   { \\mathbf}{e}_i^h e_{{\\mathbf}{y}|x_i}[{\\mathbf}{y}|x_i ] + c_i .",
    "\\label{equ : culmmse050a}\\ ] ] a closer inspection of reveals that @xmath19 = x_i$ ] can be fulfilled for all possible @xmath8 if the conditional mean @xmath20 $ ] is a linear function of @xmath8 . for jointly gaussian @xmath3 and @xmath6",
    "this is the case and we have @xmath21 = e_{{\\mathbf}{y}}[{\\mathbf}{y } ] +                                       ( \\sigma_{x_i}^2)^{-1}{\\mathbf}{c}_{{\\mathbf}{y } x_i } ( x_i -e_{x_i}[x_i]),\\ ] ] where @xmath22)(x_i - e_{x_i}[x_i])^h]$ ] , and @xmath23 is the variance of @xmath8 .",
    "@xmath19 = x_i$ ] is fulfilled if @xmath24 and @xmath25 - { \\mathbf}{e}_i^h e_{{\\mathbf}{y}}[{\\mathbf}{y } ] .",
    "\\label{equ : cwculmmse011}\\ ] ] inserting , and in the bayesian mse cost function @xmath26 $ ] immediately leads to the optimization problem @xmath27 where  cl  shall stand for cwcu lmmse .",
    "the solution can be found with the lagrange multiplier method and is given by @xmath28 using @xmath29^h$ ] together with and immediately leads us to the first part of the    [ prop : cwculmmse001 ] if @xmath0 and @xmath1 are jointly gaussian then the cwcu lmmse estimator minimizing the bayesian mses @xmath26 $ ] under the constraints @xmath19=x_i$ ] for @xmath30 is given by @xmath31 + { \\mathbf}{e}_{\\mathrm{cl } } ( { \\mathbf}{y } - e_{{\\mathbf}{y}}[{\\mathbf}{y } ] ) , \\label{equ : cwculmmse001}\\ ] ] with @xmath32 where the elements of the real diagonal matrix @xmath33 are @xmath34_{i , i } = \\frac{\\sigma_{x_i}^2}{{\\mathbf}{c}_{x_i{\\mathbf}{y}}{\\mathbf}{c}_{{\\mathbf}{y}{\\mathbf}{y}}^{-1}{\\mathbf}{c}_{{\\mathbf}{y}x_i}}. \\label{equ : cwculmmse003}\\ ] ] the mean of the error @xmath35 ( in the bayesian sense ) is zero , and the error covariance matrix @xmath36 which is also the minimum bayesian mse matrix @xmath37 is @xmath38 with @xmath39 .",
    "the minimum bayesian mses are @xmath40_{i , i}$ ] .",
    "the part on the error performance can simply be proofed by inserting in the definition of @xmath41 and @xmath42 , respectively . from it",
    "can be seen that the cwcu lmmse estimator matrix can be derived as the product of the diagonal matrix @xmath33 with the lmmse estimator matrix @xmath43 .",
    "furthermore , we have @xmath44 = [ { \\mathbf}{d}]_{i , i}^{-1 } x_i + ( 1-[{\\mathbf}{d}]_{i , i}^{-1})e_{x_i}[x_i]$ ] for the lmmse estimator .",
    "it can be shown that @xmath33 can also be written as @xmath45    the cwcu lmmse estimator will in general not commute over linear transformations , an exception is discussed in @xcite .",
    "in the following it will be seen that some of the prerequisites of proposition [ prop : cwculmmse001 ] can be relaxed when incorporating details of the data model into the derivation of the estimator . from now on we limit our considerations to the linear model @xmath46 where @xmath47 is a known observation matrix , @xmath48 is a parameter vector with mean @xmath49 $ ] and covariance matrix @xmath50 , and @xmath51 is a zero mean noise vector with covariance matrix @xmath52 and independent of @xmath3 .",
    "additional assumptions on @xmath3 and @xmath53 will vary in the following .",
    "we note that the cwcu lmmse estimator for the linear model under the assumption of white gaussian noise has already been derived in @xcite .      for the linear model",
    "the covariance matrices required in and become @xmath54 , @xmath55 , @xmath56 and @xmath57 . if the assumptions made on the linear model above hold and if @xmath3 and @xmath53 are both gaussian , then they are jointly gaussian . furthermore , since @xmath58^t$ ] is a linear transformation of @xmath59^t$ ] , @xmath3 and @xmath6 are jointly gaussian , too .",
    "we could therefore simply insert the above covariance matrices into the equations given in proposition [ prop : cwculmmse001 ] .",
    "however , the jointly gaussian assumption for @xmath3 and @xmath53 can significantly be relaxed .",
    "this can be shown by incorporating the linear model assumption already earlier in the derivation of the estimator .",
    "let @xmath60 be the @xmath9 column of @xmath61 , @xmath62 the matrix resulting from @xmath61 by deleting @xmath63 , and @xmath64 the vector resulting from @xmath3 after deleting @xmath8 .",
    "then we can write the @xmath9 component of @xmath65 in the form @xmath66 the conditional mean of @xmath18 becomes @xmath11 = { \\mathbf}{e}_i^h{\\mathbf}{h}_i x_i +                       { \\mathbf}{e}_i^h\\bar{{\\mathbf}{h}}_i e_{\\bar{{\\mathbf}{x}}_i|x_i}[\\bar{{\\mathbf}{x}}_i|x_i ] + c_i .",
    "\\label{equ : cwculmmse016}\\ ] ] the cwcu constraint @xmath19 = x_i$ ] for a particular @xmath67 can be fulfilled if @xmath68 $ ] is a linear function of @xmath8 .",
    "this is true if @xmath3 is gaussian , i.e. @xmath69 , { \\mathbf}{c}_{{\\mathbf}{x}{\\mathbf}{x}})$ ] , since then we have @xmath68=e_{\\bar{{\\mathbf}{x}}_i}[\\bar{{\\mathbf}{x}}_i]+ ( \\sigma_{x_i}^2)^{-1}{\\mathbf}{c}_{\\bar{{\\mathbf}{x}}_i x_i}(x_i - e_{x_i}[x_i])$ ] .",
    "note that the only requirement on the noise vector so far was its independence on @xmath3 .",
    "following similar arguments as above we end up at the constrained optimization problem @xmath70 solving leads to    [ prop : cwculmmse003 ] if the observed data @xmath6 follow the linear model in , where @xmath1 is the data vector , @xmath47 is a known observation matrix , @xmath0 is a parameter vector with prior pdf @xmath71 , { \\mathbf}{c}_{{\\mathbf}{x}{\\mathbf}{x}})$ ] , and @xmath72 is a zero mean noise vector with covariance matrix @xmath52 and independent of @xmath3 ( the joint pdf of @xmath3 and @xmath53 is otherwise arbitrary ) , then the cwcu lmmse estimator minimizing the bayesian mses @xmath26 $ ] under the constraints @xmath19=x_i$ ] for @xmath30 is given by with @xmath73 where the elements of the real diagonal matrix @xmath33 are @xmath34_{i , i } = \\frac{\\sigma_{x_i}^2}{{\\mathbf}{c}_{x_i{\\mathbf}{x}}{\\mathbf}{h}^h({\\mathbf}{h}{\\mathbf}{c}_{{\\mathbf}{x}{\\mathbf}{x}}{\\mathbf}{h}^h+{\\mathbf}{c}_{{\\mathbf}{n}{\\mathbf}{n}})^{-1 }                                      { \\mathbf}{h}{\\mathbf}{c}_{{\\mathbf}{x}x_i}}. \\label{equ : cwculmmse005}\\ ] ]    the same formulas would result by inserting for the covariance matrices in the equations given by proposition [ prop : cwculmmse001 ] , however , the prerequisites in proposition 1 and 2 differ .",
    "also the error measures can formally be derived by inserting in the equations of proposition [ prop : cwculmmse001 ] .      for mutually independent parameters it is possible to further relax the prerequisites on @xmath3 .",
    "in this situation becomes @xmath11 = { \\mathbf}{e}_i^h{\\mathbf}{h}_i x_i +                       { \\mathbf}{e}_i^h\\bar{{\\mathbf}{h}}_i e_{\\bar{{\\mathbf}{x}}_i}[\\bar{{\\mathbf}{x}}_i ] + c_i,\\ ] ] since @xmath68 $ ] is no longer a function of @xmath8 .",
    "the cwcu constraints are fulfilled if @xmath74 and @xmath75 $ ] , and no further assumptions on the pdf of @xmath3 are required . again following similar arguments as above we end up at a constrained optimization problem @xcite . solving it leads to    [ prop : cwculmmse004 ]",
    "if the observed data @xmath6 follow the linear model where @xmath1 is the data vector , @xmath47 is a known observation matrix , @xmath0 is a parameter vector with mean @xmath49 $ ] , mutually independent elements and covariance matrix @xmath76 , @xmath72 is a zero mean noise vector with covariance matrix @xmath52 and independent of @xmath3 ( the joint pdf of @xmath3 and @xmath53 is otherwise arbitrary ) , then the cwcu lmmse estimator minimizing the bayesian mses @xmath26 $ ] under the constraints @xmath19=x_i$ ] for @xmath30 is given by and , where the elements of the real diagonal matrix @xmath33 are @xmath34_{i , i } = \\frac{1}{\\sigma_{x_i}^2{\\mathbf}{h}_i^h({\\mathbf}{h}{\\mathbf}{c}_{{\\mathbf}{x}{\\mathbf}{x}}{\\mathbf}{h}^h+{\\mathbf}{c}_{{\\mathbf}{n}{\\mathbf}{n}})^{-1}{\\mathbf}{h}_i}.                                          \\label{equ : cwculmmse006}\\ ] ]    in @xcite we showed that for mutually independent parameters @xmath77 is independent of @xmath23 and also given by @xmath78 , where @xmath79 .",
    "furthermore , we showed that @xmath34_{i , i } = ( { \\mathbf}{e}_{\\text{l},i}^h { \\mathbf}{h}_i)^{-1 } , \\label{equ : cwcu_journal034}\\ ] ] where @xmath80 is the @xmath9 row of the lmmse estimator .",
    "it therefore holds that @xmath81 .",
    "if @xmath3 is whether gaussian nor a parameter vector with mutually independent parameters , then we have the following possibilities : if @xmath19 $ ] is a linear function of @xmath8 for all @xmath30 then we can derive the cwcu lmmse estimator similar as in section [ sec : linearmodelgaussian ] . in the remaining cases",
    "still an estimator can be found that fulfills the cwcu constraints . by studying it",
    "can be seen that the choice @xmath82 , @xmath83 together with @xmath84 for all @xmath30 will do the job . inserting these constraints into the bayesian mse cost functions and solving the constrained optimization problem leads to @xmath85 with @xmath86 as the bayesian error covariance matrix .",
    "@xmath87 is equivalent to @xmath88 .",
    "this implies @xmath89 .",
    "it is seen that the estimator in fulfills the global unbiasedness condition @xmath90={\\mathbf}{x}$ ] for every @xmath0 .",
    "this estimator , which is the blue if @xmath3 can be any vector in @xmath91 , does not account for any prior knowledge about @xmath3 . in some situations a better globally unbiased estimator exists . if for example it is known that @xmath3 lies in a linear subspace of @xmath92 spanned by the columns of the full rank matrix @xmath93 with @xmath94 , such that @xmath95 , then each estimator with @xmath96 fulfills @xmath97={\\mathbf}{x}$ ] for every @xmath95 lying in the subspace @xcite .",
    "however , @xmath96 is less stringent than @xmath88 , so inserting this weaker constraints into the bayesian mse cost functions and solving the constrained optimization problem leads to a better performing estimator which is @xmath98 with the bayesian error covariance matrix @xmath99 in case the assumption that @xmath3 lies in the linear subspace defined by the full rank matrix @xmath100 holds , this estimator is in fact the blue .",
    "note that the estimator in ( and the one in once its underlying assumptions are fulfilled ) can of course also be applied if the prerequisites in proposition [ prop : cwculmmse003 ] are fulfilled .",
    "however , in this case the cwcu lmmse estimator is always the one given in proposition [ prop : cwculmmse003 ] .",
    "as an application to demonstrate the properties of the cwcu lmmse estimator we choose the well - known channel estimation problem for ieee 802.11a / g / n wlan standards @xcite .",
    "the standards define two identical preamble symbols @xmath101 , cf .",
    "[ fig : preamble]a , which are designed such that the frequency domain version @xmath102 , shows @xmath103 at 52 subcarrier positions ( indexes @xmath104 ) and zeros at the remaining unused 12 subcarriers ( indexes @xmath105 ) . here",
    "@xmath106 is the length @xmath107 discrete fourier transform ( dft ) matrix , and @xmath108 denotes a vector in the frequency domain . with the carrier selection matrix @xmath109 , cf .",
    "@xcite , the vector of nonzero ( used ) subcarrier symbols can be written as @xmath110 .",
    "@xmath111 deletes the elements of @xmath112 that correspond to the zero - subcarriers .",
    "we furthermore introduce the diagonal matrix @xmath113 which fulfills @xmath114 because of @xmath115 .    the channel impulse response ( cir )",
    "is modeled as @xmath116 , with @xmath117 and exponentially decaying power delay profile according to @xmath118 for @xmath119 . here , @xmath120 is the length of the cir .",
    "@xmath121 and @xmath122 are the sampling time and the channel delay spread , respectively , which are chosen as @xmath123ns , @xmath124ns in our setup .",
    "note that the channel length @xmath120 can be assumed to be considerably smaller than the dft length @xmath125 . in the following we assume @xmath126 .",
    "let @xmath127 and @xmath128 be the two received , channel distorted time domain preamble symbols , cf .",
    "[ fig : preamble]b , @xmath129 for @xmath130 , and @xmath131",
    ". then @xmath132 can be modeled as @xmath133 here @xmath134 is the frequency response at the occupied subcarriers , @xmath135 is the full length frequency response including the unused frequency bins , and @xmath136 is a complex white gaussian noise vector with covariance matrix @xmath137 , where @xmath138 is the time domain noise variance .",
    "@xmath139 consists of the first @xmath120 columns of @xmath106 .            from the blue",
    ", the lmmse and the cwcu lmmse estimator for the channel impulse response @xmath140 follow to @xmath141 here @xmath33 shall be used from proposition [ prop : cwculmmse004 ] , since the elements of @xmath140 are mutually independent .",
    "[ fig : cir_time_domain ] shows the bayesian mses of the estimated cir coefficients for the different estimators ( for @xmath142 ) .",
    "the performance drawback of the blue mainly comes from the fact that no measurements are available at the large gap from subcarrier 27 to 37 .",
    "the lmmse estimator and the cwcu lmmse estimator incorporate the prior knowledge from which results in a huge performance gain over the blue .",
    "the cwcu lmmse estimator almost reaches the lmmse estimator s performance , and in contrast to the latter it additionally shows the beneficial property of conditional unbiasedness .",
    "we now turn to frequency response estimators . from a straight forward trivial estimator for @xmath143",
    "follows to @xmath144 .",
    "this estimator fulfills the unbiasedness condition for every @xmath134 , but since @xmath143 lies in a linear subspace of @xmath145 ( spanned by the columns of @xmath146 ) , this estimator is not the blue .",
    "the lmmse estimator which commutes over linear transformations is @xmath147 .",
    "@xmath148 corresponds to the blue as discussed in .",
    "the cwcu lmmse estimator does not commute over general linear transformations , but by using the prior covariance matrix @xmath149 , @xmath150 ( which is not @xmath151 ) can easily be derived from @xmath152 by applying proposition [ prop : cwculmmse003 ] .",
    "[ fig : complete_cir_frequency_domain ] shows the bayesian mses of @xmath153 , @xmath152 , @xmath150 , and @xmath154 . practically we are usually only interested in estimates at the occupied 52 subcarrier positions .",
    "however , in this work we study the estimators performances at all 64 subcarrier positions , since this highlights some interesting properties . the blue significantly outperforms @xmath154 , and in contrast to the latter it is able to estimate the frequency response at all subcarriers .",
    "however , the performance at the large gap from subcarrier 27 to 37 , where no measurements are available , is extremely poor .",
    "( the maximum bayesian mse appears at subcarrier 32 and exhibits the huge value of around 36 . ) in contrast , the lmmse estimator and the cwcu lmmse estimator show excellent interpolation properties along the huge gap . as in the time domain",
    "the cwcu lmmse estimator comes close to the lmmse estimator s performance .    , @xmath152 , @xmath150 , and @xmath154.,width=316 ]",
    "in this work we investigated the cwcu lmmse estimator under different model assumptions .",
    "first we derived the estimator for the case that the measurements and the parameters are jointly gaussian .",
    "then we concentrated on the linear model , where the only assumption made on the noise vector is its independence on the parameter vector .",
    "the cwcu lmmse estimator has been derived for correlated gaussian parameter vectors , and for the case that the parameters are mutually independent ( and otherwise distributed arbitrarily ) . for the remaining cases the cwcu lmmse estimator may correspond to a globally unbiased estimator .",
    "l. fu - mller , m. lunglmayr , m. huemer ,  reducing the gap between linear biased classical and linear bayesian estimation ,  in _ proc .",
    "ieee statistical signal processing workshop ( ssp 2012 ) _ , pp .",
    "65 - 68 , ann arbor , michigan , usa , august 2012 .",
    "m. triki , d.t.m .",
    "slock ,  component - wise conditionally unbiased bayesian parameter estimation : general concept and applications to kalman filtering and lmmse channel estimation ,  in _ proc .",
    "asilomar conference on signals , systems and computers _ ,",
    "pp . 670 - 674 , pacific grove , usa , nov . 2005 .",
    "m. triki , a. salah , d.t.m .",
    "slock ,  interference cancellation with bayesian channel models and application to tdoa / ipdl mobile positioning ,  in _ proc .",
    "international symposium on signal processing and its applications _ , pp.299 - 302 , august 2005 . m. triki , d.t.m .",
    "slock ,  investigation of some bias and mse issues in block - component - wise conditionally unbiased lmmse ,  in _ proc .",
    "asilomar conference on signals , systems and computers _ , pp.1420 - 1424 , pacific grove , usa , nov .",
    "m. huemer , o. lang , `` on component - wise conditionally unbiased linear bayesian estimation , '' _ to be published in the proceedings of the asilomar conference on signals , systems , and computers _ , pacific grove , usa , november 2014 ."
  ],
  "abstract_text": [
    "<S> the classical unbiasedness condition utilized e.g. by the best linear unbiased estimator ( blue ) is very stringent . by softening the  global  unbiasedness condition and introducing component - wise conditional unbiasedness conditions instead , the number of constraints limiting the estimator s performance can in many cases significantly be reduced . in this work </S>",
    "<S> we investigate the component - wise conditionally unbiased linear minimum mean square error ( cwcu lmmse ) estimator for different model assumptions . </S>",
    "<S> the prerequisites in general differ from the ones of the lmmse estimator . </S>",
    "<S> we first derive the cwcu lmmse estimator under the jointly gaussian assumption of the measurements and the parameters . </S>",
    "<S> then we focus on the linear model and discuss the cwcu lmmse estimator for jointly gaussian parameters , and for mutually independent ( and otherwise arbitrarily distributed ) parameters , respectively . in all these cases the cwcu lmmse estimator incorporates the prior mean and the prior covariance matrix of the parameter vector . for the remaining cases optimum linear cwcu estimators exist , but they may correspond to globally unbiased estimators that do not make use of prior statistical knowledge about the parameters . finally , the beneficial properties of the cwcu lmmse estimator are demonstrated with the help of a well - known channel estimation application .    estimation , bayesian estimation , best linear unbiased estimator , blue , linear minimum mean square error , lmmse , cwcu , channel estimation . </S>"
  ]
}