{
  "article_text": [
    "parallel linear algebra algorithms have been developed for many hardware architectures with the aim of accelerating routine calculations @xcite . in this paper",
    "we report on our experiences with parallel calculation of determinants of large nearly singular matrices with very high accuracy .",
    "the need to solve such a problem came from research on the famous riemann s _",
    "zeta function_. the distribution of zeroes of zeta function has puzzled mathematicians for over a century .",
    "the famous riemann hypothesis @xcite , which was included by d. hilbert at the very end of xix century as part of his 8-th problem , and which is also one of the clay institute seven millennium problems @xcite , is to prove or disprove that all non - real zeroes of @xmath0 lie on the critical line @xmath1 . among other things , riemann s zeta function",
    "is closely related to the distribution of prime numbers , whose fundamental role in mathematics is well known .",
    "recently , the second author has proposed what he called _",
    "artless method _ for studying zeroes of the zeta function ( see @xcite and visit @xcite ) .",
    "it consists in approximating zeroes by using a special interpolant built on already known zeroes of zeta function . in the process of building the interpolant ,",
    "it is necessary to compute with high accuracy the determinant of a large matrix , as a function of a parameter in its last column , and hence the signed minors corresponding to the last column are needed .",
    "the matrix is nearly singular , and it was required to perform calculations with the accuracy of ten thousand decimal places .",
    "furthermore , certain patterns in the behaviour of these minors as a function of matrix size @xmath2 needed to be investigated @xcite , and therefore the sequence of arrays of minors for matrix sizes @xmath3 was required .",
    "all those special requirements prevented us from mapping directly the problem at hand to one of the standard parallel linear algebra solutions and using existing tools such as scalapack or plasma @xcite .",
    "in this work we describe our approaches to solving the problem of calculating the required sequence of minors with high precision on various architectures .",
    "initially we attempted using graphics processing units ( gpus ) for this task , and designed and implemented a parallel algorithm for this architecture , using quad - precision ( 256-bits ) and arbitrary precision libraries @xcite .",
    "our initial experiments , though , indicated that gpu architecture delivered about the same efficiency as one single core of a modern cpu when using arbitrary precision libraries ( with over 1000 bits accuracy ) .",
    "subsequently we implemented a parallel algorithm for shared memory multicore architecture using ` pthread ` library , which exhibited linear reduction of cpu time when using up to 16 cores .",
    "however , it soon became clear that only a sufficiently large cluster would be capable of solving the problem with ten thousand decimal places ( 32kbits accuracy ) for @xmath2 in the order of 10 thousand .",
    "indeed , just to store the elements of one matrix of that size , 400 gb of ram was needed .",
    "therefore we designed and implemented an mpi ( message - passing interface ) algorithm and verified it on several computer clusters , running on up to 200 cores .",
    "we dealt appropriately with the issue of load balancing , and at the end observed nearly linear gain in performance with the number of nodes used .",
    "we successfully verified the algorithm on the computations with two types of matrices needed for the artless method .",
    "this paper is structured as follows . in section [ sec1 ]",
    "we will formally describe the problem and give a brief introduction to some elements of the artless method . in section [ sec2 ] we present computational algorithm and its various parallelizations for gpu - cuda , pthread , openmp and mpi platforms . in section [ sec3 ]",
    "we detail some of the numerical experiments and quantify the performance of our algorithm , including the loss of accuracy .",
    "section [ sec_conc ] concludes .",
    "zeta function is defined in the complex plane by the analytic continuation of the series @xmath4 which converges for @xmath5 .",
    "it satisfies the functional equation @xmath6 riemann also defined a symmetric version of the above , using function @xmath7 , @xmath8 which yields a simpler functional equation @xmath9    the _ trivial zeroes _ of @xmath10 are negative even integers , and they are its only real zeroes .",
    "the other , _ non - trivial zeroes _ can only be found in the strip @xmath11 .",
    "the zeroes of the function @xmath7 are exactly the non - trivial zeroes of the function @xmath10 . also following riemann",
    ", we make a change of variables @xmath12 and define @xmath13 the functional equation implies that @xmath14 is even function , @xmath15 . with this notation , the riemann hypothesis states that all zeroes of @xmath14 are real numbers .",
    "an important relation of zeta function to prime numbers was given by von mangoldt .",
    "let chebyshev function @xmath16 be @xmath17 then for non - integer @xmath18 greater than  @xmath19 , by von mangoldt s theorem @xmath20 can be expressed as @xmath21 the first sum runs over the trivial zeroes of zeta , and the second sum runs over the non - trivial zeroes .",
    "in fact , knowing zeroes of zeta function , one could compute primes by merely looking at the graph of the right - hand side of , and identifying the powers of primes by jumps in the graph .    assuming the riemann hypothesis and that all zeroes are simple ,",
    "let us denote by @xmath22 the real zeroes of @xmath14 listed by increasing of absolute values .",
    "further , let @xmath23 thanks to the functional equation we formally have @xmath24    we define the _ interpolating determinant _ as @xmath25    clearly , the determinant @xmath26 vanishes as soon at @xmath27 is equal to @xmath28 , because for such a @xmath27 there are two equal columns in .",
    "the surprising observation is that @xmath26 vanishes also at certain points extremely close to a number of the next zeroes @xmath29 .",
    "the larger is @xmath2 , the more subsequent zeroes approximately coincide with the zeroes of @xmath30 .",
    "for example determinant @xmath31 has zeroes having more than 500 common decimal places with @xmath32 .",
    "the determinant @xmath26 can be expanded into the linear combination of functions @xmath33 , @xmath34 where @xmath35    the object of interest are normalized minors @xmath36 in particular , function @xmath37 has the same zeroes as @xmath26 .",
    "the second matrix of interest had a slightly different structure @xmath38 where @xmath39 are the non - trial zeroes of @xmath10 and @xmath40 .",
    "similarly , zeroes of @xmath41 for @xmath42 have more than 1000 common decimal places with @xmath43 .",
    "the second author observed several patterns of behaviour of the sequences of coefficients @xmath44 for various @xmath2 @xcite .",
    "these patterns have clear number - theoretical meaning , but in order to discover and assess such patterns , which have very fine structure , one has to compute the minors given in ( [ minors ] ) with extremely high accuracy to avoid numerical artifacts .",
    "further , the interpolating matrices are nearly singular , and the values @xmath45 approach zero very rapidly with @xmath2 ( e.g. @xmath46 ) , so one expects very large losses of precision in numerical calculation .",
    "all this dictates the need to use very high accuracy in calculations , of order of ten thousand decimal places .    at this point",
    "we abandon the topic of riemann s zeta function , and focus squarely on calculating the determinants @xmath47 ( or @xmath48 ) and the corresponding normalized minors , as the main numerical complexity of the artless method comes from these calculations .",
    "there is a number of ways determinants can be evaluated numerically .",
    "matrix factorisation is a standard method , although not necessarily the best when computations are parallelised .",
    "our initial thought was to look at parallel versions of the condensation method .",
    "this condensation method was inspired by the celebrated dodgson s condensation method @xcite . in dodgson",
    "s method , an @xmath49 matrix determinant is  condensed \" into an @xmath50 determinant by calculating @xmath51 connected subdeterminants of sizes @xmath52 . this algorithm is trivially parallel , yet it suffers from the requirement of having no zeroes in the interior of the matrix .",
    "inspired by the parallel nature of the condensation steps , the authors of @xcite proposed a variant of the condensation method , in which the determinant of an @xmath49 matrix @xmath53 is replaced by the determinant of an @xmath50 matrix @xmath54 , calculated as follows @xmath55 for @xmath56 , and @xmath57 for @xmath58 , where @xmath59 is the smallest column index of a non - zero element in the first row of @xmath53 .",
    "the relation between @xmath53 and @xmath54 is that @xmath60 after @xmath61 condensation steps the determinant of @xmath53 reduces to a single number divided by a product of powers of pivot factors .",
    "the condensation method from @xcite was taken up by @xcite , where it was slightly modified to avoid division by @xmath62 , by factoring out the pivot element @xmath63 from the first column of @xmath53 ( hence getting a modified matrix @xmath64 ) , so that the relation between the determinants of the matrices becomes @xmath65 with @xmath66 obtained from @xmath64 in ( [ condensation ] ) .",
    "successive condensation steps produce the array of pivot elements , whose product is the determinant of @xmath53 .",
    "this modification of the condensation method is data parallel as elements of @xmath54 are computed independently of each other .",
    "the authors of @xcite developed a parallel version of their algorithm suitable for gpus and assessed its performance .",
    "we started with formula ( [ condensation ] ) and also developed our own gpu parallel algorithm , differing from that of @xcite in that all computations were performed in place ( instead of having two rotating matrices for input and output ) , and that we used high precision arithmetic for all steps ( we used quad - precision qd library for gpus and gpuprec arbitrary precision libraries for gpus @xcite ) .",
    "we soon realised though , that condensation steps in ( [ condensation ] ) were exactly the steps of gaussian elimination , and hence the version of condensation algorithm in @xcite and @xcite was in fact not different from gaussian elimination without pivoting .",
    "optional pivoting ( as it was done in @xcite ) can be trivially added .    knowing the equivalence of with the gaussian elimination steps , calculations could be streamlined , and computational complexity can be confirmed to be @xmath67 .",
    "however , the question of calculation of minors corresponding to the last column of the matrix @xmath53 was still outstanding .",
    "the naive approach by calculating @xmath2 determinants ( [ minorsdef ] ) was not appealing because of computational cost .",
    "it is known that one can compute a determinant of size @xmath2 and all its @xmath68 minors in the same time @xmath67 .",
    "it is based on the result from @xcite which establishes that the complexity of evaluation of all partial derivatives of certain functions ( in particular multivariate polynomials ) is a constant multiple of the complexity of evaluating the function itself .",
    "since all the @xmath68 minors are partial derivatives of the determinant , the result follows . additionally , would it be possible to compute not just one array of signed minors @xmath45 for a particular @xmath2 , but the whole series of such arrays for @xmath3 in one non - redundant computation ?    to answer these questions we used the following trick",
    "let us construct the augmented matrix @xmath69,\\ ] ] where @xmath70 is the identity matrix of size @xmath2 .",
    "perform gauss elimination on @xmath71 . for completeness",
    ", the algorithm is shown on fig .",
    "[ alg : gauss ] .",
    "the last @xmath2 elements of the last row will contain the signed minors of @xmath53 corresponding to the last column , divided by @xmath72 . to see this , recall",
    "that gauss elimination ( without pivoting ) of a non - singular matrix @xmath53 is equivalent to multiplying it by a lower triangular matrix @xmath73 from the left , @xmath74 where @xmath75 is in row - echelon form .",
    "the same operations applied to @xmath70 result in @xmath76 .",
    "now , the product of the last row of @xmath73 and the last column of @xmath53 yield @xmath77 , since @xmath75 is in row - echelon form . if we now multiply @xmath78 by @xmath72 , we obtain the result , @xmath79 .",
    "we can see that all the required signed minors can be computed automatically using gaussian elimination ( the determinant of @xmath53 is found from the product of diagonal elements of @xmath73 ) with the help of additional row operations .",
    "the computational cost of it is twice that of gaussian elimination , and computational complexity remains @xmath67 .",
    "storagewise , the matrix @xmath73 can be stored in the lower triangular part of @xmath53 , because once the row @xmath80 is processed in gaussian elimination , the elements @xmath81 below diagonal with @xmath82 are no longer required ( indeed they are zero in row - echelon form ) . on the other hand , at step @xmath80 of the elimination process , only the elements @xmath83 with @xmath82 and the main diagonal are different from zero .",
    "therefore , nonzero elements @xmath83 can be stored below the diagonal of @xmath53 , and the only extra storage required is for the overlapping diagonals of @xmath53 and @xmath73 .",
    "therefore all calculations can be done in memory @xmath84 , as only one extra array of size @xmath2 is required .",
    "next we turn to the second question of whether it is possible to compute the whole series of the minors , for different sizes of the matrices @xmath3 .",
    "the answer here is also affirmative , and in fact no extra work or storage is required at all . to see this",
    ", note that at step @xmath80 of gaussian elimination , the row @xmath80 of matrix @xmath73 contains the desired ( up to a factor ) minors , corresponding to the @xmath80th column of @xmath53 .",
    "we apply the same reasoning as before to show this is the case : note that after step @xmath80 , the diagonal element of the row echelon form @xmath85 , which is the product of the @xmath80th row of @xmath73 and @xmath80th column of @xmath53 .",
    "to put it into the context of our motivating problem , all normalised minors @xmath45 ( [ minors ] ) , @xmath86 , @xmath87 can be computed with cpu cost @xmath88 and storage @xmath89 using gaussian elimination .",
    "there are methods for matrix inversion and calculation of determinant based on fast matrix multiplication , e.g. schnhage - strassen @xcite and coppersmith - winograd algorithms @xcite , which have complexity @xmath90 and @xmath91 respectively . through the result of @xcite ,",
    "all @xmath68 minors can also be computed in the same time , although we were unable to find implementations of such algorithms in the literature .",
    "however , it appears that the fast matrix multiplicaiton algorithms are not structured to facilitate computation of the whole sequence of arrays of minors for @xmath87 in one run .",
    "therefore we settle on gaussian elimination as the most efficient way of computing all the normalised minors @xmath45 .",
    "the question of available ram becomes important for computations with larger @xmath2 of order of 10000 . as we mentioned previously , to store a matrix of that size , with 10 thousand decimal places accuracy",
    ", 400 gb of ram is required .",
    "one approach we pursued is to partition the matrix into blocks and use memory paging to fit the blocks into available ram .",
    "let us partition the matricies into @xmath92 blocks of size @xmath93 , with @xmath94 .",
    "each block is stored on a hard disk and is loaded to ram when needed .",
    "as soon as certain blocks are in ram , we perform all possible steps of gaussian elimination with data in these blocks . at each point in time at most four blocks are needed in ram , hence ram required is @xmath95 .",
    "however , now we do not use shared space to keep both matrix @xmath53 and @xmath73 , so memorywise the algorithm uses @xmath96 space .",
    "the operations within blocks were executed in parallel in different threads on a single host , using openmp library , however distinct sets of blocks could be processed in parallel on different hosts in a specified order but otherwise asynchronously . this way a cluster of hosts can be used for parallel execution",
    "@xmath97 , @xmath98 ( identity matrix ) 2 .   for @xmath99",
    "@xmath100 2 .",
    "for @xmath101 do : 1 .   @xmath102 2 .   for @xmath103 do : + @xmath104 3 .",
    "@xmath105 4 .",
    "for @xmath106 do : + @xmath107 3 .",
    "return @xmath108 and @xmath73 .",
    "we implemented a variant of gaussian elimination with high accuracy on four platforms :    1 .   on gpu using cuda and cump @xcite arbitrary precision and gqd @xcite quad - double ( 256 bits ) precision libraries , connected to a single host ; 2 .",
    "on a cluster of multicore cpus using ` openmp ` for multithreading and gmp @xcite arbitrary precision library , partitioning the matrix into blocks and using memory paging ; 3 .   on a multicore cpu using ` pthread ` for multithreading and gmp @xcite arbitrary precision library ; 4 .   on a computer cluster using mpi , and using gmp , mpfr @xcite and mpigmp @xcite libraries .",
    "the following parallelisation strategies were adopted . on gpu , at every step @xmath109 of the gauss elimination algorithm , we spawned @xmath2 threads , and each thread @xmath110 was performing calculations in the @xmath110th column of matrix @xmath53 starting from row @xmath109 .",
    "while this was efficient in terms of coalescent global memory access pattern for data type ` double ` and to some degree for ` quad - double ` , for arbitrary precision numbers the way gpu threads accessed elements of the matrix @xmath53 did not matter , as the data were stored in non - sequential locations anyway , and hence was misalligned .",
    "we did not observe any differences in cpu time when parallelisation was done columnwise , rowwise or elementwise .    when we used memory paging and matrix partitioning in the second approach , parallelization was performed in two ways .",
    "first , we created a queue of tasks , from which tasks were taken by idle hosts based on certain pre - conditions .",
    "each task consisted of a block in which gaussian elimination was performed , and pre - conditions ensured that a block was processed only when all the blocks containing rows and columns with smaller indices have been processed .",
    "second , within each block the computations were parallelized with openmp .    common to the remaining implementations",
    ", we parallelised the algorithm rowwise , that is , each thread was performing calculations in a specific row , or a group of rows .",
    "the simplest way is to break the matrix into @xmath111 blocks of rows sequentially , where @xmath111 is the number of threads , and let each thread process its own block .",
    "some load balancing strategy is needed , as the first thread will complete processing of its rows before the rest and will be then idle , then it will be the second thread and so on . to avoid this , on shared memory architectures we reassigned the blocks to each thread every now and then , so that at all steps the sizes of the blocks were approximately the same .",
    "this was easily done by simply changing the pointers to blocks of rows each thread was responsible for , hence no copying of data took place , and overheads were negligible .    on distributed memory architectures , though , such as clusters running mpi ,",
    "this method was not suitable as it would involve copying large chunks of data between the hosts .",
    "instead , we interleaved the rows of the matrix , i.e. , thread @xmath110 was processing rows @xmath110 , @xmath112 , @xmath113 , ... and so on , @xmath114 .",
    "the threads processed the pivot rows of the matrix @xmath53 in turns , in round robin fashion .",
    "this way load balancing was implicit , and all threads had equal job to do until the very end of the computations .",
    "each pivot row was broadcast to all the threads at every step @xmath109 of the algorithm , hence @xmath2 rows were broadcast altogether , which is the smallest number when the matrix is partitioned among the threads , and complexity of the data transfers was therefore @xmath115 ( we remind that broadcast in mpi has logarithmic time ) .",
    "the row being broadcast was packed into a binary array using ` mpigmp ` library @xcite , and then unpacked at the receiving end .",
    "here we make an observation regarding the value of partitioning the matrix into blocks and using paged memory as opposed to storing the whole matrix in ( combined ) ram .",
    "it has two main advantages : 1 ) less ram is needed , and 2 ) the upper limit on the matrix size @xmath116 needs not be fixed in advance : we can gradually get determinants of sizes @xmath117 .",
    "extra blocks can be added later on . on the other hand ,",
    "less ram means that some blocks were processed sequentially rather than in parallel .",
    "the cost of data transfer between hard disk and ram was not significant compared to the cost of the actual computations .",
    "however the overall storage requirement was 50% higher , as the matrix @xmath73 could not share the same space as @xmath53 .",
    "we opted for the fourth method in our final computations , based on mpi and using combined ram of the available hosts , for the following reasons : 1 ) cpu and ram resources available to us were sufficient to perform computations for @xmath118 with 10 thousand decimal places accuracy , and in fact the amount of combined ram exceeded our permanent storage quota ; 2 ) coding the algorithm was more straightforward ; 3 ) at that moment in time , we did not need to increase @xmath116 beyond 12000 , because our computations have shown that the loss of accuracy during numerical computations was such that the results were unreliable beyond that number .",
    "we compared the speed of determinant calculations on various platforms using the following hardware : tesla c2070 gpus with 6 gb of ram , and clusters of intel e5 - 2670 nodes with @xmath119 gb of ram , connected by 4x qdr infiniband interconnect , running centos 6 linux .",
    "the hardware was provided by the vpac and monash e - research centre http://www.vpac.org , http://www.monash.edu.au / eresearch/.    while tesla gpu was quite efficient in evaluating determinants with gqd package , when we passed to much higher accuracy ( from 256 to 8192 bits and more ) , gpu performance became equivalent to that of one cpu core , see table [ tab1 ] .",
    "the computing time grew with the accuracy at the rate of @xmath120 , @xmath121 being the number of bits used for multiprecision float numbers .",
    "this is consistent with the schoolbook multiplication algorithm employed in cump library .",
    "subsequently we implemented a parallel shared memory algorithm using ` pthread ` library , and ran it on up to 16 intel cpu cores ( on the same host ) .",
    "the efficiency increased linearly with the number of cores employed , as no significant interprocess communication took place , see table [ tab2 ] . therefore the cpu time has decreased 16-fold compared to running the algorithm on a gpu .",
    "furthermore , since gmp library @xcite uses a faster karatsuba multiplication algorithm with complexity @xmath122 , the benefit was even greater .",
    "as we mentioned previously , current shared memory architecture has a limitation of the total ram that could be used and the limited number of cores .",
    "therefore we opted to parallelising the algorithm for an mpi cluster .",
    "here we observed linear gain in performance as evidenced by table [ tab3 ] .",
    "this is consistent with the fact that the dominating term in the overall complexity of the algorithm @xmath67 is due to computations , and the complexity of data transfers @xmath115 is comparatively small .      given that the matrices involved in the computations are large and ill conditioned , a valid question of the accuracy of the end result arises . to estimate the loss of accuracy we performed computations with various precisions : 8 kbits , 32 kbits , and to a limited extent , 64 kbits .",
    "the input data , the zeroes of riemann s zeta function were taken with just over 32 kbits accuracy ( 10000 decimal places ) @xcite . by comparing the results calculated with different accuracies",
    ", we could estimate empirically the rate of accuracy loss during gaussian elimination .",
    "figures [ accuracy64 ] , [ accuracy8 ] present graph of the number of coinciding decimal places in the normalised minors @xmath123 as a function of matrix size @xmath2 .",
    "we compared both the average and the smallest number of coinciding decimal places for @xmath124 , which turn out to be very close ( difference @xmath125% ) .",
    "what we observe is that the accuracy decays linearly .",
    "the equation of regression line in figure [ accuracy64 ] is @xmath126 .",
    "this indicates that the accuracy of calculations with 32 kbits accuracy is predicted to have 1400 correct decimal places . on the other hand , calculations with 8",
    "kbits accuracy were valid only up to @xmath127 , and after that point were completely unreliable .",
    "therefore the choice of 10000 decimal places accuracy was sufficient , and also necessary for calculations with matrices up to @xmath128 in size .",
    "the coefficient @xmath129 indicates the number of decimal places lost in each row operation .",
    "it corresponds to @xmath130 bits average accuracy loss per one row operation .",
    "this result also shows that if the input values were calculated with 10000 decimal places , then it only makes sense to increase @xmath2 to 13500 , because after that size the loss of accuracy will make the results unreliable .",
    "the above analysis was performed for determinants @xmath48 in ( [ betanew ] ) .",
    "a similar analysis was performed for @xmath131 in ( [ det ] ) , which yielded a slightly different regression lines , e.g.  the loss of accuracy at @xmath132 kbits was approximated by @xmath133 .",
    "the coefficient @xmath134 indicates that for that matrix , the loss of accuracy was @xmath135 bits for each row operation .",
    "hence it appears that matrix in ( [ det ] ) is slightly better conditioned than that in ( [ betanew ] ) .              after completing initial evaluations , we performed two production runs of the algorithm on vpac ( http://www.vpac.org ) and massive ( http://www.massive.org.au ) clusters , using up to 168 processes .",
    "we used the upper limit on the size of the matrix of @xmath118 and accuracy of 32768 bits .",
    "one cluster was used for @xmath136 from ( [ det ] ) and the second for @xmath137 from ( [ betanew ] ) .",
    "the accurate values of riemann s zeroes were precomputed @xcite .",
    "the algorithms ran for 5 and 7 days respectively .",
    "the results of our computations constituted almost 700 gb of compressed high accuracy data .",
    "our results helped to confirm earlier calculations in @xcite for @xmath136 up to @xmath138 , which were performed with block partitioning of the matrix @xmath53 .",
    "however we noticed some deviations from the earlier results for larger @xmath2 .",
    "it was later confirmed that our current results were correct , hence these calculations were valuable in correcting previous computational errors .",
    "the results with the determinant @xmath137 from ( [ betanew ] ) were new , and they helped establish various patterns in the values of the normalised minors @xmath45 , in particular their fine structure related to prime numbers , as presented recently in @xcite .",
    "as we discussed earlier , the loss of accuracy in arithmetical operations was quite significant due to nearly degenerate matrices .",
    "in addition , matrix entries themselves were computed up to 10 thousand decimal places ; that was the accuracy of zeroes of zeta function we started with .",
    "however , our analysis is empirical , it provides only an indication , but not a guarantee , that the results are accurate to the indicated number of decimal places .",
    "interval arithmetic can be employed to obtain rigorous results . at every operation ,",
    "the upper and lower bounds on the result can be computed . a library ` mpfi ` @xcite is available for such calculations with arbitrary precision .",
    "however the cost of such computations is doubling the cpu time and memory requirements .",
    "in addition , interval computations provide pessimistic error bounds , much larger than the actual errors .",
    "a posteriori interval analysis @xcite provides more realistic error bounds , but it requires @xmath67 memory , and hence bears prohibitive cost in our case .",
    "it might be possible to reduce its cost by not keeping all intermediate values , and in addition it may help save cpu time , as due to the inevitable loss of accuracy , later iterations of the algorithm need not be performed with full precision .",
    "we leave this analysis for future work .",
    "calculation of matrix determinant is one of the standard linear algebra operations needed for many computational tasks .",
    "determinants of ill - conditioned matrices are a particular challenge because of rapidly degrading accuracy , hence very high precision calculations are needed .",
    "we have presented an algorithm based on gaussian elimination which computes not only the determinant but a series of determinants and minors corresponding to one column or row for matrix sizes @xmath87 in one run .",
    "the algorithm has @xmath88 cpu and @xmath139 memory complexity .",
    "we parallelized and implemented this algorithm on four different parallel architectures , including gpu and mpi - based clusters , using arbitrary precision arithmetics .",
    "we confirmed that the cpu wall time decreases linearly with the number of cpu cores used ( for fixed matrix size and accuracy ) .",
    "our production runs involved up to 168 cores , 400 gb combined ram and determinants of up to @xmath118 in size with 10 thousand decimal places accuracy .",
    "we empirically estimated the losses of accuracy during gaussian elimination and found that the accuracy used was appropriate for computations with that matrix size .",
    "we applied the algorithm to studies of the riemann zeta function as part of the 8-th hilbert problem .",
    "the results helped the second author to observe various theoretically interesting patterns in the values of the normalized minors related to prime numbers @xcite .",
    "the authors wish to acknowledge support by victorian partnership in advanced computing ( vpac ) and monash e - research centre for providing computing resources at their clusters , and specifically mr .",
    "s. michnowicz for his help in developing mpi parallelization code .",
    "part of the calculations was performed on the `` chebyshev '' supercomputer of moscow state university supercomputing center .",
    "the work of the second author was partly supported by the programme of fundamental research ",
    "modern problems of theoretical mathematics  of the mathematics branch of the russian academy of sciences .",
    "e.  agullo , j.  demmel , j.  dongarra , b.  hadri , j.  kurzak , j.  langou , h.  ltaief , p.  luszczek , and s.  tomov . numerical linear algebra on emerging architectures : the plasma and magma projects .",
    ", 180:012037 , 2009 .",
    "l.  s. blackford , j.  choi , a.  cleary , e.  dazevedo , j.  demmel , i.  dhillon , j.  dongarra , s.  hammarling , g.  henry , a.  petitet , k.  stanley , d.  walker , and r.  c. whaley . .",
    "society for industrial and applied mathematics , philadelphia , pa , 1997 .",
    "new conjectures about zeroes of riemann s zeta function .",
    "research report ma12 - 03 of the department of mathematics of the university of leicester , available online http://www2.le.ac.uk/departments/mathematics/research/research-reports-2/reports-2012/ma12_03matiyasevich.pdf and from @xcite , 2012 .",
    "calculation of riemann s zeta function via interpolating determinants .",
    "preprint 2013 - 18 of max planck institute for mathematics in bonn , available online http://www.mpim-bonn.mpg.de/preblob/5368 and from @xcite , 2013 .",
    "matiyasevich and g.  beliakov .",
    "zeroes of riemann s zeta function on the critical line with 20000 decimal digits accuracy , research data australia , available online http://hdl.handle.net/10536/dro/du:30051725 , 2013 .      t.  nakayama and d.  takahashi .",
    "implementation of multiple - precision floating - point arithmetic library for gpu computing . in _ proc .",
    "23rd iasted international conference on parallel and distributed computing and systems ( pdcs 2011 ) _ , pages 343349 .",
    "iasted , 2011 ."
  ],
  "abstract_text": [
    "<S> we present a parallel algorithm for calculating very large determinants with arbitrary precision on computer clusters . </S>",
    "<S> this algorithm minimises data movements between the nodes and computes not only the determinant but also all minors corresponding to a particular row or column at a little extra cost , and also the determinants and minors of all submatrices in the top left corner at no extra cost . </S>",
    "<S> we implemented the algorithm in arbitrary precision arithmetic , suitable for very ill conditioned matrices , and empirically estimated the loss of precision . </S>",
    "<S> the algorithm was applied to studies of riemann s zeta function .    </S>",
    "<S> * keywords * determinant , linear algebra , parallel algorithms , message passing interface , gpu , riemann s zeta function .    </S>"
  ]
}