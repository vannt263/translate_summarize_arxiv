{
  "article_text": [
    "as is well known , a basic problem in information retrieval is to determine how relevant a particular document is to a query . in the automatic ad hoc retrieval setting ,",
    "examples of relevant documents are not supplied .",
    "given this absence of explicit relevance evidence , it is important to consider what other information sources can be exploited .    in methods patterned after the classic tf.idf document - vector approach to text representation ,",
    "the focus is mostly on utilizing within - document features , such as term frequencies .",
    "information drawn from the corpus as a whole generally consists of aggregates of statistics gathered from each document considered in isolation ; for example , the inverse document frequency is based on checking , for each document , whether that document contains a particular term .",
    "recent work has demonstrated the effectiveness of an alternative approach wherein probabilistic models of text generation are constructed from documents , and these induced _ language models _ ( lms ) are used to perform document ranking @xcite .",
    "like tf.idf and related techniques , though , language - modeling methods typically use only individual - document features and corpus - wide aggregates of the same .",
    "( corpus term counts are generally employed for _ smoothing _ , so that unseen text can be assigned non - zero probability . )",
    "neither of the aforementioned approaches typically makes use of a potentially very powerful source of information : the _ similarity structure _ of the corpus .",
    "clusters are a convenient representation of similarity whose potential for improving retrieval performance has long been recognized @xcite . from our point of view",
    ", one key advantage is that they provide smoothed , more representative statistics for their elements , as has been recognized in statistical natural language processing for some time @xcite .",
    "for example , we could infer that a document not containing a certain query term is still relevant if the document belongs to a cluster whose component documents generally do contain the term .    however , relying on clusters alone has some potential drawbacks .",
    "clustering at retrieval time can be very expensive , but off - line clustering seems , by definition , query - independent and therefore may be based on factors that are irrelevant to user information need .",
    "also , cluster statistics may over - generalize with respect to specific member documents .",
    "we therefore propose a framework for incorporating both corpus - structure information  using pre - computed , overlapping clusters  and individual - document information .",
    "importantly , although cluster formation is query - independent , within our framework the _ choice _ of which clusters to incorporate _ can _ depend on the query .",
    "we then consider several of the many possible algorithms arising as specific instantiations of our framework .",
    "these include both novel methods and , as special cases , both the standard , non - cluster - based lm approach and a variant of the cluster - based aspect model @xcite .",
    "our empirical evaluation consists of experiments in an array of settings created by varying several parameters and meta - parameters ; these include the corpus , the information representation ( e.g. , language models versus tf.idf-style vectors ) , and , when applicable , the smoothing method selected .",
    "we find that even the worst - performing of our novel algorithms is competitive with the lm approach , and indeed always provides substantial improvement in recall .",
    "in general , our algorithms provide good performance in comparison to a number of recently proposed methods , thus demonstrating that our integration approach to incorporating document and corpus - structure information is an effective way to improve ad hoc retrieval .",
    "[ [ notational - conventions ] ] notational conventions + + + + + + + + + + + + + + + + + + + + + +    we use @xmath0 and @xmath1 to denote a document , query , cluster , and corpus , respectively . a fixed vocabulary is assumed .",
    "we use the notation @xmath2 for the _ language model _  which assigns probabilities to text strings over the fixed vocabulary  induced from @xmath3 by some pre - specified method , and @xmath4 for the language model induced from @xmath5 .",
    "( section [ sec : lmbasics ] describes the induction methods we used in our experiments . )",
    "it is convenient to use _ kronecker delta _",
    "notation @xmath6}$ ] to set up some definitions .",
    "the argument @xmath7 is a statement ; @xmath6 } = 1 $ ] if @xmath7 holds , 0 otherwise .",
    "as noted above , when we rank documents with respect to a query , we desire per - document scores that rely both on information drawn from the particular document s contents and on how the document is situated within the similarity structure of the ambient corpus .    [ [ structure - representation - via - overlapping - clusters ] ] structure representation via overlapping clusters + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    document clusters are an attractive choice for representing corpus similarity structure ( see @xcite for extended discussion ) .",
    "clusters can be thought of as _ facets _ of the corpus that users might be interested in . given that a particular document can be relevant to a user for several reasons , or to different users for different reasons , we believe that a set of overlapping clusters forms a better model for similarity structure than a partitioning of the corpus . furthermore , employing intersecting clusters may reduce information loss due to the generalization that clustering can introduce ( * ? ? ?",
    "[ [ information - representation ] ] information representation + + + + + + + + + + + + + + + + + + + + + + + + + +    motivated by the empirical successes of language - modeling - based approaches @xcite , we use language models induced from documents and clusters as our information representation . thus , @xmath8 and @xmath9 specify our initial knowledge of the relation between the query @xmath10 and a particular document @xmath3 or cluster @xmath5 , respectively .",
    "( however , section [ sec : results ] shows that using a tf.idf representation also yields performance improvements with respect to the appropriate baseline , though not to the same degree as using language models does . )",
    "[ [ information - integration ] ] information integration + + + + + + + + + + + + + + + + + + + + + + +    to assign a ranking to the documents in a corpus @xmath1 with respect to @xmath10 , we want to score each @xmath11 against @xmath10 in a way that incorporates information from query - relevant corpus facets to which @xmath3 belongs .",
    "while one could compute clusters specific to @xmath10 at retrieval time , efficiency considerations compel us to create @xmath12 , the set of clusters , in advance , and hence in a query - independent fashion .",
    "to compensate , at retrieval time we base the _ choice _ of appropriate facets on the query .",
    "how might cluster information be used ?",
    "our discussion above indicates that clusters can serve two roles .",
    "insofar as they approximate true facets of the corpus , they can aid in the _ selection _ of relevant documents : we would want to retrieve those that belong to clusters corresponding to facets of interest to the user . on the other hand ,",
    "clusters also have the capacity to _ smooth _ individual - document language models , since they pool statistics from multiple documents .",
    "finally , we must remember that over - reliance on @xmath9 can over - generalize by failing to account for document - specific information encoded in @xmath8 .",
    "these observations motivate the algorithm template shown in figure [ fig : alg - template ] .",
    "this template is fairly general : both the standard language - modeling approach @xcite and the aspect model @xcite are concrete instantiations . in the template",
    ", the choice of @xmath13 corresponds to utilizing clusters in their selection role .",
    "the scoring step can be thought of as integrating @xmath8 with cluster - based language models in their smoothing role .",
    "the optional re - ranking step is used as a way to further bias the final ranking towards document - specific information , if desired .",
    "note that re - ranking can change the average non - interpolated precision but not the absolute precision or recall of the retrieval results ; we therefore use it , when necessary , to enhance average precision .",
    "( section [ sec : results ] reports experiments studying its efficacy . )    in the next section , we describe a number of specific algorithms arising from this template , concentrating on their degree of dependence on cluster - induced language models .",
    "@xmath14 ? + & n / a & @xmath8 & ( redundant ) + & @xmath15 & @xmath16}$ ] & ( redundant ) + & @xmath17 & @xmath16}$ ] & ( redundant ) + & @xmath17 & @xmath18 & yes + & @xmath17 & @xmath19 & yes + & @xmath17 & @xmath20 & yes + & @xmath17 & @xmath21 & no +    table [ tab : models ] summarizes the algorithms we consider , which represent a few choices out of the many possible ways to instantiate the template of figure [ fig : alg - template ] .",
    "our preference in picking these algorithms has been towards simpler methods , so as to focus on the impact of using cluster information ( as opposed to the impact of tuning many weighting parameters ) .",
    "[ [ first - step - cluster - formation - and - selection ] ] first step : cluster formation and selection + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    there are many algorithms that can be used to create @xmath12 , the set of overlapping document clusters required by figure [ fig : alg - template ] s template . in our experiments , we simply have each document @xmath3 form the _ basis _ of a cluster @xmath22 consisting of @xmath3 and its @xmath23 nearest neighbors , where @xmath24 is a free parameter . (",
    "note that two clusters with different basis documents may contain the same set of documents . )",
    "inter - document distance is measured by the kullback - leibler ( kl ) divergence between the corresponding ( smoothed ) language models , as in @xcite .",
    "the idea behind our use of cohorts is that a document s nearest neighbors in similarity space represent a local `` fragment '' or `` tile '' of the overall similarity structure of the corpus .",
    "our evaluation results show that even this relatively unsophisticated way to approximate facets enables effective leveraging of corpus structure ; at the very least , it serves as a form of nearest - neighbor smoothing ( see below ) .",
    "the first retrieval - time action specified by our algorithm template is to choose @xmath13 , a query - dependent subset of @xmath12 . in all the algorithms described below except the baseline ( which does nt use cluster information )",
    ", there is a document - selection aspect to this subset , in that only documents in some @xmath25 can appear in the final ranked - list output .",
    "ideally , we would use the clusters best approximating those ( true ) facets of the corpus that are most representative of the user s interests , as expressed by @xmath10 ; therefore , we require that @xmath13 be a subset of @xmath26 , the top @xmath27 clusters @xmath5 with respect to @xmath9 .",
    "but we also want to evaluate @xmath3 only with respect to the facets it actually exhibits .",
    "thus , in what follows ( except for the baseline ) , @xmath13 is always defined to be a subset of @xmath28 ; we assume @xmath27 is large enough to produce the desired number of retrieved documents @xmath29 .    [ [ baseline - method ] ] baseline method + + + + + + + + + + + + + + +    the baseline for our experiments , denoted * lm * , is to simply rank documents by @xmath8  no cluster information is used .",
    "details of our particular implementation are given in section [ sec : experiments ] .",
    "[ [ selection - methods ] ] selection methods + + + + + + + + + + + + + + + + +    in this class of algorithms , the cluster - induced language models play a very small role once the set @xmath13 is selected .",
    "in essence , the standard language - modeling approach ( that is , ranking by @xmath8 ) is invoked to rank ( some of ) the documents comprising the clusters in @xmath13 .",
    "this method of scoring is intended to serve as a precision - enhancing mechanism , downgrading documents that happen to be members of some @xmath30 by dint of similarity to @xmath3 in respects not pertinent to @xmath10 .    in the * basis - select * algorithm , the net effect of the definition given in table [ tab : models ]",
    "is that only the _ basis _ documents of the clusters in @xmath26 are allowed to appear in the final output list .",
    "thus , this algorithm uses the pooling of statistics from documents in @xmath22 simply to decide whether @xmath3 is worth ranking ; the rank itself is based solely on @xmath8 .",
    "the * set - select * algorithm differs in that _ all _ the documents in the clusters in @xmath26 may appear in the final output list  the `` set '' referred to in the name is the union of the clusters in @xmath26 .",
    "the idea is that any document in a `` best '' cluster , basis or not , is potentially relevant and should be ranked .",
    "again , the ranking of the selected documents is by @xmath8 .",
    "be the number of documents in the @xmath31 highest - ranked clusters .",
    "then , only the @xmath32 documents in the @xmath27th cluster that are closest , in the kl - divergence sense , to the cluster s basis are allowed into @xmath33 . ]",
    "another natural variant of the same idea is that documents appearing in more than one cluster in @xmath26 should get extra consideration , given that they appear in several ( approximations of ) facets thought to be of interest to the user .",
    "this idea gives rise to the * bag - select * algorithm , so named in reference to the incorporation of a document s multiplicity in the _ bag _ formed from the `` multi - set union '' of all the clusters in @xmath13 .",
    "first , each selected document @xmath3 is assigned a score consisting of the product of its language - modeling score @xmath8 and the number of `` top '' clusters it belongs to .",
    "the @xmath29 top - scoring documents are then re - ranked via @xmath8 and presented in the new sorted order .    [",
    "[ aspect - xmethods ] ] aspect - xmethods + + + + + + + + + + + + + + +    we now turn to algorithms making more explicit use of clusters as smoothing mechanisms .",
    "in particular , we study what we term `` aspect - x '' methods .",
    "our choice of name is a reference to the work of hofmann and puzicha , which conceives of clusters as explanatory latent variables underlying the observed data .",
    "( the `` x '' stands for `` extended '' ) . in our setting",
    ", this idea translates to using @xmath9 as a proxy for @xmath8 , where the degree of dependence on a particular @xmath9 is based on the strength of association between @xmath3 and @xmath5 .",
    "the * aspect - x * algorithm measures this association by @xmath34 ; the * uniform - aspect - x * algorithm assumes that every @xmath35 has the same degree of association to @xmath5 . in both cases ,",
    "re - ranking by @xmath8 is applied .",
    "the scoring function we use for our aspect - xalgorithm can be motivated by appealing to the probabilistic derivation of the aspect model @xcite , as follows .",
    "it is a fact that @xmath36 the aspect model assumes that a query is conditionally independent of a document given a cluster ( which is a way of using clusters to smooth individual - document statistics ) , in which case @xmath37 . if we further assume that @xmath38 and @xmath39 are constant , we can write @xmath40 , where @xmath41 is a constant that does nt affect ranking . our aspect - xalgorithm then arises by replacing the conditional probabilities with the corresponding language models and only summing over the clusters in @xmath13 . constraining which clusters participate in the sum to those of relatively high rank is important : experiments indicate that using a large number of clusters could be detrimental .",
    "we note , however , that it appears difficult within the strictly probabilistic framework of the original aspect model to incorporate such a constraint : a particular cluster s rank depends on all the other clusters , but none of the terms in the basic aspect - model equation explicitly conditions on them .",
    "[ [ a - hybrid - algorithm ] ] a hybrid algorithm + + + + + + + + + + + + + + + + + +    the selection - only algorithms emphasize @xmath8 in scoring a document @xmath3 ; in contrast , the aspect - xalgorithms rely on @xmath9 .",
    "we created the * interpolation * algorithm to combine the advantages of these two approaches .",
    "the algorithm can be derived by dropping the original aspect model s conditional independence assumption  namely , that @xmath42  and instead setting @xmath43 in equation [ eqn : factor ] to @xmath44 , where @xmath45 indicates the degree of emphasis on individual - document information .",
    "if we do so , then via some algebra we get @xmath46 . finally , applying the same assumptions as described in our discussion of the aspect - xalgorithm yields a score function that is the linear interpolation of the score of the standard lm approach and the score of the aspect - xalgorithm . note",
    "that no re - ranking step occurs ; as we shall see , the interpolationalgorithm s incorporation of document - specific information yields higher precision .",
    "document clustering has a long history in information retrieval @xcite ; in particular , approximating topics via clusters is a recurring theme @xcite .",
    "arguably the work most related to ours by dint of employing both clustering and language modeling in the context of ad hoc retrieval is that on latent - variable models , e.g. , @xcite , of which the classic aspect model is one instantiation .",
    "such work takes a strictly probabilistic approach to the problems we have discussed with standard language modeling , as opposed to our algorithmic viewpoint .",
    "also , a focus in the latent - variable work has been on sophisticated cluster induction , whereas we find that a very simple clustering scheme works rather well in practice .",
    "interestingly , hofmann @xcite linearly interpolated his probabilistic model s score , which is based on ( soft ) clusters , with the usual cosine metric ; this is quite close in spirit to what our interpolationalgorithm does .",
    "implicit corpus structure is also exploited by lafferty and zhai s _ expanded query language model _ @xcite .",
    "their method uses interleaved document - term markov chains ( which can be thought of as tracing `` paths '' between related documents ) to enhance language models built from queries .",
    "this is similar conceptually to our framework s use of inter - document similarities to enhance the performance of document language models , although in our work the notion of similarity is more explicit .    [ cols=\"<,^,^,^,^,^,^,^,^\",options=\"header \" , ]",
    "in summary , we have proposed a general framework that enables the development of a variety of algorithms for integrating corpus similarity structure , modeled via clusters , and document - specific information .",
    "although our proposal is motivated by the recent language - modeling approach to information retrieval , and the specific algorithms presented here do use language models for representation purposes to good effect , we observed that the framework also can be used with basic classic ir techniques such as tf.idf .",
    "an interesting direction for future work is to explore the effect of using alternative clustering algorithms .",
    "we would also like to study the role that overlapping plays in our framework : is most of the performance gain due to the ( high ) degree of overlap in our clusters or to the way structure and individual - document information are integrated ?",
    "another interesting direction is to examine whether other algorithms , such as the lm - based pseudo - feedback methods we used for reference comparisons @xcite , can benefit if we replace the basic lm retrieval algorithm they employ with one of ours .",
    "most importantly , we would like to develop a principled probabilistic interpretation of the framework we have proposed .",
    "we have done some preliminary work based on considering the factorization @xmath47 ; some of the components of our scoring functions can be considered to be ( very rough ) approximations of the terms in this factorization .",
    "creating a rigorous probabilistic foundation for the work described here is one of our main future goals .",
    "*   we thank eric breck , claire cardie , shimon edelman , thorsten joachims , art munson , bo pang , ves stoyanov , and the anonymous reviewers for valuable comments .",
    "thanks to chengxiang zhai and victor lavrenko for answering questions about their work , and andrs corrada - emmanuel for responding to queries about lemur .",
    "this paper is based upon work supported in part by the national science foundation under grants itr / im iis-0081334 and iis-0329064 and by an alfred p. sloan research fellowship .",
    "any opinions , findings , and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the national science foundation or sloan foundation ."
  ],
  "abstract_text": [
    "<S> most previous work on the recently developed _ language - modeling _ approach to information retrieval focuses on document - specific characteristics , and therefore does not take into account the structure of the surrounding corpus . </S>",
    "<S> we propose a novel algorithmic framework in which information provided by document - based language models is enhanced by the incorporation of information drawn from _ clusters _ of similar documents . using this framework </S>",
    "<S> , we develop a suite of new algorithms . </S>",
    "<S> even the simplest typically outperforms the standard language - modeling approach in precision and recall , and our new _ interpolation _ </S>",
    "<S> algorithm posts statistically significant improvements for both metrics over all three corpora tested . </S>"
  ]
}