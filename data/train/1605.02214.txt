{
  "article_text": [
    "since its invention by @xcite , the lasso estimator has become increasingly important in statistics and other related fields such as machine learning and econometrics , and large number of papers have studied its properties .",
    "many of these papers have been concerned with the choice of the penalty parameter @xmath0 required for the implementation of the lasso estimator . as a result , several methods to choose @xmath0 have been developed and theoretically justified ; see , for example , @xcite , @xcite , and @xcite . however , in practice researchers often rely upon cross - validation to choose @xmath0 ( see @xcite , @xcite , and @xcite for examples ) , and to the best of our knowledge , there exist few results in the literature about properties of the lasso estimator when @xmath0 is chosen using cross - validation ; see a review of existing results below .",
    "the purpose of this paper is to fill this gap and to derive a rate of convergence of the cross - validated lasso estimator .",
    "we consider the regression model @xmath10 = 0,\\ ] ] where @xmath11 is a dependent variable , @xmath12 a @xmath5-vector of covariates , @xmath13 unobserved scalar noise , and @xmath14 a @xmath5-vector of coefficients .",
    "assuming that a random sample of size @xmath4 , @xmath15 , from the distribution of the pair @xmath16 is available , we are interested in estimating the vector of coefficients @xmath17 .",
    "we consider triangular array asymptotics , so that the distribution of the pair @xmath16 , and in particular the dimension @xmath5 of the vector @xmath18 , is allowed to depend on @xmath4 . for simplicity of notation",
    ", however , we keep this dependence implicit .",
    "we assume that the vector of coefficients @xmath17 is sparse in the sense that @xmath19 is ( potentially much ) smaller than @xmath5 . under this assumption ,",
    "the effective way to estimate @xmath17 was introduced by @xcite who suggested the lasso estimator : @xmath20 where for @xmath21 , @xmath22 denotes the @xmath23 norm of @xmath24 , and @xmath0 is some penalty parameter ( the estimator suggested in tibshirani s paper takes a slightly different form but over the time the version has become more popular , probably for computational reasons ) .",
    "whenever the solution of the optimization problem in is not unique , we assume for concreteness that one solution is chosen according to some pre - specified rule ; in particular , we assume that a solution with the smallest number of non - zero components is selected .    to perform the lasso estimator @xmath25 , one has to choose the penalty parameter @xmath0 .",
    "if @xmath0 is chosen appropriately , the lasso estimator is consistent with @xmath26 rate of convergence in the prediction norm under fairly general conditions ; see , for example , @xcite or @xcite . on the other hand , if @xmath0 is not chosen appropriately , the lasso estimator may not be consistent or may have slower rate of convergence ; see @xcite .",
    "therefore , it is important to select @xmath0 appropriately . in practice , it is often recommended to choose @xmath0 using cross - validation as described in the next section . in this paper , we analyze properties of the lasso estimator @xmath25 when @xmath27 is chosen using ( @xmath1-fold ) cross - validation and in particular , we demonstrate that under certain mild regularity conditions , if the conditional distribution of @xmath28 given @xmath18 is gaussian and @xmath29 , then @xmath30 with probability @xmath31 up - to some constant @xmath32 , where for @xmath21 , @xmath33 denotes the prediction norm of @xmath24 .",
    "thus , under our conditions , the cross - validated lasso estimator @xmath34 achieves the fastest possible rate of convergence in the prediction norm up - to the logarithmic factor @xmath7 .",
    "we do not know whether this logarithmic factor can or can not be dropped .    under the same conditions as above",
    ", we also derive a sparsity bound for the cross - validated lasso estimator ; in particular , we show that @xmath35 with probability @xmath31 up - to some constant @xmath32 .",
    "moreover , we demonstrate that our proof technique generates a non - trivial rate of convergence in the prediction norm for the cross - validated lasso estimator even if @xmath5 is ( potentially much ) larger than @xmath4 ( high - dimensional case ) and the gaussian assumption fails . because some steps used to derive do not apply , however , the rate turns out to be sub - optimal , and our bound is probably not sharp in this case .",
    "nonetheless , we are hopeful that our proof technique will help to derive the sharp bound for the non - gaussian high - dimensional case in the future .    given that cross - validation is often used to choose the penalty parameter @xmath0 for the lasso estimator and given how popular the lasso estimator is , deriving a rate of convergence of the cross - validated lasso estimator is an important question in the literature ; see , for example , @xcite , where further motivation for the topic is provided .",
    "yet , to the best of our knowledge , the only results in the literature about cross - validated lasso estimator are due to @xcite .",
    "@xcite showed that if the penalty parameter is chosen using @xmath1-fold cross - validation from a range of values determined by their techniques , the lasso estimator is risk consistent , which under our conditions is equivalent to consistency in the @xmath36 norm .",
    "@xcite derived a similar result for leave - one - out cross - validation .",
    "@xcite derived a rate of convergence of the cross - validated lasso estimator that depends on @xmath4 via @xmath37 but they substantially restricted the range of values over which cross - validation search is performed .",
    "these are useful results but we emphasize that in practice the cross - validation search is often conducted over a fairly large set of values of the penalty parameter , which could potentially be much larger than required in their results . in contrast , we derive a rate of convergence that depends on @xmath4 via @xmath38 , and we impose only minor conditions on the range of values of @xmath0 used by cross - validation .",
    "other papers that have been concerned with cross - validation in the context of the lasso estimator include @xcite and @xcite .",
    "@xcite developed a novel cross - validation - type procedure to choose @xmath0 and showed that the lasso estimator based on their choice of @xmath0 has a rate of convergence depending on @xmath4 via @xmath37 .",
    "their procedure to choose @xmath0 , however , is related to but different from the classical cross - validation procedure used in practice .",
    "@xcite studied classical cross - validation but focused on estimators that differ from the lasso estimator in important ways .",
    "for example , one of the estimators they considered is the average of subsample lasso estimators , @xmath39 , for @xmath40 defined in in the next section . although the authors studied properties of cross - validated version of such estimators in great generality , it is not immediately clear how to apply their results to obtain bounds for the cross - validated lasso estimator itself .",
    "we emphasize that deriving a rate of convergence of the cross - validated lasso estimator is a non - standard problem .",
    "in particular , classical techniques to derive properties of cross - validated estimators developed for example in @xcite do not apply to the lasso estimator as those techniques are based on the linearity of the estimators in the vector of dependent variables @xmath41 , which does not hold in the case of the lasso estimator .",
    "more recent techniques , developed for example in @xcite , help to analyze sub - sample lasso estimators like those studied in @xcite but are not sufficient for the analysis of the full - sample lasso estimator .",
    "see @xcite for an extensive review of results on cross - validation available in the literature .",
    "the rest of the paper is organized as follows . in the next section ,",
    "we describe the cross - validation procedure . in section [ sec :",
    "regularity conditions ] , we state our regularity conditions . in section",
    "[ sec : main results ] , we present our main results . in section [ sec : simulations ] , we describe results of our simulation experiments . in section [ sec :",
    "proofs ] , we provide proofs of the main results . in section",
    "[ sec : technical lemmas ] , we give some technical lemmas that are useful for the proofs of the main results .    * notation .",
    "* throughout the paper , we use the following notation . for any vector @xmath21",
    ", we use @xmath42 to denote the number of non - zero components of @xmath24 , @xmath22 to denote its @xmath23 norm , @xmath43 to denote its @xmath36 norm ( the euclidean norm ) , @xmath44 to denote its @xmath45 norm , and @xmath33 to denote its prediction norm .",
    "in addition , we use the notation @xmath46 if @xmath47 for some constant @xmath32 that is independent of @xmath4 .",
    "moreover , we use @xmath48 to denote the unit sphere in @xmath49 , that is , @xmath50 .",
    "further , for any matrix @xmath51 , we use @xmath52 to denote its spectral norm .",
    "also , with some abuse of notation , we use @xmath53 to denote the @xmath54th component of the vector @xmath55 and we use @xmath56 to denote the @xmath57th realization of the vector @xmath18 in the random sample @xmath15 from the distribution of the pair @xmath16 .",
    "finally , for any finite set @xmath58 , we use @xmath59 to denote the number of elements in @xmath58 .",
    "we introduce more notation in the beginning of section [ sec : proofs ] , as required for the proofs in the paper .",
    "as explained in the introduction , to choose the penalty parameter @xmath0 for the lasso estimator @xmath25 , it is common practice to use cross - validation . in this section ,",
    "we describe the procedure in details .",
    "let @xmath1 be some strictly positive ( typically small ) integer , and let @xmath60 be a partition of the set @xmath61 ; that is , for each @xmath62 , @xmath63 is a subset of @xmath64 , for each @xmath65 with @xmath66 , the sets @xmath63 and @xmath67 have empty intersection , and @xmath68 . for our asymptotic analysis , we will assume that @xmath1 is a constant that does not depend on @xmath4 .",
    "further , let @xmath69 be a set of candidate values of @xmath0 .",
    "now , for @xmath70 and @xmath71 , let @xmath72 be the lasso estimator corresponding to all observations excluding those in @xmath63 where @xmath73 is the size of the subsample @xmath63 .",
    "as in the case with the full - sample lasso estimator @xmath74 in , whenever the optimization problem in has multiple solutions , we choose one with the smallest number of non - zero components .",
    "then the cross - validation choice of @xmath0 is @xmath75 the cross - validated lasso estimator in turn is @xmath76 . in the literature ,",
    "the procedure described here is also often referred to as @xmath1-fold cross - validation . for brevity",
    ", however , we simply refer to it as cross - validation .",
    "below we will study properties of @xmath34 .",
    "recall that we consider the model given in , the lasso estimator @xmath74 given in , and the cross - validation choice of @xmath0 given in .",
    "let @xmath77 , @xmath78 , @xmath79 , and @xmath80 be some strictly positive numbers where @xmath81 and @xmath82 .",
    "also , let @xmath83 , @xmath84 , and @xmath85 be sequences of positive numbers , possibly growing to infinity . to derive our results ,",
    "we will impose the following regularity conditions .",
    "[ as : covariates ] the random vector @xmath55 is such that we have @xmath86)^{1/2}\\leq c_1 $ ] and @xmath87)^{1/4}\\leq \\gamma_n$ ] for all @xmath88 . in addition , @xmath89)^{1/4}\\leq \\gamma_n$ ] and @xmath90 .",
    "the first part of assumption [ as : covariates ] means that all eigenvalues of the matrix @xmath91 $ ] are bounded from above and below from zero .",
    "the second part of this assumption , that is , the condition that @xmath87)^{1/4}\\leq \\gamma_n$ ] for all @xmath88 , is often assumed in the literature with @xmath92 ; see @xcite for an example . to develop some intuition about this and other parts of assumption [ as : covariates ] , we consider three examples .",
    "[ ex : gaussian ] suppose that the vector @xmath18 consists of independent standard gaussian random variables .",
    "then for all @xmath88 , the random variable @xmath93 is standard gaussian as well , and so the condition that @xmath87)^{1/4}\\leq \\gamma_n$ ] for all @xmath88 is satisfied with @xmath94 .",
    "similarly , the condition that @xmath89)^{1/4}\\leq \\gamma_n$ ] holds with @xmath95 .",
    "in addition , @xmath96 is a chi - square random variable with @xmath5 degrees of freedom in this case , and so for all @xmath97 , we have @xmath98 ; see , for example , section 2.4 and example 2.7 in @xcite . setting @xmath99 in this inequality shows that the condition that @xmath100 is satisfied with @xmath101 .",
    "[ ex : bounded independent ] suppose that the vector @xmath18 consists of independent zero - mean bounded random variables .",
    "in particular , suppose for simplicity that @xmath102 almost surely .",
    "then for all @xmath97 and @xmath88 , we have @xmath103 by hoeffding s inequality . therefore ,",
    "the condition that @xmath87)^{1/4}\\leq \\gamma_n$ ] for all @xmath88 is satisfied with @xmath104 by the standard calculations . also , the condition that @xmath89)^{1/4}\\leq \\gamma_n$ ] is satisfied with @xmath105 , and the condition that @xmath100 is satisfied with @xmath106 .",
    "[ ex : bounded ] suppose that the vector @xmath18 consists of not necessarily independent bounded random variables .",
    "in particular , suppose for simplicity that @xmath102 almost surely .",
    "then the condition that @xmath87)^{1/4}\\leq \\gamma_n$ ] for all @xmath88 is satisfied with @xmath107 since @xmath108\\leq { { \\mathrm{e}}}[(x'\\delta)^2\\|x\\|^2\\|\\delta\\|^2]\\leq p { { \\mathrm{e}}}[(x'\\delta)^2]\\leq c_1 ^ 2 p$ ] . also , like in example [ ex : bounded independent ] , the conditions that @xmath89)^{1/4}\\leq \\gamma_n$ ] and that @xmath90 are satisfied with @xmath105 and @xmath106 .",
    "[ as : heterogeneity ] we have @xmath109\\leq c_1 $ ] almost surely .",
    "this assumption means that the variance of the conditional distribution of @xmath13 given @xmath18 is bounded from above and below from zero .",
    "the lower bound is needed to avoid potential super - efficiency of the lasso estimator .",
    "such bounds are typically imposed in the literature .",
    "[ as : growth condition ] we have @xmath110 where @xmath111)^{1/q}$ ] .",
    "in addition , @xmath112 and @xmath113 .",
    "assumption [ as : growth condition ] is a mild growth condition restricting some moments of @xmath18 and also the number of non - zero coefficients in the model , @xmath6 . in the remark below , we discuss conditions of this assumption in three examples given above .",
    "in example [ ex : gaussian ] above , this assumption reduces to the following conditions : ( i ) @xmath114 for some constant @xmath115 and ( ii ) @xmath116 since in this case , @xmath117 for all @xmath82 and some constant @xmath118 that depends only on @xmath80 .",
    "in example [ ex : bounded independent ] , assumption [ as : growth condition ] reduces to the following conditions : ( i ) @xmath119 for some constant @xmath115 and ( ii ) @xmath116 since in this case , @xmath120 for all @xmath82 . in example",
    "[ ex : bounded ] , assumption [ as : growth condition ] reduces to the following conditions : ( i ) @xmath116 and ( ii ) @xmath121 . indeed , under assumptions of example [",
    "ex : bounded ] , we have @xmath120 for all @xmath82 , and so the condition that @xmath110 follows from the condition that @xmath122 but for @xmath80 large enough , this condition follows from @xmath123 and @xmath121 . note that our conditions in examples [ ex : gaussian ] and [ ex : bounded independent ] allow for the high - dimensional case , where @xmath5 is ( potentially much ) larger than @xmath4 but conditions in example [ ex : bounded ] hold only in the moderate - dimensional case , where @xmath5 is asymptotically smaller than @xmath4 .",
    "[ as : candidate set ] the candidate set @xmath69 takes the following form : @xmath124 .",
    "it is known from @xcite that the optimal rate of convergence of the lasso estimator in the prediction norm is achieved when @xmath0 is of order @xmath125 . since under assumption [ as : growth condition ] , we have @xmath126 , it follows that our choice of the candidate set @xmath69 in assumption [ as : candidate set ] makes sure that there are some @xmath0 s in the candidate set @xmath127 that would yield the lasso estimator with the optimal rate of convergence in the prediction norm .",
    "note also that assumption [ as : candidate set ] gives a rather flexible choice of the candidate set @xmath69 of values of @xmath0 ; in particular , the largest value , @xmath78 , can be set arbitrarily large and",
    "the smallest value , @xmath128 , converges to zero rather fast .",
    "in fact , the only two conditions that we need from assumption [ as : candidate set ] is that @xmath69 contains a `` good '' value of @xmath0 , say @xmath129 , such that the subsample lasso estimators @xmath130 satisfy the bound in lemma [ lem : 1 ] with probability @xmath31 , and than @xmath131 up - to a constant that depend only on @xmath77 and @xmath78 .",
    "thus , we could for example set @xmath132 .",
    "[ as : dataset partition ] for all @xmath70 , we have @xmath133 .    assumption [ as : dataset partition ]",
    "is mild and is typically imposed in the literature on @xmath1-fold cross - validation .",
    "this assumption ensures that all subsamples @xmath63 are balanced and their sizes are of the same order .",
    "recall that for @xmath134 , we use @xmath33 to denote the prediction norm of @xmath24 .",
    "our first main result in this paper derives a rate of convergence of the cross - validated lasso estimator @xmath135 in the prediction norm for the gaussian case where @xmath136 .",
    "as explained in remark [ rem : condition xi ] below , the last condition implies that this is a moderate - dimensional case , where @xmath5 is asymptotically smaller than @xmath4 .",
    "[ thm : cross validation ] suppose that assumptions [ as : covariates ]  [ as : dataset partition ] hold .",
    "in addition , suppose that @xmath136 .",
    "finally , suppose that the conditional distribution of @xmath28 given @xmath18 is gaussian .",
    "then @xmath137 with probability @xmath138 up - to a constant depending only on @xmath77 , @xmath78 , @xmath1 , @xmath79 , and @xmath80 .",
    "[ rem : 1 ] let @xmath139 be a constant such that @xmath140\\leq \\sigma^2 $ ] almost surely . the results in @xcite imply that under assumptions of theorem [ thm : cross validation ] , setting @xmath141 for sufficiently large constant @xmath32 gives the lasso estimator @xmath142 satisfying @xmath143 , and it follows from @xcite that this is the optimal rate of convergence ( in the minimax sense ) for the estimators of @xmath17 in the model . therefore , theorem [ thm : cross validation ] shows that the cross - validated lasso estimator @xmath135 has the fastest possible rate of convergence in the prediction norm up - to the logarithmic factor @xmath7 .",
    "note , however , that implementing the cross - validated lasso estimator does not require knowledge of @xmath139 , which makes this estimator attractive in practice .",
    "the rate of convergence established in theorem [ thm : cross validation ] is also very close to the oracle rate of convergence , @xmath144 , that could be achieved by the ols estimator if we knew the set of covariates @xmath53 having non - zero coefficient @xmath145 ; see , for example , @xcite .",
    "[ rem : proof ] one of the ideas in @xcite is to show that outside of the event @xmath146 where @xmath147 is some constant , the lasso estimator @xmath74 satisfies the bound @xmath148 .",
    "thus , to obtain the lasso estimator with fast rate of convergence , it suffices to choose @xmath0 such that @xmath0 is small enough but the event holds at most with probability @xmath149 . the choice @xmath150 described in remark [ rem : 1 ] satisfies these two conditions .",
    "the difficulty with cross - validation , however , is that , as we demonstrate in section [ sec : simulations ] via simulations , it typically yields a rather small value of @xmath0 , so that the event with @xmath27 holds with non - trivial probability even in large samples , and little is known about properties of the lasso estimator @xmath74 when the event does not hold , which is perhaps one of the main reasons why there are only few results on the cross - validated lasso estimator in the literature .",
    "we therefore take a different approach .",
    "first , we use the fact that @xmath151 is the cross - validation choice of @xmath0 to derive bounds on @xmath152 and @xmath153 for the subsample lasso estimators @xmath154 defined in .",
    "second , we use the `` degrees of freedom estimate '' of @xcite to derive a sparsity bound for these estimators , and so to bound @xmath155 .",
    "third , we use the two point inequality @xmath156 which can be found in @xcite , with @xmath157 , a convex combination of the subsample lasso estimators @xmath154 , and derive a bound for its right - hand side using the definition of estimators @xmath154 and bounds on @xmath152 and @xmath155 .",
    "finally , we use the triangle inequality to obtain a bound on @xmath158 from the bounds on @xmath159 and @xmath153 .",
    "the details of the proof , including a short proof of the two point inequality , can be found in section [ sec : proofs ] .",
    "[ rem : condition xi ] note that in examples [ ex : gaussian ] , [ ex : bounded independent ] , and [ ex : bounded ] above , the condition that @xmath160 reduces to @xmath29 , which we used in the abstract and in the introduction . in fact , lemma [ lem : xi bound ] in section [ sec : technical lemmas ] shows that under assumptions [ as : covariates ] and [ as : growth condition ] , we have @xmath161 , so that @xmath5 is necessarily asymptotically smaller than @xmath4 under the condition @xmath162 .",
    "this is why we refer to the case where @xmath162 as the moderate - dimensional case .",
    "in addition to the bound on the prediction norm of the estimation error of the cross - validated lasso estimator given in theorem [ thm : cross validation ] , we derive in the next theorem a bound on the sparsity of the estimator .",
    "[ thm : sparsity ] suppose that all conditions of theorem [ thm : cross validation ] are satisfied .",
    "then @xmath163 with probability @xmath31 up - to a constant depending only on @xmath77 , @xmath78 , @xmath1 , @xmath79 , and @xmath80 .",
    "@xcite showed that if @xmath0 is chosen so that the event holds at most with probability @xmath149 , then the lasso estimator @xmath74 satisfies the bound @xmath164 with probability @xmath31 , so that the number of covariates that have been mistakenly selected by the lasso estimator is at most of the same order as the number of non - zero coefficients in the original model .",
    "as explained in remark [ rem : proof ] , however , cross - validation typically yields a rather small value of @xmath0 , so that the event with @xmath27 holds with non - trivial probability even in large samples , and it is typically the case that smaller values of @xmath0 lead to the lasso estimators @xmath74 with a larger number of non - zero coefficients . however , using the result in theorem [ thm : cross validation ] and the `` degrees of freedom estimate '' of @xcite , we are still able to show that the cross - validated lasso estimator is typically rather sparse , and in particular satisfies the bound with probability @xmath31 .    with the help of theorems [ thm : cross validation ] and [ thm : sparsity ] , we immediately arrive at the following corollary for the bounds on @xmath36 and @xmath23 norms of the estimation error of the cross - validated lasso estimator :    [ cor : gaussian case ] suppose that all conditions of theorem [ thm : cross validation ] are satisfied",
    ". then @xmath165 with probability @xmath31 up - to a constant depending only on @xmath77 , @xmath78 , @xmath1 , @xmath79 , and @xmath80 .    to conclude this section , we consider the non - gaussian case .",
    "one of the main complications in our derivations for this case is that without the assumption of gaussian noise , we can not apply the `` degrees of freedom estimate '' derived in @xcite that provides a bound on the number of non - zero coefficients of the lasso estimator , @xmath166 , as a function of the prediction norm of the estimation error of the estimator , @xmath167 ; see lemmas [ lem : 0.5 ] and [ lem : sparsity bound low dimensional ] in the next section .",
    "nonetheless , we can still derive an interesting bound on @xmath168 in this case even if @xmath5 is much larger than @xmath4 ( high - dimensional case ) :    [ thm : cross validation non gaussian high dimensional ] suppose that assumptions [ as : covariates ]  [ as : dataset partition ] hold .",
    "in addition , suppose that for all @xmath169 , we have @xmath170 \\leq c_1 t^2 $ ] .",
    "finally , suppose that @xmath171 .",
    "then @xmath172 with probability @xmath138 up - to a constant depending only on @xmath77 , @xmath78 , @xmath1 , @xmath79 , and @xmath80 .    this theorem does not require the noise @xmath13 to be gaussian conditional on @xmath18 . instead , it imposes a weaker condition that for all @xmath169 , we have @xmath173\\leq c_1 t^2 $ ] , which means that the conditional distribution of @xmath13 given @xmath18 is sub - gaussian ; see , for example , @xcite . also , we want to emphasize that the condition that @xmath171 is not necessary to derive a non - trivial bound on @xmath168 but it does simplify the bound . inspecting the proof of theorem [ thm : cross validation non gaussian high dimensional ]",
    "reveals that without this condition , the bound would take the form : @xmath174 with probability @xmath31 up - to a constant depending only on @xmath77 , @xmath78 , @xmath1 , @xmath79 , and @xmath80 .",
    "in this section , we present results of our simulation experiments .",
    "the purpose of the experiments is to investigate finite - sample properties of the cross - validated lasso estimator . in particular , we are interested in ( i ) comparing estimation error of the cross - validated lasso estimator in different norms to the lasso estimator based on other choices of @xmath0 ; ( ii ) studying sparsity properties of the cross - validated lasso estimator ; and ( iii ) estimating probability of the event for @xmath27 , the cross - validation choice of @xmath0 .    we consider two data generating processes ( dgps ) . in both dgps , we simulate the vector of covariates @xmath18 from the gaussian distribution with mean zero and variance - covariance matrix given by @xmath175 = 0.5^{|j - k|}$ ] for all @xmath176 . also , we set @xmath177 .",
    "we simulate @xmath28 from the standard gaussian distribution in dgp1 and from the uniform distribution on @xmath178 $ ] in dgp2 . in both dgps , we take @xmath28 to be independent of @xmath18 .",
    "further , for each dgp , we consider samples of size @xmath179 and @xmath180 . for each dgp and each sample size",
    ", we consider @xmath181 , @xmath182 , and @xmath180 . to construct the candidate set @xmath69 of values of the penalty parameter @xmath0",
    ", we use assumption 4 with @xmath183 , @xmath184 and @xmath185 .",
    "thus , the set @xmath69 contains values of @xmath0 ranging from @xmath186 to @xmath187 when @xmath179 and from @xmath188 to @xmath187 when @xmath189 , that is , the set @xmath69 is rather large in both cases . in all experiments , we use 5-fold cross - validation ( @xmath190 ) .",
    "we repeat each experiment @xmath191 times .    as a comparison to the cross - validated lasso estimator",
    ", we consider the lasso estimator with @xmath0 chosen according to the bickel - ritov - tsybakov rule : @xmath192where @xmath193 and @xmath194 are some constants , @xmath139 is the standard deviation of @xmath28 , and @xmath195 is the inverse of the cumulative distribution function of the standard gaussian distribution ; see @xcite .",
    "following @xcite , we choose @xmath196 and @xmath197 .",
    "the noise level @xmath198 is typically have to be estimated from the data but for simplicity we assume that @xmath139 is known , so we set @xmath199 in dgp1 and @xmath200 in dgp2 . in",
    "what follows , this lasso estimator is denoted as p - lasso and the cross - validated lasso estimator is denoted as cv - lasso .",
    "figure 5.1 contains simulation results for dgp1 with @xmath179 and @xmath181 .",
    "the first three ( that is , the top - left , top - right , and bottom - left ) panels of figure 5.1 present the mean of the estimation error of the lasso estimators in the prediction , @xmath36 , and @xmath23 norms , respectively . in these panels ,",
    "the dashed line represents the mean of estimation error of the lasso estimator as a function of @xmath0 ( we perform the lasso estimator for each value of @xmath0 in the candidate set @xmath69 ; we sort the values in @xmath69 from the smallest to the largest , and put the order of @xmath0 on the horizontal axis ; we only show the results for values of @xmath0 up to order 32 as these give the most meaningful comparisons ) .",
    "this estimator is denoted as @xmath0-lasso .",
    "the solid and dotted horizontal lines represent the mean of the estimation error of cv - lasso and p - lasso , respectively .    from these three panels of figure 5.1",
    ", we see that estimation error of cv - lasso is only slightly above the minimum of the estimation error over all possible values of @xmath0 not only in the prediction and @xmath36 norms but also in the @xmath23 norm . in comparison",
    ", p - lasso tends to have much larger estimation error in all three norms .",
    "the bottom - right panel of figure 5.1 depicts the histogram for the the number of non - zero coefficients of the cross - validated lasso estimator .",
    "overall , this panel suggests that the cross - validated lasso estimator tends to select too many covariates : the number of selected covariates with large probability varies between @xmath201 and @xmath202 even though there are only 4 non - zero coefficients in the true model .    for all other experiments , the simulation results on the mean of estimation error of the lasso estimators can be found in table 5.1 . for simplicity , we only report the minimum over @xmath71 of mean of the estimation error of @xmath203-lasso in table 5.1 .",
    "the results in table 5.1 confirm findings in figure 5.1 : the mean of the estimation error of cv - lasso is very close to the minimum mean of the estimation errors of the @xmath0-lasso estimators under both dgps for all combinations of @xmath4 and @xmath5 considered in all three norms .",
    "their difference becomes smaller when the sample size @xmath4 increases .",
    "the mean of the estimation error of p - lasso is much larger than that of cv - lasso in most cases and is smaller than that of cv - lasso only in @xmath23-norm when @xmath179 and @xmath204 .",
    "table 5.2 reports model selection results for the cross - validated lasso estimator .",
    "more precisely , the table shows probabilities for the number of non - zero coefficients of the cross - validated lasso estimator hitting different brackets .",
    "overall , the results in table 5.2 confirm findings in figure 5.1 : the cross - validated lasso estimator tends to select too many covariates .",
    "the probability of selecting larger models tends to increase with @xmath5 but decreases with @xmath4 .",
    "table 5.3 provides information on the finite - sample distribution of the ratio of the maximum score @xmath205 over @xmath151 , the cross - validation choice of @xmath0 .",
    "more precisely , the table shows probabilities for this ratio hitting different brackets . from table 5.3",
    ", we see that this ratio is above 0.5 with large probability in all cases and in particular this probability exceeds 99% in most cases . hence , with @xmath206 holds with large probability , meaning that deriving the rate of convergence of the cross - validated lasso estimator requires new arguments since existing arguments only work for the case when does not hold ; see discussion in remark [ rem : proof ] above .",
    "in this section , we prove theorems [ thm : cross validation ] , [ thm : sparsity ] , [ thm : cross validation non gaussian high dimensional ] , and corollary [ cor : gaussian case ] . since the proofs are long , we start with a sequence of preliminary lemmas . for convenience ,",
    "we use the following additional notation . for @xmath70",
    ", we denote @xmath207 for all @xmath208 .",
    "we use @xmath209 and @xmath32 to denote constants that can change from place to place but that can be chosen to depend only on @xmath77 , @xmath78 , @xmath1 , @xmath79 , and @xmath80 .",
    "we use the notation @xmath210 if @xmath211 .",
    "in addition , we denote @xmath212 .",
    "moreover , for @xmath213 and @xmath214 , we use @xmath215 to denote the vector in @xmath216 consisting of all elements of @xmath217 corresponding to indices in @xmath218 ( with order of indices preserved ) . finally ,",
    "for @xmath219 , we denote @xmath220}.    in lemmas [ lem : 1 ]  [ lem : 5 ] , we will impose the condition that for all @xmath169 , we have @xmath170 \\leq c_1 t^2 $ ] .",
    "note that under assumption [ as : heterogeneity ] , this condition is satisfied if the conditional distribution of @xmath28 given @xmath18 is gaussian .",
    "[ lem : 1 ] suppose that assumptions [ as : covariates ]  [ as : dataset partition ] hold .",
    "in addition , suppose that for all @xmath169 , we have @xmath170 \\leq c_1 t^2 $ ] .",
    "then there exists @xmath221 , possibly depending on @xmath4 , such that for all @xmath70 , we have @xmath222 with probability @xmath138 .",
    "let @xmath223 and @xmath224 . also , for @xmath70 , denote @xmath225 and @xmath226 to prove the first asserted claim",
    ", we will apply theorem 1 in @xcite that shows that for any @xmath70 and @xmath71 , on the event @xmath227 , we have @xmath228 thus , it suffices to show that there exists @xmath229 such that @xmath230 for all @xmath70 , and that there exist @xmath231 , possibly depending on @xmath4 , such that @xmath232 for all @xmath70 and @xmath233 to prove , note that by jensen s inequality , @xmath234\\big)^{1/2}\\leq \\big({{\\mathrm{e}}}\\big[\\max_{1\\leq i\\leq n}\\max_{1\\leq j\\leq p}|x_{i j}|^q\\big]\\big)^{1/q}\\\\ & \\leq \\big(\\textstyle{\\sum_{i=1}^n}\\displaystyle{{{\\mathrm{e}}}}\\big[\\max_{1\\leq j\\leq p}|x_{i j}|^q\\big]\\big)^{1/q } \\leq n^{1/q } m_n.\\end{aligned}\\ ] ] thus , for @xmath235 , @xmath236 by assumption [ as : growth condition ] . hence , noting that ( i ) all eigenvalues of the matrix @xmath237 $ ] are bounded from above and below from zero by assumption [ as : covariates ] and that ( ii ) @xmath238 by assumption [ as : dataset partition ] and applying lemma [ lem : rv ] with @xmath239 , @xmath1 , and @xmath240 there replaced by @xmath241 , @xmath242 , and @xmath243 here shows that @xmath244 with probability @xmath31 uniformly over all @xmath208 such that @xmath245 and @xmath246 and all @xmath70 .",
    "hence , follows from lemma 10 in @xcite applied with @xmath247 there equal to @xmath248 here .    to prove and fix @xmath70 and note that @xmath249\\lesssim n\\ ] ] by assumptions [ as : covariates ] and [ as : heterogeneity ]",
    "also , @xmath250\\big)^{1/2 } & \\leq \\big({{\\mathrm{e}}}\\big[\\max_{1\\leq i\\leq n}\\max_{1\\leq j\\leq p } |x_{i j}{\\varepsilon}_i|^q\\big]\\big)^{1/q}\\\\ & \\leq \\big(\\textstyle{\\sum_{i=1}^n}\\displaystyle{{{\\mathrm{e}}}}\\big[\\max_{1\\leq j\\leq p } |x_{i j}{\\varepsilon}_i|^q\\big]\\big)^{1/q } \\lesssim n^{1/q } m_n\\end{aligned}\\ ] ] by jensen s inequality , the definition of @xmath251 , and the assumption on the moment generating function of the conditional distribution of @xmath13 given @xmath18 .",
    "thus , by lemma [ lem : maximal inequality ] and assumption [ as : growth condition ] , @xmath252\\lesssim \\sqrt{n\\log p } + n^{1/q}m_n \\log p \\lesssim \\sqrt{n\\log p}.\\ ] ] hence , applying lemma [ lem : maximal inequality 2 ] with @xmath253 and @xmath254 there replaced by @xmath255 here and noting that @xmath256 by assumption [ as : growth condition ] implies that @xmath257 with probability @xmath31 . hence , noting that @xmath258 by assumption [ as : growth condition ] , it follows from assumption [ as : candidate set ] that there exists @xmath259 such that and hold .",
    "further , to prove the second asserted claim , note that using and and applying theorem 2 in @xcite with @xmath260 there shows that @xmath261 with probability @xmath31 for all @xmath70 .",
    "hence , @xmath262 with probability @xmath31 for all @xmath70 , where the second inequality follows from , and the third one from the first asserted claim .",
    "this completes the proof of the lemma .",
    "[ lem : 2 ] suppose that assumptions [ as : covariates ]  [ as : dataset partition ] hold .",
    "in addition , suppose that for all @xmath169 , we have @xmath170 \\leq c_1 t^2 $ ] .",
    "then we have for all @xmath70 that @xmath263 with probability @xmath31 for @xmath129 defined in lemma [ lem : 1 ] .",
    "fix @xmath70 and denote @xmath264 .",
    "we have @xmath265\\big)(\\widehat\\beta - \\beta)\\big|\\\\ & \\quad + \\big|(\\widehat\\beta - \\beta)'\\big(\\frac{1}{n_k}\\sum_{i\\in i_k}x_i x_i ' - { { \\mathrm{e}}}[x x']\\big)(\\widehat\\beta - \\beta)\\big| \\\\ & \\leq \\|\\widehat\\beta - \\beta\\|_1 ^ 2\\max_{1\\leq j , l\\leq p}\\big|\\frac{1}{n - n_k}\\sum_{i\\notin i_k}x_{i j}x_{i l } - { { \\mathrm{e}}}[x_j x_l]\\big|\\\\ & \\quad + \\|\\widehat\\beta - \\beta\\|_1 ^ 2\\max_{1\\leq j , l\\leq p}\\big|\\frac{1}{n_k}\\sum_{i\\in i_k}x_{i j}x_{i",
    "l } - { { \\mathrm{e}}}[x_j x_l]\\big| \\ ] ] by the triangle inequality .",
    "further , by lemma [ lem : 1 ] , @xmath266 with probability @xmath31 and by lemma [ lem : maximal inequality ] , @xmath267\\big|\\big]\\lesssim \\displaystyle{\\left(\\frac{\\gamma_n^4\\log p}{n}\\right)^{1/2 } } + \\frac{m_n^2\\log p}{n^{1 - 2/q}},\\\\ & { { \\mathrm{e}}}\\big[\\max_{1\\leq j , l\\leq p}\\big|\\frac{1}{n_k}\\textstyle{\\sum_{i\\in i_k}}x_{i j}x_{i",
    "l } - { { \\mathrm{e}}}[x_j x_l]\\big|\\big]\\lesssim \\displaystyle{\\left(\\frac{\\gamma_n^4\\log p}{n}\\right)^{1/2 } } + \\frac{m_n^2\\log p}{n^{1 - 2/q}},\\end{aligned}\\ ] ] since @xmath268 and @xmath269 by assumption [ as : dataset partition ] and @xmath270\\leq \\max_{1\\leq j\\leq p}{{\\mathrm{e}}}[x_{i j}^4]\\leq \\gamma_n^4\\ ] ] by hlder s inequality and assumption [ as : covariates ] .",
    "noting that @xmath271 which hold by assumption [ as : growth condition ] , and combining presented inequalities implies that @xmath272 with probability @xmath31 .",
    "in addition , by lemma [ lem : 1 ] , @xmath273 with probability @xmath274 .",
    "therefore , it follows that @xmath275 with probability @xmath31 .",
    "this completes the proof .",
    "[ lem : 3 ] suppose that assumptions [ as : covariates ]  [ as : dataset partition ] hold .",
    "in addition , suppose that for all @xmath169 , we have @xmath170 \\leq c_1 t^2 $ ] .",
    "then we have for all @xmath70 that @xmath276 with probability @xmath31 .",
    "we have @xmath277 for @xmath129 defined in lemma [ lem : 1 ] .",
    "therefore , @xmath278 further , by assumptions of the lemma , for @xmath279 , @xmath70 , and @xmath280 , we have for all @xmath169 that @xmath281\\lesssim t^2 n_k\\|\\widehat\\beta_{-k}(\\lambda ) - \\widehat \\beta_{-k}(\\bar \\lambda_0)\\|_{2,n , k}^2.\\ ] ] therefore , since @xmath131 by assumption [ as : candidate set ] , we have with probability @xmath31 that for all @xmath70 and @xmath71 , @xmath282 by the union bound and markov s inequality ; in particular , since @xmath283 , we have with probability @xmath31 that for all @xmath284 , @xmath285 hence , since @xmath286 by assumption [ as : dataset partition ] , we have with probability @xmath31 that @xmath287 let @xmath288 be a @xmath70 that maximizes @xmath289",
    ". then with probability @xmath31 , @xmath290 and so , by lemma [ lem : 2 ] and the triangle inequality , with probability @xmath31 , @xmath291 conclude that for all @xmath70 , with probability @xmath31 , @xmath292 this completes the proof .",
    "[ lem : 4 ] suppose that assumptions [ as : covariates ]  [ as : dataset partition ] hold .",
    "in addition , suppose that for all @xmath169 , we have @xmath170 \\leq c_1 t^2 $ ] .",
    "then we have for all @xmath70 that @xmath293 with probability @xmath31 .",
    "fix @xmath70 . for @xmath71 ,",
    "let @xmath294 .",
    "observe that conditional on @xmath295 , @xmath296 is non - stochastic .",
    "therefore , @xmath297\\lesssim",
    "\\gamma_n^4 n$ ] by assumption [ as : covariates ] since @xmath298 for all @xmath71 .",
    "in addition , @xmath299\\big)^{1/2 } & \\leq \\gamma_n^2\\cdot ( n|\\lambda_n|)^{1/2}.\\end{aligned}\\ ] ] so , by lemma [ lem : maximal inequality ] , @xmath300\\big)\\right|\\end{aligned}\\ ] ] satisfies @xmath301 \\lesssim \\sqrt{\\frac{\\gamma_n^4\\log |\\lambda_n|}{n } } + \\frac{\\gamma_n^2\\cdot ( n|\\lambda_n|)^{1/2}\\log|\\lambda_n|}{n } = o(1)\\ ] ] by assumption [ as : growth condition ] since @xmath131 by assumption [ as : candidate set ] .",
    "moreover , by assumption [ as : covariates ] , for any @xmath71 , @xmath302\\\\ & \\leq \\frac{1}{n_k}\\sum_{i\\in i_k}(x_i'(\\widehat\\beta_{-k}(\\lambda ) - \\beta))^2 + r\\|\\widehat\\beta_{-k}(\\lambda ) - \\beta\\|^2\\\\ & = \\|\\widehat\\beta_{-k}(\\lambda ) - \\beta\\|_{2,n , k}^2 + r\\|\\widehat\\beta_{-k}(\\lambda ) - \\beta\\|^2.\\end{aligned}\\ ] ] therefore , with probability @xmath31 , @xmath303 where the second inequality follows from lemma [ lem : 3 ] .",
    "the asserted claim follows .",
    "[ lem : 5 ] suppose that assumptions [ as : covariates ]  [ as : dataset partition ] hold .",
    "in addition , suppose that for all @xmath169 , we have @xmath170 \\leq c_1 t^2 $ ] .",
    "finally , suppose that @xmath136 .",
    "then we have for all @xmath70 that @xmath304 with probability @xmath138 .    since @xmath305 and for all @xmath88 , we have @xmath306)^{1/2}\\leq c_1 $ ] and @xmath87)^{1/4}\\leq \\gamma_n$ ] by assumption [ as : covariates ] , applying lemma [ lem : tropp ] shows that with probability @xmath31 , @xmath307 in addition , @xmath308 by assumptions [ as : covariates ] and [ as : growth condition ] . also , @xmath309 since we have @xmath162 and it follows from lemma [ lem : xi bound ] that @xmath310 .",
    "thus , the asserted claim follows from lemma [ lem : xi bound ] .",
    "[ lem : 0.5 ] for all @xmath311 , the lasso estimator @xmath74 given in based on the data @xmath312 has the following properties : ( i ) the function @xmath313 mapping @xmath314 to @xmath314 for a fixed value of @xmath212 is lipschitz - continuous with lipschitz constant one whenever the matrix @xmath315 has full column rank ; ( ii ) if for all @xmath316 , the conditional distribution of @xmath317 given @xmath56 is @xmath318 and the pairs @xmath319 are independent across @xmath57 , then @xmath320 = \\sum_{i=1}^n \\sigma_i^{-2 } { { \\mathrm{e}}}[{\\varepsilon}_i x_i'({\\widehat\\beta}(\\lambda ) - \\beta)\\mid x_1^n]\\ ] ] on the event that the matrix @xmath315 has full column rank .",
    "this lemma is an extension of the main result in @xcite to the heteroscedastic case ( we allow @xmath321 s to vary over @xmath57 ) .",
    "fix @xmath71 and @xmath212 such that the matrix @xmath315 has full column rank .",
    "denote @xmath322 and @xmath323 .",
    "note that @xmath324 is well - defined because the solution of the optimization problem is unique since the optimized function is strictly convex under the condition that the matrix @xmath315 has full column rank .",
    "also , let @xmath325 denote the set of all vectors in @xmath326 whose elements are either @xmath327 or @xmath328 .",
    "moreover , let @xmath329 be a subset of @xmath330 consisting of all values of @xmath331 such that for some @xmath214 with @xmath332 , @xmath333 , and @xmath334 , we have @xmath335 where @xmath336 is a vector in @xmath49 such that @xmath337 and @xmath338 for all @xmath339 . note that @xmath340 is well - defined because the matrix @xmath341 has full column rank under the condition that the matrix @xmath315 has full column rank .",
    "it follows that @xmath329 is contained in a finite set of hyperplanes in @xmath330 .",
    "next , by the kuhn - tucker conditions , for all @xmath342 , we have @xmath343 and for all @xmath344 , we have @xmath345 thus , since the matrix @xmath346 has full column rank under the condition that the matrix @xmath315 has full column rank , we have for all @xmath347 that @xmath348 moreover , since @xmath324 is the unique solution of the optimization problem , it follows that the functions @xmath349 and @xmath350 are well - defined and are locally constant for all values of @xmath351 satisfying @xmath352 .",
    "now we are ready to show that the function @xmath353 is lipschitz - continuous with lipschitz constant one , which is the first asserted claim . to this end , consider @xmath324 as a function of @xmath351 .",
    "let @xmath354 and @xmath355 be two values of @xmath351 , and let @xmath356 and @xmath357 be corresponding values of @xmath324 .",
    "suppose first that the line segment @xmath358\\}$ ] does not intersect @xmath329 .",
    "then @xmath359 and @xmath360 are constant on @xmath361 , and so implies that @xmath362 second , suppose that @xmath361 has a non - empty intersection with @xmath329 .",
    "recall that the set @xmath329 is contained in a finite collection of hyperplanes , and so we can find @xmath363 such that @xmath359 remains constant on each line segment @xmath364 , @xmath365 , of @xmath361 .",
    "in addition , note that the function @xmath353 is continuous ( otherwise we could use , for example , the fact that the optimized function in is strictly convex to arrive at a contradiction ) .",
    "hence , holds in this case as well by the triangle inequality .",
    "this gives the first asserted claim .",
    "next , we prove , which is the second asserted claim .",
    "note that since for all values of @xmath351 satisfying @xmath352 , the functions @xmath349 and @xmath350 are locally constant , it follows from that for the same values of @xmath351 , the functions @xmath366 are differentiable .",
    "moreover , @xmath367 and so @xmath368 whenever @xmath369 . since @xmath370",
    ", it follows that @xmath371 = { { \\mathrm{e}}}[|{\\widehat t}|\\mid x_1^n].\\ ] ] in addition , the first asserted claim implies that the functions @xmath366 are absolutely continuous , and so applying stein s lemma ( see , for example , lemma 2.1 in @xcite ) conditional on @xmath372 and using the fact that pairs @xmath319 are independent across @xmath57 shows that @xmath373   & = \\sum_{l=1}^n { { \\mathrm{e}}}\\left[\\frac{\\partial ( x_l'{\\widehat\\beta})}{\\partial { \\varepsilon}_l}\\mid x_1^n\\right ] = \\sum_{l=1}^n \\sigma_l^{-2 } { { \\mathrm{e}}}[{\\varepsilon}_l x_l'{\\widehat\\beta}\\mid x_1^n ] = \\sum_{l=1}^n \\sigma_l^{-2}{{\\mathrm{e}}}[{\\varepsilon}_l x'_l({\\widehat\\beta}- \\beta)\\mid x_1^n],\\end{aligned}\\ ] ] which gives , the second asserted claim , since @xmath374 .",
    "this completes the proof of the lemma .",
    "[ lem : 0.7 ] suppose that assumptions [ as : heterogeneity ] and [ as : dataset partition ] hold .",
    "in addition , suppose that the conditional distribution of @xmath13 given @xmath18 is gaussian .",
    "then for all @xmath71 and @xmath97 , we have @xmath375\\big|>t\\mid x_1^n\\big)\\leq ce^{-c n t^2},\\ ] ] and for all @xmath70 , @xmath311 , and @xmath97 , we have @xmath376\\big|>t\\mid x_1^n\\big)\\leq ce^{-c n t^2}\\ ] ] on the event that the matrix @xmath315 has the full column rank , where @xmath229 and @xmath377 are some constants that depend only on @xmath77 and @xmath78 .    fix @xmath311 and @xmath212 such that the matrix @xmath315 has full column rank . by lemma [ lem : 0.5 ] ,",
    "the function @xmath378 is lipschitz - continuous with lipschitz constant one , and so is @xmath379 . in turn ,",
    "thus , by the gaussian concentration inequality ( see , for example , theorem 2.1.12 in @xcite ) , @xmath381\\big|>t\\mid x_1^n\\big)\\leq ce^{-c t^2},\\ ] ] for some constants @xmath229 and @xmath377 that depend only on @xmath77 and @xmath78 . replacing @xmath382 by @xmath383 in this inequality gives the first asserted claim .",
    "the second asserted claim follows similarly .",
    "this completes the proof of the theorem .",
    "[ lem : 6 ] for some sufficiently large constant @xmath32 , let @xmath384 and for @xmath70 , let @xmath385\\leq t_n\\right\\}.\\ ] ] suppose that assumptions [ as : covariates ]  [ as : dataset partition ] hold .",
    "in addition , suppose that the conditional distribution of @xmath13 given @xmath18 is gaussian .",
    "finally , suppose that @xmath162 .",
    "then @xmath386 for all @xmath70 with probability @xmath274 .",
    "fix @xmath70 .",
    "we have @xmath387\\big|^2>t_n^2/4\\big).\\end{aligned}\\ ] ] the first term on the right - hand side of this inequality is @xmath149 by lemma [ lem : 5 ] ( recall that the fact that the conditional distribution of @xmath28 given @xmath18 is gaussian combined with assumption [ as : heterogeneity ] implies that @xmath170\\leq c_1 t^2 $ ] for all @xmath97 if @xmath78 in this inequality is large enough ) .",
    "further , since @xmath90 and for all @xmath88 , we have @xmath388)^{1/2 } \\leq c_1 $ ] and @xmath87)^{1/4 } \\leq \\gamma_n$ ] by assumption [ as : covariates ] , lemma [ lem : tropp ] implies that all eigenvalues of the matrix @xmath389 are bounded below from zero with probability @xmath31 , like in lemma [ lem : 5 ] .",
    "in addition , on the event that the matrix @xmath389 is non - singular , we have by lemma [ lem : 0.7 ] and the union bound that the expression @xmath390\\big|^2>t_n^2/4\\mid x_1^n\\right)\\ ] ] is bounded from above by @xmath391 for arbitrarily large constant @xmath32 as long as the constant @xmath32 in the statement of the lemma is large enough . since @xmath392 by assumption",
    "[ as : candidate set ] , it follows that @xmath390\\big|^2>t_n^2/4\\right ) = o(1).\\ ] ] hence , the asserted claim follows .",
    "[ lem : sparsity bound low dimensional ] suppose that assumptions [ as : covariates ]  [ as : dataset partition ] hold .",
    "in addition , suppose that the conditional distribution of @xmath13 given @xmath18 is gaussian .",
    "finally , suppose that @xmath136 .",
    "then for all @xmath70 , @xmath393 with probability @xmath31 .",
    "fix @xmath70 .",
    "similarly to the proof of lemma [ lem : 5 ] , we have by lemma [ lem : tropp ] that the smallest eigenvalue of the matrix @xmath394 is bounded from below by @xmath395 with probability @xmath31 since the smallest eigenvalue of the matrix @xmath91 $ ] is bounded from below by @xmath396 by assumption [ as : covariates ] .",
    "fix @xmath212 such that the smallest eigenvalue of the matrix @xmath397 is bounded from below by @xmath395 . also , fix @xmath398 for @xmath399 defined in the statement of lemma [ lem : 6 ]",
    ". then @xmath400 \\leq t_n$ ] for @xmath401 defined in the statement of lemma [ lem : 6 ] .",
    "hence , by fubini s theorem and lemma [ lem : 0.7 ] , we have @xmath402 & = \\int_0^\\infty { { \\mathrm{p}}}\\big(\\|{\\widehat\\beta}_{-k}(\\lambda ) - \\beta\\|_{2,n ,- k}^4>t\\mid x_1^n\\big)dt\\\\ & \\leq t_n^4 + \\int_{t_n^4}^\\infty { { \\mathrm{p}}}\\big(\\|{\\widehat\\beta}_{-k}(\\lambda ) - \\beta\\|_{2,n ,- k}>t^{1/4}\\mid x_1^n\\big)dt\\\\ & \\lesssim t_n^4 + \\int_{t_n^4}^\\infty \\exp\\big(-cn(t^{1/4 } - t_n)^2\\big)d t\\\\ & \\lesssim t_n^4 + \\frac{1}{\\sqrt n}\\int_0^\\infty \\big(t/\\sqrt n + t_n\\big)^3\\exp(-c t^2)d t \\lesssim t_n^4.\\end{aligned}\\ ] ] thus , @xmath403\\big)^{1/4 } \\lesssim t_n.\\ ] ] then by assumption [ as : heterogeneity ] and lemma [ lem : 0.5 ] applied to the data @xmath404 and the lasso estimator @xmath40 , @xmath405 & \\lesssim \\textstyle{\\sum_{i\\notin i_k}}{{\\mathrm{e}}}[{\\varepsilon}_i x_i'({\\widehat\\beta}_{-k}(\\lambda ) -",
    "\\beta)\\mid x_1^n]\\\\ & \\lesssim { { \\mathrm{e}}}\\big [ \\big\\| \\textstyle{\\sum_{i\\notin i_k}}{\\varepsilon}_i x_i \\big\\|_{\\infty } \\cdot \\|{\\widehat\\beta}_{-k}(\\lambda ) - \\beta\\|_1 \\mid x_1^n\\big]\\\\ & \\leq { { \\mathrm{e}}}\\big [ \\big\\| \\textstyle{\\sum_{i\\notin i_k } } { \\varepsilon}_i x_i \\big\\|_{\\infty } \\cdot \\|{\\widehat\\beta}_{-k}(\\lambda ) - \\beta\\| \\cdot \\sqrt{\\|{\\widehat\\beta}_{-k}(\\lambda)\\|_0 + s } \\mid x_1^n\\big]\\\\ & \\leq \\big({{\\mathrm{e}}}\\big [ \\big\\| \\textstyle { \\sum_{i\\notin i_k}}{\\varepsilon}_i x_i \\big\\|_{\\infty}^2 \\cdot \\|{\\widehat\\beta}_{-k}(\\lambda ) -",
    "\\beta\\|^2 \\mid x_1^n\\big]\\cdot { { \\mathrm{e}}}[\\|{\\widehat\\beta}_{-k}(\\lambda)\\|_0 + s\\mid x_1^n]\\big)^{1/2},\\end{aligned}\\ ] ] where the last line follows from hlder s inequality . in turn , with probability @xmath31 , @xmath406\\big)^{1/2}\\\\ & \\qquad \\leq \\big({{\\mathrm{e}}}\\big [ \\big\\|   \\textstyle{\\sum_{i\\notin i_k}}{\\varepsilon}_i x_i \\big\\|_{\\infty}^4\\mid x_1^n\\big]\\cdot { { \\mathrm{e}}}\\big [ \\|{\\widehat\\beta}_{-k}(\\lambda ) - \\beta\\|^4 \\mid x_1^n\\big]\\big)^{1/4}\\\\ & \\qquad \\lesssim \\sqrt{n \\log p}\\big({{\\mathrm{e}}}\\big [ \\|{\\widehat\\beta}_{-k}(\\lambda ) -",
    "\\beta\\|^4 \\mid x_1^n\\big]\\big)^{1/4 } \\lesssim \\sqrt{n \\log p}\\big({{\\mathrm{e}}}\\big [ \\|{\\widehat\\beta}_{-k}(\\lambda ) - \\beta\\|_{2,n ,-",
    "k}^4 \\mid x_1^n\\big]\\big)^{1/4}\\end{aligned}\\ ] ] and the last expression is bounded from above up - to a constant @xmath32 by @xmath407 by .",
    "hence , with probability @xmath31 , @xmath408\\lesssim s\\cdot ( \\log p)\\cdot(\\log p + \\log\\log n ) + ( \\log p)\\cdot(\\log\\log n)^2.\\ ] ] so , by markov s inequality and the union bound , @xmath409 for all @xmath410 with probability @xmath31 since @xmath411 by assumption [ as : candidate set ] and since @xmath412 by the assumption that @xmath136 ( recall that by lemma [ lem : xi bound ] , @xmath310 ) .",
    "the asserted claim follows since by lemma [ lem : 6 ] , @xmath413 for all @xmath70 with probability @xmath31 .",
    "[ lem : chatterjee ] for all @xmath71 and @xmath414 , we have @xmath415    the result in this lemma is sometimes referred to as the two point inequality ; see @xcite . here",
    "we give a short proof of this inequality using an argument similar to that of lemma 5.1 in @xcite .",
    "fix @xmath71 and denote @xmath322 .",
    "take any @xmath416 .",
    "we have @xmath417 hence , @xmath418 and so @xmath419 since @xmath416 is arbitrary , we obtain @xmath420 thus , @xmath421 the asserted claim follows .",
    "let @xmath422 so that @xmath24 is a convex combination of @xmath423 .",
    "we have @xmath424 where the second line follows from the definition of @xmath154 s and the third from the triangle inequality .",
    "also , @xmath425 thus , by lemma [ lem : chatterjee ] , @xmath426 where @xmath427 next , for all @xmath70 , we have @xmath428 now , @xmath429 with probability @xmath31 .",
    "in addition , with probability @xmath31 , for all @xmath70 , @xmath430 where the first line follows from the triangle inequality and , the second from lemma [ lem : sparsity bound low dimensional ] , and the third from lemma [ lem : 4 ] ( again , recall that the fact that the conditional distribution of @xmath28 given @xmath18 is gaussian combined with assumption [ as : heterogeneity ] implies that @xmath170\\leq c_1 t^2 $ ] for all @xmath97 if @xmath78 in this inequality is large enough ) and the observation that @xmath412 , which follows from @xmath162 and lemma [ lem : xi bound ] .",
    "thus , with probability @xmath31 , for all @xmath70 , @xmath431 also , with probability @xmath138 , for all @xmath70 , @xmath432 where the first line follows from hlder s inequality and the second from and lemmas [ lem : 3 ] and [ lem : 5 ] since @xmath412 .",
    "combining presented inequalities shows that with probability @xmath31 , @xmath433 finally , with probability @xmath31 , @xmath434 by lemmas [ lem : 3 ] and [ lem : 5 ] .",
    "thus , by the triangle inequality , @xmath435 with probability @xmath31 .",
    "this completes the proof of the theorem .",
    "let @xmath436\\leq c\\cdot\\big(\\frac{s\\log p}{n}\\big)^{1/2}\\cdot(\\log^{7/8 } n)\\right\\}\\ ] ] for some sufficiently large constant @xmath32 .",
    "since by theorem [ thm : cross validation ] , @xmath437 with probability @xmath31 if @xmath32 is large enough , it follows by the same argument as that used in the proof of lemma [ lem : 6 ] that @xmath438 with probability @xmath31 .",
    "further , as in the proof of lemma [ lem : sparsity bound low dimensional ] , fix @xmath212 such that the smallest eigenvalue of the matrix @xmath439 is bounded from below by @xmath395 , which happens with probability @xmath31 , and fix @xmath440 . then @xmath441\\big)^{1/4}\\lesssim \\left(\\frac{s\\log p}{n}\\right)^{1/2}\\cdot(\\log^{7/8}n),\\ ] ] and so @xmath442 \\lesssim \\sum_{i=1}^n { { \\mathrm{e}}}[{\\varepsilon}_i x_i'({\\widehat\\beta}(\\lambda ) - \\beta)\\mid x_1^n]\\\\ & \\qquad\\lesssim\\big({{\\mathrm{e}}}\\big[\\big\\|\\textstyle{\\sum_{i=1}^n } { \\varepsilon}_i x_i\\big\\|_{\\infty}^4\\mid x_1^n\\big]\\big)^{1/4}\\cdot\\big({{\\mathrm{e}}}[\\|{\\widehat\\beta}(\\lambda ) - \\beta\\|_{2,n}^4\\mid x_1^n]\\big)^{1/4}\\cdot\\big({{\\mathrm{e}}}[\\|{\\widehat\\beta}(\\lambda)\\|_0 + s\\mid x_1^n]\\big)^{1/2}\\\\ & \\qquad \\lesssim s^{1/2}\\cdot(\\log p)\\cdot(\\log^{7/8}n)\\cdot\\big({{\\mathrm{e}}}[\\|{\\widehat\\beta}(\\lambda)\\|_0 + s\\mid x_1^n]\\big)^{1/2}\\\\ & \\qquad \\lesssim s^{1/2}\\cdot(\\log^{15/8}n)\\cdot \\big({{\\mathrm{e}}}[\\|{\\widehat\\beta}(\\lambda)\\|_0 + s\\mid x_1^n]\\big)^{1/2}\\end{aligned}\\ ] ] by the same argument as that used in the proof of lemma [ lem : sparsity bound low dimensional ] .",
    "therefore , @xmath443 \\lesssim s\\log^{15/4}n.\\ ] ] hence , given that @xmath444 by assumption [ as : candidate set ] , it follows from markov s inequality and the union bound that @xmath445 with probability @xmath31 .",
    "this completes the proof of the theorem .      like in the proof of lemma [ lem : 5 ] , it follows from lemma [ lem : tropp ] that @xmath446 with probability @xmath31 , and so the first asserted claim follows from theorem [ thm : cross validation ] and the assumption that @xmath447 .",
    "the second asserted claim follows from the first claim combined with the observation that @xmath448 and theorem [ thm : sparsity ] .",
    "this completes the proof of the corollary .",
    "[ lem : 0 ] for all @xmath71 and @xmath70 , we have @xmath449 .    fix @xmath71 and @xmath70 .",
    "denote @xmath450 and @xmath323 .",
    "suppose to the contrary that @xmath451 .",
    "then the matrix @xmath346 does not have full column rank , and there exists @xmath452 with @xmath453 and @xmath454 such that @xmath455 for all @xmath456 .",
    "also , the function @xmath457 mapping @xmath458 into @xmath458 is constant in some neighborhood around zero , and so as @xmath459 increases in this neighborhood , some of the components of the vector @xmath460 increase and others decrease .",
    "thus , as we move @xmath459 , we can always find a vector @xmath460 that is a solution of the optimization problem but is also such that @xmath461 , which contradicts to our assumption that whenever the lasso optimization problem in has multiple solutions , we choose one with the smallest number of non - zero components .",
    "this completes the proof of the lemma .",
    "[ lem : 13 ] suppose that assumptions [ as : covariates ]  [ as : dataset partition ] hold .",
    "then we have for all @xmath70 that @xmath462 with probability @xmath138 .",
    "note that @xmath463 by lemma [ lem : 0 ] . also , recall that @xmath464 .",
    "in addition , @xmath465\\right)^{1/2 } \\leq n^{1/q } m_n\\ ] ] for @xmath251 defined in assumption [ as : growth condition ] .",
    "therefore , since all eigenvalues of the matrix @xmath237 $ ] are bounded from above by assumption [ as : covariates ] , applying lemma [ lem : rv ] with @xmath466 shows that with probability @xmath31 , @xmath467 combining this inequality with lemma [ lem : 4 ] and noting that @xmath468 gives the asserted claim .      define @xmath24 as in .",
    "also , for @xmath70 , define @xmath469 and @xmath470 as in .",
    "then it follows as in the proof of theorem [ thm : cross validation ] that @xmath471 fix @xmath70 . to bound @xmath469 , we have by the triangle inequality and lemmas [ lem : 4 ] and [ lem : 0 ] that @xmath472 with probability @xmath31",
    ". thus , @xmath473 with probability @xmath31 .",
    "further , to bound @xmath470 , we have by hlder s inequality and lemmas [ lem : 3 ] and [ lem : 13 ] that @xmath474 with probability @xmath31 .",
    "hence , @xmath475 with probability @xmath31 where the second inequality holds by the assumption that @xmath476 finally , with probability @xmath31 , @xmath477 by lemmas [ lem : 3 ] and [ lem : 13 ] . combining these inequalities and using",
    "the triangle inequality shows that @xmath478 with probability @xmath31 .",
    "this completes the proof of the theorem .",
    "[ lem : maximal inequality ] let @xmath479 be independent centered random vectors in @xmath49 with @xmath480 .",
    "define @xmath481 , @xmath482 , and @xmath483 $ ] .",
    "then @xmath484 \\leq k\\big(\\sigma\\sqrt{\\log p } + \\sqrt{{{\\mathrm{e}}}[m^2]}\\log p\\big)\\ ] ] where @xmath1 is a universal constant .",
    "[ lem : maximal inequality 2 ] consider the setting of lemma [ lem : maximal inequality ] .",
    "for every @xmath485 , @xmath97 , and @xmath486 , we have @xmath487 + t\\big)\\leq \\exp(-t^2/(3\\sigma^2 ) ) + k{{\\mathrm{e}}}[m^q]/t^q\\ ] ] where the constant @xmath1 depends only on @xmath488 and @xmath80 .",
    "[ lem : rv ] let @xmath479 be i.i.d .",
    "random vectors in @xmath49 with @xmath480 .",
    "also , let @xmath489)^{1/2}$ ] and for @xmath490 , let @xmath491 moreover , let @xmath492 .",
    "then @xmath493\\big|\\right]\\lesssim \\delta_n^2 + \\delta_n\\sup_{\\theta\\in\\mathcal s^p\\colon \\|\\theta\\|_0\\leq k}\\big({{\\mathrm{e}}}[(x_1'\\theta)^2]\\big)^{1/2}\\ ] ] up - to an absolute constant .",
    "[ lem : tropp ] let @xmath479 be a random sample from the distribution of a @xmath5-dimensional random vector @xmath18 such that for all @xmath88 , we have @xmath494 and @xmath495 for some constants @xmath32 and @xmath496 . then for any constants @xmath497 and @xmath97 such that @xmath97 , we have @xmath498\\big\\|>t + \\gamma^2\\cdot(p(\\|x\\|>\\xi))^{1/2}\\big)\\\\ & \\qquad \\leq \\exp\\big(\\log(2p ) - \\frac{a",
    "n t^2}{\\xi^2(1 + t)}\\big ) + n p(\\|x\\|>\\xi)\\end{aligned}\\ ] ] where @xmath499 is a constant depending only on @xmath32 .",
    "an application of corollary 6.2.1 in @xcite shows that @xmath500\\right\\|>t\\right)\\leq \\exp\\left(\\log(2p )",
    "- \\frac{a n t^2}{\\xi^2(1 + t)}\\right)\\ ] ] where @xmath499 depends only on @xmath32 ; see , for example , lemma 10 in @xcite .",
    "now , let @xmath325 be the event that @xmath501 for all @xmath456 and let @xmath502 be its complement",
    ". then @xmath503 by the union bound . also , @xmath504",
    "\\big\\|&= \\sup_{\\delta\\in\\mathbb r^p\\colon \\|\\delta\\|=1}{{\\mathrm{e}}}\\big[|x'\\delta|^21\\{\\|x\\|>\\xi\\}\\big]\\\\ & \\leq \\sup_{\\delta\\in\\mathbb r^p\\colon \\|\\delta\\|=1}\\big({{\\mathrm{e}}}[\\|x'\\delta|^4]\\big)^{1/2}\\big(p(\\|x\\|>\\xi)\\big)^{1/2 } \\leq \\gamma^2\\cdot\\big(p(\\|x\\|>\\xi)\\big)^{1/2}\\end{aligned}\\ ] ] by hlder s inequality . hence , @xmath505\\big\\|>t + \\gamma^2\\cdot(p(\\|x\\|>\\xi))^{1/2}\\big)\\\\ & \\qquad \\leq \\displaystyle{p}\\big(\\big\\{\\big\\|\\frac{1}{n}\\textstyle{\\sum_{i=1}^n } x_i x_i ' - { { \\mathrm{e}}}[x x']\\big\\|>t + \\gamma^2\\cdot(p(\\|x\\|>\\xi))^{1/2}\\big\\}\\cap \\mathcal d\\big ) + p(\\mathcal d^c)\\\\ & \\qquad \\leq p\\big(\\big\\|\\frac{1}{n}\\textstyle{\\sum_{i=1}^n } x_i x_i'1\\{\\|x_i\\|\\leq \\xi\\ } - { { \\mathrm{e}}}[x x ' 1\\{\\|x\\|\\leq \\xi\\}]\\big\\|>t\\big ) + n p(\\|x\\|>\\xi)\\end{aligned}\\ ] ] by the triangle inequality .",
    "the asserted claim follows by combining this inequality with .",
    "[ lem : xi bound ] let @xmath18 be a random vector in @xmath49 implicitly indexed by @xmath4 , where the dimension @xmath506 is also allowed to depend on @xmath4 .",
    "suppose that @xmath306)^{1/2 } \\geq c_1 $ ] for all @xmath88 , @xmath507 , and some constant @xmath508 .",
    "also , suppose that @xmath509 as @xmath510 for some sequence of constants @xmath511 .",
    "in addition , suppose that @xmath512)^{1/4}$ ] satisfies @xmath513 as @xmath514 .",
    "then @xmath515 for all sufficiently large @xmath4 .",
    "note that if @xmath516 , the assumption that @xmath306)^{1/2}\\geq c_1 $ ] for all @xmath517 implies that @xmath306)^{1/2}\\geq \\sqrt 2 $ ] for all @xmath517 .",
    "hence , it suffices to consider the case where @xmath518 and to show that @xmath519 for all sufficiently large @xmath4 in this case .    to this end , suppose to the contrary that @xmath520 for all @xmath521 , @xmath490 , where @xmath522 is some increasing sequence of integers .",
    "then @xmath523 \\leq \\xi_n^2 \\leq c_1 ^ 2 p / 2\\ ] ] for all @xmath521 . on the other hand , @xmath524 = { { \\mathrm{e}}}[x'x ] = \\text{tr}({{\\mathrm{e}}}[x'x ] )",
    "= { { \\mathrm{e}}}[\\text{tr}(x'x ) ] = { { \\mathrm{e}}}[\\text{tr}(x x ' ) ] = \\text{tr}({{\\mathrm{e}}}[x x'])\\geq c_1 ^ 2 p\\ ] ] for all @xmath507 by the assumption that @xmath306)^{1/2}\\geq c_1 $ ] for all @xmath88 , where for any @xmath525 matrix @xmath499 , we use @xmath526 to denote the sum of its diagonal terms , that is , @xmath527 .",
    "hence , @xmath528\\geq",
    "c_1 ^ 2 p / 2\\ ] ] for all @xmath521 .",
    "in addition , the assumption that @xmath509 implies that @xmath529 = o(p / n)\\ ] ] as @xmath510 , and so it follows from that @xmath530 \\geq c_1 ^ 2 p /4\\ ] ] for all @xmath521 with sufficiently large @xmath239 .",
    "let us now use to bound @xmath512)^{1/4}$ ] from below to obtain a contradiction with the assumption that @xmath531 .",
    "let @xmath532 be a standard gaussian random vector in @xmath49 that is independent of @xmath18",
    ". then @xmath533 is distributed uniformly on @xmath534 and is independent of @xmath535 .",
    "further , let @xmath536 .",
    "then for all @xmath537 , @xmath538 \\geq { { \\mathrm{e}}}\\big[|x'\\delta|^41\\{\\|x\\|>\\sqrt p\\}\\big ] = { { \\mathrm{e}}}\\big[\\|x\\|^4|z'\\delta|^41\\{\\|x\\|>\\sqrt p\\}\\big],\\ ] ] and so @xmath539 \\geq { { \\mathrm{e}}}\\big[\\|x\\|^4 |z ' u|^4 1\\{\\|x\\|>\\sqrt p\\}\\big ]   = { { \\mathrm{e}}}\\big[\\|x\\|^41\\{\\|x\\|>\\sqrt p\\}{{\\mathrm{e}}}[|z'u|^4\\mid x]\\big].\\ ] ] on the other hand , for any non - stochastic @xmath540 , @xmath541 = { { \\mathrm{e}}}[\\|n\\|^4 \\cdot |z'u|^4 ] = { { \\mathrm{e}}}[\\|n\\|^4]\\cdot { { \\mathrm{e}}}[|z'u|^4 ] \\leq 3 p^2 { { \\mathrm{e}}}[|z'u|^4],\\ ] ] so that @xmath542 \\geq p^{-2}. $ ] hence , it follows from that @xmath543 \\geq p^{-2}{{\\mathrm{e}}}\\big[\\|x\\|^41\\{\\|x\\|>\\sqrt p\\}\\big].\\ ] ] however , implies by hlder s inequality that @xmath544 = { { \\mathrm{e}}}\\big[\\|x\\|^21\\{\\|x\\|>\\sqrt p\\}\\cdot 1\\{\\|x\\|>\\sqrt p\\}\\big ] \\\\ & \\leq \\big({{\\mathrm{e}}}\\big[\\|x\\|^41\\{\\|x\\|>\\sqrt p\\}\\big]\\cdot p(\\|x\\|>\\sqrt p)\\big)^{1/2},\\end{aligned}\\ ] ] so that @xmath545 \\geq",
    "\\frac{c_1 ^ 4 p^2}{16 p(\\|x\\|>\\sqrt p)}\\ ] ] for all @xmath521 with sufficiently large @xmath239 .",
    "therefore , it follows from that @xmath546 \\geq \\frac{c_1 ^ 4}{16p(\\|x\\|>\\sqrt p)}\\ ] ] for all @xmath521 with sufficiently large @xmath239 .",
    "this contradicts to the assumptions that @xmath509 and @xmath547 since under our condition that @xmath548 , we have @xmath549 and @xmath550 for all @xmath521 .",
    "this completes the proof of the lemma .",
    "belloni , a. , chernozhukov , v. , chetverikov , d. , and wei , y. ( 2015b ) .",
    "uniformly valid post - regularization confidence regions for many functional parameters in z - estimation framework .",
    "_ arxiv:1512.07619_.                  chernozhukov , v. , chetverikov , d. , and kato , k. ( 2013 ) .",
    "gaussian approximations and multiplier bootstrap for maxima of sums of high - dimensional random vectors . _ the annals of statistics _ , * 41 * , 2786 - 2819 ."
  ],
  "abstract_text": [
    "<S> in this paper , we derive a rate of convergence of the lasso estimator when the penalty parameter @xmath0 for the estimator is chosen using @xmath1-fold cross - validation ; in particular , we show that in the model with gaussian noise and under fairly general assumptions on the candidate set of values of @xmath0 , the prediction norm of the estimation error of the cross - validated lasso estimator is with high probability bounded from above up - to a constant by @xmath2 as long as @xmath3 and some other mild regularity conditions are satisfied , where @xmath4 is the sample size of available data , @xmath5 is the number of covariates , and @xmath6 is the number of non - zero coefficients in the model . </S>",
    "<S> thus , the cross - validated lasso estimator achieves the fastest possible rate of convergence up - to the logarithmic factor @xmath7 . </S>",
    "<S> in addition , we derive a sparsity bound for the cross - validated lasso estimator ; in particular , we show that under the same conditions as above , the number of non - zero coefficients of the estimator is with high probability bounded from above up - to a constant by @xmath8 . </S>",
    "<S> finally , we show that our proof technique generates non - trivial bounds on the prediction norm of the estimation error of the cross - validated lasso estimator even if @xmath5 is much larger than @xmath4 and the assumption of gaussian noise fails ; in particular , the prediction norm of the estimation error is with high - probability bounded from above up - to a constant by @xmath9 under mild regularity conditions . </S>"
  ]
}