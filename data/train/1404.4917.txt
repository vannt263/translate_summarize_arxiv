{
  "article_text": [
    "principal components analysis ( pca ) has been popular since its inception due to its two main features : 1 .",
    "explaining well the variation of the information and 2 . because of the first one , being able to reduce the space to those variables that explain most of the variation . in the context of regression",
    ", pca has been used to explain the response in terms of the principal components ( pc ) of the original input variables ( see for instance , @xcite ,  p. 75 ; @xcite ; @xcite ,  p. 307 ; @xcite ) . however , because one of the main features of pca is dimension reduction ( feature 2 mentioned above ) it is not always clear that pca can be used and to still get that the response is well explained by the reduced space of the main pc ( see for instance @xcite ,  p. 72 ; @xcite ; @xcite ) .",
    "in fact , as early as 1957 , in one of the first references to pca in regression , hotelling was doubtful of the method because of this same reason ( @xcite ) . for a historical review of pca in regression settings",
    "see @xcite .",
    "note the interesting fact that the earliest of our references advocating the use of pca in regression ( @xcite ) was proposed in the same year that its first criticism was known ( @xcite ) .",
    "the questionings of pca in regression seem to be as old as the method itself .",
    "now , the method and its questionings transcend regression going further to more general learning settings . but whether regression or any other learning setting , the two interesting questions to ask in order to determine the usefulness of the method are the following : 1 . is it the case that ( most of the times ) the response variable is better explained by the pc than by the original predictors ? and , if the answer to this question is yes , then 2 .",
    "are the leading pc more able to explain the response than the rest of the pc ?",
    "question 2 , as mentioned above , was posed from the beginning in order to determine the usefulness of pca in learning settings . and",
    "this question has been answered recently by artemiou and li ( @xcite ) .",
    "they proved that if there are @xmath1 pc , and @xmath6 , then the probability of the response being closer to the @xmath7-th pc than to the @xmath8-th pc is higher than the probability of the complementary event the response being closer to the @xmath8-th pc than to the @xmath7-th pc .",
    "however , question 2 only is important if question 1 has positive answer , because if the original input variables explain better the answer than the pc , then a pca is not justified .",
    "therefore , we focus our interest here in question 1 .",
    "the aim here is to propose an explanation for the answer to this question showing , in general , that the probability of all the pc explaining better ( than all the original variables ) a response variable is high even in low dimensions , and increases exponentially as @xmath1 grows to infinite .    the main result of this article is theorem [ teopcs ] .",
    "it answers partially question 1 on why pc usually do a better job explaining a response than the original input variables .",
    "theorem [ teopcs ] is in fact a particular version of theorem [ teolines ] , a geometric result that establishes the following : let us assume we have a given orthogonal base of a @xmath1-dimensional vectorial space , assume also that we select according to a uniform distribution a @xmath1-dimensional base for the same space ( so it does not have two orthogonal vectors a.s . , and it generates the space a.s . ) .",
    "then , if we select a new vector uniformly at random , this new vector is going to be closer ( in terms of angular distance ) to the orthogonal base than to the random base with probability no less than 1/2 for every @xmath1 and this probability converges approximately to 0.6826 as @xmath9 .",
    "consider the space @xmath10 with orthogonal base @xmath11 . take @xmath12 to be the space of all unidimensional lines traversing the origin .",
    "select from @xmath12 uniformly and independently a set of @xmath2 lines @xmath13 and @xmath4 .",
    "note that @xmath13 form almost surely a basis for @xmath10 . here _ uniformly _ means that we have a unit closed ball centered around the origin @xmath14 $ ] with surface @xmath15 .",
    "then , calling @xmath16 the lebesgue measure in @xmath1 dimensions , for every @xmath17 , we have that the probability of an element in @xmath12 to traverse @xmath18 is given by    @xmath19    where @xmath20 .",
    "let @xmath21 be the angle between @xmath4 and @xmath22 , and define @xmath23 .",
    "let @xmath24 be the angle between @xmath4 and @xmath25 , and define @xmath26 .",
    "let @xmath27 be the angle between @xmath28 and @xmath25 , and define @xmath29 .",
    "call @xmath30 the orthant ( hyper - octant ) where @xmath4 is located in , and , analogously , @xmath31 and @xmath32 the orthants where @xmath22 and @xmath33 live in , respectively . for convenience ,",
    "let us refer to the orthants by the combination of signs they correspond to ( i.e. , when @xmath34 the four quadrants can be denoted by @xmath35 @xmath36 ) .",
    "figure [ fig1 ] gives a geometric representation of expectations of uniformly and independently chosen random variables ( @xmath37 , @xmath38 ) for @xmath34 .",
    "figure [ fig2 ] gives a geometric representation of realizations of uniformly and independently chosen random variables ( @xmath37 , @xmath38 ) and @xmath4 for @xmath34 illustrating how which set , ( @xmath37 , @xmath38 ) or ( @xmath39 , @xmath40 ) , explains better the r.v .",
    "@xmath4 in terms of angular distance .     of the conditional expectation of @xmath37 and @xmath38 ( red vectors )",
    "given the quadrant in which they are living .",
    "blue vectors represent @xmath39 and @xmath40.,scaledwidth=100.0% ]     of @xmath41 and @xmath4 .",
    "blue vectors represent @xmath39 and @xmath40 , red vectors represent realizations of @xmath37 and @xmath38 , and the green vector represents @xmath4 .",
    "areas of circle sectors of @xmath42 explaining @xmath4 better than @xmath43 are shaded in blue , and sectors of @xmath43 explaining @xmath4 better than @xmath42 are shaded in red .",
    "the overlapping sectors of variables @xmath43 appear in darker red .",
    "notice that when `` @xmath42 explains @xmath4 better than @xmath43 '' , then the red shaded regions of @xmath37 and @xmath38 overlap .",
    "also , notice that the case of `` @xmath42 explaining @xmath4 as well as @xmath43 '' includes the situation where ( @xmath37 , @xmath38 ) are mutually orthogonal . in all plots ,",
    "illustrations are for an arbitrary realization of @xmath44 , as well as @xmath45 , and any realizations of @xmath46 in the indicated intervals.,title=\"fig:\",scaledwidth=100.0% ] -10pt    we prove the following result :    [ teolines ]    1 .   for @xmath47 fixed and @xmath48 , the probability of @xmath4 being closer to @xmath49 than to @xmath25 is bigger than or equal 1/2",
    ". 2 .   for @xmath47",
    "fixed and @xmath48 , the probability of @xmath4 being closer to @xmath49 than to @xmath25 converges to @xmath50 .",
    "call @xmath51 the event `` @xmath4 is located in the orthant @xmath30 '' and , analogously , let @xmath52 as the event `` @xmath25 is living in the orthant @xmath53 '' . because of ( [ unif ] ) these events have each probability @xmath54 .",
    "assume without loss of generality that @xmath30 is @xmath55",
    ". then @xmath56=(1,\\ldots,1)p^{-1/2},\\end{aligned}\\ ] ] and from this we obtain @xmath57=p^{-1/2},\\end{aligned}\\ ] ] for @xmath58 .",
    "analogously to ( [ cosw ] ) , we have that @xmath59=p^{-1/2},\\end{aligned}\\ ] ] for @xmath60 .",
    "now , for @xmath61 , let @xmath8 be the number of different signs between @xmath30 and @xmath53 .",
    "then we obtain that @xmath62=1 - 2jp^{-1},\\end{aligned}\\ ] ] and , from ( [ cosw ] ) and ( [ coszw ] ) , @xmath63=p^{-1/2}+2jp^{-1}-1,\\end{aligned}\\ ] ] so we get that ( [ cosdifs ] ) is nonnegative iff @xmath64 . call @xmath65 .",
    "then , for @xmath66 we get that , @xmath67 therefore @xmath68    as a visualization of the rate of convergence , we show in figure [ fig3 ] the corresponding convergence plot of @xmath69 as a function of @xmath1 .     for @xmath70.,title=\"fig:\",scaledwidth=100.0% ]",
    "-10pt    now , by ( [ unif ] ) , total probability and independence , @xmath71=\\textbf e\\textbf 1\\left\\{g_{ik}^+|e_w , e_{v_i}\\right\\}.\\end{aligned}\\ ] ]    [ condexp ] @xmath72\\geq1/2 $ ] , for all @xmath73 .",
    "some comments are in order before proceeding . because of uniformity we are only considering half of the space ( @xmath74 orthants ) , since for all @xmath75 , if @xmath76 lives in a given orthant @xmath77 , then it is also living in @xmath78 . from this observation",
    "we see why all the combinations considered in ( [ dif11 ] ) are located on the left half of the pascal triangle for each @xmath1 .",
    "call @xmath79 the total number of terms in this half of the pascal triangle ; then we have @xmath80 . for a given @xmath1 ,",
    "call @xmath81 the number of terms being added in ( [ dif11 ] ) , and we have that @xmath82 .",
    "call also @xmath83 .",
    "finally , let @xmath84 be @xmath72-\\textbf p\\left[\\left(g_{ik}^+\\right)^c\\right]$ ] , then the statement in proposition [ condexp ] is equivalent to prove that @xmath85 and it is enough to look at the numerators of these probabilities .    [ simpleprop ] @xmath86 , @xmath87 , @xmath79 and @xmath84 satisfy the following properties :    1 .",
    "@xmath88 for all even @xmath1 .",
    "@xmath89 for all even @xmath1 .",
    "@xmath90 for all @xmath1 .",
    "@xmath91 is increasing and either @xmath92 .",
    "@xmath93 6 .",
    "@xmath94 . 7 .",
    "let @xmath73 and define @xmath96 .",
    "then @xmath97 , the cardinality of @xmath98 , is at least 2 and at most 3 for all @xmath73 .",
    "9 .   for @xmath99 and @xmath100",
    ", we have that @xmath101 .",
    "( 1 ) , ( 2 ) and ( 3 ) follow from the definitions .",
    "( 4 ) follows from the fact that @xmath102 is an increasing sequence and @xmath103 .",
    "the following ones can be easily deducted from the former properties .",
    "note that @xmath84 is not monotonically increasing .",
    "when @xmath104 , we have that @xmath105 is either @xmath81 or @xmath106 . for such cases , @xmath107",
    "however , the next proposition proves that despise this fact , the subsequence @xmath108 is increasing .",
    "[ increasingsubseq ] @xmath109 , for all @xmath110 .",
    "first , note that for all @xmath111 , we have that @xmath112 .",
    "thus , @xmath113 ( see properties ( 3 ) , ( 6 ) and ( 7 ) in lemma [ simpleprop ] ) . using this fact and basic combinatorics properties , it is easily seen that @xmath114 , which proves the lemma .",
    "lemmas [ simpleprop ] and [ increasingsubseq ] , together with the fact that @xmath115 , prove the result .",
    "part 1 follows directly from proposition [ condexp ] .    to prove the part 2 ,",
    "let @xmath116 be a bin@xmath117 .",
    "by the central limit theorem we have : @xmath118\\rightarrow\\phi(-1 ) ,      \\end{aligned}\\ ] ] therefore , by symmetry of the sequence @xmath119 , we obtain the result .    as a visualization of the rate of convergence , we show in figure [ fig3 ] the corresponding convergence plot of ( [ dif11 ] ) as a function of @xmath1 .    [ r8 ] seeing ( [ probg ] ) from the viewpoint of the unconditioned probability of @xmath120 at the left - hand side , we are looking at the event of , given @xmath7 and @xmath47 , having @xmath49 closer to @xmath4 than @xmath25 . therefore , given @xmath47 , the r.v .",
    "@xmath121bin@xmath122)$ ] tells us the probability that @xmath49 explains better @xmath4 than @xmath123 of the @xmath43 s .",
    "for instance , since @xmath124\\approx 2\\textbf p[(g^+_{ik})^c]$ ] ( indeed , it can be shown that @xmath124>2\\textbf p[(g^+_{ik})^c]$ ] for @xmath125 ) , then @xmath126\\approx 2^p\\textbf p[u_k=0]$ ] .",
    "that is , the probability of @xmath49 explaining better @xmath4 than all the @xmath42 s is exponentially higher than the probability of all the @xmath43 s explaining better @xmath4 than @xmath49 .",
    "[ r9 ] on the other hand , the right - hand side of ( [ probg ] ) , because of the condition on @xmath51 , implies that in expectation @xmath4 is closer to _ all _",
    "@xmath0 than to @xmath25 , since being in `` the middle '' of an orthant means that @xmath4 is at the same distance of each @xmath22 , @xmath127 .",
    "therefore , as in the previous remark , for large @xmath1 , theorem [ teolines ] reveals the conditionally expected proportion of @xmath43 s that do not explain @xmath4 as well as _ all _ the @xmath42 s . the value is around 2/3 .",
    "the downside of this conditioning is that around 1/3 of the @xmath43 s are expected to explain better @xmath4 than all the @xmath42 s .",
    "finally , note that having compared the distance from @xmath4 to @xmath25 and to @xmath49 as we did above , it is difficult to add to the analysis a second orthogonal vector @xmath128 , with @xmath129 , since the angle between @xmath4 and @xmath128 is going to be dependent on the angle betwen @xmath4 and @xmath49 .",
    "one advantage of the approach used in the former section is the relatedness of the cosine similarity and the correlation coefficient of two random variables .",
    "the only difference being that the correlation coefficient is invariant to translations , while the cosine similarity is not . from the geometrical point of view",
    ", the problem is solved by restricting ourselves to consider only the vectors traversing the origin of the @xmath1-dimensional euclidean space , precisely in the way @xmath12 is defined . from the probabilistic point of view , however , this restriction does not affect , since there is no loss of generality once we consider random vectors centered around the origin of the space :    let @xmath130 be the space of all the continuous random variables in @xmath10 with expected value 0 and such that any @xmath1-vector @xmath131 of @xmath1 of them has a covariance matrix @xmath132 . let @xmath133 be the set of principal components of @xmath131 . then the @xmath1 lines partition the space in ( almost ) equal regions .",
    "consider any continuous random variable @xmath134 living in @xmath10 such that @xmath135 .",
    "let @xmath12 be the space of unidimensional supports of the random variables in @xmath10 .",
    "we have the following result :    [ teopcs ]    1 .   for @xmath47 fixed and @xmath48",
    ", we have that @xmath136\\geq1/2 .",
    "\\end{aligned}\\ ] ] 2 .   for @xmath47 fixed and @xmath48 , we have that @xmath136\\rightarrow\\phi(1)-\\phi(-1)\\approx 0.6826 .          \\end{aligned}\\ ] ]    take @xmath137 to be the ( unidimensional ) supports of @xmath138 , respectively .",
    "take @xmath139 the ( unidimensional ) supports of @xmath140 , respectively .",
    "take @xmath141 to be the support of @xmath134 .",
    "then the result follows from theorem [ teolines ] .",
    "some comments are in order here :    * in the context of geometry , theorem [ teolines ] is true for every set of orthogonal vectors , but their being orthogonal through uniform random selection is not happening a.s .",
    "however , even if @xmath142 are arbitrarily selected to be orthogonal , then it can be seen that they explain the space as good as @xmath143 , but no better . * because of the former point , the uncorrelated random variables do not need to be the set of principal components .",
    "however , the condition of knowing the covariance matrix @xmath132 allows us to identify a particular set of mutually orthogonal random variables for which we know explicitly the joint distribution and the marginals .",
    "* when it comes to pca we are not doing two things : first , we are not using the variance of the pc in any sense in this proof , except to find the directions of the components themselves . to be sure , in @xcite ,",
    "the authors used the variance of the principal components to prove that it is more probable that the response @xmath134 is closer to the principal component @xmath144 than to the principal component @xmath145 , for @xmath146 . * second , we are not projecting the space to any lower dimension when using principal components .",
    "as it is known , reducing the space in contexts of regression might be problematic ( see for instance @xcite and @xcite ) . but theorem [ teolines ] is true for all the @xmath1 pc in the space .",
    "* remarks [ r8 ] and [ r9 ] also admit natural extensions to this scenario of random variables and principal components .",
    "so we get , for instance , that the probability of _ all _ principal components explaining better the response is exponentially higher than the probability of _ all _ the original input explaining it better . of course",
    ", not even principal components are immune to the curse of dimensionality , but it shows that it affects the pc explanatory power at a lower speed .",
    "principal components analysis ( pca ) is a widely used technique but some mystery has remained as to why it s a reasonable thing to do when modeling a response - predictor relationship since it was introduced in @xcite and @xcite about 60 years ago .",
    "we offer a partial answer to this question .",
    "a more comprehensive answer will have to look at the probability of the response being closer to at least one of the pc than to all of the original input variables , but this is still an open problem . 0.5 in * acknowledgements * : all authors supported in part by nih grant nci r01-ca160593a1 .",
    "we would like to thank rob tibshirani , steve marron and hemant ishwaran for helpful discussions of the work .",
    "dd would like to thank juan saenz for his suggestions on how to prove the results and federico ardila for an early very useful comment on proposition [ condexp ] ."
  ],
  "abstract_text": [
    "<S> we show that if we have an orthogonal base ( @xmath0 ) in a @xmath1-dimensional vector space , and select @xmath2 vectors @xmath3 and @xmath4 such that the vectors traverse the origin , then the probability of @xmath4 being to closer to all the vectors in the base than to @xmath3 is at least 1/2 and converges as @xmath1 increases to infinity to a normal distribution on the interval [ -1,1 ] ; i.e. , @xmath5 . </S>",
    "<S> this result has relevant consequences for principal components analysis in the context of regression and other learning settings , if we take the orthogonal base as the direction of the principal components . </S>"
  ]
}