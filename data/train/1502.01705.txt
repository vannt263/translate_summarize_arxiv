{
  "article_text": [
    ", deep learning models ( e.g. , deep belief networks ( dbn)@xcite , stacked denoising auto - encoder @xcite , deep boltzmann machine ( dbm ) @xcite and etc . ) have drawn increasing attention due to their impressive empirical performance in various application areas , such as computer vision @xcite@xcite@xcite , natural language processing @xcite and information retrieval @xcite@xcite . despite of these practical successes , there have been debates on the fundamental principle of the design and training of those deep architectures . in most situations ,",
    "searching the parameter space for deep learning models is difficult . to tackle this difficulty , _ unsupervised pre - training _ has been introduced as an important process . in @xcite",
    ", it has been empirically shown that the unsupervised pre - training could fit the network parameters in a region of the parameter space that could well capture the data distribution , thus alleviating generalization error of the trained deep architectures .    the process of pre - training aims to discover the latent representation of the input data based on the learnt generative model , from which we could regenerate the input data",
    ". a better generative model would generally lead to more meaningful latent representations . from the density estimation point of view",
    ", pre - training can be interpreted as an attempt to recover a set of parameters for a generative model that describes the underlying distribution of the observed data .",
    "since boltzmann machines ( bm ) are building blocks for many deep architectures ( e.g. , dbn and dbm ) , we will focus on a formal analysis of the essential parts of the target density that the bm can capture in model selection .    in practice ,",
    "the datasets that we deal with are often high - dimensional .",
    "thus we would require a model with high - dimensional parameter space in order to effectively depict the underlying real distribution .",
    "overfitting usually occur when the model is excessively complex with respect to a small dataset . on the other hand ,",
    "if a large dataset is available , underfitting would occur when the model is too simple to capture the underlying trend of the data .",
    "moreover , this connection becomes more complicated if the observed samples contain noises .",
    "thus , to alleviate overfitting or underfitting , a basic model selection criterion is needed to adjust the complexity of the model with respect to the available observations ( usually insufficient or perturbed by noises ) .",
    "next , for density estimation , we will restate the model selection problem as parametric reduction on the parameter space of multivariate distributions , which could lead to our general parameter reduction criterion , i.e. , the confident - information - first ( cif ) principle .",
    "assuming there exists an universal parametric probabilistic model @xmath0 ( with @xmath1 free parameters ) that is general enough to represent all system phenomena , the goal of the parametric reduction is to derive a lower - dimensional sub - model @xmath2 ( with @xmath3 free parameters ) by reducing the number of free parameters in @xmath0 .",
    "note that the number of free parameters is adopted as a model complexity measure , which is in line with various model selection criteria ( such as akaike information criterion ( aic)@xcite , bayesian information criterion ( bic)@xcite and etc ) .",
    "be a two - dimensionality manifold with two free parameters @xmath4 and @xmath5 , and @xmath6 with free parameter @xmath4 and @xmath7 with free parameter @xmath5 are the submanifold of @xmath0 ; as an illustration in euclidean space , we show @xmath8 ( on which the true distribution @xmath9 located on ) as the surface of a hyper - ellipsoid centered at sample distribution @xmath10 determined by the fisher - rao metric ; only part of the original distance between @xmath9 and @xmath10 ( @xmath11 ) can be preserved after projection on submanifold @xmath2 ; the preferred @xmath2 is the one that maximally preserves the original distance after projection .",
    "note that the scale of the distances in fig [ fig : distanceprojection ] are shown as a demo , and are not exactly proportional to the real riemann distances induced by fisher - rao metric , scaledwidth=50.0% ]    in this paper , we formalize the parametric reduction in the theoretical framework of information geometry ( ig ) . in ig , the general model @xmath0 can be seen as a @xmath1-dimensionality manifold and @xmath2 is a smoothed submanifold of @xmath0 .",
    "the number of free parameters in @xmath2 is restricted to be a constant @xmath12 ( @xmath3 ) .",
    "then , the major difficulty in the parametric reduction procedure is the choice of parameters to keep or to cut . in this paper",
    ", we propose to reduce parameters such that the original geometric structure of @xmath0 can be preserved as much as possible after projecting on the submanifold @xmath2 .",
    "let @xmath13 be the true distribution and the sampling distribution ( maybe perturbed from @xmath9 by sampling bias or noises ) respectively .",
    "it can be assumed that the true distribution @xmath9 is located somewhere in a @xmath14-sphere surface @xmath8 centered at @xmath10 , i.e. , @xmath15 , where @xmath16 denotes the distance measure on the manifold @xmath0 , and @xmath14 is a small number .",
    "this assumption is made without losing generality , since the @xmath14 is a small variable .",
    "for a distribution @xmath17 , the best approximation of @xmath17 on @xmath2 is the point @xmath18 that belongs to @xmath2 and is the closest to @xmath17 in terms of the distance measure , i.e. , @xmath19 , which is defined as the projection of @xmath17 onto @xmath2 ( denoted as @xmath20 ) .",
    "then , the parametric reduction can be defined as the optimization problem to maximally preserve the expectation of the fisher information distance with respect to the constraint of the parametric number , when projecting distributions from the parameter space of @xmath0 onto that of the reduced submanifold @xmath2 : @xmath21 here , the fisher information distance ( fid ) , i.e. , the riemannian distance induced by the fisher - rao metric @xcite , is adopted as the distance measure between two distributions , since it is shown to be the unique metric meeting a set of natural axioms for the distribution metric @xcite@xcite@xcite , e.g. , the invariant property with respect to reparametrizations and the monotonicity with respect to the random maps on variables .",
    "let @xmath22 be the distribution parameters .",
    "for two close distributions @xmath23 and @xmath24 with parameters @xmath25 and @xmath26 , the fisher information distance between @xmath23 and @xmath24 is : @xmath27 where @xmath28 is the fisher information matrix @xcite .",
    "note that the solution to this optimization problem ( equation [ eq : parametricreduction2 ] ) is not unique , since we can assign different fixed values for non - free parameters in @xmath2 .",
    "intuitively , to determine the appropriate values for non - free parameters , our best choice is the @xmath2 that intersects at @xmath10 .",
    "however , in general cases where @xmath10 is not specified in advance , it is natural to assign non - free parameters to a neutral value ( e.g. , zero ) .",
    "this treatment is used by the general cif ( see section [ sec : cif ] ) .",
    "if @xmath10 is specified in advance , we can , in principle , further select a @xmath2 as close to @xmath10 as possible .",
    "it turn out that we can develop a sample - specific cif w.r.t given samples ( see section [ sec : cd - cif ] ) .",
    "the rationality of maximally preserving the fisher information distance can also be interpreted from the maximum - likelihood ( ml ) estimation point of view .",
    "let @xmath29 be the ml estimators for @xmath22 .",
    "the asymptotic normality of ml estimation implies that the distribution of @xmath29 is the normal distribution with mean @xmath22 and covariance @xmath30 , i.e. , @xmath31 where the inverse of @xmath30 can be asymptotically estimated using the fisher information matrix @xmath28 , as suggested by the cramr - rao bound @xcite and the asymptotic normality of ml estimation . from the fisher information distance given in equation [ eq : fisherdistance ] , the exponent part of equation [ eq : normalapproximation ] is just the opposite of the half squared fisher information distance between two distributions @xmath17 and @xmath32 determined by the close parameters @xmath22 and @xmath29 , respectively .",
    "hence a large fisher information distance means a lower likelihood .",
    "it turns out that , in density estimates , maximally preserving the expected fisher information distance after the projection @xmath33 ( equation [ eq : parametricreduction2 ] ) is equivalent to maximally preserving the likelihood - structure among close distributions .",
    "in supervised learning ( e.g. , classification ) , maximally preserving fid can also effectively preserve the likelihood - structure among different class densities ( the underlying distributions of classes ) , which is beneficial against sample noises . recall that sample noises always reduce the fid among class densities in a statistical sense , which lead to the reduced discrimination marginality between two class densities .",
    "hence , for noisy data , the model that maximally preserving fid can capture the dominant discrimination between class densities . to solve the optimization problem in equation [ eq : parametricreduction2 ]",
    ", we propose a parameter reduction criterion called the _ confident - information - first _",
    "( cif ) principle , described as follows .",
    "the fisher information distance @xmath34 can be decomposed into the distances of two orthogonal parts @xcite .",
    "moreover , it is possible to divide the system parameters in @xmath0 into two categories ( corresponding to the two decomposed distances ) , _ i.e. _ , the parameters with `` major '' variations and the parameters with `` minor '' variations , according to their contributions to the whole information distance .",
    "the former refers to parameters that are important for reliably distinguishing the true distribution from the sampling distribution , thus considered as ",
    "confident \" . on the other hand ,",
    "the parameters with minor contributions can be considered as less reliable .",
    "hence , the cif principle can be stated as parametric reduction that preserves the confident parameters and rules out less confident parameters .",
    "we will theoretically show that cif leads to an optimal submanifold @xmath2 in terms of the optimization problem defined in equation [ eq : parametricreduction2 ] .",
    "it is worth emphasizing that the proposed cif as a principle of parametric reduction is fundamentally different from the traditional feature reduction ( or feature extraction ) methods @xcite .",
    "the latter focus on directly reducing the dimensionality on feature space by retaining maximal variations in the data , e.g. , principle components analysis ( pca ) @xcite , while cif offers a principled method to deal with high - dimensional data in the parameter spaces by a strategy that is derived from the first principle , independent of the scales of features .",
    "the main contributions of this paper are :    1 .",
    "we incorporate the fisher information distance into the modelling of the intrinsic variations in the data that give rise to the desired model in the framework of ig .",
    "we propose a cif principle for parametric reduction to maximally preserve the confident parameters and ruling out less confident ones .",
    "3 .   for binary multivariate distributions , we theoretically show that cif could analytically lead to an optimal submanifold w.r.t . the parametric reduction problem in equation [ eq : parametricreduction2 ] .",
    "4 .   the utility of cif , i.e.",
    ", the derivation of probabilistic models , is illustrated by revisiting the boltzmann machines ( bm ) .",
    "we show by examples that some existing probabilistic models , e.g. , the fully visible bm ( vbm ) and the bm with hidden units , comply with the cif principle and can be derived from it .",
    "5 .   given certain samples",
    ", we propose a sample - specific cif - based model selection scheme for the bolzmann machines .",
    "it leads to a significant improvement in a series of density estimation experiments .",
    "in this section , we introduce and develop the theoretical foundations of ig@xcite for the manifold @xmath0 of binary multivariate distributions with a given number of variables @xmath1 , i.e. , the open simplex of all probability distributions over binary vector @xmath35 .",
    "this will lay the foundation for our theoretical deviation of the _ cif_.      in ig , a family of probability distributions is considered as a differentiable manifold with certain parametric coordinate systems . in the case of binary multivariate distributions , four basic coordinate systems are often used @xcite@xcite : @xmath17-coordinates , @xmath36-coordinates , @xmath37-coordinates and the mixed @xmath38-coordinates .",
    "the @xmath38-coordinates is of vital importance for our analysis .",
    "for the @xmath17-coordinates @xmath39 $ ] , the probability distribution over @xmath40 states of @xmath41 can be completely specified by any @xmath42 positive numbers indicating the probability of the corresponding exclusive states on @xmath1 binary variables .",
    "for example , the @xmath17-coordinates of @xmath43 variables could be @xmath39=(p_{01 } , p_{10 } , p_{11})$ ] .",
    "note that ig requires all probability terms to be positive @xcite . for simplicity",
    ", we use the capital letters @xmath44 to index the coordinate parameters of probabilistic distribution .",
    "an index @xmath45 can be regarded as a subset of @xmath46 .",
    "additionally , @xmath47 stands for the probability that all variables indicated by @xmath45 equal to one and the complemented variables are zero . for example , if @xmath48 and @xmath49 , we have : @xmath50 note that the null set can also be a legal index of the @xmath17-coordinates , which indicates the probability that all variables are zero , denoted as @xmath51 .",
    "the @xmath36-coordinates @xmath52 $ ] are defined by : @xmath53=prob\\{\\prod_{i\\in i}x_i = 1\\}\\ ] ] where the value of @xmath54 is given by @xmath55 and the expectation is taken with respect to the probability distribution over @xmath41 . grouping the coordinates by their orders ,",
    "the @xmath36-coordinates are denoted as @xmath52=(\\eta^1_i , \\eta^2_{ij},\\dots , \\eta^n_{1,2 ...",
    "n})$ ] , where the superscript indicates the order number of the corresponding parameter . for example , @xmath56 denotes the set of all @xmath36 parameters with the order number two .",
    "the @xmath37-coordinates ( natural coordinates ) @xmath57 $ ] are defined by : @xmath58 where @xmath59 is the cumulant generating function and its value equals to @xmath60 . by solving the linear system [ eq : thetacoordinate ] , we have @xmath61 .",
    "the @xmath37-coordinate is denoted as @xmath57=(\\theta^{i}_1 , \\theta^{ij}_2,\\dots , \\theta^{1, ...",
    ",n}_n)$ ] , where the subscript indicates the order number of the corresponding parameter .",
    "note that the order indices locate at different positions in @xmath52 $ ] and @xmath57 $ ] following the convention in @xcite .",
    "the relation between coordinate systems @xmath52 $ ] and @xmath57 $ ] is bijective .",
    "more formally , they are connected by the legendre transformation : @xmath62 where @xmath63 is given in equation [ eq : thetacoordinate ] and @xmath64 is the negative of entropy . it can be shown that @xmath63 and @xmath65 meet the following identity @xcite : @xmath66    the @xmath67-mixed @xmath38-coordinates @xmath68_l$ ] are defined by : @xmath69_l\\!\\!=\\!\\![\\eta^{l- } , \\theta_{l+}]\\!\\!=\\!\\!(\\eta^1_i , \\eta^2_{ij},\\dots , \\eta^l_{i , j,\\dots , k } , \\theta^{i , j,\\dots , k}_{l+1},\\dots , \\theta^{1, ...",
    ",n}_n)\\ ] ] where the first part consists of @xmath36-coordinates with order less or equal to @xmath67 and the second part consists of @xmath37-coordinates with order greater than @xmath67 , @xmath70 .      for a general coordinate system",
    "@xmath71 $ ] , the @xmath72-th row and @xmath73-th column element of the fisher information matrix for @xmath71 $ ] ( denoted by @xmath74 ) is defined as the covariance of the scores of @xmath75 $ ] and @xmath76 $ ] @xcite : @xmath77\\ ] ] under the regularity condition that the partial derivatives exist .",
    "the fisher information measures the amount of information in the data that a statistic carries about theunknown parameters @xcite .",
    "the fisher information matrix is of vital importance to our analysis , because the inverse of fisher information matrix gives an asymptotically tight lower bound to the covariance matrix of any unbiased estimate for the considered parameters @xcite .",
    "another important concept related to our analysis is the orthogonality defined by fisher information .",
    "two coordinate parameters @xmath78 and @xmath79 are called orthogonal if and only if their fisher information vanishes , _ i.e_. , @xmath80 , meaning that their influences on the log likelihood function are uncorrelated .",
    "the fisher information for @xmath57 $ ] can be rewritten as @xmath81 , and for @xmath52 $ ] , it is @xmath82 @xcite .",
    "let @xmath83 and @xmath84 be the fisher information matrices for @xmath57 $ ] and @xmath52 $ ] , respectively .",
    "it can be shown that @xmath85 and @xmath86 are mutually inverse matrices , _",
    "i.e_. , @xmath87 , where @xmath88 if @xmath89 and zero otherwise @xcite . in order to generally compute @xmath85 and @xmath86",
    ", we develop the following propositions [ prop : fishermatrix ] and [ prop : fishermatrix_eta ] .",
    "note that proposition [ prop : fishermatrix ] is a generalization of theorem 2 in @xcite .",
    "[ prop : fishermatrix ] the fisher information between two parameters @xmath90 and @xmath91 in @xmath57 $ ] , is given by : @xmath92    in appendix [ appendix : thetafisher ] .    [ prop : fishermatrix_eta ] the fisher information between two parameters @xmath93 and @xmath94 in @xmath52 $ ] , is given by : @xmath95 where @xmath96 denotes the cardinality operator .    in appendix",
    "[ appendix : etafisher ] .",
    "we take the probability distribution with three variables for example .",
    "based on equation [ eq : proposiationfishermetriceta ] , the fisher information between @xmath93 and @xmath94 can be calculated , e.g. , @xmath97 if @xmath98 and @xmath99 , @xmath100 if @xmath98 and @xmath101 , and etc .    based on @xmath86 and @xmath85 , we can calculate the fisher information matrix @xmath102 for the @xmath68_l$ ] .",
    "[ prop : fishermatrix_mix ] the fisher information matrix @xmath102 of @xmath68_l$ ] is given by : @xmath103 where @xmath104 , @xmath105 , @xmath86 and @xmath85 are the fisher information matrices of @xmath52 $ ] and @xmath57 $ ] , respectively , @xmath106 is the index set of the parameters shared by @xmath52 $ ] and @xmath68_l$ ] , _",
    "i.e_. , @xmath107 , and @xmath108 is the index set of the parameters shared by @xmath57 $ ] and @xmath68_l$ ] , _",
    "i.e_. , @xmath109 .    in appendix [",
    "appendix : mixfisher ] .",
    "the general manifold @xmath0 of all probability distributions over binary vector @xmath35 could be exactly represented using the @xmath110 parametric coordinates .",
    "given a target distribution @xmath111 , we consider the problem of realizing it by a lower - dimensionality submanifold @xmath2 .",
    "this is defined as the problem of parametric reduction for multivariate binary distributions .    in this section",
    ", we will formally illuminate the general cif for parametric reduction . intuitively ,",
    "if we can construct a coordinate system so that the confidences of its parameters entail a natural hierarchy , in which high confident parameters are significantly distinguished from and orthogonal to lowly confident ones , then we can conveniently implement cif by keeping the high confident parameters unchanged and setting the lowly confident parameters to neutral values . as described in section [ sec : intro ] , the confidence of parameters should be assessed according to their contributions to the expected information distance .",
    "therefore , the choice of coordinates in cif is crucial to its usage .",
    "this strategy is infeasible in terms of @xmath17-coordinates , @xmath36-coordinates or @xmath37-coordinates , since the orthogonality condition can not hold in these coordinate systems . in this section",
    ", we will show that the @xmath67-mixed - coordinates @xmath68_l$ ] meets the requirement of cif .    to grasp an intuitive picture for the general cif strategy and its significance w.r.t mixed - coordinates @xmath68_l$ ]",
    ", we will first show that the @xmath67-mixed - coordinates @xmath68_l$ ] meets the requirement of cif in typical distributions that generate real - world datasets",
    ". then we will prove that cif could lead to an optimal submanifold w.r.t .",
    "the parametric reduction problem in equation [ eq : parametricreduction2 ] , in general cases .      to facilitate our analysis",
    ", we make a basic assumption on the underlying distributions @xmath112 that at least @xmath113 @xmath17-coordinates are of the scale @xmath114 , where @xmath114 is a sufficiently small value .",
    "thus , residual @xmath17-coordinates ( at most @xmath115 ) are all significantly larger than zero ( of scale @xmath116 ) , and their sum approximates one . note that these assumptions are common situations in real - world data collections @xcite , since the frequent ( or meaningful ) patterns are only a small fraction of all of thesystem states .",
    "next , we introduce a small perturbation @xmath117 to the @xmath17-coordinates @xmath39 $ ] for the true distribution @xmath112 . the perturbed distribution is denoted as @xmath118 . for @xmath17-coordinates that are significantly larger than zero",
    ", the scale of each fluctuation @xmath119 is assumed to be proportional to the standard variation of corresponding @xmath17-coordinate @xmath47 by some small coefficients ( upper bounded by a constant @xmath120 ) , which can be approximated by the inverse of the square root of its fisher information via the cramr  rao bound .",
    "it turns out that we can assume the perturbation @xmath119 to be @xmath121 . for @xmath17-coordinates with",
    "a small value ( approximates zero ) , the scale of each fluctuation @xmath119 is assumed to be proportional to @xmath122 .    in this section ,",
    "we adopt the @xmath67-mixed - coordinates @xmath68_l = ( \\eta^{l-};\\theta_{l+})$ ] , where @xmath123 is used in the following analysis .",
    "let @xmath124 be the incremental of mixed - coordinates after the perturbation .",
    "the squared fisher information distance @xmath125 could be decomposed into the direction of each coordinate in @xmath68_l$ ] .",
    "we will clarify that , under typical cases , the scale of the fisher information distance in each coordinate of @xmath126 ( will be reduced by cif ) is asymptotically negligible , compared to that in each coordinate of @xmath127 ( will be preserved by cif ) .",
    "the scale of squared fisher information distance in the direction of @xmath128 is proportional to @xmath129 , where @xmath130 is the fisher information of @xmath128 in terms of the mixed - coordinates @xmath68_2 $ ] . from equation  [ eq : etacoordinate ] , for any @xmath45 of order one ( or two ) ,",
    "@xmath131 is the sum of @xmath132 ( or @xmath133 ) @xmath17-coordinates , and the scale is @xmath134 .",
    "hence , the incremental @xmath135 is proportional to @xmath134 , denoted as @xmath136 .",
    "it is difficult to give an explicit expression of @xmath130 analytically .",
    "however , the fisher information @xmath130 of @xmath128 is bounded by the @xmath137-th element of the inverse covariance matrix @xcite , which is exactly @xmath138 ( see proposition  [ prop : fishermatrix_mix ] ) . hence , the scale of @xmath130 is also @xmath134 .",
    "it turns out that the scale of squared fisher information distance in the direction of @xmath128 is @xmath139 .",
    "similarly , for the part @xmath140 , the scale of squared fisher information distance in the direction of @xmath141 is proportional to @xmath142 , where @xmath143 is the fisher information of @xmath141 in terms of the mixed - coordinates @xmath68_2 $ ] .",
    "the scale of @xmath141 is maximally @xmath144 based on equation  [ eq : thetacoordinate ] , where @xmath12 is the order of @xmath141 and @xmath145 is the number of @xmath17-coordinates of scale @xmath116 that are involved in the calculation of @xmath141 . since we assume that @xmath146 , the maximum scale of @xmath141 is @xmath147 .",
    "thus , the incremental @xmath148 is of a scale bounded by @xmath149 .",
    "similar to our previous deviation , the fisher information @xmath143 of @xmath141 is bounded by the @xmath150-th element of the inverse covariance matrix , which is exactly @xmath151 ( see proposition  [ prop : fishermatrix_mix ] ) . hence , the scale of @xmath143 is @xmath152 . in summary ,",
    "the scale of squared fisher information distance in the direction of @xmath141 is bounded by the scale of @xmath153 .",
    "since @xmath114 is a sufficiently small value and @xmath120 is constant , the scale of squared fisher information distance in the direction of @xmath141 is asymptotically zero .    according to our above analysis",
    ", the confidences of coordinate parameters ( measured by the decomposed fisher information distance ) in @xmath68_l$ ] entail a natural hierarchy : the first part of high confident parameters @xmath154 $ ] are significantly larger than the second part of low confident parameters @xmath155 $ ] .",
    "additionally , those low confident parameters @xmath155 $ ] have the neutral value of zero .",
    "moreover , the parameters in @xmath154 $ ] are orthogonal to the ones in @xmath155 $ ] , indicating that we could estimate these two parts independently @xcite .",
    "hence , we can implement the cif for parametric reduction in @xmath68_l$ ] by replacing low confident parameters with neutral value zero and reconstructing the resulting distribution .",
    "it turns out that the submanifold of @xmath0 tailored by cif becomes @xmath68_{l_t}=(\\eta_{i}^1 , ... ,",
    "k}^l,0,\\dots,0)$ ] .",
    "we call @xmath68_{l_t}$ ] the @xmath67-tailored - mixed - coordinates .    to verify our theoretical analysis , we conduct a simulation on the ratio of fid that is preserved by the @xmath67-tailored - mixed - coordinates ( @xmath123 ) @xmath68_{l_t}$ ] w.r.t . the original mixed - coordinates @xmath68 $ ]",
    ". first we randomly select real distribution @xmath9 with @xmath1 variables , where the distribution satisfies the basic assumption that we make in the beginning of this section ( the @xmath115 significant @xmath17-coordinates are generated based on jeffery prior , left @xmath17-coordinates are set to a small constant ) .",
    "then we generate the sample distribution @xmath10 based on random samples drawn from the real distribution .",
    "last , we calculate the fid between @xmath9 and @xmath10 in terms of the @xmath68 $ ] and @xmath68_{l_t}$ ] respectively . the result is shown in table [ tab : fidpreservesimulation ] .",
    "we can see that the @xmath68_{l_t}$ ] can indeed preserve most of the fid , which is consistent with our theoretical analysis .",
    ".simulation on the fid preserved by @xmath68_{l_t}$ ] ( @xmath123 ) [ cols=\"^,^,^,^ \" , ]      on @xmath0 to a submanifold @xmath2 , the @xmath67-tailored mixed - coordinates @xmath68_{l_t}$ ] gives a desirable @xmath2 that maximally preserves the expected fisher information distance when projecting a @xmath14-neighborhood centered at @xmath112 onto @xmath2.,scaledwidth=40.0% ]      let @xmath156 be a @xmath14-sphere surface centered at @xmath112 on manifold @xmath0 , _",
    "i.e_. , @xmath157 , where @xmath158 denotes the kl divergence and @xmath14 is small .",
    "additionally , @xmath118 is a neighbor of @xmath112 uniformly sampled on @xmath156 , as illustrated in figure  [ fig : expectedfd ] .",
    "recall that , for a small @xmath14 , the kl divergence can be approximated by half of the squared fisher information distance .",
    "thus , in the parameterization of @xmath68_l$ ] , @xmath156 is indeed the surface of a hyper - ellipsoid ( centered at @xmath112 ) determined by @xmath102 .",
    "the following proposition shows that the general cif would lead to an optimal submanifold @xmath2 that maximally preserves the expected information distance , where the expectation is taken upon the uniform neighborhood , @xmath156 .",
    "[ prop : geometricview ] consider the manifold @xmath0 in @xmath67-mixed - coordinates @xmath68_l$ ] .",
    "let @xmath12 be the number of free parameters in the @xmath67-tailored - mixed - coordinates @xmath68_{l_t}$ ] .",
    "then , among all @xmath12-dimensional submanifolds of @xmath0 , the submanifold determined by @xmath68_{l_t}$ ] can maximally preserve the expected information distance induced by the fisher  rao metric .",
    "in appendix [ appendix : geometriccif ] .",
    "in previous section , a general cif is uncovered in the @xmath68_l$ ] coordinates for multivariate binary distributions .",
    "now we consider the implementations of cif when @xmath67 equals to 2 using the boltzmann machines ( bm ) .      in general , a bm @xcite is defined as a stochastic neural network consisting of visible units @xmath159 and hidden units @xmath160 , where each unit fires stochastically depending on the weighted sum of its inputs .",
    "the energy function is defined as follows : @xmath161 where @xmath162 are the parameters : visible - visible interactions ( @xmath163 ) , hidden - hidden interactions ( @xmath164 ) , visible - hidden interactions ( @xmath165 ) , visible self - connections ( @xmath166 ) and hidden self - connections ( @xmath167 ) .",
    "the diagonals of @xmath163 and @xmath164 are set to zero .",
    "we can express the boltzmann distribution over the joint space of @xmath41 and @xmath168 as below : @xmath169 where @xmath170 is a normalization factor .",
    "+    let @xmath171 be the set of boltzmann distributions realized by bm .",
    "actually , @xmath171 is a submanifold of the general manifold @xmath172 over @xmath173 . from equation ( [ eq : probbm ] ) and ( [ eq : energybm ] )",
    ", we can see that @xmath162 plays the role of @xmath171 s coordinates in @xmath37-coordinates ( equation [ eq : thetacoordinate ] ) as follows : @xmath174 so the @xmath37-coordinates for bm is given by : @xmath175_{bm}=(\\underbrace{\\theta^{x_i}_1 , \\theta^{h_j}_1}_{1-order } , \\underbrace{\\theta^{x_ix_j}_2 , \\theta^{x_ih_j}_2 , \\theta^{h_ih_j}_2}_{2-order } , \\underbrace{0 , \\dots , 0 } _ { orders>2}).\\ ] ]    the vbm and restricted bm are special cases of the general bm . since vbm has @xmath176 and all the visible units are connected to each other , the parameters of vbm are @xmath177 and @xmath178 are all set to zero . for rbm , it has connections only between hidden and visible units .",
    "thus , the parameters of rbm are @xmath179 and @xmath180 are set to zero .",
    "+    given the sample @xmath181 that generated from the underlying distribution , the _ maximum - likelihood _ ( ml ) is a commonly used gradient ascent method for training bm in order to maximize the log - likelihood @xmath182 of the parameters @xmath22 @xcite .",
    "based on equation ( [ eq : probbm ] ) , the log - likelihood is given as follows : @xmath183 differentiating the log - likelihood , the gradient with respect to @xmath22 is as follows : @xmath184}{\\partial \\xi } \\nonumber \\\\ \\!\\!\\!\\ ! & & - \\sum_{x',h ' } p(h'|x';\\xi)\\frac{\\partial [ -e(x',h';\\xi)]}{\\partial \\xi}\\end{aligned}\\ ] ] where @xmath185 can be easily calculated from equation ( [ eq : energybm ] ) .",
    "then we can obtain the stochastic gradient using gibbs sampling @xcite in two phases : sample @xmath186 given @xmath181 for the first term , called the positive phase , and sample @xmath187 from the stationary distribution @xmath188 for the second term , called the negative phase .",
    "now with the resulting stochastic gradient estimation , the learning rule is to adjust @xmath22 by : @xmath189 where @xmath14 is the learning rate , @xmath190 denotes the average using the sample data and @xmath191 denotes the average with respect to the stationary distribution @xmath192 after the corresponding gibbs sampling phases . to avoid the difficulty of computing the log - likelihood gradient , the contrastive divergence ( cd ) @xcite realizes the gradient descent of a different objective function , shown as follows : @xmath193 where @xmath194 is the sample distribution , @xmath195 is the distribution by starting the markov chain with the data and running @xmath196 steps and @xmath197 denotes the kl divergence .",
    "cd can be seen as an approximation to ml by replacing the last expectation @xmath191 with @xmath198 .",
    "consider the parametric reduction on the manifold @xmath0 over @xmath199 and end up with a @xmath12-dimensional submanifold @xmath2 of @xmath0 , where @xmath200 is the number of free parameters in @xmath2 .",
    "@xmath2 is set to be the same dimensionality as vbm , i.e. , @xmath201 , so that all candidate submanifolds are comparable to the submanifold @xmath202 endowed by vbm .",
    "next , the rationale underlying the design of @xmath202 can be illuminated using the general cif .",
    "+    in the following corollary , we will show that the statistical manifold @xmath202 is the optimal parameter subspace spanned by those directions with high confidences in terms of cif .",
    "[ prop : sbmcif ] given the general manifold @xmath0 in @xmath203-mixed - coordinates @xmath68_2 $ ] , vbm ( with coordinates @xmath68_{2_t}$ ] ) defines an @xmath12-dimensional submanifold of @xmath0 that can maximally preserve the expected fisher information distance induced by fisher - rao metric .    in appendix",
    "[ appendix : sbmcif ] .",
    "+    to learn such @xmath68_{2_t}$ ] , we need to learn the parameters @xmath22 of vbm such that its stationary distribution preserves the same coordinates @xmath204 $ ] as target distribution @xmath112 . actually , this is exactly what traditional gradient - based learning algorithms intend to do .",
    "next proposition shows that the ml learning of vbm is equivalent to learn the coordinates @xmath68_{2_t}$ ] .",
    "[ prop : sbmmlcloseform ] given the target distribution @xmath112 with 2-mixed coordinates : @xmath205_2=(\\eta^1_i , \\eta^2_{ij},\\theta_{2+})\\ ] ] the coordinates of the stationary distribution of vbm trained by ml are uniquely given by : @xmath205_{2_t}=(\\eta_{i}^1 , \\eta_{ij}^2,\\theta_{2+}=0)\\ ] ]    in appendix [ appendix : sbmmlcloseform ] .      in previous section ,",
    "the cif is applied to models without hidden units and leads to vbm by preserving the @xmath206-order and @xmath203-order @xmath36-coordinates . in this section",
    ", we will investigate the cases where hidden units are introduced .",
    "let @xmath172 be the manifold of distributions over the joint space of visible units @xmath41 and hidden units @xmath168 .",
    "a general bm produces a stationary distribution @xmath207 over @xmath173 .",
    "let @xmath171 denote the submanifold of @xmath172 with probability distributions @xmath192 realizable by bm",
    ".    given any target distribution @xmath112 , only the marginal distribution of bm over the visible units are specified , leaving the distributions on hidden units vary freely .",
    "let @xmath208 be the submanifold of @xmath172 with probability distributions @xmath209 that have the same marginal distribution as @xmath112 and the conditional distribution @xmath210 of hidden units is realised by the bm s activation functions with some parameter @xmath211 .",
    "then , the best bm is the one that minimizes the distance between @xmath171 and @xmath208 . due to the existence of hidden units ,",
    "the solution may not be unique . in this section ,",
    "the training process of bm is analysed in terms of manifold projection ( described in section [ sec : intro ] ) , following the framework of the learning rule proposed in @xcite . and",
    "we will show that the invariance in the learning of bm is the cif .",
    "+    the learning algorithm using iterative manifold projection is first proposed in @xcite and theoretically compared to em ( expectation and maximization ) algorithm in @xcite .",
    "the learning of rbm can be implemented by the following iterative projection process : let @xmath212 be the initial parameters of bm and @xmath213 be the corresponding stationary distribution .    for @xmath214 ,    1 .",
    "put @xmath215 2 .",
    "put @xmath216    where @xmath217 denotes the projection of @xmath218 to @xmath208 , and @xmath219 denotes the projection of @xmath209 to @xmath171 .",
    "the iteration ends when we reach the fixed points of the projections @xmath220 and @xmath221 , that is @xmath222 and @xmath223 .",
    "the iterative projection process is illustrated in figure [ fig : iterativelearning ] .",
    "the convergence property of this iterative algorithm is guaranteed using the following proposition :    [ prop : monotonicdivergence ] the monotonic relation holds in the iterative learning algorithm : @xmath224 \\geq d[q_{i+1},p_{i+1 } ] \\geq d[q_{i+2},p_{i+1}]\\ ] ] where the equality holds only for the fixed points of the projections .    in appendix",
    "[ appendix : monotonicdivergence ] .     and @xmath171 , we first choose an initial bm @xmath194 and then perform projections @xmath217 and @xmath219 iteratively , until the fixed points of the projections @xmath220 and @xmath221 are reached . with different initializations ,",
    "the iterative projection algorithm may end up with different local minima on @xmath208 and @xmath171 , respectively.,scaledwidth=40.0% ]    next two propositions show how the projection @xmath217 and @xmath219 are obtained .",
    "[ prop : hqtorbm ] given a distribution @xmath225 , the projection @xmath226 that gives the minimum divergence @xmath227 from @xmath208 to @xmath218 is the @xmath228 that satisfies @xmath229 .    in appendix [",
    "appendix : hptorbm ] .",
    "[ prop : projectionrbmcloseform ] given @xmath230 with mixed coordinates : @xmath231_{q}=(\\eta_{x_i}^1 , \\eta_{h_j}^1 , \\eta_{x_ix_j}^2 , \\eta_{x_ih_j}^2 , \\eta_{h_ih_j}^2,\\theta_{2+})$ ] , the coordinates of the learnt projection @xmath232 are uniquely given by the tailored mixed coordinates : @xmath233_{\\gamma_b(q)}=(\\eta_{x_i}^1 , \\eta_{h_j}^1 , \\eta_{x_ix_j}^2 , \\eta_{x_ih_j}^2 , \\eta_{h_ih_j}^2 , \\theta_{2+}=0)\\ ] ]    this proof comes in three parts :    1 .",
    "the projection @xmath219 of @xmath209 on @xmath171 is unique ; 2 .",
    "this unique projection @xmath219 can be achieved by minimizing the divergence @xmath234 $ ] using gradient descent method ; 3 .",
    "the mixed coordinates of @xmath219 is exactly the one given in equation ( [ eq : mixedcoordinatenewprojectionrbm ] ) .",
    "see appendix [ appendix : projectionrbmcloseform ] for the detailed proof .",
    "+    the iterative projection learning ( ip ) gives us an alternative way to investigate the learning process of bm .",
    "based on the cif principle in section [ sec : cif ] , we can see that the process of the projection @xmath235 can be derived from cif , i.e. , highly confident coordinates @xmath236 $ ] of @xmath237 are preserved while lowly confident coordinates @xmath238 $ ] are set to neutral value zero , given in equation [ eq : mixedcoordinatenewprojectionrbm ] .",
    "in summary , the essential parts of the real distribution that can be learnt by bm ( with and without hidden units ) are exactly the confident coordinates indicated by the cif principle .",
    "in this section , we will empirically investigate the sample - specific cif principle in density estimation tasks for boltzmann machines .",
    "more specifically , we aim to adaptively determine free parameters in bm , such that bm can trained be as close to the sample distribution as possible w.r.t . specific samples . for vbm",
    ", we will investigate how to use cif to modify the topology of vbm by reducing less confident connections among visible units with respect to given samples .",
    "for bm with hidden units , we extend the traditional restricted bm ( rbm ) by allowing connections among visible units , called vrbm",
    ". then we apply cif on vrbm to emphasis the learning on confident connections among visible units .      based on our theoretical analysis in section [ sec : cif ] and section [ sec : geometrybm ]",
    ", bm uses the most confident information ( i.e. , @xmath204 $ ] ) for approximating distributions in an expected sense .",
    "however , for the distribution with specific samples , _ can cif further recognize less - confident parameters and reduce them properly _ ?",
    "next , inspired by the general cif , we introduce an adaptive network design for vbm based on given samples , which could automatically balance the model complexity of bm and the sample size .",
    "the data constrains the state of knowledge about the unknown distribution .",
    "let @xmath112 denote the sampling distribution ( representing the data ) .",
    "in order to force the estimate of our probabilistic model ( denoted as @xmath239 ) to meet the data , we could incorporate the data into cif by recognizing the confidence of parameters @xmath22 in terms of @xmath112 .",
    "then , parametric reduction procedure can be further applied to modify the topology of vbm adaptively according to the data , as shown in algorithm [ alg : vbm ] and explained as in the following . note that this algorithm can also be used in bm with hidden units , such as vrbm ( see section [ sec : experimentrbm ] ) .",
    "samples @xmath240 ; significance level @xmath241 ; nodes @xmath242 ; edges @xmath243 ; set of confident edges @xmath244 @xmath245 estimate marginal distribution @xmath246 from samples * * * * parameterize to @xmath38-coordinates : @xmath68 $ ] * * * * @xmath247 ; \\eta_j \\leftarrow e_p[x_j]$ ] @xmath248 @xmath68 \\leftarrow \\{\\eta_i , \\eta_j , \\theta^{ij}\\}$ ] * * * * fisher information of @xmath249 in @xmath68 $ ] * * * * @xmath250 * * * * confidence of @xmath249 in @xmath68 $ ] * * * * @xmath251 * * * * hypothesis test : @xmath252 against @xmath253 * * * * @xmath254 * * * * reject null hypothesis : @xmath252 * * * * @xmath255 @xmath256    as a graphical model , the vbm comprises a set of vertices @xmath257 together with a set of connections @xmath258 . the confidence for each connection parameter @xmath259 can be assessed by the parameter selection criterion in cif , i.e. , the contribution to the fisher information distance . based on the theorem 1 in @xcite , @xmath259",
    "could be expressed as follows : @xmath260 where the relation hold for any conditions @xmath261 on the rest variables .",
    "however , it is often infeasible for us to calculate the exact value of @xmath259 because of data sparseness . to tackle this problem",
    ", we propose to approximate the value of @xmath259 by using the marginal distribution @xmath246 to avoid the effect of condition @xmath261 .",
    "let @xmath68_{ij}=(\\eta_{i } , \\eta_{j } , \\theta^{ij})$ ] be the mixed - coordinates for the marginal distribution @xmath246 of vbm .",
    "note that each @xmath249 corresponds to one connection @xmath259 .",
    "since @xmath249 is orthogonal to @xmath262 and @xmath263 , the fisher information distance between two distributions can be decomposed into two independent parts : the information distance contributed by @xmath264 and @xmath265 . for the purpose of parameter reduction",
    ", we consider the two close distributions @xmath23 and @xmath24 with coordinates @xmath266 and @xmath267 respectively .",
    "the confidence of @xmath249 , denoted as @xmath268 , can be estimated by its contribution to fisher information distance between @xmath23 and @xmath24 : @xmath269 where @xmath102 is the fisher information matrix in proposition [ prop : fishermatrix_mix ] and @xmath270 is the fisher information for @xmath249 . note that the second equality holds since @xmath249 is orthogonal to @xmath262 and @xmath263 .",
    "to decide whether the fisher information distance in the coordinate direction of @xmath249 is significant or negligible , we set up the hypothesis test for @xmath271 , i.e. , null hypothesis @xmath272 versus alternative @xmath273 .",
    "based on the analysis in @xcite , we have @xmath274 asymptotically , where the @xmath275 is chi - square distribution with degree of freedom 1 and @xmath276 is the sampling number . for example , for coordinate @xmath249 with @xmath277 , we have a @xmath278 confidence to reject the null hypothesis .",
    "that is , we can ensure that the fisher information distance in the direction of @xmath249 is significant with probability 95% .      in this section",
    ", we investigate the density estimation performance of cif - based model selection methods for vbm .",
    "three methods are compared :    1 .",
    "rand - cv : we perform random selection of vbm s connections and the best model is selected based on @xmath12-fold cross validation .",
    "cif - cv : connections are selected in descend order based on their confidences ( defined in equation [ eq : fisherdistance2 ] ) , constrained on the number of model free parameters . then , the best model is selected based on @xmath12-fold cross validation .",
    "cif - htest : the topology of vbm is determined by the adaptive algorithm described in algorithm [ alg : vbm ] .",
    "+    the artificial binary dataset is generated as follows : we first randomly select the target distribution @xmath112 , which is randomly chosen from the open probability simplex over the @xmath1 random variables using the jeffreys prior @xcite .",
    "then , the dataset with @xmath276 samples are generated from @xmath112 . for computation simplicity ,",
    "the artificial dataset is set to be 10-dimensional .",
    "the cd learning algorithm is used to train the vbms .",
    "the full - vbm , i.e. , the vbm with full connections are used as baseline .",
    "@xmath12 is set to 5 for cross validation .",
    "kl divergence is used to evaluate the goodness - of - fit of the vbm trained by various algorithms . for sample size @xmath276 ,",
    "we run 20 randomly generated distributions and report the averaged kl divergences .",
    "note that we focus on the case that the variable number is relatively small ( @xmath279 ) in order to analytically evaluate the kl divergence and give a detailed study on algorithms . changing",
    "the number of variables only offers a trivial influence for experimental results since we obtained qualitatively similar observations on various variable numbers ( not reported here ) . * results and summary : *            the averaged kl divergences between vbm and the underlying real distribution are shown in figure [ fig : result_vbm ] .",
    "we can see that all model selection methods could improve density estimation results of vbm , especially when the sample size is small ( n=100 to 1100 ) . with relatively large samples ,",
    "the effect of parameter reduction gradually becomes marginal .    comparing cif - cv with rand - cv ,",
    "the performances on relatively small sample size ( n=100 , 300 , 500 ) are similar ( see figure [ fig : result_vbm ] ) .",
    "this is because that the vbm reduced by cross validation is the trivial one with no connections . in figure",
    "[ fig : result_vbm_change ] ( first column ) , we illustrate how the kl divergence between vbm and real / sample distribution changes along with different model complexities of vbm .",
    "we can see that although the cif is worse than rand in terms of the kl divergence between vbm and the real distribution in most setups of model complexity , cif is better in estimating the sampling distribution .",
    "this is consistent with our previous theoretical insight that cif preserves the most confident parameters in vbm where the confidence is estimated based on sampling distribution .    as sample size increases ( n=700 , 900 , 1100 )",
    ", the cif - cv gradually outperforms rand - cv ( see figure [ fig : result_vbm ] ) .",
    "this could be explained by the cif principle .",
    "the cif preserves the most confident parameters of vbm with respect to sample distribution . as sample size increases",
    ", the sampling distribution grows closer to real distribution , which could benefit cif in the way that the kl divergence with both sample / real distributions can be simultaneous better than rand , as shown in figure [ fig : result_vbm_change ] ( second and third column ) .    with relatively large sample size ( n=1500 , 3000 ) ,",
    "the cif - cv and rand - cv have similar performance in terms of the kl divergence between vbm and the real distribution ( see figure [ fig : result_vbm ] ) .",
    "this is because complex model is preferred when the samples are sufficient and both cif - cv and rand - cv tend to select the trivial vbm with full connections .",
    "therefore , the difference between cif - cv and rand - cv becomes marginal .",
    "however , cif is still more powerful in describing sample / real distributions compare to rand , as shown in figure [ fig : result_vbm_change ] ( fourth column ) .",
    "for the two cif - based algorithm , cif - htest is worse than cif - cv when sample size is small and gradually achieves similar or better performance with cif - cv along with the increasing sample size .",
    "the main advantage of cif - htest , w.r.t .",
    ", cif - cv , is that there is no need for the time - consuming cross validation .    in summary ,",
    "the cif - based model selection ( e.g. , cif - cv and cif - htest ) indicates the balance between vbm s model complexity and the amount of information learnt from samples . for nontrivial cases of model selection ( trivial cases are the selected vbms with no connections or full connections ) , cif - based methods could reduce the kl divergence between vbm and real distributions without hurting the vbm too much in describing sample distribution , as compared to rand ( fig .",
    "[ fig : result_vbm_change ] ) .",
    "moreover , the cif - htest provides us an automatic way to adaptively select suitable vbm with respect to given samples .",
    "+        in this section , we empirically investigate how the cif - based model selection algorithm works on real - world datasets in the context of density estimation .",
    "in particular , we use the vbm to learn the underlying probability density over 100 terms of the _ 20 news groups _ binary dataset , with different model complexities ( changing the ratio of preserved fisher information distance ) .",
    "there are 18000 documents in 20 news groups in total , which is partitioned into two set : train set ( @xmath280 ) and test set ( @xmath281).the learning rate for cd is manually tuned in order to converge properly and all set to 0.01 . since it is infeasible to compute the kl devergence due to the high dimensionality , the averaged hamming distance between the samples in the dataset and those generated from the vbm is used to evaluate the goodness - of - fit of the vbm s trained by various algorithms .",
    "let @xmath282 denote the dataset of @xmath276 documents , where each document @xmath283 is a 100-dimensional binary vector . to evaluate a vbm with parameter @xmath284 , we first randomly generate @xmath276 samples from the stationary distribution @xmath285 , denoted as @xmath286 .",
    "then the averaged hamming distance @xmath287 is calculated as follows : @xmath288=\\frac{\\sum_{d_i } ( \\min_{v_j } ( ham[d_i , v_j])}{n}\\ ] ] where @xmath289 $ ] is the number of positions at which the corresponding values are different .",
    "three kinds of vbms are compared : the full vbm without parametric reduction ; the vbms of different model complexities using rand ; the vbms of different model complexities using cif .",
    "after training all vbms on the training dataset , we evaluate the trained vbm on the test dataset .",
    "the result is shown in figure [ fig:20newsgroup ] .",
    "we also mark the vbm that is automatically selected by cif - htest .",
    "we can see that the model selection ( cif and rand ) achieves significantly better performances than the full vbm for a wide range of @xmath290 , which is consistent with our observations with the experiments on artificial datasets when the samples is insufficient . and the best performance for cif outperforms that of rand .",
    "the performance of cif - htest ( ratio=0.3 ) is also shown in fig [ fig:20newsgroup ] , which is close to the optimal solution ( ratio=0.2 ) .",
    "the bm with hidden units is practically more interesting than vbm , since it has higher representation power .",
    "particularly , one of the fundamental problem in neural network research is the unsupervised representation learning @xcite , which attempts to characterize the underlying distribution through the discovery of a set of latent variables ( or features ) .",
    "the restricted bm ( rbm ) is one of the most widely used models for learning one level of feature extraction .",
    "then , in deep learning models@xcite , the representation learnt at one level is used as input for learning the next level . in this section",
    ", we will extend the rbm by allowing connections among visible units ( called vrbm , shown in figure [ fig : vrbm ] ) and further investigate the cif - based model selection algorithm ( see section [ sec : samplespecificcif ] ) empirically . note that in the model selection for vrbm , only parameters in @xmath163 ( connections within visible units ) are affected and all parameters in @xmath291 ( connections between hidden and visible units , visible / hidden self - connections ) are preserved , so as to maintain the structure of vrbm .",
    "+    for computational simplicity , the artificial dataset is of 10 dimensionality , and the number of hidden units in vrbm is set to 10 . for the model selection of vrbm , three methods are compared : cif - cv , rand - cv , cif - htest , which are the same model selection scheme for vbm in section [ sec : experimentsbm ] . the standard rbm is adopted as baseline .",
    "note that for model selection , we are actually adding connections among visible units to the standard rbm .",
    "the learning algorithms is cd .",
    "kl divergence is used to evaluate the goodness - of - fit of the vrbm s trained by various algorithms .",
    "+            the averaged kl divergences between vrbm and the underlying distribution are shown in figure [ fig : result_vrbm ] .",
    "for a small sample size ( n=100,300 ) , there is not so much need to increase the model complexity of bm and hence adding connections among visible units does not improve the density estimation results .",
    "while , as sample size increases ( from 500 to 3000 ) , model selection methods gradually outperforms standard rbm significantly by adding connections among visible units .    comparing cif - cv with rand - cv ,",
    "the performances on relatively small sample size ( n=100 , 300 ) are similar ( see figure [ fig : result_vrbm ] ) .",
    "this is because that the vrbm selected by cross validation is the trivial one with no connections added to rbm . in figure",
    "[ fig : result_vrbm_change ] ( first column ) , we illustrate how the kl divergence between vrbm and real / sample distribution changes along with different model complexities of vrbm .",
    "we can see that cif is better in estimating the sample distribution compared with rand ( first column , first row ) .",
    "similary with vbm , the performance for estimating the real distribution of cif becomes similar or worse than rand along with the increasing model complexity ( ratio@xmath292 ) due to overfitting ( first column , second row ) . as sample size increases ( n=500 to 1500 ) ,",
    "the cif - cv gradually outperforms rand - cv ( see figure [ fig : result_vrbm ] ) .",
    "this could be explained by the cif principle .",
    "the cif - cv preserves the most confident parameters of vrbm with respect to sample distribution . as sample size increases",
    ", the sampling distribution grows closer to real distribution , which could benefit cif in the way that the kl divergence with both sample / real distributions can be simultaneous better than rand for all model complexities , as shown in figure [ fig : result_vrbm_change ] ( second and third column ) .    with larger sample size ( n@xmath2933000 ) ,",
    "the cif - cv and rand - cv have similar performance in terms of the kl divergence between vrbm and the real distribution .",
    "this is because complex model is preferred when the samples are sufficient and both cif - cv and rand - cv tend to select the trivial vrbm with all connections between visible units being added to rbm .",
    "therefore , the difference between cif - cv and rand - cv becomes marginal .",
    "however , cif are still more powerful in describing sample distribution compare to rand for the vrbm with the same number of connections , as shown in figure [ fig : result_vrbm_change ] ( fourth column ) .",
    "for the two cif - based algorithm , cif - htest is worse than cif - cv when sample size is small and gradually outperforms cif - cv along with the increasing sample size .    in summary",
    ", the cif - based model selection could balance between vrbm s model complexity and the amount of information learnt from samples and could simultaneously reduce the kl divergence between vrbm and real / sample distributions .",
    "this indicates that the cif is also useful for bm with hidden units .",
    "in this paper , we study the parametric reduction and model selection problem of boltzmann machines from both theoretical and applicational perspectives . on the theoretical side , we propose the cif principle for the parametric reduction to maximally preserve the confident parameters and ruling out less confident ones . for binary multivariate distributions ,",
    "we theoretically show that cif could lead to an optimal submanifold in terms of equation [ eq : parametricreduction2 ] .",
    "furthermore , we illustrate that the boltzmann machines ( with or without hidden units ) can be derived from the general manifold based on cif principle . in future works , the cif could be the start of an information - oriented interpretation of deep learning models where bm is used as building blocks .",
    "for deep boltzmann machine ( dbm ) @xcite , several layers of rbm compose a deep architecture in order to achieve a representation at a sufficient abstraction level .",
    "the cif principle describes how the information flows in those representation transformations , as illustrated in figure [ fig : deeparchitecture ] .",
    "we propose that each layer of dbm determines a submanifold @xmath2 of @xmath0 , where @xmath2 could maximally preserve the highly confident information on parameters .",
    "then the whole dbm can be seen as the process of repeatedly applying cif in each layer , achieving the tradeoff between the abstractness of representation features and the intrinsic information confidence preserved on parameters .",
    "the more detailed analysis on deep models will be left as further works .     and hidden layers @xmath294 , @xmath295 and @xmath296 .",
    "the greedy layer - wise training of deep architecture is to maximally preserve the confident information layer by layer .",
    "note that the prohibition sign indicates that the fisher information on lowly confident coordinates is not preserved.,scaledwidth=50.0% ]    on the applicational side , we propose a sample - specific cif - based model selection scheme for bm , i.e. , cif - htest , that could automatically adapt to the given samples .",
    "it is studied in a series of density estimation experiments . in the further work",
    ", we plan to incorporate the cif - htest into deep learning models ( such as dbm ) to modify the network topology such that the most confident information in data can be well captured .",
    "by definition , we have : @xmath297 where @xmath63 is defined by equation ( [ eq : legedre ] ) .",
    "hence , we have : @xmath298 by differentiating @xmath93 , defined by equation ( [ eq : etacoordinate ] ) , with respect to @xmath91 , we have : @xmath299 p(x;\\theta ) } = \\eta_{i\\cup j}-\\eta_i \\eta_j \\nonumber\\end{aligned}\\ ] ] this completes the proof .      by definition",
    ", we have : @xmath300 where @xmath65 is defined by equation ( [ eq : legedre ] ) .",
    "hence , we have : @xmath301 based on equations ( [ eq : thetacoordinate ] ) and ( [ eq : etacoordinate ] ) , the @xmath90 and @xmath302 could be calculated by solving a linear equation of @xmath39 $ ] and @xmath52 $ ] , respectively .",
    "hence , we have : @xmath303 therefore , the partial derivation of @xmath90 with respect to @xmath94 is : @xmath304 this completes the proof .      the fisher information matrix of @xmath68 $ ] could be partitioned into four parts : @xmath305 .",
    "it can be verified that in the mixed coordinate , the @xmath37-coordinate of order @xmath12 is orthogonal to any @xmath36-coordinate less than @xmath12-order , implying the corresponding element of the fisher information matrix is zero ( @xmath306 ) @xcite .",
    "hence , @xmath102 is a block diagonal matrix .    according to the cramr ",
    "rao bound @xcite , a parameter ( or a pair of parameters ) has a unique asymptotically tight lower bound of the variance ( or covariance ) of the unbiased estimate , which is given by the corresponding element of the inverse of the fisher information matrix involving this parameter ( or this pair of parameters ) . recall that @xmath106 is the index set of the parameters shared by @xmath52 $ ] and @xmath68_l$ ] and that @xmath108 is the index set of the parameters shared by @xmath57 $ ] and @xmath68_l$ ] ; we have @xmath307 and @xmath308 , _",
    "i.e_. , @xmath309 since @xmath102 is a block tridiagonal matrix , the proposition follows .",
    "let @xmath156 be a @xmath14-ball surface centered at @xmath112 on manifold @xmath0 , _",
    "i.e_. , @xmath157 , where @xmath158 denotes the kullback ",
    "leibler divergence and @xmath14 is small .",
    "@xmath310 is the coordinates of @xmath112 .",
    "let @xmath311 be a neighbor of @xmath112 uniformly sampled on @xmath156 and @xmath312 be its corresponding coordinates .",
    "for a small @xmath14 , we can calculate the expected square fisher information distance between @xmath112 and @xmath311 as follows : @xmath313 where @xmath102 is the fisher information matrix at @xmath112 .    since fisher information matrix @xmath102 is both positive definite and symmetric",
    ", there exists a singular value decomposition @xmath314 where @xmath163 is an orthogonal matrix and @xmath315 is a diagonal matrix with diagonal entries equal to the eigenvalues of @xmath102 ( all @xmath316 ) , i.e. , @xmath317 .    applying the singular value decomposition into equation ( [ eq : expectdistance ] ) ,",
    "the expectation becomes : @xmath318 note that @xmath163 is an orthogonal matrix , and the transformation @xmath319 is a norm - preserving rotation .",
    "now , we need to show that among all tailored @xmath12-dimensional submanifolds of @xmath0 , @xmath68_{l_t}$ ] is the one that preserves maximum information distance .",
    "assume @xmath320 is the index of @xmath12 coordinates that we choose to form the tailored submanifold @xmath321 in the mixed - coordinates @xmath68 $ ] .",
    "first , according to the fundamental analytical properties of the surface of the hyper - ellipsoid , we will show that there exists a strict positive monotonicity between the expected information distance @xmath322 for @xmath321 and the sum of eigenvalues of the sub - matrix @xmath323 .    based on the definition of hyper - ellipsoid ,",
    "the @xmath14-ball surface @xmath156 is indeed the surface of a hyper - ellipsoid ( centered at @xmath112 ) determined by @xmath102 .",
    "let the eigenvalues of @xmath102 be @xmath324 .",
    "then , the surface integral on @xmath156 can be decomposed as follows : @xmath325 where @xmath326 and @xmath327 is the diagonal matrix with the main diagonal @xmath328 .",
    "we can show the monotonicity between the integration values and eigenvalues : @xmath329    the surface of the ellipsoid @xmath156 may be parameterized in several ways . in terms of cartesian coordinate systems ,",
    "the equation of @xmath156 is : @xmath330 where @xmath331 denotes the squares of the semi - axes and is determined by the reciprocals of the eigenvalues , i.e. , @xmath332 .",
    "consider the two - dimensional ellipsoid @xmath156 : @xmath333 then we can transform the cartesian coordinates to spherical coordinates as follows : @xmath334 where @xmath335 . to prove that @xmath336",
    ", we need to show that : @xmath337 .",
    "since the ellipsoid is symmetric , we only need to prove that : @xmath338\\ ] ]      next we will prove the above inequality based on the definition of riemann integral . the integral interval @xmath340 $ ] of @xmath37 can be partitioned into a finite sequence of subintervals , i.e. , @xmath341 $ ] , where @xmath342    let @xmath343 be the maximum length of subintervals .",
    "when @xmath344 approaches infinitesimal ( hence @xmath345 ) , the definite integral equals to the riemann integral , i.e. , the limit of the riemann sums .",
    "therefore , the inequality [ eq : definiteintegral ] can be transformed in to the comparison between two limitations : @xmath346 where the riemann sums are denoted by vector multiplications and @xmath347 and @xmath348 . since @xmath349 and @xmath350 , @xmath351 and @xmath352 can be seen as two vectors that share the same components while arranged in different orders .",
    "we can also see that there is a positive correlation between the components in @xmath351 and @xmath353 , while there is a negative correlation between the components in @xmath352 and @xmath353 .",
    "then , the inequality [ eq : definiteintegral ] is proved .",
    "therefore , the monotonicity [ eq : monotonicinequality ] holds for the two - dimensional ellipsoid .",
    "similarly , the above reimann integral analysis on the spherical coordinates can be extended to the @xmath1-dimensional ellipsoid .",
    "hence , the monotonicity [ eq : monotonicinequality ] holds for standard ellipsoids .",
    "thus , based on the monotonicity [ eq : monotonicinequality ] , the expected fisher information distance that can be preserved by a @xmath12-dimensional standard ellipsoid is monotonic with the sum of eigenvalues of the selected @xmath12 eigenvectors . to maximize the preserved information distance",
    ", we should choose the top-@xmath12 eigenvalues , i.e. , @xmath354 .    since @xmath102 is a block diagonal matrix , the eigenvalues of @xmath102 are the combined eigenvalues of its blocks .",
    "based on lemma [ prop : fishermatrix_mixdiagonal ] , the elements on the main diagonal of the sub - matrix @xmath261 are lower bounded by one and those of @xmath171 upper bounded by one . since the the sum of eigenvalues equals to the trace , the top-@xmath12 eigenvalues is exactly the eigenvalues of sub - matrix @xmath261 .",
    "thus , we have : @xmath355    now , let us consider the best selection of coordinates @xmath356 in @xmath68 $ ] .",
    "it is easy to see that the rotation operation @xmath163 in equation [ eq : expectdistance2 ] does not affect the integral value of the expected fisher information distance .",
    "therefore , based on equation [ eq : rotationinvairant ] , @xmath357 gives the maximum fisher information distance .",
    "this completes the proof .",
    "assume the fisher information matrix of @xmath57 $ ] to be : @xmath358 , which is partitioned based on @xmath106 and @xmath108 .",
    "based on proposition [ prop : fishermatrix_mix ] , we have @xmath359 . obviously , the diagonal elements of @xmath163 are all smaller than one . according to the succeeding lemma [ lemma : diagofmix ] , we can see that the diagonal elements of @xmath261 ( _ i.e_. , @xmath360 ) are greater than one .",
    "next , we need to show that the diagonal elements of @xmath171 are smaller than @xmath206 . using the schur complement of @xmath85 , the bottom - right block of @xmath361 , _",
    "i.e_. , @xmath362 , equals to @xmath363 .",
    "thus , the diagonal elements of b : @xmath364 .",
    "hence , we complete the proof .      since @xmath366 is positive definite , it is a gramian matrix of @xmath67 linearly independent vectors @xmath369 , _",
    "i.e_. , @xmath370 ( @xmath371 denotes the inner product ) .",
    "similarly , @xmath372 is the gramian matrix of @xmath67 linearly independent vectors @xmath373 and @xmath374 .",
    "it is easy to verify that @xmath375 .",
    "if @xmath367 , we can see that the norm @xmath376 .",
    "since @xmath377 , we have @xmath378 .",
    "hence , @xmath379 .",
    "let @xmath202 be the set of all probability distributions realized by sbm .",
    "@xcite proves that the mixed - coordinates of the resulting projection @xmath380 on @xmath202 is @xmath68_{p}=(\\eta^1_i , \\eta^2_{ij } , 0,\\dots,0)$ ] , given the 2-mixed - coordinates of @xmath112 .",
    "@xmath202 is equivalent to the submanifold tailored by cif , i.e. @xmath68_{2_t}$ ] .",
    "the corollary follows from proposition [ prop : geometricview ] .",
    "based on equation [ eq : thetaforbm ] , the coordinates @xmath238 $ ] for vbm is zero : @xmath381 .",
    "next , we show that the stationary distribution @xmath239 learnt by ml has the same @xmath382 $ ] with @xmath112 .      thus , based on equation [ eq : learnrulestochasticgradient ] ,",
    "the gradients for @xmath385 are as follows : @xmath386 where @xmath190 denotes the average using the sample data and @xmath191 denotes the average with respect to the stationary distribution @xmath239 .",
    "since vbm defines an @xmath387-flat submanifold @xmath202 of @xmath0 @xcite , then ml converges to the unique solution that gives the best approximation @xmath388 of @xmath112 .",
    "when ml converges , we have @xmath389 and hence @xmath390 .",
    "thus , we can see that ml converges to stationary distribution @xmath239 that preserves coordinates @xmath382 $ ] of @xmath112 .",
    "this completes the proof .      since @xmath391 and @xmath392 is the projection of @xmath393 , then @xmath394 \\geq d[q_{i+1},p_{i+1}]$ ] .",
    "similarly , @xmath395 and @xmath396 is the projection of @xmath397 , thus @xmath398 \\geq d[q_{i+2},p_{i+1}]$ ] .",
    "this completes the proof .",
    "based on the definition of divergence , the following relation holds : @xmath399 = d[q(x)q(h|x),p(x)p(h|x ) ] \\nonumber \\\\        & = & e_{q(x , h)}[log\\frac{q(x)}{p(x)}+log\\frac{q(h|x)}{p(h|x ) } ] \\nonumber \\\\        & = & d[q(x),p(x ) ] + e_{q(x)}[d[q(h|x),p(h|x ) ] ] \\nonumber\\end{aligned}\\ ] ] where @xmath400 $ ] and @xmath401 $ ] are the expectations taken over @xmath209 and @xmath112 respectively .    therefore , the minimum divergence between @xmath218 and @xmath208 is given as : @xmath402 \\nonumber \\\\        & = & \\min_{\\xi_q}\\ { d[q(x),p(x ) ] + e_q(x)[d[q(h|x;\\xi_q),p(h|x;\\xi_p)]]\\ } \\nonumber \\\\        & = & d[q(x),p(x ) ] + \\min_{\\xi_q}\\ { e_{q(x)}[d[q(h|x;\\xi_q),p(h|x;\\xi_p)]]\\ } \\nonumber \\\\        & = & d[q(x),p(x ) ] \\nonumber\\end{aligned}\\ ] ] in the last equality , the expected divergence between @xmath403 and @xmath404 vanishes if and only if @xmath405 .",
    "this completes the proof .. ]      first , we prove the uniqueness of the projection @xmath219 . from the @xmath57 $ ] of bm in equation ( [ eq : thetaforbm ] ) ,",
    "@xmath171 is an @xmath387-flat smooth submanifold of @xmath172 .",
    "thus the projection is unique .",
    "second , in order to find the @xmath225 with parameter @xmath406 that minimizes the divergence between @xmath407 and @xmath171 , the gradient descent method iteratively adjusts @xmath406 in the negative gradient direction that the divergence @xmath408 $ ] decreases fastest : @xmath409}{\\partial \\xi_p } \\nonumber\\ ] ] where @xmath408 $ ] is treated as a function of bm s parameters @xmath406 and @xmath410 is the learning rate .",
    "as shown in @xcite , the gradient descent method converges to the minimum of the divergence with proper choices of @xmath410 , and hence achieves the projection point @xmath219 .",
    "last , we show that the mixed coordinates @xmath231_{\\gamma_b(q)}$ ] in equation ( [ eq : mixedcoordinatenewprojectionrbm ] ) is exactly the convergence point of the ml learning for bm . for distributions on the manifold @xmath172 ,",
    "the states of all hidden units is also visible and hence the bm with hidden units is equivalent to vbm by treating hidden units as visible ones .",
    "based on proposition [ prop : sbmmlcloseform ] , ml converges to the projection point @xmath219 with a stationary distribution @xmath218 that preserves coordinates @xmath236 $ ] of @xmath411 .",
    "this completes the proof .",
    "p.  vincent , h.  larochelle , i.  lajoie , y.  bengio , and p .- a .",
    "manzagol , `` stacked denoising autoencoders : learning useful representations in a deep network with a local denoising criterion , '' _ j. mach .",
    "_ , vol .  11 , pp .",
    "33713408 , 2010 .",
    "d.  erhan , y.  bengio , a.  courville , p .- a .",
    "manzagol , p.  vincent , and s.  bengio , `` why does unsupervised pre - training help deep learning ? '' _ journal of machine learning research _ , vol .  11 , pp . 625660 , 2010 .",
    "h.  jeffreys , `` an invariant form for the prior probability in estimation problems , '' _ proceedings of the royal society of london .",
    "series a. mathematical and physical sciences _ , vol .",
    "1007 , pp . 453461 , 1946 .",
    "f.  x. albizuri , a.  danjou , m.  grana , j.  torrealdea , and m.  c. hernandez , `` the high - order boltzmann machine : learned distribution and topology , '' _ neural networks , ieee transactions on _ , vol .  6 , no .  3 , pp .",
    "767770 , 1995 ."
  ],
  "abstract_text": [
    "<S> typical dimensionality reduction ( dr ) methods are often data - oriented , focusing on directly reducing the number of random variables ( features ) while retaining the maximal variations in the high - dimensional data . in unsupervised situations , </S>",
    "<S> one of the main limitations of these methods lies in their dependency on the scale of data features . </S>",
    "<S> this paper aims to address the problem from a new perspective and considers model - oriented dimensionality reduction in parameter spaces of binary multivariate distributions .    </S>",
    "<S> specifically , we propose a general parameter reduction criterion , called confident - information - first ( cif ) principle , to maximally preserve confident parameters and rule out less confident parameters . </S>",
    "<S> formally , the confidence of each parameter can be assessed by its contribution to the expected fisher information distance within the geometric manifold over the neighbourhood of the underlying real distribution .    </S>",
    "<S> we then revisit boltzmann machines ( bm ) from a model selection perspective and theoretically show that both the fully visible bm ( vbm ) and the bm with hidden units can be derived from the general binary multivariate distribution using the cif principle . </S>",
    "<S> this can help us uncover and formalize the essential parts of the target density that bm aims to capture and the non - essential parts that bm should discard . </S>",
    "<S> guided by the theoretical analysis , we develop a sample - specific cif for model selection of bm that is adaptive to the observed samples . </S>",
    "<S> the method is studied in a series of density estimation experiments and has been shown effective in terms of the estimate accuracy .    </S>",
    "<S> information geometry , boltzmann machine , parametric reduction , fisher information </S>"
  ]
}