{
  "article_text": [
    "the luminosity function ( lf ) has been an important tool for understanding the evolution of galaxies and quasars , as it provides a census of the galaxy and quasar populations over cosmic time .",
    "quasar luminosity functions have been estimated for optical surveys ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , x - ray surveys ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , infrared surveys ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , radio surveys ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) , and emission lines @xcite .",
    "in addition , luminosity functions across different bands have been combined to form an estimate of the bolometric luminosity function @xcite .",
    "besides providing an important constraint on models of quasar evolution and supermassive black hole growth ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) , studies of the lf have found evidence for ` cosmic downsizing ' , where the space density of more luminous quasars peaks at higher redshift .",
    "attempts to map the growth of supermassive black holes start from the local supermassive black hole distribution , and employ the argument of @xcite , using the quasar luminosity function as a constraint on the black hole mass distribution .",
    "these studies have found evidence that the highest mass black holes grow first ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , suggesting that this cosmic downsizing is the result of an anti - hierarchical growth of supermassive black holes .    similarly , galaxy luminosity functions have been estimated in the optical ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "? * ; * ? ? ?",
    "* ) , x - ray ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) , infrared ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) , ultraviolet ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) , radio ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) , for galaxies in clusters ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) , and for galaxies in voids @xcite .",
    "the galaxy luminosity function probes several aspects of the galaxy population ; namely ( a ) the evolution of stellar populations and star formation histories ( e.g. , * ? ? ?",
    "* ) , ( b ) the local supermassive black hole mass distribution ( e.g , * ? ? ?",
    "* ; * ? ? ?",
    "* ) via the magorrian relationship @xcite , ( c ) the dependence of galaxy properties on environment ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) , and ( d ) places constraints on models of structure formation and galaxy evolution ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    ".    given the importance of the luminosity function as an observational constraint on models of quasar and galaxy evolution , it is essential that a statistically accurate approach be employed when estimating these quantities .",
    "however , the existence of complicated selection functions hinders this , and , as a result , a variety of methods have been used to accurately account for the selection function when estimating the lf .",
    "these include various binning methods ( e.g. , * ? ? ?",
    "? * ; * ? ? ?",
    "* ) , maximum - likelihood fitting ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) , and a powerful semi - parametric approach @xcite .",
    "in addition , there have been a variety of methods proposed for estimating the cumulative distribution function of the lf ( e.g. , * ? ? ? * ; * ? ? ? * ; * ? ? ? * ) .",
    "each of these statistical methods has advantages and disadvantages .",
    "statistical inference based on the binning procedures can not be extended beyond the support of the selection function , and the cumulative distribution function methods typically assume that luminosity and redshift are statistically independent .",
    "furthermore , one is faced with the arbitrary choice of bin size .",
    "the maximum - likelihood approach typically assumes a restrictive and somewhat _ ad hoc _ parametric form , and has not been used to give an estimate of the lf normalization ; instead , for example , the lf normalization is often chosen to make the expected number of sources detected in one s survey equal to the actual number of sources detected .",
    "in addition , confidence intervals based on the errors derived from the various procedures are typically derived by assuming that the uncertainties on the lf parameters have a gaussian distribution . while this is valid as the sample size approaches infinity",
    ", it is not necessarily a good approximation for finite sample sizes .",
    "this is particularly problematic if one is employing the best fit results to extrapolating the luminosity function beyond the bounds of the selection function .",
    "it is unclear if the probability distribution of the uncertainty in the estimated luminosity function below the flux limit is even asymptotically normal .",
    "motivated by these issues , we have developed a bayesian method for estimating the luminosity function .",
    "we derive the likelihood function of the lf by relating the observed data to the true lf , assuming some parametric form , and derive the posterior probability distribution of the lf parameters , given the observed data . while the likelihood function and posterior are valid for any parametric form , we focus on a flexible parametric model where the lf is modeled as a weighted sum of gaussian functions .",
    "this is a type of ` non - parametric ' approach , where the basic idea is that the individual gaussian functions do not have any physical meaning , but that given enough gaussian functions one can obtain a suitably accurate approximation to the true lf ; a similar approach has been taken by @xcite for estimating galaxy lfs , and by @xcite within the context of linear regression with measurement error .",
    "modeling the lf as a mixture of gaussian functions avoids the problem of choosing a particular parametric form , especially in the absence of any guidance from astrophysical theory .",
    "the mixture of gaussians model has been studied from a bayesian perspective by numerous authors ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "in addition , we describe a markov chain monte carlo ( mcmc ) algorithm for obtaining random draws from the posterior distribution .",
    "these random draws allow one to estimate the posterior distribution for the lf , as well as any quantities derived from it .",
    "the mcmc method therefore allows a straight - forward method of calculating uncertainties on any quantity derived from the lf , such as the redshift where the space density of quasars or galaxies peaks ; this has proven to be a challenge for other statistical methods developed for lf estimation . because the bayesian approach is valid for any sample size , one is therefore able to place reliable constraints on the lf and related quantities even below the survey flux limits .",
    "because of the diversity and mathematical complexity of some parts of this paper , we summarize the main results here .",
    "we do this so that the reader who is only interested in specific aspects of this paper can conveniently consult the sections of interest .    * in   [ s - lik ] we derive the general form of the likelihood function for luminosity function estimation .",
    "we show that the commonly used likelihood function based on the poisson distribution is incorrect , and that the correct form of the likelihood function is derived from the binomial distribution .",
    "however , because the poisson distribution is the limit of the binomial distribution as the probability of including a source in a survey approaches zero , the maximum - likelihood estimates derived from the two distribution give nearly identical results so long as a survey s detection probability is small .",
    "the reader who is interested in using the correct form of the likelihood function of the lf should consult this section . * in ",
    "[ s - posterior ] we describe a bayesian approach to luminosity function estimation .",
    "we build on the likelihood function derived in   [ s - lik ] to derive the probability distribution of the luminosity function , given the observed data ( i.e. , the posterior distribution ) .",
    "we use a simple example based on a schechter function to illustrate the bayesian approach , and compare it with the maximum - likelihood approach . for this example , we find that confidence intervals derived from the posterior distribution are valid , while confidence intervals derived from bootstrapping the maximum - likelihood estimate can be too small .",
    "the reader who is interested in a bayesian approach to luminosity function estimation , and how it compares with maximum - likelihood , should consult this section . * in ",
    "[ s - smodel ] we develop a mixture of gaussian functions model for the luminosity function , deriving the likelihood function and posterior distribution for the model . under this model ,",
    "the lf is modeled as a weighted sum of gaussian functions .",
    "this model has the advantage that given a suitably large enough number of gaussian functions , it is flexible enough to give an accurate estimate of any smooth and continuous lf .",
    "this allows the model to adapt to the true lf , thus minimizing the bias that can result when assuming a parametric form of the lf .",
    "this is particularly useful when extrapolating beyond the flux limits of a survey , where bias caused by parametric misspecification can be a significant concern .",
    "the reader who are interested in employing the mixture of gaussian functions model should consult this section . * because of the large number of parameters often associated with luminosity function estimation , bayesian inference is most easily performed by obtaining random draws of the lf from the posterior distribution . in ",
    "[ s - mha ] we describe the metropolis - hastings algorithm ( mha ) for obtaining random draws of the lf from the posterior distribution . as an example",
    ", we describe a mha for obtaining random draws of the parameters for a schechter function from the posterior distribution .",
    "then , we describe a more complex mha for obtaining random draws of the parameters for the mixture of gaussian functions model .",
    "the reader who is interested in the computational aspects of ` fitting ' the mixture of gaussian functions model , or who is interested in the computational aspects of bayesian inference for the lf , should consult this section .",
    "a computer routine for performing the metropolis - hastings algorithm for the mixture of gaussian functions model is available on request from b. kelly . * in ",
    "[ s - sim ] we use simulation to illustrate the effectiveness of our bayesian gaussian mixture model for luminosity function estimation . we construct a simulated data set similar to the sloan digital sky survey dr3 quasar catalog @xcite .",
    "we then use our mixture of gaussian functions model to recover the true lf and show that our mixture model is able to place reliable constraints on the lf .",
    "we also illustrate how to use the mha output to constrain any quantity derived from the lf , and how to use the mha output to assess the quality of the fit .",
    "the reader who is interested in assessing the effectiveness of our statistical approach , or who is interested in using the mha output for statistical inference on the lf , should consult this section .",
    "we adopt a cosmology based on the the wmap best - fit parameters ( @xmath0 , * ? ? ?",
    "we use the common statistical notation that an estimate of a quantity is denoted by placing a ` hat ' above it ; e.g. , @xmath1 is an estimate of the true value of the parameter @xmath2 .",
    "the parameter @xmath2 may be scalar or multivalued .",
    "we denote a normal density ( i.e. , a gaussian distribution ) with mean @xmath3 and variance @xmath4 as @xmath5 , and we denote as @xmath6 a multivariate normal density with @xmath7-element mean vector @xmath3 and @xmath8 covariance matrix @xmath9 .",
    "if we want to explicitly identify the argument of the gaussian function , we use the notation @xmath10 , which should be understood to be a gaussian with mean @xmath3 and variance @xmath4 as a function of @xmath11 .",
    "we will often use the common statistical notation where `` @xmath12 '' means `` is drawn from '' or `` is distributed as '' .",
    "this should not be confused with the common usage of implying `` similar to '' .",
    "for example , @xmath13 states that @xmath11 is drawn from a normal density with mean @xmath3 and variance @xmath4 , whereas @xmath14 states that the value of @xmath11 is similar to one .    in this work , the maximum - likelihood estimate of the luminosity function refers to an estimate of the lf obtained by maximizing the likelihood function of the unbinned data .",
    "therefore , the maximum - likelihood estimate does not refer to an estimate obtained by maximizing the likelihood function of binned data , such as fitting the results obtained from the @xmath15 technique .",
    "the luminosity function , denoted as @xmath16 , is the number of sources per comoving volume @xmath17 with luminosities in the range @xmath18 .",
    "the luminosity function is related to the probability density of @xmath19 by @xmath20 where @xmath21 is the total number of sources in the observable universe , and is given by the integral of @xmath22 over @xmath23 and @xmath17 .",
    "note that @xmath24 is the probability of finding a source in the range @xmath18 and @xmath25 .",
    "equation ( [ eq - phiconvert ] ) separates the lf into its shape , given by @xmath26 , and its normalization , given by @xmath21 .",
    "once we have an estimate of @xmath26 , we can easily convert this to an estimate of @xmath27 using equation ( [ eq - phiconvert ] ) .",
    "in general , it is easier to work with the probability distribution of @xmath23 and @xmath28 , instead of directly with the lf , because @xmath26 is more directly related to the likelihood function .",
    "if we assume a parametric form for @xmath27 , with parameters @xmath2 , we can derive the likelihood function for the observed data .",
    "the likelihood function is the probability of observing one s data , given the assumed model .",
    "the presence of flux limits and various other selection effects can make this difficult , as the observed data likelihood function is not simply given by equation ( [ eq - phiconvert ] ) . in this case , the set of luminosities and redshifts observed by a survey gives a biased estimate of the true underlying distribution , since only those sources with @xmath23 above the flux limit at a given @xmath28 are detected . in order to derive the observed data likelihood function , it is necessary to take the survey s selection method into account .",
    "this is done by first deriving the joint likelihood function of both the observed and unobserved data , and then integrating out the unobserved data .",
    "because the data points are independent , the likelihood function for all @xmath21 sources in the universe is @xmath29 in reality , we do not know the luminosities and redshifts for all @xmath21 sources , nor do we know the value of @xmath21 , as our survey only covers a fraction of the sky and is subject to a selection function . as a result",
    ", our survey only contains @xmath30 sources . because of this",
    ", the selection process must also be included in the probability model , and the total number of sources , @xmath21 , is an additional parameter that needs to be estimated .",
    "we can incorporate the sample selection into the likelihood function by including the random detection of sources .",
    "we introduce an @xmath21-element indicator vector @xmath31 that takes on the values @xmath32 if the @xmath33 source is included in our survey and @xmath34 otherwise .",
    "note that @xmath31 is a vector of size @xmath21 containing only ones and zeros . in this case",
    ", the selection function is the probability of including a source given @xmath23 and @xmath28 , @xmath35 .",
    "the complete data likelihood is then the probability that all objects of interest in the universe ( e.g. , all quasars ) have luminosities @xmath36 and redshifts @xmath37 , and that the selection vector @xmath38 has the values @xmath39 , given our assumed luminosity function : @xmath40 here , @xmath41 is the binomial coefficient , @xmath42 denotes the set of @xmath30 included sources , and @xmath43 denotes the set of @xmath44 missing sources .",
    "the number of sources detected in a survey is random , and therefore the binomial coefficient is necessary in normalizing the likelihood function , as it gives the number of possible ways to select a subset of @xmath30 sources from a set of @xmath21 total sources .    because we are interested in the probability of the observed data , given our assumed model , the complete data likelihood function is of little use by itself .",
    "however , we can integrate equation ( [ eq - complik1 ] ) over the missing data to obtain the observed data likelihood function .",
    "this is because the marginal probability distribution of the observed data is obtained by integrating the joint probability distribution of the observed and the missing data over the missing data : @xmath45^{n - n }       \\prod_{i \\in { \\cal a}_{obs } } p(l_i , z_i | \\theta ) ,      \\label{eq - obslik1 }    \\end{aligned}\\ ] ] where the probability that the survey misses a source , given the parameters @xmath2 , is @xmath46 here , we have introduced the notation that @xmath47 and @xmath48 denote the set of values of @xmath23 and @xmath28 for those sources included in one s survey , and we have omitted terms that do not depend on @xmath2 or @xmath21 from equation ( [ eq - obslik1 ] ) . equation ( [ eq - obslik1 ] ) is the observed data likelihood function , given an assumed luminosity function ( eq.[[eq - phiconvert ] ] ) .",
    "qualitatively , the observed data likelihood function is the probability of observing the set of @xmath30 luminosities @xmath49 and redshifts @xmath50 given the assumed luminosity function parameterized by @xmath2 , multiplied by the probability of not detecting @xmath44 sources given @xmath2 , multiplied by the number of ways of selecting a subset of @xmath30 sources from a set of @xmath21 total sources .",
    "the observed data likelihood function can be used to calculate a maximum likelihood estimate of the luminosity function , or combined with a prior distribution to perform bayesian inference .",
    "the observed data likelihood given by equation ( [ eq - obslik1 ] ) differs from that commonly used in the luminosity function literature . instead",
    ", a likelihood based on the poisson distribution is often used . @xcite",
    "give the following equation for the log - likelihood function based on the poisson distribution : @xmath51 inserting equation ( [ eq - phiconvert ] ) for @xmath52 , the log - likelihood based on the poisson likelihood becomes @xmath53 where , @xmath54 , and @xmath55 is given by equation ( [ eq - selprob ] ) .",
    "in contrast , the log - likelihood we have derived based on the binomial distribution is the logarithm of equation ( [ eq - obslik1 ] ) : @xmath56 the likelihood functions implied by equations ( [ eq - poislik2 ] ) and ( [ eq - loglik ] ) are functions of @xmath21 , and thus the likelihoods may also be maximized with respect to the lf normalization .",
    "this is contrary to what is often claimed in the literature , where the lf normalization is typically chosen to make the expected number of sources observed in one s survey equal to the actual number observed .",
    "the binomial likelihood , given by equation ( [ eq - obslik1 ] ) , contains the term @xmath57 , resulting from the fact that the total number of sources included in a survey , @xmath30 , follows a binomial distribution .",
    "for example , suppose one performed a survey over one quarter of the sky with no flux limit .",
    "assuming that sources are uniformly distributed on the sky , the probability of including a source for this survey is simply @xmath58 .",
    "if there are @xmath21 total sources in the universe , the total number of sources that one would find within the survey area follows a binomial distribution with @xmath21 ` trials ' and probability of ` success ' @xmath59 .",
    "however , the poisson likelihood is derived by noting that the number of sources detected in some small bin in @xmath19 follows a poisson distribution . since the sum of a set of poisson distributed random variables also follows a poisson distribution , this implies that the total number of sources detected in one s survey , @xmath30 , follows a poisson distribution .",
    "however , @xmath30 actually follows a binomial distribution , and thus the observed data likelihood function is not given by the poisson distribution .",
    "the source of this error is largely the result of approximating the number of sources in a bin as following a poisson distribution , when in reality it follows a binomial distribution .",
    "although the poisson likelihood function for the lf is incorrect , the previous discussion should not be taken as a claim that previous work based on the poisson likelihood function is incorrect . when the number of sources included in one s sample is much smaller than the total number of sources in the universe , the binomial distribution is well approximated by the poisson distribution . therefore",
    ", if the survey only covers a small fraction of the sky , or if the flux limit is shallow enough such that @xmath60 , then the poisson likelihood function should provide an accurate approximation to the true binomial likelihood function .",
    "when this is true , statistical inference based on the poisson likelihood should only exhibit negligible error , so long as there are enough sources in one s survey to obtain an accurate estimate of the lf normalization . in ",
    "[ s - schechter2 ] we use simulate to compare results obtained from the two likelihood functions , and to compare the maximum - likelihood approach to the bayesian approach .",
    "we can combine the likelihood function for the lf with a prior probability distribution on the lf parameters to perform bayesian inference on the lf .",
    "the result is the posterior probability distribution of the lf parameters , i.e. , the probability distribution of the lf parameters given our observed data .",
    "this is in contrast to the maximum likelihood approach , where the maximum likelihood approach seeks to relate the observed value of the mle to the true parameter value through an estimate of the sampling distribution of the mle . in appendix   [ a - mle_vs_bayes ] we give a more thorough introduction to the difference between the maximum likelihood and bayesian approaches .",
    "the posterior probability distribution of the model parameters is related to the likelihood function and the prior probability distribution as @xmath61 where @xmath62 is the prior on @xmath63 , and @xmath64 is is the observed data likelihood function , given by equation ( [ eq - obslik1 ] ) .",
    "the posterior distribution is the probability distribution of @xmath2 and @xmath21 , given the observed data , @xmath47 and @xmath48 . because the luminosity function depends on the parameters @xmath2 and @xmath21 , the posterior distribution of @xmath2 and @xmath21",
    "can be used to obtain the probability distribution of @xmath27 , given our observed set of luminosities and redshifts .",
    "it is of use to decompose the posterior as @xmath65 ; here we have dropped the explicit conditioning on @xmath31 .",
    "this decomposition separates the posterior into the conditional posterior of the lf normalization at a given @xmath2 , @xmath66 , from the marginal posterior of the lf shape , @xmath67 . in this work",
    "we assume that @xmath21 and @xmath2 are independent in their prior distribution , @xmath68 , and that the prior on @xmath21 is uniform over @xmath69 .",
    "a uniform prior on @xmath69 corresponds to a prior distribution on @xmath21 of @xmath70 , as @xmath71 . under this prior",
    ", one can show that the marginal posterior probability distribution of @xmath2 is @xmath72^{-n }       \\prod_{i \\in { \\cal a}_{obs } } p(l_i , z_i|\\theta ) ,      \\label{eq - thetapost}\\ ] ] where @xmath54 .",
    "we derive equation ( [ eq - thetapost ] ) in appendix   [ a - margpost_deriv ] ( see also * ? ? ?",
    ". under the assumption of a uniform prior on @xmath2 , equation ( [ eq - thetapost ] ) is equivalent to equation ( 22 ) in @xcite , who use a different derivation to arrive at a similar result .    under the prior @xmath73 ,",
    "the conditional posterior distribution of @xmath21 at a given @xmath2 is a negative binomial distribution with parameters @xmath30 and @xmath74 .",
    "the negative binomial distribution gives the probability that the total number of sources in the universe is equal to @xmath21 , given that we have observed @xmath30 sources in our sample with probability of inclusion @xmath74 : @xmath75^n       \\left[p(i=0|\\theta)\\right]^{n - n}.      \\label{eq - npost}\\ ] ] here , @xmath55 is given by equation ( [ eq - selprob ] ) and @xmath54 .",
    "further description of the negative binomial distribution is given in   [ a - densities ] .",
    "the complete joint posterior distribution of @xmath2 and @xmath21 is then the product of equations ( [ eq - thetapost ] ) and ( [ eq - npost ] ) , @xmath76 .    because it is common to fit a luminosity function with a large number of parameters , it is computationally intractable to directly calculate the posterior distribution from equations ( [ eq - thetapost ] ) and ( [ eq - npost ] ) . in particular ,",
    "the number of grid points needed to calculate the posterior will scale exponentially with the number of parameters .",
    "similarly , the number of integrals needed to calculate the marginal posterior probability distribution of a single parameters will also increase exponentially with the number of parameters . instead",
    ", bayesian inference is most easily performed by simulating random draws of @xmath21 and @xmath2 from their posterior probability distribution .",
    "based on the decomposition @xmath77 , we can obtain random draws of @xmath78 from the posterior by first drawing values of @xmath2 from equation ( [ eq - thetapost ] ) .",
    "then , for each draw of @xmath2 , we draw a value of @xmath21 from the negative binomial distribution .",
    "the values of @xmath21 and @xmath2 can then be used to compute the values of luminosity function via equation ( [ eq - phiconvert ] ) .",
    "the values of the lf computed from the random draws of @xmath21 and @xmath2 are then treated as a random draw from the probability distribution of the lf , given the observed data .",
    "these random draws can be used to estimate posterior means and variances , confidence intervals , and histogram estimates of the marginal distributions .",
    "random draws for @xmath2 may be obtained via markov chain monte carlo ( mcmc ) methods , described in ",
    "[ s - mha ] , and we describe in   [ a - densities ] how to obtain random draws from the negative binomial distribution . in ",
    "[ s - simmcmc ] we give more details on using random draws from the posterior to perform statistical inference on the lf .      before moving to more advanced models , we illustrate the bayesian approach by applying it to a simulated set of luminosities drawn from a schechter function .",
    "we do this to give an example of how to calculate the posterior distribution , how to obtain random draws from the posterior and use these random draws to draw scientific conclusions based on the data , and to compare the bayesian approach with the maximum - likelihood approach ( see   [ s - schechter2 ] ) .",
    "the schechter luminosity function is : @xmath79 for simplicity , we ignore a @xmath28 dependence .",
    "the schechter function is equivalent to a gamma distribution with shape parameter @xmath80 , and scale parameter @xmath81 .",
    "note that @xmath82 and @xmath83 ; otherwise the integral of equation ( [ eq - schechter ] ) may be negative or become infinite . for our simulation , we randomly draw @xmath84 galaxy luminosities from equation ( [ eq - schechter ] ) using a value of @xmath85 and @xmath86 .    to illustrate how the results depend on the detection limit",
    ", we placed two different detection limits on our simulated survey .",
    "the first limit was at @xmath87 and the second was at @xmath88 .",
    "we used a hard detection limit , where all sources above @xmath89 were detected and all sources below @xmath89 were not : @xmath90 and @xmath91 .",
    "note that the first detection limit lies below @xmath81 , while the second detection limit lies above @xmath81 .",
    "we were able to detect @xmath92 sources for @xmath93 and @xmath94 sources for @xmath95    the marginal posterior distribution of @xmath96 and @xmath81 can be calculated by inserting into equation ( [ eq - thetapost ] ) an assumed prior probability distribution , @xmath97 , and the likelihood function , @xmath98 . because we are ignoring redshift in our example ,",
    "the likelihood function is simply @xmath99 . in this example , we assume a uniform prior on @xmath100 and @xmath96 , and therefore @xmath101 . from equations",
    "( [ eq - thetapost ] ) and ( [ eq - schechter ] ) , the marginal posterior distribution of the parameters is @xmath102^{-n }       \\prod_{i=1}^n \\frac{1}{l^ * \\gamma(\\alpha + 1 ) } \\left ( \\frac{l_i}{l^ * } \\right)^{\\alpha }      e^{-l_i / l^ * } ,      \\label{eq - schechpost}\\ ] ] where the survey detection probability is @xmath103 the conditional posterior distribution of @xmath21 at a given @xmath2 is given by inserting in equation ( [ eq - schechdet ] ) into equation ( [ eq - npost ] ) , and the joint posterior of @xmath104 and @xmath21 is obtained by multiplying equation ( [ eq - schechpost ] ) by equation ( [ eq - npost ] ) .",
    "we perform statistical inference on the lf by obtaining random draws from the posterior distribution . in order to calculate the marginal posterior distributions , @xmath105 and @xmath106",
    ", we would need to numerically integrate the posterior distribution over the other two parameters .",
    "for example , in order to calculate the marginal posterior of @xmath96 , @xmath107 , we would need to integrate @xmath108 over @xmath81 and @xmath21 on a grid of values for @xmath96 . while feasible for the simple 3-dimensional problem illustrated here , it is faster to simply obtain a random draw of @xmath109 and @xmath21 from the posterior , and then use a histogram to estimate @xmath110 .",
    "further details are given in ",
    "[ s - simmcmc ] on performing bayesian inference using random draws from the posterior .",
    "we used the metropolis - hastings algorithm described in   [ s - mha_schechter ] to obtain a random draw of @xmath111 , and @xmath21 from the posterior probability distribution .",
    "the result was a set of @xmath112 random draws from the posterior probability distribution of @xmath104 and @xmath21 . in figure [ f - schechpost ]",
    "we show the estimated posterior distribution of @xmath113 , and @xmath21 for both detection limits . while @xmath81 is fairly well constrained for both detection limits , the uncertainties on @xmath96 and @xmath21 are highly sensitive to whether the detection limit lies above or below @xmath81 .",
    "in addition , the uncertainties on these parameters are not gaussian , as is often assumed for the mle .     and @xmath81 , for the simulated sample described in   [ s - schechter ] .",
    "the top three panels show the posterior when the luminosity limit of the survey is @xmath114 $ ] , and the bottom three panels show the posterior distribution when the luminosity limit of the survey is @xmath115 $ ] .",
    "the vertical lines mark the true values of the parameters , @xmath116 and @xmath117 $ ] .",
    "the uncertainty on the parameters increases considerably when @xmath118 , reflecting the fact that the bright end of the schechter lf contains little information on @xmath96 or @xmath21 .",
    "[ f - schechpost ] ]    .",
    "the random draws of @xmath113 , and @xmath21 can also be used to place constraints on the lf .",
    "this is done by computing equation ( [ eq - schechter ] ) for each of the random draws of @xmath104 and @xmath21 , and plotting the regions that contain , say , @xmath119 of the probability . in figure [ f - schechbounds ] we show the posterior median estimate of the lf , as well as the region containing 90% of the posterior probability . as can be seen",
    ", the 90% bounds contain the true value of the lf , and increase or decrease to reflect the amount of data available as a function of @xmath23 .",
    "furthermore , unlike the traditional mle , these bounds do not rely on an assumption of gaussian uncertainties , and therefore the confidence regions are valid for any sample size .     and",
    "@xmath81 ( solid line ) , from the simulated sample described in   [ s - schechter ] .",
    "the left panel summarizes the posterior probability distributin of the lf when the luminosity limit is @xmath114 $ ] , and the right panel summarizes the posterior distribution of the lf when the luminosity limit is @xmath120 $ ] . in both panels",
    "the shaded region contains @xmath119 of the posterior probability , and the vertical line marks the lower luminosity limit of the simulated survey . the uncertainty on the lf below the luminosity limit increases considerably when @xmath121 , reflecting the fact that the bright end of the schechter lf contains little information on @xmath96 or @xmath21 , and therefore contains little information on the faint end of the lf .",
    "[ f - schechbounds],title=\"fig : \" ]   and @xmath81 ( solid line ) , from the simulated sample described in   [ s - schechter ] .",
    "the left panel summarizes the posterior probability distributin of the lf when the luminosity limit is @xmath114 $ ] , and the right panel summarizes the posterior distribution of the lf when the luminosity limit is @xmath120 $ ] . in both panels",
    "the shaded region contains @xmath119 of the posterior probability , and the vertical line marks the lower luminosity limit of the simulated survey .",
    "the uncertainty on the lf below the luminosity limit increases considerably when @xmath121 , reflecting the fact that the bright end of the schechter lf contains little information on @xmath96 or @xmath21 , and therefore contains little information on the faint end of the lf . [ f - schechbounds],title=\"fig : \" ]    .",
    "we also use monte carlo simulation to compare the bayesian approach to maximum - likelihood for both the binomial and poisson likelihood functions .",
    "we simulated 20 data sets for four types of surveys : ( 1 ) a large area shallow survey , ( 2 ) a large area medium depth survey , ( 3 ) a small area deep survey , and ( 4 ) a large area deep survey for rare objects , such as @xmath122 quasars ( e.g. , * ? ? ? * ) .",
    "for all four survey types we simulated quasars from a schechter luminosity function with parameters the same as in   [ s - schechter ] .",
    "for the large area shallow survey we used a total number of sources of @xmath123 , an area of @xmath124 , and a lower luminosity limit of @xmath125 .",
    "only @xmath126 sources are expected to be detected by this survey . for the large area medium",
    "depth survey we also used a lf normalization of @xmath123 and area of @xmath127 , but instead used a lower luminosity limit of @xmath128 .",
    "the large area medium depth survey is expected to detect @xmath129 sources .",
    "for the small area deep survey we used a survey area of @xmath130 , a lf normalization of @xmath131 sources , and a lower luminosity limit of @xmath132 .",
    "this survey is expected to detect @xmath133 sources .",
    "finally , for the large area deep rare object survey we used an area of @xmath134 , a lf normalization of @xmath135 sources , and a lower luminosity limit of @xmath136 .",
    "only @xmath137 sources are expected to be detected by the rare object survey .",
    "we fit each of the 20 simulated data sets by maximum - likelihood for both the binomial and poisson likelihood functions .",
    "the 95% confidence intervals on the best - fit parameters were determined using 2000 bootstrap samples .",
    "we generate each bootstrap sample by randomly drawing n data points with replacement from the original sample , and then calculate the maximum - likelihood estimates for each bootstrap sample .",
    "we use the bootstrap to estimate the confidence intervals because the bootstrap does not assume that the errors are gaussian , and because it is a common technique used in the lf literature .",
    "we estimate the 95% confidence intervals directly from the 0.025 and 0.975 percentiles of the bootstrap sample . while bootstrap confidence intervals derived in this manner",
    "are known to be biased ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) , additional corrections to the bootstrap samples are complicated .",
    "in addition , it is common practice to estimate bootstrap confidence intervals in this manner , and it is worth testing their accuracy . for the bayesian approach , we used the mha algorithm described in ",
    "[ s - mha_schechter ] to simulate @xmath138 random draws from the posterior distribution .",
    "the mha algorithm was faster than fitting the 2000 bootstrap samples using maximum likelihood .",
    "for each of the simulated samples , we counted the number of times that the true values of @xmath139 and @xmath81 were contained within the estimated 95% confidence interval .",
    "the results are summarized in table [ t - mlesim ] . because we estimated values of three parameters for 20 simulated data sets of 4 different types of surveys , we had 240 ` trials ' with probability of ` success ' @xmath140 .",
    "if the estimated 95% confidence regions corresponded to the true regions , then with @xmath141 probability between 220 and 236 of the ` trials ' would fall within the estimated confidence region . for the binomial likelihood ,",
    "the true value of a parameter was within the estimated 95% confidence region only 210 times ( 88% ) , and for the poisson likelihood the true value of a parameter was within the estimated 95% confidence region only 202 times ( 84% ) .",
    "in contrast , the bayesian approach was able to correctly constrain the true value of a parameter to be within the 95% confidence region 233 times ( 97% ) .",
    "therefore , for our simulations confidence regions derived from bootstrapping the maximum - likelihood estimate are too narrow , while the confidence regions derived from the bayesian method are correct .",
    "lcccc poisson & 19 & 16 & 10 & 11 + binomial & 19 & 19 & 12 & 10 + bayesian & 20 & 19 & 19 & 20 + poisson & 18 & 17 & 18 & 17 + binomial & 18 & 19 & 18 & 18 + bayesian & 20 & 20 & 18 & 19 + poisson & 20 & 19 & 19 & 18 + binomial & 20 & 20 & 19 & 18 + bayesian & 20 & 20 & 18 & 20    most of the failure in the maximum - likelihood confidence intervals came from the difficulty of the maximum - likelihood approach in constraining the lf normalization , @xmath21 , for the small area deep survey and for the rare object survey .",
    "in particular , for these two surveys the bootstrap 95% confidence intervals for both the binomial and poisson likelihood function only contained the true value of @xmath21 roughly 50% of the time .",
    "in general , there was nt a significant difference among the simulated surveys in the ability of the three different statistical approaches to constrain @xmath142 and @xmath81 at 95% confidence .",
    "however , the poisson and binomial likelihood functions gave slightly different results for the larger area medium depth survey .",
    "for this survey the 95% confidence intervals for the maximum - likelihood estimate derived from the poisson distribution were somewhat smaller than those for the binomial distribution , only correcting including the true values of @xmath21 , @xmath142 , and @xmath81 roughly 85% of the time .",
    "this is expected , because the poisson distribution is the limit of the binomial distribution as the probability of including a source approaches zero ; however , the detection probability for the large area medium depth survey is @xmath143 .",
    "the results of our simulations imply that the poisson likelihood function may lead to biased estimates of confidence intervals on the luminosity function parameters , so long as an astronomical survey is able to detect a significant fraction of the objects of interest .",
    "use of the poisson likelihood function may thus be problematic for the large astronomical surveys becoming common , so long as they are deep enough to achieve a moderate detection fraction .",
    "for the simulations performed here , using the poisson likelihood function resulted in confidence intervals that are too narrow .",
    "however , these simulations were for a schechter luminosity function , and other parameterizations may be affected differently .",
    "considering that there are no obvious computational advantages to using the poisson likelihood function , we recommend that the correct binomial likelihood function be used , as it is the correct form .",
    "in this section we describe a mixture of gaussian functions model for the luminosity function .",
    "the mixture of gaussians model is a common and well studied ` non - parametric ' model that allows flexibility when estimating a distribution , and is often employed when there is uncertainty regarding the specific functional form of the distribution of interest .",
    "the basic idea is that one can use a suitably large enough number of gaussian functions to accurately approximate the true lf , even though the individual gaussians have no physical meaning . in this sense ,",
    "the gaussian functions serve as a basis set of the luminosity function . as a result",
    ", we avoid the assumption of a more restrictive parametric form , such as a power - law , which can introduce considerable bias when extrapolating beyond the bounds of the observable data .",
    "we have not experimented with other luminosity function basis sets , although mixture models are very flexible and need not be limited to gaussian functions .    in this work",
    "we assume the mixture of gaussian functions for the joint distribution of @xmath144 and @xmath145 , as the logarithm of a strictly positive variable tends to more closely follow a normal distribution than does the untransformed variable .",
    "therefore , we expect that a fewer number of gaussians will be needed to accurately approximate the true lf , thus reducing the number of free parameters . assuming a mixture of gaussian functions for the joint distribution of @xmath144 and @xmath145 is equivalent to assuming a mixture of log - normal distributions for the distribution of @xmath23 and @xmath28 .",
    "the mixture of @xmath146 gaussian functions model for the @xmath147 data point is @xmath148,\\ \\",
    "\\theta = ( \\pi , \\mu , \\sigma ) ,      \\label{eq - mixmod}\\ ] ] where @xmath149 . here ,",
    "@xmath150 , @xmath151 is the 2-element mean ( i.e. , position ) vector for the @xmath152 gaussian , @xmath153 is the @xmath154 covariance matrix for the @xmath152 gaussian , and @xmath155 denotes the transpose of @xmath156 .",
    "in addition , we denote @xmath157 , and @xmath158 .",
    "the variance in @xmath144 for gaussian @xmath142 is @xmath159 , the variance in @xmath145 for gaussian @xmath142 is @xmath160 , and the covariance between @xmath144 and @xmath145 for gaussian @xmath142 is @xmath161 . in this work",
    "we consider the number of gaussian components , @xmath146 , to be specified by the researcher and fixed . if the number of gaussian functions is also considered to be a free parameter , then methods exist for performing bayesian inference on @xmath146 as well ( e.g. , * ? ? ?",
    "statistical inference on the luminosity functions studied in this work were not sensitive to the choice of @xmath146 so long as @xmath162 , and we conclude that values of @xmath162 should be sufficient for most smooth and unimodal luminosity functions .    under the mixture model , the lf can be calculated from equations ( [ eq - phiconvert ] ) and ( [ eq - mixmod ] ) .",
    "noting that @xmath163 , the mixture of gaussian functions model for the lf is @xmath164       \\label{eq - mixlf},\\ ] ] where , as before , @xmath165 .",
    "a mixture of gaussian functions models was also used by @xcite to estimate the @xmath166 galaxy lf from the sloan digital sky survey ( sdss ) .",
    "our mixture of gaussian functions model differs from that used by @xcite in that we do not fix the gaussian function centroids to lie on a grid of values , and their individual widths are allowed to vary .",
    "this flexibility enables us to use a smaller number of gaussian functions ( typically @xmath167 ) to accurately fit the lf .      in this section",
    "we describe the prior distribution that we adopt on the mixture of gaussian functions parameters .",
    "while one may be tempted to assumed a uniform prior on @xmath168 and @xmath9 , this will lead to an improper posterior , i.e. , the posterior probability density does not integrate to one @xcite",
    ". therefore , a uniform prior can not be used , and we need to develop a more informative prior distribution . following @xcite , we assume a uniform prior on @xmath169 under the constraint that @xmath170 ; formally , this is a @xmath171 prior , where @xmath172 denotes a dirichlet density with parameters @xmath173 .",
    "we give further details on the dirichlet probability distribution in appendix   [ a - densities ] .",
    "although our prior knowledge of the lf is limited , it is reasonable to assume a priori that the lf should be unimodal , i.e. , that the lf should not exhibit multiple peaks .",
    "currently , the observed distributions of galaxies and agn are consistent with this assumption . for galaxies , unimodal luminosity function",
    "is implied by simple models of galaxy formation ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) , and the luminosity functions for galaxies classified by morphology are not sufficiently separated to create multimodality ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "however , the luminosity function for agn may be bimodal due to the possible existence of two difference accretion modes .",
    "this is largely due to the fact that the distribution of accretion rates relative to the eddington rate , @xmath174 , is likely bimodal @xcite as a result of a transition from a radiatively inefficient flow to an efficient one at @xmath175 ( e.g. , * ? ? ?",
    "because @xmath176 , the distribution of luminosities for agn may be bimodal . if a survey is unable to detect those agn in the faint radiatively inefficient mode , then the assumption of unimodality is violated and the estimated agn luminosity function only refers to those agn in the bright radiatively efficient mode .    to quantify our prior assumption that the lf is more likely to be unimodal , we construct our prior distribution to place more probability on situations where the individual gaussian functions are close together in terms of their widths .",
    "in addition , we only specify the parametric form of the prior distribution , but allow the parameters of the prior distribution to vary and to be determined by the data .",
    "this allows our prior distribution to be flexible enough to have a minimal effect on the final results beyond conveying our prior consideration that the lf should be unimodal .",
    "we introduce our prior to place more probability on unimodal luminosity functions , to ensure that the posterior integrates to one , and to aid in convergence of the mcmc .",
    "figure [ f - prior ] illustrates the general idea that we are attempting to incorporate into our prior distribution . in this figure",
    ", we show a situation where the gaussian functions are close together with respect to their widths , and far apart with respect to their widths .",
    "when the distances between the individual gaussian functions , normalized by their covariance matrices ( the measure of their ` width ' ) , is small , the lf is unimodal ; however , when the distances between the gaussian functions are large with respect to to their covariance matrices , the lf exhibits multiple modes .",
    "we construct a prior distribution that places less probability on the latter situation .",
    "gaussian functions . shown",
    "are a case when the gaussian functions used in modelling the luminosity function are close together with respect to their covariances ( left ) , and when the gaussian functions are far apart with respect to their covariance matrices ( right ) .",
    "the marginal distributions of @xmath145 are shown above the plots , and the marginal distributions of @xmath144 are shown to the right of the plots . when the gaussian functions are close , the lf is unimodal , but when the gaussian functions are far apart , the lf is multimodal . because our prior distribution is constructed to place more probability on situations when the gaussian functions are closer together with respect to their individual covariance matrices , it would place more probability on the situation shown in the left plot _",
    "a priori_. our prior therefore reflects our expectation that the lf should not exhibit multiple peaks ( modes ) . [ f - prior],title=\"fig : \" ]",
    "gaussian functions . shown",
    "are a case when the gaussian functions used in modelling the luminosity function are close together with respect to their covariances ( left ) , and when the gaussian functions are far apart with respect to their covariance matrices ( right ) .",
    "the marginal distributions of @xmath145 are shown above the plots , and the marginal distributions of @xmath144 are shown to the right of the plots . when the gaussian functions are close , the lf is unimodal , but when the gaussian functions are far apart , the lf is multimodal . because our prior distribution is constructed to place more probability on situations when the gaussian functions are closer together with respect to their individual covariance matrices , it would place more probability on the situation shown in the left plot _",
    "a priori_. our prior therefore reflects our expectation that the lf should not exhibit multiple peaks ( modes ) .",
    "[ f - prior],title=\"fig : \" ]    .",
    "our prior on the gaussian mean vectors and covariance matrices is similar to the prior described by @xcite , but generalized to 2-dimensions . for our prior ,",
    "we assume an independent multivariate cauchy distribution for each of the gaussian means , @xmath3 , with 2-dimensional mean vector @xmath177 and @xmath154 scale matrix @xmath178 .",
    "a cauchy distribution is equivalent to a student s @xmath179 distributions with 1 degree of freedom , and when used as a function in astronomy and physics it is commonly referred to as a lorentzian ; we describe the cauchy distribution further in   [ a - densities ] of the appendix .",
    "the scale matrix is chosen to be the harmonic mean of the gaussian function covariance matrices : @xmath180 qualitatively , this prior means that we consider it more likely that the centroids of the individual gaussian functions should scatter about some mean vector @xmath177 , where the width of this scatter should be comparable to the typical width of the individual gaussian functions .",
    "the prior mean , @xmath177 , is left unspecified and is an additional free parameter to be estimated from the data .",
    "we choose a cauchy prior because the cauchy distribution is heavy tailed , and therefore does not heavily penalize the gaussian functions for being too far apart . as a result ,",
    "the cauchy prior is considered to be robust compared to other choices , such as the multivariate normal distribution .",
    "because we use a random walk computational technique to explore the parameter space and estimate the posterior distribution , we find it advantageous to impose additional constraints on the gaussian centroids .",
    "both @xmath3 and @xmath177 are constrained to the region @xmath181 and @xmath182 , where @xmath183 is the mean in @xmath144 for the @xmath152 gaussian , @xmath184 is the mean in @xmath145 for the @xmath152 gaussian .",
    "these constraints are imposed to keep the markov chains ( see   [ s - mha ] ) from ` wandering ' into unreasonable regions of the parameter space .",
    "the flux limit sets a lower limit on the luminosity of detected sources as a function of @xmath28 , and therefore there is nothing in the observed data to ` tell ' the random walk that certain values of @xmath185 are unreasonable .",
    "for example , suppose our survey is only able to detect quasars with @xmath186 .",
    "because of this , there is nothing in our data , as conveyed through the likelihood function , that says values of , say , @xmath187 are unreasonable , and thus the markov chains can get stuck wandering around values of @xmath188 .",
    "however , we know _ a priori _ that values of @xmath188 are unphysical , and therefore it is important to incorporate this prior knowledge into the posterior , as it is not reflected in the likelihood function .",
    "the values of these limits should be chosen to be physically reasonable .",
    "as an example , for the sdss dr3 quasar lf with luminosities measured at @xmath189@xmath190 , it might be reasonable to take @xmath191 , @xmath192 , @xmath193 , and @xmath194 .    generalizing the prior of @xcite",
    ", we assume independent inverse wishart priors on the individual gaussian covariance matrices with @xmath195 degrees of freedom , and common scale matrix @xmath196 . we give a description of the wishart and inverse wishart distributions in   [ a - densities ] .",
    "this prior states that the individual @xmath153 are more likely to be similar rather than different .",
    "the common scale matrix , @xmath196 , is left unspecified so it can adapt to the data . as with @xmath3",
    ", we recommend placing upper and lower limits on the allowable values of dispersion in @xmath144 and @xmath145 for each gaussian ..    mathematically , our prior is @xmath197 under the constraints given above . here , @xmath198 denotes a 2-dimensional cauchy distribution as a function of @xmath151 , with mean vector @xmath177 and scale matrix @xmath178 .",
    "in addition , inv - wishart@xmath199 denotes an inverse wishart density as a function of @xmath153 , with one degree of freedom and scale matrix @xmath196 .",
    "we have also experimented with using a uniform prior on the parameters , constricted to some range . in general , this did not change our constraints on the lf above the flux limit , but resulted in somewhat wider confidence regions on the lf below the flux limit .",
    "this is to be expected , since our adopted cauchy prior tends to restrict the inferred lf to be unimodal , and therefore limits the number of possible luminosity functions that are considered to be consistent with the data .",
    "now that we have formulated the prior distribution , we can calculate the posterior distribution for the mixture of gaussians model of @xmath27 . because we have formulated the mixture model for the lf in terms of @xmath144 and @xmath145 , the marginal posterior distribution of @xmath2 is @xmath200^{-n } \\prod_{i \\in { \\cal a}_{obs } }       p(\\log l_i,\\log z_i|\\theta),\\ \\",
    "\\theta = ( \\pi,\\mu,\\sigma ) , \\label{eq - thetapost_log}\\ ] ] where @xmath201 is given by equation ( [ eq - prior ] ) , @xmath202 is given by equation ( [ eq - mixmod ] ) , and @xmath203 is the probability of including a source , given the model parameters @xmath2 .",
    "the conditional posterior distribution of @xmath21 given @xmath204 and @xmath9 is given by inserting equation ( [ eq - selprob_log ] ) into ( [ eq - npost ] ) .",
    "the complete joint posterior distribution is then @xmath205",
    "for our statistical model , @xmath177 has 2 free parameters , @xmath196 has 3 free parameters , and each of the @xmath146 gaussian components has 6 free parameters .",
    "because the values of @xmath206 are constrained to sum to one , there are only @xmath207 free parameters for the gaussian mixture model .",
    "the number of free parameters in our statistical model is therefore @xmath208 .",
    "the large number of parameters precludes calculation of the posterior on a grid of @xmath209 and @xmath21 .",
    "furthermore , the multiple integrals needed for marginalizing the posterior , and thus summarizing it , are numerically intractable . because of this , we employ markov chain monte carlo ( mcmc ) to obtain a set of random draws from the posterior distribution .",
    "a markov chain is a random walk , where the probability distribution of the current location only depends on the previous location . to obtain random numbers generated from the posterior distribution ,",
    "one constructs a markov chain that performs a random walk through the parameter space , where the markov chain is constructed to eventually converge to the posterior distribution .",
    "once convergence is reached , the values of the markov chain are saved at each iteration , and the values of these locations can be treated as a random draw from the posterior distribution .",
    "these draws may then be used to estimate the posterior distribution of @xmath27 , and thus an estimate of the lf and its uncertainty can be obtained .    in this work",
    "we use the metropolis - hastings algorithm ( mha , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) to perform the mcmc .",
    "we describe the particular mha we employ for bayesian inference on the lf ; however for a more general and complete description of the mha , we refer the reader to @xcite or @xcite .",
    "we use the mha to obtain a set of random draws from the marginal posterior distribution of @xmath2 , given by equation ( [ eq - thetapost_log ] ) . then , given these random draws of @xmath2 , random draws for @xmath21 may be obtained directly from the negative binomial distribution ( see eq.[[eq - npost ] ] and   [ a - densities ] of the appendix ) .    the basic idea behind the mha",
    "is illustrated in figure [ f - mha ] for the special case of a symmetric jumping distribution .",
    "first , one starts with an initial guess for @xmath2 .",
    "then , at each iteration a proposed value of @xmath2 is randomly drawn from some ` jumping ' distribution .",
    "for example , this jumping distribution could be a normal density with some fixed covariance matrix , centered at the current value of @xmath2 . then , if the proposed value of @xmath2 improves the posterior , it is stored as the new value of @xmath2 .",
    "otherwise , it is stored as the new value with probability equal to the ratio of the values of the posterior distribution at the proposed and current value of @xmath2 . if the proposed value of @xmath2 is rejected , then the value of @xmath2 does not change , and the current value of @xmath2 is stored as the ` new ' value of @xmath2 .",
    "the process is repeated until convergence .",
    "if the jumping distribution is not symmetric , then a correction needs to be made to the acceptance rule in order to account for asymmetry in the jumping distribution .",
    "a jumping distribution is symmetric when the probability of jumping from a current value @xmath2 to a new value @xmath210 is the same as jumping from @xmath210 to @xmath2 .",
    "for example , the normal distribution is symmetric , while the log - normal distribution is not .      before describing our mha algorithm for the mixture of gaussian functions model",
    ", we describe a simpler mha algorithm for the schechter function model given by equation ( [ eq - schechter ] ) in ",
    "[ s - schechter ] .",
    "we do this to illustrate the mha using a more familiar luminosity function .",
    "an mha for obtaining random draws of @xmath104 and @xmath21 from equation ( [ eq - schechpost ] ) is :    1 .",
    "[ i - sstart ] start with an initial value of @xmath96 and @xmath81 , denoted as @xmath211 and @xmath212 .",
    "a good initial value is the maximum - likelihood estimate .",
    "[ i - slstar ] draw a proposal value of @xmath100 from a normal distribution centered on the current value of @xmath100 , @xmath213 .",
    "the variance in the jumping distribution of @xmath214 should be fixed at the beginning of the mha",
    ". a larger jumping variance will lead to jumps that travel greater distances , but will then lead to lower mha acceptance rates .",
    "the value of the jumping variance should be tuned to give acceptance rates @xmath215 .",
    "we use a normal jumping distribution to vary @xmath100 because @xmath100 is defined on @xmath216 , while @xmath81 is only defined on @xmath217 .",
    "while we could use a jumping distribution to directly vary @xmath81 , it is not always easy to simulate random variables directly from distributions that are only defined for @xmath218 .",
    "+ denoting the proposal value of @xmath81 as @xmath219 , calculate the ratio @xmath220 here , @xmath221 is the posterior distribution for the schechter function , given by equation ( [ eq - schechpost ] ) . if @xmath222 , then keep the proposal and set @xmath223 .",
    "if @xmath224 , then draw a random number @xmath225 uniformly distributed between 0 and 1 . if @xmath226 , then keep the proposal and set @xmath227",
    "otherwise , the proposal is rejected and the value of @xmath212 is unchanged .",
    "the factor of @xmath228 is necessary in equation ( [ eq - rlstar ] ) in order to correct for the asymmetry in the log - normal jumping distribution .",
    "[ i - salpha ] draw a proposal value of @xmath229 from a normal distribution centered at the current value of @xmath230 , @xmath231 .",
    "similar to the mha step for @xmath81 , we use a normal jumping distribution to vary @xmath232 because @xmath230 is defined on @xmath216 , while @xmath96 is only defined on @xmath233 .",
    "+ denoting the proposal value of @xmath142 as @xmath234 , the proposal value of @xmath235 is @xmath236 . using the values of @xmath211 and @xmath235 ,",
    "calculate the ratio @xmath237 if @xmath238 , then keep the proposal and set @xmath239 .",
    "if @xmath240 , then draw a random number @xmath225 uniformly distributed between 0 and 1 .",
    "if @xmath241 , then keep the proposal and set @xmath242 . otherwise , the proposal is rejected and the value of @xmath211 is unchanged . as with the mha step for @xmath81 ,",
    "the factor of @xmath243 is necessary in equation ( [ eq - ralf ] ) in order to correct for the asymmetry in the log - normal jumping distribution .",
    "[ i - srepeat ] repeat steps ( [ i - slstar])([i - salpha ] ) until the mha algorithm converges .",
    "techniques for monitoring convergence are described in @xcite . after convergence ,",
    "use equation ( [ eq - npost ] ) to directly simulate random draws of the lf normalization , @xmath21 , for each simulated value of @xmath96 and @xmath81 obtained from the above random walk .",
    "equation ( [ eq - npost ] ) has the form of a negative binomial distribution , and a method for simulated random variables from the negative binomial distribution is described in   [ a - densities ] of the appendix .",
    "our mha for the mixture of gaussian functions model is a more complex version of that used for the schechter function model .",
    "as before , we denote the current value of a parameter by placing a @xmath244 over its symbol , and we denote the proposed value by placing a @xmath245 over its symbol . for example , if one were updating @xmath206 , then @xmath246 denotes the current value of @xmath206 in the random walk , and @xmath247 denotes the proposed value of @xmath206 .",
    "we will only update one parameter at a time , so , if we are drawing a proposal for @xmath206 , the current value of @xmath2 is denoted as @xmath248 , and the proposed value of @xmath2 is denoted as @xmath249 .",
    "our mha for the mixture of gaussian functions model is :    1 .",
    "[ i - mstart ] start with initial guesses for @xmath250 and @xmath178 .",
    "[ i - pi ] draw a proposal value for @xmath206 from a @xmath251 density , where @xmath252 , @xmath30 is the number of sources in the survey , and @xmath253 is a fixed positive constant that controls how far the ` jumps ' in @xmath206 go . because @xmath253 controls the variance of the dirichlet density , a smaller value of @xmath253 produces values of @xmath247 that are further from @xmath246 .",
    "the value of @xmath253 should be chosen so that about @xmath254@xmath255 of the mha proposals are accepted . + after drawing a proposal for @xmath206 ,",
    "calculate the value of the posterior distribution at the new value of @xmath2 , @xmath1 , and at the old value of @xmath2 , @xmath256 .",
    "then , use these values to calculate the ratio @xmath257 where @xmath258 .",
    "the ratio of dirichlet densities in equation ( [ eq - pijump ] ) corrects the mha acceptance rule for the asymmetry in the dirichlet jumping distribution . if @xmath259 then keep the proposed value of @xmath206 : @xmath260 .",
    "otherwise keep the proposal with probability @xmath261 .",
    "this is done by drawing a uniformly distributed random variable between 0 and 1 , denoted by @xmath225 . if @xmath262 , then set @xmath263 . if @xmath264 then keep the current value of @xmath206 .",
    "+ methods for simulating from the dirichlet distribution , as well as the functional form of the dirichlet distribution , are given in   [ a - densities ] .",
    "[ i - mu ] for each gaussian function , draw a proposal for @xmath151 by drawing @xmath265 , where @xmath266 is some set covariance matrix . because the jumping density is symmetric , the mha acceptance ratio is just given by the ratio of the posterior distributions at the proposed and current value of @xmath151 : @xmath267 . if @xmath268 then set @xmath269 , otherwise set @xmath270 with probability @xmath271 .",
    "the mha update should be performed separately for each gaussian .",
    "the covariance matrix of the jumping kernel , @xmath266 , should be chosen such that @xmath272 of the mha jumps are accepted .",
    "+ since we have constructed the prior distribution with the constraint @xmath181 and @xmath273 for all @xmath142 , any values of @xmath274 that fall outside of this range should automatically be rejected .",
    "[ i - covar ] for each gaussian , draw a proposal for @xmath153 by drawing @xmath275 wishart@xmath276 , where @xmath277 is some set degrees of freedom .",
    "larger values of @xmath277 will produce values of @xmath278 that are more similar to @xmath279 .",
    "the mha acceptance ratio is @xmath280 \\right\\ }        \\frac{p(\\hat{\\theta}|x_{obs})}{p(\\tilde{\\theta}|x_{obs } ) } ,        \\label{eq - sigjump}\\ ] ] where @xmath281 denotes the trace of a matrix . if @xmath282 then set @xmath283 , otherwise set @xmath283 with probability @xmath284 .",
    "the mha update should be performed separately for each gaussian .",
    "the degrees of freedom of the jumping kernel , @xmath277 , should be chosen such that @xmath285@xmath255 of the mha jumps are accepted .",
    "+ if there are any bounds on @xmath153 incorporated into the prior distribution , then values of @xmath153 that fall outside of this range should automatically be rejected .",
    "methods for simulating from the wishart distribution , as well as the functional form of the wishart distribution , are given in ",
    "[ a - densities ] .",
    "[ i - mu0 ] draw a proposal for the prior parameter @xmath177 as @xmath286 .",
    "the acceptance ratio only depends on the prior distribution and is @xmath287        \\left [ \\frac{\\int_{\\log l_{low}}^{\\log l_{high } } \\int_{\\log z_{low}}^{\\log z_{high } }         { \\rm cauchy}_2(\\mu_k|\\tilde{\\mu}_0,t)\\ d\\mu_k }      { \\int_{\\log l_{low}}^{\\log l_{high } } \\int_{\\log z_{low}}^{\\log z_{high } }         { \\rm cauchy}_2(\\mu_k|\\hat{\\mu}_0,t)\\ d\\mu_k } \\right]^k .",
    "\\label{eq - mu0jump}\\ ] ] here , @xmath178 is given by equation ( [ eq - priorscale ] ) and the integrals are needed because of the prior constraints on @xmath3 . if @xmath288 then set @xmath289 , otherwise set @xmath289 with probability @xmath290 .",
    "we have found a good choice for @xmath291 to be the sample covariance matrix of @xmath292 .",
    "[ i - aupdate ] finally , update the value of @xmath196 , the common scale matrix . because we can approximately calculate the conditional distribution of @xmath196 , given @xmath9",
    ", we can directly simulate from @xmath293 . directly simulating from the conditional distributions",
    "is referred to as a gibbs sampler .",
    "we perform a gibbs update to draw a new value of @xmath294 : @xmath295 for the gibbs sampler update , we do not need to calculate an acceptance ratio , and every value of @xmath296 is accepted : @xmath297 .",
    "if there are any prior bounds set on @xmath9 , then this is technically only an approximate gibbs update , as it ignores the constraint on @xmath9 . a true mha update would account for the constraint on @xmath9 by renormalizing the conditional distribution appropriately ; however , this involves a triple integral that is expensive to compute .",
    "if there are prior bounds on @xmath9 , then equation ( [ eq - ajump ] ) is approximately correct , and ignoring the normalization in @xmath293 does not did not have any effect on our results .",
    "steps [ i - pi][i - aupdate ] are repeated until the mcmc converges , where one saves the values of @xmath256 at each iteration .",
    "after convergence , the mcmc is stopped , and the values of @xmath256 may be treated as a random draw from the marginal posterior distribution of @xmath2 , @xmath298 . techniques for monitoring convergence of the markov chains are described in @xcite . if one wishes to assume a uniform prior on @xmath3 and @xmath9 , constrained within some set range , instead of the prior we suggest in   [ s - prior ] , then only steps [ i - pi][i - covar ] need to be performed .",
    "given the values of @xmath2 obtained from the mcmc , one can then draw values of @xmath21 from the negative binomial density ( cf .",
    "eq.[[eq - npost ] ] ) . in ",
    "[ a - densities ] we describe how to simulate random variables from a negative binomial distribution .",
    "the speed of our mha algorithm depends on the sample size and the programming language . as a rough guide , on a modern computer",
    "our mha can take a couple of hours to converge for sample sizes of @xmath299 , and our mha can take as long as a day or two to converge for sample sizes @xmath300 .",
    "when performing the mcmc it is necessary to perform a ` burn - in ' stage , after which the markov chains have approximately converged to the posterior distribution .",
    "the values of @xmath2 from the mcmc during the burn - in stage are discarded , and thus only the values of @xmath2 obtained after the burn - in stage are used in the analysis .",
    "we have found it useful to perform @xmath300 iterations of burn - in , although this probably represents a conservative number .",
    "in addition , the parameters for the mha jumping distributions should be tuned during the burn - in stage . in particular , the parameters @xmath301 and @xmath277 should be varied within the burn - in stage to make the mha more efficient and have an acceptance rate of @xmath302@xmath303 @xcite .",
    "these jumping distribution parameters can not be changed after the burn - in stage . @xcite and @xcite described additional complications and considerations developing mhas for mixture models .",
    "some post processing of the markov chains is necessary .",
    "this is because some chains can get ` stuck ' wandering in regions far below flux limit , likely in the presence of a local maximum in the posterior .",
    "while such chains will eventually converge and mix with the other chains , they do not always do so within the finite number of iterations used when running the random walk mha .",
    "we argued in ",
    "[ s - prior ] that the gaussian centroids should be limited to some specified range in l and z to prevent the chains from getting stuck .",
    "however , this is only a partial fix , as the minimum luminosity for the gaussian function means may be significantly fainter than the flux limit at a given redshift , @xmath304 and @xmath305 . in general , we have found that divergent chains are easy to spot . because the divergent chains usually get stuck in regions far below the flux limit , they correspond to luminosity functions with implied extremely low detection probabilities , i.e. , @xmath306 . as a result , the random draws of @xmath21 from the posterior for these chains tend to have values that are too high and far removed from the rest of the posterior distribution of @xmath21 .",
    "the divergent chains are therefore easily found and removed by inspecting a histogram of @xmath69 .",
    "in fact , we have found that the divergent chains often become too large for the long integer format used in our computer routines , and therefore are returned as negative numbers . because negative values of @xmath21 are unphysical , it is easy to simply remove such chains from the analysis .    having obtained random draws of @xmath21 and @xmath2 from @xmath307 , one can then use these values to calculate an estimate of @xmath27 , and its corresponding uncertainty .",
    "this is done by inserting the mcmc values of @xmath2 and @xmath21 directly into equation ( [ eq - mixlf ] ) .",
    "the posterior distribution of @xmath27 can be estimated for any value of @xmath23 and @xmath28 by plotting a histogram of the values of @xmath27 obtained from the mcmc values of @xmath2 and @xmath21 . in ",
    "[ s - sim ] , we illustrate in more detail how to use the mha results to perform statistical inference on the lf .",
    "as an illustration of the effectiveness of our method , we applied it to a simulated data set .",
    "we construct a simulated sample , and then recover the luminosity function based on our mixture of gaussian functions model .",
    "we assume the effective survey area and selection function reported for the dr3 quasar sample @xcite .",
    "we first drew a random value of @xmath308 quasars from a binomial distribution with probability of success @xmath309 and number of trials @xmath310 . here , @xmath311 is the effective sky area for our simulated survey , and we chose the total number of quasars to be @xmath310 in order to ultimately produce a value of @xmath312 observed sources , after accounting for the sdss selection function .",
    "this first step of drawing from a binomial distribution simulates a subset of @xmath313 sources from @xmath21 total sources randomly falling within an area @xmath314 on the sky . for simplicity , in this simulation we ignore the effect of obscuration on the observed quasar population .",
    "while our choice of @xmath310 produces a much smaller sample than the actual sample of @xmath315 quasars from the sdss dr3 luminosity function work @xcite , we chose to work with this smaller sample to illustrate the effectiveness of our method on more moderate sample sizes .",
    "for each of these @xmath313 sources , we simulated values of @xmath23 and @xmath28 .",
    "we first simulated values of @xmath316 from a marginal distribution of the form @xmath317 where @xmath318 . the parameters @xmath319 and @xmath320 were chosen to give an observed redshift distribution similar to that seen for sdss dr3 quasars ( e.g. , * ?",
    "values of @xmath145 are easily drawn from equation ( [ eq - zmarg ] ) by first drawing @xmath321 , and then setting @xmath322 ; here , @xmath323 is a beta probability density , and @xmath324 is the logit function .    for each simulated value of @xmath28 , we simulated a value of @xmath23 using a similar functional form .",
    "the conditional distribution of @xmath144 given @xmath28 is @xmath325^{\\alpha(z)+\\beta(z ) } }      \\label{eq - mcond } \\\\",
    "\\alpha(z ) & = & 6 + \\log z \\\\",
    "\\beta(z ) & = & 9 + 2 \\log z \\\\",
    "l^*(z ) & = & 10^{45 } z^2 ,    \\end{aligned}\\ ] ] where @xmath326 approximately marks the location of the peak in @xmath327 , @xmath328 is the age of the universe in gyr at redshift @xmath28 , @xmath329 is the slope of @xmath330 for @xmath331 , and @xmath332 is the slope of @xmath330 for @xmath333 . in this simulated ` universe ' , both the peak and logarithmic slopes of the lf evolve .",
    "the form of the luminosity function assumed by equation ( [ eq - mcond ] ) is similar to the double power - law form commonly used in the quasar lf literature , but has a more gradual transition between the two limiting slopes .",
    "after using equations ( [ eq - zmarg ] ) and ( [ eq - mcond ] ) to generate random values of @xmath23 and @xmath28 , we simulated the effects at a selection function .",
    "we randomly kept each source for @xmath334 , where the probability of including a source given its luminosity and redshift was taken to be the sdss dr3 quasar selection function , as reported by @xcite .",
    "after running our simulated sample through the selection function , we were left with a sample of @xmath335 sources .",
    "therefore , our simulated survey is only able to detect @xmath336 of the @xmath310 total quasars in our simulated ` universe ' .",
    "the distributions of @xmath23 and @xmath28 are shown in figure [ f - simdist ] for both the detected sources and the full sample .",
    "as can be seen , the majority of sources are missed by our simulated survey .",
    "the joint probability distribution of @xmath23 and @xmath28 is @xmath337 , and therefore equations ( [ eq - zmarg ] ) and ( [ eq - mcond ] ) imply that the true lf for our simulated sample is @xmath338 figure [ f - truebhmf ] shows @xmath339 at several redshifts . also shown in figure [ f - truebhmf ] is the best fit for a mixture of @xmath340 gaussian functions . despite the fact that @xmath339 has a rather complicated parametric form , a mixture of four gaussian functions is sufficient to achieve an excellent approximation to @xmath339 ; in fact , the mixture of four gaussian functions approximation is indistinguishable from the true lf .    , and the best @xmath341 gaussian function fit ( dashed black line ) . in this case , approximating the lf with four 2-dimensional gaussian functions provides an excellent fit.[f - truebhmf ] ]    .",
    "we performed the mha algorithm described in ",
    "[ s - mha ] to obtain random draws from the posterior probability distribution for our this simulated sample , assuming the gaussian mixture model described in   [ s - smodel ] .",
    "we performed @xmath342 iterations of burn - in , and then ran the markov chains for @xmath343 more iterations .",
    "we ran 20 chains simultaneously in order to monitor convergence ( e.g. , see * ? ? ?",
    "* ) and explore possible multimodality in the posterior .",
    "we saved the values of @xmath2 for the markov chains after the initial @xmath342 burn - in iterations , and , after removing divergent chains with @xmath344 we were left with @xmath345 random draws from the posterior distribution , @xmath346 .",
    "the output from the mcmc can be used to perform statistical inference on the lf .",
    "denote the @xmath347 random draws of @xmath2 and @xmath21 obtained via the mha as @xmath348 and @xmath349 , respectively .",
    "the individual values of @xmath350 can then be used to construct histograms as estimates of the posterior distribution for each parameter .",
    "for each random draw of @xmath2 and @xmath21 , we can also calculate a random draw of @xmath27 from its posterior distribution .",
    "in particular , the @xmath351 random draw of the lf under the mixture of normals model , denoted as @xmath352 , is calculated by inserting @xmath353 and @xmath354 into equation ( [ eq - mixlf ] ) .",
    "the @xmath178 values of @xmath352 can then be used to estimate the posterior distribution of @xmath27 for any given value of @xmath23 and @xmath28 .",
    "furthermore , random draws from the posterior for quantities that are computed directly from the lf , such as the location of its peak as a function of @xmath28 , are obtained simply by computing the quantity of interest from each of the @xmath178 values of @xmath352 .    in figures [ f - philin ] and",
    "[ f - philog ] we show @xmath355 at several different redshifts , on both a linear scale and a logarithmic scale . in general , we find it easier to work with @xmath356 , as @xmath357 can span several orders of magnitude in @xmath23 .",
    "figures [ f - philin ] and [ f - philog ] show the true value of the lf , @xmath358 , the best - fit estimate of @xmath357 based on the mixture of gaussian functions model , and the regions containing @xmath119 of the posterior probability . here , as well as throughout this work , we will consider the posterior median of any quantity to be the ` best - fit ' for that quantity .",
    "in addition , in this work we will report errors at the @xmath119 level , and therefore the regions containing @xmath119 of the posterior probability can be loosely interpreted as asymmetric error bars of length @xmath359 .",
    "the region containing @xmath119 of the probability for @xmath357 is easily estimated from the mcmc output by finding the values of @xmath360 and @xmath361 such that @xmath119 of the values of @xmath362 have @xmath363 . as can be seen ,",
    "the true value of @xmath357 is contained within the @xmath119 probability region for all almost values of @xmath23 , even those below the survey detection limit .",
    "figure [ f - mzmarg ] compares the true integrated @xmath364 number distribution of @xmath144 , @xmath365 , with the mixture of gaussian functions estimate .",
    "the quantity @xmath366 gives the number of quasars at @xmath364 with black hole masses between @xmath144 and @xmath367 .",
    "it is calculated as @xmath368 which , for the mixture of normals model , is @xmath369       \\label{eq - mnumbdist_mix } \\\\",
    "e(\\log z|l , k ) & = & \\mu_{z , k } + \\frac{\\sigma_{lz , k}}{\\sigma^2_{z , k } }       \\left(\\log l - \\mu_{l , k } \\right ) \\\\",
    "var(\\log z|l , k ) & = & \\sigma^2_{z , k } - \\frac{\\sigma^2_{lz , k}}{\\sigma^2_{z , k}}.    \\end{aligned}\\ ] ] here , @xmath370 is the cumulative distribution function for the standard normal density .",
    "similar to figures [ f - philin ] and [ f - philog ] , the true value of @xmath371 is contained within the @xmath119 probability region for all values of @xmath23 , even those below the survey detection limit .",
    "quasar number density ( number per @xmath144 interval , left ) and the quasar comoving quasar number density as a function of @xmath28 ( number per @xmath372 , right ) for the simulated sample described in   [ s - simconst ] . as with figure [ f - philin ]",
    ", the solid red line denotes the true value for the simulation , the dashed blue line denotes the posterior median for the mixture of gaussian functions model , and the shaded region contain @xmath119 of the posterior probability .",
    "the posterior median provides a good fit to the true values , and the uncertainties derived from the mcmc algorithm based on the gaussian mixture model are able to accurately constrain the true values of these quantities , despite the flux limit.[f - mzmarg],title=\"fig : \" ]   quasar number density ( number per @xmath144 interval , left ) and the quasar comoving quasar number density as a function of @xmath28 ( number per @xmath372 , right ) for the simulated sample described in   [ s - simconst ] . as with figure [ f - philin ] ,",
    "the solid red line denotes the true value for the simulation , the dashed blue line denotes the posterior median for the mixture of gaussian functions model , and the shaded region contain @xmath119 of the posterior probability .",
    "the posterior median provides a good fit to the true values , and the uncertainties derived from the mcmc algorithm based on the gaussian mixture model are able to accurately constrain the true values of these quantities , despite the flux limit.[f - mzmarg],title=\"fig : \" ]    in addition , in figure [ f - mzmarg ] we show the comoving number density of broad line agn as a function of redshift , @xmath373 .",
    "this is obtained by integrating @xmath27 over all possible values of @xmath23 . for the mixture of normals model ,",
    "this becomes @xmath374 where the marginal distribution of @xmath375 is @xmath376 as before , the true value of @xmath373 is contained within the @xmath119 probability region , despite the fact that the integration extends over _ all _ @xmath23 , even those below the detection limit .",
    "the wider confidence regions reflect additional uncertainty in @xmath373 resulting from integration over those @xmath23 below the detection limit . in particular ,",
    "the term @xmath377 becomes small at low redshift , making the estimate of @xmath373 more unstable as @xmath378 , and thus inflating the uncertainties at low @xmath28 .",
    "two other potentially useful quantities are the comoving luminosity density for quasars , @xmath379 , and its derivative .",
    "the comoving quasar luminosity density is given by @xmath380 .",
    "for the mixture of gaussian functions model it may be shown that @xmath381 where @xmath382 is given by equation ( [ eq - zmargmix ] ) .",
    "we calculate the derivative of @xmath379 numerically .",
    "figure [ f - rhoz ] compares the true values of @xmath379 and its derivative with the posterior distribution for @xmath379 inferred from the mixture model , both as a function of @xmath28 and the age of the universe at redshift @xmath28 , @xmath328 .",
    "comparison with figure [ f - mzmarg ] reveals that the comoving quasar luminosity density , @xmath379 , is a better constrained quantity than the comoving quasar number density , @xmath373 .",
    "furthermore , @xmath373 appears to peak much later than @xmath379 .",
    "in addition , we can correctly infer that the comoving quasar luminosity density reaches it point of fastest growth at @xmath383 gyr , and its point of fastest decline at @xmath384 gyr .",
    "figure [ f - peaks ] quantifies the suggestion that @xmath373 peaks later than @xmath379 by displaying the posterior distribution for the location of the respective peaks in @xmath373 and @xmath379 .",
    "we can still constrain the peak in @xmath373 to be at @xmath385 .",
    "in contrast , the location of the peak in @xmath379 is constrained to occur earlier at @xmath386 .",
    "this is a consequence of the fact that while there were more quasars per comoving volume element in our simulated universe at @xmath385 , their luminosities were much higher at higher redshift .",
    "this evolution in characteristic @xmath23 is quantified in figure [ f - mpeakevol ] , which summarizes the posterior distribution for the location of the peak in @xmath357 as a function of redshift and @xmath328 . as can be seen",
    ", the location of the peak in the lf shows a clear trend of increasing ` characteristic ' @xmath23 with increasing @xmath28 , although there is considerable uncertainty on the actual value of the location of the peak .    . for clarity",
    "we only show the posterior distribution for the peak in @xmath373 at @xmath387 , since values of the peak at @xmath388 arise because the term @xmath389 becomes very large at low @xmath28 .",
    "the vertical lines denote the true values .",
    "the posterior distribution inferred from the mcmc output is able to accurately constrain the true values of the argumentative maximum in @xmath373 and @xmath390.[f - peaks],title=\"fig : \" ] . for clarity",
    "we only show the posterior distribution for the peak in @xmath373 at @xmath387 , since values of the peak at @xmath388 arise because the term @xmath389 becomes very large at low @xmath28 .",
    "the vertical lines denote the true values .",
    "the posterior distribution inferred from the mcmc output is able to accurately constrain the true values of the argumentative maximum in @xmath373 and @xmath390.[f - peaks],title=\"fig : \" ]     ( left ) and cosmic age ( right ) for the simulated sample described in   [ s - simconst ] .",
    "the plot symbols are the same is in figure [ f - mzmarg ] . in general the posterior median of the gaussian mixture model",
    "provides a good estimate of the true peak locations , although the uncertainty is high due to the survey flux limit .",
    "however , it is clear from these plots that the location of the peak in @xmath27 evolves.[f - mpeakevol],title=\"fig : \" ]   ( left ) and cosmic age ( right ) for the simulated sample described in ",
    "[ s - simconst ] .",
    "the plot symbols are the same is in figure [ f - mzmarg ] . in general the posterior median of the gaussian mixture model",
    "provides a good estimate of the true peak locations , although the uncertainty is high due to the survey flux limit .",
    "however , it is clear from these plots that the location of the peak in @xmath27 evolves.[f - mpeakevol],title=\"fig : \" ]      throughout this section we have been analyzing the mcmc results by comparing to the true lf .",
    "however , in practice we do not have access to the true lf , and thus a method is needed for assessing the quality of the fit .",
    "the statistical model may be checked using a technique known as posterior predictive checking ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    ", the basic idea is to use each of the mcmc outputs to simulate a new random observed data set .",
    "the distributions of the simulated observed data sets are then compared to the true observed data in order to assess whether the statistical model gives an accurate representation of the observed data .",
    "it is important to construct simulated data sets for each of the mcmc draws in order to incorporate our uncertainty in the model parameters .    for each value of @xmath354 and @xmath353 obtained from the mcmc output ,",
    "a simulated data set of @xmath391 may be obtained through a similar procedure to that described in   [ s - simconst ] .",
    "first , one draws a value of @xmath392 from a binomial distribution with @xmath354 trials and probability of ` success ' @xmath393 .",
    "then , one draws @xmath392 values of @xmath394 and @xmath395 from @xmath396 .    for our model , @xmath397 is a mixture of normal densities , and one needs to employ a two - step process in order to simulate a random value from @xmath397 .",
    "first , one needs to randomly assign the @xmath33 data point to one of the gaussian distributions . since @xmath398 gives the probability that a data point will be drawn from the @xmath152 gaussian distribution , one first needs to simulate a random vector @xmath399 from a multinomial distribution with one trial and probability of success for the @xmath152 class @xmath400 ; i.e. , first draw @xmath401 .",
    "the vector @xmath402 gives the class membership for the @xmath33 data point , where @xmath403 if the @xmath33 data point comes from the @xmath404 gaussian , and @xmath405 if @xmath406 .",
    "then , given @xmath403 , one then simulates a value of @xmath407 from a 2-dimensional gaussian distribution with mean @xmath408 and covariance matrix @xmath409 .",
    "this is repeated for all @xmath410 sources , leaving one with a random sample @xmath411 .",
    "a random draw from @xmath412 , may be obtained as a sequence of binomial random draws .",
    "first , draw @xmath413 .",
    "if @xmath414 , then assign the data point to the first gaussian distribution , i.e. , set @xmath415 . if @xmath416 , then draw @xmath417 .",
    "if @xmath418 , then assign the data point to the second gaussian distribution , i.e. , set @xmath419 .",
    "if @xmath420 , then the process is repeated for the remaining gaussian distribution as follows . for @xmath421 , sequentially draw @xmath422 .",
    "if at any time @xmath423 , then stop the process and assign the data point to the @xmath424 gaussian distribution . otherwise , if none of the @xmath425 , then assign the data point to the @xmath426 gaussian distribution .",
    "once one obtains a random draw of @xmath427 , randomly ` observe ' these sources , where the probability of including a source given @xmath428 and @xmath429 is given by the selection function .",
    "this will leave one with a simulated observed data set , @xmath430 .",
    "this process is repeated for all @xmath178 values of @xmath354 and @xmath353 obtained from the mcmc output , leaving one with @xmath178 simulated data sets of @xmath431 .",
    "one can then compare the distribution of the simulated data sets with the true values of @xmath432 and @xmath48 to test the statistical model for any inconsistencies .    in figure [ f - postcheck ]",
    "we show histograms for the true observed distributions of @xmath28 and @xmath144 .",
    "these histograms are compared with the posterior median of the distributions based on the mixture of gaussian functions model , as well as error bars containing @xmath119 of the simulated values . also shown",
    "is a plot comparing the true values of the maximum of @xmath47 as a function of @xmath28 with those based on @xmath433 and @xmath434 .",
    "as can be seen , the distributions of the observed data assuming the mixture of gaussian functions model are consistent with the true distributions of the observed data , and therefore there is no reason to reject the mixture model as providing a poor fit .",
    "we have derived the observed data likelihood function which relates the quasar lf to the observed distribution of redshifts , luminosities . this likelihood function",
    "is then used in a bayesian approach to estimating the lf , where the lf is approximated as a mixture of gaussian functions . because much of this work was mathematically technical ,",
    "we summarize the important points here .",
    "* equation [ eq - obslik1 ] gives the likelihood function for an assumed parametric luminosity function .",
    "this likelihood function differs from the poisson likelihood commonly used in the lf literature because it correctly models the sample size as a binomial random variable , whereas the poisson likelihood approximates the sample size as a poisson random variable . in practice ,",
    "the difference in the maximum - likelihood estimates obtained from the two likelihood functions do not seem to be significantly different so long as the probability of including a source in a survey is small . *",
    "the product of equations ( [ eq - thetapost ] ) and ( [ eq - npost ] ) is the joint posterior probability distribution of the lf , given the observed data .",
    "these equations may be used to perform bayesian inference on the lf , after assuming a prior distribution on the lf parameters .",
    "bayesian inference is often most easily performed by simulating random variables drawn from the posterior probability distribution .",
    "these random draws may be used to estimate the posterior distribution for the lf , as well as to estimate the posterior distribution for any quantities calculated from the lf .",
    "the posterior distribution provides statistically accurate uncertainties on the lf and related quantities , even when the sample size is small and one is including information below the survey detection limits .",
    "in contrast , confidence intervals derived from bootstrapping the maximum - likelihood estimate can be too small .",
    "* we describe a flexible model for the lf , where the lf is modeled as a mixture of gaussian functions .",
    "equation ( [ eq - mixmod ] ) describes the probability distribution of @xmath144 and @xmath145 under the mixture of gaussian functions model , and ( [ eq - mixlf ] ) describes the lf under the mixture of gaussian functions model .",
    "+ equation ( [ eq - prior ] ) gives our prior distribution for the gaussian function parameters .",
    "the marginal posterior distribution of the mixture model parameters is given by equation ( [ eq - thetapost_log ] ) , the conditional posterior distribution of @xmath21 at a given @xmath2 is given by equation ( [ eq - npost ] ) , and the complete joint posterior distribution is the product of equations ( [ eq - thetapost_log ] ) and ( [ eq - npost ] ) .",
    "* we describe in ",
    "[ s - mha ] a metropolis - hastings algorithm for obtaining random draws from the posterior distribution for the lf assuming a schechter function or mixture of gaussian functions model . in   [ s - sim ] , we use a simulated sample , modeled after the sdss dr3 quasar catalog , to illustrate the effectiveness of our statistical method , as well as to give an example on how to use the metropolis - hastings output to perform statistical inference on the lf and assess the lf fit .",
    "so long as the mixture of gaussian functions model is an accurate approximation to the true lf over all luminosities , the uncertainties on the lf , assuming the gaussian mixture model , are trustworthy because they are calculated directly from the probability distribution of the lf , given the observed data .",
    "statistical inference on the lf below the flux limit can become significantly biased if one assumes an incorrect and restrictive parametric form , as extrapolation errors can become large . in this case , the derived posterior may not contain the true lf because the wrong parametric model was assumed ; this type of error is known as model misspecification . for example , consider a case when the true lf is a schechter function , and one is only able to detect sources brighter than @xmath81 . if one were to assume a power - law for the lf , extrapolation below the flux limit would be significantly biased .",
    "in contrast , the mixture of gaussian functions model , while incorrect , is flexible enough to accurately approximate the true schechter function form , thus minimizing extrapolation bias due to model misspecification . of course , in this example , the most accurate results would be obtained by fitting a schechter function to the data , since it is the correct parametric form .",
    "therefore , the mixture of gaussian functions model will not perform as well as assuming the correct parametric model , or at least as well as an alternative parametric model that better approximates the true lf .",
    "although we have focused on the mixture of gaussian functions model , the likelihood and posterior distribution are applicable for any parametric form , as illustrated in   [ s - schechter ] .",
    "the observed data likelihood function for the lf is given by equation ( [ eq - obslik1 ] ) , and the posterior distribution is given by the product of equations ( [ eq - thetapost ] ) and ( [ eq - npost ] ) .",
    "then , one can use equation ( [ eq - phiconvert ] ) to ` plug - in ' any parametric form of the lf into the appropriate likelihood function and posterior distribution , as was done in equation ( [ eq - schechpost ] ) for a schechter function .",
    "in addition , the metropolis - hastings algorithm is a general method for obtaining random draws from the posterior , and can be developed for any parametric form of the lf .",
    "this therefore allows one to perform bayesian inference for any variety of parametric models of the lf , and one is not merely limited to the mixture of gaussian functions model or schechter function considered in this work .    an idl computer routine written for performing the metropolis - hastings algorithm for the mixture of gaussian functions model is available on request from b. kelly .",
    "we acknowledge support from nsf grant ast 03 - 07384 and a david and lucile packard fellowship in science and engineering .",
    "in this section we compare the maximum likelihood approach with the bayesian approach . we do this for readers who are unfamiliar with some of the more technical aspects of the two approaches , with the hope that the discussion in this section will facilitate interpretation of our results in the main body of the paper .    in maximum - likelihood analysis ,",
    "one is interested in finding the estimate that maximizes the likelihood function of the data . for a given statistical model , parameterized by @xmath2 , the likelihood function , @xmath435 ,",
    "is the probability of observing the data , denoted by @xmath11 , as a function of the parameters @xmath2 .",
    "the maximum - likelihood estimate , denoted by @xmath1 , is the value of @xmath2 that maximizes @xmath435 . under certain regularity conditions",
    ", @xmath1 enjoys a number of useful properties .",
    "in particular , as the sample size becomes infinite , @xmath1 becomes an unbiased estimate of @xmath2 .",
    "an unbiased estimator is an estimator with expectation equal to the true value , i.e. , @xmath436 , where @xmath437 is the true value of @xmath2 .",
    "therefore , on average , an unbiased estimator will give the true value of the parameter to be estimated .    because the maximum likelihood estimate is a function of the data",
    ", @xmath1 has a sampling distribution .",
    "the sampling distribution of @xmath1 is the distribution of @xmath1 under repeated sampling from the probability distribution of the data . under certain regularity conditions",
    ", the sampling distribution of @xmath1 is asymptotically normal with covariance matrix equal to the average value of the inverse of the fisher information matrix , @xmath438 , evaluated at @xmath437 .",
    "the fisher information matrix is the expected value of the matrix of second derivatives of the log - likelihood , multiplied by @xmath439 .",
    "formally , this result states that as @xmath440 , then @xmath441 where @xmath7 is the number of parameters in the model , and the expectation in equation ( [ eq - fishinf ] ) is taken with respect to the sampling distribution of @xmath11 , @xmath442 . because we do not know @xmath437 , it is common to estimate @xmath443 by @xmath444 .",
    "in addition , it is common to estimate @xmath438 as the matrix of second derivatives of the log - likelihood of one s data , since the sample average is a consistent estimate for the expectation value .",
    "qualitatively , equation([eq - mle_asympt ] ) states that as the sample size becomes large , @xmath1 approximately follows a normal distribution with mean @xmath437 and covariance matrix @xmath444 .",
    "this fact may be used to construct confidence intervals for @xmath2 .    while the asymptotic results are useful , it is not always clear how large of a sample is needed until equation ( [ eq - mle_asympt ] ) is approximately true .",
    "the maximum likelihood estimate can be slow to converge for models with many parameters , or if most of the data is missing . within the context of luminosity function estimation ,",
    "the maximum - likelihood estimate will be slower to converge for surveys with shallower flux limits .",
    "in addition , equation ( [ eq - mle_asympt ] ) does not hold if the regularity conditions are not met .",
    "in general , this is not a concern , but it is worth noting that the asymptotics do not hold if the true value of @xmath2 lies on the boundary of the parameter space .",
    "for example , in the case of a schechter luminosity function , if the true value of the shape parameter , @xmath96 ( see [ [ eq - schechter ] ] ) , is @xmath445 , then equation ( [ eq - mle_asympt ] ) does not hold , since @xmath446 .",
    "if @xmath447 , then equation ( [ eq - mle_asympt ] ) is still valid , but it will take a large sample before the asymptotics are valid , as @xmath448 lies near the boundary of the parameter space",
    ".    in bayesian analysis , one attempts to estimate the probability distribution of the model parameters , @xmath2 , given the observed data @xmath11 .",
    "the probability distribution of @xmath2 given @xmath11 is related to the likelihood function as @xmath449 the term @xmath435 is the likelihood function of the data , and the term @xmath450 is the prior probability distribution of @xmath2 ; the result , @xmath451 is called the posterior distribution .",
    "the prior distribution , @xmath450 , should convey information known prior to the analysis . in general",
    ", the prior distribution should be constructed to ensure that the posterior distribution integrates to one , but to not have a significant effect on the posterior . in particular ,",
    "the posterior distribution should not be sensitive to the choice of prior distribution , unless the prior distribution is constructed with the purpose of placing constraints on the posterior distribution that are not conveyed by the data .",
    "the contribution of the prior to @xmath451 becomes negligible as the sample size becomes large .    from a practical standpoint ,",
    "the primary difference between the maximum likelihood approach and the bayesian approach is that the maximum likelihood approach is concerned with calculating a point estimate of @xmath2 , while the bayesian approach is concerned with mapping out the distribution of @xmath2 .",
    "the maximum likelihood approach uses an estimate of the sampling distribution of @xmath1 to place constraints on the true value of @xmath2 .",
    "in contrast , the bayesian approach directly calculates the probability distribution of @xmath2 , given the observed data , to place constraints on the true value of @xmath2 .",
    "it is illustrative to consider the case when the prior is taken to be uniform over @xmath2 ; assuming the posterior integrates to one , the posterior is then proportional to the likelihood function , @xmath452 . in this case",
    ", the goal of maximum likelihood is to calculate an estimate of @xmath2 , where the estimate is the most probable value of @xmath2 , given the observed data .",
    "then , confidence intervals on @xmath2 are derived from the maximum likelihood estimate , @xmath1 , usually by assuming equation ( [ eq - mle_asympt ] ) .",
    "in contrast , the bayesian approach is not concerned with optimizing the likelihood function , but rather is concerned with mapping out the likelihood function . under the bayesian approach with a uniform prior , confidence intervals on @xmath2 are derived directly from likelihood function , and an estimate of @xmath2 can be defined as , for example , the value of @xmath2 averaged over the likelihood function .",
    "so , the maximum likelihood attempts to obtain the ` most likely ' value of @xmath2 , while the bayesian approach attempts to directly obtain the probability distribution of @xmath2 , given the observed data . because the bayesian approach directly estimates the probability distribution of @xmath2 , and because it does not rely on any asymptotic results , we consider the bayesian approach to be preferable for most astronomical applications .",
    "here , we give a derivation of the posterior probability distribution of @xmath2 , given by equation ( [ eq - thetapost ] ) . if we assume a uniform prior on @xmath69 , then this is equivalent to assuming the prior @xmath453 . in this case",
    ", the posterior distribution is given by @xmath454^{n - n } \\prod_{i \\in { \\cal a}_{obs } } p(l_i , z_i | \\theta).\\ ] ] the marginal posterior distribution of @xmath2 is obtained by summing the joint posterior over all possible values of @xmath21 . for the choice of prior @xmath455 , the marginal posterior of @xmath2 is @xmath456 \\sum_{n = n}^{\\infty } n^{-1 } c^n_n   \\left [ p(i=0|\\theta ) \\right]^{n - n }       \\label{eq - thetamarg1 } \\\\",
    "& \\propto & p(\\theta ) \\left [ p(i=1|\\theta ) \\right]^{-n } \\left[\\prod_{i \\in { \\cal a}_{obs } }         p(l_i , z_i|\\theta ) \\right ] \\sum_{n = n}^{\\infty } c^{n-1}_{n-1 } \\left [ p(i=0|\\theta ) \\right]^{n - n }      \\left [ p(i=1|\\theta ) \\right]^{n } , \\label{eq - thetamarg2 }    \\end{aligned}\\ ] ] where we arrived at the second equation by multiplying and dividing the first equation by @xmath457 and noting that @xmath458 .",
    "the term within the sum is the mathematical expression for a negative binomial distribution as a function of @xmath21 ( see eq.[[eq - negbin ] ] ) . because probability distributions must be equal to unity when summed over all possible values , the sum is just equal to one .",
    "we therefore arrive at equation ( [ eq - thetapost ] ) by replacing the summation in equation ( [ eq - thetamarg2 ] ) with the value of one .",
    "in this section of the appendix we briefly describe some probability distribution that we employ , but may be unfamiliar to some astronomers .",
    "the negative binomial distribution is closely related to the binomial distribution .",
    "the binomial distribution gives the probability of observing @xmath30 ` successes ' , given that there have been @xmath21 trials and that the probability of success is @xmath7 .",
    "in contrast , the negative binomial distribution gives the probability of needing @xmath21 trials before observing @xmath30 successes , given that the probability of success is @xmath7 .",
    "within the context of this work , the binomial distribution gives the probability of detecting @xmath30 sources , given that there are @xmath21 total sources and that the detection probability is @xmath7 .",
    "the negative binomial distribution gives the probability that the total number of sources is @xmath21 , given that we have detected @xmath30 sources and that the detection probability is @xmath7 .",
    "the negative binomial distribution is given by @xmath459    a random draw from the negative binomial distribution with parameters @xmath30 and @xmath7 may be simulated by first drawing @xmath30 random values uniformly distributed on @xmath460 $ ] , @xmath461 .",
    "then , calculate the quantity @xmath462 where @xmath463 is the floor function , i.e. , @xmath464 denotes the greatest integer less than or equal to @xmath11 .",
    "the quantity @xmath465 will then follow a negative binomial distribution with parameters @xmath30 and @xmath7 .",
    "the dirichlet distribution is a multivariate generalization of the beta distribution , and it is commonly used when modeling group proportions .",
    "dirichlet random variables are constrained to be positive and sum to one .",
    "the dirichlet distribution with argument @xmath466 and parameters @xmath467 is given by @xmath468 to draw a random value @xmath469 from a dirichlet distribution with parameters @xmath467 , first draw @xmath470 independently from gamma distributions with shape parameters @xmath467 and common scale parameter equal to one .",
    "then , set @xmath471 .",
    "the set of @xmath2 will then follow a dirichlet distribution .",
    "the student-@xmath179 distribution is often used as a robust alternative to the normal distribution because it is more heavily tailed than the normal distribution , and therefore reduces the effect of outliers on statistical analysis . a @xmath179 distribution with @xmath195 degree of freedom is referred to as a cauchy distribution , and it is functionally equivalent to a lorentzian function . a @xmath7-dimensional multivariate @xmath179 distribution with @xmath7-dimensional argument @xmath472 , @xmath7-dimensional mean vector @xmath3 , @xmath8 scale matrix @xmath9 , and degrees of freedom @xmath473 is given by @xmath474^{-(\\nu + p)/2}.      \\label{eq - tdist}\\ ] ] the 1-dimensional @xmath179 distribution is obtained by replacing matrix and vector operations in equation ( [ eq - tdist ] ) with scalar operations .",
    "although we do not simulate from a @xmath179 distribution in this work , for completeness we include how to do so . to simulate a random vector @xmath475 from a multivariate @xmath179 distribution with mean vector @xmath3 , scale matrix @xmath9 , and degrees of freedom @xmath473 , first draw @xmath476 from a zero mean multivariate normal distribution with covariance matrix @xmath9 .",
    "then , draw @xmath11 from a chi - square distribution with @xmath473 degrees of freedom , and compute the quantity @xmath477 . the quantity @xmath475 is then distributed according to the multivariate @xmath179 distribution .",
    "the wishart distribution describes the distribution of the @xmath478 sample covariance matrix , given the @xmath8 population covariance matrix , for data drawn from a multivariate normal distribution .",
    "conversely , the inverse wishart distribution describes the distribution of the population covariance matrix , given the sample covariance matrix , when the data are drawn from a multivariate normal distribution .",
    "the wishart distribution can be thought of as a multivariate extension of the @xmath479 distribution . a wishart distribution with @xmath8 argument @xmath480 , @xmath8 scale matrix @xmath9 , and degrees of freedom @xmath473 is given by @xmath481^{-1 }      |\\sigma|^{-\\nu/2 } |s|^{(\\nu - p - 1 ) / 2 } \\exp\\left \\ { -\\frac{1}{2 }       tr ( \\sigma^{-1 } s ) \\right \\ } , \\label{eq - wishart}\\ ] ] where the matrices @xmath480 and @xmath9 are constrained to be positive definite .",
    "an inverse wishart distribution with @xmath8 argument @xmath9 , @xmath8 scale matrix @xmath480 , and degrees of freedom @xmath473 is @xmath482^{-1 }      |s|^{\\nu/2 } |\\sigma|^{-(\\nu + p + 1 ) / 2 } \\exp\\left\\{-\\frac{1}{2 } tr(\\sigma^{-1 } s)\\right\\ } ,      \\label{eq - invwishart}\\ ] ] where the matrices @xmath480 and @xmath9 are constrained to be positive definite .    to draw a @xmath8 random matrix from a wishart distribution with scale matrix @xmath9 and @xmath473 degrees of freedom ,",
    "first draw @xmath483 from a zero mean multivariate normal distribution with @xmath8 covariance matrix @xmath9 .",
    "then , calculate the sum @xmath484 .",
    "the quantity @xmath480 is then a random draw from a wishart distribution .",
    "note that this technique only works when @xmath485 .",
    "a random draw from the inverse wishart distribution with scale matrix @xmath480 and degrees of freedom @xmath473 may be obtained by first obtaining a random draw @xmath486 from a wishart distribution with scale matrix @xmath487 and degrees of freedom @xmath473 .",
    "the quantity @xmath488 will then follow an inverse wishart distribution .",
    "avni , y. , & bahcall , j.  n.  1980 , , 235 , 694 babbedge , t.  s.  r. , et al .",
    "2006 , , 370 , 1159 barger , a.  j. , cowie , l.  l. , mushotzky , r.  f. , yang , y. , wang , w .- h . , steffen , a.  t. , & capak , p.  2005",
    ", , 129 , 578 blanton , m.  r. , et al .",
    "2003 , , 592 , 819 bower , r.  g. , benson , a.  j. , malbon , r. , helly , j.  c. , frenk , c.  s. , baugh , c.  m. , cole , s. , & lacey , c.  g.  2006 , , 370 , 645 brown , m.  j.  i. , dey , a. , jannuzi , b.  t. , brand , k. , benson , a.  j. , brodwin , m. , croton , d.  j. , & eisenhardt , p.  r.  2007 , , 654 , 858 budavri , t. , et al .",
    "2005 , , 619 , l31 cao , x. , & xu , y .- d .  2007 , , 377 , 425 chib , s. , & greenberg , e.  1995 , amer .",
    "stat . , 49 , 327 cirasuolo , m. , et al .",
    "2007 , , 380 , 585 croom , s.  m. , smith , r.  j. , boyle , b.  j. , shanks , t. , miller , l. , outram , p.  j. , & loaring , n.  s.  2004 , , 349 , 1397 croton , d.  j. , et al .",
    "2005 , , 356 , 1155 dahlen , t. , mobasher , b. , somerville , r.  s. , moustakas , l.  a. , dickinson , m. , ferguson , h.  c. , & giavalisco , m.  2005 , , 631 , 126 davison , a.  c. , & hinkley , d.  v.  1997 , bootstrap methods and their application ( cambridge : cambridge university press ) dellaportas , p. , & papageorgiou , i.  2006 , stat .",
    "comput . , 16 , 57 efron , b.  1987 , j.  americ .",
    "assoc . , 82 , 171 efron , b. , & petrosian , v.  1992 , , 399 , 345 faber , s.  m. , et al .",
    "2007 , , 665 , 265 fan , x. , et al .",
    "2001 , , 121 , 54 fan , x. , et al .",
    "2006 , , 131 , 1203 finlator , k. , dav , r. , papovich , c. , & hernquist , l.  2006 , , 639 , 672 gelman , a. , carlin , j.  b. , stern , h.  s. , & rubin , d.  b.  2004 , bayesian data analysis ( 2nd ed . ;",
    "boca raton : chapman & hall / crc ) gelman , a. , meng , x.  l. , & stern , h.  s.  1998 , statistica sinica , 6 , 733 gelman , a. , roberts , g. , & gilks , w.  1995 , in bayesian statistics 5 , ed . j.  m. bernardo , j.  o. berger , a.  p. dawid , & a.  f.  m. smith ( oxford : oxford university press ) , 599 hao , l. , et al .",
    "2005 , , 129 , 1795 harsono , d. , & de propris , r.  2007 , , 380 , 1036 hastings , w.  k.  1970 , biometrika , 57 , 97 ho , l.  c.  2002 , , 564 , 120 hopkins , p.  f. , hernquist , l. , cox , t.  j. , di matteo , t. , robertson , b. , & springel , v.   2006 , , 163 , 1 hopkins , p.  f. , narayan , r. , & hernquist , l.  2006 , , 643 , 641 hopkins , p.  f. , richards , g.  t. , & hernquist , l.  2007 , , 654 , 731 hoyle , f. , rojas , r.  r. , vogeley , m.  s. , & brinkmann , j.  2005 , , 620 , 618 huynh , m.  t. , frayer , d.  t. , mobasher , b. , dickinson , m. , chary , r .-",
    "r . , & morrison , g.  2007 , , 667 , l9 jasra , a. , holmes , c.c . , & stephens , d.a .  2005 , statistical science , 20 , 50 jester , s.  2005 , , 625 , 667 jiang , l. , et al .  2006 , , 131 , 2788 kelly , b.  c.  2007 , , 665 , 1489 kim , d .- w . , et al .",
    "2006 , , 652 , 1090 la franca , f. , et al .  2005 , , 635 , 864 lauer , t.  r. , et al .",
    "2007 , , 662 , 808 lin , y .- t . , & mohr , j.  j.  2007 , , 170 , 71 lynden - bell , d.  1971 , , 155 , 95 magorrian , j. , et al .",
    "1998 , , 115 , 2285 maloney , a. , & petrosian , v.  1999 , , 518 , 32 marchesini , d. , celotti , a. , & ferrarese , l.  2004 , , 351 , 733 marchesini , d. , et al .",
    "2007 , , 656 , 42 marchesini , d. , & van dokkum , p.  g.  2007 , , 663 , l89 marconi , a. , risaliti , g. , gilli , r. , hunt , l.  k. , maiolino , r. , & salvati , m.  2004 , , 351 , 169 marshall , h.  l. , tananbaum , h. , avni , y. , & zamorani , g.  1983 , , 269 , 35 matute , i. , la franca , f. , pozzi , f. , gruppioni , c. , lari , c. , & zamorani , g.  2006 , , 451 , 443 mauch , t. , & sadler , e.  m. 2007 , , 375 , 931 merloni , a.  2004 , , 353 , 1035 metropolis , n. , & ulam , s.  1949 , j.  amer .",
    "assoc . , 44 , 335 metropolis , n. , rosenbluth , a.  w. , rosenbluth , m.  n. , teller , a.  h. , & teller , e.  1953 , j.  chem .",
    "phys . , 21 , 1087 nakamura , o. , fukugita , m. , yasuda , n. , loveday , j. , brinkmann , j. , schneider , d.  p. , shimasaku , k. , & subbarao , m.  2003 , , 125 , 1682 neal , r.  m.  1996 , statistics and computing , 6 , 353 page , m.  j. , & carrera , f.  j.  2000 , , 311 , 433 paltani , s. , et al .  2007 , , 463 , 873 press , w.  h. , & schechter , p.  1974",
    ", , 187 , 425 popesso , p. , biviano , a. , bhringer , h. , & romaniello , m.  2006 , , 445 , 29 ptak , a. , mobasher , b. , hornschemeier , a. , bauer , f. , & norman , c.  2007 , , 667 , 826 richards , g.  t. , et al .",
    "2006 , , 131 , 2766 richardson , s. , & green , p.  j.  1997 , j.  roy .",
    "b , 59 , 731 roeder , k. , & wasserman , l.  1997 , j.  amer .",
    "assoc . , 92 , 894 rubin , d.  b.  1981 , j.  educational statistics , 6 , 377 rubin , d.  b.  1984 , annals of statistics , 12 , 1151 scarlata , c. , et al .",
    "2007 , , 172 , 406 schafer , c.  m.  2007 , , 661 , 703 schechter , p.  1976",
    ", , 203 , 297 schneider , d.  p. , et al .  2005 , , 130 , 367 soltan , a.  1982 , , 200 , 115 spergel , d.  n. , et al .",
    "2003 , , 148 , 175 steffen , a.  t. , barger , a.  j. , cowie , l.  l. , mushotzky , r.  f. , & yang , y.  2003 , , 596 , l23 schmidt , m.  1968 , , 151 , 393 ueda , y. , akiyama , m. , ohta , k. , & miyaji , t.  2003 , , 598 , 886 waddington , i. , dunlop , j.  s. , peacock , j.  a. , & windhorst , r.  a.  2001 , , 328 , 882 willott , c.  j. , rawlings , s. , blundell , k.  m. , lacy , m. , & eales , s.  a.  2001 , , 322 , 536 wolf , c. , wisotzki , l. , borch , a. , dye , s. , kleinheinrich , m. , & meisenheimer , k.  2003 , , 408 , 499 wyithe , j.  s.  b. , & loeb , a.  2003 , , 595 , 614 yu , q. , & tremaine , s.  2002 , , 335 , 965"
  ],
  "abstract_text": [
    "<S> we describe a bayesian approach to estimating luminosity functions . </S>",
    "<S> we derive the likelihood function and posterior probability distribution for the luminosity function , given the observed data , and we compare the bayesian approach with maximum - likelihood by simulating sources from a schechter function . for our simulations confidence intervals derived from bootstrapping the maximum - likelihood estimate can be too narrow , while confidence intervals derived from the bayesian approach are valid . we develop our statistical approach for a flexible model where the luminosity function is modeled as a mixture of gaussian functions . statistical inference is performed using markov chain monte carlo ( mcmc ) methods , and we describe a metropolis - hastings algorithm to perform the mcmc . the mcmc simulates random draws from the probability distribution of the luminosity function parameters , given the data , and we use a simulated data set to show how these random draws may be used to estimate the probability distribution for the luminosity function . in addition , we show how the mcmc output may be used to estimate the probability distribution of any quantities derived from the luminosity function , such as the peak in the space density of quasars . the bayesian method we develop </S>",
    "<S> has the advantage that it is able to place accurate constraints on the luminosity function even beyond the survey detection limits , and that it provides a natural way of estimating the probability distribution of any quantities derived from the luminosity function , including those that rely on information beyond the survey detection limits . </S>"
  ]
}