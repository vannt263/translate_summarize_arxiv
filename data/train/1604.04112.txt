{
  "article_text": [
    "the vision community has been mesmerized by the effectiveness of deep convolutional neural networks ( cnns ) @xcite that have led to a breakthrough in computer vision - related problems .",
    "hence , there has been a notable shift towards cnns in many areas of computer vision @xcite .",
    "convolutional neural networks were popularized through alexnet @xcite in 2009 and their much celebrated victory at the 2012 imagenet competiton @xcite .",
    "after that , there have been several attempts at building deeper and deeper cnns like the vgg network and googlenet in 2014 which have 19 and 22 layers respectively @xcite .",
    "but , very deep models introduce problems like vanishing and exploding gradients @xcite , which hamper their convergence .",
    "the _ vanishing gradient _ problem is trivial in very deep networks . during the backpropagation phase ,",
    "the gradients are computed by the chain rule .",
    "multiplication of small numbers in the chain rule leads to an exponential decrease in the gradient . due to this",
    ", very deep networks learn very slowly . sometimes , the gradient in the earlier layer gets larger because derivatives of some activation functions can take larger values .",
    "this leads to the problem of _ exploding gradient_. these problems have been reduced in practice through normalized initialization @xcite and most recently , batch normalization @xcite .    _",
    "exponential linear unit _ ( elu )",
    "@xcite also reduces the vanishing gradient problem .",
    "elus introduce negative values which push the mean activation towards zero .",
    "this reduces the _ bias shift _ and speeds up learning .",
    "elus give better accuracy and learning speed - up compared to the combination of relu @xcite and batch normalization @xcite .",
    "after reducing the vanishing / exploding gradient problem , the networks start converging .",
    "however , the accuracy degrades in such very deep models @xcite .",
    "the most recent contributions towards solving this problem are highway networks @xcite and residual networks @xcite .",
    "these networks introduce _ skip connections _ , which allow information flow into the deeper layers and enable us to have deeper networks with better accuracy .",
    "the 152-layer resnet outperforms all other models @xcite .    in this paper",
    ", we propose to use exponential linear unit instead of the combination of relu and batch normalization . since exponential linear units reduce the vanishing gradient problem and give better accuracy compared to the combination of relu and batch normalization , we use it in our model to further increase the accuracy of residual networks .",
    "we also notice that elu speeds up learning in very deep networks as well .",
    "we show that our model increases the accuracy on datasets like cifar-10 and cifar-100 , compared to the original model .",
    "it is seen that as the depth increases , the difference in accuracy between our model and the original model increases .",
    "deeper neural networks are very difficult to train .",
    "the vanishing / exploding gradients problem impedes the convergence of deeper networks @xcite .",
    "this problem has been solved by normalized initialization @xcite . a notable recent contribution towards reducing the vanishing gradients problem",
    "is batch normalization @xcite . instead of normalized initialization and keeping a lower learning rate ,",
    "batch normalization makes normalization a part of the model and performs it for each mini - batch .    once the deeper networks start converging , a _ degradation _",
    "problem occurs . due to this , the accuracy degrades rapidly after it is saturated .",
    "the _ training error _ increases as we add more layers to a deep model , as mentioned in @xcite . to solve this problem , several authors introduced skip connections to improve the information flow across several layers .",
    "highway networks @xcite have parameterized skip connections , known as _ information highways _ , which allow information to flow unimpeded into deeper layers . during the training phase ,",
    "the skip connection parameters are adjusted to control the amount of information allowed on these _",
    "highways_.     residual block in a residual networks , height=188 ]    * residual networks ( resnets ) * @xcite utilize shortcut connections with the help of identity transformation . unlike highway networks , these neither introduce extra parameter nor computation complexity .",
    "this improves the accuracy of deeper networks . with increasing depth ,",
    "resnets give better function approximation capabilities as they gain more parameters .",
    "the authors hypothesis is that the plain deeper networks give worse function approximation because the gradients vanish when they are propagated through many layers . to fix this problem",
    ", they introduce skip connections to the network .",
    "formally , if the output of @xmath0 layer is @xmath1 and @xmath2 represents multiple convolutional transformation from layer @xmath3 to @xmath4 , we obtain    @xmath5    where @xmath6 represents the identity function and @xmath7 @xcite is the default activation function . fig .",
    "[ fig : resblock ] illustrates the basic building block of a residual network which consists of multiple convolutional and batch normalization layers .",
    "the identity transformation , @xmath6 is used to reduce the dimensions of @xmath8 to match those of @xmath9 . in residual networks , the gradients and features learned in earlier layers are passed back and forth between the layers via the identity transformations @xmath6 .    * exponential linear unit ( elu ) * @xcite alleviates the vanishing gradient problem and also speeds up learning in deep neural networks which leads to higher classification accuracies .",
    "the _ exponential linear unit _ ( elu ) is @xmath10    the relus are non - negative and thus have mean activations larger than zero , whereas elus have negative values , which push the mean activations towards zero .",
    "elus saturate to a negative value when the input gets smaller .",
    "this decreases the forward propagated variation and information , which draws the mean activations to zero .",
    "units with non - zero mean activations act as a bias for the next layer .",
    "if these units do not cancel each other out , then the learning causes a _ bias shift _ for units in the next layer .",
    "therefore , elus decrease the bias shift as the mean activations are closer to zero .",
    "less bias shift also speeds up learning by bringing standard gradient closer towards the unit natural gradient .",
    "[ fig : elu ] shows the comparison of relu and elu ( @xmath11 ) .    ) , height=188 ]    .24     .24     .24     .24",
    "the residual network in @xcite is a functional composition of @xmath12 _ residual blocks _ ( resblocks ) , each encoding the update rule ( [ eq:1 ] ) .",
    "fig [ fig : resblock ] shows the schematic illustration of the @xmath13 resblock . in this example",
    ", @xmath14 consists of a sequence of layers : * conv - bn - relu - conv - bn * , where conv and bn stands for convolution and batch normalization respectively .",
    "this construction scheme is adopted in all our experiments while reproducing the results of @xcite .",
    "the function @xmath14 is parameterized by some set of parameters @xmath15 , which we omit for notational simplicity .",
    "normally , we use 64 , 32 or 16 filters in the convolutional layers .",
    "the size of receptive field is @xmath16 .",
    "although it does not seem attractive but , in practice it gives better accuracy without adding any overhead costs , as compared to plain networks .      in comparison with the resnet model @xcite ,",
    "we use exponential linear unit ( elu ) in place of a combination of relu with batch normalization .",
    "[ fig : elublocks ] illustrates our different experiments with elus in resblock .",
    "in this model , @xmath14 consists of a sequence of layers : * conv - elu - conv - elu*. fig .",
    "[ fig : conv - elu - conv - elu ] represents the basic building block of this experiment .",
    "we trained our model using the specification mentioned in [ cifar10analysis ] .",
    "but we found that after few iterations , the gradients blew up .",
    "when the learning rate is decreased , the 20-layer model starts converging but to very less accuracy .",
    "the deeper models like 56 and 110-layer still do not converge after decreasing the learning rate .",
    "this model clearly fails as the trivial problem of exploding gradient can not be reduced in very deep models .",
    "this is a * full pre - activation unit * resblock @xcite with elu .",
    "the sequence of layers is * elu - conv - elu - conv*. fig .",
    "[ fig : elu - conv - elu - conv ] highlights the basic resblock of this experiment . during the training of this model",
    "too , the gradients exploded after few iterations . due to the exponential function",
    ", the gradients get larger and lead to exploding gradient problem . even decreasing the learning rate",
    "also does not reduce this problem .",
    "we decided to add a batch normalization layer before addition to control this problem .",
    ".23     .23     to control the exploding gradient , we added a batch normalization before addition .",
    "so , the sequence of layers in this resblock is * conv - elu - conv - bn * and elu after addition .",
    "[ fig : conv - elu - conv - bn1 ] represents the resblock used in this experiment .",
    "thus in this resblock , the update rule ( [ eq:1 ] ) for the @xmath13 layer is    @xmath17    the batch normalization layer reduces the exploding gradient problem found in the previous two models .",
    "we found that this model gives better accuracy for 20-layer model .",
    "however , as we increased the depth of the network , the accuracy degrades for the deeper models . if the elu activation function is placed after addtion , then the mean activation of the output pushes towards zero .",
    "this could be beneficial .",
    "however , this forces each skip connection to perturb the output .",
    "this has a harmful effect and we found that this leads to degradation of accuracy in very deep resnets .",
    "[ fig : elu_after_add ] depicts the effects of including elu after addition in this resblock .",
    ".33     .33     .33     .4     .4     fig .",
    "[ fig : conv - elu - conv - bn2 ] gives an illustration of the basic building block of our model .",
    "thus in our model , @xmath14 represents the following sequence of layers : * conv - elu - conv - bn*. the update rule ( [ eq:1 ] ) for the @xmath13 layer is    @xmath18    this is the basic building block for all our experiments on cifar-10 and cifar-100 datasets .",
    "we show that not including elu after addition does not degrade the accuracy , unlike the previous model .",
    "this resblock improves the learning behavior and the classification performance of the residual network .",
    "we empirically demonstrate the effectiveness of our model on a series of benchmark data sets : cifar-10 and cifar-100 . in our experiments , we compare the learning behavior and the classification performance of both the models on the cifar-10 and cifar-100 datasets .",
    "the experiments prove that our model outperforms the original resnet model in terms of learning behavior and classification performance on both the datasets .",
    "finally , we compare the classification performance of our model with other previously published state - of - the - art models .",
    "the first experiment was performed on the cifar-10 dataset @xcite , which consists of 50k training images and 10k test images in 10 classes . in our experiments , we performed training on the training set and evaluation on the test set .",
    "the inputs to the network are @xmath19 images which are color - normalized .",
    "we use a @xmath16 receptive field in the convolution layer .",
    "we use a stack of @xmath20 layers with @xmath16 convolution on the feature maps of sizes @xmath21 respectively , with @xmath22 on each feature map .",
    "the number of filters are @xmath23 respectively .",
    "the original resnet model ends with a global average pooling , a 10-way fully - connected layer and a softmax layer . in our model , we add an elu activation function just before the global average pooling layer .",
    "these two models are trained on a aws g2.2xlarge instance ( which has a single gpu ) with a mini batch - size of 128 .",
    "we use a weight decay of 0.0001 and a momentum of 0.9 , and adopt the weight initialization in @xcite and bn @xcite but with no dropout .",
    "we start with a learning rate of 0.1 and divide by 10 after 81 epochs , and again divide by 10 after 122 epochs .",
    "we use the data augmentation mentioned in @xcite during the training phase : add 4 pixels on each side and do a random @xmath19 crop from the padded image or its horizontal flip . during the testing phase",
    ", we only use a color - normalized @xmath19 image .",
    "our experiments are executed on 20 , 32 , 44 , 56 and 110-layer networks .",
    "[ fig : cifar10trainloss ] shows the comparison of learning behaviours between our model and the original resnet model on cifar-10 dataset for 20 , 32 , 44 , 56 and 110-layers .",
    "the graphs prove that for all the different number of layers , our model possesses a superior learning behavior and converges many epochs before the original model . as the depth of the model increases , our model also learns faster than the original model .",
    "the difference between the learning rate of these two models increases as the depth increases . comparing fig .",
    "[ fig : cifar10trainloss20 ] and fig .",
    "[ fig : cifar10trainloss110 ] , we can easily notice the huge difference in learning rates for 20-layer and 110-layer models .",
    "after 125 epochs , both the models converge to almost the same value .",
    "but , our model has a slightly lower training loss compared to the original model .      fig .",
    "[ fig : cifar10testerror ] illustrates the comparison of classification performance between our model and the original one on cifar-10 dataset for 20 , 32 , 44 , 56 and 110 layers .",
    "we observe that for the 20-layer model , the test error is nearly the same for both the models .",
    "but , as the depth increases , our model significantly outperforms the original model .",
    "table [ table : cifar10testerrortable ] shows the test error for both the models from the epoch with the lowest validation error .",
    "[ fig : cifar10testerror ] shows that the gap between the test error of the two models increases as the depth is also increased .    .test error ( % ) of our model compared to the original resnet model .",
    "the test error of the original resnet model refers to our reproduction of the experiments by he et al .",
    "@xcite [ cols=\"^,^,^ \" , ]     .23     .23",
    "in this paper , we introduce residual networks with exponential linear units which learn faster than the current residual networks .",
    "they also give better accuracy than the original ones when the depth is increased . on datasets like cifar-10 and cifar-100 , we improve beyond the current state - of - the - art in terms of test error , while also learning faster than these models using elus .",
    "elus push the mean activations towards zero as they introduce small negative values .",
    "this reduces the bias shift and increases the learning speed .",
    "our experiments show that not only does our model have superior learning behavior , but it also provides better accuracy as compared to the current model on cifar-10 and cifar-100 datasets .",
    "this enables the researchers to use very deep models and also increase their learning behavior and classification performance at the same time .",
    "deng , j. , dong , w. , socher , r. , li , l.j .",
    ", li , k. , fei - fei , l. : imagenet : a large - scale hierarchical image database . in : computer vision and pattern recognition , 2009 .",
    "cvpr 2009 .",
    "ieee conference on , ieee ( 2009 ) 248255              szegedy , c. , liu , w. , jia , y. , sermanet , p. , reed , s. , anguelov , d. , erhan , d. , vanhoucke , v. , rabinovich , a. : going deeper with convolutions . in : proceedings of the ieee conference on computer vision and pattern recognition .",
    "( 2015 ) 19      wan , l. , zeiler , m. , zhang , s. , cun , y.l .",
    ", fergus , r. : regularization of neural networks using dropconnect . in dasgupta , s. , mcallester , d. , eds . : proceedings of the 30th international conference on machine learning ( icml- 13 ) .",
    "volume 28 .",
    ", jmlr workshop and conference proceedings ( may 2013 ) 10581066              snoek , j. , rippel , o. , swersky , k. , kiros , r. , satish , n. , sundaram , n. , patwary , m. , ali , m. , adams , r.p . , et al .",
    ": scalable bayesian optimization using deep neural networks .",
    "arxiv preprint arxiv:1502.05700 ( 2015 )"
  ],
  "abstract_text": [
    "<S> very deep convolutional neural networks introduced new problems like _ vanishing gradient _ and _ degradation_. the recent successful contributions towards solving these problems are residual and highway networks . </S>",
    "<S> these networks introduce _ skip connections _ that allow the information ( from the input or those learned in earlier layers ) to flow more into the deeper layers . </S>",
    "<S> these very deep models have lead to a considerable decrease in test errors , on benchmarks like imagenet and coco . in this paper </S>",
    "<S> , we propose the use of _ exponential linear unit _ instead of the combination of relu and batch normalization in residual networks . </S>",
    "<S> we show that this not only speeds up learning in residual networks but also improves the accuracy as the depth increases . </S>",
    "<S> it improves the test error on almost all data sets , like cifar-10 and cifar-100 .    </S>",
    "<S> = 1 </S>"
  ]
}