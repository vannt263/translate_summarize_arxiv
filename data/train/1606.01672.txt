{
  "article_text": [
    "predictive coding is a brain plausible principle to account for how diversity of perceptual input sequences can be predicted or generated for different intentions in the top - down pathway as well as how the corresponding intention can be inferred or recognized for a particular observation of perceptual sequence in the bottom - up pathway by using the prediction error minimization principle . within this predictive coding framework",
    ", it has been largely assumed that such prediction / generation and inference / recognition can be conducted through multiple levels across different cortical areas whereas necessary functional hierarchy can be developed via accumulated learning [ 1 , 2 , 3 , 4 ] . the current study examines how a spatio - temporal hierarchy that is adequate for robust generation and recognition of compositional dynamic visual patterns in the pixel level can be developed by proposing a novel deterministic predictive coding type deep recurrent neural network model .",
    "the proposed model , referred to as the predictive multiple spatio - temporal scales rnn ( p - mstrnn ) is a combination of prior - proposed models of the multiple timescales rnn ( mtrnn ) [ 4 ] and the deconvolutional neural network model [ 15 ] where both spatial and temporal multiple scales properties are used as macroscopic constraints to develop an effective functional hierarchy .",
    "there have been prior studies on predicting and generating dynamic visual datasets [ 6 , 7 , 12 , 13 ] . in particular ,",
    "similar to the current paper , lotter et al . in 2016",
    "[ 13 ] also adopted predictive coding in their work .",
    "however , p - pmstrnn differs from the previously proposed generative models for dynamic vision processing , because in the current model not only prediction in the top - down pathway but also inference in the bottom - up pathway can be performed . also , p - mstrnn is advantageous because it can deal with temporal hierarchy in addition to the spatial hierarchy by using the multiple temporal scales properties in the model .",
    "the current study is assumed to be the first attempt to achieve both generation / prediction and recognition / inference of dynamic visual images within one neural network model .",
    "the model was tested with using a video dataset consisting of multiple types of human movement patterns generated by multiple subjects .",
    "after the model was trained for multiple movement patterns , we examined how those multiple patterns are regenerated by using a functional hierarchy self - organized during the learning .",
    "we also scrutinized how concatenation of the learned movement patterns is developed further via additional learning .",
    "furthermore , we examined how test movement patterns are recognized by inferring the corresponding intention .",
    "this recognition test was conducted by following a test framework , referred to as imitative synchronization [ 11 ] , where p - mstrnn attempts to imitate perceived test movement patterns .",
    "we closely analyzed how the model network recognizes possible shifts in test movement patterns by inferring the corresponding shifts of intentions using the on - line error regression scheme .",
    "the video dataset used in training and testing consists of multiple cyclic human movement patterns performed by multiple subjects .",
    "a set of simulation experiments examined the spatio - temporal hierarchy as it develops during learning , and revealed that robust recognition of test patterns depends on learning spatial - temporal interdependencies with exemplar patterns exhibiting high degrees of variance .",
    "the newly proposed model , the predictive multiple spatio - temporal scales rnn ( p - mstrnn ) is a hierarchical neural network model that utilizes spatio - temporal features for generating and recognizing dynamic visual images .",
    "the p - mstrnn model is based on the multiple spatio - temoral scales rnn ( mstrnn ) [ 8 ] .",
    "the original mstrnn is a classification model for dynamic visual images without prediction mechanism .",
    "p - mstrnn differs from mstrnn because it learns , generates , and recognizes patterns using the principle of prediction error minimization in the predictive coding framework .",
    "p - mstrnn consists of a series of context layers and one input / output layer in the bottom , as shown in figure 1 .",
    "each context layer predicts its own neural state at each next time step by receiving the top - down and bottom - up signals from the neighboring layers .",
    "the neural state of each context layer represents its top - down intention .",
    "the first context layer predicts the next step visual inputs by receiving both the current visual input and the top - down signal from the second layer . in the upward direction , the prediction error signal back - propagates inversely in the same pathway for the purpose of updating the connectivity weights as well as the intention at each context layer in the learning and recognition process .",
    "the model utilizes a convolutional neural network ( cnn ) and a deconvolutional neural network [ 9 , 15 ] .",
    "however , unlike the cnn , every context layer in the p - mstrnn model is composed of two different types of units that function differently from the units composing cnn layers .",
    "the first type is the feature unit and the other is the context unit .",
    "both feature unit and context unit are based on the leaky integrator neural units which decay to the value of the previous time step , thereby enabling both types of units to reflect a temporal hierarchy that is not possible in a cnn [ 10 ] .",
    "the set of feature units in the same layer forms a feature map ( fm ) and context units form a context map ( cm ) .",
    "fms mainly contribute to spatial processing by receiving synaptic inputs from neighboring feature units .",
    "the cms , on the other hand , contribute mainly to development of adequate dynamic information processing by taking advantage of their adjustable recurrent connectivity .",
    "the timescale that determines decay rate is differently set in all layers .",
    "the layers in the lower level have smaller time constants and the higher level layers have larger time constants .",
    "therefore , the neural activity in the lower context layer is constrained to be faster while the one in the higher layers is constrained to be slower .",
    "the forward dynamics of fms for the generalized layer are shown in equation ( 1 ) and ( 2 ) .",
    "@xmath0    @xmath1    where @xmath2 is the @xmath3 fm s internal state in the @xmath4 layer at time step t , @xmath5 is the internal state of @xmath3 fm in the @xmath4 layer at t-1 time steps .",
    "@xmath6 is the timescale in the @xmath4 layer and @xmath7 is the number of fms in the @xmath8 layer .",
    "the operation @xmath9 is the convolution operator .",
    "@xmath10 is the kernel connecting the @xmath11 fm in the @xmath8 layer at t-1 time steps and @xmath3 fm in the @xmath4 layer at time step t. @xmath12 is the total number of cm in the @xmath4 layer and @xmath13 is the activation of the @xmath14 cm in the @xmath4 layer at time step t-1 .",
    "@xmath15 is the kernel connecting the @xmath14 cm in @xmath4 layer at the previous time step t-1 and the @xmath3 fm in the @xmath4 layer at the current time step t. @xmath16 is the input data frame from outside of the network and @xmath17 is the kernel between input and the current fm .",
    "@xmath18 is the bias of @xmath3 fms in the @xmath4 layer .",
    "the first term of equation ( 1 ) represents the decayed internal states of fms from the previous time step t-1 with the decay rate @xmath19 .",
    "the second term is the input to the current fm from the activation value of fms in the @xmath8 layer .",
    "the third term represents the input from the cms in the same layer at the previous time step t-1 .",
    "the fourth term indicates the data frame fed to the current fm .",
    "after the internal state of the current fm is computed , the activation value is obtained through a hyperbolic tangent activation function ( equation ( 2 ) ) . when the layer which the current @xmath3 fm belongs to is other than the first layer , the term from the input data frame @xmath16 no longer exists .",
    "also , highest layer internal dynamics do not include the second term in equation 1 due to the absence of any higher layer to provide values .",
    "equations ( 3 ) and ( 4 ) detail the forward dynamics of cms .",
    "@xmath20    @xmath21    where @xmath22 is the internal state of the @xmath23 cm in the @xmath4 layer at time step t and @xmath13 is the activation value of the @xmath14 cm in the @xmath4 layer at time step t-1 .",
    "@xmath24 is the recurrent weight connecting the @xmath23 cm in time step t with the previous @xmath14 cm in time step t-1 .",
    "@xmath25 is the weight connecting the current @xmath23 cm with the @xmath11 fm in the @xmath8 layer at time step t-1 .",
    "@xmath26 is kernel connecting the @xmath27 fm in @xmath28 layer in t-1 step to current cm .",
    "@xmath29 is the element - wise multiplication operator .",
    "the first term of equation ( 3 ) is the leaky integrator input from the internal state of the cm from the previous time step t-1 .",
    "the second term is the recurrent input to the current cm from the previous time step s cm activation value .",
    "@xmath30 is same size as cms in the same layer .",
    "accordingly , the activation values of the previous cm are multiplied with @xmath31 in an element - wise manner . in the cms",
    ", this second term represents the recurrency and reinforces the temporal processes of the network .",
    "the third term represents the input from the activation value of fms in the @xmath8 layer .",
    "the fourth term is from the lower layer fms .",
    "this term utilizes the bottom - up pathway allowing input data frames from outside of the network to be processed through all layers of the network . for the internal state calculation of the cms ,",
    "if the layer is the highest layer , then there is no input from the upper l+1 layer and the third term in equation ( 3 ) is unused . likewise ,",
    "if the layer is the lowest layer , then an input from the lower layer does not exist and the fourth term in equation ( 3 ) is unused . after the internal state is calculated , an activation value is obtained using the hyperbolic tangent function in equation ( 4 ) .    when calculating the convolution , there are some cases in which the input map size is smaller than the size of the output map . in these cases ,",
    "zero - padding is used for the input maps . as for the element - wise multiplication ,",
    "the map and weight sizes are always the same .    in the output layer ,",
    "the output is calculated as , @xmath32    @xmath33    where @xmath34 is the internal state of the output layer , @xmath35 is the number of fms in the @xmath36 layer .",
    "@xmath37 is the kernel connecting the @xmath11 fm in the @xmath36 layer with the output map .",
    "@xmath38 is the bias for the output map .",
    "the internal state of the output is calculated from the fms in the first layer by convolution , and the activation value of the output layer is obtained by applying the scaled hyperbolic tangent function as suggested by lecun et al . [ 16 ] .",
    "the open loop generation method was used for training the network .",
    "when the network utilizes the open loop generation , the network receives the current input frame from the dataset and generates a ( single or multiple ) step prediction as an output frame for each step .",
    "we adopted mean square error ( mse ) for the cost function . a conventional back - propagation through time ( bptt ) method was used for training the network .",
    "the weights , kernels , biases and initial states of the context layers were optimized using gradient descent .    along with open loop generation ,",
    "closed loop generation was also used for training the network .",
    "the closed loop generation method is a scheme in which next step output prediction is computed by using a copy of the output prediction from the previous step as the current input . in the closed loop method ,",
    "error is also calculated in comparison to the target signals . during training ,",
    "error from open loop generation was used to optimize network parameters . as the training proceeded , both open loop error and closed loop error decreased .",
    "however , closed loop error was always higher than that of open loop because closed loop prediction generates accumulated error over time steps without correction from factors outside of the network .",
    "therefore , network training was terminated when the closed loop error reached a predefined lower bound threshold . at this point ,",
    "the network was guaranteed to successfully perform both open loop generation and closed loop generation .    in order for the network to learn to generate multiple data sequences",
    ", it must infer optimal initial states of context units in all layers for each sequence , as well as optimal connectivity weights .",
    "inferred initial states for each training sequence represent intentions to generate corresponding sequence .",
    "is the prediction of the next step input and @xmath39 is the target . ]",
    "the novelty of this model is that the trained network can be used for both generation and recognition of sequence patterns .",
    "once the network is trained , intended patterns are generated through the top - down pathway by setting the intention states of context units .",
    "the intention then propagates to the lower layers , eventually producing the intended or predicted image in a pixel level . on the other hand ,",
    "current input patterns can be recognized by inferring optimal intention states of context units .",
    "the process , imitative synchronization via error regression which will be explained later , is performed by first going through the bottom - up pathway for recognition to find the optimal internal states .",
    "optimal prediction and generation are then achieved through the top - down pathway .    in the framework of neuroscience ,",
    "the brain receives sensory input by a bottom - up pathway and predicts the next input through a top - down path at every moment .",
    "when the actual input is different from what the brain predicted , the brain is surprised and modifies its prediction by reflecting the discrepancies . in this way",
    ", the brain always accurately predicts the future and actively interacts with the outer world [ 1 , 14 ] .",
    "the idea of imitative synchronization via error regression is the same as the way the brain recognizes and predicts sensory inputs .",
    "suppose the network is already trained to generate several types of human movement patterns in a pixel level video sequence given corresponding initial states .",
    "the network now attempts to actively imitate incoming streams of human movement patterns from an online camera at every moment by predicting future frames .",
    "this task is called imitative synchronization . to achieve imitative synchronization",
    ", the network must predict the next input pattern based on the recognized intention of the current input .",
    "the recognition process is performed by finding the network s optimal internal states that minimize error between predictions and actual inputs .",
    "this error minimizing method is called error regression . during the test ,",
    "if the human suddenly changes current movement pattern to another type of pattern , the network must quickly read the intention of the changed pattern and modify its generation in a pixel level . this active recognition and generation task of imitative synchronization",
    "can be achieved by error regression . through the error regression , for recognizing the input at the current time step @xmath40 , thereby producing optimal prediction for future steps , the network utilizes past information .",
    "the network adapts its internal states by trying to imitate input history that is already known to the network .",
    "when the network produces optimal reconstruction outputs for past inputs , future prediction is also made based on the same intention the network has .",
    "this process , error regression , occurs only within the temporal window from time step @xmath41 to @xmath40 .",
    "the network first produces closed loop prediction starting from the time step @xmath41 to @xmath40 and error is calculated by comparing the actual input history and the output of the network in the same range of time .",
    "the error is then back propagated through time ( bptt ) until it reaches the first step of window at @xmath41 , which is called intention states .",
    "this scheme is shown in figure 2 in detail . in the figure",
    ", the prediction error is back propagated through the path presented as red arrows and finally reaches initial states inside the temporal window at time step @xmath41 .",
    "the initial states are then modified to produce minimum reconstruction error between closed loop generation and the actual input history within the window while maintaining weights and biases unchanged .",
    "until the prediction error decreases to produce an output pattern close to the input history , forward and backward propagation and modification of intention states are iterated multiple times inside the temporal window . when the error is low enough that the network successfully imitates input history , prediction for future frame @xmath42",
    "is computed based on modified internal states .",
    "after that , the temporal window shifts one step forward in order to cover next time steps . in this manner , the test input pattern can be recognized in an on - line manner step by step and therefore allowing active prediction as close as possible to the input pattern .",
    "an experimental dataset was designed to reveal the spatio - temporal structure of the proposed p - msrtnn .",
    "the dataset consists of whole body human movement patterns generated by following a hierarchically defined movement syntax .",
    "sub - primitives ( arms and legs ) were defined first .",
    "each whole body movement primitive was composed of these sub - primitives , with sub - primitives being shared by all movements .",
    "experiment 1 analyzed the self - organization of the spatio - temporal hierarchy as the model learned one subject s six primitive movements .",
    "after training these primitives , the network was trained with additional ( previously unlearned ) concatenations of prior - learned primitive patterns .",
    "experiment 2 examined the recognition capability of the trained model .",
    "specifically , how the robustness of recognition is dependent on variance in training exemplar patterns was tested by changing the number of subjects preparing training patterns .",
    "the dataset for this experiment consists of six whole body movement patterns .",
    "each whole body movement pattern was hierarchically generated by combining predefined sub - primitives using legs and arms .",
    "figure 3 describes the sub - primitives .",
    "there are three types of arm sub - primitives .",
    "sub - primitive 1 ( a1 ) is laterally extending arms .",
    "sub - primitive 2 ( a2 ) is vertically extending arms .",
    "sub - primitive 3 ( a3 ) is drawing a large circle with arms .",
    "in action space , these arm sub - primitives are represented as a1r ( arm sub primitive 1 , right ) , a1l(arm sub primitive 1 , left ) , a2r , a2l , a3r and a3l . leg sub - primitives appear in the second row of figure 3 .",
    "there are three types of leg sub - primitives .",
    "the first leg sub - primitive ( l1 ) is raising the right and left leg alternatively .",
    "the second leg sub - primitive ( l2 ) is standing still , moving neither leg .",
    "the third leg sub - primitive ( l3 ) is bending both legs .",
    "all are shown in figure 3 .",
    "there are a total of six whole body action primitives .",
    "their syntax is presented in table 1 ,    [ cols=\"^,^,^,^,^,^,^,^,^,^,^,^,^ \" , ]     & 0.0391 & 0.0522 +    table 3 shows the network trained under the multi - subject training condition yielded better results in imitative synchronization than the network trained under the single - subject training condition .",
    "this suggests that increased variance in training results in more robust recognition ability .",
    "this result is analogous to that of [ 11 ] which used lower dimensional data compared to the present study .    under the multi - subject condition ,",
    "identical movement primitives differed slightly from one another , and this variance was well preserved in the closed - loop output generation of dynamic images .",
    "figure 9 shows the corresponding analysis of neural activity in different layers in the network for five subjects .",
    "( a ) shows neural activations of cms in the first layer and ( b ) shows those of cms in the fourth layer . in the lower layer ( figure 9.a ) , the same primitives formed clusters . at the same time , variations of neural activation patterns corresponding to the different subjects in the same cluster are well preserved ( note the different shapes and the positions of the trajectories ) . as in the analysis of experiment 1 ,",
    "higher layer intentions represented in terms of different fixed points in ( b ) determine the corresponding cyclic activation patterns in the lower level shown in ( a ) by means of parameter bifurcation . in training with multiple subjects",
    "as shown in figure 9.b , three clusters emerge ( one for each primitive ) , and fixed points vary within these clusters ( reflecting subject variations ) .",
    "this implies that the higher level influences the lower level through the differentiation of fixed points within clusters of more or less robustly established primitives .",
    "the current paper introduced a novel dynamic neural network model for both generating and recognizing dynamic visual image patterns at the pixel level within a predictive coding framework .",
    "p - msrtnn is distinguished from other generative models in that generation and recognition are possible in one model .",
    "our simulation experiments showed that the proposed model characterized by its multiple spatio - temporal scales property can learn to generate dynamic visual images representing human movement patterns through the development of an internal spatio - temporal hierarchy .",
    "the model can also robustly recognize test movement patterns containing multiple transitions performed by unfamiliar subjects through imitative synchronization by inferring intention states with the error regression scheme applied to context units .",
    "future studies should focus on scaling the model in terms of size of frames , as well as in terms of number and complexity of movement patterns .",
    "also , applying the proposed model to the robotics domain is considered . since the model utilizes temporal hierarchy , top - down and bottom - up pathway which are also used in the human brain , this model would be beneficial to robots to act more natural and human - like way .",
    "moreover , robots with error regression scheme , which can be regarded as memory retrieving process , would be interesting .",
    "this work was supported by a national research foundation of korea ( nrf ) grant funded by the korea government ( msip ) ( no.2014r1a2a2a01005491 ) .",
    "we thank ahmadreza ahmadi for his help with the cuda program .",
    "[ 1 ] rao , r.p . and",
    "ballard , d.h . , 1999 .",
    "predictive coding in the visual cortex : a functional interpretation of some extra - classical receptive - field effects . _ nature neuroscience _ , * 2*(1 ) , pp.79 - 87 .",
    "[ 8 ] lee , h. , jung , m. and tani , j. , 2016 .",
    "characteristics of visual categorization of long - concatenated and object - directed human actions by a multiple spatio - temporal scales recurrent neural network model . _ arxiv preprint _ arxiv:1602.01921 .",
    "[ 11 ] ahamdi , a. and tani , j. , 2016 , october . towards robustness to fluctuated perceptual patterns by a deterministic predictive coding model in a task of imitative synchronization with human movement patterns . in international conference on neural information processing ( pp .",
    "393 - 402 ) .",
    "springer international publishing .",
    "[ 12 ] xingjian , s.h.i .",
    ", chen , z. , wang , h. , yeung , d.y . , wong , w.k . and woo , w.c .",
    "convolutional lstm network : a machine learning approach for precipitation nowcasting . in advances in neural information processing systems ( pp .",
    "802 - 810 ) .",
    "[ 15 ] zeiler , m.d . ,",
    "taylor , g.w . and fergus , r. , 2011 , november .",
    "adaptive deconvolutional networks for mid and high level feature learning . in 2011 international conference on computer vision ( pp .",
    "2018 - 2025 ) . ieee ."
  ],
  "abstract_text": [
    "<S> the current paper presents a novel recurrent neural network model , predictive multiple spatio - temporal scales rnn ( p - mstrnn ) , which can generate as well as recognize dynamic visual patterns in a predictive coding framework . </S>",
    "<S> the model is characterized by multiple spatio - temporal scales imposed on neural unit dynamics through which an adequate spatio - temporal hierarchy develops via learning from exemplars . </S>",
    "<S> the model was evaluated by conducting an experiment of learning a set of whole body human movement patterns , which was generated by following a hierarchically defined movement syntax . </S>",
    "<S> the analysis of the trained model clarifies what types of spatio - temporal hierarchy develops in dynamic neural activity as well as how robust generation and recognition of movement patterns can be achieved by using the error minimization principle . </S>"
  ]
}