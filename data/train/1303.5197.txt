{
  "article_text": [
    "dictionary - based representations proceed by approximating a signal via a linear combination of dictionary elements , referred to as atoms .",
    "sparse dictionary - based representations , where each signal involves few atoms , have been thoroughly investigated for their good properties , as they enable robust transmission ( compressed sensing @xcite ) or image in - painting @xcite . the dictionary is either given , based on the domain knowledge , or learned from the signals @xcite .",
    "the so - called sparse approximation algorithm aims at finding a sparse approximate representation of the considered signals using this dictionary , by minimizing a weighted sum of the approximation loss and the representation sparsity ( see @xcite for a survey ) .",
    "when available , prior knowledge about the application domain can also be used to guide the search toward `` plausible '' decompositions . +",
    "this paper focuses on sparse approximation enforcing a structured decomposition property , defined as follows .",
    "let the signals be structured ( e.g. being recorded in consecutive time steps ) ; the structured decomposition property then requires that the signal structure is preserved in the dictionary - based representation ( e.g. the atoms involved in the approximation of consecutive signals have `` close '' weights ) .",
    "the structured decomposition property is enforced through adding a total variation ( tv ) penalty to the minimization objective .    in the 1d case",
    ", the minimization of the above overall objective can be tackled using the fused - lasso approach first introduced in @xcite . in the case of multi - dimensional (",
    "also called multi - channel ) signals however , the minimization problem presents additional difficulties .",
    "the first contribution of the paper is to show how this problem can be handled efficiently , by extending the ( mono - dimensional ) split bregman fused - lasso solver presented in @xcite , to the multi - dimensional case .",
    "the second contribution is a comprehensive experimental study , comparing state - of - the - art algorithms to the presented approach referred to as  and establishing their relative performance depending on diverse features of the structured signals .",
    "this paper is organized as follows .",
    "the section  [ sec : prob ] introduces the formal background .",
    "the proposed optimization approach is described in section  [ sec : optim ] .",
    "section [ sec : experiments ] presents our experimental settings and reports on the results .",
    "the presented approach is discussed w.r.t .",
    "related work in section  [ sec : rel ] and the paper concludes with some perspectives for further researches .",
    "let @xmath0 \\in \\mathbb{r}^{c \\times t}$ ] be a matrix made of @xmath1 @xmath2-dimensional signals , and @xmath3 an overcomplete dictionary of @xmath4 normalized atoms ( @xmath5 ) .",
    "we consider the linear model : @xmath6 in which @xmath7 \\in \\mathbb{r}^{n \\times t}$ ] stands for the decomposition matrix and @xmath8 \\in \\mathbb{r}^{c \\times t}$ ] is a gaussian noise matrix .",
    "the sparse structured decomposition problem consists of approximating the @xmath9 , @xmath10 , by decomposing them on the dictionary @xmath11 , such that the structure of the decompositions @xmath12 reflects that of the signals @xmath9 .",
    "this goal is formalized as the minimization of the objective function : @xmath13 where @xmath14 and @xmath15 are regularization coefficients and @xmath16 encodes the signal structure ( provided by the prior knowledge ) as in @xcite . in the remainder of the paper ,",
    "the considered structure is that of the temporal ordering of the signals , _",
    "i.e. _ @xmath17 .",
    "bregman iterations have shown to be very efficient for @xmath18 regularized problems  @xcite . for convex problems with linear constraints ,",
    "the split bregman iteration technique is equivalent to the method of multipliers and the augmented lagrangian one @xcite .",
    "the iteration scheme presented in @xcite considers an augmented lagrangian formalism .",
    "we have chosen here to present ours with the initial split bregman formulation .",
    "+ first , let us restate the sparse approximation problem : @xmath19 this reformulation is a key step of the split bregman method .",
    "it decouples the three terms and allows to optimize them separately within the bregman iterations . to set - up this iteration scheme , eq .",
    "( [ eqn : base ] ) must be transform to an unconstrained problem : @xmath20    the split bregman scheme could then be expressed as  @xcite : @xmath21    thanks to the split of the three terms , the minimization of eq .",
    "( [ eqn : primal ] ) could be performed iteratively by alternatively updating variables in the system : @xmath22    only few iterations of this system are necessary for convergence . in our implementation , this update is only performed once at each iteration of the global optimization algorithm .",
    "( [ eqn : a_update ] ) and eq .",
    "( [ eqn : b_update ] ) could be resolved with the soft - thresholding operator : @xmath23    solving eq .",
    "( [ eqn : theta_update ] ) requires the minimization of a convex differentiable function which can be performed via classical optimization methods .",
    "we propose here to solve it deterministically .",
    "the main difficulty in extending @xcite to the  signals case rely on this step .",
    "let us define @xmath24 from eq .",
    "( [ eqn : theta_update ] ) such as : @xmath25 differentiating this expression with respect to @xmath26 yields : @xmath27 where @xmath28 is the identity matrix .",
    "the minimum @xmath29 of eq .  (",
    "[ eqn : theta_update ] ) is obtained by solving @xmath30 which is a sylvester equation : @xmath31 with @xmath32 , @xmath33 and @xmath34 .",
    "fortunately , in our case , @xmath35 and @xmath36 are real symmetric matrices .",
    "thus , they can be diagonalized as follows : @xmath37 where @xmath38 and @xmath39 are orthogonal matrices .",
    "( [ eqn : syl ] ) becomes : @xmath40 with @xmath41 and @xmath42 .",
    "+ @xmath43 is then obtained by : @xmath44 where the notation @xmath45 indices the column @xmath46 of matrices .",
    "going back to @xmath47 could be performed with : @xmath48 .",
    "+ @xmath35 and @xmath36 being independent of the iteration ( @xmath49 ) considered , their diagonalizations are done only once and for all as well as the computation of the terms @xmath50 , @xmath51 .",
    "thus , this update does not require heavy computations .",
    "the full algorithm is summarized below .",
    "inputs : @xmath52 , @xmath11 , @xmath16 .",
    "parameters : @xmath14 , @xmath15 , @xmath53 , @xmath54 , @xmath55 , @xmath56 , @xmath57    init @xmath58 , @xmath59 , @xmath60 and set @xmath61 ,   @xmath62 , @xmath32 and @xmath63 .",
    "compute @xmath64 , @xmath65 , @xmath38 and @xmath39 from @xmath35 and @xmath36 .",
    "precompute ( @xmath66 ) , @xmath67 .",
    "@xmath68 @xmath69 @xmath70 ; @xmath71 ; @xmath72 @xmath73 @xmath74 @xmath75 @xmath76 @xmath77 @xmath78 ; @xmath79 ; @xmath80 @xmath81 @xmath82 @xmath83",
    "the following experiment aims at assessing the efficiency of our approach in decomposing signals built with particular regularities . we compare it both to algorithms coding each signal separately , the orthogonal matching pursuit ( omp ) @xcite and the lars @xcite ( a lasso solver ) , and to methods performing the decomposition simultaneously , the simultaneous omp ( somp ) and an proximal method solving the group - lasso problem ( fista  @xcite ) .      from a fixed random overcomplete dictionary @xmath11 , a set of @xmath84 signals having piecewise constant structures have been created .",
    "each signal @xmath52 is synthesized from the dictionary @xmath11 and a built decomposition matrix @xmath26 with @xmath85 + the tv penalization of the fused - lasso regularization makes him more suitable to deal with data having abrupt changes .",
    "thus , the decomposition matrices of signals have been built as linear combinations of specific activities which have been generated as follows : @xmath86 where @xmath87 , @xmath88 is the heaviside function , @xmath89 is the index of an atom , @xmath90 is the center of the activity and @xmath91 its duration .",
    "each decomposition matrix @xmath26 could then be written : @xmath92 where @xmath93 is the number of activities appearing in one signal and the @xmath94 stand for the activation weights .",
    "an example of such signal is given in the figure [ fig : data_synth ] below .",
    "+    [ cols= \" < ,",
    "< , < \" , ]     dictionaries have been randomly generated using gaussian independent distributions on individual elements and present low coherence .      in order to evaluate the proposed algorithm , for each point",
    "@xmath95 in the above grid of parameters , the mean ( among test signals ) of the previously defined distance @xmath96 has been computed for each method and compared to the mean obtained by the . a paired t - test ( @xmath97 )",
    "has then been performed to check the significance of these differences .",
    "+ results are displayed in figure  [ fig : valid ] . in the ordinate axis ,",
    "the number of patterns increases from the top to the bottom and in the abscissa axis , the duration grows from left to right .",
    "the left image displays the mean distances obtained by the .",
    "unsurprisingly , the difficulty of finding the ideal decomposition increases with the number of patterns and their durations .",
    "the middle and right figures present its performances compared to other methods by displaying the differences ( point to point ) of mean distances in grayscale .",
    "these differences are calculated such that , negative values ( darker blocks ) means that our method outperform the other one .",
    "the white diamonds correspond to non - significant differences of mean distances .",
    "results of the omp and the lars are very similar as well as those of the somp and the group - lasso solver .",
    "we only display here the matrices comparing our method to the lars and the group - lasso solver .         compared to the omp and the lars , our method obtains same results as them when only few atoms are active at the same time .",
    "it happens in our artificial signals when only few patterns have been used to create decomposition matrices and/or when the pattern durations are small . on the contrary , when many atoms are active simultaneously , the omp and lars are outperformed by the above algorithm which use inter - signal prior information to find better decompositions .",
    "+ compared to the somp and the group - lasso solver , results depend more on the duration of patterns . when patterns are long and not too numerous , theirs performances is similar to the fused - lasso one .",
    "the somp is outperformed in all other cases . on the contrary , the group - lasso solver is outperformed only when patterns have short / medium durations .",
    "the simultaneous sparse approximation of  signals has been widely studied during these last years  @xcite and numerous methods developed  @xcite .",
    "more recently , the concept of structured sparsity has considered the encoding of priors in complex regularizations  @xcite .",
    "our problem belongs to this last category with a regularization combining a classical sparsity term and a total variation one .",
    "this second term has been studied intensively for image denoising as in the rof model  @xcite .",
    "+ the combination of these terms has been introduced as the fused - lasso @xcite . despite its convexity , the two @xmath18",
    "non - differentiable terms make it difficult to solve .",
    "the initial paper  @xcite transforms it to a quadratic problem and uses standard optimization tools ( sqopt ) . increasing the number of variables , this approach can not deal with large - scale problems .",
    "a path algorithm has been developed but is limited to the particular case of the fused - lasso signal approximator  @xcite .",
    "more recently , scalable approaches based on proximal sub - gradient methods @xcite , admm @xcite and split bregman iterations @xcite have been proposed for the general fused - lasso .",
    "+ to the best of our knowledge , the  fused - lasso in the context of overcomplete representations has never been studied .",
    "the closest work we found considers a problem of multi - task regression  @xcite .",
    "the final paper had been published under a different title  @xcite and proposes a new method based on the approximation of the fused - lasso tv  penalty by a smooth convex function as described in  @xcite .",
    "this paper has shown the efficiency of the proposed  based on a split bregman approach , in order to achieve the sparse structured approximation of multi - dimensional signals , under general conditions .",
    "specifically , the extensive validation has considered different regimes in terms of the signal complexity and dynamicity ( number of patterns simultaneously involved and average duration thereof ) , and it has established a _ relative competence map _ of the proposed  approach comparatively to the state of the art .",
    "further work will apply the approach to the motivating application domain , namely the representation of eeg signals ."
  ],
  "abstract_text": [
    "<S> the paper focuses on the sparse approximation of signals using overcomplete representations , such that it preserves the ( prior ) structure of multi - dimensional signals . </S>",
    "<S> the underlying optimization problem is tackled using a multi - dimensional split bregman optimization approach . </S>",
    "<S> an extensive empirical evaluation shows how the proposed approach compares to the state of the art depending on the signal features . </S>",
    "<S> +    sparse approximation , regularization , fused - lasso , split bregman , multidimensional signals . </S>"
  ]
}