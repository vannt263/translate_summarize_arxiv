{
  "article_text": [
    "the selection of a model from a set of fitted candidate models requires objective data - driven criteria .",
    "one such criterion often used in practice is akaike s information criterion ( aic ) , which was designed to be an asymptotically unbiased estimator of the expected kullback - leibler information of a fitted model @xcite . in finite samples",
    ", aic has a non - vanishing bias that depends on the number of fitted parameters .",
    "this limits its effectiveness as a model selection criterion , particularly in instances where the sample size is not much larger than the number of fitted parameters of the most complex candidate model . for such instances , hurvich and",
    "tsai @xcite extended the bias - corrected aic ( aicc ) originally suggested by sugiura @xcite for linear regression models , to non - linear regression models and autoregressive models .",
    "also , hurvich and tsai @xcite demonstrated the small - sample superiority of aicc over aic as a model selection criterion .",
    "since then , aicc has been extended to many other models , such as autoregressive moving average models @xcite , vector autoregressive models @xcite and multivariate linear regression models @xcite .    the objective of this work is to define aicc for seemingly unrelated regressions models",
    ". these are models of multiple response variables that follow a joint distribution @xcite .",
    "in contrast to the multivariate linear regression model of ref .",
    "@xcite , the response variables of a seemingly unrelated regressions model do not need to depend on the same covariates .",
    "seemingly unrelated regressions models play a central role in econometrics @xcite but also appear in other contexts @xcite .",
    "the remainder of this paper is organized as follows . in sec .",
    "[ aicaicc ] , the bias of aic is calculated in seemingly unrelated regressions models with the assumption that the candidate model is either correctly specified or overspecified .",
    "the same assumption is required for aic to be asymptotically unbiased @xcite and has been used to calculate its bias in finite samples in other models @xcite .",
    "expanded in inverse powers of the sample size @xmath0 , the bias of aic ( @xmath1 ) takes the form @xmath2 , where the positive coefficient @xmath3 depends on the unknown true @xmath4 covariance matrix @xmath5 of the @xmath6 response variables . in sec .",
    "[ aicaicc ] , a lower bound @xmath7 of @xmath8 , where the minimization is over all @xmath4 symmetric positive definite matrices @xmath9 , is found in terms of the number of fitted parameters and aicc is defined as @xmath10 .",
    "the performance of aicc as a model selection criterion is simulated in sec .",
    "[ simulation ] and compared to that of aic and the bayesian information criterion ( bic ) of schwarz @xcite . finally , we give some concluding remarks in sec .",
    "[ discussion ] .",
    "details about the calculation of @xmath11 , its lower bound @xmath12 and the simulation study are given in , respectively , appendices [ appendixa ] , [ appendixb ] and [ appendixc ] . appendix [ appendixc ] also holds additional simulation results .",
    "we consider the seemingly unrelated regressions model @xmath13 here , @xmath14 is an @xmath15 matrix of @xmath6 response variables on @xmath0 subjects , @xmath16 is a known @xmath17 matrix of @xmath0 values of @xmath18 covariates , each row of the @xmath15 matrix @xmath19 has independent @xmath20 distribution , and @xmath21 is an @xmath22 matrix holding @xmath23 regression coefficients and @xmath24 zeroes .",
    "the restriction @xmath25 means that response variable @xmath26 of the @xmath27-th column of @xmath14 does not depend on covariate @xmath28 of the @xmath29-th column of @xmath16 .",
    "the entries of the elements of the @xmath27-th column of @xmath21 that are not restricted to zero , are collected in the set @xmath30 .",
    "each column of the matrix @xmath21 holds at least one regression coefficient , which means that @xmath30 is non - empty for all @xmath27 . throughout this work ,",
    "we assume that the @xmath31 matrix @xmath32 is positive definite , that @xmath6 and @xmath18 do not scale with @xmath0 , and that @xmath33 is finite and positive definite .",
    "suppose that @xmath14 is not generated by the model of eq .",
    "( [ model ] ) , but by the model @xmath34 here , @xmath35 is an @xmath36 matrix of @xmath0 values of @xmath37 unknown true covariates , @xmath38 is an @xmath39 matrix of unknown coefficients and each row of the @xmath15 matrix @xmath40 has independent @xmath41 distribution with unknown covariance matrix @xmath5 .",
    "the entries of the non - vanishing elements of the @xmath27-th column of @xmath38 are collected in the set @xmath42 .",
    "a measure of the discrepancy between the candidate ( or approximating ) model of eq .",
    "( [ model ] ) and the data - generating model of eq .",
    "( [ operating ] ) is the kullback - leibler information @xmath43 where @xmath44 denotes expectation under the data - generating model and @xmath45 is the log - likelihood function of the candidate model , @xmath46 aic is an estimator of the expected kullback - leibler information @xmath47 , where @xmath48 and @xmath49 are the maximum likelihood estimators of , respectively , @xmath21 and @xmath50 .",
    "it is defined as the sum of @xmath51 and twice the number of fitted parameters , @xmath52    in appendix [ appendixa ] , with the assumption that the candidate model is either correctly specified or overspecified ( @xmath53 and @xmath54 for all @xmath29 ) , we demonstrate that @xmath55 where @xmath3 takes the form @xmath56 here , the @xmath57 oblique projection matrix @xmath58 is given by @xmath59 where @xmath60 is an @xmath61 block - diagonal matrix of @xmath6 blocks of @xmath62 matrices @xmath63 holding the @xmath64 columns of @xmath16 corresponding to @xmath65 with @xmath66 , @xmath67 in eq .",
    "( [ bias ] ) , the operators ` @xmath68 ' and ` @xmath69 ' denote partial traces over , respectively , the @xmath0 subjects and the @xmath6 response variables . given an @xmath57 matrix @xmath70 , @xmath71 is the @xmath4 matrix defined componentwisely as @xmath72 , where @xmath73 is multi - index notation for @xmath74 .",
    "similarly , @xmath75 is the @xmath76 matrix with elements @xmath77 .",
    "finally , in eq .",
    "( [ bias ] ) , ` @xmath78 ' denotes the partial transpose of subjects : @xmath79 .",
    "if @xmath80 for all @xmath29 and @xmath27 , @xmath11 collapses to @xmath81 which equals the coefficient of the first term of the expansion of @xmath82 of ref .",
    "@xcite in inverse powers of @xmath0 . in appendix",
    "[ appendixb ] , we demonstrate that @xmath83 where the minimization is over all @xmath4 symmetric positive definite matrices @xmath9 and the equality sign is attained if and only if @xmath80 for all @xmath29 and @xmath27 . we define aicc as @xmath84 because @xmath85 , @xmath86 satisfies @xmath87",
    "we compare the performance of aic , aicc and bic in the selection of seemingly unrelated regressions models . for this purpose ,",
    "1000 samples of sizes @xmath88 , @xmath89 and @xmath90 are created from the data - generating model ( [ operating ] ) with @xmath91 . for each sample and each criterion , the fitted candidate model with the smallest value of the criterion",
    "is selected from a set of candidate models .",
    "the matrix @xmath16 holds the values of 10 covariates and its @xmath92 elements are fixed after drawing them independently from @xmath93 .",
    "we consider 25 candidate models specified by @xmath94 and @xmath95 , where @xmath29 and @xmath27 are integers ranging from 1 to 5 . for the data - generating model",
    ", we set @xmath53 and take @xmath96 and @xmath97 .",
    "the 4 non - vanishing elements of @xmath38 equal unity and the covariance matrix @xmath5 has parametrization @xmath98 , where @xmath99 is the @xmath4 matrix of ones and @xmath100 .",
    "the samples are constructed based on 1000 independent drawings of @xmath40 , where each row of @xmath40 is independently drawn from @xmath41 .",
    "the candidate models are fitted with the constrained maximization ( cm ) algorithm @xcite : @xmath101 here , @xmath102 and @xmath103 are estimators of , respectively , @xmath50 and @xmath21 , @xmath104 is a positive integer and ` @xmath105 ' is the column - wise vectorization operator .",
    "the algorithm is started with @xmath106 and terminated if @xmath107 , with @xmath108 .",
    "if the log - likelihood function @xmath45 is globally concave , then @xmath102 and @xmath103 converge to , respectively , @xmath49 and @xmath48 and the numerical error of @xmath109 is of the order of magnitude of @xmath110 .",
    "if @xmath45 is multi - modal , the cm algorithm does not necessarily converge to the global maximum , but may end up in a local maximum or a saddle point @xcite .",
    "although multi - modality is rare , we choose several other initial estimators @xmath111 and calculate @xmath109 with a numerical error of about @xmath112 ( see appendix [ appendixc ] for details ) .",
    "this means that the difference between two values of a criterion has a numerical error of @xmath113 .",
    "the frequencies of selecting the 25 candidate models with the three criteria are given in table [ modelselection1 ] for @xmath114 and @xmath88 .",
    "the correct model ( @xmath115 ) is more often selected with aicc than with aic and bic . to see how the improvement of aicc on aic is related to the bias correction ,",
    "we have plotted @xmath116 , @xmath117 and @xmath118 as a function of @xmath29 ( with @xmath119 ) in fig .",
    "[ biasplot ] .",
    "the expected kullback - leibler information has a minimum at @xmath120 and increases rapidly with @xmath29 for @xmath121 .",
    "this increase is more precisely followed by @xmath117 than by @xmath118 , which explains why aic more often selects models that are too complex . in appendix",
    "[ appendixc ] , the frequencies of selecting the correct model with the three criteria are given for @xmath122 and @xmath123 .",
    "the frequencies do not depend much on @xmath124 and , as expected , the improvement of aicc on aic decreases as @xmath0 increases . for @xmath89 , aicc and bic perform equally well , while for @xmath90 , the asymptotically consistent bic outperforms aicc . in appendix [ appendixc ] , we also demonstrate that @xmath110 is sufficiently small and that the results of table [ modelselection1 ] are not affected by numerical errors .",
    "akaike , h. ( 1973 ) . information theory and an extension of the maximum likelihood principle . in _",
    "2nd international symposium on information theory _",
    "b. n. petrov and f. csaki , pp .",
    "267 - 281 .",
    "budapest : akademia kiado ."
  ],
  "abstract_text": [
    "<S> a bias correction to akaike s information criterion ( aic ) is derived for seemingly unrelated regressions models . </S>",
    "<S> the correction is of particular use when the sample size is not much larger than the number of fitted parameters . </S>",
    "<S> a small - sample simulation study indicates that the bias - corrected aic ( aicc ) provides better model choices than other model selection criteria . </S>"
  ]
}