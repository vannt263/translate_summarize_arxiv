{
  "article_text": [
    "in popular monte carlo simulation algorithms for qcd and other similar quantum field theories the main difficulty is the evaluation of the determinant of the fermion action matrix .",
    "this can be achieved by stochastic procedures with the help of auxiliary bosonic `` pseudofermion '' fields .    in the _ two - step multi - bosonic ( tsmb )",
    "algorithm _",
    "@xcite an approximation of the fermion determinant is achieved by the pseudofermion fields corresponding to a polynomial approximation of some negative power @xmath0 of the fermion matrix  @xcite .",
    "the auxiliary bosonic fields are updated according to the _ multi - bosonic action _",
    "the error of the polynomial approximation is corrected in a global accept - reject decision by using better polynomial approximations .",
    "sometimes a reweighting of gauge configurations in the evaluation of expectation values is also necessary .",
    "this can also be performed by high order polynomials .",
    "the polynomials used in the tsmb algorithm have to approximate the function @xmath1 in some non - negative interval @xmath2,\\ ; 0 \\leq \\epsilon < \\lambda$ ] . here",
    "@xmath3 is a known polynomial , typically another cruder approximation of @xmath0 .",
    "the approximation scheme and optimization procedure can be chosen differently .",
    "the least - squares optimization @xcite is an efficient and flexible possibility .",
    "( for other approximation schemes see @xcite . )    in this review the basic relations for least - squares optimized polynomials are presented as introduced in @xcite .",
    "particular attention is paid to a recurrence scheme which can be applied for determining the necessary high order polynomials and for evaluating them numerically .",
    "the details of the tsmb algorithm will not be considered . for a comprehensive summary and references",
    "see @xcite .",
    "for experience on the application of tsmb in a recent large scale numerical simulation see @xcite .",
    "least - squares optimization provides a general and flexible framework for obtaining the necessary optimized polynomials in multi - bosonic fermion algorithms . here",
    "we introduce the basic formulae in the way it has been done in @xcite .",
    "we want to approximate the real function @xmath4 in the interval @xmath2 $ ] by a polynomial @xmath5 of degree @xmath6 .",
    "the aim is to minimize the deviation norm @xmath7 ^ 2 \\right\\}^{\\frac{1}{2}}\\ .\\ ] ] here @xmath8 is an arbitrary real weight function and the overall normalization factor @xmath9 can be chosen by convenience , for instance , as @xmath10 a typical example of functions to be approximated is @xmath11 with @xmath12 and some polynomial @xmath3 .",
    "the interval is usually such that @xmath13 . for optimizing the relative deviation",
    "one takes a weight function @xmath14 .",
    "@xmath15 is a quadratic form in the coefficients of the polynomial which can be straightforwardly minimized .",
    "let us now consider , for simplicity , only the relative deviation from the simple function @xmath16 .",
    "let us denote the polynomial corresponding to the minimum of @xmath17 by @xmath18 performing the integral in @xmath15 term by term we obtain @xmath19 where @xmath20 @xmath21 the coefficients of the polynomial corresponding to the minimum of @xmath15 , or of @xmath17 , are @xmath22 the value at the minimum is @xmath23 the solution of the quadratic optimization in ( [ eq_06])-([eq_07 ] ) gives in principle a simple way to find the required least - squares optimized polynomials .",
    "the practical problem is , however , that the matrix @xmath24 is not well conditioned because it has eigenvalues with very different magnitudes . in order to illustrate this",
    "let us consider the special case @xmath25 with @xmath26 . in this case",
    "the eigenvalues are : @xmath27   & & 0.7698917100e-8 \\ , \\hspace{1.0em } 0.3571195735e-6 \\ , \\hspace{1.0em } 0.1211873623e-4 \\ , \\nonumber \\\\[0.5em ] & & 0.3120413130e-3 \\ , \\hspace{1.0em } 0.6249495675e-2 \\ , \\hspace{1.0em } 0.9849331094e-1 \\ , \\nonumber \\\\[0.5em ] & & 1.075807246   \\ .\\end{aligned}\\ ] ] a numerical investigation shows that , in general , the ratio of maximal to minimal eigenvalues is of the order of @xmath28 .",
    "it is obvious from the structure of @xmath24 in ( [ eq_05 ] ) that a rescaling of the interval @xmath29 $ ] does not help .",
    "the large differences in magnitude of the eigenvalues implies through ( [ eq_06 ] ) large differences of magnitude in the coefficients @xmath30 and therefore the numerical evaluation of the optimal polynomial @xmath5 for large @xmath6 is non - trivial .",
    "let us now return to the general case with arbitrary function @xmath4 and weight @xmath8 .",
    "it is very useful to introduce orthogonal polynomials @xmath31 satisfying @xmath32 and expand the polynomial @xmath5 in terms of them : @xmath33 besides the normalization factor @xmath34 let us also introduce , for later purposes , the integrals @xmath35 and @xmath36 by @xmath37 p_\\nu & \\equiv & \\int_\\epsilon^\\lambda dx\\ , w(x)^2 \\phi_\\nu(x)^2 x \\ , \\nonumber \\\\[0.5em ] s_\\nu & \\equiv & \\int_\\epsilon^\\lambda dx\\ , w(x)^2 x^\\nu \\ .\\end{aligned}\\ ] ] it can be easily shown that the expansion coefficients @xmath38 minimizing @xmath17 are independent of @xmath6 and are given by @xmath39 where @xmath40 the minimal value of @xmath15 is @xmath41 rescaling the variable @xmath42 by @xmath43 allows for considering only standard intervals , say @xmath44 $ ] .",
    "the scaling properties of the optimized polynomials can be easily obtained from the definitions .",
    "let us now again consider the simple function @xmath45 and relative deviation with @xmath46 when the rescaling relations are : @xmath47 p_n(\\alpha;\\epsilon\\rho,\\lambda\\rho;x ) & = & \\rho^{-\\alpha } p_n(\\alpha;\\epsilon,\\lambda;x/\\rho ) \\",
    ", \\nonumber \\\\[0.7em ] c_{n\\nu}(\\alpha;\\epsilon\\rho,\\lambda\\rho ) & = & \\rho^{\\nu - n-\\alpha } c_{n\\nu}(\\alpha;\\epsilon,\\lambda ) \\ .\\end{aligned}\\ ] ] in applications to multi - bosonic algorithms for fermions the decomposition of the optimized polynomials as a product of root - factors is needed .",
    "this can be written as @xmath48 \\ .\\ ] ] the rescaling properties here are : @xmath49 r_{nj}(\\alpha;\\epsilon\\rho,\\lambda\\rho ) & = & \\rho\\ , r_{nj}(\\alpha;\\epsilon,\\lambda ) \\ .\\end{aligned}\\ ] ] the root - factorized form ( [ eq_16 ] ) can also be used for the numerical evaluation of the polynomials with matrix arguments if a suitable optimization of the ordering of roots is performed  @xcite .",
    "the above orthogonal polynomials satisfy three - term recurrence relations which are very useful for numerical evaluation .",
    "in fact , at large @xmath6 the recursive evaluation of the polynomials is numerically more stable than the evaluation with root factors . for general @xmath4 and @xmath8 ,",
    "the first two ortogonal polynomials with @xmath50 are given by @xmath51 the higher order polynomials @xmath52 for @xmath53 can be obtained from the recurrence relation @xmath54 where the recurrence coefficients are given by @xmath55 defining the polynomial coefficients @xmath56 by @xmath57",
    "the above recurrence relations imply the normalization convention @xmath58 the rescaling relations for the orthogonal polynomials easily follow from the definitions . for the simple function @xmath45 and relative deviation with @xmath46 we have @xmath59 for the quantities introduced in ( [ eq_11 ] )",
    "this implies @xmath60 p_\\nu(\\alpha;\\rho\\epsilon,\\rho\\lambda ) & = & \\rho^{2\\alpha+2 + 2\\nu}\\ , p_\\nu(\\alpha;\\epsilon,\\lambda ) \\",
    ", \\nonumber \\\\[0.7em ] s_\\nu(\\alpha;\\rho\\epsilon,\\rho\\lambda ) & = & \\rho^{2\\alpha+1+\\nu}\\ , s_\\nu(\\alpha;\\epsilon,\\lambda ) \\ .\\end{aligned}\\ ] ] for the expansion coefficients defined in ( [ eq_12])-([eq_13 ] ) one obtains @xmath61 d_\\nu(\\alpha;\\rho\\epsilon,\\rho\\lambda ) & = & \\rho^{-\\alpha-\\nu}\\ , d_\\nu(\\alpha;\\epsilon,\\lambda ) \\ , \\end{aligned}\\ ] ] and the recurrence coefficients in ( [ eq_19])-([eq_20 ] ) satisfy @xmath62 \\gamma_{\\mu-1}(\\alpha;\\rho\\epsilon,\\rho\\lambda ) & = & \\rho^2\\ , \\gamma_{\\mu-1}(\\alpha;\\epsilon,\\lambda ) \\ .\\end{aligned}\\ ] ] for general intervals @xmath29 $ ] and/or functions @xmath63 the orthogonal polynomials and expansion coefficients have to be determined numerically . in some special cases , however ,",
    "the polynomials can be related to some well know ones .",
    "an example is the weight factor @xmath64 taking , for instance , @xmath65 this weight is similar to the one for relative deviation from the function @xmath45 , which would be just @xmath66 . in fact , for @xmath67 these are exactly the same and for small @xmath68 the difference is negligible .",
    "the corresponding orthogonal polynomials are simply related to the jacobi polynomials @xcite , namely @xmath69 comparing different approximations with different @xmath70 the best choice is usually @xmath65 which corresponds to optimizing the relative deviation ( see the appendix of @xcite ) .    for large condition numbers @xmath71",
    "least - squares optimization is much better than the chebyshev approximation used for the approximation of @xmath72 in @xcite .",
    "the chebyshev approximation is minimizing the maximum of the relative deviation @xmath73 for the deviation norm @xmath74 } |r(x)|\\ ] ] the least - squares approximation is slightly worse than the chebyshev approximation .",
    "an example is shown by fig .  [ chebyfig ] .",
    "in the left lower corner the chebyshev approximation has @xmath75 compared to @xmath76 for the least - squares optimization . for smaller condition numbers the chebyshev approximation is not as bad as is shown by fig .",
    "[ chebyfig ] .",
    "nevertheless , in qcd simulations in sufficiently large volumes the condition number is of the order of the light quark mass squared in lattice units which can be as large as @xmath77 .",
    "figure  [ chebyfig ] also shows that the least - squares optimization is quite good in the minimax norm in ( [ eq_30 ] ) , too .",
    "it can be proven that @xmath78}\\,|ro(x)| =   |ro(\\epsilon)| \\ .\\ ] ] therefore the least squares - optimization is also well suited for controlling the minimax norm , if for some reason it is required .    in qcd simulations",
    "the inverse power to be approximated ( @xmath79 ) is related to the number of dirac fermion flavours : @xmath80 .",
    "if only @xmath81- and @xmath82-quarks are considered we have @xmath83 and the function to be approximated is @xmath72 .",
    "the dependence of the ( squared ) least - squares norm in ( [ eq_01 ] ) on the polynomial order @xmath6 is shown by fig .",
    "[ alpha=1fig ] for different values of the condition number @xmath71 .",
    "the dependence on @xmath80 is illustrated by fig .",
    "[ 10 ^ 5fig ] .",
    "another possible application of least - squares optimized polynomials is the numerical evaluation of the zero mass lattice action proposed by neuberger  @xcite .",
    "if one takes , for instance , the weight factor in ( [ eq_27 ] ) corresponding to the relative deviation , then the function @xmath84 has to be expanded in the jacobi polynomials @xmath85 .",
    "the expansion in orthogonal polynomials is very useful because it allows for a numerically stable evaluation of the least - squares optimized polynomials by the recurrence relation ( [ eq_19 ] ) .",
    "the orthogonal polynomials themselves can also be determined recursively .",
    "a recurrence scheme for obtaining the recurrence coefficients @xmath86 and expansion coefficients @xmath87 has been given in @xcite . in order to obtain @xmath88 contained in ( [ eq_20 ] )",
    "one can use the relations @xmath89 the coefficients themselves can be calculated from @xmath90 and ( [ eq_19 ] ) which gives @xmath91 the orthogonal polynomial and recurrence coefficients are recursively determined by ( [ eq_20 ] ) and ( [ eq_33])-([eq_34 ] ) .",
    "the expansion coefficients for the optimized polynomial @xmath5 can be obtained from @xmath92 the ingredients needed for this recursion are the basic integrals @xmath36 defined in ( [ eq_11 ] ) and @xmath93 the recurrence scheme based on the coefficients of the orthogonal polynomials @xmath94 in ( [ eq_34 ] ) is not optimal for large orders @xmath6 , neither for arithmetics nor for storage requirements . a better scheme can be built up on the basis of the integrals @xmath95 & & \\mu=0,1,\\ldots , n ; \\hspace{1.5em } \\nu=\\mu,\\mu+1,\\ldots,2n-\\mu \\ .\\end{aligned}\\ ] ] the recurrence coefficients @xmath96 can be expressed from @xmath97 and eq .",
    "( [ eq_20 ] ) as @xmath98 it follows from the definition that @xmath99 r_{1\\nu } & = & \\int_\\epsilon^\\lambda dx\\ , w(x)^2   ( x^{\\nu+1 } + f_{11}x^\\nu ) = s_{\\nu+1 } + f_{11}s_\\nu \\ .\\end{aligned}\\ ] ] the recurrence relation ( [ eq_19 ] ) for the orthogonal polynomials implies @xmath100 this has to be supplemented by @xmath101 and by the first equation from ( [ eq_34 ] ) : @xmath102 eqs .",
    "( [ eq_39])-([eq_43 ] ) define a complete recurrence scheme for determining the orthogonal polynomials @xmath52 .",
    "the moments @xmath36 of the integration measure defined in ( [ eq_11 ] ) serve as the basic input in this scheme .",
    "the integrals @xmath103 in ( [ eq_13 ] ) , which are necessary for the expansion coefficients @xmath104 in ( [ eq_12 ] ) , can also be calculated in a similar scheme built up on the integrals @xmath105 & & \\mu=0,1,\\ldots , n ; \\hspace{1.5em } \\nu=0,1,\\ldots , n-\\mu \\ .\\end{aligned}\\ ] ] the relations corresponding to ( [ eq_40])-([eq_41 ] ) are now @xmath106 & & b_{1\\nu } = \\int_\\epsilon^\\lambda dx\\ , w(x)^2 f(x )   ( x^{\\nu+1 } + f_{11}x^\\nu ) = t_{\\nu+1 } + f_{11}t_\\nu \\ , \\nonumber \\\\[0.5em ] & & b_{\\mu+1,\\nu } = b_{\\mu,\\nu+1 } + \\beta_\\mu b_{\\mu\\nu } + \\gamma_{\\mu-1}b_{\\mu-1,\\nu } \\ .\\end{aligned}\\ ] ] the only difference compared to ( [ eq_40])-([eq_41 ] ) is that the moments of @xmath107 are now replaced by the ones of @xmath108 .",
    "it is interesting to collect the quantities which have to be stored in order that the recurrence can be resumed .",
    "this is useful if after stopping the iterations , for some reason , the recurrence will be restarted .",
    "let us assume that the quantities @xmath109 , @xmath110 , @xmath111 and @xmath112 are already known and one wants to resume the recurrence in order to calculate these quantities for higher indices .",
    "for this it is enough to know the values of @xmath113 & & r^{(0)}_{0 \\ldots n } \\equiv   ( r_{0,2n+1},r_{1,2n},\\ldots , r_{n , n+1 } ) \\",
    ", \\nonumber \\\\[0.5em ] & & r^{(1)}_{0 \\ldots n } \\equiv   ( r_{0,2n},r_{1,2n-1},\\ldots , r_{n , n } ) \\ , \\nonumber \\\\[0.5em ] & & b^{(1)}_{0 \\ldots n } \\equiv   ( b_{0,n},b_{1,n-1},\\ldots , b_{n,0 } ) \\",
    ", \\nonumber \\\\[0.5em ] & & b^{(2)}_{0 \\ldots n-1 } \\equiv   ( b_{0,n-1},b_{1,n-2},\\ldots , b_{n-1,0 } ) \\ .\\end{aligned}\\ ] ] this shows that for maintaining a _ resumable recurrence _ it is enough to store a set of quantities linearly increasing in @xmath6 .",
    "an interesting question is the increase of computational load as a function of the highest required order @xmath6 . at the first sight this seems to go just like @xmath114 , which is surprising because , as eq .",
    "( [ eq_06 ] ) shows , finding the minimum requires the inversion of an @xmath115 matrix .",
    "however , numerical experience shows that the number of required digits for obtaining a precise result does also increase linearly with @xmath6 .",
    "this is due to the linearly increasing logarithmic range of eigenvalues , as illustrated by ( [ eq_08 ] ) .",
    "using , for instance , maple v for the arbitrary precision arithmetic , the computation slows down by another factor going roughly as ( but somewhat slower than ) @xmath114 .",
    "therefore , the total slowing down in @xmath6 is proportional to @xmath116 .",
    "for the same reason the storage requirements increase by @xmath114 .",
    "in the tsmb algorithm for monte carlo simulations of fermionic theories , besides the simple function @xmath0 , also the function @xmath117 has to be approximated . here @xmath3 is typically a lower order approximation to @xmath0 . in this case , if one chooses to optimize the relative deviation , the basic integrals defined in ( [ eq_11 ] ) and ( [ eq_36 ] ) are , respectively , @xmath118 t_\\nu & = & \\int_\\epsilon^\\lambda dx\\ , \\bar{p}(x ) x^{\\alpha+\\nu } \\ .\\end{aligned}\\ ] ] it is obvious that , if the recurrence coefficients for the expansion of the polynomial @xmath3 in orthogonal polynomials are known , the recursion scheme can also be used for the evaluation of @xmath36 and @xmath119 .",
    "another observation is that the integrals in ( [ eq_47 ] ) can be simplifyed if , instead of choosing the weight factor @xmath120 , one takes @xmath121 which leads to @xmath122 t_\\nu & = & \\int_\\epsilon^\\lambda dx\\ , x^\\nu \\ .\\end{aligned}\\ ] ] since @xmath3 is an approximation to @xmath0 , the function @xmath123 is close to one and the difference between @xmath124 and @xmath125 is small . therefore the least - squares optimized approximations with the weights @xmath126 and @xmath127 are also similar .",
    "it turns out that the second choice is , in fact , a little bit better because the largest deviation from @xmath0 typically occurs at the lower end of the interval @xmath128 where @xmath3 is smaller than @xmath0 . as a consequence , @xmath129 and @xmath130",
    "this means that choosing the weight factor in ( [ eq_48 ] ) is emphasising more the lower end of the interval where @xmath3 as an approximation of @xmath0 is worst .    in summary",
    ": least - squares optimization is a flexible and powerful tool which can serve as a basis for applying the two - step multi - bosonic algorithm for monte carlo simulations of qcd and other similar theories . with the help of the recurrence scheme described in the previous section",
    "one can determine the necessary polynomial approximations to high enough orders .",
    "99 montvay , i. : an algorithm for gluinos on the lattice .",
    "b466 * ( 1996 ) 259284 montvay , i. : quadratically optimized polynomials for fermion simulations .",
    ". commun . * 109 * ( 1998 ) 144160 lscher , m. : a new approach to the problem of dynamical quarks in numerical simulations of lattice qcd .",
    "b418 * ( 1994 ) 637648 rivlin , t.j . : _ an introduction to the approximation of functions , _ blaisdell publ .",
    "company , 1969 .",
    "wolff , u. : multiboson simulation of the schrdinger functional .",
    "* 63 * ( 1998 ) 937 - 939 de forcrand , p. : uv - filtered fermionic monte carlo .",
    "* 73 * ( 1999 ) 822 - 824 ; also see this proceedings .",
    "montvay , i. : multi - bosonic algorithms for dynamical fermion simulations .",
    "workshop on molecular dynamics on parallel computers , jlich , germany , february 1999 , to appear in the proceedings ; hep - lat/9903029 .",
    "kirchner , r. et al .",
    ": evidence for discrete chiral symmetry breaking in n=1 supersymmetric yang - mills theory .",
    "b446 * ( 1999 ) 209215 campos , i. et al .",
    ": monte carlo simulation of su(2 ) yang - mills theory with light gluinos .",
    "j. c ; doi 10.1007/s100529900183 .",
    "neuberger , h. : exactly massless quarks on the lattice .",
    "b417 * ( 1998 ) 141144 ."
  ],
  "abstract_text": [
    "<S> least - squares optimized polynomials are discussed which are needed in the two - step multi - bosonic algorithm for monte carlo simulations of quantum field theories with fermions . </S>",
    "<S> a recurrence scheme for the calculation of necessary coefficients in the recursion and for the evaluation of these polynomials is introduced . </S>"
  ]
}