{
  "article_text": [
    "the wide - field infrared survey explorer ( wise ; * ? ? ?",
    "* ) mapped the entire sky in four bands centered at wavelengths of 3.4 , 4.6 , 12 , and 22 @xmath1 m ( hereafter w1 , w2 , w3 , and w4 ) from january 7 , 2010 to february 1 , 2011 that spanned both cryogenic and post - cryogenic phases .",
    "wise conducted its survey from a sun - synchronous polar orbit using a 40 cm cryogenically cooled telescope equipped with four 1024 @xmath2 1024 infrared",
    "( ir ) array detectors that simultaneously imaged the same 47  @xmath2 47  field - of - view in all bands .",
    "the wise survey strategy alternated stepping the scan path forward and backward on subsequent orbits in an asymmetric pattern that approximately matched the @xmath3 per day orbital precession rate . in this way , each point near the _ ecliptic plane _ was observed on every other orbit , or every 191 minutes , yielding typically 12 independent exposures over one day .",
    "the number of exposures increases with ecliptic latitude , reaching over 6000 at the ecliptic poles .",
    "wise surveyed the sky approximately 1.5 times during its cryogenic phase that ended on september 29 , 2010 .",
    "data continued to be collected for another four months to support the discovery of near earth objects ( the neowise program ; * ? ? ? * ) . during this post - cryogenic phase ,",
    "70% of the sky was scanned , with only the w1 and w2 detectors returning scientifically useful data .",
    "overall , wise covered the full sky slightly more than twice , with each sky - coverage separated by approximately six months . when all the mission data are combined , the median single - exposure depth - of - coverage on the ecliptic becomes @xmath4 and the effective observation timespan ( without the six month phase shift ) @xmath5 days .",
    "the wise single - exposure source databases @xcite contain the photometry from each individual wise exposure .",
    "these offer a unique opportunity to search for variable stars over the entire sky at wavelengths that are relatively immune to dust extinction .",
    "in particular , the most common pulsational variables ( e.g. , cepheids , rr lyrae , and miras ) have served as standard candles that enabled measurements of the size scale of the milky way and the universe ( e.g. , * ? ? ?",
    "products from previous wise data releases have already had a significant impact on the calibration of the rr lyrae period - luminosity relation at mid - ir wavelengths @xcite .",
    "an all - sky census of pulsating variables offers an opportunity to improve our understanding of milky way tomography and the distribution of dark matter in the galaxy through their association with relic streams from disrupted star clusters and dwarf galaxies @xcite .",
    "pulsational variables are also crucial for understanding stellar birth , structure , mass loss , and evolution @xcite . on the other hand , variability associated with eclipsing binaries ( e.g. , algol , @xmath6 lyrae , and w ursae majoris types )",
    "provide laboratories for probing accretion , mass transfer , binary evolution , and exoplanets ( for a review , see * ? ? ?",
    "the deluge of data from current and future time - domain surveys presents an enormous challenge for human - based vetting , classification , and follow - up .",
    "fortunately , computers and efficient machine - learning ( ml ) algorithms are starting to revolutionize the taxonomic problem . a broad class of ml methods are generically referred to as `` supervised '' .",
    "these methods entail defining a set of rules or models that best describe the relationship between the properties ( features ) of a data set and some known outcomes ( e.g. , classifications ) , and then using this `` trained '' mapping to predict the outcomes for new data .",
    "on the other hand , `` unsupervised '' methods attempt to blindly identify patterns and structures amongst the features of a data set , that is , with no pre - labelled classes or outcomes to learn from . for an overview of ml methods in general ,",
    "see @xcite . for a review of ml applications in astronomy ,",
    "see @xcite .",
    "ml also provides a statistical framework to make probabilistic statements about the class(es ) that a particular object with a set of observables or features could belong to .",
    "given the same training model , these statements are also reproducible and deterministic , whereas a human - based classification approach is not .    supervised ml techniques have gained considerable popularity in the automated classification of variable stars @xcite . in particular , @xcite compared several classification frameworks for variable stars selected from multiple surveys and concluded that the _ random forest _",
    "ml classifier ( a tree - based technique popularized by * ? ? ?",
    "* ) generally performed best . due to their flexibility and robustness ,",
    "_ random forests _ have also attained popularity in the real - time discovery and classification of astronomical transients in general ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "guided by its success , we adopt the _ random forest _ method as the primary ml classifier in this study .",
    "the wise all - sky release source catalog @xcite contains approximately 8 million sources where likely flux variables were flagged using a methodology similar to that described in @xcite .",
    "we are in the process of constructing the wise variable source database ( wvsdb ) that builds upon this basic variability flagging in the catalog .",
    "this database will contain confirmed variable sources with their light curves , derived properties such as periods and amplitudes , and where appropriate , the probabilities of belonging to specific known variability classes .",
    "the first step towards the construction of the wvsdb is a framework that can automatically and probabilistically classify variables using features and diagnostics derived from the wise single - exposure time - series measurements .",
    "this paper describes the methodology behind this framework , starting with the construction of a training ( or `` truth '' ) sample leveraged on known variables classified in previous optical surveys and cross - matched to wise to define robust mid - ir light curve features for classification . in particular , we show how fourier decomposition techniques can be extended into the mid - ir to define the relevant features for discriminating between the various classes .",
    "this training sample is then used to fit and validate the popular _ random forest _ machine - learning technique to assist with the future classification of wise flux variables for the wvsdb .",
    "this paper is organized as follows . in section  [ cscheme ]",
    "we define the variability classes that wise is most sensitive to .",
    "section  [ tsampf ] describes the construction of the training sample and selection of the mid - ir light curve features .",
    "an analysis of these features for the various classes of interest is presented in section  [ fan ] .",
    "random forest _ machine - learning method for automated classification is described and evaluated in section  [ ml ] , where we also compare this method to other state - of - the - art machine - learning methods .",
    "section  [ cpred ] gives an overview of the wvsdb , the classification plan , and how a feedback mechanism based on `` active - learning '' could be used to allow for selection biases in the input training sample .",
    "results are summarized in section  [ conc ] .",
    "the wise _ full mission _ baseline and observing cadence is well - suited for studying periodic variable stars with periods of @xmath7 days , where 3 days is approximately the maximum period that can be recovered using our period estimation technique ( section  [ fgen ] ) for light curves constructed from observations within a few tens of degrees of the ecliptic .",
    "the most common variables in this category are rr lyrae ( rr lyr ) pulsational variables and algol , @xmath6 lyrae , and w ursae majoris ( w uma ) eclipsing binaries .",
    "optical surveys generally find w uma variables ( a class of contact binaries ) to be the most abundant , comprising @xmath895% of all variable stars in the solar neighborhood @xcite . despite their small variability amplitudes and relatively weak emission in the mid - infrared ,",
    "wise is sensitive to the brightest w uma variables at the high end of the amplitude distribution .",
    "@xmath6 lyrae eclipsing binaries are a class of semi - detached binary stars where one member of the pair fills the roche lobe of the other .",
    "previous optical studies have shown that @xmath6 lyrae are generally difficult to separate from algol - type ( detached ) eclipsing binaries based on light - curve shape alone @xcite .",
    "our analyses of the wise light curve features ( section  [ fan ] ) also show these variables to largely overlap with algols , and also perhaps with w uma variables to some extent .",
    "the degeneracies can only be resolved with supplementary spectral information .",
    "therefore , we are left with three broad periodic variable star classes that are best suited for wise s observing constraints : algols ( with the inclusion of many @xmath6 lyrae ) ; w uma ; and rr lyr .",
    "periodic variables that are _ not _ assigned to any of these three broad classes ( according to some probability threshold ; see section  [ cpred ] ) will be initially flagged as `` unknown '' ; for example , cepheid variables .",
    "they may be reclassified and associated with new classes ( not in the initial training sample described in section  [ tsamp ] ) if more objects with similar features are identified following a first classification pass .",
    "subsequent classification passes will use refined training samples augmented with new classes using an `` active - learning '' approach .",
    "details are given in section  [ al ] .",
    "in order to calibrate and validate a classification method for wise light curves , we assembled a `` truth '' list of variable stars that were previously classified with measured periods from a number of optical variability surveys .",
    "this list includes all eclipsing binaries and rr lyr stars in the general catalogue of variable stars ( gcvs ; * ? ? ?",
    "* ) ; the macho variable star database @xcite , and the all - sky automated survey ( asas ; * ? ? ?",
    "this list of known variables was then positionally cross - matched to the wise all - sky source catalog @xcite .",
    "a match - radius tolerance of @xmath9 was used and the brightest wise source was selected if multiple matches were present .",
    "furthermore , only sources with a wise catalog variability flag @xcite of @xmath10 in the w1 band were retained .",
    "this criterion ensured that the wise source had a relatively good time - averaged photometric signal - to - noise ratio ( with typically s / n @xmath11 in the w1 single - exposures ) and a high likelihood of also being variable in the mid - ir .",
    "after cross - matching , 1320 objects were rejected because they were labeled as either ambiguous ( assigned to two or more classes ) , uncertain , or had different classifications between the optical catalogs .",
    "a further 2234 objects had duplicate entries amongst the optical catalogs ( assigned to the same class ) and one member of each pair was retained . in the end , we are left with a training sample of 8273 known variables with wise photometry . of these 8273 objects , 1736 are rr lyr , 3598 are algol - type eclipsing binaries , and 2939 are w uma - type eclipsing binaries according to classifications reported in previous optical variability surveys .",
    "we assume here that the bulk of classifications reported in the asas , gcvs , and macho surveys ( primarily those objects labeled as algol , rr lyr , or w uma ) are correct , or accurate enough as to not significantly affect the overall core - class definitions in the wise parameter ( feature ) space .",
    "the removal of objects with ambiguous and uncertain classifications , or with discrepant classifications between the catalogs is expected to have eliminated the bulk of this uncertainty . a comparison of our classification performance to other studies that also used classifiers trained on similar optical catalogs ( section  [ cv ] ) shows that possible errors in the classification labels of the training sample are not a major source of uncertainty .",
    "however , these errors can not be ignored .",
    "there is no guarantee that the training sample described here represents an unbiased sample of all the variable types that wise can recover or discover down to fainter flux levels and lower s / n ratios over the entire sky .",
    "this training sample will be used to construct an initial classifier to first assess general classification performance ( section  [ cv ] ) . in future",
    ", this classifier will be used to initially classify wise flux - variables into the three broad variability classes defined above .",
    "inevitably , many objects will remain unclassified in this first pass . to mitigate training sample biases and allow more of the wise - feature space to mapped and characterized",
    ", we will use an active - learning framework to iteratively refine the training sample as objects are accumulated and ( re)classified . this will allow more classes to be defined as well as sharpen the boundaries of existing ones , hence alleviating possible errors in the input classification labels ( see above ) .",
    "details are discussed under our general classification plan in section  [ cpred ] .",
    "a requirement of any classification system is a link between the features used as input and the classes defined from them .",
    "we review here the seven mid - ir light curve features that we found work best at discriminating between the three broad classes that wise is most sensitive to ( defined in section  [ cscheme ] ) .",
    "these were motivated by previous variable star classification studies ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* )    we first extracted the time - resolved mid - ir photometry and constructed light curves for all sources in our training sample from the wise all - sky , 3-band , and post - cryo single - exposure source databases .",
    "light curves were constructed using only the w1 band .",
    "this is because w1 generally has better sensitivity than w2 for the variable stars under consideration . for each source",
    ", a period was estimated using the generalized lomb - scargle periodogram ( * ? ? ?",
    "* ; * ? ? ?",
    "* see section  [ fan ] for this choice ) .",
    "the light curves were phased to the highest peak in the periodogram that fell in the period range : 0.126 day ( @xmath8 3 hours ) to 10 days .",
    "the lower value corresponds to the characteristic wise single - exposure sampling and the upper limit is based on the known periods of algol - type binaries that could be recovered by wise given the typical time - span of observations , with a generous margin .",
    "these recovered periods ( @xmath12 ) constitute our first feature to assist with classification .",
    "section  [ fan ] compares these estimates to the available periods derived from optical light curves .",
    "the second feature derived from the wise light curves in our training sample is the stetson-@xmath13 variability index @xcite .",
    "this index quantifies the degree of synchronous variability between two bands . because the band w1 and w2 measurements generally have the highest s / n ratio for objects in the classes of interest , only these bands are used in the calculation of this index .",
    "the stetson-@xmath13 index is the scaled product of the stetson @xmath14 and @xmath15 indices : @xmath16 stetson-@xmath14 is a measure of the correlation between two bands ( @xmath17 , @xmath18 ; or w1 , w2 respectively ) and is defined as @xmath19 where @xmath20 is the index for each data point , @xmath21 is the total number of points , sgn(@xmath22 ) is the sign of @xmath22 , @xmath23 is the photometric magnitude of flux measurement @xmath20 in band @xmath17 , and @xmath24 is its uncertainty .",
    "the stetson-@xmath15 index is a measure of the kurtosis of the magnitude distribution and is calculated by collapsing a single - band light curve : @xmath25 for a pure sinusoid , @xmath26 , while for a gaussian magnitude distribution , @xmath27 .",
    "this is also the scaling factor in the @xmath13-index ( equation  [ stetl ] ) .",
    "our third derived feature is the magnitude ratio ( @xmath28 ; * ? ? ?",
    "this measures the fraction of time a variable star spends above or below its median magnitude and is useful for distinguishing between variability from eclipsing binaries and pulsating variables .",
    "this is computed using the magnitude measurements @xmath29 for an individual band and is defined as @xmath30 for example , if a variable star spends @xmath3150% of its time at near constant flux that only falls occasionally , @xmath32 . if its flux rises occasionally , @xmath33 .",
    "a star whose flux is more sinusoidal will have @xmath34 .",
    "the remaining four features are derived from a fourier decomposition of the w1 light curves .",
    "fourier decomposition has been shown to be a powerful tool for variable star classification @xcite . to reduce the impact of noise and outliers in the photometric measurements",
    ", we first smooth a mid - ir light curve using a local non - parametric regression fit with gaussian kernel of bandwidth ( @xmath35 ) 0.05 days .",
    "we then fit a 5@xmath36 order fourier series to the smoothed light curve @xmath37 , parameterized as @xmath38\\ ] ] where @xmath39 is the orbital phase at observation time @xmath40 relative to some reference time @xmath41 and is computed using our recovered period @xmath12 ( see above ) as @xmath42 where `` int '' denotes the integer part of the quantity and @xmath43 .",
    "the parameters that are fit in equation ( [ eq : fourier ] ) are the amplitudes @xmath44 and phases @xmath45 .",
    "the quantities that we found useful for classification ( see section  [ fan ] with some guidance from * ? ? ?",
    "* ) are the two relative phases @xmath46 @xmath47 and the absolute values of two fourier amplitudes : @xmath48 and @xmath49 .",
    "to summarize , we have a feature - vector consisting of seven metrics for each mid - ir light curve in our training sample : the recovered period @xmath12 , the magnitude ratio @xmath28 ( equation  [ mr ] ) , the stetson-@xmath13 index ( equation  [ stetl ] ) , and the four fourier parameters : @xmath48 , @xmath49 , @xmath50 , and @xmath51 .",
    "together with available class information from the literature ( the dependent variable ) , we constructed a data matrix consisting of 8273 labeled points ( `` truth '' samples ) in a seven - dimensional space .",
    "section  [ ml ] describes how this data matrix is used to train and validate a machine - learned classifier .",
    "before using our training sample to construct an automated classification algorithm , we present here a qualitative analysis of the derived light - curve features across the three different classes of interest .",
    "that is , how well they perform individually and in combination , in a qualitative sense , for discriminating between classes .",
    "the relative feature importance across ( and within ) classes will be explored in more detail using metrics generated during the classifier - training phase in section  [ import ] .",
    "the accuracy of period recovery is an important factor in the classification process , in particular since this metric is also used ( indirectly ) to derive the features from fourier decomposition . given all three target classes overlap at least partially in period , it is important to minimize any period aliasing as much as possible , that is , any inadvertent phasing of the time - series data to an integer harmonic of the true period . the generalized lomb - scargle periodogram ( gls ; * ? ? ?",
    "* ) was superior to other methods in recovering the correct period and minimizing period aliasing .",
    "the other methods we explored were the standard lomb - scargle algorithm ; phase dispersion minimization method ( pdm ; * ? ? ? * ) ; multiharmonic analysis of variance ( aov ; * ? ? ?",
    "* ) ; and the string - length algorithm @xcite .",
    "the gls method recovered the correct period for the largest number of variables and minimized period aliasing ( for mostly the rr lyr class ; see below ) .",
    "this is likely due to the relatively sparse temporal sampling of many wise sources where gls is most robust .",
    "gls also has the advantage of incorporating measurement uncertainties in the calculation , whereas many other methods do not .",
    "as shown in figure [ fig : period ] , period recovery is good for periods of less than @xmath52 days .",
    "longer periods are more difficult to recover from wise data due to the observing cadence , as is evident by the increased scatter at longer periods .",
    "nearly all the rr lyr variables are recovered at the fundamental period , while the algol and w uma variables are recovered at half the period .",
    "this separation arises from the fact that eclipsing systems usually have two minima per cycle ( the primary and seconday eclipses ) while pulsating variable stars have only one .",
    "this half - period aliasing for the periods of eclipsing binaries does not impact their classification ( or separability from pulsating variables in general ) since we find they can be reliably distinguished using other features ( see below ) .",
    "thus , once an eclipsing binary has been identified using all the available light curve features , their measured period can be doubled to recover the correct period .",
    "one should also note several alias groupings in figure [ fig : period ] , particularly for the w uma class .",
    "these are due to the sinusoidal nature of their light curves and their relatively short periods ( @xmath53 day ) where sparse - sampling of the wise data can significantly affect period recovery .",
    "each feature in our seven - dimensional feature vector is compared against every other in figure [ fig : featureplot ] for the three classes of interest in our training sample .",
    "there are 21 unique sets of feature pairs .",
    "this scatter - plot matrix provides both a qualitative sense of the degree of correlation between features as well as class separability in each two - dimensional projection .",
    "feature correlations and possible redundancies are further explored in section  [ fcor ] . the features that tend to separate the classes relatively well involve combinations of the fourier amplitudes ( @xmath48 , @xmath49 ) and relative phase parameters ( @xmath50 , @xmath51 ) , but the separation is also relatively strong in the @xmath13-index versus magnitude ratio ( @xmath28 ) plane .",
    "we expand on the details for three pairs of features below .",
    "figures [ fig : a ] and [ fig : phi ] show the distribution of the fourier amplitudes and relative phase parameters for each class . the rr lyr and w uma classes in particular appear to partially overlap in each 2-d plane formed by each pair of parameters .",
    "this is because many rr lyr , especially those of the rrc subclass , have nearly sinusoidal light curves that are very similar to some w uma variables .",
    "the periods of these two classes ( figure [ fig : period ] ) also overlap to some extent .",
    "figure [ fig : mr - l ] shows the benefit of including the magnitude ratio and stetson @xmath13-index to assist in distinguishing rr lyr from the eclipsing binary ( algol and w uma ) classes in general , which may not be achieved using the fourier amplitudes or phase parameters alone .",
    "the algol class appears to isolate itself rather well from the rr lyr and w uma classes in figures [ fig : a ] and [ fig : mr - l ] , and only from the rr lyr class in figure [ fig : phi ] .",
    "this is due to the asymmetrical nature of the algol - type light curves and the fact that their primary and secondary minima are separated by an orbital phase of 0.5 .",
    "this is common amongst algol - type eclipsing binaries since most of them have orbital eccentricities of approximately zero .",
    "this also explains why there is a tight clump centered at @xmath54 and @xmath55 in figure  [ fig : phi ] .",
    "there is however some small overlap between algols and the rr lyr and w uma classes ( which is smaller than that between the rr lyr and w uma classes ) .",
    "this primarily occurs for the shorter period algols with similar depths in their primary and secondary eclipses , indicating the component stars are similar in nature . for these , the light curves become more sinusoidal and indistinguishable from the other classes .    from this preliminary exploratory analysis of the light curve features ( using simple pairwise comparisons ) ,",
    "it is clear that we need to explore class separability using the full joint seven - dimensional feature space , with some quantitative measure for assigning class membership .",
    "this is explored in the following sections . in particular ,",
    "the relative feature importance is revisited and explored in more detail in section  [ import ] .",
    "the classes and their features defined above form the basis of a `` supervised '' classification framework . this method uses a sample of objects ( here variable sources ) with known classes to train or learn a non - parametric function ( model ) that describes the relationship between the derived features and these classes .",
    "this sample of labeled classes is referred to as the training sample .",
    "our ultimate goal is to use this model to automatically predict the most probable class of future objects from its derived features , or in general , the probability that it belongs to each of the pre - defined classes .",
    "these probabilities quantify the degree to which a particular object could belong to specific class , therefore making the classification process less subjective , or more open to interpretation and further analysis .",
    "section  [ train ] describes how these probabilities are defined .",
    "many previous studies have used machine learning ( ml ) methods to classify variable stars from their photometric time series , in particular , in large surveys capable of identifying 20 or more variability classes .",
    "the intent has been to develop a generic classifier that is accurate , fast and robust , and can be used to classify objects from surveys other than those used to construct the classifier .",
    "@xcite , @xcite , and @xcite discuss some of the challenges on this front .",
    "the main challenge here is the presence of survey - dependent systematics , for example , varying cadence , flux sensitivity , signal - to - noise ratio , number of epochs , etc .",
    "this heterogeneity introduces systematic differences in the derived light - curve features , leading to biased training models and degraded classifier performance when attempting to classify new objects .",
    "a related issue is class - misrepresentation in the training model , i.e. , where classes are inadvertently omitted because of limitations in the specific survey(s ) used to construct the training sample .",
    "this will impact classifications for other surveys whose properties may allow these additional classes to be detected . training a classifier using a subset of labeled ( pre - classified ) objects drawn from the _ same _ target sample for which bulk",
    "classifications are required ( i.e. , with similar properties ) minimizes these biases , but does not eradicate them .",
    "methods to mitigate training - sample biases for the classification of wise flux - variables are discussed in section  [ al ] .",
    "some of the ml methods used to classify variable stars include support vector machines @xcite , kohonen self - organizing maps @xcite , bayesian networks and mixture - models @xcite , principle component analysis @xcite , multivariate bayesian and gaussian mixture models @xcite for the _ kepler _ mission , and thick - pen transform methods @xcite .",
    "@xcite explored a range of methods applied to several large surveys that included _ hipparcos _ and _ ogle _ : artificial neural networks , bayesian networks , gaussian mixture models , and support vector machines .",
    "all these methods appear to achieve some level of success ; however , using the same input data , @xcite and @xcite explored the performance of tree - based classification schemes that include _ random forests _ and found these to be generally superior to the methods in @xcite in terms of accuracy , robustness to outliers , ability to capture complex structure in the feature space , and relative immunity to irrelevant and redundant features .",
    "unlike the complex heterogeneous nature of the many - class / multi - survey classification problem noted above , the good overall homogeneity of the wise survey provides us with a well - defined sample of uniformly sampled light - curves from which we can train and tune a ml classifier and use it for initial classification in future . as discussed in section  [ tsamp ] and",
    "further in section  [ al ] , these initial classifications will be used to refine the classifer and mitigate possible biases in the input training sample .",
    "below , we focus on training and validating a _ random forest _ ( rf ) classifier , then compare its performance to some other state - of - the - art methods : artificial neural networks ( nnet ) , @xmath56-nearest neighbors ( @xmath56nn ) , and support vector machines ( svm )",
    ". we do not delve into the details of these other methods , as our intent is simply to provide a cross - check with the rf classifier .",
    "ml methods based on classification and regression trees ( cart ) were popularized by @xcite .",
    "decision trees are intuitive and simple to construct .",
    "they use recursive binary partitioning of a feature space by splitting individual features at values ( or decision thresholds ) to create disjoint rectangular regions  the nodes in the tree .",
    "the tree - building process selects both the feature and threshold at which to perform a split by minimizing some measure of the inequality in the response between the two adjacent nodes ( e.g. , the fractions of objects across all _ known _ classes , irrespective of class ) .",
    "the splitting process is repeated recursively on each subregion until some terminal node - size is reached ( _ nodesize _ parameter below ) .",
    "classification trees are powerful non - parametric classifiers that can deal with complex non - linear structures and dependencies in the feature space .",
    "if the trees are sufficiently deep , they generally yield a small bias with respect to the true model that relates the feature space to the classification outcomes .",
    "unfortunately , single trees do rather poorly at prediction since they lead to a high variance , e.g. , as encountered when overfitting a model to noisy data .",
    "this is a consequence of the hierarchical structure of the tree : small differences in the top few nodes can produce a totally different tree and hence wildly different outcomes .",
    "therefore , the classic bias versus variance tradeoff problem needs to be addressed . to reduce this variance , @xcite introduced the concept of _ bagging _ ( bootstrap aggregation ) . here",
    ", many trees ( @xmath57 of them ) are built from randomly selected ( bootstrapped ) subsamples of the training set and the results are then averaged .",
    "to improve the accuracy of the final averaged model , the _ random forest _",
    "( rf ) method @xcite extends the bagging concept by injecting further randomness into the tree building process .",
    "this additional randomness comes from selecting a _ random subset _ of the input features ( @xmath58 parameter below ) to consider in the splitting ( decision ) process at each node in an individual tree .",
    "this additional randomization ensures the trees are more de - correlated prior to averaging and gives a lower variance than bagging alone , while maintaining a small bias . these details may become clearer in section  [ train ] where we outline the steps used to tune and train the rf classifier . for a more detailed overview ,",
    "we refer the interested reader to ch.15 of @xcite and @xcite .",
    "_ random forests _ are popular for classification - type problems and are used in many disciplines such as bioinformatics , earth sciences , economics , genetics , and sociology .",
    "they are relatively robust against overfitting and outliers , weakly sensitive to choices of tuning parameters , can handle a large number of features , can achieve good accuracy ( or minimal bias and variance ) , can capture complex structure in the feature space , and are relatively immune to irrelevant and redundant features .",
    "furthermore , rfs include a mechanism to assess the relative importance of each feature in a trivial manner .",
    "this is explored in section  [ import ] . for our tuning , training , and validation analyses , we use tools from the ` r ` statistical software environment . in particular , we make extensive use of the ` caret ` ( classification and regression training ) machine learning package @xcite , version 5.17 - 7 , august 2013 .      as mentioned earlier , the rf method is relatively immune to features that are correlated with any other feature or some linear combination of them , i.e. , that show some level of redundancy in `` explaining '' the overall feature space .",
    "however , it is recommended that features that are strongly correlated with others in the feature set be removed in the hope that a model s prediction accuracy can be ever slightly improved by reducing the variance from possible over - fitting .",
    "given our training classes contain a relatively large number of objects ( @xmath59 ) and our feature set consists of only seven predictor variables , we do not expect over - fitting to be an issue , particularly with rfs .",
    "our analysis of feature collinearity is purely exploratory .",
    "combined with our exploration of relative feature importance in section  [ import ] , these analyses fall under the general topic of `` feature engineering '' and are considered common practice prior to training any ml classifier .    to quantify the level of redundancy or correlation amongst our @xmath60 features , we used two methods : ( i ) computed the full pair - wise correlation matrix and ( ii ) tested for general collinearity by regressing each feature on a linear combination of the remaining @xmath61 features and examining the magnitude and significance of the fitted coefficients .",
    "figure  [ fig : corrmat ] shows the pair - wise correlation matrix where elements were computed using pearson s linear correlation coefficient @xmath62 for two features @xmath63 where cov is their sample covariance and @xmath64 are their sample standard deviations .",
    "it s important to note that this only quantifies the degree of _ linear _ dependency between any two features .",
    "this is the type of dependency of interest since it can immediately allow us to identify redundant features and hence reduce the dimensionality of our problem .",
    "the features that have the largest correlation with any other feature are @xmath48 , @xmath49 , and @xmath13 index .",
    "although relatively high and significant ( with a @xmath65 chance of being spurious ) , these correlations are still quite low compared to the typically recommended value of @xmath66 at which to consider eliminating a feature .",
    "the pair - wise correlation matrix provided a first crude check , although there could still be hidden collinearity in the data whereby one or more features are captured by a linear combination of the others .",
    "our second test was therefore more general and involved treating each feature in turn as the dependent variable and testing if it could be predicted by any or all of the remaining @xmath61 features ( the independent variables in the regression fit ) .",
    "we examined the @xmath67 values ( coefficients of determination ) from each fit .",
    "these values quantify the proportion of the variation in the dependent variable that can be `` explained '' by some linear combination of all the other variables ( features ) .",
    "the highest @xmath67 value was 0.68 and occured when @xmath48 was the dependent variable .",
    "this was not high enough to warrant removing this feature .",
    "we also explored the fitted coefficients and their significance from each linear fit .",
    "most of them were _ not _ significant at the @xmath68 level .",
    "we conclude that none of the features exhibit sufficiently strong correlations or linear dependencies to justify reducing the dimensionality of our feature space .      before attempting to tune and train a rf classifier , we first partition the input training sample described in section  [ tsamp ] into two random subsamples , containing 80% and 20% of the objects , or 6620 and 1653 objects respectively .",
    "these are respectively referred to as the _ true training sample _ for use in tuning and training the rf model using recursive cross - validation ( see below ) , and a _ test sample _ for performing a final validation check and assessing classification performance by comparing known to predicted outcomes .",
    "this _ test sample _ is sometimes referred to as the _ hold - out _ sample .",
    "the reason for this random 80/20 split is to ensure that our performance assessment ( using the _ test sample _ ) is independent of the model development and tuning process ( on the _ true training sample _ ) .",
    "that is , we do nt want to skew our performance metrics by a possibly over - fitted model , however slight that may be .    in figure  [",
    "fig : mags ] , we compare the w1 magnitude distributions for the _ true _ training sample ( referred to as simply `` training sample '' from hereon ) , the test sample to support final validation , and from this , an even smaller test subsample consisting of 194 objects with w1 magnitudes @xmath69 mag .",
    "this bright subsample will be used to explore the classification performance for objects with a relatively higher signal - to - noise ( s / n ) ratio .",
    "the shape of the training and test - sample magnitude distributions are essentially equivalent given both were randomly drawn from the same input training sample as described above .",
    "that is , the ratio of the number of objects per magnitude bin from each sample is approximately uniform within poisson errors .",
    "the magnitudes in figure  [ fig : mags ] are from the wise all - sky release catalog @xcite and derived from simultaneous point spread function ( psf ) fit photometry on the stacked single - exposures covering each source from the 4-band cryogenic phase of the mission _",
    "only_. these therefore effectively represent the time - averaged light curve photometry .",
    "w1 saturation sets in at approximately 8 mag , although we included objects down to 7 mag after examining the quality of their psf - fit photometry and light curves .",
    "the psf - fit photometry was relatively immune to small amounts of saturation in the cores of sources .",
    "the s / n limits in figure  [ fig : mags ] are approximate and based on the rms - noise in repeated single - exposure photometry for the _ non - variable _ source population ( section iv.3.b.ii . in * ? ? ?",
    "these represent good overall proxies for the uncertainties in the light curve measurements at a given magnitude .",
    "for example , the faintest sources in our training sample have a time - averaged w1 magnitude of @xmath70 mag where s / n @xmath71 and hence @xmath72 mag .",
    "this implies the fainter variables need to have progressively larger variability amplitudes in order to be reliably classified , e.g. , with say @xmath73 or @xmath74 mag at w1 @xmath75 mag .",
    "this therefore sets our effective sensitivity limit for detecting and characterizing variability in the wise single - exposure database .",
    "the paucity of faint high - amplitude variables amongst the _ known _ variable - source population in general explains the gradual drop - off in numbers beyond w1 @xmath76 mag .",
    "an overview of the rf machine learning algorithm was given in section  [ rf ] . even though the rf method is referred to as a _ non - parameteric _ classification method",
    ", it still has a number of tuning parameters to control its flexibility . in this paper , these are ( 1 ) the number of decision trees @xmath57 to build from each boostrapped sample of the training set ; ( 2 ) the number of features @xmath58 to randomly select from the full set of @xmath60 features to use as candidates for splitting at each tree node ; and ( 3 ) the size of a terminal node in the tree , _ nodesize _ , represented as the minimum number of objects allowed in the final subregion where no more splitting occurs . for classification problems ( as opposed to regression where the response is a multi - valued step function ) , @xcite recommends building each individual tree right down to its leaves where _ nodesize _",
    "@xmath77 , i.e. , leaving the trees `` unpruned '' .",
    "this leaves us with @xmath57 and @xmath58 .",
    "the optimal choice of these parameters depends on the complexity of the classification boundaries in the high - dimensional feature space .",
    "we formed a grid of @xmath57 and @xmath58 test values and our criterion for optimality ( or figure - of - merit ) was chosen to be the _ average _ classification accuracy .",
    "this is defined as the ratio of the number of correctly predicted classifications from the specific rf model to the total number of objects in all classes .",
    "this metric is also referred to as the average classification efficiency and `` 1 - accuracy '' is the error rate .",
    "this is a suitable metric to use here since our classes contain similar numbers of objects and the overall average accuracy will not be skewed towards the outcome for any particular class .",
    "this metric would be biased for instance if one class was substantially larger than the others since it would dictate the average classification accuracy .",
    "fortunately , the classification accuracy is relatively insensitive to @xmath57 when @xmath57 is large ( @xmath31 a few hundred ) and @xmath58 is close to its optimum value .",
    "the only requirement is that @xmath57 be large enough to provide good averaging ( `` bagging '' ) to minimize the tree - to - tree variance and bring the prediction accuracy to a stable level , but not too large as to consume unnecessary compute runtime .",
    "therefore , we first fixed @xmath57 at a relatively large value of 1000 , then tuned @xmath58 using a 10-fold cross - validation on the _ true _ training sample defined in section  [ prep ] and selecting the @xmath58 value that maximized the classification accuracy ( see below ) .",
    "once an optimal value of @xmath58 was found , we then explored the average classification accuracy as a function of @xmath57 to select an acceptable value of @xmath57 .",
    "10-fold cross - validation ( or @xmath15-fold in general ) entails partitioning the training sample into ten subsamples where each subsample is labeled @xmath78 , then training the rf model on nine combined subsamples and predicting classifications for the remaining one .",
    "these predictions are compared to the known ( true ) classifications to assess classification performance .",
    "this prediction subsample is sometimes referred to as the `` hold - out '' or `` out - of - bag '' sample .",
    "given @xmath21 objects in the training sample , we iterate until every subsample @xmath56 containing @xmath79 objects has served as the prediction dataset using the model trained on the remaining @xmath80 objects .",
    "the final classification ( or prediction ) performance is then the average of all classification accuracies from all 10 iterations .",
    "the ` caret ` package in ` r ` provides a convenient interface to train and fit a rf model using @xmath15-fold cross - validation .",
    "this calls the higher level _ randomforest _ ( ) function , an implementation of the original @xcite algorithm written in fortran .",
    "we were unable to find a precise description of the algorithm implemented in tandem with cross - validation by the ` r ` ` caret ` package . given that there is a lot happening in the training and tuning phase , we lay out the steps in appendix  [ appa ] .",
    "figure  [ fig : mtry ] shows the average classification accuracy as a function of the trial values of @xmath58 .",
    "the optimal value is @xmath81 and close to the rule - of - thumb suggested by @xcite : @xmath82 , where @xmath60 is the total number of features ( 7 here ) . as mentioned earlier",
    ", the classification accuracy is relatively insensitive to @xmath57 when @xmath57 is large and @xmath58 is close to its optimum value .",
    "the results in figure  [ fig : mtry ] assume a fixed value @xmath83 .",
    "figure  [ fig : ntree ] shows the average classification accuracy as a function of @xmath57 for @xmath84 2 , 3 , and 4 .",
    "the achieved accuracies are indeed independent of @xmath57 for @xmath85 .",
    "however , to provide good tree - averaging ( `` bagging '' ) and hence keep the variance in the final rf model fit as small as possible , we decided to fix @xmath57 at 700 .",
    "this also kept the compute runtime at a manageable level .    when predicting the classification for a new object with feature vector @xmath86 , it is pushed down the tree .",
    "that is , it is assigned the label of the training sample in the terminal node it ends up in .",
    "this procedure is iterated over all @xmath57 trees in the ensemble , and the mode ( or majority ) vote of all trees is reported as the predicted class .",
    "however , instead of the winning class , one may want to quote the probabilities that an object belongs to each respective class .",
    "this allows one to make a more informed decision .",
    "the probability that a new object with feature vector @xmath86 belongs to some class @xmath87 where @xmath88 is given by @xmath89 where @xmath90 is an indicator function defined to be 1 if tree @xmath20 predicts class @xmath87 and 0 otherwise . in other words , for the rf classifier , the class probability is simply the fraction of @xmath57 trees that predicted that class .",
    "it s important to note that the probability computed via equation  [ eq : prob ] is a _",
    "conditional _ class probability and only has meaning _ relative _ to the probabilities of obtaining the same features @xmath86 conditioned on the other contending classes .",
    "that is , we say an object with features @xmath86 is relatively more likely to have been generated by the population of objects defined by class @xmath91 than classes @xmath92 or @xmath93 , etc .    the definition in equation  [ eq : prob ]",
    "should not be confused with the _ posterior _ probability that the object belongs to class @xmath87 in an `` absolute '' sense , i.e. , as may be inferred using a bayesian approach .",
    "this can be done by assuming some _ prior _ probability @xmath94 derived for example from the proportion that each class contributes to the _ observable _ population of variable sources .",
    "the probability in this case would be : @xmath95 where @xmath96 is the normalization factor that ensures the integral of @xmath97 over all @xmath87 is 1 and @xmath98 , the `` likelihood '' , is given by equation  [ eq : prob ] .",
    "unfortunately , plausible values for the priors @xmath94 are difficult to derive at this time since the relative number of objects across classes in our training sample are likely to be subject to heavy selection biases , e.g. , brought about by both the wise observational constraints and heterogeneity of the input optical variability catalogs used for the cross - matching . the current relative proportions of objects will not represent the true mix of variable sources one would observe in a controlled flux - limited sample according to the wise selection criteria and sensitivity to each class . this bayesian approach will be considered in future as classification statistics and selection effects are better understood . for our initial classifications , we will quote the relative ( conditional ) class probabilites defined by equation  [ eq : prob ] ( see section  [ cpred ] for details ) .",
    "an initial qualitative assessment of the separability of our three broad variability classes according to the seven mid - ir light curve features was explored in section  [ fan ] . given the relatively high dimensionality of our feature space ,",
    "this separability is difficult to ascertain by looking at pairwise relationships alone .",
    "our goal is to explore class separability using the full feature space in more detail .",
    "this also allows us to identify those features that best discriminate each class as well as those that carry no significant information , both overall and on a per - class basis .",
    "_ random forests _ provide a powerful mechanism to measure the predictive strength of each feature for each class , referred generically to as _ feature importance_. this quantifies , in a relative sense , the impact on the classification accuracy from randomly permuting a feature s values , or equivalently , forcing a feature to provide no information , rather than what it may provide on input .",
    "this metric allows one to determine which features work best at distinguishing between classes and those that physically define each class .",
    "feature importance metrics can be additionally generated in the rf training / tuning phase using the `` hold - out '' ( or `` out - of - bag '' ) subsamples during the cross - validation process ( section  [ train ] ) , i.e. , excluded from the bootstrapped training samples .",
    "the prediction accuracies from the _ non - permuted_-feature and _ permuted_-feature runs are differenced then averaged over all the @xmath57 trees and normalized by their standard - error ( @xmath99 ) .",
    "the importance metrics are then placed on a scale of 0 - 100 where the `` most important '' metric is assigned a value of 100 for the class where it is a maximum .",
    "the intent here is that features that lead to large differences in the classification accuracy for a specific class when their values are randomly permuted are also likely to be more important for that class .",
    "it s worth noting that even though this metric is very good at finding the _ most important _ features , it can give misleading results for highly - correlated features which one might think are important .",
    "a feature could be assigned a low rf importance score ( i.e. , with little change in the cross - validation accuracy after its values are permuted ) simply because other features that strongly correlate with it will stand in as `` surrogates '' and carry its predictive power .",
    "figure  [ fig : import ] illustrates the relative importance of each feature for each class using the predictions from our initially tuned rf classifier .",
    "period is the most important feature for all classes .",
    "this is no surprise since the three classes are observed to occupy almost distinct ranges in their _ recovered _ periods ( figure  [ fig : period ] ) , therefore providing good discriminative and predictive power . in general ,",
    "not all features are equally as important across classes .",
    "for example , the relative phase @xmath50 and @xmath13-index are relatively weak predictors on their own for the w uma and algol eclipsing binaries respectively while they are relatively strong predictors for rr lyr pulsating variables . in practice",
    ", one would eliminate the useless features ( that carry no information ) across all classes and retrain the classifier .",
    "however , since all features have significant predictive power for at least one class in this study , we decided to retain all features .",
    "we explored the impact on the _ per - class _ prediction accuracy of randomly permuting the @xmath50 and @xmath13-index feature values , i.e. , so they provide no useful information .",
    "the rf classifier was retrained and the final validation _ test sample _ defined in section  [ prep ] was used to assess the classification accuracy .",
    "this provided more of an `` absolute '' measure of the importance of these features than in figure  [ fig : import ] .",
    "we found that if either @xmath50 or @xmath13-index were forced to be useless , the classification accuracies for the algol and w uma classes were not significantly altered .",
    "however , the accuracy dropped by @xmath100% and @xmath101% for the rr lyr class by forcing these features to be useless respectively .",
    "if both @xmath50 and @xmath13-index were forced to be useless , the change in classification accuracies for the algol and w uma classes were still insignificant ( dropping by @xmath102% ) , but the drop for the rr lyr class was @xmath103% .",
    "this not only confirms the results in figure  [ fig : import ] , but also the fact that rf classifiers are relatively immune to features that carry little or no class information .",
    "we validate the overall accuracy of the rf classifier that was fit to the training sample by predicting classifications for the two _ test samples _ defined in section  [ prep ] ( figure  [ fig : mags ] ) and comparing these to their `` true '' ( known ) classifications .",
    "these test samples are independent of the training sample and hence allow an unbiased assessment of classifier performance .",
    "this was explored by computing the _ confusion matrix _ across all classes . the confusion matrix for our largest test sample ( consisting of 1653 objects to w1@xmath104 mag )",
    "is shown in figure  [ fig : cv ] .",
    "the quantities therein represent the proportion of objects in each true ( known ) class that were predicted to belong to each respective class , including itself .",
    "the columns are normalized to add to unity .",
    "when compared to itself ( i.e. , a quantity along the diagonal going from top - left to bottom - right in figure  [ fig : cv ] ) , it is referred to as the _ sensitivity _ in machine learning parlance .",
    "it is also loosely referred to as the per - class _ classification accuracy _ , _",
    "efficiency _ ( e.g. , as in section  [ import ] ) , or _ true positive rate_. we obtain classification efficiencies of 80.7% , 82.7% , and 84.5% for algols , rr lyrae , and w uma type variables respectively . the overall classification efficiency , defined as the proportion of all objects that were correctly predicted ( irrespective of class ) is @xmath105% . the corresponding 95% confidence interval ( from bootstrapping ) is 80.5% to 84.3% , or approximately @xmath02% across all three classes .    for comparison",
    ", @xcite obtained an overall classification efficiency of @xmath106% on a 25-class dataset of 1542 variable stars from the ogle and hipparcos surveys .",
    "however , if we isolate their algol ( predominately @xmath6 lyrae ) , rr lyrae ( all types ) and w uma statistics , we infer an overall classification efficiency of @xmath107% , implying an improvement of @xmath108 over our estimate for wise variables .",
    "this difference is likely due to their higher quality , longer timespan optical light - curves  specially selected to have been well studied in the first place . nonetheless , our classification performance is still good given the wise cadence , sparsity and timespan of observations , and possible uncertainties in classifications from the literature used to validate the predictions .",
    "the off - diagonal quantities of the confusion matrix in figure  [ fig : cv ] can be used to compute the reliability ( or equivalently the _ purity _ , _ specificity _ , or `` @xmath109 _ false positive rate _ '' ) for a specific class .",
    "that is , the proportion of objects in all _ other _ classes that are _ correctly _ predicted to _ not _ contaminate the class of interest .",
    "this can be understood by noting that the only source of unreliability ( or contamination ) to each class are objects from other classes .",
    "for example , the false positive rate ( @xmath110 ) for the algol class is @xmath111 and hence its purity is @xmath112% .",
    "similarly , the purity levels for the rr lyrae and w uma classes from figure  [ fig : cv ] are 96.2% and 87.6% respectively . for comparison , @xcite obtain purity levels of up to 95% for these and most other classes in their study .    for the smaller ,",
    "higher s / n test sample of 194 objects with w1 magnitudes @xmath69 ( figure  [ fig : mags ] ) , the classification accuracy for the algol class improves to @xmath113% , compared to 80.7% for the large test sample .",
    "however for the rr lyr class , the classification accuracy drops to 55.5% ( from 82.7% ) and for the w uma class , it drops to 79.3% ( from 84.5% ) . in general",
    ", we would have expected an increase in classification accuracy across all classes when only higher s / n measurements , and hence objects with more accurately determined features are used .",
    "this indeed is true for the algol class which appears to be the most populous in this subsample with 127 objects .",
    "the drop in classification performance for the other two classes can be understood by low number statistics with only 9 rr lyr and 58 w uma objects contributing .",
    "their sampling of the feature space density distribution in the training set for their class is simply too sparse to enable reliable classification metrics to be computed using ensemble statistics on the predicted outcomes . in other words",
    ", there is no guarantee that most of the 9 rr lyr in this high s / n test sample would fall in the densest regions of the rr lyr training model feature space so that they can be assigned high enough rf probabilities to be classified as rr lyr .",
    "the primary output from the rf classifier when predicting the outcome for an object with given features is a vector of _ conditional class _ likelihoods as defined by equation  [ eq : prob ] . by default ,",
    "the `` winning '' class is that with the highest likelihood . a more reliable and secure scheme to assign the winning class will be described in section  [ cpred ] .",
    "distributions of all the classification probabilities for our largest test sample are shown in figure  [ fig : rfprob ] .",
    "these probabilities are conditioned on the winning class that was assigned by the rf classifier so that histograms at the high end of the probability range in each class - specific panel correspond to objects in that winning class . the spread in winning class probabilities is similar across the three classes , although the algol class has slightly more mass at @xmath114 ( figure  [ fig : rfprob]a ) .",
    "this indicates that the 7-d feature space sample density is more concentrated ( or localized ) for this class than for the other classes .",
    "figure  [ fig : roc ] shows the receiver operating characteristic ( roc ) curves for each class in our largest test sample .",
    "these are generated by thresholding on the classification probabilities of objects in each class ( i.e. , with @xmath115 from _ left to right _ in figure  [ fig : roc ] ) , then computing the confusion matrix for each thresholded subclass .",
    "the _ true positive rate _",
    "( @xmath116 or classification accuracy ) and _ false positive rate _ ( @xmath110 or impurity ) were then extracted to create the roc curve .",
    "the trends for these curves are as expected . given the class probability quantifies the degree of confidence that an object belongs to that class , the larger number of objects sampled to a lower _ cumulative _ probability level will reduce both the overall @xmath116 and @xmath110 .",
    "that is , a smaller fraction of the truth is recovered , but the number of contaminating objects ( false positives ) from other classes does not increase much and the larger number of objects in general keeps the @xmath110 relatively low .",
    "the situation reverses when only objects with a higher classification probability are considered . in this case",
    "there are fewer objects in total and most of them agree with the true class ( higher @xmath116 ) . however , the number of contaminants is not significantly lower ( or does not decrease in proportion to the reduced number of objects ) and hence the @xmath110 is slightly higher overall .",
    "it is also interesting to note that even though the _ full _ test sample confusion matrix in figure  [ fig : cv ] indicates that w uma objects have the highest classification accuracy ( at 84.5%  corresponding to far left on the roc curve in figure  [ fig : roc ] ) , this is overtaken by rr lyrae at internal probability thresholds of @xmath117 where the classification accuracy ( @xmath116 ) becomes @xmath118 .",
    "this however is at the expense of an increase in the @xmath110 to @xmath119 .",
    "therefore , the roc curves contain useful information for selecting ( class - dependent ) classification probability thresholds such that specific requirements on the @xmath116 and @xmath110 can be met .",
    "we compare the performance of the rf classifier trained above to other popular machine - learned classifiers .",
    "the motive is to provide a cross - check using the same training data and validation test samples .",
    "we explored artificial neural networks ( nnet ) , @xmath56-nearest neighbors ( @xmath56nn ) , and support vector machines ( svm ) .",
    "a description of these methods can be found in @xcite .",
    "the ` r ` ` caret ` package contains convenient interfaces and functions to train , tune , test , and compare these methods @xcite .",
    "more ml methods are available , but we found these four to be the simplest to set - up and tune for our problem at hand .",
    "furthermore , these methods use very dissimilar algorithms and thus provide a good comparison set .",
    "parameter tuning was first performed automatically using large grids of test parameters in a 10-fold cross - validation ( defined in section  [ train ] ) ; then the parameter ranges were narrowed down to their optimal ranges for each method using grid sizes that made the effective number of computations in training approximately equal across methods .",
    "this enabled a fair comparison in training runtimes .    the classification accuracies ( or efficiencies ) , runtimes , and the significance of the difference in mean accuracy relative to the rf method",
    "are compared in table  [ comp ] .",
    "the latter is in terms of the @xmath17-value of obtaining an observed difference of zero ( the null hypothesis ) by chance according to a paired @xmath40-test .",
    "it appears that the nnet method performs just as well as the rf method in terms of classification accuracy , although the rf method has a slight edge above the others .",
    "this can be seen in figure  [ fig : cfbox ] where the overall distributions in accuracy are compared . aside from the similarity in classification performance between nnet and rf , the added benefits of the rf method , e.g. , robustness to outliers , flexibility and ability to capture complex structure , interpretability , relative immunity to irrelevent and redundant information , and simple algorithms to measure feature importance and proximity for supporting active learning frameworks ( section  [ al ] ) , makes rf our method of choice .",
    "our goal for the wvsdb is to report all _ periodic _ variable star types as allowed by the wise observing constraints using the best quality photometric time - series data from the primary - mission ( cryogenic and post - cryogenic ) single - exposure source databases .",
    "candidate variables will be selected using a relatively high value of the wise source catalog variability flag ( @xmath10 ) .",
    "recently , @xmath120 was made more reliable compared to the version initially used to construct our training sample ( section  [ tsamp ] ) .",
    "the new @xmath120 is included in the recently released allwise source catalog ( * ? ? ?",
    "* b.vi ) and is based on a combination of metrics derived directly from the single - exposure flux time - series .",
    "this includes the significance of correlated variability in the w1 and w2 bands .",
    "in addition , candidates will be selected using other quality and reliability metrics , statistically significant periodicity estimates that are well sampled for the available time - span , and single - exposure measurements with a relatively high signal - to - noise ratio ( e.g. , s / n @xmath121 ) in w1 or w2 .",
    "we expect to reliably classify close to one million periodic variable candidates .",
    "the wvsdb will list derived properties such as periods , amplitudes , phased light curves , a vector of probabilities of belonging to specific classes ( see below ) and from these , the `` most likely '' ( or winning ) class",
    ".    the classification probabilities will be _ conditional class _ likelihoods as defined by equation  [ eq : prob ] . by default , the rf classifier assigns the winning class @xmath87 for an object with features @xmath86 as that with the highest probability @xmath98 , with no margin for possible classification error .",
    "for example , for the three broad classes in our input training model , @xmath98 only needs to be @xmath122 to stand a chance of being assigned class @xmath87 .",
    "therefore , if the probabilities for algol , rr lyrae , and w uma are 0.34 , 0.33 , and 0.33 respectively , the winning class is algol .",
    "this assignment is obviously not significant in a relative sense and we want to be more certain ( or less ambiguous ) when reporting the most likely class . examining the conditional probability histograms in figure  [ fig : rfprob ] , a workable threshold for assigning a secure classification ( setting aside other biases ; see below ) may be @xmath123 .",
    "the fractions of objects in our final validation _ test sample _",
    "( section  [ cv ] ) initially classified as algol , rr lyrae , and w uma that have @xmath123 ( and hence securely classified ) are @xmath124% , @xmath125% , and @xmath126% respectively .",
    "the remaining @xmath127% of objects with class probabilities @xmath128 would be initially classified as `` unknown '' .",
    "this is a consequence of the `` fuzzy '' classification boundaries in our input training model .",
    "can these less probable ( or more ambiguous ) cases be classified into a more secure ( sub-)class in future ?",
    "below we discuss an approach to mitigate this limitation .",
    "it is known that rf classifiers trained using supervised methods perform poorly outside their `` learned boundaries '' , i.e. , when extrapolating beyond their trained feature space . the rf training model constructed in section  [ train ]",
    "was tuned to predict the classifications of only three broad classes : algol , rr lyrae , and w uma  the most abundant types that could be reliably identified given the wise sensitivity and observing cadence . furthermore",
    ", this model is based on confirmed variables and classifications from previous optical surveys ( section  [ tsamp ] ) which no doubt contain some incorrect labels , particularly since most of these studies also used some automated classification scheme .",
    "therefore , our initial training model is likely to suffer from sample selection bias whereby it will not fully represent all the variable types that wise can recover or discover down to fainter flux levels and lower s / n ratios ( figure  [ fig : mags ] ) . setting aside the three broad classes , our initial training model will lead to biased ( incorrect ) predictions for other rare types of variables that are close to or distant from the `` fuzzy '' classification boundaries of the input model .    figure  [ fig : eglcs ] illustrates some of these challenges . here",
    "we show example w1 and w2 light curves for a collection of known variables from the literature ( including one `` unknown '' ) and their predicted class probabilities using our input training model .",
    "the known short - period cepheid ( top left ) would have its period recovered with good accuracy given the available number of wise exposures that cover it . however , it would be classified as an algol with a relatively high probability . that s because our training sample did not include short - period cepheids .",
    "its period of @xmath129 days is at the high end of our fitted range ( figure  [ fig : period ] ) and overlaps with the algol class .",
    "for the given optimum observation timespan covered by wise , the number of short - period cepheids after cross - matching was too low to warrant including this class for reliable classification in future .",
    "better statistics at higher ecliptic latitudes ( where observation timespans are longer ) are obviously needed .",
    "the known algol and one of the two known rr lyrae in figure  [ fig : eglcs ] are securely classified , although the known w uma achieves a classification probability of only 0.535 according to our training model .",
    "this would be tagged as `` unknown '' if the probability threshold was 0.6 for instance .",
    "the two lower s / n objects on the bottom row ( a known rr lyra and a fainter variable we identify as a possible rr lyra `` by eye '' ) would also be classified as `` unknown '' according to our initial model , even though their light - curves can be visually identified as rr lyrae .",
    "this implies that when the s / n is low and/or the result from an automated classification scheme is ambiguous ( following any refinement of the training model ; see below ) , visual inspection can be extremely useful to aid the classification process .",
    "we need a mechanism that fills in the undersampled regions of our training model but also improves classification accuracies for the existing classes .",
    "* and references therein ) presented methods to alleviate training - sample selection biases and we use their concepts ( albeit slightly modified ) to optimize and extend classifications for the wvsdb . these methods fall under the general paradigm of _ semi - supervised _ or _ active learning _ whereby predictions and/or contextual follow - up information for new data is used to update ( usually iteratively ) a supervised learner to enable more accurate predictions .",
    "@xcite were more concerned with the general problem of minimizing the bias and variance of classifiers trained on one survey for use on predicting the outcomes for another .",
    "our training sample biases are not expected to be as severe since our training model was constructed more - or - less from the same distribution of objects with properties that we expect to classify in the long run .",
    "our goal is simply to strengthen predictions for the existing ( abundant ) classes as well as explore whether more classes can be teased out as the statistics improve and more of the feature space is mapped .",
    "our classification process will involve at least two - passes where the second pass ( or subsequent passes ) will use _ active learning _",
    "concepts to refine the training model .",
    "a review of this classification process is given in appendix  [ appb ] .",
    "we have described a framework to classify _ periodic _ variable stars identified using metrics derived from photometric time - series data in the wise single - exposure source databases .",
    "this framework will be used to construct an _ all - sky _ database of variable stars in the mid - ir ( the wvsdb ) , the first of its kind at these wavelengths .",
    "the reduced effects of dust - extinction will improve our understanding of milky way tomography , the distribution of dark matter , stellar structure , and evolution in a range of environments .",
    "we identified several light - curve features to assist with the automated classification of wise periodic variables , and found that fourier decomposition techniques can be successfully extended into the mid - ir to define features for unambiguously classifying variable stars .",
    "guided by previous automated classification studies of variable stars , we trained a machine - learned classifier based on the _ random forest _ method to probabilistically classify objects in a seven - dimensional feature space .",
    "random forests satisfy our needs in terms of flexibility , ability to capture complex patterns in the feature space , assessing feature importance , their relative immunity to outliers and redundant features , and for providing simple methodologies to support active - learning frameworks that can extend and refine training models to give more accurate classifications .",
    "we constructed a training sample of 6620 periodic variables with classifications from previous optical variability surveys ( macho , gcvs , and asas ) and found that the most common types that separate rather well and are reliably identified by wise ( given its sensitivity , observing cadences and time - spans ) are algols , rr lyrae , and w ursae majoris type variables .",
    "this sample was used to construct an initial rf training model to assess classification performance in general and hence whether our method was suitable for constructing a wvsdb . from cross - validating a separate sample of 1653 pre - classified objects ,",
    "our rf classifier achieves classification efficiencies of 80.7% , 82.7% , and 84.5% for algols , rr lyr , and w uma types respectively , with 2-@xmath35 uncertainties of @xmath1302% .",
    "these are achieved at purity ( or reliability ) levels of @xmath131 where the only source of `` impurity '' to each specific class is contamination from the other two contending classes .",
    "these estimates are similar to those of recent automated classification studies of periodic variable stars in the optical that also use rf classifiers .",
    "future work will consist of selecting good quality candidates for the wvsdb , the computation of light - curve features , further selection to retain periodic objects ( above some statistical significance ) , then construction of the wvsdb .",
    "the three - class rf training model defined above will form the basis for initially predicting the classes with associated probabilities for the wvsdb .",
    "these probabilities will be thresholded to secure the `` winning '' classes .",
    "this first classification pass will inevitably leave us with a large fraction of unclassified objects .",
    "our input training model has considerable room for expansion and improvement since during its construction , there were variable types that had to be removed since they were either too scarce or too close to an existing larger class to enable a reliable classification .",
    "therefore , following the first classification pass , we will refine the training model using an active - learning approach ( tied with contextual and/or follow - up information ) where the improved statistics will enable us to better map and divide the feature space into more classes as well as sharpen the boundaries of existing ones .",
    "this will appropriately handle the `` known unknowns '' , but wise s all - sky coverage and sensitivity offers a unique opportunity to discover new and rare variable types , or new phenomena and sub - types in existing classes of pulsational variables and eclipsing binaries .",
    "this work was funded by nasa astrophysics data analysis program grant nnx13af37 g .",
    "we thank the anonymous referee for invaluable comments that helped improve the quality of this manuscript .",
    "we are grateful to max kuhn for reviewing some of the details of our analyses and descriptions of the algorithms implemented in the ` r ` ` caret ` package .",
    "this publication makes use of data products from the the wide - field infrared survey explorer , which is a joint project of the university of california , los angeles , and the jet propulsion laboratory / california institute of technology , funded by the national aeronautics and space administration .",
    "long - term archiving and access to the wise single - exposure database is funded by neowise , which is a project of the jet propulsion laboratory / california institute of technology , funded by the planetary science division of the national aeronautics and space administration .",
    "this research has made use of the nasa/ ipac infrared science archive , which is operated by the jet propulsion laboratory , california institute of technology , under contract with the national aeronautics and space administration .",
    "_ facilities : _",
    "following the discussion in section  [ train ] , we outline the steps used to train and tune the random forest classifier as implemented in the ` r ` ` caret ` package . to our knowledge , this is not documented elsewhere .",
    "concise generic descriptions of the algorithm exist in the literature , but here we present the details for the interested reader .",
    "these were inferred from our in - depth experimentation with the software and dissecting the source code .    1 .",
    "select a trial value for @xmath58 : the number of input features to randomly select from the full set of @xmath60 features for determining the best split at each node of a tree .",
    "2 .   select an iteration @xmath56 in the 10-fold cross validation with a training subsample consisting of @xmath80 objects partitioned from the input training sample of @xmath21 objects .",
    "take a bootstrap sample from this training subsample of @xmath132 objects by randomly choosing @xmath132 times with replacement .",
    "4 .   grow an un - pruned tree on this bootstrap sample where at each node of the tree , use the @xmath58 randomly selected features to calculate the best split using the _ gini index _  a measure of the class inequality or impurity across a node .",
    "for some trial node @xmath133 , this is defined as @xmath134 where @xmath135 is the number of objects in node @xmath133 that are distributed amongst known classes @xmath136 with respective numbers @xmath137 .",
    "the best splitting hyperplane with respect to another adjacent node ( that ultimately defines the new node @xmath133 ) is that which maximizes @xmath138 .",
    "each tree is fully grown ( up to its leaves ) and not pruned .",
    "for the given value of @xmath58 , repeat steps 3 to 5 for @xmath57 bootstrapped training samples to create @xmath57 trees in the _",
    "random forest_. 7 .",
    "predict classifications for every object in the @xmath56th `` hold - out '' subsample in the 10-fold cross - validation set using all the @xmath57 trees ( see equation  [ eq : prob ] in section  [ train ] ) .",
    "compare these predictions to the known ( true ) classifications to compute the classification accuracy .",
    "store the average classification accuracy over all objects for the given @xmath56th iteration and trial value of @xmath58 . 8 .",
    "move to the next cross - validation iteration @xmath56 with new training subsample ( step 2 ) and repeat steps 3 to 7 .",
    "when all 10 cross - validation iterations are done , average the classification accuracies from all 10 iterations .",
    "this represents the average classification accuracy for the given value of @xmath58 selected in step 1 .",
    "move to the next trial value of @xmath58 and repeat steps 1 to 9 .",
    "when all trial values of @xmath58 are tested , select the optimal value of @xmath58 based on the highest average classification accuracy from all the cross - validation runs .",
    "12 . using this optimal @xmath58 value , construct the final rf model using _ all _ @xmath21 objects in the initial training sample from a final run of steps 3 to 6 with @xmath139 .",
    "the final rf model consists of a `` ` r ` object '' that stores information for all the @xmath57 trees .",
    "this can then be used to predict the classifications for new objects ( see section  [ train ] ) .",
    "below we given an overview of our classification plan for the wvsdb .",
    "this uses two _ active learning _ methods to mitigate the limitations of our initial training set discussed in section  [ al ] .",
    "these methods are not new ; they were formulated ( with slight modifications ) from the concepts presented in @xcite .",
    "details and results of overall performance will be given in a future paper .",
    "depending on details of the manual follow - up of `` unclassifiable '' but good quality light curves , we expect at minimum , two passes in the classification process .",
    "the first pass uses our initial rf training model to compute and store the conditional - class probabilities for each object ( algol , rr lyrae , and w uma ) . in preparation for the _ active learning _ and second classification pass , we compute the averaged _ rf proximity _",
    "metrics for each object initially classified as `` unknown '' according to some probability cut ( see section  [ cpred ] ) .",
    "a proximity metric quantifies the relative separation between any two feature vectors amongst the @xmath57 decision trees of the rf and is defined as @xmath140 this represents the fraction of trees for which two objects with features @xmath86 and @xmath141 occupy the same terminal node @xmath142 ( leaf ) where @xmath143 if the statement in parenthesis is true and 0 otherwise .",
    "we compute average proximity measures for _ unknown _ object @xmath86 with respect to ( i ) all objects in the input training set @xmath144 and ( ii ) all other objects in the test set @xmath145 under study .",
    "these are defined as @xmath146 and @xmath147 respectively .",
    "the summation in equation  [ eq : avprox2 ] will be over a random subset of objects in the test sample ( both with known and unknown initial classifications ) .",
    "this is to minimize runtime since we expect to encounter at least a million variable ( test ) candidates .",
    "we expect to use of order 20% of the test objects",
    ". we will be primarily interested in the ratio @xmath148 .",
    "@xmath149 will be larger for objects that reside in regions of feature space where the test - sample density is higher relative to that in the training set .",
    "cuts in probability ( from the first pass ) versus @xmath149 and versus @xmath150 can be used to identify regions occupied by new or missed classes whose statistics were scarce when the training set was first constructed . some analysis will be needed to assign class labels to these ensembles of new objects . this can be done by comparing to known variable light curves ( again ) from the literature ( e.g. , that could be associated with cepheids , @xmath151 scuti , @xmath6 lyrae , or perhaps rr lyrae subtypes ) , or , if insufficient information is available , as possibly new interim classes to be labelled in future .",
    "the above is the first form of active learning we will use to refine the training set .",
    "prior to the second classification pass , we will also augment the input training set by adding the most confidently classified test objects ( algol , rr lyrae , and w uma ) , i.e. , with a relatively high probability .",
    "this is a simple form of _ self - training _ ( also known as _ co - training _ ) in the machine learning toolbox .",
    "this will `` sharpen '' and possibly extend classification boundaries for the dominant classes and hence improve their prediction accuracy . following the addition of new classes ( from the proximity analysis ) and new secure identifications to existing classes in the training set",
    ", we will retrain the rf classifier on the new training data and reclassify all objects in a second pass .",
    "some fraction of objects will remain unclassified , but we expect their numbers to be considerably lower .",
    "nonetheless , these `` outliers '' will be potentially interesting objects for further study .",
    "as the rf classifier is refined according to the above scheme , we will also explore improvements to the training model using _ boosting _ methods and generalizations thereof such as _ gradient boosted trees _",
    "* ; * ? ? ?",
    "* and references therein ) . here",
    ", the training model is iteratively refit using re - weighted versions of the training sample , i.e. , progressively more weight ( importance ) is placed on misclassified observations during training .",
    "these methods have been shown to improve predictive performance over simple conventional rf classifiers .",
    "alcock , c. , el al . , 2003 , ycat , 2247 ball , n.m .",
    ", & brunner , r.j .",
    ", 2010 , international journal of modern physics d , 19 , 1049 blomme , j. , debosscher , j. , de ridder , j. , et al . , 2010 , , 713 , l204 blomme , j. , sarro , l.m .",
    ", odonovan , f.t . , et al . , 2011 , , 418 , 96 bloom , j. , richards , j.w . ,",
    "nugent , p.e .",
    ", et al . , 2012 , , 124 , 1175 brett , d.r . ,",
    "west , r.g .",
    ", & wheatley , p.j . , 2004 , , 353 , 369 breiman , l. , friedman , j.h . , olshen , r.a . , & stone , c.i . , 1984 , classification and regression trees , belmont , calif .",
    ": wadsworth breiman , l. , 1996 , machine learning , 24 , 123 breiman , l. , 2001 , machine learning , 45 , 5 breiman , l. , & cutler , a. , 2004 , random forests , http://oz.berkeley.edu/~breiman/randomforests/ brink , h. , richards , j.w . ,",
    "poznanski , d. , et al . , 2013 , , 435 , 1047 bhlmann , p. , & hothorn , t. , 2007 , statistical science , 22 , no . 4 , 477 cutri , r.m . ,",
    "wright , e.l . , conrow , t. , et al .",
    ", 2012 , explanatory supplement to the wise all - sky data release products , http://wise2.ipac.caltech.edu/docs/release/allsky/expsup/ cutri , r.m . , wright , e.l . ,",
    "conrow , t. , et al .",
    ", 2013 , explanatory supplement to the allwise data release products , http://wise2.ipac.caltech.edu/docs/release/allwise/expsup/ dambis , a.k .",
    ", rastorguev , a.s .",
    ", & zabolotskikh , m.v . , 2014 , , in press , ( arxiv:1401.5523 ) deb , s. , & singh , h.p . , 2009 , , 507 , 1729 debosscher , j. , sarro , l.m . , aerts , c. , et al . , 2007 , , 475 , 1159 dubath , p. , rimoldini , l. , sveges , m. , et al . ,",
    "2011 , , 414 , 2602 eggen , o.j . , 1967 , , 70 , 111 eyer , l. , & blake , c. , 2005 , , 358 , 30 eyer , l. , & mowlavi , n. , 2008 , journal of physics : conference series , 118 , 012010 eyer , l. , jan , a. , dubath , p. , et al . ,",
    "2008 , in aip conference proceedings , vol .",
    "1082 , aip conference series , ed .",
    "c. a. l. bailer - jones , 257 - 262 grillmair , c.j . , 2010 , in galaxies and their masks , eds .",
    "block , d. , freeman , k.c .",
    ", & puerari , i. , springer , new york , 247 hastie , t. , tibshrirani , r. , & freidman , j. , 2009 , the elements of statistical learning : data mining , inference , and prediction . , 2nd edn .",
    "( springer , new york ) hoffman , d.i . ,",
    "harrison , t.e . ,",
    "coughlin , j.l . ,",
    "mcnamara , b.j . ,",
    "holtzman , j.a .",
    ", taylor , g.e . , & vestrand , w.t . , 2008 , , 136 , 1067 hoffman , d.i . ,",
    "cutri , r.m . ,",
    "masci , f.j . ,",
    "fowler , j.w . ,",
    "marsh , k.a . , & jarrett , t.h .",
    ", 2012 , , 143 , 118 kim , d. , protopapas , p. , byun , y. , et al . , 2011 , , 735 , 68 klein , c.r . ,",
    "richards , j.w . , butler , n.r .",
    ", & bloom , j.s . , 2011 ,",
    ", 738 , 185 klein , c.r . ,",
    "richards , j.w . , butler , n.r .",
    ", & bloom , j.s . , 2014 , , 440 , l96",
    "kinemuchi , k. , smith , h.a . , woniak , p.r . ,",
    "& mckay , t.a .",
    ", 2006 , , 132 , 1202 kuhn , m. , 2008 , journal of statistical software , 28 , 5 lafler , j. , & kinman , t.d . , 1965 , , 11 , 216 long , j.p . ,",
    "karoui , e.n . , rice , j.a .",
    ", et al . , 2012 , , 124 , 280 lucy , l.b . , 1968 , , 153 , 877 madore , b.f . ,",
    "hoffman , d.i . ,",
    "freedman , w.l .",
    ", et al . , 2013 , , 776 , 135 mahabal , a. , djorgovski , s.g . , turmon , m. , et al . , 2008 , astronomische nachrichten , 329 , 288 mainzer , a. , bauer , j. , grav , t. , et al . , 2011 , , 731 , 53 malkov , o.y . ,",
    "oblak , e. , avvakumova , e.a . , & torra , j. , 2007 , , 465 , 549 morgan , a.n . , long , j. , richards , j.w . , et al . , 2012 , , 746 ,",
    "170 park , m. , oh , h .- s .",
    ", & kim , d. , 2013 , , 125 , 470 percy , j.r . , 2007 , understanding variable stars ( cambridge university press ) , ch . 5 pojmanski , g. , maciejewski , g. , pilecki , b. , & szczygiel , d. , 2006 , ycat , 2264 richards , j.w . ,",
    "starr , d.l . ,",
    "butler , n.r .",
    ", et al . , 2011 , , 733 , 10 richards , j.w . , starr , d.l . , brink , h. , et al . , 2012 , , 744 , 192 rucinski , s.m . , 1993 , , 105 , 1433 samus , n. , durlevich , o. , et al . , 2013 , ycat , 102025s scargle , j.d",
    ". , 1982 , , 263 , 835 schwarzenberg - czerny , a. , 1998 , baltic astronomy , 7 , 43 stellingwerf , r.f .",
    ", 1978 , , 224 , 953 stetson , p.b . , 1996 , , 108 , 851 tammann , g.a . ,",
    "sandage , a. , & reindl , b. , 2008 , the astronomy and astrophysics review , 15 , issue 4 , 289 woniak , p.r . , williams , s.j . ,",
    "vestrand , w.t . , & gupta , v. , 2004 , , 128 , 2965 wright , e. , et al . , 2010 , , 140 , 1868 zechmeister , m. & krster , m. , 2009 , 496 , 577                                                     blue ( thick bars ) ; w2 = red ( thin bars ) .",
    "horizontal dashed lines are median w1 magnitudes .",
    "each panel shows the variable type ( if known ) , period , and the three class probabilities predicted by our initial rf training model : for algol , rr lyr , and w uma types respectively . ]"
  ],
  "abstract_text": [
    "<S> we describe a methodology to classify periodic variable stars identified using photometric time - series measurements constructed from the wide - field infrared survey explorer ( wise ) _ full - mission _ single - exposure source databases . </S>",
    "<S> this will assist in the future construction of a wise variable source database that assigns variables to specific science classes as constrained by the wise observing cadence with statistically meaningful classification probabilities . </S>",
    "<S> we have analyzed the wise light curves of 8273 variable stars identified in previous optical variability surveys ( macho , gcvs , and asas ) and show that fourier decomposition techniques can be extended into the mid - ir to assist with their classification . </S>",
    "<S> combined with other periodic light - curve features , this sample is then used to train a machine - learned classifier based on the _ random forest _ ( rf ) method . </S>",
    "<S> consistent with previous classification studies of variable stars in general , the rf machine - learned classifier is superior to other methods in terms of accuracy , robustness against outliers , and relative immunity to features that carry little or redundant class information . for the three most common classes identified by wise : algols , rr lyrae , and w ursae majoris type variables , we obtain classification efficiencies of 80.7% , 82.7% , and 84.5% respectively using cross - validation analyses , with 95% confidence intervals of approximately @xmath02% . </S>",
    "<S> these accuracies are achieved at purity ( or reliability ) levels of 88.5% , 96.2% , and 87.8% respectively , similar to that achieved in previous automated classification studies of periodic variable stars .    </S>",
    "<S> = 1 </S>"
  ]
}