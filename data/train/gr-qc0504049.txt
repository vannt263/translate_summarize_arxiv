{
  "article_text": [
    "the motivation for controlling the false discovery rate ( fdr ) i.e . the fraction of false alarms in a collection of candidate detections jumped to our attention as we were involved in data analysis for the igec  @xcite , the network of resonant detectors that searched for coincident burst gravitational wave ( gw ) signals in the years 1997 - 2000 .",
    "even if the detectors involved in igec were rather similar , there were obvious configurations ( special choice of detector pairs , three - fold instead of double coincidence ) or cuts of the data ( higher or lower threshold on event amplitude ) characterized by lower background counts , or higher duty time .",
    "we did not have _ a priori _ a good reason to prefer one configuration or cut more than others , as we do not know a priori the intensity of the signal , hence the efficiency .",
    "therefore , we decided at the beginning a fairly long list of interesting choices , in order to perform many analyses in parallel , and eventually to quote the results for each trial .",
    "the results were expressed as confidence intervals on the expectation value for the number of counts in coincidence due to gw .",
    "when unveiling the final results , one of the confidence intervals at 90% coverage was not including the null hypothesis ( i.e. zero counts ) .",
    "of course this can be somewhat expected by chance when the number of trials is very high .",
    "it was possible to compute accurately that with 30% probability we had a chance that at least one of the tests falsely rejected the null hypothesis .",
    ".[tab : notation ] quick - reference notation chart for the variables used in .",
    "@xmath0 is the total number of performed tests ( trial factor ) , @xmath1 and @xmath2 the real number of underlying off - source and on - source tests .",
    "the number of _ actually _ positive tests is r , given by s true positives and b spurious claims .",
    "an ideal experiment would neither treat background as signal ( type i error ) nor do the reverse ( type ii errors ) . [",
    "cols=\"^,^,^,^\",options=\"header \" , ]     the probability of at least one false claim in a set of trials is known as _ family - wise error rate _ ( fwer ) .",
    "it is not difficult to devise a method to control this quantity _ before _ going to the results : we just have to increase the confidence in the single trial ( say 99% , or 99.99% coverage ) in order to keep the fwer much lower than one .",
    "the drawback is that the resulting confidence interval would be much larger , and consequently the power of the search would fall dramatically . this is a consequence of the request that _ not even in a single case _ the null hypothesis is rejected when it is true .",
    "a very reasonable compromise was suggested by benjamini and hochberg@xcite .",
    "they remark that in many practical cases , when having one or more false claim is not by itself unacceptable , we could just be happy if on average _ most _ of the claims were real . in other words , they propose to bound fdr instead of fwer .",
    "there are many topics in gw search which would benefit from this kind of procedure .",
    "for instance :    * all sky surveys : many source directions and polarizations are tried in parallel ; * template banks ; * eyes - wide - open searches : many alternative analysis pipelines , with different amplitude thresholds , signal duration , and so on are applied on the same data . * periodic updates of results :",
    "every new science run is a chance for a  discovery \" (  maybe next one is the good one \" ) ; * many graphical representations or aggregations of the data (  with a slight change in the binning , the ` signal ' shows up better \" )    this work means not to be a complete review of the state - of - art techniques about fdr control , but hopefully it will be a stimulus for whoever is involved in multiple - test data analysis issues .    in the following sections , we shall use the notation reported in .",
    "in order to decide whether the results of a measurement are compatible with being generated by noise only ( _ null hypothesis _ , h0 ) or instead they contain a signal ( _ alternative hypothesis _ , h1 ) the textbook procedure is to set up a test statistic @xmath3 from the measures themselves .",
    "if @xmath4 is the distribution of @xmath3 when the h0 holds , then the @xmath5-value of @xmath3 is defined as @xmath6 . by construction @xmath5",
    "is uniformly distributed between 0 and 1 :    @xmath7    it is of paramount importance that the distribution @xmath8 is known .",
    "it is always wise to check a priori models with a goodness - of - fit test , when there are enough off - source data available .",
    "this is not always the case , but often there are surrogate procedures ( e.g. data permutation ) which give fresh independent samples of the background process , removing at the same time the effect of real signals , if any are present in the data .",
    "for instance , in the case of igec , the _ resampling procedure _ consisted in adding a delay to the time reference of one of the detectors in the network , such that the coincident signal is lost , while the background expectation value of coincidence counts is approximately unchanged . in case",
    "the data are not compliant with the model , at worse resampled data may allow to estimate @xmath8 by empirical fit .    as for h1 , it is usually unknown , but for our purposes it is sufficient assuming that the signal can be distinguished from the noise , i.e. @xmath9 .",
    "the sketch in ( _ top left _ ) illustrates the concept .    for a single hypothesis test , the condition  reject null if @xmath10 \" leads to false positives with probability @xmath11 . in case of multiple tests",
    ", we deal with a set @xmath12 of @xmath5-levels , which need not to derive from the same test statistics , nor they should refer to same tested null hypothesis .",
    "@xmath0 is called the trial factor .",
    "we select discoveries using a threshold @xmath13 :  reject null if @xmath14 \" .",
    "the _ uncorrected testing _ would just use the same threshold for each test : @xmath15 .",
    "the probability that at least one rejection is wrong grows as @xmath16 .",
    "therefore , as in the igec case , false discovery is guaranteed for @xmath0 large enough .",
    "the other extreme solution , usually referred to as the _ bonferroni procedure _ @xcite , controls the fwer in the most stringent manner , by requiring that @xmath17 .",
    "this is achieved by the choice @xmath18",
    "while this approach makes mistakes rare , the cost is low efficiency ( @xmath19 ) .      in order to",
    "trade - off between b=0 and s=0 , the fdr control focuses on the ratio of false discoveries to the total number of claims :    @xmath20    this can be done with a proper choice of @xmath13 .",
    "the original procedure suggested by benjamini and hochberg ( bh ) is extremely simple , involving only trivial algebraic operations .",
    "it consists in the following steps .    *",
    "sort the @xmath5-values in ascending order : @xmath21 ; * choose your desired fdr @xmath22 ( in case no signal source is actually present during the observation , then the procedure is equivalent to the bonferroni procedure with @xmath23 ) ; * define @xmath24 if @xmath5-values are independent or positively correlated ; otherwise @xmath25 ; * determine the threshold @xmath26 by finding the index @xmath27 such that @xmath28 when @xmath29 ( see for a visual representation of this condition ) .",
    "the above procedure with @xmath24 was shown @xcite to control the expectation value - values . ] of fdr at least at level @xmath22 in the case when all @xmath0 tests are independent .",
    "however , it was later proved to control fdr when tests are positively correlated @xcite ( for instance , multivariate normal data where the covariance matrix has all positive elements ) .",
    "the alternative definition of @xmath30 given above controls fdr in the most general case @xcite , but at the cost of reduced efficiency .",
    "there is a nice back - of - the - envelope plausibility argument which can be found in  @xcite for the simple case when signals are easily separable ( e.g. signals with high signal - to - noise ratios ) . in this case",
    "we expect their @xmath5-level to be very low , and correspondingly in the cumulative histogram of @xmath5-levels we shall see a step with height @xmath31 near @xmath32 , see ( _ bottom right _ ) .",
    "we see also that there is only one intersection point for the bh procedure , such that @xmath33 on the other hand , the threshold @xmath13 can be expressed on average by @xmath34 ( this is a special case of ) . substituting this value in we obtain @xmath35 for a rigorous proof see @xcite .",
    "( _ a _ ) the probability density function of @xmath5-values when data come from a mixed model can be thought as the sum of a uniform distribution ( background ) and a biased one ( signal ) .",
    "( _ b _ ) the benjamini - hochberg procedure ( bh ) consists in plotting the cumulative histogram of the @xmath5-values of the @xmath0 trials ( _ continuos line _ ) and looking for intersections with a line drawn from the origin and with slope equal to @xmath36 ( _ dashed line _ ) .",
    "the null hypothesis is rejected for all data with @xmath5-value between 0 and the abscissa of the highest intersection point . + ( _ c _ ) sketch of histogram of @xmath5-values and ( _ d _ ) corresponding cumulative histogram , in a case of easily separable signals .",
    "the bh procedure applied to this case can be easily shown to control fdr ( see for details).,title=\"fig:\",width=302 ] ( _ a _ ) the probability density function of @xmath5-values when data come from a mixed model can be thought as the sum of a uniform distribution ( background ) and a biased one ( signal ) .",
    "( _ b _ ) the benjamini - hochberg procedure ( bh ) consists in plotting the cumulative histogram of the @xmath5-values of the @xmath0 trials ( _ continuos line _ ) and looking for intersections with a line drawn from the origin and with slope equal to @xmath36 ( _ dashed line _ ) .",
    "the null hypothesis is rejected for all data with @xmath5-value between 0 and the abscissa of the highest intersection point .",
    "+ ( _ c _ ) sketch of histogram of @xmath5-values and ( _ d _ ) corresponding cumulative histogram , in a case of easily separable signals . the bh procedure applied to this case can be easily shown to control fdr ( see for details).,title=\"fig:\",width=264 ]",
    "we now demonstrate this procedure with a simple example .",
    "suppose we are given the results of 50 counting experiments , labeled by the index @xmath37 .",
    "their background is modeled as a poisson random variable , with the same - values ) , we actually spread the background of the experiments in a range @xmath38 around @xmath39 ) . ]",
    "known expectation value @xmath39 for all @xmath37 .",
    "we consider two possible cases : in the first one , we draw 50 independent measures , in the other case we generate correlation by summing neighbor bins ( i.e. , if @xmath40 represent independent counts in the @xmath37-th bin , then the 50 correlated counts @xmath41 are defined as @xmath42 , where @xmath43 ) .",
    "we investigated different background levels ( from @xmath44 to @xmath45 ) and different number of detected signals ( @xmath46 ) , assuming for sake of simplicity that each bin can have either one or zero counts due to true signals .    in order to decide the presence of a signal we use the one - tail poisson probability for the expected number of counts in each bin . in the results of a monte carlo simulation",
    "are shown .",
    "for each configuration ( differing by average background and extent of true signals ) we compute the average number of claims @xmath47 , i.e. the number of bins for which the null hypothesis is rejected .",
    "we present the results for bonferroni and bh tests , both tuned to bound the fwer at 1% when no signal is present .",
    "both procedures are working as expected , controlling the fwer and the fdr respectively at the desired level .",
    "for high background values they give as expected similar results . on the other side",
    "the efficiency of the bonferroni procedure falls to zero for @xmath48 , while the bh procedure is still effective , up to @xmath49 in this example .    in",
    "we can visualize how the bh procedure manage to grasp the signals promptly , as the background level lowers ( see also ) .",
    "ccccccccccccc @xmath50 & * 0.01 * & * 0.02 * & * 0.05 * & * 0.1 * & * 0.2 * & * 0.5 * & * 1 * & * 5 * & * 10 * & * 50 * + * 0 * & 0.005 & - & - & @xmath51 & 3@xmath52 & 0.003 & 0.007 & 0.008 & 0.003 & 0.004 + & 0.005 & 2@xmath52 & - & @xmath51 & 3@xmath52 & 0.003 & 0.007 & 0.008 & 0.003 & 0.004 + * 1 * & 1.005 & 4@xmath52 & 0.001 & 0.002 & 0.005 & 0.013 & 0.028 & 0.012 & 0.004 & 0.005 + & 1.010 & 0.019 & 0.001 & 0.002 & 0.005 & 0.013 & 0.028 & 0.012 & 0.004 & 0.005 + * 2 * & 2.004 & 5@xmath52 & 0.002 & 0.004 & 0.009 & 0.021 & 0.047 & 0.016 & 0.005 & 0.005 + & 2.010 & 2.019 & 0.002 & 0.004 & 0.009 & 0.021 & 0.047 & 0.016 & 0.005 & 0.005 + * 3 * & 3.005 & 0.001 & 0.003 & 0.006 & 0.013 & 0.032 & 0.069 & 0.021 & 0.006 & 0.005 + & 3.010 & 3.019 & 0.004 & 0.006 & 0.013 & 0.032 & 0.069 & 0.021 & 0.006 & 0.005 + * 4 * & 4.005 & 0.001 & 0.004 & 0.008 & 0.017 & 0.043 & 0.086 & 0.027 & 0.007 & 0.006 + & 4.010 & 4.018 & 0.125 & 0.008 & 0.017 & 0.043 & 0.086 & 0.027 & 0.007 & 0.006 + * 5 * & 5.004 & 0.002 & 0.005 & 0.009 & 0.020 & 0.053 & 0.106 & 0.029 & 0.008 & 0.006 + & 5.009 & 5.018 & 5.046 & 0.009 & 0.020 & 0.053 & 0.106 & 0.029 & 0.008 & 0.006 + * 6 * & 6.004 & 0.002 & 0.006 & 0.013 & 0.024 & 0.061 & 0.124 & 0.034 & 0.010 & 0.007 + & 6.009 & 6.017 & 6.043 & 0.013 & 0.024 & 0.061 & 0.124 & 0.034 & 0.010 & 0.008 +    ccccccccccccc @xmath50 & * 0.01 * & * 0.02 * & * 0.05 * & * 0.1 * & * 0.2 * & * 0.5 * & * 1 * & * 5 * & * 10 * & * 50 * + * 0 * & 0.006 & - & - & @xmath51 & 2@xmath52 & 0.003 & 0.008 & 0.008 & 0.003 & 0.004 + & 0.010 & 0.010 & - & @xmath51 & 2@xmath52 & 0.003 & 0.009 & 0.008 & 0.003 & 0.004 + * 1 * & 1.005 & 3@xmath52 & 0.001 & 0.002 & 0.005 & 0.013 & 0.030 & 0.012 & 0.004 & 0.005 + & 1.010 & 0.029 & 0.001 & 0.002 & 0.005 & 0.013 & 0.031 & 0.012 & 0.004 & 0.005 + * 2 * & 2.005 & 7@xmath52 & 0.002 & 0.004 & 0.008 & 0.023 & 0.046 & 0.017 & 0.005 & 0.005 + & 2.009 & 2.018 & 0.006 & 0.004 & 0.008 & 0.023 & 0.047 & 0.017 & 0.005 & 0.006 + * 3 * & 3.004 & 0.001 & 0.003 & 0.005 & 0.012 & 0.032 & 0.067 & 0.022 & 0.006 & 0.005 + & 3.009 & 3.019 & 0.060 & 0.005 & 0.012 & 0.032 & 0.068 & 0.022 & 0.006 & 0.006 + * 4 * & 4.005 & 0.002 & 0.004 & 0.008 & 0.017 & 0.043 & 0.084 & 0.025 & 0.007 & 0.005 + & 4.009 & 4.017 & 0.143 & 0.008 & 0.017 & 0.043 & 0.085 & 0.025 & 0.007 & 0.006 + * 5 * & 5.004 & 0.002 & 0.005 & 0.010 & 0.019 & 0.051 & 0.107 & 0.029 & 0.008 & 0.007 + & 5.009 & 5.018 & 5.047 & 0.010 & 0.019 & 0.051 & 0.108 & 0.029 & 0.008 & 0.007 + * 6 * & 6.004 & 0.003 & 0.007 & 0.011 & 0.024 & 0.061 & 0.127 & 0.035 & 0.010 & 0.007 + & 6.009 & 6.016 & 6.044 & 0.013 & 0.024 & 0.061 & 0.127 & 0.035 & 0.010 & 0.007 +    a few samples from the monte carlo used to produce are displayed in detail .",
    "they refer to @xmath53 , and the background is ( _ a _ ) @xmath45 ( _ b _ ) @xmath54 ( _ c _ ) @xmath44 . in the plots above the cumulative histogram of the @xmath5-values",
    "is compared with the threshold given by the bonferroni ( ) and the bh ( ) procedures.,width=411 ]",
    "when multiple tests are tried for the same data set , controlling fdr seems in general a wiser idea than just limiting type - i errors . robust but simple procedures exist which ( conservatively ) control fdr in positively correlated tests , and also in the more general case ( but at the cost of reduced efficiency ) .",
    "this idea is relatively new in the astrophysics community , and we are not aware of any application in the gw community . its application should be encouraged .",
    "notice however that bh procedure is not the only one , and more complex but approximate strategies have been investigated ( see for instance @xcite ) .",
    "we are indebted to james t. linnemann ( msu ) for introducing us to fdr .",
    "baggio acknowledges the hospitality of icrr and the grant of tokyo university .",
    "1 benjamini y and hochberg y 1995 _ j. roy .",
    "soc . ser .",
    "b _ * 57 * 289 - 300 benjamini y and yekutieli d 2001 _ ann . statist . _",
    "* 29 * 1165 - 88 astone p 2003 _ phys .",
    "d _ * 68 * 022001 hochberg y and tamhane a c 1987 _ multiple comparison procedures _ ( new york : wiley ) miller c j 2001 _ a.j . _ * 122 * 3492 - 505 yekutieli d and benjamini y 1999 _ j. statist .",
    "plann . inference _  * 82 * 171 - 96 storey j d 2002 _ j. roy .",
    "b _ * 64 * 479 - 98"
  ],
  "abstract_text": [
    "<S> when testing multiple hypothesis in a survey e.g . </S>",
    "<S> many different source locations , template waveforms , and so on the final result consists in a set of confidence intervals , each one at a desired confidence level . but the probability that at least one of these intervals does not cover the true value increases with the number of trials . with a sufficiently large array of confidence intervals , one can be sure that at least one is missing the true value . </S>",
    "<S> in particular , the probability of false claim of detection becomes not negligible . in order to compensate for this , one should increase the confidence level , at the price of a reduced detection power . </S>",
    "<S> false discovery rate control@xcite is a relatively new statistical procedure that bounds the number of mistakes made when performing multiple hypothesis tests . </S>",
    "<S> we shall review this method , discussing exercise applications to the field of gravitational wave surveys . </S>"
  ]
}