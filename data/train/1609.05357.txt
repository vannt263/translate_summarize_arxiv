{
  "article_text": [
    "the omnipresence of power - laws in natural , socio - economic , technical , and living systems has triggered immense research activity to understand their origins .",
    "it has become clear in the past decades that there exist several distinct ways to generate power - laws ( or asymptotic power - laws ) , for an overview see for example @xcite . in short , power - laws of the form @xmath0",
    "arise in critical phenomena @xcite , in systems displaying self - organized criticality @xcite , preferential attachment type of processes @xcite , multiplicative processes with constraints @xcite , systems described by generalized entropies @xcite , or sample space reducing processes @xcite , i.e. processes that reduce the number of possible outcomes ( sample space ) as they unfold .",
    "literally thousands of physical , natural , man - made , social , and cultural processes exhibit power - laws , the most famous being earthquake magnitudes @xcite , city sizes @xcite , foraging and distribution pattern of various animal species @xcite , evolutionary extinction events @xcite , or the frequency of word occurrences in languages , known as zipf s law @xcite .",
    "it is obvious that estimating power - law exponents from data is a task that sometimes should be done with high precision .",
    "for example if one wants to determine the universality class a given process belongs to , or when one estimates probabilities of extreme events . in such situations small errors in the estimation of exponents may lead to dramatically wrong predictions with potentially serious consequences .    estimating power - law exponents from data is not an entirely trivial task .",
    "many reported power - laws are simply not exact power - laws , but follow other distribution functions . despite the importance of developing adequate methods for distinguishing real power - laws from alternative hypotheses , we will not address this issue here since good standard literature on the topic of bayesian _ alternative hypotheses testing _ exists , see for example @xcite .",
    "for power - laws some of these matters have been discussed also in @xcite .",
    "here we simply focus on estimating power - law exponents from data on a sound probabilistic basis , using a classic bayesian parameter estimation approach , see e.g. @xcite , that provides us with _ maximum likelihood _ ( ml ) estimators for estimating power - law exponents over the full range of reasonably accessible values .",
    "having such estimators is of particular interest for a large classes of situations where exponents close to @xmath1 appear ( zipf s law ) .",
    "we will argue here that whenever dealing with data we can assume discrete and bounded samples spaces ( domains ) , which guarantees that power - laws are normalizable for arbitrary powers @xmath2 .",
    "we then show that the corresponding ml estimator can then also be used to estimate exponents from data that is sampled from continuous sample spaces , or from sample spaces that are not bounded from above .      in physics",
    "the theoretical understanding of a process sometimes provides us with the luxury of knowing the exact form of the distribution function that one has to fit to the data .",
    "for instance think of critical phenomena such as ising magnets in 2 dimensions at the critical temperature , where it is understood that the susceptibility follows a power - law of the form @xmath3 , with @xmath4 a critical exponent , that occasionally even can be predicted mathematically . however , often  and especially when dealing with complex systems  we do not enjoy this luxury and usually do not know the exact functions to fit to the data .    in such a case ,",
    "let us imagine that you have a data set and from first inspection you think that a power - law fit could be a reasonable thing to do .",
    "it is then essential , before starting with the fitting procedures , to clarify what one knows about the process that generated this data .",
    "the following questions may help to do so .",
    "plfit and r@xmath5plhistfit can be used .",
    "[ fig : diagram ] ]    * do you have information about the dynamics of the process that is generating what appears to be a power - law ? * is the data generated by a bernoulli process ( e.g. tossing dice ) , or not ( e.g. preferential attachment ) ?",
    "* is the data available as a collection of samples ( a list of measurements ) , or only coarse - grained in form of a histogram ( binned or aggregated data ) .",
    "* is the data sampled from a discrete ( e.g. text ) or continuous sample space ( e.g. earthquakes ) ? * does the data have a natural ordering ( e.g. magnitudes of earthquakes ) , or not ( e.g. word frequencies in texts ) ?",
    "the decisions one has to take before starting to estimate power - law exponents are shown as a decision - tree in fig .",
    "( [ fig : diagram ] ) . if it is known that the process generating the data is not a bernoulli process ( for example if the process belongs to the family of history dependent processes such as e.g. preferential attachment ) , then one has the chance to use this information for deriving parameter estimators that are tailored exactly for the particular family of processes .",
    "if no such detailed information is available one can only treat the process as if it were a bernoulli process , i.e. information about correlations between samples is ignored .",
    "if we know ( or assume ) that the data generation process is a bernoulli process , the next thing to determine is whether the data is available as a collection of data points , or merely as coarse grained information in form of a histogram that collects distinct events into bins ( e.g. histograms of logarithmically binned data ) .    if data is available in form of a data set of samples ( not binned ) , a surprisingly general maximum likelihood ( ml ) estimator can be used to predict the exponent of an underlying power - law @xmath6 .",
    "this estimator that we refer to as @xmath7 , will be derived in the main section .",
    "its estimates for the underlying exponent @xmath2 , are denoted by @xmath8 .",
    "the code for the corresponding algorithm we refer to as ` r_plfit ` .",
    "if information is available in form of a histogram of binned data , a different estimator becomes necessary .",
    "the corresponding algorithm ( ` r_plhistfit ` ) is discussed in appendix a and in the section below on discrete and continuous sample spaces .",
    "both algorithms are available as matlab code @xcite . for how to use these algorithms ,",
    "see appendix b.    if we have a dataset of samples ( not binned ) , so that the ` r_plfit ` algorithm can be used , it still has to be clarified whether the data has a natural order or not ?",
    "numerical observables such as earthquake magnitudes are _ naturally _ ordered .",
    "one earthquake is always stronger or smaller than the other .",
    "if observables are non - numeric , such as word types in a text , then a natural order can not be known _ a priori_. the natural order",
    "can only be inferred approximately by using so - called _ rank - ordering _ ; or alternatively  by using the so - called _ frequency distribution _ of the data .",
    "details are discussed below in the section on rank - order , frequency distributions , and natural order .",
    "other issues to clarify are to see if a given sample space is continuous or discrete , and if the sample space is bounded or unbounded .",
    "these questions however , turn out to be not critical .",
    "one might immediately argue that for unbounded power - law distribution functions normalization becomes an issue for exponents @xmath9 .",
    "however , this is only true for bernoulli processes on _ unbounded _ sample spaces . since all real - world data sets are collections of finite discrete values one never has to actually deal with normalization problems .",
    "moreover , since most experiments are performed with apparati with finite resolution , most data can be treated as being sampled from a bounded , discrete sample space , or as binned data . for truly continuous processes",
    "the probability of two sampled values being identical is zero .",
    "therefore , data sampled from continuous distributions can be recognized by sample values that are unique in a data set .",
    "see appendix a for more details .",
    "statistically sound ways to fit power - laws were advocated and discussed in @xcite .",
    "they overcome intrinsic limitations of the _ least square _",
    "( ls ) fits to logarithmically scaled data , which were and are widely ( and often naively ) used for estimating exponents .",
    "the ml estimator that was presented in @xcite we refer to as the @xmath10 ( for clauset - shalizi - newman ) estimator ; its estimates for the exponent we denote by @xmath11 .",
    "the approach that leads to @xmath10 focuses on continuous data @xmath12 that follows a power - law distribution from eq .",
    "( [ pow ] ) , and that is bounded from below @xmath13 but is not bounded from above ( i.e. @xmath14 with @xmath15 ) . in @xcite emphasis",
    "is put on how ml estimators can be used to infer whether an observed distribution function is likely to be a power - law or not .",
    "also the pros and cons of using cumulative distribution functions for ml estimates are discussed , together with ways of treating discrete data as continuous data . for the continuous and unbounded case ,",
    "simple explicit equations for the @xmath10 estimator can be derived .",
    "the continuous approach however , even though it seemingly simplifies computations , introduces unnecessary self - imposed limitations with respect to the range of exponents that can be reliably estimated .",
    "@xmath10 works brilliantly for a range of exponents between @xmath16 and @xmath17 .",
    "here we show how to overcome these limitations  and by doing so extend the accessible range of exponents  by presenting the exact methodology for estimating @xmath2 for discrete bounded data with the estimator @xmath7 . while this approach appears to be more constrained than the continuous one we can show also theoretically that data from continuous and potentially unbounded sample spaces can be handled within essentially the same general ml framework as well .",
    "the key to the @xmath7 estimator is that it is not necessary to derive explicit equations for finding @xmath8 .",
    "implicit equations in @xmath2 exist for power - law probability distributions over discrete or continuous sample spaces that are both bounded from below _ and _ above .",
    "solutions @xmath8 can be easily obtained numerically .",
    "an implementation of the respective algorithms can be found in @xcite , for a tutorial see appendix b.      there exist three distinct types of distribution functions that are of interest in the context of estimating power - law exponents : +    i : :    the _ probability distribution _",
    "@xmath18 assigns a    probability to every observable state - value @xmath12 .",
    "discrete    and bounded sample spaces are characterized by @xmath19    state - types @xmath20 , with each type @xmath21    being associated with a distinct value @xmath22 .",
    "ii : :    the _ relative frequencies _ , @xmath23 , where    @xmath24 is the number of times that state - type    @xmath21 is observed in @xmath25 experiments .",
    "@xmath26 is the _ histogram _ of the data . as    explained below in detail",
    ", the relative frequencies can be ordered in    two ways .",
    "+    @xmath27 if @xmath28 is ordered according to    their descending magnitude this is called the _",
    "rank ordered _    distribution .",
    "+    @xmath27 if @xmath28 is ordered according to the    descending magnitude of the probability distribution    @xmath29 , then they are _ naturally ordered _",
    "relative    frequencies .",
    "iii : :    the _ frequency distribution _ @xmath30 counts how many    state - types @xmath21 fulfill the condition @xmath31 .    in fig .",
    "( [ fig : rankvsfreq ] ) we show these distribution functions .",
    "there @xmath32 data points are sampled from @xmath33 , with probabilities @xmath34 .",
    "the probability distribution is shown ( red ) .",
    "the relative frequency distribution @xmath35 is plotted in natural order ( blue ) , the rank - ordered distribution is shown with the yellow line , which clearly exhibits an exponential decay towards the the tail .",
    "the inset shows the frequency distribution @xmath30 of the same data .",
    "we next discuss how different sampling processes can be characterized in terms of natural order , rank - order , or frequency distributions .      for some sampling processes the ordering of the observed states",
    "for example think of @xmath12 representing the numerical values of earthquake magnitudes . here",
    "any two observations @xmath12 and @xmath36 can be ordered with respect to their numerical value , or their _",
    "natural order_. since power - law distributions @xmath6 are monotonic this is equivalent to ranking observations according to the probability distribution @xmath37 they are sampled from : the most likely event has _",
    "natural rank _ @xmath38 , the second most likely rank @xmath39 , etc . in other words",
    ", we can order state - types @xmath12 in a way that over the sample space @xmath40 , @xmath41 is a monotonic and decreasing function .     with an exponent @xmath42 ( red line ) .",
    "the relative frequencies @xmath28 are shown for @xmath32 sampled data points according to their natural ( prior ) ordering that is associated with @xmath37 ( blue ) . the rank - ordered distribution ( posterior )",
    "is shown in yellow , where states @xmath21 are ordered according to their observed relative frequencies @xmath28 .",
    "the rank - ordered distribution follows a power - law , except for the exponential decay that starts at rank@xmath43 .",
    "a low frequency cut - off should be used to remove this part for estimating exponents .",
    "the inset shows the frequency distribution @xmath30 that describes how many states @xmath12 appear @xmath44 times ( green ) .",
    "the frequency distribution has a maximum and a power - law tail with exponent @xmath45 . to estimate @xmath46",
    ", one should only consider the tail of the frequency distribution function .",
    "[ fig : rankvsfreq ] ]      if @xmath37 is not known _ a priori _ because the state - types @xmath21 have no numerical values @xmath47 attached , as happens for example with words in a text , we can only count relative frequencies @xmath28 ( a normalized histogram ) of states of type @xmath21 , _ a posteriori _ ,",
    "i.e. after sampling . to be clear ,",
    "let @xmath26 be the histogram of @xmath25 recorded states .",
    "@xmath24 is the number of times we observed type @xmath21 , then @xmath23 is the relative frequency of observing states of type @xmath21 .",
    "after all samples are taken , one can now order states with respect to @xmath28 , such that the rank @xmath38 is assigned to state @xmath21 with the largest @xmath28 , rank @xmath39 to @xmath48 with the second largest @xmath49 , etc .",
    "@xmath50 is called the _ rank - ordered _ distribution of the data .    the natural order imposed by @xmath37 and",
    "the rank - order imposed by @xmath35 are not identical for finite @xmath25 .",
    "however , if data points have been sampled independently , then @xmath35 converges toward @xmath37 ( for @xmath51 ) and the rank - order induced by @xmath35 will asymptotically approach the natural order induced by @xmath37 . the highest uncertainty on estimating the order induced by @xmath37 using @xmath35",
    "is associated with the least frequent observations .",
    "therefore , when estimating exponents from rank - ordered distributions , one might consider to use a low - frequency cut - off to exclude infrequent data .",
    "exponents of power - laws can also be estimated from _ frequency _",
    "distributions @xmath30 .",
    "these counts how many distinct state - types @xmath21 occur exactly @xmath44 times in the data .",
    "it does not depend on the natural ( prior ) order of states and therefore is sometimes preferred to the ( posterior ) rank - ordered distribution .",
    "however , complications may appear also when using @xmath30 .",
    "the frequency distribution @xmath30 that is associated with a power - like probability distribution @xmath52 ( and asymptotically to @xmath35 ) is not an exact power - law but a non - monotonic distribution ( with a maximum ) .",
    "only its tail decays as a power - law , @xmath53 .",
    "the exponents @xmath2 and @xmath46 are related through the well known equation @xmath54 if the probability distribution has exponent @xmath2 , the tail of the associated frequency distribution has exponent @xmath46 . since the frequency distribution behaves like a power - law only in its tail , estimating @xmath46 makes it necessary to constrain the observed data to large values of @xmath44 .",
    "note that this is equivalent to using a low - frequency cut - off .",
    "one option to do that is to derive a maximum entropy functional for @xmath30 and fit the resulting ( approximate ) max - ent solution to the data .",
    "we do not follow this route here .",
    "if the natural order of the data is known , one can directly use the natural ordered data in the ml estimates for the exponents . if it is not known , either the rank - ordered distribution can be used to estimate @xmath2 , or the frequency distribution to estimate @xmath46 , see fig .",
    "( [ fig : diagram ] ) .",
    "one might also estimate both , @xmath2 in the rank ordered distribution , and @xmath46 in the frequency distribution of the data .",
    "( [ alphalambda ] ) to compare the two estimates may be used as a rough quality - check .",
    "if estimates do not reasonably coincide one should check whether the used data ranges have been appropriately chosen .",
    "if large discrepancies remain between @xmath46 and @xmath55 this might indicate that the observed distribution function in question is only an approximate power - law , for which eq .",
    "( [ alphalambda ] ) need not hold . for a tutorial on how to use ` r_plfit ` to perform estimates see appendix b.      data can originate from continuous sample spaces @xmath56 $ ] , or discrete ones @xmath57 .",
    "to each state - type @xmath20 , there is assigned a state - value @xmath47 .",
    "whether a distribution function @xmath58 , with @xmath59 , is normalizable or not , can only be decided once the sample space @xmath60 has been specified .",
    "the normalization factors for continuous and discrete @xmath60 are @xmath61 for bounded sample spaces with @xmath62 , power - laws are always normalizable for arbitrary exponents @xmath2 , and a well defined ml estimator of @xmath8 exists ( see below ) .",
    "the normalization constants in eq .",
    "( [ norm ] ) can be specified in ` r_plfit ` ( see appendix b ) .",
    "data sampled from a continuous sample space @xmath63 can essentially be treated as if it were sampled from a discrete sample space @xmath64 , where @xmath65 are given by the unique collection of distinct values in the data set .",
    "that is , the data set @xmath66 contains @xmath25 data points @xmath67 ( that have @xmath19 unique values @xmath47 , the states of type @xmath21 ) which we collect in the discrete sample space @xmath68 . for truly continuous data we have @xmath69 , since the probability of @xmath70 for @xmath71 is vanishing . as a consequence the histogram @xmath24 , which counts the number of times @xmath47 appears in the data , is essentially given by @xmath72 for all @xmath20",
    "this provides us with a practical criterion for when to use the normalization constant for discrete or continuous data . for details",
    "see appendix a.    the equation for the ml estimator @xmath7 , that yields the estimate @xmath8 , only requires the knowledge of the relative frequency distribution @xmath23 ( in natural- or rank - order ) of the observed state - types @xmath21 , as we will see in eq .",
    "( [ bayes4 ] ) below .",
    "therefore ` r_plfit ` can work either with data sets @xmath12 or histograms @xmath73 over the unique values in the data sets . if data comes in coarse grained form , i.e. histograms , where each bin may contain a whole range of observable values @xmath12 , then an estimator is required that is different from @xmath7 @xcite , see also appendix a. the corresponding code ` r_plhistfit ` can also be downloaded from @xcite .",
    "consider a family of random processes @xmath75 that is characterized by the parameters @xmath76 .",
    "let @xmath75 be defined on a discrete sample space @xmath77 , with @xmath78 .",
    "the process @xmath75 samples values @xmath79 with probability , @xmath80 let us repeat the process @xmath75 in @xmath25 independent experiments to obtain a data set @xmath81 .",
    "@xmath26 is the histogram of the events recorded in @xmath82 , i.e. @xmath24 is the number of times @xmath47 appears in @xmath82 .",
    "note that @xmath83 . as a consequence of independent sampling , the probability to sample",
    "exactly @xmath73 is , @xmath84 where @xmath85 is the multinomial factor .",
    "bayes formula allows us to get an estimator for the parameters @xmath86 , @xmath87 obviously , @xmath88 does not depend on @xmath86 . without further available information",
    "we must assume that the parameters @xmath86 are uniformly distributed between their upper and lower limits .",
    "as a consequence , @xmath89 also does not depend on @xmath86 within the limits of the parameter range and can be treated as a constant such as @xmath2 does not work for parameters such as @xmath90 and @xmath91 . for those variables it turns out that @xmath89 can not be assumed to be constant between upper and lower bounds of the respective parameter values .",
    "bayesian estimators for @xmath90 and @xmath91 require to explicitly consider a non - trivial function @xmath89 .",
    "though in principle feasible , we ignore the possibility of deriving bayesian estimates for @xmath90 and @xmath91 in this paper . ] . from eq .",
    "( [ bayes ] ) it follows that the value @xmath92 that maximizes @xmath93 also maximizes @xmath94 .",
    "the most likely parameter values @xmath95 are now found by maximizing the log - likelihood , @xmath96 for all parameters @xmath97 . here",
    "@xmath98 , is the so - called _ cross - entropy_. in other words , ml - estimates maximize the cross - entropy with respect to the parameters @xmath99 .      to apply eq .",
    "( [ bayes3 ] ) for ml - estimates of power - law exponents , one specifies the finite sample space @xmath77 , and the family of probability density functions is , @xmath100 with @xmath59 .",
    "note that the set of parameters @xmath86 defined above now only contains @xmath2 , or @xmath101 .",
    "the normalization constant is @xmath102 .",
    "the derivative with respect to @xmath2 of the cross - entropy , @xmath103 , has to be computed , and setting @xmath104 yields @xmath105 the solution to this implicit equation , @xmath106 , can not be written in closed form but can be easily solved numerically .",
    "see @xcite for the corresponding algorithm and appendix b for a tutorial .",
    "one possibility to find the solution @xmath106 from the implicit equation eq .",
    "( [ bayes4 ] ) , is to iteratively refine approximate solutions . for this , select @xmath107 values @xmath2 from the interval @xmath108 $ ] , where @xmath109 is a finite fixed number , say @xmath110 .",
    "those values may be chosen to be given by the expression @xmath111 for @xmath112 .",
    "the parameters @xmath113 and @xmath114 are defined in the following way : first define @xmath115 , and @xmath116 , where @xmath117 and @xmath118 are parameters of the algorithm .",
    "then define @xmath119 with @xmath120 . if @xmath121 is the optimal solution of eq .",
    "( [ bayes4 ] ) for some @xmath122 , then we can choose @xmath123 , and @xmath124 and @xmath125 .",
    "one then continues by iterating @xmath126 times until @xmath127 , where @xmath128 is the desired accuracy of the estimate of @xmath8 . as a consequence ,",
    "the value @xmath129 , for which @xmath130 holds , optimally estimates @xmath8 in the @xmath126th iteration with an error smaller than @xmath128 .",
    "note that @xmath128 is the error of the @xmath7-estimator with respect to the exact value of the predictor @xmath8 , and is not the error of @xmath8 with respect to the ( typically unknown ) value of the exponent @xmath2 of the sampling distribution .    , and @xmath7 .",
    "for @xmath131 values of @xmath2 in the range between 0 and 4 , we sample @xmath132 events from @xmath133 , from a power - law probability distribution @xmath134 .",
    "the estimated exponents @xmath135 for the estimators @xmath136 ( red ) , the @xmath10 ( green , @xmath137 ) , and the new @xmath7 ( black , @xmath138 ) , are plotted against the true value of the exponent @xmath2 of the probability distribution samples are drawn from .",
    "clearly , below @xmath139 the @xmath10 estimator no longer works reliably . @xmath10 and @xmath7 work equally well in a range of @xmath140 . outside this range",
    "@xmath7 performs consistently better than the other methods .",
    "the inset shows the mean - square error @xmath141 of the estimated exponents .",
    "the ls - estimator has a much higher @xmath141 over the entire region , than the @xmath7-estimator .",
    "the blue dot represents the @xmath7 estimate for the zipf exponent of c. dickens `` a tale of two cities '' .",
    "clearly , this exponent could never reliably be obtained from the rank ordered distribution using @xmath10 , whereas @xmath7 works fine even for values of @xmath142 .",
    "[ fig : comparison ] , title=\"fig : \" ] +    controlling the fit region over which the power - law should be obtained therefore becomes a matter of restricting the sample space to a convenient @xmath143 .",
    "this can be used for dynamically controlling low - frequency cut - offs .",
    "these cut - offs are set to exclude states for which , @xmath144 where @xmath145 is the minimal number of times that any state - type @xmath21 is represented in the data set .",
    "this means that we re - estimate @xmath2 on @xmath143 with @xmath146 we see in eq .",
    "( [ bayes4 ] ) that iteratively adapting @xmath60 to subsets @xmath147 , and then re - evaluating @xmath2 , requires to solve , @xmath148 where @xmath149 is the restricted sample - size and @xmath150 are the relative frequencies re - normalized for @xmath147 .",
    "@xmath151 is the index - set of @xmath147 .    iterating this procedure either leads to a fixed point or to a limit cycle between two low - frequency cut - offs with two slightly different estimates for @xmath8 .",
    "these two possibilities need to be considered in order to implement an efficient stopping criterion for the iterative search of the desired low - frequency cut - off in the data .",
    "the algorithm therefore consists of two nested iterations .",
    "the `` outer iteration '' searches for the low - frequency cut - off , the `` inner iteration '' solves the implicit equation for the power - law exponent . the matlab code for the algorithm is found in @xcite , see appendix b for a tutorial .",
    "to test the proposed algorithm implementing the estimator @xmath7 , we first perform numerical experiments and then test its performance on a number of well known data sets .      for 400 different values of @xmath2 , ranging from @xmath152 to @xmath153 , we sample @xmath132 data points @xmath154 , with @xmath155 states , with probabilities @xmath134 .",
    "we fit the data in three ways , using ( i ) least square fits ( ls ) , ( ii ) the csn algorithm @xmath10 providing estimates @xmath11 , and ( iii ) the implicit @xmath7 method providing estimates @xmath8 . in fig .",
    "[ fig : comparison ] we show these estimates for the power exponents , as a function of the true _ values _ of @xmath2 .",
    "the @xmath136 , @xmath10 , @xmath7 estimators are shown as the red , green , and black curves respectively . obviously @xmath7 and @xmath10 work equally well for power - law exponents @xmath2 with values @xmath140 . in this range",
    "the three approaches coincide .",
    "however , note that in the same region the mean square error , where @xmath156 is the number of repetitions , i.e. the number of data - sets we sampled from the @xmath157 , @xmath158 .",
    "@xmath159 is the value estimated for @xmath2 from the @xmath160th data set .",
    "depending on the estimator @xmath135 corresponds to @xmath11 ( @xmath10 ) , @xmath8 , ( @xmath7 ) , or the ls estimator .",
    "we used @xmath155 and @xmath161 for any given @xmath2 . ] @xmath141 for the ls method is much larger than for @xmath7 and @xmath10 .",
    "outside this range the assumptions and approximations used for @xmath10 start to lose their validity and both @xmath136 and @xmath7 estimates outperform the @xmath10 estimates .",
    "the inset also shows that @xmath7 consistently estimates @xmath2 much better than the @xmath136 estimator ( two orders of magnitude better in terms of @xmath141 ) for the entire range of @xmath2 .",
    "the blue dot in fig .",
    "[ fig : comparison ] represents the @xmath7 estimate for the zipf exponent of c. dickens ` a tale of two cities ' .",
    "clearly , this small exponent could never be obtained by @xmath10 , see also tab .",
    "[ table1 ] .",
    ".comparison of the estimators @xmath7 and @xmath10 on empirical data sets that were used in @xcite .",
    "these include the frequency of surnames , intensity of wars , populations of cities , earthquake intensity , numbers of religious followers , citations of scientific papers , counts of words , wealth of the forbes 500 firms , numbers of papers authored , solar flare intensity , terrorist attack severity , numbers of links to websites , and forest fire sizes .",
    "we added the word frequencies in the novel  a tale of two cities \" ( c. dickens ) .",
    "the second column states if @xmath46 or @xmath2 were estimated .",
    "the exponents reported in @xcite are found in column @xmath162 , those reproduced by us applying their algorithm to data @xcite is shown in column @xmath163 .",
    "the latter correspond well with the new @xmath7 algorithm . for values",
    "@xmath164 , @xmath165 can not be used .",
    "we list the corresponding values for kolmogorov - smirnov test for the two estimators , @xmath166 and @xmath167 .",
    "[ table1 ] [ cols=\"<,^,^,^,^,^,^\",options=\"header \" , ]      we finally compare the new estimator @xmath7 on several empirical data sets that were used for demonstration in @xcite . in tab .",
    "[ table1 ] we collect the results .",
    "the second column states if @xmath2 or @xmath46 were estimated .",
    "column @xmath162 presents the value of the estimator @xmath10 as presented in @xcite .",
    "column @xmath163 contains the values of the same estimator using the data from @xcite and using the algorithm provided by @xcite .",
    "the results for the @xmath7 estimator agrees well with those of @xmath10 in the range where the latter works well . to demonstrate how @xmath7 works perfectly outside of the comfort zone of @xmath10 ( for @xmath164 )",
    ", we add the result of the rank distribution of word counts in the novel  a tale of two cities \" ( charles dickens , 1859 ) , which shows an exponent of @xmath168 .",
    "this exponent can be fitted directly from the data using the proposed @xmath7 algorithm , while @xmath10 can not access this range , at least not without the detour of first producing a histogram from the data and then fitting the tail of the frequency distribution .",
    "the values for the corresponding kolmogorov - smirnov tests ( see e.g. @xcite ) for the two estimates , @xmath166 and @xmath167 , are similar for most cases .",
    "we discuss the generic problem of estimating power - law exponents from data sets .",
    "we list a series of questions that must be clarified before estimates can be performed .",
    "we present these questions in form of a decision tree that shows how the answers to those questions lead to different strategies for estimating power - law exponents .    to follow this decision tree",
    "can be seen as a recipe for fitting power exponents from empirical data .",
    "the corresponding algorithms were presented and can be downloaded as matlab code .",
    "the two algorithms we provide are based on a very general ml estimator that maximizes an appropriately defined cross entropy .",
    "the method can be seen as a straight forward generalization of the idea developed in @xcite .",
    "the two estimators ( one for binned histograms and @xmath7 for raw data sets ) allow us to estimate power - law exponents in a much wider range than was previously possible .",
    "in particular , exponents lower than @xmath164 can now be reliably obtained .",
    "this work was supported in part by the austrian science foundation fwf under grant p29252 .",
    "b.l . is grateful for the support by the china scholarship council , file - number 201306230096 .",
    "10 url # 1`#1`urlprefix[2]#2 [ 2][]#2 m.e.j .",
    "newman , _ power - laws , pareto distributions and zipf s law _ , contemporary physics 2005 ; * 46 * 32351 .    m. mitzenmacher , _ a brief history of generative models for power - law and lognormal distributions _ , internet mathematics 2004 ; * 1 * 22651",
    "kadanoff , et al . , _",
    "static phenomena near critical points : theory and experiment _ , rev .",
    "1967 ; * 39 * 395413 .",
    "d. sornette , _ critical phenomena in natural sciences _ , springer , berlin , 2006 .",
    "p. bak , c. tang , and k. wiesenfeld , _ self - organized criticality : an explanation of 1/f noise _ , phys .",
    "lett . 1987 ; * 59 * 38184 .",
    "simon , _ on a class of skew distribution functions _ , biometrika 1955 ; * 42 * 42540",
    ".    a.rka , and a.l .",
    "barabsi , _ statistical mechanics of complex networks _ , rev . mod . phys .",
    "2002 ; * 74 * 4797 .",
    "barabsi , and a. rka , _ emergence of scaling in random networks _ ,",
    "science 1999 ; * 286 * 509 - 12 .",
    "yule , _ a mathematical theory of evolution , based on the conclusions of dr .",
    "j. c. willis , f.r.s _ , phil .",
    "royal soc .",
    "b 1925 ; * 213 * 2187 .",
    "h. takayasu , a .- h .",
    "sato , and m. takayasu , _ stable infinite variance fluctuations in randomly amplified langevin systems _ , phys .",
    "1997 ; * 79 * 96667 .    c. tsallis , _ introduction to nonextensive statistical mechanics _ , springer , new york , 2009 .",
    "r. hanel , s. thurner , s , and m. gell - mann , _ how multiplicity of random processes determines entropy : derivation of the maximum entropy principle for complex systems _ , proc .",
    "usa 2014 ; * 111 * 690510",
    ".    b. corominas - murtra , r. hanel , and s. thurner , _ understanding scaling through history - dependent processes with collapsing sample space _ ,",
    "usa 2015 ; * 112 * , 5348 - 53",
    ".    b. gutenberg , and c.f .",
    "richter , _ frequency of earthquakes in california _ , bull .",
    "1944 ; * 34 * 18588 .",
    "k. christensen , l. danon , t. scanlon , and p. bak , _ unified scaling law for earthquakes _ proc .",
    "usa 2002 ; * 99 * 2509 - 13",
    ".    f. auerbach , _ das gesetz der bevlkerungskonzentration _",
    ", petermanns geographische mitteilungen 1913 ; * 59 * 74 - 76 .",
    "x. gabaix , _",
    "zipf s law for cities : an explanation _ ,",
    "1999 ; * 114 * 73967 .",
    "shaffer , _ spatial foraging in free ranging bearded sakis : traveling salesmen or lvy walkers ? _ , amer .",
    "j. primatology 2014 ; * 76 * 47284 .",
    "newman , and r.g .",
    "palmer , _ modeling extinction _ , oxford university press , 2003 .",
    "zipf , _ human behavior and the principle of least effort _ , addison - wesley , cambridge , massachusetts , 1949 .",
    "press , _ subjective and objective bayesian statistics : principles , models , and applications _ , wiley series in probability and statistics , 2010 .",
    "berger , _ statistical decision theory and bayesian analysis _ , springer , new york , 1985 .",
    "fisher , _ on an absolute criterion for fitting frequency curves _ , messenger of mathematics 1912 ; * 41 * 15560 .",
    "a. clauset , c.r .",
    "shalizi , and m.e.j .",
    "newman , _ power - law distributions in empirical data _ , siam review 2009 ; * 51 * 661703",
    ".    y. virkar , and a. clauset , _ power - law distributions in binned empirical data _ , annals of applied statistics 2014 ; * 8 * 89119 .",
    "a. deluca , and a. corral , _ fitting and goodness - of - fit test of non - truncated and truncated power - law distributions _",
    "acta geophysica 2013 ; * 61 * 135194    a. broder , r. kumar , f. maghoul , p. raghavan , s. rajagopalan , r. stata , a. tomkins , and j. wiener , _ graph structure in the web _ , computer networks 2000 ; * 33 * 30920 .    d.c .",
    "roberts , and d.l .",
    "turcotte , _ fractality and self - organized criticality of wars _ , fractals 1998 ; * 6 * 35157",
    ".    s. redner , _ how popular is your paper ?",
    "an empirical study of the citation distribution _ , epj b 1998 ; * 4 * 13134 .",
    "a. clauset , m. young , and k.s .",
    "gleditsch , _ on the frequency of severe terrorist events _ , journal of conflict resolution 2007 ; * 51 * 5887 .",
    "http://tuvalu.santafe.edu/@xmath169aaronc/powerlaws/ http://www.complex-systems.meduniwien.ac.at/ + si2016/r_plfit.m + http://www.complex-systems.meduniwien.ac.at/ + si2016/r_plhistfit.m h.s .",
    "heaps , _ information retrieval : computational and theoretical aspects _ , academic press , 1978 .",
    "g. herdan , _ type - token mathematics _ ,",
    "gravenhage , mouton & co , 1960 .",
    "if events @xmath12 are drawn from a continuous sample space @xmath170 $ ] , for instance the magnitude of earthquakes , then the ` natural order ' of possible events is simply given by the magnitude @xmath12 of the observation .",
    "events @xmath12 are drawn from a continuous power - law distribution @xmath171 , with @xmath172)$ ] ( compare eq .",
    "( [ norm ] ) first line ) .    to work with well defined probabilities we have to bin the data first .",
    "probabilities to observe events within a particular bin depend on the margins of the @xmath19 bins @xmath173 , with @xmath174 and @xmath175 .",
    "the histogram @xmath26 counts the number @xmath24 of events @xmath12 falling into the bin @xmath176 , and the probability of observing @xmath12 in the @xmath21th bin is given by @xmath177 binning events sampled from a continuous distribution may have practical reasons .",
    "for instance data may be collected from measurements with different physical resolution levels , so that binning should be performed at the lowest resolution of data points included in the collection of samples .",
    "we will not discuss the ml estimator for binned data in detail but only remark that for given bin margins @xmath178 it is sufficient to insert @xmath179 of eq ( [ appa1 ] ) into eq .",
    "( [ bayes3 ] ) with @xmath101 , to derive the appropriate ml condition for binned data .",
    "an algorithm for binned data ` r_plhistfit ` , where we assume the bin margins @xmath180 to be given , is found in @xcite .",
    "we point out that if margins for binning have not been specified prior to the experiments , then specifying the optimal margins for binning the data becomes a parameter estimation problem in itself , i.e. the optimal margins @xmath180 have to be estimated from the data as well .",
    "one major source of uncertainty in the estimates of @xmath2 from binned data is related to the uncertainty in choosing the upper and lower bounds @xmath90 and @xmath91 of the data , i.e. specifying the bounds of the underlying continuous sample space .",
    "binning becomes irrelevant for clean continuous data for the following reason .",
    "suppose we fix the sample space @xmath181 $ ] and cut this domain into @xmath109 bins of width @xmath182 .",
    "since the data @xmath183 is drawn from a continuous sample space , the chance for two observations @xmath184 and @xmath67 to be exactly equal becomes zero for @xmath185 , if @xmath109 has been chosen sufficiently large .",
    "then each bin almost certainly contains either one sample @xmath67 or none .",
    "the probability of observing @xmath12 then is asymptotically ( as @xmath186 approaches zero ) given by @xmath187 the parameter estimation problem of finding the optimal @xmath2 is equivalent to maximizing @xmath188 ( or equivalently @xmath189 ) with respect to @xmath2 . in this maximization problem",
    "@xmath186 becomes irrelevant and only the choice of @xmath190 and @xmath191 and the data @xmath12 remains relevant for the estimate . as a consequence ,",
    "one obtains an equation @xmath192 for the ml estimate of the exponent @xmath2 over continuous sample spaces .",
    "equation ( [ bayes4 ] ) and eq .",
    "( [ bayesx ] ) differ only in @xmath193 . in eq .",
    "( [ bayes4 ] ) the normalization constant of discrete samples spaces gets used while in eq .",
    "( [ bayesx ] ) @xmath193 is the normalization constant for a continuous sample space .",
    "switching between continuous and discrete sample spaces therefore is simply a matter of choosing the one or the other normalization constant in the algorithm .",
    "whether data should be assumed to be sampled from continuous or discrete sample spaces is not always totally clear .",
    "many measurements have an intrinsic resolution and implicitly bin the data .",
    "for instance if real numbers sampled in an experiment are given only with a three digit precision , such as @xmath194 and we know that @xmath195 and @xmath196 then we better treat the data as discrete data on @xmath197 if we have sufficiently many samples for the histogram over @xmath64 not to be flat . a primitive test to see whether one should regard data as sampled from a continuous sample space or not is to make a histogram over the unique values of the recorded data .",
    "if each distinct value appears only once in the data ( i.e. if the histogram over the unique data - points is flat ) then one should treat the sample - space as continuous .",
    "while for the discrete case we need not estimate @xmath90 and @xmath91 this remains necessary for the continuous case .",
    "the method of cutting the @xmath198 $ ] into segments of length @xmath186 and then taking @xmath186 to zero explains why typically tha _ primitive _ estimates , @xmath199 and @xmath200 , provides fairly good results .",
    "alternatively , strategies such as suggested in @xcite could be used to optimize the choices for @xmath90 and @xmath91 . however",
    ", this procedure can not be directly derived from bayesian arguments .",
    "neither will we discuss this approach in this paper nor implement such an option in ` r_plfit ` .",
    "however , bayesian estimates of @xmath90 and @xmath91 exist .",
    "although we will not discuss those estimators in detail here we will eventually implement them in ` r_plfit ` to replace the primitive estimates .",
    "the idea of constructing such estimators is the following .",
    "for instance , one asks how likely can the maximal value @xmath201 of the sampled data @xmath66 be found to be larger than some value @xmath82 . by deriving @xmath202)$ ] and @xmath203)$ ] , as a consequence , it becomes possible to derive bayesian estimators for @xmath90 and @xmath91 .",
    "the matlab function    ` function out = r_plfit(data , varargin ) `    implements the algorithm discussed in the main paper .",
    "the function returns a struct ` out ` that contains information about the data , the data range , but most and for all ` out.exponent ` returns the estimated exponent of the power - law .",
    "whether the exponent ` out.exponent ` is the exponent @xmath2 of the sample distribution or the exponent @xmath46 of the frequency distribution of the data depends on how ` function out = r_plfit(data , varargin ) ` gets used as explained below . in the code the sample space @xmath60 is equivalent to a vector @xmath204 $ ] containing @xmath19 distinct event magnitudes @xmath47 , @xmath20 .",
    "+ the variable ` data ` can be used to import data while a variable number of arguments can be set by ` varargin ` to tell the algorithm which type of data it should handle and to control the range of the data . by default the only argument that has to be set is ` data ` . `",
    "r_plfit ` filters data from data points ` data<=0 , nan , inf ` .",
    "the data passed on to ` data ` can be    * a vector of observations ` data ` @xmath205 $ ] ( default ) * a histogram ` data ` @xmath206 $ ] of recorded event types @xmath20    ` out = r_plfit(data , varargin ) ` can be used in three basic modes    * ` out = r_plfit(x ) ` returns the estimated exponent @xmath2 of the * probability distribution * given the observation @xmath12 ( default ) * ` out = r_plfit(k,'hist ' ) ` returns the estimated exponent @xmath2 of the * probability distribution * given the histogram of observations @xmath73 * ` out = r_plfit(k ) ` returns the estimated exponent @xmath46 of the * frequency distribution * given the histogram of observations @xmath73    the third mode ` out = r_plfit(k ) ` is in fact identical to the first mode ` out = r_plfit(x ) ` , only that passing a histogram as sample data to the algorithm is identical to asking how many of the @xmath19 states @xmath21 have been observed @xmath44 times .",
    "but this is exactly the frequency distribution of the process , which possesses a tail with exponent @xmath207 .",
    "depending on the mode ` r_plfit ` returns the exponent @xmath2 or @xmath46 in ` out.exponent ` + * fitting with observations @xmath12 * : if we run ` out = r_plfit(x ) ` without further options ` r_plfit ` assumes by default that the data @xmath12 consists of natural numbers , and that the process samples have been sampled from the sample space @xmath208 , i.e. @xmath209 .",
    "if this is not the case one can either specify the data range using all @xmath19 _ unique _ values @xmath204 $ ] occurring in the data @xmath12 by using the option ` out = r_plfit(x,'urange ' ) ` . in order to define a fit range maximal and minimal data values taken into account",
    "can be set by ` out = r_plfit(x,'urange','rangemin',minval , ... ` ` ... ' rangemax',maxval ) ` such that ` r_plfit ` only takes into account data in the range ` minval ` @xmath210 ` maxval ` . to control the data range individually use ` out = r_plfit(x,'range',z ) ` .",
    "if the data has been sampled from a continuous sample space , and the histogram over the unique data is flat , i.e. each value in the data only appears once ( more or less ) , then one can tell ` r_plfit ` that the data is sampled from a continuous sample space by setting the option ` ' cdat ' ` , i.e. by running ` out = r_plfit(x,'cdat ' , ... ) ` .",
    "this option tells the algorithm to use the normalization constant for continuous sample spaces and estimates @xmath211 and @xmath212 . moreover , ` ' cdat ' ` implicitly sets the ` ' urange ' ` and the ` ' nolf ' ` option . ` ' nolf ' ` ( see below ) switches off the search of the algorithm for an optimal low frequency cut - off . + * fitting with histograms @xmath73 * : using histograms @xmath73 as input works in exactly the same way as for fitting @xmath12 if we want to estimate the exponent @xmath46 of the frequency distribution and use ` r_plfit ` in the ` out = r_plfit(k ) ` mode .",
    "if we use ` r_plfit ` in the ` out = r_plfit(k,'hist ' ) ` mode , the algorithm assumes by default that the sample space @xmath213 is given by @xmath214 $ ] .",
    "the option ` ' urange ' ` has no effect in this mode and gets ignored if set .",
    "otherwise one can again use the ` ' range ' ` property to set the event magnitudes @xmath213 ( the sample space ) using ` out = r_plfit(k,'hist','range',z ) ` .",
    "the ` ' minrange ' ` and ` ' maxrange ' ` options work in exactly the same way as before .",
    "+ * dynamic low frequency cut - off * : by default ` r_plfit(data ) ` runs an iterative search for an optimal low frequency cut - off that is set at a range value @xmath47 such that the expected number of samples for @xmath47 equals the variable @xmath215 ( default value @xmath38 , reset using option ` ' nmin ' ` ) .",
    "this means the algorithm performs a low frequency cut - off for observations @xmath12 .",
    "if however ` maxval ` is smaller than the predicted cut - off then the low frequency cut - off has no effect .",
    "one should note that in the mode ` out = r_plfit(k ) ` the low frequency cut - off mechanism effectively acts as a high frequency cut - off with respect to the data @xmath12 .",
    "one can switch this mechanism off by setting the option ` ' nolf ' ` ( no low frequency cut - off ) .",
    "+ the ` ' plot ' ` option , ` out = r_plfit(data , ... ` ` ... , ' plot ' ) ` , can be used for visualization . `",
    "r_plfit ` plots the fit over the data in double logarithmic coordinates ( loglog plot ) .",
    "using the option ` ' figure ' ` behaves like ` ' plot ' ` but explicitly opens a new figure . ` ' exp_min ' ` can be used to specify the minimal search value for the exponents ( default is @xmath152 ) and ` ' exp_max ' ` to set the maximal search value ( default is @xmath216 ) . ` ' eps ' ` can be used to set the precision of the implicit algorithm ( default @xmath217 ) .",
    "several other options exist to control the performance of the algorithm , which all can be listed by using ` r_plfit('help ' ) ` in the command line , which prints a brief manual on the usage of ` r_plfit ` and available options .",
    "if one works with binned data , e.g. histogram data counting the number of events falling into exponentially scaled bins ( log - binning ) , then ` r_plhistfit ` needs to be used instead of ` r_plfit ` .",
    "the function ` function out = r_plhistfit(data , varargin ) ` like ` r_plfit ` , by default , uses only data as input and other variables can be set optionally . `",
    "data ` is always a histogram @xmath73 that is a vector @xmath218 $ ] .",
    "bins can be specified by giving bin margins @xmath219 $ ] such thatevents counted in @xmath24 had a magnitude @xmath12 such that @xmath220 .",
    "usage , ` r_plhistfit(k,'margins',b ) ` . by default ` r_plhistfit ` assumes that @xmath221 .",
    "other options work similar to the ones available for ` r_plfit ` and can be reviewed by typing ` r_plhistfit('help ' ) ` in the matlab command line ."
  ],
  "abstract_text": [
    "<S> it has been repeatedly stated that maximum likelihood ( ml ) estimates of exponents of power - law distributions can only be reliably obtained for exponents smaller than minus one . </S>",
    "<S> the main argument that power laws are otherwise not normalizable , depends on the underlying sample space the data is drawn from , and is true only for sample spaces that are unbounded from above . </S>",
    "<S> here we show that power - laws obtained from bounded sample spaces ( as is the case for practically all data related problems ) are always free of such limitations and maximum likelihood estimates can be obtained for arbitrary powers without restrictions . here </S>",
    "<S> we first derive the appropriate ml estimator for arbitrary exponents of power - law distributions on bounded discrete sample spaces . </S>",
    "<S> we then show that an almost identical estimator also works perfectly for continuous data . </S>",
    "<S> we implemented this ml estimator and discuss its performance with previous attempts . </S>",
    "<S> we present a general recipe of how to use these estimators and present the associated computer codes . </S>"
  ]
}