{
  "article_text": [
    "in this paper , we tackle the problem of employing infinite - dimensional covariance descriptors ( covds ) for classification .",
    "covds are becoming increasingly popular in many computer vision tasks due to their robustness to measurement variations  @xcite .",
    "such descriptors take the form of , , region covariance matrices for pedestrian detection  @xcite and texture categorization  @xcite , human joint covariances for activity recognition  @xcite , and covariance matrices of the local brownian motion of water molecules in diffusion tensor imaging ( dti )  @xcite .",
    "as the name implies , covds are obtained by computing the second order statistics of feature vectors extracted at a finite number of observation points , such as the pixels of an image .",
    "the resulting descriptors are symmetric positive definite ( spd ) matrices and naturally lie on non - linear manifolds known as tensor , or spd manifolds . as a consequence , euclidean geometry is often not appropriate to analyze covds  @xcite . to overcome the drawbacks of euclidean geometry and better",
    "account for the riemannian structure of covds , state - of - the - art methods make use of non - euclidean metrics ( ,  @xcite ) . in particular , bregman divergences have recently been successfully employed in a number of covd - based applications  @xcite .",
    "nevertheless , all previous studies work with relatively small covds ( , at most @xmath0 , to the best of our knowledge ) built from feature vectors whose dimension is typically much smaller than the number of observations .",
    "while this could be thought of as a filtering operation , it also implies that the information encoded in such a covd is inherently poorer than the information jointly contained in all the observations .",
    "recently , it was shown that covds could be mapped to reproducing kernel hilbert space ( rkhs ) via the use of spd - specific kernels  @xcite . while this may , to some degree , enhance the discriminative power of the low - dimensional covds , it is unlikely to be sufficient to entirely recover the information lost when constructing them .    in this paper",
    ", we overcome this issue by introducing an approach to building and analyzing infinite - dimensional covds from a finite number of observations . to this end , we map the original features to rkhs and compute covds in the resulting space . since the dimensionality of the rkhs is much larger than the dimensionality of the observations , the resulting descriptor will encode more information than a covd constructed in the original lower - dimensional space , and is therefore better suited for classification .    in practice , of course , the mapping to rkhs is unknown and the covds can not be explicitly computed . however , here , we show that several bregman divergences can be derived in hilbert space via the use of kernels , thus alleviating the need for the explicit mapping . in particular , we consider the burg  @xcite , jeffreys  @xcite and stein  @xcite divergences , that have proven powerful to analyze spd matrices .",
    "these divergences allow us to perform classification in hilbert space via a simple nearest - neighbor ( nn ) classifier , or by making use of more sophisticated distance - based classifiers , such as support vector machines ( svm ) with a gaussian kernel .",
    "we evaluated the resulting descriptors on the tasks of image - based material , texture and virus recognition , person re - identification , and action recognition from motion capture data .",
    "our experimental evaluation clearly evidences the importance of keeping all the data information by mapping to hilbert space before computing the covds .",
    "furthermore , our empirical results show that , with this new representation , a simple nn classifier can achieve accuracies comparable to those of much more sophisticated methods , and that these accuracies can even be boosted beyond the state - of - the - art when using more powerful classifiers .",
    "in this section , we review several bregman divergences and discuss the properties that motivated our decision to use them to compare covds in rkhs .    throughout the paper ,",
    "we use bold upper - case letters to denote matrices ( , @xmath1 ) and bold lower - case letters for column vectors ( , @xmath2 ) .",
    "the @xmath3 identity matrix is written as @xmath4 .",
    "@xmath5 denotes the general linear group , , the group of real invertible @xmath3 matrices .",
    "@xmath6 is the space of @xmath3 symmetric positive definite matrices , , @xmath7 .",
    "[ def : bregman_divergence ] let @xmath8 be a strictly convex and differentiable function defined on the symmetric positive cone @xmath6 .",
    "the bregman matrix divergence @xmath9 is defined as @xmath10 where , and @xmath11 is the gradient of @xmath12 evaluated at @xmath13 .",
    "the bregman divergence is non - negative and definite ( , @xmath14 ) .",
    "[ def : frob_norm ] the euclidean ( frobenius ) distance is obtained by using @xmath15 as seed function in the bregman divergence of eq .",
    "[ eqn : bregman_div ] .",
    "[ def : burg_divergence ] the burg , or @xmath16- , divergence is obtained by using @xmath17 as seed function in the bregman divergence of eq .",
    "[ eqn : bregman_div ] , where @xmath18 denotes the determinant of a matrix .",
    "the b - divergence can be expressed as @xmath19    while bregman divergences exhibit a number of useful properties  @xcite , their general asymmetric behavior is often counter - intuitive and undesirable in practical applications .",
    "therefore , here , we also consider two symmetrized bregman divergences , namely the _ jeffreys _ and the _ stein _ divergences .",
    "[ def : kl_divergence ] the jeffreys , or @xmath20- , divergence is obtained from the burg divergence , and can be expressed as @xmath21    [ def : stein_divergence ] the stein , or @xmath22- , divergence ( also known as the jensen - bregman logdet divergence  @xcite ) is also obtained from the burg divergence , but through _ jensen - shannon _ symmetrization .",
    "it can be written as @xmath23     invariance & p.d .",
    "gaussian kernel * frobenius * & @xmath24 & rotation & yes * burg * & @xmath25 & affine & no * jeffreys * & @xmath26 & affine & yes * stein * & @xmath27 & affine & partial      here , we present the properties of bregman divergences that make them a natural choice as a measure of dissimilarity between two covds . in particular , we discuss these properties in comparison to the popular affine invariant riemannian metric ( airm ) on @xmath6  @xcite , which was introduced as a geometrically - motivated way to analyze covds .",
    "as indicated by the name , the airm was designed to be invariant to affine transformations , which often is an attractive property in computer vision algorithms . in our case",
    ", the @xmath16-divergence exhibits the same invariance property .",
    "more specifically , given @xmath28 , @xmath29 .",
    "this can easily be shown from the definition of the @xmath16-divergence .",
    "since the @xmath20- and @xmath22-divergences are obtained from the @xmath16-divergence , it can easily be verified that they inherit this affine invariance property .",
    "furthermore , these two divergences are also invariant to inversion , , @xmath30 finally , we also note that @xmath31 .",
    "recently , kernel methods have been successfully employed on riemannian manifolds  @xcite .",
    "in particular , an attractive solution is to form a kernel by replacing the euclidean distance in the popular gaussian kernel with a more accurate metric on the manifold .",
    "however , the resulting kernel is not necessarily positive definite for any metric . in particular",
    ", the airm does not yield a positive definite gaussian kernel in general .",
    "in contrast , both the @xmath20- and the @xmath22-divergences admit a hilbert space embedding via a gaussian kernel .    more specifically , for the @xmath20-divergence , it was shown in  @xcite that the kernel @xmath32 is conditionally positive definite ( cpd ) .",
    "cpd kernels correspond to hilbertian metrics and can be exploited in a wide range of machine learning algorithms .",
    "an example of this is kernel svm , whose optimal solution was shown to only depend on the hilbertian property of the metric  @xcite .",
    "note that while the kernel @xmath33 was claimed to be positive definite  @xcite , we are not aware of any formal proof of this claim .    for the @xmath22-divergence ,",
    "the kernel @xmath34 is not positive definite for all @xmath35 .",
    "however , as was shown in  @xcite , @xmath36 is positive definite iff @xmath37    note that , here , we are not directly interested in positive definite gaussian kernels on @xmath6 to derive our infinite - dimensional covds , but only to learn a kernel - based classifier with the divergences between our infinite - dimensional covds as input .",
    "the properties of the bregman divergences that we use in the remainder of this paper are summarized in table  [ tbl : divergences ] .",
    "in this section , we show how covds can be computed in infinite - dimensional spaces . to this end , we first review some basics on hilbert spaces .",
    "[ def : hilbert_space ] a hilbert space is a ( possibly infinite - dimensional ) inner product space which is complete with respect to the norm induced by the inner product .    an rkhs is a special type of hilbert space with the additional property that the inner product can be defined by a bivariate function known as the _ reproducing kernel_. for an rkhs @xmath38 on a non - empty set @xmath39 with @xmath40 there exists a kernel function @xmath41 such that @xmath42 .",
    "the concept of reproducing kernel is typically employed to recast algorithms that only exploit inner products to high - dimensional spaces ( , svm ) .    given these definitions , we now turn to the problem of computing a covariance matrix in an rkhs .",
    "let be an @xmath43 matrix , obtained by stacking @xmath44 independent observations @xmath45 from an image or a video .",
    "the covariance descriptor @xmath46 is defined as @xmath47 where @xmath48 is the mean of the observations , @xmath49 is a centering matrix , and @xmath50 is a square matrix with all elements equal to 1 .",
    "let @xmath51 be a mapping to an rkhs whose corresponding hilbert space @xmath52 has dimensionality @xmath53 ( @xmath53 could go to @xmath54 ) .",
    "following eq .",
    "[ eqn : covd ] , a covd in this rkhs can be written as @xmath55 where @xmath56 $ ] . if @xmath57 , then @xmath58 is rank - deficient , which would make any divergence derived from the burg divergence indefinite .",
    "more precisely , the resulting matrix would be on the boundary of the positive cone , which would make it at an infinite distance from any positive definite matrix , not only for burg - based divergences , but also according to the airm .    here",
    ", we address this issue by exploiting ideas developed in the context of covariance matrix estimation from a limited number of observations  @xcite .",
    "more specifically , we seek to keep the positive eigenvalues of @xmath58 intact and replace the zero ones with a very small positive number @xmath59 , thus making the covd positive definite .",
    "first , using a standard result  @xcite , we note that the positive eigenvalues of @xmath58 , denoted by @xmath60 , can be computed from @xmath61 , where @xmath62 is the @xmath63 kernel matrix whose elements are defined by the kernel function @xmath64 . by eigenvalue decomposition , we can write @xmath65 this lets us write a ( regularized ) estimate of @xmath58 as @xmath66 where @xmath67 with @xmath68 the identity matrix whose dimension is the number of positive eigenvalues of @xmath58  @xcite .",
    "note that this derivation can also be employed to model points in @xmath52 with lower - dimensional latent variables by retaining only the top @xmath69 eigenvalues and eigenvectors of @xmath70 to form @xmath71  @xcite .",
    "in this section , we derive different bregman divergences for the infinite - dimensional covds introduced in section  [ sec : covd_rkhs ] . in these derivations",
    ", we will make use of the equivalence @xmath72 whose derivation is provided in supplementary material .",
    "the frobenius norm can easily be computed as @xmath73 note that , although not a desirable property  @xcite , the euclidean metric is definite for positive semi - definite matrices , which makes it possible to set @xmath59 to zero .      using the sylvester determinant theorem  @xcite",
    ", we first note that @xmath74    then , from the woodbury matrix identity  @xcite , we have @xmath75    this lets us write , @xmath76 by combining eqs .  [ eqn : det_cov_rkhs ] and",
    "[ eqn : tr_xinv_y ] , we then obtain @xmath77 note that the burg divergence is independent of @xmath53 .",
    "this property is inherited by the jeffreys and stein divergences derived below .      from the definition in section  [ sec : preliminaries ]",
    ", the jeffreys divergence can be obtained directly from the burg divergence .",
    "this yields @xmath78      to compute the stein divergence in @xmath52 , let us first define @xmath79 .",
    "\\label{eqn : q}\\end{aligned}\\ ] ]    this lets us write @xmath80 \\mat{q }    \\mat{q}^t      \\left [ \\begin{array}{c } \\phi_{\\mat{x}}^t\\vspace{1ex } \\\\",
    "\\phi_{\\mat{y}}^t \\end{array } \\right ] .",
    "\\label{eqn : cx_cy}\\ ] ]    similarly as in eq .",
    "[ eqn : det_cov_rkhs ] , @xmath81 becomes @xmath82       \\mat{q}\\mat{q}^{t }        \\left [ \\begin{array}{c } \\phi_{\\mat{x}}^t \\vspace{1ex}\\\\ \\phi_{\\mat{y}}^t \\end{array } \\right ]   \\bigg ) \\\\",
    "= & \\rho^{\\mathcal{|h|}}\\det \\bigg(\\mathbf{i}_{\\mat{x } + \\mat{y } } + \\dfrac{1}{2\\rho}\\mat{q}^{t }        \\left [ \\begin{array}{c } \\phi_{\\mat{x}}^t \\vspace{1ex } \\\\ \\phi_{\\mat{y}}^t \\end{array } \\right ]        \\big [   \\phi_{\\mat{x } } \\ : \\phi_{\\mat{y } } \\big ] \\mat{q}\\bigg ) \\\\      = & \\rho^{\\mathcal{|h| } } \\det \\bigg(\\mathbf{i}_{\\mat{x } + \\mat{y } } + \\dfrac{1}{2\\rho}\\mat{q}^{t }      \\mathbb{k}_{\\mat{x},\\mat{y}}\\mat{q}\\bigg)\\ ; ,          \\label{eqn : proof_cx_cy}\\end{aligned}\\ ] ] where @xmath83   \\;.",
    "\\label{eqn : big_k_stein}\\ ] ]    therefore , we have @xmath84      when computing divergences in rkhs , it is desirable to minimize the effect of the parameter @xmath59 , and thus have divergences that do not depend on its inverse . to this end , let us assume that the same number of eigenvectors were kept to build @xmath85 and @xmath86 . in this case",
    ", the stein divergence can be written as @xmath87 where the term @xmath88 can be thought of as a regularizer for @xmath89 .",
    "for the jeffreys divergence , we can define @xmath90 in our experiments , we used the definitions of eqs .",
    "[ eqn : simple_jefferys_rkhs ] and  [ eqn : stein_rkhs_simple ] .",
    "here we compare the complexity of computing @xmath91 and @xmath92 against that of @xmath93 and @xmath94 .",
    "let @xmath95 and @xmath96 be two given sets of observation , with @xmath97 .    computing the @xmath3 covds based on eq .",
    "[ eqn : covd ] requires @xmath98 .",
    "the inverse of an @xmath3 spd matrix can be computed by cholesky decomposition in @xmath99 flops .",
    "therefore , computing the @xmath20-divergence requires @xmath100 flops , which is dominated by @xmath101 .",
    "the complexity of computing the determinant of an @xmath3 matrix by cholesky decomposition is @xmath102 .",
    "therefore , computing the @xmath22-divergence requires @xmath103 flops , which is again dominated by @xmath101 .    in rkhs",
    ", computing @xmath104 , @xmath105 and @xmath106 requires @xmath107 flops for each matrix .",
    "therefore , evaluating eq .",
    "[ eqn : svd_kxx ] requires for @xmath108 flops . assuming that @xmath69 , @xmath109 , eigenvectors are used to create @xmath110 in eq .",
    "[ eqn : w_x ] , computing @xmath111 according to eq .",
    "[ eqn : jefferys_rkhs ] requires @xmath112 . for @xmath113 , evaluating eq .",
    "[ eqn : stein_rkhs ] takes @xmath114 flops .    generally speaking , the complexity of computing the jeffreys and stein divergences in the observation space is linear in @xmath44 while it is cubic when working in rkhs .",
    "our experimental evaluation shows , however , that working in rkhs remains practical . to illustrate this",
    ", we compare the runtimes required to compute the stein divergence between @xmath115 pairs of covds on @xmath116 using eq .",
    "[ eqn : stein_div ] and eq .",
    "[ eqn : stein_rkhs_simple ] .",
    "each covd on @xmath116 was obtained from @xmath117 observations . in the observation space ,",
    "computing the stein divergence on an i7 machine using matlab took 53s . for @xmath118",
    ", it took 452s , 566s and 868s when keeping 10 , 20 and 50 eigenvectors to estimate the covariances , respectively . while slower , these runtimes remain perfectly acceptable , especially when considering the large accuracy gain that working in rkhs entails , as evidenced by our experiments .",
    "we now present our empirical results obtained with the infinite - dimensional covds and their bregman divergences defined in sections  [ sec : covd_rkhs ] and  [ sec : bregman_divergence_rkhs ] .",
    "in particular , due to their symmetry and the fact that they yield valid gaussian kernels , we utilized the jeffreys and stein divergences , and relied on two different classifiers for each divergence : a simple nearest neighbor classifier , which clearly evidences the benefits of using infinite - dimensional covds , and an svm classifier with a gaussian kernel , which further boosts the performance of our infinite - dimensional covds .    the different algorithms evaluated in our experiments are referred to as :    * * @xmath119-nn : * jeffreys / stein based nearest neighbor classifier on covds in the observation space . * * @xmath119-svm : * jeffreys / stein based kernel svm on covds in the observation space . * * @xmath120-nn : * jeffreys / stein based nearest neighbor classifier on infinite - dimensional covds . * * @xmath120-svm : * jeffreys / stein based kernel svm on infinite - dimensional covds .",
    "we also provide the results of the pls - based covariance discriminant learning ( cdl ) technique of  @xcite , which can be considered as the state - of - the - art for covd - based classification . in all our experiments",
    ", we used the rbf kernel to create infinite - dimensional covds .",
    "the parameters of our algorithm , , the rbf bandwidth and the number of eigenvectors @xmath69 , were determined by cross - validation .    _",
    "scale1.0    .",
    "recognition accuracies for the virus dataset  @xcite . [ cols=\"<,^\",options=\"header \" , ]     [ tab : table_mocap_performance ]",
    "we have introduced an approach to computing infinite - dimensional covds , as well as several bregman divergences to compare them . our experimental evaluation has demonstrated that the resulting infinite - dimensional covds lead to state - of - the art recognition accuracies on several challenging datasets . in the future ,",
    "we intend to explore how other types of similarity measures , such as the airm , can be computed over infinite - dimensional covds .",
    "furthermore , we are interested in studying how the frchet mean of a set of infinite - dimensional covds can be evaluated .",
    "this would allow us to perform clustering , and would therefore pave the way to extending well - known methods , such as bag of words , to infinite dimensional covds .",
    "in the following , we provide the detailed derivation of the bregman divergences in rkhs considered in section   of the main paper . we also provide the cmc curves for the person re - identification experiment of section  , which were left out of the main paper due to space limitation .",
    "recall that in section   of the main paper , we have exploited the equivalence @xmath121 ( eq .  ) to derive bregman divergences in rkhs .",
    "we prove this equivalence below : @xmath122 we now provide additional details for the specific bregman divergences considered in the paper .",
    "the euclidean metric in rkhs can be derived as @xmath123    for the burg and related divergences ( , jeffreys and stein divergences ) , we first show that @xmath124 . to this end",
    ", we use the sylvester determinant theorem , which states that , for two matrices @xmath125 and @xmath126 of size @xmath127 and @xmath128 , @xmath129 . therefore ,"
  ],
  "abstract_text": [
    "<S> we introduce an approach to computing and comparing covariance descriptors ( covds ) in infinite - dimensional spaces . </S>",
    "<S> covds have become increasingly popular to address classification problems in computer vision . </S>",
    "<S> while covds offer some robustness to measurement variations , they also throw away part of the information contained in the original data by only retaining the second - order statistics over the measurements . here </S>",
    "<S> , we propose to overcome this limitation by first mapping the original data to a high - dimensional hilbert space , and only then compute the covds . </S>",
    "<S> we show that several bregman divergences can be computed between the resulting covds in hilbert space via the use of kernels . </S>",
    "<S> we then exploit these divergences for classification purpose . </S>",
    "<S> our experiments demonstrate the benefits of our approach on several tasks , such as material and texture recognition , person re - identification , and action recognition from motion capture data . </S>"
  ]
}