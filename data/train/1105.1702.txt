{
  "article_text": [
    "words are the building blocks of sentences , yet the meaning of a sentence goes well beyond the meanings of its words . indeed ,",
    "while we do have dictionaries for words , we do nt seem to need them to infer meanings of sentences .",
    "but where human beings seem comfortable doing this , machines fail to deliver .",
    "automated search engines that perform well when queried by single words , fail to shine when it comes to search for meanings of phrases and sentences . discovering the process of meaning assignment in natural language is among the most challenging as well as foundational questions of linguistics and computer science .",
    "the findings thereof will increase our understanding of cognition and intelligence and will also assist in applications to automating language - related tasks such as document search .    to date",
    ", the compositional type - logical  @xcite and the distributional vector space models  @xcite have provided two complementary partial solutions to the question . the logical approach is based on classic ideas from mathematical logic , mainly frege s principle that meaning of a sentence can be derived from the relations of the words in it .",
    "the distributional model is more recent , it can be related to wittgenstein s philosophy of ` meaning as use ' , whereby meanings of words can be determined from their context .",
    "the logical models have been the champions of the theory side , but in practice their distributional rivals have provided the best predictions .    in a cross - disciplinary approach",
    ",  @xcite used techniques from logic , category theory , and quantum information to develop a compositional distributional semantics that brought the above two models together .",
    "they developed a hybrid categorical model which paired contextual meaning with grammatical form and defined meaning of a string of words to be a function of the tensor product of the meanings of its words .",
    "as a result , meanings of sentences became vectors which lived in the same vector space and it became possible to measure their synonymity the same way lexical synonymity was measured in the distributional models .",
    "this sentence space was taken to be an abstract space and it was only shown how to instantiate it for the truth - functional meaning . later  @xcite introduced a concrete construction using structured vector spaces and exemplified the application of logical methods , albeit only a toy vector space . in this paper",
    "we report on this and on a second construction which uses plain vector spaces .",
    "we also review results on implementing and evaluating the setting on real large scale data from the british national corpus and two disambiguation experiments  @xcite .",
    "to compute the meaning of a sentence consisting of @xmath0 words , meanings of these words must _ interact _ with one another . in the logical models of meaning ,",
    "this further interaction is represented in a function computed from the grammatical structure of the sentence , but meanings of words are empty entities .",
    "the grammatical structure is usually depicted as a parse - tree , for instance the parse - tree of the transitive sentence ` dogs chase cats ' is as follows :    the function corresponding to this tree is based on a relational reading of the meaning of the verb ` chase ' , which makes the subject and the object interact with each other via the relation of _",
    "this methodology is used to translate sentences of natural language into logical formulae , then use computer - aided automation tools to reason about them  @xcite .",
    "the major drawback is that the result can only deal with truth or falsity as the meaning of a sentence and does poorly on lexical semantics , hence do not perform well on language tasks such as search .",
    "the vector space model , on the other hand , dismisses the further interaction and is solely based on lexical semantics .",
    "these are obtained in an operational way , best described by a frequently cited quotation due to firth  @xcite that  you shall know a word by the company it keeps . \" .",
    "for instance , beer and sherry are both drinks , alcoholic , and often make you drunk .",
    "these facts are reflected in the text : words ` beer ' and ` sherry ' occur close to ` drink ' , ` alcoholic ' and ` drunk ' .",
    "hence meanings of words can be encoded as vectors in a highly dimensional space of context words . the raw weight in each base",
    "is related to the number of times the word has appeared close ( in an @xmath0-word window ) to that base .",
    "this setting offers geometric means to reason about meaning similarity , e.g.  via the cosine of the angle between the vectors .",
    "computational models along these lines have been built using large vector spaces ( tens of thousands of basis vectors ) and large bodies of text ( up to a billion words )  @xcite .",
    "these models have responded well to language processing tasks such as word sense discrimination , thesaurus construction , and document retrieval  @xcite .",
    "their major drawback is their non - compositional nature : they ignore the grammatical structure and logical words , hence can not compute ( in the same efficient way that they do for words ) meanings of phrases and sentences .",
    "the key idea behind the approach of  @xcite is to import the compositional element of the logical approaches into the vector space models by making the grammar of the sentence _",
    "act _ on , hence relate , its word vectors .",
    "the trouble is that it does not make so much sense to ` make a parse tree act on vectors '",
    ". some higher order mathematics , in this case category theory , is needed to encode the grammar of a sentence into a morphism compatible with vector spaces .",
    "these morphisms turn out to be the grammatical reductions of a type - logic called a lambek pregroup  @xcite .",
    "pregroups and vector spaces both have a _ compact _ categorical structural .",
    "the grammatical morphism of a pregroup can be transformed into a linear map that acts on vectors .",
    "meanings of sentences become vectors whose angles reflect similarity . hence , at least theoretically , one should be able to build sentence vectors and compare their synonymity , in exactly the same way as measuring synonymity for words .",
    "the pragmatic interpretation of this abstract idea is as follows . in the vector space models ,",
    "one has a meaning vector for each word , @xmath1 .",
    "the logical recipe tells us to _ apply _ the meaning of verb to the meanings of subject and object . but how can a vector _ apply _ to other vectors ?",
    "if we strip the vectors off the extra information provided in their basis and look at them as mere sets of weights , then we can apply them to each other by taking their point - wise sum or product .",
    "but these operations are commutative , whereas meaning is not .",
    "hence this will equalize meaning of any combination of words , even with the non - grammatical combinations such as ` dogs cats chase ' .",
    "the proposed solution above implies that one needs to have different levels of meaning for words with different functionalities .",
    "this is similar to the logical models whereby verbs are relations and nouns are atomic sets .",
    "so verb vectors should be built differently from noun vectors , for instance as matrices that relate and act on the atomic noun vectors .",
    "the general information , as to which words should be matrices and which atomic vectors , is in fact encoded in the type - logical representation of the grammar .",
    "that is why the grammatical structure of the sentence is a good candidate for the process that relates its word vectors .    in a nutshell ,",
    "pregroup types are either atomic or compound .",
    "atomic types can be simple ( _ e.g. _  @xmath0 for noun phrases , @xmath2 for statements ) or left / right superscripted  referred to as adjoint types ( _ e.g. _  @xmath3 and @xmath4 ) .",
    "an example of a compound type is that of a verb @xmath5 .",
    "the superscripted types express that the verb is a relation with two arguments of type @xmath0 , which have to occur to the ight and to the eft of it , and that it outputs an argument of the type @xmath2 .",
    "a transitive sentence is typed as shown below .",
    "( 50,50)(150,150 ) ( 200,185)dogs ( 208,170)@xmath0 ( 230,185)chase ( 228,170)@xmath3 ( 240,170)@xmath2 ( 248,170)@xmath4 ( 260,185)cats .",
    "( 270,170)@xmath0    here , the verb interacts with the subject and object via the underlying wire cups , then produces a sentence via the outgoing line .",
    "these interactions happen in real time .",
    "the type - logical analysis assigns type @xmath0 to ` dogs ' and ` cats ' , for a noun phrase , and the type @xmath5 to ` chase ' for a verb , the superscripted types @xmath3 and @xmath4 express the fact that the verb is a function with two arguments of type @xmath0 , which have to occur to the _",
    "_ r__ight and _ _ l__eft of it .",
    "the reduction computation is @xmath6 , each type @xmath0 cancels out with its right _ adjoint _",
    "@xmath3 from the right , i.e. @xmath7 and its left adjoint @xmath4 from the left , i.e. @xmath8 , and 1 is the unit of concatenation @xmath9 .",
    "the algebra advocates a linear method of parsing : a sentence is analyzed as it is heard , i.e.  word by word , rather than by first buffering the entire string then re - adjusting it as necessary on a tree .",
    "it s been argued that the brain works in this one - dimensional linear ( rather than two - dimensional tree ) manner  @xcite .    according to  @xcite and based on a general completeness theorem between compact categories , wire diagrams , and vector spaces , meaning of sentences",
    "can be canonically reduced to linear algebraic formulae , for example the following is the meaning vector of our transitive sentence : @xmath10 here @xmath11 is the linear map that encodes the grammatical structure .",
    "the categorical morphism corresponding to it is denoted by the tensor product of 3 components : @xmath12 , where @xmath13 and @xmath14 are subject and object spaces , @xmath15 is the sentence space , the @xmath16 s are the cups , and @xmath17 is the straight line in the diagram .",
    "the cups stand for taking inner products , which when done with the basis vectors imitate substitution .",
    "the straight line stands for the identity map that does nothing . by the rules of the category",
    ", the above equation reduces to the following linear algebraic formula with lower dimensions , hence the dimensional explosion problem for tensor products is avoided :    @xmath18    in the above equation , @xmath19 are basis vectors of @xmath13 and @xmath14 .",
    "the meaning of the verb becomes a superposition , represented as a linear map .",
    "the inner product @xmath20 substitutes the weights of @xmath21 into the first argument place of the verb ( similarly for object and second argument place ) and results in producing a vector for the meaning of the sentence .",
    "these vectors live in sentence spaces @xmath15 , for which @xmath22 is a base vector .",
    "the degree of synonymity of sentences is obtained by taking the cosine measure of their vectors .",
    "@xmath15 is an abstract space , it needs to be instantiated to provide concrete meanings and synonymity measures .",
    "for instance , a truth - theoretic model is obtained by taking the sentence space @xmath15 to be the 2-dimensional space with basis vector true @xmath23 and false @xmath24 .",
    "this is done by using the weighting factor @xmath25 to define a model - theoretic meaning for the verb as follows : @xmath26 the definition of our meaning map ensures that this value propagates to the meaning of the whole sentence .",
    "so @xmath27 becomes true whenever ` dogs chase cats ' is true and false otherwise .",
    "the above construction is based on the assumptions that @xmath21 is a base of @xmath13 and that @xmath28 is a base of @xmath14 . in other words , we assume that @xmath13 is the vector space spanned by the set of all men and @xmath14 is the vector space spanned by the set of all women .",
    "this is not the usual construction in the distributional models . in what follows we present two concrete constructions for these , which will then yield a construction for the sentence space . in",
    "both of these approaches @xmath13 and @xmath14 will be the same vector space , which we will denote by @xmath29 .",
    "we take @xmath29 to be a _ structured vector space _ , as in  @xcite .",
    "the bases of @xmath29 are annotated by ` properties ' obtained by combining dependency relations with nouns , verbs and adjectives .",
    "for example , basis vectors might be associated with properties such as `` arg - fluffy '' , denoting the argument of the adjective fluffy , `` subj - chase '' denoting the subject of the verb chase , `` obj - buy '' denoting the object of the verb buy , and so on .",
    "we construct the vector for a noun by counting how many times in the corpus a word has been the argument of ` fluffy ' , the subject of ` chase ' , the object of ` buy ' , and so on .    for transitive sentences ,",
    "we take the sentence space @xmath15 to be @xmath30 , so its bases are of the form @xmath31 .",
    "the intuition is that , for a transitive verb , the meaning of a sentence is determined by the meaning of the verb together with its subject and object .",
    "the verb vectors @xmath32 are built by counting how many times a word that is @xmath33 ( e.g. has the property of being fluffy ) has been subject of the verb and a word that is @xmath34 ( e.g. has the property that it s bought ) has been its object , where the counts are moderated by the extent to which the subject and object exemplify each property ( e.g. _ how fluffy _ the subject is ) . to give a rough paraphrase of the intuition behind this approach , the meaning of  dog chases cat \" is given by : the extent to which a dog is fluffy and",
    "a cat is something that is bought ( for the @xmath30 property pair  arg - fluffy \" and  obj - buy \" ) , and the extent to which fluffy things _ chase _ things that are bought ( accounting for the meaning of the verb for this particular property pair ) ; plus the extent to which a dog is something that runs and a cat is something that is cute ( for the @xmath30 pair  subj - run \" and  arg - cute \" ) , and the extent to which things that run _ chase _ things that are cute ( accounting for the meaning of the verb for this particular property pair ) ; and so on for all noun property pairs .    for sentences with intransitive verbs , the sentence space suffices to be just @xmath29 . to compare the meaning of a transitive sentence with an intransitive one , we embed the meaning of the latter from @xmath29 into the former @xmath30 , by taking @xmath35 ( the ` object ' of an intransitive verb ) to be @xmath36 , i.e. the superposition of all basis vectors of @xmath29 .",
    "a similar method is used while dealing with sentences with ditransitive verbs , where the sentence space will be @xmath37 , since these verbs have three arguments .",
    "transitive and intransitive sentences are then embedded in this bigger space , using the same embedding described above .",
    "adjectives are dealt with in a similar way .",
    "we give them the syntactic type @xmath38 and build their vectors in @xmath30 .",
    "the syntactic reduction @xmath39 associated with applying an adjective to a noun gives us the map @xmath40 by which we semantically compose an adjective with a noun , as follows : @xmath41 we can view the @xmath42 counts as determining what sorts of properties the arguments of a particular adjective typically have ( e.g. arg - red , arg - colourful for the adjective `` red '' ) .",
    "as an example , consider a hypothetical vector space with bases ` arg - fluffy ' , ` arg - ferocious ' , ` obj - buys ' , ` arg - shrewd ' , ` arg - valuable ' , with vectors for ` bankers ' , ` cats ' , ` dogs ' , ` stock ' , and ` kittens ' .    [ cols=\"^,<,^,^,^,^,^\",options=\"header \" , ]     [ results ]    according to the literature ( e.g. see  @xcite ) , the main measure of success is demonstrated by the @xmath43 column . by this measure in the second experiment",
    "our method outperforms the other two with a much better margin than that in the first experiment .",
    "the high ( similarly low ) columns are the average score that high ( low ) similarity sentences ( as decided by us ) get by the program .",
    "these are not very indicative , as the difference between high mean and the low mean of the categorical model is much smaller than that of the both the baseline model and multiplicative model , despite better alignment with annotator judgements .",
    "the data set of the first experiment has a very simple syntactic structure where the context around the verb is just its subject . as a result ,",
    "in practice the categorical method becomes very similar to the multiplicative one and the similar outcomes should not surprise us .",
    "the second experiment , on the other hand , has more syntactic structure , thereby our categorical shows an increase in alignment with human judgements .",
    "finally , the increase of @xmath43 from the first experiment to the second reflects the compositionality of our model : its performance increases with the increase in syntactic complexity . based on this , we would like to believe that more complex datasets and experiments which for example include adjectives and adverbs shall lead to even better results .",
    "we have provided a brief overview of the categorical compositional distributional model of meaning as developed in  @xcite .",
    "this combines the logical and vector space models using the setting of compact closed categories and their diagrammatic toolkit and based on ideas presented in  @xcite on the use of tensor product as a meaning composition operator .",
    "we go over two concrete constructions of the setting , show examples of one construction on a toy vector space and implement the other construction on the real data from the bnc .",
    "the latter is evaluated on a disambiguation task on two experiments : for intransitive verbs from @xcite and for transitive verbs developed by us .",
    "the categorical model slightly improves the results of the first experiment and betters them in the second one .    to draw a closer connection with the subject area of the workshop",
    ", we would like to recall that sentences of natural language are compound systems , whose meanings exceed the meanings of their parts .",
    "compound systems are a phenomena studied by many sciences , findings thereof should as well provide valuable insights for natural language processing .",
    "in fact , some of the above observations and previous results were led by the use of compact categories in compound quantum systems  @xcite .",
    "the caps that connect subject and verb from afar are used to model nonlocal correlations in entangled bell states ; meanings of verbs are represented as superposed states that let the information flow between their subjects and objects and further act on it . even on the level of single quantum systems , there are similarities to the distributional meanings of words : both are modeled using vector spaces",
    ". motivated by this  @xcite have used the methods of quantum logic to provide logical and geometric structures for information retrieval and have also obtained better results in practice .",
    "we hope and aim to study the modular extension of the quantum logic methods to tensor spaces of our approach .",
    "there are other approaches to natural language processing that use compound quantum systems but which do not focus on distributional models , for example see  @xcite .",
    "other areas of future work include creating and running more complex experiments that involve adjectives and adverbs , working with larger corpora such as the wacky , and interpreting stop words such as relative pronouns _ who , which _ , conjunctives _ and , or _ , and quantifiers _ every , some_.      h.  alshawi ( ed ) .",
    "1992 . _ the core language engine_. mit press .",
    "m.  baroni and r.  zamparelli .",
    "_ nouns are vectors , adjectives are matrices_. proceedings of conference on empirical methods in natural language processing ( emnlp ) .",
    "p.  bruza , k.  kitto , d.  l.  nelson and c.  l.  mcevoy .",
    "_ entangling words and meaning_. proceedings of aaai spring symposium on quantum interaction . . .",
    "b.  coecke , m.  sadrzadeh and s.  clark .",
    "_ mathematical foundations for distributed compositional model of meaning_. lambek festschrift .",
    ". j. van benthem , m. moortgat and w. buszkowski ( eds . ) .",
    "j.  curran .",
    "_ from distributional to semantic similarity_. phd thesis , university of edinburgh",
    ". j.  r.  firth .",
    "1957 . _ a synopsis of linguistic theory 1930 - 1955_. .",
    "e.  grefenstette , m.  sadrzadeh , s.  clark , b.  coecke , s.  pulman .",
    "2011 . _ concrete compositional sentence spaces for a compositional distributional model of meaning_. international conference on computational semantics ( iwcs11 ) .",
    "oxford .",
    "g.  grefenstette .",
    "_ explorations in automatic thesaurus discovery_. kluwer . e.  guevara . 2010 . _ a regression model of adjective - noun compositionality in distributional semantics_. proceedings of the acl gems workshop .    j.  lambek .",
    "_ from word to sentence_. polimetrica , milan . t.  landauer , and s.  dumais . 2008 . _ a solution to plato s problem : the latent semantic analysis theory of acquisition , induction , and representation of knowledge_. psychological review .    c.  d.  manning , p.  raghavan , and h.  schtze .",
    "_ introduction to information retrieval_. cambridge university press . j.  mitchell and m.  lapata .",
    "2008 . _ vector - based models of semantic composition_. proceedings of the 46th annual meeting of the association for computational linguistics , 236244 .",
    "r.  montague .",
    "_ english as a formal language_. , 189223 .",
    "j.  nivre 2003 .",
    "_ an efficient algorithm for projective dependency parsing_. ."
  ],
  "abstract_text": [
    "<S> we provide an overview of the hybrid compositional distributional model of meaning , developed in  @xcite , which is based on the categorical methods also applied to the analysis of information flow in quantum protocols . </S>",
    "<S> the mathematical setting stipulates that the meaning of a sentence is a linear function of the tensor products of the meanings of its words . </S>",
    "<S> we provide concrete constructions for this definition and present techniques to build vector spaces for meaning vectors of words , as well as that of sentences . </S>",
    "<S> the applicability of these methods is demonstrated via a toy vector space as well as real data from the british national corpus and two disambiguation experiments .    </S>",
    "<S> * keywords . </S>",
    "<S> * logic , natural language , vector spaces , tensor product , composition , distribution , compact categories , pregroups .    </S>",
    "<S> = 1 </S>"
  ]
}