{
  "article_text": [
    "web users are increasingly looking for information beyond the traditional sources .",
    "this is manifested in search engines like google and bing by the inclusion of answers beyond 10 page links and in the tremendous growth of specialized search engines such as amazon .",
    "often the rich experience is provided via the use of semantic information that comes from ( semi-)structured data sources in the form of tables , xml files or databases .",
    "for example , structured data can be used to answer queries ranging such as electronic goods ( e.g. `` 50 inch samsung led tv ` ' ) , fashion ( e.g. `` $ 1600 prada handbags ` ' ) , movie - showtimes listings ( e.g. `` avatar showtimes near san francisco ` ' ) , and weather prediction ( e.g. `` weather in new york ` ' ) .",
    "a major challenge in using structured data to answer web queries is that users often lack domain expertise and may pose queries that lead to very few or no result due to unfamiliarity with the underlying data sources .",
    "for example , consider the query `` 50 inch samsung led tv ` ' .",
    "there exists work in the literature  @xcite that can correctly classify and semantically interpret the query to attribute - value pairs that correspond to underlying structured attributes . so the query can be thought as ( 50 inch @xmath0 display size , samsung @xmath0 brand , led tv @xmath0 display type ) .",
    "however , if the query is directly evaluated as specified , there will be no results that can satisfy the interpretation as samsung does not make 50-inch led tvs .",
    "on the other hand , samsung makes 46-inch and 55-inch led tvs and 50-inch plasma tvs .",
    "arguably , the users would prefer to see such results that are close to their original query instead of looking at an empty page with no results because they did not know the appropriate precise values when typing the query .",
    "the challenge is common to today s systems and not restricted to the electronics domain but applies broadly to answering web queries in a variety of domains including handbags or shoes , for example consider the query $ 1600 prada handbags .",
    "one strategy for handling this challenge is to _ rewrite the query _ to broaden its coverage . in the context of online search ,",
    "such rewrites include a variety of techniques such as query term deletion , phrasal substitution , and mining of similar queries .",
    "in fact , the query $ 1600 prada handbags does not return any products on amazon and is handled using term deletion as shown in figure  [ fig : amazon - example ] .",
    "however this approach provides no quality guarantees and does not take advantage of the rich meta - data information available in structured data sources , thus producing results that leave a lot to be desired to the user .",
    "we are interested in rewriting the queries through semantic term expansion .",
    "for example , the above queries may be rewritten as `` ( 46 to 52 inch ) ( samsung or sony ) ( led or plasma ) tv ` ' and `` ( $ 1400 to $ 1800 ) ( prada or gucci ) handbags ` ' respectively .",
    "this query rewriting problem can be viewed as a generalization of query rewriting through synonyms to increase recall , for example , from `` women shoes ` ' to `` ( women or women s ) ( footwear or shoes ) ` ' .",
    "note that we are not interested in a set of static rewrite rules , such as those used in synonym detection and stemming , but rather a query rewrite algorithm that can understand the query intent and adapt accordingly .",
    "the quality of the rewrites depends on two factors .",
    "first , the rewritten query should preserve the meaning of the original query as closely as possible .",
    "we measure the fidelity of the rewrite by computing how far away the set of results retrieved are to the original query , as estimated by user preferences learned through click logs .",
    "second , the rewritten query should ensure there are sufficiently many results returned to the user .",
    "we measure the coverage of the rewrite by counting how many times when a certain number of results is requested , the expectation is met .",
    "if efficiency had not been an issue , a candidate solution would have been to expand the terms a little at a time , issue the rewritten query to the index , and repeat as necessary until the minimum number of results requested is retrieved .",
    "this solution would not be applicable to web search , however , as users expect results to be returned in under half a second , thus placing a strict performance requirement on the query rewriting component .",
    "hence , many search engines place a restriction on the number of re - written queries ( also called query augmentations ) that can be issued to the index as part of the original query execution . to ensure this requirement",
    "is met , we require that the techniques may only use precomputed statistics of the index but may not access the index at run time , as index access contributes the lion s share of running time .",
    "similarly , we also require that the techniques can take an input parameter that limits the number of alternative rewrites they can examine .    in this paper , we formulate the above problem as time - bound query rewriting for structured web queries .",
    "as part of our contributions we formally describe an optimization framework that takes as input a candidate query @xmath1 , a desired number of results @xmath2 , and a parameter @xmath3 that governs how many rewrites can be considered , and produces a rewritten query that aims to retrieve at least @xmath2 results and that the results match the original query well .",
    "we show that finding the optimal solution to this problem is np - hard .",
    "we introduce a greedy algorithm and a dynamic programming solution that rewrite the query in a principled and controlled fashion .",
    "we also study the effect of functional dependencies in the data and how they affect query rewrite .",
    "we evaluate the proposed solution using real queries from a commercial search engine s shopping vertical against a prototype commerce search engine .",
    "the rest of the paper is organized as follows . in section 2 , we discuss related work . in section 3 , we describe our model and assumptions about structured web search , and formulate the problem of predicate relaxation . to meet the performance requirement",
    ", one needs to pre - compute statistics on the database to be used at runtime . in section 4 , we describe two kinds of statistics  histograms and functional dependencies  and give two heuristics for using these statistics to perform fast predicate relaxation . in section 5 , we report our experimental evaluation of these heuristics conducted over data from a commercial search engine s vertical .",
    "we summarize and conclude in section 6 .",
    "structured data is abundant on the web , and there have been studies on how to retrieve them in a manner suitable to web search  @xcite .",
    "there is also work on how to retrieve and rank information from structured data  @xcite .",
    "when answering web queries over structured data , however , direct application of textual similarity may produce low quality results due to possible misinterpretations of data types .",
    "for example , a database might store the television diagonal as the string ` 50 inches ' while users may type `` 50 \" ` ' . to this end , recent work has studied how to analyze keyword queries as typed in a web search box and interpret them as structured queries  @xcite .",
    "these past works form the basic components over which we build our system for answering web queries using structured data .    rewriting user queries to broaden coverage",
    "is a common technique employed by all search engines .",
    "for example , search engines routinely make spelling corrections to queries when retrieving results . in the context of search over structured data sources ,",
    "textual similarity approaches that treat the query as a bag of words will generally perform poorly . in the example query given in the introduction",
    ", there is no textual relaxation between _ samsung _ and _ sony _ , and little can be done for generating term expansions or substitutions for the diagonal size in a controlled manner .",
    "past approaches based on log mining  @xcite may be able to discover relationship between terms that do not exhibit textual similarity , but they do not address how such knowledge can be exploited in conjunction with statistics of the documents to come up with good rewrites of the queries that preserve fidelity and ensure coverage .",
    "_ proposed a method to relax text queries using taxonomies  @xcite .",
    "their approach can also be viewed as rewriting queries taking advantage of a taxonomy created by experts , and thus solves a similar problem to ours .",
    "however , creating a good taxonomy requires significant domain knowledge , and is an expensive process . in our application domain",
    ", we do not have such a taxonomy available , and hence the work is not directly comparable .    these has been work in the database community that investigate the problem of keyword search over structured data  @xcite .",
    "they assume that the expansion of the keywords is handled through some probabilistic methods or captured in the ranking function , and focus on performance issues . like these work",
    ", we are concerned about performance issues , and capture the requirements by explicitly specifying them in our framework ; our work is different in that it allows a more controlled behavior in the rewriting that provides quality guarantees .    finally , given a query and a distance function",
    ", one can think of the problem we are trying to solve as a nearest neighbor problem .",
    "nearest neighbor problems have been studied in the past , for example  @xcite .",
    "more recently there are even k - nearest neighbor considerations , like  @xcite , that are applicable in the setting of searching over a database .",
    "similar to the k - nearest neighbors , but from a join relaxation problem in databases is the work described in  @xcite .",
    "we find such work very valuable in relaxing the user query and finding good quality results within a reasonable distance around what was specified in the query .",
    "although useful , the techniques described have a fundamental difference with our work . in the web search over structured data setting",
    "we need both quality guarantees with regards relaxation but at the same time we have strict performance guarantees requiring an upper time bound .",
    "further , these approaches admit relaxations of numeric attributes only and extensions to categorical attributes are non - trivial .",
    "in contrast , our approaches come with two advantages - 1 ) they are very simple to implement ; and 2 ) support distance functions on both categorical and numeric attributes .",
    "in fact , we will precisely use one such distance function in our experiments and show that our algorithms perform well in practice .",
    "we first describe a model of structured web queries , and assumptions on how they are parsed , and how items are evaluated with respect to the parsed queries .",
    "we then formally define the problem of time bound query rewrites .",
    "given a keyword web query , we assume the existence of a semantic parser that identifies the attributes requested in the query and extracts their associated desired values , based on past work such as  @xcite .",
    "for example , the query `` 50 inch samsung led tv ` ' is parsed as a _ structured query _",
    "@xmath4__table : tv , brand : samsung , type : led , diagonal:50__@xmath5 .",
    "denote a generic parsed query by its attribute - value pairs , @xmath6 .",
    "denote the value of attribute @xmath7 in query @xmath1 by @xmath8 .",
    "for our example query , @xmath9 .",
    "consistent with the interpretation of web queries as conjunctions of keywords , we interpret the structured query under the and - semantics as well .",
    "for the rest of the paper we assume that structured queries are given to us in the form of attribute - value pairs . in practice ,",
    "not all terms in a query will be understood by the parser .",
    "the terms that are not understood are treated as keywords used by the ranking function as additional signals .",
    "let @xmath10 be a database of items , from which we retrieve results to serve the query .",
    "for each item @xmath11 , we represent it as a set of attribute - value pairs @xmath12 , and the value of attribute @xmath7 by @xmath13 .",
    "we assume that the semantic parser will only identify attributes for which we have data , hence the query specifies the values of a subset of these @xmath14 attributes . henceforth , when a query @xmath15 is given , we only focus on the @xmath16 attributes mentioned .",
    "we give an example table of the database for tvs in table  [ tab : database ] , which we use throughout the paper for illustration .",
    "the size of the database will be significantly larger in practice .",
    "as discussed in the introduction , users may lack domain expertise and may be unfamiliar with the attribute specification of the underlying structured data .",
    "consider the sample query @xmath4__table : tv , brand : samsung , type : led , diagonal:50__@xmath5 .",
    "for the database table in table  [ tab : database ] , there is no tv that matches all the requested attribute values .",
    "nonetheless , it is desirable that a search engine should return results that are close to the query , for example , samsung led tvs of 46 inches or 55 inches , or sharp led tvs of 52 inches",
    ". it would be less desirable , however , if the search engine returns a samsung led tv of 32 inches , since that tv is much smaller than requested , or a sony crt tv of 50 inches , since the type of tv is significantly different than requested .",
    ".example database for tvs .",
    "[ cols=\"<,<,<,<,<\",options=\"header \" , ]     similar to greedy - rewrite , if no relaxed query with an estimated number of matches of at least @xmath2 is found at the end of having evaluated @xmath3 relaxed queries , the relaxed query with the largest amount of relaxation is returned .",
    "there is a trade - off between the two heuristics described . on the one hand , for any fixed @xmath17 , if the maximum number of relaxed queries allowed is large , dp - rewrite  is guaranteed to find a relaxed query @xmath1 with @xmath18 no larger than the one found by greedy - rewrite .",
    "however , when the number of relaxed queries allowed is small , dp - rewrite  will be able to investigate solutions of only small total amount of relaxation , and fails to find a solution when greedy - rewrite  may succeed .",
    "we explore this trade - off more fully in the experiments .",
    "in this section , we study the behavior and performance of our algorithms on effectively rewriting real user queries .      for our experimental evaluation we built a prototype search engine and we populated it with real data from the shopping vertical of a commercial search engine . to this end , we downloaded the detailed descriptions for about 5 million products related to 73 categories about electronics ( such as televisions , equalizers , gps receivers , etc . ) from  @xcite .",
    "each product is provided in structured form with its attributes clearly specified like  @xcite .",
    "we indexed the product details and computed the histograms for the attributes as described in section  [ sec : relaxation ] .",
    "as our query set we used a random sample of one thousand queries of a major commercial search engine s log that were provided to us .",
    "we selected the queries that were directed to the categories described above and for which we extracted attribute value pairs to form the corresponding structured queries .",
    "we use well - known techniques  @xcite to extract the attribute information from the queries .",
    "the categorization and translation to structured queries was verified manually to be correct .",
    "we ran all thousand queries through our system and we selected the ones that triggered a query rewrite because they return too few ( less than @xmath2 ) results .",
    "out of the thousand queries , 343 would benefit from query rewrites . since the queries were a random sample of queries targeted towards the structured data that we have available , on average approximately 34% of such queries could potential benefit . in the remainder of this section",
    ", we use these 343 queries as our query set to evaluate in depth our techniques .      as observed in the introduction , for queries that trigger very few results , ` amazon.com ` rewrites the query by dropping words from the query . to take advantage of the semantics parser , instead of dropping words from the query",
    ", we implemented a version that removes attributes from the structured interpretation of the query .",
    "the attribute to remove is selected based on which attribute is the most constraining .",
    "we compare our method to this approach which we termed attribute - removal .",
    "we present its performance in section  [ ssec : num - steps ] .",
    "note that there is no parameter to tune for this algorithm .      within our prototype search engine",
    ", we also implemented a distance function to be used for ranking and evaluating our results after query rewrite .",
    "as our aggregate distance function @xmath19 we considered the average distance of the query to the items in our data set . more specifically , for a given query @xmath20 and an item @xmath21 , @xmath22 , where @xmath23 is the individual distance between the @xmath1 and @xmath21 for attribute @xmath7 .",
    "one natural definition of distance ( or similarity ) between attribute values is based on the notion of _ substitutability _ , i.e. , the likelihood of a user substituting her desired attribute value @xmath24 ( specified in the query ) by eventually choosing a product with a different attribute value @xmath25 .",
    "for example , a user looking for a _ nikon _ digital camera is much more likely to substitute the brand for another well - recognized brand such as _ canon _ rather than an obscure one like _",
    "yashica_. thus , the distance between _ nikon _ and _ canon _ is expected to be smaller than that between _ nikon _ and _ yashica_. similar intuition holds for a numerical attribute as well .",
    "consider a user buying a @xmath26 lcd tv .",
    "she is more likely to eventually buy a @xmath27 than a @xmath28 lcd tv .    in our implementation",
    ", we define @xmath29 as the normalized distance of the two attribute values when they are numeric , i.e. , @xmath30 . for categorical attributes , we compute this distance measure using a methodology similar to the one described in @xcite based on browsed trails originating from search engines . as these distances are based on search logs , certain attribute values appear very rarely , leading to no estimate for certain pairs of attribute values .",
    "for example , for the attribute _ model _ , distances between pairs of _ model numbers _ could be missing .",
    "in such cases , we take the conservative position that the missing distances to be the maximum possible distance of @xmath31 .    for our performance metric mean - dist",
    ", we will use the mean distance ( as captured by @xmath19 ) over all items in our result set , i.e. we will use equation  . to penalize for the cases where the algorithm fails to find at least @xmath2 results , which could happen due to poor estimates that overestimates the number of matches of a relaxed query , or an algorithm having attempted @xmath3 different relaxed queries , we treat any shortfall as having retrieved documents that are at a maximum possible distance of @xmath31 from the query . under this _ penalty",
    "_ , an algorithm that finds a relaxed query that obtains at least @xmath2 results will do better than one that does not .    finally , for the experiments presented in this section we set the number of returned results @xmath32",
    "we start our experimental evaluation by studying the effect of the step size @xmath17 in the performance of our query rewrite algorithms .",
    "both greedy - rewrite  and dp - rewrite  use a parameter @xmath17 that determines the amount of relaxation of an attribute at a step of the algorithm .",
    "intuitively , for small @xmath17 , we are making smaller , more careful steps when relaxing so we expect that the furthest item will be quite close to the @xmath33 item .",
    "on the other hand , if @xmath17 is large , we are relaxing more aggressively and we may identify significantly more than @xmath2 , and thus our performance metric may be worse .    to study this effect in more detail",
    ", we evaluated our algorithms over our data and we plot the graphs shown in figure  [ fig : greedy - perf - over - epsilon ] for greedy - rewrite  and in figure  [ fig : dp - perf - over - epsilon ] for dp - rewrite .",
    "the results for attribute - removal  is not affected by the step size @xmath17 or the number of steps @xmath3 .",
    "the data is shown in figure  [ fig : algs - perf - over - timesteps ] and is not shown in figures  [ fig : greedy - perf - over - epsilon ] and [ fig : dp - perf - over - epsilon ] for presentation clarity .",
    "the algorithms were allowed upto a total of 20 steps , which ensured that they would consider rewrites that would return at least @xmath2=10 results .",
    "the horizontal axis shows increasing values of @xmath17 and the vertical axis shows the average mean - dist  at a given @xmath17 value .",
    "lower values in the vertical axis indicate better performance . in the case of greedy - rewrite",
    ", we observe that increasing step sizes lead to a larger value under our performance metric , i.e. , worse results . as our algorithms",
    "become more aggressive ( increasing @xmath17 ) they allow for the result set to grow much larger than @xmath2 and thus mean - dist  increases . of course",
    ", smaller @xmath17 values imply better performance but at the cost of requiring more steps until completion .",
    "the picture for dp - rewrite  is more complicated .",
    "when the number of steps is very few , it faces a trade - off in choosing the step size .",
    "when the step size is small , dp - rewrite  fails to find rewrites that retrieve at least @xmath2 results , leading to poor performance as it is penalized for the shortfall ; when the step size is large , dp - rewrite  finds rewrites that obtains at least @xmath2 results , but now with large total amount of relaxation across all attributs .",
    "hence , we see a u - shaped curve for small number of steps .",
    "when the number of steps is large , the performance of dp - rewrite  is closer to monotonically increasing in step sizes , as relaxations of at least @xmath2 results are found for any step sizes , and hence smaller step sizes lead to better performance .",
    "indeed , we see that the three curves for number of steps @xmath34 overlaps one another , indicating that the same rewrite is found .",
    "the small dip from @xmath35 to @xmath36 is due to a couple of queries where the best relaxation is by rewriting an attribute to include values that are @xmath37 ( and @xmath38 ) away , whence for @xmath35 these attributes have to include values that are @xmath39 ( and @xmath40 ) away .    in both cases",
    ", we found that @xmath41 gives a reasonable performance for our practical setting for the number of steps @xmath42 , so we will use this value for the remainder of our experiments .",
    "we also observe that , overall , dp - rewrite  performs better than greedy - rewrite  because of the fact that it can keep a tab on the best rewrite among all the candidate rewrites it has explored for any given @xmath3 .",
    "finally , for both algorithms , increasing @xmath43 from 1 to 10 helps performance but increasing beyond a point ( i.e. for @xmath44 ) performance decreases .",
    "the reason for this is that @xmath43 captures the error in our result estimation function .",
    "so when @xmath43=1 we barely estimate over @xmath2 results , but in reality we retrieved fewer and more erratic results .",
    "when @xmath43 is too large we do not stop our heuristic until the estimation size is too large , thus introducing a lot more results that creates problem .",
    "empirically , for our system we found out that @xmath43=10 is a good value and we will use it for the remainder of our experiments .",
    "we now turn to study the performance of our algorithms in terms of the amount of steps that is allocated to them .",
    "we fixed the step size to 0.1 and look at different step values . at a high level , we assume that , on average , each query rewrite estimation will take approximately the same time to be computed . to this end , we ran all three algorithms over our data set and we compared their performance which is shown in figure  [ fig : algs - perf - over - timesteps ] . in the figure , the horizontal axis is the number of steps , and the vertical axis is the average mean - dist  at a given number of steps .",
    "the first observation is that both greedy - rewrite  and dp - rewrite  perform substantially better than attribute - removal .",
    "the second observation is that a larger of number steps does not necessarily translate to a better performance .",
    "this may appear counter - intuitive as one would assume that with more steps , the relaxation algorithm would get to `` explore '' the attribute space more fully to arrive at the right attribute combinations to relax . for greedy - rewrite , however , this needs not be the case .",
    "this is because in cases where the estimation routine underestimates the number of results , greedy - rewrite  will continue to relax beyond the point necessary , leading to a set of results with higher mean - dist , whereas a run with fewer number of steps will terminate with a relaxed query that it returns due to exhaustion of number of steps but lucks out in being one that retrieves sufficient number of results , leading to lower mean - dist .",
    "indeed , the performance of greedy - rewrite  deteriorates after @xmath45 steps since the additional relaxation of the attributes only results in adding more unrelated results to the result set .",
    "in contrast , for dp - rewrite , increasing the number of steps leads to steady improvements in mean - dist . while in principle dp - rewrite  may be plagued by the aforementioned problem for greedy - rewrite  due to underestimation , because it explores the space of relaxed queries more completely , it is less affected by poor estimation compared to greedy - rewrite . nonetheless , by around @xmath46 steps",
    ", the quality of the results do not improve any further as it starts to find exactly the same relaxed query .",
    "finally , figure  [ fig : algs - perf - over - epsilon ] summarizes the relative performance of all three algorithms for different values of the step size @xmath17 fixing the number of steps @xmath47 .",
    "again , we observe that both greedy - rewrite  and dp - rewrite  outperform attribute - removal .      as we discussed in section  [ sec : relaxation ] , one preprocessing step that we may apply to our algorithms is to identify attribute dependencies and drop dependent attributes from the query before rewriting it .",
    "attribute ( or functional ) dependencies are very useful in optimizing queries in database systems as they can capture the relations between attributes .",
    "our high - level intuition is that if attribute dependencies are present in our data set , it would help to take it into consideration as these dependencies point to _ dependence _ across attributes , hence accounting for them can help with estimation , which in turn helps to find better relaxed queries . to study the presence and effect of attribute dependencies to our algorithms ,",
    "we computed the conditional probabilities for all pairs of attributes and we kept only those that were higher than @xmath38 .    given a pair of attributes @xmath48 and @xmath49 where @xmath50 for a large number of pairs of attribute values @xmath24 and @xmath51 , we need to decide whether we should drop attribute @xmath48 or @xmath49 from the query before relaxing . as we discussed in section",
    "[ sec : relaxation ] the best choice depends on the distribution of values and the distance function for attribute @xmath48 or @xmath49 . for example , if @xmath48 is more selective ( i.e. appears in less tuples ) than @xmath49 , it may be better to drop attribute @xmath48 as it is expected to relax the query more than if we dropped @xmath49 . on the other hand , dropping @xmath49",
    "may also help since tuples it appears in are already partially implied by @xmath48 .    to this end",
    ", we repeated the experiment for identifying a good step size with the attribute - dependency preprocessing enabled .",
    "we computed the results for both alternatives for dropping an attribute ( that is , either @xmath48 or @xmath49 ) .",
    "we report the results in figure  [ fig : fd - greedy - perf - over - epsilon ] for the greedy - rewrite  algorithm using small and large values of @xmath3 ( @xmath52 and @xmath47 ) respectively .",
    "the findings for the dp - rewrite  algorithm are similar .",
    "the overall result is surprising , as we find that either approach of incorporating attribute dependencies by dropping attribute @xmath48 or @xmath49 have not led to better performance , and in some cases even worse performances . to understand this better",
    ", we perform a query - by - query analysis of the results , and found that the problem manifests itself due to a complex chain of interactions .",
    "first , a significant fraction of these queries are `` < brand > < model > query ` ' . the attribute dependencies we found are also between attribute _ brand _ and _ model _ , where each model is associated with a unique brand . as mentioned in section  [ sssec : distance - function ] , we do not have many distances estimated between models due to data sparsity .",
    "when the attribute _ model _ is dropped , we retrieve a number of different models of the same brand , all of which are considered to be quite far away from the query as we treat missing distances as @xmath31 .",
    "when the attribute _ brand _ is dropped , the situation is even worse as the algorithm will now relax the attribute _ model _ to close to distance @xmath31 in order to find sufficient number of results due to missing distances .",
    "hence , in such cases , performances are worse than not dropping attribute at all , as the results are now no longer constrained by _ brand_.      in another experiment , we measured the work done by the index in terms of the number of documents processed by the index .",
    "the processing done by the index typically includes computing ranking features and scoring the document for the given query .",
    "as the processing takes time , one would like the number of documents processed by the index close to the documents estimated by the rewrite algorithm .",
    "figure  [ fig : new - algs - numhits - over - numsteps ] illustrates the performance of the algorithms in terms of processing done by the index for step size @xmath17@xmath530.1 .",
    "the general trend is that dp - rewrite  produces rewrites that give close to the desired number of results of @xmath54 , and generates the least work for the index among the three algorithms . on the other extreme , attribute - removal  produces rewrites that generate the most work for the index due to its choice of removing the chosen attribute completely .",
    "greedy - rewrite  spans the performance gap between these two algorithms . for lower values of @xmath3",
    ", it results in smaller number of documents to be included in the filter set while at the higher values of @xmath3 , it comes close to attribute - removal  in terms of the number of documents admitted into the filter set .",
    "the reason for the behavior exhibited by greedy - rewrite  is as follows .",
    "as greedy - rewrite  explores one attribute at a time , and chooses its next step based on its current relaxed query , it performs a depth - first - like search through the space of relaxed queries . in many cases , due to its choice in prioritizing the relaxation in favor of the most selective attribute",
    ", it ends up repeatedly relaxing the same attribute leading to completely relaxing an attribute .",
    "these type of relaxed queries typically leads to retrieving significantly more number of results .",
    "note however that the result set may still have similar average quality as measured by mean - dist , as confirmed by the figures in the previous sections .",
    "in this paper we propose a query - rewrite framework for answering structured web queries when users pose queries that would have led to very few results .",
    "our framework takes into account the stringent time requirement of answering web queries , and balances it with the need of retrieving results close to the user queries .",
    "we describe two approaches to solving this problem , and show experimentally that both solutions produce meaningful results given our constraints .    after studying the performance of the three algorithms with respect to parameters like step size and the number of rewrites to explore ,",
    "we conclude that if time envelope admits more rewrites , then dp - rewrite  is more applicable . in the case of extremely small latency restrictions ,",
    "greedy - rewrite  is a better choice .",
    "the approaches proposed in this paper is especially important in domains where there is an underlying source of structured data , but for which users lacking domain expertise may end up issuing queries that have few or even zero matches .",
    "this contributes to the growing literature on how to efficiently surface structured results in response to web queries .",
    "v.  hristidis , l.  gravano , and y.  papakonstantinou .",
    "efficient ir - style keyword search over relational databases . in _",
    "vldb 2003 : proceedings of the 29th international conference on very large data bases _ , pages 850861 .",
    "vldb endowment , 2003 ."
  ],
  "abstract_text": [
    "<S> web search engines and specialized online verticals are increasingly incorporating results from structured data sources to answer semantically rich user queries . </S>",
    "<S> for example , the query `` samsung 50 inch led tv ` ' can be answered using information from a table of television data . </S>",
    "<S> however , the users are not domain experts and quite often enter values that do not match precisely the underlying data . </S>",
    "<S> samsung makes 46- or 55- inch led tvs , but not 50-inch ones . </S>",
    "<S> so a literal execution of the above mentioned query will return zero results . for optimal user experience </S>",
    "<S> , a search engine would prefer to return at least a minimum number of results as close to the original query as possible . </S>",
    "<S> furthermore , due to typical fast retrieval speeds in web - search , a search engine query execution is time - bound .    in this paper , we address these challenges by proposing algorithms that rewrite the user query in a principled manner , surfacing at least the required number of results while satisfying the low - latency constraint . </S>",
    "<S> we formalize these requirements and introduce a general formulation of the problem . </S>",
    "<S> we show that under a natural formulation , the problem is np - hard to solve optimally , and present approximation algorithms that produce good rewrites . </S>",
    "<S> we empirically validate our algorithms on large - scale data obtained from a commercial search engine s shopping vertical . </S>"
  ]
}