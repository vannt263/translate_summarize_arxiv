{
  "article_text": [
    "the development trends in microprocessor architecture show two different ways of scaling .",
    "the first is to increase the number of cores . for massively multi - core microprocessors , such as gpus , the cores are relatively simple , offering lower performance than standard general purpose cores , while the resources per single core , e.g. in the form of fast memories , are usually small .",
    "access times associated with variables in executed programs depend on the storage locations for variables , with three levels of memory usually explicitly available to programmers : registers , fast memory ( shared by several threads ) and slower global memory @xcite .",
    "another trend is to increase the width of vector execution units within single processor cores .",
    "this trend is visible e.g. in the recent general purpose x86 cores ( 256-bit vector units in intel sandy bridge and haswell cores ) and in the xeon phi co - processor @xcite , having cores equipped with 512-bit wide vector units . in co - processors , the cores are less numerous than in gpus , dozens as opposed to hundreds , and their complexity lies in between standard , general purpose cores ( with out - of - order execution , branch prediction etc . ) and simple gpu cores , with e.g. no instruction decoding units .    the powerxcell processor that we consider specifically in the current paper can be considered as the predecessor of architectures with wide vector units .",
    "it has 128 vector registers , having width of 128 bits , and no scalar registers .",
    "its performance depends heavily on the proper vectorization of the code .",
    "the cellbe architecture @xcite is well known in hpc community mainly due to being the essential ingredient of the first petaflops system , the roadrunner computer developed by ibm in 2008 .",
    "the cellbe architecture @xcite has attracted the interest of many researchers , who investigated its performance in several application domains . in the context of finite element calculations , the most important research concerned linear equations solvers and their computational kernels @xcite .    both contemporary microprocessor architectures presented above , gpus and xeon phi , share the same co - processor design , with slow pci express bus connecting coprocessors with host cpu and its memory .",
    "similarly , the throughput of powerxcell connection to dram memory is much lower than the combined throughput of links between powerxcell vector cores and their local memories . for such architectures ,",
    "data movement between different levels of memory may become an issue of primary importance @xcite .",
    "access times to different levels of memory can differ by several orders of magnitude , so an improper design of algorithms can result in slow execution times .    in consequence",
    ", many existing algorithms have to be redesigned for new architectures .",
    "redesigning should begin with a proper analysis of an algorithm , indicating operation count and detailed data movement requirements .",
    "when counting operations one should take into account whether operations are performed by scalar or vector units , while the analysis of data movement should incorporate different levels of memory hierarchy .",
    "the designs can be different for different architectures , since optimization performed for one architecture may turn out to be ineffective for another @xcite .",
    "numerical integration is the part of fem codes where local , element stiffness matrices are calculated for a given weak statement of the problem solved .",
    "the matrices are then assembled into a global system of linear equations which is solved subsequently .",
    "the time required for the solution of the system of linear equations usually strongly dominates the time of the whole fem solution process .",
    "this justifies the fact that most of research in the area of porting finite element codes to new processor architectures concentrates on solvers of linear equations @xcite .",
    "often the main stress is put on computational kernels of linear solvers that are properly optimized for particular architectures @xcite .",
    "however , the time necessary for numerical integration should not be neglected .",
    "computational complexity estimates , that will be presented later in the paper , show that in some cases the time required for numerical integration can be comparable to the time for solving the system of linear equations ( or even larger for iterative solvers ) .",
    "linear equations solvers employed in fem codes are often highly optimized for each appearing computer architecture .",
    "if such a module is combined with an improperly designed and implemented numerical integration algorithm , the latter can become a serious performance bottleneck .",
    "the relative significance of the performance of numerical integration depends on the problem ( its weak statement ) and the approximation applied . for low order",
    "finite element approximations , e.g. linear elements , the time required for element stiffness matrix creation is usually short , especially when compared to the time necessary to solve the associated system of linear equations . for many simple problems",
    ", integrals can even be precomputed analytically and the creation of stiffness matrix entries changes into purely algebraic operations , with closed formulas into which element parameters , like e.g. vertices coordinates , are substituted .",
    "in such cases creating the global stiffness matrix consists mainly of assembling , performed for quickly calculated element stiffness matrices @xcite .",
    "the situation is different for higher order approximations where polynomials with high degree @xmath0 are used . in such cases special element shape functions",
    "are often applied and special integration methods designed @xcite .",
    "these special methods can be relatively simple , as e.g. for hexahedral elements with tensor product shape functions , where standard @xmath1 complexity of numerical integration can be reduced to @xmath2 @xcite .",
    "for other types of elements , it is also possible to achieve lower complexity , but the costs associated with integrating special shape functions make the algorithms less efficient than standard techniques for lower orders of approximation @xcite .",
    "since low order polynomials are the most popular in practice , investigations concerning the creation of finite element stiffness matrices on modern processor architectures often concentrate on assembly procedures @xcite .",
    "the cited articles all consider assembly on gpus , the authors of the current paper are not aware of any attempts to explicitly vectorize assembly procedures on powerxcell or other processors with wide simd registers",
    ".    for higher order approximations , investigations concerning the porting of finite element codes to new computer architectures also focus mainly on gpu implementations @xcite .",
    "although several issues are common to powerxcell and gpu implementations , the general design philosophy and flow of execution are different .",
    "one should also mention here attempts to create formal specifications of finite element calculations , from which procedures for particular architectures are automatically created by suitable compilers @xcite .",
    "although very interesting in themselves , the specifications usually embody substantial knowledge concerning problems , algorithms and hardware in question .",
    "they can be considered as a further step in the investigations on the interplay between algorithms and computer architectures , that we try to develop in the current paper .      in the present article",
    "we consider a powerxcell implementation of the standard finite element numerical integration technique for 3d problems and orders of approximation up to 7 .",
    "the limiting order 7 is chosen , to certain extent , arbitrarily . for low order approximations",
    "( usually up to 3 ) the standard integration technique is more efficient and simpler than special techniques @xcite . for higher orders , it can be , in some cases , profitable to switch to special techniques of integration even for orders in the range 4 - 5 @xcite .",
    "the optimal integration technique may be different for different problems ( weak statements ) and different approximation methods .",
    "we choose standard integration techniques with an eye to applicability to different types of elements and the generic character of the approach , valid for different finite element formulations and orders of approximation .",
    "we do not consider particular weak statements for different problems in science and engineering , instead , we concentrate on a single term that appears in many problems .",
    "the present article is devoted to a thorough analysis of computational aspects of standard finite element numerical integration and the implementation of related algorithms for the cellbe architecture , in particular the powerxcell processor .",
    "it is a continuation and extension of works presented in @xcite .    as new developments , we present the analysis of data movement during calculations , that leads to the selection of implementation strategy",
    "moreover , we show how vector capabilities of current microprocessors can be utilized for numerical integration calculations .",
    "we present the results of computational tests that illustrate the performance of created implementations .",
    "the results were obtained using a modular finite element code modfem , developed for standard and discontinuous galerkin finite element simulations @xcite . the entire work reported in the paper",
    "is undertaken in an effort to port the code to modern computer architectures , including gpus and many - core processors .",
    "we are not aware of any other investigations concerning finite element numerical integration on the powerxcell processor and , in a broader context , the explicit use of vector capabilities of modern processors in finite element numerical integration procedures .",
    "calculation of integrals from a weak statement in finite element codes is usually performed assuming that the whole computational domain , as the domain of integration , is divided into finite elements , of one or several types  triangles , quadrilaterals , tetrahedra , prisms , hexahedra , etc . for distributed memory implementations",
    ", there exists also a partition of the computational domain into subdomains , with each subdomain associated with a single process performing calculations .",
    "when considering such domain decomposition approach , numerical integration appears as an embarrassingly parallel algorithm ( there are no dependencies and for proper overlapping decompositions there is also no communication necessary ) @xcite . in the rest of the paper",
    "we assume the domain decomposition technique for distributed memory machines and for further parallelization we concentrate on a single subdomain and a single cpu process .    for a single subdomain , in a loop over all subdomain elements , at each element , integrals corresponding to pairs of finite element basis functions are computed and the results stored in local , element stiffness matrices .",
    "local load vectors are obtained through integration of corresponding terms as well , but this procedure is computationally much less demanding and we neglect it in the present article .",
    "computed stiffness matrices are then assembled into a global stiffness matrix , the matrix of the associated system of linear equations .",
    "a single entry in the global matrix can be a sum of entries from several local matrices .",
    "hence , the assembly can not be considered as an ",
    "embarrassingly  parallel algorithm with no dependencies .",
    "however , classical techniques of coloring can be used to remove dependencies .",
    "the crucial observation is that two local matrices contribute to a single global entry if there is a neighborhood relation between them @xcite .",
    "using a proper coloring scheme , one can obtain sets of elements with different colors , with two elements having the same color if their local stiffness matrices do not coincide at any global matrix entry .",
    "given this partition of elements , it is assumed that the algorithm of numerical integration ( followed by immediate matrix assembly ) proceeds color by color .",
    "since we consider large scale calculations , it can be safely assumed that each set of elements of the same color has sufficient number of elements , in order to fully utilize parallel capabilities of the hardware .",
    "hence , when considering creation of local stiffness matrices by a single microprocessor , one can assume that there are no dependencies during assembly procedure . in consequence ,",
    "numerical integration for different elements can also be considered as dependence free and perfectly parallelizable .",
    "we consider numerical integration for an example second order term with partial derivatives of trial and test functions ( the term that appears e.g.  for approximations of laplace operator ) .",
    "the formula for obtaining an entry @xmath3 to the local stiffness matrix @xmath4 , associated with element @xmath5 can be expressed as : a^e_ij = _ _ e_k=1 ^ 3 d[wzor1 ] where @xmath6 and @xmath7 are global basis functions .    in order to compute integrals of the form ( [ wzor1 ] ) the change of variables",
    "is applied , which in practice means that we always perform integration on a reference element of a particular type . the transformation from the reference element to the real element",
    "is denoted by @xmath8 . for the reference element we use its shape functions @xmath9 , instead of global basis functions @xmath6 , and apply some form of numerical quadrature .",
    "the numerical quadrature transforms the integral into a sum over integration points within the reference domain .    from different possible quadratures we concentrate on the most popular gaussian quadratures .",
    "we assume that we use @xmath10 integration points with local coordinates @xmath11 and the associated weights @xmath12 .",
    "the number of integration points is determined by the requirement to accurately compute products of shape functions , neglecting non - linearity of coefficients and element geometry .",
    "this should suffice for the convergence of finite element approximations @xcite .",
    "after performing the steps described above , integral ( [ wzor1 ] ) is transformed to the sum : a^e_ij _ i=1^n_i _ k=1 ^ 3 _ _ ew^i [ wzor2 ] where @xmath13 is the jacobian matrix of transformation @xmath8 .",
    "the number of shape functions for an element , @xmath14 ( the range of indices @xmath15 and @xmath16 in ( [ wzor1 ] ) and ( [ wzor2 ] ) ) depends on the degree of approximating polynomials @xmath0 . for several typical 3d elements it is given in table [ tablica_1 ] . in the table , complete basis and tensor product basis refer to the way shape functions are defined for an element @xcite ( for prismatic elements shape functions are obtained as products of complete basis for horizontal triangular faces and 1d shape functions along the vertical direction ) . in any case it can be seen that the number of shape functions is @xmath17 for 2d elements and @xmath18 for 3d elements . according to our assumptions specified above , the number of gauss points is of the same order .",
    "[ section_accuracy ]    .the number of shape functions associated with a single element of a given type for different orders of approximation @xmath0 . [ cols=\"^,^\",options=\"header \" , ]     the results of experiments for the model problem and different orders of approximation are presented in tables [ times ] and [ gigaflops ]",
    ". table [ times ] shows the execution times for a single element , calculated by dividing the execution times for all elements by the number of elements .",
    "the table presents raw execution times measured by system tools ( for opencl measured at host side ) .",
    "the results for the powerxcell processor are split into different variants of implementation and additionally into the initialization phase ( that takes a significant portion of the execution time for @xmath19 ) and the total execution that includes the opencl initialization and the kernel operations ( calculations and global memory transfers ) .",
    "the results are illustrated in fig .",
    "[ fig_times ] .",
    "it can be seen that , for low order approximations , powerxcell does not offer significant improvements , even with respect to a single core of modern processors . with the increasing order @xmath0 and the growing ratio of calculations to global memory transfers the advantages of powerxcell become more evident .",
    "p=6 & p=7 + & 4.50 & 8.04 & 13.66 & 13.58 & 15.96 & 17.23 & 17.13 + & 18.02 & 32.16 & 54.63 & 54.34 & 63.84 & 68.94 & 68.51 +   + & 8.16 & 17.33 & 33.40 & 54.38 & 86.77 & 93.26 & 79.86 + & 5.22 & 10.66 & 27.12 & 43.83 & 74.61 & 81.51 & 73.22 + & 9.66 & 21.39 & 43.09 & 65.34 &  &  &  + & 6.67 & 15.77 & 34.34 & 51.69 &  &  &  + & 11.43 & 32.50 & 64.29 & 100.39 & 118.46 & 112.32 & 108.53 + & 6.06 & 13.08 & 33.29 & 47.84 & 61.80 & 59.22 & 58.84 + & 14.43 & 40.14 & 84.29 & 111.52 &  &  &  + & 7.94 & 18.72 & 42.07 & 51.92 &  &  & ",
    "+    the second table presents characteristics computed on the basis of measured quantities .",
    "the performance of the single sandy bridge core is calculated and then extrapolated for a typical contemporary quad - core processor , assuming perfect scalability with the growing number of cores . for each algorithm run on the powerxcell processor two results",
    "are presented .",
    "the first is the performance as observed by an external user  the number of operations theoretically necessary to obtain results ( taken from table  [ tablica_2 ] , the same as for the sandy bridge processor ) is divided by the total time of opencl execution , measured on host side and including initialization phase .",
    "the second value , denoted  internal  , is computed by dividing the number of operations actually performed by the hardware by the time of pure kernel execution .",
    "the difference between the two values for the wf algorithms is associated not only with the existence of initialization phase , but also with the fact that the processor performs more operations than it is required by the original , sequential algorithm .",
    "figure [ fig_perf ] shows graphically the performance results .",
    "it can be seen that , despite the fact that intel architecture represents much more recent design , the performance of both platforms is comparable . for the sandy bridge core ,",
    "the high performance was achieved by exploiting vector operations and single precision calculations . in the domain of general purpose scientific computing",
    "the ranges of performance obtained for this core and higher orders approximations should be considered very high .    for the powerxcell processor",
    "the results can be considered satisfactory for higher orders of approximation . similarly to x86 sandy bridge cores , spe cores made good use of the increasing ratio of calculations performed inside vectorized loops to the calculations performed outside the loops . despite substantial global memory transfers and the existence of parts of algorithms where neither fma operations nor vectorization can not be applied ( and hence the performance drops at least by the factor of 8) , the performance of pure calculations reaches more than 79 gflops ( 19% of theoretical maximum ) for the dd_sc version and more than 108 gflops ( 26% of theoretical maximum ) for the wf_sc variant .    for lower orders of approximation",
    ", the overhead associated with the initialization of calculations and the transfers from / to global dram memory significantly slows down execution .",
    "the number of operations for a single transferred entry of stiffness matrices is too small , considering also the fact that scalar operations , outside the double loop over shape functions , form high percentage of all calculations .",
    "one more characteristic of the parallel execution of numerical integration on powerxcell processor , that can be deduced form data in table [ times ] , is the classical measure of speed - up .",
    "it is depicted in fig .",
    "[ fig_speed_up ] .",
    "it can be seen that for 16 cores , the speed - up ranges from approx .",
    "11 to almost 16 ( with parallel efficiency always above 68% and for @xmath20 reaching more than 95% ) .    comparing the different versions of the implementation of numerical integration algorithm , one can conclude that the sr variants ( with shape functions read from global memory ) give always better performance results , while the sc versions ( with shape functions computed locally ) show better flexibility , by requiring much less memory resources .",
    "the choice between the dd and wf variants is definitely problem dependent .",
    "we have developed the wf algorithms to present the possibility of efficient parallelization that does not use data decomposition approach associated with element stiffness matrices . for certain situations ( e.g.",
    "when data decomposition is used for spreading calculation over different cores , as is the case for gpus ) , the option of parallelization based on a particular form of weak statement may become optimal .",
    "for our example case of laplacian term , both approaches turned out to be roughly equivalent .",
    "the performance results presented for the finite element numerical integration algorithm running on the powerxcell processor prove that the algorithm can be successfully ported to multi - core processors with manually managed memory hierarchy and vector execution units .",
    "the obtained range of performance numbers shows that in many situations high utilization of vector capabilities can be achieved .",
    "this seems to be an important conclusion in the light of a recently observed trend to equip standard processor cores with wide vector registers and execution units .",
    "another conclusion is that opencl can be used for relatively simple porting of scientific codes to complex heterogeneous multi - core architectures , such as cbe .",
    "moreover , opencl allows one to obtain high performance code , thanks to the support of explicit memory hierarchy management and vector operations .",
    "again , the experience gathered with powerxcell can be used in designing code for other types of processors , sharing architectural details with powerxcell , such as e.g. xeon phi .    as a further step of our investigations",
    "we plan to combine the experiences gathered in porting numerical integration procedures to powerxcell processor and gpus @xcite and try to design parametrized kernels for different architectures , including new xeon phi co - processors .    in a more general context",
    ", we see our efforts in transferring the numerical integration algorithm to the powerxcell processor as a step towards better understanding the relation between the software , in our case the finite element numerical integration algorithm , and the hardware of modern multi - core processors .",
    "we hope to utilize this knowledge in porting the whole finite element code to other modern multi- and many - core processors .",
    "k.  rojek , l.  szustak , adaptation of double - precision matrix multiplication to the cell broadband engine architecture , in : ppam09 : proceedings of the 8th international conference on parallel processing and applied mathematics , springer - verlag , berlin , heidelberg , 2010 , pp .",
    "535546 .",
    "n.  kushida , element - wise implementation of iterative solvers for fem problems on the cell processor , in : y.  cotronis , m.  danelutto , g.  a. papadopoulos ( eds . ) , proceedings of the 19th international euromicro conference on parallel , distributed and network - based processing , pdp 2011 , ayia napa , cyprus , 9 - 11 february 2011 , ieee computer society , 2011 , pp .",
    "401408 .",
    "s.  rul , h.  vandierendonck , j.  dhaene , k.  de  bosschere , an experimental study on performance portability of opencl kernels , in : application accelerators in high performance computing , 2010 symposium , papers , knoxville , tn , usa , 2010 , p.",
    "d.  gddeke , h.  wobker , r.  strzodka , j.  mohd - yusof , p.  mccormick , s.  a. turek , co  processor acceleration of an unmodified parallel solid mechanics code with feastgpu , international journal of computational science and engineering 4  ( 4 ) ( 2009 ) 254269 .",
    "d.  gddeke , r.  strzodka , j.  mohd - yusof , p.  mccormick , s.  h. buijssen , m.  grajewski , s.  turek , exploring weak scalability for fem calculations on a gpu - enhanced cluster , parallel computing 33  ( 10 - 11 ) ( 2007 ) 685699 .",
    "v.  volkov , j.  w. demmel , benchmarking gpus to tune dense linear algebra , in : proceedings of the 2008 acm / ieee conference on supercomputing , sc 08 , ieee press , piscataway , nj , usa , 2008 , pp . 31:131:11 .",
    "l.  demkowicz , j.  kurtz , d.  pardo , m.  paszynski , w.  rachowicz , a.  zdunek , computing with hp - adaptive finite elements , vol .",
    "2 : frontiers . three dimensional elliptic and maxwell problems with applications , chapman & hall / crc , 2007 .",
    "p.  e.  j. vos , s.  j. sherwin , r.  m. kirby , from h to p efficiently : implementing finite and spectral / hp element methods to achieve optimal performance for low- and high - order discretisations , j. comput .",
    "( 2010 ) 51615181 .      c.  cecka , a.  j. lew , e.  darve , http://dx.doi.org/10.1002/nme.2989[assembly of finite element methods on graphics processors ] , international journal for numerical methods in engineering 85  ( 5 ) ( 2011 ) 640669 .    c.  cecka , a.  j. lew , e.  darve , application of assembly of finite element methods on graphics processors for real - time elastodynamics , in : w .-",
    "w.  hwu ( ed . ) , gpu computing gems . jade edition , morgan kaufmann , 2011 , pp . 187205 .",
    "g.  r. markall , a.  slemmer , d.  a. ham , p.  h.  j. kelly , c.  d. cantwell , s.  j. sherwin , http://dx.doi.org/10.1002/fld.3648[finite element assembly strategies on multi - core and many - core architectures ] , international journal for numerical methods in fluids 71  ( 1 ) ( 2013 ) 8097 .",
    "d.  komatitsch , d.  micha , g.  erlebacher , porting a high - order finite - element earthquake modeling application to nvidia graphics cards using cuda , journal of parallel and distributed computing 69  ( 5 ) ( 2009 ) 451460 .",
    "a.  dziekonski , p.  sypek , a.  lamecki , m.  mrozowski , http://dx.doi.org/10.1002/nme.4452[generation of large finite - element matrices on multiple graphics processors ] , international journal for numerical methods in engineering ( 2012 )              b.  marker , j.  poulson , d.  s. batory , r.  a. van  de geijn , designing linear algebra algorithms by transformation : mechanizing the expert developer , in : m.  j. dayd , o.  marques , k.  nakajima ( eds . ) , high performance computing for computational science - vecpar 2012 , 10th international conference , kobe , japan , july 17 - 20 , 2012 , revised selected papers , vol .",
    "7851 of lecture notes in computer science , springer , 2013 , pp . 362378 .",
    "f.  kruel , k.  bana , finite element numerical integration on powerxcell processors , in : ppam09 : proceedings of the 8th international conference on parallel processing and applied mathematics , springer - verlag , berlin , heidelberg , 2010 , pp .",
    "517524 .",
    "k.  bana , a modular design for parallel adaptive finite element computational kernels , in : m.  bubak , g.  van albada , p.  sloot , j.  dongarra ( eds . ) , computational science  iccs 2004 , 4th international conference , krakw , poland , june 2004 , proceedings , part ii , vol .",
    "3037 of lecture notes in computer science , springer , 2004 , pp . 155162 .",
    "k.  bana , parallelization of large scale adaptive finite element computations , in : r.  wyrzykowski , j.  dongarra , m.  paprzycki , j.  waniewski ( eds . ) , parallel processing and applied mathematics , proceedings of vth international conference , ppam 2003 , czstochowa , poland , 2003 , vol .",
    "3019 of lecture notes in computer science , springer , 2004 , pp . 431438 .",
    "k.  bana , a model for parallel adaptive finite element software , in : r.  kornhuber , r.  hoppe , j.  priaux , o.  pironneau , o.widlund , j.  xu ( eds . ) , domain decomposition methods in science and engineering , vol .",
    "40 of lecture notes in computational science and engineering , springer , 2004 , pp . 159166 .",
    "paszewski , k.  bana , p.  macio , higher order fem numerical integration on gpus with opencl , in : computer science and information technology ( imcsit ) , proceedings of the 2010 international multiconference on , 2010 , pp .",
    "337 342 .",
    "p.  paszewski , p.  macio , k.  bana , finite element numerical integration on gpus , in : ppam09 : proceedings of the 8th international conference on parallel processing and applied mathematics , springer - verlag , berlin , heidelberg , 2010 , pp ."
  ],
  "abstract_text": [
    "<S> in our work we analyze computational aspects of the problem of numerical integration in finite element calculations and consider an opencl implementation of related algorithms for processors with wide vector registers .    as a platform for testing the implementation </S>",
    "<S> we choose the powerxcell processor , being an example of the cell broadband engine ( cellbe ) architecture . </S>",
    "<S> although the processor is considered old for today s standards ( its design dates back to year 2001 ) , we investigate its performance due to two features that it shares with recent xeon phi family of coprocessors : wide vector units and relatively slow connection of computing cores with main global memory . </S>",
    "<S> the performed analysis of parallelization options can also be used for designing numerical integration algorithms for other processors with vector registers , such as contemporary x86 microprocessors . </S>",
    "<S> we consider higher order finite element approximations and implement the standard algorithm of numerical integration for prismatic elements . </S>",
    "<S> original contributions of the paper include the analysis of data movement and vector operations performed during code execution . </S>",
    "<S> several versions of the implementation are developed and tested in practice .    </S>",
    "<S> finite elements , numerical integration , powerxcell processor , opencl , higher order approximation , vectorization </S>"
  ]
}