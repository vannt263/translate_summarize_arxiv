{
  "article_text": [
    "estimating the sparse precision matrix , i.e.  the inverse covariance matrix , from data is a very widely used method for exploring the dependence structure of continuous variables .",
    "the motivation for the approach stems from the fact that for a gaussian markov random field model , zeros in the precision matrix translate exactly to absent edges in the corresponding undirected gaussian graphical model , thus being informative about the marginal and conditional independence relationships among the variables .",
    "the full @xmath1-dimensional covariance matrix contains @xmath2 parameters , making its accurate estimation from limited data difficult .",
    "additionally , the structure learning requires the inverse of the covariance , and matrix inversion is in general a very fragile operation . to make the problem tractable",
    ", some form of regularisation is typically needed .",
    "direct optimisation of the sparse structure would easily lead to very difficult combinatorial optimisation problems . to avoid these computational difficulties ,",
    "several convex @xmath0-penalty - based approaches have been proposed .",
    "popular examples include @xmath0-penalised maximum likelihood estimation @xcite , which also forms the basis for the highly popular graphical lasso ( glasso ) algorithm @xcite .",
    "@xmath0 regularisation has also been used for example in a non - probabilistic alternative with linear - programming - based constrained @xmath0 minimisation ( clime ) algorithm of @xcite .    at the heart of the optimisation problems considered by all these methods",
    "is a term depending on the @xmath0 norm of the estimated precision matrix .",
    "@xmath0-penalisation - based approaches such as lasso are popular for sparse regression , but they have a known weakness : in addition to promoting sparsity they also push true non - zero elements toward zero  @xcite . in the context of precision matrix estimation",
    "this effect would be expected to be especially strong when some elements of the precision matrix are large , which happens for scaled covariance matrices when the covariance matrix becomes ill - conditioned .",
    "this phenomenon occurs frequently under the circumstances where some of the variables are nearly linearly dependent .    in this paper",
    "we demonstrate a drastic failure of the @xmath0 penalised sparse covariance estimation methods for a class of models that have a linear latent variable structure where some variables depend linearly on others . for such models even in the limit of infinite data , popular @xmath0 penalised methods can not yield results that are significantly better than based on random guessing on any setting of the regularisation parameter . yet",
    "these models have a very clear sparse structure that becomes obvious from the empirical precision matrix with an increasing @xmath3 .    given the huge popularity and success of linear models in modelling data , structures like the one considered in our work are natural for various real world data sets .",
    "motivated by our discovery , we also explore the inconsistency of @xmath0 penalised methods on models derived from real gene expression data and find them poorly suited for such applications .",
    "we start with a quick recap on the basics of gaussian graphical models in order to formulate the problem of structure learning . for a more comprehensive treatment of the subject",
    ", we refer to ( @xcite ; @xcite ) .",
    "let @xmath4 denote a random vector following a multivariate normal distribution with zero mean and a covariance matrix @xmath5 , @xmath6 .",
    "let @xmath7 be an undirected graph , where the @xmath8 is the set of nodes and @xmath9 stands for the set of edges .",
    "the nodes in the graph represent the random variables in the vector @xmath10 and absences of the edges in the graph correspond conditional independence assertions between these variables .",
    "more in detail , we have that @xmath11 and @xmath12 if and only if @xmath13 is conditionally independent of @xmath14 given the remaining variables in @xmath10 .    in the multivariate normal",
    "setting , there is a one - to - one correspondence between the missing edges in the graph and the off - diagonal zeros of the precision matrix @xmath15 , that is , @xmath16 ( see , for instance , @xcite , p.  129 ) . given an undirected graph @xmath17 , a gaussian graphical model is defined as the collection of multivariate normal distributions for @xmath10 satisfying the conditional independence assertions implied by the graph @xmath17 .",
    "assume we have a complete ( no missing observations ) i.i.d .",
    "sample @xmath18 from the distribution @xmath19 .",
    "based on the sample @xmath20 , our goal in structure learning is to find the graph @xmath17 , or equivalently , learn the zero - pattern of @xmath21 .",
    "the usual assumption is that the underlying graph is sparse .",
    "a naive estimate for @xmath21 by inverting the sample covariance matrix is practically never truly sparse for any real data .",
    "furthermore , if @xmath22 the sample covariance matrix is rank - deficient and thus not even invertible .    one common approach to overcome these problems is to impose an additional @xmath23-penalty on the elements of @xmath21 when estimating it .",
    "this kind of regularisation effectively forces some of the elements of @xmath21 to zero , thus resulting in sparse solutions . in the context of regression models , this method applied on the regression coefficients goes by the name of _ lasso _ @xcite .",
    "there exists a wide variety of methods making use of @xmath23-regularisation in the setting of gaussian graphical model structure learning ( @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ) .      in this section",
    "we provide a brief review of selected examples of different types of @xmath0-penalised methods .",
    "we begin with the widely used graphical lasso - algorithm ( glasso ) by @xcite .",
    "glasso - method maximises an objective function consisting of the gaussian log - likelihood and an @xmath24-penalty : @xmath25where @xmath26 denotes the sample covariance matrix and @xmath27 is the regularisation parameter controlling the sparsity of the solution .",
    "the @xmath0 penalty , @xmath28 , is applied on all the elements of @xmath29 , but the variant where the diagonal elements are omitted is also common .",
    "the objective function ( [ eq : glassoobj ] ) is maximised over all positive definite matrices @xmath21 and the optimisation is carried out in practice using a block - wise coordinate descent .",
    "@xcite approach the problem of sparse precision matrix estimation from a slightly different perspective .",
    "their clime - method ( constrained @xmath0-minimisation for inverse matrix estimation ) seeks matrices @xmath29 with a minimal @xmath0-norm under the following constraint @xmath30 is the tuning parameter and @xmath31 is the element - wise maximum .",
    "the optimisation problem @xmath32 subject to the constraint ( [ eq : clime ] ) does not explicitly force the solution to be symmetric , which is resolved by picking from estimated values @xmath33 and @xmath34 the one with a smaller magnitude into the final solution . in practice ,",
    "the optimisation problem is decomposed over variables into @xmath1 sub - problems which are then efficiently solved using linear programming .",
    "@xcite introduced recently a method called sparse column - wise inverse operator ( scio ) .",
    "the scio - method decomposes the estimation of @xmath29 into a following smaller problems @xmath35where @xmath26 and @xmath36 are defined as before and @xmath37 is an @xmath38:th standard unit vector .",
    "the regularisation parameter @xmath36 can in general vary with @xmath38 but this is omitted in our notation .",
    "the solutions @xmath39 form the columns for the estimate of @xmath29 .",
    "also for scio , the symmetry of the resulting precision matrix must be forced , and this is done as described in the case of clime .",
    "in addition to the above - mentioned @xmath0-penalised methods , we consider two alternative approaches . in a `` naive '' approach",
    ", we simply take the sample covariance matrix , invert it , and then threshold the resulting matrix to obtain a sparse estimate for the precision matrix .",
    "the threshold value is chosen using the ground truth graph so that the naive estimator will have as many non - zero entries as there are edges in the true graph .",
    "setting the threshold value according to the ground truth is of course unrealistic , however , it is nevertheless interesting to compare the accuracy of this simple procedure to the performance of the more refined @xmath0-methods , when also their tuning parameters are chosen in a similar fashion .",
    "lastly , we consider a bayesian approach which is based on finding a graph with a highest fractional marginal pseudo - likelihood ( fmpl ) by @xcite .",
    "the fractional marginal pseudo - likelihood is an approximation of the marginal likelihood and it has been shown to be a consistent scoring function in the sense that the true graph maximises it as the sample size tends to infinity , under the assumption that data are generated from a multivariate normal distribution .",
    "the fmpl - score decomposes over variables and in practice , the method identifies optimal markov blankets for each of the variables , which are then combined into a proper undirected graph using any of the three different schemes commonly employed in graphical model learning : or , and and hc .",
    "the assumptions required for a consistent model selection with an @xmath24-penalised gaussian log - likelihood have been studied , for instance , in @xcite .",
    "the authors provide a number of conditions in the multivariate normal model that are sufficient for the recovery of the zero pattern of the true precision matrix @xmath40 with a high probability when the sample size is large . for our purposes ,",
    "the most relevant condition is the following :    [ assumption1 ] there exists @xmath41,$ ] such that @xmath42    here @xmath43 is a set defining the support of @xmath40 , that is , the non - zero elements of @xmath40 ( diagonal and the elements corresponding to the edges in the graphical model ) and @xmath44 refers to the complement of @xmath45 in @xmath46 .",
    "the @xmath47 term is defined via kronecker product @xmath48 as @xmath49 and @xmath50 refers to the specific rows and columns of @xmath47 indexed by @xmath51 and @xmath52 , respectively .",
    "the norm in the equation is defined as @xmath53 .",
    "the above result applies to glasso .",
    "however , a quite similar result was presented for scio in @xcite :    [ assumption2 ] there exists @xmath54 such that @xmath55    here @xmath56 and @xmath57 .",
    "assumption [ assumption2 ] under the multivariate normality guarantees that the support of @xmath40 is recovered by scio with a high probability as the sample size gets large .",
    "methods for sparse precision matrix estimation generally depend on an objective function ( such as log - likelihood ) and a penalty function or regulariser , which in a bayesian setting is usually represented by the prior . the ideal penalty function for many problems",
    "would be the @xmath58 `` norm '' counting the number of non - zero elements : @xmath59 .",
    "this @xmath58 function is not a proper norm , but it provides a very intuitive notion of sparsity . the main problem with its use is computational : using @xmath58-penalisation leads to very difficult non - convex combinatorial optimisation problems .",
    "the most common approach to avoid the computational challenges is to use @xmath0-penalisation as a convex relaxation of @xmath58 .",
    "as mentioned above this works well in many cases but it comes with a price , since in addition to providing the sparsity , @xmath0 also regularises large non - zero values .",
    "depending on the problem , as we demonstrate here , this effect can be substantial and may cause @xmath0-regularised methods to return totally meaningless results .    intuitively , @xmath0-regularised methods are expected to fail when some elements of the true precision matrix become so large that their contribution to the penalty completely overwhelms the other parts of the objective and the penalty .",
    "one example where this happens is when some set of variables depends linearly on another set of variables .",
    "in such situation the covariance matrix can become ill - conditioned and the elements of its inverse , the precision matrix , grow .",
    "one example of when this happens is models with a linear latent variable structure .",
    "let us consider a model for @xmath60 , where @xmath61 .",
    "the graphical structure of the model and the corresponding precision matrix structure are illustrated in fig .",
    "[ fig : latent_variable_structure ] . assuming @xmath62 ,",
    "the covariance of the concatenated vectors @xmath63 is given by the block matrix @xmath64 the covariance matrix has an analytical block matrix inverse @xcite @xmath65 this precision matrix recapitulates the conditional independence result for gaussian markov random fields : the lower right block is diagonal because the variables in @xmath66 are conditionally independent of each other given @xmath67 .",
    "the matrix is clearly sparse , so we would intuitively assume sparse precision matrix estimation methods should be able to recover it .",
    "the non - zero elements do , however , depend on @xmath68 which can make them very large if the noise @xmath69 is small .",
    "it is possible to evaluate and bound the different terms of eq .",
    "( [ eq : glassoobj ] ) evaluated at the ground truth for these models : @xmath70 the magnitude of the last penalty term clearly grows very quickly as @xmath69 decreases .",
    "clearly the magnitude of the two first log - likelihood terms grows much more slowly as they only depend on @xmath71 .",
    "thus the total value of eq .",
    "( [ eq : glassoobj ] ) decreases without bound as @xmath69 decreases .    forgetting the ground truth , it is easy to see that one can construct an estimate @xmath21 for which the objective remains bounded . if we assume all values of @xmath72 to be @xmath73 ( after normalisation ) , @xmath74 as the other terms only depend on @xmath21 it is easy to choose @xmath21 so that they remain bounded .",
    "the estimate @xmath21 that yields these values will in many cases not have anything to do with @xmath75 , as seen in the experiments below .",
    "we tested the performance of glasso , scio and clime as well as fmpl using the model structure introduced in sec .",
    "[ sec : latent - variable - like ] .",
    "the performance of the methods was investigated by varying the noise variance @xmath69 , and the sample size @xmath3 .",
    "the model matrix @xmath76 was created as a @xmath77-array of independent normal random variables with mean @xmath78 and variance @xmath79 .",
    "the majority of the tests were run using input dimensionality @xmath80 , output dimensionality @xmath81 and noise variance @xmath82 but we also tested varying these settings . for each individual choice of noise and sample",
    "size , @xmath83 different matrices @xmath76 were generated and the results were averaged .",
    "generating @xmath3 samples using model described , data were normalised and analysed using the five different methods .",
    "we calibrated the methods in a way that number of edges in the resulting graph would match the true number .",
    "similarly , we thresholded the naive method by taking inverse matrix directly to output the correct number of edges .",
    "the fmpl method has no direct tuning parameters so we used its or mode results as such .",
    "similar tuning is not possible in a real problem where the true number of edges is now known .",
    "the tuning represents the best possible results the methods could obtain with an oracle that provides an optimal regularisation parameter .",
    "we evaluated the results using the hamming distance between the ground truth and the inferred sparsity pattern , i.e.  the number of incorrect edges and non - edges which were treated symmetrically . for methods returning the correct number of edges , this value",
    "is directly related to the precision @xmath84 through @xmath85 or conversely @xmath86 we will nevertheless use the hamming distance as it enables fair comparison with fmpl that sometimes returns a different number of edges .",
    "[ fig : noise_n100 ] and [ fig : noise_n1000 ] show the hamming distance obtained by the different methods as a function of the noise level when using 100 and 1000 samples , respectively .",
    "the results show that especially for low but also for high noise levels , the @xmath0-based methods all perform very poorly with especially glasso and clime performing very close to random guessing level for low noise levels @xmath87 .",
    "the naive inverse and fmpl work much better up to moderate noise levels of @xmath88 after which the noise starts to dominate the signal and the performance of all methods starts to drop .",
    "scio is a little better than the other @xmath0-based methods but clearly worse than fmpl and naive in the low noise regime .",
    "[ fig : outdim_n1000 ] shows the results when changing the output dimensionality @xmath89 from 10 .",
    "the results show that the performance of all @xmath0-based methods is very poor across all @xmath89 .",
    "glasso performance is close to random guessing level across the entire range considered , while clime is slightly better for @xmath90 and scio slightly better across the entire range .",
    "both fmpl and naive are significantly better than any of the @xmath0-based methods .",
    "[ fig : indim_n1000 ] shows the corresponding result when changing the input dimensionality @xmath91 .",
    "the results are now quite different as all methods are better than random especially for larger values .",
    "scio still outperforms clime which outperforms glasso .",
    "fmpl is really accurate for small @xmath91 but degrades for larger @xmath91 while the naive method is the most accurate in almost all cases .    ) for latent variable like model with 1000 samples .",
    "the green curves show the contributions of the first two terms of eq .",
    "( [ eq : glassoobj ] ) and the blue curves show the contributions of the last penalty term .",
    "solid lines show the result of the glasso optimal solution while dashed lines show the result for the true solution . ]    to further illustrate the behaviour of glasso on these examples , fig .",
    "[ fig : obj_n100 ] shows the contributions of the different parts of the glasso objective function ( [ eq : glassoobj ] ) as a function of the noise level both for the true solution ( `` truth '' ) as well as the glasso solution .",
    "the results show that for low noise levels the penalty incurred by the true solution becomes massive .",
    "the glasso solution has a much lower log - likelihood ( `` logl '' ) than ground truth but this is amply compensated by the significantly smaller penalty . as the noise increases , the penalty of the true solution decreases and the glasso solution converges to similar values .",
    "it can be checked that the norm @xmath92 in assumption 1 and eq .",
    "( [ eq : assumption1 ] ) for latent - variable - like models depends on the scale of @xmath76 .",
    "we took advantage of this by creating examples with different values of @xmath92 and testing the precision of glasso using the true covariance which corresponds to infinite data limit .",
    "the results of this experiment are shown in fig .",
    "[ fig : latent_assumption1 ] .",
    "the results verify that glasso consistently yields perfect results when @xmath93 which is a part of the sufficient conditions for consistency of glasso .",
    "as @xmath92 grows and the sufficient conditions are no longer satisfied , it is clearly seen that the accuracy of glasso starts to deteriorate rapidly .",
    "this suggests that the sufficient condition of assumption 1 is in practice also necessary to ensure consistence .     of assumption 1 and eq .",
    "( [ eq : assumption1 ] ) .",
    "values to the left of the green vertical line satisfy this condition while values to the right violate it .",
    "( higher values are better . ) ]",
    "we tested how often the problems presented above appear in real data using the `` tcga breast invasive carcinoma ( brca ) gene expression by rnaseq ( illuminahiseq ) '' data set @xcite downloaded from https://genome - cancer.ucsc.edu / proj / site / hgheatmap/. the data set contains gene expression measurements for 20530 genes for @xmath94 samples . after removing genes with a constant expression across all samples there are @xmath95 genes remaining .    in order to test the methods we randomly sampled subsets of @xmath96 genes and considered the correlation matrix @xmath97 over that subset .",
    "we generated sparse models with known ground truth by computing the corresponding precision matrix @xmath98 from the empirical correlation matrix , setting elements with absolute values below chosen cutoff @xmath99 to 0 to obtain @xmath100 and the testing covariance matrix @xmath101 .",
    "the cutoff lead to networks that were sparse with on average 60% zeros in the precision matrix .    ) on real gene expression data showing the fraction of random subsets of @xmath96 genes that fulfil the requirement and various relaxations .",
    "the condition  ( [ eq : assumption1 ] ) requires @xmath93 , but the figure shows results also for larger @xmath92 cutoffs . ]    fig .",
    "[ fig : tcga_assumption1 ] shows the fraction of covariances derived from random subsets of @xmath96 genes that satisfy the assumption 1 of @xcite ( @xmath102 ) as well as the fraction of values below more relaxed bounds .",
    "the figure shows that the assumption is reliably satisfied only for very small @xmath96 while for @xmath103 , the assumption is essentially never satisfied .",
    "based on the results of fig .",
    "[ fig : latent_assumption1 ] it is likely that glasso results will degrade significantly by for @xmath104 and beyond which are very common for large networks .",
    "we further studied how accurately glasso can recover the graphical structures when the data were generated using the precision matrices described above .",
    "we used a similar thresholding with a cut - off value of @xmath105 in order to first form sparse precision matrices for a random subset of genes with given dimension .",
    "these matrices were then inverted to obtain covariance matrices .",
    "we checked that the resulting matrices were positive definite and then used them to sample multivariate normal data with zero mean with different sample sizes .",
    "the obtained data sets were centred and scaled before computing the sample covariance which was used as input to the glasso algorithm .",
    "the regularisation parameter was chosen with the aid of the ground truth graph , so that the the graph identified by glasso would contain as many edges as there were in the real graph .",
    "results are shown in figure [ fig : tcga_precision ] .",
    "the results show that glasso performance decreases as the network size increases and is approaching that of random guessing for the largest networks considered here .        ) on real gene expression data over random subsets of @xmath96 genes .",
    "the values are shown for the @xmath0 penalty term as well as the unnormalised log - likelihood , divided by @xmath96 to make them comparable .",
    "solid lines show the values for glasso result while dashed lines show the result for ground truth . ]",
    "[ fig : tcga_objective ] shows the contributions of different parts of the glasso objective function ( [ eq : glassoobj ] ) as a function of the number of genes @xmath96 .",
    "the regularisation parameter @xmath36 of glasso was tuned to return a solution with the same number of edges as in the true solution .",
    "we used the glasso implementation of scikit - learn @xcite , which ignores the diagonal terms of @xmath29 when computing the penalty .",
    "the figure shows clearly how the penalty term for the true solution increases superlinearly as a function of @xmath96 .",
    "( a linear increase would correspond to a horizontal line . )",
    "the result is even more striking given that the optimal @xmath36 decreases slightly as @xmath96 increases .",
    "the penalty contribution for glasso solution increases much more slowly . the excess loss in log - likelihood from glasso solution increases as @xmath96 increases , but this is compensated by a larger saving in the penalty .",
    "together these suggest that glasso solutions are likely to remain further away from ground truth as @xmath96 increases .",
    "the class of latent variable like models presented in sec .",
    "[ sec : latent - variable - like ] is an interesting example of models that have a very clear sparse structure , which all @xmath0-penalisation - based methods seem unable to recover even in the limit of infinite data .",
    "this class complements the previously considered examples of models where glasso is inconsistent including the `` two neighbouring triangles '' model of @xcite and the star graph of @xcite , the latter of which can be seen as a simple special case of our example .",
    "an important question arising from our investigation is how significant the discovered limitation to inferring sparse covariance matrices is in practice , i.e. how common are the latent variable like structures in real data sets .",
    "given the popularity and success of linear models in diverse applications it seems plausible such structures could often exist in real data sets , either as an intrinsic property or as a result of some human intervention , e.g.  through inclusion of partly redundant variables .",
    "the gene expression data set is a natural example of an application where graphical model structure learning has been considered .",
    "the original glasso paper  @xcite contained an example on learning gene networks , although from proteomics data .",
    "other authors ( e.g. * ? ? ?",
    "* ) have applied gaussian graphical models and even glasso ( e.g. * ? ? ?",
    "* ) to gene network inference from expression data .",
    "our experiments on the tcga gene expression data suggest that in such applications it is advisable to consider the conditions for the consistency of @xmath0 penalised methods very carefully when planning to apply those .",
    "previous publications presenting new methods for sparse precision matrix have typically tested the method on synthetic examples where the true precision matrix is specified to contain mostly small values . specifying the precision matrix provides a convenient way to generate test cases as the sparsity pattern can be defined very naturally through it . at the same time , this excludes any models that have an ill - conditioned covariance . as shown by our example , such ill - conditioned covariances arise very naturally from model structures that are plausible from the application perspective .    ultimately , our results suggest that users of the numerous @xmath0 penalised methods should be much more careful about checking whether the conditions of consistency for precision matrix estimation are likely to be fulfilled in the application area of interest",
    ".          o.  banerjee , l.  el  ghaoui , and a.  daspremont .",
    "model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data .",
    "_ journal of machine learning research _ , 9:0 485516 , june 2008 .",
    "p.  menndez , y.  a.  i. kourmpetis , c.  j.  f. ter braak , and f.  a. van eeuwijk .",
    "gene regulatory networks from multifactorial perturbations using graphical lasso : application to the dream4 challenge . _ plos one _ , 50 ( 12):0 e14147 , 2010 .",
    "f.  pedregosa , g.  varoquaux , a.  gramfort , v.  michel , b.  thirion , o.  grisel , m.  blondel , p.  prettenhofer , r.  weiss , v.  dubourg , j.  vanderplas , a.  passos , d.  cournapeau , m.  brucher , m.  perrot , and e.  duchesnay .",
    "scikit - learn : machine learning in python .",
    "_ journal of machine learning research _ , 12:0 28252830 , 2011 .",
    "p.  ravikumar , m.  j. wainwright , g.  raskutti , and b.  yu . high - dimensional covariance estimation by minimizing @xmath0-penalized log - determinant divergence .",
    "_ electronic journal of statistics _ , 5:0 935980 , 2011 ."
  ],
  "abstract_text": [
    "<S> various @xmath0-penalised estimation methods such as graphical lasso and clime are widely used for sparse precision matrix estimation . </S>",
    "<S> many of these methods have been shown to be consistent under various quantitative assumptions about the underlying true covariance matrix . </S>",
    "<S> intuitively , these conditions are related to situations where the penalty term will dominate the optimisation . in this paper </S>",
    "<S> , we explore the consistency of @xmath0-based methods for a class of sparse latent variable -like models , which are strongly motivated by several types of applications . </S>",
    "<S> we show that all @xmath0-based methods fail dramatically for models with nearly linear dependencies between the variables . </S>",
    "<S> we also study the consistency on models derived from real gene expression data and note that the assumptions needed for consistency never hold even for modest sized gene networks and @xmath0-based methods also become unreliable in practice for larger networks . </S>"
  ]
}