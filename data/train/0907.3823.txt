{
  "article_text": [
    "currently , the world wide web is the largest source of information .",
    "huge amount of data is present on the web and large amount of data is added to the web constantly . often the information pertaining to a topic is present across several web pages .",
    "it is a tedious task for the user to go through all these documents as the number of documents available on a topic will range from tens to thousands .",
    "it will be of great help for the user if a query specific multi - document summary is generated .",
    "summary generation can be broadly divided as abstractive and extractive . in abstractive summary generation ,",
    "the abstract of the document is generated .",
    "the summary so formed need not have exact sentences as present in the document .",
    "in extractive summary generation , important sentences are extracted from the document .",
    "the generated summary contains all such extracted sentences arranged in a meaningful order . in this paper ,",
    "generated summaries are extractive .",
    "summary can be generated either on a single document or on several documents . in multi - document summary generation ,",
    "other issues like time , ordering of extracted sentences , scalability etc . will arise .",
    "summary can be either generic or query specific .",
    "in generic summary generation , the important sentences from the document are extracted and the sentences so extracted are arranged in appropriate order . in query specific summary generation ,",
    "the sentences are scored based on the query given by the user .",
    "the highest scored sentences are extracted and presented to the user as summary . for a set of documents on a topic and a query related to the topic , suppose a summary is available .",
    "if a new document is now made available to the system then summary has to be regenerated with the new document included into the input set by running the query - specific summarizer . but this is not a good solution as it takes considerable amount of time to run the summarizer afresh and a lot of space to store all the original documents . also , most of the times the documents used for summarization may not be accessible .",
    "broadly the methodology used to summarize multiple documents is to combine all the documents into a unified structure in an intelligent ( each system / approach has its own methodology ) fashion and then the summary is generated by taking the unified structure as the input .",
    "the construction of unified structure is a time taking task .",
    "so , the time complexity of query - specific multi - document summarization is high .    in this paper",
    "we address the following problem : given an extractive summary that is generated for a given query on a set of documents , upon the arrival of a new document , the summary has to be updated without considering the initial set of documents .",
    "the proposed system will be given the present summary and the ( @xmath0 ) document as the input and the output should be the updated summary . in this paper",
    "we propose a novel and efficient model for query - specific update summary generation using extractive mechanism . to the best of our knowledge",
    "this problem is not addressed in the literature .",
    "the rest of the paper is organized as follows : in section [ relatedwork ] we discuss the related work",
    ". generation of embedded document is discussed in section [ model ] . in section [ algorithm ]",
    "we introduce the methodology to accomplish the task of update summary generation .",
    "experimental setup is discussed in section [ experimentalsetup ] . in section [ experimentalresults ] results",
    "are discussed and conclusions are given in section [ conclusions ] .",
    "text summarization has gained popularity in the recent years .",
    "a generic summary generation on single document is discussed by cajun wan et al .",
    "@xcite . both summary and keywords",
    "are extracted from a single document by following iterative reinforcement approach . to extract summary from the document ,",
    "the following relations are used : sentence - sentence relation , word - word relation and sentence - word relation .",
    "a generic summary generation on multiple documents is discussed by radev et al . in @xcite .",
    "centroid based approach is followed by this system , called mead , to generate summary .",
    "given a set of documents about a particular topic i.e. , a cluster of documents , the centroid of the cluster is calculated .",
    "a score is given to each sentence in the cluster with respect to the centroid .",
    "sentences are selected in decreasing order of sentence scores and are arranged with respect to the chronological order of their respective documents .",
    "extractive summary generation is discussed in @xcite .",
    "the input to extractive summarizers is the set of documents that are to be summarized and the output is the sentences extracted from the input documents .",
    "the sentences so extracted are arranged in a manner which increases coherence ( logical flow ) to the generated summary . in particular ,",
    "the former criteria is addressed in @xcite .",
    "single document generic summary is discussed in @xcite , here , extraction of the sentences from a document to generate a summary is accomplished by using sentence - sentence , word - word and sentence - word relationships .",
    "single document query - specific summary generation is discussed in @xcite , here , a connected sub - graph of sentences are extracted from the document graph .",
    "sentences are said to be connected if the similarity measure between them is above a threshold .",
    "multi - document generic summary generation is discussed in @xcite . in @xcite , all the sentences from the documents are given scores and the sentences are selected into the summary in the decreasing order of their scores . in @xcite , sentences are given scores based on the model inspired by pagerank @xcite .",
    "multi - document query - specific summary generation is discussed in @xcite . in @xcite ,",
    "query is also considered as one of the sentences in a document .",
    "similarities between all the pairs of sentences in the documents are calculated and these similarity values are used while giving the scores to the individual sentences . in @xcite ,",
    "two types of scores are calculated , first one is based on the similarity between sentences and the second is based on the similarity of a sentence with respect to the query .",
    "centrality based approaches are discussed in @xcite . in centrality",
    "based approaches , the salience of a sentence is calculated based on both the contribution of the sentence and the type of neighbouring sentences it is surrounded .",
    "degree centrality is discussed in @xcite and eigenvector centrality is discussed in @xcite .",
    "concept of bushy path was introduced by salton et al . in @xcite .",
    "nodes with high degree are called bushy nodes .",
    "bushy path is defined as a path connecting top @xmath1 bushy nodes .",
    "eigenvector centrality of a node is calculated by taking into consideration both the degree of the node and the degree of the nodes connecting to it .",
    "this is inspired by pagerank @xcite .",
    "redundancy handling is addressed in @xcite .",
    "this principal is followed by many other systems .",
    "mean marginal relevancy(mmr ) principal is as follows : node scores are calculated w.r.t the query .",
    "summary is generated incrementally .",
    "a node with highest score is selected into the summary .",
    "all the scores of remaining nodes are recalculated based on the nodes already selected into summary and the node score they possess . from the recalculated scores",
    ", the highest scored node will be added to summary .    in all the above approaches ,",
    "a summary is generated from scratch . in this paper",
    "we address the problem of updating the extracted summary with the availability of a new document . here",
    "we update the summary for a given query .",
    "this problem of update summary generation is proposed by us and the detailed procedure to accomplish this task is explained in the following sections .",
    "we follow a graph based approach to accomplish the task of update summary generation .",
    "every sentence in the document is a node and the edges are placed between the nodes if the similarity score between them is above a threshold .",
    "hereafter we use the words ,  node \" and  sentence \" , interchangeably .",
    "similarity between the nodes is calculated using the equation [ edgescore ] .",
    "@xmath2 where @xmath3 and @xmath4 are term vectors for the nodes @xmath5 and @xmath6 respectively .",
    "the weight of each term in @xmath3 is calculated as @xmath7 . here",
    "@xmath8 is _ term frequency _ and @xmath9 is _ inverse sentential frequency_. @xmath10 is defined as the number of times a term occurs in a sentence .",
    "_ inverse sentential frequency _ is defined as @xmath11 , where @xmath12 is total number of sentences in the document and @xmath13 is number of sentences in which the term is present .    in this section",
    "we propose an approach to embed the summary into the new document .",
    "algorithm [ algo : updatedocument ] sketches the details of the embedding of the current summary into the new document .",
    "[ algo : updatedocument ] * input * : currentsummary and newdocument * output * : document with summary embedded into it swap currentsummary and newdocument [ 6 ] let @xmath14 be the nodes in document + let @xmath15 be the nodes in summary + embeddeddocument = newdocument [ 1 ] insert the last sentence of the summary into the embeddeddocument(all the nodes in the embeddeddocument are considered for insertion ) using the strategy explained in section [ insertionstrategy ] [ 2 ] insert the first sentence of the summary into the embeddeddocument(only the nodes above the @xmath16 in the embeddeddocument are considered for insertion ) using the strategy explained in section [ insertionstrategy ] [ 3 ] [ 4 ] insert the summary node @xmath17 into the embeddeddocument(only the nodes between @xmath18 and @xmath16 in embeddeddocument are considered for insertion ) using the strategy explained in section [ insertionstrategy ] [ 5 ] return embeddeddocument    the algorithm [ algo : updatedocument ] gives the method of embedding the sentences from summary into the document .",
    "line 3 is very crucial , here the @xmath19 gives the number of sentences in @xmath20 .",
    "idea is that if the size of the summary is less than the new document s size then the summary will be embedded into the new document otherwise the new document will be embedded into the summary .",
    "this section gives the detailed explanation of insertion strategy .",
    "a node @xmath21 in the summary is placed in the document appropriately .",
    "the steps to be followed are given below :    * similarity ( calculated using equation [ edgescore ] ) of @xmath21 is calculated with the nodes ( the nodes that are specified in algorithm [ algo : updatedocument ] ) in the document .",
    "* let @xmath22 be a node in the document which has maximum similarity with the node in the summary .",
    "* let @xmath23 and @xmath24 be the preceding and following nodes of @xmath22 respectively .",
    "* calculate the similarity of @xmath21 with @xmath23 and @xmath24 .",
    "* @xmath21 is placed in between @xmath23 and @xmath22 if @xmath21 has greater similarity value with @xmath23 than @xmath24 , otherwise @xmath21 will be placed in between @xmath22 and @xmath24 .",
    "when the similarity value of node @xmath17 is zero with every node of the embeddeddocument then the node is inserted immediately after @xmath18 in embeddeddocument . here",
    "node @xmath17 is the node that is following node @xmath18 in the summary .",
    "if @xmath18 is not present then the node is placed immediately before @xmath25 in the embeddeddocument ( in this case , @xmath25 is inserted before inserting @xmath17 ) .",
    "the former process is recursive in nature . even after calling recursively",
    "if the nodes in the summary are not embedded then the summary will be appended to the document .",
    "this exception handling module will be used rarely by the system .",
    "we assume that the new document which arrived is related to the topic and therefore it is unlikely that the sentences in summary will have similarity value of zero with the sentences in the new document .",
    "even otherwise the strategy holds good i.e. , if the new document is an outlier(document that does not contain any information related to the query ) then none of the sentences will be selected from the new document and the sentences of old summary alone will be selected .",
    "in this section , summary generation on the embedded document is discussed . here the score of the node is calculated based on the query posed by the user i.e. , the node gets score based on its relevance to the query .",
    "node score calculation is based on the equation [ nodescore ] .",
    "@xmath26 here @xmath27 is the number of query terms in the given query , @xmath1 is the sentence and @xmath28 is the query term .",
    "if the query term is present in the sentence then a non - zero value is assigned otherwise zero is assigned .",
    "@xmath29    here @xmath30 is the number of sentences adjacent to @xmath21 that have the query term and have non - zero similarity with @xmath21 .",
    "@xmath31 is the bias factor . in equation [ nodescore ] ,",
    "the first part captures the importance of the sentence with respect to the query term and the second part captures the type of neighbours ( adjacent sentences ) .",
    "two sentences are said to be adjacent if the similarity value between them is above a threshold(=0.001 ) . _",
    "the score of a node is the summation of equation [ nodescore ] over all the query terms .",
    "_ unlike the node score equation in @xcite , the equation [ nodescore ] is not iterative .",
    "also , this equation considers only immediate neighbours while assigning node scores .",
    "this makes the system efficient .",
    "node scores are calculated for all the nodes and summary generation is explained in this section .",
    "[ algo : summarygeneration ] * input * : embeddeddocument * output * : summary summary = null count = null select the highest scored node in embeddeddocument into the summary[11 ] recalculate the scores of the nodes using equation [ mmr1][12 ] select a node into summary that maximizes number of query terms in the summary[13 ] count++ [ 14 ] select the next highest scored node from embeddeddocument using equation [ mmr2][15 ] add the highest scored node to summary count++ calculate temporary node scores using equation [ mmr1 ] return summary    in algorithm [ algo : summarygeneration ] , it is assumed that @xmath32 ( number of sentences that user wants as a summary ) is not greater than the number of sentences in the embeddeddocument . from lines",
    "7 to 9 , the completeness of the summary is achieved .",
    "a summary is complete if all the query terms are present in it .",
    "then the nodes are added from the remaining pool as given in lines 12 to 15 . in line 7 ,",
    "equation [ mmr1 ] is used to recalculate the node scores and in line 12 the maximum scored node is selected using the equation [ mmr2 ] . note that here , scores are assigned to nodes temporarily using equation [ mmr1 ] and equation [ mmr2 ] is used to select the highest scored node into summary . after the selection , the node scores are reverted to their original scores(as calculated in section [ sec : nodescore ] ) .",
    "@xmath33    @xmath34    here @xmath5 and @xmath35 represents document and summary nodes respectively .",
    "@xmath36 is the temporary node score of @xmath5 .",
    "equation [ mmr1 ] is inspired from @xcite .",
    "the sentences in the summary generated using algorithm [ algo : summarygeneration ] are rearranged in the document order .",
    "this summary is complete , coherent and also non - redundant .",
    "the value of @xmath37 is taken from @xcite .",
    "@xmath38 is used as a scaling factor and it is fixed empirically .        while selecting sentences into summary , the sentences which will cover maximum uncovered query terms are given highest preference .",
    "the generation of summary is carried out by adding one sentence followed by another .",
    "fist sentence which is included into summary will be the highest scored sentence .",
    "the sentences selected after that are targeted towards maximizing the number of query terms coverage . if more than one sentence is contributing the same number of query terms then the highest scored sentence among them will be selected to be included into the summary .",
    "this process is repeated till all the query terms are included into the summary .",
    "the sentences selected into the summary are arranged in the embeddeddocument order .",
    "the insertion strategy discussed in section [ model ] ensures that the embeddeddocument is coherent i.e. , the sentences in the embeddeddocument are well connected and there is a logical flow within sentences",
    ". therefore updated summary is coherent .",
    "after achieving the task of complete summary , the nodes that are included are purely based on two criteria : first one is the node s importance w.r.t the query and second is its contribution to the summary .",
    "contribution is the amount of new information it is adding to the summary .",
    "in other words it is non - redundancy .",
    "so , equation [ mmr2 ] is used to select the sentences which ensures the non - redundancy and thus the quality of the summary .",
    "recall that before selecting the highest scored node , equation [ mmr1 ] is used to calculate the temporary node scores .",
    "in this system we embed the summary into new document in a coherent manner and then the summary is generated by extracting sentences from the embedded document .",
    "the complexity of the system is @xmath39 , @xmath40 and @xmath41 are number of sentences in current summary and new document respectively .",
    "the complexity of a multi - document summarizer is @xmath42 .",
    "evaluating the proposed system is a difficult task .",
    "update summary generation is evaluated on duc 2006 corpus .",
    "duc has 50 topic clusters and each topic is described in 25 documents .",
    "initial summary is generated using the mead @xcite system for the query and the document cluster provided by duc .",
    "this summary is generated on the first 15 of the 25 documents .",
    "the summary generated will be the input for the update summary generation task .",
    "the @xmath43 document will be the new document into which the summary is to be embedded .",
    "the summary is generated for the given query on the embedded document and this generated summary will be embedded into @xmath44 document .",
    "the process is repeated till the summary on the last embedded document(@xmath45 ) is generated .",
    "the block diagram for the experimental setup is shown in figure [ experimentalsetupfig ] .",
    "mead @xcite follows centroid based approach to generate summaries .",
    "it deals with both single and multi - document summarization . in our setup",
    "we use mead s multi - document summarization approach .",
    "mead computes a score for each sentence from the given cluster of related documents by considering a linear combination of several features .",
    "we have used centroid score , position and cosine similarity with query as features with 1,1,10 as their weights respectively .",
    "mmr(maximum marginal relevance ) re - ranker is used for redundancy removal with a similarity threshold of 0.6 .",
    "the updated summaries so formed are all stored and evaluated against the model summaries given by duc . in duc , the model summaries are of fixed length i.e. , 250 words .",
    "so , all the generated summaries are truncated to 250 words .",
    "this problem is first posed by us and therefore there is no other system available to be compared with the performance of our system . as this is an update summary generation task there is no meaningful baseline system that can be compared with our system .",
    "the following alternatives were thought of for a baseline system : 1 ) generate a baseline summary using mead with all the 25 documents as input .",
    "as our system generates the summary by considering only the current summary and new document , this is not a fair comparison .",
    "2 ) if baseline summary for @xmath46 document inclusion is available then baseline summary for @xmath47 document inclusion can be calculated using mmr approach .",
    "but the former approach requires the presence of all the @xmath48 documents to generate a baseline summary .",
    "so , it also will not be appropriate baseline .",
    "so , we give the rouge results generated by the best performing system of duc 2006(system-24 ) , these values are for summaries generated by considering all the 25 documents .",
    "but usum s rouge values are _ not _ obtained by considering all the 25 documents .",
    "so , the values of the best system of duc 2006 would naturally be better than our systems values .",
    "the ten updated summaries for each cluster are evaluated according to duc 2006 specifications .",
    "duc uses rouge measures to evaluate the quality of the summary generated by comparing with the model summaries .",
    "recall is calculated for the generated summaries w.r.t this model summaries .",
    "rouge@xcite stands for recall - oriented understudy for gisting evaluation .",
    "rouge measures the quality of a summary by comparing it to the summaries created by volunteers .",
    "n is n - gram recalls between system generated summaries and the summaries generated by the volunteers(models ) . rouge -",
    "n is calculated based on the equation [ rougeequation ] @xmath49 here @xmath1 is the length of n - gram .",
    "@xmath50 stands for n - gram .",
    "@xmath51 is the maximum number of n - grams co - occurring in both the generated summary and in the reference summaries .",
    "rouge-1 and rouge-2 are the recall measures of unigrams and bi - grams respectively .",
    "rouge - w is the weighted longest common subsequences matching . in longest common subsequence matching ,",
    "the distance between the words is not considered as an important issue but in weighted longest common subsequence matching , weight is given to the distance between the words .",
    "rouge - su4 is the recall measure which computes the skip bi - grams with skip distance four and uni - grams are also considered while computing this measure .    in table",
    "[ tab:4 ] the rouge values for updated summaries generated on duc 2006 are given .",
    "the values in the table are averaged values over 50 clusters .",
    "updated summary 1 is the summary obtained by updating the summary generated on first 15 documents with the sixteenth document .",
    "updated summary 2 is the summary obtained by updating the updated summary 1 with the seventeenth document .",
    "we empirically found that the rouge values are better for @xmath38 value of 20 .",
    "we also give the rouge values for the system-24(best performing system ) of duc 2006 in table [ bestsystem ] .",
    "the values in table [ bestsystem ] are for the summaries generated by considering all the 25 documents of the cluster .",
    "so , the rouge values of table [ bestsystem ] will be better than the rouge values of our systems .",
    "but the rouge values of our system are very close to the rouge values of the system-24 .",
    "this indicates that our system is performing well .",
    "the proposed system is implemented on the system with the following configuration : 256 mb main memory , 1.7 ghz intel pentium processor and the operating system is fc3 .",
    "the system is implemented in java .",
    "the time taken to compute the update summaries on 50 clusters is 56 minutes .",
    "so , it is slightly greater than 1 minute per cluster . on average",
    "it is less than 7 seconds per update summary(there are 10 update summaries per cluster ) .    [",
    "cols=\"^,^,^,^,^\",options=\"header \" , ]     [ bestsystem ]",
    "in this paper , the current summary is cleverly embedded into the new document in a meaningful and coherent way .",
    "a query specific summary is generated on the embedded document .",
    "the sentences which are extracted from the document form a _ complete _ summary .",
    "the algorithm proposed will not select sentences which have redundant information .",
    "all the sentences are arranged in the embedded document order to maintain the coherence and flow in the summary .",
    "the system is efficient and the quality of the update summary is satisfactory .",
    "the results are highly encouraging .",
    "usum gives efficient solution for update summary generation which is a challenging and useful task .",
    "wan , x. , yang , j. , xiao , j. : towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction . in : proceedings of the 45th annual meeting of the association of computational linguistics , prague , czech republic , acl ( 2007 ) 552559          varadarajan , r. , hristidis , v. : a system for query - specific document summarization .",
    "in : cikm 06 : proceedings of the 15th acm international conference on information and knowledge management , new york , ny , usa , acm press ( 2006 ) 622631      page , l. , brin , s. , motwani , r. , winograd , t. : the pagerank citation ranking : bringing order to the web . in : proceedings of the 7th international world wide web conference , brisbane , australia ( 1998 ) 161172          mihalcea , r. : graph - based ranking algorithms for sentence extraction , applied to text summarization . in : proceedings of the acl 2004 on interactive poster and demonstration sessions , morristown , nj , usa , association for computational lingu ( 2004 )  20      lin , c.y . ,",
    "och , f.j . : automatic evaluation of machine translation quality using longest common subsequence and skip - bigram statistics . in : acl 04 : proceedings of the 42nd annual meeting on association for computational linguistics , morristown , nj , usa , association for computational linguistics ( 2004 ) 605612"
  ],
  "abstract_text": [
    "<S> _ huge amount of information is present in the world wide web and a large amount is being added to it frequently . </S>",
    "<S> a query - specific summary of multiple documents is very helpful to the user in this context . </S>",
    "<S> currently , few systems have been proposed for query - specific , extractive multi - document summarization . </S>",
    "<S> if a summary is available for a set of documents on a given query and if a new document is added to the corpus , generating an updated summary from the scratch is time consuming and many a times it is not practical / possible . in this paper </S>",
    "<S> we propose a solution to this problem . </S>",
    "<S> this is especially useful in a scenario where the source documents are not accessible . </S>",
    "<S> we cleverly embed the sentences of the current summary into the new document and then perform query - specific summary generation on that document . </S>",
    "<S> our experimental results show that the performance of the proposed approach is good in terms of both quality and efficiency . _ </S>"
  ]
}