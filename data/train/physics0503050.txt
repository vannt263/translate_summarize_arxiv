{
  "article_text": [
    "least squares fitting is a well - known and powerful method for combining information from a set of related experimental measurements to estimate the underlying theoretical parameters ( see , for instance , reference  @xcite ) .",
    "we discuss a specific implementation of this method for use in high - energy physics experiments , where the free parameters , denoted by the vector @xmath2 , are extracted from event yields for signal processes . typically , these yields are subject to corrections for background , crossfeed , and efficiency .",
    "because the sizes of these corrections depend on the values of the free parameters , we make all yield adjustments directly in the fit . often , the uncertainties on these corrections are ignored during the fit and are propagated to the free parameters afterwards .",
    "however , if these uncertainties modify the relative weights of the measurements , then the above two - step procedure would bias both the fitted central values and the estimated uncertainties .",
    "therefore , we build the @xmath3 variable from a full description of the uncertainties , statistical and systematic , as well as their correlations , on both the yields and their corrections . thus , the input measurements  event yields , signal efficiencies , parameters quantifying the background processes , and background efficiencies  and their uncertainties are all treated in a uniform fashion . in the @xmath3 minimization",
    ", we account for the @xmath2 dependence of the yield corrections .",
    "below , we denote matrices by upper case bold letters and one - dimensional vectors by lower case bold letters .",
    "let @xmath4 represent a set of @xmath5 event yield measurements , each for a different signal process .",
    "each measurement may receive crossfeed contributions from other signal processes as well as backgrounds from non - signal sources .",
    "the background processes are described by @xmath6 , a vector of @xmath7 estimated production yields , which can be functions of experimentally measured quantities , such as branching fractions , cross sections , and luminosities . in principle , the free parameters @xmath2 can also appear in @xmath6 , although no additional degrees of freedom are introduced by @xmath6 .",
    "the rates at which these background processes contaminate the signal yields are given by the @xmath8 background efficiency matrix , @xmath9 .",
    "thus , the vector @xmath10 represents the background - subtracted yields .",
    "we use an @xmath11 signal efficiency matrix , @xmath12 , to describe simultaneously detection efficiencies ( diagonal elements ) and crossfeed probabilities ( off - diagonal elements ) .",
    "the elements @xmath13 are defined to be the probabilities that an event of signal process @xmath14 is reconstructed and counted in yield @xmath15 .",
    "the corrected yields , denoted by @xmath16 , are obtained by acting on @xmath17 with the inverse of @xmath12 : @xmath18 thus , @xmath16 encapsulates all the experimental measurements .",
    "the variance matrix of @xmath16 , denoted by @xmath19 , receives contributions , both statistical and systematic , from each element of @xmath4 , @xmath6 , @xmath12 , and @xmath9 .    in the least squares fit , we define @xmath20 , where @xmath21 is the vector of predicted yields , which are also functions of @xmath2 . because both @xmath21 and @xmath16 ( through @xmath6 ) depend on @xmath2 , minimizing this @xmath3 amounts to a nonlinear version of the total least squares method  @xcite .",
    "we solve this problem by extending the conventional least squares fit to include contributions from both @xmath21 and @xmath16 in @xmath22 .",
    "given a set of seed values , @xmath23 , the optimized estimate , @xmath24 , and its variance matrix , @xmath25 , are @xmath26\\\\ \\label{eq : fittederror } \\mathbf{v_m } & = & \\frac{1}{2}\\frac{\\partial^2\\chi^2}{\\partial\\mathbf{m}\\ ,      \\partial\\mathbf{m}^t } =      \\left(\\mathbf{d}\\mathbf{v}_{\\mathbf{c}}^{-1}\\mathbf{d}^t\\right)^{-1},\\end{aligned}\\ ] ] where the @xmath27 derivative matrix @xmath28 is defined to be @xmath29 in general , @xmath21 and @xmath16 are nonlinear functions of @xmath2 , so the linearized solution @xmath24 is approximate , and the above procedure is iterated until the @xmath3 converges . between iterations ,",
    "all the fit inputs that depend on @xmath2 are reevaluated with the updated values of @xmath24 .",
    "nonlinearities also occur when @xmath19 contains multiplicative or poisson uncertainties that depend on the measurement values . with the least squares method",
    ", these nonlinearities result in biased estimators unless these variable uncertainties are evaluated using the predicted yields @xmath21 instead of the measured @xmath16 .",
    "therefore , all three ingredients in the @xmath3  @xmath16 , @xmath21 , and @xmath19  are functions of @xmath2 .",
    "however , we do not include the derivatives @xmath30 in @xmath28 because doing so would generate biases in @xmath24 .    for a simple demonstration of the aforementioned biases , we consider two measured yields , @xmath31 and @xmath32 , which are both estimators of a true yield @xmath33 .",
    "we assume that the uncertainties on @xmath31 and @xmath32 are uncorrelated , multiplicative , and of the same fractional size , @xmath34 .",
    "we construct an improved estimator , @xmath35 , by minimizing @xmath36 with respect to @xmath37 .",
    "if , following the prescription given above , we neglect the @xmath38 terms in @xmath39 and assign ( iteratively ) the uncertainties @xmath40 , then @xmath31 and @xmath32 are equally weighted , and @xmath35 is an unbiased estimate of @xmath33 : @xmath41 on the other hand , including the @xmath38 terms in @xmath39 results in an upward bias : @xmath42 finally , if we assign uncertainties based on the measured yields , not the predicted yields , such that @xmath43 , @xmath44 , and @xmath45 , then the resulting estimate is biased low : @xmath46 thus , even though @xmath47 and @xmath48 are smaller than @xmath49 , the corresponding estimators possess undesired properties .",
    "the uncertainties on the @xmath5 elements of @xmath4 and the @xmath7 elements of @xmath6 are characterized by the @xmath11 matrix @xmath50 and the @xmath51 matrix @xmath52 , respectively .",
    "usually , the elements of @xmath12 and @xmath9 share many common correlated systematic uncertainties , so we construct a joint variance matrix from the submatrices @xmath53 , @xmath54 , and @xmath55 , where @xmath53 ( @xmath56 ) and @xmath54 ( @xmath57 ) are the variance matrices for the elements of @xmath12 and @xmath9 , respectively , and @xmath55 ( @xmath58 ) contains the correlations between @xmath12 and @xmath9 .",
    "below , we label each element of @xmath12 or @xmath9 by two indices ( @xmath13 or @xmath59 ) , and the two dimensions of @xmath12 or @xmath9 are mapped onto one dimension of @xmath53 or @xmath54 .",
    "we form @xmath19 by propagating the statistical and systematic uncertainties on @xmath4 , @xmath6 , @xmath12 , and @xmath9 to @xmath16 via @xmath60 where appropriate , we substitute @xmath21 for @xmath16 , as discussed in section  [ sec : formalism ] .",
    "the first term of equation  [ eq : errorpropagation1 ] is simply @xmath61 , and the second term is @xmath62 . for the third term",
    ", we evaluate the partial derivatives and find @xmath63 where @xmath64 and @xmath65 , with elements given in terms of the kronecker delta ( @xmath66 ) : @xmath67 .",
    "the matrices @xmath68 and @xmath69 have rows labeled by two indices , which refer to the elements of @xmath12 and @xmath9 , respectively , and columns labeled by one index , which refers to the elements of @xmath16 . in other words , the @xmath70-th row of @xmath68 is given by @xmath71 , where @xmath72 .",
    "therefore , the elements of @xmath68 and @xmath69 are @xmath73 and @xmath74 . for @xmath75 ,",
    "these matrices are @xmath76 this treatment of error propagation in matrix inversion agrees with that derived in reference  @xcite .",
    "the above relations allow us to reexpress @xmath19 as @xmath77 where @xmath78 . as a result",
    ", we have @xmath79 , where @xmath80 .",
    "thus , the @xmath3 minimization can be formulated equivalently in terms of @xmath4 instead of @xmath16 : @xmath81 and @xmath82 , where @xmath83 .",
    "systematic uncertainties on the efficiencies are often multiplicative and belong to one of three categories : those that depend only on the reconstructed mode ( row - wise ) , those that depend only on the generated mode ( column - wise ) , and those that are uncorrelated among elements of @xmath12 and @xmath9 . for row - wise efficiency uncertainties , all the elements in",
    "any given row of @xmath12 and @xmath9 have the same fractional uncertainty , which we denote by @xmath84 .",
    "the correlation coefficients between elements of different rows are @xmath85 , where @xmath86 characterizes the uncertainties common to @xmath87 and @xmath88 . for instance , if @xmath89 is the fractional uncertainty associated with the charged particle tracking efficiency , then @xmath90 and @xmath91 , where @xmath92 and @xmath93 are the track multiplicities in modes @xmath15 and @xmath14 , respectively .",
    "note that @xmath94 .",
    "similarly , for column - wise uncertainties , we define the fractional uncertainties @xmath95 and correlation coefficients @xmath96 .",
    "we denote the uncorrelated fractional uncertainty on any element of @xmath12 or @xmath9 by @xmath97 .",
    "table  [ tab : vefelements ] gives expressions for the elements of @xmath53 , @xmath54 , and @xmath55 , as well as their contributions to @xmath19 for row - wise , column - wise , and uncorrelated uncertainties .",
    ".expressions for the elements of @xmath53 , @xmath54 , and @xmath55 , as well as their contributions to @xmath19 .",
    "repeated external indices are not summed over . [ cols=\"^,^,^,^\",options=\"header \" , ]     slight asymmetries can be observed in the pull distributions , especially in those for @xmath98 and @xmath99 .",
    "these asymmetries are caused by the nonlinear nature of the multiplicative efficiency uncertainties and of the functions @xmath100 . because the fit parameters are effectively estimated from ratios of the input yields , gaussian fluctuations in the denominators produce non - gaussian fluctuations in the ratios , which are most visible in @xmath98 and @xmath99 , where the uncertainties in the denominators are dominant .",
    "similarly , multiplicative uncertainties , which affect only the branching fractions , scale with the fitted values and , therefore , give rise to asymmetric @xmath101 pulls . in both cases",
    ", larger fractional uncertainties would heighten the asymmetries .",
    "if we form the matrix @xmath68 in equation  [ eq : adefinition ] using the measured yields @xmath16 rather than the predicted yields @xmath21 , then the variance matrix @xmath19 need not be reevaluated after each fit iteration . however , in this case , the pull distributions become significantly biased , as shown in figure  [ fig : toymcpullsbiased ] .",
    "thus , obtaining unbiased fit results and the correct uncertainties requires proper handling of the efficiency variance matrices @xmath53 and @xmath54 .",
    "calculated using @xmath16 instead of @xmath21 , for @xmath98 ( a ) , @xmath102 ( b ) , @xmath103 ( c ) , @xmath104 ( d ) , @xmath99 ( e ) , @xmath105 ( f ) , and @xmath106 ( g ) , overlaid with gaussian curves with zero mean and unit width .",
    "the fit confidence level distribution ( h ) is overlaid with a line with zero slope . ]",
    "we have developed a least squares fit that simultaneously incorporates statistical and systematic uncertainties , as well as their correlations , on all the input experimental measurements .",
    "biases from nonlinearities are reduced by introducing fit parameter dependence in the input variance matrix .",
    "this fitting method is used to measure absolute branching fractions of hadronic @xmath0 meson decays , and toy monte carlo studies validate the performance of the fitter . by including all known sources of measurement uncertainty in the @xmath3 , we obtain unbiased fit parameters with correct estimated uncertainties .",
    "we wish to thank roy briere , david cassel , lawrence gibbons , wolfgang rolke , anders ryd , and ian shipsey for many helpful discussions .",
    "this work was supported in part by the national science foundation under grant no ."
  ],
  "abstract_text": [
    "<S> we present a least squares method for estimating parameters from measurements of event yields in the presence of background and crossfeed . </S>",
    "<S> we adopt a unified approach to incorporating the statistical and systematic uncertainties on the experimental measurements input to the fit . </S>",
    "<S> we demonstrate this method with a fit for absolute hadronic @xmath0 meson branching fractions , measured in @xmath1 transitions . </S>"
  ]
}