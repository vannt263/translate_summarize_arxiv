{
  "article_text": [
    "let @xmath1 be a positive integer and denote the ground set by @xmath4 throughout this paper .",
    "suppose @xmath5 is an @xmath6 positive definite matrix .",
    "for any subset @xmath7 , let @xmath8 be the sub - matrix of @xmath5 obtained by removing those rows and columns of @xmath5 indexed by @xmath9 and its determinant be denoted by @xmath10 .",
    "note that when @xmath11 is the empty set , we will simply define @xmath8 as the scalar of value 1 .",
    "there are many determinant inequalities in the existing literature that involve only the principle minors of the matrix .",
    "these include    1 .",
    "hadamard inequality @xmath12 + for any @xmath13 .    as pointed out in @xcite and to be illustrated in section [ sec : framework ] , many of such determinant inequalities ( including the above two inequalities ) can be proved via an information - theoretic approach . despite",
    "that many determinant inequalities can be found in this approach , a complete characterisation of all determinant inequalities is still missing . in this paper",
    ", we aim to understand determinant inequalities by using the information inequality framework proposed in @xcite .",
    "the framework proposed in @xcite provides a geometric approach to understanding information inequalities .",
    "its idea will be illustrated shortly .",
    "a _ rank function _ over the ground set @xmath14 is a real - valued function defined on all subsets of @xmath14 .",
    "the _ rank function space _ over the ground set @xmath15 , denoted by @xmath16 , is the set of all rank functions over @xmath14 .    as usual",
    ", @xmath16 will be treated as a @xmath17-dimensional euclidean space , so that concepts such as metric and limits can be defined accordingly .",
    "let @xmath18 be a rank function over @xmath14 .",
    "then @xmath18 is called _ entropic _ if there exists a set of discrete random variables @xmath19 such that @xmath20 is the shannon entropy @xmath21 , or @xmath22 for short , for all @xmath23 .    on the other hand ,",
    "if @xmath19 is a set of continuous scalar random variables such that @xmath20 is the differential entropy @xmath24 for all @xmath23 , then @xmath18 is called _ s - entropic_.    consider any nonempty finite ground set @xmath25 . define the following `` entropy regions '' : @xmath26    understanding the above entropic regions is one of the most fundamental problems in information theory .",
    "it is equivalent to determining the set of all information inequalities  @xcite .    in this paper",
    ", we will use the following notation . for any subset @xmath27",
    ", @xmath28 is defined as the set of all rank functions @xmath29 such that @xmath30 for some @xmath31 and @xmath32 .",
    "the closure of @xmath28 will be denoted by @xmath33 .",
    "finally , the smallest closed and convex cone containing @xmath34 will be denoted by @xmath35 . clearly , @xmath36    [ thm : yeungframework ] a linear information inequality @xmath37 is valid for all discrete random variables @xmath38 if and only if for all @xmath39 @xmath40    by theorem [ thm : yeungframework ] , characterising the set of all valid information inequalities is thus equivalent to characterising the set @xmath41 .",
    "similar results can be obtained for the set @xmath42 . in the following",
    ", we will extend this geometric framework to study determinant inequalities .",
    "a rank function @xmath18 over @xmath14 is called _ log - determinant _ if there exists an @xmath6 positive definite matrix @xmath5 such that @xmath43 for all @xmath44 .",
    "let @xmath45 be the set of all log - determinant functions over @xmath14 .",
    "then , we have the following theorem .    [ thm:2 ]",
    "let @xmath46 be any real numbers .",
    "the determinant inequality @xmath47 holds for all positive definite matrix @xmath5 if and only if @xmath48 for all @xmath49 .    by taking logarithm on both sides of the inequality ,",
    "is equivalent to that @xmath50 for all positive definite matrix @xmath5 . as is a linear inequality , it is satisfied by all @xmath51 if and only if it is satisfied by all @xmath49 . the theorem then follows .    in other words ,",
    "the characterisation of the set of all determinant inequalities is equivalent to determining the set @xmath52 . in the rest of the paper",
    ", we will obtain inner and outer bounds on @xmath53 .    to achieve our goal",
    ", we will take an information theoretic approach  @xcite .",
    "the idea is very simple : let @xmath54 be a set of scalar gaussian random variables whose covariance matrix is equal to @xmath55 .",
    "then the differential entropy of @xmath56 is given by @xmath57    [ df : gaussian ] a function @xmath58 is called _ s - gaussian _ if there exists scalar gaussian variables @xmath59 where @xmath60 for all @xmath61 .    from , a rank function @xmath18 is log - determinant if and only if @xmath62 is -gaussian .",
    "let @xmath63 be the set of all -gaussian functions . then @xmath64 consequently , we have the following theorem .",
    "the determinant inequality @xmath65 holds for all positive definite matrix @xmath5 if and only if @xmath66 for all scalar gaussian variables @xmath59 .",
    "in fact , the hadamard inequality and szasz inequality are respectively the counterparts of the following basic information inequalities  @xcite @xmath67    in the following sections , we will obtain inner and outer bounds on the set @xmath68 .",
    "the following corollaries of theorem [ thm:2 ] show how these bounds can be used for proving or disproving a determinant inequality .",
    "suppose @xmath69 contains @xmath70 as a subset .",
    "the determinant inequality holds for all positive definite matrix @xmath5 if @xmath71 for all @xmath72 .",
    "therefore , any explicit outer bound on @xmath70 can lead to the discovery of new determinant inequalities . on the other hand ,",
    "an inner bound on @xmath70 can be used for disproving a determinant inequality .",
    "suppose @xmath73 .",
    "the determinant inequality does not hold for all positive definite matrices if there exists @xmath74 such that @xmath75",
    "as discussed earlier , log - determinant functions are essentially the same as -gaussian functions .",
    "our objective is thus to characterise @xmath68 , or at least to understand its basic properties . since scalar gaussian random variables are continuous scalar random variables , the next lemma follows immediately from the definition .",
    "[ lemma : outerbd ] @xmath76 and consequently , @xmath77    it is well known that @xmath78 ( i.e. , the closure of @xmath79 ) is a closed and convex cone  @xcite .",
    "it was established in @xcite that @xmath80 where @xmath81    in the following , we prove an inner bound on @xmath82 by using representable functions .    a rank function @xmath18 over @xmath25 is called _ s - representable _ if there exists real - valued vectors ( of the same length ) @xmath83 such that for all @xmath84 , @xmath85 in other words , @xmath86 is the maximum number of independent vectors in the set @xmath87 .",
    "[ thm : repisgaussian ] if @xmath18 is -representable , then @xmath88    suppose the length of each row vector @xmath89 is @xmath90 .",
    "let @xmath91 be a set of independent standard gaussian random variables .",
    "therefore , its covariance matrix is the @xmath92 identity matrix .",
    "let @xmath31 .",
    "for each @xmath93 , define a real - valued continuous random variable as follows @xmath94^{\\top } + v_{i}.\\ ] ] let @xmath95^{\\top}$ ] .",
    "then @xmath96^{\\top } + { \\bf v}\\ ] ] where @xmath97 is an @xmath98 matrix whose @xmath99 row is @xmath89 and @xmath100^{\\top}.\\ ] ]    since @xmath101 is zero - mean , @xmath102   \\\\ & = \\frac{1}{c } e\\left [ a   [ w_{1 } , \\ldots , w_{k } ] ^{\\top }   [ w_{1 } , \\ldots , w_{k } ] a^{\\top } \\right ] +   { \\bf i } \\\\ & = \\frac{1}{c }   a a^{\\top }   +   { \\bf i } .\\end{aligned}\\ ] ] consequently , @xmath103 where @xmath104 is the diagonal matrix obtained by using singular - value decomposition ( svd ) over @xmath105 .",
    "let @xmath106 be the diagonal entries of @xmath104 and @xmath107 be the rank of the matrix @xmath105 ( or equivalently , the rank of @xmath97 ) .",
    "hence , @xmath108 if and only if @xmath109",
    ". then @xmath110 it is easy to see that @xmath111    similarly , for any @xmath112 , we can prove that @xmath113 thus , @xmath114 and the theorem is proved .",
    "[ lemma:5 ] let @xmath115 be a set of scalar jointly continuous random variables with differential entropy function @xmath18 . for any @xmath116 , define the set of random variables @xmath117 by @xmath118 and let @xmath29 be the differential entropy function of @xmath117 .",
    "then @xmath119 for all @xmath61 .",
    "consequently , if @xmath18 is -gaussian , then so is @xmath29 .",
    "let @xmath120 and @xmath121 be respectively the probability density functions ( pdfs ) of @xmath115 and @xmath117",
    ". then @xmath122 and can be directly verified .",
    "[ cor:3 ] @xmath123 where @xmath124 is the set of all -representable functions .    a direct consequence of lemmas [ lemma : outerbd ] and [ lemma:5 ] , theorem [ thm : repisgaussian ] and .    [ prop : nis3 ] for @xmath125 , @xmath126    by corollary [ cor:3 ] , to prove the proposition",
    ", it suffices to prove that for @xmath2 , @xmath127 in @xcite , the cone @xmath128 ( when @xmath129 ) was explicitly determined by identifying the set of extreme vectors of the cone .",
    "it can be proved that all the extreme vectors are -representable . ] and hence is a subset of @xmath130 .",
    "consequently , holds and the proposition follows .",
    "proposition [ prop : nis3 ] does not hold when @xmath131 . in fact , @xmath132 is in general a proper subset of @xmath68 when @xmath131 . in @xcite",
    ", it was proved that all -representable functions satisfy the ingleton inequalities .",
    "it can also be directly verified that all the functions @xmath133 also satisfy the ingleton inequalities .",
    "therefore , all the functions in @xmath132 also satisfy the ingleton inequalities .",
    "however , in @xcite , it was proved that there exists @xmath134 for @xmath135 that violates the the ingleton inequality .",
    "thus , @xmath132 is indeed a proper subset of @xmath68 .",
    "by definition , the set @xmath136 ( which is the focus of our interest ) is close under addition .",
    "however , this is not necessarily true for @xmath137 .",
    "in fact , @xmath138 is not necessarily equal to @xmath52 .    in the previous section",
    ", we showed that the set @xmath137 is essentially equivalent to the set of -gaussian functions , defined via sets of scalar gaussian random variables .",
    "it turns out that , if we relax the constraint by allowing the gaussian random variables to be vectors , instead of scalars , we will obtain an outer bound for @xmath137 and also @xmath52 .",
    "a function @xmath58 is called _ v - gaussian _ if there exists @xmath1 gaussian random vectors @xmath59 such that @xmath60 for all @xmath61 .",
    "[ lemma:3 ] @xmath139 .",
    "it is clear from the definition that @xmath140 .",
    "now , consider positive integers @xmath141 and @xmath142 .",
    "it is easy to see that @xmath143 hence , @xmath144 since @xmath141 are arbitrary positive integers , for any positive numbers @xmath145 , @xmath146 and the lemma follows .",
    "@xmath147    a direct consequence of that @xmath148 and lemma [ lemma:3 ] .",
    "so far , we have established two outer bounds and for @xmath68 . in the following",
    ", we will prove that is in fact a tighter one .",
    "a rank function @xmath18 is called _ v - entropic _ if there exists a set of random vectors @xmath149 , not necessarily of the same length , such that @xmath150 also , let @xmath151    clearly , @xmath152 .",
    "thus , @xmath153 to show that is tighter , it suffices to prove the following result .",
    "[ thm : main ] @xmath154 .",
    "theorem  [ thm : main ] basically states that replacing the real - valued random variables @xmath155 in the vector @xmath156 by random vectors does not enlarge the closure of the space of differential entropy vectors .",
    "the discrete counterpart of this result is trivial , because as far as the probability masses and the entropy are concerned , a discrete random vector can be replaced by a scalar discrete random variable .",
    "however , in the continuous domain , it is not clear how a probability density function on @xmath157 or more generally @xmath158 can be mapped to a pdf on @xmath159 without changing the entropies .",
    "in particular , there does not exist a continuous mapping from @xmath157 to @xmath159  @xcite .",
    "the proof of theorem  [ thm : main ] exploits the relationship between the differential entropy of a continuous vector and the entropy of a discrete vector obtained through quantisation .",
    "moreover , the entropy of the discrete random variable is equal to the differential entropy of a continuous random variable with piece - wise constant pdf . given the @xmath1-tuple @xmath160 whose entries are vectors , we `` quantise '' @xmath160 by a discrete vector and then construct a continuous vector with @xmath1 scalar entries whose entropy vector arbitrarily approximates that of @xmath160 .",
    "before we prove the theorem , we need several intermediate supporting results .",
    "[ lemma : addition ] if @xmath161 and @xmath162 are v - entropic ( or entropic ) functions over @xmath25 , then their sum @xmath163 is also v - entropic ( or entropic ) .    direct verification .",
    "[ prop : claim ] if @xmath164 , then for any @xmath31 , @xmath165 .",
    "let @xmath166 be a real - valued random vector with a probability density function .",
    "for any positive integer @xmath167 , let @xmath168 be @xmath167 independent replicas of @xmath169 ( by a replica we mean a random object with identical distribution ) .",
    "similarly , let @xmath170 be a real - valued random vector such that @xmath171 are mutually independent and each of them is uniformly distributed on the interval @xmath172 $ ] .",
    "again , for any positive integer @xmath167 , let @xmath173 be @xmath167 independent replicas of @xmath174 .",
    "it is easy to see that the joint density function of @xmath173 is uniform on a hypercube with unit volume and hence has zero differential entropy .",
    "consider any @xmath31 .",
    "let @xmath175 be a binary random variable such that @xmath176 where @xmath167 is a positive integer .",
    "assume that @xmath175 is independent of @xmath177 let @xmath178 where each @xmath179 is a random vector of length @xmath167 such that for any @xmath180 , @xmath181 @xmath160 is evidently continuous with a pdf , which is a mixture of two pdfs induced by that of @xmath169 and @xmath174 . for any @xmath182 , we can directly verify that @xmath183 and @xmath184 consequently , @xmath185    hence , @xmath186 where @xmath187 is the entropy of a binary random variable with probabilities @xmath188 and @xmath189 .",
    "thus , @xmath190 . let @xmath191 and @xmath29 be respectively the entropy function induced by @xmath192 and @xmath193",
    ". then @xmath191 is @xmath194-entropic by definition and @xmath195 hence , @xmath165 for all @xmath31 and our proposition follows .",
    "[ prop : cone ] @xmath196 is a closed and convex cone .    for any @xmath197 , by definition , there exists a sequence of @xmath194-entropic functions @xmath198 such that @xmath199 thus , for any @xmath31 , @xmath200 then , by proposition [ prop : claim ] , @xmath201 and consequently , @xmath202 .",
    "consider any @xmath203 , and @xmath145 .",
    "since @xmath204 there exists sequences of @xmath194-entropic functions @xmath205 and @xmath206 such that @xmath207 by lemma [ lemma : addition ] , @xmath208 is also @xmath194-entropic .",
    "thus , @xmath209 the proposition is proved .",
    "given @xmath210 , let the @xmath211-quantization of any real number @xmath188 be denoted as : @xmath212_m = \\frac { \\lfloor m x \\rfloor } m\\end{aligned}\\ ] ] where @xmath213 denotes the largest integer not exceeding @xmath214 .",
    "similarly , let the @xmath211-quantization of a real vector @xmath215 be the element - wise @xmath211-quantization of the vector , denoted by @xmath216_m$ ] , i.e. , @xmath217_m = ( [ x_1]_m , \\dots , [ x_n]_m ) \\ .\\end{aligned}\\ ] ]    evidently , @xmath218_m$ ] can only take values from the set @xmath219 hence for every real - valued random variable @xmath220 , @xmath221_m $ ] is a discrete random variable taking value in the set .",
    "by definition , @xmath222_m = \\frac{i}m } = 1.\\end{aligned}\\ ] ]    [ pr : renyi ] if @xmath220 is a real - valued random vector of dimension @xmath1 with a probability density function , then @xmath223_m ) - n \\log m = h(x ) \\",
    ".\\end{aligned}\\ ] ]    under the assumption that the pdf of a random variable @xmath220 is riemann - integrable , proposition  [ pr : renyi ] is established in  @xcite by treating @xmath224_m ) - n \\log m$ ] as the approximation of the riemann integration of @xmath225 .",
    "it is nontrivial to establish the result in general , where the pdf is not necessarily rieman - integrable .",
    "an example of such a pdf can be defined by using the smith - volterra - cantor set . nonetheless   can be shown to hold using the lebesgue convergence theorem along with some truncation arguments  @xcite .",
    "[ lemma : diss ] let @xmath226 be a set of discrete random variables such that its entropy function is @xmath18 .",
    "for any positive numbers @xmath227 , let @xmath29 be defined as @xmath228 then @xmath29 is -entropic .",
    "as @xmath101 is discrete , we may assume without loss of generality that the sample space of @xmath101 is the set of integers @xmath229 .",
    "let @xmath230 be the probability mass function of @xmath226 .",
    "construct a set of continuous scalar random variables @xmath231 whose probability density function is defined as follows : @xmath232 it can then be directly verified that @xmath233 consequently , @xmath29 is -entropic .",
    "clearly , @xmath234 .",
    "we will now prove that @xmath235 .",
    "let @xmath236 consist of @xmath1 random vectors , where @xmath237 let us define the @xmath211-quantization of @xmath179 , denoted as @xmath238_m$ ] , be the element - wise @xmath211-quantization of @xmath179 , i.e. , it consists of @xmath239_m$ ] for @xmath240 . by proposition  [ pr : renyi ] ,",
    "@xmath241_{m } , i\\in\\alpha ) -   \\left(\\sum_{i \\in \\alpha } k_i \\right)\\log m \\right ] =   h(z_\\alpha ) .",
    "\\end{aligned}\\ ] ]    let @xmath242 be such that @xmath243_{m } , i\\in\\alpha ) \\\\ g^{m}(\\alpha ) & = r^{m}(\\alpha ) - \\left(\\sum_{i \\in \\alpha } k_i \\right)\\log m   .\\end{aligned}\\ ] ] by , @xmath244 . also , since @xmath245 , @xmath246 by lemma [ lemma : diss ]",
    "consequently , @xmath247 .",
    "we have thus proved that @xmath248 and as a result , @xmath249 . finally , by proposition [ prop : cone ]",
    ", @xmath250 is a closed and convex cone and is equal to @xmath251 . then by , @xmath252 the theorem is proved .",
    "in theorem [ thm : repisgaussian ] , we have constructed an inner bound for @xmath253 by using -representable functions .",
    "the same trick can also be used for constructing an inner bound for the set @xmath254 .",
    "a rank function @xmath18 over @xmath25 is called _ v - representable _ if for @xmath255 , there exists a set of real - valued vectors ( of the same length ) @xmath256 such that for all @xmath84 , @xmath257    the following theorem is a counterpart of theorem [ thm : repisgaussian ] .",
    "the proving technique is the same as before .",
    "we will omit the proof for brevity .",
    "[ thm : vrepisgaussian ] suppose that @xmath18 is v - representable , then @xmath258 .",
    "theorem [ thm : vrepisgaussian ] is of great interest . characterising",
    "the set of v - representable functions have been a very important problem in linear algebra and information theory .",
    "it is also extremely difficult . for many years",
    ", it is only known that v - representable functions are polymatroidal and satisfies the ingleton inequalities  @xcite .",
    "the set of representable functions is only known when @xmath259 .",
    "however , there were some recent breakthrough in this areas . in @xcite ,",
    "many new subspace rank inequalities which are required to be satisfied by representable functions are discovered . in particular , via a computer - assisted mechanical approach , the set of all representable functions when @xmath260 has been completely characterised .",
    "interesting properties about the set of v - representable functions were also obtained @xcite .",
    "theorems [ thm : repisgaussian ] and [ thm : vrepisgaussian ] thus opens a new door to exploit results obtained about representable functions to characterise the set of gaussian functions .",
    "@xmath261 where @xmath262 is the set of all v - representable functions .",
    "while @xmath263 it is still an open question whether @xmath264 or not .",
    "we will end this section with a discussion of a related concept in a recent work  @xcite .",
    "gaussian rank functions were studied in @xcite .",
    "however , their definitions are slightly different from ours .",
    "[ df : normalised ] let @xmath265 be a set of @xmath1 jointly distributed vector valued gaussian random variables such that each vector @xmath101 is a vector of length @xmath175 .",
    "its _ normalised gaussian entropy function _",
    "@xmath18 is a function in @xmath266 such that @xmath267    the only difference between definitions [ df : gaussian ] and [ df : normalised ] is the scalar multiplier @xmath268 .",
    "hence , a normalised gaussian entropy function must be contained in the set @xmath269 . in one sense , our proposed definition is slightly more general as we do not require all the random vectors @xmath101 to have the same length .",
    "on the other hand , the `` normalising factor '' @xmath268 in definition [ df : normalised ] can lead to some interesting results .",
    "for example , while we can not prove that the closure of @xmath270 is closed and convex , @xcite proved that the closure of the set of all normalised gaussian entropy functions is indeed closed and convex .",
    "let @xmath271 is a mnemonic for the word `` normalised '' .",
    "] be the set of all normalised gaussian entropy functions .",
    "then @xmath272    it can be directly verified from definitions that @xmath273 .",
    "now , consider any @xmath274 .",
    "then by definition , there exists @xmath1 gaussian random vectors @xmath59 such that @xmath60 for all @xmath61 .",
    "let @xmath275 be the length of the random vector @xmath101 .",
    "assume without loss of generality that @xmath276 for all @xmath277 .",
    "let @xmath278 and @xmath279 be a set of scalar gaussian random variables with identity covariance matrix and independent of @xmath59 .",
    "for each @xmath280 , let @xmath281 and @xmath282 clearly , each @xmath179 is a gaussian vector with the same length @xmath283 .",
    "let @xmath29 be the normalised entropy function induced by @xmath284 .",
    "it is easy to verify that @xmath285 .",
    "consequently , @xmath286 and the proposition thus follows .",
    "our proposition [ prop : nis3 ] can also be derived from ( * ? ? ?",
    "* theorem 5 ) , which proved that for any @xmath274 when @xmath287 , there exists a @xmath288 such that for all @xmath289 , @xmath290 is vector gaussian .",
    "however , their proof techniques are completely different .",
    "in this paper , we took an information theoretic approach to study determinant inequalities for positive definite matrices .",
    "we showed that characterising all such inequalities for an @xmath6 positive definite matrix is equivalent to characterising the set of all scalar gaussian entropy functions for @xmath1 random variables .",
    "while a complete and explicit characterisation of the set is still missing , we obtained inner and outer bounds respectively by means of linearly representable functions and vector gaussian entropy functions .",
    "it turns out that for @xmath2 , the set of all scalar gaussian entropy functions is the same as the set of all differential entropy functions .",
    "the latter set is completely characterized by shannon - type information inequalities .",
    "consequently , the aforementioned inner and outer bounds agree with each other . for @xmath131 , we showed the contrary , and the problem is seeming very difficult ."
  ],
  "abstract_text": [
    "<S> in this paper , we show that the characterisation of all determinant inequalities for @xmath0 positive definite matrices is equivalent to determining the smallest closed and convex cone containing all entropy functions induced by @xmath1 scalar gaussian random variables . </S>",
    "<S> we have obtained inner and outer bounds on the cone by using representable functions and entropic functions . in particular , these bounds are tight and explicit for @xmath2 , implying that determinant inequalities for @xmath3 positive definite matrices are completely characterized by shannon - type information inequalities .    shell : bare demo of ieeetran.cls for journals    _ v _    entropy , gaussian distribution , rank functions </S>"
  ]
}