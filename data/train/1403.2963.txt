{
  "article_text": [
    "consider the linear regression model @xmath1 where @xmath2 is the vector of @xmath3 response variables , @xmath4 is the @xmath5 design matrix with the @xmath6th column @xmath7 , @xmath8 is the vector of regression coefficients and @xmath9 is the vector of random errors .",
    "we assume that the responses and covariates are centered so that the intercept term is zero .",
    "we are interested in estimating the vector of regression coefficients @xmath10 .",
    "penalized regression methods accomplish this by minimizing an objective function @xmath11 that is composed of the sum of squared residuals plus a penalty .",
    "the penalized least squares estimator is defined as the minimizer of @xmath12 where @xmath13 is a penalty function indexed by a regularization parameter @xmath14 that controls the balance between the fit of the model and the penalty , and the penalty function may depend on one or more tuning parameters @xmath15 .    here",
    "we focus on optimization algorithms for penalized regression methods .",
    "there has been much work on developing efficient algorithms for many problems with various penalties , including @xcite , @xcite , and @xcite for the least absolute selection operator ( lasso ) , and @xcite , @xcite , and @xcite for nonconvex penalties such as smoothly clipped absolute deviation ( scad ) and the minimax concave penalty ( mcp ) . recently ,",
    "several authors have investigated rules for discarding variables during certain steps of the above algorithms , thereby saving computational time through dimension reduction .    for the lasso ,",
    "@xcite proposed the basic safe rule discards the @xmath6th variable if @xmath16 where @xmath17 is the smallest tuning parameter value for which all estimated coefficients are zero .",
    "they proved that the estimated coefficient for any variable satisfying the basic safe rule ( [ safe : rule ] ) must be zero in the solution at @xmath14 .",
    "@xcite proposed the basic strong rule by modifying the basic safe rule ( [ safe : rule ] ) . for a standardized design matrix ( @xmath18 for all @xmath6 )",
    ", we have @xmath19 by the cauchy - schwarz inequality and therefore @xmath20 is an upper bound of the quantity on the right hand side of .",
    "the strong rule therefore discards the @xmath6th variable if @xmath21 being an upper bound of the safe rule , the strong rule ( [ strong : rule ] ) discards more variables than the safe rule . unlike the safe rule",
    ", however , it is possible for the strong rule to be violated . because strong rules can mistakenly discard active variables ( i.e. , variables whose solution is nonzero for that value of @xmath14 ) , @xcite proposed checking the discarded variables against the karush - kuhn - tucker ( kkt ) conditions to correct for any violations that may have occurred during the optimization",
    "these basic rules are most useful at large values of @xmath14 and rarely eliminate variables at smaller @xmath14 values .",
    "this is unfortunate from an algorithmic perspective , since the majority of time required to fit a regularization path is spent during optimization for the small @xmath14 values . to overcome this drawback , @xcite proposed sequential strong rules . for a decreasing sequence of tuning parameter @xmath22 ,",
    "the sequential strong rule discards the @xmath6th variable from the optimization problem at @xmath23 if @xmath24 where @xmath25 is the vector of residuals at @xmath26 . unlike the basic rules , the sequential strong rule discards a large proportion of inactive variables at all values of @xmath14 .",
    "in addition , the rule is rarely violated , and is therefore unlikely to discard active variables by mistake .    in this paper , we investigate sequential strong rules for discarding variables in penalized regression with nonconvex penalties , as well as strategies for incorporating these rules into coordinate descent algorithms for fitting these models . in addition , we derive rules for discarding variables in various related problems with nonconvex penalties , such generalized linear models , @xmath0-stabilized penalties ( the `` mnet '' estimator ) , and grouped penalties .",
    "we provide a publicly available implementation of these algorithms in the updated ` ncvreg ` package ( available at http://cran.r-project.org ) , which was used to fit all the models in this paper .",
    "the basic idea of the sequential strong rule @xcite is that the solution path @xmath27 is a continuous function ; furthermore , one can obtain an approximate bound on how fast the solution path can change as a function of lambda .",
    "thus , when solving for @xmath28 at @xmath26 and then again at @xmath29 , we can exclude certain variables from the optimization procedure because they are nt close enough to the threshold for inclusion in the model to reach that threshold in the distance between @xmath26 and @xmath29 .",
    "the effect is that the dimension of the optimization problem is reduced  instead of cycling over all variables ,",
    "the estimation procedure needs only to cycle over a much smaller set of variables capable of entering the model at @xmath29 .",
    "the bound investigated by @xcite is given by @xmath30 where @xmath31 is the correlation is not a correlation , since @xmath32 is not standardized , and thus only proportional to the correlation between @xmath33 and @xmath32 .",
    "however , the term is widely used ; see , e.g. , @xcite . ] between variable @xmath6 and the residual at @xmath34 .",
    "this condition is equivalent to @xmath35 being continuous everywhere , differentiable almost everywhere , and satisfying @xmath36 wherever this derivative exists .",
    "tibshirani et al .",
    "called condition ( [ unit : slope ] ) the unit slope bound . if condition ( [ unit : slope ] ) holds , then for any variable @xmath6 satisfying the sequential strong rule ( [ seq : strong : rule ] ) , we have @xmath37 , and thus , @xmath38 by the kkt conditions of the lasso .    in this section",
    ", we examine whether a variation of condition holds for the mcp and scad penalties , and use a modified version of to develop strong rules for those nonconvex penalties . also , we provide numerical examples to illustrate the application of the strong rules on a simulated data set .",
    "lastly , we note that unlike the lasso case , for nonconvex penalties the function @xmath39 is not guaranteed to be continuous ; we explore the consequences of this fact in section  [ sec : local - convexity ] .",
    "@xcite proposed the mcp which is defined as @xmath40 for @xmath41 and @xmath42 .",
    "we begin by noting the kkt conditions for the penalized problem ( [ obj : non ] ) , @xmath43 where @xmath44 is the active set .",
    "variables in @xmath45 are continuously changing as a function of @xmath34 , but many variables in @xmath46 remain zero from one @xmath34 value to the next .",
    "our aim , then , is to develop a screening rule to can discard the variables in the inactive set @xmath46 that are likely to remain zero . in high dimensions ,",
    "doing so should yield substantial computational savings . from the kkt conditions ( [ kkt : con ] ) , we have the form of @xmath35 , @xmath47 where @xmath48 and @xmath49 denote the subvector and submatrix of @xmath10 and @xmath50 , respectively , and @xmath51 stands for constant terms not depending on @xmath14 . unlike the lasso",
    ", the above expression for @xmath35 does not permit a closed - form expression for @xmath52 for variables in the active set .",
    "hence , we investigate an approximation for @xmath52 based on an orthogonal design matrix . in this case , the coefficient estimates have closed form solution @xmath53 , where @xmath54 is the ordinary least squares estimator , and the second term of ( [ grd : mcp ] ) is @xmath55 .",
    "this suggests the bound @xmath56 . this slope bound is larger than the corresponding bound for the lasso , as the nonconvexity of mcp allows the solution path  and",
    "thus , @xmath39  to change more rapidly as a function of @xmath34 than it does for lasso .",
    "note that in the limiting case @xmath57 , mcp is equal to the lasso penalty , and the bounds coincide .",
    "conversely , as @xmath58 , mcp is equivalent to hard thresholding .",
    "the bound diverges in this case , and there is no limit to the rate at which the solution path may change and no possibility of discarding variables based on this argument .    as in the lasso case , a slope bound for variables in the active set does not necessarily extend to variables in the inactive set .",
    "nevertheless , it is reasonable to expect that the correlation with the residuals is changing more rapidly for variables in the active set than variables in the inactive set .",
    "this line of thinking that allows us to establish an explicit rule for screening predictors during optimization .",
    "if , for @xmath59 , the bound @xmath60 holds , we can obtain the following rule , which we call the ( sequential ) strong rule for mcp : @xmath61 note that , for any variable @xmath6 satisfying and , we have @xmath62 and thus , @xmath38 .    indeed , as we shall see , the heuristic argument that residual correlation changes more rapidly in the active set than the inactive set holds up quite well in practice",
    "nevertheless , violations are possible , and thus it is necessary to check the discarded variables against the kkt conditions as a final step in the optimization algorithm .      the scad penalty proposed by @xcite is defined as @xmath63 for @xmath41 and @xmath64 . from the kkt conditions ( [ kkt : con ] ) , we have @xmath65 where @xmath51 stands for constant terms not depending on @xmath14 . for scad , the orthogonal design solution is @xmath66 . applying the same reasoning as in section  [ sec : strong - mcp ] , we obtain the approximate slope bound @xmath67 and the sequential strong rule for scad @xmath68 like mcp , the scad solution path is capable of changing more rapidly with respect to @xmath34 than lasso , and thus requires a larger bound for its strong rule .",
    "we now provide an illustration of the how the strong rules perform using a simulated example .",
    "the design of the simulation , which we also use for the simulation study in section  [ simulation ] , is as follows .",
    "all covariates marginally follow standard gaussian distributions , with a common correlation @xmath69 between any two covariates .",
    "the response variable @xmath70 is generated from the linear model ( [ linear : model ] ) with errors drawn from the standard gaussian distribution .",
    "for each independently generated data set , we set @xmath71 and @xmath72 , with @xmath73 nonzero coefficients set to be @xmath74 for linear regression and the remaining @xmath75 coefficients equal to zero . throughout this paper",
    ", we fix @xmath76 for mcp and @xmath77 for scad , roughly in line with recommendations suggested in @xcite and @xcite , respectively .",
    "application of the strong rules ( [ seq : strong : rule ] , [ rule : mcp ] , and [ rule : scad ] ) for two simulated data sets , one with @xmath78 ( top panel ) and the other with @xmath79 ( bottom panel ) .",
    "the solid line is the number of variables left after filtering by strong rules and the dotted line is the actual number of active variables for each @xmath14 .",
    "the region of the coefficient path that does not satisfy local convexity is shaded gray .",
    "vertical lines are drawn for any value of @xmath34 at which a violation of the strong rules occurred . ]",
    "figure  [ loc - con ] displays the performance of the strong rules for lasso , mcp , and scad on two simulated data sets , one with uncorrelated covariates , the other with a pairwise correlation of @xmath79 .",
    "the figure displays the number of variables remaining ( i.e. , @xmath80 minus the number of discarded variables ) after applying the strong rules for a decreasing sequence of @xmath34 values , alongside the actual number of nonzero coefficients in the model for those @xmath34 values .",
    "vertical lines are drawn for each value of @xmath34 for which a violation of the strong rules occurred .",
    "so in this example , there were no violations for any of the methods when @xmath78 , but we observe 3 violations for mcp and 7 violations for scad when @xmath79 .",
    "the strong rules perform remarkably well here , especially for @xmath78 .",
    "the vast majority of the @xmath72 variables are discarded by the strong rules .",
    "in fact , nearly all of the variables that should be discarded are discarded , across the entire path of @xmath34 values . with so many variables discarded by the strong rules ,",
    "it is surprising how rare it is for a variable to be erroneously discarded .",
    "nevertheless , violations do occur , and are more common for nonconvex penalties than for the lasso , as we discuss in the next section .",
    "violations are important , but not fatal  an algorithm based on dimension reduction through discarding variables can always check the validity of the dimension reduction by inspecting the kkt conditions for @xmath27 upon convergence for each value of @xmath34 , and include any variables that were erroneously discarded . in this manner , we ensure that all solutions @xmath28 returned by the algorithm are indeed a ( local ) minimum of the objective function .",
    "details for constructing algorithms based on strong rules are given in section  [ algorithm ] .",
    "although none occurred in this example , violations are also possible for the lasso , and a similar kkt - checking step is required in the lasso algorithm proposed by @xcite .",
    "a systematic numerical study of the frequency of violations for mcp and scad is provided in section  [ simulation ] .",
    "unlike the lasso solution path , for nonconvex penalties @xmath28 is not necessarily a continuous function of @xmath34 .",
    "it is possible for the objective function to possess multiple local minima , and for @xmath27 to `` jump '' from one local minimum to a different local minimum between @xmath26 and @xmath29 .",
    "such a discontinuity undermines the entire premise of strong rules .",
    "it is possible , however , to characterize the regions of the solution path where such discontinuities may and may not occur .",
    "the portion of the solution path guaranteed to be continuous was referred to in @xcite as the locally convex region . letting @xmath81 denote the active set of variables at @xmath14 , and @xmath82 denote the minimum eigenvalue of @xmath83 , the solution path is said to be locally convex at @xmath34 if @xmath84 for mcp , and @xmath85 for scad .",
    "correspondingly , the locally convex region is defined as @xmath86 , where @xmath87 is the first ( i.e. , largest ) value of @xmath34 for which the solution is no longer locally convex .",
    "as demonstrated in @xcite , coefficient paths for nonconvex penalties are smooth and well behaved in the locally convex region , but may be discontinuous and erratic in the non - locally convex region .",
    "we would therefore expect strong rules to be less reliable in the non - locally convex region , and this is precisely what we see in figure  [ loc - con ] , where all the observed violations occur in the non - locally convex region .",
    "indeed , although this is not apparent in the figure , several violations tend to occur simultaneously when a discontinuity arises in the solution path .",
    "for example , at @xmath88 , there were 11 variables excluded by the strong rules that were discovered during the kkt check to be nonzero at the new local minimum @xmath27 .",
    "the presence of discontinuities in the solution paths for nonconvex penalties places an inherent limitation on the use of sequential rules to improve optimization efficiency during model fitting .",
    "nevertheless , as we will see , even in highly correlated settings , only a small number of @xmath34 values experience violations , and strong rules may be profitably incorporated into optimization algorithms for nonconvex penalized models despite these violations , solving for the solution path @xmath27 substantially faster than cyclic coordinate descent approaches .",
    "to stabilize the solution path for nonconvex penalties , especially in @xmath89 problems with highly correlated predictors , @xcite proposed the mnet estimator , which is defined as the minimizer of @xmath90 where @xmath91 is the mcp .",
    "the logic behind the estimator is the same as that of the elastic net ( or enet , * ? ? ?",
    "* ) , but with mcp replacing the lasso in the penalty . let @xmath92 where @xmath93 is the @xmath94 identity matrix and @xmath95 is the @xmath80-dimensional vector whose all elements are zero . then the criterion ( [ def : mnet ] )",
    "may be rewritten as @xmath96 hence , we can directly apply the sequential strong rule ( [ rule : mcp ] ) to discard variables . reparameterizing the problem in terms of @xmath97 and @xmath98 ,",
    "the strong rule for mnet becomes @xmath99 since @xmath100 and @xmath101 .",
    "for inactive variables ( @xmath102 ) , the above rule reduces to @xmath103     application of strong rule to a simulated data set with @xmath79 for different values of @xmath104 . as in figure",
    "[ loc - con ] , the solid line is the number of variables left after filtering by strong rules , the dotted line is the actual number of active variables for each @xmath14 , the region of the coefficient path that does not satisfy local convexity is shaded gray , and vertical lines are drawn for any value of @xmath34 for which a violation of the strong rule occurred . ]",
    "figure  [ mnet ] illustrates the application of strong rules to the mnet estimator for a simulated data set with @xmath79 as we vary the parameter @xmath104 that controls the mcp/@xmath0 balance in the penalty . as in section",
    "[ sec : local - convexity ] , one may characterize the locally convex region ; for the mnet estimator , this consists of the values of @xmath14 satisfying @xmath105 .    from figure  [ mnet ]",
    ", we can see that as we decrease @xmath104 and thereby increase the @xmath0 proportion of the penalty , the locally convex region is extended , the model becomes less sparse , and fewer issues with discontinuities and strong rule violations arise .",
    "indeed , at @xmath106 , the objective function is locally convex over the entire solution path and no violations occurred .",
    "for all @xmath104 values , the strong rules were successful in discarding a large proportion of the inactive variables .",
    "suppose that the distribution of @xmath107 falls within the framework of the generalized linear model ( glm ) , with link function @xmath108 , where @xmath109 and @xmath110 where @xmath111 is the vector of linear predictors .",
    "the nonconvex penalized estimator for a glm is defined as the minimizer of the negative log - likelihood plus the penalty term .",
    "for example , for logistic regression , @xmath112    the extension of strong rules to glms is straightforward . indeed , both the kkt conditions and the strong rules themselves ( [ rule : mcp ] , [ rule : scad ] ) are the same as in the linear case , although the residual vector must now incorporate the link function : @xmath113 .",
    "for example , @xmath114 for logistic regression and @xmath115 for poisson regression .",
    "nonconvex penalties have also been proposed in the context of group variable selection .",
    "suppose that the covariates may be grouped into @xmath116 groups , with the grouping structure non - overlapping and known in advance : @xmath117 where @xmath118 is the @xmath119 design matrix corresponding to the @xmath120th group and @xmath121 is the vector of corresponding regression coefficients of the @xmath120th group .",
    "the nonconvex group penalized estimator is defined as the minimizer of @xmath122 where @xmath123 is a penalty function applied to the @xmath0-norm of @xmath124 .",
    "it is common practice @xcite to adjust the regularization parameter for each group using @xmath125 to account for differences in group size . by the kkt conditions ,",
    "the local minimizer @xmath126 satisfies @xmath127 where @xmath128 is the index set of nonzero groups .",
    "similar to standard variable selection , we can derive strong rules to discard the @xmath120th group for group mcp and group scad as follows : @xmath129 although we framed this derivation in the linear regression setting , note that these rules apply to the generalized linear model case as well , provided that the link function is included in the calculation of @xmath130 , as in section  [ glm ] .",
    "in this section , we carry out a more thorough investigation of the illustration presented in figure  [ loc - con ] in terms of the frequency of strong rule violations .",
    "we first consider linear and logistic regression for mcp and scad .",
    "the simulation design follows the description in section  [ strong : non ] for the linear regression case ; for logistic regression , the design is the same except that @xmath131 follows a bernoulli distribution with the logistic link function and the nonzero regression coefficients equal @xmath132 . for 100 independently generated data sets , we record the number of eliminated variables , the number of @xmath14 values at which a violation occurred , and the total number of erroneously discarded variables . for the violations",
    ", we also record whether the violation occurred in the locally convex region of the solution path or not .",
    ".simulation results for mcp and scad strong rules with @xmath71 and @xmath72 .",
    "results averaged over 100 independent data sets . [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]     [ tab - grp ]    table [ tab - grp ] shows the number of discarded groups , as well as the number of strong rule violations , averaged over 100 independent data sets . as in the non - grouped case , violations occur only for a small fraction of the 100 @xmath34 values along the solution path , and almost always in the non - locally convex region .",
    "in this section , we discuss the incorporation of strong rules into the coordinate descent ( cd ) algorithm of @xcite for fitting nonconvex penalized regression models .",
    "the algorithms we propose may be viewed as modifications of the idea behind coordinate descent : rather than cycling over the full set of variables with every iteration , the availability of strong rules and other heuristics allow one to carry out _ targeted cycling _ in which computational effort is concentrated on the variables most likely to be nonzero and therefore change from one @xmath34 value to the next .",
    "we consider three targeted cycling algorithms : one based on strong rules , one based on active set cycling , and a hybrid algorithm combining the two heuristics .",
    "calculate the strong set @xmath133 and let @xmath134 find the solution @xmath135 using only the variables in @xmath136 find @xmath137 update @xmath136 by @xmath138 @xmath139    algorithm  [ alg : basic ] describes the incorporation of strong rules into the coordinate descent algorithm ; we refer to this approach as the _ strong rule algorithm_. the algorithm relies on computing the _ strong set _",
    "@xmath140 , which we define as the set of variables remaining after discarding variables according to the strong rules ( [ rule : mcp ] , [ rule : scad ] ) , and then using this set as the target set @xmath136 that we cycle over until convergence .",
    "as discussed previously , it is possible for strong rules to be violated , and therefore necessary to calculate the set of violations @xmath141 in order to ensure that all solutions @xmath28 satisfy the kkt conditions at convergence .",
    "an alternative approach is to use the active set @xmath142 as the target set for calculating the solution at the next step in the solution path , @xmath143 . the algorithm , which we refer to as _ active set cycling _",
    "@xcite , is the same as algorithm  [ alg : basic ] with the active set @xmath142 replacing the strong set @xmath144 .",
    "set @xmath145 and @xmath146 find the solution @xmath135 using only the variables in @xmath136 find @xmath147 update @xmath136 by @xmath148 @xmath149 find @xmath150 update @xmath136 by @xmath151 @xmath152    the final approach we consider combines the active set and strong sets into an algorithm that involves two - stage targeted cycling .",
    "the details are provided in algorithm  [ alg : hybrid ] , which we refer to as the _ hybrid algorithm_.    contrasting the four algorithms ( cyclic , strong , active , and hybrid ) , there is a tradeoff between how aggressive the algorithms are in terms of discarding variables and how often violations involving erroneously discarded variables occur .",
    "discarding variables naturally increases the speed of optimization over the target set ; however , violations introduce a computational cost as well , since the iterative targeted cycling procedure must be restarted and the kkt conditions re - checked . at one extreme ,",
    "active set cycling discards the largest number of variables , but its targeted cycling rule is violated every time a new variable enters the active set . on the other extreme , full cyclic coordinate descent does not have to contend with violations or re - check any kkt conditions , but must contend with the full set of variable at every step . the strong and hybrid algorithms attempt to occupy a middle ground between these two extremes , reducing dimensionality as much as possible without introducing a large number of violations .",
    "comparison of the cyclic cd algorithm and targeted cycling algorithms in terms of computing time required to fit the entire coefficient path down to @xmath153 for linear regression as a function of the number of covariates @xmath80 .",
    "both axes are on the log scale .",
    "median times over 20 replications are displayed . ]",
    "figure  [ fig - ncv23 ] demonstrates that all three targeted cycling algorithms are considerably faster than full cyclic coordinate descent , and that the magnitude of the difference is substantial for high dimensional problems . for example , the median time required to fit a scad model with @xmath79 and @xmath154 was 1,711 seconds using cyclic coordinate descent and just 23 seconds using the strong rule algorithm .",
    "it is worth noting that even though strong rules are more likely to be violated as correlation increases , the fact that optimization algorithms must go through a larger number of iterations in this case results in an even greater advantage for targeted cycling in the correlated case than in the uncorrelated case .     comparison of targeted cycling algorithms in terms of computational time required to fit the entire coefficient path for linear ( top ) , logistic ( middle ) and poisson regression ( bottom ) .",
    "median computing times over 20 replications are displayed . ]    in figure  [ fig - ncv23 ] , it is clear that targeted cycling is more efficient than full cyclic cd , but it is unclear how the target cycling algorithms compare to each other . in figure  [ fig - time ] , we compare the speed of the three targeted cycling algorithms for linear , logistic , and poisson regression . in each case , 20 variables are set to @xmath132 with the remaining variables set to zero , and the outcome follows the distribution assumed in the glm . in all cases ,",
    "the strong rule and hybrid algorithms were seen to be more efficient than active set cycling .",
    "although the difference in computing time between the targeted cycling algorithms is minor for small @xmath80 , active set cycling can take almost twice as long for high - dimensional models .",
    "for example , fitting a scad - penalized logistic regression model with @xmath154 required a median computing time of 11 seconds for active cycling and only 7 seconds for the strong rule and hybrid algorithms .",
    "comparison of the size of strong and active sets for each @xmath14 for simulated gaussian data with @xmath78 , @xmath71 and @xmath155 . in `` case 1 ''",
    "( top panel ) , nonzero coefficients are equal to @xmath74 ; in `` case 2 '' ( bottom panel ) , all nonzero coefficients equal @xmath156 . ]    although the strong rule algorithm was slightly faster than the hybrid algorithm in figure  [ fig - time ] , we have found that there are situations in which the hybrid algorithm offers considerably better performance than the strong rule approach .",
    "we depict one such situation for linear regression in figure  [ str - act ] .",
    "the top panel ( `` case 1 '' ) of figure  [ str - act ] is similar to the situations we have examined so far , with @xmath71 , @xmath155 , @xmath78 , and 20 nonzero coefficients equal to @xmath74 . here",
    ", the variance of gaussian noise was chosen so that the signal - to - noise ratio was equal to 3 .",
    "the setting for the bottom panel ( `` case 2 '' ) is the same , except that the nonzero coefficients all have coefficients equal to @xmath156 . in case 1",
    ", the size of the target set for the strong rule algorithm matches the active set quite closely , and nearly all the variables that can be eliminated are eliminated by the strong rules . in case 2 , however , although the strong rules are still valid and rarely violated , they do not yield a target set that closely matches the active set , and fail to discard hundreds of variables that remained inactive .    in case 1",
    ", there were minimal differences between the computing time of the three targeted cycling algorithms ( all were within 1 second of each other ) . in case 2 , however , the strong rule algorithm was substantially slower than active cycling and the hybrid algorithm due to the much larger size of its target set .",
    "for scad , where the difference in target set size was most dramatic , the strong rule algorithm required 19 seconds , while active cycling required just 5 . as it is designed to do , the hybrid algorithm utilizes the best features of each heuristic and requires just 4 seconds to compute the solution path .    in summary , we find the hybrid algorithm to be the most robust of the targeted cycling approaches  never much slower than the strong rule algorithm , and in some cases much faster .",
    "for this reason , we have implemented the hybrid algorithm for lasso , scad , and mcp - penalized linear , logistic , and poisson regression in the ncvreg package .",
    "in this section , we apply the algorithms described in section  [ algorithm ] to real data from a genome - wide association study ( gwas ) of preeclampsia .",
    "the data were collected during the study of pregnancy hypertension in iowa ( sophia ) , a population - based case - control study .",
    "we provide a brief description of the data here ; the study is described in greater detail in @xcite .",
    "the sample consists of 177 mothers diagnosed with preeclampsia according to national heart , lung and blood institute guidelines and 115 mothers with normal blood pressure to serve as controls .",
    "all 292 mothers were genotyped using the affymetrix genome - wide human snp array 6.0 ( affymetrix , santa clara , ca ) . after applying quality control procedures and eliminating monomorphic markers ,",
    "we were left with 810,198 single - nucleotide polymorphisms ( snps ) to serve as potential predictors of preeclampsia risk .",
    "we analyzed this data using mcp - penalized logistic regression with case - control status as the response variable .",
    "allele effects were assumed to be additive and independent , thereby yielding a design matrix with @xmath157 and @xmath158 . due to the fact that @xmath159 , we fit the penalized regression model over a relatively small portion of the coefficient path , down to @xmath160 , at which point 22 snps had entered the model . despite the large number of features in the design matrix , the penalized regression models could be fit very rapidly : using the strong rule algorithm , the solution path could be fit in just 4.7 seconds on a standard desktop computer ( 3.60ghz intel xeon processor , 16 gb ram ) .",
    "the active cycling algorithm took somewhat longer , at 7.9 seconds , while the performance of the hybrid algorithm was similar to that of the strong rule approach ( 4.9 seconds to fit the solution path ) .",
    "the snps selected by the penalized regression model are consistent with the top - ranked snps in terms of univariate hypothesis testing using fisher s exact test , as reported in @xcite .",
    "however , we reach the same conclusion that the authors of the previous study reached  namely , that there is insufficient evidence in the data to perform variable selection with any meaningful degree of reliability . in particular , when we carry out 10-fold cross - validation for the purposes of selecting @xmath14 , we find that the optimal model is the intercept - only model .",
    "although this particular study was negative in terms of identifying genetic risk factors for preeclampsia , it illustrates the feasibility of fitting penalized regression models to very high - dimensional data .",
    "the current genome - wide association literature is overwhelmingly focused on univariate tests , which have many shortcomings compared to multivariate modeling : inefficiency , increased risk of confounding , and limited predictive inference , among others .",
    "several authors have recommended penalized regression as an alternative , and discussed its benefits in comparison with univariate testing @xcite .",
    "others , however , have judged the problem to be computationally impractical for the very high dimensions that prevail at the genome - wide scale and developed multi - stage or iterative screening proposals to reduce the dimensionality of the problem @xcite .",
    "we demonstrate here that such approaches are not necessary  or , depending on your perspective , that screening is indeed a very useful idea , but it can be incorporated directly into coordinate descent algorithms through targeted cycling .",
    "concern over the computational burden of penalized regression in very high dimensions has prevented its use in many fields , particularly in genetics .",
    "this concern , in turn , has led many researchers to pre - screening procedures to reduce the dimensionality of the problem before fitting the penalized regression model . at best",
    ", this complicates both the theoretical study of such procedures and the practical implementation of procedures such as cross - validation . at worst",
    ", it opens the door for bad statistical practice by obfuscating the multiple comparison problem .",
    "for example , if pre - screening is used to select candidate variables on the full data set , and then cross - validation is used to select a tuning parameter @xmath14 , the resulting inference is heavily biased by the fact that the external validation data is not truly external , as it has already been used for screening .",
    "it is possible to carry out unbiased cross - validation in the presence of screening , but it is also very easy for a well - intentioned investigator to make a mistake ( a thorough discussion of this issue may be found in * ? ? ?",
    "in contrast , cross - validation is both straightforward and computationally feasible , and already implemented existing software such as glmnet and ncvreg . in particular , for the analysis in section  [ sec : gwas ] , ten - fold cross validation was carried out in under a minute despite fitting nonconvex penalized logistic regression models with @xmath158 variables .    with this work",
    ", we have demonstrated that fitting high - dimensional nonconvex penalized regression models can be made computationally feasible through the use of targeted cycling and strong rules to achieve dimension reduction .",
    "furthermore , by sharing implementations of these algorithms in the publicly available r package ncvreg , we hope to encourage researchers to adopt these methods with greater regularity for analyzing high - dimensional data .",
    "shi , g. , boerwinkle , e. , morrison , a.  c. , gu , c.  c. , chakravarti , a. and rao , d. ( 2011 ) . mining gold dust under the genome wide significance level : a two - stage approach to analysis of gwas .",
    "_ genetic epidemiology _ , * 35 * 111118 .      tibshirani , r. , bien , j. , friedman , j. , hastie , t. , simon , n. , taylor , j. and tibshirani , r.  j. ( 2012 ) .",
    "strong rules for discarding predictors in lasso - type problems .",
    "_ journal of the royal statistical society : series b ( statistical methodology ) _ , * 74 * 245266 .              zhao , l. , triche , e. , walsh , k. , bracken , m. , saftlas , a. , hoh , j. and dewan , a. ( 2012 ) . genome - wide association study identifies a maternal copy - number deletion in psg11 enriched among preeclampsia patients . _ bmc pregnancy and childbirth _ , * 12 * 61 ."
  ],
  "abstract_text": [
    "<S> we consider approaches for improving the efficiency of algorithms for fitting nonconvex penalized regression models such as scad and mcp in high dimensions . in particular , we develop rules for discarding variables during cyclic coordinate descent . </S>",
    "<S> this dimension reduction leads to a substantial improvement in the speed of these algorithms for high - dimensional problems . </S>",
    "<S> the rules we propose here eliminate a substantial fraction of the variables from the coordinate descent algorithm . </S>",
    "<S> violations are quite rare , especially in the locally convex region of the solution path , and furthermore , may be easily detected and corrected by checking the karush - kuhn - tucker conditions . </S>",
    "<S> we extend these rules to generalized linear models , as well as to other nonconvex penalties such as the @xmath0-stabilized mnet penalty , group mcp , and group scad . </S>",
    "<S> we explore three variants of the coordinate decent algorithm that incorporate these rules and study the efficiency of these algorithms in fitting models to both simulated data and on real data from a genome - wide association study .    * strong rules for nonconvex penalties and their implications for efficient algorithms in high - dimensional regression * +   + _ the university of iowa _ +    _ keywords _ : coordinate descent algorithms , local convexity , nonconvex penalties , dimension reduction . </S>"
  ]
}