{
  "article_text": [
    "importance sampling ( see for instance @xcite ) is a well - established simulation method used to overcome the difficulties connected with the intractability of a target distribution @xmath0 .",
    "its shortcomings are also well - documented , in particular the degradation of its performances against the dimensionality of the problem .",
    "when computing importance weights , @xmath1 ( @xmath2 and @xmath3 are the densities of , respectively , the target and the importance distributions with respect to the same dominating measure @xmath4 and , @xmath5 is chosen such that @xmath0 is absolutely continuous with respect to @xmath5 ) associated with samples @xmath6 from an importance distribution @xmath5 , the distribution of those weights customarily deteriorates as the dimension of @xmath7 increases ( @xmath7 takes values in @xmath8 ) . since , in practical settings",
    ", the fine tuning of the importance distribution against the target is difficult , alternative markov chain monte carlo approaches have often been advocated as being more appropriate for large dimensional problems ( see @xcite ) but recent attempts have been made to construct importance functions that automatically adapt to the target distribution based on earlier importance samples ( see , e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "those methods are called adaptive importance sampling but they also relate to particle filters @xcite and sequential monte carlo methods @xcite .",
    "there are many different strategies of adaptive importance sampling .",
    "for instance , the generic population monte carlo ( pmc ) scheme of @xcite can be implemented as the @xmath9-kernel @xcite algorithm , which tries to fit a mixture of @xmath9 given kernels or importance sampling distributions in terms of either minimum variance or minimum kullback - leibler divergence .",
    "while this algorithm is shown to converge to the best solution ( either minimum variance or minimum kullback - leibler divergence ) within the class of @xmath9 kernels , it is concerned with a specific type of proposal and it still uses a small family of distributions that may fail to properly represent the target .    in this paper , we propose a novel perspective to pool together importance samples .",
    "those importance samples originate from different importance sampling distributions and are associated with corresponding importance weights , of the form @xmath10 where @xmath11 and @xmath12 are proper densities and where @xmath13 @xmath14 . while those @xmath15 samples can be put together by keeping these original importance weights ( robert and casella , 2004 , chapter 14 ) , there exists a stabilising alternative called _ deterministic multiple mixture _ due to @xcite and popularised by @xcite .",
    "this alternative is akin to the defensive sampling approach of @xcite in that it consists in modifying the denominators of all importance weights @xmath16 from the density @xmath17 of the distribution under which each point @xmath18 was truly simulated to a mixture of all the densities that have been used for the @xmath15 different samples , namely @xmath19 resulting in the deterministic mixture weight @xmath20 this idea has originally been proposed by @xcite and @xmath21 =   \\sum_{t=0}^t\\sum_{i=1}^{n_t } \\mathbb{e}_{q_t}\\left[\\frac{\\pi({\\mathbf{y}}_i^t)}{\\sum_{l=0}^t n_l q_l({\\mathbf{y}}_i^t)}h({\\mathbf{y}}_i^t)\\right]=\\ ] ] @xmath22 the name _ deterministic mixture weights _ stems from the fact that the weights of the mixture are not estimated or varying over time .",
    "this is a major difference with the pmc schemes of @xcite where the weights of the proposals are optimised against an efficiency criterion like the kullback ",
    "leibler divergence .",
    "the novelty in our approach called amis ( for adaptive multiple importance sampling ) is that , when compared with the previous works on multiple mixtures , our family @xmath23 of importance sampling distributions is constructed sequentially _ and _ adaptively , in the sense that the importance sampling distribution used at each iteration @xmath24 @xmath25 is based on the past @xmath26 weighted samples .",
    "therefore , at each step @xmath24 of the algorithm ,    1 .",
    "the weights of all ( present and past ) simulated variables @xmath27 @xmath28 are modified , based on the current collection of proposals ( importance sampling distributions ) @xmath29 , and 2 .",
    "[ recy ] the entire collection of importance samples is used to build the next importance function .",
    "note that , while is a classical feature of population monte carlo , most implementations that construct @xmath30 based on past iterations @xcite only use the sample produced at the previous generation , @xmath26 . using the entire past of the simulation process provides a natural stabilisation that speeds up convergence .",
    "a similar type of methodology has been independently studied by @xcite .    in most practical settings , notably in bayesian estimation ,",
    "a self - normalised estimator is used instead , because the density of the target is known only up to a normalising constant . in such cases ,",
    "the importance weights can only be evaluated up to this normalising constant and , by construction , the self - normalised estimator does not depend on this constant . in the examples",
    ", we always use the self - normalised amis estimator , even for the benchmark banana shape target considered in  [ sec : banana ] .",
    "we detail the reasons for promoting multiple mixture importance sampling in  [ sec : m&m ] and analyse some associated algorithms in  [ sec : amis ] , while discussing their theoretical properties in  [ sec : con ] .",
    "the performances of the amis algorithm are tested in  [ sec : banana ] over a challenging banana shape target distribution and in  [ sec : imp ] over a population genetic application .",
    "this latter type of model has motivated the development of the proposed methodology .",
    "indeed , the likelihood of a genetic model most often is not tractable and regardless of the approximation method used , its derivation involves a non - negligible cost .",
    "we point out that @xcite have resorted to our amis algorithm to handle complex population genetics models , avoiding the dramatic impact of a poor first proposal .",
    "the modification in the importance weights from the original ratios to the mixture ratios may sound surprising or even paradoxical in that the simulated values ( and therefore the distributions used to simulate those ) have not changed .",
    "we thus detail in this section the motivations for using multiple mixtures .",
    "there exists a fundamental methodological difficulty in using several importance functions at once .",
    "indeed , if @xmath0 is the target density and @xmath31 are @xmath15 different importance functions , samples @xmath32 , @xmath33 , @xmath34 that are simulated from these importance functions , with associated standard importance weights @xmath35 , can be merged together in that the empirical distribution function @xmath36 produces ( in the marginal sense ) an output approximatively distributed from the target @xmath12 .",
    "+ unfortunately , this property is not sufficient to ensure that the resulting sample performs satisfactorily .",
    "for instance , if one of the importance functions @xmath11 is associated with an infinite variance in the weights @xmath16 , i.e.  if @xmath37=+\\infty$ ] for one @xmath38 , the potentially very large weights resulting from this importance experiment will remain very large in the cumulated sample , no matter how efficient the other importance functions are .",
    "therefore , the poorly performing sample will overwhelmingly dominate the other samples in the final approximation and thus ruin the overall performances of the method .",
    "the conclusion of this point is that the raw mixing of importance samples and of their importance weights , when using different proposals , can be quite harmful , when compared with using a single sample , even when most proposals are efficient .    as discussed at large in @xcite , using a deterministic mixture as a representation of the production of the simulated sample has the potential to exploit the most efficient proposals in the sequence @xmath31 without rejecting any simulated value nor sample .",
    "the poorly performing importance functions are simply eliminated through the erosion of their weights @xmath39 as @xmath15 increases .",
    "indeed , for all @xmath40 not necessarily equals , if @xmath41 is the poorly performing proposal , while the @xmath42 s @xmath43 are good approximations of @xmath12 , for a value @xmath44 such that @xmath45 is large , because @xmath46 is small , @xmath47 will behave like @xmath48 and decrease to zero as @xmath15 increases .",
    "the paradoxical feature of having several acceptable importance weights for the same simulated value is well - understood in the case of rao - blackwellisation @xcite . in the setting of multiple mixtures ,",
    "the argument is however more intricate than the marginalisation inherent to the rao - blackwellisation schemes in that only the unbiasedness aspect ( in the restricted sense of ) remains .",
    "the modification of the weights is indeed difficult to perceive as an extra rao - blackwell step that would eliminate the randomness related to the mixture structure , i.e.  the selection of the component , since the cumulation of the samples from the components of the mixture is _ not _ a sample from the mixture .",
    "as explained in the introduction , the idea at the core of the amis algorithm is that , for each time - step @xmath24 , we should update not only the weights @xmath16 of the @xmath49 current particles , @xmath18 , but also the weights @xmath50 of all past particles @xmath51 .",
    "our algorithm can thus be interpreted as a rao - blackwell type of importance sampling where the whole sample of @xmath52 points can be envisioned of as being homogeneously sampled from a deterministic mixture made of the overall sum of proposals .",
    "( once again , the term _ deterministic mixture _ is a misnomer in that the overall sample is not the outcome of a mixture simulation . )",
    "the major difference with various pmc versions @xcite is that every single simulated value is recycled and reweighted at every step of our iterative algorithm by virtue of selecting the appropriate deterministic mixture .",
    "indeed , at each iteration @xmath24 of the algorithm , a new adaptive importance sampling distribution is constructed by using , not only the particles corresponding to the current iteration , but all the weighted particles , based on a well - chosen efficiency criterion as in earlier pmc versions @xcite . in the most standard case when the proposal @xmath30 is parameterised , i.e.  when @xmath30 is of the form @xmath53 within a parametric family of distributions @xmath54 , the adaptivity consists in estimating @xmath55 by @xmath56 at each iteration , using all the weighted samples accumulated so far ; this estimation is obtained by using specific criterion like moment matching , variance minimisation or kullback - leibler minimisation .",
    "a pseudo - code representation of the generic amis algorithm is given as follows :    [ algo : genamis]*generic amis *    at iteration @xmath57 ,    * independently generate @xmath58 particles @xmath44 ( @xmath59 ) from @xmath60 . * for @xmath59 ,",
    "compute @xmath61 * compute the importance sampling parameter estimate @xmath62 of the parametric family @xmath54 using the weighted particles @xmath63 and a well - chosen estimation criterion .    at iteration @xmath64    * independently generate @xmath49 particles @xmath18 ( @xmath65 ) as @xmath66 * for @xmath67 , compute the multiple mixture at @xmath68 @xmath69 and derive the importance weight of particle @xmath18 , @xmath70\\,.\\ ] ] * for @xmath71 and @xmath72 , update the past importance weights as @xmath73\\,.\\ ] ] * compute the parameter estimate @xmath74 using all the weighted particles @xmath75 and the same estimation criterion .",
    "after @xmath15 iterations of the amis algorithm , for any @xmath0-integrable function @xmath76 , the self - normalised amis estimator of @xmath77 is : @xmath78    since the above algorithm is set in generic terms , we describe a first special case that applies to many settings and can be seen as a vanilla amis algorithm . as in the most recent pmc algorithm of @xcite ,",
    "the proposal distribution @xmath5 is a student s @xmath24 proposal , @xmath79 whose mean @xmath80 and covariance @xmath81 parameters are updated by estimating both first moments of the target distribution @xmath0 using self - normalised amis estimators : @xmath82 and @xmath83 note that the degrees of freedom of the @xmath24 distribution are always set to @xmath84 as the lowest value allowing for finite first moments but they could also be estimated at each iteration .",
    "moreover , instead of using the previous `` moments matching '' criterion , we can also used the kullback - leibler divergence between @xmath0 and @xmath5 in order to choose the parameter @xmath85 , @xmath86 here , the best choice for the parameter @xmath87 is the maximum likelihood estimate of @xmath87 where the observations are weighted by their corresponding importance weight .",
    "these two different strategies give essentially the same results .",
    "quite obviously and as illustrated by the next section , more elaborate proposals are possible , depending on the information available on @xmath0 .",
    "for instance , if the potential for multimodality of the target @xmath0 is high enough , a mixture of student s @xmath24 distributions as in @xcite would be more appropriate .",
    "when dealing with a bayesian hierarchical model , creating classes ( or blocks ) of components of the parameter in agreement with the hierarchical levels ( as in gibbs sampling ) and designing the student s @xmath24 proposals block by block should also be more efficient .",
    "similarly , matching the expectation and the covariance structure of the student s proposal distribution with both first moments of the target distribution is only one among many efficiency criteria that can be used to calibrate the parameters of the proposal distribution at each step of the algorithm .",
    "for instance , as done in the next section , we can alternatively minimise the kullback - leibler divergence between the target and the proposal distribution following the approach of @xcite .",
    "although we do not elaborate on this possible improvement , note also that , once the weighted sample based on @xmath88 simulations is obtained , it is possible to apply a final clustering ( standard ) algorithm on this sample , based on a gaussian mixture representation . those clusters can be used to estimate local covariance and mean structures and then simulate a final and global sample based on the cluster representation but using student s @xmath24 distributions . because all weights are controlled",
    ", we can then merge this final sample with the sequence of earlier samples without losing the deterministic representation .",
    "a special version of interest of the amis algorithm is based on the used of mixtures of multivariate gaussian densities .",
    "that is @xmath89 where @xmath90 denotes a multivariate gaussian density with mean @xmath80 and covariance matrix @xmath81 , as in the @xmath9-kernel approach to pmc algorithms of @xcite .",
    "we also use the kullback - leibler divergence between @xmath0 and @xmath5 in order to choose the parameter @xmath91 , @xmath92 as already mentioned , the best choice for the parameter @xmath87 is then the maximum likelihood estimate of @xmath87 . in the amis setting ,",
    "the observations are weighted by their corresponding importance weight : at iteration @xmath24 the whole sequence of samples @xmath27 @xmath93 with their updated weights @xmath50 is used inside a weighted em algorithm , which is solved using the mixmod software @xcite .",
    "the number @xmath94 of components used for the mixture can be either set in advance or , more realistically , estimated at iteration @xmath57 by the icl criterion of @xcite and a substantial number @xmath58 of iterations .",
    "we do not reproduce the earlier pseudo - codes for this special case since the differences are minimal .",
    "note that the extension to a mixture of @xmath24 densities is equally feasible since there exists a corresponding em algorithm @xcite .",
    "a major difficulty with all adaptive importance algorithms is that the starting distribution is paramount to achieve proper performances . according to the `` what - you - get - is - what - you - see '' features of such algorithms , it is quite difficult to recover from a poor starting sample , the adaptivity only focussing on the visited parts of the simulation space . therefore , we insist on the requirement that a significant part of the computing effort must be spent on the initialisation stage .",
    "if we consider an importance sample of size @xmath58 having used the importance distribution @xmath60 , the effective sample size ( ess ) is given by @xmath95}$ ] @xcite .",
    "this measure of efficiency does not depend on @xmath76 and , in practice , @xmath96=\\int \\left\\{\\pi({\\mathbf{y}})/q_0({\\mathbf{y}})-{\\mathbb{e}}_{q_0}[\\pi({\\mathbf{y}})/q_0({\\mathbf{y}})]\\right\\}^2q_0({\\mathbf{y}})\\nu(\\text{d}{\\mathbf{y}})\\ ] ] can be estimated using the coefficient of variation of the importance weights .",
    "the ess can be used to calibrate the initial importance distribution .",
    "the initialisation solution we propose works in two steps :    * independently generate @xmath58 particles from the product of @xmath97 standard logistic distribution , + such a distribution admits the following density with respects to the lebesgue measure @xmath98 . that strategy corresponds first to use an inverse logistic transformation that maps the @xmath97 components into the unit interval and then to sample from a uniform distribution over this hypercube . * rescale component - wise this sample by maximising the ess .    note that to maximise the ess is equivalent to minimise the variance of the importance weights .",
    "moreover , in order to maximise the ess , we only need simulate one such sample since we can adapt the scale for this sample . obviously , this solution is far from being fool - proof and we favour an informed alternative implementation as soon as pieces of information on the target distribution are a priori available .",
    "the nelder and mead algorithm @xcite is used to maximise the ess .",
    "that is a simplex method which depends on the comparison of the ess values at the @xmath99 vertices of a general simplex , followed by the replacement of the vertex with the smallest value by another point .",
    "the simplex adapts itself to the local landscape , and contracts on to the final maximum .",
    "while establishing unbiasedness and convergence of the deterministic mixture estimator of @xcite is straightforward , the introduction of an adaptive mechanism in the construction of the sequence of proposals , highly complicates both issues .",
    "the estimator is no longer unbiased and its convergence ( in @xmath15 for a fixed values of @xmath49 ) can not be established without imposing restrictions on the simulation space . in order to explain those difficulties , we concentrate on the student s @xmath24 amis algorithm . we restrict ourselves to the extreme case @xmath100 , meaning that each iteration sees only one simulation ,",
    "i.e.  the proposal is updated after each new iteration .",
    "we also simplify the update of the student s @xmath24 proposal by only considering learning about the mean @xmath101 , the covariance matrix being set to an arbitrary value .",
    "this is a formalised setting that we do not advocate in practice .",
    "we use the notation @xmath102 to denote the corresponding student s @xmath24 density with 3 degrees of freedom and mean @xmath80 . the update of @xmath80 after iteration @xmath24 is @xmath103 where @xmath104 .    first , establishing the unbiasedness of the estimator @xmath101 for every @xmath105 does not follow from the same argument as in the original version of @xcite because of the dependence of the importance weight of @xmath106 on subsequent @xmath107 s @xmath108",
    "indeed , for @xmath109 , we have @xmath110 & = \\sum_{k=0}^t\\mathbb{e}\\left [ \\frac{\\pi({\\mathbf{y}}_k){\\mathbf{y}}_k}{q_0({\\mathbf{y}}_k)+\\sum_{i=1}^{t}t_3({\\mathbf{y}}_k;u_i({\\mathbf{y}}_{0:i-1}))}\\right ] \\\\ & = \\sum_{k=0}^t \\int \\frac{\\pi({\\mathbf{y}}_k){\\mathbf{y}}_k}{q_0({\\mathbf{y}}_k)+\\sum_{i=1}^{t}t_3({\\mathbf{y}}_k;u_i({\\mathbf{y}}_{0:i-1 } ) ) }      t_3({\\mathbf{y}}_k;u_k({\\mathbf{y}}_{0:k-1 } ) ) \\,\\text{d}{\\mathbf{y}}_k\\\\ & \\qquad\\qquad \\times\\,g_k({\\mathbf{y}}_{0:k-1 } ) \\,\\text{d}{\\mathbf{y}}_{0:k-1 } h_k({\\mathbf{y}}_{k+1:t}|{\\mathbf{y}}_k)\\,\\text{d}{\\mathbf{y}}_{k+1:t}\\end{aligned}\\ ] ] where @xmath111 is the joint distribution of the past simulations and @xmath112 is the conditional distribution of the future simulations given the current one .",
    "due to this latter term , the full conditional distribution of @xmath113 given the past and future simulations @xmath114 and @xmath115 is no longer @xmath116 and this modification implies that @xmath101 is biased .",
    "furthermore , the dependence of this bias on @xmath24 is so intricate that we can not manage the asymptotic bias .",
    "a similar block occurs when studying the variance , hence jeopardising the study of the convergence properties of the amis algorithm .",
    "moreover , the standard convergence results on triangular arrays @xcite do not apply contrary to the pmc algorithm @xcite .",
    "we note that a simple if artificial modification of the amis algorithm brings a solution to the bias difficulty , by using two different simulation threads , one for the calibration of the proposal distribution and a parallel one only used for the computation of the weight .",
    "obviously , the amis algorithm converges when @xmath24 is fixed and @xmath49 tends to infinity for standard importance sampling reasons .",
    "this however is not the set - up in which amis should be used .",
    "the number of iterations @xmath15 and the numbers of simulations @xmath49 ( @xmath64 ) should be related to the dimension @xmath97 of the target distribution .",
    "we recommend to use @xmath49 in the range @xmath117 : @xmath118 when @xmath97 is small ( typically @xmath119 or @xmath120 ) and @xmath121 when @xmath97 is large ( typically @xmath122 ) . for this choice , we tested the amis algorithm on various target distributions until @xmath122 .",
    "we always used @xmath123 .",
    "however , it can be eventually interesting to increase the numbers of simulations as the accuracy of the proposal distributions increases and then to specify @xmath124 .",
    "the amis algorithm can be used for different goals :    * an integral approximation for a fixed function @xmath76 ; that is typically the case when a joint distribution has to be marginalised ; * a global approximation of a target distribution .",
    "depending on @xmath125 and the previous reason , we need a minimal value for the ess . therefore , as stopping rule , we propose to iterate the amis algorithm using the previous settings until the desired ess is reached .    in the population genetics paradigm , @xcite proposed an original bayesian method for inferring population histories from unlinked single - nucleotide polymorphism .",
    "it is used an approximation to the neutral wright - fisher diffusion to model random fluctuations in allele frequencies . for inferences about the tree topology",
    ", the posterior distribution should be marginalised over a drift parameter . in order to avoid this difficult problem",
    ", the authors used the amis algorithm . for @xmath126 populations ,",
    "the drift parameter is a positive vector of dimension @xmath127 .",
    "they used a product of independent beta distributions , as initial importance distribution ( @xmath60 ) , then the remaining importance distributions ( @xmath128 ) are defined as multivariate student s distributions whose parameters are adapted at fixed interval . in their tests , they chose @xmath129 and @xmath130 depending on @xmath88 . typically , for simulated datasets where @xmath131 , they used @xmath132 and @xmath133 and , for an analysis of human data where @xmath134 , they computed the posterior probability of two topologies with @xmath135 .",
    "the goal of the next section is to show that the proposed algorithm can outperform standard adaptive importance sampling solutions on some benchmark target distributions .",
    "that is , with the same type of adaptive scheme , we can get improvement by pooling together all the particles in the sequential multiple mixture way . as the standard importance sampling algorithms work",
    "well when @xmath15 is small and @xmath49 large only , we also implemented the amis algorithm in that setup and chose : @xmath136 , @xmath137 and @xmath138 .    in section",
    "[ sec : imp ] , we consider a population genetics example and the amis algorithm was implemented with @xmath139 , @xmath140 and @xmath141 .",
    "the initial importance sampling distribution @xmath60 was defined as the prior distribution .",
    "we did not used any optimisation procedure related to the ess . indeed , for such a target distribution",
    ", it is quite easy to reach the interesting region of the space and we do not need a lot of adaptation steps . what is important is that the calculation of the target density is expensive and then we should recycle all the simulations .",
    "in order to evaluate the performances of the amis algorithm , we resorted to the benchmark target density set by @xcite , which can be calibrated to be extremely challenging .",
    "the target density is based on a centred @xmath97-multivariate gaussian , @xmath142 with covariance matrix @xmath143 which is twisted by a change of variable in the second coordinate from @xmath144 to @xmath145 . other coordinates remain unchanged .",
    "this change of variable leads to a twisted ( or banana shaped ) distribution that has expectation equal to 0 and uncorrelated components .",
    "since the jacobian of the twisting transformation is equal to 1 , the target density is : @xmath146 where @xmath147 denotes the density of the centred @xmath97-multivariate gaussian distribution with covariance @xmath81 .",
    "one of the appeals of this benchmark is to allow for various degrees of heavy tails while allowing to work in high dimensional settings .    for our comparison",
    ", we only consider a mild example of banana shape density , with @xmath148 and @xmath149 .",
    "figure [ fig : banana - target ] shows how twisted the resulting marginal distribution of @xmath150 is for this choice of parameters ( it does not depend on the dimension @xmath97 ) .",
    "( more twisted distributions , i.e.  ones with fatter tails can be obtained by using higher values of @xmath151 and/or @xmath152 . ) in our case , the target distribution satisfies @xmath153 for all @xmath154 , @xmath155 , @xmath156 , and @xmath157 for all @xmath158 .     for the parameters @xmath159 and @xmath149 ( @xmath160 corresponds to the x - axe ) .",
    "contours represent @xmath161 ( clear gray ) , @xmath162 ( dark gray ) and @xmath163 ( black ) confidence regions in the marginal space .",
    "[ fig : banana - target ] ]    considering such a target , we compare an iterative importance sampling algorithm that uses the classical and not the deterministic mixture version with the gaussian mixture version of the amis algorithm .",
    "the reference algorithm , called ais ( for adaptive importance sampling ) , uses the past simulations as well for creating a new gaussian mixture proposal , but with the usual importance weights .",
    "given the recent work on pmc algorithms @xcite , it can be considered as state - of - the - art for the comparison .    for both schemes ,",
    "an initial sample of @xmath164 particles is simulated from a standard logistic distribution and rescaled component - wise to ensure a maximal ess . in the following ,",
    "@xmath137 iterations and @xmath165 particles are used . the clustering step fitting a mixture to the weighted samples is solved via the mixmod software @xcite , with the number of components in the mixture being calibrated via the icl criterion @xcite during the first iteration .",
    "both schemes take approximatively the same computing time ( depending of course on the dimension @xmath97 of the problem ) and end up with weighted @xmath166 particles .",
    "note that , for @xmath122 , to maximise the ess using the nelder and mead algorithm in the initialisation step takes almost the same amount of time than @xmath137 iterations of the amis algorithm with @xmath167 particles per iteration .",
    "moreover , using the icl criterion , we found that we need to use a mixture of 4 components to correctly fit the banana shape target in two dimensions .",
    ".mean square errors calculated over 10 replications of the amis and ais schemes for different target functions for different values of @xmath97 and in parenthesis the corresponding standard errors . [ cols=\"^,^,^,^\",options=\"header \" , ]     .",
    "the total number of particles is equal to @xmath168 . ]",
    "( first line ) , @xmath169 ( second line ) and @xmath170 ( third line ) obtained by using the amis and ais schemes for @xmath171 . ]",
    "( first line ) , @xmath169 ( second line ) and @xmath170 ( third line ) obtained by using the amis and ais schemes for @xmath171 . ]",
    "( first line ) , @xmath169 ( second line ) and @xmath170 ( third line ) obtained by using the amis and ais schemes for @xmath171 . ]",
    "( first line ) , @xmath172 ( second line ) and @xmath173 ( third line ) obtained by using the amis and ais schemes for @xmath171 . ]",
    "( first line ) , @xmath172 ( second line ) and @xmath173 ( third line ) obtained by using the amis and ais schemes for @xmath171 . ]",
    "( first line ) , @xmath172 ( second line ) and @xmath173 ( third line ) obtained by using the amis and ais schemes for @xmath171 . ]",
    "the results of this experiment are reported in table [ tab : rmse ] and on figures [ fig : banana - ess][fig : banana - var ] .",
    "these results are all consistent with a domination of the amis scheme .",
    "the gain in ess is quite spectacular , but justified by the strong stabilisation brought by the amis averaging .",
    "the improvement in root mean square error shown in table [ tab : rmse ] typically varies with the target function as well as with the overall dimension @xmath97 , but may go as far as a threefold reduction .",
    "the boxplots of the absolute errors convey the same message of a uniform domination of amis in this setting .",
    "to illustrate the potential of the amis algorithm , we used it in a population genetics problem which amounts to estimate parameters of an evolutionary scenario in which two populations have diverged from a common and unknown ancestral population .",
    "data consist of the genotypes at a single microsatellite locus of 50 diploid individuals sampled from each population .",
    "this locus is assumed to evolve according to the strict stepwise mutation model ( smm ) , i.e. , when a mutation occurs , the number of repeats of the mutated gene increases or decreases by one unit with equal probability . after divergence",
    ", we also assume that populations do not exchange genes ( no migration ) . the four parameters to estimate are the three effective population sizes ( @xmath174 , @xmath175 ) and the time of divergence ( @xmath176 ) , all scaled by the mutation rate ( @xmath80 ) of the locus : @xmath177 ( = 4@xmath178 ) , @xmath179 ( = 4@xmath180 ) , @xmath181 ( = 4@xmath182 ) and @xmath183 ( = @xmath184 ) .",
    "the likelihood of such a model is costly to obtain , which is why we selected this benchmark example .",
    "we consider the bayesian paradigm .",
    "uniform priors @xmath185 $ ] and @xmath186 $ ] were chosen for parameters @xmath187 and @xmath183 .",
    "our target is the posterior distribution of @xmath188 .",
    "five data files have been simulated with the software _ diyabc _",
    "@xcite , with the following values of parameters : @xmath189 , @xmath190 , @xmath191 , and @xmath192 , leading to @xmath193 , @xmath194 , and @xmath195 .",
    "each dataset has been submitted to two types of analyses .",
    "the first analysis , used as a control , is based on an mcmc in which the gene tree of the sampled genes is updated together with the four demographic parameters .",
    "this has been performed with the software _",
    "i m _ @xcite .",
    "the second analysis combines the _ amis _ algorithm and an estimation of the likelihood based on importance sampling ( is ) for gene genealogies in the same way as @xcite embedded an is computation of the likelihood in a mcmc exploration of the parameter space .",
    "we note that the likelihood of a set of demographic parameters is computed by averaging importance weights of gene trees simulated event by event according to proposal distributions and parameter values .",
    "each gene tree is built in three steps looking backward in time : i ) between present time and time of divergence , lineages are coalesced or mutated following stephens and donnelly s algorithm ( 2000 ) @xcite , monitoring times of events as in @xcite , ii ) at time of divergence , remaining lineages of both populations are merged and iii ) after divergence , the gene tree is completed according to the _ sdpac _ algorithm of @xcite .    to assess the repeatability of both methods , each analysis was repeated four times ( that means with four different random seeds for each dataset ) .",
    "each mcmc ( _ i m _ ) was run as a single chain of @xmath196 updates after a burn in period of @xmath197 updates .",
    "the is - amis was run with @xmath139 , @xmath140 and @xmath198 .",
    "we do not used an optimisation procedure based on the ess to calibrate the initial importance function : we simply used the prior distribution . indeed , here , the prior distribution is sufficiently concentrated and there is no difficulty in finding the right region of space .    both methods provided similar outputs as shown on figure [ fig : popgen ] , thus validating the is - amis approach .",
    "however the major conclusion of this study , whereas each mcmc run lasted about 2 hours , the is - amis lasted only around 20 min with a slightly better repeatability in that mcmc outputs are often more variable .",
    "the calculation of the likelihood function of those population genetics models has a non - negligible cost .",
    "we use here an importance sampling approximation as in @xcite and the cost of such an approximation considerably increases with the number of simulated gene trees .",
    "this type of models is then adequate for the adoption and the development of the amis algorithm : all the particles simulated during the process are recycled , which minimises the number of calls to the likelihood function . due to this recycling process , the amis algorithm can not be easily compared to other adaptive importance sampling schemes which do not naturally involve any recycling procedure .",
    "for 5 simulated datasets obtained through is - amis ( continuous line ) and mcmc ( dashed line ) . each analysis has been repeated four times to evaluate the impact of repeatability . ]",
    "we have investigated in this paper an adaptive importance sampling method that extends the scope of the original deterministic multiple mixture approach of @xcite in that the sequence of importance proposals builds on the samples produced so far .",
    "the generality of the amis algorithm is that it offers a super - efficiency compared with other adaptive importance sampling techniques by allowing for an integral recycling of the past simulations .",
    "it thus provides a scope for processing those heterogeneous simulations as a whole and for treating the computing cost @xmath88 as a single entity .",
    "even though we were unable to achieve a complete generality in the convergence properties of the method , we believe this is one of the few available algorithms that converge in @xmath15 rather than in @xmath49 at each iteration @xmath24 .",
    "the authors are grateful to many colleagues for helpful discussions on the convergence properties of the amis algorithm , in particular to olivier capp , pierre del moral , randal douc , nadia oujdane , judith rousseau and vivek roy .",
    "the authors wish to thank the associate editor for its encouraging comments and two reviewers whose suggestions were very benefit to improve the presentation of this work .",
    "this work has been supported by the agence nationale de la recherche ( anr , 212 , rue de bercy 75012 paris ) through the 2005 - 2008 projects adapmc and misgepop ."
  ],
  "abstract_text": [
    "<S> the adaptive multiple importance sampling ( amis ) algorithm is aimed at an optimal recycling of past simulations in an iterated importance sampling scheme . the difference with earlier adaptive importance sampling implementations like </S>",
    "<S> population monte carlo is that the importance weights of all simulated values , past as well as present , are recomputed at each iteration , following the technique of the deterministic multiple mixture estimator of owen and zhou ( 2000 ) . </S>",
    "<S> although the convergence properties of the algorithm can not be investigated , we demonstrate through a challenging banana shape target distribution and a population genetics example that the improvement brought by this technique is substantial .    </S>",
    "<S> * keywords : * adaptive importance sampling , sequential monte carlo , population monte carlo , particle filters , deterministic mixture weights , banana shape target , population genetics . </S>"
  ]
}