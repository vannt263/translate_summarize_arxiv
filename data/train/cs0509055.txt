{
  "article_text": [
    "classification is a machine learning task that requires construction of a function that classifies examples into one of a discrete set of possible categories .",
    "formally , the examples are vectors of _ attribute _ values and the discrete categories are the _ class _ labels .",
    "the construction of the classifier function is done by training on preclassified instances of a set of attributes .",
    "this kind of learning is called _ supervised _ learning as the learning is based on labeled data .",
    "a few of the various approaches for supervised learning are artificial neural networks , decision tree learning , support vector machines and bayesian networks @xcite .",
    "all these methods are comparable in terms of classification accuracy .",
    "bayesian networks are especially important because they provide us with useful information about the structure of the problem itself .",
    "one highly simple and effective classifier is the naive bayes classifier @xcite .",
    "the naive bayes classifier is based on the assumption that the attribute values are conditionally independent of each other given the class label .",
    "the classifier learns the probability of each attribute @xmath0 given the class @xmath1 from the preclassified instances .",
    "classification is done by calculating the probability of the class @xmath1 given all attributes @xmath2 .",
    "the computation of this probability is made simple by application of bayes rule and the rather naive assumption of attribute independence . in practical classification problems ,",
    "we hardly come across a situation where the attributes are truly conditionally independent of each other . yet",
    "the naive bayes classifier performs well as compared to other state - of - art classifiers .",
    "an obvious question that comes to mind is whether relaxing the attribute independence assumption of the naive bayes classifier will help improve the classification accuracy of bayesian classifiers . in general",
    ", learning a structure ( with no structural restrictions ) that represents the appropriate attribute dependencies is an np - hard problem .",
    "several authors have examined the possibilities of adding arcs ( augmenting arcs ) between attributes of a naive bayes classifier that obey certain structural restrictions .",
    "for instance , friedman , geiger and goldszmidt @xcite define the tan structure in which the augmenting arcs form a tree on the attributes .",
    "they present a polynomial time algorithm that learns an optimal tan with respect to mdl score .",
    "keogh and pazzani @xcite define augmented bayes networks in which the augmenting arcs form a forest on the attributes ( a collection of trees , hence a relaxation of the structural restriction of tan ) , and present heuristic search methods for learning good , though not optimal , augmenting arc sets .",
    "the authors , however , evaluate the learned structure only in terms of observed misclassification error and not against a scoring metric , such as mdl .",
    "sacha in his dissertation ( unpublished , @xmath3 ) , defines the same problem as forest augmented naive bayes ( fan ) and presents polynomial time algorithm for finding good classifiers with respect to various quality measures ( not mdl ) .",
    "the author however , does not claim the learned structure to be optimal with respect to any quality measure .    in this paper , we present a polynomial time algorithm for finding optimal augmented bayes networks / forest augmented naive bayes with respect to mdl score . the rest of the paper is organized as follows . in section 2 ,",
    "we define the augmented bayes structure .",
    "section 3 , defines the mdl score for bayesian networks .",
    "the reader is referred to the friedman paper @xcite for details on mdl score , as we present only the necessary details in section 3 .",
    "section 4 provides intuition about the problem and section 5 and 6 present the polynomial time algorithm and prove that its optimal .",
    "the augmented bayes network ( abn ) structure is defined by keogh and pazzani @xcite as follows :    * every attribute @xmath0 has the class attribute @xmath1 as its parent . *",
    "an attribute @xmath0 may have at most one other attribute as its parent .",
    "note that , the definition is similar to the tan definition given in @xcite .",
    "the difference is that whereas tan necessarily adds @xmath4 augmenting arcs ( where @xmath5 is the number of attributes ) ; abn adds any number of augmenting arcs up to @xmath4 .",
    "figure 1 shows a simple abn .",
    "the dashed arcs represent augmenting arcs .",
    "note that attributes @xmath6 and @xmath7 in the figure do not have any incoming augmenting arcs .",
    "thus the abn structure does not enforce the tree structure of tan , giving more model flexibility .",
    "in this section we present the definitions of bayesian network and its mdl score .",
    "this section is derived from the friedman paper @xcite .",
    "we refer the reader to the paper @xcite for more information as we only present the necessary details .",
    "a bayesian network is an annotated directed acyclic graph ( dag ) that encodes a joint probability distribution of a domain composed of a set of random variables ( attributes ) .",
    "let @xmath8 be a set of @xmath5 discrete attributes where each attribute @xmath9 takes values from a finite domain .",
    "then , the bayesian network for @xmath10 is the pair @xmath11 , where @xmath12 is a dag whose nodes correspond to the attributes @xmath13 and whose arcs represent direct dependencies between the attributes . the graph structure @xmath12 encodes the following set of independence assumptions : each node @xmath9 is independent of its non - descendants given its parents in @xmath12 .",
    "the second component of the pair @xmath14 contains a parameter @xmath15 for each possible value @xmath16 of @xmath9 and @xmath17 of @xmath18 .",
    "@xmath19 defines a unique joint probability distribution over @xmath10 defined by : @xmath20 the problem of learning a bayesian network can be stated as follows .",
    "given a _ training set _",
    "@xmath21 of instances of @xmath10 , find a network that best fits @xmath22 .",
    "we now review the _ minimum description length _ ( mdl ) @xcite of a bayesian network . as mentioned before , our algorithm learns optimal abns with respect to mdl score .",
    "the mdl score casts learning in terms of data compression .",
    "the goal of the learner is to find a structure that facilitates the shortest description of the given data @xcite .",
    "intuitively , data having regularities can be described in a compressed form . in context of bayesian network learning",
    ", we describe the data using dags that represent dependencies between attributes .",
    "a bayesian network with the least mdl score ( highly compressed ) is said to model the underlying distribution in the best possible way .",
    "thus the problem of learning bayesian networks using mdl score becomes an optimization problem .",
    "the mdl score of a bayesian network @xmath19 is defined as @xmath23 where , @xmath24 is the number of instances of the set of attributes , @xmath25 is number of parameters in the bayesian network @xmath19 , @xmath5 is number of attributes , and @xmath26 is the mutual information between an attribute @xmath0 and its parents in the network . as per the definition of the abn structure , the class attribute does not have any parents .",
    "hence we have @xmath27 .",
    "also , each attribute has as its parents the class attribute and at most one other attribute .",
    "hence for the abn structure , we have @xmath28 the first term on r.h.s in equation ( 2 ) represents all attributes with an incoming augmenting arc .",
    "the second term represents attributes without an incoming augmenting arc .",
    "consider the chain law for mutual information given below @xmath29 applying the chain law to the first term on r.h.s of equation ( 2 ) we get @xmath30 for any abn structure , the second term of equation ( 4 ) - @xmath31 is a constant .",
    "this is because , the term represents the arcs from the class attribute to all other attributes in the network , and these arcs are common to all abn structures ( as per the definition ) . using equations ( 1 ) and ( 4 ) , we rewrite the non - constant terms of the mdl score for abn structures as follows @xmath32 where",
    ", @xmath33 denotes an abn structure .",
    "looking at the mdl score given in equation ( 5 ) , we present a few insights on the learning abn problem .",
    "the first term of the mdl equation - @xmath34 represents the length of the abn structure .",
    "note that the length of any abn structure depends only on the number of augmenting arcs , as the rest of the structure is the same for all abns .",
    "if we annotate the augmenting arcs with mutual information between the respective head and tail attributes , then the second term - @xmath35 represents the sum of costs of all augmenting arcs .",
    "since the best mdl score is the minimum score , our problem can be thought of as balancing the number of augmenting arcs against the sum of costs of all augmenting arcs , where we wish to maximize the total cost .",
    "the mdl score for abn structures is decomposable on attributes .",
    "we can rewrite equation ( 5 ) as @xmath36\\ ] ] where @xmath37 are the number of parameters stored at attribute @xmath0 .",
    "the number of parameters stored at attribute @xmath0 depends on the number of parents of @xmath0 in @xmath33 , and hence on whether @xmath0 has an incoming augmenting arc .",
    "since we want to minimize the mdl score of our network , we should add an augmenting arc to an attribute @xmath0 only if its cost @xmath38 dominates the increase in the number of parameters of @xmath0 .",
    "for example , consider an attribute @xmath0 with no augmenting arc incident on it .",
    "then the number of parameters stored at the attribute @xmath0 in abn will be @xmath39 , where @xmath40 and @xmath41 are the number of states of the attributes @xmath1 and @xmath0 respectively . thus @xmath42 .",
    "if now an augmenting arc @xmath43 having a cost of @xmath44 is made incident on the attribute @xmath0 , then the number of parameters stored at @xmath0 will be @xmath45 , where @xmath46 is the number of states of the attribute @xmath47 . note that the addition of the augmenting arc has increased the number of parameters of the network . since we want to add an augmenting arc on @xmath0 only if it reduces the mdl score , the following condition must be satisfied @xmath48 which is equivalent to @xmath49 note that this equivalence implies that the overall change in mdl score is independent of the arc direction . that is , adding an augmenting arc @xmath50 changes the network score identically to adding the arc @xmath51 .",
    "thus any augmenting arc is eligible to be added to an abn structure if it has a cost at least the defined threshold @xmath52 and if it does not violate the abn structure .",
    "note that , this threshold depends only on the number of discrete states of the attributes and the number of cases in the input database , and is independent of the direction of the augmenting arc .",
    "we now present a polynomial time greedy algorithm for learning optimal abn with respect to mdl score .",
    "1 .   construct a complete undirected graph @xmath53 , such that @xmath54 is the set of attributes ( excluding the class attribute ) .",
    "2 .   for each edge",
    "@xmath55 , compute @xmath56 .",
    "annotate @xmath57 with @xmath58 .",
    "3 .   remove from the graph @xmath12 any edges that have a cost less than the threshold @xmath52 .",
    "this will possibly make the graph @xmath12 unconnected .",
    "run the kruskal s maximum spanning tree algorithm on each of the connected components of @xmath12 .",
    "this will make @xmath12 a maximum cost forest ( a collection of maximum cost spanning trees ) .",
    "5 .   for each tree in @xmath12 , choose a root attribute and set directions of all edges to be outward from the root attribute .",
    "add the class variable as a vertex @xmath1 to the set @xmath54 and add directed edges from @xmath1 to all other vertices in @xmath12 .",
    "return @xmath12 .",
    "the algorithm constructs an undirected graph @xmath12 in which all edges have costs above the defined threshold @xmath52 .",
    "as seen in the previous section , all edges having costs greater than the threshold improve the overall score of the abn structure . running the maximum spanning",
    "tree algorithm on each of the connected components of @xmath12 ensures that the abn structure is preserved and at the same time maximizes the second term of the mdl score given in equation ( 5 ) .",
    "note that , if in step 3 of the algorithm the graph @xmath12 remains connected , our algorithm outputs a tan structure . in this sense",
    ", our algorithm can be thought of as a generalization of the tan algorithm given in @xcite .",
    "the next section proves that the augmented bayes structure output by our algorithm is optimal with respect to the mdl score .",
    "we prove that the abn output by our algorithm is optimal by making the observation that no optimal abn can contain any edge that was removed in step 3 of the algorithm .",
    "this is because , removing any such edge lowers the mdl score and leaves the structure an abn .",
    "consequently , an optimal abn can contain only those edges that remain after step 3 of the algorithm .",
    "if an optimal abn does not connect some connected component of the graph @xmath12 that results following step 3 , edges with costs greater than or equal to @xmath52 can be added without increasing overall mdl score until the component is spanned .",
    "hence there exists an optimal abn that spans each component of the graph @xmath12 that results from step 3 . by",
    "the correctness of kruskal s algorithm run on each connected component to find a maximum cost spanning tree , an optimal abn is found .",
    "thus the abn output by our algorithm is an optimal abn .",
    "n.  friedman and m.  goldszmidt .",
    "discretization of continuous attributes while learning bayesian networks . in",
    "_ proceedings of the thirteenth international conference on machine learning _ , pages 157165 , 1996 .",
    "e.  keogh and m.  pazzani .",
    "learning augmented bayesian classifiers : a comparison of distribution - based and classification - based approaches . in _ proceedings of the seventh international workshop on artificial intelligence and statistics _ ,",
    "pages 225230 , 1999 ."
  ],
  "abstract_text": [
    "<S> naive bayes is a simple bayesian classifier with strong independence assumptions among the attributes . </S>",
    "<S> this classifier , despite its strong independence assumptions , often performs well in practice . </S>",
    "<S> it is believed that relaxing the independence assumptions of a naive bayes classifier may improve the classification accuracy of the resulting structure . while finding an optimal unconstrained bayesian network ( for most any reasonable scoring measure ) is an np - hard problem , it is possible to learn in polynomial time optimal networks obeying various structural restrictions . </S>",
    "<S> several authors have examined the possibilities of adding augmenting arcs between attributes of a naive bayes classifier . </S>",
    "<S> friedman , geiger and goldszmidt define the tan structure in which the augmenting arcs form a tree on the attributes , and present a polynomial time algorithm that learns an optimal tan with respect to mdl score . </S>",
    "<S> keogh and pazzani define augmented bayes networks in which the augmenting arcs form a forest on the attributes , and present heuristic search methods for learning good , though not optimal , augmenting arc sets . in this paper </S>",
    "<S> , we present a simple , polynomial time greedy algorithm for learning an optimal augmented bayes network with respect to mdl score . </S>"
  ]
}