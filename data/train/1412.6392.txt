{
  "article_text": [
    "high performance computing ( hpc ) platforms are designed to be explored by intensive computing applications to obtain performance , such as seismic analysis ( for the exploration of oil and gas and natural disasters ) , weather and flooding forecasts , computational fluid dynamics , dna sequencing , simulations of electromagnetic equation , among others . they are dedicated to provide their maximum throughput ; and both applications and resources are usually fine - tuned to perform as optimally as possible for their specific challenges .",
    "companies , government laboratories , and research / educational institutions acquire such hpc resources built on clusters of supercomputers , which are typically expensive to acquire and maintain .",
    "however , there is a considerable amount of time that such environments can stay idle , while other times it can be overloaded or even has capacity limitations that , together , prevents a particular project or simulation deadline to be achieved .",
    "cloud computing , on the other hand , proposes a resource - shared model in which users allocate resources on demand from providers only when necessary in a `` pay - as - you - run '' model , spending budget only for utilized processing time .",
    "cloud was initially created to serve mostly web applications deployed on virtual machines ( vm ) that share the same physical hardware with other vms at the same time .",
    "as it matured , this model was coined public cloud and another model has emerged to deal with private data and also hpc requirements : the private cloud .",
    "it focuses on hosting computing intensive industrial and scientific workloads that deal with sensitive and large amounts of data  on demand , but in a private and specialized space .",
    "this latter model have then specific requirements that are challenging to be met by cloud providers , like the use of specialized resources ( e.g. , accelerators , gpus , infiniband , parallel file systems , etc ) , transfer and storage of large data sets , visualization , security and privacy , control , and a collaboration infrastructure .",
    "currently , providers are starting to offer private clouds with such specialized resources and this movement can eventually allows a hybrid execution , i.e. , the use of on - premise hpc clusters together with clouds for boosting the execution platform at a certain period and to allow particular projects to meet their deadlines .    we propose a method to partition tasks and data of an application in both environments in an unified view .",
    "we use a seismic application as use case . this particular application can reflect the characteristics of a broad range of scientific and industrial applications : it is computational demand , data and task parallel , communication bound , utilizes solvers for differential equations , produces large amounts of data to be visualized as post - processing , etc .",
    "our approach is able to monitor the application at runtime and to predict its total execution time so that it can perform a self - adaptation towards a cloud bursting , i.e. , automatically shifts part of the data from a cluster to be processed by the cloud , in an unified and synchronized way , with the goal to meet a time deadline .",
    "this article is organized as the following : section  [ sec : approach ] describes the proposed approach by emphasizing how the method performs the self - adaptation , auto - scaling , and automatic bursting , i.e. , the application adapts its execution on both platforms at runtime ; section  [ sec : study ] exemplifies the approach using a seismic application as an example and shows preliminary outcomes with a discussion ; and section  [ sec : rw ] finalizes the proposal with related work , followed by conclusions on section  [ sec : conclusion ] .",
    "let us consider an application that is data and task parallel , and which also evolves over time .",
    "initially , the user starts the application according to specific parameters : _ data _ ( domain size ) , _ tasks _ , number of _ timesteps _ , _ partitions _ of the data to be executed on - premise and in the cloud ( data can be initialized fully placed on - premise ) .",
    "the system immediately starts to monitor the execution time for each timestep .",
    "the simulation continues to execute until the system detects that the _ threshold _ time , i.e. , the _ deadline _ will not be satisfied . this situation can happen for a number of reasons , like concurrency in the local cluster , nodes down , dynamic change of the deadline , less on - premise capacity for a given problem , etc .",
    "once the `` _ _ time monitor _ _ '' detects a change in the estimated execution time , the system verifies this new estimation against the deadline time provided by the user .",
    "it is important to note that initial sequential timesteps are monitored in order to reason whether they are predictable . in most cases , these industrial and scientific applications are simulations that evolves over time by solving a number of partial differential equations represented as matrices operations .",
    "analyzing these characteristics , the timesteps can have similar execution times , but working on distinct data and , thus , being fairly predictable .",
    "if the monitoring module detects that the system will not attend the time threshold , this estimated time is used to compute the number of cores it would be needed to fit the simulation time within the deadline .",
    "this is done using a pre - processing phase to estimate the behavior of the application or it could also be an input parameter given by the user . then , the system automatically starts the re - partitioning phase to migrate part of the domain to be computed by the elastic cloud platform .",
    "figure [ fig : method ] and the following steps depict the proposed solution :    1 .   monitoring the application and analyzing estimated execution time to decide about a cloud bursting",
    "if a bursting is needed , save current state ; 3 .   compute the number of cores to be allocated in the cloud extended environment ; 4 .",
    "compute the size of domain ( part of the full domain ) to be placed in the cloud ; 5 .   move current state data to new nodes ( cloud - burst ) ; 6 .",
    "assimilate current state as initial conditions ; 7 .",
    "restart simulation at the stopped step with new configuration ( on - premise cluster and cloud ) ; 8 .",
    "synchronize the simulation at each timestep running in both environments and merge results .",
    "in order to compute the estimated deadline and for validation purposes , we empirically defined the computational behavior of the application . by executing it for several cores and nodes configuration , a function that can represent the overall behavior",
    "was estimated :    @xmath0    @xmath1    equations [ eq : logt1 ] and [ eq : logt2 ] represent the application response as a function of number of cores , respectively for the cloud environment and for an on - premise cluster platform .",
    "the coefficients a , b , d and e were empirically computed for the experimental application using a small test job ( small data set and few timesteps ) as pre - processing  see section [ sec : study ] .",
    "the number of cores needed to attend the time threshold would satisfy only the cluster environment , but since there is a possibility to place part of the domain to be executed in a cloud environment , one must consider the network delays for migrating the data and restart the application as the reduced computing performance inherent in a cloud environment when compared to the cluster .",
    "the correction factor for the difference between the performances ( cluster x cloud ) can be further defined as follows :    @xmath2    where @xmath3 is the computational performance correction factor and a , b , c and d are coefficients computed for a given application ( section [ sec : study ] ) .",
    "once the system computed the new number of cores required to attend the deadline , the correction factor can be applied :    @xmath4    where @xmath5 is the number of cores in the extended environment , @xmath6 is the estimated number of cores ( equation [ eq : logt2 ] ) and @xmath7 is number of cores available in the on - premise cluster .     is the size to be placed in a cloud ) ]    the next step is to split the domain into partitions to be executed in the hybrid environment . in order to simplify the method ,",
    "one of the two dimensions in domain ( width and height ) was fixed .",
    "as depicted in figure [ fig : split - domain ] , we chose to fix the height dimension to compute the value of @xmath8 as a function of execution time .",
    "that approach allows to define a linear relationship between the execution time ( t ) and the domain size through the value of @xmath8 :    @xmath9    where the linear coefficient * @xmath10 * and the offset * @xmath11 * must be computed for each application . *",
    "@xmath8 * must be any integer value , since it represents the number of column partitions to be placed in the extended environment .",
    "the equation [ eq : gamma1 ] can be written as a function of time ( @xmath12 ) : @xmath13 the difference between estimated total time and the time for deadline is applied in equation [ eq : gamma2 ] to compute the size of the partial domain @xmath14 to be executed by the cloud environment .",
    "the goal of this study is to analyze the approach previously described for the purpose of creating a proof - of - concept model based on the application characteristics .",
    "such analysis was carried out from the perspective of the developers of cloud bursting scientific applications targeting a seismic application .",
    "obviously , the study involves two computing environments : on - premise cluster environment and the cloud computing environment .",
    "both environments have different hardware ( _ e.g. _ , memory , cpu ) and software configurations ( _ e.g. _ , operating system ) .",
    "the cloud environment is the _ _ ibm softlayer platform__. table  [ tab : hw ] shows the hardware configuration of both environments .",
    "we have picked the cloud node configuration offer that was the most similar to the one in our local cluster for more correct evaluations .",
    ".hardware configuration of the cluster and cloud environments [ cols=\"^,^,^\",options=\"header \" , ]      to validate our assumptions , we have performed some experiments on the target application .",
    "the first equations were empirically determined through the results acquired from the simulations .",
    "the chart on figure  [ fig : timexcores ] depicts that both curves have similar behavior , but as the number of processor - cores decreases to 10 , the elapsed time for the fwi simulation showed an increase of 150% ( @xmath15 ) worse than the on - premise cluster environment . on the other hand , as the number of processor - cores increases up to 40 , the elapsed time of _ softlayer _ environment was only around 50% ( @xmath16 ) longer than the cluster environment , tending to be the same with the increasing of cores .",
    "furthermore , it is worth mentioning that these preliminary tests were executed with only a small number of cores and nodes to verify the behavior of cloud , while running a high cpu and i / o intensive application .    from the figure [ fig : timexcores ] , we defined the equation [ eq : logt1 ] and [ eq : logt2 ] as follows :        @xmath17    @xmath18    @xmath19    by solving equations [ eq : logt1 _ ] to [ eq : gamma2 _ ] for @xmath6 , we can dynamically compute the value of @xmath5 in equation [ eq : newc ] and * @xmath8 * in [ eq : gamma2 _ ] .",
    "those values are , respectively , the number of cores and the size of the domain placed in the cloud environment . after applying the movement ,",
    "the system is now capable to execute the simulation and satisfy the deadlines .",
    "this process is repeated while the system detects that the deadline time limit would not be attended for any reason , thus delaying the execution .      in this research , we propose a method that allows to dynamically move part of a domain to cloud environments .",
    "in addition , the benefit of running the numerical solver in a hybrid infrastructure ( cluster and a private cloud ) showed interesting behavior when we forced the local hpc cluster to exhaust its limited resources . besides application performance gains",
    ", the proposed self - adaptive method reduces the time span needed for the `` time to production '' infrastructure , since there is no need to wait the acquisition , installation , and setup processes as would be with in acquiring new local resources , i.e. , the co - existence in a balanced way of both platforms can bring better price - performance to such applications .",
    "we performed the simulations over a few number of configurations and load - balancing strategies .",
    "the graph provided by figure [ fig : timexelements ] shows the application response for several grid sizes while holding one dimension constant . such an experiment allowed to understand how the domain split strategy would work when migrating part of the domain to an external environment",
    "this study ",
    "execution time when varying processor cores  allowed to understand how to provision the cloud cores for a given application size and deadline .",
    "we then observed that the inverse process would also be an interesting way to understand how the execution time can be extrapolated to a specific number of cores .",
    "the self - adaptive strategy is based on the equations [ eq : logt1 ] to [ eq : logt2 ] , which allows to determine a few parameters to burst the application towards the cloud .",
    "those parameters , _ viz . _ @xmath8 and @xmath5 , are sufficient to define the external infrastructure .    , width=321,height=245 ]",
    "the overhead of the monitoring and partitioning components can be neglected .",
    "results show that total message size is only @xmath20 size .",
    "the second - level partitioning strategy is based on stripes , which reduces the amount of communications between partitions and between environments .",
    "it is important to note that the chosen load - balancing method over cores inside an environment is the simple greedy - based algorithm , where the striped - partitions are assigned to processor cores as long as they are available in each node .",
    "this choice reduces the inter - nodal communications as the neighbors partitions tend to be assigned to only one cluster node . in our case study , shots are independent tasks and traces have dependencies inside a shot .",
    "on the other hand , the overhead caused by saving the actual state ( checkpoint ) , the transferring corresponding data to the cloud , and the provision the cloud nodes need to be accounted .",
    "these values are not neglected and need to be considerable shorter than letting the application to finish its execution on the local cluster .",
    "although we need to carefully analyze these measurements , in a seismic application  which usually takes months to produce the final result  such automatic _ checkpoint - restart _ process is cost - effective as the total execution time of this application is inferred in the beginning of execution .",
    "we understand that this assumption would cause an inaccurate estimation and we are extending this work to measure and include such overhead as an offset value in equations [ eq : logt1 ] and [ eq : logt2 ] .    despite of the potential feature",
    "for the system to be elastic , in the sense that infrastructure can expand and shrink as needed , the current approach just computes and applies expansions on the cloud environment .",
    "further studies and advances on this system can even consider to reduce the amount of processors when the deadline threshold can be achieved with reduced infrastructure .",
    "finally , the proposed method can be incorporated in a framework to be reused in different applications .",
    "for that , one needs to investigate the applications behavior before specifying the coefficient values as described in section [ sec : approach ] . considering that we simplified the mesh partitioning by fixing one of the dimensions ,",
    "we are planing to extend this work by investigating the full 2d mesh partitioning and also full 3d numerical solvers .",
    "high performance computing applications are being tested on cloud platforms .",
    "works like @xcite , @xcite and @xcite performed a performance evaluation of a set of benchmarks and complex hpc applications on a range of platforms , from supercomputers to clusters , both in - house and in the cloud .",
    "these studies show that a cloud can be effective for such applications mainly in the case of complementing supercomputers using models such as cloud burst and application - aware mapping to achieve significant cost benefits .",
    "although these findings do not propose an automatic and adaptive approach for using both environments , their empirical studies opened the opportunity for proposals of tools that promote a hybrid approach based on these environments , like ours .",
    "analyzing cloud as stand alone execution platform for hpc applications , like seismic , the authors of @xcite evaluated the linpack workload on the amazon ec2 cloud .",
    "their conclusions indicate that the tested cloud environment has a potential , but it is not mature to provide a price - performance for hpc applications . @xcite also evaluated the ec2 for a number of kernels used by hpc applications , coming also to a conclusion that such cloud services need an order of magnitude in performance improvement to better serve the scientific community .",
    "it is hard to evaluate one provider or another , but they are evolving to offers that are private and with specialized infrastructures , like with gpus and infiniband . in our study , we evaluated the _ softlayer _",
    "cloud ( virtualized nodes ) and preliminary results indicate the environment as cost - effective in budget and performance when at least combined with on - premise clusters in a dynamic changing scenario .",
    "more recently , @xcite evaluated a computational fluid dynamics application over a heterogeneous environment of a cluster and the ec2 cloud .",
    "the results indicated that there is a need to adjust the cpu power ( configuration ) and workload by means of load - balancing .",
    "we are in line with this study and went further with the present work  a dynamic self - adaptive method for application load - balancing over a hybrid platform composed of cluster and cloud .",
    "we presented a first step towards a framework for self - adaptation of industrial and scientific applications in terms of being executed on a hybrid and heterogeneous environment composed of on - premise hpc clusters and the cloud .",
    "it dynamically monitors and reasons when to migrate part of the computation from one platform to the other at runtime to maximize performance and meet deadline constraints .",
    "we demonstrated the core method applied to a seismic application , which is data and task parallel and could reflect the behavior of a broad range of industrial and scientific applications .",
    "the method is based on a self - adaptation of the application at runtime to shift part of the computation to the cloud when the prediction of the total execution time is identified to overcome the given execution deadline ( which could also change dynamically ) .",
    "the approach prevents additional expensive capital expenditure and has an intrinsic overhead , but due to the on - demand cloud elasticity of nodes provisioning , the needed amount of nodes can be aggregated to achieve the deadline .",
    "ongoing work is based on the tool refinement , including overheads detection and shaping into a framework to be incorporated on the ibm s _ softlayer _ private and public clouds and/or even on the _ _ bluemix platform _ _ as a service targeting fwi as a service",
    ". moreover , the roadmap includes to work with data from a real seismic exploration field acquisition and therefore a more robust cluster capable of processing the problem .",
    "also , we intend to proceed with the analysis of nodes composed by cpu and gpu , thus including another level of heterogeneity inside the execution nodes ( i.e. , dynamically scheduling over the processors , like cpu and gpu , as reported in @xcite ) .    finally , the approach presented here at first sight might seem like a counter - intuitive idea : replace a straightforward embarrassingly parallel implementation of a challenging real world problem with a more sophisticated , self - adaptive , auto - scaled , synchronized , communication intense , domain partitioned approach .",
    "however , we have shown that a seamless combination of on - premise clusters with the cloud has an important contribution to boost the performance of computing intensive hpc applications with better price - performance and time - to - production .",
    "this work has been partially supported by finep / mcti under grant no . 03.14.0062.00 .",
    "a.  gupta , l.  v. kale , f.  gioachin , v.  march , c.  h. suen , b .- s .",
    "lee , p.  faraboschi , r.  kaufmann , and d.  milojicic .",
    "the who , what , why and how of high performance computing applications in the cloud . in _ proceedings of the ieee international conference on cloud computing technology and science ( cloudcom ) _ , 2013 .",
    "s.  ostermann , a.  iosup , n.  yigitbasi , r.  prodan , t.  fahringer , and d.  epema . a performance analysis of ec2 cloud computing services for scientific computing . in",
    "international conference on cloud computing ( cloudcomp09)_. 2010 .",
    "a.  binotto , m.  wehrmeister , a.  kuijper and c.  pereira .",
    "sm@rtconfig : a context - aware runtime and tuning system using an aspect - oriented approach for data intensive engineering applications .",
    ", 21(2):204217 , 2013 ."
  ],
  "abstract_text": [
    "<S> high intensive computation applications can usually take days to months to finish an execution . during this time </S>",
    "<S> , it is common to have variations of the available resources when considering that such hardware is usually shared among a plurality of researchers / departments within an organization . on the other hand </S>",
    "<S> , high performance clusters can take advantage of cloud computing bursting techniques for the execution of applications together with on - premise resources . in order to meet deadlines </S>",
    "<S> , high intensive computational applications can use the cloud to boost their performance when they are data and task parallel . </S>",
    "<S> this article presents an ongoing work towards the use of extended resources of an hpc execution platform together with cloud . </S>",
    "<S> we propose an unified view of such heterogeneous environments and a method that monitors , predicts the application execution time , and dynamically shifts part of the domain  previously running in local hpc hardware  to be computed in the cloud , meeting then a specific deadline . </S>",
    "<S> the method is exemplified along with a seismic application that , at runtime , adapts itself to move part of the processing to the cloud ( in a movement called bursting ) and also auto - scales ( the moved part ) over cloud nodes . </S>",
    "<S> our preliminary results show that there is an expected overhead for performing this movement and for synchronizing results , but the outcomes demonstrate it is an important feature for meeting deadlines in the case an on - premise cluster is overloaded or can not provide the capacity needed for a particular project .    </S>",
    "<S> [ cloud computing ] [ self - organization ] . </S>"
  ]
}