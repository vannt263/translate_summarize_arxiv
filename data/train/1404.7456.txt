{
  "article_text": [
    "many methods in machine learning require the evaluation of derivatives .",
    "this is particularly evident when one considers that most traditional learning algorithms rely on the computation of gradients and hessians of an objective function , with examples in artificial neural networks ( anns ) , natural language processing , and computer vision @xcite .",
    "derivatives in computational models are handled by four main methods :    machine learning researchers devote considerable effort for the manual derivation of analytical derivatives for a novel model they introduce , subsequently using these in standard optimization procedures such as l - bfgs or stochastic gradient descent .",
    "manual differentiation has the advantage of avoiding approximation errors and instability known to be present in numerical differentiation , but can be prone to error and labor intensive .",
    "symbolic computation methods address weaknesses of both manual and numerical methods , but often result in complex and cryptic expressions plagued with the problem of `` expression swell '' .",
    "the fourth technique , automatic differentiation ( ad ) works by systematically applying the chain rule of calculus at the elementary operator level .",
    "ad allows accurate evaluation of derivatives with only a small constant factor of overhead and ideal asymptotic efficiency . unlike the need for arranging algorithms into monolithic mathematical expressions for symbolic differentiation",
    ", ad can be applied to existing code with minimal change . owing to this ,",
    "it is an established tool in applications such as real - parameter optimization @xcite , sensitivity analysis , and probabilistic inference @xcite .    despite its widespread use in other fields ,",
    "ad has been underused , if not unknown , by the machine learning community .",
    "how relevant ad can be for machine learning tasks is exemplified by the backpropagation method for anns , modeling learning as gradient descent in ann weight space and utilizing the chain rule to propagate error values .",
    "the resulting algorithm can be obtained by transforming the network evaluation function through reverse mode ad .",
    "thus , a modest understanding of the mathematics underlying the backpropagation method gives one already sufficient basis to grasp the technique .",
    "here we review ad from a machine learning perspective and bring up some possible applications in machine learning .",
    "it is our hope that the review will be a concise introduction to the technique for machine learning practitioners .",
    "the term `` automatic differentiation '' has undertones that it is either symbolic or numerical differentiation .",
    "the output of ad is indeed numerical derivatives , while the steps in its computation do depend on algebraic manipulation , giving it a two - sided nature partly symbolic and partly numerical .",
    "let us start by stressing how ad is different from , and in some aspects superior to , these two commonly encountered techniques ( figure  [ figuredifferentiation ] ( a ) ) .    * _ ad is not numerical differentiation .",
    "_ * finite difference approximation of derivatives uses the original function evaluated at sample points . in its simplest form",
    ", it uses the standard definition @xmath0 and approximates the left - hand side by evaluating right - hand side with a small nonzero @xmath1 .",
    "this is easy to implement , but inherently prone to truncation and round - off errors .",
    "truncation tends to zero as @xmath2 ; however , at the same time , round - off increases and becomes dominant .",
    "improvements such as higher - order finite differences or richardson extrapolation do not completely eliminate approximation errors .",
    "* _ ad is not symbolic differentiation . _",
    "* one can generate exact symbolic derivatives through manipulation of expressions via differentiation rules such as @xmath3 .",
    "this perfectly mechanistic process is realized in computer algebra systems such as mathematica , maple , and maxima .",
    "symbolic results can give insight into the problem and allow analytical solutions of optima ( e.g. @xmath4 ) in which case derivatives are no longer needed .",
    "then again , they are not always efficient for run - time calculations , as expressions can get exponentially larger through differentiation ( `` expression swell '' ) .",
    "for accurate numerical derivatives , it is possible to simplify symbolic calculations by only storing values of intermediate steps in memory . for efficiency , we can interleave , as much as possible , the differentiation and storage steps .",
    "this `` interleaving '' idea forms the basis of `` forward accumulation mode ad '' : apply symbolic differentiation to each elementary operation , keeping intermediate numerical results , in lockstep with the evaluation of the original function .",
    "all computations are ultimately compositions of a finite set of elementary operations with known derivatives .",
    "combining derivatives of constituent operations through the chain rule gives the derivative of the overall composition . in table",
    "[ tableforwardadexample ] , we have the example @xmath5 represented as an _ evaluation trace _ of elementary operations  also called a wengert list . using the `` three - part '' notation of @xcite ,",
    "a trace of @xmath6 is constructed from we can also represent a given trace of operations as a data flow graph , as shown in figure  [ figuredifferentiation ] ( b ) , which makes the dependency relations between intermediate variables explicit .",
    "for computing the derivative with respect to , say @xmath7 , we associate with each variable @xmath8 a corresponding @xmath9 . applying the chain rule to each elementary operation in the forward trace ,",
    "we generate the derivative trace on the right - hand side .",
    "evaluating variables @xmath8 one by one together with @xmath10 gives us the required derivative in the final variable @xmath11 .",
    "in general , for an @xmath12 with @xmath13 independent @xmath14 as inputs and @xmath15 dependent @xmath16 as outputs , each forward pass of ad is initialized by setting the derivative of only one of inputs @xmath17 . with given values of @xmath14 , a forward run",
    "would then compute derivatives of @xmath18 .",
    "forward mode is ideal for functions @xmath19 , as all the required derivatives @xmath20 can be calculated with one forward pass .",
    "conversely , in the other extreme of @xmath21 , forward mode would require @xmath13 forward passes to compute all @xmath22 . in general , for @xmath12 where @xmath23",
    ", reverse ad is faster .      like its familiar cousin backpropagation ,",
    "reverse ad works by propagating derivatives backward from an output .",
    "it does this by supplementing each @xmath8 with an adjoint @xmath24 representing the sensitivity of output @xmath16 to @xmath8 .",
    "derivatives are found in two stages : first , the original function is evaluated _ forward _ , computing @xmath8 that will be subsequently needed .",
    "second , derivatives are calculated in _",
    "reverse _ by propagating @xmath25 from the output to the inputs . in table",
    "[ tablereverseadexample ] , the backward sweep of adjoints on the right - hand side starts with @xmath26 and we get both derivatives @xmath27 and @xmath28 in just one reverse sweep .",
    "an advantage of reverse mode is that it is significantly less costly to evaluate than forward mode for functions with a large number of input variables  at least , in terms of operation count . in the case of @xmath21 , only one application of reverse mode",
    "would be sufficient to compute all partial derivatives @xmath29 , compared with the @xmath13 sweeps that forward mode would need .",
    "in general , for an @xmath12 , if @xmath30 is required for evaluating @xmath31 , the time it takes to calculate the @xmath32 jacobian by forward ad is @xmath33 , whereas the same can be done via reverse ad in @xmath34 .",
    "that is to say , reverse mode ad performs better when @xmath35 . on the other hand ,",
    "forward ad has only a constant factor overhead in space , while reverse ad requires storage of intermediate results which increases its space complexity .",
    "machine learning applications where computation of derivatives is necessary can include optimization , regression analysis , anns , support vector machines , clustering , and parameter estimation .",
    "let us examine some main uses of derivatives in machine learning and how these can benefit from the use of ad",
    ".    given a function @xmath21 , classical gradient descent has the goal of finding a ( local ) minimum @xmath36 via updates @xmath37 , where @xmath38 is the step size .",
    "these methods make use of the fact that @xmath31 decreases steepest if one goes in the direction of the negative gradient . for large @xmath13 , reverse mode ad provides a highly efficient and exact method for gradient calculation , as we have outlined .    more sophisticated quasi - newton methods , such as the bfgs algorithm and its variant l - bfgs , use both the gradient and the hessian @xmath39 of a function . in practice ,",
    "the full hessian is not computed but approximated using rank - one updates derived from gradient evaluations .",
    "ad can be used here for efficiently computing an exact hessian - vector product @xmath40 , via applying forward mode on a gradient found through reverse mode .",
    "thus , @xmath40 is computed with @xmath41 complexity , even though @xmath39 is a @xmath42 matrix @xcite .",
    "hessians arising in large - scale applications are typically sparse .",
    "this sparsity , along with symmetry , can be exploited by ad techniques such as elimination on computational graph of the hessian @xcite or matrix coloring and compression @xcite .",
    "another approach for improving the asymptotic rate of convergence of gradient methods is to use gain adaptation methods such as stochastic meta - descent ( smd ) , where stochastic sampling is introduced to avoid local minima .",
    "an example using smd with ad is given by @xcite on conditional random fields ( crf ) , for the probabilistic segmentation of data .",
    "in anns , training is an optimization task with respect to the set of weights , which can in principle be attacked via any method including stochastic gradient descent or bfgs @xcite . as we have pointed out , the highly successful backpropagation algorithm is a special case of reverse mode ad and there are instances in literature  albeit few  where anns are trained with explicit reference to ad , such as @xcite using ad for large - scale feed - forward networks , and @xcite where ad is used to train an ann - based pid controller . beyond backpropagation ,",
    "the generality of ad opens up new possibilities .",
    "an example is given for continuous time recurrent neural networks ( ctrnn ) by @xcite , where ad is used for training ctrnns predicting dynamic behavior of nonlinear processes in real time .",
    "ad is used to calculate derivatives higher than second order , resulting in significantly reduced network training times as compared with other methods .    in computer vision , first and second order derivatives play an important role in tasks such as edge detection and sharpening @xcite .",
    "however , in most applications , these fundamental operations are applied on discrete functions of integer coordinates , approximating those derived on a hypothetical continuous spatial image function . as a consequence",
    ", derivatives are approximated using numerical differences .",
    "on the other hand , some computer vision tasks can be formulated as minimization of appropriate energy functionals .",
    "this minimization is usually accomplished via calculus of variations and the euler - lagrange equation , opening up the possibility of taking advantage of ad . in this area ,",
    "the first study introducing ad to computer vision is given by @xcite which considers denoising , segmentation , and information recovery from stereoscopic image pairs and notes the benefit of ad in isolating sparsity patterns in large jacobian and hessian matrices .",
    "@xcite use reverse ad for gpu - accelerated medical 2d/3d registration , a task concerning the alignment of data from different sources such as x - ray images or computed tomography . a six - fold increase in speed ( compared with numerical differentiation using center difference )",
    "is reported .",
    "nested applications of ad would facilitate compositional approaches to machine learning tasks , where one can , for example , perform gradient optimization on a system of many components that can in turn be internally using other derivatives or performing optimization @xcite .",
    "this capability is relevant to , e.g. , hyperparameter optimization , where using gradient methods on model selection criteria has been proposed as an alternative to the established grid search and randomized search methods .",
    "examples include the application to linear regression and time - series prediction @xcite and support vector machines @xcite .",
    "it is important to note that ad is applicable to not only mathematical expressions in classical sense , but also algorithms of arbitrary structure , including those with control flow statements ( figure  [ figuredifferentiation ] ) .",
    "computations involving if - statements , loops , and procedure calls are in the end evaluated as straight - line traces of elementary operations ",
    "conditionals turned into actual paths taken , loops unrolled , and procedure calls inlined .",
    "in contrast , symbolic methods can not be applied to such algorithms without significant manual effort .",
    "a concerted effort to generalize ad to make it suitable for a wider range of machine learning tasks has been undertaken @xcite .",
    "the resulting ad - enabled research prototype compilers generate very efficient code ( * ? ? ?",
    "* also dvl https://github.com/axch/dysvunctional-language/ ) , but these technologies are not yet available in production - quality systems .",
    "in practice , ad is used via feeding an existing algorithm into a tool , which augments it with the corresponding extra code to compute derivatives .",
    "this can be implemented through calls to a library ; as a source transformation where a given code is automatically modified ; or through operator overloading , which makes the process transparent to the user .",
    "implementations exist for most programming languages and a taxonomy of tools is given by @xcite .",
    "the ubiquity of differentiation in machine learning renders ad a highly capable tool for the field . needless to say",
    ", there are occasions where we are interested in obtaining more than just the numerical values for derivatives .",
    "symbolic methods can be useful for analysis and gaining insight into the problem domain .",
    "however , for any non - trivial function of more than a handful of variables , analytic expressions for gradients or hessians increase rapidly in complexity to render any interpretation unlikely .",
    "combining the expressive power of ad operators and functional programming would allow very concise implementations for a range of machine learning applications , which we intend to discuss in an upcoming article .",
    "r.  k. al seyab and y.  cao .",
    "nonlinear system identification for predictive control using continuous time recurrent neural networks and automatic differentiation . _ journal of process control _ , 180 ( 6):0 568581 , 2008 .",
    "j.  eriksson , m.  gulliksson , p.  lindstrm , and p.  wedin .",
    "regularization tools for training large feed - forward neural networks using automatic differentiation .",
    "_ optimization methods and software _ , 100 ( 1):0 4969 , 1998 .",
    "t.  pock , m.  pock , and h.  bischof .",
    "algorithmic differentiation : application to variational problems in computer vision .",
    "_ ieee trans . on pattern analysis and machine",
    "intelligence _ , 290 ( 7):0 11801193 , 2007 .        j.  m. siskind and b.  a. pearlmutter . using polyvariant union - free flow analysis to compile a higher - order functional - programming language with a first - class derivative operator to efficient fortran - like code .",
    "technical report tr - ece-08 - 01 , ece , purdue univ . , 2008 .",
    "s.  v.  n. vishwanathan , n.  n. schraudolph , m.  w. schmidt , and k.  p. murphy . accelerated training of conditional random fields with stochastic gradient methods . in _ proceedings of the 23rd international conference on machine learning ( icml 06 ) _ , pages 96976 , 2006 .",
    "w.  yang , y.  zhao , l.  yan , and x.  chen .",
    "application of pid controller based on bp neural network using automatic differentiation method . in _ advances in neural networks _ ,",
    "volume 5264 , pages 702711 ."
  ],
  "abstract_text": [
    "<S> automatic differentiation  the mechanical transformation of numeric computer programs to calculate derivatives efficiently and accurately  dates to the origin of the computer age . </S>",
    "<S> reverse mode automatic differentiation both antedates and generalizes the method of backwards propagation of errors used in machine learning . despite this , practitioners in a variety of fields , including machine learning , have been little influenced by automatic differentiation , and make scant use of available tools . </S>",
    "<S> here we review the technique of automatic differentiation , describe its two main modes , and explain how it can benefit machine learning practitioners . to reach the widest possible audience </S>",
    "<S> our treatment assumes only elementary differential calculus , and does not assume any knowledge of linear algebra .    automatic differentiation , machine learning , optimization </S>"
  ]
}