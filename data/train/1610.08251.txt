{
  "article_text": [
    "here we provide further details for the results presented in the main text .",
    "the structure of this appendix follows the structure of the main text , but we provide an outline for the benefit of the reader . in section [ qaep ] we introduce the tested quantum agent - environment paradigm , give detailed proofs of the results presented in the main text . in section [ environment - oracles ] we give details on the oracular instantiations of task environments , and give constructions for all oracles used later . following this , in section [ improvements ] we give a detailed specification of luck - favoring settings , and give a detailed statement and proof of theorem 1 from the main text , along with further discussions .",
    "finally , in section [ general ] we give further details on how the simpler constructions of the previous sections generalize to broader classes of task environments .",
    "parts of this appendix reproduce some of the results of an unpublished , earlier version of this work , available on arxiv @xcite .",
    "the basic components of an agent - environment paradigm @xcite are the set of percepts @xmath93 and the set of actions @xmath94 , which are interchangeably issued by the environment , and agent , respectively .",
    "we assume these sets are finite . a realized interaction up to time step @xmath17 , between the agent and the environment ,",
    "is a sequence @xmath95 of alternating percepts and actions is called _ the @xmath54step history _ of interaction . with @xmath96",
    "we denote the set of all ( in principle possible ) histories .",
    "@xmath97 denotes the set of all ( in principle possible ) histories of length @xmath17 .",
    "the agent and environment are formalized as stochastic maps with memory .    at the @xmath98 time - step , and given the elapsed history @xmath99 the behavior of the agent ( the environment ) is characterized with the maps respectively , where @xmath100 denotes the set of probability distributions over the set @xmath101 .",
    "the superscripts denote the realized history up to time step @xmath102 . to exemplify , the agent outputs , at time step @xmath17 , given percept @xmath103 and history @xmath99 , the action @xmath104 which is sampled from the distribution @xmath105 .",
    "the agent , and the environment , are defined by the sequences of the maps @xmath106 , @xmath107 indexed over the set of histories",
    ".    the agent and the environment may be stochastic . in this case , with @xmath108 we denote the probability distribution over @xmath54step histories , and with @xmath109 the distribution over all histories @xmath110 , realized by the agent @xmath9 and environment @xmath10 .",
    "the random variable @xmath108 is sometimes referred to as _ the interaction _ between the agent @xmath9 and environment @xmath10 .",
    "we will assume that the interaction begins with the environment outputting the first percept . to make this formal , we assume that the action and percept spaces contain the empty percept / action element @xmath75 , which is also the first element of any history , and the first percept of any interaction . then , given an agent and and environment , the first action output is given with @xmath111 , followed by the environment s step @xmath112 . here , with @xmath113 we mean that the element @xmath114 is distributed according to the distribution @xmath115 over the state space @xmath101 .",
    "if we require the agent to be on the move first , we will simply assume that the first percept is the empty percept @xmath75 .",
    "the definition of interaction is given recursively .",
    "the distribution @xmath116 is specified with @xmath117 where we view the characteristic maps as random variables .",
    "the indexing of the interaction starts with 2 , as the first move of the environment is defined to be the trivial percept @xmath75 .",
    "even length interactions ( i.e. ending with the agent s move ) are specified with where @xmath118}_{-2}$ ] and @xmath118}_{-1}$ ] denote the history of length @xmath119 and and history of length @xmath120 , obtained by dropping the last two elements , and the last element from @xmath121 , respectively .",
    "@xmath118}_{2t}$ ] denotes the last element of the same history .",
    "the odd length interactions are defined analogously .",
    "we do not explicitly model the rewarding step , and assume that the percept space contains a reward - specifying component , so @xmath122 , where @xmath50 is the set of possible rewards , as explained in the main text .    to extend this setting to the quantum domain ,",
    "the percepts are represented as orthogonal basis states of the percept hilbert space @xmath123 .",
    "analogously , for the action space we have @xmath124 , also satifying @xmath125 , where @xmath126 is the kronecker - delta function .",
    "the agent and the environment have internal memory : finite - sized registers @xmath36 and @xmath37 which can storine histories , so with hilbert spaces of the form @xmath127 .",
    "next , we specify the interface of the agent and the environment - that is parts of the system of the agent ( environment ) to which they both have access , in contrast to @xmath36 ( @xmath37 ) which are reserved for the agent ( environment ) exclusively .",
    "we define the unique common communication register @xmath34 - the interface - with associated hilbert space @xmath128 sufficient to represent both actions and percepts , thus @xmath129 .",
    "the actions and percepts are mutually orthogonal , so @xmath128 is isomorphic to @xmath130 .",
    "the agent ( environment ) is specified by sequences of completely positive trace preserving ( cptp ) maps @xmath131 ( @xmath132 ) acting on the concatenated registers @xmath133 ( @xmath134 ) .",
    "it will sometimes be useful to dilate the maps above to unitary maps , by using appended registers added to @xmath36 and @xmath37 if needed .",
    "an agent - environment interaction is then specified by the sequential application of the maps .",
    "unless otherwise specified , we will always assume that the initial state of the registers @xmath135 is a fixed product state .",
    "since the initial state does not correlate the agent and the environment , the actual choice of the initial state is not important , as its preparation can be subsumed in the first maps of the agent and environment .",
    "the classical setting is recovered as follows .",
    "we will call any state , which is a tensor product of percept / action basis states , _ a classical state_. probabilistic mixtures of such states ( that is , states whose density operators are convex combinations of the corresponding projectors ) are also classical states , and no other states are classical . for completeness we note that classical mixed states are defined relative to a register / system under consideration : for instance , a bell - pair state of two qubits is not classical , whereas the reduced states of both individual qubit are , as they are equiprobable mixtures of any two orthogonal states .",
    "the definition of classical states is analogous to the standard concept of computational basis states in quantum computing .",
    "the particular choice of such a basis will , in practice , depend on the particular systems forming the agent and the environment .",
    "the agent @xmath9 is _ classical _ , if for every map @xmath136 acting on @xmath137 the following holds : if the state of the register @xmath138 is of the product form @xmath139 where @xmath140 is a classical state , and @xmath141 is a classical percept state , then where @xmath142 are classical states , @xmath143 are classical action states and all @xmath144 are real . in other words ,",
    "the agent @xmath9 is classical if its maps neither generate entanglement , nor coherent superpositions of classical states , when acting on classical states .",
    "note that , in the definition above we refer to the defining maps of the agent , not their dilations - the purifying systems ( which are not parts of the agents or environments memory ) could , naturally , be entangled to both the registers @xmath36 and @xmath34 .",
    "a classical environment is defined analogously .",
    "the setting of a classical agent which interacts with a classical environment constitutes a natural starting point for a sensible definition of a classical interaction .",
    "however , for the interaction itself , the internal states of the agent / environment should not matter .",
    "thus we give the following , more general , definition of classical interaction .",
    "[ def - class - int ] the interaction between the agent @xmath9 and the environment @xmath10 is called _ a classical interaction _ if at every stage of the interaction , the state of the combined registers can be represented in the form where each @xmath145 is a convex mixture of classical states , all @xmath146 and @xmath147 are unit trace density matrices , and @xmath148 are ( real ) probabilities .",
    "given definition [ def - class - int ] , a classical interaction between an agent and the environment does not entail that the agent and the environment are internally in classical states",
    ". however , we do prohibit entanglement between the the registers @xmath149 and @xmath37 , and also coherent superpositions in the interface registers .",
    "next , we consider what , in the context of quantum agent - environment interactions , a proper analog of a history should be . in the cases where ,",
    "for instance , the states of the agent , environment and the interface are entangled , there is no straightforward analog of the classical history .",
    "intuitively , since we are dealing with quantum systems , the history should be an observable of the systems .",
    "more precisely , it should surmount to a sequence of observables , defined for all the time steps of the interaction .",
    "we formalize and and characterize a quantum history more broadly by introducing a third entity - a _ tester_.    the tester is a sequence of cptp maps @xmath150 which act on an external register @xmath40 and the communication register @xmath34 ( analogously @xmath151 ) .",
    "often we may assume that the maps are unitary ( by dilating the maps , if needed ) .",
    "in the _ tested _ interaction between an agent and the environment , the map of the tester is applied after each map of both the agent and the environment .",
    "the tester is not meant to change the dynamics of the interaction between the agent and the environment , but rather just to ` observe ' ( at least , in the case of a classical interaction ) . to this end , all the maps of the tester are controlled unitary maps satisfying where @xmath38 and @xmath152 are arbitrary unitary maps acting on ( the subsystems of ) @xmath40 , for all @xmath153 .    a _ classical tester _",
    "copies all the states of the @xmath34 register to its own .    to avoid misunderstandings , by copy we mean a unitary map which implements @xmath154 where @xmath155 is any percept or action basis state .",
    "if the duplicate is traced out , the realized map is just a classical basis measurement of the input state .",
    "a tested interaction between the agent and the environment is illustrated in fig .",
    "[ fig2 ] .     tested agent - environment interaction .",
    "note that , in general , each map of the tester @xmath156 acts on a fresh subsystem of the register @xmath40 , which is outside the control of the agent or the environment .",
    "the environments and agents @xmath157 @xmath37 are subsystems of @xmath158 @xmath159 , respectively , along with the purifying registers ( possibly ) needed for the unitary representation of the maps .",
    "the maps of the tester can be assumed be unitary .",
    "each quantum `` wire '' corresponds to an arbitrary number of quantum systems ( denoted with the `` @xmath160 '' symbol on the wire ) .",
    ", scaledwidth=80.0% ]    the classical history is then recovered by the sequence of states of the register @xmath40 , relative to the classical tester .",
    "a general quantum history is given by considering the state of the register @xmath40 without placing any ( additional ) restrictions on the maps of the tester , aside from the fact that we require them to be of the ` classically controlled ' form given in eq .",
    "( [ tester2 ] ) .",
    "a few remarks are in order . in the case of stochastic classical agents ( environments ) , the agent ( environment ) will , at each time step , output a particular action ( percept ) with some probability . in the quantum model ,",
    "this will be represented by the agent outputting a convex mixture of action states , specified by the corresponding probabilities of the particular action .",
    "thus , in the setting of a classical tester @xmath161 , the state of the register @xmath40 , at time - step @xmath17 , can be expressed , in terms of the classical agent - environment interactions , as follows :    the above is exactly the classical history , defined previously , represented in the standard quantum formalism .",
    "the quantum history state @xmath162 will , in general , attain significantly different forms for different testers , and we will refer to it as _ the quantum history between agent and the environment , relative to the tester @xmath56_. in the quantum interaction case , a figure of merit for learning will be a function of a quantum history of the interaction .",
    "the presence of a classical tester changes nothing in the case of classical agents and environments .",
    "it is also not problematic for agents and environments which only have a classical interaction , which is slightly more general :    [ le - equiv ] for any agent @xmath9 and environment @xmath10 , @xmath9 and @xmath10 have a classical interaction if and only if the ( reduced ) state of the three registers of @xmath163 is the same in the presence and absence of a classical tester .",
    "@xmath164 if @xmath9 and @xmath10 have a classical interaction , by definition , at each stage of interaction , the state of the three registers @xmath165 is of the form moreover , we have that @xmath166 for @xmath167 . applying the classical tester yields the following state of @xmath168 :    where we have , for clarity , commuted the sums over @xmath169 and over @xmath170 .",
    "it is now obvious that tracing out @xmath40 just recovers @xmath171 so this implication holds .",
    "+    @xmath172 we prove this direction by induction over interaction steps .",
    "suppose the claim holds up to step @xmath173 so at that time - step , the state of the three registers is next , it is either the environmental or the agent s move .",
    "we will assume it is the agent s move , and the claim for the case of the environment s move can be shown analogously .",
    "the agent s map only sees registers @xmath137 so we can write the state of the subsequent step as now , each @xmath174 can be written as a convex combination of pure states : and each pure component @xmath175 can be decomposed w.r.t . a separable basis : where @xmath176 are classical states and @xmath177 is a percept or an action state .",
    "putting it all together we have : copying the @xmath34 register ( w.r.t . the classical basis ) , and then tracing out the copy system , reduces to eliminating all cross terms @xmath178 where @xmath179 in other words , the following must hold , in order for the state to be invariant under classical testing : where @xmath180 is the kronecker - delta .",
    "so , and by defining @xmath181 we get    the expression above is of the desired form , as soon as the sum is re - written with one running index .",
    "analogously , we obtain the claim for the environment s first move .",
    "this shows the step of the inductive proof , as the invariance under classical testing guarantees we always go from desired form states to desired form states .",
    "to finish the inductive proof , we must establish the base of the induction .",
    "however , as we have clarified before , we assume that the initial state of the registers of the agent and environment is in product form , so this is trivial .",
    "+ examples of such ` internally quantum ' agents which interact classically is , for instance , a standard ( classical input - classical output ) quantum computer , where the environment would be the users . such ( internally only ) quantum agents and environments which have a classical interaction can not offer different behaviors , compared to classical agents interacting with classical environments , relative to any tester : [ le - clas - int ] for any agent @xmath9 and environment @xmath10 , which have classical interaction ( when untested ) , there exists a classical agent @xmath182 and a classical environment @xmath183 , such that @xmath184 for any tester @xmath64 and any history length @xmath17 .",
    "this lemma essentially follows from the classical simulability of quantum mechanics .",
    "in particular , we can consider the classical agent @xmath185 and the classical environment @xmath186 , which , internally , instead of storing quantum states , store the classical descriptions of the same quantum states : if the joint system , at time step @xmath102 of the registers @xmath187 is : the corresponding state generated by @xmath185 and @xmath186 would be    where with @xmath188 $ ] we denote the numerical matrix of the quantum state @xmath189 . to clarify , the classical description @xmath188 $ ] of the quantum state @xmath189 is also a quantum state",
    ". however , note that it is always also a classical state as @xmath188 $ ] and @xmath190 $ ] are orthogonal whenever @xmath191 ( we can perfectly distinguish two distinct _ matrices _ , regardless of the fact that they may _ represent _ non - orthogonal _ quantum states _ ) .",
    "this may imply an exponential blow up in the number of registers needed , ( and in the computation time ) , but this is irrelevant in the synchronous model of agent - environment interaction .",
    "the transition to state @xmath17 is achieved by applying a map of the agent , or environment .",
    "suppose it is the agent s move , as the argument will be analogous for the environmental move case .",
    "at this point , the agent will apply a quantum map @xmath192 to its system and the register @xmath193 which maps where the particular structure is ensured by the assumption the interaction is classical .",
    "the classical agent can then be defined to apply a corresponding map mapping    which is possible because the state @xmath194 is already a classical state , as the interaction is classical .",
    "this establishes an inductive step .",
    "the basis of the induction also holds , provided that the initial state of the registers @xmath187 is a classical state , which , as we have clarified , we assume to be the case . by inspecting equations @xmath195 and @xmath196 specifying the structure of the states of the registers realized by @xmath9 and @xmath10 and the classical counterparts @xmath185 and @xmath186",
    "it is clear that the quantum histories generated by the two will be the same for all testers .",
    "in particular , this also implies that the two learning settings will have identical figures of merit , relative to any figure of merit which depends only on the history .",
    "similarly , in the presence of a classical tester , quantum improvements are also not possible :    [ le - clastest ] let @xmath9 and @xmath10 be any agent and any environment over compatible percept / action spaces .",
    "then there exists a classical agent @xmath182 and a classical environment @xmath183 , such that @xmath197 for the classical tester @xmath198 and any history length @xmath17 .",
    "adding an additional classical tester ( instead of just one ) still generates the same quantum history within the original classical tester .",
    "however , tracing out the register of the additional tester reduces the interaction of @xmath9 and @xmath10 to a classical interaction , as all non - classical terms ( off - diagonal components in the states of @xmath34 ) are removed by the trace - out .",
    "but then by lemma [ le - clas - int ] , the same quantum interaction generated by a classical tester can be achieved by a classical agent and a classical environment .",
    "note that we can not use the same argument for other testers - adding a second classical tester may change the quantum history generated by another type of tester .",
    "the results above should not be particularly surprising  classical interactions simply lack the capacity for sufficiently subtle control to allow for any quantum effects ( including speed - ups ) almost by definition .",
    "thus , we consider other types of testers to achieve improvements . in this work",
    ", we will focus on the _ sporadic _ tester , which allows for periods of untested , fully coherent interaction , followed by classically tested interaction .",
    "while this is still a restricted setting , maintaining the tester fully classical at periods will allow for a straightforward comparison between quantum and fully classical agents .    in the remainder of this appendix , we will use the term _ fully classical agent _ to refer to an agent which is classical , but also forces the interaction ( for any environment ) to be classical . here , forcing implies that , within the model , the agent always de - phases the register @xmath34 ( equivalently , registers @xmath199 ) , by e.g. classical basis measurements , whose outcomes are discarded . since , for the purposes of this work , we are interested in quantum enhancements of classical learning agents , in the next section we consider what kinds of quantum extensions classically specified environments in principle allow .",
    "suppose we are faced with a classical learning scenario , with a fully classical agent @xmath9 and an environment @xmath10 , which is , _ a priori _ unknown .",
    "we would then like to asses the properties of the interaction of a quantum agent @xmath63 , for the purposes of comparison , with the _ same _ environment @xmath43 which can now be accessed via a quantum ( not classical , in the sense of the definition we have in the previous section ) interaction .",
    "the question then is , in general , when can we consider two environments @xmath10 and @xmath200 to be ` the same ' .",
    "there are a few natural answers .",
    "the strongest notion of sameness would demand that two environments are equal , if they are specified by the same sequence of cptp maps .",
    "a weaker notion of sameness is _ equivalence relative to the tester @xmath56 _ : @xmath10 and @xmath200 are equal relative to @xmath56 if the quantum histories of @xmath10 and @xmath200 , relative to the tester @xmath56 , are the same for any agent . if two environments satisfy the stronger notion of sameness , then they are equal relative to all testers .",
    "note that all environments are equal relative to trivial testers , which apply the same map irrespective of the states in the communication register .",
    "however , since we are adopting the approach of extending classical learning scenarios to quantum for the purpose of comparison , we are interested in the following definition : two environments @xmath10 and @xmath200 are the equal _ in the classical sense _ , denoted @xmath201 if they are the equal relative to the classical tester .",
    "the above is equivalent to saying that @xmath10 and @xmath200 are the same in the sense of realizing identical classical distributions over histories for any fully classical agent .",
    "the definitions above could also be relaxed to approximate equalities ( within some distance ) by relaxing the equalities on the quantum histories ( using e.g. an approximate equality on states induced by the trace distance ) .",
    "it is easy to see that the equality in the classical sense is an equivalence relation on environments .",
    "for each environment @xmath10 we can then identify the classical equivalence class @xmath202 all the elements of the class @xmath203 share the property that the classical maps they realize ( in the sense of the classical definition of agent - environment interaction ) , in a classical interaction , are equal for all environments in the class .",
    "this sequence of classical maps ( i.e. this classical environment ) we will call _ the classical specification _ of the class @xmath203 . then we will also say an environment @xmath10 is _ only classically specified _ if only its classical specification is known .",
    "recall , in fully classical learning , classical specification is all there even is .",
    "the next simple lemma states that if only the classical specification of an environment is known , no quantum enhancement can be generically guaranteed .",
    "[ lem - extens ] let @xmath203 be the classical equivalence class for some environment @xmath10 .",
    "then there exits a quantum environment @xmath204 which prohibits any quantum improvement  that is , any possible quantum history ( relative to any tester ) can be realized with a fully classical agent and this environment @xmath205 take any environment @xmath206 and sandwich every cptp map which specifies the environment @xmath200 with a classical basis measurement of the register @xmath34 ( equivalently , @xmath207 ) .",
    "this is a new environment , @xmath208 it is clearly in @xmath203 , but it also forces a classical interaction .",
    "then by lemma [ le - clas - int ] , no quantum advantage is possible in this environment for any agent .",
    "the lemma above should be clear . with the permission of a bit of poetic license",
    ", it asserts that just putting on our  quantum eyeglasses \" , that is , acknowledging that any real system is a quantum system with quantum degrees of freedom , does not turn , for instance , a classical computer into a quantum computer .",
    "even with fully coherent quantum input , most devices ( or environments ) will have decoherence processes which prevent any true quantum dynamics on any useful scale . while this observation is straightforward , it is nonetheless relevant for our case . in",
    "what follows , we will begin by specifying an interaction between a classical environment and a quantum agent .",
    "then , we will ask whether the quantum agent could do better , if the environment can be accessed as a quantum system , and the agent is free to exploit quantum coherence .",
    "lemma [ lem - extens ] then asserts that the answer may be a trivial no , unless further assumptions are made on _ how _ the environment extends to a full quantum system .",
    "we acknowledge that , from a physics point of view , it would be more natural to consider this problem in reverse .",
    "any physical system is fundamentally quantum , and one can consider classical limits of the quantum system , rather than ` quantum extensions ' of an otherwise classical systems .",
    "however , in the spirit of the mainstream approaches to artificial intelligence , systems , and task environments are usually assumed to be classical , both in the computational tradition and in robotics .",
    "from this perspective , since we start from such classical problem , it makes sense to talk about quantum extensions , that is , quantum systems which are compatible with the given classical limit .",
    "the question of what are useful quantum extensions of classically specified _",
    "functions _ is also vital in the case of quantum computation with the aid of a quantum oracle .",
    "first , we consider the special case of deterministic environments @xmath10 where the state of the environment is re - set after exactly @xmath42 steps . in this case , all dependence of the environmental memory on the actions of the agent is lost after each block of @xmath42 steps . in other words , we can express the environment as a reversible map acting on @xmath42 moves simultaneously . for simplicity we will use bold - face fonts to denote a sequence of indexed symbols , so e.g. @xmath209 .",
    "such an @xmath42-block instantiation is given with the following expression : where @xmath210 and @xmath211 is an element of the percept set @xmath212 , and @xmath213 and @xmath214 denotes a group operation on the set @xmath215 e.g. the modulo-@xmath216 addition on the set of indices specifying the elements of the @xmath212 . here , @xmath217 is the sequence of percepts that the environment will deterministically output if the agent outputs the sequence of actions @xmath218 .",
    "the group operation can be chosen such that the induced group is of order 2 ( except for the identity element ) - if @xmath219 for some @xmath220 then the indices can be represented as binary string , and the addition is bitwise mod-2 addition .",
    "then , @xmath221 is also self - inverse ( hence also hermitian ) .    in the case only one , last , percept carries a binary reward status , then @xmath222 can easily be turned into the phase - flip oracle we used in the main text , where @xmath223 induces a global ( -1 ) phase , whenever the reward status of any percept is rewarding .",
    "this is the standard `` phase kick - back '' method .",
    "explicitly , the oracle is given by the mapping @xmath224,$ ] where @xmath225 is an arbitrary state , for any quantum state @xmath226 .",
    "a stochastic environment which deterministically re - sets after @xmath42 steps can be represented by the following cptp mapping : where @xmath227 is the probability of the environment outputting the percept sequence @xmath228 when the agent performs the sequence of actions @xmath229 .",
    "recall , in the standard classical setting , the exchange of percept - actions is interchangeable , but we can nonetheless represent this interactive process as the @xmath47 block map above . for any such stochastic map there exists a purifying unitary map which realizes the same dynamics on the reduced system , and this map can be easily constructed . if we represent the classical stochastic process as an invertible map which is also acting on a register containing a random bit - string , the purification of this process would be the corresponding unitary mapping , acting on a purification of the random bit string . in this case",
    ", the conditional mixed state of percepts @xmath230 is purified by a part of the environment , and the purification is ( up to local unitaries ) equivalent to the state then , if this purification , instead of the mixed state is returned to the agent , for the simplest case of single reward , the environment can be represented by a unitary mapping performing where the register @xmath231 represents the rewarding status , and everything is implemented reversibly .",
    "unfortunately , the state in eq .",
    "( [ stoch1 ] ) is not of the suitable form to realize the stochastic oracle assumed in the main text , as the percept register @xmath232 is , in general , entangled to the reward register @xmath231 .",
    "this can be resolved in special cases only , and to achieve the generic form of the oracle , we will introduce further assumptions on how the environment is constructed ( i.e. further specify its quantum extension ) . in particular , note that the aspect of the environment which determines the raw percept sequences can be realized by a controlled map : where @xmath233 .",
    "if we assume that the set of maps @xmath234 has a known common @xmath235 eigenstate @xmath236 then the construction of the stochastic oracle can always be achieved .",
    "this assumption is justified , e. g. if the hilbert space of the percept space is increased by one dimension to include the state @xmath236 defined to be orthogonal to the basic hilbert space @xmath237 and the unitaries @xmath234 are extended such that they act as the identity on that additional one - dimensional subspace .",
    "we note that this is essentially equivalent to assuming that the environment can be forced to just output the reward status , rather than the entire sequence . in the classical case",
    "this is a trivial assumption as we can always just ignore the percepts , but in the quantum case , this would correspond to a trace - out operation which would break the desired superposition of the reward status , so we must be more careful .    assuming , however , that such a common @xmath235 eigenstate @xmath238 is known , we can obtain the mapping which is isometrically equivalent to the stochastic oracle defined in the main text , and it can be realized in a self - inverse fashion .",
    "counting oracles can be constructed in a similar manner : starting with the reversible , self - inverse instantiation of the environment @xmath239 we append a count register and apply the operation @xmath240 which counts the total reward of the sequence : where @xmath241 denotes the total of the rewards appearing in the sequence @xmath242 overall we obtain the mapping which , as we have clarified earlier , can be hermitian , hence self - inverse . using phase - kick back again , we can achieve a reflection operator about the subspace of all sequences @xmath243 satisfying the property that the total reward is above a certain chosen value .",
    "note , the construction remains unchanged even if the rewarding set contains rewards of different magnitudes , with the understanding that the reward - carrying register is large enough to represent any sums which may appear .",
    "here we prove the main theorem from the main text in full detail .",
    "first , we give a detailed definition of luck - favoring settings .",
    "let @xmath9 be a learning model / agent and @xmath10 a legitimate ( with matching percept - action structure ) environment of a.    let @xmath15 denote a learning - related figure of merit , defined on histories and extended to distributions over histories by convex - linearity ( e.g. the average reward of a history per time - step ) .",
    "then we say that the pair @xmath244 is _ monotonically luck - favoring _ for histories @xmath245 and @xmath246 relative to the merit function @xmath15 if    where @xmath245 and @xmath246 denote two ( classical ) histories of length @xmath17 that could have been generated by an interaction of @xmath9 with @xmath10 , thus : if eq .",
    "( [ main - eq - luck ] ) holds for any two histories , then we say @xmath62 is monotonically luck favoring for all histories .    more specifically , we may be interested in the behavior for specified numbers of interactions @xmath247 .",
    "then we say that @xmath244 is _ monotonically luck - favoring _ for the merit function @xmath248 with a t - step preparation @xmath249 , followed by @xmath250 step evaluation if    a few comments regarding the definition above are in order .",
    "first , @xmath60 and @xmath251 denote agents / environments which have undergone history @xmath52 . technically speaking @xmath60",
    "can be thought of as a different agent from @xmath51 which we can write as @xmath252 that is , the agent @xmath9 which has undergone the trivial history @xmath75 , and the same holds for the environment . nonetheless , the ` raw ' agent @xmath9 and an ` experienced ' agent @xmath60 have the same percept - action structure . the assumptions around eq .",
    "( [ assumptionnotzero ] ) guarantee that the given histories could have been generated by the interaction of the agent and environment , and this technical assumption will be relevant in the construction of the quantum enhanced agent .    the basic idea behind this construction is to first use quantum access to the environment to find a rewarding sequence @xmath69 faster than a classical agent could . following this , using this sequence ,",
    "an internal simulation of a classical agent @xmath9 can be trained to produce exactly that sequence - in other words , the interaction between the environment and the agent is simulated iteratively , until , by chance alone , the agent @xmath9 ` gets lucky ' and produces the winning sequence in the first try . in luck - favoring settings",
    ", such agent will outperform an agent which was not lucky later on , by definition .",
    "however , there are a few details which me need to iron out first .    to present our result regarding the speed - up in learning in a clean form",
    ", we shall place additional assumptions on the environment @xmath10 aside from it being deterministic , single - win and fixed - time .",
    "additionally , we will assume that there is only one winning action path of the length @xmath253 where @xmath42 is also the allotted fixed time ( in this case , there is only one winning history of length @xmath42 ) .",
    "recall that , in the case of the maze environment we have described , this implies that the agent traversing the maze is always returned to the @xmath254 vertex after exactly @xmath42 steps .",
    "let @xmath20 be the size of the action space , thus @xmath255 then , the classical agent will require , on average , @xmath256 interaction steps with the environment , before encountering the winning path ( note that each ` testing ' of a particular sequence of action costs @xmath42 interaction steps ) . the quantum agent , given access to the oracular instantiation @xmath257 can achieve the same in expected time @xmath258 , using the ( randomized ) grover s algorithm @xcite .",
    "this constitutes a quadratic improvement in the exploration phase of learning , and what remains to be seen is how to embed this into the complete learning package .",
    "both the classical and the quantum agent we will now construct are situated in the same controllable environment , namely , @xmath259 .",
    "the classical agent @xmath9 has nothing to gain from quantum oracular access and its access to this environment is only via its classical instantiation @xmath10 .",
    "for the classical agent @xmath9 ( and its underlying learning model ) we will next define a corresponding quantum agent @xmath63 .",
    "following the precise specification , we will briefly comment on the basic ideas behind the construction .",
    "since @xmath9 is fixed and known , we will assume @xmath63 has black - box access to ( a simulation of ) the agent @xmath9 .",
    "in particular , @xmath63 can , internally , feed the simulation of @xmath9 with any sequence of percepts , and observe the output actions .",
    "moreover , it can always reset the simulation to the initial state as defined for the agent @xmath9 . since we are constructing @xmath63 given a classical agent @xmath51 we in principle have access to every aspect of @xmath9 ( its program , realization and specification of each characteristic map ) , but for our purposes , black - box access , and the capacity to reset will suffice . as a technical assumption , we will assume that the agent @xmath9 has a non - zero probability of hitting the rewarding sequence of actions , starting from its initial configuration .",
    "we give a formal specification of the quantum agent @xmath63 next , followed by an explanation of the purpose of each of the steps .    1 .   [ prep1 ] for the first @xmath260 time steps",
    ", @xmath63 engages in a grover - type search for the awarded sequence of actions , interacting with @xmath65 .",
    "recall that each access to the oracle incurs @xmath42 interaction steps , thus we total @xmath261 oracular queries , where @xmath153 is an integer we specify later .",
    "the agent @xmath63 succeeds in finding the winning sequence @xmath262 except with probability in @xmath263 , since the fraction of winning versus the total number of sequences is @xmath264 .",
    "recall , grover s algorithm may fail to produce the target element , but this occurs with probability less than 1/2 .",
    "iterating the algorithm @xmath153 times ensures that a failure can occur at most with an exponentially decaying probability in @xmath153 , as stated .",
    "[ prep2 ] for the next @xmath42 time - steps , the agent @xmath63 engages the non - oracularized quantum extension @xmath44 of @xmath10 ( or @xmath10 itself ) , outputs the ( classical ) actions @xmath265 sequentially and collects the unique corresponding outputs @xmath266 from the environment ( by convention , we set the first percept of the environment to be the empty percept @xmath75 ) .",
    "the entire rewarding history is this step is necessary as the oracular access , by construction , does not provide the perceptual responses of the environment .",
    "[ training ] between the time steps @xmath267 and @xmath268 , @xmath63 ` trains ' a simulation of @xmath9 internally : it runs a simulated interaction with @xmath51 by giving percepts @xmath269 .",
    "it aborts and restarts the procedure ( with a reset of the simulation of the agent @xmath9 ) until @xmath9 responds with @xmath270 . by the technical assumption we mentioned earlier",
    ", the expected time of this event is finite .",
    "the training procedure itself , for the @xmath42 time steps , is repeated sequentially , until the same winning sequence of actions of the simulated agent @xmath9 is produced @xmath271 times , again contiguously . since one sequence can be attained in finite time , so can any finite repetition of the sequence .",
    "this technicality we further explain later .",
    "+ during this time the agent @xmath63 does not communicate to the environment , and uses up no interaction rounds .",
    "[ trained ] internally , @xmath63 has a simulation of the agent @xmath272 , with and @xmath273 denotes the ( string - wise ) concatenation of histories . from this point on",
    ", @xmath63 simply forwards the percepts and rewards between the simulation and the environment .    to talk about learning properties of the defined quantum agent we need to specify the tester . to optimize our result ,",
    "we select the sporadic classical tester @xmath274 which is defined as follows :    for the first @xmath275 time - steps , with @xmath276 the sporadic tester allows for completely untested interaction . after the @xmath17 steps",
    ", the tester @xmath274 behaves as the classical tester .",
    "this finishes our specification of the quantum - enhanced learning setting , and it is illustrated in fig .",
    "[ construction ] .",
    "we now briefly clarify the purpose of the steps in the construction .",
    "the construction is designed to guarantee improvement in luck - favoring settings .",
    "steps [ prep1 ] and [ prep2 ] simply utilize grover - like search to obtain ( at least ) one winning sequence of steps in the given environment , in time quadratically faster than would be possible for a classical agent . to understand the rest of the construction , we can ignore the quantum aspects and consider how one could utilize the knowledge of an agent @xmath9 given a winning sequence , without specifying the internal model .",
    "step [ training ] aims to achieve just that - it simulates an interaction with the agent @xmath51 and resets the agent , until the desired sequence has been achieved . in quantum information terminology ,",
    "the runs of the agent @xmath9 get post - selected to the winning branch . however , the number of interactions that have been experienced to this point are ( @xmath277 times ) larger than the length of the winning sequence ( @xmath42 ) . to compensate for this , and to put @xmath9 and @xmath63 on equal footing , this ` postselection ' is iterated on a larger scale - until the agent ( by chance alone ) reproduces the winning sequence @xmath277 times in a row .",
    "we have omitted this technical detail in the main text for clarity of presentation . alternatively to this",
    ", one can consider a broader definition of luck - favoring settings , where the two histories @xmath278 and @xmath279 ( experienced by the ` lucky ' , and ` unlucky ' agent respectively ) may be of unequal lengths .",
    "but while we can argue that most reasonable environment - agent pairs are luck - favoring regarding the definition we have given , this will not be the case if the sequences can arbitrarily differ in lenght .",
    "this choice of the process of ` training ' a reinforcement learning model , given a winning sequence ( or many winning sequences ) is not crucial for our main point .",
    "however , regarding the optimization of the performance of the learning agent @xmath63 , depending on how much is known about the learning model underlying @xmath51 it should be chosen such that it maximizes the expected performance . to get further insight into the expected performance of @xmath9 versus @xmath63 ,",
    "consider the average configurations ( relative to input - output behavior ) of the agents @xmath9 and @xmath63 after the first @xmath17 steps .    concerning agent @xmath63 , after the time - step @xmath280 and except with probability @xmath263",
    ", its behavior will be identical to the behavior of @xmath281 where @xmath278 is the history containing @xmath282 successful move sequences glued together .",
    "the configuration of the classical agent @xmath9 , facing the same environment , is a bit more complicated , and what can be said is restricted by the fact that we do not specify the learning model of @xmath9 .",
    "agent @xmath9 has also undergone @xmath17 interactions with the environment , that is , @xmath282 complete epochs .",
    "the probability , however , of @xmath9 having seen at least one winning sequence ( assuming there is no prior knowledge available to the agent ) is upper bounded by the following expression : where we have taken into account the fact that the agent may ( in the optimal case ) never re - try a sequence which was not rewarded . that expression further simplifies to which decays exponentially to zero , for any fixed @xmath153 , in @xmath42 .",
    "if we , for concreteness , set @xmath283 we have that both the probability @xmath284 , and the failure probability of the quantum agent @xmath285 decays exponentially in @xmath42",
    ". thus , except exponentially small probability in @xmath42 , the quantum agent will , from time - step @xmath17 onwards behave as @xmath281 where @xmath286 has a maximal rate of rewards , whereas the classical agent will behave as @xmath287 where @xmath288 has not one rewarded percept .",
    "then , relative to any figure of merit @xmath289 which is increasing in the reward frequency ( and depends only on the rewards ) we have that @xmath290 .",
    "now , if the environment is luck - favoring , by eq .",
    "( [ main - eq - luck ] ) , from time - step @xmath17 onwards , the average performance of @xmath291 will beat the performance of @xmath272 except with exponentially small probability , relative to the classical tester .",
    "these observations form the first qualitative result , here given in full detail :    [ th1 ] let @xmath10 be a controllable environment , over action space @xmath292 , thus it is , on the agent s demand , accessible in the form @xmath259 . moreover , let @xmath10 correspond to a deterministic , fixed - time @xmath42 , single - win game , with a unique winning sequence of length @xmath42 , for the period of @xmath293 time - steps ( after which it no longer needs to be controllable , nor deterministic , fixed - time , single win ) .",
    "let @xmath9 be a learning agent such that @xmath294 are luck - favoring for all histories , relative to some figure of merit @xmath15 , which is increasing in the number of rewards in the history , and which only depends on the rewards .",
    "then there exists a quantum learning agent @xmath63 based on @xmath9 which outperforms @xmath9 in terms of @xmath15 and relative to a chosen sporadic classical tester .",
    "the above is the least one can establish .",
    "if we start specifying the scenario further , by e.g. fixing the @xmath15 to be an effective ( normalized ) counter of the rewards , then we can also consider the average number of interaction steps which the classical agent needs to perform ( relative to the quantum agents @xmath295 ) before the two agents can even in principle start achieving approximately equal behaviors in terms of the rate .",
    "note that every sensible learning agent will , given a sufficient number of steps , start producing the winning sequence every subsequent game . in this case , the rate will be maximal for all such agents .",
    "as we have clarified , the classical agent requires an average @xmath296 interaction steps ( so @xmath297 complete epochs ) , before a rewarded sequence is seen even once , on average .",
    "thus this establishes a reasonable lower bound on the order of the number of steps required for a classical agent to start approaching the performance of the quantum agent .",
    "this constitutes a quadratic improvement .",
    "however , making such claims more formal requires further specifying the underlying learning model . here , we wish to establish more general claims , and leave more specific analyses for future work .",
    "nonetheless , for concreteness , we can list examples of learning models , and task environments , where the quadratic improvement mentioned above is easy to argue .    if we additionally label each percept ( think positions , or directions of optimal moves in maze problems ) , then many well - studied reinforcement learning models ( e.g. q - learning  @xcite , policy iteration  @xcite or the more recent projective simulation  @xcite model ) , together with the maze environment ( with a unique winning path ) do form luck - favoring pairs for all histories , so theorem [ th1 ] applies . to further explain why this is the case ( but without going into the details of these learning models ) , recall that in this single - win , bounded maximal time @xmath298 case , there is only one @xmath42-length history which has a reward",
    "moreover , a rewarding percept can only appear after exactly @xmath42 interaction steps , as the game is reset after each @xmath42 steps .",
    "next , note that every history length , since it is an integer , can be written as @xmath299 with @xmath300 , for some integers @xmath301 .",
    "in such a history the last @xmath302 percepts can not be rewarding , so we can focus on histories of lengths @xmath303 .",
    "this can be interpreted as an @xmath304-fold concatenation of histories of lengths @xmath42 .",
    "each one of these @xmath304 sub - histories either has exactly one rewarding percept , or does not , and it does only if that sub - history is the unique winning sequence . in the learning models we have mentioned , applied to such an environment , for every game where a winning sequence has been executed",
    ", the probability of executing the same winning sequence typically increases .",
    "this implies that for any two histories ( independently of their length ) eq .",
    "[ main - eq - luck2 ] holds .",
    "moreover , if the environment does not change , it holds for all execution lengths , hence theorem [ th1 ] does apply .",
    "going beyond the learning models we have mentioned , it is arguable that any learning model which is _ not _ luck favoring with such a environment is a deficient learning model , as this would imply that the performance of the model ( or the agent ) does not monotonically improve as the agent encounters new short(er ) paths .",
    "in contrast , environments which are not luck - favoring with standard learning models are possible to concoct .",
    "simplest examples include malicious environments that change the rules depending on the initial success of the agent . in this case , having a low efficiency in the exploration phase may be beneficial in the long run , but such scenarios are quite artificial . in the next section , we will consider further generalizations of environments where a speed - up is possible , and in the process touch upon more reasonable ( and more general ) settings where being lucky may be not as advantageous , and comment how to deal with such settings .",
    "[ improvements ]",
    "here we give examples of how oracles which correspond to multiply rewarding environments and stochastic environments can be utilized , and provide directions in which our approach can further be developed    note that , in the main text , we have shown how stochasic environments can be mapped to oracles which present the probability of a reward being issued : where @xmath305 is a representation of an approximation of the rewarding probability .",
    "note , representing low probabilities is expensive , as each bit of the representation requires an additional layer of phase estimation .",
    "importantly , @xmath306 can be realized such that it is hermitian , so self - inverse . in section",
    "[ counting ] , we have shown how counting oracles can be implemented , which realize the mapping      in both cases , we can thus use phase - kick back to amplitude - amplify @xcite sequences of actions satisfying a desired criterion , and note that relative to most figures of merit , the expected reward of a sequence in the stochastic environment has the same operational meaning as the total reward in the multiply rewarding deterministic environments .    here",
    ", for instance , one can employ the methods of quantum optimization over discrete sets @xcite .",
    "in essence , in multiply rewarding environments ( also stochastic environments ) , we can choose a threshold of minimal ( probability of ) reward @xmath308 , and perform the amplification of all amplitudes of action sequences which yield a reward ( with probability ) of at least @xmath308 .",
    "the number of environmental oracle calls will be @xmath309 where @xmath310 is the total number of sequences and @xmath311 is the number of sequences satisfying the threshold criterion .",
    "this can be achieved either by using a randomized grover approach @xcite or using the optimal fixed - point approach of @xcite . in the latter case",
    ", we do not use phase kick back to mark the sequences but rather introduce an ancillary system and realize a ` bit - flip ' oracle , to put the abstract problem in the formulation given in @xcite .    as we have show in the main text , in the case of stochastic environments",
    ", we also must take the cost of finding the estimate of the probability of reward , and this incurs an additional cost of @xmath90 ( note , in this setting @xmath308 is a probability ) .",
    "if the minimal probability is constant for a family of environments , then this is a constant cost as well . in the multiply rewarding setting",
    ", we do not have this cost as no phase estimation is needed .",
    "thus in both cases ( stochastic with a promise on the minimal relevant reward , and multiply rewarding case ) , we can obtain one sequence satisfying the criterion that the reward is above threshold ( or occurs with probability above the threshold ) quadratically faster than through classical interaction .",
    "however , as in the basic case of single - reward deterministic environments , faster finding does not generically imply improved learning .",
    "nonetheless , it is easy to identify some settings where this follows immediately in luck - favoring settings .",
    "one example are settings where the large rewards are scarce , and nearly all moves of the agent yield a ( bounded ) small reward .",
    "more formally , whenever high ( or high - probability rewards ) occur only for a constant number ( or log - sized number , in the total number of sequences ) of sequences , whereas the low rewarding sequences occur for a fraction of sequences , we obtain an improvement in learning .",
    "this is achieved by using analogous constructions as for the single - win deterministic case .",
    "the stochastic environments where our proposed constructions help are those where action sequences can a - priori have a higher or lower probability of being rewarded . however , there are settings in which this is manifestly not the case .",
    "consider the settings where each percept is chosen uniformly at random by the environment , but once this choice is made , there is only one correct action for the agent , different for each choice of the environment . in this picture , the agent is to correctly respond to a random percept of the environment .",
    "this problem becomes more interesting when the reward is issued only after @xmath42 steps ( this is a modification of the contextual bandit problem , or the invasion game in @xcite ) . in this case",
    ", a search over just the action space does not reveal useful information , as any sequence of actions ( if we trace over the percepts of the environment ) is equally likely to yield a reward .        where the second part of the state @xmath312 purifies the otherwise stochastic dynamics , and we , for simplicity , assume the percept choice of the environment is independent from the actions .",
    "this too can be relaxed , in principle .",
    "note that applying a pauli - z to the register @xmath231 is equivalent to reflecting about the state where @xmath313 is a normalization factor .",
    "note , in the state above , the second register contains only the encodings of all percept sequences @xmath314 which , in conjunction with the action sequence @xmath218 yield a reward .",
    "finally , assume also that the overall mapping is self - inverse ( and as we have clarified earlier , there always exist realizations of the same environment where this can be enforced ) .",
    "now note that @xmath315 , where @xmath316 is such that @xmath317 constitutes a reflection about the state    also note that , on the subspace spanned by @xmath318 where the operator @xmath319 ( pauli - z applied to the third register ) constitutes a reflection about the state @xmath320 since both operations are implementable by the agent , it can perform amplitude amplification , amplifying the amplitudes of the state @xmath321 , ( approximately ) reaching it using @xmath322 oracular calls to the environment . by measuring @xmath323",
    "a quantum agent can learn one pair @xmath324 which yield a reward .",
    "this can then be iterated @xmath322 many times before any classical agent finds even one pair .",
    "the quantum agent , at that point , has @xmath322 many samples from a conditional distribution of the actions and percept pairs which get rewarded .",
    "this sample set can then be used to train the classical agent , similar to the approach we used in the proof of the main theorem .",
    "effectively , the sample suffices for a partial representation of the actual environment .",
    "we can use this representation for the repeat - until - success approach again , post - selecting the interaction between the simulation of the environment and the agent , and allowing only runs in which the agent responds in a manner compatible with the sample set .",
    "note , only in this case can we faithfully simulate the environment .    what this realizes is again a ( particularly ) lucky agent in the sense of definition of luck - favoring environments .",
    "then , by the same arguments as before , such a trained agent will outperform a classical agent in all luck favoring settings .",
    "we leave the details of this construction for future work ."
  ],
  "abstract_text": [
    "<S> the emerging field of quantum machine learning has the potential to substantially aid in the problems and scope of artificial intelligence . </S>",
    "<S> this is only enhanced by recent successes in the field of classical machine learning . in this work </S>",
    "<S> we propose an approach for the systematic treatment of machine learning , from the perspective of quantum information . </S>",
    "<S> our approach is general and covers all three main branches of machine learning : supervised , unsupervised and reinforcement learning . </S>",
    "<S> while quantum improvements in supervised and unsupervised learning have been reported , reinforcement learning has received much less attention . within our approach </S>",
    "<S> , we tackle the problem of quantum enhancements in reinforcement learning as well , and propose a systematic scheme for providing improvements . as an example , we show that quadratic improvements in learning efficiency , and exponential improvements in performance over limited time periods , can be obtained for a broad class of learning problems .    _ </S>",
    "<S> introduction. _ the field of artificial intelligence ( ai ) has lately had remarkable successes , especially in the area of machine learning @xcite . a recent milestone , until recently believed to be decades away  a computer beating an expert human player in the game of go @xcite  clearly illustrates the potential of learning machines . in parallel , we are witnessing the emergence of a new field : quantum machine learning ( qml ) , which has a further , profound potential to revolutionize the field of ai , much like quantum information processing ( qip ) has influenced its classical counterpart @xcite .    </S>",
    "<S> the evidence for this is already substantiated with improvements reported in classification and clustering @xcite problems . </S>",
    "<S> such tasks are representative of two of the three main branches of machine learning . </S>",
    "<S> the first , supervised learning , considers the problem of learning the conditional distribution @xmath0 ( e.g. , a function @xmath1 ) which assigns labels @xmath2 to data @xmath3 ( i.e. classifies data ) , based on correctly - labeled examples , called _ the training set _ , </S>",
    "<S> provided from a distribution @xmath4 the second , unsupervised learning , uses samples to identify a structure in a distribution @xmath5 e.g. , identifies clusters . </S>",
    "<S> the quantum analog of the first task corresponds to a tomography - type problem where conditional states @xmath6(states of a partition of a system , given a measurement outcome of another partition ) should be reconstructed from the measurement statistics of the joint state @xmath7 which encodes the distribution @xmath4 the unsupervised case is similar . </S>",
    "<S> the third branch , reinforcement learning ( rl ) constitutes an interactive mode of learning , and is more general . here the _ learning agent _ ( or learning algorithm ) learns how to behave correctly through the use of reinforcement signals  rewards , or punishments . </S>",
    "<S> rl has been less investigated from a quantum information perspective , although some results have been reported @xcite .    </S>",
    "<S> the key question of how quantum processing can help in learning requires us to clarify what constitutes a _ good _ learning model </S>",
    "<S> . this can be involved , but two characteristics are typically considered . </S>",
    "<S> the first is the _ computational complexity _ of the algorithm of the learner . </S>",
    "<S> the second , _ sample complexity _ , is standard for supervised learning , and quantifies how large the training set has to be , for the algorithm to learn the distribution @xmath0 . </S>",
    "<S> that is , in a tomography context , it counts the number of copies of @xmath8 required until the learning algorithm can reconstruct the states @xmath6 to desired confidence .    in rl </S>",
    "<S> , sample complexity is usually substituted by _ learning efficiency _  the number of interaction steps needed for the agent to learn to obtain the rewards with high probability . </S>",
    "<S> the recent results in qml have focused on improving computational complexity @xcite , with only few recent works considering sample complexity aspects @xcite or supervised computational learning @xcite . however , </S>",
    "<S> the broader question of how , and to what extent , ai can ultimately benefit from quantum mechanics , in general learning settings , remains largely open .    in this work </S>",
    "<S> we address this question , with emphasis on the more general , and less explored , rl setting . </S>",
    "<S> we propose a paradigm for considering qml , which allows us to better understand its limits and its power . </S>",
    "<S> using this , we present a schema for identifying settings where quantum effects can help . to illustrate how the schema works , </S>",
    "<S> we provide a method for achieving quantum improvements ( polynomial in the required number of interaction rounds and exponential improvements in success rate ) in many rl settings .    _ a paradigm for qml. _ all three learning settings fit in the paradigm of so - called learning agents @xcite , standard in the field of artificial intelligence . here </S>",
    "<S> we consider a learning agent @xmath9 ( equivalently a learning program @xmath9 ) which interacts with an unknown environment @xmath10 ( the so - called _ task environment _ , or _ problem setting _ ) via the exchange of messages , interchangeably issued by @xmath9 ( called _ actions _ </S>",
    "<S> @xmath11 ) and @xmath10 ( called _ percepts _ @xmath12 ) . in the quantum extension , </S>",
    "<S> these sets become hilbert spaces , @xmath13 @xmath14 and form orthonormal bases . </S>",
    "<S> the percept and action states , and their mixtures , are referred to as classical states . </S>",
    "<S> any figure of merit @xmath15 of the performance of an agent @xmath9 in @xmath10 is a function of the _ history of interaction _ </S>",
    "<S> @xmath16 collecting the exchanged percepts and actions . </S>",
    "<S> the history of interaction is thus the central concept in learning . </S>",
    "<S> the correct quantum generalization of the history is not trivial , and we will deal with this momentarily .    if either @xmath9 or @xmath10 are stochastic , the interaction of @xmath9 and @xmath10 is described by a distribution over histories ( of length @xmath17 ) , denoted by @xmath18 . </S>",
    "<S> most figures of merit are then extended to such distributions by convex - linearity .    to recover , e.g. , supervised learning in this paradigm , take @xmath10 to be characterized by the distribution @xmath19 where the agent is given the training set  @xmath20 labeled data points ( pairs @xmath21 ) sampled from @xmath22  as the first @xmath20 percepts . </S>",
    "<S> after this , the agent is to respond with the correct labels as actions ( responses ) to the presented percepts , which are now the _ unlabeled _ data - points @xmath3 . </S>",
    "<S> reinforcement learning is naturally phrased as such an agent - environment interaction , where the percept space also contains the reward . </S>",
    "<S> we denote the percept space including the reward status as @xmath23 ( e.g. , if rewards are binary then @xmath24 ) .    formally , the agent - environment paradigm is a two - party interactive setting , and thus convenient for a quantum information treatment of qml . </S>",
    "<S> all the existing results group into four categories @xcite : @xmath25 and @xmath26 , depending on whether the agent ( first symbol ) or the environment ( second symbol ) are classical ( @xmath27 ) or quantum ( @xmath28 ) . </S>",
    "<S> this classification is reminiscent to , but should not be confused with , the classification of quantum computational universality @xcite where @xmath29 specify whether the input / outputs of a quantum computation are classical . </S>",
    "<S> the @xmath30 scenario covers classical machine learning . </S>",
    "<S> the @xmath31 setting asks how classical learning techniques may aid in quantum tasks , such as quantum control @xcite , quantum metrology @xcite , adaptive quantum computing @xcite and the design of quantum experiments @xcite . </S>",
    "<S> here we deal with , for example , nonconvex or nonlinear optimization problems arising in quantum experiments , tackled by machine learning techniques . </S>",
    "<S> @xmath32 corresponds to quantum variants of learning algorithms @xcite facing a classical environment . figuratively speaking , </S>",
    "<S> this studies the potential of a learning robot , enhanced with a `` quantum chip '' . in @xmath26 settings , </S>",
    "<S> the focus of this work , both @xmath9 and @xmath10 are quantum systems . here , the interaction can be fully quantum , and even the question of what it means `` to learn '' becomes problematic as , for instance , the agent and environment may become entangled .    _ framework. _ since learning constitutes a two - player interaction , standard quantum extensions can be applied : the action and percept sets are represented by the aforementioned hilbert spaces @xmath33 . the agent and the environment act on a common communication register @xmath34 ( capable of representing both percepts and actions ) </S>",
    "<S> thus , the agent ( environment ) is described as a sequence of completely positive trace - preserving ( cptp ) maps @xmath35 one for each time - step  which acts on the register @xmath34 , but also a private register @xmath36 ( @xmath37 ) which constitutes the internal memory of the agent ( environment ) . </S>",
    "<S> this is illustrated in fig . </S>",
    "<S> [ fig2 ] above the dashed line .    </S>",
    "<S> the central object characterizing an interaction , namely its history , is , for the quantum case , generated by performing periodic measurements on @xmath34 in the classical ( often called computational ) basis . </S>",
    "<S> the generalization of this process for the quantum case is a _ tested interaction _ : we define the _ tester _ as a sequence of controlled maps of the form where @xmath38 and @xmath39 are unitary maps acting on the tester register @xmath40 , for all steps @xmath17 . </S>",
    "<S> the history , relative to a given tester , is defined to be the state of the register @xmath40 . </S>",
    "<S> a tested interaction is shown in fig . </S>",
    "<S> [ fig2 ] .    </S>",
    "<S> the restriction that testers are controlled maps relative to the classical basis guarantees that , for any choice of the local maps @xmath41 , the interaction between classical @xmath9 and @xmath10 remains unchanged . </S>",
    "<S> a _ classical tester _ copies the content of @xmath34 relative to the classical basis , which has essentially the same effect as measuring @xmath34 and copying the outcome . </S>",
    "<S> in other words , the interface between @xmath9 and @xmath10 is then classical . </S>",
    "<S> it can be shown that , in the latter case , for any quantum agent and/or environment there exist classical @xmath9 and @xmath10 which generate the same history under any tester ( see the appendix for details ) . in other words , classical agents can , in @xmath32 settings and , equivalently , in classically tested @xmath26 settings , achieve the same performance as quantum agents , in terms of any history - dependent figure of merit . </S>",
    "<S> thus , the only improvements can then be in terms of computational complexity .    </S>",
    "<S> _ scope and limits of quantum improvements. _ what is the ultimate potential of quantum improvements in learning ? in the @xmath32 and </S>",
    "<S> classically tested settings , we are bound to computational complexity improvements , which have been achieved in certain cases . </S>",
    "<S> improvements in learning efficiency require special type of access to the environments , which is not fully tested . </S>",
    "<S> exactly this is done in @xcite , for the purpose of improving computational complexity , with great success , as the improvement can be exponential . there , the classical source of samples is substituted by a quantum ram @xcite architecture , which allows for the accessing of many samples in superposition . such a substitution comes naturally in ( un)supervised settings , as the basic interaction comprises only two steps and is memoryless  the agent requests @xmath42 samples , and the environment provides them . </S>",
    "<S> however , in more general settings , environments are ill - suited for such quantum parallel approaches : in general , the environment stores all the actions of the agent it in its memory , never to return them again . </S>",
    "<S> this effectively breaks the entanglement in the agent s register @xmath36 , and prohibits all interference effects . </S>",
    "<S> nonetheless , for many environmental settings , it is still possible to ` dissect ' the maps of the environment , and to provide oracular variants , which we can use to help the agent learn .    _ </S>",
    "<S> an approach to quantum improvements in reinforcement learning. _ this brings us to our schema for improving rl agents . </S>",
    "<S> first , given a classical environment @xmath43 we define _ fair unitary oracular equivalents _ @xmath44 . here </S>",
    "<S> , fair is meant in the same sense as quantum oracles of boolean functions are fair analogues of classical boolean functions </S>",
    "<S>  @xmath44 should not provide more information than @xmath10 under classical access , which is guaranteed , e.g. , when @xmath44 is realizable from a _ </S>",
    "<S> reversible _ version of @xmath10 . </S>",
    "<S> second , as access to any _ quantum environment _ @xmath44 can not generically speed - up all aspects of an interaction ( e.g. , while quantum walks can find target vertices faster , the price is that the traversed path is undefined ) , we identify particular environmental properties which can be more efficiently ascertained using @xmath44 , and which are relevant for learning . </S>",
    "<S> third , we construct an improved agent which uses the properties from the previous points . </S>",
    "<S> we now illustrate our approach on a restricted scenario , for the ease of presentation , and show how the examples generalize later .    </S>",
    "<S> _ application of the framework. _ given any task environment , we can separately consider the map which specifies the next percept the environment will present in general , a stochastic function @xmath45 mapping elapsed histories onto the next percept  and the reward function . function . </S>",
    "<S> the latter is described as the map @xmath46 which also depends on the history , and complements the percept by setting its reward status . in environments which are simple and strictly epochal ( meaning </S>",
    "<S> the environment is re - set after @xmath42 steps and at most one reward is given ) , although the interaction is turn - based , it can be represented as sequences of @xmath47step maps : where the  bar \" on @xmath48 highlights that it includes a reward status . </S>",
    "<S> moreover , in deterministic environments , the maps @xmath49 and @xmath50 only depend on the actions of the agent , as the percept responses are fixed . for such deterministic , simple strictly epochal environments , </S>",
    "<S> the construction of an appropriate oracle is dramatically simplified . </S>",
    "<S> the actions can be returned to the agent after each block of @xmath42 steps , as the next block is independent . </S>",
    "<S> moreover , using phase kick - back , the reward map can be modified ( see the appendix for details ) such as to influence just the global phase of returned action states . </S>",
    "<S> this leads to the `` phase - flip '' oracle realizing this constitutes the first step of our proposed schema . </S>",
    "<S> next , we focus on step two : obtaining a useful property of the environment , and identifying settings where it provably helps </S>",
    "<S> . the constructed oracle points towards the use of grover - type search to find rewarding action sequences . </S>",
    "<S> this alone suffices for improvements only in special environments where learning reduces to searching . </S>",
    "<S> we can do better by combining fast search with a classical learning model . in canonical rl settings , </S>",
    "<S> what the agent learns ( should learn ! ) is not a correct sequence of moves per se , but rather the correct association of actions given percepts . to illustrate this , </S>",
    "<S> imagine navigating a maze where the percepts encode correct directions of movement . </S>",
    "<S> if the correct association is learned , then the agent will perform well , even when the maze changes . nonetheless , for the agent to learn the correct association </S>",
    "<S> , it first must encounter an instance of rewarding sequences , and here quantum access helps . </S>",
    "<S> thus we aim at assisting in the exploration phase of the balancing act between exploration ( trying out behaviors to find optima ) and exploitation ( reaping rewards by using learned information ) characteristic for rl @xcite . </S>",
    "<S> this idea can be made fully precise by considering the class of environments where more successful exploration phases are guaranteed to lead to a better overall learning performance . whether this is the case , however , also depends also on the learning model of the agent . thus we identify agent - environment pairs , where such better performance in the past ( in exploration ) implies better performance in the future ( on average ) , which we call _ luck - favoring settings_.    more formally , consider environments @xmath43 and agents @xmath51 such that if @xmath52 and @xmath53 are @xmath54length histories , then @xmath55 ( i.e. @xmath52 is a history with a better performance than @xmath53 ) for some future period @xmath56 . </S>",
    "<S> here @xmath57 and @xmath58 denote the environment and agent , respectively , which have undergone the history @xmath59 ( note that @xmath58 and @xmath9 are , technically , different agents ) .    </S>",
    "<S> we will say @xmath60 is luckier than @xmath61 . </S>",
    "<S> such environment - agent pairs @xmath62 , satisfying the formal conditions above , are thus luck - favoring , and we may additionally specify the periods @xmath17 and @xmath56 for which the implication ( [ eq - lf ] ) holds . </S>",
    "<S> this brings us to step three of the schema , given as a theorem .    </S>",
    "<S> * theorem 1 * _ let @xmath10 be a deterministic , strictly epochal environment . </S>",
    "<S> then there exists an oracular variant @xmath44 of @xmath10 , such that for any classical learning model @xmath9 which is luck - favoring relative to @xmath10 , and figure of merit _ rate _ which is monotonically increasing in the number of rewards in the history , we can construct a quantum agent @xmath63 such that @xmath63 , by interacting with @xmath44 , outperforms @xmath9 in terms of the figure of merit _ rate _ relative to a chosen tester . _    </S>",
    "<S> this theorem states that , in the restricted settings of determinisic epochal environments , it is possible to generically improve the learning efficiency of all learning agents , provided the environments are luck - favoring for those agents . </S>",
    "<S> we note that most reasonable learning models are luck - favoring relative to most typically considered task environments ( see the appendix for a longer discussion ) . in the statement of theorem 1 , </S>",
    "<S> we have omitted additional specifications pertaining to @xmath17 and @xmath64 but it should be understood that if the luck favoring property holds for @xmath17 and @xmath64 then the improved performance holds relative to these periods .    to prove theorem 1 we construct @xmath63 , </S>",
    "<S> given @xmath9 . </S>",
    "<S> the construction is illustrated step by step in fig . </S>",
    "<S> [ construction - main ] , where for illustrative purposes , the classical interaction of agent @xmath9 is contrasted against the quantum interaction of agent @xmath63 . </S>",
    "<S> step 1 : @xmath63 will use the quantum oracle variant of @xmath10 ( @xmath65 ) for time @xmath66 where @xmath42 is the epoch length , and @xmath67 is the number of the actions , to find a rewarding action sequence @xmath68 , using grover search . during this period </S>",
    "<S> the interaction is untested , and the interaction is fully classically tested thereafter . </S>",
    "<S> step 2 : @xmath63 will play out one epoch by outputting actions from @xmath68 sequentially , now with the classical environment , to obtain the responses of the environment ( recall , @xmath65 can not provide these ) , obtaining the entire rewarding history @xmath69 . </S>",
    "<S> thus far , @xmath63 used @xmath70 interaction steps . step 3 : @xmath63 `` trains '' an internal simulation of @xmath9 , simulating the interaction between @xmath9 and @xmath43 and restarting the simulation until the history @xmath69 occurs ( we assume such an occurrence has a non - zero probability ) . </S>",
    "<S> this may require many internally simulated interactions , but no interaction with the real environment . in step 4 </S>",
    "<S> , the internal simulation of @xmath71 corresponds to the luckiest agent possible , and @xmath63 relinquishes control to it .    </S>",
    "<S> finally , we consider what happens with @xmath9 during the same time periods . unless additional information about the environment is given , in @xmath72 steps @xmath9 has only an exponentially small ( @xmath73 ) probability of having seen the rewarding sequence . </S>",
    "<S> thus , the quantum agent is luckier than the classical , and in luck - favoring settings , this implies that @xmath63 will continue to outperform @xmath9 after the @xmath17 steps . </S>",
    "<S> the statement of theorem 1 is not quantitative , due to the generality of the definition of luck - favoring settings . </S>",
    "<S> we can , however , trade off generality for exactness . </S>",
    "<S> if an agent @xmath9 employs a variant of @xmath74greedy @xcite behavior  that is , it outputs the rewarding sequence ( exploits ) with probability @xmath75 and explores with probability @xmath76 , then the ratio of the performances of @xmath63 and @xmath9 will be exponential in @xmath42 : the constant reward probability @xmath75 of @xmath63 versus the exponentially diminishing @xmath77 of @xmath9 at step @xmath17 . </S>",
    "<S> this exponential gap holds for time - scales @xmath78 however , the improvement in terms of learning efficiency ( number of interaction steps ) is quadratic .    </S>",
    "<S> our results achieve solid improvements using simple techniques , at the cost of restricting the task environments . </S>",
    "<S> however , our example can be further generalized in two directions .    first , </S>",
    "<S> as long as the re - set occurs at step @xmath42 , multiple and multi - valued rewards can also be handled by defining oracles which reversibly count the rewards . </S>",
    "<S> highly rewarding sequences can then be found through quantum optimization techniques  @xcite , as worked out in the appendix .    </S>",
    "<S> second , under stronger assumptions on @xmath44 , using more involved quantum subroutines , we can deal with stochastic environments . for instance , in the setting with one reward per epoch , the oracle where @xmath79 is the probability of a reward , given the action sequence @xmath80 , can be constructed from a reversible implementation of the environment where randomness is represented as a subsystem of an entangled state ( see the appendix for details ) .    from here , by using phase kick - back and phase estimation the agent can realize the mapping where @xmath81 is an @xmath82bit precision estimate of the reward probability as specified by the angle @xmath83 . </S>",
    "<S> next , amplitude amplification is used to amplitude - amplify all sequences @xmath80 where the reward probability @xmath84 given sequence @xmath85 is above a threshold @xmath86    given @xmath87 such sequences ( out of @xmath88 sequences in total ) , the overall number of interactions steps multiplies @xmath42 with the amplitude amplification cost ( @xmath89 ) , and with phase estimation cost @xmath90 . </S>",
    "<S> overall , we have @xmath91 interaction steps . </S>",
    "<S> the classical agent s interaction cost of the same process is @xmath92    if the minimal relevant success probability is constant for a family of task environments , then this constitutes a quadratic improvement in finding good action sequences . </S>",
    "<S> this approach can also be generalized to a wider class of settings ( see the appendix for details ) .    in many settings , </S>",
    "<S> e.g. , robotics , the classical environments do not allow `` oracularization '' . </S>",
    "<S> nonetheless , the presented constructions can be used in model - based learning @xcite , where the agent constructs an internal representation of the environment to facilitate better learning through simulation . </S>",
    "<S> then , the `` quantum chip '' can help in speeding - up internal processing , which is the most that can be done in @xmath32 settings . </S>",
    "<S> a tantalizing exception to this may be nano - scale robots ( e.g. intelligent versions of in - situ probes in @xcite ) in future quantum experiments , as on these scales the environment is manifestly quantum and exquisite control becomes a possibility .    _ </S>",
    "<S> conclusions. _ in this work we have extended the general agent - environment framework of artificial intelligence @xcite to the quantum domain . based on this , we have established a schema for quantum improvements in learning , beyond computational complexity . using this schema , </S>",
    "<S> we have given explicit constructions of quantum - enhanced reinforcement learning agents , which outperform their classical counterparts quadratically in terms of learning efficiency , or even exponentially in performance over limited periods . </S>",
    "<S> this constitutes an important step towards a systematic investigation of the full potential of quantum machine learning , and the first step in the context of reinforcement learning under quantum interaction .    </S>",
    "<S> vd and hjb acknowledge the support by the austrian science fund ( fwf ) through the sfb foqus f 4012 , and the templeton world charity foundation grant twcf0078/ab46 . </S>",
    "<S> vd thanks christopher portmann , petros wallden and peter wittek for useful discussions which helped in parts of this work . </S>"
  ]
}