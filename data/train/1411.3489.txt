{
  "article_text": [
    "we introduce a brain - computer interface ( bci ) application based on rapid serial visual presentation ( rsvp ) of polygon primitives for image reconstruction .",
    "our paradigm relies on the decomposition of a collection of images ( figure 1 ) into a set of approximate constituent parts ( polygon primitives ) .",
    "these primitives are presented to the experimental subjects in bursts .",
    "event related potentials ( erps ) associated with the oddball paradigm @xcite are used as eeg correlates to detect those pieces that contribute to the reconstruction of the target image ( which is on the user s mind ) in an incremental fashion . the operational basis ( rsvp )",
    "has already been employed for bci - based gaze - independent spellers @xcite and an icon messenger @xcite .",
    "the results presented in this paper invite us to be optimistic about this new paradigm for bci image reconstruction .",
    "more experiments should be carried out to expand the design outside the chosen collection of images , ideally moving towards a free - painting device .",
    "some ideas are proposed based on the outcome of the current work .",
    "the experiments also suggest ways to increase the reliability , speed , and accuracy of the current framework .",
    "our work draws from recent advances in cortically - coupled computer vision ( c3vision ) @xcite where human vision is enhanced through an efficient data mining of eeg patterns while stimuli are presented .",
    "such an approach has shown to boost the search for target images , and how to speed up the localization of salient details from within a large image @xcite .",
    "even further improvement can be achieved with a _ closed loop _ philosophy in which human vision and eeg are not only coupled , but engage in a cycle where the artificial intelligence behind the eeg - based classifiers offers feedback in real time @xcite .",
    "these seminal studies provide interesting insights about the capabilities of such bcis and suggest that the limits of our own design could be pushed further .",
    "+ in @xcite we find an early approach closer to the line of research that we pursue .",
    "physiological states  including eeg and indicators such as ventilation , heart rate , and others  are correlated to ` positive ' and ` negative ' feelings elicited by visual stimuli .",
    "the ability to prompt positive reactions is then used as fitness functions for new , randomly generated images that evolve by means of a genetic algorithm .",
    "the decisions based on these cues are compared offline to the deliberate choices made by the experimental subjects who should decide what images were more artsy .",
    "these physiological correlates predict up to @xmath3 of the subjects choices .",
    "it is more complicated to assess the global goal of this research  i.e. evolving art .",
    "the main drawback of this paradigm is the extended exposure time needed to gather the relevant physiological data .",
    "subjects are exposed to pictures for one second so that neural correlates of mood can be recorded .",
    "this stretches a single generation of the genetic algorithm to @xmath4 minutes .",
    "because of this large exposure , it is likely that only coarse features of a picture are relevant for selection .",
    "this might be a problem for practical applications , if we wanted to direct the evolution towards detailed visual features . additionally , the only selective pressures are `` positive '' feelings elicited by the drawings , for which it is difficult to quantify a progress : does the last image render more positive feelings than the original one ?",
    "how long can we advance in positiveness before the physiological correlates saturate ?",
    "these thorny details , rather than flaws in the original design , reveal the difficulty of the task under consideration .",
    "while the authors of @xcite intended to explore the undetermined space of art and feelings evoked by drawings , shamlo and makeig @xcite sketch a procedure to evolve towards a definite target image .",
    "bursts of randomly generated drawings are presented to the subjects , whose eegs are recorded .",
    "a burst might or might not contain a target image that resembles `` two eyes '' .",
    "the subject presses a button after each burst to indicate if the target has been consciously spotted out .",
    "eeg patterns are extracted that correlate with the presence of the target image during a burst .",
    "the authors focus on processing the data during a posterior offline analysis . using ten fold cross - validation on data from a single trial ,",
    "very high accuracy is obtained ( up to @xmath5 area under roc curve of correctly classified target pictures ) .    encouraged by these good results , and in the spirit of c3vision",
    ", the authors propose to use their bci paradigm to bypass the slow feedback that subjects have to provide manually nowadays .",
    "one straightforward application suggested is to use this eeg activity to evolve images . currently existing software @xcite , accepts user - provided images and attempts to evolve them towards a desired target picture .",
    "random mutations and crossover are applied to the original seed to generate new drawings . in the standard approach",
    ", the user manually chooses among the new candidates that fall closer to an arbitrary goal ( e.g. `` two eyes '' as in @xcite ) .",
    "this manual procedure is cumbersome .",
    "using eeg correlates to identify what images should be fed to the algorithm could speed up the evolutionary process .",
    "unfortunately , such interesting possibility is only suggested in @xcite based on the exceptional performance of the eeg - driven classification task .",
    "actual experiments should be implemented to test the complete bci  evolutionary - algorithm loop .",
    "when this is done , it will be possible to address an important aspect of the bci design .",
    "the performance reported in @xcite refers to the identification of target images within bursts that the experimental subject _ manually _ identified as containing a target .",
    "the high accuracy of this manual selection ( @xmath6 correct classifications ) indicates that the task ( the identification of a broad feature such as `` two eyes '' ) might be simple enough .",
    "it is an open question about how accurate the procedure will be once the tasks become harder , e.g. if we would attempt to evolve more detailed visual structures .",
    "the _ p300-brain painting bci _",
    "@xcite inspired by the early p300-speller bcis @xcite is the device closer to ours in goals and performance . using the oddball paradigm , colors , shapes , and a variety of tools",
    "are selected from a matrix of highlighted rows and columns .",
    "the selected operators modify an existing canvas in a similar way that a mouse interface would do . in @xcite",
    "this bci was evaluated in healthy and als patients finding , for certain design specifications , performances comparable to those of the equivalent matrix speller .",
    "the insights from these works  in terms of speed and accuracy  are complicated to translate to ours due to the important differences in implementation .",
    "we investigate two different  albeit similar in purpose  designs , and this will complicate the comparison between bcis as discussed below .",
    "the painting tools available in @xcite are rather scarce : two shapes ( square and circle ) , eight colors , and four sizes . despite this potential limitation",
    ", the subjects can produce quite complex paintings ( figure 3 in @xcite ) .",
    "if we wanted to incorporate more shapes or colors , we might face an important limitation since the number of painting tools must fit in the symbol matrix : a larger matrix ( more tools ) will translate in lower selection accuracy .",
    "as the authors point out , this problem affects the movement of the mouse cursor too : only eight directions are allowed along which the cursor moves one single unit at a time .",
    "it is suggested to use sensorimotor rhythms to control a mouse pointer over the canvas . that would release space in the symbol matrix for other needs . despite these minor issues ,",
    "the bci in @xcite must be regarded as an important breakthrough for bci - painting .",
    "the paradigm explored in @xcite relies on physiological signals .",
    "this slows down the interface drastically , as tens of seconds are necessary to collect reliable data .",
    "but the approach in @xcite and others @xcite set an interesting precedent for our bci given the high rate at which stimuli are presented ( up to @xmath4 hz ) .",
    "we chose more conservative rates ( around @xmath7 hz ) for the proof of concept introduced here .",
    "another crucial difference between most examples in the literature and ours is that we proceed bottom - up to reconstruct a set of images : our stimuli are polygon primitives that might resemble smaller details of a target drawing , which allows for a finer grained reconstruction than those in @xcite . in these studies ,",
    "each stimulus is a whole picture and targets are based on broader features such as the existence of two eyes .",
    "this would be important , if we wanted to extrapolate the technical setup ( mainly the stimulus rate ) to our design .",
    "furthermore experiments actual image reconstructions were undertaken , thus we offer the first serious test of this novel and promising bci paradigm .",
    "+ the paper is organized as follows : in section [ sec:2 ] the proposed bci is described along with the experimental and data analysis details . in section [ sec:3 ]",
    "the results are summarized .",
    "we close with a discussion of these results ( in comparison with the existing literature ) in section [ sec:4 ] , where future lines of research are proposed together with possible improvements and alternatives to the current design .",
    "appendix [ app:1 ] includes a description of the choices made regarding the preprocessing of the images and the experimental setup .",
    "this is compared to similar bci schemes that inspired the research .",
    "appendix  [ app:2 ] analyzes in detail an anomaly found in some image reconstructions .",
    "ten participants ( five women and five men , ages ranging from early twenties to early thirties ) took part in the experiment on a voluntary , non rewarded basis .",
    "one of the subjects was associated to the bci research group .",
    "this subject and two others had previous experiences with bci experiments .",
    "the rest of the subjects were nave with respect to bci technologies .",
    "all of them had normal or corrected to normal vision and did not report any health issues during the experiment .",
    "eeg was recorded at 1000 hz using brainamp amplifiers and an acticap active electrode system with @xmath8 channels ( brain products , munich , germany ) .",
    "the electrodes used were fp1,2 , af3,4,7,8 , fz , f1 - 10 , fcz , fc1 - 6 , ft7,8 , cz , c1 - 6 , t7,8 , cpz , cp1 - 6 , tp7,8 , pz , p1 - 10 , poz , po3,4,7,8 , oz , o1,2 . all electrodes were referenced to the left mastoid using a forehead ground . for offline analyses ,",
    "electrodes were re - referenced to linked mastoids .",
    "all the impedances were kept below @xmath9 .",
    "stimuli were presented on a @xmath10  tft screen with a refresh rate of @xmath11 and a resolution of @xmath12 .",
    "the experiment was implemented in python using the open - source bci framework pyff @xcite with pygame @xcite and vision - egg @xcite .",
    "data analysis and classification were performed with matlab ( the matlabworks , natick , ma , usa ) using an in - house bci toolbox ( www.bbci.de/toolbox ) .",
    "the design of the experiment includes a pre - processing of the images to extract the primitives that are shown during rsvp bursts .",
    "important choices regarding stimulus presentation were also made . because we explore a novel paradigm , almost any design feature is open to debate . in the following ,",
    "we report the actual choices made for the experiment . for an introduction and discussion of the other possibilities the reader",
    "is referred to appendix [ app:1 ] .",
    "the nine drawings of fruits and vegetables collected in figure [ fig:1 ] were chosen as potential targets for reconstruction from the revised snodgrass and vanderwart s object database @xcite .",
    "they are easily recognizable , have bright , plain colors , and combine basic shapes with some minor details  such as the stalk of a cherry  that could act as landmarks during a reconstruction task .",
    "they are always displayed upon a blank background .",
    "similar to the role played by characters in rsvp spellers @xcite , we need nuclear units that constitute stimuli during the bursts of our bci paradigm . these stimuli must be able to reconstruct the target images as they accumulate in the screen .",
    "we submitted the chosen images to a preprocessing step during which a genetic algorithm @xcite extracted a series of primitives ( polygons ) that approximated the drawings up to a satisfactory degree ( final product shown in figure [ fig:1 ] ) .",
    "we refer to these primitives throughout the text as the _ polygon decomposition _ of each of the targets .",
    "a brief description of the genetic algorithm is found in appendix [ app:1.01 ] .",
    "the actual code that we used and the values of different parameters chosen for our implementation can be found in a public repository @xcite .",
    "the resulting decompositions contain between @xmath13 and @xmath14 polygons ( with @xmath15 on average ) depending on the target drawing .",
    "we restricted polygons to have between @xmath7 and @xmath16 vertices and solid colors ",
    "i.e. no transparency was allowed .",
    "polygons are overlaid on a blank background and on each other , thus their order matters for the reconstruction task .    the _ fitness _ function used as a selection criterion by the genetic algorithm ( see appendix [ app:1.01 ] ) is a pixel - by - pixel distance between an image and its polygon decomposition . by computing the fitness drop of a decomposition when one polygon is removed",
    ", we have a measure of the impact of each polygon in the final reconstruction .",
    "we dub this measure _ visual information _ and report it as a percentage . when running the bci experiments we only used polygons with a visual information higher than @xmath17 ( @xmath18 during the calibration phase ",
    "[ sec:2.03.02 ] ) . on average ,",
    "the polygons eventually selected for the experiments bore a visual information of @xmath19 with a standard deviation of @xmath20 .",
    "this visual information also allows us to rank the polygons within a decomposition according to their importance in the reconstruction .",
    "we used this ranking to select what target polygons are displayed in the initial bursts of the reconstruction , and we moved towards less informative polygons as the reconstruction proceeded , as explained in section [ sec:2.03.02 ] .",
    "finally , the oddball paradigm targets are displayed among allegedly neutral non - target stimuli .",
    "whenever a picture was selected for reconstruction , the polygons in its decomposition were considered target primitives .",
    "the polygons from the decomposition of all non - target images were held in a pool from which non - target primitives were drawn at the beginning of each burst ( see appendix [ app:1.01 ] for discussion ) .      the experiment consisted of a _ calibration _ phase and a _ reconstruction _",
    "phase , schematically depicted in figure [ fig:2 ] . in either phase , a drawing from figure [ fig:1 ] was selected as a target and displayed for @xmath21 seconds prior to each burst . a burst consisted of the rapid presentation of polygons that included target and non - target stimuli ",
    "i.e. primitives from the decomposition of the target and non - target images respectively . for each burst ,",
    "@xmath21 non - target polygons were selected for display together with the ( one ) corresponding target polygon .",
    "a burst involves a number of blocks that may be different for the calibration and reconstruction phases .",
    "each block consists of the display of all @xmath22 polygons in a shuffled order .",
    "each polygon is shown just once per block .",
    "the only constraint to the random order within a burst is that no primitive may appear twice in immediate succession . because a burst is a series of blocks , this restriction only affects the randomness of a block given the last stimulus of the previous block .",
    "the stimulus onset asynchrony ( soa ) between successive polygons was @xmath23 : during @xmath24 the visual stimulus ( the polygon ) was shown and during the last @xmath25 the corresponding polygon was substituted by a transparent rectangle ",
    "i.e. by a _ void _ stimulus .",
    "_ calibration phase _ : we restricted the visual stimuli to those polygons contributing more than @xmath18 visual information . by avoiding polygons that might not be reliably recognized as targets by the participant ( due to little visual information ) , erps elicited by the target polygons were likely a valid signal to calibrate the classifier . before each burst , a picture was selected as the target image and one random polygon from its decomposition ( among those with a visual information above @xmath18 ) was selected as the target primitive . during this phase , a burst consisted of @xmath13 blocks for subjects vpmao , vpmap , and vpjam , and of @xmath14 blocks for any other subject .",
    "after each burst , a new target image was selected .",
    "the subjects were told that , following the display of a drawing , random polygons would be presented and one or more of them could resemble the drawing or some salient feature of it .",
    "they were not explicitly instructed to seek for these stimuli in an active way .",
    "they were requested to avoid abrupt facial movements , to avoid blinking and to relax the jaw .",
    "approximately each @xmath26  minutes a self - paced pause was offered . during the calibration phase",
    ", there was no feedback provided to the participants .",
    "_ reconstruction phase _",
    ": we considered all polygons from the decompositions that contributed more than @xmath17 . for each subject , @xmath26 reconstructions were completed . after choosing one of the images for reconstruction",
    ", it was displayed for @xmath21 seconds .",
    "then the first burst proceeded with the most informative polygon of the chosen image as target , together with @xmath21 non - targets drawn from the pool of polygons .",
    "bursts during this phase consisted of @xmath13 blocks for subjects vpmai and vpmao , and @xmath14 for any other subject . during each burst of the reconstruction phase",
    ", the classifier would score the likelihood that each polygon had elicited an erp , and at the end of the burst the primitive being most likely the target was determined , as explained in section [ sec:2.04 ] .",
    "this primitive was displayed for @xmath21 seconds as a feedback alongside the correct target polygon .",
    "then the original target picture under reconstruction was shown again for @xmath21 seconds .",
    "the next burst proceeded with the next most informative polygon as target and with @xmath21 new non - targets from the polygon pool .",
    "the correct stimuli from previous bursts were retained after they had been played out , so that the reconstruction could proceed . after a reconstruction was completed ,",
    "a self - paced pause was offered .",
    "the instructions to the subjects were the same as during the calibration phase , except now subjects were told that the reconstruction would proceed until a fair , schematic rendering of the target image had been reached .",
    "a video of a successful reconstruction is available @xcite .      _",
    "erp analysis _ : eeg signals were lowpass filtered with a chebyshev filter using a bandpass up to @xmath27 and a stopband starting at @xmath28 , and then down - sampled to @xmath29 .",
    "continuous signals were divided into epochs ranging from @xmath30 to @xmath31 relative to each stimulus onset .",
    "baseline correction was performed on the pre - stimulus interval of @xmath32 .",
    "epochs containing strong eye movements were detected and rejected using the following criterion : epochs in which the difference of the maximum and the minimum values in one of the channels f9 , fz , f10 , af3 , and af4 exceeded @xmath33 were rejected . only those non - target epochs were used in which the three preceding and the three following symbols were also non - targets in order to avoid overlap from erps of preceding or successive targets . for the grand average",
    "the erp curves were averaged across all trials and participants . to compare the erp curves of two classes ( target and non - target ) signed @xmath34-values were calculated .",
    "_ classification of what polygons the subject attended to _",
    ": we employed binary classifiers based on spatio - temporal features .",
    "we sought for discrimination between epochs related to targets vs. non - targets .",
    "as preprocessing , eeg signals were down - sampled to @xmath29 by calculating the average for consecutive data points in non - overlapping stretches of @xmath35 each .",
    "epochs with an excessive power in a broadband ( @xmath36 ) indicating , e.g. muscular artifacts , were rejected from the calibration data .",
    "the aim of the heuristic is to find five time intervals that have a stationary ( target minus non - target difference ) pattern and maximal @xmath34 differences .",
    "occasionally the intervals determined by the heuristic were adjusted by the experimenter before starting the on - line runs ( see appendix [ app:1.02 ] ) .",
    "features were calculated from @xmath37 channels ( all except for fp1,2 , af3,4 , f9,10 , ft7,8 ; which are the electrodes more exposed to facial movements ) by averaging voltages within each of the five chosen time windows resulting in @xmath38 dimensional feature vectors . for classification , a linear discriminant analysis ( lda ) with shrinkage of the covariance matrix @xcite was trained on calibration data .",
    "a polygon was determined by averaging the classifier output for all displayed polygons across the @xmath13 or @xmath14 blocks of each burst and then by choosing the primitive that received the largest average output .    in order to investigate the rate / accuracy tradeoff ,",
    "the classifier that was used online was also applied offline to the image reconstruction data .",
    "the polygon selection depended then on averages across the first n blocks , with @xmath39 .",
    "for @xmath40 the full neural records are analyzed , so we obtain the same results as when operating the classifier online . for @xmath41 ,",
    "some of the data within each trial is discarded , so we are estimating the performance of the classifier if shorter neural activity records were available .",
    "we also computed the information transfer rate per decision ( @xmath42 )  i.e. the number of bits involved in each classification  using : @xmath43 where @xmath44 represents the number of primitives among which the classifier chooses and pc represents the empirically measured probability of making the right classification @xcite . in the offline analysis , by normalizing over the number n of blocks for @xmath39 we get a proxy for the number of bits that the classifier can extract per block and can thus trade off between the redundant information ( offered by the repeated presentation of the same stimuli ) and fast recovery ( obtained by less presentations , but hindered by a lower accuracy ) .",
    "additionally , we can normalize by the time consumed by a whole burst to obtain an information transfer rate in bits per seconds ( noted @xmath45 , without subscript ) .",
    "@xmath45 approximates the amount of information extracted and can be overoptimistic , as noted in @xcite .",
    "the @xmath45 is not a realistic performance measure for the bci context as it considers redundant coding on the side of the transmitter to achieve error robustness , which is not a reasonable assumption for the bci user .",
    "in contrast , bci applications require some mechanism which allows the user to undo false selections .",
    "since the design of such a mechanism is not straightforward for our application and requires a full discussion of its own , we still use the @xmath45 here as a good way to investigate the speed / accuracy trade - off without stressing the absolute value of the @xmath45 .",
    "the erp analysis of the data ( figure [ fig:3 ] ) presents the grand average of brain activity over all subjects and trials under target and non - target stimuli .",
    "when plotting the scalp map of this activity ( figure [ fig:3]*a * ) we immediately recognize how the erps of interest for our classification tasks are notable mainly in the frontal and central channels , being the occipital and temporal areas of null or little interest .",
    "this is consistent with the p300 erp associated to the oddball paradigm .",
    "-values distributed over the scalp and throughout time as a plot for the cz and po7 channels.,scaledwidth=40.0% ]    for each subject , time intervals of interest have been selected to provide the classifier with discriminative data as it was indicated in section [ sec:2.04 ] . in the grand average",
    "we can see how signals usually diverge between target and non - target activity ( figure [ fig:3]*b * ) and what time intervals are more useful on average ; the time interval between @xmath46 and @xmath47 is the most discriminative , which is once again consistent with the prominent role that the p300 erp should play in the designed bci .      during the reconstruction phase",
    ", the classification implemented after each burst was considered successful if the classifier had identified the corresponding target polygon from the current target drawing .",
    "this is different to setups where the subjects report whether a stimulus is target or not @xcite .",
    "note that in our paradigm a non - target stimulus might be considered target by a subject and erps might be elicited for such stimuli as well , but these would be scored as wrong classifications if selected by the classifier .",
    "this is further discussed in section [ sec:3.04 ] . on the other hand ,",
    "different polygons carry different visual information about the target drawing : making mistakes late in the reconstruction would not be as dramatic as getting some of the first primitives wrong .",
    "we quantified how much of the visual information was correctly recalled with respect to the maximum possible ( note that polygons contributing less than @xmath17 are never displayed ) .",
    "we refer to this number as _ weighted selection accuracy _ as opposed to the raw _ selection accuracy _ that measures the percentage of correct classifications irrespective of their importance",
    ".     per number of blocks reveals a good operating point at @xmath7 blocks per burst .",
    "this is a very optimistic result , and the approximative power of @xmath45 is further diminished by the design of the experiment , which does not allow ( and does not require either ) for corrections over selected polygons.,scaledwidth=50.0% ]    the average online selection accuracy across all subjects is @xmath48 .",
    "if we take into account only those subjects whose classification was based on bursts with @xmath14 blocks , then the online selection accuracy raises to @xmath49 .",
    "this must be compared to the chance level for one target among @xmath22 stimuli : @xmath50 .",
    "the weighted accuracy is @xmath51 ( @xmath52 for subjects with @xmath14 blocks ) .    in an offline analysis we studied what would be the performance of the bci if , within each burst , the classifier would consider less blocks to compute its output , as explained in section [ sec:2.04 ] .",
    "the results are collected in figure [ fig:4 ] .",
    "there is a drop in performance for lower block numbers , as expected , but the average selection accuracy remains relatively high  above @xmath53 for any choice with more than @xmath13 blocks per burst ( figure [ fig:4]*a * ) .    in a real free - painting application",
    "it would not be necessary to present a template ( target ) image .",
    "this and the presentation of feedback after each burst affect the time normalization term of the itr , whose calculation underlies the plot in figure [ fig:4]*b*. for that figure we assumed @xmath54 seconds for the display of the selected polygon .",
    "we can speculate what would happen under extremely fast conditions , say @xmath47 for feedback presentation .",
    "then , a peak of @xmath55 would be registered also at @xmath7 blocks per burst ( not shown ) . the opposite , conservative case with @xmath56 for feedback ( as in our experiment ,",
    "also not shown ) presents its peak at @xmath22 blocks with an entropy rate of @xmath57 , but the maximum is flat and extends from @xmath7 to @xmath58 blocks per burst .      as indicated in section [ sec:2.03.02 ] , during the reconstruction task a polygon is selected based on the output of the classifier .",
    "this might be the correct polygon ",
    "the one belonging to the target image reconstruction  or an incorrect one .",
    "both the correct and the selected polygons are shown as a feedback to the subject and , irrespective of the outcome , the correct polygon is held fixed on the background as new randomized stimuli are displayed in the following bursts .",
    "this certainly limits reconstruction freedom , but it allows us to proceed to tinier details and assess how the accuracy behaves in more complicated scenarios .",
    "@xmath59 of the paintings were completely reconstructed to its full extent without selecting any wrong polygon .",
    "this rises to @xmath60 if we consider only subjects whose bursts consisted of @xmath14 blocks .",
    "( the probability of reconstructing a picture by chance alone is lower than @xmath61 even for the picture whose reconstruction consists of less polygons ) .",
    "if we would consider the reconstructions only until the first wrong polygon is selected , a @xmath62 ( @xmath63 with @xmath14 blocks ) of the tasks would have been accomplished across subjects .",
    "if we take into account the visual information conveyed by each polygon and use the weighted accuracy to account for the percentage of reconstruction complete until the first wrong classification takes place , we find that @xmath64 of the visual information available is correctly retrieved ( @xmath65 for subjects with @xmath14 blocks ) .     and @xmath21 .",
    "both panels were elaborated using data from experiments with @xmath14 blocks per burst .",
    "there is not any remarkable difference considering similar plots that include all data.,scaledwidth=50.0% ]    because the right polygon was always preserved despite the outcome of the classifier , we could proceed with each reconstruction until the end and analyze the performance of the bci as it explores more complicated scenarios in which target polygons convey very little information ( down to @xmath17 ) . as a result we found the selection accuracy drop for polygons bearing less visual information about the target ( figure [ fig:5 ] ) .",
    "besides the obvious decay in accuracy for less informative primitives , we observe an increase in heterogeneity . as we move towards more difficult tasks , a range of selection accuracies emerge : some less informative polygons are rarely ( two of them never ) recognized as part of the target image while others are still correctly selected to a great extent .",
    "polygons bearing more visual information tend to be accurately selected most of the time .",
    "only @xmath22 polygons have always been correctly selected , all of them had a visual information below @xmath66 and one of them is at the edge of the @xmath17 threshold .",
    "these results were obtained for the eight subjects with @xmath14 blocks per burst during the reconstruction task .",
    "the outcome considering all subjects is broadly the same .    in accordance with the experimental design ,",
    "less informative polygons appear later in the reconstruction task , as shown in figure [ fig:5]*b*. the selection accuracy almost always decays as a reconstruction proceeds . remarkably , the selection accuracy for the first polygon is @xmath67 ( @xmath68 for subjects with @xmath14 blocks per burst ) . in figure [ fig:5]*b * we also see an unexpected drop ( followed by a rise ) of the selection accuracy for intermediate polygons .",
    "the reason for this is discussed in appendix [ app:2 ] and relies on the particularly difficult task that polygons @xmath7 or @xmath13 of some reconstructions posed to the subjects .      in choosing our pictures for reconstruction ,",
    "we want them to be iconic with easily recognizable parts and with as little overlap as possible .",
    "however , it can not be avoided that some drawings ( or parts of them ) resemble some other one .",
    "this leads to an undesired effect during the experimental sessions : some polygons did not belong to the reconstruction of the target drawing , but they bore some resemblance to it and were often classified as target . because these polygons are strictly non - targets , they were excluded from the selection accuracy results reported in section [ sec:3.02 ] .",
    "this does not imply a malfunction of the classifier because such polygons could have tricked the subjects as well .",
    "these pieces introduce an interesting ambiguity as they could contribute to several reconstructions .",
    "we refer to them as _",
    "ambiguous polygons_.     blocks per burst .",
    "the scenario is similar if we use all data for this analysis.,scaledwidth=50.0% ]    as explained in section [ sec:2.03.01 ] , along with each target polygon we chose @xmath21 non - target polygons from one common pool .",
    "one same polygon might have shown up several times as a non - target during one reconstruction , or for the reconstruction of the same drawing by different subjects .",
    "if in such cases the classifier repeatedly selected the non - target , we might have strong evidence that the subject perceives that polygon as contributing to the target drawing .",
    "based on this , we computed @xmath69-values to ascertain what polygons had been more probably not selected by chance alone , but presumably because an honest interference existed between those non - targets and the target drawing .",
    "we show the top @xmath21 such pieces in figure [ fig:6 ] along with their @xmath69-values and the target images for which they were significantly over - selected .",
    "we see that these could perfectly contribute to sketch the target ( like the polygon from the banana in the case of the lemon reconstruction ) or to the refinement of a smaller detail ( like the green polygons seemingly selected to complete the leaves of the carrot and the strawberry ) . in these paradigmatic cases",
    "the wrongly classified non - targets would show up roughly in the same area as the actual target polygons .",
    "in this paper we show how rapid serial visual presentation using bursts of polygons together with the oddball paradigm can be used for bci image reconstruction .",
    "our purpose was merely to attain a proof of concept . for that end",
    ", different design choices have been made and tested during the experimental sessions .",
    "a systematic search of the best working settings was never intended and is left for future work . notwithstanding this , the results reported in section [ sec:3 ] invite us to be optimistic about the paradigm and demand that further , more rigorous experiments be performed . in this section",
    "we comment on our results and compare them to previous approaches to bci - painting .",
    "we also propose future lines of research focusing on the development of a free - painting bci based on the current paradigm .      in section [ sec:3.02 ]",
    "we reported a classification accuracy of @xmath70 ( @xmath49 for subjects with @xmath14 blocks ) .",
    "this rises to @xmath51 ( @xmath52 ) if we acknowledge that some correct classifications are more important than others , and use the percentage of visual information retrieved to weight our results .",
    "this means that we can reproduce up to @xmath71 of the visual information that the polygon decompositions capture of the original images .",
    "these numbers would gain relevance if compared with those of other bci - painting paradigms .",
    "we attempt this now , but a series of limitations exist due to the differences in bci designs .",
    "for example , the guided evolution in @xcite towards images that produce ` positive ' feelings can not be properly quantified .",
    "the authors report a @xmath3 accuracy , but this might be a biased result .",
    "after three generations of the genetic algorithm ( which takes around @xmath72 minutes ) , subjects in [ 13 ] were asked to recall their actual choices  those picture that elicited more ` positive ' feelings according to their conscious experience .",
    "unfortunately , this approach is biased : the image selected by the classifier is used to generate variations that are then persistently shown to the subject , while the un - selected pictures are lost .",
    "one further limitation to compare our results to this work was pointed out at the introduction : in @xcite , there are no correlates with actual visual structures .",
    "while we quantify the overlap of our reconstructions with the original drawing ( though naively , through the visual information ) , pictures in @xcite evolve towards positive - looking images according to physiological feedback .",
    "guiding the evolution towards visual details that we could quantify is rather difficult .",
    "the impressive classification accuracy obtained in @xcite clearly outperform ours",
    ". this work shall be a good reference for future bci - painting paradigms , especially if it is possible to attain their high presentation rate ( @xmath58 images per second during each burst ) while keeping up in selection accuracy when the design is tested in actual image evolution tasks .",
    "comparison with other aspects of our work is more difficult and we have to wait until this image evolution paradigm is put to an online test .",
    "then , we will be able to study a sense of _ progression _ towards a final target and we could quantify properly how well detailed visual arrangements get reconstructed .",
    "if the accuracy results reported in @xcite persist , this is a truly promising scheme .    finally , the _ p300-brain painting bci _",
    "@xcite performs very well in selection accuracies and it also provides two measures of @xmath45 , reporting values that beat our bci design .",
    "this is a very promising paradigm that mimics the equally successful matrix spellers .",
    "however , a few points should be considered . in usual spellers ,",
    "a sentence is provided for a subject to copy . in @xcite ,",
    "a picture is produced if we follow a series of instructions precisely : these indicate what shape to place next on the canvas , what color and size it shall be , and where it must be placed ",
    "all these instructions are stored as symbols in the matrix interface .",
    "these are the symbols that are accurately retrieved from the matrix for the results in @xcite to make sense .",
    "hence this is an _ instruction - copying _",
    "task rather than a _ copy - painting _ one .",
    "if , otherwise , a subject were provided an image and were requested to copy - paint it , there would be several different sets of instructions leading to the same outcome , and such variations are more complicated to account for in terms of symbol accuracy .",
    "a similar problem restricts our bci so that we have to dismiss ambiguous polygons , as explained above .",
    "while this implies some limitations for our paradigm , the problem is lightly deeper in @xcite : copying a set of symbols ( instructions ) does not necessarily relate to visual cues on the canvas and , accordingly , makes quantification of the result more difficult . specifically , it is not straightforward to find a meaningful measure equivalent to our visual information . while our selection accuracy and , specifically , our weighted accuracy report directly about the objective overlap that we attain with an actual target drawing , the results in @xcite are more difficult to quantify in visual space .",
    "we expect that future research on bci - painting will make possible a more systematic comparison between different paradigms and visual outcomes .",
    "there is , though , a relevant caveat of our own bci that deserves a closer analysis . in section [ sec:3.03 ]",
    "we report a performance drop as the difficulty of the task increases . because of this",
    ", only @xmath1 of the reconstructions could proceed as intended .",
    "this should be improved in future implementations of the paradigm , and hence a series of alternatives are available :    * the rather conservative experimental settings ( soas , some display options for the polygons , number of blocks per burs , etc ) make possible an improvement to achieve faster @xmath45 and equally better accuracies . gaining a few seconds per burst could allow us to introduce some error correcting mechanism , as discussed below , while keeping a high @xmath45 . *",
    "the classification results in @xcite might be traced back to an easy cognitive task : @xmath6 of the bursts containing a target are consciously detected by the experimental subject , perhaps because the classification involves a clear - cut task . in our case",
    "( due to the presence of ambiguous situations ) conscious wrong decisions might be prominent .",
    "this strongly suggests that to improve our accuracy we should pay more attention to the input stimuli .",
    "we could either exploit the role of ambiguous polygons or attempt to ban them altogether by building orthogonal primitives that produce a maximum diversity of designs with a minimum number of patterns .",
    "an interesting alternative could be to separate the generation of new shapes and their filling with color .",
    "this is an open and interesting problem at the frontier between bci and natural image decomposition .",
    "admittedly a @xmath1 complete reconstructions falls short , but we suggest that this is not the best indicator of the performance of the bci .",
    "it is desirable to complete the reconstruction of an image but this is a very stringent condition : finer grained primitives become more difficult to classify , while they might not be as important as the overarching primitives .",
    "tinier details might also be open to more subjective appreciations by the bci users .",
    "furthermore , towards the end of a reconstruction the quality of the current primitives might degenerate since the genetic algorithm favors convergence of broad details first .",
    "we believe that weighting the classification accuracy by the amount of visual information that each classification contributes is a better indicator of the performance of our bci paradigm .",
    "then , as reported in section [ sec:3.02 ] , up to @xmath64 ( @xmath65 for subjects with @xmath14 blocks per burst ) of visual information can be retrieved before the first classification mistake .",
    "if we wanted to move towards an rsvp - based free - painting bci machine , we could exploit the existence of ambiguous primitives and we should consider seriously the necessity of error correcting mechanisms .",
    "other improvements should direct the evolution in an active way , avoiding full reliance on the randomness of primitives as discussed below , e.g. , for the @xmath54-d location of new polygons .",
    "we speculate about the future of free - painting bcis based on our paradigm in the following .",
    "we consider first the possibility of including error correcting mechanisms to improve the classification accuracy and @xmath45 . in computing the speed / accuracy trade - off by the @xmath45 formula , we neglected the problem of explicit correction of false bci selections .",
    "these corrections that would have to be performed explicitly by the user are more time consuming than being accounted for by that equation ( which assumes optimal error robust coding ) .",
    "accordingly , a realistic trade - off would result in a higher number of blocks than the three suggested by figure [ fig:4]*b * , reflecting the delay introduced by error correction .",
    "other rsvp tasks have mechanisms to correct for wrong outputs . in rsvp spellers ,",
    "a symbol can be explicitly incorporated among the letters to represent the backspace @xcite .",
    "a similar approach could be taken for the present bci application but it would likely interfere with the display of polygons .",
    "an appealing alternative are error potentials , a large scale signal elicited by unexpected feedback after a classification task , which can be used to automatically cancel incorrect selections @xcite .",
    "one further alternative is inspired by @xcite where erp - based bcis are explored .",
    "the selection accuracy is well characterized and a confirmation step ( based on a similar rsvp task ) is analyzed rendering a @xmath73 success with no false positives . incorporating one such confirmation step would raise our selection accuracy to @xmath74 . for this we computed the probability ( across all subjects ) that the second highest ranked polygon was the correct one given that the first choice was a wrong classification  this spares us a second burst but has a lower accuracy .",
    "if we would incorporate two confirmation steps ( and , consequently , use the probability that the correct polygon was the third highest ranked one , provided the two first were not ) , the selection accuracy would rise to @xmath75 . alternatively , we could proceed with a second burst discarding the first ( wrongly selected ) polygon and introduce a new non - target one . this way , the accuracy would reach a @xmath76 for just one confirmation step . to include these details in an estimation of the @xmath45 would be very speculative : we should take into account not only the increased time lapse of the error correction task , but also potential undesired effects due to the disruption of the main painting task .    before using solutions that need more steps ,",
    "as the error correcting mechanisms discussed , we note that some available , relevant information is dismissed in the current design .",
    "a winner - takes - all decision is forced at the end of each burst while it is possible that several polygons get large scores from the classifier , especially if ambiguous primitives are present . on the other hand , easier classification at the beginning of a reconstruction",
    "might have a clear winner early in the burst .",
    "we could quantify the uncertainty of a decision ( e.g. through an entropy - based classifier ) and exploit this information , which is already captured by our bci .",
    "the length of a burst could be dynamically tuned until the confidence of a decision would rise above a threshold .",
    "a sustained uncertainty could be taken as ` select nothing ' , an interesting option that free - painting applications should allow .",
    "+ regarding ambiguous polygons , these are pieces that might contribute to the reconstruction of several different images as reported in section [ sec:3.02 ] .",
    "usually , this is because the original drawings themselves share some common traits , as in the case of the green leaves located around the same position for the carrot , the strawberry , etc ( figure [ fig:5 ] ) .",
    "they do not represent a generalized situation : it would just affect a few polygons , and scoring these cases as correct classifications would not change our selection accuracy significantly .    for our results ,",
    "these ambiguous polygons had a negative effect because they do not count as correct classifications ( and thus lower our selection accuracy ) ; but in a wider scope they might be extremely useful . while we imposed that the image decomposition be unique for each drawing , we can conceive of intermediate decompositions of multiple images with shared primitives . then , a reconstruction should proceed from ambiguous , generalized descriptions towards more particular ones .",
    "this would establish a hierarchy that would cluster together pictures that are closer to each other in visual terms . by exploiting this feature",
    "we could discard non - targets quickly if they are very distant from our target but , as a consequence , we will progress towards more difficult classification tasks . to work out this situation",
    "we must research what is the finer detail that our bci paradigm is able to resolve .",
    "linked to this , we could seek the use of maximally discriminating primitives at each reconstruction stage .",
    "note the few constraints that we imposed upon the bursting polygons : many might be displayed within the same burst that convey redundant information , thus diminishing the exploratory capabilities of rsvp .",
    "also , shape and color are tightly linked together in the current design .",
    "a two - stage decision that separates these arguably orthogonal features might be of great help towards a free - painting bci .",
    "how we should design our primitives to maximally exploit the current paradigm is one of the open research lines proposed for the future .",
    "+ finally , there is an important aspect that is left to random chance in the current paradigm that should be corrected , if we really wanted our bci users to explore freely the space of possible drawings . right now",
    ", correct polygons are placed where they belong because their location is part of the specifications of each polygon decomposition .",
    "if we did not know the correct location of a primitive , it is extremely unlikely that it would be placed at the right spot by chance alone .",
    "we envision the next design : instead of the whole screen , consider a bursting area , a rectangle of restricted size laid upon the canvas within which all bursting polygons fit .",
    "accordingly , the drawing will only suffer modifications in the framed area .",
    "the position and size of the bursting area could be controlled by a joystick , but this would not be appropriate for impaired users . instead , the position of the frame could be modified through an eye tracking device or by sensory motor rhythms ( as proposed in @xcite for the _ p300-brain painting bci _ ) .",
    "this last option would be the preferred choice for patients who can not focus their sight ( e.g. those suffering completely locked - in syndrome ) , but in this last case the drawing beneath the bursting area should be displaced while the bursting area remains fixed under the focus of the bci user .",
    "note that bursting is halted while the bursting area is manipulated .",
    "to control the size of the bursting area when a joystick is not a viable option , we propose using the edges of the canvas as part of the interface : bringing the bursting area to the left - most edge of the canvas and insisting that it moves further than allowed would enlarge the horizontal dimension of the frame , while moving it all the way to the right and insisting that it goes further ( e.g. focusing the sight outside the canvas region ) would diminish the horizontal dimension . the same scheme would work for increasing / decreasing the vertical dimension using the top and bottom edges respectively , and the top - left and bottom - right corners would operate on both dimensions at the same time .",
    "we propose also that the top - right and bottom - left corners could be used to zoom in and out the canvas ",
    "note that this is strictly different from enlarging / shrinking the bursting area , since it allows for control over ever tinier details while the bursting polygons appear large to the bci user .    locating polygons in depth seems to be the remaining challenge . if necessary",
    ", we could dismiss the simultaneous modification of horizontal and vertical dimension and use the top - left and bottom - right corners to increase / decrease the layer where new primitives are bursting .",
    "this work was supported in part by grants of the bmbf : 01gq0850 and 16sv5839 .",
    "the research leading to this results has received funding from the european union seventh framework programme ( fp7/2007 - 2013 ) under grant agreement 611570 .",
    "we thank two anonymous reviewers for very constructive comments and edward zhong for his valuable revision and corrections of a late draft of the paper .",
    "seoane acknowledges the support of the fundacin pedro barri de la maza and useful discussion with members of the complex systems lab , especially sergi valverde .",
    "both writing and painting are emergent processes , although at very different levels . while in the former minimal components",
    "are clearly identified ( written characters , letters ) the latter is a truly emerging outcome of the very strongly , non - linearly interacting pieces that compose an image .",
    "it may be impossible to come down to some basic components of a drawing . if we intend to produce a picture from scratch , our choice of building blocks ( say our _ alphabet _ for drawing ) conditions the complexity that can be generated , the difficulty required to render each picture , and the speed to which we can produce it .",
    "we need to find adequate primitives that can compose a range of images quickly by combining the minimal units .",
    "our pieces should be simple and schematic , and as pivotal to our targets as letters are to writing .",
    "we think that this is an open problem . for this study we adopted a provisional solution that we describe in the following .    as targets for reconstruction we sought iconic images from the snodgrass and vanderwart s object database @xcite .",
    "the chosen pictures ( figure [ fig:1 ] ) are drawings of fruits and vegetables with basic shapes and colors ; all of them laid on a white background , so that the reconstruction focuses in clear motives and not in peripheral details .",
    "note anyway that the subject s attention is free to wander over the screen .",
    "we discuss subject focus again in section [ app:1.02 ] in comparison with previous rsvp applications .",
    "we need schematic , yet faithful , representations of the selected pictures .",
    "that was a preprocessing step completed weeks before the experiments .",
    "to extract useful primitives from our pool of drawings we found the perfect tool in recent applications of genetic algorithms ( gas ) for image decomposition @xcite .",
    "such algorithms proceed through mutation and artificial selection from an arbitrary population of polygons to a set of polygons carefully arranged as to mimic a desired image .    for a ga",
    ", a fitness function needs to be introduced .",
    "take a set of random polygons laid upon a white background , and some on each other to create a sense of depth .",
    "these compose an arbitrary image . given an rgb color scheme , we use as fitness function the pixel - by - pixel euclidean distance between the original picture and the image rendered by the collection of polygons .",
    "the better the fitness , the closer the collection of polygons resembles the original image .    as a seed for the algorithm we use an arbitrary collection of polygons @xmath77 , where @xmath78 labels iterations and @xmath79 labels the @xmath80 polygons composing the collection .",
    "note that @xmath80 might change from one iteration to another . at each iteration",
    "the fitness of the current collection @xmath81 is evaluated .",
    "some mutations are applied to generate an alternative polygon composition @xmath82 .",
    "the collection with better fitness is retained : @xmath83 , where @xmath84 stands for the fitness function . the algorithm continues until a satisfactory convergence towards the original image has been reached or until the fitness does not improve for several iterations .",
    "the collection @xmath85 when the algorithm halts is referred to as the _ polygon decomposition _ of the original image .",
    "the possible mutations applied at each iteration are : removal of a polygon or insertion of a new random one ; swapping two polygons ( recall that some polygons overlay some others ) ; random addition , deletion , or modification of polygon s vertices ; change of a polygon color .",
    "details on the probability of each operation ( provided along the code @xcite ) are not relevant as long as a satisfactory approximation of the original images is achieved ( which was done , as appreciated in figure [ fig:1 ] ) .",
    "the algorithm allows options regarding the kind of polygons that could be used .",
    "this turned out to be very important .",
    "we wished to discard very complex pieces ; therefore only polygons with @xmath7 to @xmath16 vertices were allowed .",
    "we did not use partially transparent polygons  as implemented by the @xmath86 parameter in the rgb color scheme .",
    "this was a valid option for the original ga @xcite , but it introduced important non - linearities in the interactions between polygons . as an instance , if transparency were allowed it might happen that some desired color shows up only after two polygons have been stacked in the right position . if only one of the polygons were present , we would not get quite the exact shade .",
    "but our paradigm only allows one polygon at a time , so we would risk rejecting the right polygon because it does not show the final result yet . by using only opaque polygons we partially solved this problem . +",
    "once an image has been decomposed in its primitives , the fitness function also offers a measure of the importance of each piece in the reconstruction . for each polygon",
    "@xmath87 we compute the fitness function of the whole arrange of polygons , and the fitness of the same arrange when @xmath88 is removed .",
    "we defined the _",
    "visual information _ carried by the polygon as the normalized drop in fitness .",
    "there is no intention of connecting this visual information to actual theoretical information measures .    using this visual information we ranked the polygons for each of our original images and retained only those contributing more than a @xmath17 ( or @xmath18 , see section [ sec:2.03.02 ] ) .",
    "this choice renders fine enough reconstructions ( see figure [ fig:1 ] ) . adding more polygons",
    "would only help with very tiny details and would make the experiments unnecessarily tedious .",
    "+ the oddball paradigm requires that target stimuli are presented intermixed with neutral stimuli .",
    "when an original image from our pool was chosen as the object for reconstruction , all the polygons in its polygon decomposition became target stimuli .",
    "as for neutral stimuli , we used a pool containing all the polygons belonging to the decomposition of all other drawings .",
    "this decision made the reconstruction task a fair one .",
    "all the polygons in our pool of neutral stimuli have been generated following the same procedure , only they belong to the polygon decomposition of different images .",
    "differences between the two classes would ideally arise from their belonging or not to the target decomposition .",
    "if we would use , e.g. , completely random polygons as neutral stimuli these could take any aspect , obviously including shapes that would hardly contribute to the generation of natural images .",
    "such instances could be readily identified when opposed to polygons that account for details of some natural image .",
    "the reconstruction task would be artificially simplified by a preselection that greatly reduced the uncertainty about the target primitives .",
    "the task would effectively become a _ recognition of the less random- looking polygon_.      the natural guidelines for our bci design are the experiments with rsvp spellers by acqualagna and blankertz @xcite .",
    "these give us a vague idea of settings under which a bci for image reconstruction could function .",
    "we chose rather conservative specifications , given the novelty of the procedure .",
    "for example , during rsvp a burst soa of @xmath89 between consecutive polygons was used .",
    "this lapse includes @xmath90 during which a void stimulus was intercalated .",
    "these settings can be compared to the soas of @xmath91 to @xmath92 from @xcite , where letters succeeded each other without inserting any voids .",
    "these differences ( together with the good results reported here and those from rsvp spellers ) suggest that there is room for improvement should we seek more ambitious experimental settings .",
    "the original motivation to introduce rsvp to bci spellers was that letters could be presented always at the focal point .",
    "stimuli falling right in the visual focus of the subject show enhanced erps which are detected more easily .",
    "this is not an asset in our case : we must allow the subjects to focus on different areas of their visual field when peripheral details of the target images are being reconstructed .",
    "we did not impose any conditions on the focus of the subjects .",
    "looking at the long term goals of this research , the interest of rsvp will be on its exploratory potential because of the random , combinatorial nature of the bursting primitives and its interplay with the subject s ( sub)conscious driving of the image reconstruction process .",
    "we are convinced that these ideas are worth exploring .",
    "as indicated in section [ sec:2.04 ] , occasionally , the intervals determined by the heuristic were adjusted by the experimenter before starting the on - line runs .",
    "these adjustments where generic for all subjects and intended to guide the classifier towards informative time intervals . in all cases , the linear classifier converged towards five discriminative temporal intervals ( ideally towards the most discriminative ones ) , partly erasing the manual setup . table [ tab:1 ] collects the eventual time intervals chosen by the classifier for each subject .",
    "[ cols=\"<,^,^,^,^,^\",options=\"header \" , ]     for the present proof of concept , we focused on achieving a classification accuracy good enough , for which we took into account as much spatio - temporal information as possible . underlying this choice are the patterns inferred by the classifier , which involve the activity of most channels available during a series of time intervals ( see table [ tab:1 ] )",
    "however , the current design relies strongly on the p300 oddball paradigm whose macroscopic imprint is a frontal / central positive deflection of the scalp potential .",
    "more classic accounts of this signal rely on measurements on central electrodes alone , prominently on the cz channel .",
    "it is fair to ask whether these central channels alone have discriminative power to accomplish the image - reconstruction task of our bci . in figure [ fig:7 ]",
    "we plot the average selection accuracy per channel if that channel alone were considered by the classifier .",
    "we observe that the central and fronto - central channels are the most predictive ones , strongly supporting that the p300 fronto - central potential underlies the analyzed erp .",
    "we also observe a notable drop in classification accuracy compared to the quality obtained with compound spatio - temporal filters ( below @xmath93 for all individual channels compared to @xmath94 and higher accuracies reported in the body of the paper ) .",
    "this might suggest a relevant role for other sources than the fronto - central channels .",
    "notwithstanding , this does not imply that the enhanced accuracy of extended spatio - temporal filters necessarily stems from signals other than the p300 deflection .",
    "due to the availability of data from multiple channels , the signal of interest may be enhanced by implicit spatial filtering . with the available data",
    ", we can not distinguish between these possibilities .",
    "in section [ sec:3.03 ] it was reported the performance drop as the difficulty of the reconstruction task increased .",
    "two approaches were taken : i ) the selection accuracy was plotted against the visual information carried by each polygon and ii ) the selection accuracy was plotted as a function of the rank that each polygon occupied in the reconstruction .",
    "the first method asserts that the selection accuracy drops in average for polygons that contribute less to the reconstruction of the image ( figure [ fig:5]*a * ) .",
    "highly informative polygons are usually correctly classified , but the least informative polygons display a great variation . some of them are often well classified and some of them are not , with a range of selection accuracies in - between .",
    "this indicates that the reconstruction task does not always become more difficult as we move towards less informative polygons .",
    "figure [ fig:5]*b * reports the average selection accuracy across all reconstructions against the rank that a given polygon occupies in the reconstruction .",
    "the reconstruction task proceeds from the most informative polygons to the least informative ones , given the target picture .",
    "the latter contain less information about the original image , and hence it should be more difficult to classify them correctly .",
    "unexpectedly , the decay in performance is not a monotone function : polygons @xmath21 , @xmath22 , and @xmath16 in the reconstruction are selected with notably more accuracy than polygons @xmath7 and @xmath13 .",
    "this indicates that there are some less informative polygons that are correctly selected more often than the average of other , more informative polygons .",
    "figure [ fig:8 ] clarifies this drop in performance for intermediate polygons . when we look at the average accuracy per polygon for single reconstructions we note that some of them present a strongly marked fall in selection accuracy for polygons @xmath7 and @xmath13 : the strawberry in polygon @xmath7 ; and the carrot , the pear , and the pineapple in polygon @xmath13 .",
    "all four cases present a selection accuracy lower than @xmath93 for these polygons , contributing to an average performance drop across reconstructions .",
    "these polygons happen to be just difficult to classify .    in two of these cases ( strawberry and pear )",
    "the problematic polygons are very light gray pieces that contribute to the reconstruction ( perhaps by occluding some spurious detail brought in by earlier pieces , as with the third polygon of the cherry @xcite ) . because they are almost white , they are easy to miss against the blank background . in the case of the carrot",
    "the performance drop is not so dramatic .",
    "also , this polygon number @xmath13 contributes to the leaves of the carrot and , as we saw in section [ sec:3.04 ] , that was precisely a common location of ambiguous polygons .",
    "if some ambiguous non- targets have been overly miss - classified , some actual targets must necessarily be missed .",
    "nothing remarkable has been recognized in the case of the pineapple : the conflicting polygon seems to be a working , non - ambiguous piece of the reconstruction and we just assume that this precise primitive was more difficult for the subjects ( note that the performance raises back to average for polygons @xmath22 and @xmath16 of the pineapple ) . among the four important deviations at intermediate stages ,",
    "this is the case with the lowest drop .",
    "ahani a , wiegand k , orhan u , akcakaya m , moghadamfalahi m , nezamfar h , patel r , erdogmus d , rsvp iconmessenger : icon - based brain - interfaced alternative and augmentative communication .",
    "_ brain - computer interfaces _ * 1*(3 - 4 ) , 192 - 203 ( 2014 ) .",
    "parra lc , christoforou c , gerson ad , dyrholm m , luo a , wagner m , philistiades mg , sajda p , spatiotemporal lineaer decoding of brain state : application to performance augmentation in high - throughput tasks .",
    "_ ieee signal process .",
    "_ * 25 * , 95 - 115 ( 2008 ) .",
    "sajda p , pohlmeyer e , wang j , parra lc , christoforou c , dmochowski j , hanna b , bahlmann c , singh mk , chang s - f , in a blink of an eye and a switch of a transistor : cortically coupled computer vision . _",
    ". ieee _ * 98 * , 462 - 478 ( 2010 ) .",
    "bigdely - shamlo n , vankov a , ramirez rr , makeig s , brain activity - based image classification from rapid serial visual presentation . in _ neural systems and rehabilitation engineering , ieee transactions on _ , * 16*(5 ) , 432 - 441 ( 2008 ) .",
    "pohlmeyer ea , wang k , jangraw dc , lou b , chang s - f , sajda p , closing the loop in a cortically - coupled computer vision : a brain  computer interface for searching image databases",
    ". _ j. neural eng . _",
    "* 8 * , 036025 ( 2011 ) .",
    "msinger ji , halder s , kleih sc , furdea a , raco v , hsle a , kbler a , brain painting : first evaluation of a new brain  computer interface application with als - patients and healthy volunteers . _",
    "neurosci . _ * 4 * , 182 ( 2010 ) .",
    "venthur b , scholler s , williamson j , dhne s , treder ms , kramarek mt , mller k - r , blankertz b , pyff  a pythonic framework for feedback applications and stimulus presentation in neuroscience . frontiers neurosci .",
    "00179 , doi : 10.3389/fnins.2010 ( 2010 ) ."
  ],
  "abstract_text": [
    "<S> this paper provides a proof of concept for an eeg - based reconstruction of a visual image which is on a user s mind . </S>",
    "<S> our approach is based on the rapid serial visual presentation ( rsvp ) of polygon primitives and brain - computer interface ( bci ) technology . </S>",
    "<S> the presentation of polygons that contribute to build a target image ( because they match the shape and/or color of the target ) trigger attention - related eeg patterns . </S>",
    "<S> accordingly , these target primitives can be determined using bci classification of event - related potentials ( erps ) . </S>",
    "<S> they are then accumulated in the display until a satisfactory reconstruction is reached . </S>",
    "<S> selection steps have an average classification accuracy of @xmath0 . </S>",
    "<S> @xmath1 of the images could be reconstructed completely , while more than @xmath2 of the available visual details could be captured on average . </S>",
    "<S> most of the misclassifications were not misinterpretations of the bci concerning users intent ; rather , users tried to select polygons that were different than what was intended by the experimenters . </S>",
    "<S> open problems and alternatives to develop a practical bci - based image reconstruction application are discussed . </S>"
  ]
}