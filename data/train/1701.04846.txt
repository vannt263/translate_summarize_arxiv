{
  "article_text": [
    "statistical models can be broadly classified into _",
    "parametric _ and _ nonparametric _ models .",
    "parametric models , indexed by a finite dimensional set of parameters , are focused , easy to analyse and have the big advantage that when correctly specified , they will be very efficient and powerful .",
    "however , they can be sensitive to misspecifications and even mild deviations of the data from the assumed parametric model can lead to unreliabilities of inference procedures .",
    "nonparametric models , on the other hand , do not rely on data belonging to any particular family of distributions . as they make fewer assumptions ,",
    "their applicability is much wider than that of corresponding parametric methods .",
    "however , this generally comes at the cost of reduced efficiency compared to parametric models .",
    "standard time series literature is dominated by parametric models like autoregressive integrated moving average models @xcite , the more recent autoregressive conditional heteroskedasticity models for time - varying volatility @xcite , state - space @xcite , and markov switching models @xcite . in particular , bayesian time series analysis @xcite is inherently parametric in that a completely specified likelihood function is needed .",
    "nonetheless , the use of nonparametric techniques has a long tradition in time series analysis .",
    "@xcite introduced the periodogram which may be regarded as the origin of spectral analysis and a classical nonparametric tool for time series analysis @xcite .",
    "_ frequentist _ time series analyses especially use nonparametric methods @xcite including a variety of bootstrap methods , computer - intensive resampling techniques initially introduced for independent data , that have been taylored to and specifically developed for time series @xcite . an important class of nonparametric methods is based on frequency domain techniques , most prominently smoothing the periodogram .",
    "these include a variety of frequency domain bootstrap methods strongly related to the whittle likelihood @xcite and found important applications in a variety of disciplines @xcite .",
    "despite the fact that _ nonparametric bayesian _ inference has been rapidly expanding over the last decade , as reviewed by @xcite , @xcite , and @xcite , only very few nonparametric bayesian approaches to time series analysis have been developed .",
    "most notably , @xcite , @xcite , @xcite , @xcite , @xcite , and @xcite used the whittle likelihood @xcite for bayesian modelling of the spectral density as the main nonparametric characteristic of stationary time series .",
    "the whittle likelihood is an approximation of the true likelihood .",
    "it is exact only for gaussian white noise .",
    "however , even for non - gaussian stationary time series which are not completely specified by their first and second - order structure , the whittle likelihood results in asymptotically correct statistical inference in many situations .",
    "as shown in @xcite , the loss of efficiency of the nonparametric approach using the whittle likelihood can be substantial even in the gaussian case for small samples if the autocorrelation of the gaussian process is high .",
    "on the other hand , parametric methods are more powerful than nonparametric methods if the observed time series is close to the considered model class but fail if the model is misspecified . to exploit the advantages of both parametric and nonparametric approaches ,",
    "the autoregressive - aided periodogram bootstrap has been developed by @xcite within the frequentist bootstrap world of time series analysis .",
    "it fits a parametric working model to generate periodogram ordinates that mimic the essential features of the data and the weak dependence structure of the periodogram while a nonparametric correction is used to capture features not represented by the parametric fit .",
    "this has been extended in various ways ( see @xcite ) .",
    "its main underlying idea is a nonparametric correction of a parametric likelihood approximation .",
    "the parametric model is used as a proxy for rough shape of the autocorrelation structure as well as the dependency structure between periodogram ordinates .",
    "sensitivities with respect to the spectral density are mitigated through a nonparametric amendment .",
    "we propose to use a similar nonparametrically corrected likelihood approximation as a pseudo - likelihood in the bayesian framework to compute the pseudo - posterior distribution of the power spectral density ( psd ) and other parameters in time series models .",
    "this will yield a pseudo - likelihood that generalises the widely used whittle likelihood which , as we will show , can be regarded as a special case of a nonparametrically corrected likelihood under the gaussian i.i.d.working model .",
    "software implementing the methodology is available in the ` r ` package ` beyondwhittle ` , which is available on the comprehensive r archive network ( cran ) , see @xcite .",
    "the paper is structured as follows : in chapter  [ sec : likelihood ] , we briefly revisit the whittle likelihood and demonstrate that it is a nonparametrically corrected likelihood , namely that of a gaussian i.i.d .  working model .",
    "then , we extend this nonparametric correction to a general parametric working model .",
    "the corresponding pseudo - likelihood turns out to be equal to the true likelihood if the parametric working model is correctly specified but also still yields asymptotically unbiased periodogram ordinates if it is not correctly specified . in chapter",
    "[ sec : bayesian ] , we propose a bayesian nonparametric approach to estimating the spectral density using the pseudo - posterior distribution induced by the corrected likelihood of a fixed parametric model .",
    "we describe the gibbs sampling implementation for sampling from the pseudo - posterior .",
    "this nonparametric approach is based on the bernstein polynomial prior of @xcite and used to estimate the spectral density via the whittle likelihood in @xcite .",
    "we show posterior consistency of this approach and discuss how to incorporate the parametric working model in the bayesian inference procedure .",
    "chapter  [ sec : simulations ] gives results from a simulation study , including case studies of sunspot data , and gravitational wave data from the sixth science run of the laser interferometric gravitational wave observatory ( ligo ) .",
    "this is followed by discussion in chapter  [ sec : summary ] , which summarises the findings and points to directions for future work .",
    "the proofs , the details about the bayesian autoregressive sampler as well as some additional simulation results are deferred to the appendices  [ sec : proofs ]  [ sec_appendixsims ] .",
    "while the likelihood of a mean zero gaussian time series is completely characterised by its autocovariance function , its use for nonparametric frequentist inference is limited as it requires estimation in the space of positive definite covariance functions . similarly for nonparametric bayesian inference , it necessitates the specification of a prior on positive definite autocovariance functions which is a formidable task . a quick fix is to use parametric models such as arma models with data - dependent order selection , but these methods tend to produce biased results when the arma approximation to the underlying time series is poor .",
    "a preferable nonparametric route is to exploit the correspondence of the autocovariance function and the spectral density via the wiener - khinchin theorem and nonparametrically estimate the spectral density . to this end , @xcite defined a pseudo - likelihood , known as the whittle likelihood , that directly depends on the spectral density rather than the autocovariance function and that gives a good approximation to the true gaussian and certain non - gaussian likelihoods . in the following subsection",
    "we will revisit this approximate likelihood proposed by @xcite , before introducing a semiparametric approach which extends the whittle likelihood .",
    "assume that @xmath0 is a real zero mean stationary time series with absolutely summable autocovariance function @xmath1 .",
    "under these assumptions the spectral density of the time series exists and is given by the fourier transform ( ft ) of the autocovariance function @xmath2 consequently , there is a one - to - one - correspondence between the autocovariance function and the spectral density , and estimation of the spectral density is amenable to smoothing techniques .",
    "the idea behind these smoothing techniques is the following observation , which also gives rise to the so - called whittle approximation of the likelihood of a time series : consider the periodogram of @xmath3 , @xmath4 the periodogram is given by the squared modulus of the discrete fourier coefficients , the fourier transformed time series evaluated at fourier frequencies @xmath5 , for @xmath6 .",
    "it can be obtained by the following transformation : define for @xmath7 @xmath8 where @xmath9 and for @xmath10 even , @xmath11 is defined analogously . then , @xmath12 is an orthonormal @xmath13 matrix ( cf .",
    "e.g.@xcite , paragraph 10.1 ) .",
    "real- and imaginary parts of the discrete fourier coefficients are collected in the vector @xmath14 and the periodogram can be written as @xmath15 it is well known that the periodograms evaluated at two different fourier frequencies are asymptotically independent and have an asymptotic exponential distribution with mean equal to the spectral density , a statement that remains true for non - gaussian and even non - linear time series @xcite .",
    "similarly , the fourier coefficients @xmath16 are asymptotically independent and normally distributed with variances equal to @xmath17 times the spectral density at the corresponding frequency .",
    "this result gives rise to the following whittle approximation in the frequency domain @xmath18 by the likelihood of a gaussian vector with diagonal covariance matrix @xmath19 as explicitly shown in appendix  [ sec : proofs ] , this yields the famous whittle likelihood in the time domain via the transformation theorem @xmath20 which provides an approximation of the true likelihood .",
    "it is exact only for gaussian white noise in which case @xmath21 .",
    "it has the advantage that it depends directly on the spectral density in contrast to the true likelihood that depends on @xmath22 indirectly via wiener - khinchin s theorem .",
    "sometimes , the summands corresponding to @xmath23 as well as @xmath24 ( the latter for @xmath10 even ) are omitted in the likelihood approximation .",
    "in fact , the term corresponding to @xmath23 contains the sample mean ( squared ) while the term corresponding to @xmath24 gives the alternating sample mean ( squared ) . both have somewhat different statistical properties and usually need to be considered separately .",
    "furthermore , the first term is exactly zero if the methods are applied to time series that have been centered first , while the last one is approximately zero and asymptotically negligible ( refer also remark  [ rem_firstlast ] ) .    the density of  @xmath25 under the i.i.d .",
    "standard gaussian working model is the whittle likelihood .",
    "it has two potential sources of approximation errors : the first one is the assumption of independence between fourier coefficients which holds only asymptotically but not exactly for a finite time series , the second one is the gaussianity assumption . in this paper , we restrict our attention to the first problem , extending the proposed methods to non - gaussian situations will be a focus of future work .",
    "in fact , the independence assumption leads to asymptotically consistent results for gaussian data .",
    "but even for gaussian data with relatively small sample sizes and relatively strong correlation the loss of efficiency of the nonparametric approach using the whittle likelihood can be substantial as shown in @xcite or by the simulation results of @xcite .",
    "the central idea in this work is to extend the whittle likelihood by proceeding from a certain parametric working model ( with mean 0 ) for @xmath26 rather than an i.i.d .",
    "standard gaussian working model before making a correction analogous to the whittle correction in the frequency domain .",
    "to this end , we start with some parametric likelihood in the time domain , such as e.g.  obtained from an arma - model , that is believed to be a reasonable approximation to the true time series .",
    "we denote the spectral density that corresponds to this parametric working model by @xmath27 . if the model is misspecified , then this spectral density is also wrong and needs to be corrected to obtain the correct second - order dependence structure . to this end",
    ", we define a correction matrix @xmath28 this is analogous to the whittle correction in the previous section as , in particular , @xmath29 with @xmath30 as in .",
    "however , the corresponding periodogram ordinates are no longer independent under this likelihood but instead inherit the dependence structure from the original parametric model ( see proposition [ cor_31 ] c ) .",
    "such an approach in a bootstrap context has been proposed and successfully applied by @xcite using an ar(@xmath31 ) approximation .",
    "this concept of a nonparametric correction of a parametric time domain likelihood is illustrated in the schematic diagram : @xmath32 as a result we obtain the following nonparametrically corrected likelihood function under the parametric working model @xmath33 where @xmath34 denotes the parametric likelihood .    [ rem_identifiability ] parametric models with a multiplicative scale parameter @xmath35 yield the same corrected likelihood as the one with @xmath36 , i.e.  if @xmath37 is used as working model this leads to the same corrected likelihood for all @xmath38 .",
    "for instance , if the parametric model is given by i.i.d .",
    "@xmath39 random variables with @xmath40 arbitrary , then the correction also results in the whittle likelihood ( for a proof we refer to appendix  [ sec : proofs ] ) .",
    "analogously , for linear models @xmath41 , @xmath42 , which includes the class of arma - models , the corrected likelihood is independent of  @xmath43",
    ".    we can now prove the following proposition which shows two important things : first , the corrected likelihood is the exact likelihood in case the parametric model is correct .",
    "second , the periodograms associated with this likelihood are asymptotically unbiased for the true spectral density regardless of whether the parametric model is true .    [ cor_31 ] let @xmath44 be a real zero mean stationary time series with absolutely summable autocovariance function @xmath45 and let @xmath46 for @xmath47 be the spectral density associated with the ( mean zero ) parametric model used for the correction .    1 .   if @xmath48 , then @xmath49 .",
    "the periodogram associated with the corrected likelihood is asymptotically unbiased for the true spectral density , i.e. @xmath50 where the convergence is uniform in @xmath51 .",
    "furthermore , @xmath52    the proof shows that the vector of periodograms under the corrected likelihood has exactly the same distributional properties as the vector of the periodograms under the parametric likelihood multiplied with @xmath53 .",
    "hence , asymptotic properties as the ones derived in theorem 10.3.2 in @xcite carry over with the appropriate multiplicative correction .    in the remainder of the paper",
    "we describe how to make use of this nonparametric correction in a bayesian set - up .",
    "to illustrate the bayesian semiparametric approach and how to sample from the pseudo - posterior distribution , in the following we restrict our attention to an ar(@xmath31 ) model as our parametric working model for the time series , i.e. @xmath54 , where @xmath55 are i.i.d .",
    "n(@xmath56 random variables with density denoted by @xmath57 .",
    "note that without loss of generality , @xmath58 , cf .",
    "remark  [ rem_identifiability ] .",
    "this yields the parametric likelihood of our working model , depending on the order  @xmath59 and on the coefficients  @xmath60 : @xmath61 with spectral density @xmath62 we assume the time series to be stationary and causal a priori .",
    "thus , @xmath63 is restricted such that  @xmath64 has no zeros inside the closed unit disc , c.f .",
    "theorem 3.1.1 . in @xcite . for",
    "now , we assume that the parameters  @xmath65 of the parametric working model are fixed ( and in practice set to bayesian point estimates obtained from a preceding parametric estimation step ) .",
    "an extension to combine the estimation of the parametric model with the nonparametric correction will be detailed later in section  [ sec_priorparametric ] .      for a bayesian analysis using",
    "either the whittle or nonparametrically corrected likelihood , we need to specify a nonparametric prior distribution for the spectral density . here",
    "we employ the approach by @xcite which is essentially based on the bernstein polynomial prior of @xcite as a nonparametric prior for a probability density on the unit interval .",
    "we briefly describe the prior specification and refer to @xcite for further details .",
    "in contrast to the approach in @xcite , we do not specify a nonparametric prior distribution for the spectral density  @xmath66 , but for a pre - whitened version thereof , incorporating the spectral density of the parametric working model into the estimation .",
    "to elaborate , for  @xmath67 , consider the _ eta - damped correction function _",
    "@xmath68 this corresponds to a reparametrization of the likelihood   by replacing @xmath69 with @xmath70 .",
    "[ rem_etacorrection ] the parameter @xmath71 models the confidence in the parametric model : if @xmath71 is close to  @xmath72 and the model is well - specified , then  @xmath73 will be much smoother than the original spectral density , since @xmath27 already captures the prominent spectral peaks of the data very well . as a consequence ,",
    "nonparametric estimation of  @xmath74 should involve less effort than nonparametric estimation of  @xmath66 itself .",
    "this remains true in the misspecified case , as long as the parametric model does describe the essential features of the data sufficiently well in the sense that it captures at least the more prominent peaks .",
    "however , it is possible that the parametric model introduces erroneous spectral peaks if the model is misspecified . in that case",
    ", @xmath71 close to zero ensures a damping of the model misspecification , such that nonparametric estimation of  @xmath74 should involve less effort than nonparametric estimation of  @xmath53 .",
    "the choice of @xmath71 will be detailed in section  [ sec_priorparametric ] , but for now , @xmath71 is assumed fixed .",
    "we reparametrise @xmath74 to a density function @xmath75 on @xmath76 $ ] via @xmath77 with normalization constant @xmath78 .",
    "thus , a prior for @xmath73 may be specified by putting a bernstein polynomial prior on @xmath75 and then an independent inverse - gamma@xmath79 prior on @xmath80 , its density denoted by @xmath81 .",
    "the bernstein polynomial prior of @xmath82 is specified in a hierarchical way as follows :    1 .",
    "@xmath83 \\beta(\\omega|j , k - j+1)$ ] where @xmath84=g(v)-g(u)$ ] for a distribution function @xmath85 and @xmath86 is the beta density with parameters @xmath87 and @xmath88 .",
    "2 .   @xmath85 has a dirichlet process distribution with base measure @xmath89 , where @xmath90 is a constant and @xmath91 a distribution function with lebesgue density @xmath92 .",
    "3 .   @xmath93 has a discrete distribution on the integers @xmath94 , independent of @xmath85 , with probability function @xmath95 .",
    "note that smaller values of @xmath93 yield smoother densities .",
    "furthermore , we achieve an approximate finite - dimensional characterization of this nonparametric prior in terms of @xmath96 parameters @xmath97 by employing the truncated sethuraman ( 1994 ) representation of the dirichlet process @xmath98 with @xmath99 , @xmath100 for @xmath101 , @xmath102 , and @xmath103 , all independent .",
    "this gives a prior finite mixture representation of the eta - damped correction @xmath104 where @xmath105 and @xmath106 .",
    "the joint prior density of  @xmath107 by means of this finite - dimensional approximation can be written as @xmath108    here , we specify a diffuse prior by choosing the uniform distribution for @xmath91 and @xmath109 .",
    "we set @xmath110 , @xmath111 and follow the recommendation by @xcite for the truncation point @xmath112 .",
    "the prior   on @xmath73 induces a prior on @xmath66 by multiplication with  @xmath113 , see .",
    "accordingly , the pseudo - posterior distribution of @xmath66 can be computed as prior times the corrected parametric likelihood : @xmath114 where  @xmath115 and  @xmath116 as in  .",
    "samples from the pseudo - posterior distribution can be obtained via gibbs sampling following the steps outlined in @xcite .",
    "the full conditional for @xmath93 is discrete and readily sampled , as is the conjugate full conditional of @xmath80 .",
    "we use the metropolis algorithm to sample from each of the full conditionals of @xmath117 and @xmath118 using the uniform proposal density of @xcite .",
    "[ rem_firstlast ] as in @xcite , we omit the first and last terms in the corrected likelihood that correspond to @xmath119 and @xmath120 ( and setting @xmath121 as well as @xmath122 ) .",
    "this is due to the role that the corresponding fourier coefficients play ( being equal to the sample mean respectively alternating sample mean ) , which typically requires a special treatment ( see proposition 10.3.1 and ( 10.4.7 ) in @xcite ) .",
    "for the application to spectral density estimation in this paper this leads to more stable statistical procedures irrespective of the true mean of the time series .",
    "however , in situations , where the time series is merely used as a nuisance parameter such as regression models , change point or unit - root testing , these coefficients should be included and the likelihood used for the time series @xmath123 , where @xmath124 is the mean ( not the sample mean ) of the time series .      in this section",
    ", we will show consistency of the pseudo - posterior distribution based on the bernstein polynomial prior and the corrected likelihood for a given working model under the same assumptions as @xcite . throughout the section",
    ", we will make the following assumption :    [ ass_1 ]    1 .",
    "denote by @xmath125 and @xmath27 the autocovariance function respectively spectral density of the parametric working model .",
    "assume that @xmath126 2 .",
    "let @xmath44 be a stationary mean zero gaussian time series with autocovariance function @xmath127 and spectral density @xmath128 fulfilling @xmath129 denote by  @xmath130 and  @xmath131 the density and the distribution of  @xmath132 .",
    "an important first observation is , that the corrected likelihood , the whittle likelihood as well as the true likelihood are all mutually contiguous in the gaussian case .",
    "this fact may also be of independent interest :    [ theorem_contiguity ] under assumptions  @xmath133.[ass_1 ] the true density @xmath130 , the whittle likelihood @xmath134 given in as well as the corrected ( gaussian ) parametric likelihood @xmath135 given in   are all mutually contiguous .    with the help of this theorem we are now able to prove posterior consistency under certain assumptions on the time series and prior .",
    "[ theorem : consistency - corrected ] let  @xmath136 fixed . let assumptions  @xmath133.[ass_1 ] hold in addition to the following assumptions on the prior for  @xmath73 :    * for all @xmath93 , @xmath137 for some constants @xmath138 , * @xmath92 is bounded , continuous and bounded away from 0 , * the parameter @xmath80 is assumed fixed and known .",
    "let  @xmath139 .",
    "then the posterior distribution is consistent , i.e.  for any @xmath140 , @xmath141 in @xmath131-probability , where @xmath142 denotes the pseudoposterior distribution computed using the corrected likelihood .      in the previous sections ,",
    "the parameters of the working model were assumed to be fixed , as e.g. obtained in an initial pre - estimation step . from a bayesian perspective , it is desirable to couple the inference about the parametric working model with the nonparametric correction , allowing for the inclusion of prior knowledge about the model and for uncertainty quantification about the interaction of model and correction .",
    "thus , for a fixed order  @xmath31 , we include both the autoregressive parameters  @xmath143 and the spectral shape confidence  @xmath71 from   into the bayesian inference .",
    "the introduction of the parameter @xmath71 effectively robustifies the procedure in the sense that it guarantees our method will not be worse than a corresponding fully nonparametric one .    to ensure stationarity and causality ( and hence identifiability ) of the parametric model",
    ", we put a prior on the partial autocorrelations  @xmath144 with @xmath145 for @xmath146 .",
    "the autoregressive parameters  @xmath147 can be readily obtained from this parametrisation ( see appendix  [ sec_appendixsampling ] ) .",
    "we consider the following prior specification for the spectral density : @xmath148 with a bernstein - dirichlet prior on  @xmath73 as in section  [ sec_bernsteindirichlet ] , a uniform prior on @xmath71 and uniform priors on the @xmath149 s , all a priori independent .",
    "of course , it is possible to employ different prior models ( see @xcite ) . in conjunction with the corrected parametric likelihood",
    ", we obtain samples from the joint pseudo - posterior distribution @xmath150 analogously to section  [ sec_posteriorcomputation ] via gibbs sampling .",
    "note that , since the corrected parametric likelihood is the lebesgue density of a probability measure , it is sufficient that the prior distributions are proper for the posterior distribution to be proper .",
    "we use random walk metropolis - within - gibbs steps with normal proposal densities to sample from the full conditionals of  @xmath71 and  @xmath151 respectively . the proposal variance for  @xmath71",
    "is set to  0.01 , where proposals larger than 1 ( smaller than 0 ) are truncated at 1 ( at 0 ) . to achieve proper mixing of the parametric model parameters , the proposal variances  @xmath152 for  @xmath149",
    "are determined adaptively as described in @xcite during the burn - in period , aiming for an acceptance rate of  0.44 , where proposals with absolute value larger or equal to one are discarded .",
    "[ rem_arorder ] the autoregressive order  @xmath31 is assumed to be fixed . in our approach , it is determined in a preliminary model selection step",
    ". however , it is also possible to include the autoregressive order in the bayesian inference by using a reversible - jump markov chain monte carlo scheme @xcite or stochastic search variables @xcite .",
    "in this section , we evaluate the finite sample behavior of our _ nonparametrically corrected ( npc ) _ approach to bayesian spectral density estimation numerically .",
    "to demonstrate the trade - off between the parametric working model and the nonparametric spectral correction , we compare our approach to both fully parametric and fully nonparametric approaches .",
    "we first present the results of a simulation study with arma data in section  [ sec : simarma ] before considering sunspot data in section  [ sec : sunspot ] and gravitational wave data in section  [ sec : ligo ] .",
    "an implementation of all procedures presented below is provided in the ` r ` package ` beyondwhittle ` , which is available on cran , see @xcite .",
    "we consider data generated from the arma model @xmath153 with standard gaussian white noise  @xmath154 and different values of  @xmath155 and  @xmath10 .",
    "the following competing approaches are compared with npc :    _ nonparametric estimation ( np ) . _",
    "the procedure from @xcite , which is based on the whittle likelihood and a bernstein - dirichlet prior on the spectral density . note that this coincides with the npc approach with a white noise parametric working model ( @xmath156 ) , c.f .",
    "remark  [ rem_identifiability ] .",
    "_ autoregressive estimation ( ar ) .",
    "_ for  @xmath157 , an autoregressive model of order  @xmath31 is fitted to the data using a bayesian approach with the same partial autocorrelation parametrization and the same prior assumptions as for the parametric working model within the nonparametrically corrected likelihood procedure , see section  [ sec_priorparametric ] ( for details on the sampling scheme we refer to appendix  [ sec_appendixsampling ] ) .",
    "the order  @xmath158 minimizing the dic is then chosen for model comparison .    the working model in the npc approach",
    "is chosen to be the  ar(@xmath158 ) model from the ar procedure .",
    "the prior for the working model parameters is as described in section  [ sec_priorparametric ] and the prior for the nonparametric correction is as described in section  [ sec_bernsteindirichlet ] . for the np approach ,",
    "the same bernstein - dirichlet prior for  @xmath66 is used as for  @xmath73 in the npc approach .",
    "we compare the average integrated absolute error ( aiae ) of the posterior median spectral density estimate and the empirical coverage probability of a uniform credible interval ( cuci ) .",
    "note that pointwise posterior credible intervals are not suited for investigating coverage , since they do not take the multiple testing problem into account that arises at different frequencies .",
    "following @xcite ( see also @xcite ) , a uniform credible interval for the spectral density can be constructed as follows : denote by  @xmath159 the posterior spectral density samples obtained by one of the procedures . then for  @xmath160 the uniform @xmath161-credible interval is given by @xmath162\\ ] ] where  @xmath163 denotes the sample median at frequency  @xmath164 , @xmath165",
    "the median absolute deviation of  @xmath166 and  @xmath167 is chosen such that @xmath168 the intervals are constructed on a logarithmic scale to ensure that their covered range contains only positive values .",
    "because small values of  @xmath169 lead to very large absolute values on a log scale , we do not employ the usual logarithm , but the fuller - logarithm as described in  @xcite , page 496 , i.e. @xmath170 for some small  @xmath171 .",
    "we use  @xmath172 and @xmath173 in our simulations .",
    "the chains were run for 12,000 iterations for ar ( after a burn - in period of 8,000 iterations ) and for 20,000 iterations for np and npc ( after a burn - in period of 30,000 iterations ) , where a thinning of  4 was employed for np and npc .",
    "we choose  @xmath174 and consider lengths  @xmath175 from model   with  @xmath176 replicates ( @xmath177 a power of  2 to use the computational resources efficiently ) respectively .",
    "the results are shown in table  1 . for  ar(1 ) data , the ar procedure yields the best results ( in terms of both aiae and cuci ) , whereas the np performs worst .",
    "it can be seen that ar and npc benefit from the well - specified parametric model . for  ma(1 )",
    "data , however , the ar approach yields the worst results , whereas npc benefits from the nonparametric correction , yielding only slighly worse integrated errors than np , although with superior uniform credible intervals for  @xmath178 . in case of arma(1,1 ) data , the estimation does not benefit from the autoregressive fit , i.e. the moving average misspecification dominates the estimation .",
    "thus the results are similar to the ma(1 ) case .",
    "further results for data from the arma model can be found in appendix  [ sec_appendixsims ] .",
    "max width=    [ rem_visualinspection ]     under relatively weak conditions ( see e.g.  @xcite ) a linear process can be written as an ar(@xmath179)-process with white noise errors ( similarly to the famous wold representation ) .",
    "consequently , an ar(@xmath31)-model with sufficiently large order captures the structure of a ( gaussian ) linear process to any degree of accuracy . in this sense , the use of an ar - model with a sufficiently large order can be viewed as a nonparametric procedure , a fact , that has been exploited by ar - sieve - bootstrap methods for quite some time .",
    "for a recent mathematical analysis of the validity and limitations of this approach we refer to  @xcite .",
    "consequently , an ar - model can still be used for spectral density estimation under misspecification as long as the order is sufficiently large . in this sense",
    "standard order selection techniques such as dic - minimization tend to choose large orders in this situation . however , looking at scree - like plots of the negative maximum log likelihood for increasing orders one can often see a clear bend ( elbow ) in the curve ( with a slow decay from that point on that is not slow enough to be captured by standard penalization techniques ) .",
    "similar to the use of scree plots in the context of pca , that point can be seen as a reasonable truncation point ( elbow criterion ) where those features best explained by the parametric model have been captured . while this small model does not yet fully explain the data , adding more parameters is not helping the nonparametrically corrected procedure that we propose .",
    "in other words , we are not interested in an elaborate ar(@xmath179 ) approximation but rather in a proxy model that captures the main parametric features of the data .    in the context of an autoregressive working model , we approximate the negative maximum log - likelihood by the negative log - likelihood evaluated at the yule - walker estimate .",
    "this is to ensure numerical stability and computational speed , especially for large orders .",
    "the approximation is motivated by the asymptotic equivalence of both estimates , see e.g. chapter 8 in  @xcite .",
    "the estimate is referred to as _ negative maximum log - likelihood _ in the text .",
    "figure  [ fig : visualinspectionarma ] shows the scree - like plots for three exemplary arma(1,1 ) realizations from the above model as well as the sunspot data set . in all three realizations the elbow is clearly at @xmath180 ( which is consistent with the ar - part of the model ) , while the dic - criterion choses orders between 4 and 6 .",
    "table  2 shows the simulation results for the arma(1,1 ) model and a fixed order of @xmath180 . while a parametric ar(@xmath72 ) model is clearly not able to explain the data ( see e.g.  the zero coverage of the uniform credibility intervals ) , this choice of the order significantly improves the results of the npc procedure for the arma(1,1 ) data . in fact , the latter is now better than both the ar procedure as well as the np procedure while at the same time the confidence in the model as indicated by @xmath181 increases .",
    "max width=    for the sunspot data that effect can also be seen clearly as the above procedure proposes to use @xmath182 in the nonparametric procedure while the dic - criterion suggests @xmath183 . for a detailed discussion of this data analysis",
    "we refer to section  [ sec : sunspot ] , similar effects for the ligo data are discussed in section  [ sec : ligo ] .      in this section , we analyse the yearly sunspot data from 1700 until 1987 .",
    "we take the mean - centered version of the square root of the 288 observations as input data .",
    "we compare the ar and the npc procedure for fixed values  @xmath184 .",
    "while  @xmath183 minimises the dic , @xmath182 captures the main ar - features of the data as indicated by the elbow criterion ( see remark  [ rem_visualinspection ] and figure  [ fig : visualinspectionarma ]  ( d ) ) .",
    "the results are shown in figure  [ fig_sunspot ] .    .",
    "the log - periodogram is visualised in grey . ]",
    "while for @xmath180 the bernstein polynomials of the nonparametric correction can not yet capture the peaks sufficiently well , this is clearly the case for @xmath182 ( the choice obtained from the elbow criterion ) : while the parametric model itself can clearly not yet explain the data well , enough features are captured to improve the nonparametric correction . for larger order choices , the estimate from the npc method does not change much anymore , so that the correction does indeed not profit from additional parameters in the ar - model .",
    "in fact for @xmath183 ( as indicated by dic ) , the bayes estimator of ar and npc are very similar .      gravitational waves , ripples in the fabric of spacetime caused by accelerating massive objects , were predicted by albert einstein in 1916 as a consequence of his general theory of relativity , see @xcite .",
    "gravitational waves originate from non - spherical acceleration of mass - energy distributions , such as binary inspiraling black holes , pulsars , and core collapse supernovae , propagating outwards from the source at the speed of light .",
    "however , they are very small ( a thousand times smaller than the diameter of a proton ) so that their measurement has provided decades of enormous engineering challenges .    on sept .",
    "14 , 2015 , the laser interferometric gravitational wave observatory ( ligo ) , see @xcite , made the first direct detection of a gravitational wave signal , gw150914 , originating from a binary black hole merger  @xcite .",
    "the two l - shaped ligo instruments ( in hanford , washington and livingston , louisiana ) each consist of two perpendicular arms , each 4 kilometers long .",
    "a passing gravitational wave will alternately stretch one arm and squeeze the other , generating an interference pattern which is measured by photo - detectors .",
    "the detector output is a time series that consists of the time - varying dimensionless strain @xmath185 , the relative change in spacing between two test masses .",
    "the strain can be modelled as a deterministic gravitational wave signal @xmath186 depending on a vector @xmath187 of unknown waveform parameters plus additive noise @xmath188 , such that @xmath189    there are a variety of noise sources at the ligo detectors .",
    "this includes _",
    "seismic _ noise , due to the motion of the mirrors from ground vibrations , earthquakes , wind , ocean waves , and vehicle traffic , _",
    "thermal _ noise , from the microscopic fluctuations of the individual atoms in the mirrors and their suspensions , and _ shot _ noise , due to the discrete nature of light and the statistical uncertainty from the `` photon counting '' that is performed by the photo - detectors .",
    "in particular , ligo noise includes high power , narrow band , spectral lines , visible as sharp peaks in the log - periodogram .",
    "as the ligo spectrum is time - varying and subject to short - duration large - amplitude transient noise events , so - called `` glitches '' , a precise and realistic modelling and estimation of the noise component jointly with the signal is important for an accurate inference of the signal parameters @xmath187 .",
    "the current approach , which was also used for estimating the parameters of gw150914 in @xcite , is to first use the welch method @xcite to estimate the spectral density from a separate stretch of data , close to but not including the signal and then to assume stationary gaussian noise with this known spectral density in order to estimate the signal parameters .",
    "several approaches have been suggested in the recent gravitational wave literature to simultaneously estimate the noise spectral density and signal parameters .",
    "these include generalising the whittle likelihood to a student - t likelihood as in @xcite , similarly modifying the likelihood to include additional scale parameters and then marginalising over the uncertainty in the psd as in @xcite , using cubic splines for smoothly varying broad - band noise and lorentzians for narrow - band line features as in @xcite , a morlet - gabor continuous wavelet basis for both gravitational wave burst signals and glitches as in @xcite , the nonparametric approach of @xcite using a dirichlet - bernstein prior @xcite and a generalisation of this using a b - spline prior , see @xcite .",
    "we consider 1 s of real ligo data collected during the sixth science run ( s6 ) , recoloured to match the target noise sensitivity of advanced ligo @xcite .",
    "the data is differenced and then multiplied by a hann window to mitigate spectral leakage .",
    "a low - pass butterworth filter ( of order 20 and attenuation 0.25 ) is then applied before downsampling from a ligo sampling rate of 16384 hz to 4096 hz , reducing the volume of data .    )",
    "models applied to advanced ligo s6 data . ]",
    "we first run a pure nonparametric model , corresponding to a nonparametrically corrected likelihood with an ar(0 ) working model ( i.e.  the whittle likelihood ) to estimate the spectral density .",
    "we then compare this to a nonparametrically corrected model with an order of @xmath190 , where a clear elbow can be seen in the negative log - likelihood plot ( see figure  [ fig_screeligo ] and remark  [ rem_visualinspection ] ) .",
    "we run these simulations for 100,000 mcmc iterations , with a burn - in of 50,000 , and thinning factor of 5 .",
    "results are illustrated in figure  [ fig_gw_npc ]  ( a ) .",
    "even though @xmath93 converged to @xmath191 mixture components , it is clear that the bernstein - dirichlet prior together with the whittle likelihood is not flexible enough to estimate the sharp peaks of the ligo spectral density .",
    "the parametric ar(14 ) model ( estimated using the bayesian autoregressive sampler described in appendix  [ sec_appendixsampling ] ) captures the four main peaks but not their sharpness .",
    "additionally , it does not capture the structure well in the frequency bands  0 to  450  hz as well as larger than  1100  hz . when compared to the ar(0 ) model , the nonparametrically corrected model based on @xmath192 estimates the sharp peaks much better .",
    "furthermore , it sharpens all four peaks of the ar(14)-model ( with a slight exception around 400  hz , where seemingly two very sharp peaks overlap , a feature that is not captured by the ar(14 ) model at all ) . in the frequency bands 0 to 450  hz as well as larger than 1100  hz , where the parametric model fails altogether , the correction yields similar results to the nonparametric whittle procedure . similarly to the nonparametric whittle procedure @xmath93 tends towards @xmath193 indicating that the bernstein - dirichlet prior together with an ar(@xmath194)-model is not yet flexible enough for this data set .",
    "looking closer at figure  [ fig_screeligo ] , the negative log - likelihood between the models ar(14 ) and ar(35 ) decreases by 533 , which is not as sharp as the elbow at @xmath192 , but still significant  keeping in mind that the ligo data is a very complex data set ",
    "much more so than the arma(1,1 ) or the sunspot data .",
    "there is a moderately sized jump before @xmath195 , while afterwards the descend slows down significantly .",
    "in fact the bic chooses an order of @xmath196 , where the log - likelihood reaches the level of 879 , showing that the difference between @xmath192 and @xmath195 ( of 533 ) is comparable to the one between @xmath195 and @xmath197 ( of 512 ) .",
    "this indicates that there is indeed another change of gradient around @xmath195 .",
    "when looking at penalized likelihoods , this is also the point , where different penalizations start to obviously diverge .",
    "the results for @xmath195 can be found in figure  [ fig_gw_npc ]  ( b ) .",
    "the parametric ar(@xmath198 ) model already provides a reasonable fit to the periodogram , picking up the major peaks ( with the exception around 100  hz ) , but under- and overestimates some of the peaks .",
    "in particular there are still major problems in the frequency bands 0 to 300 hz and 400 to 700  hz .",
    "the npc procedure with @xmath195 keeps the peaks that have been capture well by the parametric model but corrects problems most prominently in the above mentioned frequency bands .",
    "it is worth mentioning that the correction works in several ways : sharpening existing peaks ( e.g.  at 0 hz ) , adding new peaks ( e.g.  at 100 hz ) as well as smoothing out some erroneous peaks ( e.g.  at 600 hz ) .",
    "overall , the resulting estimate seems to capture the structure quite well in all frequency bands . this impression is complemented by the results of the npc method with ar(@xmath198 ) working model together with the pointwise and uniform credible bounds obtained from the procedure in figure  [ fig_gw_ci ] .",
    "in this work we propose a nonparametric correction of a parametric likelihood to obtain an approximation of the true ( inaccessible ) likelihood of a stationary time series .",
    "this approach extends the famous approximation by whittle . for gaussian data , the whittle likelihood , the nonparametric correction as well as the true likelihood are asymptotically equivalent .",
    "secondly , we propose a bayesian procedure for spectral density estimation , where a parametric likelihood is used with a bayesian nonparametric spectral correction .",
    "we show consistency of the resulting pseudo - posterior distribution for a fixed parametric likelihood .",
    "furthermore , we present a bayesian semiparametric procedure that combines inference about the parametric working model with the nonparametric correction .",
    "the extent of the contribution of the parametric spectral density to the spectral density estimate is controlled by a shape confidence parameter .",
    "simulation results have shown that this procedure inherits the benefits from the parametric working model if the latter is well - specified or describe a part of the features of the data well , while in the misspecified case the results are comparable to the usage of the whittle likelihood .    regarding future work , it is interesting to investigate whether in the non - gaussian case the class of time series for which asymptotically consistent inference holds can be enlarged by choosing an appropriate model .",
    "it is important to understand the distributional influence in the non - gaussian case both in finite samples and asymptotically . as an example , in a bootstrap context , it suffices to capture the fourth order structure of a linear model to obtain asymptotically valid second - order frequentist properties of the autocovariance structure @xcite . as suggested by @xcite in a parametric",
    "setting , this property does not simply carry over to a bayesian context .",
    "preliminary results for non - gaussian autoregressive time series however have shown considerable benefits ( with respect to first and second order frequentist properties ) when the innovation distribution is well - specified in comparison to a gaussian model .",
    "since any parametric likelihood is susceptible to misspecification , the ultimate goal is to consider a bayesian nonparametric model for the innovation distribution , such as dirichlet mixtures of normals .",
    "further directions for future work are automation of the elbow criterion discussed in remark  [ rem_visualinspection ] . instead of choosing a fixed order in advance , an automation might as well serve as a guideline for specifying a prior on the ar order parameter , which can be included in the inference by means of rjmcmc ( c.f .",
    "remark  [ rem_arorder ] ) .",
    "this work was supported by dfg grant ki 1443/3 - 1 .",
    "furthermore , the research was initiated during a visit of the fourth author at karlsruhe institute of technology ( kit ) , which was financed by the german academic exchange service ( daad ) .",
    "some of the preliminary research was conducted while the first author was at kit , where her position was financed by the stifterverband fr die deutsche wissenschaft by funds of the claussen - simon - trust .",
    "we also thank the new zealand escience infrastructure ( nesi ) and the universittsrechenzentrum ( urz ) magdeburg for their high performance computing facilities , and the centre for eresearch at the university of auckland and jrg schulenburg for their technical support .",
    "since @xmath199 is orthonormal it holds @xmath200 and hence by an application of the transformation theorem @xmath201 where @xmath202 . finally , by , @xmath203 as well as @xmath204 it holds @xmath205 yielding the assertion .",
    "the spectral density corresponding to a gaussian white noise with variance @xmath43 is given by @xmath206 , hence @xmath207 and @xmath208 which can be shown to be the whittle likelihood analogously to the proof of equation  [ eq_whittle ] since @xmath209 .    if @xmath48 then @xmath210 and @xmath211 , hence a ) follows .",
    "for b ) , consider a time series distributed according to the corrected likelihood , i.e.  @xmath212 .",
    "an application of the transformation theorem shows that @xmath213 on noting that @xmath214 and @xmath215 . by",
    "it holds @xmath216 as @xmath217 . by  @xcite , proposition 10.3.1 . , it holds @xmath218 where convergence is uniform in @xmath219 ( recall that the time series is mean zero ) . from this",
    "the assertion follows because @xmath220 is bounded from below by assumption .",
    "@xcite proved mutual contiguity of the true gaussian and the whittle likelihood in the frequency domain which carries over to the time domain by an application of the transformation theorem because @xmath199 is bijective .",
    "hence , it is sufficient to show mutual contiguity of the corrected parametric likelihood and the whittle likelihood . following the proof of theorem 1 in @xcite it is enough to show that their log - likelihood ratio is a tight sequence under both @xmath221 as well as @xmath222 . to this end , note that it holds @xmath223 where @xmath224 is the covariance matrix of the corresponding parametric time series , e.g. for the ar(@xmath31)-case the covariance matrix associated with likelihood  .",
    "hence , the log - likelihood ratio is given by @xmath225 defining @xmath226 analogously to @xmath30 as in with the nonparametric spectral density @xmath22 replaced by the parametric version @xmath220 as e.g. for ar(@xmath31 ) given in , we get @xmath227 where the boundedness follows from lemma a.1 in @xcite .",
    "to obtain stochastic boundedness of @xmath228 under @xmath222 as well as @xmath221 we will show boundedness of the expectation and variance .",
    "following @xcite we get under @xmath222 ( i.e.  @xmath229 ) @xmath230 because @xmath231 , it holds by @xmath232 @xmath233 the assertion follows by lemma a.2 in @xcite by the linearity of the trace .",
    "similar arguments yield the assertion under @xmath221 ( i.e.  @xmath234 noting that @xmath235 concerning the variance we get under @xmath222 @xmath236 similar arguments as above yield @xmath237 together with this yields @xmath238 by @xmath239 .",
    "hence , the assertion follows by lemma a.2 in @xcite .",
    "analogous assertions yield the result under @xmath221 .",
    "@xcite give sufficient conditions for the consistency of the posterior distribution when using bernstein polynomial priors in terms of the existence of exponentially powerful tests for testing @xmath240 and prior positivity of a kullback - leibler neighbourhood .",
    "but these require i.i.d . observations . to prove posterior consistency under the whittle likelihood ,",
    "@xcite extend this result to independent but not identically distributed observations and apply this to the periodogram ordinates which are independent exponential random variables under the whittle likelihood . however , periodogram ordinates under the corrected likelihood are no longer independent , therefore this theorem is not applicable .",
    "we give an extension to non - independent random variables in the following theorem , which is needed to prove theorem  [ theorem : consistency - corrected ] :    [ theorem : consistency - general ] let @xmath241 be random vectors with probability distribution @xmath242 and corresponding pdf @xmath243 .",
    "let @xmath244 , @xmath245 , where @xmath246 denotes the borel @xmath247-algebra on @xmath248 , and @xmath249 a probability distribution on @xmath250 .",
    "define @xmath251 \\mbox { and } \\\\",
    "v_n(\\theta_0,\\theta ) & = & { \\operatorname{var}}_{\\theta_0}\\left [ \\log \\frac{p_n({{\\mathbf{z}}}_n|\\theta_0)}{p_n({{\\mathbf{z}}}_n|\\theta ) } \\right ] .\\end{aligned}\\ ] ] under the following assumptions of prior positivity of neighbourhoods and existence of tests :    * there exists a set @xmath252 with @xmath253 such that * * @xmath254 , and * * @xmath255 .",
    "* there exists test functions @xmath256 , subsets @xmath257 , and constants @xmath258 such that * * @xmath259 .",
    "* * @xmath260 , and * * @xmath261    then @xmath262    the proof is completely analogous to the proof of theorem a.1 in @xcite",
    ".    the following lemma replaces lemma b.3 in @xcite .",
    "[ lemma_b3 ] let @xmath263 where @xmath264 is a symmetric positive definite matrix and @xmath265 .",
    "with @xmath266 consider testing @xmath267 where @xmath268 do not depend on @xmath10",
    ". then there exists a test @xmath269 and constants @xmath270 depending only on @xmath271 and @xmath272 such that @xmath273    consider a test @xmath269 that rejects @xmath274 if @xmath275 with a critical value @xmath276 with @xmath277 and @xmath278 .    denote @xmath279 , then @xmath280 consequently , the moment generating function of @xmath281 under @xmath274 is given by @xmath282=(1 - 2t)^{-n/2}$ ] and exists for @xmath283 . by an application of the markov inequality , we get for all @xmath284 @xmath285=e^{-n [ -zx+\\frac 1 2 \\log(1 + 2z)]}.\\end{aligned}\\ ] ] the function @xmath286 attains its maximum at @xmath287 and @xmath288 as @xmath289 for @xmath290 . thus ,",
    "setting @xmath291 , we obtain @xmath292 similarly , under @xmath293 , we get @xmath294 since @xmath295s_n^{-1/2})=\\mbox{det}(d_n^{1/2}(f_1/f_0)-\\lambda i_n)\\end{aligned}\\ ] ] the matrix @xmath296 has the eigenvalues @xmath297 , @xmath298 .",
    "since  @xmath296 is a normal matrix ( recall that  @xmath264 is symmetric positive definite ) , we find that @xmath299 has the eigenvalues @xmath300 , @xmath298 .",
    "consequently , @xmath301 consequently , @xmath302 analogously to the proof under the null hypothesis , we get for any @xmath303 @xmath304 now @xmath305 , @xmath306 , attains its maximum at @xmath307 with @xmath308 as @xmath309 for @xmath310 , i.e.  @xmath278 . setting @xmath311 yields @xmath312 , completing the proof .",
    "we follow @xcite , proof of theorem 1 , and show ( c1 ) and ( c2 ) of theorem  [ theorem : consistency - general ] above .",
    "let @xmath313 denote the corrected likelihood and define @xmath314}|c_{0,\\eta}(\\lambda)|$ ] and @xmath315 }    |c_{0,\\eta}(\\lambda)|$ ] .",
    "an analogous argument as in appendix b.1 .",
    "of @xcite shows that for all @xmath140 the set @xmath316 has positive prior probability under the bernstein polynomial prior on @xmath248 .",
    "we need to show ( c1)(a ) and ( b ) . to this end , let @xmath317 where @xmath318 for @xmath317 we have @xmath319 . to prove ( c1 ) ,",
    "first note that @xmath320 where @xmath321 .",
    "for @xmath322 it holds @xmath323 as well as @xmath324 .",
    "furthermore , by analogous argument as in the proof of lemma  [ lemma_b3 ] we get that the eigenvalues of @xmath325 are given by @xmath326 , @xmath298 , hence @xmath327 as a consequence , we get @xmath328 similar arguments yield @xmath329\\\\ & = \\frac{1}{2n^2}\\sum_{i=1}^n\\left ( \\frac{c_{0,\\eta}(\\lambda_i)-c(\\lambda_i)}{c(\\lambda_i ) } \\right)^2    = o(1/n)\\;\\|c - c_{0,\\eta}\\|_{\\infty}^2 \\to 0.\\end{aligned}\\ ] ] the proof can now be concluded as in @xcite by replacing lemma b.3 by lemma  [ lemma_b3 ] above .",
    "for fixed order  @xmath330 , the autoregressive model  @xmath331 with i.i.d .",
    "n(@xmath332 ) random variables is parametrized by the the innovation variance  @xmath43 and the partial autocorrelation structure  @xmath333 , where @xmath334 is the conditional correlation between  @xmath335 and  @xmath336 given  @xmath337 .",
    "note that  @xmath338 and that there is a one - to - one relation between  @xmath339 and  @xmath340 .",
    "to elaborate , we follow @xcite and introduce the auxiliary variables  @xmath341 as solutions of the following yule - walker - type equation with the autocorrelations @xmath342/{\\operatorname{e}}z_1 ^ 2 $ ] : @xmath343 as shown in  @xcite , the well - known relationships @xmath344 readily imply  @xmath345 and ( recall that  @xmath346 for  @xmath347 ) the recursive formula @xmath348 we specify the following prior assumptions for the model parameters : @xmath349 all a priori independent .",
    "we use  @xmath350 .",
    "furthermore , we employ the gaussian likelihood @xmath351 with the n(@xmath332 ) density  @xmath352 and the @xmath353 autocovariance matrix @xmath354 of the autoregressive model . to draw samples from the corresponding posterior distribution",
    ", we use a gibbs sampler , where the conjugate full conditional for  @xmath43 is readily sampled . for",
    "@xmath355 the full conditional for  @xmath356 is sampled with the metropolis algorithm using a normal random walk proposal density with proposal variance  @xmath152 . as for the parameters of the working model within the corrected parametric approach ( see section  [ sec_priorparametric ] ) , the proposal variances are adjusted adaptively during the burn - in period , aiming for a respective acceptance rate of  0.44 .",
    "table  2 depicts further results of the ar , np and npc procedures for normal arma data .      for white noise ,",
    "all procedures yield good results , whereas np is superior due to the implicitly well - specified white noise working model ( recall that the order  @xmath31 within ar and npc are estimated with the dic ) . the results for ma(2 )",
    "data are qualitatively similar to the results for ma(1 ) data in table  1 .",
    "the spectral peaks of the ar(2 ) model are not as strong as the spectral peak of the ar(1 ) model considered in table  1 .",
    "accordingly , it can be seen that the np results are better in this case .",
    "the npc benefits again from the well - specified parametric model , yielding results that are comparable to the ar procedure ."
  ],
  "abstract_text": [
    "<S> * abstract . * the whittle likelihood is widely used for bayesian nonparametric estimation of the spectral density of stationary time series . </S>",
    "<S> however , the loss of efficiency for non - gaussian time series can be substantial . </S>",
    "<S> on the other hand , parametric methods are more powerful if the model is well - specified , but may fail entirely otherwise . </S>",
    "<S> therefore , we suggest a nonparametric correction of a parametric likelihood taking advantage of the efficiency of parametric models while mitigating sensitivities through a nonparametric amendment . using a bernstein - dirichlet prior for the nonparametric spectral correction , we show posterior consistency and illustrate the performance of our procedure in a simulation study and with ligo gravitational wave data . </S>"
  ]
}