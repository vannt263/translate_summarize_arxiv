{
  "article_text": [
    "suppose that we are given a set of @xmath1 observations of a fixed but unknown vector @xmath0 . in a variety of settings , it is known _ a priori _ that the vector @xmath4 is sparse , meaning that its support set @xmath6corresponding to those indices @xmath7 for which @xmath8 is non - zero  is relatively small , say with .",
    "sparsity recovery refers to the problem of correctly estimating the support set @xmath6 based on a set of noisy observations .",
    "this sparsity recovery problem is of broad interest , arising in various areas , including subset selection in regression  @xcite , structure estimation in graphical models  @xcite , sparse approximation  @xcite , signal denoising  @xcite , and compressive sensing  @xcite .",
    "a great deal of work over the past few years has focused on the performance of computationally tractable methods , many based on @xmath5 or other convex relaxations , both for recovering the exact sparsity pattern as well as related problems in sparse approximation .",
    "we provide a brief overview of those parts of this extensive literature most relevant to our work in section  [ secrelatedwork ] below . of equal interest and complementary in nature , however , are the information - theoretic limits associated with the performance of _ any _ procedure for sparsity recovery .",
    "such understanding of fundamental limitations is crucial in assessing the behavior of computationally tractable methods . in particular",
    ", there is little point in proposing novel methods for sparsity recovery , possibly with higher computational complexity , if currently extant and computationally tractable methods achieve the information - theoretic limits . on the other hand ,",
    "an information - theoretic analysis can reveal where there currently exists a gap between the performance of computationally tractable methods , and the fundamental limits .",
    "indeed , the information - theoretic analysis of this paper makes contributions of both types .    with this motivation in mind , the focus of this paper is on the information - theoretic limitations of sparsity recovery .",
    "in particular , our analysis focuses on the noisy and high - dimensional setting , meaning that the observations are contaminated by noise , and all three problem parameters  the _ number of observations _",
    "@xmath1 , the _ model dimension",
    "_ @xmath2 , and the _ sparsity index _ @xmath3 , defined below  may tend to infinity .",
    "our main results , stated more precisely in section  [ secmainresults ] , are necessary and sufficient conditions on the triplet @xmath9 for exact recovery .",
    "in particular , given noisy linear observations based on measurement vectors drawn from the standard gaussian ensemble , we derive both a set of sufficient conditions for asymptotically perfect recovery using the optimal decoder , as well as a set of necessary conditions that any decoder must satisfy for perfect recovery .",
    "the analysis given here complements our earlier paper  @xcite that established precise thresholds on the success / failure of the lasso ( i.e. , @xmath5-constrained quadratic programming ) for sparsity recovery .",
    "the remainder of this paper is organized as follows . in section  [ secrelatedwork ] ,",
    "we provide a more precise formulation of the problem , and a brief discussion of past work , whereas section  [ secmainresults ] provides a precise statement of our main results , and a discussion of their consequences .",
    "section  [ secanalysis ] and the appendices are devoted to the proofs of our main results , and we conclude in section  [ secdiscussion ] with a discussion of open directions .",
    "we begin with a more precise formulation of the problem , as well as a discussion of previous work , with emphasis on that most closely related to the results in this paper .",
    "let @xmath0 be a fixed but unknown vector ; we refer to the ambient dimension @xmath2 as the _ model dimension_. define the support set of @xmath4 as @xmath10 we refer to its size @xmath11 as the _ sparsity index_. finally , suppose that we are given a set of @xmath1 observations , of the form @xmath12 where each @xmath13 is a measurement vector , and @xmath14 is additive gaussian noise . of interest",
    "are conditions on the triplet @xmath15 under which a given method either succeeds or fails in recovering the sparsity pattern @xmath6 .    [ [ observation - models ] ] observation models : + + + + + + + + + + + + + + + + + + +    the linear observation model   can be studied in either its noiseless variant ( @xmath16 ) , or the noisy setting ( @xmath17 ) ; this paper focuses exclusively the noisy setting .",
    "in addition , previous work has addressed both deterministic families and random ensembles of measurement vectors @xmath18 . the analysis in this paper",
    "is based on the _ standard gaussian measurement ensemble _ , in which each measurement vector @xmath19 is drawn from the zero - mean isotropic gaussian distribution @xmath20 .",
    "[ [ error - metrics ] ] error metrics : + + + + + + + + + + + + + +    consider some method that generates the vector @xmath21 as an estimate of the truth @xmath4 .",
    "there are various distinct criteria for assessing how close the estimate is to the truth , including    various @xmath22 norms @xmath23 , especially @xmath24 and @xmath5 , or    some measurement of predictive power ( e.g. , @xmath25 $ ] , where @xmath26 is the estimate based on @xmath27 ) . given the abundance of recent results on sparse approximation ( not all of which are mutually comparable ) , it is particularly important to specify up front the choice of error metric . in this paper , we focus exclusively on the sparsity recovery problem , for which the appropriate error metric is simply the @xmath28 loss associated with the event of recovering the correct support @xmath6viz . :",
    "@xmath29.\\end{aligned}\\ ] ]    [ [ past - work ] ] past work : + + + + + + + + + +    closely related in its information - theoretic spirit is the earlier paper of fletcher et al .  @xcite that analyzed the standard gaussian ensemble from a rate - distortion perspective , studying the average @xmath24-error of the optimal decoder .",
    "the results given here also address the information - theoretic limitations , albeit of the sparsity recovery problem , using the error metric   as opposed to @xmath24-norm . in a related but distinct line of work",
    ", the use of @xmath5-relaxation for sparse approximation has a lengthy history ; relatively early papers from the 1990s include the work of chen , donoho and saunders  @xcite , as well as tibshirani  @xcite on @xmath5-constrained quadratic programming ( known as the lasso in the statistics literature ) .",
    "a great deal of subsequent work has analyzed the performance of @xmath5-relaxations , both in the noiseless  @xcite and noisy setting  @xcite for deterministic ensembles , as well as the noiseless  @xcite and noisy setting  @xcite for random ensembles .",
    "other work has provided conditions under which estimation of a noise - contaminated vector via the lasso  @xcite or other types of convex relaxation  @xcite is stable in the @xmath24 sense ; however , such @xmath24-stability does not guarantee exact recovery of the underlying sparsity pattern .",
    "a notable feature of the results given here is that they apply to completely general scaling of the triplet @xmath9 .",
    "in contrast , most previous work has addressed one of two possible special cases of sparsity scaling : ( a ) either the _ linear sparsity regime _",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* e.g ) , in which @xmath30 for some @xmath31 ; or ( b ) the _ sublinear sparsity regime _",
    "* ; * ? ? ?",
    "* e.g. , ) , in which @xmath32 tends to zero .",
    "depending on the underlying motivation for sparse approximation , both of these sparsity regimes are of independent interest . in covering the full range of scaling ,",
    "the results given here are complementary to those of our previous paper  @xcite that provided threshold results , also applicable to general scaling of @xmath15 , for the success / failure of the lasso when used for sparsity recovery with random gaussian measurement ensembles .",
    "we discuss connections to previous work in more technical detail following the statement of our main results below .",
    "the analysis of this paper procedure is asymptotic in nature , focusing on scaling conditions on the triplet @xmath33 under which asymptotically exact recovery is either possible or impossible . as mentioned previously , we focus on the linear observation model   in the noisy setting ( @xmath34 ) , and with the measurement vectors @xmath19 drawn in an i.i.d .",
    "manner from the standard gaussian @xmath35 ensemble .",
    "a decoder is a mapping from the @xmath1-vector of observations @xmath36 to an estimated subset  say of the form @xmath37 .",
    "we think of the underlying true vector @xmath38 with its support @xmath6 randomly chosen , uniformly over all @xmath39 subspaces of size @xmath3 .",
    "accordingly , the average error probability @xmath40 of any decoder is given by @xmath41 $ ] corresponds to the probability , conditioned on the true underlying support being @xmath6 and averaging over the measurement noise @xmath42 , the choice of gaussian random matrix @xmath43 , and the choice of the entries @xmath44 on the fixed support @xmath6 , that the decoder makes an error .",
    "we say that    * the sparsity recovery is _ asymptotically reliable _",
    "( error - free ) if @xmath45 as @xmath46 , and * the sparsity recovery is _ asymptotically unreliable _ if for some constant @xmath47 , the error probability stays bounded @xmath48 as @xmath49 .",
    "in addition to the three parameters @xmath50 , our results also involve the minimum value of the unknown vector @xmath4 on its support , given by @xmath51 we begin by stating a set of conditions on the triplet @xmath52 which are sufficient to ensure asymptotically perfect recovery of the sparsity pattern : [ thmsuff ] if @xmath53 , then the following condition suffices to ensure asymptotically reliable recovery : for some fixed constant @xmath54 , @xmath55 the proof of this claim , given in section  [ secsuff ] , is constructive in nature , based on direct analysis of the error probability associated with the optimal decoder .",
    "[ thmnec ] asymptotically reliable recovery is impossible under the following condition : for some fixed constant @xmath56 : @xmath57 \\ ; { \\ensuremath{s}}\\log \\frac{{{\\ensuremath{p}}}}{{\\ensuremath{s}}}.\\end{aligned}\\ ] ] the proof of this claim , given in section  [ secnec ] , is somewhat more indirect in nature , based on exploiting a corollary of fano s inequality  @xcite , in order to lower bound the probability of error for a restricted hypothesis testing problem .",
    "to interpret these results , we consider two distinct regimes of sparsity :    [ [ regime - of - sublinear - sparsity ] ] regime of sublinear sparsity : + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    first suppose that the sparsity is sublinear , meaning that based on the two theorems , we identify the critical scaling as @xmath58 . with this scaling ,",
    "the sufficient condition in theorem  [ thmsuff ] reduces to @xmath59 , whereas the necessary condition in theorem  [ thmnec ] reduces to @xmath60 . for many choices of sublinear sparsity ( e.g. , @xmath61 ) , we have @xmath62 , so that we can summarize the two conditions as a threshold of the order @xmath63 . to compare with our previous work  @xcite on computationally tractable methods , we established that @xmath5-constrained quadratic programming ( lasso ) has a threshold , where @xmath64 is any function such that @xmath65 . ] for success / failure of order @xmath66 , so that the lasso essentially achieves the information - theoretic bounds .",
    "[ [ regime - of - linear - sparsity ] ] regime of linear sparsity : + + + + + + + + + + + + + + + + + + + + + + + + + +    next consider the regime of linear sparsity , in which @xmath30 for some @xmath67 . considering first the sufficient conditions of theorem  [ thmsuff ] , we see that as long as @xmath68 , then @xmath69 observations are sufficient to ensure asymptotically reliable recovery .",
    "this information - theoretic condition should be compared with our earlier analysis  @xcite of @xmath5-constrained quadratic programming ( the lasso ) ; one consequence of this work is that if then the lasso fails with probability converging to one , even if @xmath70 stays bounded away from zero .",
    "given that @xmath71 for linear sparsity @xmath30 , we see that there is a substantial gap between the performance of the lasso and the optimal decoder in the linear sparsity regime .",
    "thus , theorem  [ thmsuff ] raises the interesting question as to the existence of computationally efficient techniques for asymptotically reliable recovery in the regime of linear sparsity .",
    "this section is devoted to the proofs of theorems  [ thmsuff ] and  [ thmnec ] .",
    "we begin by setting up some useful notation to be used throughout the remainder of the paper .      for compactness in notation ,",
    "let us use @xmath43 to denote the @xmath72 matrix formed with the vectors @xmath73 as rows , and the vectors @xmath74 as columns , as follows : @xmath75 using @xmath36 and @xmath42 to denote the @xmath1-dimensional observation and noise vectors respectively , we can re - write our linear observation model   in matrix - vector form as follows : @xmath76 given any subset @xmath77 , we use the notation @xmath78 to denote the @xmath79-dimensional subvector @xmath80 , and similarly for other vectors ( e.g. , @xmath36 , etc . ) .",
    "in an analogous manner , we use @xmath81 to denote the @xmath82 matrix with columns @xmath83 . from herein",
    ", we assume without loss of generality that @xmath84 , so that @xmath85 ) is simply a standard gaussian vector .",
    "( note that any scaling of @xmath86 can be accounted for in the scaling of @xmath4 , via the parameter @xmath87 . )    in addition , we use the following standard notation for asymptotics of real sequences @xmath88 and @xmath89 : ( i ) @xmath90 means that @xmath91 for some constant @xmath92 ; ( ii ) @xmath93 means that @xmath94 for some constant @xmath95 ; ( iii ) @xmath96 is shorthand for @xmath90 and @xmath93 , and ( iv ) @xmath97 means that @xmath98 .      [ [ optimal - decoding ] ] optimal decoding : + + + + + + + + + + + + + + + + +    we begin by describing the `` best '' decoder , that is optimal in terms of minimizing the probability of error @xmath99 over all decoding rules .",
    "it is based on the following real - valued function , defined on the subsets @xmath100 , as @xmath101 we frequently write @xmath102 as a shorthand ; note that this value corresponds to the error associated with the best estimator of @xmath103 that lies in @xmath104 .",
    "the optimal decoder chooses the best subset @xmath105 based on the minimal value of this error , ranging over all subsets @xmath106 of size @xmath3 : @xmath107 note that by symmetry , the error probability is in fact the same regardless of which underlying set @xmath6 acts as the true one .",
    "consequently , we can view the choice of @xmath6 as fixed ( and hence non - random ) , and write @xmath108,\\ ] ] which should now be understood as an unconditional probability ( with @xmath6 fixed ) .",
    "[ [ analysis - of - error - probability ] ] analysis of error probability : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    consider the difference @xmath109 between the reconstruction error @xmath110 using the true subset @xmath6 , versus the error @xmath102 candidate subset @xmath106 . for any subset @xmath106 such that @xmath111 is full rank , define the @xmath112 matrices    @xmath113^{-1 } } } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{u}}}}}^t } } , \\qquad \\qquad \\mbox{and } \\\\ { \\ensuremath{\\pi^\\perp_{{\\ensuremath{u } } } } } & \\defn & i_{{\\ensuremath{n}}\\times { \\ensuremath{n } } } - { \\ensuremath{{\\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{u } } } } } { \\ensuremath{\\left [ { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{u}}}}}^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{u } } } } } \\right]^{-1 } } } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{u}}}}}^t}}.\\end{aligned}\\ ] ]    note that @xmath114 and @xmath115 are both orthogonal projection matrices , associated with the @xmath3-dimensional range space @xmath104 and @xmath116-dimensional nullspace @xmath117 respectively . with these definitions , we state the following result ( see appendix  [ appalgebra ] for a proof ) : [ lemanalyzeform ] for a given vector @xmath4 with support @xmath6",
    ", the optimal decoder declares @xmath106 over @xmath6 if and only if the random variable @xmath118 is negative .",
    "overall , the optimal decoder fails if and only if at least one @xmath106 ( with cardinality @xmath119 ) is preferable to @xmath6 ; consequently , the probability of error can be written as @xmath120 & = & { \\ensuremath{\\mathbb{p}}}\\big[\\bigcup_{{\\ensuremath{u}}\\neq { \\ensuremath{s } } ,      \\ ; |{\\ensuremath{u}}| = { \\ensuremath{s } } } \\ { { \\ensuremath{\\delta}}({\\ensuremath{u } } ) < 0",
    "\\ } \\big].\\end{aligned}\\ ] ] in order to analyze this error probability , we begin by considering the range of possible integers @xmath121 , corresponding to the complement of the overlap .",
    "the following lemma characterizes the exponential decay rates of the random variable @xmath122 : [ lemexpupper ] for fixed @xmath123 ( with @xmath124 ) , we have for any @xmath106 with @xmath125 , @xmath126 & \\leq & \\exp \\left \\{\\frac{-({\\ensuremath{n}}-    { \\ensuremath{s}})\\|{\\ensuremath{\\beta^*}}_{{\\ensuremath{s \\backslash u}}}\\|^2}{12    \\left(\\|{\\ensuremath{\\beta^*}}_{{\\ensuremath{s",
    "\\backslash u}}}\\|^2 + 4\\right ) } \\right \\ } + 2 \\ , \\exp    \\left \\ { -\\frac{k}{4 } \\left[-1 + \\frac{1}{4}({\\ensuremath{n}}- { \\ensuremath{s } } )    \\frac { \\|{\\ensuremath{\\beta^*}}_{{\\ensuremath{s \\backslash u}}}\\|^2}{k } \\right]^2 \\right \\}. \\ ] ] we begin by conditioning on the gaussian noise vector @xmath42 .",
    "since each element of @xmath127 is standard normal , each entry of the random vector @xmath128 is zero - mean gaussian with variance @xmath129 .",
    "consequently , if we rescale by the standard deviation , then the random vector @xmath130 is an @xmath1-dimensional gaussian random vector with independent entries , each with with unit variance , and mean vector @xmath42 .",
    "applying the orthogonal transform @xmath115 reduces the number of degrees of freedom to @xmath131 , so that we conclude that @xmath132 is a non - central @xmath133 variate with @xmath134 degrees of freedom , and non - centrality parameter @xmath135 . with these choices of @xmath136",
    ", we have @xmath137 & = & { \\ensuremath{\\mathbb{p}}}\\left[\\chi^2(d , \\nu ) < t \\right]\\end{aligned}\\ ] ] where we have set @xmath138 for shorthand .",
    "thus , conditioned on @xmath42 , our problem reduces to bounding the tail of a non - central @xmath133 variate . in appendix  [ appchitail ] , we state some known tail bounds  @xcite on such variates , which we use here . in order to apply these bounds , we condition on the following `` good event '' , defined in terms of @xmath42 @xmath139",
    "note that the first event defining @xmath140 ensures that @xmath141 consequently , conditioned on @xmath140 , we may set @xmath142 in equation   to obtain the upper bound @xmath143 & \\leq & -\\frac{\\left ( d + \\nu - t \\right)^2}{4 ( d + 2\\nu ) } \\nonumber \\\\ & = & -\\frac { \\left ( [ { \\ensuremath{n}}- { \\ensuremath{s } } ] + \\frac{\\|{\\ensuremath{\\pi^\\perp_{{\\ensuremath{u } } } } } \\wsca\\|^2 - \\|{\\ensuremath{\\pi^\\perp_{{\\ensuremath{s } } } } } \\wsca\\|^2}{\\|{\\ensuremath{\\beta^*}}_{{\\ensuremath{s \\backslash u}}}\\|^2 } \\right)^2}{4 \\left ( [ { \\ensuremath{n}}-{\\ensuremath{s } } ] + 2 \\frac{\\|{\\ensuremath{\\pi^\\perp_{{\\ensuremath{u } } } } } \\wsca\\|^2}{\\|{\\ensuremath{\\beta^*}}_{{\\ensuremath{s \\backslash u } } } \\|^{2 } } \\right ) } \\nonumber \\\\ & = & - \\left({\\ensuremath{n}}- { \\ensuremath{s}}\\right ) \\ ; \\frac { \\left ( 1 + \\frac{\\|{\\ensuremath{\\pi^\\perp_{{\\ensuremath{u } } } } } \\wsca\\|^2 - \\|{\\ensuremath{\\pi^\\perp_{{\\ensuremath{s } } } } } \\wsca\\|^2}{({\\ensuremath{n}}-{\\ensuremath{s } } ) \\ ; \\|{\\ensuremath{\\beta^*}}_{{\\ensuremath{s \\backslash u}}}\\|^2 } \\right)^2}{4 \\left(1 + 2 \\frac{\\|{\\ensuremath{\\pi^\\perp_{{\\ensuremath{u } } } } } \\wsca\\|^2}{({\\ensuremath{n}}- { \\ensuremath{s } } ) \\ ; \\|{\\ensuremath{\\beta^*}}_{{\\ensuremath{s \\backslash u } } } \\|^{2 } } \\right ) } \\nonumber \\\\ & \\stackrel{(b)}{\\leq } & - \\left({\\ensuremath{n}}- { \\ensuremath{s}}\\right ) \\ ; \\frac{1/2}{4\\left(1 + 4/\\|{\\ensuremath{\\beta^*}}_{{\\ensuremath{s \\backslash u}}}\\|^2 \\right ) } \\nonumber \\\\",
    "\\label{eqntermone } & = & - \\left({\\ensuremath{n}}- { \\ensuremath{s}}\\right ) \\ ; \\frac{\\|{\\ensuremath{\\beta^*}}_{{\\ensuremath{s \\backslash u}}}\\|^2}{8 \\left(\\|{\\ensuremath{\\beta^*}}_{{\\ensuremath{s \\backslash u}}}\\|^2 + 4\\right)},\\end{aligned}\\ ] ] where inequality ( b ) makes use of the second event defining @xmath140 .",
    "we complete the proof by observing that @xmath144 & \\leq & { \\ensuremath{\\mathbb{p } } } [ { \\ensuremath{\\delta}}({\\ensuremath{u } } ) < 0 \\ ; \\mid \\ ; { \\ensuremath{\\mathcal{a } } } ] + { \\ensuremath{\\mathbb{p}}}[{\\ensuremath{\\mathcal{a}}}^c],\\end{aligned}\\ ] ] so that it suffices to upper bound @xmath145 $ ] . by union",
    "bound , we have @xmath146 & \\leq & { \\ensuremath{\\mathbb{p}}}\\left[\\left | \\frac {    \\|{\\ensuremath{\\pi^\\perp_{{\\ensuremath{u } } } } } \\wsca\\|^2 - \\|{\\ensuremath{\\pi^\\perp_{{\\ensuremath{s } } } } } \\wsca    \\|^2}{\\|{\\ensuremath{\\beta^*}}_{{\\ensuremath{s \\backslash u}}}\\|^2 } \\right | \\geq \\frac{{\\ensuremath{n}}-    { \\ensuremath{s}}}{2 } \\right ] + { \\ensuremath{\\mathbb{p}}}\\left[\\|{\\ensuremath{\\pi^\\perp_{{\\ensuremath{u } } } } } \\wsca\\|^2    \\geq 2 ( { \\ensuremath{n}}- { \\ensuremath{s } } ) \\right].\\end{aligned}\\ ] ]",
    "since @xmath147 is a central @xmath133 with @xmath131 degrees of freedom , we may apply the tail bounds from appendix  [ appchitail ] to conclude that @xmath148 & \\leq & \\exp(-({\\ensuremath{n}}- { \\ensuremath{s}})/12).\\end{aligned}\\ ] ] turning to the first term on the rhs on equation  , we observe that @xmath149 where @xmath150 are i.i.d .",
    "standard normal variates",
    ". now if the difference @xmath151 is to exceed @xmath152 , then at least one of the terms must exceed @xmath153 . moreover",
    ", we observe that @xmath154 is @xmath155 , where @xmath156 .",
    "hence , we have @xmath157 & \\leq & \\log 2 \\ , { \\ensuremath{\\mathbb{p}}}\\left [ \\frac{\\chi^2_k}{k } \\geq \\frac{1}{4}({\\ensuremath{n}}- { \\ensuremath{s } } )    \\frac{\\|{\\ensuremath{\\beta^*}}_{{\\ensuremath{s \\backslash u}}}\\|^2}{k } \\right ] \\\\ & = & \\log 2 { \\ensuremath{\\mathbb{p}}}\\left[\\chi^2_k -k \\geq k \\left\\ { -1 +    \\frac{1}{4}({\\ensuremath{n}}- { \\ensuremath{s } } ) \\frac{\\|{\\ensuremath{\\beta^*}}_{{\\ensuremath{s \\backslash u}}}\\|^2}{k }    \\right \\ } \\right ] \\\\ & \\leq & -\\frac{k}{4 } \\left[-1 + \\frac{1}{4}({\\ensuremath{n}}- { \\ensuremath{s } } )   \\frac { \\|{\\ensuremath{\\beta^*}}_{{\\ensuremath{s \\backslash u}}}\\|^2}{k } \\right]^2 + \\log 2,\\end{aligned}\\ ] ] where we have used the upper bound   from appendix  [ appchitail ] with @xmath158 in the final inequality .",
    "[ [ weakened - but - simpler - bound ] ] weakened but simpler bound : + + + + + + + + + + + + + + + + + + + + + + + + + + +    in order to make further progress , we simplify the bound   from lemma  [ lemexpupper ] , at the expense of weakening it , by noting that for all @xmath159 , we have @xmath160 , so that @xmath161 & \\leq & \\exp \\left \\{\\frac{-({\\ensuremath{n}}-    { \\ensuremath{s } } ) k \\ { \\ensuremath{\\mathcal{m}}}^2({\\ensuremath{\\beta^*}})}{12 \\left(k \\ ,    { \\ensuremath{\\mathcal{m}}}^2({\\ensuremath{\\beta^ * } } ) + 4\\right ) } \\right \\ } + 2 \\ , \\exp \\left    \\{-\\frac{k}{4 } \\left[\\frac{{\\ensuremath{n}}- { \\ensuremath{s}}}{4 } \\ :    { \\ensuremath{\\mathcal{m}}}^2({\\ensuremath{\\beta^ * } } ) - 1 \\right]^2 \\right \\}.\\end{aligned}\\ ] ] the advantage of this weakened bound is that it is independent of the subset @xmath106 , and depends only on the parameter @xmath156 .    from this",
    "weakened bound  , we see the necessity ( at least for this analysis ) of the requirement @xmath162 , so that the second error term decays asymptotically . under this requirement",
    ", we have ( for sufficiently large @xmath1 ) that the second error exponent can be bounded as @xmath163 ^ 2 & \\leq & -\\frac{k}{12 } \\left [    \\frac{{\\ensuremath{n}}- { \\ensuremath{s}}}{4 } \\ : { \\ensuremath{\\mathcal{m}}}^2({\\ensuremath{\\beta^ * } } ) - 1\\right ] \\\\   & \\leq & -\\frac{k}{4 } \\frac{{\\ensuremath{n}}- { \\ensuremath{s}}}{8 } \\ :   { \\ensuremath{\\mathcal{m}}}^2({\\ensuremath{\\beta^ * } } ) \\\\ & \\leq & \\frac{-({\\ensuremath{n}}- { \\ensuremath{s } } ) k \\ , { \\ensuremath{\\mathcal{m}}}^2({\\ensuremath{\\beta^*}})}{12   \\left(k \\ , { \\ensuremath{\\mathcal{m}}}^2({\\ensuremath{\\beta^ * } } ) + 8\\right)}.\\end{aligned}\\ ] ] the first error exponent is also upper bounded by this same quantity , so that we can simplify the upper bound to @xmath164 & \\leq & 3\\ , \\exp \\left \\ {    \\frac{-({\\ensuremath{n}}- { \\ensuremath{s } } ) k \\ , { \\ensuremath{\\mathcal{m}}}^2({\\ensuremath{\\beta^*}})}{12 \\left(k \\ ,    { \\ensuremath{\\mathcal{m}}}^2({\\ensuremath{\\beta^ * } } ) + 8\\right ) } \\right \\}.\\end{aligned}\\ ] ] denote by @xmath165 the number of subsets @xmath106 of size @xmath3 , with overlap exactly equal to @xmath123 .",
    "a standard counting argument yields that , for each @xmath123 with @xmath124 , there are @xmath166 such subsets . using this simple bound   and union",
    "bound applied to the representation  , we can upper bound the error probability as @xmath167 & \\leq & 3 \\sum_{k=1}^{\\ensuremath{s}}{{\\ensuremath{s}}\\choose k } \\ , { { { \\ensuremath{p}}}- { \\ensuremath{s}}\\choose k } \\ ; \\exp \\left \\ { \\frac{-({\\ensuremath{n}}- { \\ensuremath{s } } )",
    "k \\ , { \\ensuremath{\\mathcal{m}}}^2({\\ensuremath{\\beta^*}})}{12 \\left(k \\ , { \\ensuremath{\\mathcal{m}}}^2({\\ensuremath{\\beta^ * } } ) + 8\\right ) } \\right \\}.\\end{aligned}\\ ] ]    [ [ analysis - of - the - upper - bound ] ] analysis of the upper bound : + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we now analyze the upper bound  ; in particular , our goal is to derive sufficient conditions for each of the terms in the summation to vanish asymptotically . in order to deal with the binomial coefficients , we make use of the bounds ( see appendix  [ appbinbound ] ) @xmath168 applying these two bounds , we conclude that the ( logarithm of the ) @xmath169 term is upper bounded by @xmath170 - \\frac{({\\ensuremath{n}}- { \\ensuremath{s } } ) k \\ , { \\ensuremath{\\mathcal{m}}}^2({\\ensuremath{\\beta^*}})}{12 \\left(k \\ , { \\ensuremath{\\mathcal{m}}}^2({\\ensuremath{\\beta^ * } } ) + 8\\right)}.\\ ] ] requiring this term to be negative asymptotically is equivalent to having @xmath171 \\nonumber \\\\ \\label{eqninterbound } & = & 12 \\left(k \\ , + \\frac{8}{{\\ensuremath{\\mathcal{m}}}^2({\\ensuremath{\\beta^ * } } ) } \\right ) \\left \\ { 2 + \\log \\frac{{\\ensuremath{s}}}{k } + \\log \\frac{{{\\ensuremath{p}}}- { \\ensuremath{s}}}{k } \\right\\}.\\end{aligned}\\ ] ]    in order to understand the behavior of this lower bound , we consider @xmath123 in two distinct regimes :    on one hand , if @xmath172 for some @xmath173 , then the second term on the rhs of the bound   is dominated by the term @xmath174 , so that the overall lower bound is dominated by @xmath175 .    on the other hand ,",
    "if @xmath176 , the lower bound is dominated by the maximum of linear growth @xmath3 , and the quantity overall , we conclude that the condition @xmath177 for some constant @xmath54 is sufficient in order to achieve asymptotically reliable recovery , as claimed in theorem  [ thmsuff ] .",
    "we now turn to the proof of the necessary conditions given in theorem  [ thmnec ] .",
    "[ [ fano - method ] ] fano method : + + + + + + + + + + + +    our analysis is based on a well - known lower bound on the probability of error in a multiway hypothesis testing problem in terms of kullback - leibler divergences . in the non - parametric statistics literature  @xcite , this approach is referred to as the fano method , since the bound is a corollary of fano s inequality from information theory  @xcite . here we state and make use of the following variant  @xcite : consider a family of @xmath178 distributions @xmath179 .",
    "then the average probability of error in performing in a hypothesis test over this family is lower bounded as @xmath180 where @xmath181 denotes the kullback - leibler divergence between distributions @xmath182 and @xmath183 .",
    "[ [ restricted - problem ] ] restricted problem : + + + + + + + + + + + + + + + + + + +    consider the collection of all @xmath184 subsets of size @xmath3 chosen from @xmath185 . in order to produce lower bounds , we analyze the behavior of the optimal decoder for a restricted problem , in which we assume that for any fixed support @xmath6 , it is known _ a priori _ that @xmath186 for all indices @xmath187 .",
    "( recall that @xmath87 is the minimum absolute value of entries in the support of @xmath4 . )",
    "this problem is simply an @xmath178-way hypothesis testing problem , in which the observation under the hypothesis associated with subset @xmath106 takes the form @xmath188 where @xmath189 is a rescaled @xmath3-vector of ones , and @xmath190 .",
    "let us index the collection of all @xmath3-sized subsets with @xmath191 , and use @xmath192 $ ] to denote the corresponding support .",
    "for each index @xmath7 , let @xmath182 denote the multivariate gaussian distribution with mean @xmath193 } } } { \\ensuremath{\\vec{v}}}$ ] and covariance matrix @xmath194 ; note that @xmath182 is simply the class - conditional distribution of @xmath36 under the hypothesis @xmath192 $ ] .",
    "moreover , the kullback - leibler divergence between any such pair is given by @xmath195 } } } { \\ensuremath{\\vec{v}}}- { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{u}}[j ] } } } { \\ensuremath{\\vec{v}}}\\|_2 ^ 2 $ ] , so that the corresponding fano bound takes the form @xmath196 } } } { \\ensuremath{\\vec{v}}}- { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{u}}[j ] } } } { \\ensuremath{\\vec{v}}}\\|_2 ^ 2 + 2 \\log 2}{\\log [ n-1]}.\\end{aligned}\\ ] ]    [ [ upper - bounds - via - concentration ] ] upper bounds via concentration : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    thus , in order to ensure that @xmath197 stays bounded away from zero , we need to ( upper ) bound the quantity @xmath198 } } } { \\ensuremath{\\vec{v}}}- { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{u}}[j ] } } } { \\ensuremath{\\vec{v}}}\\|_2 ^ 2 \\big / \\log [ { \\ensuremath{n}}-1]$ ] away from one . for a given pair of subsets @xmath199 in our collection , consider the random variable a little calculation shows that @xmath200 , where @xmath201 .",
    "[ lemconcen ] the tail of @xmath202 obeys the bound @xmath203 & \\leq & \\frac{1}{2}.\\end{aligned}\\ ] ] using this lemma ( see appendix  [ appconcen ] for a proof of this claim ) , we are guaranteed that at least @xmath204 of the gaussian ensembles satisfy the upper bound @xmath205 } & = & \\frac{1}{2 } \\ ; \\frac{\\frac{1}{{\\ensuremath{n}}^2 } \\sum_{{\\ensuremath{u}}\\neq { \\ensuremath{v } } } z_{{\\ensuremath{u } } , { \\ensuremath{v}}}}{\\log[{\\ensuremath{n}}-1 ] } \\ ; \\leq \\ ;   \\frac{4 { \\ensuremath{\\mathcal{m}}}^2({\\ensuremath{\\beta^ * } } ) { \\ensuremath{s}}{\\ensuremath{n}}}{\\log [ { \\ensuremath{n}}-1]}.\\end{aligned}\\ ] ] hence , as long as the quantity   remains bounded from above away from one , the fano bound implies that the probability of error averaged over the whole ensemble will remain bounded away from zero .",
    "consequently , we obtain the necessary condition that @xmath206}{4 { \\ensuremath{\\mathcal{m}}}^2({\\ensuremath{\\beta^ * } } ) { \\ensuremath{s}}}\\end{aligned}\\ ] ] for reliable recovery with probability one asymptotically . to obtain a more transparent bound , we first lower bound @xmath207 via @xmath208 \\geq \\frac{1}{2 } \\log { \\ensuremath{n}}$ ] , and then further via @xmath209 as stated in appendix  [ appbinbound ] .",
    "consequently , we obtain the necessary condition @xmath210 as stated in theorem  [ thmnec ] .",
    "in this paper , we have analyzed the information - theoretic limits of the sparsity recovery problem for the linear observation model   with measurement vectors drawn from the standard gaussian ensemble .",
    "we have established both lower and upper bounds on the number of observations @xmath1 as a function of the model dimension @xmath2 and sparsity index @xmath3 that are required for asymptotically reliable recovery .",
    "there are a variety of open questions raised by our analysis .",
    "first , while our upper and lower bounds are essentially matching for certain regimes of scaling ( e.g. , sublinear sparsity with the minimum @xmath58 ) , it is likely that the analysis can be tightened in other regimes . in particular , the analysis of the necessary conditions ( see proof of theorem  [ thmnec ] ) involves some slack since it is based on analyzing a very restricted ensemble .",
    "second , our results ( in particular , a corollary of theorem  [ thmsuff ] ) reveal that with the sparsity index scaling linearly ( @xmath30 for some @xmath31 ) , as long , as the minimum value @xmath70 decays sufficiently slowly , then asymptotically reliable recovery is possible with only a linear number of observations ( i.e. , @xmath211 for some @xmath212 ) . since our previous work  @xcite established that the lasso ( @xmath5-constrained quadratic programming ) can not achieve reliable recovery in this particular @xmath15 regime , it remains to determine a computationally tractable method that approaches such performance in the regime of linear sparsity .",
    "third , whereas the current analysis has focused on a very special class of gaussian ensemble , the analysis given here could be extended to a broader class of measurement ensembles .",
    "this work was partially supported by nsf career award ccf-0545862 , nsf grant dms-0605165 , and an alfred p. sloan foundation fellowship .",
    "we thank peter bickel for helpful discussions and pointers .",
    "we begin by showing that for any subset @xmath106 for which @xmath111 is full rank , the function @xmath213 has the equivalent form @xmath214 . under the given rank condition ,",
    "the linear least squares estimator of @xmath215 is given by @xmath216^{-1 } } } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{u}}}}}^t \\ysca$ ] .",
    "noting that @xmath217 , we substitute into the quadratic norm and expand , thereby obtaining @xmath218 as claimed .",
    "lastly , to establish equation  , we note that @xmath219 since @xmath220 for any vector @xmath221 belonging to the range of @xmath111 .",
    "note that @xmath222 is a rescaled sum of a total number @xmath223 variables ( neither independent nor identically distributed ) .",
    "however , since @xmath202 is a non - negative random variable , we may apply markov s inequality for any @xmath224 to conclude that @xmath225 & \\leq & \\frac{\\exs[z]}{t}.\\end{aligned}\\ ] ] since each @xmath226 has distribution @xmath227 , we have @xmath228 = { \\ensuremath{\\gamma}}({\\ensuremath{u } } , { \\ensuremath{v } } ) { \\ensuremath{n}}$ ] . from equation",
    ", we note that @xmath229 , and hence @xmath230 & \\leq & \\max_{{\\ensuremath{u}}\\neq { \\ensuremath{v } } } \\left({\\ensuremath{\\gamma}}({\\ensuremath{u } } , { \\ensuremath{v}})\\right ) { \\ensuremath{n}}\\ ; = \\ ; 2 { \\ensuremath{\\mathcal{m}}}^2({\\ensuremath{\\beta^ * } } ) { \\ensuremath{s}}{\\ensuremath{n}},\\end{aligned}\\ ] ] hence setting @xmath231 in the bound   yields the claim .",
    "although more refined results are certainly possible , we make frequent use of the following crude bounds on the binomial coefficients @xmath232",
    "the following large - deviations bounds for centralized @xmath133 are taken from laurent and massart  @xcite .",
    "given a centralized @xmath133-variate @xmath233 with @xmath234 degrees of freedom , then for all @xmath235 ,    @xmath236 \\ ; \\leq \\ ; { \\ensuremath{\\mathbb{p}}}\\left[x - d \\geq 2\\sqrt{d x } \\right ] & \\leq & \\exp(-x ) , \\qquad \\mbox{and } \\\\",
    "\\label{eqncleandowncent } { \\ensuremath{\\mathbb{p}}}\\left[x - d \\leq -2 \\sqrt{d x } \\right ] & \\leq & \\exp(-x).\\end{aligned}\\ ] ]    more generally , the analogous tail bounds for _ non - central _ @xmath133 , taken from birg  @xcite , can be established via the chernoff bound .",
    "let @xmath233 be a non - central @xmath133 variable with @xmath237 degrees of freedom and non - centrality parameter @xmath238 .",
    "then for all @xmath239 ,    @xmath240 & \\leq & \\exp(-x ) , \\qquad \\mbox{and } \\\\",
    "\\label{eqnnoncentb } \\prob \\left [ x \\leq ( \\df + { \\ensuremath{\\nu } } ) -2 \\sqrt{(\\df + 2 { \\ensuremath{\\nu } } ) x } \\right ] & \\leq   & \\exp(-x).\\end{aligned}\\ ] ]      l.  birg .",
    "an alternative point of view on lepski s method . in _ state of the art in probability and statistics _ , number  37 in ims lecture notes , pages 113133 .",
    "institute of mathematical statistics , 2001 .",
    "d.  m. malioutov , m.  cetin , and a.  s. willsky .",
    "optimal sparse representations in general overcomplete bases . in _ int . conf . on acoustics , speech , and signal processing _ , volume  2 , pages ii793796 , may 2004 .",
    "m.  j. wainwright .",
    "sharp thresholds for high - dimensional and noisy sparsity recovery using @xmath5-constrained quadratic programs . in _ proc .",
    "allerton conference on communication , control and computing _ , october 2006 .",
    "long version appeared as uc berkeley technical report 709 ."
  ],
  "abstract_text": [
    "<S> the problem of recovering the sparsity pattern of a fixed but unknown vector @xmath0 based on a set of @xmath1 noisy observations arises in a variety of settings , including subset selection in regression , graphical model selection , signal denoising , compressive sensing , and constructive approximation . of interest </S>",
    "<S> are conditions on the model dimension @xmath2 , the sparsity index @xmath3 ( number of non - zero entries in @xmath4 ) , and the number of observations @xmath1 that are necessary and/or sufficient to ensure asymptotically perfect recovery of the sparsity pattern . </S>",
    "<S> this paper focuses on the information - theoretic limits of sparsity recovery : in particular , for a noisy linear observation model based on measurement vectors drawn from the standard gaussian ensemble , we derive both a set of sufficient conditions for asymptotically perfect recovery using the optimal decoder , as well as a set of necessary conditions that _ any _ decoder , regardless of its computational complexity , must satisfy for perfect recovery . </S>",
    "<S> this analysis of optimal decoding limits complements our previous work  @xcite on sharp thresholds for sparsity recovery using the lasso ( @xmath5-constrained quadratic programming ) with gaussian measurement ensembles .    * information - theoretic limits on sparsity recovery in the high - dimensional and noisy setting *    technical report , uc berkeley , department of statistics + january 2007    * keywords : * high - dimensional statistical inference ; subset selection ; signal denoising ; compressive sensing ; model selection ; sparsity recovery ; information - theoretic bounds ; fano s method . </S>"
  ]
}