{
  "article_text": [
    "merely a few years ago the computational power on the order of tflops was an attribute of a supercomputer . today , the latest commodity graphics processing units ( gpu ) are capable of more than 5 tflops and have much lower energy requirements than a set of central processing units ( cpus ) of comparable performance  a result achieved by employing a massively parallel architecture with hundreds of cores on a single device .    in order to take advantage of the parallel hardware ,",
    "appropriate algorithms have to be developed , and this generally needs to be done on a problem - specific basis .",
    "ideally , the problem naturally decomposes into a large number of ( semi-)independent tasks whose results can be combined in a simple way , as in e.g. numerical monte - carlo solution of a stochastic differential equation  @xcite , parameter space studies of dynamical systems , or event simulation and reconstruction in particle physics  @xcite .    in recent years ,",
    "the lattice boltzmann method ( lbm ) emerged as an interesting alternative to more established methods for fluid flow simulations .",
    "originally developed as an extension of lattice gas automata , nowadays lbm stands on well - established theoretical foundations and serves as a method of choice for many researchers in various fields  @xcite , while still retaining its relative simplicity .",
    "perhaps its most important feature in practice is its suitability for parallel architectures ",
    "the algorithm is executed on a regular lattice and typically only interactions between nearest neighbor nodes are necessary .",
    "this paper discusses various issues related to the implementation of the lbm on gpus , and presents a concrete and flexible solution , taking a new overall approach to the problem of lb software design .",
    "our _ sailfish _ code has been under development since april 2009 and is publicly available under an open source license . with support for both single and binary fluid simulations , a wide variety of boundary conditions , and calculations on both a single gpu and multiple gpus , it is , to the best of our knowledge , the most comprehensive open source lbm code for gpus .",
    "the text is organized as follows .",
    "first , a short overview of the discussed lattice boltzmann models is presented .",
    "then , the primary design principles of the sailfish project are laid out , and various lb gpu implementations are compared .",
    "this is followed by performance measurements on diverse gpu hardware , both on single machines and in clusters . in the next section ,",
    "the code is validated on 4 standard test cases .",
    "the final section concludes the paper and provides some directions for future work .",
    "in this section , _ all _ models implemented in sailfish as of september 2013 will be briefly overviewed .",
    "we start with the basic concepts and nomenclature of lbm .",
    "we then describe single fluid models : single - relaxation time lbgk and regularized dynamics , multiple relaxation times and the entropic lbm , and multi - component models : the shan - chen model and the free energy model .",
    "finally we give a short overview of our implementation of body forces and various boundary conditions .",
    "we refer the reader to the reviews @xcite for a more detailed discussion of the lbm .    in an lb simulation space",
    "is discretized into a regular cartesian grid , where each node represents a small pocket of fluid .",
    "the state of the fluid is encoded by a particle distribution function @xmath0 where @xmath1 spans a set of discrete velocity vectors @xmath2 indicating the allowed directions of mass movement between the nodes of the lattice .",
    "@xmath0 are often also called _",
    "mass fractions_. lb lattices are typically named using the dxqy scheme , where @xmath3 indicates the dimensionality of the lattice , and @xmath4 is the number of discrete velocity vectors .",
    "figure  [ fig : lattices ] shows some common lb lattices .        macroscopic fluid fields , such as density ( @xmath5 ) or velocity ( @xmath6 ) are defined as moments of the distribution function : @xmath7    in the simplest case the system dynamics is described by : @xmath8 where @xmath9 is a set of equilibrium distributions which are functions of the macroscopic fields ( @xmath5 , @xmath6 ) at node @xmath10 at time @xmath11 , @xmath12 is a relaxation time related to the kinematic viscosity @xmath13 via @xmath14 , and where the positions and time are expressed in the lattice unit system , i.e. the time unit represents a single simulation step and the lattice nodes are separated by 1 distance unit along the primary axes .",
    "this lb model is often referred to as lattice bhatnagar - gross - krook ( lbgk ) , which owes its name to the bgk operator from kinetic theory @xcite , and by which the collision operator on the right hand side of is inspired .",
    ".weights @xmath15 for different lattices implemented in sailfish .",
    "[ cols=\"<,^,^,^,^ \" , ]     [ tbl : bgkweights ]    when @xmath16 takes the form : @xmath17 where @xmath18 is a set of weights ( see table  [ tbl : bgkweights ] ) , the navier - stokes equations can be recovered from in the limit of low mach numbers through chapman - enskog expansion @xcite or expansion in a hermite basis @xcite .",
    "it can be shown that is second - order accurate in space and time in the bulk of the fluid  @xcite .",
    "it should be noted that and the right hand side of are fully local , i.e. only information from node @xmath10 is necessary to calculate the system state at the next step of the simulation . when data is exchanged between nodes ,",
    "the process is limited to the nearest neighbors ( left hand side of ) .",
    "this makes the lb method ideally suited for implementation on massively parallel computer architectures .",
    "conceptually , is often decomposed into two steps : relaxation ( evaluation of the collision operator , right hand side ) and propagation ( movement of mass fractions to neighboring nodes , left hand side ) .",
    "while such a decomposition is often suboptimal from the point of view of a software implementation , it still provides a useful mental framework to discuss lb algorithms , and we will use these two names throughout the text .    many extended lb models have been proposed to ( a ) to improve numerical stability , or ( b ) simulate more complex systems .",
    "a comprehensive discussion of such models would be prohibitively long , so here we limit ourselves to a short overview of models that are currently implemented within the sailfish framework . in the first group",
    ", we discuss the multiple - relaxation times ( mrt ) model , the entropic lb model ( elbm ) and the smagorinsky large eddy model . in the second group , sailfish implements two multicomponent fluid models : the shan - chen model @xcite , and the free energy model @xcite .",
    "one practical problem with the lbgk approximation is the limited range of viscosities at which the model remains stable , which directly translates into a limited range of reynolds numbers @xmath19 at which fluid flows can be simulated ( @xmath20 being the spatial extent of the domain expressed as a number of nodes ) . with @xmath21",
    "limited to about @xmath22 by the low mach number requirement for most lb models where the speed of sound @xmath23 and the stable range of numerical viscosities at these speeds being about @xmath24 , the highest attainable reynolds number assuming @xmath25 is on the order of @xmath26 .",
    "in practice , the range of reynolds numbers and viscosities is often even narrower due to instabilities introduced by complex geometry or boundary conditions . to address this problem ,",
    "a number of models exhibiting enhanced numerical stability have been developed .",
    "perhaps the simplest modification of the lbgk model aiming at improving its stability is the regularized model  @xcite .",
    "the basic idea of this approach is to replace the nonequilibrium part of the distributions @xmath27 with a new first order regularized value @xmath28 derived from the non - equilibrium stress tensor @xmath29 as @xmath30 .",
    "after this regularization step , the collision and propagation proceed as in the standard lbgk scheme .      if the particle distributions @xmath0 are considered to form vectors @xmath31 , the collision operator in the lbgk approximation can be interpreted as a diagonal matrix @xmath32 , with @xmath33 acting on the vector difference @xmath34 .",
    "the basis of the distribution vector can be changed using an invertible matrix @xmath35 : @xmath36 . in particular ,",
    "the matrix @xmath35 can be chosen such that @xmath37 is a vector of moments of the particle distributions @xmath0 .",
    "the idea of mrt is to perform the collision in the moment basis , and then change the basis to recover the original form of the particle distributions for the streaming step @xcite .",
    "the mrt collision operator can be written as : @xmath38 where the collision matrix @xmath39 is diagonal with its elements @xmath40 representing the inverses of relaxation times for individual moments .",
    "collision matrix elements corresponding to conserved moments , such as density or momentum are set to 0 .",
    "some of the remaining elements are related to kinematic and bulk viscosities , but the other ones can be tuned to increase the numerical stability of the simulation without changing its physical results .",
    "we refer the reader to the paper of dhumieres et al .",
    "@xcite for a detailed discussion of optimal choices for relaxation times , and the form of the matrix @xmath35 .      in the entropic lb model ,",
    "a discrete h - function @xmath41 is defined and the equilibrium distribution function is defined as the extremum of @xmath42 under constraints of mass and momentum conservation : @xmath43 where @xmath44 is the dimensionality of the lattice .",
    "the relaxation process is also modified to include a new dynamically adjusted parameter @xmath45 : @xmath46 with @xmath47 .",
    "@xmath45 is evaluated at every simulation step as the solution of the @xmath48-function monotonicity constraint : @xmath49 this procedure guarantees unconditional stability of the numerical scheme , as the @xmath42 function can be proven to be a lyapunov function for the system .",
    "when @xmath50 as is the case when the system is close to equilibrium , has the same form as .",
    "the corrections resulting from h - theorem compliance can be both positive and negative , temporarily increasing and decreasing the effective local viscosity of the fluid  @xcite .",
    "the idea behind the smagorinsky subgrid model is to locally modify the fluid viscosity by adding an eddy viscosity term dependent on the magnitude of the strain rate tensor @xmath51 : @xmath52 where @xmath53 is the baseline viscosity of the fluid and @xmath54 is an eddy viscosity , the form of which depends on the subgrid model used in the simulation . in the smagorinsky model , the relaxation time @xmath55 can be calculated using the momentum flux tensor @xmath56 : @xmath57 where @xmath58 is the smagorinsky constant , which for lb models can be taken to be @xmath59 .",
    "this effectively abandons the single relaxation time approximation by making it a spatially and temporally varying quantity dependent on the local gradients of the fluid velocity @xcite .",
    "the multi - fluid models discussed here are all diffuse interface models , without any explicit interface tracking ",
    "the dynamics of the interface emerges naturally from the interactions between the fluid components .",
    "all models in this class use an additional lattice ( whose mass fractions we will call @xmath60 ) to represent the second fluid species and nonlocal interactions between the lattices in the collision process .      in the shan - chen model , both lattices",
    "are used to represent fluid components , with the 0-th moment of @xmath0 and @xmath60 representing the density of the first ( a ) and second fluid components ( b ) , respectively .",
    "the equilibrium function and relaxation schemes remain unchanged for both lattices , but an additional coupling term in the form of a body force : @xmath61 is introduced into ( see section  [ sec : body_forces ] for a short overview of ways of adding body forces to an lb simulation ) , where @xmath62 is a pseudopotential function , dependent on the density @xmath63 at node @xmath10 and @xmath64 is a coupling constant . a similar term @xmath65 of the form with @xmath66 replaced by @xmath67 and vice versa",
    "is added to the collision operator for the second component .",
    "a commonly used pseudopotential function is @xmath68 .",
    "the velocity of the fluid becomes a weighted average of the first moments of the distribution functions : @xmath69 , where @xmath70 , @xmath71 and @xmath72 , @xmath63 are velocities and densities computed respectively from @xmath0 and @xmath60 using , and @xmath73 , @xmath74 are relaxation times for the two fluid components .",
    "the free energy model is based on a landau free energy functional for a binary fluid @xcite . in the lb realization ,",
    "the 0-th moment of @xmath0 represents the density @xmath5 of both components , while the 0-th moment of @xmath60 is a smoothly varying order parameter @xmath75 , with @xmath76 indicating a pure first component and @xmath77 indicating a pure second component : @xmath78 the macroscopic dynamics of the fluid is described by the system of equations @xmath79 where @xmath35 is a mobility parameter , @xmath80 is the chemical potential and the pressure tensor @xmath81 is defined as : @xmath82 and @xmath83 is the bulk pressure .    in the corresponding lb scheme ,",
    "distributions on both lattices are relaxed and streamed using the standard lbgk scheme using the relaxation times @xmath84 and @xmath85 , respectively .",
    "the following equilibrium functions @xcite are used to recover in the macroscopic limit : @xmath86 \\rho u_{\\alpha } u_{\\beta }              \\right ) + \\kappa w_i^{\\alpha \\beta } \\partial_{\\alpha } \\phi \\partial_{\\beta } \\phi \\\\",
    "g_i^{\\mathrm{eq } } & = w_i \\left ( \\gamma \\mu + e_{i \\alpha } u_{\\alpha } \\phi +              \\frac{3}{2 } \\left [ e_{i \\alpha } e_{i \\beta } - \\frac{\\delta_{\\alpha \\beta}}{3 } \\right ] \\phi u_{\\alpha } u_{\\beta } \\right )      \\label{eq : free_energy_eq}\\end{aligned}\\ ] ] where @xmath87 is a tunable parameter related to mobility via @xmath88 , and the chemical potential @xmath89 . @xmath90 and @xmath91 are constant parameters related to the surface tension @xmath92 and interface width @xmath93 .",
    "the values of the weights @xmath15 and @xmath94 can be found in the paper by kusumaatmaja and yeomans  @xcite .    in order to minimize spurious currents at the interface between the two fluids , we use optimized gradient and laplacian stencils @xcite .",
    "sailfish implements various boundary conditions , which taken together make it possible to model a wide range of physical situations :    * periodic boundary conditions ( used to simulate a system spatially infinite along a given axis ) , * no - slip ( solid walls ) : half - way bounce - back , full - way bounce - back ( both with configurable wetting when used with the free energy model ) , tamm - moth - smith  @xcite , * density / pressure : guo s method , zou - he s method , equilibrium distribution , regularized  @xcite , * velocity : zou - he s method , equilibrium distribution , regularized  @xcite , * outflow : grad s approximation  @xcite , yu s method  @xcite , neumann s , nearest - neighbor copy , , , do nothing   @xcite .",
    "( b ) streaming , time @xmath95 ( c ) relaxation , time @xmath95 ( arrows ending with disks represent the post - relaxation state ) , ( d ) streaming , time @xmath96.,title=\"fig : \" ] [ fig : bb ]    the bounce - back method is a simple idea originating from lattice gas automatons ",
    "a particle distribution is streamed to the wall node , and scattered back to the node it came from ( see figure  [ fig : bb ] ) . in the full - way bounce - back scheme the wall nodes do not represent any fluid ( , , dry  nodes ) and are only used to temporarily store distributions before scattering .",
    "this approach is slightly easier to implement , but has the downside of the distributions needing two time steps to reach back to the originating fluid node ( one step to reach the wall node , then another to be reflected back and streamed to the fluid node ) .",
    "the physical wall is effectively located between the last fluid node and the wall node .",
    "in contrast , in the half - way bounce - back the wall nodes do represent fluid , the wall is located beyond the wall node ( away from the fluid domain ) , and the distributions are reflected in the same time step in which they reach the wall node  @xcite .    for other types of boundary conditions ,",
    "the problem is more complex , as the mass fractions pointing into the fluid domain are undefined ( there are no nodes streaming to them in these directions ) .",
    "various schemes have been proposed to work around this problem . in the equilibrium approach ,",
    "the missing incoming distributions are assumed to have the same values as those opposite to them ( bounce - back ) , and the post - collision distributions are defined to be @xmath97 with one of @xmath98 being specified as part of the boundary condition , and the other calculated from the local distribution function .",
    "zou s and he s method @xcite uses the idea of the bounce - back of the non - equilibrium parts of the distributions to calculate the missing mass fractions",
    ". the regularized boundary conditions also use the non - equilibrium bounce - back idea , but only for an intermediate step to calculate the 2-nd moment of the non - equilibrium distribution , which is then used to reset all mass fractions at the boundary node via a regularization procedure .",
    "we refer the reader to the excellent paper by latt et al .",
    "@xcite for a detailed overview of these and other boundary conditions .",
    "the grad s approximation method uses a formula dependent on the density , velocity and the pressure tensor @xmath99 to replace all distributions on the boundary node .",
    "the three macroscopic quantities can be taken to be those from the boundary node at the previous time step ( less precise , fully local ) or extrapolated from neighboring fluid nodes ( more precise , nonlocal ) .",
    "other outflow methods , such as the neumann s boundary implementation or yu s method , are also nonlocal and use a simple extrapolation scheme . the simplest methods ( nearest - neighbor copy , do nothing ) do not require extrapolation , but also give less control over the resulting macroscopic fields .",
    "sailfish implements two popular ways of adding a body force @xmath100 to the simulation : guo s method @xcite , and the exact difference method ( edm ) @xcite . in both schemes , the actual fluid velocity @xmath101 can be computed as @xmath102 and a force term @xmath103 is added to the right hand side of .",
    "guo et al .",
    "analyzed a number of popular schemes and concluded that the optimal choice for @xmath103 in an lbgk simulation is @xmath104 \\cdot \\vec{f}.      \\label{eq : guo_force}\\ ] ]    in the edm , the body force term added to the collision operator is equal to the difference of equilibrium distribution functions computed using momentum after and before the action of the force @xmath100 : @xmath105 with @xmath106 .",
    "the advantages of the edm are its lack of any spurious terms in the macroscopic navier - stokes equations and its applicability to any collision operator ( not only lbgk ) .",
    "computational software is traditionally developed in one of two different paradigms .",
    "_ prototype _ code is written with relatively little effort in a high level environment such as matlab or python for exploratory or teaching purposes . due to the relatively low performance of these environments , this type of code is unsuitable for large scale problems .",
    "in contrast , _ production _ code is written in lower level languages such as c++ or fortran in a higher effort process , which can sometimes span many years and involve teams of developers .",
    "the resulting programs are more efficient , but also longer , less readable , and potentially difficult to maintain .    in our implementation of the lattice boltzmann method for gpus released under the name _ project sailfish _",
    ", we took a hybrid approach .",
    "we use the python programming language with a template - based code generation module ( using the mako language ) and a computer algebra system ( sympy ) to generate code in cuda c or opencl .",
    "we chose python because it is a very expressive language , with bindings to many system libraries and great support for gpu programming via the pycuda and pyopencl packages  @xcite .",
    "in sailfish , python is used for setting up the simulation ( initial conditions , boundary conditions , selecting an lb model ) , for simulation control , communication ( e.g. between compute nodes in a cluster ) and input / output ( loading geometry , saving simulation results ) . we also employ the numpy package to perform matrix operations efficiently .    .... from sympy",
    "import rational def bgk_equilibrium(grid ) :      out = [ ]      for ei , weight in zip(grid.basis , grid.weights ) :          out.append(weight * ( s.rho + s.rho * ( 3 * ei.dot(grid.v ) + rational(9 , 2 ) * ( ei.dot(grid.v))**2 - rational(3 , 2 ) * grid.v.dot(grid.v ) ) ) )        return out ....    mathematical formulas , such as , , are stored in the form of sympy expressions .",
    "we decided to do this after noticing that computer code for numerical calculations is often very repetitive , e.g. , when evaluated on a d3q19 lattice , would expand to 19 long lines of code , even though the initial formula easily fits in one line ( see listing  [ lst : equilibrium ] ) .",
    "this approach makes the code easier to read ( the formulas can also be automatically converted to latex expressions ) , allows for automated consistency checks ( e.g. one can easily , in a single line of code , verify that the 0th moment of @xmath107 is @xmath5 ) , enables easy experimentation with code optimization techniques ( formulas can be reorganized e.g. for speed or precision of floating - point operations ) , and enables code reuse ( e.g. the formula for equilibrium or the bounce - back rule is written down once , but can generate code for various types of lattices , both in 2d and 3d , see listing  [ lst : bb ] for an example ) .    finally , the mako template language is used to render the formulas into low - level cuda / opencl code .",
    "the generated code is fairly small , contains comments , and is automatically formatted , making it suitable for instructional purposes as well as for one - off , ad - hoc modifications .",
    "this is in stark contrast to large , multimodular codes where the architecture of the system can be difficult to understand without extensive documentation .",
    "sailfish was designed from scratch and optimized for modern massively parallel architectures , such as gpus .",
    "we wanted to achieve high flexibility of the code , ease of use when running or defining new simulations , and not take any performance hits compared to code written directly in cuda c or opencl . in order to achieve",
    "that , we decided to use run - time code generation techniques .",
    "this provides some isolation from low - level hardware details , important in the case of gpus which are still evolving rapidly and changing with every new gpu architecture .",
    "it also makes it possible to generate optimized code on a case - by - case basis and to automatically explore parameter spaces to find optimal solutions , thus saving programmer time and increasing their productivity . with many details , such as the number of compute units , size of on - chip memory , speed of access patterns to on - chip and off - chip memory , host - device latency and bandwidth , memory bandwidth to computational performance ratio directly impacting the performance of the code , experimentation and microbenchmarking",
    "are necessary to find combinations of parameters that work well .",
    "we also considered other metaprogramming approaches to code generation , such as domain - specific languages , and templates in c++ .",
    "we deemed the first solution to have too much overhead , and decided against the latter one since the expanded code can not be saved for inspection and modification , and there were no open source computer algebra libraries providing a level of flexibility and sophistication comparable to sympy .    ....    $ { device_func } inline void bounce_back(dist * fi )        float t ;        % for i in sym.bb_swap_pairs(grid ) :          < % a = grid.idx_name[i ]             opp_i = grid.idx_opposite[i ]             b = grid.idx_name[opp_i ] % >",
    "t = fi->${a } ;          fi->${a } = fi->${b } ;          fi->${b } = t ;      % endfor } ! \\vfill\\columnbreak ! _ _ device _ _ inline void bounce_back(dist * fi ) {      float t ;      t = fi->fe ;      fi->fe = fi->fw ;      fi->fw = t ;      t = fi->fn ;      fi->fn = fi->fs ;      fi->fs = t ;      t = fi->fne ;      fi->fne = fi->fsw ;      fi->fsw = t ;      t = fi->fnw ;      fi->fnw = fi->fse ;      fi->fse = t ; } ....      sailfish takes extensive advantage of objected - oriented programming techniques for modularization and code reuse purposes .",
    "each simulation is defined using two classes  a simulation class , and a geometry class ( see listing  [ lst : ldc_example ] ) .",
    "the simulation domain can be divided into cuboid subdomains , which do not need to fill the parts of the domain that do not contain any fluid ( this makes it possible to handle complex geometries ) .",
    "the geometry class defines initial and boundary conditions for a single subdomain .",
    "the simulation class derives from a base class specific to the lb model used such as ` lbfluidsim ` ( single component simulations ) or ` lbbinaryfluidfreeenergy ` ( binary fluids using the free energy model ) .",
    "the simulation class can optionally also add body forces or define custom code to be run after selected steps of the simulation ( e.g. to display a status update , check whether the steady state has been reached , etc ) .",
    "the base simulation class specifies the details of the used lb model , such as the form of the equilibrium distribution function ( in symbolic form , stored as a sympy expression ) , number and names of macroscopic fields ( density , velocity , order parameter , etc ) , and names of gpu code functions that need to be called at every simulation step .    when executed , every sailfish simulation starts a controller process which parses any command line parameters and reads configuration files , decides how many computational nodes and gpus are to be used ( in case of distributed simulations ) , and how the subdomains are going to be assigned to them .",
    "it then starts a master process on every computational node ( see figure  [ fig : system_arch ] ) , which in turn starts a subdomain handler process for every subdomain assigned to the computational node .",
    "we use subprocesses instead of threads in order to avoid limitations of the python runtime in this area ( the global interpreter lock preventing true multithreading ) , as well as to simplify gpu programming .",
    "the subdomain handlers instantiate the simulation and geometry classes for their respective subdomains .",
    "the system state is initialized in the form of numpy arrays describing the initial macroscopic fields , and these are then used to initialize the distribution functions on the gpu ( using the equilibrium distribution function ) .",
    "optionally , a self - consistent initialization procedure @xcite can be performed to compute the density field from a known velocity field .",
    "once the system is fully initialized , the simulation starts running , with the subdomain handlers exchanging information in a peer - to - peer fashion as determined by the connectivity of the global geometry .    .... from sailfish.subdomain import subdomain2d from sailfish.node_type import ntfullbbwall , ntzouhevelocity from sailfish.controller import lbsimulationcontroller from sailfish.lb_single import lbfluidsim    class ldcsubdomain(subdomain2d ) :      max_v = 0.1        def boundary_conditions(self , hx , hy ) :          wall_map = ( hx = = self.gx-1 ) | ( hx = = 0 ) | ( hy = = 0 )          self.set_node((hy = = self.gy-1 ) & ( hx > 0 ) & ( hx < self.gx-1 ) , ntzouhevelocity((self.max_v , 0.0 ) ) )          self.set_node(wall_map , ntfullbbwall )        def initial_conditions(self , sim , hx , hy ) :          sim.rho [ : ] = 1.0          sim.vx[hy = = self.gy-1 ] = self.max_v    class ldcsim(lbfluidsim ) :      subdomain = ldcsubdomain    if _ _ name _ _ = = ' _ _ main _ _ ' :      lbsimulationcontroller(ldcsim).run ( ) ....          modern gpus are massively parallel computational devices , capable of performing trillions of floating - point operations per second .",
    "we will now briefly present the architecture of cuda - compatible devices as a representative example of the hardware architecture targeted by sailfish .",
    "other devices not supporting cuda but supporting opencl are based on the same core concepts .",
    "cuda devices can be grouped into three generations , called tesla , fermi , and kepler  each one offering progressively more advanced features and better performance .",
    "the cuda gpu is organized around the concept of a streaming multiprocessor ( mp ) .",
    "such a multiprocessor consists of several scalar processors ( sps ) , each of which is capable of executing a thread in a simt ( single instruction , multiple threads ) manner .",
    "each mp also has a limited amount of specialized on - chip memory : a set of 32-bit registers , a shared memory block and l1 cache , a constant cache , and a texture cache .",
    "the registers are logically local to the scalar processor , but the other types of memory are shared between all sps in a mp , which allows data sharing between threads .",
    "perhaps the most salient feature of the cuda architecture is the memory hierarchy with 1 - 2 orders of magnitude differences between access times at each successive level .",
    "the slowest kind of memory is the host memory ( ram ) .",
    "while the ram can nowadays be quite large , it is separated from the gpu by the pcie bus , with a maximum theoretical throughput in one direction of 16 gb / s ( pci express 3.0 , x16 link ) .",
    "next in line is the global device memory of the gpu , which is currently limited to several gigabytes and which has a bandwidth of about 100 - 200  gb / s .",
    "global memory accesses are however high - latency operations , taking several hundred clock cycles of the gpu to complete .",
    "the fastest kind of memory currently available on gpus is the shared memory block residing on mps .",
    "it is currently limited in size to just 48  kb ( 16  kb on tesla devices ) , but has a bandwidth of ca  1.3  tb / s and a latency usually no higher than that of a  sp register access .",
    "the above description readily suggests an optimization strategy which we will generally follow in the next section and which can be summarized as : move as much data as possible to the fastest kind of memory available and keep it there as long as possible , while minimizing accesses to slower kinds of memory .",
    "when memory accesses are necessary , it also makes sense to try to overlap them with independent computation , which can then be executed in parallel effectively hiding the memory latency .    from the programmer s point of view , cuda programs are organized into kernels .",
    "a kernel is a function that is executed multiple times simultaneously on different mps .",
    "each instance of this function is called a thread , and is assigned to a single scalar processor .",
    "threads are then grouped in one- , two- or three - dimensional blocks assigned to multiprocessors in an 1 - 1 manner ( 1 block - 1 mp ) .",
    "the blocks are organized into a one- or two - dimensional grid .",
    "the size and dimensionality of the grid and blocks is determined by the programmer at the time of kernel invocation .",
    "knowledge of the grid position , and the in - block position makes it possible to calculate a thread i d that is unique during the kernel execution . within a single block threads",
    "can synchronize their execution and share information through the on - chip shared memory .",
    "synchronization between blocks is not supported in any way other than serializing the execution of kernels , and through atomic operations on global memory .",
    "the great potential of modern gpus as a hardware platform for simulating both two - dimensional @xcite and three - dimensional flows @xcite with the lattice boltzmann method was quickly realized after the initial release of the cuda programming environment in 2007 .",
    "recently , gpus have also been used to investigate more complex , two - dimensional multicomponent flows @xcite .",
    "there are three main factors that need to be carefully optimized in order to fully utilize the computational power of modern gpus : memory access patterns , register utilization , and overlap of memory transfers and arithmetic operations .",
    "the first factor has received the most attention in the literature , but in this work we show that the remaining two are also important .",
    "the most important data structure used in an lb simulation are the particle distributions @xmath0 .",
    "they are stored in the global gpu memory in a structure of arrays ( soa ) fashion , effectively forming a 4d or 3d array ( for 3d and 2d simulations , respectively ) , with the following index ordering : @xmath108 , where @xmath109 is the number of discrete velocities in the lattice .",
    "an array of structures ( aos ) approach , while elegant from the point of view of object - oriented programming , is completely unsuitable for gpu architectures due to how global reads and writes are performed by the hardware .",
    "global memory accesses are performed by thread warps ( 32 threads ) in transactions of 32 , 64 , or 128 bytes .",
    "on fermi devices , accesses cached in the l1 cache are serviced with 128-byte transactions ( the size of a full l1 cache line ) , while those cached in the l2 cache are serviced with 32-byte transactions . in order to attain good bandwidth utilization",
    ", the memory has to be accessed in contiguous blocks so that all bytes in a transaction are used for meaningful data .",
    "the memory location also has to be naturally aligned , i.e. the first address in the transferred segment must by a multiple of the segment s size .    in sailfish",
    ", we run all kernels in 1-dimensional thread blocks spanning the x axis of the subdomain , with a typical block size being 64 , 128 or 192 nodes .",
    "each thread handles a single node of the subdomain . due to the layout of the distributions in global memory , we issue @xmath109 read requests to load a full set of mass fractions in a thread block .",
    "each request results in a fully utilized memory transaction . to ensure natural alignment of transactions ,",
    "the x dimension of the distributions array in global memory is padded with unused nodes so that @xmath3 is a multiple of 32 or 16 , for single and double precision floating point numbers , respectively .",
    "in addition to the distributions array , we also store a node type map ( see section  [ sec : boundary ] ) , and macroscopic field arrays in global memory , all following the same memory layout and padding as described above .",
    "a simple lb simulation in sailfish repeatedly calls a single cuda kernel called ` collideandpropagate ` , which implements both the collision and propagation step ( see e.g. ) , as well as any boundary conditions . a high - level summary of this kernel",
    "is presented in algorithm  [ alg : cnp ] .",
    "allocate shared memory array @xmath110 $ ] compute global array index @xmath111 for the current node @xmath1 load and decode node type from global memory to @xmath112 load distributions from global memory to @xmath113 compute macroscopic variables @xmath5 and @xmath101 apply boundary conditions save @xmath5 and @xmath101 in global memory arrays compute the equilibrium distribution @xmath107 using @xmath5 and @xmath101 relaxation : @xmath114 write @xmath115 to global memory at @xmath116 @xmath117 \\gets f_{k}$ ] write @xmath115 to global memory at @xmath118 synchronize threads within the block write @xmath119 $ ] to global memory at @xmath116 @xmath120 \\gets f_{k}$ ] write @xmath115 to global memory at @xmath118 synchronize threads within the block write @xmath119 $ ] to global memory at @xmath116    two basic patterns of accessing the distributions have been described in the literature , commonly called ab and aa  @xcite . in the ab access pattern , there are two copies of the distributions in global memory ( a and b ) . the simulation step alternates between reading from a and writing to b , and vice versa . in the aa access pattern ,",
    "only a single copy of the distributions array is stored in global memory .",
    "since there are no guarantees about the order of execution of individual threads , care has to be taken that a thread reads from and writes to exactly the same locations in memory .",
    "this is illustrated in  figure  [ fig : memory ] .",
    "a third access pattern that is used in practice is the so - called _ indirect addressing_. in this mode , in order to access data for a node , its address has to be read first .",
    "this causes overhead both in storage ( need to store the addresses ) and in memory accesses , but can be very useful for complex geometries where the active nodes are only a tiny fraction of the volume of the bounding box .",
    "indirect addressing can be combined with both aa and ab access patterns . for a dense subdomain using indirect addressing",
    ", the performance can be 5 - 25% lower than when using direct addressing , with c2050 showing better results with the aa access pattern than in ab , and the gtx 680 exhibiting the opposite tendency .",
    "the exact performance is however necessarily geometry - dependent , and as such it is not discussed further here .     [",
    "fig : memory ]    the propagation step of the lb algorithm shifts the distributions by @xmath121 node in all directions  when this happens for the x axis , it results in misaligned reads / writes . to reduce the impact of misaligned writes , sailfish utilizes shared memory to shift data in the x direction within the thread block ( see  algorithm  [ alg : cnp ] ) .",
    "we performed numerical tests to evaluate the impact of various data access patterns on the overall performance of the simulation .",
    "earlier works @xcite indicate that the cost of unaligned writes is significantly higher than the cost of unaligned reads , and that a propagate - on - read strategy results in up to 15% performance gain .",
    "our experiments confirmed this on older gt200 hardware ( gtx 285 ) , however we failed to replicate this effect on fermi and kepler devices ( tesla c2050 , k10 , k20 ) , where the performance of both implementations was nearly identical ( typically , with a few % loss for propagate - on - read ) .",
    "this is most likely caused by hardware improvements in the fermi and later architectures .",
    "we also compared the performance for the ab and aa access patterns .",
    "on tesla - generation devices ( gtx 285 , tesla c1060 ) , the aa memory layout results in a slightly higher performance , and is therefore clearly preferred . on newer devices",
    "the ab scheme is typically a little faster , but the performance gains over the aa scheme are minor ( @xmath122 ) , and as such the ab scheme is only preferable when ample gpu memory is available .",
    "boundary conditions are handled in sailfish with the help of a _ node type map _  an unsigned 32-bit integer array stored in the global gpu memory .",
    "each entry in this array contains encoded information about the node type ( fluid , unused , ghost , type of boundary condition to use ) , orientation ( vector normal to the boundary , pointing into the fluid ) and a parameter i d .",
    "the parameter i d is an index to a global array of values used by boundary conditions ( e.g. densities , velocities ) .",
    "the encoding scheme uses variable - size bitfields , which are dynamically chosen for every simulation depending on the usage of different boundary conditions in a subdomain ( see figure  [ fig : encoding ] ) .",
    "time - dependence is supported for all types of boundary conditions .",
    "when a value changing in time is required , it is typically specified in the form of a sympy expression .",
    "this expression is then transformed into a gpu function and assigned an i d , analogous to the parameter i d for static boundary conditions .",
    "models with more than one distribution function , such as the shan - chen model or the free energy model introduce a nonlocal coupling via one or more macroscopic fields . to minimize the amount of data read from global memory , we split the simulation step into two logical parts , implemented as two gpu kernels . in the first kernel , ` computemacrofields ` , we load the distributions and compute the macroscopic field value for every field which needs to be accessed in a nonlocal manner in the collision kernel .",
    "the collision step is implemented similarly to single fluid models , and the nonlocal quantities ( shan - chen force , gradient and laplacian of the order parameter in the free energy model ) are computed by directly accessing the macroscopic field values in global memory .",
    "we considered three approaches to realizing the collision and propagation part of the algorithm for multifluid models . in the first variant",
    ", we used a single collision kernel , which loaded both distributions into registers and ran the collision and propagation step for both lattices . in the second variant",
    ", we tried to speed - up the calculations of nonlocal values by binding the macroscopic fields to gpu textures and accessing them through these bindings . in the last variant ,",
    "we split the collision process into two separate kernels , one for every lattice .",
    "the second variant yielded minimal speed - ups ( on the order of a few percent ) on old tesla - generation devices , which however did not carry over to fermi and kepler ones .",
    "we abandoned the idea as it introduced unnecessary complexity to the code . the third approach using split kernels proved to be the most efficient one .",
    "we were able to obtain 5.4% ( d2q9 , free energy ) ",
    "53% ( d3q19 , free energy ) speed - ups as compared to a monolithic kernel , mainly due to the kernels using fewer registers and being able to achieve higher occupancy , which hid the minor overhead introduced by a slightly increased number of global memory accesses . as a side benefit , this approach also resulted in simpler code , so we decided to standardize on it in the sailfish framework .",
    "we also expect it to scale well to models requiring more than 2 lattices .      in sailfish ,",
    "the mechanisms that support distributed simulations are very similar to those supporting multiple subdomains on a single gpu or many gpus on one computational node .",
    "every subdomain has a layer of _ ghost _ nodes around it .",
    "these nodes do not participate in the simulation , but are used for data storage for mass fractions leaving the subdomain or for macroscopic fields of neighboring nodes located in remote subdomains .",
    "once mass fractions are streamed into the ghost nodes , we run additional cuda kernels to collect the data leaving the subdomain into a linear buffer in global memory , which is then transferred back to the host and sent to remote nodes using a network link .    the subdomain is also split into two areas called _ bulk _ and _ boundary _ , which are simulated via two separate kernel calls .",
    "the boundary area is defined as all nodes belonging to cuda thread blocks where at least one node touches the subdomain boundary .",
    "the simulation step is first performed for nodes in the boundary area , so that communication with remote subdomain runners can start as soon as possible and can overlap in time with the simulation of the bulk of the subdomain . as a further optimization ,",
    "we limit the data sent between computational nodes exactly to the mass fractions that actually need to be sent ( e.g. if two nodes are connected along the x axis , then only the mass fractions corresponding to discrete velocities with a positive x component will be transferred from the left subdomain to the right one ) .",
    "we also make it possible to optionally compress the data before sending it to the remote node , which can be helpful if the simulation is run on multiple machines with slow network links between them , or when only a small fraction of nodes on the subdomain interface plane is active .",
    "many gpus are significantly faster when calculations are done in single precision as opposed to the standard double precision which is typically used in scientific and engineering applications .",
    "the speed - up factor can vary between 10 and 2 , depending on the device model and generation .",
    "the performance gap is smaller in newer devices ( e.g. kepler generation ) .",
    "earlier works @xcite used the lid - driven cavity benchmark to verify that single precision calculations produce satisfactory results .    here",
    ", we use the 2d taylor - green decaying vortex flow  a benchmark problem with known analytical solution  to study the accuracy of our lb implementation in both single ( sp ) and double ( dp ) precision .",
    "the simulations are done using the lbgk relaxation model on a d2q9 lattice .",
    "for single precision , both the standard formulation and the round - off minimizing formulation @xcite ( sro ) are tested .",
    "the taylor - green vortex can be described by the following system of equations : @xmath123 where @xmath124 is a velocity constant and @xmath125 .",
    "can be easily verified to satisfy the incompressible navier - stokes equations .",
    "we performed multiple runs of the simulation on a @xmath126 lattice with periodic boundary conditions in both directions , varying the viscosity @xmath13 but keeping the reynolds number constant at @xmath127 .",
    "each simulation was run until it reached a specific point in physical time , corresponding to @xmath128 iterations at @xmath129 .",
    "we use the l2 norm of the velocity and density field difference to characterize the deviation of the numerical solution from the analytical one : @xmath130 where @xmath131 , @xmath132 is the numerical solution .",
    "the results presented in  figure  [ fig : precision ] illustrate interesting differences between double precision and single precision calculations . in double precision ,",
    "the error stays constant until @xmath133 and raises quadratically for higher values of velocity , as could be expected from the @xmath134 accuracy of the lbgk model . in single precision however , lower speeds lead to higher error values .",
    "the interplay between model accuracy and numerical round - off leads to a sweet spot around @xmath135 , where the errors are minimized . for higher speeds",
    ", there is no difference between double and single precision .",
    "this result shows that the conventional wisdom that lower speeds always lead to better precision is not true when applied to single precision simulations .",
    "since many practical simulations are run at velocities @xmath22 and higher for reasons of efficiency , it also explains why in many cases no differences are observed between single and double precision results .",
    "the density error shows that the round - off minimization model generates significantly better results than the standard single precision implementation , and as such should be preferred in cases where the accuracy in the density field is important .",
    "all performance figures in the this section are quoted in mlups ( millions of lattice - site updates per second ) .",
    "the tests were run using cuda toolkit 5.0 , pycuda 2013.1.1 on 64-bit linux systems .      .",
    "both logical gpus were used for the k10 tests .",
    "left panel : single precision .",
    "right panel : double precision . ]",
    "[ fig : memory_grid ]    single fluid models were benchmarked using a lid - driven cavity test case , with a @xmath136 lattice at @xmath127 , using full bounce - back walls and equilibrium velocity boundary conditions for the lid . to illustrate the impact of the entropy - stabilized time - stepping , the elbm solver was also tested at @xmath137 .",
    "the binary fluid models were benchmarked using a simple spinodal decomposition test case , where a uniform mix of both fluid components fills the whole simulation domain ( @xmath138 ) and periodic boundary conditions are enabled in all directions . whenever the domain size was too large to fit on a specific gpu we tested , we reduced the size in the z direction until it fit .",
    "we find our results to be comparable or better to those reported by habich et al .",
    "@xcite , who used hardware very similar to ours ( tesla c2050 and c2070 differ only in memory size , according to nvidia specifications ) .",
    "the performance of single precision simulations can be shown to be limited by global memory bandwidth .",
    "we find that our code runs at @xmath139 of the theoretical bandwidth as reported in nvidia whitepapers , and close to 100% of the real bandwidth measured by sample code from the nvidia sdk .",
    "double precision simulations run at @xmath140 of the theoretical maximum .",
    "they are limited by the double precision instruction throughput on the gpu on fermi - class hardware and by memory bandwidth in kepler hardware .",
    "overall , we find that the memory bandwidth is a reliable indicator of expected performance for single precision simulations across all three generations of nvidia devices ( see also  figure  [ fig : memory_grid ] , which shows that the simulation performance is inversely proportional to the lattice connectivity @xmath141 in single precision ) .    for double precision simulations on fermi hardware",
    ", we have found increasing the l1 cache size to 48 kb , disabling l1 cache for global memory accesses and replacing division operations by equivalent multiplication operations to have a large positive impact on the performance of the code ( @xmath142 speed - up in total ) .",
    "the impact of the l1 cache is understandable if one considers the fact that double precision codes use significantly more registers .",
    "this causes the compiler to spill some of them over to local memory , accesses to which always go via the l1 cache .",
    "the larger the size of the unused part of that cache , the more operations can execute without actually accessing the global memory .",
    "this optimization strategy does not apply to kepler - class devices , where both in single and double precision we found that disabling the preference for l1 cache for the main computational kernel had a positive impact on performance .",
    "we also tested our code on lower end gpus ( mobile versions used in laptops ) with compute capability 3.0 , where we found a significant speed up ( up to 40% ) by using shuffle operations for in - warp propagation , and limiting shared memory for data exchange between warps only .",
    "this optimization strategy does not work with the higher end gpus discussed in this paper , where the performance is limited by global memory bandwidth already .",
    "overall , we unsurprisingly find that more recent gpus perform noticeably better .",
    "the k10 is an interesting option if only single precision is required as it delivers the highest overall performance at a level higher than 1.3 glups per board with d3q19 and simple fluid models . for double precision ,",
    "the k20x card being the most advanced nvidia gpu available on the market when this paper is written , is a clear winner performance - wise .      while the computational power provided by a single gpu is impressive",
    ", practical simulations often require large domains , and for these the total size of gpu memory ( a few gbs ) is an important limitation .",
    "the natural solution of this problem is to run a distributed simulation using multiple gpus , which can be physically located in a single computer or multiple computers in a network .    in order to measure performance of distributed simulations",
    ", we ran a 3d duct flow test case ( periodic boundary conditions in the streamwise z direction , bounce - back walls at other boundaries ) on the zeus cluster ( part of the plgrid infrastructure ) , consisting of computational nodes with 8 m2090 gpus and interconnected with an infiniband qdr network .      the first test we performed measured weak scaling , i.e. code performance as a function of increasing size of the computational domain .",
    "the domain size was @xmath143 , where @xmath144 is the number of gpus .",
    "we used the d3q19 lattice , the aa memory access pattern , a cuda block size of 128 , and single precision .",
    "figure  [ fig : weak_scaling ] shows excellent scaling up to 64 gpus , which was the largest job size we were able to run on the cluster .",
    "the 1.5% efficiency loss takes place as soon as more than 1 subdomain is used , and does not degrade noticeably as the domain size is increased .",
    "this small efficiency loss could be further minimized by employing additional optimization techniques , such as using peer - to - peer copies when the gpus are located on the same host .          the second test we ran measured strong scaling , i.e. code performance with a constant domain size , as a function of increasing number of gpus .",
    "we used a @xmath145 geometry ( largest domain size that fit within the memory of a single m2090 gpu ) and other settings as in the weak scaling test , and divided the domain into equal - length chunks along the z axis as more gpus were used for the simulation .",
    "the results of this test are presented in figure  [ fig : strong_scaling ] .",
    "a slightly worse performance is visible in comparison to the weak scaling test , but even when the simulation is expanded to run on 8 gpus , only a 3.5% efficiency loss can be observed .",
    "it should also be noted , that there is a minimum domain size below which performance quickly degrades due to the overhead of starting kernels on the gpus .",
    "this size can be seen in figure  [ fig : min_domain_size ] to be about 14% of the gpu memory or 8.2 m lattice nodes .",
    "cuda gpus provide an alternative hardware implementation of various transcendental functions such as the exponent , logarithm or trigonometric functions .",
    "these functions , known as intrinsic functions , are faster but less precise than their normal counterparts , especially when their arguments do not fall within a narrow range specified for each function .",
    "we analyzed the impact of these functions on the performance and precision of the lb models that can take advantage of them , namely the shan - chen model with a non - linear pseudopotential and the entropic lbm .",
    "with elbm the use of intrinsic functions , together with the fmad ( fused multiply - add ) instruction yields a speed - up of @xmath146% without any noticeable impact on the correctness of the results ( in terms of global metrics such as the total kinetic energy , enstrophy and the kinetic energy spectrum ) . while testing these optimizations , we also found that the ftz ( denormalize to 0 ) option of the cuda compiler causes the elbm simulation to crash .    with the proposed optimizations ,",
    "the performance of elbm is at 72% of the lbgk performance , making it a very interesting alternative for some simulations .    for the shan - chen model with a nonlinear pseudopotential we saw 17 - 20% speed - ups in 2d and 3d , with relative changes in the density fields smaller than 1.5% after 10000 steps .",
    "unfortunately , the same approach does not yield speed - ups in double precision , as most intrinsic functions are available in single precision only .",
    "in order to validate our implementation of the lbm , we performed simulations for four classical computational fluid dynamics test cases and compared our results with those published in the literature .",
    "the lid - driven cavity geometry consists of a cube cavity with a face length @xmath20 .",
    "the geometric center of the cavity is located at the origin of the coordinate system .",
    "the cube face at @xmath147 moves tangentially in the y - direction with a constant velocity @xmath148 , while all other faces are no - slip walls .",
    "we carried out simulations of this problem at @xmath127 with various lb models ( bgk , mrt , regularized bgk , elbm ) using a @xmath149 d3q19 lattice , full - way bounce - back for no - slip walls , and the regularized velocity boundary condition with @xmath150 for the moving wall .",
    "our results ( both in single and double precision ) agree with those published by albensoeder et al .",
    "@xcite ( see figure  [ fig : ldc - comparison ] )     and @xmath151 .",
    "round dots : data from tables  5 and  6 in albensoeder et al .",
    "@xcite solid line : results from sailfish simulations on a @xmath149 lattice , after @xmath152 steps using lbgk , mrt , regularized bgk and elbm in single and double precision .",
    "the results from all sailfish simulations are in agreement to within the width of the line on the plot .",
    "lb results are rescaled using : @xmath153.,scaledwidth=50.0% ]      the kida vortex flow is a free decay from the initial conditions  @xcite : @xmath154 defined on a cubic domain with face length @xmath155 , with periodic boundary conditions in all directions . to validate our code",
    ", we performed simulations for @xmath156 , @xmath157 , and @xmath158 and compared them with results published in @xcite and @xcite , respectively .",
    "the simulations were run using @xmath159 on a @xmath160 grid ( @xmath161 for @xmath162 ) , using both single and double precision ( with no noticeable difference between them ) .",
    "the @xmath162 and @xmath163 cases were investigated using the lbgk , mrt , regularized lbgk , smagorinsky - les and entropic models . at @xmath164 ,",
    "only simulations using the entropic model and the smagorinsky subgrid model ( with @xmath165 ) remained stable . during the simulation time kinetic energy @xmath166 and enstrophy @xmath167 , where @xmath168 is the volume of the simulation domain ,",
    "were tracked directly on the gpu for optimal efficiency .",
    "vorticity was computed using the first order central difference scheme in the bulk of the fluid and forward / backward differences at subdomain boundaries .     for various collision models , ( d ) kinetic energy spectrum for selected collision models and reynolds numbers . for panels ( a)-(c ) time",
    "is rescaled assuming a domain size of @xmath169 and @xmath170 . ]    for @xmath171 all four models gave the same results ( figure  [ fig : ke - ens ] ) . at @xmath163",
    "some minor differences are visible , particularly in the evolution of enstrophy .",
    "its peak value is slightly underpredicted by both models that locally modify effective viscosity ( smagorinsky , elbm ) ( see figure  [ fig : ke - ens](c ) ) . at @xmath164 ,",
    "the differences are more pronounced and we observe that the smagorinsky model underpredicts the absolute value of peak enstrophy .",
    "the kinetic energy spectrum shown on figure  [ fig : ke - ens](d ) was computed as @xmath172 , for @xmath173 .",
    "a good agreement is visible in comparison to the kolmogorov scaling @xmath174 , especially for the high reynolds number cases .",
    "all collision models lead to similar spectra , with elbm at @xmath162 predicting a slightly higher value around @xmath175 than lbgk or other models , and with elbm keeping a slightly flatter spectrum for high @xmath176 values at @xmath164 . in all cases",
    "the simulation results show the same features as those discussed in previous papers on this topic  @xcite .      to verify the binary fluid models we consider a 2d poiseuille flow in the @xmath3-direction .",
    "no - slip walls are imposed at @xmath177 using the half - way bounce - back boundary conditions , and periodic boundary conditions are used in the @xmath3-direction .",
    "a body force @xmath64 drives the flow . in the core flow region ( @xmath178 ) a fluid of viscosity @xmath179 is placed , while in the boundary flow region ( @xmath180 ) the viscosity of the fluid is @xmath53 . the analytical solution for this case",
    "can be expressed as : @xmath181 we run the simulation on a @xmath182 grid with @xmath183 , and @xmath184 .",
    "the simulation starts with @xmath185 in the whole domain and we let the flow reach the stationary state on its own .",
    "the free energy simulation was run with @xmath186 , @xmath187 , and @xmath188 , while for the shan - chen model , @xmath189 was used .",
    "the parameters were chosen to ensure that the interface remains as sharp as possible without destabilizing the simulation .",
    "the exact difference method was used to introduce body forces .",
    "the shan - chen model permits residual mixing between the fluid components , so the effective density and viscosity were calculated as @xmath190 and @xmath191 , respectively .",
    "figure  [ fig : poiseuille_comparison ] illustrates the good agreement of the simulation results with .     due to mixing between the two fluid components .",
    "the free energy model shows better agreement with the theoretical profile due to a thinner interface . ]      in order to verify the binary fluid models also in a dynamic case , we simulate the decay of the amplitude of a capillary wave .",
    "the boundary conditions of the system are the same as in the binary poiseuille flow case , but we now use a larger @xmath192 lattice in order to accommodate higher frequency waves . the region @xmath193 is filled with one component ( a ) and the region @xmath194 is filled with another component ( b ) . for simplicity , we choose both components to have the same viscosities @xmath195 .",
    "the interface between the two fluids is initialized to a sinusoidal curve of wavelength @xmath196 chosen such that an integer number of wavelengths fits exactly in the simulation domain .",
    "the interface is then allowed to relax naturally with no external forces , resulting in a damped capillary wave . at each timestep of the simulation ,",
    "the height of the interface at @xmath197 is recorded . in order to recover the frequency of the wave ,",
    "an exponentially damped sine function is fit to the interface height data .",
    "for the shan - chen model , we used @xmath198 and for the free energy model we used @xmath199 , @xmath200 , and @xmath201 .",
    "for the capillary wave for the free energy and shan - chen models.,scaledwidth=50.0% ]    as expected @xcite , the dispersion relation shows a power law form @xmath202 for both the shan - chen and free energy models ( see figure  [ fig : capillary ] ) .",
    "in the previous sections we have demonstrated our sailfish code as a flexible framework for implementing lattice boltzmann models on modern graphics processing units . with novel optimization techniques for complex models",
    "it provides a very efficient tool for a wide range of simulations .",
    "we hope that our observations collected while running the presented benchmarks will serve as a guideline in the choice of both lb models and computational hardware for users of sailfish and of other similar codes .    for single precision simulations , we advocate a careful choice of parameters and correctness testing via comparisons to similar test cases in double precision .",
    "while all of our benchmark problems did not show any noticeable differences between single and double precision , the taylor - green test case clearly demonstrates that these do exist and can significantly impact the results if the simulation is in the slow velocity regime .",
    "whenever possible , the round - off minimizing model should be used to reduce precision losses without any impact on performance .",
    "while the capabilities of sailfish are already quite extensive , much work remains to be done . among the most important remaining tasks , we mention ongoing efforts to implement a hierarchical lattice , allowing for local grid refinement and a fluid - structure interaction model based on the immersed boundary method .",
    "since the code is freely available under an open source license , we would like to invite the reader to participate in its development and contribute new enhancements according to their interests .",
    "this work was supported by the twing project co - financed by the european social fund , as well as in part by pl - grid infrastructure .",
    "m.j . thanks s.  chikatamarla and i.  karlin for useful discussions and sharing reference data from their simulations .",
    "the authors would also like to thank nvidia for providing hardware resources for development and benchmarking .",
    "m.  januszewski , m.  kostur , http://www.sciencedirect.com/science/article/b6tj5-4x7r842-1/2/ff8a1e76fd160f7aba015ca73e39a3ad[accelerating numerical solution of stochastic differential equations with cuda ] , computer physics communications 181  ( 1 ) ( 2010 ) 183  188 .",
    "http://dx.doi.org/doi : 10.1016/j.cpc.2009.09.009 [ ] .",
    "http://www.sciencedirect.com/science/article/b6tj5-4x7r842-1/2/ff8a1e76fd160f7aba015ca73e39a3ad      s.  chen , g.  d. doolen , http://dx.doi.org/10.1146/annurev.fluid.30.1.329[lattice boltzmann method for fluid flows ] , annual review of fluid mechanics 30  ( 1 ) ( 1998 ) 329364 .",
    "http://dx.doi.org/10.1146/annurev.fluid.30.1.329 [ ] .",
    "http://dx.doi.org/10.1146/annurev.fluid.30.1.329    c.  k. aidun , j.  r. clausen , http://dx.doi.org/10.1146/annurev-fluid-121108-145519[lattice-boltzmann method for complex flows ] , annual review of fluid mechanics 42  ( 1 ) ( 2010 ) 439472 .",
    "http://dx.doi.org/10.1146/annurev-fluid-121108-145519 [ ] .",
    "d.  dhumires , m.  bouzidi , p.  lallemand , http://link.aps.org/doi/10.1103/physreve.63.066702[thirteen-velocity three - dimensional lattice boltzmann model ] , phys .",
    "e 63 ( 2001 ) 066702 . http://dx.doi.org/10.1103/physreve.63.066702 [ ] .",
    "http://link.aps.org/doi/10.1103/physreve.63.066702    p.",
    "l. bhatnagar , e.  p.",
    "gross , m.  krook , http://dx.doi.org/10.1103/physrev.94.511[a model for collision processes in gases .",
    "i. small amplitude processes in charged and neutral one - component systems ] , physical review online archive ( prola ) 94  ( 3 ) ( 1954 ) 511525 . http://dx.doi.org/10.1103/physrev.94.511 [ ] .",
    "x.  shan , h.  chen , http://dx.doi.org/10.1103/physreve.47.1815[lattice boltzmann model for simulating flows with multiple phases and components ] , physical review e 47  ( 3 ) ( 1993 ) 18151819 .",
    "j.  latt , b.  chopard , http://www.sciencedirect.com/science/article/pii/s0378475406001583[lattice boltzmann method with regularized pre - collision distribution functions ] , mathematics and computers in simulation 72  ( 26 ) ( 2006 ) 165  168 , <",
    "ce : title > discrete simulation of fluid dynamics in complex systems</ce : title>. http://dx.doi.org/http://dx.doi.org/10.1016/j.matcom.2006.05.017 [ ] .",
    "s.  ansumali , i.  v. karlin , h.  c. ttinger , http://iopscience.iop.org/0295-5075/63/6/798[minimal entropic kinetic models for hydrodynamics ] , epl ( europhysics letters ) 63  ( 6 ) ( 2007 ) 798 .",
    "s.  chikatamarla , s.  ansumali , i.  karlin , http://link.aps.org/doi/10.1103/physrevlett.97.010201[entropic lattice boltzmann models for hydrodynamics in three dimensions ] , physical review letters 97 . http://dx.doi.org/10.1103/physrevlett.97.010201 [ ] . http://link.aps.org/doi/10.1103/physrevlett.97.010201    h.  yu , s.  s. girimaji , l .- s .",
    "luo , http://linkinghub.elsevier.com/retrieve/pii/s0021999105001907[dns and les of decaying isotropic turbulence with and without frame rotation using lattice boltzmann method ] , journal of computational physics 209  ( 2 ) ( 2005 ) 599616 . http://dx.doi.org/10.1016/j.jcp.2005.03.022 [ ] . http://linkinghub.elsevier.com/retrieve/pii/s0021999105001907    c.  m. pooley , h.  kusumaatmaja , j.  m. yeomans , http://link.aps.org/doi/10.1103/physreve.78.056709[contact line dynamics in binary lattice boltzmann simulations ] , phys .",
    "e 78 ( 2008 ) 056709 . http://dx.doi.org/10.1103/physreve.78.056709 [ ] .",
    "h.  kusumaatmaja , j.  m. yeomans , http://pre.aps.org/abstract/pre/v78/i5/e056709[contact line dynamics in binary lattice boltzmann simulations ] , physical review e 78  ( 5 ) ( 2008 ) 056709 .",
    "http://pre.aps.org/abstract/pre/v78/i5/e056709    c.  m. pooley , k.  furtado , http://link.aps.org/doi/10.1103/physreve.77.046702[eliminating spurious velocities in the free - energy lattice boltzmann method ] , phys .",
    "e 77 ( 2008 ) 046702 . http://dx.doi.org/10.1103/physreve.77.046702 [ ] .",
    "s.  chikatamarla , i.  karlin , http://linkinghub.elsevier.com/retrieve/pii/s0378437113000113[entropic lattice boltzmann method for turbulent flow simulations : boundary conditions ] , physica a : statistical mechanics and its applicationshttp://dx.doi.org/10.1016/j.physa.2012.12.034 [ ] . http://linkinghub.elsevier.com/retrieve/pii/s0378437113000113    j.  latt , b.  chopard , o.  malaspinas , m.  deville , a.  michler , http://link.aps.org/doi/10.1103/physreve.77.056703[straight velocity boundaries in the lattice boltzmann method ] , phys .",
    "e 77 ( 2008 ) 056703 . http://dx.doi.org/10.1103/physreve.77.056703 [ ] . http://link.aps.org/doi/10.1103/physreve.77.056703    s.  s. chikatamarla , s.  ansumali , i.  v. karlin , http://stacks.iop.org/0295-5075/74/i=2/a=215[grads approximation for missing data in lattice boltzmann simulations ] , epl ( europhysics letters ) 74  ( 2 ) ( 2006 ) 215 . http://stacks.iop.org/0295-5075/74/i=2/a=215    d.  yu , r.  mei , w.  shyy , improved treatment of the open boundary in the method of lattice boltzmann equation : general description of the method , progress in computational fluid dynamics , an international journal 5  ( 1 ) ( 2005 ) 312 .",
    "m.  junk , z.  yang , outflow boundary conditions for the lattice boltzmann method , progress in computational fluid dynamics , an international journal 8 3848 . http://dx.doi.org/10.1504/pcfd.2008.018077 [ ] .",
    "a.  j.  c. ladd , http://dx.doi.org/10.1017/s0022112094001771[numerical simulations of particulate suspensions via a discretized boltzmann equation part i. theoretical foundation ] , journal of fluid mechanics 271 ( 1993 ) 285309 .",
    "http://arxiv.org/abs/comp-gas/9306004 [ ] , http://dx.doi.org/10.1017/s0022112094001771 [ ] . http://dx.doi.org/10.1017/s0022112094001771    q.  zou , x.  he , http://link.aip.org/link/?phf/9/1591/1[on pressure and velocity boundary conditions for the lattice boltzmann bgk model ] , physics of fluids 9  ( 6 ) ( 1997 ) 15911598 .",
    "http://dx.doi.org/10.1063/1.869307 [ ] .",
    "http://link.aip.org/link/?phf/9/1591/1      a.  kupershtokh , d.  medvedev , d.  karpov , http://www.sciencedirect.com/science/article/pii/s0898122109001011[on equations of state in a lattice boltzmann method ] , computers & mathematics with applications 58  ( 5 ) ( 2009 ) 965  974 .",
    "http://dx.doi.org/10.1016/j.camwa.2009.02.024 [ ] .",
    "m.  bernaschi , l.  rossi , r.  benzi , m.  sbragaglia , s.  succi , graphics processing unit implementation of lattice boltzmann models for flowing soft systems , phys .",
    "e 80  ( 6 ) ( 2009 ) 066707 . http://dx.doi.org/10.1103/physreve.80.066707 [ ] .",
    "p.  bailey , j.  myre , s.  d.  c. walsh , d.  j. lilja , m.  o. saar , accelerating lattice boltzmann fluid flow simulations using graphics processors , in : parallel processing , 2009 .",
    "icpp09 . international conference on , 2009 , p. 550557 .    c.  obrecht , f.  kuznik , b.  tourancheau , j .-",
    "roux , http://www.sciencedirect.com/science/article/pii/s089812211000091x[a new approach to the lattice boltzmann method for graphics processing units ] , computers & mathematics with applications 61  ( 12 ) ( 2011 ) 3628  3638 .",
    "http://dx.doi.org/10.1016/j.camwa.2010.01.054 [ ] .",
    "http://www.sciencedirect.com/science/article/pii/s089812211000091x    c.  obrecht , f.  kuznik , b.  tourancheau , j .-",
    "roux , http://dl.acm.org/citation.cfm?id=1964238.1964257[global memory access modelling for efficient implementation of the lattice boltzmann method on graphics processing units ] , in : proceedings of the 9th international conference on high performance computing for computational science , vecpar10 , springer - verlag , berlin , heidelberg , 2011 , pp .",
    "f.  kuznik , c.  obrecht , g.  rusaouen , j .- j .",
    "roux , http://www.sciencedirect.com/science/article/pii/s0898122109006361[lbm based flow simulation using gpu computing processor ] , computers & mathematics with applications 59  ( 7 ) ( 2010 ) 2380  2392 . http://dx.doi.org/10.1016/j.camwa.2009.08.052 [ ] . http://www.sciencedirect.com/science/article/pii/s0898122109006361    c.  obrecht , f.  kuznik , b.  tourancheau , j .- j .",
    "roux , http://www.sciencedirect.com/science/article/pii/s0898122111001064[multi-gpu implementation of the lattice boltzmann method ] , computers & mathematics with applications  ( 0 ) ( 2011 ) .",
    "http://dx.doi.org/10.1016/j.camwa.2011.02.020 [ ] .",
    "j.  habich , c.  feichtinger , h.  kstler , g.  hager , g.  wellein , http://www.sciencedirect.com/science/article/pii/s0045793012000679[performance engineering for the lattice boltzmann method on gpgpus : architectural requirements and performance results ] , computers & fluids  ( 0 ) ( 2012 ) . http://dx.doi.org/10.1016/j.compfluid.2012.02.013 [ ] .",
    "s.  albensoeder , h.  kuhlmann , http://www.sciencedirect.com/science/article/pii/s0021999105000033[accurate three - dimensional lid - driven cavity flow ] , journal of computational physics 206  ( 2 ) ( 2005 ) 536  558 .",
    "http://dx.doi.org/10.1016/j.jcp.2004.12.024 [ ] .",
    "s.  kida , http://jpsj.ipap.jp/link?jpsj/54/2132/[three-dimensional periodic flows with high - symmetry ] , journal of the physical society of japan 54  ( 6 ) ( 1985 ) 21322136 .",
    "http://dx.doi.org/10.1143/jpsj.54.2132 [ ] .",
    "s.  s. chikatamarla , c.  e. frouzakis , i.  v. karlin , a.  g. tomboulides , k.  b. boulouchos , http://dx.doi.org/10.1017/s0022112010002740[lattice boltzmann method for direct numerical simulation of turbulent flows ] , journal of fluid mechanics 656 298308 .",
    "http://dx.doi.org/10.1017/s0022112010002740 [ ] .",
    "http://dx.doi.org/10.1017/s0022112010002740    b.  keating , g.  vahala , j.  yepez , m.  soe , l.  vahala , http://link.aps.org/doi/10.1103/physreve.75.036712[entropic lattice boltzmann representations required to recover navier - stokes flows ] , phys .",
    "e 75 ( 2007 ) 036712",
    "http://link.aps.org/doi/10.1103/physreve.75.036712    k.  langaas , j.  m. yeomans , lattice boltzmann simulation of a binary fluid with different phase viscosities and its application to fingering in two dimensions , european physical journal b 15 ( 2000 ) 133141 .",
    "http://dx.doi.org/10.1007/s100510051107 [ ] .",
    "j.  chin , e.  boek , p.  v. coveney , http://dx.doi.org/10.1098/rsta.2001.0953[lattice boltzmann simulation of the flow of binary immiscible fluids with different viscosities using the shan - chen microscopic interaction model ] , mathematical , physical and engineering sciences 360  ( 1792 ) ( 2002 ) 547558 .",
    "http://dx.doi.org/10.1098/rsta.2001.0953 [ ] ."
  ],
  "abstract_text": [
    "<S> we present sailfish , an open source fluid simulation package implementing the lattice boltzmann method ( lbm ) on modern graphics processing units ( gpus ) using cuda / opencl . </S>",
    "<S> we take a novel approach to gpu code implementation and use run - time code generation techniques and a high level programming language ( python ) to achieve state of the art performance , while allowing easy experimentation with different lbm models and tuning for various types of hardware . </S>",
    "<S> we discuss the general design principles of the code , scaling to multiple gpus in a distributed environment , as well as the gpu implementation and optimization of many different lbm models , both single component ( bgk , mrt , elbm ) and multicomponent ( shan - chen , free energy ) . </S>",
    "<S> the paper also presents results of performance benchmarks spanning the last three nvidia gpu generations ( tesla , fermi , kepler ) , which we hope will be useful for researchers working with this type of hardware and similar codes .    </S>",
    "<S> lattice boltzmann , lbm , computational fluid dynamics , graphics processing unit , gpu , cuda    * program summary *    _ manuscript title : _ sailfish : a flexible multi - gpu implementation of the lattice boltzmann method + _ authors : _ michal januszewski , marcin kostur + _ program title : _ sailfish + _ code repository : _ https://github.com/sailfish-team/sailfish + _ journal reference : _ </S>",
    "<S> + _ catalogue identifier : _ </S>",
    "<S> + _ licensing provisions : lgplv3 _ + _ programming language : _ python , </S>",
    "<S> cuda c , opencl + _ computer : _ any with an opencl or cuda - compliant gpu + _ operating system : _ no limits ( tested on linux and mac os x ) + _ ram : _ hundreds of megabytes to tens of gigabytes for typical cases . </S>",
    "<S> + _ keywords : _ lattice boltzmann , lbm , cuda , opencl , gpu , computational fluid dynamics , python . </S>",
    "<S> + _ classification : _ 12 , 6.5 + _ external routines / libraries : _ pycuda / pyopencl , numpy , mako , zeromq ( for multi - gpu simulations ) + _ nature of problem : _ </S>",
    "<S> + gpu - accelerated simulation of single- and multi - component fluid flows . </S>",
    "<S> + _ solution method : _ + a wide range of relaxation models ( lbgk , mrt , regularized lb , elbm , shan - chen , free energy , free surface ) and boundary conditions within the lattice boltzmann method framework . </S>",
    "<S> simulations can be run in single or double precision using one or more gpus . </S>",
    "<S> + _ restrictions : _ + the lattice boltzmann method works for low mach number flows only . </S>",
    "<S> + _ unusual features : _ </S>",
    "<S> + the actual numerical calculations run exclusively on gpus . </S>",
    "<S> the numerical code is built dynamically at run - time in cuda c or opencl , using templates and symbolic formulas . </S>",
    "<S> the high - level control of the simulation is maintained by a python process . </S>",
    "<S> + _ running time : _ problem - dependent , typically minutes ( for small cases or short simulations ) to hours ( large cases or long simulations ) + </S>"
  ]
}