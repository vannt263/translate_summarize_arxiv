{
  "article_text": [
    "the computation of the inverse square root of a matrix is a special problem in scientific computing .",
    "it is related to the matrix sign and polar decomposition @xcite .",
    "one may define the _ matrix sign _ by : @xmath0 where @xmath1 is a complex matrix with no pure imaginary eigenvalues .    in polar coordinates , a complex number @xmath2 ,",
    "is represented by @xmath3 in analogy , the polar decomposition of a matrix @xmath1 is defined by : @xmath4 where @xmath5 is the polar part and the second factor corresponds to the absolute value of @xmath1 .",
    "the mathematical literature invloving the matrix sign function traces back to 1971 when it was used to solve the lyapunov and algebraic riccati equations @xcite .    in computational physics",
    "one may face a similar problem when dealing with monte carlo simulations of fermion systems , the so - called _ sign problem _ @xcite .",
    "in this case the integration measure is proportional to the determinant of a matrix and the polar decomposition may be helpful to monitor the sign of the determinant .",
    "the example brought in this paper comes from the recent progress in formulating quantum chromodynamics ( qcd ) on a lattice with exact chiral symmetry @xcite .    in continuum ,",
    "the massless dirac propagator @xmath6 is chirally symmetric , i.e. @xmath7 on a regular lattice with spacing @xmath8 the symmetry is suppressed according to the ginsparg - wilson relation : @xcite : @xmath9 where @xmath10 is the lattice dirac operator .",
    "an explicit example of a dirac operator obeying this relation is the so - called overlap operator @xcite : @xmath11 where @xmath12 is a shift parameter in the range @xmath13 , which i have fixed at one .",
    "@xmath14 is the wilson operator , @xmath15 which is a nearest - neighbors discretization of the continuum dirac operator ( it violates the ginsparg - wilson relation ) . @xmath16 and",
    "@xmath17 are first and second order covariant differences given by :    @xmath18    @xmath19    where @xmath20 is a fermion field at the lattice site @xmath21 and @xmath22 an @xmath23 lattice gauge filed associated with the oriented link @xmath24 .",
    "these are unitary 3 by 3 complex matrices with determinant one .",
    "a set of such matrices forms a lattice gauge `` configuration '' .",
    "@xmath25 are @xmath26 dirac matrices which anticommute with each - other .",
    "therefore , if there are @xmath27 lattice points in four dimensions , the matrix @xmath1 is of order @xmath28 .",
    "a restive symmetry of the matrix @xmath1 that comes from the continuum is the so called @xmath29 which is the hermiticity of the @xmath30 operator .    by definition , computation of @xmath10",
    "involves the inverse square root of a matrix .",
    "this is a non - trivial task for large matrices .",
    "therefore several algorithms have been proposed by lattice qcd physicists @xcite .",
    "all these methods rely on matrix - vector multiplications with the sparse wilson matrix @xmath14 , being therefore feasible for large simulations .",
    "in fact , methods that approximate the inverse square root by legendre @xcite and chebyshev polynomials @xcite need to know _",
    "apriori _ the extreme eigenvalues of @xmath31 to be effective .",
    "this requires computational resources of at least one conjugate gradients ( cg ) inversion .    in @xcite",
    "the inverse square root is approximated by a rational approximation , which allows an efficient computation via a multi - shift cg iteration .",
    "storage here may be an obstacle which is remedied by a second cg step @xcite .",
    "the pade approximation used by @xcite needs the knowledge of the smallest eigenvalue of @xmath31 .",
    "therefore the method becomes effective only in connection with the @xmath10 inversion @xcite .",
    "the method presented earlier by the author @xcite relies on taking exactly the inverse square root from the ritz values .",
    "these are the roots of the lanczos polynomial approximating the inverse of @xmath31 .    in that work",
    "the lanczos polynomial was constructed by applying the hermitian operator @xmath30 .",
    "the latter is indefinite , thereby responsible for observed oscillations in the residual vector norm @xcite .    here",
    "i use a lanczos polynomial on the positive definite matrix @xmath31 . in this case",
    "the residual vector norm decreases monotonically and leads to a stable method .",
    "this is a crucial property that allows a reliable stopping criterion that i will present here .",
    "the paper is selfcontained : in the next section i will briefly present the lanczos algorithm and set the notations . in section 3 ,",
    "i use the algorithm to solve linear systems , and in section 4 , the computation of the inverse square root is given .",
    "the method is tested in section 5 and conclusions are drawn in the end .",
    "the lanczos iteration is known to approximate the spectrum of the underlying matrix in an optimal way and , in particular , it can be used to solve linear systems @xcite .",
    "let @xmath32 $ ] be the set of orthonormal vectors , such that @xmath33 where @xmath34 is a tridiagonal and symmetric matrix , @xmath35 is an arbitrary vector , and @xmath36 a real and positive constant .",
    "@xmath37 denotes the unit vector with @xmath38 elements in the direction @xmath39 .    by writing down the above decomposition in terms of the vectors @xmath40 and",
    "the matrix elements of @xmath34 , i arrive at a three term recurrence that allows to compute these vectors in increasing order , starting from the vector @xmath41 .",
    "this is the @xmath42 : @xmath43 where @xmath44 is a tolerance which serves as a stopping condition .",
    "the lanczos algorithm constructs a basis for the krylov subspace @xcite : @xmath45 if the algorithm stops after @xmath38 steps , one says that the associated krylov subspace is invariant .    in the floating point arithmetic",
    ", there is a danger that once the lanczos algorithm ( polynomial ) has approximated well some part of the spectrum , the iteration reproduces vectors which are rich in that direction @xcite . as a consequence ,",
    "the orthogonality of the lanczos vectors is spoiled with an immediate impact on the history of the iteration : if the algorithm would stop after @xmath38 steps in exact arithmetic , in the presence of round off errors the loss of orthogonality would keep the algorithm going on .",
    "here i will use this algorithm to solve linear systems , where the loss of orthogonality will not play a role in the sense that i will use a different stopping condition .",
    "i ask the solution in the form @xmath47 by projecting the original system on to the krylov subspace i get : @xmath48 by construction , i have @xmath49 substituting @xmath50 and using ( [ hq_qt ] ) , my task is now to solve the system @xmath51 therefore the solution is given by @xmath52 this way using the lanczos iteration one reduces the size of the matrix to be inverted .",
    "moreover , since @xmath34 is tridiagonal , one can compute @xmath53 by short recurences .",
    "if i define : @xmath54 where @xmath55 , it is easy to show that @xmath56 therefore the solution can be updated recursively and i have the following _",
    "algorithm1 for solving the system @xmath46 : _ @xmath57",
    "now i would like to compute @xmath59 and still use the lanczos algorithm . in order to do so",
    "i make the following observations :    let @xmath60 be expressed by a matrix - valued function , for example the integral formula @xcite : @xmath61 from the previous section , i use the lanczos algorithm to compute @xmath62 it is easy to show that the lanczos algorithm is shift - invariant .",
    "i.e. if the matrix @xmath31 is shifted by a constant say , @xmath63 , the lanczos vectors remain invariant .",
    "moreover , the corresponding lanczos matrix is shifted by the same amount .",
    "this property allows one to solve the system @xmath64 by using the same lanczos iteration as before .",
    "since the matrix @xmath65 is better conditioned than @xmath31 , it can be concluded that once the original system is solved , the shifted one is solved too .",
    "therefore i have : @xmath66 using the above integral formula and putting everything together , i get : @xmath67 there are some remarks to be made here :    \\a ) as before , by applying the lanczos iteration on @xmath31 , the problem of computing @xmath68 reduces to the problem of computing @xmath69 which is typically a much smaller problem than the original one .",
    "but since @xmath70 is full , @xmath53 can not be computed by short recurences .",
    "it can be computed for example by using the full decomposition of @xmath34 in its eigenvalues and eigenvectors ; in fact this is the method i have employed too , for its compactness and the small overhead for moderate @xmath38 .",
    "\\b ) the method is not optimal , as it would have been , if one would have applied it directly for the matrix @xmath71 . by using @xmath31",
    "the condition is squared , and one looses a factor of two compared to the theoretical case !",
    "\\c ) from the derivation above , it can be concluded that the system @xmath58 is solved at the same time as the system @xmath46 .",
    "\\d ) to implement the result ( [ result ] ) , i first construct the lanczos matrix and then compute @xmath72 to compute @xmath50 , i repeat the lanczos iteration .",
    "i save the scalar products , though it is not necessary .    therefore i have the following _",
    "algorithm2 for solving the system @xmath58 : _",
    "@xmath73 where by @xmath74 i denote a vector with zero entries and @xmath75 the matrices of the eigenvectors and eigenvalues of @xmath34 .",
    "note that there are only four large vectors necessary to store : @xmath76 .",
    "i propose a simple test : i solve the system @xmath46 by applying twice the @xmath77 , i.e. i solve the linear systems @xmath78 in the above order . for each approximation @xmath79 , i compute the residual vector @xmath80 the method is tested for a su(3 ) configuration at @xmath81 on a @xmath82 lattice , corresponding to an order @xmath83 complex matrix @xmath1 .    in fig.1",
    "i show the norm of the residual vector decreasing monotonically .",
    "the stagnation of @xmath84 for small values of @xmath44 may come from the accumulation of round off error in the @xmath85-bit precision arithmetic used here .",
    "this example shows that the tolerance line is above the residual norm line , which confirms the expectation that @xmath44 is a good stopping condition of the @xmath77 .",
    "i have presented a lanczos method to compute the inverse square root of a large and sparse positive definite matrix .",
    "the method is characterized by a residual vector norm that decreases monotonically and a consistent stopping condition .",
    "this stability should be compared with a similar method presented earlier by the author @xcite , where the underlying hermitian but indefinite matrix @xmath30 led to appreciable instabilities in the norm of the residual vector .    in terms of complexity",
    "this algorithm requires less operations for the same accuracy than its indefinite matrix counterpart .",
    "this property is guaranteed by the monotonicity of the residual vector norm .",
    "nontheless , the bulk of the work remains the same .",
    "it shares with methods presented in @xcite the same underlying lanczos polynomial . as it is wellknown @xcite cg and",
    "lanczos methods for solving a linear system produce the same results in exact arithmetic .",
    "in fact , cg derives from the lanczos algorithm by solving the coupled two - term recurences of cg for a single three - term recurence of lanczos .",
    "however , the coupled two - term recurences of cg accumulate less round off .",
    "this makes cg preferable for ill - conditioned problems .",
    "\\a ) since cg and lanczos are equivalent , they produce the same lanczos matrix .",
    "therefore , any function of @xmath31 translates for both algorithms into a function of @xmath34 ( given the basis of lanczos vectors ) .",
    "the latter function translates into a function of the ritz values , the eigenvalues of @xmath34 .",
    "that is , whenever the methods of papers @xcite try to approximate the inverse square root of @xmath31 , the underlying cg algorithm shifts this function to the ritz values .",
    "it is clear now that if i take the inverse square root from the ritz values exactly , i do nt have any approximation error .",
    "this is done in @xmath77 .",
    "\\b ) @xmath77 sets no limits in the amount of memory required , whereas the multi - shift cg needs to store as many vectors as the number of shifts . for high accuracy approximations",
    "the multi - shift cg is not practical .",
    "however , one may lift this limit in expense of a second cg iteration ( two - step cg ) @xcite .",
    "therefore @xmath77 and the two - step cg have the same iteration workload , with @xmath77 computing exactly the inverse square root .    additionally , @xmath77 requires the calculation of ritz eigenpairs of @xmath34 , which makes for an overhead proportional to @xmath86 when using the qr algorithm for the eigenvalues and the inverse iteration for the eigenvectors @xcite .",
    "since the complexity of the lanczos algorithm is @xmath87 , the relative overhead is proportional to @xmath88 . for moderate gauge couplings and lattice sizes",
    "this is a small percentage ."
  ],
  "abstract_text": [
    "<S> i construct a lanczos process on a large and sparse matrix and use the results of this iteration to compute the inverse square root of the same matrix . </S>",
    "<S> the algorithm is a stable version of an earlier proposal by the author . </S>",
    "<S> it can be used for problems related to the matrix sign and polar decomposition . </S>",
    "<S> the application here comes from the theory of chiral fermions on the lattice . </S>"
  ]
}