{
  "article_text": [
    "in recent years , adaptive beamforming techniques have attracted considerable interest and found applications in radar , wireless communications and sonar @xcite .",
    "the adaptive beamforming techniques are used in systems equipped with antenna arrays and usually have a trade - off between performance and computational complexity which depends on the designer s choice of the adaptation algorithm @xcite . the optimal linearly constrained   minimum   variance ( lcmv )",
    "beamformer is designed in such a way that it attempts to minimize the array output power while maintaining a constant response in the direction of a signal of interest ( soi ) @xcite . however",
    ", this technique requires the computation of the inverse of the input data covariance matrix and the knowledge of the cross - correlation vector , rendering the method very complex for practical applications when the system is large .",
    "adaptive versions of the lcmv beamformer were subsequently reported with stochastic gradient ( sg ) @xcite and recursive least squares ( rls ) @xcite algorithms .",
    "these algorithms require estimates of the input data covariance matrix , which is a task that may become challenging in large systems and in highly dynamic situations such as those found in wireless communications and radar applications .",
    "this is because the convergence speed and tracking properties of adaptive filters depend on the number of sensor elements @xmath0 @xcite and on the eigenvalue spread of the input data covariance matrix .",
    "given this dependency on the number of sensor elements @xmath0 , it is thus intuitive to reduce @xmath0 while simultaneously extracting the key features of the original signal via an appropriate transformation .",
    "a cost - effective technique in short - data record scenarios and , in particular , with systems containing a large number of parameters is reduced - rank signal processing .",
    "the advantages are their superior convergence properties and enhanced tracking performance when compared with full - rank schemes operating with a large number of parameters , and their ability to exploit the low - rank nature of the signals encountered in beamforming applications .",
    "several reduced - rank methods have been proposed to generate the signal subspace @xcite-@xcite .",
    "they range from computationally expensive eigen - decomposition techniques @xcite-@xcite to alternative approaches such as the auxiliary - vector filter ( avf ) @xcite,@xcite , @xcite , the multistage wiener filter ( mswf ) @xcite , @xcite , @xcite , @xcite which are based on the krylov subspace , and joint optimization approaches @xcite . despite the improved convergence and tracking performance achieved with krylov methods @xcite-@xcite ,",
    "@xcite-@xcite they are relatively complex to implement and can suffer from numerical problems .",
    "the joint optimization techniques reported in @xcite outperform the eigen - decomposition- and krylov - based methods and are amenable to efficient adaptive implementations .",
    "however , the design and analysis of adaptive lcmv reduced - rank algorithms based on joint optimization approaches have not been considered so far .",
    "this work proposes lcmv reduced - rank algorithms based on constrained joint iterative optimization of filters for antenna - array beamforming . the proposed scheme , whose initial results were reported in @xcite , jointly optimizes a projection matrix and a reduced - rank filter that operates at the output of the projection matrix . the essence of the proposed approach is to change the role of adaptive lcmv filters .",
    "the bank of adaptive filters is responsible for performing dimensionality reduction , whereas the reduced - rank filter effectively forms the beam in the direction of the soi .",
    "we describe lcmv expressions for the design of the projection matrix and the reduced - rank filter and present sg and rls algorithms for efficiently implementing the method .",
    "we also introduce an automatic rank estimation algorithm for determining the most adequate rank for the proposed algorithms .",
    "an analysis of the stability and the convergence properties of the proposed algorithms is presented and semi - analytical expressions are derived for predicting their performance .",
    "this paper is organized as follows .",
    "the system model is described in section ii .",
    "the full - rank and the reduced - rank lcmv filtering problems are formulated in section iii .",
    "section iv is dedicated to the proposed method , whereas section v is devoted to the derivation of the adaptive sg and rls algorithms and the rank adaptation technique .",
    "section vi focuses on the analysis of the proposed algorithms .",
    "section vii presents and discusses the simulation results and section viii gives the concluding remarks .",
    "let us consider a smart antenna system equipped with a uniform linear array ( ula ) of @xmath0 elements , as shown in fig .",
    "1 . assuming that the sources are in the far field of the array , the signals of @xmath1 narrowband sources impinge on the array @xmath2 with unknown directions of arrival ( doa ) @xmath3 for @xmath4 .",
    "the input data from the antenna array can be organized in an @xmath5 vector expressed by @xmath6 where @xmath7\\ ] ] is the @xmath8 matrix of signal steering vectors .",
    "the @xmath9 signal steering vector is defined as @xmath10 ^t\\ ] ] for a signal impinging at angle @xmath11 , @xmath12 , where @xmath13 is the inter - element spacing , @xmath14 is the wavelength and @xmath15 denotes the transpose operation .",
    "the vector @xmath16 denotes the complex vector of sensor noise , which is assumed to be zero - mean and gaussian with covariance matrix @xmath17 .",
    "in this section , we formulate the problems of full - rank and reduced - rank lcmv filters . in order to perform beamforming with a full - rank lcmv filter ,",
    "we linearly combine the data vector @xmath18^{t}$ ] with the full - rank filter @xmath19^t$ ] to yield @xmath20    the optimal lcmv filter is the @xmath9 vector @xmath21 , which is designed to solve the following optimization problem @xmath22 & = { \\boldsymbol w}^{h } { \\boldsymbol r } { \\boldsymbol w}\\\\ { \\rm subject~ to } ~ { \\boldsymbol w}^h{\\boldsymbol a}(\\theta_k)~ & = 1 \\label{flcmv } \\end{split}\\ ] ] the solution to the problem in ( [ flcmv ] ) is given by @xcite @xmath23 where @xmath24 is the steering vector of the soi , @xmath25 is the received data , the covariance matrix of @xmath25 is described by @xmath26 $ ] , @xmath27 denotes hermitian transpose and @xmath28 $ ] stands for expected value .",
    "the filter @xmath29 can be estimated via sg or rls algorithms @xcite .",
    "however , the laws that govern their convergence and tracking behaviors imply that they depend on @xmath0 and on the eigenvalue spread of @xmath30 .",
    "a reduced - rank algorithm must extract the most important features of the processed data by performing dimensionality reduction .",
    "this mapping is carried out by a @xmath31 projection matrix @xmath32 on the received data as given by @xmath33 where , in what follows , all @xmath34-dimensional quantities are denoted with a `` bar '' .",
    "the resulting projected received vector @xmath35 is the input to a filter represented by the @xmath36 vector @xmath37^t$ ] .",
    "the filter output is @xmath38 in order to design the reduced - rank filter @xmath39 we consider the following optimization problem @xmath40 & = \\bar{\\boldsymbol w}^{h } \\bar{\\boldsymbol r } \\bar{\\boldsymbol w } \\\\ { \\textrm { subject to } }   ~ \\bar{\\boldsymbol w}^h\\bar{\\boldsymbol",
    "a}(\\theta_k ) & = 1 \\end{split}\\ ] ] the solution to the above problem is @xmath41 where the reduced - rank covariance matrix is @xmath42={\\boldsymbol s}_d^h{\\boldsymbol r}{\\boldsymbol s}_d$ ] and the reduced - rank steering vector is @xmath43 .",
    "the associated minimum variance ( mv ) for a lcmv filter with rank @xmath34 is @xmath44 the above development shows that the main problem is how to cost - effectively design @xmath45 to perform dimensionality reduction on @xmath25 , resulting in improved convergence and tracking performance over the full - rank filter . in the appendix",
    ", we provide a necessary and sufficient condition for @xmath46 to preserve the mv of optimal full - rank filter and discuss the existence of multiple solutions . in the following",
    ", we detail our proposed reduced - rank method .",
    "in this section , we introduce the principles of the proposed reduced - rank scheme . the proposed scheme , depicted in fig . 2 , employs a matrix @xmath47 with dimensions @xmath48 to perform dimensionality reduction on a data vector @xmath25 with dimensions @xmath9 .",
    "the reduced - rank filter @xmath49 with dimensions @xmath50 processes the reduced - rank data vector @xmath51 in order to yield a scalar estimate @xmath52 .",
    "the projection matrix @xmath53 and the reduced - rank filter @xmath49 are jointly optimized in the proposed scheme according to the mv criterion subject to a constraint that ensures that the reduced - rank array response is equal to unity in the direction of the soi .    in order to describe the proposed method ,",
    "let us first consider the structure of the @xmath31 projection matrix @xmath54\\ ] ] where the columns @xmath55 for @xmath56 constitute a bank of @xmath34 full - rank filters with dimensions @xmath5 as given by @xmath57^t \\nonumber\\ ] ] the output @xmath52 of the proposed reduced - rank scheme can be expressed as a function of the input vector @xmath58 , the projection matrix @xmath59 and the reduced - rank filter @xmath60 : @xmath61 it is interesting to note that for @xmath62 , the proposed scheme becomes a conventional full - rank lcmv filtering scheme with an addition weight parameter @xmath63 that provides an amplitude gain . for @xmath64 ,",
    "the signal processing tasks are changed and the full - rank lcmv filters compute a subspace projection and the reduced - rank filter provides a unity gain in the direction of the soi .",
    "this rationale is fundamental to the exploitation of the low - rank nature of signals in typical beamforming scenarios .",
    "the lcmv expressions for the filters @xmath59 and @xmath65 can be computed via the proposed optimization problem @xmath66 & = \\bar{\\boldsymbol w}^{h}(i ) { \\boldsymbol s}_d^h(i ) { \\boldsymbol r } { \\boldsymbol s}_d(i ) \\bar{\\boldsymbol w}(i ) \\\\ { \\textrm { subject to } }   ~ \\bar{\\boldsymbol w}^h(i){\\boldsymbol",
    "s}_d^h(i ) { \\boldsymbol a}(\\theta_k ) & = 1 \\label{propt } \\end{split}\\ ] ] in order to solve the above problem , we resort to the method of lagrange multipliers @xcite and transform the constrained optimization into an unconstrained one expressed by the lagrangian @xmath67 + 2\\re [ \\lambda ( \\bar{\\boldsymbol w}^h(i){\\bf s}_d^h(i){\\boldsymbol a}(\\theta_k)-1 ) ] , \\label{uopt } \\end{split}\\ ] ] where @xmath68 is a scalar lagrange multiplier , @xmath69 denotes complex conjugate and the operator @xmath70 $ ] selects the real part of the argument . by fixing @xmath49 , minimizing ( [ uopt ] ) with respect to @xmath71 and solving for @xmath68",
    ", we get @xmath72 where @xmath73 $ ] and @xmath74 $ ] . by fixing @xmath71 , minimizing ( [ uopt ] ) with respect to @xmath49 and solving for @xmath68",
    ", we arrive at the expression @xmath75 where @xmath76 = e[\\bar{\\boldsymbol r}(i ) \\bar{\\boldsymbol r}^{h}(i)]$ ] , @xmath77 .",
    "the associated mv is @xmath78 note that the filter expressions in ( [ filts ] ) and ( [ filtw ] ) are not closed - form solutions for @xmath79 and @xmath71 since ( [ filts ] ) is a function of @xmath49 and ( [ filtw ] ) depends on @xmath71 .",
    "thus , it is necessary to iterate ( [ filts ] ) and ( [ filtw ] ) with initial values to obtain a solution .",
    "an analysis of the optimization problem in ( [ propt ] ) is given in appendix ii .",
    "unlike existing approaches based on the mswf @xcite and the avf @xcite methods , the proposed scheme provides an iterative exchange of information between the reduced - rank filter and the projection matrix and leads to a much simpler adaptive implementation .",
    "the projection matrix reduces the dimension of the input data , whereas the reduced - rank filter yields a unity response in the direction of the soi .",
    "the key strategy lies in the joint optimization of the filters .",
    "the rank @xmath34 must be set by the designer to ensure appropriate performance or can be estimated via another algorithm . in the next section ,",
    "we seek iterative solutions via adaptive algorithms for the design of @xmath71 and @xmath49 , and automatic rank adaptation algorithms .",
    "in this section we present adaptive sg and rls versions of the proposed scheme for efficient implementation . we also consider the important issue of automatically determining the rank of the scheme via the proposal of an adaptation technique .",
    "we then provide the computational complexity in arithmetic operations of the proposed reduced - rank algorithms .",
    "in this part , we present a low - complexity sg adaptive reduced - rank algorithm for efficient implementation of the proposed method .",
    "these algorithms were reported in @xcite and are reproduced here for convenience . by computing the instantaneous gradient terms of ( [ uopt ] ) with respect to @xmath80 and @xmath81 , we get @xmath82 by introducing the positive step sizes @xmath83 and @xmath84 , using the gradient rules @xmath85 and @xmath86 , enforcing the constraint and solving the resulting equations , we obtain @xmath87 , \\label{recsd}\\ ] ] @xmath88 \\bar{\\boldsymbol r}(i)\\label{recw},\\ ] ] where @xmath89 . the proposed scheme trades - off a full - rank filter against one projection matrix @xmath71 and one reduced - rank adaptive filter @xmath49 operating simultaneously and exchanging information .",
    "here we derive an rls adaptive reduced - rank algorithm for efficient implementation of the proposed method . to this end , let us first consider the lagrangian @xmath90 \\end{split}\\ ] ] where @xmath91 is the forgetting factor chosen as a positive constant close to , but less than @xmath92 .",
    "fixing @xmath49 , computing the gradient of ( [ costls ] ) with respect to @xmath93 , equating the gradient to a null vector and solving for @xmath68 , we obtain @xmath94 where @xmath95 is the input covariance matrix , and @xmath96 is the reduced - rank weight matrix at time instant @xmath97 .",
    "the computation of ( [ filtsd ] ) includes the inversion of @xmath98 and @xmath99 , which may increase significantly the complexity and create numerical problems .",
    "however , the expression in ( [ filtsd ] ) can be further simplified using the constraint @xmath100 .",
    "the details of the derivation of the proposed rls algorithms and the simplification are given in appendix iii .",
    "the simplified expression for @xmath71 is given by @xmath101 where we defined the inverse covariance matrix @xmath102 for convenience of presentation .",
    "employing the matrix inversion lemma @xcite , we obtain @xmath103 @xmath104 where @xmath105 is the @xmath9 kalman gain vector .",
    "we set @xmath106 to start the recursion of ( [ 20 ] ) , where @xmath107 is a positive constant and @xmath108 is an @xmath109 identity matrix .",
    "assuming @xmath93 is known and taking the gradient of ( [ costls ] ) with respect to @xmath49 , equating the terms to a null vector and solving for @xmath68 , we obtain the @xmath110 reduced - rank filter @xmath111 where @xmath112 and @xmath113 is the reduced - rank input covariance matrix . in order to estimate @xmath114",
    ", we use the matrix inversion lemma @xcite as follows @xmath115 @xmath116 where @xmath117 is the @xmath110 reduced - rank gain vector and @xmath118 is referred to as the reduced - rank inverse covariance matrix .",
    "hence , the covariance matrix inversion @xmath119 is replaced at each step by the recursive processes ( [ 15 ] ) and ( [ 16 ] ) for reducing the complexity .",
    "the recursion of ( [ 16 ] ) is initialized by choosing @xmath120 , where @xmath121 is a positive constant and @xmath122 is a @xmath123 identity matrix .",
    "the proposed rls algorithm trade - off a full - rank filter with @xmath0 coefficients against one projection matrix @xmath71 , given in ( [ 18])-([20 ] ) and one @xmath124 reduced - rank adaptive filter @xmath49 , given in ( [ 13])-([16 ] ) , operating simultaneously and exchanging information .      here",
    ", we evaluate the computational complexity of the proposed and analyzed lcmv algorithms . the complexity expressed in terms of additions and multiplications",
    "is depicted in table i. we can verify that the proposed reduced - rank sg algorithm has a complexity that grows linearly with @xmath125 , which is about @xmath34 times higher than the full - rank sg algorithm and significantly lower than the mswf - sg @xcite . if @xmath126 ( as we will see later ) then the additional complexity can be acceptable provided the gains in performance justify them . in the case of the proposed reduced - rank rls algorithm",
    "the complexity is quadratic with @xmath127 and @xmath128 .",
    "this corresponds to a complexity slightly higher than the one observed for the full - rank rls algorithm , provided @xmath34 is significantly smaller than @xmath0 , and comparable to the cost of the mswf - rls @xcite and the avf @xcite .    in order to illustrate the main trends in what concerns the complexity of the proposed and analyzed algorithms , we show in fig .",
    "3 the complexity in terms of additions and multiplications versus the number of input samples @xmath0 .",
    "the curves indicate that the proposed reduced - rank rls algorithm has a complexity lower than the mswf - rls algorithm @xcite and the avf @xcite , whereas it remains at the same level of the full - rank rls algorithm .",
    "the proposed reduced - rank sg algorithm has a complexity that is situated between the full - rank rls and the full - rank sg algorithms .",
    "the performance of the algorithms described in the previous subsections depends on the rank @xmath34 .",
    "this motivates the development of methods to automatically adjust @xmath34 on the basis of the cost function .",
    "unlike prior methods for rank selection which utilize mswf - based algorithms @xcite or avf - based recursions @xcite , we focus on an approach that jointly determines @xmath34 based on the ls criterion computed by the filters @xmath71 and @xmath129 , where the subscript @xmath34 denotes the rank used for the adaptation . in particular , we present a method for automatically selecting the ranks of the algorithms based on the exponentially weighted _ a posteriori _ least - squares type cost function described by @xmath130 where @xmath91 is the forgetting factor and @xmath131 is the reduced - rank filter with rank @xmath34 . for each time interval @xmath97",
    ", we can select the rank @xmath132 which minimizes @xmath133 and the exponential weighting factor @xmath91 is required as the optimal rank varies as a function of the data record . the key quantities to be updated are the projection matrix @xmath71 , the reduced - rank filter @xmath134 , the associated reduced - rank steering vector @xmath135 and the inverse of the reduced - rank covariance matrix @xmath114 ( for the proposed rls algorithm ) . to this end , we define the following extended projection matrix @xmath136 and the extended reduced - rank filter weight vector @xmath137 as follows : @xmath138 ~~{\\rm and } ~~ \\bar{\\boldsymbol w}_{d}(i ) = \\left[\\begin{array}{c } w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_{d_{\\rm min } } \\\\ \\vdots \\\\ w_{d_{\\rm max } } \\end{array}\\right]\\ ] ] the extended projection matrix @xmath136 and the extended reduced - rank filter weight vector @xmath137 are updated along with the associated quantities @xmath135 and @xmath114 ( only for the rls ) for the maximum allowed rank @xmath139 and then the proposed rank adaptation algorithm determines the rank that is best for each time instant @xmath97 using the cost function in ( [ eq : costadap ] ) . the proposed rank adaptation algorithm",
    "is then given by @xmath140 where @xmath141 is an integer , @xmath142 and @xmath139 are the minimum and maximum ranks allowed for the reduced - rank filter , respectively .",
    "note that a smaller rank may provide faster adaptation during the initial stages of the estimation procedure and a greater rank usually yields a better steady - state performance .",
    "our studies reveal that the range for which the rank @xmath34 of the proposed algorithms have a positive impact on the performance of the algorithms is limited , being from @xmath143 to @xmath144 for the reduced - rank filter recursions .",
    "these values are rather insensitive to the system load ( number of users ) , to the number of array elements and work very well for all scenarios and algorithms examined .",
    "the additional complexity of the proposed rank adaptation algorithm is that it requires the update of all involved quantities with the maximum allowed rank @xmath139 and the computation of the cost function in ( [ eq : costadap ] ) .",
    "this procedure can significantly improve the convergence performance and can be relaxed ( the rank can be made fixed ) once the algorithm reaches steady state . choosing an inadequate rank for",
    "adaptation may lead to performance degradation , which gradually increases as the adaptation rank deviates from the optimal rank .",
    "a mechanism for automatically adjusting @xmath142 and @xmath139 based on a figure of merit and the processed data would be an important technique to be investigated .",
    "for example , this mechanism could in principle adjust @xmath142 and @xmath139 in order to address the needs of the model and the performance requirements .",
    "this remains a topic for future investigation .",
    "one can also argue that the proposed rank adaptation may not be universally applied to signal processing problems , even though it has been proven highly effective to the problems we dealt with .",
    "another possibility for rank adaptation is the use of the cross - validation ( cv ) method reported in @xcite .",
    "this approach selects the lengths of the filters that minimize a cost function that is estimated on the basis of data that have not been used in the process of building the filters themselves .",
    "this approach based on the concept of `` leave one out '' can be used to determine the rank without requiring any prior knowledge or the setting of a range of values @xcite .",
    "a drawback of this method is that it may significantly increase the length of the filters , resulting in higher complexity .",
    "other possible approaches for rank selection may rely on some prior knowledge about the environment and the system for inferring the required rank for operation .",
    "the development of cost - effective methods for rank selection remains an interesting area for investigation .",
    "in this section , we present the stability and the mse convergence analyses of the proposed sg algorithms . specifically",
    ", we consider the joint optimization approach and derive conditions of stability for the proposed sg algorithms .",
    "we then assume that the algorithms will converge and carry out the mse convergence analysis in order to semi - analytically determine the mse upon convergence .",
    "the rls algorithms are expected to converge to the optimal lcmv filter and this has been verified in our studies . a discussion on the preservation of the mv performance , the existence of multiple solutions and an analysis of the optimization of the proposed scheme valid for both sg and rls algorithms is included in the appendices i and ii .",
    "in order to establish conditions for the stability of the proposed sg algorithms , we define the error matrices at time @xmath97 as @xmath145 and @xmath146 where @xmath147 and @xmath148 are the optimal parameter estimators .",
    "since we are dealing with a joint optimization procedure , both filters have to be considered jointly . by substituting the expressions of @xmath149 and @xmath150 in ( [ recsd ] ) and ( [ recw ] ) , respectively , and rearranging the terms we obtain @xmath151 { \\boldsymbol r}(i ) { \\boldsymbol r}^h(i ) \\big\\ } { \\boldsymbol e}_{{\\boldsymbol s}_d}(i ) \\\\ & \\quad - \\mu_s [ { \\boldsymbol i } - ( { \\boldsymbol a}^h(\\theta_k){\\boldsymbol a}(\\theta_k ) ) ^{-1}{\\boldsymbol a}(\\theta_k ) { \\boldsymbol a}^h ( \\theta_k ) ] { \\boldsymbol r}(i ) \\bar{\\boldsymbol w}^h(i ) { \\boldsymbol r}^h(i){\\boldsymbol s}_d(i ) { \\boldsymbol e}_{\\bar{\\boldsymbol w}}(i ) \\\\ & \\quad + \\mu_s [ { \\boldsymbol i } - ( { \\boldsymbol a}^h(\\theta_k){\\boldsymbol a}(\\theta_k ) ) ^{-1}{\\boldsymbol a}(\\theta_k ) { \\boldsymbol a}^h ( \\theta_k ) ] { \\boldsymbol r}(i ) { \\boldsymbol r}^h(i ) [ { \\boldsymbol s}_d(i)({\\bf i } - \\bar{\\boldsymbol w}_{\\rm opt } \\bar{\\boldsymbol w}^h(i ) ) - { \\boldsymbol s}_{d,{\\rm opt } } ] \\label{esd } \\end{split}\\ ] ] @xmath152 \\bar{\\boldsymbol r}(i ) \\bar{\\boldsymbol r}^h(i ) \\big\\ } { \\boldsymbol e}_{{\\boldsymbol w}}(i ) \\\\ &   \\quad - \\mu_w [ { \\boldsymbol i } - ( \\bar{\\boldsymbol a}^h(\\theta_k ) \\bar{\\boldsymbol a}(\\theta_k))^{-1}\\bar{\\boldsymbol",
    "a}(\\theta_k ) \\bar{\\boldsymbol a}^h(\\theta_k ) ] \\bar{\\boldsymbol r}(i ) { \\boldsymbol r}^h(i )   { \\boldsymbol e}_{{\\boldsymbol s}_d}(i ) \\\\ &   \\quad + \\mu_w [ { \\boldsymbol i } - ( \\bar{\\boldsymbol a}^h(\\theta_k ) \\bar{\\boldsymbol a}(\\theta_k))^{-1}\\bar{\\boldsymbol a}(\\theta_k ) \\bar{\\boldsymbol a}^h(\\theta_k)){\\boldsymbol s}_{d}^h(i ) ] \\bar{\\boldsymbol r}(i ) \\bar{\\boldsymbol r}^h(i ) ( { \\boldsymbol s}_{d}(i ) ( { \\boldsymbol i}- \\bar{\\boldsymbol w}_{\\rm opt } ) - { \\boldsymbol s}_{d,{\\rm opt } } ) \\label{ew } \\end{split}\\ ] ]    taking expectations and simplifying the terms , we obtain @xmath153 \\\\",
    "e[{\\boldsymbol e}_{\\bar{\\boldsymbol w}}(i+1 ) ] \\end{array}\\right ] & = { \\boldsymbol p } \\left[\\begin{array}{c }    e[{\\boldsymbol e}_{{\\boldsymbol s}_d}(i ) ] \\\\",
    "e[{\\boldsymbol e}_{\\bar{\\boldsymbol w}}(i ) ] \\end{array}\\right ] + { \\boldsymbol t } \\\\   \\end{split}\\ ] ] where @xmath154 { \\boldsymbol r}(i ) { \\boldsymbol r}^h(i ) \\big\\ } } & { \\small - \\mu_s [ { \\boldsymbol i } - { \\boldsymbol a}(\\theta_k ) { \\boldsymbol a}^h ( \\theta_k ) ] { \\boldsymbol r}(i ) \\bar{\\boldsymbol w}^h(i ) { \\boldsymbol r}^h(i){\\boldsymbol",
    "s}_d(i ) } \\\\    { \\small - \\mu_w [ { \\boldsymbol i } - ( \\bar{\\boldsymbol a}^h(\\theta_k ) \\bar{\\boldsymbol a}(\\theta_k))^{-1}\\bar{\\boldsymbol a}(\\theta_k ) \\bar{\\boldsymbol a}^h(\\theta_k ) ] \\bar{\\boldsymbol r}(i ) { \\boldsymbol r}^h(i ) }   & { \\small \\big\\ { { \\boldsymbol i } - \\mu_w [ { \\boldsymbol i } - ( \\bar{\\boldsymbol a}^h(\\theta_k ) \\bar{\\boldsymbol a}(\\theta_k))^{-1}\\bar{\\boldsymbol a}(\\theta_k ) \\bar{\\boldsymbol a}^h(\\theta_k ) ] \\bar{\\boldsymbol r}(i ) \\bar{\\boldsymbol r}^h(i ) \\big\\ } } \\end{array}\\right],\\nonumber\\ ] ] @xmath155 { \\boldsymbol r}(i ) { \\boldsymbol r}^h(i ) [ { \\boldsymbol s}_d(i)({\\bf i } - \\bar{\\boldsymbol w}_{\\rm opt } \\bar{\\boldsymbol w}^h(i ) ) - { \\boldsymbol s}_{d,{\\rm opt } } ] } \\\\ { \\small   \\mu_w [ { \\boldsymbol i } - ( \\bar{\\boldsymbol a}^h(\\theta_k ) \\bar{\\boldsymbol a}(\\theta_k))^{-1}\\bar{\\boldsymbol a}(\\theta_k ) \\bar{\\boldsymbol a}^h(\\theta_k)){\\boldsymbol s}_{d}^h(i ) ] \\bar{\\boldsymbol r}(i ) \\bar{\\boldsymbol r}^h(i ) ( { \\boldsymbol s}_{d}(i ) ( { \\boldsymbol i}- \\bar{\\boldsymbol w}_{\\rm opt } ) - { \\boldsymbol s}_{d,{\\rm opt } } ) } \\end{array}\\right].\\nonumber\\ ] ]",
    "the previous equations imply that the stability of the algorithms depends on the spectral radius of @xmath156 . for convergence",
    ", the step sizes should be chosen such the eigenvalues of @xmath157 are less than one .",
    "unlike the stability analysis of most adaptive algorithms @xcite , in the proposed approach the terms are more involved and depend on each other as evidenced by the equations in @xmath156 and @xmath158 .",
    "let us consider in this part an analysis of the mse in steady state .",
    "this follows the general steps of the mse convergence analysis of @xcite even though novel elements will be introduced in the proposed framework .",
    "these novel elements in the analysis are the joint optimization of the two adaptive filters @xmath49 and @xmath71 of the proposed scheme and a strategy to incorporate the effect of the step size of the recursions in ( [ recsd ] ) and ( [ recw ] ) .",
    "let us define the mse at time @xmath159 using the relations @xmath160 and @xmath161,\\ ] ] where the filter @xmath162 with @xmath0 coefficients is the @xmath34-rank approximation of a full - rank filter obtained with an inverse mapping performed by @xmath163 .",
    "the mse of the proposed scheme can be expressed by : @xmath164 \\\\ & = \\epsilon_{min } + \\xi(i ) - \\xi_{min } - e[{\\boldsymbol e}^{h}_{\\boldsymbol w}(i ) ] { \\boldsymbol a}(\\theta_k ) - { \\boldsymbol a}^{h}({\\theta_k})e[{\\boldsymbol e}_{\\boldsymbol w}(i)]\\\\ & = \\epsilon_{min } + \\xi_{ex}(i ) - e[{\\boldsymbol e}^{h}_{\\boldsymbol w}(i ) ] { \\boldsymbol a}({\\theta_k } ) - { \\boldsymbol a}^{h}(\\theta_{k})e[{\\boldsymbol e}_{\\boldsymbol w}(i ) ] \\end{split}\\ ] ] where @xmath165 corresponds to the desired signal , @xmath166 $ ] , @xmath167 $ ] is the mse with @xmath168 where @xmath169 is the minimum variance , and @xmath170 is the excess mse due to the adaptation process at the time instant @xmath97 . since @xmath171 = 0 $ ] we have @xmath172 where the @xmath173 term in ( [ exmse ] ) is the steady - state excess mse resulting from the adaptation process .",
    "the main difference here from prior work lies in the fact that this refers to the excess mse produced by a @xmath34-rank approximation filter @xmath29 . in order to analyze the trajectory of @xmath174 ,",
    "let us rewrite it as @xmath175 \\\\",
    "& = e [ \\bar{\\boldsymbol w}^h(i ) { \\boldsymbol s}_d^h(i ) { \\boldsymbol r}(i ) { \\boldsymbol",
    "r}^h(i ) { \\boldsymbol s}_d(i ) \\bar{\\boldsymbol w}(i ) ] \\\\ & = tr~e[{\\boldsymbol r}_{\\boldsymbol w } ( i){\\boldsymbol r } ] \\label{exmv } \\end{split}\\ ] ] where @xmath176 = { \\boldsymbol w}_{opt}{\\boldsymbol w}_{opt}^{h } + e[{\\boldsymbol e}_{w}(i)]{\\boldsymbol w}_{opt}^{h } + { \\boldsymbol w}_{opt}e[{\\boldsymbol e}_{\\boldsymbol w}^{h}(i ) ] + { \\boldsymbol r}_{e_{\\boldsymbol w}}(i)$ ] @xcite .    to proceed with the analysis , we must define the quantities @xmath177 , where the columns of @xmath178 are the eigenvectors of the symmetric and positive semi - definite matrix @xmath179 and @xmath180 is the diagonal matrix of the corresponding eigenvalues , @xmath181 $ ] , the rotated tap error vector @xmath182 , the rotated signal vectors @xmath183 , @xmath184 and @xmath185=\\boldsymbol{\\phi}^{h}{\\boldsymbol r}_{{\\boldsymbol e}_{\\boldsymbol w}}(i)\\boldsymbol{\\phi}$ ] . rewriting ( [ exmv ] ) in terms of the above transformed quantities we have : @xmath186 \\\\ & = \\xi_{min } + tr [ e[\\tilde{\\boldsymbol e}_{\\boldsymbol w}(i ) ] \\tilde{\\boldsymbol a}^{h}({\\theta_k } ) + \\tilde{\\boldsymbol a}({\\theta_k } ) e[\\tilde{\\boldsymbol e}_{\\boldsymbol w}^{h}(i ) ] \\\\ & \\quad + \\lambda{\\boldsymbol r}_{\\tilde{\\boldsymbol e}_{\\boldsymbol w}}(i ) ] \\end{split}\\ ] ] since @xmath187=0 $ ] , then @xmath188 $ ] .",
    "thus , it is evident that to assess the evolution of @xmath174 it is sufficient to study @xmath189 .",
    "using @xmath149 and @xmath150 and combining them to compute @xmath190 , we get @xmath191 substituting the expressions for @xmath192 and @xmath193 in ( [ esd ] ) and ( [ ew ] ) , respectively , to compute @xmath194 , we get @xmath195 where @xmath196 @xmath197 @xmath198 we can further rewrite the expressions above in order to obtain a more compact and convenient representation as @xmath199 where @xmath200 @xmath201 @xmath202    now , we need to compute @xmath203 $ ] by using the result in ( [ ewg2 ] ) , which yields @xmath204 since @xmath205={\\boldsymbol 0}$ ] and @xmath206 = { \\boldsymbol 0}$ ] , we can simplify the previous expression and obtain @xmath207 solving for @xmath208 , the mse can be computed by @xmath209 \\\\ & = \\epsilon_{min } + tr [   \\lambda { \\boldsymbol \\phi } { \\boldsymbol r}_{{\\boldsymbol e}_{{\\boldsymbol w}}}(i){\\boldsymbol \\phi}^h ] \\label{fmse } \\end{split}\\ ] ] it should be remarked that the expression for @xmath210 is quite involved and requires a semi - analytical approach with the aid of computer simulations for its computation .",
    "this is because the terms resulting from the joint adaptation create numerous extra terms in the expression of @xmath211 , which are very difficult to isolate .",
    "we found that using computer simulations to pre - compute the terms of @xmath210 as a function of the step sizes was more practical and resulted in good match between the semi - analytical and simulated curves . in the following section",
    ", we will demonstrate that it is able to predict the performance of the proposed sg algorithm .",
    "in this section we evaluate the performance of the proposed and the analyzed beamforming algorithms via computer simulations .",
    "we also verify the validity of the mse convergence analysis of the previous section .",
    "a smart antenna system with a ula containing @xmath0 sensor elements is considered for assessing the beamforming algorithms . in particular , the performance of the proposed scheme and sg and rls algorithms is compared with existing techniques , namely , the full - rank lcmv - sg @xcite and lcmv - rls @xcite , and the reduced - rank algorithms with @xmath71 designed according to the mswf @xcite , the avf @xcite and the optimal linear beamformer that assumes the knowledge of the covariance matrix @xcite . in particular , the algorithms are compared in terms of the mean - squared error ( mse ) and the signal - to - interference - plus - noise ratio ( sinr ) , which is defined for the reduced - rank schemes as @xmath212 where @xmath213 is the autocorrelation matrix of the desired signal and @xmath214 is the cross - correlation matrix of the interference and noise in the environment .",
    "note that for the full - rank schemes the @xmath215 assumes @xmath216 , where @xmath217 is an identity matrix with dimensionality @xmath0 .",
    "for each scenario , @xmath218 runs are used to obtain the curves . in all simulations ,",
    "the desired signal power is @xmath219 , and the signal - to - noise ratio ( snr ) is defined as @xmath220 .",
    "the filters are initialized as @xmath221 $ ] and @xmath222 $ ] , where @xmath223 is a @xmath224 matrix with zeros in all experiments .      in this part of the section",
    ", we verify that the results in ( [ mre ] ) and ( [ fmse ] ) of the section on mse convergence analysis of the proposed reduced - rank sg algorithms can provide a means of estimating the mse upon convergence .",
    "the steady state mse between the desired and the estimated symbol obtained through simulation is compared with the steady state mse computed via the expressions derived in section vi . in order to illustrate the usefulness of our analysis",
    "we have carried out some experiments . to semi - analytically compute the mse for the sg recursion , we have used ( [ optf ] ) and assumed the knowledge of the data covariance matrix @xmath225 .",
    "we consider @xmath226 interferers ( @xmath227 users in total - the soi and the interferers ) at @xmath228 , @xmath229 , @xmath230 , @xmath231 , @xmath232 with powers following a log - normal distribution with associated standard deviation @xmath233 db around the soi s power level , which impinges on the array at @xmath234 .",
    "we compare the results obtained via simulations with those obtained by the semi - analytical approach presented in section vi .",
    "in particular , we consider two sets of parameters in order to check the validity of our approach .",
    "one of the sets has larger step sizes ( @xmath235 and @xmath236 ) , whereas the other set employs smaller step sizes ( @xmath237 and @xmath238 ) for the recursions .",
    "the results shown in fig .",
    "4 indicate that the curves obtained with the semi - analytical approach agrees with those obtained via simulations for both sets of parameters , verifying the validity of our analysis .",
    "note that the algorithms with smaller step sizes converge slower than the algorithms equipped with larger step sizes .",
    "however , the proposed algorithms with smaller step sizes converge to the same level of mse as the optimal lcmv , whereas the proposed algorithms with larger step sizes exhibit a higher level of misadjustment . in what follows",
    ", we will consider the convergence rate of the proposed reduced - rank algorithms in comparison with existing algorithms .",
    "in the first two experiments , we consider @xmath239 interferers at @xmath228 , @xmath240 , @xmath241 , @xmath242 , @xmath230 , @xmath231 , @xmath232 with powers following a log - normal distribution with associated standard deviation @xmath233 db around the soi s power level .",
    "the soi impinges on the array at @xmath243 .",
    "the parameters of the algorithms are optimized .",
    "we first evaluate the sinr performance of the analyzed algorithms against the rank @xmath34 using optimized parameters ( @xmath83 , @xmath84 and forgetting factors @xmath68 ) for all schemes and @xmath244 snapshots .",
    "the results in fig . 5 indicate that the best rank for the proposed scheme is @xmath245 ( which will be used in the second scenario ) and it is very close to the optimal full - rank lcmv filter . our studies with systems with different sizes show that @xmath34 is relatively invariant to the system size , which brings considerable computational savings . in practice",
    ", the rank @xmath34 can be adapted in order to obtain fast convergence and ensure good steady - state performance and tracking after convergence .",
    "we show another scenario in fig .",
    "6 where the adaptive lcmv filters are set to converge to the same level of sinr .",
    "the parameters used to obtain these curves are also shown .",
    "the sg version of the mswf is known to have problems in these situations since it does not tridiagonalize its covariance matrix @xcite , being unable to approach the optimal lcmv .",
    "the curves show an excellent performance for the proposed scheme which converges much faster than the full - rank - sg algorithm , and is also better than the more complex mswf - rls and avf schemes .    in the next experiment",
    ", we consider the design of the proposed adaptive reduced - rank lcmv algorithms equipped with the automatic rank selection method described in section v.d .",
    "we consider @xmath226 interferers at @xmath228 , @xmath241 , @xmath230 , @xmath231 , @xmath232 with equal powers to the soi , which impinges on the array at @xmath234 .",
    "specifically , we evaluate the proposed rank selection algorithms against the use of fixed ranks , namely , @xmath246 and @xmath247 for both sg and rls algorithms .",
    "the results show that the proposed automatic rank selection method is capable of ensuring an excellent trade - off between convergence speed and steady - state performance , as illustrated in fig [ fig : auto ] . in particular",
    ", the proposed algorithm can achieve a significantly faster convergence performance than the scheme with fixed rank @xmath247 , whereas it attains the same steady state performance .    in the last experiment",
    ", we consider a non - stationary scenario where the system has @xmath248 users with equal power and the environment experiences a sudden change at time @xmath249 .",
    "the @xmath226 interferers impinge on the ula at @xmath228 , @xmath241 , @xmath230 , @xmath231 , @xmath232 with equal powers to the soi , which impinges on the array at @xmath234 . at time",
    "instant @xmath249 we have @xmath233 interferers with @xmath226 db above the soi s power level entering the system with doas @xmath250 , @xmath251 and @xmath243 , whereas one interferer with doa @xmath231 and a power level equal to the soi exits the system .",
    "the proposed and analyzed adaptive beamforming algorithms are equipped with automatic rank adaptation techniques and have to adjust their parameters in order to suppress the interferers .",
    "we optimize the step sizes and the forgetting factors of all the algorithms in order to ensure that they converge as fast as they can to the same value of sinr .",
    "the results of this experiment are depicted in fig .",
    "[ fig : ns ] .",
    "the curves show that the proposed reduced - rank algorithms have a superior performance to the existing algorithms .",
    "we proposed reduced - rank lcmv beamforming algorithms based on joint iterative optimization of filters .",
    "the proposed reduced - rank scheme is based on a constrained joint iterative optimization of filters according to the minimum variance criterion .",
    "we derived lcmv expressions for the design of the projection matrix and the reduced - rank filter and developed sg and rls adaptive algorithms for their efficient implementation along with an automatic rank selection technique .",
    "an analysis of the stability and the convergence properties of the proposed algorithms was presented and semi - analytical expressions were derived for predicting the mse performance . the numerical results for a digital beamforming application with a ula showed that the proposed scheme and algorithms outperform in convergence and tracking the existing full - rank and reduced - rank algorithms at comparable complexity .",
    "the proposed algorithms can be extended to other array geometries and applications .",
    "in this appendix we discuss the conditions for which the mv obtained for the full - rank filter is preserved and the existence of multiple solutions in the proposed optimization method . given an @xmath31 projection matrix @xmath71 , where @xmath252 , the @xmath253 is achieved if and only if @xmath21 which minimizes ( [ flcmv ] ) belongs to the @xmath254 , i.e. @xmath29 lies in the subspace generated by @xmath71 . in this case , we have @xmath255 for a general @xmath163 , we have @xmath256 from the above relations , we can conclude that there exists multiple solutions to the proposed optimization problem .",
    "in this appendix , we carry out an analysis of the proposed reduced - rank method and its optimization .",
    "our approach is based on expressing the output of the proposed scheme and the proposed constraint in a convenient form that renders itself to analysis .",
    "let us rewrite the proposed constrained optimization method in ( [ propt ] ) using the method of lagrange multipliers and express it by the lagrangian @xmath257   + 2\\re [ \\lambda ( \\bar{\\boldsymbol w}^h(i){\\boldsymbol s}_d^h(i){\\boldsymbol a}(\\theta_k)-1 ) ] % \\\\ &   = e\\big [ |x(i)|^2 \\big ]   + 2\\re [ \\lambda % ( \\bar{\\boldsymbol w}^h(i){\\boldsymbol s}_d^h(i){\\boldsymbol % a}(\\theta_k)-1 ) ] , \\label{uopt2 } \\end{split}\\ ] ] in order to proceed , let us express @xmath52 in an alternative and more convenient form as @xmath258^t \\left[\\begin{array}{c } { \\boldsymbol s}_1^*(i ) \\\\",
    "{ \\boldsymbol s}_2^*(i ) \\\\ \\vdots \\\\ { \\boldsymbol s}_d^*(i ) \\end{array } \\right ] \\\\ & = \\bar{\\boldsymbol w}^h(i ) { \\boldsymbol \\re}^t(i ) { \\boldsymbol s}_v^*(i ) \\end{split}\\ ] ] where @xmath259 is a @xmath260 block diagonal matrix with the input data vector @xmath25 , @xmath261 is a @xmath110 vector with a @xmath92 in the @xmath141-th position and @xmath262 is a @xmath263 vector with the columns of @xmath71 stacked on top of each other .    in order to analyze the proposed joint optimization procedure",
    ", we can rearrange the terms in @xmath52 and define a single @xmath264 parameter vector @xmath265^t$ ] .",
    "we can therefore further express @xmath52 as @xmath266 { \\boldsymbol f}(i ) \\\\ & = { \\boldsymbol f}^h(i ) { \\boldsymbol g}(i ) { \\boldsymbol f}(i ) \\end{split}\\ ] ] where @xmath267 is a @xmath268 matrix which contains @xmath259 .",
    "now let us perform a similar linear algebra transformation with the proposed constraint @xmath269 and express it as @xmath270 where the @xmath268 matrix @xmath271 is structured as @xmath272 \\nonumber\\ ] ] and the @xmath260 block diagonal matrix @xmath273 with the steering vector @xmath24 constructed as @xmath274\\ ] ] at this point , we can alternatively express the lagrangian in ( [ uopt2 ] ) as @xmath275   + 2\\re [ \\lambda ( { \\boldsymbol f}^h(i ) { \\boldsymbol a}(\\theta_k ) { \\boldsymbol f}(i)-1 ) ] .",
    "\\label{uopt3 } \\end{split}\\ ] ] we can examine the convexity of the above lagrangian by computing the hessian ( @xmath276)with respect to @xmath277 using the expression @xcite @xmath278 and testing if the terms are positive semi - definite .",
    "specifically , @xmath276 is positive semi - definite if @xmath279 for all nonzero @xmath280 @xcite .",
    "therefore , the optimization problem is convex if the hessian @xmath276 is positive semi - definite .",
    "evaluating the partial differentiation in the expression given in ( [ hess ] ) yields @xmath281 \\end{split}\\ ] ] by examining @xmath276 , we verify that the second and fourth terms are positive semi - definite , whereas the first and the third terms are indefinite .",
    "the fifth term depends on the constraint , which is typically positive in the proposed scheme as verified in our studies , yielding a positive semi - definite matrix .",
    "therefore , the optimization problem can not be classified as convex .",
    "it is however important to remark that our studies indicate that there are no local minima and there exists multiple solutions ( which are possibly identical ) .    in order to support this claim ,",
    "we have checked the impact on the proposed algorithms of different initializations .",
    "this study confirmed that the algorithms are not subject to performance degradation due to the initialization although we have to bear in mind that the initialization @xmath282 annihilates the signal and must be avoided . we have also studied a particular case of the proposed scheme when @xmath283 and @xmath62 , which yields the lagrangian @xmath284 + 2\\re \\big [ \\lambda ( \\bar{w } s_d a(\\theta_k ) -1 ) \\big]$ ] .",
    "choosing @xmath285 ( the `` scalar '' projection ) fixed with @xmath34 equal to @xmath92 , it is evident that the resulting function @xmath286 $ ] is a convex one .",
    "in contrast to that , for a time - varying projection @xmath285 the plots of the function indicate that the function is no longer convex but it also does not exhibit local minima .",
    "this problem can be generalized to the vector case , however , we can no longer verify the existence of local minima due to the multi - dimensional surface .",
    "this remains as an interesting open problem .",
    "in this appendix , we detail the derivation of the filter @xmath71 and the simplification shown in ( [ 18 ] ) for reducing the computational complexity .",
    "let us consider the derivation of @xmath71 obtained from the minimization of the lagrangian @xmath287 , \\label{uopt2 } \\end{split}\\ ] ] taking the gradient terms of the above expression with respect to @xmath80 , we get @xmath288 making the above gradient terms equal to zero yields @xmath289 using the proposed constraint @xmath290 and substituting the above filter expression , we obtain the lagrange multiplier @xmath291 .",
    "substituting @xmath68 into ( [ filtsdwl ] ) , we get @xmath292 the above expression for the matrix filter @xmath71 can be simplified by observing the quantities involved and making use of the proposed constraint @xmath293 .",
    "let us consider the term @xmath294 in the denominator of ( [ filtsd2 ] ) and multiply it by the proposed constraint as follows : @xmath295 now let us consider the term @xmath296 and rewrite it as follows : @xmath297 using the relations obtained in ( [ wrw ] ) and ( [ rw ] ) into the expression in ( [ filtsd2 ] ) , we can get a simpler expression for the projection matrix as given by @xmath298 this completes the derivation and the simplification .",
    "r. c. de lamare and r. sampaio - neto ,  low - complexity variable step - size mechanisms for stochastic gradient algorithms in minimum variance cdma receivers \" , ieee trans .",
    "signal processing , vol .",
    "54 , pp . 2302 - 2317 , june 2006 .",
    "l. s. resende , j. m. t. romano and m. g. bellanger `` a fast least - squares algoirthm for linearly constrained adaptive filtering '' _ ieee transactions on signal processing _ , vol .",
    "1168 - 1174 , may 1996 .",
    "q. haoli and s.n .",
    "batalama , ",
    "data record - based criteria for the selection of an auxiliary vector estimator of the mmse / mvdr filter \" , _ ieee transactions on communications _ , vol .",
    "2003 , pp .",
    "1700 - 1708 .",
    "r. c. de lamare and m. j. lowe ,  a reduced - rank approach to adaptive linearly constrained minimum variance beamforming based on joint iterative optimization of adaptive filters \" , _ proc .",
    "ieee 9th workshop on signal processing advances in wireless communications _",
    ", july 2008 .",
    "lcc & additions & multiplications + _ * full - rank - sg * _ @xcite & @xmath300 & @xmath301 +   + _ * full - rank - rls * _ @xcite & @xmath302 & @xmath303 +   + _ * proposed - sg * _ @xcite & @xmath304 & @xmath305 + _ * * _ & @xmath306 & @xmath307 + _ * proposed - rls * _ & @xmath308 & @xmath309 + _ * * _ & @xmath310 & @xmath311 + _ * mswf - sg * _ @xcite & @xmath312 & @xmath313 + _ * * _ & @xmath314 & @xmath315 + _ * mswf - rls * _ @xcite & @xmath316 & @xmath317 + _ _ & @xmath318 & @xmath319 + _ * avf * _ @xcite & @xmath320 & @xmath321 + _ * * _ & @xmath322 & @xmath323 +"
  ],
  "abstract_text": [
    "<S> this paper presents reduced - rank linearly constrained minimum variance ( lcmv ) beamforming algorithms based on joint iterative optimization of filters . </S>",
    "<S> the proposed reduced - rank scheme is based on a constrained joint iterative optimization of filters according to the minimum variance criterion . </S>",
    "<S> the proposed optimization procedure adjusts the parameters of a projection matrix and an adaptive reduced - rank filter that operates at the output of the bank of filters . </S>",
    "<S> we describe lcmv expressions for the design of the projection matrix and the reduced - rank filter . </S>",
    "<S> we then describe stochastic gradient and develop recursive least - squares adaptive algorithms for their efficient implementation along with automatic rank selection techniques . </S>",
    "<S> an analysis of the stability and the convergence properties of the proposed algorithms is presented and semi - analytical expressions are derived for predicting their mean squared error ( mse ) performance . </S>",
    "<S> simulations for a beamforming application show that the proposed scheme and algorithms outperform in convergence and tracking the existing full - rank and reduced - rank algorithms while requiring comparable complexity . </S>",
    "<S> +    adaptive filters , beamforming , constrained optimization , iterative methods . </S>"
  ]
}