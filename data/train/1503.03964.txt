{
  "article_text": [
    "the trade - off between the exploitation of good choices and the exploration of unknown but potentially more profitable choices is a well - known problem @xcite .",
    "a multi - armed bandit ( mab ) provides the most typical environment for studying this trade - off .",
    "it is defined by sequential decision making among multiple choices that are associated with a payoff .",
    "the mab problem involves the maximization of the total reward for a given period or budget . in a variety of circumstances ,",
    "exact or approximated optimal strategies have been proposed @xcite .",
    "recently , the mab has also provided a good environment for the trade - off between social and asocial learning @xcite . here , social learning is learning through observation or interaction with other individuals , and asocial learning is individual learning @xcite .",
    "the advantage of social learning is its cost compared with asocial learning .",
    "the disadvantage is its error - prone nature , as the information obtained by social learning might be outdated or inappropriate . in order to clarify the optimal strategy in the environment with the two trade - offs , rendell et al .",
    "held a computer tournament using a restless multi - armed bandit ( rmab ) @xcite . here",
    ", restless means that the payoff of each bandit changes over time .",
    "there are 100 bandits in an rmab , and each bandit has a distinct payoff independently drawn from an exponential distribution .",
    "the probability that a payoff changes per round is @xmath0 .",
    "an agent has three options for each round : innovate , observe , and exploit .",
    "innovate and observe correspond to asocial and social learning , respectively . for innovate",
    ", an agent obtains the payoff information of one randomly chosen bandit . for observe",
    ", an agent obtains the payoff information of @xmath6 randomly chosen bandits that were exploited by the agents during the previous round .",
    "compared to the information obtained by innovate , that obtained by observe is older by one round . for exploit ,",
    "an agent chooses a bandit that he has already explored by innovate or observe and obtains a payoff . in an rmab environment , it is extremely difficult for agents to optimize their choices @xcite .",
    "the outcome of the tournament was that the winning strategies relied heavily on social learning .",
    "this contradicted previous studies in which the optimal strategy is a mixed one that relies on some combination of social and asocial learning . in the tournament ,",
    "the cost for observe was not very low , as approximately 50% of the choices of observe returns information that the agents already knew .",
    "the results of the tournament imply the inadvertent filtering of information when an agent chooses observe , as the agents choose the best bandit during exploit .    in this paper",
    ", we discuss whether social or asocial learning is optimal in an rmab , where a player competes with many agents .",
    "we answer to the question why social learning is so adaptive in rendell s tournament .",
    "we suppose that the cost of innovate becomes higher than that of observe in the tournament . in order to reduce the cost of innovate , we control the exploration range @xmath1 for innovate , and agents obtain the best information about the bandits among @xmath1 randomly chosen bandits .",
    "an rmab is characterized by two parameters , @xmath0 and @xmath1 .",
    "we compare the average payoffs of the optimal strategies when only innovate , only observe , and both are available for learning using the complete knowledge of an rmab and the information of the bandits exploited by agents .",
    "we determine the region in which each type of learning is optimal in the @xmath7 plane and show that observe is more adaptive than innovate for @xmath8 .",
    "we define the swarm intelligence effect as the increase in the average payoff compared with the payoffs of the optimal strategies where only asocial learning is available .",
    "we have conducted a laboratory experiment where 67 human subjects competed with multiple agents in an rmab .",
    "if the parameters are chosen in the region where social learning is far more optimal than asocial learning , we observe the swarm intelligence effect .",
    "an interactive rmab game is a game in which a player competes with 120 agents using an rmab .",
    "the player aims to maximize the total payoff over 103 rounds and obtain a high ranking among all entrants .",
    "below , we term the population of all agents and a player as all entrants .",
    "the rmab has @xmath9 bandits , and we label them as @xmath10 .",
    "bandit @xmath11 has a distinct payoff @xmath12 , and we term the @xmath13 pair as bandit information .",
    "@xmath12 is an integer drawn at random from an exponential distribution ( @xmath14 ; values were squared and rounded to give integers mostly falling in the range of 010 @xcite ) .",
    "we denote the probability function for @xmath12 as @xmath15 ( left figure in figure [ fig : game ] ) .",
    "we write the expected value of @xmath12 as e@xmath16 , and it is approximately 1.68 .",
    "the payoff of each bandit changes independently between rounds with a probability @xmath0 , with new payoff drawn at random from the same distribution .    [",
    "cols=\"^,^ \" , ]     [ tab:1 ]    @xmath17 had a negative effect on the performance of the subjects , as in rendell s tournament .",
    "this result suggests that it is suboptimal to invest too much time in learning , as one can not obtain any payoffs for learning . for @xmath18 ,",
    "the results are not consistent with the results of rendell s tournament . there",
    ", the predictor had a strong positive effect , which reflected the fact that the best strategy was to almost exclusively choose observe rather than innovate . in our experiment , the predictor seems to have a positive effect for cases a , c , and d. for cases a and c , it is consistent with the results in the previous section because * o*@xmath19*i * , and observe is more optimal than innovate . in case d , as * h * is much less than both * i * and * o * , obtaining good bandit information from the agents by observe might improve the performance .",
    "in this paper , we attempt to clarify the optimal strategy in a two trade - offs environment . here , the two trade - offs are the trade - off of exploitation  exploration and that of social  asocial learning . for this purpose",
    ", we have developed an interactive rmab game , where a player competes with multiple agents .",
    "the player and agents choose an action from three options : exploit a bandit , innovate to obtain new bandit information , and observe the bandit information exploited by other agents .",
    "the rmab has two parameters , @xmath0 and @xmath1 .",
    "@xmath0 is the probability for a change in the environment .",
    "@xmath1 is the scope of exploration for asocial learning .",
    "the agents have two parameters for their decision making , @xmath4 and @xmath3 .",
    "@xmath4 is the probability for observe when the agents learn , and @xmath3 is the threshold value for exploit .",
    "we have estimated the average payoff of the optimal strategy with some restrictions on learning and complete knowledge about rmab and the bandit information exploited during the previous round .",
    "we consider three types of optimal strategies , * i+o * , * i * , and * o * , where both innovate and observe , innovate , and observe are available . in the @xmath7 plane , we have derived the strategy that is more optimal , either * o * or * i*. furthermore , we have defined the swarm intelligence effect as the surplus of the performance of * i*. the estimate of the swarm intelligence effect provides only a lower bound for it ; however , the estimation is easy and objective .",
    "we also point out that the swarm intelligence effect can be observed in the region of the @xmath7 plane where * o * is more optimal than * i*. we have performed an experiment with 67 subjects and have gathered approximately 56 samples for the four cases of @xmath7 .",
    "if @xmath7 are chosen in the region where * o * is far more optimal that * i * , we have observed the swarm intelligence effect .",
    "if @xmath7 are chosen near the boundary of the two regions or in the region where * i * is more optimal than * o * , we did not observe the swarm intelligence effect .",
    "we have performed a regression analysis of the performance of each subject in each case .",
    "only the proportion of learning is the effective factor in the four cases .",
    "in contrast , the proportion of the use of observe for learning is not significant .",
    "as the agent s decision making algorithm is too simple , it is difficult to believe that the conditions for the emergence of the swarm intelligence in figure [ fig : i+o ] are general . in addition , the analysis of the human subjects is too superficial , as we only studied the correlation between the performance and some predictive factors . with these points in mind ,",
    "we make three comments about future problems .",
    "the first one is a more elaborate and autonomous model of the decision making in an rmab environment .",
    "the algorithm needs to estimate @xmath20 , and @xmath21 for round @xmath22 on the basis of the data that the agent has obtained through his choices .",
    "then , the agent can choose the most optimal option during each round and maximize the expected total payoff on the basis of these estimates .",
    "this is an adaptive autonomous agent model . with this model",
    ", we can understand the decision making of humans in the rmab game more deeply .",
    "it is impossible to understand human decision making completely with experimental data . on the basis of the model",
    ", we can detect the deviation in human decision making and propose a decision making model for a human that can be tested in other experiments .",
    "the second one is the collective behavior of the above adaptive autonomous agents or humans .",
    "it is necessary to clarify how the conditions for the emergence of the swarm intelligence effect would change . in the case of a population of adaptive autonomous agents",
    ", they would estimate the optimal value of @xmath4 for the environment @xmath7 and collectively realize the optimal value .",
    "the optimal strategy should be neither * i * nor * o * but a mixed strategy of innovate and observe .",
    "then , the condition for the emergence of the swarm intelligence effect is that the performance of * i+o * is equal to that of * i*. if the performance of the former is greater than that of the latter for any @xmath7 , the swarm intelligence effect can always emerge , except for the noise - dominant region .",
    "after that , we can study the conditions with human subjects experimentally .",
    "a human subject participates in the rmab game as a player , as in this study , or many human players participate in the game to compete with each other .",
    "the target is how and when humans collectively solve the rmab problem .",
    "the third one is the design of an environment in which swarm intelligence works . in this study",
    ", we choose the rmab interactive game and study the conditions for the emergence of the swarm intelligence effect for a player . however , there are many degrees of freedom in the design of the game .",
    "for example , when an agent observes , there are many degrees of freedom regarding how bandit information is provided to the agent . in the present game environment ,",
    "the probability that a bandit exploited in the previous round is chosen is proportional to the number of agents who have exploited it .",
    "instead , we can consider an environment in which the bandit information of the most exploited bandit is provided , the bandit information of the agents who are near the agent is provided , or the player can choose a bandit by showing him the number of agents who have exploited the bandit .",
    "we think these changes should affect the choice and performance of the player .",
    "it was shown experimentally that by providing subjective information about a bandit , the performance of the subjects diminished @xcite .",
    "we think that the interaction between the design of the environment and the decision making , performance , and swarm intelligence effect should be a very important problem in the industrial usage of the swarm intelligence effect ."
  ],
  "abstract_text": [
    "<S> we obtain the conditions for the emergence of the swarm intelligence effect in an interactive game of restless multi - armed bandit ( rmab ) . </S>",
    "<S> a player competes with multiple agents . </S>",
    "<S> each bandit has a payoff that changes with a probability @xmath0 per round . </S>",
    "<S> the agents and player choose one of three options : ( 1 ) exploit ( a good bandit ) , ( 2 ) innovate ( asocial learning for a good bandit among @xmath1 randomly chosen bandits ) , and ( 3 ) observe ( social learning for a good bandit ) . </S>",
    "<S> each agent has two parameters @xmath2 to specify the decision : ( i ) @xmath3 , the threshold value for exploit , and ( ii ) @xmath4 , the probability for observe in learning . </S>",
    "<S> the parameters @xmath2 are uniformly distributed . </S>",
    "<S> we determine the optimal strategies for the player using complete knowledge about the rmab . </S>",
    "<S> we show whether or not social or asocial learning is more optimal in the @xmath5 space and define the swarm intelligence effect . </S>",
    "<S> we conduct a laboratory experiment ( 67 subjects ) and observe the swarm intelligence effect only if @xmath5 are chosen so that social learning is far more optimal than asocial learning .    </S>",
    "<S> kitasato university + 1 - 15 - 1 kitasato , sagamihara , kanagawa 252 - 0373 japan    financial services agency + 3 - 2 - 1 kasumigaseki , chiyoda - ku , tokyo 100 - 8967 japan    kitasato university + 1 - 15 - 1 kitasato , sagamihara , kanagawa 252 - 0373 japan -mailshintaro.mori@gmail.com    multi - armed bandit , swarm intelligence , interactive game , experiment , optimal strategy </S>"
  ]
}