{
  "article_text": [
    "a practical problem an experimental physicist would face is the following  a process ( eg",
    ". a particle moving in space ) has an observed variable ( say position of the particle ) which potentially takes @xmath0 distinct values , but the measuring device is capable of recording only @xmath1 values and @xmath2 . in such a scenario ( figure  [ figure : measurement ] ) , how can we make use of these @xmath1 states of the measuring device to capture the essential information of the source ? it may be the case that @xmath0 takes values from an infinite set , but the measuring device is capable of recording only a finite number of states .",
    "however , it shall be assumed that @xmath0 is finite but allowed for the possibility that @xmath3 ( for e.g. , it is possible that @xmath4 and @xmath5 ) .    our aim is to capture the essential information of the source ( the process is treated as a source and the observations as messages from the source ) in a lossless fashion .",
    "this problem actually goes all the way back to shannon  @xcite who gave a mathematical definition for the information content of a source .",
    "he defined it as ` entropy ' , a term borrowed from statistical thermodynamics .",
    "furthermore , his now famous noiseless source coding theorem states that it is possible to encode the information of a memoryless source ( assuming that the observables are independent and identically distributed ( i.i.d ) ) using ( at least ) @xmath6 bits per symbol , where @xmath6 stands for the shannon s entropy of the source @xmath7 . stated in other words , the average codeword length @xmath8 where @xmath9 is the length of the @xmath10-th codeword and @xmath11 the corresponding probability of occurrence of the @xmath10-th alphabet of the source .    . if @xmath5 , we are seeking binary codes . ]",
    "shannon s entropy @xmath6 defines the ultimate limit for lossless data compression .",
    "data compression is a very important and exciting research topic in information theory since it not only provides a practical way to store bulky data , but it can also be used effectively to measure entropy , estimate complexity of sequences and provide a way to generate pseudo - random numbers  @xcite ( which are necessary for monte - carlo simulations and cryptographic protocols ) .",
    "several researchers have investigated the relationship between chaotic dynamical systems and data compression ( more generally between chaos and information theory ) .",
    "jimnez - montao , ebeling , and others  @xcite have proposed coding schemes by a symbolic substitution method .",
    "this was shown to be an optimal data compression algorithm by grassberger  @xcite and also to accurately estimate shannon s entropy  @xcite and lyapunov exponents of dynamical systems  @xcite .",
    "arithmetic coding , a popular data compression algorithm used in jpeg2000 was recently shown to be a specific mode of a piecewise linear chaotic dynamical system  @xcite . in another work  @xcite , we have used symbolic dynamics on chaotic dynamical systems to prove the famous kraft - mcmillan inequality and its converse for prefix - free codes , a fundamental inequality in source coding , which also has a quantum analogue .    in this paper , we take a nonlinear dynamical systems approach to the aforementioned measurement problem .",
    "we are interested in modeling the source by a nonlinear dynamical system . by a suitable model",
    ", we hope to capture the information content of the source .",
    "this paper is organized as follows . in section",
    "ii , stochastic sources are modeled using piecewise linear chaotic dynamical systems which exhibits some important and interesting properties . in section iii",
    ", we propose a new algorithm for source coding and prove that it achieves the least average codeword length and turns out to be a re - discovery of huffman coding  @xcite  the popular lossless compression algorithm used in the jpeg international standard  @xcite for still image compression .",
    "we make some observations about our approach in section iv and conclude in section v.",
    "we shall consider stationary sources .",
    "these are defined as sources whose statistics remain constant with respect to time  @xcite .",
    "these include independent and identically distributed ( i.i.d ) sources and ergodic ( markov ) sources .",
    "these sources are very important in modeling various physical / chemical / biological phenomena and in engineering applications  @xcite .",
    "on the other hand , non - stationary sources are those whose statistics change with time",
    ". we shall not deal with them here",
    ". however , most coding methods are applicable to these sources with some suitable modifications .",
    "consider an i.i.d source @xmath7 ( treated as a random variable ) which takes values from a set of @xmath0 values @xmath12 with probabilities @xmath13 respectively with the condition @xmath14 .",
    "an i.i.d source can be simply modeled as a ( memoryless ) markov source ( or markov process  @xcite ) with the transition probability from state @xmath10 to @xmath15 as being independent of state @xmath10 ( and all previous states )  . ] .",
    "we can then embed the markov source into a dynamical system as follows : to each markov state ( i.e. to each symbol in the alphabet ) , associate an interval on the real line segment @xmath16 such that its length is equal to the probability .",
    "any two such intervals have pairwise disjoint interiors and the union of all the intervals cover @xmath16 .",
    "such a collection of intervals is known as a partition .",
    "we define a deterministic map @xmath17 on the partitions such that they form a markov partition ( they satisfy the property that the image of each interval under @xmath17 covers an integer number of partitions  @xcite ) .    the simplest way to define the map @xmath17 such that the intervals form a markov partition is to make it linear and surjective .",
    "this is depicted in figure  [ figure : glsandmodes](a ) .",
    "such a map is known as generalized lurth series ( gls ) .",
    "there are other ways to define the map @xmath17 ( for eg .",
    ", see  @xcite ) but for our purposes gls will suffice .",
    "lurth s paper in 1883 ( see reference in dajani et .",
    "@xcite ) deals with number theoretical properties of lurth series ( a specific case of gls ) .",
    "however , georg cantor had discovered gls earlier in 1869  @xcite .",
    "+ ( a ) generalized lurth series ( gls ) @xmath18",
    "+ ( b ) @xmath19 modes of gls .",
    "a list of important properties of gls is given below :    1 .",
    "gls preserves the lebesgue ( probability ) measure . 2 .",
    "every ( infinite ) sequence of symbols from the alphabet corresponds to an unique initial condition .",
    "3 .   the symbolic sequence of every initial condition is i.i.d .",
    "4 .   gls is chaotic ( positive lyapunov exponent , positive topological entropy ) . 5 .",
    "gls has maximum topological entropy ( @xmath20 ) for a specified number of alphabets ( @xmath0 ) .",
    "thus , all possible arrangements of the alphabets can occur as symbolic sequences .",
    "gls is isomorphic to the shift map and hence ergodic ( bernoulli ) .",
    "modes of gls : as it can be seen from figure  [ figure : glsandmodes](b ) , the slope of the line that maps each interval to @xmath16 can be _ chosen _ to be either positive or negative .",
    "these choices result in a total of @xmath19 _ modes _ of gls ( up to a permutation of the intervals along with their associated alphabets for each mode , these are @xmath21 in number ) .",
    "it is property 2 and 3 that allow a faithful `` embedding '' of a stochastic i.i.d source . for a proof of these properties",
    ", please refer dajani et .",
    "al .  @xcite .",
    "some well known gls are the standard binary map and the standard tent map shown in figure  [ figure : wellknowngls ] .",
    ", @xmath22 ; @xmath23 , @xmath24 ) .",
    "( b ) standard tent map ( @xmath25 , @xmath26 ; @xmath27 , @xmath24).,title=\"fig : \" ] , @xmath22 ; @xmath23 , @xmath24 ) .",
    "( b ) standard tent map ( @xmath25 , @xmath26 ; @xmath27 , @xmath24).,title=\"fig : \" ] + ( a )   ( b )      it is easy to verify that gls preserves the lebesgue measure .",
    "a probability density @xmath28 on [ 0,1 ) is invariant under the given transformation @xmath17 , if for each interval @xmath29 \\subset [ 0,1)$ ] , we have :    @xmath30    where @xmath31 ) = \\ { x | c \\leq t(x ) \\leq d \\}$ ] .",
    "for the gls , the above condition has constant probability density on @xmath16 as the only solution .",
    "it then follows from birkhoff s ergodic theorem  @xcite that the asymptotic probability distribution of the points of almost every trajectory is uniform .",
    "we can hence calculate lyapunov exponent as follows :    @xmath32    here , we measure @xmath33 in bits / iteration .",
    "@xmath28 is uniform with value 1 on [ 0,1 ) and @xmath34 since @xmath35 is linear in each of the intervals , the above expression simplifies to :    @xmath36    this turns out to be equal to shannon s entropy of the i.i.d source @xmath7 .",
    "thus lyapunov exponent of the gls that faithfully embeds the stochastic i.i.d source @xmath7 is equal to the shannon s entropy of the source .",
    "lyapunov exponent can be understood as the amount of information in bits revealed by the symbolic sequence ( measurement ) of the dynamical system in every iteration  .",
    "it can be seen that the lyapunov exponent for all the modes of the gls are the same .",
    "the lyapunov exponent for binary i.i.d sources is plotted in figure  [ fig : fighp ] as a function of @xmath37 ( the probability of symbol ` 0 ' ) .     in units of bits",
    "/ iteration plotted against @xmath37 for binary i.i.d sources .",
    "the maximum occurs at @xmath38 .",
    "note that @xmath39 . ]",
    "in this section , we address the measurement problem proposed in section  [ section : measurement problem ] . throughout our analysis , @xmath40 ( finite ) and @xmath5 is assumed .",
    "we are seeking _ minimum - redundancy binary symbol _ codes .",
    "`` minimum - redundancy '' is defined as follows  @xcite : +    a binary symbol code @xmath41 with lengths @xmath42 for the i.i.d source @xmath7 with alphabet @xmath43 with respective probabilities @xmath44 is said to have minimum - redundancy if @xmath45 is minimum .    for @xmath46 ,",
    "the minimum - redundancy binary symbol code for the alphabet @xmath47 is @xmath48 ( @xmath49 , @xmath50 ) .",
    "the goal of source coding is to minimize @xmath51 , the average code - word length of @xmath52 , since this is important in any communication system . as we mentioned before , it is always true that @xmath53  @xcite .",
    "our approach is to approximate the original i.i.d source ( gls with @xmath0 partitions ) with the _ best _ gls with a reduced number of partitions ( reduced by 1 ) . for the sake of notational convenience , we shall term the original gls as order @xmath0 ( for original source @xmath54 ) and the reduced gls would be of order @xmath55 ( for approximating source @xmath56 ) .",
    "this new source @xmath56 is now approximated further with the _ best _ possible source of order @xmath57 ( @xmath58 ) .",
    "this procedure of successive approximation of sources is repeated until we end up with a gls of order @xmath59 ( @xmath60 ) .",
    "it has only two partitions for which we know the minimum - redundancy symbol code is @xmath48 .    at any given stage @xmath61 of approximation ,",
    "the easiest way to construct a source of order @xmath62 is to merge two of the existing @xmath61 partitions",
    ". what should be the rationale for determining which is the _",
    "@xmath62 order approximating source @xmath63 for the source @xmath64 ?    among all possible @xmath62 order approximating sources",
    ", the best approximation is the one which minimizes the following quantity : @xmath65    where @xmath66 is the lyapunov exponent of the argument .",
    "the reason behind this choice is intuitive .",
    "we have already established that the lyapunov exponent is equal to the shannon s entropy for the gls and that it represents the amount of information ( in bits ) revealed by the symbolic sequence of the source at every iteration .",
    "thus , the best approximating source should be as close as possible to the original source in terms of lyapunov exponent .",
    "there are three steps to our algorithm for finding minimum redundancy binary symbol code as given below here :    1 .",
    "embed the i.i.d source @xmath7 in to a gls with @xmath0 partitions as described in  [ subsection : embedding ] .",
    "initialize @xmath67 .",
    "the source is denoted by @xmath68 to indicate order @xmath69 .",
    "approximate source @xmath68 with a gls with @xmath70 partitions by merging the _ smallest _ two partitions to obtain the source @xmath71 of order @xmath70",
    ". @xmath72 .",
    "repeat step 2 until order of the gls is 2 ( @xmath73 ) , then , stop .    we shall prove that the approximating source which merges the two _ smallest _ partitions is the _",
    "best _ approximating source .",
    "it shall be subsequently proved that this algorithm leads to minimum - redundancy , i.e. , it minimizes @xmath51 . assigning codewords to the alphabets",
    "will also be shown . +",
    "* theorem 1 : ( best successive source approximation ) * _ for a source @xmath74 which takes values from @xmath75 with probabilities @xmath76 respectively and with @xmath77 ( @xmath78 ) , the source @xmath79 which is the * best * @xmath1 - 1 order approximation to @xmath74 has probabilities @xmath80 . _ + * proof : + * by induction on @xmath1 . for @xmath81 and @xmath5",
    ", there is nothing to prove .",
    "we will first show that the statement is true for @xmath82 .",
    "* * @xmath82*. @xmath83 takes values from @xmath84 with probabilities @xmath85 respectively and @xmath86 with @xmath87 .",
    "+ we need to show that @xmath60 which takes values from @xmath88 with probabilities @xmath89 is the best @xmath90-order approximation to @xmath83 . here",
    "@xmath91 is a symbol that represents the merged partition .",
    "+ this means , that we should show that this is better than any other @xmath90-order approximation .",
    "there are two other @xmath90-order approximations , namely , @xmath92 which takes values from @xmath93 with probabilities @xmath94 and @xmath95 which takes values from @xmath96 with probabilities @xmath97 .",
    "+ this implies that we need to show @xmath98 and @xmath99 .",
    "* we shall prove @xmath98 .",
    "+ this means that we need to prove @xmath100 .",
    "this means we need to show @xmath101 .",
    "we need to show the following : @xmath102 there are two cases . if @xmath103 , then since @xmath104 , @xmath105 . if @xmath106 , then since @xmath107 , we have @xmath108 .",
    "this again implies @xmath105 .",
    "thus , we have proved that @xmath60 is better than @xmath92 .",
    "* we can follow the same argument to prove that @xmath109 .",
    "thus , we have shown that the theorem is true for @xmath82 .",
    "an illustrated example is given in figure  [ figure : huffman1 ] .",
    "* induction hypothesis : assume that the theorem is true for @xmath110 , we need to prove that this implies that the theorem is true for @xmath111 .",
    "+ let @xmath112 have the probability distribution @xmath113 .",
    "let us assume that @xmath114 ( if this is the case , there is nothing to prove ) .",
    "this means that @xmath115 .",
    "divide all the probabilities by ( @xmath116 ) to get @xmath117 .",
    "consider the set @xmath118 .",
    "this represents a probability distribution of a source with @xmath119 possible values and we know that the theorem is true for @xmath110 . +",
    "this means that the best source approximation for this new distribution is a source with probability distribution @xmath120 .",
    "+ in other words , this means : @xmath121 @xmath122 @xmath123 + where @xmath124 and @xmath125 are both different from @xmath119 and @xmath126 .",
    "multiply on both sides by @xmath127 and simplify to get : @xmath128 add the term @xmath129 on both sides and we have proved that the * best * @xmath119-order approximation to @xmath112 is the source @xmath130 , where symbols with the two least probabilities are merged together .",
    "we have thus proved the theorem.@xmath131    [ !",
    "h ]   is the closest to @xmath132 .",
    "unit of @xmath133 is bits / iteration .",
    ", title=\"fig : \" ] + ( a ) source @xmath7 : \\{@xmath134,@xmath135,@xmath136 } with probabilities \\{0.7 , 0.2 , 0.1 } , @xmath137 . +   is the closest to @xmath132 .",
    "unit of @xmath133 is bits / iteration .",
    ", title=\"fig : \" ]   is the closest to @xmath132 .",
    "unit of @xmath133 is bits / iteration .",
    ", title=\"fig : \" ] + ( b ) @xmath134 and @xmath135 merged .",
    "( c ) @xmath135 and @xmath136 merged .",
    "+ ( @xmath138 )  ( @xmath139 )     is the closest to @xmath132 .",
    "unit of @xmath133 is bits / iteration .",
    ", title=\"fig : \" ] + ( d ) @xmath134 and @xmath136 merged ( @xmath140 ) .      at the end of algorithm  [ alg : succsourcegls ]",
    ", we have order-2 approximation ( @xmath60 ) .",
    "we allocate the code @xmath141 to the two partitions .",
    "when we go from @xmath60 to @xmath83 , the two sibling partitions that were merged to form the parent partition will get the codes ` @xmath142 ' and ` @xmath143 ' where ` @xmath144 ' is the codeword of the parent partition .",
    "this process is repeated until we have allocated codewords to @xmath54 .",
    "it is interesting to realize that the codewords are actually symbolic sequences on the standard binary map . by allocating the code @xmath141 to @xmath60 we are essentially treating the two partitions to have equal probabilities",
    "although they may be highly skewed .",
    "in fact , we are approximating the source @xmath60 as a gls with equal partitions ( = 0.5 each ) which is the standard binary map .",
    "the code @xmath145 is thus the symbolic sequence on the standard binary map .",
    "now , moving up from @xmath60 to @xmath83 we are doing the same approximation .",
    "we are treating the two sibling partitions to have equal probabilities and giving them the codes ` @xmath142 ' and ` @xmath143 ' which are the symbolic sequences for those two partitions on the standard binary map . continuing in this fashion",
    ", we see that all the codes are symbolic sequences on the standard binary map .",
    "every alphabet of the source @xmath7 is _ approximated _ to a partition on the binary map and the codeword allocated to it is the corresponding symbolic sequence .",
    "it will be proved that the approximation is minimum redundancy and as a consequence of this , if the probabilities are all powers of 2 , then the approximation is not only minimum redundancy but also equals the entropy of the source ( @xmath146 ) . +",
    "* theorem 2 : ( successive source approximation ) * _ the successive source approximation algorithm using gls yields minimum - redundancy ( i.e. , it minimizes @xmath51 ) . _ + * proof : + * we make the important observation that the successive source approximation algorithm is in fact a re - discovery of the binary huffman coding algorithm  @xcite which is known to minimize @xmath51 and hence yields minimum - redundancy . since our algorithm is essentially a re - discovery of the binary huffman coding algorithm ,",
    "the theorem is proved ( the codewords allocated in the previous section are the same as huffman codes ) .",
    "@xmath131      we have described how by successively approximating the original stochastic i.i.d source using gls , we arrive at a set of codewords for the alphabet which achieves minimum redundancy .",
    "the assignment of symbolic sequences as codewords to the alphabet of the source is the process of encoding .",
    "thus , given a series of observations of @xmath7 , the measuring device represents and stores these as codewords .",
    "for decoding , the reverse process needs to be applied , i.e. , the codewords have to be replaced by the observations .",
    "this can be performed by another device which has a look - up table consisting of the alphabet set and the corresponding codewords which were assigned originally by the measuring device .",
    "we make some important observations / remarks here :    1 .",
    "the faithful modeling of a stochastic i.i.d source as a gls is a very important step .",
    "this ensured that the lyapunov exponent captured the information content ( shannon s entropy ) of the source .",
    "2 .   codewords are symbolic sequences on gls .",
    "we could have chosen a different scheme for giving codewords than the one described here .",
    "for example , we could have chosen symbolic sequences on the tent map as codewords .",
    "this would also correspond to a different set of huffman codes , but with the same average codeword length @xmath51 .",
    "huffman codes are not unique but depend on the way we assign codewords at every level .",
    "3 .   huffman codes are _ symbol codes _ ,",
    "i.e. , each symbol in the alphabet is given a distinct codeword .",
    "we have investigated binary codes in this paper .",
    "an extension to the proposed algorithm is possible for ternary and higher bases .",
    "4 .   in another related work , we have used gls to design _ stream codes_. unlike symbol codes , stream codes encode multiple symbols at a time .",
    "therefore , individual symbols in the alphabet no longer correspond to distinct codewords . by treating the entire message as a symbolic sequence on the gls",
    ", we encode the initial condition which contains the same information .",
    "this achieves optimal lossless compression as demonstrated in  @xcite .",
    "5 .   we have extended gls to piecewise non - linear , yet lebesgue measure preserving discrete chaotic dynamical systems . these have very interesting properties ( such as robust chaos in two parameters ) and are useful for joint compression and encryption applications  @xcite .",
    "source coding problem is motivated as a measurement problem . a stochastic i.i.d source can be faithfully `` embedded '' into a piecewise linear chaotic dynamical system ( gls ) which exhibits interesting properties .",
    "the lyapunov exponent of the gls is equal to shannon s entropy of the i.i.d source .",
    "the measurement problem is addressed by successive source approximation using gls with the nearest lyapunov exponent ( by merging the two least probable states ) . by assigning symbolic sequences as codewords , we re -",
    "discovered the popular huffman coding algorithm  a minimum redundancy symbol code for i.i.d sources .",
    "nithin nagaraj is grateful to prabhakar g. vaidya and kishor g. bhat for discussions on gls .",
    "he is also thankful to the department of science and technology for funding this work as a part of the ph.d .",
    "program at national institute of advanced studies , indian institute of science campus , bangalore .",
    "the author is indebted to nikita sidorov , mathematics dept .",
    ", univ . of manchester , for providing references to cantor s work .",
    "w. ebeling , and m. a. jimnez - montao , math .",
    "biosc . * 52 * , 53 ( 1980 ) ; m. a. jimnez - montao , bull .",
    "biol . * 46 * , 641 ( 1984 ) ; p. e. rapp , i. d. zimmermann , e. p. vining , n. cohen , a. m. albano , and m. a. jimnez - montao , phys .",
    "a * 192 * , 27 ( 1994 ) ; m. a. jimnez - montao , w. ebeling , and t. pschel , preprint arxiv : cond - mat/0204134 [ cond-mat.dis-nn ] ( 2002 ) .",
    "n. nagaraj , and p. g. vaidya , _ in proceedings of intl .",
    "conf . on recent developments in nonlinear dynamics 2009",
    "_ , narosa publishing house , edited by m. daniel and s. rajasekar ( school of physics , bharathidasan university , 2009 ) , p. 393 ; n. nagaraj , ph.d .",
    "thesis , national institute of advanced studies 2009 ."
  ],
  "abstract_text": [
    "<S> in this paper , source coding or data compression is viewed as a measurement problem . given a measurement device with fewer states than the observable of a stochastic source , how can one capture the essential information ? </S>",
    "<S> we propose modeling stochastic sources as piecewise linear discrete chaotic dynamical systems known as generalized lurth series ( gls ) which dates back to georg cantor s work in 1869 . </S>",
    "<S> the lyapunov exponent of gls is equal to the shannon s entropy of the source ( up to a constant of proportionality ) . by successively approximating the source with gls having fewer states ( with the closest lyapunov exponent ) , we derive a binary coding algorithm which exhibits minimum redundancy ( the least average codeword length with integer codeword lengths ) . </S>",
    "<S> this turns out to be a re - discovery of huffman coding , the popular lossless compression algorithm used in the jpeg international standard for still image compression . </S>"
  ]
}