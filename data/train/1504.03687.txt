{
  "article_text": [
    "direct simulations of star clusters have a long history . as algorithms and hardware have improved , larger numbers of stars could be simulated , allowing a more realistic representation of the dynamical evolution of globular star clusters .",
    "@xcite is a state - of - the - art direct @xmath2-body simulation code specifically designed for star clusters .",
    "it uses several algorithms to enhance the computing speed and accuracy , especially for strong interactions that arise from a large fraction of binaries and relatively short relaxation timescales ( @xmath3 myr for typical open clusters and @xmath4 gyr for typical globular clusters ) in collisional and dense stellar systems . here",
    ", the terms `` collisional '' and `` dense '' are not well defined in the literature .",
    "the classical two - body relaxation time , as defined e.g. by @xcite , describes how important distant gravitational two - body encounters are for the orbital motion of stars .",
    "if the relaxation time is very long , a system is denoted as `` collisionless '' ( for example , galactic disks or bulges ) ; the motion of stars is entirely determined by the smooth mean gravitational field of the system .",
    "if the relaxation time is short ( e.g. , shorter than the lifetime of the system ) we denote the cluster as `` collisional '' ( e.g. , globular and open star clusters , nuclear star clusters ) .",
    "if the stellar density is high enough , close two - body gravitational encounters and stellar collisions may occur .",
    "this aspect is crucial when studying `` dense '' star clusters . in",
    "dense and collisional star clusters a correct integration of stellar motions requires pairwise gravitational interactions to be included between many if not all stars in the cluster .",
    "this is the situation for which codes such as are designed .",
    "direct simulation of star clusters can be very time consuming . in a system with @xmath2 particles ,",
    "the full force calculation cost of one particle scales with @xmath5 . with individual time steps for each particle ,",
    "the cost per crossing time ( @xmath6 ) depends on the number of steps per particle ( @xmath7 ) which varies with different time step criteria , integration methods and star cluster properties . @xcite and @xcite found that for the hermite scheme with a time step criterion based on relative force change @xcite , @xmath7 is roughly proportional to @xmath8 for systems with homogeneous density .",
    "thus , when using individual time steps the total computational cost per crossing time of @xmath2 particles scales with @xmath9 . for systems with a power - law density distribution @xmath10 , @xmath7 depends on the power index @xmath11 .",
    "then the cost per crossing time scales with @xmath9 for @xmath12 and @xmath13 for @xmath14 @xcite .",
    "considering the half - mass relaxation timescale , @xmath15 is proportional to @xmath16 @xcite , the computational cost per @xmath15 is @xmath17 for homogeneous systems and for power - law systems with @xmath12 and @xmath18 for @xmath14 .",
    "thus , an efficient parallelization of a direct code is necessary for large particle numbers .",
    "@xcite discussed the fundamental problem that direct numerical simulations of globular star clusters could not be completed for decades if extrapolating the standard evolution of computational hardware ( moore s law ) .",
    "they called for the construction of a special - purpose computer grape , which finally was successfully initiated and completed by their team @xcite . in the following years",
    ", graphical processing units ( gpu ) widely replaced grape ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) and much of the grape software could be ported to gpu @xcite .",
    "@xcite and @xcite discussed several different types of hardware for parallelization and extended to for general parallel supercomputers .",
    "later , @xcite developed a gpu - based parallel force calculation for nbody6 .",
    "as a result , large @xmath2-body simulations ( @xmath19 ) became possible on a single desktop computer or workstation with gpu hardware .",
    "they also implemented the parallel force calculation based on streaming simd extensions ( sse ) and advanced vector extensions ( avx ) for recent cpu architectures . @xcite and @xcite discussed the performance of large simulations with the gpu - accelerated codes @xmath20gpu and a provisional version of nbody6++gpu .    with these parallelization methods",
    ", we can now study star clusters with a number of stars exceeding @xmath21 .",
    "@xcite simulated @xmath22 stars including @xmath23 primordial binaries with initial half - mass radius @xmath24  pc using nbody4 on a grape-6 based computer to investigate core collapse and core oscillation .",
    "later , @xcite studied the multiple stellar - mass black holes in globular clusters by simulating @xmath25 stars including @xmath26 primordial binaries with initial half - mass radius @xmath27  pc using nbody6-gpu . the current largest direct simulation modeling the globular cluster m4 used one computing node including 12 intel xeon x5650 cores ( 2.66 ghz per core ) and 2 nvidia tesla c2050 gpus with 448 cores each ( 1.15 ghz per core ) with @xcite .",
    "this simulation contained @xmath28 stars with @xmath29 binaries and a small half mass radius of @xmath30 pc .",
    "nowadays , we can make an effort to reach one million stars by using parallel supercomputers with gpus .    in this paper",
    ", we first introduce the parallel algorithm used by and in section  [ sec : feature ] .",
    "then we describe the new version of with a hybrid parallel method and also the new algorithms that are necessary for large number of particle parallelization in section  [ sec : new ] .",
    "performance tests are carried out in section  [ sec : performance ] . in section",
    "[ sec : app ] we show an application to a globular cluster with one million stars . in section  [ sec : discussion ] , we discuss the parallelization limit and future development of nbody6++gpu",
    ". finally , we present our conclusions in section  [ sec : conclusion ] .",
    "uses the fourth - order hermite integration method .",
    "@xcite presented a careful analysis of the performance and energy error of the hermite integrator .",
    "he showed that it reduces to the similar asymptotic error behaviour as the standard aarseth scheme ( fourth - order method ; see @xcite ) but it has some advantages in the time step choice and data structure .",
    "the hierarchical block time steps method is used together with the hermite integrator @xcite in nbody6 , which avoids the overheads of particle position and velocity prediction in an individual time step method . in this method ,",
    "particle time steps are adjusted to quantized values , usually an integer power of @xmath31 . then at each time step ,",
    "active particles ( the particles that satisfy the time step criterion ) are integrated together .    to speed up the force calculation , uses the ahmad - cohen ( ac ) neighbor scheme @xcite . the basic idea is to employ a neighbor list for each particle . the integration is separated into two parts : regular force integration for large time steps ( regular steps ) and irregular force integration for small time steps ( irregular steps ) .",
    "the regular force is the summation of the forces from particles outside the neighbor radius and the irregular force accumulates only the neighbor forces . during the irregular step",
    ", the regular force and its first order derivative calculated at the last regular step are used for position and velocity prediction .",
    "the ac scheme gains efficiency with sequential computing ( without parallelization ) .",
    "the speed gained by the ac scheme is roughly proportional to @xmath32 @xcite .",
    "however , in parallel computing , this gain is limited by the complexity of the implementation of this algorithm ( see section  [ sec : performance ] ) . also , the benefit of reducing overheads of particle prediction in the block time step method is strongly limited in the neighbor scheme .",
    "@xcite discussed the integration accuracy requirement for self - gravitating systems simulated with direct codes .",
    "they found that for three - body systems the integration should have total energy conserved better than @xmath33 .",
    "although this accuracy requirement is uncertain when the simulation is extended to large particle number systems , this work indicates the importance of careful integration treatment for direct systems .",
    "one important feature of is that it uses the algorithms of @xcite ( hereafter ks ) and chain regularization @xcite to deal with an accurate solution of close encounters , binaries and multiple systems , which play a significant role in star cluster dynamical evolution .",
    "these strong interactions require very small time steps during integration and may produce large errors with standard integrators such as the hermite scheme . using ks and",
    "chain regularization is also the most important feature of for star cluster simulations .    here",
    ", we have briefly introduced the main algorithm used in nbody6/6++ . in the next section",
    "we will focus on the parallelization of the codes .",
    "@xcite and @xcite developed based on using mpi parallelization with the copy algorithm .",
    "both regular and irregular forces were parallelized . here",
    "different mpi processors calculate different subsets of the active particles .",
    "each mpi processor has the complete particle dataset .",
    "another available parallel algorithm is the ring algorithm which splits the full particle dataset for different mpi processors .",
    "it reduces the memory cost in each mpi process .",
    "the benefit of the copy algorithm compared to the ring algorithm is that there is no requirement for extra communication of the neighbor particle data which is not in the same mpi process during the irregular force calculation .",
    "the disadvantage is the particle number limit due to memory size on the computing node .",
    "the mpi communication with the copy algorithm has constant time cost ( independent of mpi processor number except for latency ) .",
    "the scaling of the regular force with different mpi processors is very good . since the regular force dominates the calculation , this results in a good scaling of the total computing time .",
    "@xcite provided a detailed discussion of these communication algorithms . @xcite and @xcite suggested an efficient communication algorithm ( hypersystolic ) for extremely large processor numbers .",
    "after the gpu computing ( cuda ) became popular , the shared memory parallel code was developed for workstation and desktop computers @xcite .",
    "the openmp , gpu ( cuda ) and avx / sse parallel methods are used to make the code as fast as possible .",
    "however , can only be used in a single node ( no massively parallel mpi implementation ) so the number of particles is limited for a reasonable simulation time .      the gpu library of @xcite is used for calculating the regular force , which dominates the direct integration , and potential energy calculation . the cost for regular force calculation per particle scales with @xmath5 and for potential",
    "energy calculation scales with @xmath34 .",
    "the performance of gpu force calculation is very good since the pure force calculation is easy to parallelize .",
    "gpus also help to accumulate the neighbor list very efficiently during the regular force calculation .",
    "when gpu accelerates the regular force very efficiently , the irregular force becomes expensive .",
    "however , this part is hard to parallelize on gpus due to the complexity of the ac neighbor scheme .",
    "thus , @xcite developed the avx / sse and openmp parallel library for neighbor particle prediction and irregular force calculation .",
    "avx / sse is an instruction set for cpus developed in recent years , which supports vector calculation in the specific cache .",
    "the advantage of avx / sse with openmp is that there is no extra memory copy compared to gpu . for both avx / sse and gpu libraries ,",
    "the data needs to be copied once for changing data structure to obtain computing efficiency .",
    "this is because that has a very long development history , thus to completely change the data structure to be consistent with avx / sse and gpu libraries is very time consuming .",
    "but for gpu , there is extra data copy from the host memory on the mother board to the device memory on gpu .",
    "besides , since the neighbor force calculation is not efficient for the distributed memory parallel method ( with mpi parallelization ; see discussion below ) , this kind of shared memory parallel method is more efficient .",
    "the gpu acceleration , especially of the long - range ( regular ) gravitational forces , is very efficient so this part does not dominate the computational time any more , as we show below . secondly , the avx / sse implementation accelerates prediction and neighbor ( irregular ) forces , which is the next most time consuming part of the code .",
    "we have combined the gpu and avx / sse acceleration , which was done for a single node in nbody6-gpu , with the mpi parallelized nbody6++ designed for multi - node computing clusters for the new version nbody6++gpu .",
    "this work requires additional efforts to keep the code consistent ( see below ) .",
    "in addition , we have worked on remaining bottlenecks , such as time step scheduling and stellar evolution , which become important for million bodies because the usual computationally intensive tasks have been accelerated very effectively by gpu and avx / sse .      for the block time step method",
    ", active particles should be selected at every time step .",
    "it is very expensive to search all particles for the active ones , especially for the irregular force calculation . in this case , for one block time step the cost of selecting active particles scales with @xmath5 while the irregular force calculation cost scale with @xmath35 . if @xmath36 , the former can be more expensive .",
    "when the simulation reaches millions of particles , the block time step levels can be quite deep ( the smallest time step can reach @xmath37 ) and the deep blocks with few particles and small time steps can easily satisfy this condition",
    ". one may consider to use a temporary list to save particles with small time steps and only search all particles at some selected time interval from the temporary list each time step .",
    "however , this method is still expensive where there are many particles with small time steps ( such as the wide binaries that are not ks regularized ) . indeed , we find that the time of selecting active particles can be much larger than the irregular integration time , even with this temporary list algorithm for one million particles including @xmath1 primordial binaries .",
    "another reason that forces us to deal with this issue is that the active particles selection is very difficult to parallelize efficiently ( the cost is almost independent of processor numbers ) and would be prohibitive for a million - body simulation .",
    "thus , we propose a better algorithm that uses a time step sorting list ( hereafter sorting list algorithm ; see figures  [ fig : sortchart ] and [ fig : sortlist ] ) .",
    "@xcite implemented a similar algorithm for @xmath20-grape+gpu and evaluated its performance .",
    "the basic idea is that when we have the index list sorted by particle time step from smallest to largest , and the indicators of each boundary offset @xmath38 between the block of the same step particles ( the largest particle index with step @xmath39 ) , we only need to find the correct offset at each block time step by using the algorithm shown in figure  [ fig : sortchart ] to select active particles ( shown as black squares in figure  [ fig : sortlist ] ) .",
    "after integration , we adjust the sorted list by sorting the active particles new time steps . the specific sorting method for this adjustment",
    "can be optimized to @xmath40 if we ignore the stability of sorting ( stability means no exchange of the order for the particles with same steps ) and assume that many active particles keep the same step as before or have small time step changes .",
    "the initialization of a simulation in is relatively expensive .",
    "we improve it with mpi , gpu and openmp parallelization and a better algorithm .",
    "the initial model for million - body simulations is very important and needs to be carefully tested .",
    "this improvement is very useful for fast testing of the initial models with large particle numbers , especially for a large number of primordial binaries .      1",
    ".   reading or generating masses , positions , velocities and stellar evolution parameters of all stars ; 2 .   scaling all parameters into units ( the unitsnon as h@xmath41non time unit ( d.c .",
    "heggie , private communication ) ] are defined in @xcite ) ; 3 .",
    "initialization of forces , neighbor lists and time steps of all stars ; 4 .",
    "initialization of primordial ks binaries .    in the second part of the intialization ,",
    "the total potential energy of the system is needed and costs @xmath34 . actually , does this calculation twice for scaling purpose in the case of an external tidal field .",
    "the gpu is used in to speed up this part and it is very efficient .",
    "our new improvements are for the third and fourth parts . in the traditional version forces and",
    "neighbor lists are initialized separately without parallelization .",
    "nbody6++ parallelizes the scaling and initialization of the force parts , but only through mpi . for million - body simulations",
    "this is very slow and requires hours to be finished .",
    "we improved it by using gpu based force and neighbor list calculations ( the same as for the regular force calculation ) .",
    "the fourth part is very costly with more than @xmath1 primordial ks binaries in the traditional ( several hours ) . during initialization of ks binaries ,",
    "the force and its three derivatives ( hermite scheme ) need to be renewed for center - of - mass particles .",
    "all neighbor lists that contain ks binary component indices also need to be replaced by the center - of - mass particle indices .",
    "the cost is approximately @xmath42 where @xmath43 is the number of primordial ks binaries .",
    "we find a much simpler way to initialize ks binaries ( cost scales with @xmath44 ) by just switching the order of the third and fourth parts : initialize ks binaries first without recalculating forces , their derivatives and neighbor lists ( only the ks transformation is needed ) and then do the third process with the new center - of - mass particles data generated by former process instead of each binary component in the old way . in this case there is no need to update the forces and neighbor lists .      during the force calculation ,",
    "the predicted positions and velocities are used to calculate the force and its first derivative for the hermite integrator . in principle",
    ", we can avoid prediction of the same particles with the ac neighbor scheme and block time steps .",
    "however , in practice we need to search all neighbors of each active particle and the search itself is computationally expensive .",
    "thus , it does not save much time to avoid neighbor prediction overlap and it is much simpler to predict all neighbors and do the force calculation within one loop .",
    "the disadvantage of this method is that it costs more when the average neighbor number @xmath45 multiplied by the active particle number @xmath46 is larger than the total particle number @xmath2 , compared to all the particle predictions with a non - ac scheme block time step .",
    "one solution is to try predicting all particles once instead of predicting each neighbor when @xmath47 .",
    "but the mixture of predicting only neighbors and predicting all particles increases the complexity of code .",
    "we therefore use only neighbor prediction in the code .",
    "however , there is a major complication for the parallel neighbor prediction in nbody6++gpu , which does not exist in nbody6-gpu .",
    "since we use avx / sse and gpu and the code is mixed with ` cuda ` , ` c++ ` and ` fortran  77 ` programming language , the avx / sse and gpu libraries keep the individual copies of particle datasets .",
    "thus , the predictions of particles have overlaps and are usually inconsistent for different copies distributed on mpi processors . due to the complexity of nbody6/6++ ( e.g. , using predicted positions for regularization ) this leads to problems of synchronization later on , such as differences of time steps for the same particle on different processors . the safest but very costly way is to always predict all particles at every irregular integration step , which is the case in the older versions of nbody6++ . to solve this problem ,",
    "much effort has been made to ensure that every particle is predicted to the current time before it is used in stellar evolution , ks and hierarchical regularization , because these parts are not parallelized and should have the same computing results on every mpi processor .",
    "the neighbor scheme also leads to performance losses for the calculation of stellar evolution .",
    "when a star experiences mass loss , other stars feel a smaller force . in the neighbor scheme ,",
    "the regular force is predicted from the value calculated at the last regular time step , thus if particles outside the neighbor radius experience mass loss between the previous and next regular time steps , the regular force will be inconsistent after that . the correction for the regular force should be done for all particles which have the mass loss particle outside their neighbor radius . to avoid a large value of the third and fourth derivatives of the force",
    ", the irregular force also needs to be updated if the mass loss particle is inside the neighbor radius .",
    "when mass loss is frequent , the calculation performance will be reduced significantly .",
    "we currently use openmp to speed up the force correction , but it can not completely solve this issue since the force correction with cost of @xmath5 per particle can not be avoided .      based on the above parallel methods , we develop a new version of to include hybrid parallel procedures .",
    "the parallel structure of is shown in figure  [ fig : structure ] . in computer clusters ,",
    "each computing node uses one mpi process .",
    "each mpi process opens multiple threads via openmp for the irregular force calculation .",
    "gpus inside one node are controlled by openmp threads .",
    "each gpu has a similar particle dataset size for regular force and potential energy calculation .",
    "gpus of different nodes are isolated without communication .",
    "thus all gpus in the same node together access the complete particle dataset .",
    "the best code configuration is to use multiple cpu cores ( such as @xmath48 cores ) and several gpus ( such as @xmath49 gpus with a few thousand cores ) per node , and choose node numbers based on the total number of particles ."
  ],
  "abstract_text": [
    "<S> accurate direct simulations help to obtain detailed information about the dynamical evolution of star clusters . </S>",
    "<S> they also enable comparisons with analytical models and fokker - planck or monte - carlo methods . </S>",
    "<S> is a well - known direct code for star clusters , and is the extended version designed for large particle number simulations by supercomputers . </S>",
    "<S> we present nbody6++gpu , an optimized version of with hybrid parallelization methods ( mpi , gpu , openmp , and avx / sse ) to accelerate large direct simulations , and in particular to solve the million - body problem . </S>",
    "<S> we discuss the new features of the code , benchmarks , as well as the first results from a simulation of a realistic globular cluster initially containing a million particles . for million - body simulations , </S>",
    "<S> is @xmath0 times faster than with 320 cpu cores and 32 nvidia k20x gpus . with this computing cluster specification </S>",
    "<S> , the simulations of million - body globular clusters including @xmath1 primordial binaries require about an hour per half - mass crossing time .    </S>",
    "<S> [ firstpage ]    methods : numerical  globular clusters : general </S>"
  ]
}