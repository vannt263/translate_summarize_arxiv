{
  "article_text": [
    "the problem of recovering the sparsity pattern of an unknown vector @xmath4that is , the positions of the non - zero entries of @xmath4 based on noisy observations arises in a broad variety of contexts , including subset selection in regression  @xcite , structure estimation in graphical models  @xcite , sparse approximation  @xcite , and signal denoising  @xcite .",
    "a natural optimization - theoretic formulation of this problem is via where the @xmath12 `` norm '' of a vector corresponds to the number of non - zero elements .",
    "unfortunately , however , @xmath12-minimization problems are known to be np - hard in general  @xcite , so that the existence of polynomial - time algorithms is highly unlikely .",
    "this challenge motivates the use of computationally tractable approximations or relaxations to @xmath12 minimization .",
    "in particular , a great deal of research over the past decade has studied the use of the @xmath1-norm as a computationally tractable surrogate to the @xmath12-norm .    in more concrete terms",
    ", suppose that we wish to estimate an unknown but fixed vector @xmath13 on the basis of a set of @xmath14 observations of the form @xmath15 where @xmath16 , and @xmath17 is additive gaussian noise . in many settings ,",
    "it is natural to assume that the vector @xmath4 is _ sparse _ , in that its _ support _",
    "@xmath18 has relatively small cardinality @xmath19 .",
    "given the observation model   and sparsity assumption  , a reasonable approach to estimating @xmath4 is by solving the @xmath1-constrained quadratic program ( qp ) @xmath20 where @xmath21 is a regularization parameter .",
    "of interest are conditions on the _ ambient dimension _ @xmath2 , the _ sparsity index _ @xmath22 , and the _ number of observations _",
    "@xmath14 for which it is possible ( or impossible ) to recover the support set @xmath23 of @xmath4 .",
    "given the substantial literature on the use of @xmath1 constraints for sparsity recovery and subset selection , we provide only a very brief ( and hence necessarily incomplete ) overview here . in the _ noiseless version _ ( @xmath24 ) of the linear observation model  , one can imagine estimating @xmath4 by solving the problem @xmath25 this problem is in fact a linear program ( in disguise ) , and corresponds to a method in signal processing known as basis pursuit , pioneered by chen et al .  @xcite .",
    "for the noiseless setting , the interesting regime is the underdetermined setting ( i.e. , @xmath26 ) . with contributions from a broad range of researchers  ( * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * ; * ? ? ? * e.g. , ) , there is now a fairly complete understanding of conditions on deterministic vectors @xmath27 and sparsity index @xmath3 for which the true solution @xmath4 can be recovered exactly . without going into technical details ,",
    "the rough idea is that the _ mutual incoherence _ of the vectors @xmath27 must be large relative to the sparsity index @xmath3 , and indeed we impose similar conditions to derive our results ( e.g. , conditions   and   in the sequel ) .",
    "most closely related to the current paper  as we discuss in more detail in the sequel  are recent results by donoho  @xcite , as well as candes and tao  @xcite that provide high probability results for random ensembles .",
    "more specifically , as independently established by both sets of authors using different methods , for uniform gaussian ensembles ( i.e. , @xmath28 ) with the ambient dimension @xmath2 scaling linearly in terms of the number of observations ( i.e. , @xmath29 , for some @xmath30 ) , there exists a constant @xmath31 such that all sparsity patterns with @xmath32 can be recovered with high probability .",
    "there is also a substantial body of work focusing on the noisy setting ( @xmath33 ) , and the use of quadratic programming techniques for sparsity recovery  ( * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "? * ; * ? ? ?",
    "* ; * ? ? ?",
    "* e.g. , ) .",
    "the @xmath1-constrained quadratic program  , also known as the lasso  @xcite , has been the focus of considerable research in recent years .",
    "knight and fu  @xcite analyze the asymptotic behavior of the optimal solution , not only for @xmath1 regularization but for @xmath34-regularization with @xmath35 $ ] .",
    "fuchs  @xcite investigates optimality conditions for the constrained qp  , and provides deterministic conditions , of the mutual incoherence form , under which a sparse solution , which is known to be within @xmath36 of the observed values , can be recovered exactly . among a variety of other results , both tropp  @xcite and donoho et al .",
    "@xcite also provide sufficient conditions for the support of the optimal solution to the constrained qp   to be contained within the true support of @xmath4 .",
    "most directly related to the current paper is recent work by both meinshausen and buhlmann  @xcite , focusing on gaussian noise , and extensions by zhao and yu  @xcite to more general noise distributions , on the use of the lasso for model selection . for the case of gaussian noise ,",
    "both papers established that under mutual incoherence conditions and appropriate choices of the regularization parameter @xmath37 , the lasso can recover the sparsity pattern with probability converging to one for particular regimes of @xmath5 , @xmath2 and @xmath3 , when @xmath38 drawn randomly from random gaussian ensembles .",
    "we discuss connections to our results at more length in the the sequel .",
    "recall the linear observation model  . for compactness in notation , let us use @xmath39 to denote the @xmath40 matrix formed with the vectors @xmath41 as rows , and the vectors @xmath42 as columns , as follows : @xmath43 consider the ( random ) set @xmath44 of optimal solutions to this constrained quadratic program  . by convexity and boundedness of the cost function ,",
    "the solution set is always non - empty . for any vector @xmath45",
    ", we define the sign function @xmath46 of interest is the event that the lasso   succeeds in recovering the sparsity pattern of the unknown @xmath4 :    property @xmath47 : : :    there exists an optimal solution    @xmath48 with the    property    our main result is that for a broad class of random gaussian ensembles based on covariance matrices satisfying mutual incoherence conditions , there exist fixed constants @xmath49 and @xmath50 such that for all @xmath8 , property @xmath47 holds with high probability ( over the choice of noise vector @xmath51 and random matrix @xmath39 ) whenever @xmath52 and _ conversely _ , fails to hold with high probability whenever @xmath53 moreover , for the special case of the uniform gaussian ensemble ( i.e. , @xmath28 ) , we show that so that the threshold is sharp .",
    "this threshold result has a number of connections to previous work in the area that focuses on special forms of scaling .",
    "more specifically , as we discuss in more detail in section  [ seccomp ] , in the special case of linear scaling ( i.e. , @xmath54 for some @xmath55 ) , this theorem provides a noisy analog of results previously established for basis pursuit in the noiseless case  @xcite .",
    "moreover , our result can also be adapted to an entirely different scaling regime for @xmath56 and @xmath3 , as considered by a separate body of recent work  @xcite on the high - dimensional lasso .",
    "the remainder of this paper is organized as follows .",
    "we begin in section  [ secprelim ] with some necessary and sufficient conditions , based on standard optimality conditions for convex programs , for property @xmath47 to hold .",
    "we then prove a consistency result for the case of deterministic design matrices @xmath39 .",
    "section  [ secmain ] is devoted to the statement and proof of our main result on the asymptotic behavior of the lasso for random gaussian ensembles .",
    "we illustrate this result via simulation in section  [ secexperimental ] , and conclude with a discussion in section  [ secdiscussion ] .",
    "in this section , we provide necessary and sufficient conditions for property @xmath47 to hold . based on these conditions ,",
    "we then define collections of random variables that play a central role in our analysis .",
    "in particular , the study of @xmath47 is reduced to the study of the extreme order statistics of these random variables .",
    "we then state and prove a result about the behavior of the lasso for the case of a deterministic design matrix @xmath39 .",
    "we begin with a simple set of necessary and sufficient conditions for property @xmath57 to hold .",
    "we note that this result is not essentially new ( e.g. , see  @xcite for variants ) , and follows in a straightforward manner from optimality conditions for convex programs  @xcite ; see appendix  [ appnecsuff ] for further details .",
    "we define @xmath58 to be the support of @xmath4 , and let @xmath59 be its complement . for any subset @xmath60 ,",
    "let @xmath61 be the @xmath62 matrix with the vectors @xmath63 as columns .",
    "[ lemnecsuffcond ] assume that the matrix @xmath64 is invertible .",
    "then , for any given @xmath65 and noise vector @xmath66 , property @xmath67 holds if and only if    @xmath68 - \\frac{1}{{\\ensuremath{n } } } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s^c}}}}}^t { \\ensuremath{w}}\\right| & \\leq & \\lambda , \\quad \\mbox{and } \\\\ \\label{eqnpropb } \\left | { \\ensuremath{\\beta^*}}_{\\ensuremath{s}}+ \\left(\\frac{1}{{\\ensuremath{n } } } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } \\right)^{-1 } \\left[\\frac{1}{{\\ensuremath{n } } } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{w}}- \\lambda { \\ensuremath{\\operatorname{sgn}}}({\\ensuremath{\\beta^*}}_{\\ensuremath{s } } ) \\right ] \\right| & > & 0,\\end{aligned}\\ ] ]    where both of these vector inequalities should be taken elementwise . for shorthand , define @xmath69 , and denote by @xmath70 the vector with @xmath71 in the @xmath72 position , and zeroes elsewhere .",
    "motivated by lemma  [ lemnecsuffcond ] , much of our analysis is based on the collections of random variables , defined each index @xmath73 and @xmath74 as follows :    @xmath75 \\\\ \\label{eqndefnvvar } { \\ensuremath{v}}_j & \\defn & { \\ensuremath{x}}_j^t \\biggr \\ { { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } \\left({\\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } \\right)^{-1 } \\lambda_{{\\ensuremath{n } } } { \\ensuremath{\\vec{b}\\,}}- \\left[{\\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}\\left({\\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } \\right)^{-1 } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t - i_{n \\times",
    "n } \\right ] \\frac{{\\ensuremath{w}}}{{\\ensuremath{n } } } \\biggr \\}.\\end{aligned}\\ ] ]    recall that @xmath76 and @xmath77 . from lemma  [ lemnecsuffcond ] ,",
    "the behavior of @xmath47 is determined by the behavior of @xmath78 and @xmath79 holds . on the other hand ,",
    "if we define @xmath80 , then the event @xmath81 is sufficient to guarantee that condition   holds .",
    "consequently , our proofs are based on analyzing the asymptotic probability of these two events .",
    "we now show how lemma  [ lemnecsuffcond ] can be used to analyze the behavior of the lasso for the special case of a deterministic ( non - random ) design matrix @xmath39 .",
    "to gain intuition for the conditions in the theorem statement , it is helpful to consider the _ zero - noise condition _ @xmath82 , in which each observation @xmath83 is uncorrupted . in this case",
    ", the conditions of lemma  [ lemnecsuffcond ] reduce to    @xmath84    of course , if the conditions of lemma  [ lemnecsuffcond ] fail to hold in the zero - noise setting , then there is little hope of succeeding in the presence of noise .",
    "the zero - noise conditions motivate imposing the following set of conditions on the design matrix :    [ eqnkeycond ] @xmath85 $ , and } \\\\",
    "\\label{eqnkeycondb } { \\ensuremath{\\lambda_{min}}}(\\frac{1}{{\\ensuremath{n } } } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } ) \\geq { \\ensuremath{c_{min } } } > 0,\\end{aligned}\\ ] ]    where @xmath86 denotes the minimal eigenvalue . under these conditions",
    ", we have the following : [ propdetdesign ] suppose that we observe @xmath87 , where each column @xmath88 of @xmath39 is normalized to @xmath89-norm @xmath5 , and @xmath90 .",
    "assume @xmath4 and @xmath39 satisfy conditions  , and define if @xmath91 is chosen such that @xmath92 then @xmath93 as @xmath94 . before proving the proposition",
    ", we pause to make a number of comments .",
    "first , conditions of the form   have been considered in previous work on the lasso  @xcite . in particular , various authors  @xcite provide examples and results on matrix families that satisfy this type of condition . moreover",
    ", previous work  @xcite provides asymptotic results for particular scalings of @xmath2 , @xmath3 and @xmath5 for random design matrices , as we discuss in more detail in section  [ secmain ] . to the best of our knowledge ,",
    "proposition  [ propdetdesign ] is the first result to provide sufficient conditions for exact recovery in deterministic designs with general scaling of @xmath2 , @xmath3 and @xmath5 .",
    "second , it is worthwhile to consider proposition  [ propdetdesign ] in the classical setting ( i.e. , in which the number of samples @xmath94 with @xmath2 and @xmath3 remaining fixed ) . in this setting , the quantity @xmath95 does not depend on @xmath5 .",
    "hence , in addition to the condition  , the requirements reduce to @xmath96 and @xmath97 . note that @xmath98 is one suitable choice .",
    "this classical case is also covered by previous work  @xcite .",
    "last , consider the more general setting where all three parameters @xmath99 grow to infinity , and suppose for simplicity that @xmath100 stays bounded away from @xmath101 .",
    "the conditions @xmath102 and @xmath103 imply that the number of observations @xmath5 must grow at a rate faster than @xmath104 . in the following section , in which we consider the more general case of random gaussian ensembles",
    ", we will see that for ensembles satisfying mutual incoherence conditions , we in fact require that @xmath105 .      recall the events @xmath106 and @xmath107 defined in equations   and   respectively . to establish the claim , we must show that that @xmath108 \\rightarrow 0 $ ] , where @xmath109 and @xmath110 denote the complements of these events . by union",
    "bound , it suffices to show both @xmath111 $ ] and @xmath112 $ ] converge to zero , or equivalently that @xmath113 $ ] and @xmath114 $ ] both converge to one .",
    "[ [ analysis - of - ensuremathmathcalmensuremathv ] ] analysis of @xmath106 : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we begin by establishing that @xmath113 \\rightarrow 1 $ ] . throughout the proof",
    ", we use the shorthand @xmath115 and @xmath116 .    recalling the definition   of the random variables @xmath117 , note that @xmath106 holds holds if and only @xmath118 and @xmath119 .",
    "moreover , we note that each @xmath117 is gaussian with mean @xmath120 & = & \\lambda_{{\\ensuremath{n } } } { \\ensuremath{x}}_j^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } \\left({\\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } \\right)^{-1 } { \\ensuremath{{\\ensuremath{\\vec{b}\\,}}}}.\\end{aligned}\\ ] ] using condition  , we have @xmath121 for all indices @xmath122 , from which we obtain that @xmath123 where @xmath124 { \\ensuremath{w}}$ ] are zero - mean ( correlated ) gaussian variables .",
    "hence , in order to establish condition   of lemma  [ lemnecsuffcond ] , we need to show that @xmath125 \\rightarrow 0.\\ ] ] in fact , using lemma  [ lemsimpleinequal ] ( see appendix  [ appaux ] ) , it is sufficient to show that . by applying markov s inequality and gaussian comparison results  @xcite ( see lemma  [ lemgeneric ] in appendix  [ appgausscomp ] ) ,",
    "we obtain @xmath126 \\ ; \\leq \\ ; \\frac{\\exs[\\max_{j \\in { \\ensuremath{s^c } } }    |{\\ensuremath{\\wtil{{\\ensuremath{v}}}}}_j|]}{\\lambda_{{\\ensuremath{n } } } } \\ ; \\leq \\ ; \\frac{3 \\sqrt{\\log n }    } { \\lambda_{{\\ensuremath{n } } } } \\max_j \\sqrt{\\exs [ { \\ensuremath{\\wtil{{\\ensuremath{v}}}}}_j^2]}.\\ ] ] straightforward computation yields that @xmath127 = \\frac{{\\ensuremath{\\sigma^2}}}{{\\ensuremath{n}}^2 } \\ ; { \\ensuremath{x}}_j^t \\left [      i_{{\\ensuremath{n}}\\times { \\ensuremath{n } } } - { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } \\left({\\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t      { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } \\right)^{-1 } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t \\right ] { \\ensuremath{x}}_j \\ ; \\leq      \\frac{{\\ensuremath{\\sigma^2}}}{{\\ensuremath{n}}^2 } \\|{\\ensuremath{x}}_j\\|^2 \\ ; = \\ ;      \\frac{{\\ensuremath{\\sigma^2}}}{{\\ensuremath{n}}},\\ ] ] since the matrix @xmath128 has maximum eigenvalue equal to one , and @xmath129 by construction . consequently ,",
    "condition ( a ) in the theorem statement  namely , that @xmath130 is sufficient to ensure that @xmath131/\\lambda_{{\\ensuremath{n } } } \\rightarrow 0 $ ] .",
    "thus , we have established @xmath132 ( i.e. , that condition   holds w.p .",
    "one as @xmath133 ) .",
    "[ [ analysis - of - ensuremathmathcalmensuremathu ] ] analysis of @xmath107 : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we now show that @xmath134 . beginning with the triangle inequality , we upper bound @xmath135 \\|_\\infty$ ] as @xmath136 let @xmath137 denote the unit vector with one in position @xmath138 and zeroes elsewhere .",
    "now define , for each index @xmath73 , the gaussian random variable @xmath139 .",
    "each such @xmath140 is a zero - mean gaussian with variance given by @xmath141 hence , by a standard gaussian comparison theorem  @xcite ( in particular , see lemma  [ lemgeneric ] in appendix  [ appgausscomp ] ) , we have @xmath142 & = & \\exs \\left[\\left \\| ( \\frac{1}{{\\ensuremath{n } } } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } ) ^{-1 } \\frac{1}{{\\ensuremath{n } } } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t \\wsca \\right \\|_\\infty \\right ] \\\\ & \\leq & 3 \\sqrt{\\frac{{\\ensuremath{\\sigma^2}}\\log { \\ensuremath{s}}}{{\\ensuremath{n}}{\\ensuremath{c_{min}}}}}.\\end{aligned}\\ ] ] thus , recalling the defining @xmath143 \\right| > 0 \\right ] & \\leq & \\prob \\left [ \\frac{1}{\\rho_n } \\max_{1 \\leq i \\leq { \\ensuremath{s } } } |{\\ensuremath{u}}_i| > 1 \\right ] \\\\ & \\leq & \\prob \\left [ \\frac{1}{\\rho_n } \\left \\ { \\max_{1 \\leq i \\leq { \\ensuremath{s } } } |{\\ensuremath{z}}_i| + \\lambda_{{\\ensuremath{n } } } \\| ( \\frac{1}{{\\ensuremath{n } } } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}})^{-1 } \\|_\\infty \\right \\ } > 1 \\right ] \\\\ & \\leq & \\frac{1}{\\rho_n } \\left \\ { \\exs \\left[\\max_{1 \\leq i \\leq      { \\ensuremath{s } } } |{\\ensuremath{z}}_i|\\right ] +   \\lambda_{{\\ensuremath{n } } } \\|      ( \\frac{1}{{\\ensuremath{n } } } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}})^{-1 } \\|_\\infty \\right \\ }      \\\\ & \\leq & \\frac{1}{\\rho_n } \\ ; \\left \\ { 3 \\sqrt{\\frac{{\\ensuremath{\\sigma^2}}\\log { \\ensuremath{s}}}{{\\ensuremath{n}}{\\ensuremath{c_{min } } } } } + \\lambda_{{\\ensuremath{n } } } \\| ( \\frac{1}{{\\ensuremath{n } } } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}})^{-1 } \\|_\\infty \\right \\},\\end{aligned}\\ ] ] which converges to zero as @xmath94 , using condition ( b ) in the theorem statement .",
    "we now turn to the analysis of random design matrices @xmath39 , in which each row @xmath38 is chosen as an i.i.d .",
    "gaussian random vector with covariance matrix @xmath144 .",
    "in particular , we prove the existence of thresholds that provide a sharp description of the failure / success of the lasso as a function of @xmath145 .",
    "we begin by setting up and providing a precise statement of the main result , and then discussing its connections to previous work . in the later part of this section",
    ", we provide the proof .",
    "consider a covariance matrix @xmath144 with unit diagonal , and with its minimum and maximum eigenvalues ( denoted @xmath86 and @xmath146 respectively ) bounded as @xmath147 for constants @xmath148 and @xmath149 .",
    "given a vector @xmath0 , define its support @xmath150 , as well as the complement @xmath59 of its support .",
    "suppose that @xmath144 and @xmath23 satisfy the conditions @xmath151 for some @xmath152 , and @xmath153 for some @xmath154 $ ] . under these conditions ,",
    "we consider the observation model @xmath155 where @xmath156 and @xmath157 are independent gaussian variables for @xmath158 .",
    "furthermore , we define latexmath:[$\\rho_{\\ensuremath{n}}\\defn \\min_{i \\in { \\ensuremath{s } } }    @xmath76 .    [ thmgennoisegauss ]",
    "consider a sequence of covariance matrices @xmath160 \\}$ ] and solution vectors @xmath161\\}$ ] satisfying conditions   and  . under the observation model  , consider a sequence @xmath162 such that @xmath3 , @xmath163 and @xmath164 tend to infinity .",
    "define the thresholds @xmath165 then for any constant @xmath8 , we have the following    1 .",
    "if @xmath166 , then @xmath167 \\rightarrow 0 $ ] for any non - increasing sequence @xmath168 .",
    "conversely , if @xmath169 , and @xmath91 is chosen such that @xmath170 \\ ; \\rightarrow \\ ; 0,\\ ] ] then @xmath171 \\rightarrow 1 $ ] .",
    "* remark : * suppose for simplicity that @xmath100 remains bounded away from @xmath101 .",
    "in this case , the requirements on @xmath172 reduce to @xmath173 , and @xmath174 .",
    "one suitable choice is @xmath175 , with which we have @xmath176 and @xmath177 without a bound on @xmath100 , the second condition in equation   constrains the rate of decrease of the minimum      to develop intuition for this result , we begin by stating certain special cases as corollaries , and discussing connections to previous work .",
    "first , we consider the special case of the uniform gaussian ensemble , in which @xmath178 .",
    "previous work by donoho  @xcite as well as candes and tao  @xcite has focused on the uniform gaussian ensemble in the the noiseless ( @xmath24 ) and underdetermined setting ( @xmath54 for some @xmath179 ) . analyzing the asymptotic behavior of the linear program   for recovering @xmath4 ,",
    "the basic result is that there exists some @xmath31 such that all sparsity patterns with @xmath180 can be recovered with high probability .",
    "applying theorem  [ thmgennoisegauss ] to the noisy version of this problem , the uniform gaussian ensemble means that we can choose @xmath181 , and @xmath182 , so that the threshold constants reduce @xmath183 consequently , theorem  [ thmgennoisegauss ] provides a sharp threshold for the behavior of the lasso , in that failure / success is entirely determined by whether or not @xmath184 .",
    "thus , if we consider the particular linear scaling analyzed in previous work on the noiseless case  @xcite , we have : [ cor1 ] suppose that @xmath54 for some @xmath179",
    ". then    1 .",
    "if @xmath185 for any @xmath186 , then @xmath187 \\rightarrow 0 $ ] for any positive sequence @xmath168 .",
    "2 .   on the other hand ,",
    "if @xmath188 , then @xmath187 \\rightarrow 1 $ ] for any sequence @xmath189 satisfying the conditions of theorem  [ thmgennoisegauss](a ) .",
    "conversely , suppose that the size @xmath3 of the support of @xmath4 scales linearly with the number of parameters @xmath2 .",
    "the following result describes the amount of data required for the @xmath1-constrained qp to recover the sparsity pattern in the noisy setting ( @xmath33 ) : suppose that @xmath185 for some @xmath190 .",
    "then we require @xmath191 + \\alpha { \\ensuremath{p}}$ ] in order to obtain exact recovery with probability converging to one for large problems .",
    "these two corollaries establish that there is a significant difference between recovery using basis pursuit   in the noiseless setting versus recovery using the lasso   in the noisy setting .",
    "when the amount of data @xmath5 scales only linearly with ambient dimension @xmath2 , then the presence of noise means that the recoverable support size drops from a linear fraction ( i.e. , @xmath185 as in the work  @xcite ) to a sublinear fraction ( i.e. , @xmath192 , as in corollary  [ cor1 ] ) .",
    "we now consider more general ( non - uniform ) gaussian ensembles that satisfy conditions   and  .",
    "as mentioned earlier , previous papers by both meinshausen and buhlmann  @xcite as well as zhao and yu  @xcite treat model selection with the high - dimensional lasso .",
    "for suitable covariance matrices ( e.g. , satisfying conditions   and  ) , both sets of authors proved that the sparsity pattern can be recovered exactly under scaling conditions of the form @xmath193 applying theorem  [ thmgennoisegauss ] in this scenario , we have the following : under the scaling  , the lasso will recover the sparsity pattern with probability converging to one . substituting the conditions   into the threshold condition",
    ", we obtain that the rhs takes the form @xmath194 + o({\\ensuremath{n}}^{c_1 } ) \\\\ & = & o({\\ensuremath{n}}^{c_1 + c_2 } ) \\ ; \\ll \\ ; { \\ensuremath{n}},\\end{aligned}\\ ] ] since @xmath195 by assumption .",
    "thus , we see that under these conditions , our threshold condition   is satisfied _",
    "a fortiori_. in fact , under this stronger scaling  , both papers  @xcite proved that the probability of exact recovery converges to one at a rate exponential in some polynomial function of @xmath5 .",
    "interestingly , our results show that the lasso can recover the sparsity pattern for a much broader range of @xmath99 scaling .",
    "we now turn to the proof of part ( b ) of our main result . as with the proof of proposition  [ propdetdesign ] , the proof is based on analyzing the collections of random variables @xmath196 and @xmath197 , as defined in equations   and   respectively .",
    "we begin with some preliminary results that serve to set up the argument .",
    "we first note that for @xmath198 , the random gaussian matrix @xmath199 will have rank @xmath3 with probability one , whence the matrix @xmath64 is invertible with probability one .",
    "accordingly , the necessary and sufficient conditions of lemma  [ lemnecsuffcond ] are applicable .",
    "our first lemma , proved in appendix  [ appgausscond ] , concerns the behavior of the random vector @xmath200 , when conditioned on @xmath199 and @xmath201 .",
    "recalling the shorthand notation @xmath202 , we summarize in the following [ lemgausscond ] conditioned on @xmath199 and @xmath201 , the random vector @xmath203 is gaussian .",
    "its mean vector is upper bounded as @xmath204 \\right | & \\leq & \\lambda_{{\\ensuremath{n } } } ( 1-{\\ensuremath{\\epsilon } } ) \\ , \\ones.\\end{aligned}\\ ] ] moreover , its conditional covariance takes the form @xmath205 & = & { \\ensuremath{m_{\\ensuremath{n}}}}{\\ensuremath{\\sigma}}_{({\\ensuremath{s^c}}\\ , | \\ , { \\ensuremath{s } } ) } \\ ; = \\ ; { \\ensuremath{m_{\\ensuremath{n}}}}\\ , \\big[{\\ensuremath{\\sigma}}_{{\\ensuremath{s^c}}{\\ensuremath{s^c } } } - { \\ensuremath{\\sigma}}_{{\\ensuremath{s^c}}{\\ensuremath{s } } } ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 }    { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s^c } } } \\big],\\end{aligned}\\ ] ] where @xmath206 { \\ensuremath{w}}\\end{aligned}\\ ] ] is a random scaling factor .",
    "the following lemma , proved in appendix  [ appextstats ] , captures the behavior of the random scaling factor @xmath207 defined in equation  : [ lemextstats ] the random variable @xmath207 has mean @xmath208 & = & \\frac{\\lambda_{{\\ensuremath{n}}}^2}{{\\ensuremath{n}}-{\\ensuremath{s}}-1 } \\ , { \\ensuremath{\\vec{b}\\,}}^t ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } { \\ensuremath{\\vec{b}\\,}}+ \\frac{{\\ensuremath{\\sigma^2}}\\ , ( { \\ensuremath{n}}-{\\ensuremath{s}})}{{\\ensuremath{n}}^2}.\\end{aligned}\\ ] ] moreover , it is sharply concentrated in that for any @xmath209 , we have @xmath210 \\big | \\geq \\delta \\exs[{\\ensuremath{m_{\\ensuremath{n } } } } ] \\right ] & \\rightarrow & 0 \\qquad \\qquad \\mbox{as $ { \\ensuremath{n}}\\rightarrow + \\infty$.}\\end{aligned}\\ ] ]      with these preliminary results in hand , we now turn to analysis of the collections of random variables @xmath211 and @xmath212 .    [ [ analysis - of - ensuremathmathcalmensuremathv-1 ] ] analysis of @xmath106 : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we begin by analyzing the behavior of @xmath213 . first , for a fixed but arbitrary @xmath209 , define the event @xmath214| \\geq \\delta \\exs[{\\ensuremath{m_{\\ensuremath{n } } } } ] \\}$ ] . by conditioning on @xmath215 and its complement @xmath216^c$ ] , we have the upper bound @xmath217 & \\leq & \\prob \\left[\\max_{j \\in { \\ensuremath{s^c } } } |{\\ensuremath{v}}_j| > \\lambda_{{\\ensuremath{n } } } \\ , \\mid \\ , [ { \\ensuremath{\\mathcal{t}}}(\\delta)]^c \\right ] + \\prob[{\\ensuremath{\\mathcal{t}}}(\\delta)].\\end{aligned}\\ ] ] by the concentration statement in lemma  [ lemextstats ] , we have @xmath218 \\rightarrow 0 $ ] , so that it suffices to analyze the first term .",
    "set @xmath219 $ ] , and let @xmath220 be a zero - mean gaussian vector with @xmath221 . @xmath222 \\\\ & \\leq & ( 1-{\\ensuremath{\\epsilon } } ) \\lambda_{{\\ensuremath{n } } } + \\max_{j \\in { \\ensuremath{s^c } } } |{\\ensuremath{z}}_j|,\\end{aligned}\\ ] ] where we have used the upper bound   on the mean .",
    "this inequality establishes the inclusion of events @xmath223 thereby showing that it suffices to prove that @xmath224^c ] \\rightarrow 0 $ ] .",
    "note that conditioned on @xmath216^c$ ] , the maximum value of @xmath207 is @xmath225 $ ] .",
    "since gaussian maxima increase with increasing variance , we have @xmath226^c \\right ] & \\leq & \\prob \\left[\\max_{j \\in { \\ensuremath{s^c } } } |{\\ensuremath{\\wtil{{\\ensuremath{z}}}}}_j| > { \\ensuremath{\\epsilon}}\\lambda_{{\\ensuremath{n } } } \\right],\\end{aligned}\\ ] ] where @xmath227 is zero - mean gaussian with covariance @xmath228 .    using lemma  [ lemsimpleinequal ]",
    ", it suffices to show that @xmath229 $ ] converges to zero .",
    "accordingly , we complete this part of the proof via the following two lemmas , both of which are proved in appendix  [ apponetwo ] : [ lemone ] under the stated assumptions of the theorem , we have @xmath230 and @xmath231 & \\leq & { \\ensuremath{\\epsilon}}.\\end{aligned}\\ ] ] [ lemtwo ] for any @xmath232 , we have @xmath233 \\right ] & \\leq & \\exp \\left ( -\\frac{\\eta^2}{2 { \\ensuremath{v^ * } } } \\right).\\end{aligned}\\ ] ] lemma  [ lemone ] implies that for all @xmath209 , we have @xmath234 \\leq ( 1+\\frac{\\delta}{2 } ) { \\ensuremath{\\epsilon}}\\lambda_{{\\ensuremath{n}}}$ ] for all @xmath14 sufficiently large .",
    "therefore , setting @xmath235 in the bound  , we have for fixed @xmath236 and @xmath14 sufficiently large : @xmath237 & \\leq & \\prob \\left [ \\max_{j \\in { \\ensuremath{s^c } } } { \\ensuremath{\\wtil{{\\ensuremath{z}}}}}_j > \\frac{\\delta}{2 } \\lambda_{{\\ensuremath{n } } } { \\ensuremath{\\epsilon}}+ \\exs[\\max_{j \\in { \\ensuremath{s^c } } } { \\ensuremath{\\wtil{{\\ensuremath{z}}}}}_j ] \\right ] \\\\",
    "& \\leq & 2 \\exp \\left(- \\frac{\\delta^2 \\lambda_{{\\ensuremath{n}}}^2 { \\ensuremath{\\epsilon}}^2}{8 { \\ensuremath{v^ * } } } \\right).\\end{aligned}\\ ] ] from lemma  [ lemone ] , we have @xmath238 , which implies that @xmath239 \\rightarrow 0 $ ] for all @xmath209 . by the arbitrariness of @xmath209",
    ", we thus have @xmath240 \\rightarrow 1 $ ] , thereby establishing that property   of lemma  [ lemnecsuffcond ] holds w.p .",
    "one asymptotically .",
    "[ [ analysis - of - ensuremathu_i ] ] analysis of @xmath241 : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    next we prove that @xmath242 with probability one as @xmath94 .",
    "conditioned on @xmath199 , the only random component in @xmath243 is the noise vector @xmath201 .",
    "a straightforward calculation yields that this conditioned rv is gaussian , with mean and variance @xmath244 & = & -\\lambda_{{\\ensuremath{n } } } e_i^t \\left(\\frac{1}{{\\ensuremath{n } } } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } \\right)^{-1 } { \\ensuremath{\\vec{b}\\ , } } , \\\\",
    "{ \\ensuremath{y'}}_i \\ ; \\defn \\ ; \\var[{\\ensuremath{u}}_i \\ ; \\mid \\ ; { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } ] & = & \\frac{{\\ensuremath{\\sigma^2}}}{{\\ensuremath{n } } } e_i^t   \\left [ \\frac{1}{{\\ensuremath{n } } } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } \\right]^{-1 } e_i,\\end{aligned}\\ ] ] respectively .",
    "the following lemma , proved in appendix  [ appubehave ] , is key to our proof : [ lemubehave ] ( a ) the random variables @xmath245 and @xmath246 have means @xmath247",
    "= \\frac{-\\lambda_{\\ensuremath{n}}\\ ; { \\ensuremath{n}}}{{\\ensuremath{n}}-{\\ensuremath{s}}-1 } e_i^t \\ ; ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } \\ , { \\ensuremath{\\vec{b}\\ , } } , \\qquad \\mbox{and } \\qquad \\exs[{\\ensuremath{y'}}_i ] \\ ; = \\ ; \\frac{{\\ensuremath{\\sigma^2}}}{{\\ensuremath{n}}-{\\ensuremath{s}}-1 } \\ ; e_i^t ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } e_i,\\ ] ] respectively , which are bounded as @xmath248 \\ : \\leq \\ :   \\frac{{\\ensuremath{\\sigma^2}}{\\ensuremath{d_{\\operatorname{max}}}}}{{\\ensuremath{n}}- { \\ensuremath{s}}-1}.\\ ] ] ( b ) moreover , each pair @xmath249 is sharply concentrated , in that we have @xmath250 \\biggr ] & \\leq & \\frac{k}{{\\ensuremath{n}}- { \\ensuremath{s}}},\\end{aligned}\\ ] ] where @xmath251 is a fixed constant independent of @xmath5 and @xmath3 .",
    "+ we exploit this lemma as follows .",
    "first define the event",
    "@xmath252 \\biggr \\}.\\end{aligned}\\ ] ] by the union bound and lemma  [ lemubehave](b ) , we have @xmath253 \\ ; \\leq \\ ; { \\ensuremath{s}}\\frac{k}{{\\ensuremath{n}}-{\\ensuremath{s } } } \\nonumber \\ ; = \\ ; \\frac{k}{\\frac{{\\ensuremath{n}}}{{\\ensuremath{s } } } - 1 } \\ ; \\rightarrow \\ ; 0,\\ ] ] since @xmath254 as @xmath94 . for convenience in notation , for any @xmath255 and @xmath256 , we use @xmath257 to denote a gaussian random variable with mean @xmath258 and variance @xmath259 . conditioning on the event @xmath215 and its complement , we have @xmath260 & \\leq & \\prob[\\max_{i \\in { \\ensuremath{s } } } { \\ensuremath{u}}_i > \\rho_{\\ensuremath{n}}\\ ; \\mid \\ ; { \\ensuremath{\\mathcal{t}}}(\\delta)^c ] + \\prob[{\\ensuremath{\\mathcal{t}}}(\\delta ) ] \\nonumber \\\\ \\label{eqndoubletail } & \\leq & \\prob[\\max_{i \\in { \\ensuremath{s } } } { \\ensuremath{u}}_i({\\ensuremath{\\mu^*}}_i , { \\ensuremath{v^*}}_i ) > \\rho_{\\ensuremath{n } } ] + \\frac{k}{\\frac{{\\ensuremath{n}}}{{\\ensuremath{s } } } - 1},\\end{aligned}\\ ] ] where each @xmath261 is gaussian with mean @xmath262 and variance @xmath263 $ ] respectively . in asserting the inequality  , we have used the fact that the probability of the event @xmath264 increases as the mean and variance of @xmath245 increase . continuing the argument , we have @xmath265 & \\leq & \\prob[\\max_{i \\in { \\ensuremath{s } } } |{\\ensuremath{u}}_i({\\ensuremath{\\mu^*}}_i , { \\ensuremath{v^*}}_i)| > \\rho_{\\ensuremath{n } } ] \\\\ & \\leq & \\frac{1}{\\rho_{\\ensuremath{n } } } \\exs \\left [ \\max_{i \\in { \\ensuremath{s } } }    where the last step uses markov s inequality .",
    "we now decompose @xmath266 , and write @xmath267 & \\leq & 2 { \\ensuremath{d_{\\operatorname{max}}}}\\ , \\lambda_{\\ensuremath{n}}\\frac{{\\ensuremath{n}}}{{\\ensuremath{n}}-{\\ensuremath{s}}-1 } + \\exs \\left [ \\max_{i \\in { \\ensuremath{s } } }    with this decomposition , we use the bound   on @xmath268 $ ] and lemma  [ lemgeneric ] on gaussian maxima ( see appendix  [ appgausscomp ] ) to conclude that @xmath269 & \\leq & \\frac{1}{\\rho_{\\ensuremath{n } } } \\left [ 2 { \\ensuremath{d_{\\operatorname{max}}}}\\ , \\lambda_{\\ensuremath{n}}\\frac{{\\ensuremath{n}}}{{\\ensuremath{n}}-{\\ensuremath{s}}-1 } + 3 \\sqrt { \\frac{2 { \\ensuremath{\\sigma^2}}\\ , { \\ensuremath{d_{\\operatorname{max}}}}\\log { \\ensuremath{s}}}{{\\ensuremath{n}}- { \\ensuremath{s}}-1 } } \\right],\\end{aligned}\\ ] ] which converges to zero by the second condition   in the theorem statement .",
    "we establish the claim by proving that under the stated conditions , @xmath270 with probability one , for any positive sequence @xmath168 .",
    "we begin by writing @xmath271 + { \\ensuremath{\\wtil{{\\ensuremath{v}}}}}_j$ ] , where @xmath272 is zero - mean .",
    "now @xmath273| \\\\ & \\geq & \\max_{j \\in { \\ensuremath{s^c } } } |{\\ensuremath{v}}_j| - ( 1 - { \\ensuremath{\\epsilon } } ) \\lambda_{{\\ensuremath{n}}}\\end{aligned}\\ ] ] where have used lemma  [ lemgausscond ] .",
    "consequently , the event @xmath274 implies the event @xmath275 , so that @xmath276 & \\geq & \\prob[\\max_{j \\in { \\ensuremath{s^c } } } |{\\ensuremath{\\wtil{{\\ensuremath{v}}}}}_j| > ( 2-{\\ensuremath{\\epsilon } } ) \\ , \\lambda_{\\ensuremath{n}}].\\end{aligned}\\ ] ]    from the preceding proof of theorem  [ thmgennoisegauss](b ) , we know that conditioned on @xmath199 and @xmath201 , the random vector @xmath277 is gaussian with covariance of the form @xmath278 $ ] ; thus , the zero - mean version @xmath279 has the same covariance .",
    "moreover , lemma  [ lemextstats ] guarantees that the random scaling term @xmath207 is sharply concentrated .",
    "in particular , defining for any @xmath209 the event @xmath280| \\geq \\delta \\exs[{\\ensuremath{m_{\\ensuremath{n } } } } ] \\}$ ] , we have @xmath218 \\rightarrow 0 $ ] , and the bound @xmath281 & \\geq & ( 1-\\prob[{\\ensuremath{\\mathcal{t}}}(\\delta ) ] ) \\ ; \\prob \\left [ \\max_{j \\in { \\ensuremath{s^c } } } |{\\ensuremath{\\wtil{{\\ensuremath{v}}}}}_j| > ( 2-{\\ensuremath{\\epsilon } } ) \\ , \\lambda_{{\\ensuremath{n } } } \\ ; \\mid \\ ; { \\ensuremath{\\mathcal{t}}}(\\delta)^c \\right ] \\\\ & \\geq & ( 1 - \\prob[{\\ensuremath{\\mathcal{t}}}(\\delta ) ] ) \\ ; \\prob \\left [ \\max_{j \\in { \\ensuremath{s^c } } }    |{\\ensuremath{z}}_j({\\ensuremath{v^*}})| > ( 2-{\\ensuremath{\\epsilon } } ) \\ , \\lambda_{{\\ensuremath{n } } } \\right],\\end{aligned}\\ ] ] where each @xmath282 is the conditioned version of @xmath272 with the scaling factor @xmath207 fixed to @xmath283 $ ] .",
    "( here we have used the fact that the probability of gaussian maxima decreases as the variance decreases , and that @xmath284 when conditioned on @xmath285 . )",
    "our proof proceeds by first analyzing the expected value , and then exploiting gaussian concentration of measure .",
    "we summarize the key results in the following : [ lemexpinfinity ] under the stated conditions , one of the following two conditions must hold :    1 .   either @xmath286 , and there exists some @xmath55 such that @xmath287 \\geq ( 2-{\\ensuremath{\\epsilon } } ) \\left[1 + \\gamma\\right]$ ] for all sufficiently large @xmath5 , or 2 .",
    "there exist constants @xmath288 such that @xmath289 and @xmath287 \\geq \\gamma \\sqrt{\\log { \\ensuremath{n}}}$ ] for all sufficiently large @xmath5",
    ".    [ lemgaussconcenlower ] for any @xmath232 , we have @xmath290 - \\eta ] & \\leq & \\exp \\left ( -\\frac{\\eta^2}{2 { \\ensuremath{v^ * } } } \\right).\\end{aligned}\\ ] ] using these two lemmas , we complete the proof as follows .",
    "first , if condition ( a ) of lemma  [ lemexpinfinity ] holds , then we set @xmath291 in equation   to obtain that @xmath292 & \\geq & 1- \\exp    \\left ( -\\frac{(2-{\\ensuremath{\\epsilon}})^2 \\ , \\gamma^2 \\lambda_{{\\ensuremath{n}}}^2 } { 8    { \\ensuremath{v^ * } } } \\right).\\end{aligned}\\ ] ] this probability converges to @xmath71 since @xmath293 from lemma  [ lemexpinfinity](a ) .",
    "on the other hand , if condition ( b ) holds , then we use the bound @xmath287 \\geq \\gamma \\sqrt{\\log { \\ensuremath{n}}}$ ] and set @xmath294 in equation   to obtain @xmath295 & \\geq & \\prob [ \\frac{1}{\\lambda_{{\\ensuremath{n } } } } \\max_{j \\in { \\ensuremath{s^c } } } { \\ensuremath{z}}_j({\\ensuremath{v^ * } } ) \\geq \\frac{\\gamma \\sqrt{\\log { \\ensuremath{n}}}}{2 } \\ , ] \\\\ & \\geq & 1- \\exp \\left ( -\\frac{\\gamma^2 \\lambda_{{\\ensuremath{n}}}^2 \\log { \\ensuremath{n}}}{8 { \\ensuremath{v^ * } } } \\right).\\end{aligned}\\ ] ] this probability also converges to @xmath71 since @xmath296 and @xmath297 .",
    "thus , in either case , we have shown that @xmath298 = 1 $ ] , thereby completing the proof of theorem  [ thmgennoisegauss](a ) .",
    "in this section , we provide some simulations to confirm the threshold behavior predicted by theorem  [ thmgennoisegauss ] .",
    "we consider the following three types of sparsity indices :    1 .",
    "_ linear sparsity _ , meaning that @xmath299 for some @xmath190 ; 2 .",
    "_ sublinear sparsity _ , meaning that @xmath300 for some @xmath190 , and 3 .",
    "_ fractional power _",
    "sparsity , meaning that @xmath301 for some @xmath302 .",
    "for all three types of sparsity indices , we investigate the success / failure of the lasso in recovering the sparsity pattern , where the number of observations scales as @xmath303 .",
    "the _ control parameter _",
    "@xmath304 is varied in the interval @xmath305 .",
    "for all results shown here , we fixed @xmath306 for all three ensembles , and set @xmath307 for the fractional power ensemble .",
    "in addition , we set @xmath308 in all cases .",
    "we begin by considering the uniform gaussian ensemble , in which each row @xmath38 is chosen in an i.i.d .",
    "manner from the multivariate @xmath309 distribution .",
    "recall that for the uniform gaussian ensemble , the critical value is @xmath310 .",
    "figure  [ figresultsid ] plots the control parameter @xmath304 versus the probability of success , for linear sparsity ( a ) , sublinear sparsity pattern ( b ) , and fractional power sparsity ( c ) , for three different problem sizes ( @xmath311 ) .",
    "each point represents the average of @xmath312 trials .",
    "[ cols=\"^,^,^ \" , ]     figure  [ figresultsrho ] shows representative results for this toeplitz family with @xmath313 .",
    "panel ( a ) corresponds to linear sparsity @xmath185 with @xmath306 ) , and panel ( b ) corresponds to sublinear sparsity ( @xmath314 with @xmath306 ) .",
    "each panel shows three curves , corresponding to the problem sizes @xmath311 , and each point on each curve represents the average of @xmath312 trials .",
    "the vertical lines to the left and right of @xmath315 represent the theoretical upper and lower bounds on the threshold ( @xmath316 and @xmath317 respectively in this case ) .",
    "once again , these simulations show good agreement with the theoretical predictions .",
    "the problem of recovering the sparsity pattern of a high - dimensional vector @xmath4 from noisy observations has important applications in signal denoising , graphical model selection , sparse approximation , and subset selection .",
    "this paper focuses on the behavior of @xmath1-regularized quadratic programming , also known as the lasso , for estimating such sparsity patterns in the noisy and high - dimensional setting .",
    "the main contribution of this paper is to establish a set of general and sharp conditions on the observations @xmath5 , the sparsity index @xmath3 ( i.e. , number of non - zero entries in @xmath4 ) , and the ambient dimension @xmath2 that characterize the success / failure behavior of the lasso in the high - dimensional setting , in which @xmath5 , @xmath2 and @xmath3 all tend to infinity . for the uniform gaussian ensemble ,",
    "our threshold result is sharp , whereas for more general gaussian ensembles , it should be possible to tighten the analysis given here .",
    "we would like to thank noureddine el karoui and bin yu for helpful comments and pointers .",
    "this work was partially supported by an alfred p. sloan foundation fellowship , and an intel corporation equipment grant .",
    "by standard conditions for optimality in a convex program  @xcite , the point @xmath318 is optimal if and only if there exists a subgradient @xmath319 such that @xmath320 here the subdifferential of the @xmath1 norm takes the form @xmath321 substituting our observation model @xmath322 and re - arranging yields @xmath323 now condition @xmath324 holds if and only we have @xmath325 holds if and only if    @xmath326    using the invertibility of @xmath64 , we may solve for @xmath327 and @xmath328 to conclude that    @xmath329 - \\frac{1}{{\\ensuremath{n } } } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s^c}}}}}^t { \\ensuremath{w}}\\\\ { \\ensuremath{\\widehat{\\beta}}}_{\\ensuremath{s } } & = & { \\ensuremath{\\beta^*}}_{\\ensuremath{s}}+ \\left(\\frac{1}{{\\ensuremath{n } } } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } \\right)^{-1 } \\left[\\frac{1}{{\\ensuremath{n } } } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{w}}- \\lambda { \\ensuremath{\\operatorname{sgn}}}({\\ensuremath{\\beta^*}}_{\\ensuremath{s } } ) \\right].\\end{aligned}\\ ] ]    from these relations , the conditions @xmath330 and @xmath331 yield conditions   and   respectively .",
    "we state here ( without proof ) some well - known comparison results on gaussian maxima  @xcite .",
    "we begin with a crude but useful bound : [ lemgeneric ] for any gaussian random vector @xmath332 , we have @xmath333 next we state ( a version of ) the sudakov - fernique inequality  @xcite : [ lemsudfer ] let @xmath334 and @xmath335 be gaussian random vectors such that for all @xmath336 @xmath337 & \\leq & \\exs[(x_i - x_j)^2].\\end{aligned}\\ ] ] then @xmath338 \\leq \\exs[\\max \\limits_{1 \\leq i \\leq n } x_i]$ ] .",
    "for future use , we state formally the following elementary [ lemsimpleinequal ] given a collection @xmath339 of zero - mean random variables , for any constant @xmath340 we have    @xmath341 & \\leq & \\prob[\\max_{1 \\leq j \\leq { \\ensuremath{n } } } { \\ensuremath{z}}_j \\leq a ] , \\qquad \\mbox{and } \\\\",
    "\\label{eqnboundb } \\prob [ \\max_{1 \\leq j \\leq { \\ensuremath{n } } } |{\\ensuremath{z}}_j| > a ] & \\leq & 2 \\prob[\\max_{1 \\leq j \\leq { \\ensuremath{n } } } { \\ensuremath{z}}_j > a].\\end{aligned}\\ ] ]    the first inequality is trivial . to establish the inequality  , we write @xmath342 & = & \\prob [ ( \\max_{1 \\leq j \\leq { \\ensuremath{n } } } { \\ensuremath{z}}_j > a ) \\ ; \\mbox{or } \\ ; ( \\min_{1 \\leq j \\leq { \\ensuremath{n } } } { \\ensuremath{z}}_j < -a ) ] \\\\",
    "& \\leq & \\prob [ \\max_{1 \\leq j \\leq { \\ensuremath{n } } } { \\ensuremath{z}}_j > a ] + \\prob[\\min_{1 \\leq j \\leq { \\ensuremath{n } } } { \\ensuremath{z}}_j < -a ] \\\\ & = & 2 \\prob [ \\max_{1 \\leq j \\leq { \\ensuremath{n } } } { \\ensuremath{z}}_j > a],\\end{aligned}\\ ] ] where we have used the union bound , and the symmetry of the events @xmath343 and @xmath344 .",
    "conditioned on both @xmath199 and @xmath201 , the only random component in @xmath117 is the column vector @xmath88 . using standard llse formula  ( * ? ? ?",
    "* e.g. , ) ( i.e. , for estimating @xmath345 on the basis of @xmath199 ) , the random variable @xmath346 is gaussian with mean and covariance    @xmath347 & = & { \\ensuremath{\\sigma}}_{{\\ensuremath{s^c}}{\\ensuremath{s } } } ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t , \\\\ \\var({\\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s^c } } } } } \\ , \\mid \\ , { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } ) & = & { \\ensuremath{{\\ensuremath{\\sigma}}_{({\\ensuremath{s^c}}|   s)}}}\\ ; = \\ ; { \\ensuremath{\\sigma}}_{{\\ensuremath{s^c}}{\\ensuremath{s^c } } } - { \\ensuremath{\\sigma}}_{{\\ensuremath{s^c}}{\\ensuremath{s } } } ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s^c}}}.\\end{aligned}\\ ] ]    consequently , we have @xmath348 \\right| & = & \\left | { \\ensuremath{\\sigma}}_{{\\ensuremath{s^c}}{\\ensuremath{s } } } ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t \\biggr \\ { { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } \\left({\\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } \\right)^{-1 } \\lambda_{{\\ensuremath{n } } } { \\ensuremath{\\vec{b}\\,}}- \\left[{\\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}\\left({\\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } \\right)^{-1 } { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t - i_{{\\ensuremath{n}}\\times { \\ensuremath{n } } } \\right ] \\frac{{\\ensuremath{w}}}{{\\ensuremath{n } } } \\biggr \\ } \\right | \\nonumber \\\\ & = & \\left | { \\ensuremath{\\sigma}}_{{\\ensuremath{s^c}}{\\ensuremath{s } } } ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } \\lambda_{{\\ensuremath{n } } } { \\ensuremath{\\vec{b}\\,}}\\right| \\nonumber \\\\ & \\leq & \\lambda_{{\\ensuremath{n } } } ( 1-{\\ensuremath{\\epsilon } } ) \\ones,\\end{aligned}\\ ] ] as claimed .",
    "we begin by computing the expected value .",
    "since @xmath350 is wishart with matrix @xmath351 , the random matrix @xmath352 is inverse wishart with mean @xmath353 = \\frac { ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } } { n - s -1}$ ] ( see lemma 7.7.1 of anderson  @xcite ) .",
    "hence we have @xmath354 = \\frac{\\lambda_{{\\ensuremath{n}}}^2 } { n - s-1 } \\ ; { \\ensuremath{\\vec{b}\\,}}^t ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } { \\ensuremath{\\vec{b}\\,}}.\\ ] ] now define the random matrix @xmath355 . a straightforward calculation yields that @xmath356 , so that all the eigenvalues of @xmath357 are either @xmath101 or @xmath71 .",
    "in particular , for any vector @xmath358 in the range of @xmath199 , we have @xmath359 { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } u \\ ; = \\ ; 0.\\ ] ] hence @xmath360 .",
    "since @xmath357 is symmetric and positive semidefinite , there exists an orthogonal matrix @xmath361 such that @xmath362 , where @xmath363 is diagonal with @xmath364 ones , and @xmath22 zeros .",
    "the random matrices @xmath363 and @xmath361 are both independent of @xmath201 , since @xmath199 is independent of @xmath201 .",
    "hence we have @xmath365 & = & \\frac{1}{{\\ensuremath{n}}^2 } \\exs \\left [ { \\ensuremath{w}}^t u^t d u { \\ensuremath{w}}\\ ; \\mid { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } \\right ] \\nonumber \\\\ \\label{eqnrexp } & = & \\frac{1}{{\\ensuremath{n}}^2 } \\trace d u u^t \\exs\\left [ { \\ensuremath{w}}{\\ensuremath{w}}^t \\mid   \\ ; { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } \\right ] \\nonumber \\\\   \\label{eqncalcwrw } & = & { \\ensuremath{\\sigma^2}}\\ , \\frac{{\\ensuremath{n}}-{\\ensuremath{s}}}{{\\ensuremath{n}}^2}\\end{aligned}\\ ] ] since @xmath366 = { \\ensuremath{\\sigma^2}}i$ ] .",
    "consequently , we have established that @xmath367 = \\frac{\\lambda_{{\\ensuremath{n}}}^2}{{\\ensuremath{n}}-{\\ensuremath{s}}-1 } \\ ; { \\ensuremath{\\vec{b}\\,}}^t ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } { \\ensuremath{\\vec{b}\\,}}+ \\frac{{\\ensuremath{\\sigma^2}}\\ , ( { \\ensuremath{n}}-{\\ensuremath{s}})}{{\\ensuremath{n}}^2}$ ] as claimed .",
    "we now compute the expected value of the squared variance @xmath368 ^ 2 } + \\underbrace{2 \\frac{\\lambda_{{\\ensuremath{n}}}^2}{{\\ensuremath{n}}^2 } \\left[{\\ensuremath{\\vec{b}\\,}}^t \\left ( { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } \\right)^{-1 } { \\ensuremath{\\vec{b}\\,}}\\right ] \\left ( { \\ensuremath{w}}^t r { \\ensuremath{w}}\\right ) } + \\underbrace{\\frac{1}{{\\ensuremath{n}}^4 } \\left({\\ensuremath{w}}^t r { \\ensuremath{w}}\\right)^2 } \\\\ & & \\qquad \\qquad { \\ensuremath{t}}_1 \\qquad \\qquad \\qquad \\qquad \\qquad { \\ensuremath{t}}_2 \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad { \\ensuremath{t}}_3\\end{aligned}\\ ] ] first , conditioning on @xmath199 and using the eigenvalue decomposition @xmath363 of @xmath357 , we have @xmath369 & = & \\frac{1}{{\\ensuremath{n}}^4 } \\exs[({\\ensuremath{w}}^t d { \\ensuremath{w}})^2 ] \\nonumber \\\\ & = & \\frac{1}{{\\ensuremath{n}}^4 } \\exs\\left[(\\sum_{i=1}^{{\\ensuremath{n}}-{\\ensuremath{s } } } { \\ensuremath{w}}_i)^2\\right ] \\nonumber \\\\ \\label{eqnm3 } & = & \\frac{2 ( { \\ensuremath{n}}- { \\ensuremath{s } } ) { \\ensuremath{\\sigma^4}}}{{\\ensuremath{n}}^4 } + \\frac{({\\ensuremath{n}}- { \\ensuremath{s}})^2 { \\ensuremath{\\sigma^4}}}{{\\ensuremath{n}}^4}.\\end{aligned}\\ ] ] whence @xmath370 = \\frac{2 ( { \\ensuremath{n}}- { \\ensuremath{s } } ) { \\ensuremath{\\sigma^4}}}{{\\ensuremath{n}}^4 } + \\frac{({\\ensuremath{n}}- { \\ensuremath{s}})^2 { \\ensuremath{\\sigma^4}}}{{\\ensuremath{n}}^4}$ ] as well .    similarly , using conditional expectation and our previous calculation   of @xmath371 $ ] , we have @xmath372 & = & \\frac{2 \\lambda_{\\ensuremath{n}}^2}{{\\ensuremath{n}}^2 } \\exs \\biggr [ \\exs \\big[{\\ensuremath{\\vec{b}\\,}}^t ( { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}})^{-1 } { \\ensuremath{\\vec{b}\\,}}\\ ; ( { \\ensuremath{w}}^t r { \\ensuremath{w } } ) \\ ; \\mid \\ ; { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s } } } } } \\big ] \\biggr ] \\nonumber \\\\ & = & \\frac{2 \\lambda_{\\ensuremath{n}}^2 \\ , ( { \\ensuremath{n}}- { \\ensuremath{s } } ) { \\ensuremath{\\sigma^2}}}{{\\ensuremath{n}}^2 } \\exs \\big[{\\ensuremath{\\vec{b}\\,}}^t ( { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}}^t { \\ensuremath{{\\ensuremath{x}}_{{\\ensuremath{s}}}}})^{-1 } { \\ensuremath{\\vec{b}\\,}}\\big ] \\nonumber \\\\ & = & \\frac{2 \\lambda_{{\\ensuremath{n}}}^2 \\ , ( { \\ensuremath{n}}- { \\ensuremath{s } } ) \\ , { \\ensuremath{\\sigma^2}}}{{\\ensuremath{n}}^2 \\ , ( { \\ensuremath{n}}-{\\ensuremath{s}}-1 ) } { \\ensuremath{\\vec{b}\\,}}^t \\left({\\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s } } } \\right)^{-1 } { \\ensuremath{\\vec{b}\\,}},\\end{aligned}\\ ] ] where the final step uses lemma 7.7.1 of anderson  @xcite on the expectation of inverse wishart matrices .",
    "lastly , since @xmath352 is inverse wishart with matrix @xmath373 , we can use formula for second moments of inverse wishart matrices ( see , e.g. , siskind  @xcite ) to write , for all @xmath374 , @xmath375 & = & \\frac{\\lambda_{{\\ensuremath{n}}}^4}{({\\ensuremath{n}}-{\\ensuremath{s}})\\ ,   ( { \\ensuremath{n}}-{\\ensuremath{s}}-3 ) } \\ ; \\left[{\\ensuremath{\\vec{b}\\,}}^t ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 }   { \\ensuremath{\\vec{b}\\,}}\\right]^2 \\left \\ { 1 + \\frac{1}{{\\ensuremath{n}}-{\\ensuremath{s}}-1 } \\right \\}.\\end{aligned}\\ ] ] consequently , combining our results , we have @xmath376 - ( \\exs[{\\ensuremath{m_{\\ensuremath{n}}}}])^2 \\nonumber \\\\ & = & \\sum_{i=1}^3 \\exs[{\\ensuremath{t}}_i ] - \\left\\ { \\frac{{\\ensuremath{\\sigma^4}}({\\ensuremath{n}}- { \\ensuremath{s}})^2}{{\\ensuremath{n}}^4 } + 2 \\frac{{\\ensuremath{\\sigma^2}}({\\ensuremath{n}}-{\\ensuremath{s}})}{{\\ensuremath{n}}^2 } \\frac {    \\lambda_{{\\ensuremath{n}}}^2}{{\\ensuremath{n}}-{\\ensuremath{s}}-1 } \\ ; { \\ensuremath{\\vec{b}\\,}}^t ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } { \\ensuremath{\\vec{b}\\,}}+ \\left(\\frac{\\lambda_{{\\ensuremath{n}}}^2}{{\\ensuremath{n}}-{\\ensuremath{s}}-1 } \\ ; { \\ensuremath{\\vec{b}\\,}}^t ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } { \\ensuremath{\\vec{b}\\,}}\\right)^2 \\right\\ } \\nonumber \\\\ \\label{eqnhvarr } & = & \\underbrace{\\frac{2 ( { \\ensuremath{n}}-{\\ensuremath{s } } ) { \\ensuremath{\\sigma^4}}}{{\\ensuremath{n}}^4 } } + \\underbrace{\\frac{\\lambda_{{\\ensuremath{n}}}^4 \\ ; [ { \\ensuremath{\\vec{b}\\,}}^t ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } { \\ensuremath{\\vec{b}\\,}}]^2}{({\\ensuremath{n}}-{\\ensuremath{s}}-1 ) \\ , ( { \\ensuremath{n}}-{\\ensuremath{s}}-3 ) } \\left \\ { \\frac{1}{({\\ensuremath{n}}-{\\ensuremath{s } } ) } + \\frac{{\\ensuremath{n}}-{\\ensuremath{s}}-1}{({\\ensuremath{n}}-{\\ensuremath{s } } ) } - \\frac{({\\ensuremath{n}}- { \\ensuremath{s}}-3)}{({\\ensuremath{n}}-{\\ensuremath{s}}-1 ) } \\right \\ } } . \\\\ & & \\qquad { \\ensuremath{h}}_1 \\qquad \\qquad \\qquad \\qquad \\qquad { \\ensuremath{h}}_2 \\nonumber\\end{aligned}\\ ] ]    finally , we establish the concentration result . using chebyshev s inequality ,",
    "we have @xmath377| \\geq \\delta \\exs[{\\ensuremath{m_{\\ensuremath{n } } } } ] \\right ] & \\leq & \\frac{\\var({\\ensuremath{m_{\\ensuremath{n}}}})}{\\delta^2 ( \\exs[{\\ensuremath{m_{\\ensuremath{n}}}}])^2 } , \\end{aligned}\\ ] ] so that it suffices to prove that @xmath378)^2 \\rightarrow 0 $ ] as @xmath379",
    ". we deal with each of the two variance terms @xmath380 and @xmath381 in equation   separately .",
    "first , we have @xmath382)^2 } & \\leq & \\frac{2 ( { \\ensuremath{n}}-{\\ensuremath{s } } ) { \\ensuremath{\\sigma^4}}}{n^4 } \\frac{n^4}{({\\ensuremath{n}}-{\\ensuremath{s}})^2 { \\ensuremath{\\sigma^4 } } } \\ ; = \\frac{2}{{\\ensuremath{n}}-{\\ensuremath{s } } } \\ ; \\rightarrow \\ ; 0.\\end{aligned}\\ ] ] secondly , denoting @xmath383 for short - hand , we have @xmath384)^2 } & \\leq & \\frac{({\\ensuremath{n}}-{\\ensuremath{s}}-1)^2}{\\lambda_{{\\ensuremath{n}}}^4 a^2 } \\frac{\\lambda_{{\\ensuremath{n}}}^4 a^2}{({\\ensuremath{n}}-{\\ensuremath{s}}-1 ) \\ , ( { \\ensuremath{n}}-{\\ensuremath{s}}-3 ) } \\ ; \\left \\ { \\frac{1}{({\\ensuremath{n}}-{\\ensuremath{s } } ) } + \\frac{{\\ensuremath{n}}-{\\ensuremath{s}}-1}{({\\ensuremath{n}}-{\\ensuremath{s } } ) } - \\frac{({\\ensuremath{n}}-{\\ensuremath{s}}-3)}{({\\ensuremath{n}}-{\\ensuremath{s}}-1 ) } \\right \\ } \\\\ & = & \\frac{({\\ensuremath{n}}-{\\ensuremath{s}}-1)}{({\\ensuremath{n}}-{\\ensuremath{s}}-3 ) } \\left \\ { \\frac{1}{({\\ensuremath{n}}-{\\ensuremath{s } } ) } + \\frac{{\\ensuremath{n}}-{\\ensuremath{s}}-1}{({\\ensuremath{n}}-{\\ensuremath{s } } ) }    - \\frac{({\\ensuremath{n}}-{\\ensuremath{s}}-3)}{({\\ensuremath{n}}-{\\ensuremath{s}}-1 ) } \\right \\},\\end{aligned}\\ ] ] which also converges to @xmath101 as @xmath385 .",
    "recall that the gaussian random vector @xmath386 is zero - mean with covariance @xmath387 , where @xmath388 .",
    "for any index @xmath138 , let @xmath389 be equal to @xmath71 in position @xmath138 , and zero otherwise .",
    "for any two indices @xmath390 , we have @xmath391 & = & { \\ensuremath{v^*}}(e_i - e_j)^t { \\ensuremath{{\\ensuremath{\\sigma}}_{({\\ensuremath{s^c}}|   s)}}}(e_i - e_j ) \\\\ & \\leq & 2 { \\ensuremath{v^*}}\\lambda_{max}({\\ensuremath{{\\ensuremath{\\sigma}}_{({\\ensuremath{s^c}}|   s ) } } } ) \\\\ & \\leq & 2 { \\ensuremath{c_{max}}}{\\ensuremath{v^*}},\\end{aligned}\\ ] ] since @xmath392 by definition , and @xmath393 .",
    "letting @xmath394 , we have @xmath395 = 2 { \\ensuremath{c_{max}}}{\\ensuremath{v^*}}$ ] .",
    "hence , applying the sudakov - fernique inequality  @xcite yields @xmath396 \\leq \\exs [ \\max_j x_j ] $ ] . by asymptotic behavior of i.i.d .",
    "gaussians  @xcite , we have @xmath397}{\\sqrt { 2 { \\ensuremath{c_{max}}}{\\ensuremath{v^*}}\\log { \\ensuremath{n } } } } = 1 $ ] .",
    "consequently , for all @xmath398 , there exists an @xmath399 such that for all @xmath400 , we have @xmath401 & \\leq & \\frac{1}{\\lambda_{{\\ensuremath{n } } } } \\exs[\\max_j x_j ] \\\\ &",
    "\\leq & ( 1+{\\ensuremath{\\delta ' } } ) \\ ; \\sqrt{\\frac{2 { \\ensuremath{c_{max}}}{\\ensuremath{v^*}}\\log { \\ensuremath{n}}}{\\lambda_{{\\ensuremath{n}}}^2 } } \\\\ & = & ( 1 + { \\ensuremath{\\delta ' } } ) \\ ; \\sqrt{1 + \\delta } \\ ;",
    "\\sqrt{\\frac{2 { \\ensuremath{c_{max}}}\\log { \\ensuremath{n}}}{{\\ensuremath{n}}-{\\ensuremath{s}}-1 } { \\ensuremath{\\vec{b}\\,}}^t ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } { \\ensuremath{\\vec{b}\\,}}+ \\frac{2 { \\ensuremath{c_{max}}}{\\ensuremath{\\sigma^2}}\\ , ( 1-\\frac{{\\ensuremath{s}}}{{\\ensuremath{n } } } ) \\log { \\ensuremath{n}}}{n \\lambda_{{\\ensuremath{n}}}^2 } } \\\\ & \\leq & ( 1+{\\ensuremath{\\delta ' } } ) \\ ; \\sqrt{1 + \\delta } \\ ; \\sqrt{\\frac{2 { \\ensuremath{c_{max}}}\\ ; { \\ensuremath{s}}\\log { \\ensuremath{n}}}{{\\ensuremath{n}}-{\\ensuremath{s}}-1 } \\frac{1}{{\\ensuremath{c_{min } } } } + \\frac{2 { \\ensuremath{c_{max}}}{\\ensuremath{\\sigma^2}}\\ , \\log { \\ensuremath{n}}}{n \\lambda_{{\\ensuremath{n}}}^2 } } .\\end{aligned}\\ ] ] now , applying our condition bounding @xmath402 via @xmath403 and @xmath7 , we have @xmath401 & < & ( 1+{\\ensuremath{\\delta ' } } ) \\ ; \\sqrt{1+\\delta } \\ ; \\sqrt{{\\ensuremath{\\epsilon}}^2 \\ , \\left(1 - \\frac{{\\ensuremath{\\nu}}\\log { \\ensuremath{n}}}{{\\ensuremath{n}}-{\\ensuremath{s}}-1 } \\right ) + \\frac{2 { \\ensuremath{c_{max}}}{\\ensuremath{\\sigma^2}}\\ , \\log { \\ensuremath{n}}}{{\\ensuremath{n}}\\lambda_{{\\ensuremath{n}}}^2 } } .\\end{aligned}\\ ] ] recall that by assumption , as @xmath404 , we have that @xmath405 and @xmath406 converge to zero .",
    "consequently , the rhs converges to @xmath407 as @xmath408 .",
    "hence , we have @xmath409 & < & ( 1+{\\ensuremath{\\delta ' } } ) \\ ; \\sqrt{1+\\delta } \\ ; { \\ensuremath{\\epsilon}}.\\end{aligned}\\ ] ] since @xmath410 and @xmath411 were arbitrary , the result follows .",
    "we now bound the lipschitz constant of @xmath417 .",
    "let @xmath418 . for each @xmath419 and index @xmath420 , we have @xmath421_j - [ \\sqrt { { \\ensuremath{v^*}}r}v ] _",
    "j \\right| & \\leq & \\sqrt{{\\ensuremath{v^ * } } } \\left| \\sum_{k } r_{jk } [ w_k - v_k ] \\right| \\\\ & \\leq & \\sqrt{{\\ensuremath{v^ * } } } \\sqrt{\\sum_{k } r_{jk}^2 } \\ ; \\|w - v\\|_2 \\\\ & \\leq & \\sqrt{{\\ensuremath{v^ * } } } \\| w - v\\|_2,\\end{aligned}\\ ] ] where the last inequality follows since @xmath422_{jj } \\leq 1 $ ] . therefore , by gaussian concentration of measure for lipschitz functions  @xcite",
    ", we conclude that for any @xmath232 , it holds that @xmath423 + \\eta ] & \\leq & \\exp \\left ( -\\frac{\\eta^2}{2 { \\ensuremath{v^ * } } } \\right ) , \\qquad \\mbox{and } \\\\ \\prob [ f(w ) \\leq \\exs[f(w ) ] - \\eta ] & \\leq & \\exp \\left ( -\\frac{\\eta^2}{2 { \\ensuremath{v^ * } } } \\right).\\end{aligned}\\ ] ]      since the matrix @xmath64 is wishart with @xmath5 degrees of freedom , using properties of the inverse wishart distribution , we have @xmath353 = \\frac { ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } } { { \\ensuremath{n}}-{\\ensuremath{s}}-1}$ ] ( see lemma 7.7.1 of anderson  @xcite ) .",
    "thus , we compute    @xmath424 & = &   \\frac{-\\lambda_{\\ensuremath{n}}\\ ; { \\ensuremath{n}}}{{\\ensuremath{n}}-{\\ensuremath{s}}-1 } e_i^t \\ ; ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } \\ , { \\ensuremath{\\vec{b}\\ , } } , \\qquad \\mbox{and }",
    "\\\\ \\exs[{\\ensuremath{y'}}_i ] & = & \\frac{{\\ensuremath{\\sigma^2}}}{{\\ensuremath{n } } } \\frac{{\\ensuremath{n}}}{{\\ensuremath{n}}- { \\ensuremath{s}}-1 } \\ ; e_i^t ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } e_i \\ ; = \\ ; \\frac{{\\ensuremath{\\sigma^2}}}{{\\ensuremath{n}}-{\\ensuremath{s}}-1 } e_i^t ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } e_i.\\end{aligned}\\ ] ]      @xmath426 & = & \\frac{\\lambda_{\\ensuremath{n}}^2 \\ , { \\ensuremath{n}}^2}{({\\ensuremath{n}}- { \\ensuremath{s } } ) \\ , ( { \\ensuremath{n}}- { \\ensuremath{s}}-3 ) } \\ ; \\left [ \\left(e_i^t ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } { \\ensuremath{\\vec{b}\\,}}\\right)^2 + \\frac{1}{{\\ensuremath{n}}- { \\ensuremath{s}}-1 } \\left({\\ensuremath{\\vec{b}\\,}}^t ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } { \\ensuremath{\\vec{b}\\,}}\\right ) \\left(e_i^t ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } e_i\\right ) \\right ] \\\\",
    "\\exs[({\\ensuremath{y'}}_i)^2 ] & = & \\frac{{\\ensuremath{\\sigma^4}}{\\ensuremath{n}}^2}{({\\ensuremath{n}}-{\\ensuremath{s}}-1)^2 \\ ; ( { \\ensuremath{n}}- { \\ensuremath{s } } ) \\ ; ( { \\ensuremath{n}}- { \\ensuremath{s}}-3 ) } \\ ; \\left(e_i^t ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } e_i \\right)^2 \\ ; \\left[1 + \\frac{1}{{\\ensuremath{n}}- { \\ensuremath{s}}-1 } \\right].\\end{aligned}\\ ] ]    we now compute and bound the variance of @xmath245 .",
    "setting @xmath427 and @xmath428 for shorthand , we have @xmath429 - \\frac{\\lambda_{\\ensuremath{n}}^2 \\ ; { \\ensuremath{n}}^2}{({\\ensuremath{n}}-{\\ensuremath{s}}-1)^2 } a_i^2 \\\\ & = & \\frac{\\lambda_{\\ensuremath{n}}^2 { \\ensuremath{n}}^2}{({\\ensuremath{n}}- { \\ensuremath{s } } ) \\ ,      ( { \\ensuremath{n}}-{\\ensuremath{s}}-3 ) } \\left[a_i^2 \\ ; \\left ( 1- \\frac{({\\ensuremath{n}}-      { \\ensuremath{s } } ) \\ , ( { \\ensuremath{n}}- { \\ensuremath{s}}-3)}{({\\ensuremath{n}}- { \\ensuremath{s}}-1)^2 }      \\right ) + \\frac{1}{{\\ensuremath{n}}- { \\ensuremath{s}}-1 } a_i b_i \\right ]   \\\\ & \\leq & 2 \\lambda_{\\ensuremath{n}}^2 \\left [ \\frac{3 a_i^2}{{\\ensuremath{n}}- { \\ensuremath{s } } } + \\frac{a_i b_i}{{\\ensuremath{n}}- { \\ensuremath{s}}-1 } \\right]\\end{aligned}\\ ] ] for @xmath5 sufficiently large . using the bound @xmath430",
    ", we see that the quantities @xmath431 and @xmath432 are uniformly bounded for all @xmath138 .",
    "hence , we conclude that , for @xmath5 sufficiently large , the variance is bounded as @xmath433 for some fixed constant @xmath251 independent of @xmath3 and @xmath5 . now since @xmath434| \\leq \\frac{2 { \\ensuremath{d_{\\operatorname{max}}}}\\lambda_{\\ensuremath{n}}{\\ensuremath{n}}}{{\\ensuremath{n}}- { \\ensuremath{s}}-1}$ ]",
    ", we have @xmath435| \\ ; \\geq \\ ; |{\\ensuremath{y}}_i| -    consequently , making use of chebyshev s inequality , we have @xmath436 & = & \\prob[|{\\ensuremath{y}}_i| - \\frac{2 { \\ensuremath{d_{\\operatorname{max}}}}\\lambda_{\\ensuremath{n}}{\\ensuremath{n}}}{{\\ensuremath{n}}-{\\ensuremath{s}}-1}\\geq \\frac{4 { \\ensuremath{d_{\\operatorname{max}}}}\\lambda_{\\ensuremath{n}}{\\ensuremath{n}}}{{\\ensuremath{n}}- { \\ensuremath{s}}-1}]\\\\ & \\leq & \\prob[|{\\ensuremath{y}}_i - \\exs[{\\ensuremath{y}}_i]| \\geq \\frac{4 { \\ensuremath{d_{\\operatorname{max}}}}\\lambda_{\\ensuremath{n}}{\\ensuremath{n}}}{{\\ensuremath{n}}- { \\ensuremath{s}}-1 } ] \\\\ & \\leq & \\frac{\\var({\\ensuremath{y}}_i)}{16 { \\ensuremath{d_{\\operatorname{max}}}}^2 \\lambda_{\\ensuremath{n}}^2 } \\\\ & \\leq & \\frac{k}{16 { \\ensuremath{d_{\\operatorname{max}}}}\\ , ( { \\ensuremath{n}}- { \\ensuremath{s}})},\\end{aligned}\\ ] ] where the final step uses the bound  .",
    "we now compute and bound the variance of @xmath246 .",
    "we have @xmath437 \\right ) - \\frac{{\\ensuremath{\\sigma^4}}}{({\\ensuremath{n}}-{\\ensuremath{s}}-1)^2 } a_i^2 \\\\ & = & \\frac{{\\ensuremath{\\sigma^4}}{\\ensuremath{n}}^2}{({\\ensuremath{n}}-{\\ensuremath{s}}-1)^2 \\ ; ( { \\ensuremath{n}}-    { \\ensuremath{s } } ) \\ ; ( { \\ensuremath{n}}- { \\ensuremath{s}}-3 ) } \\ ; \\left(a_i^2 \\ ; \\left[1 +    \\frac{1}{{\\ensuremath{n}}- { \\ensuremath{s}}-1 } - \\frac{({\\ensuremath{n}}-{\\ensuremath{s } } ) \\ ; ( { \\ensuremath{n}}- { \\ensuremath{s}}- 3 ) } { { \\ensuremath{n}}^2}\\right ] \\right ) \\\\ & \\leq & \\frac{k { \\ensuremath{\\sigma^4}}}{({\\ensuremath{n}}-{\\ensuremath{s}}-1)^3}\\end{aligned}\\ ] ] for some constant @xmath251 independent of @xmath3 and @xmath5 . consequently , applying chebyshev s inequality , we have @xmath438 \\ ; = \\ ; \\prob[{\\ensuremath{y'}}_i - \\exs[{\\ensuremath{y'}}_i ] \\geq \\exs[{\\ensuremath{y'}}_i ] ] & \\leq &   \\frac{\\var({\\ensuremath{y'}}_i)}{(\\exs[{\\ensuremath{y'}}_i])^2 } \\\\ & \\leq & \\frac{k}{({\\ensuremath{n}}- { \\ensuremath{s}}-1)^3 } \\frac{1}{\\frac{{\\ensuremath{\\sigma^4}}}{{\\ensuremath{n}}^2 } e_i^t ( { \\ensuremath{\\sigma}}_{{\\ensuremath{s}}{\\ensuremath{s}}})^{-1 } e_i } \\\\ & \\leq & \\frac{k { \\ensuremath{n}}^2 { \\ensuremath{c_{max}}}}{{\\ensuremath{\\sigma^4}}({\\ensuremath{n}}- { \\ensuremath{s}}-1)^3 } \\\\ & \\leq & \\frac{k'}{{\\ensuremath{n}}- { \\ensuremath{s}}-1}\\end{aligned}\\ ] ] for some constant @xmath439 independent of @xmath3 and @xmath5 .      as in the proof of lemma  [ lemone ] ,",
    "we define and bound @xmath440 \\ ; \\leq \\ ; 2 { \\ensuremath{c_{max}}}{\\ensuremath{v^*}}.\\end{aligned}\\ ] ] now let @xmath441 be an i.i.d .",
    "zero - mean gaussian vector with @xmath442 , so that @xmath443 = 2 { \\ensuremath{c_{max}}}{\\ensuremath{v^*}}$ ] .",
    "if we set @xmath444 then , by applying a known error bound for the sudakov - fernique inequality  @xcite , we are guaranteed that @xmath445 & \\geq & \\exs [ \\max_{j \\in { \\ensuremath{s^c } } } x_j ] - \\sqrt{\\delta^ * \\log { \\ensuremath{n}}}.\\end{aligned}\\ ] ] we now show that the quantity @xmath446 is upper bounded by @xmath447 using the inversion formula for block - partitioned matrices  @xcite , we have @xmath448_{{\\ensuremath{s^c}}{\\ensuremath{s^c}}}.\\ ] ] consequently , we have the lower bound @xmath391 & = & { \\ensuremath{v^*}}(e_i -e_j)^t { \\ensuremath{{\\ensuremath{\\sigma}}_{({\\ensuremath{s^c}}|   s)}}}(e_i - e_j ) \\\\ & \\geq & 2 { \\ensuremath{v^*}}{\\ensuremath{\\lambda_{min}}}({\\ensuremath{{\\ensuremath{\\sigma}}_{({\\ensuremath{s^c}}|   s ) } } } ) \\\\ & \\geq &   2 { \\ensuremath{v^*}}{\\ensuremath{\\lambda_{min}}}({\\ensuremath{\\sigma}}^{-1 } ) \\\\ & = & \\frac{2 { \\ensuremath{v^*}}}{{\\ensuremath{c_{max}}}}.\\end{aligned}\\ ] ] in turn , this leads to the upper bound @xmath449 \\\\ & \\leq & 2 { \\ensuremath{v^*}}\\ , \\left({\\ensuremath{c_{max}}}- \\frac{1}{{\\ensuremath{c_{max } } } } \\right).\\end{aligned}\\ ] ]    we now analyze the behavior of @xmath450 $ ] . using asymptotic results on the extrema of i.i.d .",
    "gaussian sequences  @xcite , we have @xmath451}{\\sqrt { 2 { \\ensuremath{c_{max}}}{\\ensuremath{v^*}}\\log { \\ensuremath{n } } } } = 1 $ ] .",
    "consequently , for all @xmath398 , there exists an @xmath399 such that for all @xmath400 , we have @xmath452 & \\geq & ( 1-{\\ensuremath{\\delta ' } } ) \\sqrt { 2 { \\ensuremath{c_{max}}}{\\ensuremath{v^*}}\\log { \\ensuremath{n}}}.\\end{aligned}\\ ] ] applying this lower bound to the bound  , we have @xmath453 & \\geq & \\frac{1}{\\lambda_{{\\ensuremath{n } } } } \\left[(1-{\\ensuremath{\\delta ' } } ) \\ , \\sqrt{2 { \\ensuremath{c_{max}}}{\\ensuremath{v^*}}\\log { \\ensuremath{n } } } - \\sqrt { \\delta^ * \\ , \\log { \\ensuremath{n } } } \\right ] \\nonumber \\\\ & \\geq & \\frac{1}{\\lambda_{{\\ensuremath{n } } } } \\left[(1-{\\ensuremath{\\delta ' } } ) \\ , \\sqrt{2 { \\ensuremath{c_{max}}}{\\ensuremath{v^*}}\\log { \\ensuremath{n } } } - \\sqrt { 2 \\ , { \\ensuremath{v^*}}\\ , ( { \\ensuremath{c_{max}}}- \\frac{1}{{\\ensuremath{c_{max } } } } ) \\ , \\log { \\ensuremath{n } } } \\right ] \\nonumber \\\\ \\label{eqnsplit } & = & \\left [ ( 1-{\\ensuremath{\\delta ' } } ) \\sqrt{{\\ensuremath{c_{max } } } } - \\sqrt{{\\ensuremath{c_{max}}}- \\frac{1}{{\\ensuremath{c_{max } } } } } \\right ] \\",
    "; \\sqrt { 2 \\frac{{\\ensuremath{v^*}}}{\\lambda_{{\\ensuremath{n}}}^2 } \\log { \\ensuremath{n}}}.\\end{aligned}\\ ] ]    first , assume that @xmath454 does not diverge to infinity .",
    "then , there exists some @xmath31 such that @xmath455 for all sufficiently large @xmath14 . in this case",
    ", we have from the bound   that @xmath453 & \\geq & \\gamma \\sqrt{\\log { \\ensuremath{n}}}\\end{aligned}\\ ] ] where @xmath456 \\ ; \\frac{1}{\\sqrt{\\alpha } } > 0 $ ] .",
    "( note that by choosing @xmath410 sufficiently small , we can always guarantee that @xmath55 , since @xmath457 . )",
    "this completes the proof of condition ( b ) in the lemma statement .",
    "otherwise , we may assume that @xmath458 .",
    "we compute @xmath459 we now apply the condition @xmath460 ^ 2 -{\\ensuremath{\\nu}}{\\ensuremath{c_{max}}}(2-{\\ensuremath{\\epsilon}})^2        \\right]\\end{aligned}\\ ] ] to obtain that @xmath461 & \\geq & \\sqrt{(1-\\delta ) } \\ ; \\frac{(1-{\\ensuremath{\\delta ' } } ) \\ , \\sqrt{{\\ensuremath{c_{max } } } } - \\sqrt{{\\ensuremath{c_{max}}}- \\frac{1}{{\\ensuremath{c_{max}}}}}}{\\sqrt{\\left[\\sqrt{{\\ensuremath{c_{max } } } } - \\sqrt{{\\ensuremath{c_{max}}}- \\frac{1}{{\\ensuremath{c_{max}}}}}\\right]^2 - { \\ensuremath{\\nu}}{\\ensuremath{c_{max}}}\\ , ( 2-{\\ensuremath{\\epsilon}})^2 } } \\ ; ( 2-{\\ensuremath{\\epsilon}})\\end{aligned}\\ ] ]    recall that @xmath462 is fixed , and moreover that @xmath463 are arbitrary .",
    "let @xmath464 be the lower bound on the rhs  .",
    "note that @xmath465 is a continuous function , and moreover that @xmath466 ^ 2 - { \\ensuremath{\\nu}}{\\ensuremath{c_{max}}}\\ , ( 2-{\\ensuremath{\\epsilon}})^2 } } \\ ; ( 2-{\\ensuremath{\\epsilon } } ) \\ ; > \\ ; ( 2-{\\ensuremath{\\epsilon}}).\\end{aligned}\\ ] ] therefore , by the continuity of @xmath465 , we can choose @xmath467 sufficiently small to ensure that for some @xmath468 , we have @xmath469 \\geq ( 2-{\\ensuremath{\\epsilon } } ) \\ ; ( 1 + \\gamma)$ ] for all sufficiently large @xmath5 .",
    "e.  candes , j.  romberg , and t.  tao .",
    "robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information . technical report , applied and computational mathematics , caltech , 2004 .              d.  donoho . for most large undetermined system of linear equations",
    "the minimal @xmath470-norm near - solution is also the sparsest solution .",
    "technical report , statistics department , stanford university , 2004 .",
    "d.  m. malioutov , m.  cetin , and a.  s. willsky .",
    "optimal sparse representations in general overcomplete bases . in _ int .",
    "conf . on acoustics , speech , and signal processing _ , volume  2 , pages ii793796 , may 2004 ."
  ],
  "abstract_text": [
    "<S> the problem of consistently estimating the sparsity pattern of a vector @xmath0 based on observations contaminated by noise arises in various contexts , including subset selection in regression , structure estimation in graphical models , sparse approximation , and signal denoising . </S>",
    "<S> we analyze the behavior of @xmath1-constrained quadratic programming ( qp ) , also referred to as the lasso , for recovering the sparsity pattern . </S>",
    "<S> our main result is to establish a sharp relation between the problem dimension @xmath2 , the number @xmath3 of non - zero elements in @xmath4 , and the number of observations @xmath5 that are required for reliable recovery . for a broad class of gaussian ensembles satisfying mutual incoherence conditions , we establish existence and compute explicit values of thresholds @xmath6 and @xmath7 with the following properties : for any @xmath8 , if @xmath9 , then the lasso succeeds in recovering the sparsity pattern with probability converging to one for large problems , whereas for @xmath10 , then the probability of successful recovery converges to zero . for the special case of the uniform gaussian ensemble , </S>",
    "<S> we show that @xmath11 , so that the threshold is sharp and exactly determined .    * sharp thresholds for high - dimensional and noisy recovery of sparsity *    technical report , uc berkeley , department of statistics + may 2006    * keywords : * quadratic programming ; lasso ; subset selection ; consistency ; thresholds ; sparse approximation ; signal denoising ; sparsity recovery ; @xmath12-regularization ; model selection . </S>"
  ]
}