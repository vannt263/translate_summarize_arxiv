{
  "article_text": [
    "linear programming ( lp ) decoding , as an approximation to maximum - likelihood ( ml ) decoding , was proposed by feldman _ et al .",
    "many observations suggest similarities between the performance of lp and iterative message - passing methods , e.g. in @xcite .",
    "for example , we know that the existence of low - weight pseudo - codewords degrades the performance of both methods ( @xcite , @xcite ) . therefore , it is reasonable to make use of the simpler geometrical structure of lp decoding to make predictions on the performance of message - passing algorithms . on the other hand , there are differences which prevent us from making an explicit connection between these two approaches . for instance , given an ldpc code , adding additional parity checks that are satisfied by all the codewords can only improve lp decoding , while with message - passing algorithms , these parity checks may have a negative effect by introducing short cycles in the tanner graph .",
    "this property of lp decoding allows improvements by tightening the relaxation .",
    "another characteristic of lp decoding ( the _ ml certificate property _ ) is that its failure to find the ml codeword is detectable .",
    "more specifically , the decoder always gives either the ml codeword , or a nonintegral pseudo - codeword as the solution .",
    "these two properties motivate the use of an adaptive approach in lp decoding which can be summarized as follows : given a set of constraints that describe a code , start the lp decoding with a few of them , and sequentially and adaptively add more of the constraints to the problem until either the ml codeword is found or no further `` useful '' constraint exists .",
    "the goal of this paper is to explore the potential of this idea for lp decoding .",
    "we also prove some properties for the lp relaxation of ml decoding which can be useful for performance analysis of lp and/or iterative decoding algorithms .",
    "we show that by putting lp in an adaptive setting , we can obtain the same performance as if a huge number of constraints were added to the relaxation from the beginning .",
    "in particular , we have observed that while the number of constraints per check node required for convergence is exponential in the check node degrees for lp decoding , the adaptive method generally converges with a number of constraints which is a ( small ) constant , independent of degree distributions .",
    "this property makes it feasible to apply lp decoding to higher - density codes .",
    "the rest of this paper is organized as follows . in section",
    "ii , we review feldman s lp decoding . in section iii , we introduce and analyze an adaptive algorithm to solve the original lp problem more efficiently . in section iv , we study how adaptively adding additional constraints can improve the performance .",
    "finally , section v concludes the paper . due to space limitations ,",
    "some of the proofs of the results are omitted .",
    "consider a binary linear code @xmath0 of length @xmath1 .",
    "if a codeword @xmath2 is transmitted through a binary - input memoryless channel , the ml codeword given the received vector @xmath3 is the solution to the optimization problem @xmath4 where @xmath5 is the vector of log - likelihood ratios defined as @xmath6 as an approximation to ml decoding , feldman _",
    "proposed a relaxed version of this problem by first considering the convex hull of the local codewords defined by each row of the parity - check matrix , and then intersecting them to obtain what is called the _",
    "fundamental polytope _ by koetter _",
    "this polytope has a number of integral and nonintegral vertices , but the integral vertices exactly correspond to the codewords of @xmath0 .",
    "therefore , whenever lp gives an integral solution , it is guaranteed to be the ml codeword .    in feldman s relaxation of the decoding problem ,",
    "the following is done for each row @xmath7 of the parity - check matrix .",
    "suppose that the @xmath8th check node has the neighborhood set @xmath9 , i.e. @xmath10 contains the indices of the variable nodes that are directly connected to this check node .",
    "then , add the following constraints to the problem : @xmath11 throughout the paper , we refer to the constraints of this form as _ parity - check constraints_. in addition , for any element @xmath12 of the optimization variable , @xmath13 , the constraint that @xmath14 is also added .",
    "as any odd - sized subset @xmath15 of the neighborhood @xmath10 of each check node introduces a unique parity - check constraint , there are @xmath16 constraints corresponding to each check node of degree @xmath17 .",
    "therefore , the total number of constraints and hence , the complexity of the problem , is exponential in terms of the maximum check node degree , @xmath18 .",
    "this becomes more significant in a high density code where @xmath18 increases with the code length , @xmath1 . in this section , we show that lp relaxation of linear codes has some properties that allow us to solve the optimization by using a much smaller number of constraints .      given a constraint of the form @xmath19 and a vector @xmath20 , we call ( [ general const ] ) an _ active constraint _ at @xmath21 if @xmath22 and a _ violated constraint _ or , equivalently , a _ cut _ at @xmath21 if @xmath23    considering a constraint that generates a cut @xmath24 at point @xmath13 , we can immediately make the following observations : @xmath25 the following theorem reveals a special property of the constraints of the lp decoding problem .    [ one cut per check ] at any given point @xmath26^n$ ] , at most one of the constraints introduced by each check node can be a cut .",
    "( proof omitted . )    having a linear @xmath27 code with @xmath28 parity checks , a natural question is how we can find all the cuts defined by the lp relaxation at any given point @xmath29 . for any check node and an odd subset @xmath15 of its neighborhood that introduces a cut , we know from ( [ obsv2 ] ) that the members of @xmath15 are the variable nodes with the largest values among the neighbors of the check node .",
    "therefore , sorting the elements of @xmath13 before searching for a cut can simplify the procedure .",
    "consider a check node @xmath8 . without loss of generality , assume that variable nodes @xmath30 , are the neighbors of this check node , and they are sorted with respect to their values such that @xmath31 .",
    "the following algorithm is an efficient way to find the cut generated by this check node at @xmath32 , if it exists .",
    "[ find cuts ]    _ step 1 : set @xmath33 , @xmath34 and @xmath35 .",
    "_    _ step 2 : check the constraint ( [ constraints ] ) .",
    "if it is violated , we have found the cut . exit .",
    "_    _ step 3 : set @xmath36 . if @xmath37 , move @xmath38 and @xmath39 ( the two largest members of @xmath40 ) from @xmath40 to @xmath15 _    _ step 4 : if @xmath37 and ( [ obsv1 ] ) is satisfied , go to step 2 ; otherwise , the check node does not provide a cut at @xmath41 . _",
    "if redundant calculations are avoided , this algorithm can find the cut generated by the check node , if it exists , in @xmath42 time , where @xmath43 is the degree of the check node .",
    "repeating the procedure for each check node , and considering @xmath44 complexity for sorting @xmath13 , the time required to find all the cuts at point @xmath13 becomes @xmath45 .",
    "the simplex lp algorithm starts from a vertex of the problem polytope and visits different vertices of the polytope by traveling through the edges until it finds the optimum vertex .",
    "the time required to find the solution is approximately proportional to the number of vertices that have been visited , and this , in turn , is determined by the number and properties of the constraints in the problem .",
    "hence , if we eliminate some of the intermediate vertices and only keep those which are close to the optimum point , we can reduce the complexity of the algorithm . to implement this idea in the adaptive lp decoding scheme , we run the lp solver with a minimal number of constraints to ensure boundedness of the solution , and depending on the lp solution , we add only `` the useful constraints '' that cut the current solution from the feasible region .",
    "this procedure is repeated until no further cut exists .    to start the procedure ,",
    "we need at least @xmath1 constraints so that the problem has a vertex that can become the solution of the first round . using the condition that @xmath14",
    ", we add one side of these inequalities for each @xmath46 , depending on whether increasing @xmath12 increases or decreases the objective function . in other words , for each @xmath47 , we initially have the constraint @xmath48 the optimum ( and only ) vertex of this initial problem corresponds to the result of ( uncoded ) hard - decision based on the received vector .",
    "now we proceed with the following algorithm :    [ adaptive lp ]    _ step 1 : setup the initial problem according to ( [ initial constraints ] ) .",
    "_    _ step 2 : run the lp solver .",
    "_    _ step 3 : search for all the cuts for the current solution .",
    "_    _ step 4 : if one or more cuts are found , add them to the problem constraints and go to step 2 . _    if at any iteration of algorithm [ adaptive lp ] no cut is found , the current solution is the the solution of the lp decoder with all the relaxation constraints given in section ii .",
    "the claim follows from the fact that if at any stage no cut is found , the current solution is in the fundamental polytope .",
    "[ n iterations ] the adaptive algorithm ( algorithm [ adaptive lp ] ) converges with at most @xmath1 iterations .",
    "the final solution is a vertex @xmath49 of the problem space determined by the initial constraints along with those added by the adaptive algorithm .",
    "therefore , we can find @xmath1 such constraints , @xmath50 whose corresponding hyperplanes uniquely determine this vertex .",
    "this means that if we set up an lp problem with only those @xmath1 constraints , the optimal point will be @xmath49 .",
    "now , consider the @xmath51th intermediate solution , @xmath52 , that is cut off at the end of the @xmath51th iteration .",
    "at least one of the constraints , @xmath53 , should be violated by @xmath52 , otherwise since @xmath52 has a lower cost than @xmath49 , @xmath52 would be the solution of lp with these @xmath1 constraints .",
    "but we know that the cuts added at the @xmath51th iteration are all the possible constraints that are violated at @xmath52 .",
    "consequently , at least one of the cuts added at each iteration should be among @xmath54 ; hence , the number of iterations is at most @xmath1 .",
    "this theorem applies to any general lp problem where there is a fixed set of constraints , and at each iteration we add all the cuts generated by this set of constraints .",
    "the adaptive algorithm has at most @xmath55 constraints at the final iteration",
    ".    follows from theorem [ one cut per check ] and theorem [ n iterations ] .    for high - density codes of fixed rate ,",
    "this bound guarantees convergence with @xmath56 constraints , whereas the standard lp and the polytope given in @xcite for high - density codes respectively require exponential and @xmath57 constraints .      to observe the complexity reduction due to the adaptive approach for lp decoding ,",
    "we have performed simulations over random regular ldpc codes of various lengths , degrees , and rates on the awgn channel .",
    "all the experiments were performed with the low snr value of @xmath58 db , since in the high snr regime the recieved vector is likely to be close to a codeword , in which case the algorithm converges fast , rather than demonstrating its worst - case behavior .    in the first scenario , we studied the effect of changing the check node degree @xmath17 from @xmath59 to @xmath60 while keeping the code length at @xmath61 and the rate at @xmath62 .",
    "the simulation was performed over 400 blocks for each value of @xmath17 .",
    "the maximum ( average ) number of iterations required to converge started from around @xmath63 ( @xmath64 ) for @xmath65 , and decreased monotonically down to @xmath66 ( @xmath67 ) for @xmath68 .",
    "the average and maximum numbers of parity - check constraints in the final iteration of the algorithm are plotted in fig .",
    "[ const vs d_c ] .",
    "it is observed that both the average and the maximum values are almost constant , and remain below @xmath69 for all the values of @xmath17 . for comparison , the total number of constraints required for the standard ( non - adaptive ) lp decoding problem , which is equal to @xmath16 is also included in this figure .",
    "the decrease in the number of required constraints translates to a large gain for the adaptive algorithm in terms of the running time .",
    "this gain increases exponentially with @xmath17 , so that with the lp solver that we have used in our work ( glpk @xcite ) , the adaptive algorithm always converges several thousand times faster that standard lp for @xmath70 .",
    ", for fixed length @xmath61 and rate @xmath62.,width=3 ]    [ const vs d_c ]    in the second case , we studied random ( 3,6 ) codes of lengths @xmath71 to @xmath72 .",
    "for all values of @xmath1 , the maximum ( average ) number of required iterations remained between @xmath73 and @xmath74 ( @xmath75 and @xmath76 ) .",
    "the average and maximum numbers of parity - check constraints in the final iteration are plotted versus @xmath1 in fig .",
    "[ const vs n ] .",
    "we observe that the number of constraints is generally between @xmath77 and @xmath78 .    , for fixed rate @xmath62 and check node degree @xmath79.,width=3 ]    [ const vs n ]    in the third experiment",
    ", we investigated the effect of the rate of the code on the performance of the algorithm .",
    "[ const vs m ] shows the average and maximum numbers of parity - check constraints in the final iteration where the block length is @xmath80 and the number of parity checks , @xmath81 , increases from @xmath82 to @xmath83 .",
    "the variable node degree is fixed at @xmath84 .",
    "we see that the average and maximum numbers of constraints are respectively in the ranges @xmath85 to @xmath86 and @xmath87 to @xmath88 for most values of @xmath81 .",
    "the relatively large drop of the average number for @xmath89 with respect to the linear curve can be explained by the fact that at this value of @xmath81 the rate of failure of lp decoding was less than @xmath90 at @xmath58 db , whereas for all the other values of @xmath81 , this rate was close to @xmath91 . since the success of lp decoding generally indicates proximity of the received vector to a codeword , we expect the number of parity checks required to converge to be small in such a case , which decreases the average number of constraints .    , for @xmath80 and @xmath84.,width=3 ]",
    "[ const vs m ]    based on these simulation results , we observe that in practice the algorithm performs much faster than is guaranteed by theorem [ n iterations ] .",
    "these observations suggest the following conjecture .",
    "[ conjecture on complexity ] for a random parity - check code of length @xmath1 with @xmath92 parity checks and arbitrary degree distributions , as @xmath1 increases , the adaptive lp decoding algorithm converges with probability arbitrarily close to @xmath91 in at most @xmath93 iterations and with at most @xmath94 final parity - check constraints per check node , where @xmath93 and @xmath94 are constants independent of the length , rate and degree distribution of the code .",
    "[ int positions in pcw ] if for a given code of length @xmath1 , the adaptive algorithm converges with at most @xmath95 final parity - check constraints , then each pseudo - codeword of this lp relaxation should have at least @xmath96 integer elements . to see this ,",
    "note that each pseudo - codeword corresponds to the intersection of at least @xmath1 active constraints .",
    "if the problem has at most @xmath97 parity - check constraints , then at least @xmath96 constraints of the form @xmath98 or @xmath99 should be active at each pseudo - codeword , which means that at least @xmath96 positions of the pseudo - codeword are integer - valued .",
    "the complexity reduction obtained by adaptive lp decoding inspires the use of cutting - plane techniques to improve the error rate performance of the algorithm . specifically ,",
    "when lp with all the original constraints gives a nonintegral solution , we try to cut the current solution , while keeping all the possible integral solutions ( codewords ) feasible .    in the decoding problem",
    ", the new cuts can be chosen from a pool of constraints describing a relaxation of the maximum - likelihood problem which is tighter than the fundamental polytope . in this sense ,",
    "the cutting - plane technique is equivalent to the adaptive lp decoding of the previous section , with the difference that there are more constraints to choose from .",
    "the effectiveness of this method depends on how closely the new relaxation approximates the ml decoding problem , and how efficiently we can search for those constraints that introduce cuts .",
    "_ @xcite have mentioned some ways to tighten the relaxation of the ml decoding , including adding redundant parity checks ( rpc ) , and using lift - and - project methods .",
    "( for more on lift - and - project , see @xcite and references therein . )",
    "gomory s algorithm @xcite is also one of the most well - known techniques for general integer optimization problems , although it suffers from slow convergence .",
    "each of these methods can be applied adaptively in the context of cutting - plane techniques .",
    "the simple structure of rpcs makes them an interesting choice for generating cuts .",
    "there are examples where even the relaxation obtained by adding all the possible rpc constraints does not guarantee convergence to a codeword . in other words",
    ", it is possible that we obtain a nonintegral solution for which there is no rpc cut , although the general case is still not well - studied . also , finding efficient methods to search for rpc cuts for a given nonintegral solution is an open issue . on the other hand ,",
    "as observed in simulation results , rpc cuts are generally strong , and a reasonable number of them makes the resulting lp relaxation tight enough to converge to an integer - valued solution . in this work ,",
    "we focus on cutting - plane algorithms that use rpc cuts .",
    "a redundant parity check is obtained by modulo-2 addition of some of the rows of the parity - check matrix , and this new check introduces a number of constraints that may include a cut .",
    "there is an exponential number of rpcs that can be made this way , and in general , most of them do not introduce cuts . hence , we need to find the cuts efficiently by exploiting the particular structure of the decoding problem .",
    "in particular , we observe that cycles in the graph have an important role in determining whether an rpc generates a cut .",
    "this property is explained by theorem [ rpc cycles ] .",
    "[ minimal subset ] given a current solution , @xmath13 , the subset @xmath100 of check node indices is called a _ cut - generating collection _ if the rpc made by modulo-2 addition of the parity checks corresponding to @xmath101 introduces a cut .",
    "[ rpc cycles ] let @xmath102 be a collection of check node indices in the tanner graph of the code , and let @xmath103 be the subgraph made up of these check nodes , the variable nodes directly connected to them , and all the edges that connect them .",
    "then , @xmath101 can be a cut - generating collection only if @xmath103 contains a cycle that only passes through variable nodes whose corresponding current values are fractional .",
    "( proof omitted . )",
    "this result motivates the following algorithm to search for cuts .",
    "[ search for rpc cuts ]    _ step 1 : having a solution @xmath13 , prune the tanner graph by removing all the variable nodes with integer values .",
    "_    _ step 2 : starting from an arbitrary check node , randomly walk through the pruned graph until a cycle is found .",
    "_    _ step 3 : create an rpc by combining the rows of the parity - check matrix corresponding to the check nodes in the cycle .",
    "_    _ step 4 : if this rpc does not introduce a cut , go to step 2 . _    by exploiting some of the properties of the problem as described above , the search for each cut becomes much faster than the naive search ; however , this procedure is still the complexity bottle - neck of the decoding , and it becomes prohibitively complex for codes of length on the order of several hundred bits and above . on the other hand , experiments demonstrate that the number of cuts required to converge to the ml codeword ( if the convergence is possible ) does not grow very rapidly with length .",
    "this motivates more study of the properties of rpc cuts and efficient schemes to find them .",
    ".,width=3 ]    [ rpc wer ]      to demonstrate the performance improvement achieved by using the rpc cutting - plane technique , we present simulation results for a random regular @xmath104 ldpc code of length @xmath105 .",
    "the adaptive nature of the algorithm allows us to smoothly trade complexity for performance by changing the number of trials in the search for each rpc cut , i.e. the number of iterations in algorithm [ search for rpc cuts ] , as well as the total number of calls of the lp solver . in this experiment",
    ", we fix the total number of iterations of algorithm [ search for rpc cuts ] to @xmath106 , and declare decoding failure if no cut is found after @xmath106 trials .",
    "the word error rate ( wer ) of the algorithm is plotted versus snr in fig .",
    "[ rpc wer ] for different values of @xmath106 . for comparison ,",
    "the wer of pure lp decoding , i.e. with no rpc cut , and a lower bound on the wer of the ml decoder have been included , as well . in order to obtain this lower bound , we count the number of times that the cutting - plane lp algorithm with a large value of @xmath106 converges to a codeword other than the transmitted codeword , and divide that by the number of blocks . due to the ml certificate property of lp decoding , we know that ml decoding would fail in those cases , as well . on the other hand ,",
    "ml decoding may also fail in some of the cases where lp decoding does not converge to an integral solution .",
    "therefore , this estimate gives a lower bound on the wer of ml decoding .",
    "the numerical results suggest that the cutting - plane lp decoding with rpc cuts can significantly outperform the pure lp decoding , at the cost of increased complexity .",
    "in this work , we studied the potential for improving lp decoding , both in complexity and error correction capability , by using an adaptive approach .",
    "the key idea was to use the fact that we can always recognize the failure of lp decoding to find the ml codeword , a property that message - passing algorithms only have in specific cases such as the erasure channel .",
    "this feature allows us to add only constraints that are `` useful '' , depending on the current status of the algorithm .    in the algorithm proposed in section iii ,",
    "the complexity is significantly reduced and becomes independent of the degree distributions , making it possible to apply lp decoding to parity - check codes of arbitrary densities . however , since general purpose lp solvers are used at each iteration , the complexity in terms of the block length is still super - linear , as opposed to linear as in the message - passing algorithms . an interesting question is whether we can design special lp solvers for decoding of ldpc codes that can take advantage of the sparsity of the constraints and other properties of the problem to converge in linear time .",
    "section iv serves as a first step to explore the application of cutting - plane techniques in lp decoding .",
    "a desirable feature of this approach is that by changing the parameters of the algorithm we can smoothly trade complexity for performance .",
    "in contrast , if we want to get the same performance gains by tightening the relaxation in a non - adaptive setting , the required complexity increases much faster .",
    "we showed that redundant parity checks provide strong cuts , even though they may not guarantee ml performance .",
    "a major open problem is to find efficient ways to search for these cuts by exploiting their properties , and to determine specific classes of codes for which rpc cuts are more effective .",
    "furthermore , the effectiveness of cuts generated by other techniques , such as lift - and - project cuts and gomory cuts , as well as specially designed cuts , needs further study .    1 j.  feldman , m. j.  wainwright , and d.  karger , `` using linear programming to decode binary linear codes , '' _ ieee trans .",
    "inform . theory _",
    "954 - 972 , mar .",
    "o.  vontobel and r.  koetter , `` on the relationship between linear programming decoding and min - sum algorithm decoding , '' _ proc .",
    "intl symp . on inform .",
    "theory and applications _ , pp .",
    "991 - 996 , parma , italy , oct . 2004 .",
    "o.  vontobel and r.  koetter , `` graph - cover decoding and finite - length analysis of message - passing iterative decoding of ldpc codes , '' _ sumbitted to ieee trans .",
    "theory_. `` gnu linear programming kit , '' available at    ` http://www.gnu.org/software/glpk ` .",
    "m. laurent , `` a comparison of the sherali - adams , lovsz - schrijver and lasserre relaxations for 0 - 1 programming , '' _ mathematics of operations research , _ vol .",
    "470 - 496 , 2003 .",
    "r.  gomory , `` an algorithm for integer solutions to linear programs , '' _ princeton ibm mathematical research report _",
    ", nov . 1958 ."
  ],
  "abstract_text": [
    "<S> detectability of failures of linear programming ( lp ) decoding and its potential for improvement by adding new constraints motivate the use of an adaptive approach in selecting the constraints for the lp problem . in this paper </S>",
    "<S> , we make a first step in studying this method , and show that it can significantly reduce the complexity of the problem , which was originally exponential in the maximum check - node degree . </S>",
    "<S> we further show that adaptively adding new constraints , e.g. by combining parity checks , can provide large gains in the performance . </S>"
  ]
}