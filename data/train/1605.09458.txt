{
  "article_text": [
    "neural networks are widely applied in various fields , such as oil exploration in @xcite , speech recognition in @xcite , temperature control in @xcite , and so on . as a kind of neural network",
    ", deep neural network has become an increasingly popular research field . and training auto - encoders to learn useful feature extractors to inisitalize a deep neural network is a widely used approach .",
    "an auto - encoder is comprised by an encoder and a decoder . given an input example",
    ", the encoder , which consists of a group of feature extractors , produces features that constitute an abstract representation or code of the example , while the decoder reconstructs the example from the code .",
    "training an auto - encoder is to minimize the difference between the input example and its reconstruction .",
    "details of auto - encoders and their applications in deep learning can be found in @xcite .",
    "auto - encoders are usually implemented with neural networks , but over - complete ( higher dimensional hidden layer than the input layer ) and unconstrained auto - encoders may learn identical mapping , which results in useless features .",
    "therefore , many regularized auto - encoders were proposed to learn good feature extractors , such as sparse auto - encoders @xcite , contractive auto - encoders ( caes ) @xcite , denoising auto - encoders ( daes ) @xcite , marginalized denoising auto - encoders ( mdaes ) @xcite , and so on .    because of the unsupervised training , auto - encoders attempts to capture everything in input data , including task - irrelevant information if there exits@xcite . however , learning task - irrelevant information may waste the computation resources and the capability of networks , and is easy to cause overfitting . therefore , eliminating task - irrelevant information becomes one of the ways to obtain better performance and reduce computation cost .",
    "bengio et al .",
    "explored supervised pre - training ( @xcite ) , and they concluded that partially supervised pre - training ( alternately perform supervised and unsupervised training of an auto - encoder ) can lead auto - encoders to learn better feature extractors when much task - irrelevant information is contained in training data .",
    "shon et al . proposed point - wise gated boltzmann machines ( pgbm ) @xcite .",
    "they used a hidden layer containing two - group units to model the foreground and background respectively , then took the foreground group to extract task - relevant features .",
    "pgmb achieves the state - of - the - art performance on several benchmark datasets .",
    "inspired by human attention , wang et al .",
    "proposed the attentional neural network ( ann ) @xcite .",
    "they used a segmentation module to iteratively segment foreground from noisy input via a feedback loop , and employed a deep network on the foreground for classification .",
    "ann also achieves the state - of - the - art performance on one benchmark dataset .    in this work",
    ", we try to drop the task - irrelevant variables by performing variable selection on input of auto - encoders and then execute unsupervised learning on the remaining variables .",
    "we introduce an importance - based variable selection method that evaluate importance of each variable and drop the variables with low importance.for obtaining better classification performance , the method is employed for each layer of stacked auto - encoders , not only on the raw input for the first layer , which is different from methods mentioned above .",
    "the experiment results show that it helps stacked dae ( sdae ) achieve significantly improved performance on three challenging benchmark datasets .",
    "the rest of this paper is organized as following .",
    "preliminaries about dae and sdae are provided in section [ sec : preliminaries ] .",
    "the proposed variable selection method is described in detail in section [ sec : methods ] .",
    "experiments and results are reported in section [ sec : experiments - and - results ] , and conclusions are made in section [ sec : conclusion ] .",
    "we will use following notations through out the paper .",
    "@xmath0 is a training dataset .",
    "each element of @xmath0 contains a input example @xmath1 and a target label @xmath2 such that @xmath3^{m}$ ] ( or @xmath4 ) and @xmath5 . for a vector @xmath1",
    ", @xmath6 is its @xmath7-th component . for the sake of simplicity",
    ", we use @xmath0 to denote the training dataset for an auto - encoder , no matter where the auto - encoder is located in a stacked auto - encoders .",
    "auto - encoders(aes ) are often employed as building blocks of deep networks .",
    "an ae is a neural network that composed of three layers , an input layer , a hidden layer and an output layer .",
    "it tries to recover the input data @xmath1 from the hidden representation @xmath8 at the output layer .",
    "the motivation is that , if the input data can be reconstructed well enough , then it can be said that the hidden feature is a discription of the input data .",
    "an ae is also can be seen as a network that consists of an encoder and a decoder .",
    "processing from input layer to hidden layer can be seen as an encoder , while from hidden layer to output layer a decoder .    during training",
    ", it firstly maps the the input data @xmath1 to hidden representation @xmath8 by the encoder : @xmath9 where @xmath10 is a weight vector , @xmath11 is a bias and @xmath12 is the activation function of the encoder , typically the sigmoid function@xmath13 .",
    "then reconstruct input @xmath14 from the hidden representation @xmath8 through the decoder : @xmath15 where @xmath16 is a bias and @xmath17 is the activation function of the decoder , which can be sigmoid function for binary input or an identity for continuous input .    explicitly , the reconstruction @xmath18 should approach to original input @xmath1 as much as possible , this can be measured by the reconstruction error @xmath19 which is typically computed via squared error or cross - entropy .",
    "which one to be chosen depends on the activation function of the decoder .",
    "if @xmath17 is an identity , i.e.@xmath20 , then @xmath21    if @xmath17 is simoid function , i.e.@xmath22 , then @xmath23 where @xmath14 is depend on the model parameters @xmath24,@xmath25,@xmath11 and @xmath16when input data @xmath26 is given .",
    "then the auto - encoder is trained to learn model parameters that minimize the reconstruction error @xmath19 .",
    "gradient descent can be utilized to optimized the error function during training . when the training is completed , the output layer with the weights of hidden to output are dropped and the learned representation feature holds in the hidden layer , which can be used for classification or used as the input of an other autoencoder to learn more abstract feature .    however , an ae may learn identity , which leads to obtain no useful featrues . in order to prevent this situation ,",
    "aes often utilize the configuration called `` bottleneck '' of which the quantity of hidden units lower than input units .",
    "an other approch is adding regular terms on the objective function to constrain the weights . otherwise , using disturbed input data for training is also an effective means , like denoising autoencoder .",
    "denoising auto - encoder(dae ) is a variant of standard auto - encoder .",
    "it attempts to reconstruct the input @xmath1 from the encoded representation @xmath8 of noisy input @xmath27 via a decoder . by disturbing the input @xmath1 , denoising auto - encoder tries to learn robust features that can successfully recover the perturbed values to reconstruct the original input data .",
    "if a dae can recover the original input data from the code of corruped input data , it can be said that the dae has leanred robust and stable features .",
    "stacked denoising auto - encoder(sdae ) is formed by stacking multiple single - layer daes for learning more abstract representations .",
    "sdae can be used to effectively pre - train deep networks . in the process of pre - training by sdae",
    ", the hidden features learned by lower - layer dae are used as inputs for training next ( upper - layer ) dae , and the encoders of daes are used to initialize weights in the deep network .",
    "see @xcite for details .",
    "according to equation ( [ eq : reconstruction_error ] ) , aes belong to unsupervised learning without considering the label information .",
    "the hidden representation of an autoecoder is a description of the whole input data .",
    "aes do not identify useful or unuseful information for classification .",
    "they attempt to capture all the information of the input , not only task - relevant information , but also task - irrelevant information if there exists .",
    "however , learning task - irrelevant information may waste computation resources and even cause over - fitting .",
    "therefore , task - irrelevant information contained in input data should be reduced or eliminated for obtaining better performance .    to address this issue ,",
    "an importance - based variable selection method is proposed to find the task - irrelevant variables and drop them .",
    "briefly , the method is to evaluate the importance of each variable to classification and drop the variables with importance lower than a threshold .",
    "we exploit the sensitivity of the discriminant hyperplane to a variable to evaluate the importance of the variable .",
    "we argue that , the variables with higher sensitivity are more important for classification , these variables also possess higher importance value and should be reserved , while those variables with low importance(lower than a threshold ) should be dropped .",
    "the details are described as follows .",
    "we employ a trained multinominal logistic regression(mlr ) model as a pre - classifier to help us determine the importance of each input variable to classification .",
    "multinominal logistic regression ( mlr ) is a simple log - linear classifier , and can be easily analyzed .",
    "given an example @xmath1 , the mlr computes the posterior probability of each hypothesis via a softmax function , and takes the one with biggest posterior probability as prediction .",
    "see @xcite for mlr in detail .",
    "we briefly introduce the softmax function , from which the importance notation can be deduced .",
    "the softmax function can be written as @xmath28\\ ] ] where @xmath29 and @xmath30 are parameters , and @xmath31 is the index of class . the predicted class of @xmath1 is obtained by @xmath32 .",
    "the softmax function computes the estimated probability of the class label for a given input @xmath1 .",
    "now we consider any two classes of class i and class j. we suppose that , the discriminant hyperplane between class @xmath33 and class @xmath34 is consisted of the points that with equivalent estimated probabilities of the two labels .",
    "let @xmath35 then we obtain the discriminant hyperplane : @xmath36    after normalization , discriminant function between class @xmath33 and class @xmath34 can be written as @xmath37\\slash\\|\\boldsymbol{w}_{i}-\\boldsymbol{w}_{j}\\|_{2}\\ ] ] in other words , all @xmath1 that satisfy @xmath38 form a discriminant hyperplane between class @xmath33 and class @xmath34 .",
    "we denote the discriminant hyperplane as @xmath39 .",
    "the unit normal vector of @xmath39 can be written as    @xmath40    training dataset @xmath41 , importance threshold @xmath42    variables mask @xmath43    @xmath44[initilize - a ]    train a new pre - classifier mlr with the masked training data @xmath45 .",
    "[ train - mlr ]    @xmath43    update @xmath43 according to ( [ eq : normal - vector ] ) , ( [ eq : hyperplane - contribution ] ) , ( [ eq : task - contribution ] ) , and ( [ eq : task - mask ] ) .",
    "[ update - a ]    define the _ sensitivity _ of @xmath46 to an input variable as @xmath47 , which reflects the influence of the variable to @xmath46 . because @xmath46 is a linear function , @xmath48 where @xmath49 is the @xmath7-th component of @xmath50 . by normalizing the sensitivities of @xmath46 with @xmath51 ( the infinity norm of @xmath50 )",
    ", we can define the _ importance _ of the @xmath7-th variable _ to _ @xmath46 as    @xmath52    we argue that if a variable has low importances to all the discriminant functions then it can be identified as task - irrelevant variable . on the other hand ,",
    "a variable is identified as task - relevant variable if it has unignorable importance to any @xmath46 .",
    "therefore , we define the _ importance _ of the @xmath7-th variable _ to the classification _ as    @xmath53    @xmath16 is the maximum of importances of the @xmath7-th variable across all discriminant functions .    consequently , task - irrelevant variables , each of which has low importance ( below a threshold ) to classification , can be discarded in the unsupervised training of auto - encoder . in order to facilitate computation",
    ", we use a variable mask @xmath43 to represent the binary task - relevances of input variables .",
    "@xmath43 is defined as    @xmath54    where @xmath42 is a importance threshold and @xmath55 is the indicator function so that it takes 1 if the condition in the brackets is true and 0 otherwise .",
    "mask components corresponding to task - irrelevant variables will take 0 .",
    "in practice , since there might be cross - correlation between variables in input variable set , it is not easy to find out all task - irrelevant variables through training a mlr with full input variable set .",
    "a iterative method can be employed to find out task - irrelevant variables gradually , and in each iteration a new pre - classifier mlr is trained to dropping a few variables from input variable set .",
    "algorithm [ alg : ivs ] is called importance - based variable selection ( ivs ) , and describes how to find task - irrelevant variables in detail . in line [ initilize - a ]",
    ", variable mask @xmath43 is initialized by assigning 1 to each component , which means all input variables will be used in the first mlr training . for each iteration ,",
    "a new pre - classifier mlr is trained with masked training data in line [ train - mlr ] , where @xmath56 means component - wise multiplication or haddamard product . in order to prevent the mlr training from overfitting ,",
    "model selection can be done by using a validation dataset to early stop the training in line [ train - mlr ] .",
    "the variable mask @xmath43 is updated in line [ update - a ] based on the well - trained mlr .",
    "this iterative procedure will stop under conditions such as exceeding maximum iterations , no more task - irrelevant variables found , no better classification performance obtained on validation set and so on .",
    "once the variable mask @xmath43 is obtained , task - irrelevant variables indicated by @xmath43 will be dropped in the following unsupervised training for auto - encoders .",
    "algorithm [ alg : ivs ] can be employed for each higher layer of stacked auto - encoders , therefore complex task - irrelevant information not eliminated on low level can be removed gradually on higher layers . from a model selection point of view , eliminating task - irrelevant variables can reduce complexity of networks therefore obtain better performance .",
    "clccc & bg - rand & bg - img & rot - bg - img +   & * sdae-1 * & @xmath57 & @xmath58 & @xmath59 +   & * sdae - ivs-1 * & @xmath60 & @xmath61 & @xmath62 +   & * sdae-2 * & @xmath63 & @xmath64 & @xmath65 +   & * sdae - ivs-2 * & @xmath66 & @xmath67 & @xmath68 +   & * sdae-3 * & @xmath69 & @xmath70 & @xmath71 +   & * sdae - ivs-3 * & @xmath72 & @xmath73 & @xmath74 +    in our experiments , we combined the proposed method with dae @xcite , which is called dae - ivs , and compared performances produced by stacked dae - ivs ( sdae - ivs ) , stacked dae ( sdae ) @xcite , pgbm @xcite , and ann @xcite .",
    "the baseline was sdae trained with standard training strategy described in @xcite .",
    "we tested different depth of sdae and sdae - ivs from 1 layer to 3 layers .",
    "each auto - encoder had 1000 hidden units and used tied weights . both encoder and decoder used sigmoid function and took cross - entropy loss as reconstruction error .",
    "the outputs of feature extractors learned by a layer were used as input variables of upper layer .",
    "multinominal logistic regression layer was added on both top of sdae and sdae - ivs to perform supervised fine - tuning . all training processes used stochastic gradient descent for parameter learning .",
    "our datasets were several variants of mnist dataset for recognizing images of handwritten digits @xcite , including mnist with random background(bg - rand ) or with image background ( bg - img ) and the combination of rotated digits with image background ( rot - bg - img ) .",
    "each dataset was split into three subsets : a training set ( 10000 examples ) for pre - training and fine - tuning the parameters , a validation set ( 2000 examples ) for model selection and a testing set ( 50000 examples ) on which the classification performance were reported .",
    "the hyper - parameters were chosen on the validation set , including the learning rate for pre - classification in algorithm [ alg : ivs ] ( candidate set [ 0.01 , 0.02 , 0.05 , 0.1 ] ) , the importance threshold ( candidate set [ 0.2 , 0.25 , 0.3 , 0.35 , 0.4 , 0.45 , 0.5 ] ) , learning rate for pre - training and fine - tuning ( candidate set [ 0.01 , 0.05 , 0.1 , 0.2 ] ) , gaussian noise standard deviation in sdae and sdae - ivs ( candidate set [ 0.1 , 0.15 , 0.2 , 0.25 , 0.3 , 0.35 , 0.4 ] ) , and pre - training epochs ( candidate set [ 60 , 120 , 180 , 240 , 300 ] ) . the loop in algorithm [ alg : ivs ] was stopped when no more task - irrelevant variables found and no better classification performance obtained on validation set .      during applying algorithm[alg : ivs ] , we can compute the importance value of each variable on each dataset(equation ( [ eq : normal - vector ] ) , ( [ eq : hyperplane - contribution ] ) and ( [ eq : task - contribution ] ) ) .",
    "examples and visualization of the importance of variable for each dataset are shown in figure [ fig : importance ] . in importance image ,",
    "the lighter and the darker areas correspond to the variables with higher importance and lower importance respectively .",
    "it can be seen that variables with high importance concentrate on the central area where digits mainly occupy .",
    "different from other datasets , the high - importance area of",
    "rot - bg - img looks like a disc because of the rotation of digits .",
    "we take the variables with importance value higher than a threshold as the task - relevant variable , otherwise as the task - irrelevant ones ( equation ( [ eq : task - mask ] ) ) .",
    "visualizations of the task - irrelevant and the task - relevant patterns ( weights of feature extractors learned by the first layer of sdae - ivs ) are showed in figure [ fig : features ] , where the threshold is set by experience .",
    "although there exists misclassification , most task - irrelevant patterns describe the background image , and most task - relevant patterns describe the foreground digit . the algorithm [ alg : ivs ] is an iterate process , and task - irrelevant variables are dropped in each iteration .",
    "this can indirectly improve the signal to noise ratio related to classification .",
    "figure [ fig : iteration_performance ] shows that not only the number of variables is decreased ( to 36% ) , but also the classification performance of the pre - classifier is improved .",
    "similar results can also be found on different layers of sdae - ivs trained on different datasets .",
    "let @xmath75 and @xmath76 be the number of task - relevant feature extractor learned by the first layer of sdae - ivs and sdae respectively .",
    "we used the the ratio of @xmath75 to @xmath77 to measure the effectiveness improvement of learning useful feature extractors .",
    "as shown in figure [ fig : number_ratio ] , all these curves are above 1 , which means that dae - ivs could learn more task - relevant feature extractor than dae . with the importance threshold increasing ,",
    "many actual task - relevant feature extractors were also dropped , thus these curves get lower .",
    "however , they are still above 1 .",
    "note that we do not use the curves to optimize the threshold of importance , we just concentrate on illustrating that feature selection is beneficial to training ae.for showing the effect of feature selection , we try to reconstruct the lower - layer data through the decoders of aes by using just the task - relevant features in higher - layer . by observing the reconstruction result",
    ", we can see whether our algorithm effectively eliminates the task - irrelevant variables and preserves the task - relevant ones . in figure",
    "[ fig : recosntruction ] , we show the reconstructions of raw data produced by sdea and sdae - ivs with different depth on different datasets .",
    "the reconstructions are clearer after dropping task - irrelevant variables , and the background information is significantly suppressed .      in table",
    "[ tab : classification ] , we show the test classification error rate of produced by sdae and sdae - ivs with different depth . it can be seen that in each depth the performance produced by sdae - ivs significantly outperforms the performance of sdae .",
    "these results suggest that our method can effectively help auto - encoders learn more and better task - relevant feature extractor so as to get better task performance .",
    "auto - encoders attempt to capture as much as possible of information in the input data and have to expend part of its capacity to learn task - irrelevant information if there exists .",
    "more importantly , task - irrelevant information may lead the eventual classification to overfitting resulting in bad performance .",
    "the proposed method is a simple and effective variable selection method to deal with this problem . through several rounds of variable selection ,",
    "the remaining input variables are fed into an auto - encoder to learn feature extractors .",
    "because this method is employed for each layer of stacked auto - encoders , it not only eliminates task - irrelevant information , but also prunes the deep network in a certain degree so as to efficiently control the model complexity to obtain better performance .",
    "experimental results show that the method can efficiently drop task - irrelevant variables and helps the auto - encoders learn more and better feature extractor .",
    "it helps sdae achieve significant improvements on classification performances .    in the future",
    ", we will explore some variable selection method that deduced from non - linear classification models , expecting to help stacked auto - encoders get better performance .",
    "this work was partially supported by the doctoral startup foundation of china three gorges university ( grant no .",
    "kj2013b064 ) , natural science foundation of hubei ( grant nos .",
    "2015cfb336 ) and national natural science foundation of china ( grant no .",
    "61502274 ) .",
    "arisoy , e. , sethy , a. , ramabhadran , b. , and chen , s. ( 2015 ) .",
    "bidirectional recurrent neural network language models for automatic speech recognition . in _",
    "acoustics , speech and signal processing ( icassp ) , 2015 ieee international conference on _ , pages 54215425 .",
    "bengio , y. ( 2013 ) .",
    "deep learning of representations : looking forward . in dediu ,",
    ", martn - vide , c. , mitkov , r. , and truthe , b. , editors , _ statistical language and speech processing _ , volume 7978 of _ lecture notes in computer science _ , pages 137 .",
    "springer berlin heidelberg .",
    "chen , m. , weinberger , k.  q. , sha , f. , and bengio , y. ( 2014 ) . marginalized denoising auto - encoders for nonlinear representations . in",
    "_ proceedings of the 31st international conference on machine learning ( icml-14 ) _ , pages 14761484 .",
    "ciresan , d. , meier , u. , and schmidhuber , j. ( 2012 ) .",
    "multi - column deep neural networks for image classification . in _",
    "computer vision and pattern recognition ( cvpr ) , 2012 ieee conference on _ , pages 36423649 .",
    "ieee .",
    "larochelle , h. , erhan , d. , courville , a. , bergstra , j. , and bengio , y. ( 2007 ) .",
    "an empirical evaluation of deep architectures on problems with many factors of variation . in _ proceedings of the 24th international conference on machine learning _ , pages 473480 .",
    "acm .",
    "rifai , s. , mesnil , g. , vincent , p. , muller , x. , bengio , y. , dauphin , y. , and glorot , x. ( 2011b ) . higher order contractive auto - encoder . in _ machine learning and knowledge discovery in databases _ ,",
    "pages 645660 .",
    "springer .",
    "rifai , s. , vincent , p. , muller , x. , glorot , x. , and bengio , y. ( 2011c ) .",
    "contractive auto - encoders : explicit invariance during feature extraction . in _ proceedings of the 28th international conference on machine learning ( icml-11 ) _ , pages 833840 .",
    "vincent , p. , larochelle , h. , bengio , y. , and manzagol , p .- a .",
    "( 2008 ) . extracting and composing robust features with denoising autoencoders . in _ proceedings of the 25th international conference on machine learning _ ,",
    "pages 10961103 .",
    "vincent , p. , larochelle , h. , lajoie , i. , bengio , y. , and manzagol , p .- a .",
    "stacked denoising autoencoders : learning useful representations in a deep network with a local denoising criterion .",
    ", 11:33713408 ."
  ],
  "abstract_text": [
    "<S> auto - encoders are often used as building blocks of deep network classifier to learn feature extractors , but task - irrelevant information in the input data may lead to bad extractors and result in poor generalization performance of the network . in this paper , via dropping the task - irrelevant input variables the performance of auto - encoders can be obviously improved .specifically , an importance - based variable selection method is proposed to aim at finding the task - irrelevant input variables and dropping them.it firstly estimates importance of each variable , and then drops the variables with importance value lower than a threshold . in order to obtain better performance , </S>",
    "<S> the method can be employed for each layer of stacked auto - encoders . </S>",
    "<S> experimental results show that when combined with our method the stacked denoising auto - encoders achieves significantly improved performance on three challenging datasets .    </S>",
    "<S> hui shen is currently a phd student in school of automation , huazhong university of science and technology , china . </S>",
    "<S> she obtained her bachelor s in wuhan polytechnic university , china.and got her master s in huazhong university of science and technology , china . </S>",
    "<S> her main fields of interest are neural network , deep learning and machine learning .    </S>",
    "<S> dehua li is a professor in school of automation , huazhong university of science and technology , china.he got his bachelor s from wuhan university in 1970 . and </S>",
    "<S> he spent one year as senior visiting scholar in ai department of university of edinburgh , uk . </S>",
    "<S> his research interests including ai , neotic science , and machine learning .    </S>",
    "<S> hong wu obtained his bachelor s in computer science and technology from wuhan university , china . and got his master s in huazhong university of science and technology , china . </S>",
    "<S> now he is a phd student of the school of automation , huazhong university of science and technology , china . </S>",
    "<S> his main research fields are neural network , deep learning and machine learning .    </S>",
    "<S> zhaoxiang zang is an associate professor in college of computer and information technology , china three gorges university , yichang hubei , china , and he is member of hubei key laboratory of intelligent vision based monitoring for hydroelectric engineering , china three gorges university , china . </S>",
    "<S> he obtained his master s and phd from huazhong university of science and technology , china.his research interests mainly in the fields of machine learning , computational intelligence and computer game intelligence . </S>"
  ]
}