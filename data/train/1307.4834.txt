{
  "article_text": [
    "outliers are observations that depart from the pattern of the majority of the data .",
    "identifying outliers is a major concern in data analysis for at least two reasons .",
    "first , because a few outliers , if left unchecked , will exert a disproportionate pull on the fitted parameters of any statistical model , preventing the analyst from uncovering the main structure in the data . additionally , one may also want to find outliers to study them as objects of interest in their own right . in any case , detecting outliers when there are more than two variables is difficult because we can not inspect the data visually and must rely on algorithms instead .",
    "formally , this paper concerns itself with the most basic variant of the outlier detection problem in the regression context .",
    "the general setting is that of the ordinary linear model : @xmath0 where @xmath1 and @xmath2 have continuous distributions , @xmath3 and @xmath4 .",
    "then , given a @xmath5-vector @xmath6 , we will denote the residual distance of @xmath7 to @xmath8 as : @xmath9 we have a sample of @xmath10 observations @xmath11 with @xmath12 , at least @xmath13 of which are well fitted by model @xmath14 and our goal is to identify reliably the remaining ones .",
    "a more complete treatment of this topic can be found in textbooks @xcite .    in this article",
    "we introduce rcs , a new procedure for finding regression outliers .",
    "we also detail fastrcs , a fast algorithm for computing it .",
    "the main output of fastrcs is an outlyingness index measuring how much each observation departs from the linear model fitting the majority of the data .",
    "the rcs outlyingness index is affine and regression equivariant ( meaning that it is not affected by transformations of the data that do not change the ranking of the squared residuals ) and can be computed efficiently for moderate values of @xmath5 and large values of @xmath10 . for easier outlier detection problems ,",
    "we find that fastrcs yields similar results as state of the art outlier detection algorithms . when considering more difficult cases however we find that the solution we propose leads to much better outcomes .    in the next section we motivate and define the rcs outlyingness and fastrcs .",
    "then , in section 3 we compare fastrcs to several competitors on synthetic data . in section 4",
    "we conduct two real data comparisons .",
    "given a sample of @xmath10 potentially contaminated observations @xmath11 , the goal of fastrcs is to reveal the outliers .",
    "it is well known that this problem is also equivalent to that of finding a fit of model @xmath14 close to the one we would have found without the outliers . indeed , to ensures that they stand out in a plot of the fitted residuals",
    ", it is necessary to prevent the outliers from pulling the fit in their direction .",
    "other equivariant algorithms that share the same objective are fastlts @xcite and fasts @xcite .    however , in tests and real data examples , we often encounter situations where the outliers have completely swayed the fit found by fastlts and fasts yielding models that do not faithfully describe the multivariate pattern of the bulk of the data .",
    "consider the following example .",
    "the three panels in figure  [ mcs : f1a ] depict the same 100 data points @xmath15 : 70 drawn from model @xmath14 and 30 drawn from a concentrated cluster of observations .",
    "the orange , solid lines in the first two panels depict , respectively , the line corresponding to the fit found by fastlts ( left ) and fasts ( center ) , both computed using the ` r ` package ` robustbase ` @xcite with default parameters ( the dashed orange lines depict the 95% prediction intervals ) . in both cases ,",
    "the fits depicted in the first two panels do not adequately describe in the sense of model @xmath14 the pattern governing the distribution of the majority of the data .",
    "this is because the outliers have pulled the fits found by fastlts and fasts so much in their directions that their distances to it no longer reveals them .",
    "a salient feature of the algorithm we propose is its use of a new measure we call the @xmath16-index ( which we detail in the next section ) to select ( among many random such subsets ) an @xmath17-subset of uncontaminated data .",
    "the @xmath16-index characterizes the degree of spatial cohesion of a cloud of points and its main advantage lies in its insensitivity to the configuration of the outliers .",
    "as we argue below , this makes the fasthcs fit as well as the the outlyingness index derived from it more reliable .",
    "the basic outline of the algorithm is as follows .",
    "given an @xmath10 by @xmath5 data matrix , fastrcs starts by drawing @xmath18 random subsets ( @xmath19-subsets ) , denoted @xmath20 , each of size @xmath19 of @xmath21 . then , the algorithm grows each @xmath22 into a corresponding @xmath23",
    ", a subset of size @xmath17 of @xmath21 ( the letter @xmath24 without a subscript will always denote a subset of size @xmath17 of @xmath21 ) .",
    "the main innovation of our approach lies in the use of the @xmath16-index , a new measure we detail below , to characterize each of these @xmath23 s .",
    "next , fasthcs selects @xmath25 , the @xmath23 having smallest @xmath16-index .",
    "then , the @xmath17 observations with indexes in @xmath25 determine the so - called raw fastrcs fit .",
    "finally , we apply a one step re - weighting to this raw fastrcs fit to get the final fastrcs fit .",
    "our algorithm depends on two additional parameters ( @xmath26 and @xmath18 ) but for clarity , these are not discussed in detail until section  [ hcs : s24 ] .",
    "we begin by detailing the computation of the @xmath16-index for a given subset @xmath23 .",
    "denote @xmath27 the coefficients of the hyperplane ( the index @xmath28 identifies these directions ) through @xmath5 data - points from @xmath23 ( we detail below how we pick these @xmath5 data - points ) and @xmath29 the set of indexes of the @xmath17 data - points with smallest values of @xmath30 : @xmath31 where @xmath32 denotes the @xmath17-th order statistic of a vector @xmath33 .",
    "then , we define the _ incongruence index _ of @xmath23 along @xmath27 as : @xmath34 with the convention that @xmath35 . this index is always positive and will have small value if the vector of @xmath30 of the members of @xmath23 greatly overlaps with that of the members of @xmath29 . to remove the dependence of equation on @xmath27 , we measure the incongruence of @xmath23 by considering the average over many directions : @xmath36 where @xmath37 is the set of all regression hyperplanes through @xmath5 data - points with indexes in @xmath23 .",
    "we call the @xmath23 with smallest @xmath38 the _ residual congruent subset _ and denote the index set of its members as @xmath25 .",
    "next , the raw fastrcs estimates are the parameters @xmath39 fitted by ols to the observations with indexes in @xmath25 .",
    "in essence , the @xmath16 index characterizes the homogeneity of the members of a given @xmath17-subset @xmath23 in terms of how much their residuals overlap with those of the @xmath29 over many random regressions . in practice",
    ", it would be too laborious to evaluate equation over all members of @xmath37 .",
    "a practical solution is to take the average over a random sample of @xmath26 hyperplanes @xmath40 instead .",
    "the @xmath16-index is based on the observation that when the datum with indexes in @xmath23 form an homogeneous cloud of points , @xmath41 tends to be large over many projection @xmath42 , causing @xmath38 to be smaller .",
    "consider the example shown in figure  [ mcs : f1b ] .",
    "both panels depict the same set of @xmath43 data - points @xmath44 .",
    "these points form two separate cluster .",
    "the main group contains 70 points and is located on the left hand side .",
    "each panel illustrates the behavior of the @xmath16 index for a given @xmath17-subset of observations .",
    "@xmath45 ( left ) forms a set of homogeneous observations all drawn from the same cluster .",
    "@xmath46 , in contrast , is composed of data points drawn from the two disparate clusters . for each @xmath23-subset , @xmath47 , we drew two regression lines @xmath48 ( dark blue , dashed ) and @xmath49 ( light orange ) .",
    "the dark blue dots show the members of @xmath50 .",
    "similarly , light orange dots show the members of @xmath51 .",
    "the diamonds ( black squares ) show the members of @xmath52 ( @xmath53 ) that do not belong to @xmath23 .",
    "after just two regression , the number of non - overlapping residuals ( i.e.\\ { @xmath54 ) is 10 ( @xmath55 ) and 21 ( @xmath56 ) respectively .",
    "as we increase the number of regression lines @xmath27 , this pattern repeats and the difference between an @xmath17-subset that contains the indexes of an homogeneous cloud of points and one that does not grows steadily .        for a given @xmath17-subset @xmath23 , the @xmath16 index measures the typical size of the overlap between the members of @xmath23 and those of @xmath29 . given two vector of coefficients @xmath48 , @xmath49 , the members of @xmath52 and @xmath53 not in @xmath23 ( shown as diamonds and black squares in figure  [ mcs : f1b ] ) will decrease the denominator in equation @xmath57 without affecting the numerator , increasing the overall ratio .",
    "consequently , @xmath17-subsets whose members form an homogeneous cloud of points will have smaller values of the @xmath16 index .",
    "crucially , the @xmath16 index characterizes an @xmath17-subset composed of observations forming an homogeneous cloud of points independently of the configuration of the outliers .",
    "for example , the pattern shown in figure  [ mcs : f1b ] would still hold if the cluster of outliers were more concentrated .",
    "this is also illustrated in the third sub panel in figure  [ mcs : f1a ] where the parameters fitted by fastrcs are not unduly attracted by members of the cluster of concentrated outliers located on the right . in sections [ mcs :",
    "s3 ] and [ mcs : s4 ] , we show that this new characterization allows fastrcs to reliably select uncontaminated @xmath17-subsets .",
    "this includes many situations where competing algorithms fail to do so .",
    "first though , the following section details the fastrcs algorithm .      .",
    "to compute the rcs outlyingness index , we propose the fastrcs algorithm @xmath58 .",
    "an important characteristic of fastrcs is that it can detect exact fit situations : when @xmath17 or more observations lie exactly on a subspace , fastrcs will return the indexes of an @xmath17-subset of those observations and the hyperplane fitted by fastrcs will coincide with the subspace .    for each of the @xmath18",
    "starting subset @xmath22 , step @xmath59 grows the size of the corresponding @xmath60 from @xmath61 when @xmath62 to its final size ( @xmath17 ) in @xmath63 steps , rather than in one as is done in fastlts .",
    "we find that this improves the robustness of the algorithm when outliers are close to the good data .",
    "we also find that increasing @xmath63 does not improve performance much if @xmath63 is greater than 3 and use @xmath64 as default .",
    "0.25 cm    ' '' ''    0.1 cm @xmath65    ' '' ''    for = @xmath55 to @xmath18 do : + @xmath66 : @xmath67 + @xmath59:for = @xmath68 to @xmath63 do : + @xmath69 + set @xmath70 + set @xmath71 +   + end for + @xmath72 + @xmath73 : compute @xmath74 + end for + keep @xmath25 , the subset @xmath23 with lowest @xmath38 .",
    "-0.1 cm    ' '' ''    0.25 cm    empirically also , we found that small values for @xmath26 , the number of elements of @xmath40 , is sufficient to achieve good results and that we do not gain much by increasing @xmath26 above 25 , so we set @xmath75 as the default .",
    "that such a small number of random regressions suffice to reliably identify the outliers is remarkable .",
    "this is because the hyperplanes used in fastrcs are fitted to @xmath5 observations drawn from the members of @xmath23 rather than , say , indiscriminately from among the entire set of data - points .",
    "our choice always ensures a wider spread of directions when @xmath23 is uncontaminated and this yields better results .",
    "finally , in order to improve its small sample accuracy , we add a re - weighting step to our algorithm .",
    "in essence , this re - weighting strives to award some weight to those observations lying close enough to the model fitted to the members of @xmath25 .",
    "the motivation is that , typically , the re - weighted fit will encompass a greater share of the uncontaminated data . over the years ,",
    "many re - weighting procedures have been proposed @xcite .",
    "the simplest is the so called one step re - weighting ( * ? ? ?",
    "* pg 202 ) .",
    "given an optimal @xmath17-subset @xmath25 , we get the final fastrcs parameters by fitting model @xmath14 to the members of @xmath76 fastlts and fasts also use a re - weighting step ( @xcite and @xcite ) and , for all three algorithms , we will refer to the raw estimates as @xmath77 and to the final , re - weighted vector of fitted coefficients as @xmath78 .",
    "like fastlts and fasts , fastrcs uses many random @xmath19-subsets as starting points",
    ". the number of initial @xmath19-subsets , @xmath18 , must be large enough to ensure that at least one of them is uncontaminated . for fastlts and fasts , for each starting @xmath19-subset , the computational complexity scales as @xmath79 comparable to fastrcs for which it is @xmath80 .",
    "the value of @xmath18 ( and therefore the computational complexity of all three algorithms ) grows exponentially with @xmath5 .",
    "the actual run times will depend on implementation choices but in our experience are comparable for all three . in practice",
    "this means that all three methods become impractical for values of @xmath5 much larger than 25 .",
    "this is somewhat mitigated by the fact that they all belong to the class of so called ` embarrassingly parallel ' algorithms , i.e. their time complexity scales as the inverse of the number of processors meaning that they are particularly well suited to benefit from modern computing environments . to enhance user experience",
    ", we implemented fastrcs in c-0.2ex   code wrapped in a portable ` r ` package @xcite distributed through ` cran ` ( package ` fastrcs ` ) .",
    "in this section we evaluate the behavior of fastrcs numerically and contrast its performance to that of fastlts and fasts .",
    "for all three , we used their respective ` r ` implementation ( package ` robustbase ` for the last two and ` fastrcs ` for fastrcs ) with default settings except for the number of starting subsets which for all algorithms we set according to equation @xmath81 and the maximum number of iterations for fasts which we increased to 1000 .",
    "each algorithm returns a vector of estimated parameters @xmath82 as well as a an @xmath17-subset ( denoted @xmath83 ) derived from it : @xmath84 our evaluation criteria are the bias of @xmath85 and the rate of misclassification of the outliers .",
    "given a central model @xmath86 and an arbitrary distribution @xmath87 ( the index @xmath73 stands for contamination ) , consider the contamination model : @xmath88 where @xmath89 is the rate of contamination of the sample .",
    "the bias measures the differences between the coefficients fitted to @xmath90 and those governing @xmath91 and is defined as the norm @xcite : @xmath92 for an affine and regression equivariant algorithm , w.l.o.g .",
    ", we can set @xmath93 and @xmath94 so that @xmath95 reduces to @xmath96 and we will use the shorthand @xmath97 to refer to it .    evaluating the bias of an algorithm is an empirical matter . for a given sample ,",
    "it will depend on the rate of contamination and the distance separating the outliers from the good part of the data .",
    "the bias will also depends on the spatial configuration of the outliers ( the choice of @xmath87 ) .",
    "fortunately , for affine and regression equivariant algorithms the worst configurations of outliers ( those causing the largest biases ) are known and so we can focus on these cases .",
    "we can also compare the algorithms in terms of rate of contamination of their final @xmath83 , the subset of @xmath17 observations with smallest values of @xmath98 . denoting @xmath99 the index set of the contaminated observations ,",
    "the misclassification rate is : @xmath100 this measure is always in @xmath101 $ ] , thus yielding results that are easier to compare across configurations of outliers and rates of contamination .",
    "a value of 1 means that @xmath83 contains all the outliers .",
    "the main difference with the bias criterion is that the misclassification rate does not account for how disruptive the outliers are .",
    "for example , when the outliers are close to the good part of the data , it is possible for @xmath102 to be large without a commensurate increase in @xmath97 .",
    "we generate many contaminated data - sets @xmath103 of size @xmath10 with @xmath104 where @xmath105 and @xmath106 are , respectively , the genuine and outlying part of the sample . for equivariant algorithms , the worst - case configurations are known .",
    "these are the configurations of outliers that are , in a well defined sense , the most harmful . in increasing order of difficulty",
    "these are :    * shift configuration .",
    "if we constrain the adversary to ( a ) set @xmath107 and ( b ) place the @xmath108 at a distance @xmath109 of @xmath110 . then , the adversary will set @xmath111 ( theorem 1 in @xcite ) and @xmath109 in order to satisfy ( b ) .",
    "intuitively , this makes the components of the mixture the least distinguishable from one another . * point - mass configuration .",
    "if we omit constraint ( a ) above but keep ( b ) , the adversary will place @xmath112 on a subspace so that @xmath113 ( theorem 2 in @xcite ) .",
    "intuitively , this maximizes the cost of misidentifying any single outlier .",
    "we can generate the @xmath114 s and the @xmath115 s from standard normal distributions since all methods under consideration are affine and regression equivariant .",
    "likewise , the @xmath112 s are draws from a multivariate normal distribution with @xmath116 equal to @xmath117 ( shift ) or @xmath118 ( point - mass ) and @xmath119 set so that @xmath120 where @xmath121 is either one of 2 ( nearby outliers ) or 8 ( far away outliers )",
    ". finally , @xmath122 is one of @xmath123 or @xmath124 depending again on whether the outlier configuration is shift or point - mass .",
    "next , for a given value of the parameters in model @xmath14 and a data matrix @xmath112 , the bias will depend on the vertical distance between the outliers and the genuine observations .",
    "we will place the outliers such that they lie at a distance @xmath109 of the good data : @xmath125 where @xmath126 is half the asymptotic width of the usual ls prediction interval , evaluated at @xmath7 .",
    "the complete list of simulation parameters follows :    * the dimension @xmath5 is one of @xmath127 and the sample size is @xmath128 , * the configuration of the outliers is either shift or point - mass .",
    "* @xmath129 is the proportion of the sample that can be assumed to follow model @xmath14 . in section  [ mcs : sra ] ( section  [ mcs : srb ] )",
    "we consider the case where we set @xmath130 ( @xmath131 ) .",
    "* @xmath132 ( when @xmath130 ) or @xmath133 ( when @xmath131 ) . * the distance separating the outliers from the good data on the design space is @xmath134 .",
    "the distance separating the outliers from @xmath135 is @xmath136 . *",
    "the number of initial @xmath19-subsets @xmath18 is given by @xcite @xmath137 with @xmath138 so that the probability of getting at least one uncontaminated starting point is always at least 99 percent .",
    "we also considered other configurations such as so - called vertical outliers ( @xmath139 ) or where @xmath109 is extremely large ( i.e. @xmath140 ) , but they posed little challenge for any of the algorithms , so we do not discuss these results . in figures  [ rcs : f1 ] to  [ rcs : f6 ] we display the bias ( left panel ) and the misclassification rate ( right panel ) for discrete combinations of the dimension @xmath5 , contamination rate @xmath89 and the degree of separation between the outliers and the genuine observations on the design space ( which we control through the parameter @xmath121 ) . in all cases",
    ", we expect the outlier detection problem to become monotonically harder as we increase @xmath5 and @xmath89 .",
    "furthermore , undetected outliers that are located far away from the good data on the design space will have more leverage on the fitted coefficients .",
    "for that reason , we also expect the biases to increase monotonically with @xmath121 .",
    "therefore , not much information will be lost by considering a discrete grid of a few values for these parameters .",
    "the configurations also depend on the distance separating the outliers from the true model , which we control through the simulation parameter @xmath109 .",
    "the effects of @xmath109 on the bias are harder to foresee : clearly nearby outliers will be harder to detect but misclassifying distant outliers will increase the bias more .",
    "therefore , we will test the algorithms for many values ( and chart the results as a function ) of @xmath109 . for both the bias and the misclassification curves , for each algorithm",
    ", a solid colored line will depict the median and a dotted line ( of the same color ) the 75th percentile . here",
    ", each panel will be based on 1000 simulations .",
    "the first part of the simulation study covers the case where there is no information about the extent to which the data is contaminated . then , for each algorithm , we have to set the size of the active subset to @xmath17 , corresponding to the lower bound of slightly more than half of the data . for fastlts and fastrcs",
    "there is a single parameter @xmath141 controling the size of the active subset so that we set @xmath130 . for fasts , we follow ( * ? ? ?",
    "* table 19 , p. 142 ) and set the value of the tunning parameters to @xmath142 .         in figure",
    "[ rcs : f1 ] we display the @xmath102 and @xmath97 curves of each algorithm at @xmath143 as a function of @xmath109 for different values of @xmath5 and @xmath144 for the shift configuration . starting at the second column , the @xmath102 curves are much higher for fasts and fastlts than for fastrcs but the outliers included in their selected @xmath17-subsets do not exert enough pull on the fitted model to yield correspondingly large biases . from the the third column",
    "onwards , the outliers are now numerous enough to exert a visible pull on the coefficients fitted by fastlts and fasts . by the fourth column of both panel ,",
    "the performances of these two algorithms deteriorates further and they now fail to detect even outliers located very far from @xmath135 . far away",
    "outliers , if left unchecked , will exert a larger leverage on the @xmath82 and this is visible in the bias curves of fastlts and fasts which are now much higher . the performance of fastrcs , on the other hand",
    ", is not affected by @xmath109 , @xmath5 or @xmath89 and remains comparable throughout .",
    "in figure  [ rcs : f2 ] , we again examine the effects of shift contamination , but for @xmath145 .",
    "now , starting at @xmath146 and @xmath147 the @xmath102 curves of fastlts and fasts show that these algorithms can not seclude even those outliers located within a rather large slab around @xmath135 . furthermore , from @xmath148 , as the configurations get harder , the maximum of the bias curves clearly show that the parameters fitted by these two algorithms diverge from @xmath149 in an increasingly large range of values of @xmath109 . comparing the @xmath102 curves of fastlts and fasts in figure  [ rcs : f2 ] with those in figure  [ rcs : f1 ] , we see that these two algorithms are noticeably less successful at identifying outliers far removed on the design space than nearer ones . in contrast , in figure  [ rcs : f2 ]",
    ", we see that the performance of fastrcs is very good both in terms of @xmath97 and @xmath102 .",
    "furthermore , these measures of performance remain consistently good across the different values of @xmath5 , @xmath89 and @xmath109 . comparing the performance of fastrcs in figure  [ rcs : f2 ] with the results shown in figure  [ rcs : f1 ]",
    "we also see that our algorithm is unaffected by the greater separation between the outliers and the good data in the @xmath150 space .         in figure  [ rcs : f3 ]",
    ", we show the results for the more difficult case of point - mass contamination with @xmath143 . as expected , we find that concentrated outliers are causing much higher biases for fasts and fastlts , especially in higher dimensions .",
    "already when @xmath146 , fasts displays biases that are sensibly higher than their maximum values against the shift configuration . from @xmath151 ,",
    "both algorithms also yields contaminated @xmath17-subsets for most values of @xmath109 .",
    "looking at the @xmath102 curves of these two algorithm , we also see that in many of cases , the optimal @xmath17-subsets selected by fastlts and fasts actually contain a higher fraction of outliers than the original sample .",
    "in contrast , for fastrcs , the performance curves in figure  [ rcs : f3 ] are essentially similar to those shown in figure  [ rcs : f1 ] , attesting again that our algorithm is not unduly affected by the spatial concentration of the outliers .",
    "in figure  [ rcs : f4 ] , we examine the effects of point - mass contamination for @xmath145 . as in the case for shift contamination , we see that an increase in @xmath121 entails a decline in performance for both fastlts and fasts with high maximum values of the bias curves now starting at @xmath152 already . in terms of @xmath102 curves , too , both algorithm have their weakest showings , with selected subsets that have a higher rate of contamination that the original data - sets for most values of @xmath109 starting from the middle rows of the first column already . as before , the lower rate of outlier detection combined with the greater separation of the outliers on the design space means that the bias curves of fastlts and fasts are worse than those shown in figure  [ rcs : f3 ] .",
    "again , contrast this with the behavior of fastrcs which maintains low and constant bias and misclassification curves throughout . further comparing the performance curves of fastrcs for the configuration considered in figure  [ rcs : f4 ] to those considered in figures  [ rcs : f3 ] and  [ rcs : f2 ] we see , again , that fastrcs is not affected by either the spatial concentration or the degree of separation of the outliers .      in this section",
    ", we now consider the case , important in practice , where the user can confidently place an upper bound on the rate of contamination of the sample . to fix ideas",
    ", we will set the proportion of the sample assumed to follow model @xmath14 , to approximately @xmath153 so that in this section @xmath154 . for fastlts and fastrcs we adapt the algorithms by setting their respective @xmath141 parameter to 0.75 . for fasts",
    ", we again follow ( * ? ? ? * table 19 , p. 142 ) and now set @xmath155 .",
    "for all three algorithms we also reduce the number of starting subsets by setting @xmath156 in equation @xmath81 .",
    "then , as before , we will measure the effects of the various configurations of outliers on the algorithms ( but now for @xmath133 ) .",
    "figure  [ rcs : f5 ] summarizes the case where @xmath143 .",
    "the first two columns of @xmath97 ( left ) and @xmath102 contain results for the shift configuration , and the last two , for point - mass . for the shift configuration , when the outliers are far from the hyperplane fitting the good part of the data , all methods have low biases .",
    "though fasts and fastlts still deliver optimal @xmath17-subsets that are heavily contaminated by outliers , these do not exert a large enough pull on the @xmath82 to affect the biases .",
    "generally all methods yield results that are comparable with the corresponding cases shown in figure  [ rcs : f1 ] . in the case of point - mass contamination ( shown in the last two columns ) , fastlts and fasts again experience greater difficulty with the point - mass configuration and results are worse than those shown in figure  [ rcs : f3 ] for the corresponding settings . again",
    ", the results for fastrcs remains similar to those depicted in figure  [ rcs : f3 ] .",
    "figure  [ rcs : f6 ] depicts the simulation results for @xmath131 when @xmath145 . as before ,",
    "this places additional strain on fastlts and fasts . in the case of shift contamination ,",
    "the results for these two algorithms are qualitatively the same as in figure  [ rcs : f2 ] . for fastlts and fasts ,",
    "the results are worst for point - mass contamination and show , again , a clear deterioration vis -  - vis figure  [ rcs : f4 ] .",
    "now , fastlts fails to identify the outliers in any but the easiest configuration , and fasts performs poorly in all configurations .",
    "in contrast , fastrcs yields the same performance , whether it is measured in terms of @xmath97 or @xmath102 , as those obtained in the same configurations , but with a higher value of @xmath141 .",
    "overall in our tests , we observed that fastlts and fasts both exhibit considerable strain . in many situations these two algorithms yield optimal @xmath17-subsets that are so heavily contaminated as to render them wholly unreliable for finding the outliers . in these situations , both algorithm also invariably yield vectors of fitted parameters @xmath82 that are completely swayed by the outliers and , consequently ,",
    "very poorly fit the genuine observations .",
    "we note that as the configurations become more challenging , the bias curves for fastlts and fasts deviate upward , and the @xmath102 curves advance to the right in a systemic manner .",
    "we observe no corresponding effect for fastrcs .",
    "these quantitative differences between fastrcs and the other two algorithms , repeated over adversary configurations , lead us to interpret them as indicative of a qualitative difference in robustness .",
    "in the previous section , we compared fastrcs to two state of the art outlier detection algorithms in situations that were designed to be most challenging for equivariant procedures . in this section",
    ", we will also compare fastrcs to fastlts and fasts , but this time using two real data examples . the first illustrate the use of fastrcs in situations where @xmath157 is small ( typically these situations are particularly challenging for anomaly detection procedures ) while the second example illustrates the use of fastrcs in situations where @xmath5 is large .      in this section , we consider a real data problem from the field of engineering : the concrete `` slump '' test data set @xcite .",
    "this data - set consists of 7 input variables measuring the quantity of cement , fly ash , blast furnace slag , water , super - plasticizer , coarse aggregate , and fine aggregate used to make the corresponding variety of concrete .",
    "finally , we use 28-day compressive strength as response variable . the `` slump '' data - set",
    "is actually composed of two class of observations collected over two separate periods with the first set of measurements predating the second ones by several years . after excluding several data - points that are anomalous in that the values of either the slag and fly ash variable is exactly zero , we are left with 59 measurements , with first 35 data ( last 24 ) points belonging to the set @xmath158 ( @xmath159 ) of older ( newer ) observations . in this exercise",
    ", we will hide the vector of class labels , in effect casting the members of @xmath159 as the outliers and the task of all the algorithms will be to reveal them .    to ensure reproducibility we ran all algorithms with option ` seed=1 ` and default values of the parameters ( except for the number of starting subset @xmath18 which we set according to equation  ) and included both data - sets used in this section in the fastrcs package .",
    "for all three algorithms , we will denote as @xmath160 the fit found by algorithm @xmath161 , @xmath162 will denote the subset of observations classified as `` good '' and @xmath163 its complement .",
    "next , we ran all three algorithms on the `` slump '' data - set , using @xmath164 . in figure [ mcs : f7 ]",
    ", we display the standardized residual distance @xmath165 , for each algorithm in a separate panel .",
    "the dark blue residual ( light orange ) points ( triangles ) depict the @xmath165 for the 35 members of @xmath158 ( 24 members of @xmath159 ) .",
    "an horizontal line at @xmath166 shows the usual outlier rejection threshold . for both fastlts and fasts , the values of the @xmath165 ( shown in the left and middle panel of figure  [ mcs : f7 ] respectively ) corresponding to the members of @xmath158 and @xmath159 largely overlap . as a result ,",
    "in both cases , the optimal @xmath17-subset includes both members of @xmath159 and @xmath158 .",
    "looking at the composition of the @xmath83 subset found by fastlts , we see that it contains 19 ( out of @xmath167 ) members of @xmath159 . for fasts",
    ", @xmath83 contains 13 members of @xmath159 .",
    "considering now the observations classified as good ( the members of @xmath168 ) by both algorithms , we see that fastlts ( fasts ) finds only 15 ( 5 ) outliers .",
    "the outliers found by both approaches are well separated from the fitted model ( the nearest outlier lies at a standardized residuals distance of @xmath169 for both algorithms ) but they tend to be located near the good observations on the design space .",
    "for example , the nearest outlier lie at a mahalanobis distance of @xmath170 wrt to the members of @xmath171 and @xmath172 wrt to the members of @xmath173 .",
    "finally , the members of the @xmath168 subsets identified by both algorithms contains , again , many members of @xmath159 : 21 ( out of @xmath174 ) for @xmath171 and 21 ( out of 54 ) for @xmath173 .",
    "in contrast to the innocent looking residuals seen in the first two panels of figure  [ mcs : f7 ] , the plot of the @xmath165 corresponding to the fastrcs fit ( shown in rightmost panel ) , clearly reveals the presence of many observations that do not follow the multivariate pattern of the bulk of the data .",
    "for example , the outliers identified by fastrcs are much more distinctly deviating from the pattern set by the majority of the data than those identified by fasts and fastlts : the nearest outlier now stands at @xmath175 . furthermore , the composition of these two groups is now consistent with the history of the `` slump '' data - set as the @xmath83 found by fastrcs is solely populated by members of @xmath158 and @xmath176 contains all the ( and only those ) 35 observations from @xmath158 .",
    "interestingly , the two groups of observations identified by fastrcs are clearly distinct on the design space where they form two well separated cluster of roughly equal volume on the design space . for example",
    ", the closest member of @xmath177 lies at a mahalanobis distance of over @xmath178 wrt the members of @xmath176 .",
    "the blending of members of both @xmath159 and @xmath158 in both @xmath171 and @xmath173 despite the large separation between the members @xmath159 and those from @xmath158 , on the design space as well as to the hyperplane fitting the observations with indexes in @xmath158 together suggest that the optimal subset selected by both fastlts and fasts do in fact harbor many positively harmful outliers .    consecutively , because the outliers in this example are well separated from the bulk of the data , the presence of even a handful of them in @xmath171 ( @xmath173 ) indicates that they have pulled the resulting fits so much in their direction as to render parameters fitted by both algorithms altogether unreliable .      in this subsection",
    ", we illustrate the behavior of fastrcs on a second real data problem from a commercial application : the `` do nt get kicked ! ''",
    "data - set  @xcite .",
    "this data - set contains thirty - four variables pertaining to the resale value of 72,983 cars at auctions . of these , we use of the continuous ones to model the sale price of a car as a linear function of eight measures of current market acquisition prices , the number of miles on the odometer and the cost of the warranty .",
    "more specifically , we consider as an illustrative example all 488 data - points corresponding to sales of the chrysler town & country but the pattern we find below repeats for many other cars in this data - set .",
    "we ran all three algorithms on the data - set ( with @xmath179 ) and , in figure [ mcs : f8 ] , we plot the vector of fitted standardized residual distance @xmath165 returned by each in a separate panel together with , again , an horizontal line at @xmath180 demarcating the usual outlier rejection threshold . finally ,",
    "for each algorithm , we also show the observations flagged as influential as light orange triangles .",
    "we first discuss the results of fastlts and fasts jointly ( shown in the left and middle panel of figure  [ mcs : f8 ] respectively ) since they are broadly similar . here again , the first two panels depict a reasonably well behaved data - set with very few harmful outliers . overall the number of data points for which @xmath165 is larger than 2.5 is 13 for fastlts and 14 for fasts .",
    "these are quiet close to the the fitted @xmath82 and the smallest value of @xmath165 for the outliers is @xmath166 for both algorithms ( the second nearest lies at a standardized distance of @xmath181 ) .",
    "furthermore , these outliers are close , on the design space , to the majority of the data .",
    "for example , for both algorithms , the nearest outlier lies at a mahalanobis distance of @xmath182 wrt to the good observations ( the second closest is located at a distance of @xmath183 ) .",
    "overall , both algorithms suggest that observations forming the _ do nt get kicked !",
    "_ data - set is broadly consistent with model @xmath14 except for @xmath184% of outliers .    in this example",
    "again , we find that the fastrcs fit reveals a much more elaborate structure to this data - set . in this case , @xmath176 is composed of 262 observations , barely more than @xmath185 which imply a data - set beset by outliers .",
    "furthermore , even a cursory inspection of the standardized residuals clearly reveals the presence of a large group of observations not following the multivariate pattern of the bulk of the data .",
    "surprisingly in the light of the fit found by the other algorithms , we find that the outliers identified by fastrcs in this data - set are actually very far from the model fitting the bulk of the data .",
    "setting aside row 171 ( visible on the right panel of figure  [ mcs : f8 ] as the isolated outlier lying somewhat closer to the genuine observations ) , we find that the nearest outlier lies at a standardized residual distance of over 120 wrt to the fastrcs fit on the model space .",
    "considering now the design space alone , we find that here too the members of @xmath176 are clearly separated from the bulk of the data : we find that the nearest outlier ( again setting observation 171 aside ) lies at a mahalanobis distance of over @xmath186 wrt to the members of @xmath176 .",
    "a salient feature of this exercise is that the outliers in the `` do nt get kicked ! ''",
    "data - set are genuine discoveries in the sense that when examining the variables ( including the qualitative ones not used in this analysis ) , we could not find any single one exposing them , yet they materially affect the model . as with the `` slump '' data - set , the `` do nt get kicked ! ''",
    "data - set is also interesting because the outliers identified ( exclusively ) by fastrcs are not analogous to the worst case configurations we considered in the simulations . indeed , far from resembling the concentrated outliers we know to be most challenging for fasts and fastlts , the members of @xmath187 seem to form a scattered cloud of points , occupying a much larger volume on the design space than the members of the main group",
    "nevertheless , here too , the fit found by both fastlts and fasts lumps together observations stemming from very disparate groups . in this case too , the large separation between the members of @xmath176 and those of @xmath177 along the design space as well as wrt @xmath188 and the fact that many observations flagged as outliers are awarded weight in the fit found by both fasts and fastlts together suggest that these data - points exert a substantial influence on the model fitted by these algorithms .",
    "consequently , we do not expect the coefficients fitted by either to accurately describe , in the sense of model @xmath14 , any subset of the data .",
    "in this article we introduced rcs , a new outlyingness index and fastrcs , a fast and equivariant algorithm for computing it . like many other outlier detection algorithms ,",
    "the performance of fastrcs hinges crucially on correctly identifying an @xmath17-subset of uncontaminated observations .",
    "our main contribution is to characterize this @xmath17-subset using a new measure of homogeneity based on residuals obtained over many random regressions .",
    "this new characterization was designed to be insensitive to the configuration of the outliers .    through simulations , we considered configurations of outliers that are worst - case for affine and regression equivariant algorithms , and found that fastrcs behaves notably better than the other procedures we considered , often revealing outliers that would not have been identified by the other approaches . in most applications ,",
    "admittedly , contamination patterns will not always be as difficult as those we considered in our simulations and in many cases the different methods will , hopefully , concur .",
    "nevertheless , using two real data examples we were able to establish that it is possible for real world situations to be sufficiently challenging as to push current state of the art outlier detection procedures to their limits and beyond , justifying the development of better solutions . in any case , given that in practice we do not know the configuration of the outliers , as data analysts , we prefer to carry our inferences while planing for the worst contingencies .",
    "00 deepayan , s. ( 2008 ) .",
    "lattice : multivariate data visualization with r. springer , new york . kaggle data challenge ( 2012 ) .",
    "`` do nt get kicked ! '' .",
    "maronna r. a. , martin r. d. and yohai v. j. ( 2006 ) .",
    "robust statistics : theory and methods .",
    "wiley , new york .",
    "martin r. d. , yohai v. j. and zamar r. h. ( 1989 ) .",
    "min - max bias robust regression .",
    "the annals of statistics .",
    "17 , 16081630 .",
    "r core team ( 2012 ) .",
    "r : a language and environment for statistical computing .",
    "r foundation for statistical computing .",
    "vienna , austria .",
    "rocke d. m. and woodruff d. l. ( 1996 ) .",
    "identification of outliers in multivariate data .",
    "journal of the american statistical association , vol .",
    "91 , 10471061 .",
    "rousseeuw , p.j . and",
    "leroy , a.m. ( 1987 ) .",
    "robust regression and outlier detection .",
    "wiley , new york .",
    "rousseeuw p. j. and van driessen k. ( 2006 ) . computing lts regression for large data sets .",
    "data mining and knowledge discovery archive vol .",
    "12 , 2945 .",
    "rousseeuw p. , croux c. , todorov v. , ruckstuhl a. , salibian - barrera m. , verbeke t. , koller m. , maechler m. ( 2012 ) .",
    "robustbase : basic robust statistics .",
    "r package version 0.95 .",
    "salibian - barrera m. , yohai , v.j .",
    "a fast algorithm for s - regression estimates .",
    "journal of computational and graphical statistics , vol .",
    "15 , 414427 .",
    "yeh , i. ( 2007 ) .",
    "modeling slump flow of concrete using second - order regressions and artificial neural networks .",
    "cement and concrete composites , vol .",
    "29 , 474480 .",
    "yohai , v.j .",
    "high breakdown - point and high efficiency estimates for regression .",
    "the annals of statistics , 15 , 642656 ."
  ],
  "abstract_text": [
    "<S> the residual congruent subset ( rcs ) is a new method for finding outliers in the regression setting . like many other outlier detection procedures , rcs searches for a subset which minimizes a criterion . </S>",
    "<S> the difference is that the new criterion was designed to be insensitive to the outliers . </S>",
    "<S> rcs is supported by fastrcs , a fast regression and affine equivariant algorithm which we also detail . </S>",
    "<S> both an extensive simulation study and two real data applications show that fastrcs performs better than its competitors . </S>"
  ]
}