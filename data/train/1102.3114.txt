{
  "article_text": [
    "trends in cpu manufacture over the last several years suggest that the multi - core architecture of modern chips is only going to increase over the short - term future @xcite .",
    "current particle physics codes are effectively single - threaded , and sites within wlcg are currently recommended to provide one job slot in their batch system per core ( or even , for hyperthreaded processors , one per virtual core@xcite ) .",
    "this results in a large number of processes ( on the order of 8 to 12 , for commonly used quad , hex - core , dual - processor configurations ) , each attempting to perform i / o operations on the system storage device . for the workloads involved in monte - carlo production ,",
    "this additional load is almost insignificant to job efficiency , as such jobs are primarily cpu - bound . for analysis workloads , however , i / o is the limiting factor@xcite , and thus the i / o contention created by concurrent analysis jobs on a single worker node is of significance to job efficiency ( and reliability ) .",
    "much of this i / o load is caused by seeks within the root@xcite file structures used by particle physics to store event data @xcite .",
    "ssds are an emerging technology which seeks to compete with conventional `` spinning - disk '' magnetic hard - disk technologies ( `` hdds '' ) across almost all applications .",
    "ssds are usually based upon the ubiquitous flash memory technology .",
    "there are two types of flash memory , with separate applications based on their cost / capacity ratio and performance .",
    "slc flash is more expensive per gb , because it only stores a single value per cell , but is more performant for the same reason .",
    "mlc flash stores multiple values per cell , allowing it a higher storage density , and lower cost / capacity ratio . as a result ,",
    "mlc flash is more often found in commodity ssd devices aimed at hdd replacement in desktop computer and laptops , and slc flash in high - performance devices aimed at enterprise applications ( like metadata storage , or database acceleration ) .",
    "as flash storage is bus - addressable , there is little additional seek - time incurred in accessing data out - of - order from the storage array ( no physical parts need move , and the only overhead is in skipping the contents of the onboard dram readahead buffer ) , especially compared to hdds , which must wait for the physical media to rotate under the read - head to reach any out - of - order data the potential application of ssds to the analysis i / o issue is clear : if the problem for multiple jobs is seeks , why not use a technology where seeks are fast ?",
    "another approach to addressing the i / o load issues is to attempt to improve the quality of the root files themselves .",
    "if accesses within a root file can be made more in - order ( more `` linear '' ) , then each process i / o looks more like a streaming process , with the only seeks being when switching context between the i / o requests of different processes .",
    "in fact , all of the wlcg experiments have embarked on improvements to their file formats in this manner@xcite .",
    "we will consider the effect on ssd applications later in this paper , but the actual changes involved are outside of our scope .",
    "we evaluated the performance improvements in standard worker nodes in the university of glasgow compute cluster allocated for gridpp for various potential replacements for the conventional hdd .",
    "two types of ssd , both of them using mlc flash , were considered .",
    "no slc flash based ssd was evaluated for this application as the prices for suitable devices were outside the acceptable range for a tier-2 site s procurement funding ( and therefore , pragmatically , there is no point in testing them ) . a previous piece of research@xcite has already been performed by others , evaluating the performance of slc flash modules for similar analyses .",
    "they show the expected significant performance increases over hdds , although the authors note that the price and capacity of such units render them impractical .    of the two ssds used , the kingston ssd live ! v - series 128 gb is a low - end consumer - grade solution intended for laptop use .",
    "we use this as an indicator of the bottom of the budget performance possible from an ssd .",
    "the second ssd type used was the intel ssd x-25 m g2 160 gb .",
    "intel ssds are generally regarded@xcite as solid performers in both the consumer and enterprise ssd environments , and the g2 ( second generation ) models in particular are regarded as having very good price / performance for their class .",
    "these ssds are intended to represent the mid - high end of the consumer ssd market , and also the high end of the affordable price range for a tier-2 site .",
    "the low - level i / o capture tool , blktrace@xcite , was used to capture live i / o patterns on each of the test configurations for later analysis .",
    "blktrace makes use of the kernel debug filesystem to extract data about i / o activity at the kernel level in a straightforward way .",
    "as such , it is only usable on distributions supporting kernel releases post-2.6.10 , a set which excludes rhel4 ( and derivatives , like scientific linux 4 ) , but includes rhel5 ( and thus sl5 ) . as part of the intent of the work was to examine the behaviour of ` real systems ' , no attempt was made to backport more recent kernels to sl4 systems . as a result ,",
    "all of our monitoring is on then - current sl5 systems running the then - current releases of the glite middleware .",
    "blktrace output is parsed with the open - source seekwatcher tool , which produces graphical representations of the i / o pattern , and the seek rates over the sampled period .",
    "the hammercloud@xcite automated job submission and testing framework was used to manage job load and type for all tests performed for this paper .",
    "in particular , the efficiencies for the primary results presented are derived from hammercloud test runs 1332 , 1334 and 1348 .",
    "jobs were constrained to run only on a subset of the batch system , which was reserved for the user accounts that hammercloud test jobs are mapped to . as a result , these nodes only executed jobs provided by the test infrastructure .",
    "the results marked `` hdd '' are for a conventional worker node configuration , with a single 7200rpm hard disk partitioned for system and tmp filesystems .",
    "our configuration creates scratch space for incoming jobs in the tmp filesystem , so all significant i / o due to user workloads is located within this filesystem . for the ssd rows , the tmp filesystem , only , was mounted on the relevant ssd .",
    "monitoring data for blktrace was written to the system disk ( the conventional hard disk ) , to avoid interfering with the i / o for the jobs . for the raid rows , the nodes were prepared logically as for the `` hdd '' configuration , with one storage volume partitioned into system and tmp filesystems . however , the storage volume was either a raid 1 or raid 0 array created in software from two hard disks identical to the single disk found in the conventional configuration .",
    "table [ primaryresults ] summarises the result of the performance tests for hammercloud tests using the filestager data access method , and atlas reordered aods .",
    "llll cores : jobs&storage type&mean job efficiency&mean throughput +   + 8:8&kingston value ssd&60%&4.5 + 8:8&hdd&75%&5.5 + 8:8&intel x-25 ssd&80%&6 + 8:8&2@xmath1hdd raid 1&83%&6.6 + 8:8&2@xmath1hdd raid 0&90%&7 +   + 24:24&intel x-25 ssd&50%&12 + 24:24&2@xmath1hdd raid 0&86%&21 +   + 8:1&hdd&90%&0.9 +    the difference between reordered and unordered aod file access on these results can be seen from figure [ unorderedvsordered ] .",
    "as can be seen , the improved ordering process on the serialised trees in the ` reordered ' aods significantly reduces the peak seek rate when accessing the files on an ssd .",
    "preliminary results against unordered aods on ssds showed ssd performance in a better light than with the reordered aods now in use ; however , we do not include these results as they are irrelevant to the current state of play .",
    "blktrace - gathered i / o record for multi - job access to ` unordered ' and ` reordered ' aods , generated using the seekwatcher tool.,title=\"fig : \" ] a ) unordered aods    blktrace - gathered i / o record for multi - job access to ` unordered ' and ` reordered ' aods , generated using the seekwatcher tool.,title=\"fig : \" ] b ) reordered aods    another possibility for removing local i / o contention is to use remote file i / o interfaces to access files directly from the site local storage element , rather than staging them to the worker node storage . in atlas use , this is implemented by the dq2_local data access method .",
    "table [ rfioresults ] shows efficiencies for the same configuration as table [ primaryresults ] , but using the dq2_local method .",
    "as can be seen , none of the efficiencies reach the 90% efficiency seen for single jobs ( or 8 jobs on a 2@xmath1hdd raid 0 configuration ) .",
    "however , for the specific instance of the kingston value ssd , performance is measurably improved by removing the filestaging process from the workload . in all other cases , performance is lower than for the equivalent case with locally staged files . for the magny - cours node ,",
    "there is one caveat for these results ; a node with this degree of job parallelism would almost certainly be provided with a 10gige , or equivalent , interface , rather than the 1gige interface used for this test .",
    "thus , the actual efficiencies for a `` real '' high - parallelism node with remote i / o will have these results as a lower - bound .    llll cores : jobs&storage type&mean job efficiency&mean throughput +   + 8:8&kingston value ssd&67%&5.35 + 8:8&intel x-25 ssd&73%&5.86 + 8:8&2@xmath1hdd raid 1&73%&5.88 + 8:8&hdd&78%&6.25 +   + 24:24&2@xmath1hdd raid 0&73%&17.4 + 24:24&3@xmath1hdd raid 0&76%&18.2 +    after the conclusion of the first phase of testing , we concluded that the advantage of raid-0 disk configurations was proved enough to invest in retrofitting the cluster with such a configuration . for long - term testing reasons , we retained the intel x-25 ssds in those nodes fitted with them , as the performance deficit was considered significant only when the node was fully loaded with analysis work ( and no cores performing other workloads ) .",
    "it should be noted that , during the same time period , the atlas file - caching utility , pcache@xcite , was also installed on the cluster , which would have reduced the number of writes needed to the filesystems to some degree@xcite .",
    "table [ longtermtest ] shows the mean efficiencies for the raid 0 and ssd nodes when full of real atlas analysis work .",
    "as can be seen , the ` real world ' values are within a few percent of the simulated values provided by the hammercloud test .",
    "these data points were taken for the first week in september 2010 , filtering out periods when the nodes sampled were not fully occupied .",
    "ll storage type&mean job efficiency + intel x-25 ssd&78% + 2@xmath1hdd raid 0&88% +    it may be useful to put these results in context , by casting them in terms of the price / performance ratio for two metrics of performance for storage - the throughput achieved in the total worker - node system ( from table [ primaryresults ] ) , and the capacity of the storage volume ( which is particularly important as the amount of data processed by wlcg vo analysis is increasing over time ) .",
    "table [ priceperform ] shows these figures for the three test cases we examined .",
    "as can be seen , the price / performance ratios for these ` real world ' metrics are much worse for the ssds than they are for the simple raid0 array , by around a factor of 3 to 10 . of course , as a fraction of the cost of a complete worker node , these costs are relatively smaller ( perhaps 10% of the total system cost ) , but the ratios of the metrics remain the same .",
    "llll storage type&unit price(gbp)&price / throughput(8core node , gbp)&price / gb(gbp ) + kingston ssdnow&155&34.40&1.21 + intel x-25 ssd&245&40.80&1.53 + 2xhdd(500gb)&80&11.40&0.16 +",
    "although the popular view of ssds is that of an expensive , but always more performant , replacement for the hard disk drive , our results show that the facts do not always bear this out . in particular ,",
    "the performance of affordable ssds is dramatically weaker whenever widespread writes as well as reads are required , especially when the number of seeks required is relatively small .",
    "it is notable , however , that increasing the number of jobs acting on a node ( and thus the number of seeks switching between i / o threads ) does not increase the relative performance of the ssd in our testing .",
    "regardless of the fact that such ssds are pragmatically out - performed by raid0 arrays for the workloads for which tier-2 sites are concerned , it is further important to consider their performance with respect to the price spent on them .",
    "the significant costs of an ssd of a reasonable capacity compared to commodity 7200 rpm disks worsen their price / performance metrics significantly in all cases ; even against a lone 7200rpm disk , their metrics are poor .",
    "in conclusion , we find that for the average tier-2 site , where price must also be a factor in purchasing decisions , ssds are not a viable technology for worker node internal storage .",
    "it is possible that more advanced approaches to worker node storage use ( provisioning of a cache for file transfers , from which symlinks could be made for instances of repeatedly requested files on the same node ) would provide a more natural niche for ssd use , by increasing the ratio of reads to writes on the storage .",
    "unfortunately , the most mature such technology in use by a vo , pcache , is incapable of using separate filesystems for cache and job scratch space , as it uses hardlinks as an efficient means of reference counting for cache management .",
    "additionally , this space is perhaps more naturally occupied by the more performant distributed or network filesystems - lustre , gpfs , pnfs , and the like , as this allows multiple workers to access the same cache , increasing the chance of hits on any particular request .",
    "the use of ssds as metadata storage targets for lustre is already known to be useful , but it is outside of the scope of this investigation .    the authors would like to thank dan van der ster ( and johannes elmsheuser ) for providing the hammercloud framework necessary for this research , and allowing the large number of rapid tests which we submitted with it .",
    "10 geer , d. chip makers turn to multicore processors _ ieee computer _ * 38 * , issue 5 11 - 13 washbrook a , algorithm acceleration from gpgpus for the atlas upgrade , _ to be published in j. phys . :",
    "* this proceedings * vanderster d c et al functional and large - scale testing of the atlas distributed analysis facilities with ganga _ j. phys . :",
    "* 072021 brun r and rademakers f _ nucl . instrum . meth . _ a * 389 * 81 bhimji w , optimising grid storage resources for lhc data analysis , _ to be published in j. phys .",
    "* this proceedings * panitkin s y et al study of solid state drives performance in proof distributed analysis system _ j. phys . :",
    "* 219 * 052011 vukotic , i , bhimji , w , biscarat , c , brandt , g , duckeck , g , van gemmeren , p , peters , a , schaffer , r d optimization and performance measurements of root - based data formats in the atlas experiment .",
    "atl - com - soft-2010 - 081 http://cdsweb.cern.ch/record/1313244 .",
    "_ to be published in j. phys .",
    "* this proceedings * canal , p , root i / o : the fast and furious _ to be published in j. phys .",
    ": conf . series _",
    "* this proceeedings * polte , m. ; simsa , j. ; gibson , g. comparing performance of solid state devices and mechanical disks _",
    "petascale data storage workshop , pdsw 08 _ * 3rd * 1 - 7 j. axboe ,  patch : block device io tracing , \" _ http://lwn.net/articles/155340/ _",
    "waldman , c et . al _ atlas internal pcache documentation _",
    "skipsey , s et .",
    "al _ gridpp wiki page concerning pcache testing _"
  ],
  "abstract_text": [
    "<S> solid state disk technologies are increasingly replacing high - speed hard disks as the storage technology in high - random - i / o environments . </S>",
    "<S> there are several potentially i / o bound services within the typical lhc tier-2 - in the back - end , with the trend towards many - core architectures continuing , worker nodes running many single - threaded jobs and storage nodes delivering many simultaneous files can both exhibit i / o limited efficiency . </S>",
    "<S> we estimate the effectiveness of affordable ssds in the context of worker nodes , on a large tier-2 production setup using both low level tools and real lhc i / o intensive data analysis jobs comparing and contrasting with high performance spinning disk based solutions . </S>",
    "<S> we consider the applicability of each solution in the context of its price / performance metrics , with an eye on the pragmatic issues facing tier-2 provision and upgrades    _ international conference on computing in high energy and nuclear physics ( chep ) 2010 _ + _ taipei , taiwan _    </S>",
    "<S> glas - ppe/2010-??14@xmath0 february 2011 </S>"
  ]
}