{
  "article_text": [
    "in this introductory section , i will review the basics of pca .",
    "a good reference on this topic is in @xcite essentially , pca involves two key aspects : transformation of data to a zero - correlation space , and truncation of the data .",
    "depending on the application , one may choose not to truncate the data after the transformation .",
    "to proceed , let us denote the data to be transformed by * column * vectors @xmath2 .",
    "the covariance matrix of the @xmath2 s can be written as :    @xmath3    in which @xmath4 is the mean of the @xmath2 s .",
    "next , we consider the transformation @xmath5    where @xmath6 is a @xmath7 by @xmath7 matrix whose @xmath8-th row , @xmath9 , is the @xmath8-th eigenvector of the @xmath7 by @xmath7 symmetric matrix @xmath10 .",
    "we shall see that the @xmath11 s have zero correlations ( but possibly non - zero variances ) .",
    "the covariance matrix of the @xmath11 s may be expressed as :    @xmath12    but the mean , @xmath13 , of the @xmath11 s is zero because :    @xmath14    @xmath15    @xmath16    @xmath17    to show that the @xmath11 s have zero correlations , we need to show that @xmath18 is diagonal .",
    "putting equation 3 into equation 5 , we find :    @xmath19    putting @xmath11 from equation 2 into the preceding equation yields :    @xmath20    @xmath21    above , we recognize @xmath22 as @xmath10 and write :    @xmath23    the preceding equation , when viewed in the context of the spectral theorem of linear algebra , informs us that @xmath18 must be diagonal .",
    "this is because @xmath10 is real symmetric , and therefore orthogonally diagonalizable ; and because the rows of @xmath6 are the eigenvectors of @xmath10 .",
    "moreover , according to another theorem of linear algebra , the principal - diagonal element @xmath24 , which is on the @xmath8-th row and @xmath8-th column of @xmath18 is the eigenvalue corresponding to the eigenvector lying in the @xmath8-th row of @xmath6 .",
    "+   + we have seen how pca projects data to zero - correlation space .",
    "we will now describe how to harness it for dimensionality reduction .",
    "suppose we wish to transform a @xmath7-dimensional vector , @xmath2 from @xmath25 to @xmath26 , under the stipulation that @xmath27 , so that the transformation yields a @xmath28-dimensional vector @xmath29 .",
    "mathematically , the required transformation is summarized as follows :    @xmath30    where @xmath31 is the @xmath28 by @xmath7 matrix obtained by discarding the lowest @xmath7 minus @xmath28 rows of @xmath32 .",
    "observe that this is tantamount to discarding the @xmath33 eigenvectors of matrix @xmath10 that are associated with the @xmath33 lowest eigenvalues of @xmath32 .",
    "it is clear that the above transformation truncates the vector @xmath34 in the sense that it causes it to lose @xmath33 of its dimensions .",
    "this truncation results in a root mean square error , @xmath35 , given by :    @xmath36    where @xmath37 is the eigenvalue associated with the @xmath38-th eigenvector , from top down , in the original @xmath7 by @xmath7 matrix @xmath6 . from the above equation ,",
    "it is obvious that @xmath35 is just the sum of the eigenvalues associated with the discarded eigenvectors .",
    "thus , we see that error @xmath35 is proportional to the number of discarded eigenvectors .",
    "the pca transformation can either be  truncated \" or  non - truncated , \" the former case occurring when one or more of the ( eigenvector ) rows of the transformation matrix are discarded , and the latter case occurring when no row is discarded . in this section , using the background presented in the previous section , i shall state and prove two theorems which juxtapose the structures of the data when pca is performed with and without truncation .",
    "the first theorem states that pca is injective without truncation , but loses this injective property upon truncation .",
    "the second theorem says that , without truncation , pca preserves pairwise distances , but upon truncation , pca causes pairwise distances to either shrink or remain the same . in proving the theorems ,",
    "i use facts from standard linear algebra texts , such as @xcite and @xcite .",
    "a statement of the first of the two theorems follows thus :    suppose @xmath10 is any real symmetric @xmath7 by @xmath7 matrix ; suppose @xmath6 is the @xmath7 by @xmath7 matrix formed by stacking the eigenvectors of @xmath10 one atop the other in increasing order of eigenvalues , from bottom up ; and let @xmath39 be an @xmath28 by @xmath7 matrix , with @xmath27 , obtained from @xmath6 by discarding the @xmath33 lowermost rows of the latter",
    ". then , for a given vector @xmath4 @xmath40 @xmath41 , and any vector @xmath42 @xmath40 @xmath41 , the transformation @xmath43 is one - to - one , but the transformation @xmath44 is not .",
    "[ * proof * ] denote by @xmath2 and @xmath45 any two distinct vectors in @xmath41 , so that one may write @xmath2 @xmath46 @xmath45 , and one may set @xmath47 .",
    "also , one may set @xmath48 . to show that @xmath49 is one - to - one",
    ", we must show that @xmath50 .",
    "we therefore consider the linear system , @xmath51 .",
    "when @xmath52 , this linear system boils down to the homogeneous equation , @xmath53 .",
    "now , according to the fundamental theorem of linear algebra , if @xmath6 is invertible , then the only solution to this homogeneous equation is the zero vector @xmath54 .",
    "that is , up to @xmath6 being invertible , we may write : @xmath55 , and the converse of this is : @xmath56 .",
    "hence , what remains is to show that @xmath6 is invertible .",
    "firstly , by the fundamental theorem of linear algebra , @xmath57 exists .",
    "this is because the columns of @xmath58 are the orthogonal , and therefore linearly independent , eigenvectors of the real symmetric matrix @xmath10 .",
    "but , @xmath59 , from which we see that @xmath6 is invertible .    the second part of the theorem asks us to prove that the transformation @xmath44 is not one - to - one . towards this , we still set @xmath47 , as above , but now , we also set @xmath60 . we need to show that @xmath61 such that @xmath62 , but @xmath63 . to this end",
    ", we consider the system @xmath64 .",
    "when @xmath65 , this system reduces to the homogeneous equation , @xmath66 , which in augmented form is @xmath67 $ ] .",
    "now , matrix @xmath39 must be an @xmath28 by @xmath7 matrix with the property @xmath27 , because @xmath68 results from discarding at least one row of an @xmath7 by @xmath7 matrix , @xmath6 .",
    "but , a theorem of linear algebra states that for any @xmath28 by @xmath7 matrix @xmath69 , if @xmath27 then the homogeneous system @xmath70 $ ] has infinitely many solutions . moreover , only one of these solutions is the zero vector , so that there exists infinitely many non - zero solutions .",
    "thus , there is at least one non - zero solution in our homogeneous system , @xmath71 .",
    "we conclude consequently that , in the system @xmath64 , @xmath61 such that @xmath62 , but @xmath63 .",
    "i make some physical comments about the just proven theorem 1 .",
    "consider a classification problem with the feature vectors denoted @xmath2 .",
    "in particular , let us consider two * distinct * feature vectors @xmath2 and @xmath45 , belonging to two different classes ( or states of nature ) , @xmath72 and @xmath73 . according to theorem 1 , when the vectors @xmath2 and @xmath45 are passed through the truncated transformation of equation 12",
    ", both vectors may end up as the same vector , @xmath74 , on the output side of the transform .",
    "definitely , this has the negative potential of causing the vectors to lose their discriminatory power as features for distinguishing between classes @xmath72 and @xmath73 .    to proceed , i state the second theorem of this manuscript .",
    "the theorem is in two parts , with the first part having already been referred to in the work of @xcite .",
    "[ thm8]theorem    let @xmath10 be any real symmetric @xmath7 by @xmath7 matrix ; let @xmath6 be the matrix whose rows are the eigenvectors of @xmath10 ; and let @xmath39 be a matrix obtained from @xmath6 by discarding at least one row of the latter .",
    "then , for a given vector @xmath4 @xmath40 @xmath41 , and any two vectors @xmath2 @xmath40 @xmath41 , and @xmath45 @xmath40 @xmath41 , the pair of transformations @xmath75 and @xmath76 satisfies @xmath77 , which preserves pairwise distances in @xmath41 ; whereas the pair of transformations @xmath78 and @xmath79 satisfies the inequality @xmath80 , which tends to shrink pairwise distances in @xmath41 .",
    "[ * proof * ] in what follows , we use @xmath81 to denote euclidean distance .",
    "so , for the first part of the theorem , we have @xmath82 ; and @xmath83 .",
    "but , @xmath84 . hence , @xmath85 , where the penultimate equality is due to the orthogonality of @xmath58 .",
    "as a result , we easily find , @xmath77 .    now , for the second part of the theorem , given any vector @xmath2 , we shall denote by @xmath86 the @xmath87-th element of @xmath2 .",
    "further , we denote the @xmath87-th row of @xmath6 by @xmath88 and the @xmath87-th row of @xmath39 by @xmath89 .",
    "we begin by considering the relationship @xmath90 , which derives easily from the hypothesis of the theorem . after using @xmath91 to denote the @xmath87-th element of the column vector @xmath92",
    ", one may write , @xmath93 , from which one sees : @xmath94 ^ 2 = \\sum_{k = 1 } ^{m } [ \\;(\\hat{q}^t)_k \\ : ( \\textbf{x}_j - \\textbf{x}_i)\\;]^2 $ ] . now , on the other side , the relationship , @xmath84 , allows us to write : @xmath93 .",
    "hence , @xmath95 ^ 2 = \\sum_{k = 1 } ^{n } [ \\;(q^t)_k \\ : ( \\textbf{x}_j - \\textbf{x}_i)\\;]^2 = \\sum_{k = 1 } ^{m } [ \\;(q^t)_k \\ : ( \\textbf{x}_j - \\textbf{x}_i)\\;]^2   \\ ; + \\ ; \\sum_{k = m + 1 } ^{n } [ \\;(\\hat{q}^t)_k \\ : ( \\textbf{x}_j - \\textbf{x}_i)\\;]^2 $ ] .",
    "now since @xmath96 ^ 2 \\geq 0 $ ] , it follows directly that @xmath97 .",
    "consequently , we have : @xmath98 .",
    "now , in the first part of this theorem , we already established that @xmath77 .",
    "consequently , we see : @xmath99 .",
    "[ h ]    i use the just proven theorem 2 to suggest how the error embodied in equation 13 arises .",
    "that error is due to discarding the lowest @xmath33 rows of @xmath6 .",
    "more formally , notice that that error is a sum from @xmath100 to @xmath7 .",
    "now , in the proof of theorem 2 , theshrinking \" actually occurs due to the term , @xmath96 ^ 2 $ ] , which is again a sum from @xmath100 to @xmath7 .",
    "this leads me to opine that the truncation of @xmath6 by @xmath33 rows leads to the shrinking effect , which in turn leads to the error underpinned by equation 13 .",
    "more so , there is another quite interesting connection between theorems 1 and 2 , in the sense that theorem 2 can be viewed as a  mild \" form of theorem 1 .",
    "theorem 1 says that truncated pca is * not * one - to - one , so that it is possible for two points that were distinct before the transformation toshrink \" to the same point after the transformation .",
    "on the other hand , theorem 2 says that truncated pca tends to cause points to shrink , although they may or may not shrink to the same point .",
    "1 attempts to illustrate this connection .",
    "in this section , we explore a connection between reconstruction errors engendered by the truncated pca map and the associated shrinkages in pair - wise distances .",
    "we will show that , given any two points , @xmath2 and @xmath45 in @xmath41 , that are mapped by the truncated pca transformation ( or any other similar transformation for that matter ) to the points @xmath29 and @xmath101 respectively ; and if we define @xmath102 , as a form of total @xmath1 norm reconstruction error associated with @xmath2 and @xmath45 under the map ; and also define @xmath103 as the shrinkage in pair - wise distances associated with @xmath2 and @xmath45 under the map , then we must have : @xmath104 . in words",
    ", this says that the pair - wise distance shrinkage associated with the pair of points , @xmath2 and @xmath45 , is bounded from above by total @xmath1 norm reconstruction error associated with the pair of points .",
    "indeed one can give a simple formal proof for the above statement , after stating it as a theorem :    [ thm8]theorem    let @xmath2 and @xmath45 be any two points in @xmath41 which are mapped by the truncated pca transformation to the points @xmath29 and @xmath101 respectively .",
    "further , let @xmath105 be the shrinkage in pairwise distances associated with the two points under the map ; and let @xmath106 , be a total @xmath1 norm reconstrunction error associated with the two points under the map .",
    "then , we must have : @xmath107 , which asserts that the shrinkage in pairwise distance is bounded from above by the total reconstruction error .",
    "[ * proof * ] we begin with @xmath105 , and use the fact that , for any two complex numbers ( or vectors ) , @xmath108 and @xmath109 , we must have @xmath110 ( which is a variant of the triangle inequality ) to write @xmath111 .",
    "but , now by the fact @xmath112 , we have : @xmath113 .",
    "hence , @xmath107 as required .",
    "to re - iterate , the just proven theorem shows a connection between an @xmath1 norm reconstruction error and pair - wise distance shrinkages .",
    "but , intuitively , one feels that this @xmath1 norm reconstruction error should be correlated with the root mean square reconstruction error .",
    "now , according to equation 13 , the root mean square error is simply the sum of those eigenvalues associated with the discarded eigenvector rows in the truncated pca map .",
    "hence , i was lead to perform numerical experiments aimed at calculating the correlation between the sum of those eigenvalues and shrinkages in pair - wise distances .",
    "in addition , i have also performed some experiments to check respectively the effect of the sum of those eigenvalues and the effect of the shrinkages on classification accuracies under the pca map .",
    "so far , i have obtained the following results on some publicly available data from the uci machine learning repository : 1 ) .",
    "there seems to be a strong correlation between the sum of those eigenvalues associated with discarded eigenvectors and shrinkages in pair - wise distances .",
    "neither the sum of those eigenvalues nor pair - wise distances have any strong correlations with classification accuracies .",
    "this on - going@xmath0 work presented some theoretical and empirical implications of data transformations under the pca . in particular , following three results about pca maps were stated and proven : 1 ) .",
    "pca without discarding eigenvector rows is injective , but looses this injectivity when eigenvector rows are discarded 2 ) .",
    "pca without discarding eigenvector rows preserves pair - wise distances , but tends to cause pair - wise distances to shrink when eigenvector rows are discarded .",
    "3 ) . for any pair of points , the shrinkage in pair - wise distance",
    "is bounded above by an @xmath1 norm reconstruction error associated with the points .",
    "further , since the third result suggests that there might exist some correlation between shrinkages in pair - wise distances and mean square reconstruction error which is defined as the sum of those eigenvalues associated with the discarded eigenvectors , i was naturally led to perform numerical experiments to obtain the correlation between the sum of those eigenvalues and shrinkages in pair - wise distances .",
    "in addition , i have also performed some experiments to check respectively the effect of the sum of those eigenvalues and the effect of the shrinkages on classification accuracies under the pca map .",
    "so far , the following results have been obtained on some publicly available data from the uci machine learning repository : 1 ) .",
    "there seems to be a strong correlation between the sum of those eigenvalues associated with discarded eigenvectors and shrinkages in pair - wise distances .",
    "neither the sum of those eigenvalues nor pair - wise distances have any strong correlations with classification accuracies ."
  ],
  "abstract_text": [
    "<S> in this on - going@xmath0 work , i explore certain theoretical and empirical implications of data transformations under the pca . </S>",
    "<S> in particular , i state and prove three theorems about pca , which i paraphrase as follows : 1 ) . </S>",
    "<S> pca without discarding eigenvector rows is injective , but looses this injectivity when eigenvector rows are discarded 2 ) . </S>",
    "<S> pca without discarding eigenvector rows preserves pair - wise distances , but tends to cause pair - wise distances to shrink when eigenvector rows are discarded . </S>",
    "<S> 3 ) . for any pair of points , the shrinkage in pair - wise distance </S>",
    "<S> is bounded above by an @xmath1 norm reconstruction error associated with the points . </S>",
    "<S> clearly , `` 3 ) . '' </S>",
    "<S> suggests that there might exist some correlation between shrinkages in pair - wise distances and mean square reconstruction error which is defined as the sum of those eigenvalues associated with the discarded eigenvectors . </S>",
    "<S> i therefore decided to perform numerical experiments to obtain the correlation between the sum of those eigenvalues and shrinkages in pair - wise distances . </S>",
    "<S> in addition , i have also performed some experiments to check respectively the effect of the sum of those eigenvalues and the effect of the shrinkages on classification accuracies under the pca map . </S>",
    "<S> so far , i have obtained the following results on some publicly available data from the uci machine learning repository : 1 ) . </S>",
    "<S> there seems to be a strong correlation between the sum of those eigenvalues associated with discarded eigenvectors and shrinkages in pair - wise distances . </S>",
    "<S> 2 ) . </S>",
    "<S> neither the sum of those eigenvalues nor pair - wise distances have any strong correlations with classification accuracies . </S>"
  ]
}