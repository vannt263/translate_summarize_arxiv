{
  "article_text": [
    "hashing based techniques @xcite are widely used to encode a high dimensional data in a larger scale in a compact binary codes ( i , e . 0 or 1 ) . in this way , large scale multimedia data could be stored compactly and could be retrieved efficiently by calculating the hamming distance between binary codes in a shorter response time . because of its efficacy in dealing with the problems of ` curse of dimensionality ' , it has been used in many computer vision applications , including feature points matching @xcite , multimedia event detection @xcite , and video segmentation @xcite . with the rapidly increasing amount of multimedia web data , standard hashing methods face tremendous challenges in leveraging reliable ground truth for the available data due to the time - consuming process of manual annotations .",
    "humans and machines still acquire visual concepts in different ways .",
    "machines , similar to child , try to mimic the process of learning by visual examples to take decisions based on previously learned patterns . as our understanding grows , a lot of our diverse knowledge comes from information from other modalities ( particularly text and oral ) .",
    "thus , by exploiting the semantic correlations in different modalities , it is possible for human intelligence to identify an object of completely new category by transferring knowledge acquired from known categories .",
    "zero shot learning methods bridge the semantic gap between ` seen ' and ` unseen ' categories by transferring supervised knowledge ( using class - attribute descriptors ) between ` seen ' and ` unseen ' categories . by leveraging information from other modalities for example by learning word embeddings that capture distributional similarity in the textual domain @xcite from large scale text corpus such as wikipedia , we can ground visual modality and transfer such knowledge into an optimization model . in the hashing domain ,",
    "however , the zero - shot problem has rarely been studied . as per best of our knowledge , only two works @xcite and",
    "@xcite have addressed this problem previously . in section 2 , we briefly review some related work on hashing and zero - shot learning . in section 3 ,",
    "we elaborate our approach with details , together with our optimization method and an analysis of the algorithm . with extensive experiments , various results on various different datasets",
    "are reported in section 4 , followed by the conclusion of the work in section 5 .",
    "similarity search is one of the key challenges in machine learning applications .",
    "most of the existing hashing methods generate binary codes by exploiting pairwise class label information to efficiently find similar data examples to a query .",
    "broadly , hashing schemes can be categorized as either data - independent hashing or data - dependent hashing .",
    "no supervised knowledge or prior information is assumed for data - independent hashing techniques .",
    "locality sensitive hashing @xcite is one of the most popular examples in data - dependent category . in general , data - independent hashing schemes generate hash functions randomly and require large number of hash look up tables to achieve reasonable performance . to tackle this issue , in the last decade",
    ", researchers have focussed strongly to improve data - dependent hashing methods .",
    "further data - dependent hashing schemes can be categorized into supervised or unsupervised learning methods . supervised hashing methods ( example supervised discrete hashing @xcite , column sampling discrete hashing @xcite )",
    "generally achieve better performance than unsupervised learning based hashing techniques ( example iterative quantization @xcite and inductive hashing on manifolds @xcite ) .",
    "unsupervised hashing techniques , in general , use manifold learning techniques or graph cut techniques to extract the intrinsic structure of the data embedded in the feature space .",
    "algorithms have also been proposed to encode the mid - level information like attributes present in the image in its hash code @xcite . given that almost all the data available on internet is recounted in the multiple modalities , like semantic information ( for example , image captions ) and visual features .",
    "because of availability and incorporating supervised information into learning model , supervised hashing schemes , in general , perform better than unsupervised ones .",
    "however , what if we want to achieve data - dependent performance while no training example is provided ?",
    "all the mentioned hashing methods are limited to only seen categories i.e. at least one example correspond to each category is present in training set but would fail to generalize to any  unseen \" category .",
    "zero shot learning ( @xcite,@xcite,@xcite,@xcite ) have proved its efficacy in learning concepts with no available training examples , thereby tackling the problem posed by increasing large amount of online data with no ground truth annotations . because training and testing categories are disjoint",
    ", one can not directly apply the supervised learning algorithms with per - image class labels .",
    "zero shot learning algorithms assume that there is a space within which both visual and semantic features could be embedded simultaneously .",
    "most commonly adopted semantic embedding space is an attribute space where the class labels or categories are represented by a vector which quantifies the amount of each attribute generally found in the instances or examples of that class @xcite .",
    "but an attribute ontology has to be manually defined to describe a class using an attribute vector . to address this issue",
    ", methods have been developed to leverage large textual corpus such as wikipedia to obtain semantic features for class labels by exploiting the correlations between different words .",
    "@xcite and @xcite are examples of zero shot learning works which obtain semantic features from nlp techniques .",
    "our main idea is motivated from structured embedding frameworks ( @xcite , @xcite ) .",
    "we first represent both the images features and seen classes features in a common multi - dimensional hamming subspace .",
    "image features can be obtained from state - of - the - art image representations , for example from gist features @xcite .",
    "class features , as discussed above can either ( i ) be extracted automatically ( @xcite , @xcite ) from an unlabelled large text corpora ( ii ) or can be obtained using manually specified side information example attributes @xcite . in the learning procedure , we also incorporate an objective function that learns to pull images from the same class and having similar visual representation in visual space close to each other . but learning a hash function from a naive knowledge transfer using source domain without making it adapt to the target domain may lead to severe domain shift problems . to deal with this issue",
    ", we also incorporate the domain adaptation technique in hash code learning procedure . once learned",
    ", the hash function can be used to generate the hash codes for any seen as well as unseen category images and thus enabling zero - shot hashing which can at the same time , address the domain shift problem .",
    "in section 3.1 , we introduce the notations used throughout this paper . in section 3.2 , we introduce the objective function to embed the attribute / textual information of the classes and representation of images into a common hamming space . in section 3.3 ,",
    "we introduce a measure to preserve the intra - modal similarity information in the hash codes . in sections 3.4 and 3.5",
    ", we introduce an efficient optimization scheme for the proposed objective function to learn the hash function . in section 3.6",
    ", we introduced domain adaptaion in the hashing framework .",
    "let the number of training images in ` seen ' classes be @xmath0 , each of which is represented by @xmath1 dimension features .",
    "feature vector of @xmath2th image is represented by @xmath3 .",
    "an image data matrix @xmath4 is formed by arranging these image features column wise , that is , @xmath2th column of @xmath5 contains @xmath3 .",
    "let the number of ` seen ' and ` unseen ' classes be @xmath6 and @xmath7 respectively .",
    "moreover , the  unseen \" and  seen \" classes are disjoint ( as per the hypothesis of zero shot learning approaches @xcite @xcite ) .",
    "each of the @xmath8 classes is represented by the respective attributes / textual information vector . for each image ,",
    "the attribute / textual information is stored in vector of length @xmath9 . like image data matrix @xmath5 ,",
    "textual data matrix @xmath10 is generated by arranging attribute / textual information of all images column wise .",
    "each value in @xmath11 represents how strongly a particular attribute is evident in a given image .",
    "let the length of the hash codes be @xmath12 .",
    "our objective is to learn binary codes @xmath13 for all the @xmath0 images of the seen categories as well as to learn a hash function to generate hash codes for new images and images belonging to ` unseen ' categories . without any loss of generality",
    ", we consider to learn a linear form of hash function @xmath14 to generate hash codes using image features @xmath3 , where @xmath15 . to embed the information of textual feature data into a common hamming space , a transformation matrix @xmath16 is learned through the optimization scheme proposed in section 3.4 .",
    "@xmath17    eq . [ firstobjectivefunction ] represents the objective function in order to minimize the squared inter - modal loss .",
    "@xmath18 is a transformation matrix which embeds the semantic features into hamming space .",
    "though , we do not restrict @xmath19 but to take values as much as possible to @xmath20 .",
    "therefore , we do not impose the orthogonality constraint on @xmath21 .",
    "we exploit the local structural information in visual feature space to preserve the intra - modal similarity between data points i.e. , the hash codes of similar images should have less hamming distance . to preserve this intra - modal similarity , we ensure that the feature points present nearby in the higher dimension @xmath3 must be present close to each other in lower dimension ( i.e. in hamming space ) as well , which preserves _ feature - to - feature structure similarity_. moreover , the hash codes of the images belonging to the same category should be similar in order to preserve _ class - to - class information similarity_.      for similar ( dissimilar ) pairs , distance between them is expected to be minimized ( maximized ) in the hamming space .",
    "higher dimensional features having similar representations should also lie close to each other in the lower dimensional space .",
    "this reflects the manifold assumption that visually similar images are more likely to embed closer in the hamming space .",
    "specifically , if @xmath22 and @xmath23 lie near to each other in the higher dimensional space , they should share similar hash codes . to incorporate the neighborhood similarity in hash codes",
    ", we adopt the @xmath24 nearest neighbourhood function as given in @xcite in order to calculate the affinity matrix @xmath25 as shown in eq .",
    "[ featuretofeatureimilarity ] : @xmath26 where @xmath27 and @xmath28 is the @xmath24 nearest neighbour set for a given image .      to preserve the class discriminability of binary codes in hamming space",
    ", we decompose the discriminability constraint into two components , i.e. , intra - category compactness and inter - category separability .",
    "both these constraints enforce the lesser hamming distance between the hash codes of images belonging to the similar categories . to incorporate the intra class similarity information ,",
    "we define @xmath29 as shown in eq .",
    "[ intraclasssimilarity ] @xmath30 where @xmath27 .",
    "+ to incorporate the inter class similarity information , we define @xmath31 as shown in eq .",
    "[ interclasssimilarity ] @xmath32 where @xmath27 .",
    "+ by minimizing the energy function given in eq .",
    "[ similaritynew ] , we preserve both the feature - to - feature structure information and within class similarity information .",
    "@xmath33 where @xmath34 , and @xmath35 is the hash code for the @xmath2th image .",
    "eq . [ similaritynew ] could be further simplified into the eq .",
    "[ similaritynew2 ] , @xmath36 where @xmath37 is the laplacian matrix computed as @xmath38 . here",
    "@xmath39 is a @xmath40 diagonal degree matrix whose entries are given by @xmath41 .    by minimizing the energy function given in eq .",
    "[ similaritynew3 ] , we preserve between class similarity information . @xmath42 the eq .",
    "[ similaritynew3 ] could be further simplified into the eq.[similaritynew4 ] , @xmath43 where @xmath44 is the laplacian matrix computed as @xmath45 . here",
    "@xmath46 is a @xmath40 diagonal degree matrix whose entries are given by @xmath47 .",
    "the laplacian matrix @xmath48 encoding the information of intra modal similarity is given as in eq .",
    "[ mainlaplace ] .",
    "@xmath49 where @xmath50 is a trade off parameter for balancing the scale of @xmath37 and @xmath44 .",
    "thus our aim to preserve the intra - modal similarity could be achieved by minimizing the objective function in [ similaritymain ] : @xmath51      combining eq .",
    "[ similaritymain ] and eq .",
    "[ firstobjectivefunction ] , the final objective function can be formed as shown in eq .",
    "[ finalobjectivefunction ] @xmath52 eq .",
    "[ finalobjectivefunction ] could be further simplified by using the hash code matrix @xmath53 instead of @xmath54 . rewriting eq .",
    "[ finalobjectivefunction ] with this substitution we get : @xmath55 differentiating the eq .",
    "[ finalobjectivefunction2 ] with respect to @xmath21 and equating it to zero we obtain the following eq .",
    ": @xmath56 substituting the value of @xmath21 into the eq .",
    "[ finalobjectivefunction2 ] , we have : @xmath57 where , @xmath58 . thus , the eq .",
    "[ finalobjectivefunction2 ] could be re - written as shown in below eq .",
    ": @xmath59 replacing @xmath60 with the matrix @xmath61 , in eq .",
    "[ spectralhashing ] , we have : @xmath62 where @xmath63 is the identity matrix of size @xmath64",
    ". derivations of the eq .",
    "[ spectralhashing ] and eq .",
    "[ finalobjectivefunction2 ] are given in the appendix .",
    "+ eq . [ spectralhashing2 ] could be solved efficiently by removing the constraint @xmath65 .",
    "the solution of the objective function is simply the @xmath12 thresholded eigenvectors of @xmath61 corresponding to the @xmath12 smallest non - zero eigenvalues @xcite .",
    "note that we still have not obtained the hash function @xmath66 , i.e. our objective of obtaining @xmath67 has still not been achieved .",
    "previous works have tried to minimize the quantization loss @xmath68 to obtain @xmath67 ( @xcite , @xcite ) . however , in the proposed method hash codes are obtained by taking the sign of @xmath69 .",
    "hence , as suggested in already published work @xcite , relaxing the discrete constraint and minimizing the quantization loss will not be suitable for the proposed method . instead ,",
    "inspired by @xcite , we leverage support vector machines and use hinge loss , which is endowed with the max - margin property in hyperplane learning to obtain @xmath67 .",
    "each column of @xmath67 could be viewed as a split in the visual space .",
    "therefore , we minimize the energy formulated in eq .",
    "[ supportvectors ] @xmath70 where @xmath71 is the @xmath24th column of the projection matrix @xmath67 in eq .",
    "[ firstobjectivefunction ] .",
    "the hyper parameter @xmath72 balances the hyperplane margin and the empirical training error .",
    "the proposed algorithm is summarized in the * algorithm 1*. we used the liblinear library @xcite for training the max - margin classifier .",
    "[ algorithm1 ] * input : * training images @xmath73 \\in \\mathbb{r}^{(d_{x } \\times n)}$ ] , textual information matrix @xmath74 \\in \\mathbb{r}^{(d_{y } \\times   n)}$ ] and class labels for each of the training samples in @xmath5 . +",
    "* output : * @xmath75 + * steps involved : * + 1 .",
    "formulate the objective function in eq .",
    "[ finalobjectivefunction ] in the form of eq .",
    "[ spectralhashing2 ] .",
    "solve the eq .",
    "[ spectralhashing2 ] by exploiting the smallest k eigenvectors to obtain the hash code matrix @xmath53 .",
    "+ 3 . obtain the textual projection matrix @xmath21 using the eq .",
    "[ wtxt matrix ] .",
    "+ 4 . obtain @xmath67 using the hash code matrix @xmath53 .",
    "+ 5 . continue iterations to obtain @xmath67 from @xmath53 until change in @xmath67 in successive iterations is negligible .",
    "repeat steps 4 and 5 iteratively until the convergence . +",
    "* end *      though zero shot learning as a whole is not a domain adaptation problem but its main objective is to learn a projection function for target domain onto the same semantic space using the concepts learnt from the source data .",
    "this problem of projection domain shift in the context of zero shot learning was first studied in @xcite .",
    "traditional zero shot learning approaches , the learned cross - modal embedding matrix @xmath67 from the seen classes is applied directly to the unseen classes .",
    "since the training classes and testing ones are disjoint , the learned parameters may not be seamlessly suitable for unseen classes .",
    "the problem of domain shift was introduced again in @xcite , in which authors introduced dictionary learning methods to address the issue . in our work , in order to produce the correct hash codes for images from seen and unseen catagories , we further propose a new domain adaptation strategy to learn an improved transformation matrix @xmath76 for the unseen classes . domain adaptation technique has proven its effectiveness in applications where there is a lot of training data in one domain but little to none in another ( @xcite , @xcite , @xcite ) .",
    "we formulate the problem of domain adaptation in online learning fashion .",
    "practically , to ensure a more reliable gradient estimate , instead of using a single sample at a time to update @xmath77 , we use a ( mini ) batch gradient descent to consider more samples while maintaining the efficiency at each iteration . in our experiments ,",
    "the mini - batch is selected as 20 samples and the update usually takes 3 - 4 iterations .",
    "+ given feature matrix @xmath78 representing a mini - batch of images belonging to one of the unseen classes , we first project it to the class embedding space with the transformation matrix @xmath67 learned from the seen classes , such that its label can be predicted by a nearest neighbour classifier with inner - product similarity .",
    "we also want to obtain the hash code matrix @xmath79 for the mini batch of images .",
    "we then obtain the new improved transformation matrix @xmath76 by optimizing the eq .",
    "[ finaldomainadpatation ] : @xmath80 here , @xmath81 is the number of samples in the mini batch , @xmath7 is the number of unseen classes , @xmath82 denotes the class prototype and is equal to @xmath83 for the class @xmath84 , @xmath85 is the textual / attribute vector corresponding to the @xmath84th class , @xmath86 is the @xmath24th column of the updated image projection matrix @xmath77 and @xmath87 is the @xmath24th bit of the updated hash code of the @xmath2th image of the mini batch .",
    "our goal is to obtain the updated matrix @xmath77 and the hash code matrix @xmath79 for the mini batch under consideration .",
    "the first term in eq . [ finaldomainadpatation ] is a prototype term , which ensures that the hash code of each testing sample @xmath3 is close to the corresponding class prototype ( class to which it is likely to belong ) .",
    "the value @xmath88 denotes the weight to enforce closeness between the hash code of @xmath3 and the @xmath84th class prototype @xmath85 from the set of testing ( unseen ) classes , i.e. , @xmath88 = @xmath89 , where @xmath90 calculates the cosine distance between two vectors @xmath91 and @xmath92 .",
    "also , @xmath93 . the second term @xmath94 constrains the learned @xmath76 for unseen classes to be similar to @xmath67 learned from seen classes .",
    "since @xmath67 is learned by preserving the semantic consistency across different modalities , this transformation regularization term ensures that the learned @xmath76 can also effectively project the visual feature to the class embedding space .",
    "the final term of the objective function ensures that the new projection matrix @xmath77 produces hash codes in the most efficient way .",
    "we need to optimize the following objective function with respect to @xmath79 .",
    "@xmath95 differentiating above equation with respect to @xmath79 partially , we get @xmath96 where , @xmath97 is a @xmath98 diagonal matrix , whose diagonal elements are given by @xmath99 .",
    "@xmath100 is the projection of textual / attribute information of each of the unseen classes using the textual projection matrix @xmath21 and is equal to @xmath101 .",
    "@xmath102 is the matrix containing the general textual / attribute information of each of the unseen classes . each element of @xmath103 ( represented as @xmath104 ) is given in eq .",
    "[ bbb ] : @xmath105 equating the eq .",
    "[ bderivative ] to zero , followed by thresholding we have : @xmath106      to perform domain adaptation , we need to optimize the following objective function with respect to @xmath77 .",
    "@xmath107 differentiating above equation with respect to @xmath77 , we get @xmath108 where , each @xmath24-th column of @xmath109 ( represented as @xmath110 ) is given as : @xmath111 where , @xmath112 equating the above eq [ woptimization ] to zero , we have : @xmath113 our method for online domain adaptation is summarized in * algorithm 2*.    [ algorithm3 ] * input : * mini - batch of testing images @xmath114 , textual information matrix for each of the unseen classes @xmath115 \\in \\mathbb{r}^ { { ( d_{y } } \\times n_{u})}$ ] , learned projection matrices from seen classes @xmath67 and @xmath21 .",
    "+ * output : * @xmath116 + * steps involved : * + 1 .",
    "@xmath79 : @xmath79 @xmath117 @xmath53 @xmath118 .",
    "@xmath77 @xmath117 @xmath67 + 3 .",
    "calculate the weight matrix @xmath119 .",
    "fix @xmath120 to optimize @xmath79 with eq [ boptimization ] + 5 .",
    "fix @xmath79 to optimize @xmath77 with eq [ wstaroptimization2 ] + 6 .",
    "update @xmath79 : @xmath79 @xmath117 @xmath121 + 7 .",
    "repeat steps 3 to 6 iteratively until the convergence . +",
    "in our experiments , we employ three real - life image datasets , cifar-10 dataset @xcite , cub-200 - 2011 birds ( cub ) dataset @xcite and animals with attributes dataset ( awa ) @xcite .",
    "first , we present the experimental settings , then describe the datasets and finally report the experimental results .      to quantitatively evaluate our algorithm , we used two metrics i.e. , mean average precision ( map ) @xcite and the precision - recall curve calculated among the range of whole database as measurements .",
    "map focuses on the ranking of retrieval results .",
    "we compared the proposed algorithm with four state - of - the - art hashing approaches , three of them being supervised algorithms : cosdish @xcite , sdh@xcite and zsh @xcite and one unsupervised hashing algorithm : imh @xcite . to evaluate the performance and efficiency of our domain adaptation algorithm ( ours - da )",
    ", we also present the results of the hashing algorithm without using domain adaptation ( ours ) . in all experiments , irrespective of dataset , 2,500 images from the unseen category were selected as query images , and the remaining test images together with the images of seen categories were combined to form the retrieval database .",
    "we illustrate the performance our our method with all the comparing approaches with respect to hash code length of size 8 , 16 , 32 , 64 , 96 , 128 , 192 , 256 and 512 .    * parameter settings : * for imh and zsh algorithm , we randomly sampled 1,000 anchors from the training dataset . for all comparing approaches",
    ", we followed the parameter settings as suggested by the authors of the corresponding papers .",
    "we tuned our experimental parameters using grid - search with cross validation .",
    "we empirically set the regularization parameters @xmath50 to 0.2 , @xmath122 to 6 @xmath123 @xmath124 , @xmath125 to @xmath126 , @xmath72 to 0.2 , @xmath127 to 0.1 , @xmath128 to 0.1 , @xmath129 to 0.5 and set the number of @xmath24 nearest neighbours to 10 .",
    "* cifar-10 dataset : * cifar-10 dataset consists of 60,000 images which are divided into 10 classes with 6,000 samples in each class .",
    "the classes are mutually exclusive .",
    "we used gist features @xcite for cifar-10 dataset to form a feature vector of dimension 512 for each image .",
    "we randomly split the dataset into images of 8 categories of objects as seen classes and the images from the rest of the 2 classes form the unseen classes .",
    "corresponding to each class in the dataset , we extracted the 50-dimensional semantic vectors from the huang dataset @xcite .",
    "we show the comparison results in map and precision - recall curve with different code lengths for cifar-10 dataset in table .",
    "[ tablecifar ] and fig .",
    "[ precisionrecallcifar ] respectively .    * cub-200 - 2011 dataset : * cub-200 - 2011 contains 11,788 images of 200 categories of bird subspecies with 312 fine - grained attributes such as color / shape / texture of body parts .",
    "the attribute features were preprocessed to have zero mean . for cub dataset",
    ", we obtained deep features using vgg with 19-layer network @xcite using matconvnet @xcite .",
    "the image features were normalized so that each datapoint would have a unit norm . for comparing our algorithm with state of the art hashing algorithms",
    ", we followed @xcite to use 150 birds species as seen classes for training and the rest 50 species as unseen classes for testing .",
    "corresponding to each of the class category , we used the preprocessed 312 fine grained attributes as our semantic vector representing the class .",
    "the comparison results are shown in map and precision - recall curve with different code lengths for cub-200 - 2011 dataset in table .",
    "[ tablecub ] and fig .",
    "[ precisionrecallcub ] respectively .",
    "* awa dataset : * awa consists of 30,475 images of 50 mammals classes with 85 binary attributes including color , skin texture , body size , body part , affordance , food source , habitat , and behaviour .",
    "we used decaf @xcite features for awa dataset .",
    "we randomly split the dataset into images of 40 categories of mammals as seen classes and the images from the rest of the 10 classes form the unseen classes .",
    "corresponding to each of the class category , we used the 85 binary attributes as our semantic vector representing the class .",
    "we show the comparison results in map and precision - recall curve with different code lengths for cub-200 - 2011 dataset in table .",
    "[ tableawa ] and fig .",
    "[ precisionrecallawa ] respectively .",
    "* discussion : * tables ( [ tablecifar ] , [ tablecub ] , [ tableawa ] ) and figures ( [ precisionrecallcifar ] , [ precisionrecallcub ] , [ precisionrecallawa ] ) prove the superiority of our algorithm over other competitive algorithms and verifies the effectiveness of the unsupervised domain adaptation in the hashing framework .",
    "interesting observation is that imh ( unsupervised hashing algorithm ) performs better or comparative to some of the supervised hashing algorithms , in terms of map performance , for the case of small bit size .",
    "one possible reason for this is that imh encodes images solely with the distributional properties in the feature space whereas supervised methods exploits a transformation matrix to embed the information of visual features distribution into hash codes using independent semantic labels in the learning process .",
    "therefore , for supervised hashing algorithms more code length is required to ensure the discriminative and descriptive power .",
    "moreover , we could also observe that with increase in hash code length , map performance decreases especially in the case of using 512 bits as hash code length instead of 256 bits .",
    "this is because hamming spaces become sparser as we increase the hash code length .      in this section",
    ", we present the results of the set of experiments performed to evaluate the performance of our proposed algorithm with respect to different numbers of seen categories and training size .",
    "specifically , to observe the number of effect of seen categories , we varied the ratio of seen categories in the training set from 0.1 to 0.9 .",
    "we took all the images from seen categories for training .",
    "further , we randomly selected 2,500 images from the unseen set as query set to search in the retrieval dataset which is formed by combining the rest of the images of unseen categories with that of seen categories .",
    "we fixed the hash code size as 96 bits for all the experiments .",
    "we plot the map and precision curves for our method and compared it with the zsh algorithm in fig .",
    "[ effectofseencategoryratio ] .",
    "precision mainly concentrates on the retrieval accuracy and we reported the results with hamming radius r @xmath130 2 .    from the experiments , we can observe the performance of the algorithms in terms of both the precision and map metrics increases as the ratio of seen categories increases .",
    "this is because our algorithm learns hash function which could produce hash codes with higher discriminablity as the likelihood to find the relevant supervision for the unseen classes increases .",
    "it is also evident from the fig .",
    "[ effectofseencategoryratio ] that our method with domain adaptation outperforms zsh by a very large margin while our method without exploiting domain adaptation has a similar performance as that of zsh .      in this section",
    ", we present the results of the set of experiments performed to evaluate the performance of the proposed algorithm with respect to number of training samples from seen categories .",
    "we took images from 8 classes as our seen images and images from the rest 2 classes as unseen images .",
    "we plot the map and precision curves for our method in fig .",
    "[ effectoftrainingsize ] and compare it with the zsh algorithm by varying the number of training samples from seen classes from 5,000 to 45,000 in the steps of 5,000 samples . at each step , we ensured that we have equal number of samples from each of the seen classes .",
    "further , we randomly selected 2,500 images from the unseen set as query set to search in the retrieval dataset which is formed by combining the rest of the images of unseen categories with all the images ( i.e. 48,000 images ) of seen categories .",
    "we fixed the hash code size as 96 bits for all the experiments .    from the experiments",
    ", we can observe that performance of our algorithm in terms of map metric continuously increases as the size of the training set increases from 5,000 to 25,000 samples and then saturates , whereas the precision metric remains almost the same even with increasing the size of training dataset . as evident from the fig .",
    "[ effectoftrainingsize ] , our method without exploiting domain adaptation has a similar performance as that of zsh while our method integrated with domain adaptation outperforms zsh by a very large margin .",
    "with the escalation of new concepts online and due to the high cost of manual annotations , existing supervised hashing algorithms are unable to report good results on the problem of image retrieval with high accuracy due to the lack of sufficient amount of annotated data . to challenge the issue , in this paper we have proposed a new model to learn an effective hashing function using images and semantic information from limited seen classes . by connecting the dots between the semantic concepts of classes and visual features of images belonging to that class in the common hamming subspace and using the concepts of max - margin learning",
    ", we have learnt a hash function that gives superior results for the task of image retrieval as compared to the existing supervised hashing algorithms as evident from our experiments on three image datasets .",
    "the promising performance on the three real - life image datasets demonstrates the potential of our algorithm integrated with domain adaptation model in indexing and searching real - life image data . instead of performing a naive supervised knowledge transfer from seen categories to unseen categories",
    ", thee proposed unsupervised domain adaptation model which learns to embed data in a visual space to semantic space , exploiting the information from seen and unseen categories . in future , to further improve the results , we would like to explore the deep architecture models which have given superior results in many computer vision tasks , instead of using linear classifier .",
    "in this section , we present the derivation of the eq .",
    "[ spectralhashing ] from the eq .",
    "[ finalobjectivefunction2 ] .",
    "our main objective is formulated as : @xmath131 @xmath132 could be re - written as : @xmath133 therefore , the eq .",
    "[ finalobjectivefunc ] could be re - written as : @xmath134 where , @xmath135 .        c.  strecha , a.  bronstein , m.  bronstein , and p.  fua , `` ldahash : improved matching with smaller descriptors , '' _ ieee transactions on pattern analysis and machine intelligence _ , vol .",
    "34 , no .  1 ,",
    "pp . 6678 , 2012 .",
    "s.  petrovi , m.  osborne , and v.  lavrenko , `` streaming first story detection with application to twitter , '' in _ human language technologies : the 2010 annual conference of the north american chapter of the association for computational linguistics_.1em plus 0.5em minus 0.4em association for computational linguistics , 2010 , pp . 181189 .    x.  liu , d.  tao , m.  song , y.  ruan , c.  chen , and j.  bu , `` weakly supervised multiclass video segmentation , '' in _ proceedings of the ieee conference on computer vision and pattern recognition _ , 2014 , pp .",
    "j.  turian , l.  ratinov , and y.  bengio , `` word representations : a simple and general method for semi - supervised learning , '' in _ proceedings of the 48th annual meeting of the association for computational linguistics_. 1em plus 0.5em minus 0.4emassociation for computational linguistics , 2010 , pp . 384394 .",
    "y.  yang , y.  luo , w.  chen , f.  shen , j.  shao , and h.  t. shen , `` zero - shot hashing via transferring supervised knowledge , '' in _ proceedings of the 2016 acm on multimedia conference_.1em plus 0.5em minus 0.4emacm , 2016 , pp .",
    "12861295 .",
    "y.  gong and s.  lazebnik , `` iterative quantization : a procrustean approach to learning binary codes , '' in _ computer vision and pattern recognition ( cvpr ) , 2011 ieee conference on_.1em plus 0.5em minus 0.4em ieee , 2011 , pp .",
    "817824 .",
    "y.  li , r.  wang , h.  liu , h.  jiang , s.  shan , and x.  chen , `` two birds , one stone : jointly learning binary code for large - scale face image retrieval and attributes prediction , '' in _ proceedings of the ieee international conference on computer vision _ , 2015 , pp .",
    "38193827 .    c.  h. lampert , h.  nickisch , and s.  harmeling ,",
    "`` attribute - based classification for zero - shot visual object categorization , '' _ ieee transactions on pattern analysis and machine intelligence _",
    "36 , no .  3 , pp . 453465 , 2014 .",
    "z.  akata , f.  perronnin , z.  harchaoui , and c.  schmid , `` label - embedding for attribute - based classification , '' in _ proceedings of the ieee conference on computer vision and pattern recognition _ , 2013 , pp .",
    "819826 .",
    "y.  xian , z.  akata , g.  sharma , q.  nguyen , m.  hein , and b.  schiele , `` latent embeddings for zero - shot classification , '' in _ proceedings of the ieee conference on computer vision and pattern recognition _ , 2016 , pp .",
    "a.  farhadi , i.  endres , d.  hoiem , and d.  forsyth , `` describing objects by their attributes , '' in _ computer vision and pattern recognition , 2009 .",
    "cvpr 2009 .",
    "ieee conference on_.1em plus 0.5em minus 0.4emieee , 2009 , pp .",
    "17781785 .",
    "a.  frome , g.  s. corrado , j.  shlens , s.  bengio , j.  dean , t.  mikolov _ et  al .",
    "_ , `` devise : a deep visual - semantic embedding model , '' in _ advances in neural information processing systems _",
    ", 2013 , pp .",
    "21212129 .",
    "t.  mikolov , i.  sutskever , k.  chen , g.  s. corrado , and j.  dean , `` distributed representations of words and phrases and their compositionality , '' in _ advances in neural information processing systems _ , 2013 , pp .",
    "31113119 .",
    "f.  shen , w.  liu , s.  zhang , y.  yang , and h.  t. shen , `` learning binary codes for maximum inner product search , '' in _ 2015 ieee international conference on computer vision ( iccv)_.1em plus 0.5em minus 0.4emieee , 2015 , pp . 41484156 .",
    "m.  rastegari , a.  farhadi , and d.  forsyth , `` attribute discovery via predictable discriminative binary codes , '' in _",
    "european conference on computer vision_.1em plus 0.5em minus 0.4emspringer , 2012 , pp .",
    "876889 .",
    "y.  fu , t.  m. hospedales , t.  xiang , z.  fu , and s.  gong , `` transductive multi - view embedding for zero - shot recognition and annotation , '' in _",
    "european conference on computer vision_.1em plus 0.5em minus 0.4emspringer , 2014 , pp . 584599 .            c.  h. lampert , h.  nickisch , and s.  harmeling , `` learning to detect unseen object classes by between - class attribute transfer , '' in _ computer vision and pattern recognition , 2009 .",
    "cvpr 2009 .",
    "ieee conference on_. 1em plus 0.5em minus 0.4emieee , 2009 , pp . 951958 .",
    "a.  turpin and f.  scholer , `` user performance versus precision measures for simple search tasks , '' in _ proceedings of the 29th annual international acm sigir conference on research and development in information retrieval_.1em plus 0.5em minus 0.4emacm , 2006 , pp .",
    ".    e.  h. huang , r.  socher , c.  d. manning , and a.  y. ng , `` improving word representations via global context and multiple word prototypes , '' in _ proceedings of the 50th annual meeting of the association for computational linguistics : long papers - volume 1_.1em plus 0.5em minus 0.4emassociation for computational linguistics , 2012 , pp .",
    "873882 .",
    "a.  vedaldi and k.  lenc , `` matconvnet : convolutional neural networks for matlab , '' in _ proceedings of the 23rd acm international conference on multimedia_.1em plus 0.5em minus 0.4emacm , 2015 , pp ."
  ],
  "abstract_text": [
    "<S> techniques to learn hash codes which can store and retrieve large dimensional multimedia data efficiently have attracted broad research interests in the recent years . with rapid explosion of newly emerged concepts and online data , existing supervised hashing algorithms </S>",
    "<S> suffer from the problem of scarcity of ground truth annotations due to the high cost of obtaining manual annotations . </S>",
    "<S> therefore , we propose an algorithm to learn a hash function from training images belonging to ` seen ' classes which can efficiently encode images of ` unseen ' classes to binary codes . </S>",
    "<S> specifically , we project the image features from visual space and semantic features from semantic space into a common hamming subspace . </S>",
    "<S> earlier works to generate hash codes have tried to relax the discrete constraints on hash codes and solve the continuous optimization problem . </S>",
    "<S> however , it often leads to quantization errors . in this work , </S>",
    "<S> we use the max - margin classifier to learn an efficient hash function . to address the concern of domain - shift which may arise due to the introduction of new classes </S>",
    "<S> , we also introduce an unsupervised domain adaptation model in the proposed hashing framework . </S>",
    "<S> results on the three datasets show the advantage of using domain adaptation in learning a high - quality hash function and superiority of our method for the task of image retrieval performance as compared to several state - of - the - art hashing methods . </S>"
  ]
}