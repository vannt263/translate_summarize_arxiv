{
  "article_text": [
    "when dealing with functional data , the use of dimension reduction techniques arises as a most natural idea .",
    "some of these techniques are based upon the use of general ( linear ) finite dimensional projections .",
    "this is the case of functional principal component analysis ( fpca ) , see @xcite , although the so - called functional partial least squares ( pls ) methodology is in general preferable when a response variable is involved ; see @xcite for a recent reference .",
    "other common dimension reduction methods in the functional setting include sliced inverse regression ( @xcite ) and additive models ( @xcite ) .",
    "also , the methods based on random projections could offer an interesting alternative .",
    "see , e.g. , @xcite for a short overview of dimension - reduction techniques together with additional references .    _ some comments on the literature . our proposal here",
    "is concerned with a different , more radical , approach to dimension reduction , given by the so - called * variable selection methods .",
    "the aim of variable selection , when applied to functional data , is to replace every infinite dimensional observation @xmath7\\}$ ] , with a finite dimensional vector @xmath8 .",
    "the selection of the `` variables '' @xmath9 should be a consequence of a trade - off between two mutually conflicting goals : representativeness and parsimony .",
    "in other words , we want to retain as much information as possible ( thus selecting relevant variables ) employing a small number of variables ( thus avoiding redundancy ) . *",
    "_    it is clear that variable selection has , at least , an advantage when compared with other dimension reduction methods ( pca , pls ... ) based on general projections : the output of any variable selection method is always directly interpretable in terms of the original variables , provided that the required number @xmath10 of selected variables is not too large . as a matter of fact , variable selection is sometimes the main target itself in many cases where the focus is on model simplification .",
    "we are especially interested in the `` intrinsic '' approaches to variable selection , in the sense that the final output should depend only on the data , not on any assumption on the underlying model ( although the result should be interpretable in terms of the model ) .",
    "there is a vast literature on these topics published by researchers in machine learning or by mathematical statisticians .",
    "the approaches and the terminology used in these two communities are not always alike .",
    "thus , in machine learning , variable selection is often referred to as _ feature selection .",
    "also , the methods we have called `` intrinsic '' are often denoted as `` filter methods '' in machine learning .",
    "it is very common as well ( especially in the setting of regression models ) to use the terms `` sparse '' or `` sparsity '' to describe situations in which variable selection is the first natural aim ; see e.g. , @xcite and @xcite .",
    "it has been also argued in @xcite that the standard sparsity models are sometimes too restrictive so that it is advisable to combine them with other dimension reduction techniques .",
    "the `` relevant '' variables in a functional model are sometimes called `` impact points '' @xcite or `` most predictive design points '' @xcite .",
    "also , the term `` choice of components '' has been used by @xcite as a synonym of variable selection .",
    "_    let us finally mention , with no attempt of exhaustiveness in mind , that the recent literature in functional variable selection includes a version of the classical lasso procedure @xcite , a study of consistency in the variable selection setup @xcite and the use of inverse regression ideas in variable selection @xcite .",
    "the monograph @xcite contains a complete survey on feature extraction ( including selection ) from the point of view of machine learning .",
    "the overview paper by @xcite has a more statistical orientation .    _ the functional classification problem .",
    "in what follows we will focus on variable selection for the problem of supervised binary classification , with functional data .",
    "while the statement and basic ideas behind the supervised classification ( or discrimination ) problem are widely known ( see , e.g. , @xcite ) , we need to briefly recall them for the sake of clarity and for notation purposes .",
    "suppose that an explanatory random variable @xmath11 , taking values in a _ feature space @xmath12 , can be observed in the individuals of two populations @xmath13 and @xmath14 .",
    "let @xmath15 denote a binary random variable , with values in @xmath16 , indicating the membership to @xmath13 or @xmath14 .",
    "on the basis of a data set @xmath17 of @xmath18 independent observations drawn from @xmath19 , the supervised classification problem aims at predicting the membership class @xmath15 of a new observation for which only the variable @xmath11 is known . _ _    a _ classifier or _",
    "classification rule is just a measurable function @xmath20 .",
    "it is natural to assess the performance of a classifier by the corresponding _ classification error @xmath21 .",
    "it is well - known that the classification error @xmath21 is minimized by the so - called _ bayes classifier , @xmath22 , where @xmath23 .",
    "since @xmath24 is in general unknown , it must be approximated , in different ways , by data - driven classifiers .",
    "_ _ _ _    in our functional setting the feature space will be ( unless otherwise stated ) @xmath25 $ ] , the space of real continuous functions defined on @xmath26 $ ] , endowed with the usual supremum norm .",
    "thus , our data will be of type @xmath27 , where the @xmath28 are iid trajectories in @xmath29 $ ] drawn from a stochastic process @xmath30 .",
    "when no confusion is possible , we will denote the whole process by @xmath11 . when convenient , @xmath6 will be denoted @xmath31 .",
    "several functional classifiers have been considered in the literature ( see , e.g. , @xcite for a survey ) . among them , maybe the simplest one is the so - called @xmath32-nearest neighbors rule ( @xmath32-nn ) .",
    "additionally , we will also consider , as a simple standard choice , the classical linear fisher s classifier ( henceforth lda ) , applied to the selected variables .",
    "_ the purpose and contents of this paper .",
    "_    \\(a ) in section [ max - hunting ] we propose a `` maxima hunting '' ( mh ) method for variable selection .",
    "it is essentially based on the idea of selecting the local maxima @xmath2 of the function @xmath33 , where @xmath34 denotes the `` distance covariance '' association measure for random variables due to @xcite .",
    "an alternative version of the mh procedure can be obtained by replacing @xmath35 by the `` distance correlation '' @xmath36 .",
    "see section [ aux ] for a short review of the definitions and properties of @xmath34 and @xmath37 .",
    "some useful simplified versions for @xmath34 are obtained in th .",
    "[ expresiones ] of section [ max - hunting ] , for the particular case where @xmath15 is a binary variable .",
    "a result of consistent estimation ( th .",
    "[ th : consistency ] ) for the maxima of @xmath34 is also proved in that section .",
    "\\(b ) in section [ motiv ] we give several models ( identified in terms of the conditional distributions @xmath38 ) in which the optimal classification rule depends only on a finite number of variables .",
    "we also show that in some of these models the variables to be selected coincide with the maxima of @xmath34 .",
    "these results provide a theoretical basis for the techniques of variable selection in functional classification models .",
    "usually these techniques are considered from an exclusively algorithmic or computational point of view .",
    "it is therefore of some interest to motivate them in `` population terms '' , by identifying some specific models where these techniques have full sense .",
    "as pointed out by @xcite , _ `` curiously , despite a huge research activity in this area , few attempts have been made to connect the rich theory of stochastic processes with functional data analysis '' .",
    "so the present paper can be seen as a contribution to partially fill this gap .",
    "_    \\(c ) an extensive simulation study , comparing our variable selection methods with other dimension reduction procedures ( as well as with the `` baseline option '' of doing no variable selection at all ) is included in section [ sim ] .",
    "three real data examples are discussed in section [ real ] .",
    "section [ conclusiones ] includes some final conclusions as well as a ranking of all considered methods .",
    "all the proofs are included in the appendix .",
    "the problem of finding appropriate association measures between random variables ( beyond the standard linear correlation coefficient ) has received increasing attention in recent years ; see for instance @xcite .",
    "we will use here the association measure proposed by @xcite , see also @xcite .",
    "it is called _ distance covariance ( or _ distance correlation in the standardized version ) .",
    "it has a number of valuable properties : first , it can be used to define the association between two random variables @xmath11 and @xmath15 of arbitrary ( possibly different ) dimensions ; second , it characterizes independence in the sense that the distance covariance between @xmath11 and @xmath15 is zero if and only if @xmath11 and @xmath15 are independent ; third , the distance correlation can be easily estimated in a natural plug - in way , with no need of smoothing or discretization . _ _    [ def : dcov ] given two random variables @xmath11 and @xmath15 taking values in @xmath39 and @xmath40 , respectively , let @xmath41 , @xmath42 , @xmath43 be the characteristic functions of @xmath19 , @xmath11 and @xmath15 , respectively .",
    "assume that the components of @xmath11 and @xmath15 have finite first - order moments .",
    "the distance covariance between @xmath11 and @xmath15 , is the non - negative number @xmath44 defined by @xmath45 with @xmath46 , where @xmath47 is half the surface area of the unit sphere in @xmath48 and @xmath49 stands for the euclidean norm in @xmath50 . finally , denoting @xmath51 ,",
    "the ( square ) distance correlation is defined by @xmath52 if @xmath53 , @xmath54 @xmath55 otherwise .",
    "note that these definitions make sense even if @xmath11 and @xmath15 have different dimensions ( i.e. , @xmath56 ) .",
    "in addition , the association measure @xmath57 can be consistently estimated through a relatively simple average of products calculated in terms of the mutual pairwise distances @xmath58 and @xmath59 between the sample values @xmath28 and the @xmath60 ; see ( * ? ? ?",
    "* expression ( 2.8 ) ) .",
    "see also @xcite for a different use of the correlation distance in variable selection .",
    "our proposal is based on a direct use of the distance covariance association measure .",
    "we just suggest to select the values of @xmath61 corresponding to local maxima of the distance - covariance function @xmath62 or , alternatively , of the distance correlation function @xmath63 .",
    "this method has a sound intuitive basis as it provides a simple natural way to deal with the relevance vs. redundancy trade - off : the selected values must carry a large amount of information on @xmath15 , which takes into account the _ relevance of the selected variables . in addition , the fact of considering local maxima automatically takes care of the _ redundancy problem , since the highly relevant points close to the local maxima are automatically excluded from consideration .",
    "this intuition is empirically confirmed by the results of section [ sim ] , where the practical performance of the maxima - hunting method is quite satisfactory .",
    "figure [ fig : maxima ] shows how the fun ction @xmath62 looks like in two different examples . _ _    .",
    "right : logistic model l11 ( explained in subsection [ estruct ] ) with 50 ornstein  uhlenbeck trajectories .",
    "@xmath62 ( scaled ) is in black and the relevant variables are marked by vertical dashed lines . ]",
    "the extreme flexibility of these association measures allows us to consider the case of a multivariate response @xmath15 .",
    "so there is no conceptual restriction to apply the same ideas for multiple classification or even to a regression problem .",
    "however , we will limit ourselves here to the important problem of binary classification . in this case",
    "we can derive simplified expressions for @xmath57 which are particularly convenient in order to get empirical approximations .",
    "this is next shown .",
    "for the sake of generality , the results of this section will be obtained for the @xmath10-variate case , although in the rest of the paper we will use them just for @xmath10=1 .",
    "thus , throughout this subsection , @xmath10 will denote a natural number and @xmath61 will stand for a vector @xmath64 @xmath65^d$ ] . also , for a given process",
    "@xmath11 , we abbreviate @xmath66 by @xmath31 and @xmath67 will denote an independent copy of a random variable @xmath68 .",
    "we write @xmath69 and @xmath70 to denote the transposed and the euclidean norm of a vector @xmath71 .",
    "let @xmath72 so that @xmath73 where the symbol @xmath74 stands for `` is distributed as '' . observe that @xmath75 .",
    "our variable selection methodology will heavily depend on the function @xmath76 giving the distance covariance dependence measure between the marginal vector @xmath77 , for @xmath78^d$ ] and @xmath79 , and the class variable @xmath15 .",
    "the following theorem gives three alternative expressions for this function .",
    "the third one will be particularly useful in what follows .",
    "[ expresiones ] in the setting of the functional classification problem above stated , the function @xmath76 defined in ( [ dcov ] ) can be alternatively calculated with the following expressions , @xmath80 where @xmath81}}$ ] and @xmath82 is given in definition [ def : dcov ] .",
    "@xmath83}}\\nonumber \\\\ = & -2{\\ensuremath{\\mathbb{e } \\left [ { ( y - p)(y'-p)|x_t - x'_t|_d } \\right]}},\\end{aligned}\\ ] ] where @xmath84 denotes an independent copy of @xmath19 , respectively .",
    "@xmath85,\\ ] ] where @xmath86 .    in a training sample @xmath87",
    "denote by @xmath88 and @xmath89 the @xmath11-observations corresponding to values @xmath90 and @xmath91 , respectively . in this section",
    ", we use these data to obtain an estimator of @xmath62 , which is uniformly consistent in @xmath61 . as a consequence",
    ", we can estimate the local maxima of @xmath62 : using part ( c ) of theorem [ expresiones ] , a natural estimator for @xmath62 is @xmath92,\\ ] ] where @xmath93 , @xmath94 for @xmath95 , and @xmath96 the uniform strong consistency of @xmath97 is established in theorem [ th : consistency ] below .",
    "[ th : consistency ] let @xmath98 , with @xmath99^d$ ] , be a process with continuous trajectories almost surely such that @xmath100 .",
    "then , @xmath97 is continuous in @xmath61 and @xmath101^d}{\\sup}|{\\cal v}_n^2(x_t , y)-{\\cal v}^2(x_t , y)| \\to 0 \\ \\ \\mbox{a.s.,\\ as } n\\to\\infty.\\ ] ] hence , if we assume that @xmath62 has exactly @xmath102 local maxima at @xmath103 , then @xmath97 has also eventually at least @xmath102 maxima at @xmath104 with @xmath105 , as @xmath106 , a.s . , for @xmath107 .",
    "the variable selection methods we are considering here for the binary functional classification problem are aimed at selecting _ a finite number of variables .",
    "one might think that this is a `` too coarse '' approach for functional data .",
    "nevertheless , we provide here some theoretical motivation by showing that , in some relevant cases , variable selection is `` the best we can do '' in the sense that , in some relevant models , the bayes rule ( i.e. , the optimal classifier ) has an expression of type @xmath108 , so that it depends only on a finite ( typically small ) number of variables . in fact , in many situations , a proper variable selection leads to an improvement in efficiency ( with respect to the baseline option of using the full sample curves ) , due to the gains associated with a smaller noise level . _    the distribution of @xmath109 ,",
    "will be denoted by @xmath110 for @xmath111 . in all the examples below the considered processes",
    "are gaussian , i.e. , for all @xmath112 $ ] , with @xmath113 , the finite - dimensional marginal @xmath114 has a normal distribution in @xmath115 for @xmath111 .",
    "many considered models have non - smooth , brownian - like trajectories .",
    "these models play a very relevant role in statistical applications , in particular to the classification problem ; see , e.g. , @xcite .",
    "let us now recall some basic notions and results to be used throughout ( see , e.g. , ( * ? ? ?",
    "4 ) , for details ) : @xmath116 is said to be _ absolutely continuous with respect to @xmath117 ( which is denoted by @xmath118 ) if and only if @xmath119 entails @xmath120 , @xmath121 being a borel set in @xmath29 $ ] .",
    "two probability measures @xmath116 and @xmath117 are said to be _",
    "equivalent if @xmath118 and @xmath122 ; they are _ mutually singular when there exists a borelian set @xmath121 such that @xmath119 and @xmath123 .",
    "the so - called _ hajek - feldman dichotomy ( see @xcite ) states that if @xmath116 and @xmath117 are gaussian , then they are either equivalent or mutually singular .",
    "the _ radon - nikodym theorem establishes that @xmath124 if and only if there exists a measurable function @xmath125 such that @xmath126 for all borel set @xmath121 .",
    "the function @xmath125 ( which is unique @xmath116-almost surely ) is called _ radon - nikodym derivative of @xmath117 which respect to @xmath116",
    ". it is usually represented by @xmath127 . _ _ _ _ _ _    finally , in order to obtain the results in this section we need to recall ( see ( * ? ? ?",
    "1 ) ) that @xmath128^{-1},\\ \\",
    "\\mbox{for}\\ x\\in { \\mathcal s},\\ ] ] where @xmath129 is the common support of @xmath116 and @xmath117 , and @xmath130 .",
    "this equation provides the expression for the optimal rule @xmath131 in some important cases where the radon - nikodym derivative is explicitly known .    _ some examples .",
    "two non - trivial situations in which the radon - nikodym derivatives can be explicitly calculated are those problems where @xmath116 is the standard brownian motion @xmath132 , and @xmath117 corresponds to @xmath132 plus a stochastic or a linear trend . in both cases",
    "the bayes rule @xmath24 turns out to depend just on one value of @xmath61 . to be more precise",
    ", it has the form @xmath133 .",
    "this is formally stated in the following results .",
    "proofs can be found in the appendix .",
    "_    [ bvsbst ] let us assume that @xmath116 is the distribution of a standard brownian motion @xmath134 $ ] and @xmath117 is the distribution of @xmath135 , where @xmath136 is a random variable with distribution @xmath137 , independent from @xmath138 .",
    "then , the bayes rule is given by @xmath139 $ ] .",
    "as a particular case , when the prior probabilities of both groups are equal , @xmath140 , we get @xmath141 if and only if @xmath142    [ bvsblt ] let us assume that @xmath116 is the distribution of a standard brownian motion @xmath134 $ ] and @xmath117 is the distribution of @xmath143 , where @xmath144 is a constant .",
    "then , for @xmath145 $ ] the bayes rule is given by @xmath146 , if @xmath147 , and @xmath148 , if @xmath149 .    before presenting our third example",
    "we need some additional notation .",
    "let us now define the countable family of _ haar functions , @xmath150 @xmath151 , $ ] for @xmath152 , @xmath153 .",
    "the family @xmath154 is known to be an orthonormal basis in @xmath155 $ ] . moreover , define the `` peak '' functions @xmath156 by @xmath157 we want to use these peak functions to define the trend of the @xmath117 distribution in another model of type `` brownian versus brownian plus trend '' . in this case",
    "the bayes rule depends just on three points . _",
    "[ bvsbtri ] let us assume that @xmath116 is the distribution of a standard brownian motion @xmath134 $ ] and @xmath117 is the distribution of @xmath158 , where @xmath156 is one of the peak functions defined above .",
    "then , for @xmath145 $ ] the regression function @xmath159 is @xmath160      \\right)+1 \\right\\}^{-1}\\end{aligned}\\ ] ] and the bayes rule @xmath161 fulfils @xmath141 if and only if @xmath162    let us recall that , according to cameron - martin theorem ( see @xcite ) , in order to get the equivalence of @xmath117 and @xmath116 the trend function is required to belong to the dirichlet space @xmath163 $ ] of real functions @xmath164 defined in @xmath26 $ ] which have a derivative @xmath165 in @xmath155 $ ] such that @xmath166 . it can be seen ( @xcite ) that @xmath167 is an orthonormal basis for @xmath163 $ ] .",
    "analogous calculations can be performed ( still obtaining explicit expressions for the bayes rule of type @xmath168 ) , using a rescaled brownian motion @xmath169 or the brownian bridge instead of @xmath132 , or a piecewise linear trend instead of these .",
    "likewise , other models could be obtained by linear combinations in the trend functions or by finite mixtures of other simpler models .",
    "many of them have been included in the simulation study of section [ sim ] .",
    "next , we will provide some theoretical support for the maxima - hunting method , by showing that in some specific useful models the optimal classification rule depends on the maxima of the distance covariance function @xmath62 , although in some particular examples , other points ( closely linked to the maxima ) are also relevant .",
    "[ maximo - unico ] under the models assumed in propositions [ bvsbst ] and [ bvsblt ] , the corresponding distance covariance functions @xmath76 have both a unique relative maximum at the point @xmath170 .",
    "[ rem : maximo ] other similar results could be obtained for the model considered in proposition [ bvsbtri ] as well as for the brownian bridge vs. brownian motion model .",
    "the model considered in proposition [ bvsbst ] provides a clear example of the advantages of using the distance covariance measure @xmath76 rather than the ordinary covariance @xmath171 in the maxima - hunting procedure .",
    "indeed , note that in this case , @xmath172 for all @xmath99 $ ] , so that the ordinary covariance is useless to detect any difference between the values of @xmath61 .",
    "we describe here in detail the methods under study and the models to be considered together with a summary of the results . the full outputs can be found in www.uam.es/antonio.cuevas/exp/outputs.xlsx .",
    "these are the methods , and their corresponding notations as they appear in the tables and figures below .",
    "maxima - hunting*.",
    "the functional data @xmath173 @xmath99 $ ] are discretized to @xmath174 @xmath175 , so a non - trivial practical problem is to decide which points in the grid are the local maxima : a point @xmath176 is declared to be a local maximum when it is the highest local maximum on the sub - grid @xmath177 , @xmath178 .",
    "the proper choice of @xmath179 depends on the nature and discretization pattern of the data at hand .",
    "thus , @xmath179 could be considered as a smoothing parameter to be selected in an approximately optimal way . in our experiments @xmath179",
    "is chosen by a validation step explained in next section .    then , we sort the maxima @xmath176 by * relevance ( the value of the function at @xmath176 ) .",
    "this seems to be the natural order and it produces better results than other simple sorting strategies .",
    "we denote these maxima - hunting methods by * mhr * and * mhv * depending on the use of @xmath180 or @xmath181 . *",
    "\\2 . * univariate @xmath61-ranking method , denoted by * t * , is frequently used when selecting relevant variables ( see e.g. the review by @xcite ) .",
    "it is based on the simple idea of selecting the variables @xmath31 with highest student s @xmath61 two - sample scores @xmath182",
    ". *    \\3 . *",
    "mrmr*. the minimum redundancy maximum relevance algorithm , proposed in @xcite and @xcite , is a relevant intrinsic variable selection method ; see @xcite for a recent contribution .",
    "it aims at maximizing the relevance of the selected variables avoiding an excess of redundancy what seems particularly suitable for functional data . denoting the set of selected variables by @xmath183 , the variables are sequentially incorporated to @xmath183 with the criterion of maximizing the difference @xmath184 ( or alternatively the quotient @xmath185 ) .",
    "two ways of measuring relevance and redundancy have been proposed : first , we can use the fisher statistic for relevance and the standard correlation for redundancy .",
    "second , a three - fold discretized version of the so - called _ mutual information _ measure for both relevance and redundancy ( see ( * ? ? ?",
    "* equation ( 1 ) ) ) .    in principle",
    "these two approaches are intended for continuous and discrete variables respectively .",
    "however , @xcite report a good performance for the second one even in the continuous case .",
    "we have considered mrmr as a natural competitor for our maxima - hunting approximation .",
    "we have computed both fisher - correlation and mutual information approaches with both difference and quotient criteria . for the sake of clarity",
    "we only show here the results of * fcq * ( fisher correlation quotient ) and * mid * ( mutual information difference ) which outperform on average their corresponding counterparts .    \\4 .",
    "* pls*. according to the available results ( @xcite ) pls is the  ` method of choice' for dimension reduction in functional classification .",
    "note however that pls is not a variable selection procedure ; in particular it lacks the interpretability of variable selection . in some sense ,",
    "the motivation for including pls is to check how much do we lose by restricting ourselves to variable selection methods , instead of considering other more general linear projections procedures ( as pls ) for dimension reduction .",
    "\\5 . * base*. the @xmath32-nn classifier is applied to the entire curves .",
    "the base performance can be seen as a reference to assess the usefulness of dimension reduction methods . somewhat surprisingly , base is often outperformed .",
    "note that the base method can not be implemented with lda since this classifier typically fails with infinite or high - dimensional data ; see , e.g. ( * ? ? ?",
    "* section 6.1 ) , for some insights and references .",
    "the * classifiers used in all cases are either @xmath32-nn , based on the euclidean distance or lda ( applied to the selected variables ) .",
    "similar comparisons could be done with other classifiers , since the considered methods do not depend on the classifier . for comparing the different methods we use the natural accuracy measure , defined by the percentage of correct classification . *",
    "our simulation study consists of 400 experiments , aimed at comparing the practical performances of several intrinsic variable selection methods described in the previous subsection .",
    "these experiments are obtained by considering 100 different underlying models and 4 sample sizes , where by `` model '' we mean either ,    * a pair of distributions for @xmath186 and @xmath187 ( corresponding to @xmath13 and @xmath14 , respectively ) ; in all cases , we take @xmath188 . * the marginal distribution of @xmath11 plus @xmath189 .",
    "models vary in difficulty and number of relevant variables . in all the considered models",
    "the optimal bayes rule turns out to depend on a finite number of relevant variables , see section 3 .",
    "the processes involved include also different levels of smoothing .",
    "the full list of considered models is available in the supplementary material document .",
    "all of them belong to one of the following classes :    \\1 .",
    "* gaussian models : they are denoted @xmath190 .",
    "all of them are generated according to the general pattern ( m1 ) . in all cases",
    "the distributions of @xmath109 are chosen among one of the following types : first , the * standard brownian motion , @xmath138 , in @xmath26 $ ] .",
    "second , * brownian motion , @xmath191 , with a trend @xmath192 , i.e. , @xmath193 @xmath194 ( we have considered several choices for @xmath192 ) .",
    "third , the * brownian bridge : @xmath195 .",
    "our fourth class of gaussian processes is the * ornstein  uhlenbeck process , with a covariance function of type @xmath196 and zero mean ( @xmath197 ) or different mean functions @xmath192 ( @xmath198 ) .",
    "finally smoother processes have been also computed by convolving brownian trajectories with gaussian kernels .",
    "we have considered two levels of smoothing denoted by sb and ssb . * * * * *    \\2 .",
    "* logistic models : they are defined through the general pattern ( m2 ) : the process @xmath199 follows one of the above mentioned distributions and @xmath200 with @xmath201 , a function of the relevant variables @xmath202 .",
    "we have considered 15 versions of this model and a few variants , denoted @xmath203 , @xmath204 .",
    "they correspond to different choices for the link function @xmath205 ( most of them linear or polynomial ) and for the distribution of @xmath11 .",
    "for example , in the models l2 and l8 we have @xmath206 and @xmath207 , respectively . *    \\3 . * mixtures : they are obtained by combining ( via mixtures ) in several ways the above mentioned gaussian distributions assumed for @xmath186 and @xmath187 .",
    "these models are denoted m1 , ... , m11 in the output tables .",
    "*    for each model , all the variable selection methods ( as well as pls ) are checked for sample sizes @xmath208 , 50 , 100 , 200 .",
    "so we get @xmath209 experiments .",
    "all the functional simulated data are * discretized to @xmath210 , where @xmath176 are equispaced points in @xmath26 $ ] . in fact ( to avoid the degeneracy @xmath211 in the brownian - like models )",
    "we take @xmath212 .",
    "similarly , for the case of the brownian bridge , we truncate as well at the end of the interval .",
    "*    the involved parameters are : the number @xmath32 of nearest neighbors in the @xmath32-nn classifier , the dimension of the reduced space ( number of variables or pls components ) and the smoothing parameter @xmath179 in maxima - hunting methods .",
    "these are set by standard data - based validation procedures .",
    "parameter validation can be carried out mainly through a validation set or by cross - validation on the training set ( see e.g. @xcite ) . in the case of the simulation study ,",
    "validation and test samples of size 200 are randomly generated . in the real data sets we proceed by cross - validation .",
    "we have selected ( with no particular criterion in mind ) a sampling of just a few examples among the 400 experiments .",
    "the complete simulation outputs can be downloaded from www.uam.es/antonio.cuevas/exp/outputs.xlsx .",
    "table 1 provides the performance ( averaged on 200 runs ) measured in terms of classification accuracy ( percentages of correct classification ) .",
    "models are presented in rows and methods in columns .",
    "the marked outputs correspond to the winner and second best method in each row .",
    "lccccccc + models & fcq & mid & t & pls & mhr & mhv & base + l2_out & 82.47 & 82.11 & 81.68 & & 83.22 & & 82.60 + l6_ou & 88.41 & 89.81 & 86.19 & & 90.75 & & 90.56 + l10_b & 81.09 & 85.02 & 81.13 & 85.90 & & & 85.46 + l11_ssb & 82.31 & 80.85 & 82.28 & 78.81 & & & 79.89 + l12_sb & 77.24 & 75.83 & & 74.92 & & 76.62 & 74.78 + g1 & 65.86 & 70.70 & 65.57 & 66.95 & & & 70.10 + g3 & 63.09 & 73.39 & 60.57 & 60.56 & & & 65.26 + g6 & 84.27 & 91.95 & 84.14 & & 93.38 & & 92.19 + m2 & 70.77 & 69.82 & 69.16 & & 74.76 & & 71.14 + m6 & 81.15 & 83.08 & 79.73 & & & 83.35 & 80.99 + m10 & 64.93 & 68.33 & 64.58 & 68.25 & & & 68.95 +   + models & fcq & mid & t & pls & mhr & mhv & base + l2_out & 79.80 & 78.95 & 78.23 & 80.07 & & & - + l6_ou & 87.79 & 88.91 & 84.46 & & & 89.35 & - + l10_b & 75.97 & 75.44 & 76.04 & 77.60 & & & - + l11_ssb & 80.95 & 80.09 & 80.81 & 79.39 & & & - + l12_sb & 76.39 & 75.20 & & 75.02 & & 75.96 & - + g1 & 51.27 & 51.24 & 51.20 & 51.44 & & & - + g3 & 51.09 & 52.26 & 50.96 & 50.35 & & & - + g6 & 87.72 & 95.28 & 87.80 & & 96.54 & & - + m2 & 67.44 & 76.51 & 66.81 & & 82.24 & & - + m6 & 79.99 & 79.92 & 79.63 & & 81.08 & & - + m10 & 60.03 & 65.61 & 59.24 & & 67.25 & & +    the outputs of table 1 are more or less representative of the overall conclusions of the entire study .",
    "for instance , mhr appears as the overall winner on average with a slight advantage .",
    "pls and the maxima - hunting methods ( mhr and mhv ) obtain similar scores and clearly outperform the other benchmark methods .",
    "note that they also beat ( often very clearly ) the base method in almost all cases using just a few variables .",
    "this shows that dimension reduction is , in fact , `` mandatory '' in many cases .",
    "regarding the comparison of @xmath32-nn and lda in the second stage ( after dimension reduction ) the results show a slight advantage for @xmath32-nn ( on average ) . the complete failure of lda in models g1 and g3 was to be expected since in these cases the mean functions are identical in both populations . in terms of number of variables ,",
    "when @xmath32-nn is used , mhr and mhv need less variables to achieve better results than the rest of variable selection methods .",
    "when lda is used , the number of required variables is quite similar in all methods ; see the supplementary material , section s4 .",
    "we have chosen three examples due to their popularity in fda .",
    "there are many references on these datasets so we will just give brief descriptions of them ; additional details can be found in the supplementary material document .",
    "figure [ fig : reales ] shows the trajectories @xmath6 and mean functions for each set and each class .",
    "_ berkeley growth data .",
    "_ the heights of 54 girls and 39 boys measured at 31 non equidistant time points .",
    "see , e.g. , @xcite .",
    "_ tecator .",
    "_ 215 near - infrared absorbance spectra ( 100 grid points each ) of finely chopped meat , obtained using a tecator infratec food & feed analyzer .",
    "the sample is separated in two classes according to the fat content ( smaller or larger than 20% ) .",
    "tecator curves are often used in a differentiated version .",
    "we use here the second derivatives .",
    "see @xcite for details .",
    "_ phoneme .",
    "_ as in @xcite we use the `` binary '' version of these data corresponding to log - periodograms constructed from 32 ms long recordings of males pronouncing the phonemes `` aa '' and `` ao '' .",
    "the sample size is @xmath213 ( @xmath214 from `` aa '' and @xmath215 from `` ao '' ) .",
    "each curve was observed at 256 equispaced points .    in the comparisons with real data sets",
    "we have incorporated the method recently proposed by @xcite .",
    "we denote it by dhb .",
    "given a classifier , the dhb method proposes a leave - one - out choice of the best variables for the considered classification problem . while this is a worthwhile natural idea , it is computationally intensive .",
    "so the authors implement a slightly modified version , which we have closely followed .",
    "it is based on a sort of trade - off between full and sequential search , together with some additional computational savings .",
    "let us note , as an important difference with our maxima - hunting method , that the dhb procedure is a `` wrapper '' method , in the sense that it depends on the chosen classifier .",
    "following @xcite , we have only implemented the dhb method with the lda classifier .    apart from that ,",
    "we proceed as in the simulation study except for the generation of the training , validation and test samples . here",
    "we consider the usual cross - validation procedure which avoids splitting the sample ( sometimes small ) into three different sets .",
    "each output is obtained by standard leave - one - out cross - validation .",
    "the only exception is the phoneme data set for which this procedure is extremely time - consuming ( due to the large sample size ) ; so we use instead ten - fold cross - validation ( 10cv ) .",
    "the respective validation steps are done with the same resampling schemes within the training samples .",
    "this is a usual way to proceed when working with real data ; see ( * ? ? ? * subsection 7.10 ) .",
    "several outputs are given in tables 2 ( accuracy ) and 3 ( number of variables ) below .",
    "the complete results can be found in www.uam.es/antonio.cuevas/exp/outputs.xlsx .",
    "lcccccccc + data & fcq & mid & t & pls & mhr & mhv & dhb & base + growth & 83.87 & & 83.87 & 94.62 & & 94.62 & - & + tecator & 99.07 & 99.07 & 99.07 & 97.21 & & & - & 98.60 + phoneme & & 79.62 & & & 80.20 & 78.86 & - & 78.97 +   + data & fcq & mid & t & pls & mhr & mhv & dhb & base + growth & 91.40 & 94.62 & 91.40 & 95.70 & 95.70 & & & - + tecator & 94.42 & & 94.42 & 94.42 & & 94.88 & & - + phoneme & 79.38 & & 79.09 & & 80.20 & 78.92 & 77.34 & - +    lcccccccc + data & fcq & mid & t & pls & mhr & mhv & dhb & base + growth & & 3.5 & & 2.8 & 4.0 & 4.0 & - & 31 + tecator & 3.0 & 5.7 & 3.0 & 2.7 & & & - & 100 + phoneme & & 15.3 & 12.3 & 12.9 & & 12.3 & - & 256 +   + data & fcq & mid & t & pls & mhr & mhv & dhb & base + growth & 5.0 & 3.4 & 5.0 & & 4.0 & 4.0 & & - + tecator & 8.4 & 2.6 & 3.1 & 9.7 & & & 3.0 & - + phoneme & 8.5 & 17.1 & & 15.5 & 16.1 & 11.0 & & - +    these results are similar to those obtained in the simulation study .",
    "while ( as expected ) there is no clear global winner , maxima - hunting method looks as a very competitive choice .",
    "in particular , tecator outputs are striking , since mhr and mhv achieve ( with @xmath32-nn ) a near perfect classification with just one variable .",
    "note also that maxima - hunting methods ( particularly mhr ) outperform or are very close to the base outputs ( which uses the entire curves ) .",
    "pls is overcome by our methods in two of the three problems but it is the clear winner in phoneme example . in any case , it should be kept in mind , as a counterpart , the ease of interpretability of the variable selection methods .",
    "the dhb method performs well in the two first considered examples but relatively fails in the phoneme case .",
    "there is maybe some room for improvement in the stopping criterion ( recall that we have used the same parameters as in @xcite ) .",
    "recall also that , by construction , this is ( in the machine learning terminology ) a `` wrapper '' method .",
    "this means that the variables selected by dhb are specific for the lda classifier ( and might dramatically change with other classification rules ) .",
    "also note that the use of the lda classifier did nt lead to any significant gain ; in fact , the results are globally worse than those of @xmath32-nn except for a few particular cases .",
    "although our methodology is not primarily targeted to the best classification rate , but to the choice of the most representative variables , we can conclude that mh procedures combined with the simple @xmath32-nn are competitive when compared with pls and other successful and sophisticated methods in literature : see @xcite for tecator data , @xcite for growth data and @xcite for phoneme data .",
    "we have summarized the conclusions of our 400 simulation experiments in three rankings , prepared with different criteria , according to * classification accuracy . with the * relative ranking criterion , the winner method ( with performance @xmath216 ) in each of the 400 experiments gets 10 score points , and the method with the worst performance ( say @xmath217 ) gets 0 points .",
    "the score of any other method , with performance @xmath218 is just assigned in a proportional way : @xmath219 .",
    "the * positional ranking scoring criterion just gives 10 points to the winner in every experiment , 9 points to the second one , etc .",
    "finally , the * f1 ranking * rewards strongly the winner . for each experiment ,",
    "points are divided as in an f1 grand prix : the winner gets 25 points and the rest 18 , 15 , 10 , 8 , 6 and 4 successively .",
    "the final average scores are given in table 4 .",
    "the winner and the second best methods in each category appear marked .",
    "* * *    .2 cm    lccccccc + ranking criterion & fcq & mid & t & pls & mhr & mhv & base + relative & 4.42 & 5.80 & 2.93 & 6.99 & & & 3.64 + positional & 6.44 & 6.71 & 5.50 & & & 7.84 & 5.89 + f1 & 11.62 & 12.04 & 9.46 & & & 15.41 & 10.15 +   + ranking criterion & fcq & mid & t & pls & mhr & mhv & base + relative & 3.76 & 5.19 & 1.96 & 6.90 & & & - + positional & 6.70 & 6.99 & 5.92 & 8.13 & & & - + f1 & 11.95 & 12.52 & 10.22 & & & 17.47 & - +    the results are self - explanatory .",
    "nevertheless , * the following conclusions might be of some interest for practitioners : *    \\1 .",
    "the maxima - hunting methods are the global winners ( in particular when using the distance correlation measure ) , even if there is still room for improvement in the maxima identification .",
    "in fact , the maxima - hunting procedures result in accuracy improvements ( with respect to the `` base error '' , i.e. , using the whole trajectories ) in 88.00% of the considered experiments .",
    "overall , the gain of accuracy associated with * mhr variable selection is relevant ( 2.41% ) . *",
    "while the univariate ranking methods , such as the @xmath61 ranking , ( which ignore the dependence between the involved variables ) are still quite popular among practitioners , they are clearly outperformed by the `` functional '' procedures .",
    "it is quite remarkable the superiority of the maxima - hunting methods on the rest of variable selection procedures , requiring often a lesser number of variables .",
    "\\3 . as an important overall conclusion",
    ", variable selection appears as a * highly competitive alternative to pls , which is so far the standard dimension reduction method in high - dimensional and functional statistics ( whenever a response variable is involved ) .",
    "the results of the above rankings show that variable selection offers a better balance in terms of both accuracy and interpretability . *",
    "\\4 . on average ,",
    "the use of the classical fisher s discriminant rule lda ( after dimension reduction ) provides worse results than the nonparametric @xmath32-nn rule .",
    "an example of superiority of a linear classifier is shown in @xcite where an asymptotic optimality result is provided .",
    "in addition , under some conditions , the proposed classifier turns out to be `` near - perfect '' ( in the sense that the probability of classification error can be made arbitrarily small ) to discriminate between two gaussian processes .",
    "this is an interesting phenomenon which does not appear in the finite dimensional case .",
    "however , it requires that the gaussian measures under discrimination are mutually singular ( note that this situation can not happen with two non - degenerate gaussian measures in @xmath50 ) .",
    "this topic will be considered in a forthcoming manuscript by the authors .    _ a final remark .",
    "the present study shows that there are several quite natural models in which the maxima - hunting method is definitely to be recommended .",
    "the real data results are also encouraging .",
    "our results suggest that , even when there is no clear , well - founded guess on the nature of the underlying model , the idea of selecting the maxima of the distance correlation is a suitable choice , that always allows for a direct interpretation .",
    "it is natural to ask what type of models would typically be less favorable for the maxima - hunting approach . as a rough , practical guide , we might say that some adverse situations might typically arise in those cases where the trajectories are extremely smooth , or when they are very wiggly , with many noisy abrupt peaks which tend to mislead the calculation of the maxima in the distance correlation function .",
    "_    * supplementary materials . *",
    "all the proofs and two auxiliary results can be found in the appendix .",
    "some further methodological and technical details are explained in the supplementary materials document below .",
    "it also includes some extra simulation outputs and the list of the 100 considered models .",
    "the full simulation outputs are included in an excel file downloadable from www.uam.es/antonio.cuevas/exp/outputs.xlsx",
    ".    * acknowledgment .",
    "* this research has been supported by spanish grant mtm2013 - 44045-p .",
    "to prove theorem 2 we need two lemmas dealing with the uniform strong consistency of one - sample and two - sample functional u - statistics , respectively .",
    "[ lemma : ustatistics ] let @xmath220 be a process with continuous trajectories a.s .",
    "defined on the compact rectangle @xmath221\\subset \\mathbb{r}^d$ ] .",
    "let @xmath222 be a sample of @xmath18 independent trajectories of @xmath11 .",
    "define the functional u - statistic @xmath223,\\ ] ] where the kernel @xmath32 is a real continuous , permutation symmetric function .",
    "assume that @xmath224|\\big)<\\infty,\\ ] ] where @xmath11 and @xmath225 denote two independent copies of the process .",
    "then , as @xmath106 , @xmath226 where @xmath227)$ ] .",
    "first , we show that @xmath228 is continuous .",
    "let @xmath229 such that @xmath230 .",
    "then , due to the continuity assumptions on the process and the kernel , @xmath231\\to k[x(t),x'(t)]$ ] , a.s . using the assumption @xmath232|\\big)<\\infty$ ] ,",
    "dominated convergence theorem ( dct ) allows us to deduce @xmath233 @xmath234 .",
    "let @xmath235 where , for the sake of simplicity , we denote @xmath236 $ ] .",
    "the next step is to prove that , as @xmath237 , @xmath238 both @xmath239 and @xmath240 are continuous functions . since @xmath241 is uniformly continuous on @xmath242",
    ", @xmath239 is also continuous .",
    "the fact that @xmath243 is continuous follows directly from dct since @xmath244 and , by assumption , @xmath245 . by continuity , @xmath246 and @xmath247 , as @xmath248 .",
    "now , since @xmath249 implies @xmath250 , for all @xmath251 , we can apply dini s theorem to deduce that @xmath252 converges uniformly to 0 , that is , @xmath253 , as @xmath248 .",
    "the last step is to show @xmath254 a.s .",
    ", as @xmath106",
    ". for @xmath255 , denote @xmath256 , where @xmath257 $ ] , and @xmath258 .",
    "fix @xmath259 . by ( [ eq.sup ] )",
    ", there exists @xmath260 such that @xmath261 , for all @xmath251 .",
    "now , since @xmath262 is compact , there exist @xmath263 in @xmath262 such that @xmath264 , where @xmath265",
    ". then , @xmath266\\\\      & \\leq      \\max_{1\\leq k \\leq m } \\sup_{t\\in b_k}|u_n(t ) - u_n(t_k)| + \\max_{k=1,\\ldots , m}|u_n(t_k ) - u(t_k)| + \\epsilon,\\\\      \\end{aligned}\\ ] ] since @xmath267 implies @xmath268 | \\leq \\mathbb{e}|h(s)-h(t)|\\leq \\lambda_\\delta(t ) < \\epsilon.$ ]    for the second term , we have @xmath269 a.s . , as @xmath106 ,",
    "applying slln for u - statistics ( see e.g. dasgupta ( 2008 ) , theorem 15.3(b ) , p. 230 ) .",
    "as for the first term , observe that using again slln for u - statistics , @xmath270 where @xmath271 . therefore , @xmath272    [ lemma : twosampleustatistics ] let @xmath273 and @xmath274 be a pair of independent processes with continuous trajectories a.s . defined on the compact rectangle @xmath221 $ ] @xmath275 .",
    "let @xmath88 and @xmath89 be samples of @xmath276 and @xmath277 independent trajectories of @xmath278 and @xmath279 , respectively . define the functional two - sample u - statistic @xmath280,\\ ] ] where the kernel @xmath32 is a continuous , permutation symmetric function .",
    "assume that @xmath281 with @xmath282 $ ] .",
    "then , as @xmath283 , @xmath284 where @xmath285)$ ] .",
    "it is analogous to the proof of lemma [ lemma : ustatistics ] so it is omitted . we need to apply a strong law of large numbers for two - sample u - statistics .",
    "this result can be guaranteed under slightly stronger conditions on the moments of the kernel ; see ( * ? ? ?",
    "hence the condition @xmath286 in the statement of the lemma .",
    "\\(a ) from ( 2.1 ) , as @xmath31 is @xmath10-dimensional and @xmath15 is one - dimensional , taking into account @xmath287 , we have @xmath288 let s analyze the integrand , @xmath289}}-{\\ensuremath{\\mathbb{e } \\left [ { e^{iu^\\top x_t } } \\right]}}{\\ensuremath{\\mathbb{e } \\left [ { e^{ivy } } \\right]}}\\\\          & = { \\ensuremath{\\mathbb{e } \\left [ { ( e^{iu^\\top x_t}-\\varphi_{x_t}(u))(e^{ivy}-\\varphi_{y}(v ) ) } \\right ] } }          \\\\&={\\ensuremath{\\mathbb{e } \\left [ { { \\ensuremath{\\mathbb{e } \\left [ { ( e^{iu^\\top x_t}-\\varphi_{x_t}(u))(e^{ivy}-\\varphi_{y}(v))| x } \\right ] } } } \\right]}}\\\\          & = { \\ensuremath{\\mathbb{e } \\left [ { ( e^{iu^\\top x_t}-\\varphi_{x_t}(u)){\\ensuremath{\\mathbb{e } \\left [ { ( e^{ivy}-\\varphi_{y}(v))| x } \\right ] } } } \\right ] } }          \\\\&\\overset{(*)}{=}{\\ensuremath{\\mathbb{e } \\left [ { ( e^{iu^\\top x_t}-\\varphi_{x_t}(u))(e^{iv}-1)(\\eta(x)-p ) } \\right]}}\\\\          & = ( e^{iv}-1){\\ensuremath{\\mathbb{e } \\left [ { ( e^{iu^\\top x_t}-\\varphi_{x_t}(u))(\\eta(x)-p ) } \\right ] } }          \\\\&=(e^{iv}-1){\\ensuremath{\\mathbb{e } \\left [ { e^{iu^\\top x_t}(\\eta(x)-p ) } \\right ] } } = ( e^{iv}-1)\\zeta(u , t ) .",
    "\\end{aligned}\\ ] ] step ( * ) in the above chain of equalities is motivated as follows : @xmath290 } } & = { \\ensuremath{\\mathbb{e } \\left [ { e^{ivy}| x } \\right]}}-\\varphi_{y}(v )      = ( e^{iv}-1)\\eta(x )   - ( e^{iv}-1)p \\\\ & = ( e^{iv}-1 ) ( ( \\eta(x)-p ) ) .",
    "\\end{aligned}\\ ] ] therefore , since @xmath291 , @xmath292      @xmath293 { \\mathbb{e}}\\left [ ( \\eta(x')-p ) e^{-iu^\\top x'_t } \\right ] \\\\          & = { \\mathbb{e}}\\left [ ( \\eta(x)-p)(\\eta(x')-p ) e^{iu^\\top(x_t - x'_t ) } \\right]\\\\            & = { \\mathbb{e}}\\left [ ( \\eta(x)-p)(\\eta(x')-p ) \\cos(u^\\top ( x_t - x'_t ) ) \\right ] \\\\          & = - { \\mathbb{e}}\\left [ ( \\eta(x)-p)(\\eta(x')-p)(1- \\cos(u^\\top(x_t - x'_t ) ) ) \\right ] ,          \\end{aligned}\\ ] ]      @xmath296 \\\\          & = -2 { \\mathbb{e}}\\left [ ( \\eta(x)-p)(\\eta(x')-p){\\ensuremath{|{x_t - x'_t}|}}_d \\right]\\\\          & = -2 { \\mathbb{e}}\\left [ ( y - p)(y'-p){\\ensuremath{|{x_t - x'_t}|}}_d \\right ] ,          \\end{aligned}\\ ] ]      \\(c ) by conditioning on @xmath15 and @xmath298 we have @xmath299 & =          p^2 i_{00}(t ) ( 1-p)^2 - p(1-p)i_{01}(t ) 2p(1-p)\\\\          \\hspace{0.5cm}&\\hspace{10pt}+(1-p)^2i_{11}(t)p^2          = p^2(1-p)^2 ( i_{00}(t)+i_{11}(t ) - 2i_{01}(t ) ) .",
    "\\end{aligned}\\ ] ] now , using ( 3.2 ) , @xmath300 $ ] .",
    "continuity of @xmath97 is straightforward from dct .",
    "it suffices to prove the result for sequences of samples @xmath301 , and @xmath302 , drawn from @xmath186 and @xmath187 , respectively , such that @xmath303 .    from the triangle inequality",
    "it is enough to prove the uniform convergence of @xmath304 , @xmath305 and @xmath306 to @xmath307 , @xmath308 and @xmath309 , respectively .",
    "for the first two quantities we apply lemma [ lemma : ustatistics ] to the kernel @xmath310 .",
    "for the last one we apply lemma [ lemma : twosampleustatistics ] to the same kernel .",
    "observe that @xmath311 implies the moment condition of lemma [ lemma : ustatistics ] whereas @xmath100 implies the moment condition of lemma [ lemma : twosampleustatistics ] .",
    "the last statement readily follows from the uniform convergence and the compactness of @xmath26^d$ ] .",
    "we know @xmath22 .",
    "then , we use equation ( 4.1 ) , which provides @xmath312 in terms of the radon - nikodym derivative @xmath313 , and the expression for @xmath313 given in @xcite , p. 239 .",
    "this gives @xmath314^{-1}.\\ ] ]      again , we use expression ( 4.1 ) to derive the expression of the optimal rule @xmath22 . in this case",
    "the calculation is made possible using the expression of the radon - nikodym derivative for the distribution of a brownian process with trend , @xmath316 , with respect to that of a standard brownian : @xmath317 for @xmath116-almost all @xmath318 $ ] ; see , @xcite , th . 1.38 and remark 1.43 , for further details .",
    "observe that in this case we have @xmath319 .",
    "thus , from ( 4.1 ) , we finally get @xmath320 ^{-1 } ,      $ ] which again only depends on @xmath321 through @xmath322 .",
    "the result follows easily from this expression .",
    "let us first consider the model in proposition 1 ( i.e. , brownian vs. brownian with a stochastic trend ) .",
    "such model entails that @xmath326 and @xmath327 .",
    "now , recall that if @xmath328 , then , @xmath329 where @xmath330 denotes the distribution function of the standard normal .",
    "let us now consider the model in proposition 2 ( i.e. , brownian vs. brownian with a linear trend ) .",
    "again , from we have in this case , @xmath337 @xmath338 where @xmath68 and @xmath334 are iid standard gaussian variables . therefore using ( 3.3 ) , @xmath339,\\ ] ] where @xmath340 .",
    "we can check numerically that this an increasing function which reaches its only maximum at @xmath336 . according to proposition 1",
    "this is the only relevant point for the bayes rule .",
    "ballo , a. , cuevas , a. and fraiman , r. ( 2011 ) classification methods with functional data . in _",
    "oxford handbook of functional data analysis , pp-259297 .",
    "f. ferraty and y. romain , eds .",
    "oxford university press , oxford .",
    "_    berrendero , j.r . ,",
    "cuevas , a. and torrecilla , j.l .",
    "the mrmr variable selection method : a comparative study for functional data . to appear in",
    "_ j. stat .",
    "sim .. doi : 10.1080/00949655.2015.1042378 _      comminges , l. dalalyan , a. s. ( 2012 ) .",
    "tight conditions for consistency of variable selection in the context of high dimensionality .",
    "* 40 , 26672696 .",
    "cuevas , a. ( 2014 ) . a partial overview of the theory of statistics with functional data .",
    "_ j. statist .",
    "inference * 147 , 123 . * _ * _                                                      peng , h. , long , f. and ding , c. ( 2005 ) .",
    "feature selection based on mutual information : criteria of max - dependency , max - relevance , and min - redundancy .",
    "_ ieee trans .",
    "pattern anal .",
    "_ * 27 * , 12261238 ."
  ],
  "abstract_text": [
    "<S> variable selection is considered in the setting of supervised binary classification with functional data @xmath0\\}$ ] . by `` variable selection '' </S>",
    "<S> we mean any dimension - reduction method which leads to replace the whole trajectory @xmath0\\}$ ] , with a low - dimensional vector @xmath1 still keeping a similar classification error . </S>",
    "<S> our proposal for variable selection is based on the idea of selecting the local maxima @xmath2 of the function @xmath3 , where @xmath4 denotes the `` distance covariance '' association measure for random variables due to @xcite . </S>",
    "<S> this method provides a simple natural way to deal with the relevance vs. redundancy trade - off which typically appears in variable selection . </S>",
    "<S> this paper includes    \\(a ) some theoretical motivation : a result of consistent estimation for the maxima of @xmath5 is shown . </S>",
    "<S> we also show different models for the underlying process @xmath6 under which the relevant information is concentrated on the maxima of @xmath5 .    </S>",
    "<S> \\(b ) an extensive empirical study , including about 400 simulated models and real data examples , aimed at comparing our variable selection method with other standard proposals for dimension reduction .    </S>",
    "<S> * variable selection in functional data classification : a maxima hunting proposal *    jos r. berrendero , antonio cuevas , jos l. torrecilla + departamento de matemticas + universidad autnoma de madrid , spain    [ [ keywords ] ] keywords : + + + + + + + + +    distance correlation , functional data analysis , supervised classification , variable selection . </S>"
  ]
}