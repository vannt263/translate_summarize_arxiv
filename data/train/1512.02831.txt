{
  "article_text": [
    "nearest neighbor search is a fundamental problem and an ingredient of many state - of - the - art data analysis techniques . while being a conceptually very simple task , the induced computations can quickly become a major bottleneck in the overall workflow when both a large reference and a large query set are given . in the literature",
    ", many techniques have been proposed that aim at accelerating the search .",
    "typical are include the use of spatial search structures , approximation schemes , and parallel implementations  @xcite .",
    "a recent trend in the field of big data analytics is the application of massively - parallel devices such as _ graphics processing units _ ( gpus ) to speed up the involved computations . while such modern many - core devices can significantly reduce the practical runtime , obtaining speed - ups over standard cpu - based execution is often not straightforward and usually requires a careful adaptation of the sequential implementations .",
    "spatial search structures such as  treesare an established way to reduce the computational requirements induced by nearest neighbor search for spaces of moderate dimensionality ( e.g. , up to @xmath0 ) .",
    "a typical parallel  treebased search assigns one thread to each query and all threads process the same tree _",
    "simultaneously_. such an approach , however , is not suited for gpus  since each thread might induce a completely different tree traversal , which results in massive branch divergence and irregular accesses to the device s memory .",
    "recently , we have proposed a modification of the classical  treedata structure , called _  tree _ , which aims at combining the benefits of both spatial search structures and massively - parallel devices  @xcite .",
    "the key idea is to assign an additional buffer to each leaf of the tree and to _ delay _ the processing of the queries reaching a leaf until enough work has been gathered . in that case",
    ", all queries stored in all buffers are processed together in a brute - force manner via the many - core device .",
    "while the framework achieves significant speed - ups on modern many - core devices over both a massively - parallel brute - force execution on gpus  as well as over a multi - threaded  treebased search running on multi - core systems , it is limited by the amount of reference and query points that fit on a gpu . in this work ,",
    "we show how to remove this limitation by modifying the induced workflow to efficiently support huge reference and query point sets that are too large to be completely stored on the devices .",
    "this crucial modification renders  treescapable of dealing with huge data sets .",
    "for the sake of completeness , we provide the background related to massively - parallel programming on gpus  as well as to classical  tree - based nearest neighbor search .",
    "we further sketch the key ideas of the  treeextension .",
    "modern many - core devices such as gpus  offer massive parallelism and can nowadays also be used for so - called _ general - purpose computations _ such as matrix - matrix multiplication .",
    "in contrast to standard cpu - based systems , gpus  rely on simplified control units and on a memory subsystem that does not attempt to provide the illusion of a uniform access cost to memory .",
    "gpu  architectures are typically formed from a number of vector processors , several special function units and an amount of fast memory that is split between registers , l1 and l2 data caches , scratchpad and read - only memory .",
    "each vector processor consists of multiple execution units that execute in lock step , i.e. , in a single - instruction multiple data ( simd ) fashion , and each execution unit is further multi - threaded in order to hide memory latency .",
    "the gpu programming model typically reflects this hardware organization : for example , expressing a parallel computation in ` opencl `  @xcite requires the user to write a _ kernel _ that will be run simultaneously by many threads .",
    "threads are grouped into _ workgroups _ , which are run on the same vector processor , called _ streaming multiprocessor _",
    "programmers need to explicitly declare in what memory the data is stored .",
    "further , they can use fast synchronization and communication within a workgroup via _ local _ memory . for ` nvidia `  gpus , groups of @xmath1 threads execute in a simd fashion ; such a group is called _",
    "warp_.    the main ingredients for an efficient many - core implementation are ( 1 ) exposing sufficient parallelism to fully utilize the device and ( 2 ) accessing the memory in an efficient way .",
    "the latter one includes techniques that    1 .",
    "hide the latency of the memory transfer between host and gpu , for example by overlapping the kernel execution with the memory transfer , and 2 .",
    "restructure the program in order to improve both spatial and temporal locality of reference to the global memory of the gpu .",
    ", then @xmath1 sequential memory transaction are needed ) .",
    "_ temporal locality _ corresponds to the case in which most of the threads in the same workgroup access the same memory location in the same instruction . in this case , the first warp accessing it brings the corresponding data block to the l1 cache within one memory transfer ( which is then used by the subsequent warps ) . ]      we address the problem of computing the @xmath2 nearest neighbors of all points given in a query set @xmath3 w.r.t . to all points provided in a reference set @xmath4 .",
    "usually , the `` closeness '' between two points is defined via the euclidean distance ( which we will use ) , but other distance measures can be applied as well .",
    "such nearest neighbor computations form the basis for a variety of methods both in data mining and machine learning including proximity - based outlier detection , classification , regression , density estimation , and dimensionality reduction , see , e.g. , hastie  _ et al . _",
    "the task of computing the induced distances ( and keeping track of the list of neighbors per object ) can be addressed naively in a brute - force manner spending @xmath5 time , which quickly becomes computationally very demanding .",
    "massively - parallel computing can significantly reduce the runtime in this case as shown by garcia  _ et al . _",
    "still , for large query and reference sets , the computational requirements can become very large .",
    "various other approaches have been proposed in the literature that aim at taking advantage of the computational resources provided by gpus  in combination with other techniques  @xcite .",
    "the focus of this work is on massively - parallel processing of  trees .",
    "while several implementations have been proposed that address such traversals from a more general perspective ( e.g. , in the context of _ ray tracing _ )  @xcite , these approaches are not suited for nearest neighbor search in moderate - sized feature spaces ( i.e. , @xmath6 ) , except for the recently proposed  tree  extension  @xcite .",
    "spatial search structures such as  treescan be used to speed up nearest neighbor search .",
    "treescan be constructed as follows  @xcite : for a given point set @xmath7 , a  treeis a binary tree with the root corresponding to @xmath7 .",
    "the children of the root are obtained by splitting the point set into two ( almost ) equal - sized subsets , which are processed recursively . in their original form ,  treesare obtained by resorting to the median values in dimension @xmath8 to split a point set corresponding to a node @xmath9 at level @xmath10 ( starting with the root at level @xmath11 ) .",
    "the recursive process stops as soon as a predefined number of points are left in a subset .",
    "the  treestores the splitting values in its internal nodes ; the points corresponding to the remaining sets are stored in the leaves .    the tree structure can be used to accelerate nearest neighbor search : let @xmath12 be a query point",
    ". for the sake of exposition , we focus on @xmath13 ( the case of @xmath14 neighbors works similarly ) .",
    "the overall search takes place in two phases . in the first one ,",
    "the tree is traversed from top to bottom to find the @xmath15-dimensional cell ( induced by the tree / splitting process ) that contains the query point  @xmath16 .",
    "going down the tree can be conducted efficiently using the median values stored in the internal nodes of the  tree . in the second phase ,",
    "the tree is traversed bottom - up , and on the way back to the root , subtrees are checked in case the query point is close to the corresponding splitting hyperplane . if the distance of the query point @xmath16 to the hyperplane is less than the distance to the current nearest neighbor candidate , then the subtree is checked for better candidates ( recursively ) .",
    "otherwise , the whole subtree can be safely pruned ( no recursion ) . once the root is reached twice , the overall process stops and the final nearest neighbor candidate is returned .",
    "given a low - dimensional search space , it is usually sufficient to process a relatively small number of leaves , which results in a logarithmic runtime behavior ( i.e. , @xmath17 time per query in practice ) .",
    "however , the performance usually decreases for increasing @xmath15 due to the curse of dimensionality . in the worst case , all nodes and leaves of the  treeneed to be processed , which yields a linear query time ( i.e. , @xmath18 time per query ) .",
    "r0.42    a standard multi - threaded  tree - based traversal assigns one thread to each query . for gpus ,",
    "such an approach is not suited since each thread might induce a completely different path , which significantly shortens their computational benefits .",
    "the main idea of the  treeextension is to delay the processing of the queries by buffering similar patterns prior to their common processing  @xcite .",
    "the reorganized workflow is based on  trees , which are sketched next , followed by a description of the buffered nearest neighbor search .    a  treeconsists of ( 1 ) a top tree , ( 2 ) a leaf structure , ( 3 ) a set of buffers , and ( 4 ) two input queues that store the queries , see figure  [ fig : buffer_kdtree ] .",
    "the top tree corresponds to a classical  treewith its splitting values ( e.g. , medians ) laid out in memory in a pointer - less manner .",
    "the leaf structure stores the point sets that stem from the splitting process in a consecutive manner .",
    "in addition , a buffer is attached to each leaf of the top tree that can store @xmath19 query indices ( e.g. , @xmath20 ) .",
    "the input queues are used to store the input query indices and the query indices that need further processing after a ` processallbuffers ` call .",
    "a chunk @xmath3 of query points .",
    "the @xmath2 nearest neighbors for each query point .",
    "construct buffer  tree@xmath21 for @xmath4 .",
    "initialize queue ` input ` with all @xmath22 query indices .",
    "fetch @xmath23 indices @xmath24 from ` reinsert ` and ` input ` .",
    "@xmath25 = findleafbatch(@xmath24 ) insert index @xmath26 in buffer associated with leaf @xmath27 .",
    "@xmath28 = processallbuffers ( ) insert @xmath28 into ` reinsert ` .",
    "* return * list of @xmath29 nearest neighbors for each query point .",
    "a  tree  can be used to delay the processing of the queries by performing several iterations , see algorithm  [ alg : lazy_search ] : in each iteration , the procedure findleafbatch retrieves indices from both the ` input ` and ` reinsert ` queue and propagates them through the top tree .",
    "the indices , which are stored in the corresponding buffers , are processed in chunks via the procedure ` processallbuffers ` once the buffers get full .",
    "all indices that need further processing ( i.e. , their implicit tree traversal has not reached the root twice ) are inserted into ` reinsert ` again .",
    "thus , in each iteration , one ( 1 ) finds the leaves that need to be processed next and ( 2 ) updates the queries nearest neighbors .",
    "while the first phase is not well - suited for massively - parallel processing , the second one is and , since it constitutes the most significant part of the runtime , yields valuable overall speed - ups .",
    "the main advantage of the reorganized workflow is that all queries are processed in the same block - wide simd instruction and exhibit either good spatial or temporal locality of reference , i.e. , coalesced or cached global memory accesses ( see section  [ subsec : gpuarch ] ) . for details of the particular many - core implementation of the procedure processallbuffers ,",
    "we refer to gieseke  _ et al . _",
    "note that both the top tree and the leaf structure need to be stored on the gpu  given the original implementation  @xcite  this limits the amount of reference patterns that can be processed .",
    "one issue not addressed so far is the fact that the memory of modern gpus  is still relatively small compared to host memory .",
    "this limits the amount of data points that can be processed .",
    "we now describe modifications that allow the  trees  to scale to massive data sets not fitting on a gpu  anymore .",
    "as shown below , one can basically process arbitrarily large query sets by considering chunks of data points . dealing with huge reference sets , however ,",
    "is more difficult : since the top tree and the full leaf structure ( that stores the rearranged reference points ) have to be made available to all threads during the execution of processallbuffers , one can not directly split up the leaf structure .",
    "however , as explained next , one can avoid storing the leaf structure in its full entirety on the many - core device without significantly increasing the overall runtime .",
    "r0.5    we start by focusing on the space needed for the top tree : from a practical perspective , a small top tree is usually advantageous compared to the full tree used by a classical  tree - based search , since the efficiency gain on gpus  stems from processing `` big '' leaves .",
    "for instance , given a reference set @xmath30 with two million points , top trees of height @xmath31 or are usually optimal  @xcite .",
    "further , since only median values are stored in the top tree , the space consumption is negligible even given much bigger top trees . , suitable for more than one billion points , less than ten megabytes are needed .",
    "note that the space for the buffers ( e.g. , of size 128 each ) stored on the host does usually not cause any problems ( e.g. , less then a gigabyte ) . ] in addition , the top tree can be built efficiently via linear - time median finding  @xcite , which results in @xmath32 time for the whole construction phase .",
    "hence , one can  ( 1 ) build the top tree efficiently on the host system and ( 2 ) store it in its full entirety on the gpu .",
    "the main space bottleneck stems from the leaf structure .",
    "for instance , one billion points in @xmath33 occupy about 60 gigabytes of space  too much for a modern many - core device . for this reason ,",
    "we do _ not _ copy the leaf structure from the host to the device after the construction of the top tree .",
    "instead , we allocate space for two chunk buffers of fixed size on the many - core device . these chunks will be used to overlap the execution of the processallbuffers procedure with the host - to - device memory transfer for the next chunk .",
    "note that we also allocate two associated memory buffers on the host ( _ pinned memory _",
    "@xcite ) to achieve efficient concurrent compute and copy operations .",
    "the overall memory layout is shown in figure  [ fig : memory_layout ] , where the memory that is actually allocated on the device is sketched via gray rectangles .",
    "we now describe the details of the modified querying process .",
    "the idea is to keep the leaf structure on the host system and to process the buffers via chunks with concurrent compute and copy operations .      in each iteration of algorithm",
    "[ alg : lazy_search ] , the procedure processallbuffers is invoked to retrieve all query indices from the buffers attached to the leaves .",
    "the queries are processed in a massively - parallel fashion , where each thread compares a particular query with all reference points stored in the associated leaf .",
    "given the modified memory layout , one can now process the leaves in chunks in the following way : the leaf structure containing all  @xmath34 rearranged reference points is split into @xmath35 chunks @xmath36 ( e.g. , @xmath37 ) .",
    "each chunk  @xmath38 contains the points of the leaf structure at positions @xmath39 , where @xmath40 and @xmath41 .",
    "a buffer attached to the top tree corresponds to a leaf in the leaf structure with leaf bounds @xmath42 .",
    "all queries removed from the buffers are then processed in @xmath43 iterations and a query @xmath10 with leaf bounds @xmath44 and @xmath45 is processed in iteration @xmath46 if @xmath47 \\cap [ c_j^l , c_j^r ] \\neq \\emptyset$ ] , i.e. , if the leaf bounds overlap with chunk @xmath38 .",
    "the chunks are processed sequentially @xmath48 .",
    "the data needed for each chunk is copied from the host to one of the two chunk buffers allocated on the device prior to conducting the brute - force computations , see figure  [ fig : memory_layout ] . to hide the overhead induced for these copy operations ,",
    "the copy process for the next chunk is started as soon as the computations for its predecessor have been invoked .",
    "in particular , the processing of a chunk @xmath38 takes place in three phases :    1 .   _ brute : _ first , the massively - parallel brute - force nearest neighbor computations are invoked ( non - blocking kernel call ) .",
    "the data needed for these computations ( chunk @xmath38 ) have been copied in round @xmath49 ( for @xmath50 , the data is either available from an initial copy operation or from the previous round ) .",
    "copy : _ while the brute - force computations are conducted by the gpu , the data for the next chunk are copied from host to the buffer on the device that is currently not in use . , the data for chunk 0 are copied from host to the corresponding buffer on the device for next round ( i.e. , next call of processallbuffers ) . ]",
    "note that copy operations on the host system have to be conducted as well to ensure that the correct part of the leaf structure is moved to the appropriate pinned memory buffer ( which is then copied to the device ) , see again figure  [ fig : memory_layout ] .",
    "wait : _ in the final phase , one simply waits for the kernel invoked in the first phase to finish its computations ( blocking call ) .",
    "the iterative compute - and - copy processing of the chunks can be implemented via two ( ` opencl ` ) command queues  @xcite : for the first chunk , phase ( 1 ) and ( 3 ) are instantiated via command queue a , whereas the copy process ( 2 ) is instantiated via command queue b ( non - blocking for both ( 1 ) and ( 2 ) ) . for the second chunk ,",
    "phases ( 1 ) and ( 3 ) are instantiated via command queue b and ( 2 ) via command queue a. this process continues until all chunks have been processed .",
    "in essence , the use of two command queues allows the copy phase ( 2 ) to run in parallel with the brute - force computation phases ( 1 ) and ( 3 ) .",
    "given the new workflow , one can basically handle an arbitrary amount of reference patterns .",
    "the only restriction is the memory available on the host . in case",
    "not enough main memory is available , one can store the leaf structure on disk and copy the chunks from disk to device memory ( via host memory ) .",
    "assuming that a fixed amount of memory is available on the many - core device to store the query patterns and the results at all times ( e.g. , one gigabyte ) , one can process an arbitrarily large query set by removing fully processed indices and by adding new indices on - the - fly , see algorithm  [ alg : lazy_search ] .",
    "an even simpler approach is to split up the queries into chunks and to handle these chunks independently .",
    "one drawback of the latter approach could be the overhead induced by applying the procedure ` processallbuffers ` in case the buffers are not sufficiently filled ( which usually takes place as soon as no queries are available anymore ) .",
    "however , given relatively large chunks , the induced overhead is very small as shown in our experimental evaluation .    in a similar fashion",
    ", one can make use of multiple many - core devices by splitting all queries into `` big '' chunks according to the devices that are available .",
    "these chunks , which might have to be split into smaller chunks as described above , can be processed independently from each other .",
    "the purpose of the experiments provided below is to analyze the efficiency of the modified workflow and to sketch the potential of the overall approach in the context of large - scale scenarios . for a detailed experimental comparison including an analysis of the different processing phases and the influence of parameters related to the  tree  framework",
    ", we refer to our previous work  @xcite .",
    "all runtime experiments were conducted on a standard desktop computer with an ` intel(r ) core(tm ) i7 - 4790k ` cpu  running at 4.00ghz ( 4 cores ; 8 hardware threads ) , 32 gb  ram , and two ` nvidia geforce titan z `  gpus ( each consisting of two devices with 2880 shader units and 6  gb main memory ) .",
    "the operating system was ` ubuntu 14.4.3 lts ` ( 64 bit ) with kernel ` 3.13.0 - 52 ` , ` cuda 7.0.65 ` ( graphics driver 340.76 ) , and ` opencl 1.2 ` .",
    "all algorithms were implemented in ` c ` and ` opencl ` , where ` swig ` was used to obtain appropriate ` python ` interfaces .",
    "the code was compiled using ` gcc-4.8.4 ` at optimization level ` -o3 ` .    for the experimental evaluation ,",
    "we report runtimes for both the construction and the query phase ( referred to as `` train '' and `` test '' phases ) , where the focus is on the latter one ( that makes use of the gpus ) .",
    "we consider the following three implementations :    1 .   `",
    "bufferkdtree(i ) ` : the adapted  tree  implementation with both ` findleafbatch ` and ` processallbuffers ` being conducted on @xmath10 gpus .",
    "kdtree(i ) ` : a multi - core implementation of a  tree - based search , which runs @xmath10 threads in parallel on the cpu  ( each handling a single query ) .",
    "brute(i ) ` : a brute - force implementation that makes use of @xmath10  gpus  to process the queries in a massively - parallel manner .",
    "the parameters for the  treeimplementation were fixed to appropriate values . , we fixed @xmath51 and the number @xmath23 of indices fetched from ` input ` and ` reinsert ` in each iteration of algorithm  [ alg : lazy_search ] to @xmath52 .",
    "note that the particular assignments for these and other parameters did not have a significant influence on the performance as long as they were set to reasonable values . ]",
    "note that both competitors of ` bufferkdtree ` have been evaluated extensively in the literature ; the reported runtimes and speed - ups can thus be put in a broad context . for simplicity",
    ", we fix the number @xmath29 of nearest neighbors to @xmath53 for all experiments .",
    "we focus on several data - intensive tasks from the field of astronomy .",
    "note that a similar runtime behavior can be observed on data sets from other domains as well as long as the dimensionality of the search space is moderate ( e.g. , from @xmath54 to @xmath0 ) .",
    "we follow our previous work and consider the ` psf_mag ` , ` psd_model_mag ` , and ` all_mag ` data sets of dimensionality @xmath54 , @xmath55 , and @xmath56 , respectively ; for a description , we refer to gieseke  _ et al .",
    "_  @xcite .",
    "in addition , we consider a new dataset derived from the _ catalina realtime transient survey _ ( ` crts ` )  @xcite .",
    "this survey contains tens to hundreds of observations for more than 500 million sources over a large part of the sky .",
    "the resulting light - curves ( time - series of light received as a function of time ) are used to derive several statistical features . , @xmath57 , @xmath58 , @xmath59 , @xmath60 , @xmath61 , @xmath62 , @xmath63 , @xmath64 , @xmath65  @xcite .",
    "] we use ten such features on a set of 30 million light - curves for the experiments described here .",
    "the main interest is to find outliers in the large space that can lead to interesting discoveries .",
    "while the modified workflow permits the use of  treesfor massive data sets that do not fit in the memories of the many - core devices , it might also induce a certain overhead compared to its original version due to the induced copy operations and reduced workload per kernel call .",
    "in addition , the `` naive '' use of multiple gpus  might exhibit a worse performance compared to filling the ` input ` queue with new queries on - the - fly .",
    "we now investigate both potential drawbacks .",
    "the main modification is the different processing of the leaf structure in case it does not fit in the device s memory . to evaluate the potential overhead caused by the additional copy operations ( between host and device ) during the execution of processallbuffers , we consider data set instances that still fit in memory and compare the runtimes of  ( 1 ) the original workflow with ( 2 ) the workflow that is based on multiple chunks .    in figure  [ exps : leaf_chunks ] , the outcome of this comparison is shown for a varying number  @xmath43 of chunks and a varying number @xmath34 of training points . in all three cases , the number of test patterns is fixed to @xmath66",
    "further , the tree height is set to  @xmath67 and a single gpu  is used ( i.e. , ` bufferkdtree(1 ) ` ) .",
    "two observations can be made : first , the training time is very small compared to the test time for all cases ( even though the tree is constructed sequentially on the host ) .",
    "second , the performance loss induced by the chunked processing is very small for almost any number @xmath43 of chunks ( i.e. , the ratio shown as black , thick line is close to 1 ) . in particular",
    ", this is the case for smaller values of  @xmath43 ; using more chunks naturally yields more overhead , which , however , decreases again if one increases the number of training and test patterns .",
    "thus , the runtimes of the new , chunked workflow are close to the one of the original approach  indicating that the overlapping compute - and - copy process successfully hides the additional overhead for the copy operations .",
    "as outlined above , one can simply distribute the test queries to multiple devices to take advantage of the additional computational resources .",
    "again , this can lead to a certain overhead , since invoking the procedure processallbuffers becomes less efficient at the end of the overall processing ( and this happens earlier in case the test queries are split into chunks ) . similarly to the experiment provided above , we compare the efficiency of ` bufferkdtree(4 ) ` with the one of ` bufferkdtree(1 ) ` , a standard single - device processing with all test patterns fitting on the gpu .",
    "for this sake , we consider @xmath68 leaf chunks ( i.e. , no modified processing of the leaves ) , @xmath69 training patterns , and vary the number @xmath22 of test patterns .",
    "the outcome of this experiment for three different data sets is shown in figure  [ exps : multi_many_core ] : it can be seen that a suboptimal speed - up of about 2 is achieved in case a relatively small amount of test patterns is processed .",
    "however , as soon as the number of test patterns increases , the speed - up gets closer to 4 , which depicts the maximum that can be achieved .",
    "hence , the naive way of using all devices in a given workstation does not yield significant drawbacks as soon as large - scale scenarios are considered , which is the scope of this work .      to demonstrate the potential of the modified framework , we consider two large - scale tasks : ( 1 ) the application of nearest neighbor models that are based on very large training sets and ( 2 ) large - scale density - based outlier detection .",
    "the first scenario addresses nearest neighbor models  @xcite that are based on very large training sets .",
    "such models have been successfully been applied for various tasks in astronomy including the detection of distant galaxies or the estimation of physical parameters  @xcite .    for the experimental comparison",
    "we consider scenarios with both a large amount of training and test patterns .",
    "more precisely , we consider up to @xmath70 training points and up to @xmath71 test points given the ` psd_model_mag ` data set .",
    "for both tree - based methods , appropriate tree depths are set beforehand ( i.e. , optimal ones w.r.t .",
    "the runtime needed in the test phase ) .",
    "the outcome of this comparison is shown in figure  [ exps : huge_nearest ] : it can be seen that valuable speed - ups can be achieved over both competitors .",
    "further , the speed - ups generally become more significant the more patterns are processed .",
    "note that for figures  ( e ) and  ( f ) , the ` bufferkdtree ` implementation automatically considers @xmath72 chunks ( due to the training patterns exceeding the space reserved for them on the device ) , which results in a slightly worse performance for @xmath73 .      as final use case",
    ", we consider large - scale proximity - based outlier detection .",
    "various outlier scores have been proposed that are based on the computation of nearest neighbors .",
    "a typical one is to rank the points according to their average distance accordiing to their @xmath29 nearest neighbors , see , e.g. , tan  _ et al .",
    "_  @xcite .",
    "such techniques depict very promising tools in case many reference points are given in a moderate - sized feature space , which is precisely the case for many tasks in astronomy .",
    "typically , these scores require the computation of the nearest neighbors for each of the reference points ( known as _ all nearest neighbors problem _ ) .",
    "naturally , this can quickly become very time - consuming .",
    "r0.5    to show the potential of our many - core implementation , we consider the ` crts ` data set described above ( with @xmath55 features ) and vary the number @xmath34 of reference points ( here , we have @xmath74 for the full data set ) .",
    "we again compare the performances of all three competitors , where we consider both the runtime for the construction and the one for the query phase .",
    "the outcome is shown in figure  [ fig : speed_ups_outliers ] .",
    "note that the runtimes for both ` kdtree(cpu,8 ) ` and ` brute(gpu,4 ) ` depict estimates based on a reduced query due to the computational complexity ( i.e. , up to @xmath75 reference points and a fixed query set of size @xmath76 are considered ; the runtime estimates w.r.t . to the full data set instances are plotted ) .",
    "for the ` bufferkdtree(gpu,4 ) ` implementation , we fix @xmath72 .",
    "it can be seen that the  treeimplementation yields valuable speed - ups and can successfully process the whole data set in a reasonable amount of time .",
    "we provide a modified workflow for processing huge amounts of nearest neighbor queries using  trees . the key idea is to process both the reference and the query points in chunks .",
    "while the latter is relatively easy to implement ( even given several many - core devices ) , processing the reference points in chunks is more difficult . as shown in our work , one can effectively hide the overhead induced by the chunked processing of the reference points by interleaving the compute and copy operations .",
    "the experiments conducted on commodity hardware demonstrate that a single workstation is enough to efficiently process millions of reference and query points .",
    "future work might address scenarios that are based on even larger reference sets ( e.g. , hundreds of billions of points ) , which could also necessitate the efficient construction of the  tree .",
    "in addition , similar ( chunked ) buffering techniques might be useful to achieve efficient massively - parallel implementations for other techniques as well .",
    "the authors would like to thank the _ radboud excellence initiative _ of the radboud university nijmegen and _ nvidia _ for its support and generous hardware donations ( fg ) , the _ danish industry foundation _ through the _ industrial data analysis service _ ( ci , co ) , the _ the danish council for independent research@xmath77natural sciences _ through the project _ surveying the sky using machine learning _ ( ci ) , and acp , iucaa , iusstf , and nsf ( am ) ."
  ],
  "abstract_text": [
    "<S> a  tree  is a  tree  variant for massively - parallel nearest neighbor search . while providing valuable speed - ups on modern many - core devices in case both a large number of reference and query points </S>",
    "<S> are given ,  treesare limited by the amount of points that can fit on a single device . in this work , </S>",
    "<S> we show how to modify the original data structure and the associated workflow to make the overall approach capable of dealing with massive data sets . </S>",
    "<S> we further provide a simple yet efficient way of using multiple devices given in a single workstation . </S>",
    "<S> the applicability of the modified framework is demonstrated in the context of astronomy , a field that is faced with huge amounts of data . </S>"
  ]
}