{
  "article_text": [
    "with modern rich datasets , statistical models with a large number of parameters are nowadays commonplace in many application areas .",
    "a typical example is a regression or classification problem with a large number of predictor variables . in such problems ,",
    "a careful formulation of the prior distribution  or regularization in the frequentist framework  plays a key role .",
    "often it is reasonable to assume that only some of the model parameters @xmath0 ( such as the regression coefficients ) are far from zero . in the frequentist literature ,",
    "these problems are typically handled by lasso @xcite or one of its close cousins , such as the elastic net @xcite .",
    "we focus on the probabilistic approach and carry out the full bayesian inference on the problem .",
    "two prior choices dominate the bayesian literature : two component discrete mixture priors known as the spike - and - slab @xcite , and a variety of continuous shrinkage priors ( see e.g. , * ? ? ?",
    "* and references therein ) .",
    "the spike - and - slab prior is intuitively appealing as it is equivalent to bayesian model averaging ( bma ) @xcite over the variable combinations , and often has good performance in practice .",
    "the disadvantages are that the results can be sensitive to prior choices ( slab width and prior inclusion probability ) and that the posterior inference can be computationally demanding with a large number of variables , due to the huge model space .",
    "the inference could be sped up by analytical approximations using either expectation propagation ( ep ) @xcite or variational inference ( vi ) @xcite , but this comes at the cost of a substantial increase in the amount of analytical work needed to derive the equations separately for each model and a more complex implementation .",
    "the continuous shrinkage priors on the other hand are easy to implement , provide convenient computation using generic sampling tools such as stan @xcite , and can provide as good or better results .",
    "a particularly interesting example is the horseshoe prior @xcite @xmath1 which has shown comparable performance to the spike - and - slab prior in a variety of examples where a sparsifying prior on the model parameters @xmath2 is desirable @xcite .",
    "the horseshoe is one of the so called global - local shrinkage priors , meaning that there is a global parameter @xmath3 that shrinks all the parameters towards zero , while the heavy - tailed half - cauchy priors allow some parameters @xmath2 to escape the shrinkage ( see section  [ sec : horseshoe ] for more thorough discussion ) .    so far there has been no consensus on how to carry out inference for the global hyperparameter @xmath3 which determines the overall sparsity in the parameter vector @xmath4 and therefore has a large impact on the results . for reasons discussed in section  [ sec : full_bayes_vs_point_estimation ] ,",
    "we prefer full bayesian inference . for many interesting datasets ,",
    "@xmath3 may not be well identified by data , and in such situations the hyperprior choice @xmath5 becomes crucial .",
    "this is the question we address in this paper .",
    "the novelty of the paper is summarized as follows .",
    "we derive analytically the relationship between @xmath3 and @xmath6  the effective number of nonzero components in @xmath4 ( to be defined later )  and show an easy and intuitive way of formulating the prior for @xmath3 based on our prior beliefs about the sparsity of @xmath4 .",
    "we focus on regression and classification , but the methodology is applicable also to other generalized linear models and to other shrinkage priors than the horseshoe .",
    "we argue that the previously proposed default priors are dubious based on the prior they impose on @xmath6 , and that they yield good results only when @xmath3 ( and therefore @xmath6 ) is strongly identified by the data .",
    "moreover , we show with several real world examples that in those cases where @xmath3 is only weakly identified by data , one can substantially improve inferences by transforming even a crude guess of the sparsity level into @xmath5 using our method .",
    "we first briefly review the key properties of the horseshoe prior in section  [ sec : horseshoe ] , and then proceed to discuss the prior choice @xmath5 in section  [ sec : global_parameter ] .",
    "the importance of the concept is illustrated in section  [ sec : experiments ] with real world data .",
    "section  [ sec : conclusion ] concludes the paper by giving some recommendations on the prior choice based on the theoretical considerations and numerical experiments .",
    "consider the single output linear gaussian regression model with several input variables , given by @xmath7 where @xmath8 is the @xmath9-dimensional vector of inputs , @xmath10 contains the corresponding weights and @xmath11 is the noise variance .",
    "the horseshoe prior is set for the regression coefficients @xmath12 @xmath13 if an intercept term @xmath14 is included in the model  , we give it a relatively flat prior , because there is usually no reason to shrink it towards zero . as discussed in the introduction , the horseshoe prior has been shown to possess several desirable theoretical properties and good performance in practice @xcite . the intuition is the following : the global parameter @xmath3 pulls all the weights globally towards zero , while the thick half - cauchy tails for the local scales @xmath15 allow some of the weights to escape the shrinkage .",
    "different levels of sparsity can be accommodated by changing the value of @xmath3 : with large @xmath3 all the variables have very diffuse priors with very little shrinkage towards zero , but letting @xmath16 will shrink all the weights @xmath17 to zero .",
    "the above can be formulated more formally as follows .",
    "let @xmath18 denote the @xmath19-by-@xmath9 matrix of observed inputs and @xmath20 the observed targets .",
    "the conditional posterior for the coefficients @xmath10 given the hyperparameters and data @xmath21 can be written as @xmath22 where @xmath23 and @xmath24 is the maximum likelihood solution ( assuming @xmath25 exists ) .",
    "if the predictors are uncorrelated with zero mean and unit variance , then @xmath26 , and we can approximate @xmath27 where @xmath28 is the _ shrinkage factor _ for coefficient @xmath17 . the shrinkage factor describes how much coefficient @xmath17 is shrunk towards zero from the maximum likelihood solution ( @xmath29 meaning complete shrinkage and @xmath30 no shrinkage ) . from   and",
    "it is easy to verify that @xmath31 as @xmath32 , and @xmath33 as @xmath34 .",
    "the result   holds for any prior that can be written as a scale mixture of gaussians like  , regardless of the prior for @xmath15 .",
    "the horseshoe employs independent half - cauchy priors for all @xmath15 , and for this choice one can show that , for fixed @xmath3 and @xmath35 , the shrinkage factor   follows the prior @xmath36 when @xmath37 , this reduces to @xmath38 which looks like a horseshoe , see figure  [ fig : kappa_prior ] .",
    "thus , a priori , we expect to see both relevant ( @xmath39 , no shrinkage ) and irrelevant ( @xmath40 , shrinkage ) variables . by changing the value of @xmath3 , the prior for @xmath41 places more mass either close to 0 or 1 .",
    "for instance , choosing @xmath3 so that @xmath42 favors complete shrinkage ( @xmath40 ) and thus we expect more variables to be shrunk a priori . in the next section",
    "we discuss an intuitive way of designing a prior distribution for @xmath3 based on assumptions about the number of nonzero components in @xmath10 .",
    "this section discusses the prior choice for the global hyperparameter @xmath3 .",
    "we begin with a short note on why we prefer full bayesian inference for @xmath3 over point estimation , and then go on to discuss how we propose to set up the prior @xmath5 .      in principle , one could use a plug - in estimate for @xmath3 , obtained either by cross - validation or maximum marginal likelihood ( sometimes referred to as `` empirical bayes '' ) .",
    "the maximum marginal likelihood estimate has the drawback that it is always in danger of collapsing to @xmath43 if the parameter vector happens to be very sparse .",
    "moreover , rather than being computationally convenient , this approach might actually complicate matters as the marginal likelihood is not analytically available for non - gaussian likelihoods . while cross - validation avoids the latter problem and possibly also the first one , it is computationally less efficient than the full bayesian solution and fails to account for the posterior uncertainty .    for these reasons we recommend full bayesian inference for @xmath3 , and focus on how to specify the prior distribution .",
    "@xcite also recommend full bayesian inference for @xmath3 , and following @xcite , they propose prior @xmath44 whereas @xcite recommend @xmath45 here @xmath46 denotes the half - cauchy distribution with location 0 and scale @xmath47 . if the target variable @xmath48 is scaled to have marginal variance of 1 , unless the noise level @xmath35 is very small , both of these priors typically lead to quite similar posteriors .",
    "however , as we argue in section  [ sec : meff_and_tau ] , there is a theoretical justification for letting @xmath3 scale with @xmath35 .",
    "the main motivation for using a half - cauchy prior for @xmath3 is that it evaluates to a finite positive value at the origin , yielding a proper posterior and allowing even complete shrinkage @xmath16 , while still having a thick tail which can accommodate a wide range of values . for these reasons ,",
    "@xmath46 is a desirable choice when there are enough observations to let @xmath3 be identified by data .",
    "still , we show that in several cases one can clearly benefit by choosing the scale @xmath47 in a more careful manner than simply @xmath49 or @xmath50 , because for most applications these choices place far to much mass for implausibly large values of @xmath3 . this point is discussed in section  [ sec : meff_and_tau ] . moreover , in some cases , @xmath3 can be so weakly identified by the data that one can obtain even better results by using a more informative and tighter prior , such as half - normal , in place of half - cauchy ( see the experiments in section  [ sec : experiments ] ) .",
    "@xcite study the optimal selection of @xmath3 in the model @xmath51 they prove that in such a setting , the optimal value ( up to a log factor ) in terms of mean squared error and posterior contraction rates in comparison to the true @xmath52 is @xmath53 where @xmath54 denotes the number of nonzeros in the true coefficient vector @xmath52 ( assuming such exists ) .",
    "their proofs assume that @xmath55 and @xmath56 .",
    "model   corresponds to setting @xmath57 and @xmath58 in the usual regression model  .",
    "it is unclear whether and how this result could be extended to a more general @xmath18 , and how one should utilize this result when @xmath54 is unknown ( as it usually is in practice ) . in the next section ,",
    "we formulate our method of constructing the prior @xmath5 based on prior beliefs about @xmath54 , and show that if @xmath54 was known , our method would also give rise to result  , but is more generally applicable .",
    "consider the prior distribution for the shrinkage factor of the @xmath59th regression coefficient , eq .",
    ". the mean and variance can be shown to be @xmath60 } } & = \\frac{1}{1+\\sigma^{-1}\\tau\\sqrt{n } } ,       \\label{eq : kappa_mean } \\\\      { \\mathrm{var } { \\left[\\kappa_j { \\,|\\ , } \\tau,\\sigma\\right ] } } & = \\frac{\\sigma^{-1 } \\tau \\sqrt{n } } { 2 ( 1 + \\sigma^{-1}\\tau \\sqrt{n})^2}.      \\label{eq : kappa_var}\\end{aligned}\\ ] ] a given value for the global parameter @xmath3 can be understood intuitively via the prior distribution that it imposes on the effective number of coefficients distinguishable from zero ( or effective number of nonzero coefficients , for short ) @xmath61 when the shrinkage factors @xmath62 are close to 0 and 1 ( as they typically are for the horseshoe prior ) , this quantity describes essentially how many active or unshrunk variables we have in the model .",
    "it serves therefore as a useful indicator of the effective model size .    using results   and  , the mean and variance of @xmath6 given @xmath3 and @xmath35",
    "are given by @xmath63 } } & = \\frac{\\sigma^{-1}\\tau\\sqrt{n } } { 1+\\sigma^{-1}\\tau\\sqrt{n } } d , \\label{eq : meff_mean } \\\\      { \\mathrm{var } { \\left[m _ { \\text{eff } } { \\,|\\ , } \\tau , \\sigma\\right ] } } & = \\frac{\\sigma^{-1}\\tau \\sqrt{n } } { 2 ( 1 + \\sigma^{-1}\\tau \\sqrt{n})^2 } d. \\label{eq : meff_var}\\end{aligned}\\ ] ] the expression for the mean   is helpful .",
    "first , from this expression it is evident that to keep our prior beliefs about @xmath6 consistent , @xmath3 must scale as @xmath64 .",
    "priors that fail to do so , such as  , favor models of varying size depending on the noise level @xmath35 and the number of data points @xmath65 .",
    "second , if our prior guess for the number of relevant variables is @xmath66 , it is reasonable to choose the prior so that most of the prior mass is located near the value @xmath67 which is obtained by solving equation @xmath68 } } = p_0 $ ] .",
    "note that this is typically quite far from @xmath69 or @xmath35 , which are used as scales for priors   and  .",
    "for instance , if @xmath70 and @xmath71 , then prior guess @xmath72 gives about @xmath73 .    to further develop the intuition about the connection between @xmath3 and @xmath6 , it is helpful to visualize the prior imposed on @xmath6 for different prior choices for @xmath3 .",
    "this is most conveniently done by drawing samples for @xmath6 ; we first draw @xmath74 and @xmath75 , then compute the shrinkage factors @xmath76 from  , and finally @xmath6 from  .",
    "figure  [ fig : meff_prior ] shows histograms of prior draws for @xmath6 for some different prior choices for @xmath3 , with total number of inputs @xmath77 and @xmath78 , assuming @xmath79 observations with @xmath80 .",
    "the first three priors utilize the value @xmath81 which is computed from   using @xmath82 as our hypothetical prior guess for the number of relevant variables .",
    "fixing @xmath83 results in a nearly symmetric distribution around @xmath66 , while a half - normal prior with scale @xmath81 yields a skewed distribution favoring solutions with @xmath84 but allowing larger value to also be accommodated .",
    "the half - cauchy prior behaves similarly to the half - normal , but results in a distribution with a much thicker tail giving substantial mass also to values much larger than @xmath66 when @xmath9 is large .",
    "figure  [ fig : meff_prior ] also illustrates why the prior @xmath85 is often a dubious choice : it places far too much mass on large values of @xmath3 , consequently favoring solutions with most of the coefficients unshrunk .",
    "thus when only a small number of the variables are relevant  as we typically assume  this prior results in sensible inference only when @xmath3 is strongly identified by data .",
    "note also that , if we changed the value of @xmath35 or @xmath19 , the first three priors for @xmath3 would still impose the same prior for @xmath6 , but this is not true for @xmath86 .    this way , by studying the prior for @xmath6",
    ", one can easily choose the prior for @xmath3 based on the beliefs about the number of nonzero parameters . because the prior beliefs can vary substantially for different problems and the results depend on the information carried by the data ,",
    "there is no globally optimal prior for @xmath3 that works for every single problem . some recommendations , however , will be given in section  [ sec : conclusion ] based on these theoretical considerations and experiments presented in section  [ sec : experiments ] .",
    "we shall conclude this section by pointing out a connection between our reference value   and the oracle result   for the simplified model  .",
    "as pointed out in the last section , model   corresponds to setting @xmath57 ( which implies @xmath87 and @xmath88 ) in the usual regression model  . using this fact and repeating the steps needed to arrive at  , we get @xmath89 suppose now that we select @xmath90 , that is , our prior guess is oracle .",
    "using the same assumptions as @xcite , namely that @xmath55 and @xmath56 , and additionally that @xmath80 , we get @xmath91 .",
    "this result is natural , as it means it is optimal to choose @xmath3 so that the imposed prior for the effective number of nonzero coefficients @xmath6 is centered at the true number of nonzeros @xmath54 .",
    "this further motivates why @xmath6 is a useful quantity .      when the observation model is non - gaussian , the exact analysis from section  [ sec : meff_and_tau ] is analytically intractable .",
    "we can , however , perform the analysis using a gaussian approximation to the likelihood . using the second order taylor expansion for the log likelihood",
    ", the approximate posterior for the regression coefficients given the hyperparameters becomes ( see the supplementary material for details ) @xmath92 where @xmath93 , @xmath94 and @xmath95 ( assuming the first inverse exists ) . here",
    "@xmath96 denotes the possible dispersion parameter and @xmath97 the location and variance for the @xmath98th gaussian pseudo - observation .",
    "the fact that some of the observations are more informative than others ( @xmath99 vary ) makes further simplification somewhat difficult .",
    "to proceed , we make the rough assumption that we can replace each @xmath99 by a single variance term @xmath100 .",
    "assuming further that the covariates are uncorrelated with zero mean and unit variance ( as in sec .",
    "[ sec : meff_and_tau ] ) , the posterior mean for the @xmath59th coefficient satisfies @xmath101 with shrinkage factor given by @xmath102 the discussion in section  [ sec : meff_and_tau ] therefore also approximately holds for the non - gaussian observation model , except that @xmath11 is replaced by @xmath100 .",
    "still , this leaves us with the question , which value to choose for @xmath100 when using this result in practice ?",
    "we consider binary classification here as an example .",
    "it can be shown ( see the supplementary material ) that for the logistic regression @xmath103 for those points that lie on the classification boundary , we have @xmath104 , and for others @xmath105 .",
    "for this reason we propose to use the results of section  [ sec : meff_and_tau ] as they are , by plugging in @xmath106 . in practice",
    "this introduces some error , but the good thing is that we know in which way the results are biased .",
    "for instance , because the true ( unknown ) effective noise deviation would be @xmath107 , the prior mean   using @xmath106 is actually an _ overestimate _ of the true value . thus also when using the result  , we tend to favor slightly too small values of @xmath3 and thus also solutions with slightly less than @xmath66 nonzero coefficients . in practice",
    "we observe that this approach , though relying on several crude approximations , is still useful and gives reasonably accurate results .",
    "finally , we note that similar approximate substitute values for @xmath35 to be used in equations of section  [ sec : meff_and_tau ] could also be derived for other link functions and observation models , but due to limited space , we do not focus on them in this paper .",
    "we conclude this section by noting that our approach could also be used with shrinkage priors other than the horseshoe , as long as the prior can be written as scale mixtures of gaussians like   with some prior for the local scales @xmath108 .",
    "the closed form equations in section  [ sec : meff_and_tau ] apply only to the horseshoe . depending on the choice of @xmath109 , corresponding analytical results may or may not be available , but as long as one is able to sample both from @xmath5 and @xmath109 , it is always easy to draw samples from the prior distribution for @xmath6 .",
    "it must be noted though , that when @xmath109 is chosen so that most of the shrinkage factors @xmath41 are not near 0 or 1 , the values of @xmath6 can be more difficult to interpret .",
    "we illustrate the importance of the prior choice for @xmath3 with real world examples .",
    "the datasets are summarized in table  [ tab : datasets ] and can be found online . ]",
    "the first four are microarray cancer datasets , many of which have been used as benchmark datasets by several authors ( see e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* and references therein ) .",
    "the corn data @xcite consists of three sets of predictors ( ` m5 ' , ` mp5 ' , ` mp6 ' ) and four responses .",
    "we use the ` mp5 ' input data and consider prediction for all four targets .",
    ".summary of the datasets , number of predictor variables @xmath9 and dataset size @xmath19 .",
    "classification datasets are all binary . [ cols=\"<,<,^,^,^\",options=\"header \" , ]",
    "a gaussian linear model was used for the regression tasks and logistic regression for classification .",
    "the horseshoe prior was employed for the regression coefficients and a weakly informative prior @xmath110 for the intercept .",
    "the noise variance was given the standard prior @xmath111 in the regression problems .",
    "all considered models were fitted using stan ( codes in the supplementary material ) , running 4 chains , 1000 samples each , first halves discarded as warmup .",
    "_ effect on parameter estimates_we first consider the ovarian dataset as a representative example of how much the prior choice @xmath5 can affect the parameter estimates .",
    "we fitted the model to the data using three different priors for the global parameter ; @xmath112 , @xmath113 , and @xmath85 , where @xmath81 is computed from   using @xmath114 as our prior guess for the number of relevant variables and @xmath115 ( for reasons discussed in section  [ sec : nongaussian_lik ] ) .",
    "figure  [ fig : ovarian ] shows prior and posterior samples for @xmath3 and @xmath6 , and the absolute values of the posterior means for the regression coefficients , for the three prior choices .",
    "the results for @xmath85 ( last column ) illustrate how weakly @xmath3 is identified by the data : there is very little difference between the prior and posterior samples for @xmath3 and consequently for @xmath6 , and thus this `` non - informative '' prior actually has a strong influence on the posterior inference .",
    "the prior results in a severe under - regularization and implausibly large absolute values for the logistic regression coefficients ( magnitude in the hundreds ) .    replacing the scale of the half - cauchy with @xmath81 , reflecting",
    "a more sensible guess for the number of relevant variables , has a substantial effect on the posterior inference : the posterior mass for @xmath6 becomes concentrated on more reasonable values and the magnitude of the regression coefficients more sensible .",
    "replacing the half - cauchy by half - normal places a tighter constraint on @xmath3 and consequently also on @xmath6 .",
    "either of these two choices is a substantial improvement over @xmath85 , and this is also seen in the predictive accuracy ( to be discussed in a moment ) .    a potential explanation for why @xmath3 and therefore @xmath6 are not strongly identified here is that there are a lot of correlations in the data . for instance , the predictor @xmath116 which appears relevant based on its regression coefficient , has an absolute correlation of at least @xmath117 with 314 other variables , out of which 65 correlations exceed @xmath118 .",
    "this indicates that there are a lot of potentially relevant but redundant predictors in the data .",
    "note also that even though our theoretical treatment for @xmath6 assumes uncorrelated variables , the correlations do not seem to be a marked problem in this sense .    _ prediction accuracy and computation time_to investigate the effect of the prior choice @xmath5 on the prediction accuracy , we splitted each dataset into two halves , using one fifth of the data as a test set .",
    "all the results were then averaged over 50 such random splits into training and test sets . for the regression problems",
    ", we carried out the tests for priors @xmath119 and @xmath120 with @xmath81 given by eq .   for various @xmath66 .",
    "the experiments for the classification datasets were done using the same priors by plugging in @xmath106 .",
    "we also compared the prediction accuracies to lasso with the regularization parameter tuned by 10-fold cross - validation , and noise variance in regression computed by dividing the sum of squared residuals by @xmath121 , where @xmath122 is the size of the lasso active set @xcite .",
    "figures  [ fig : results_reg ]  and  [ fig : results_classif ] show the effect of the prior choice on the posterior mean for @xmath6 , test prediction accuracy , and computation time ( wall time ) .",
    "the classification datasets illustrate a clear benefit from using even a crude prior guess for the number of relevant variables @xmath66 : there is relatively wide range of values for @xmath66 which yield simpler models ( smaller @xmath123 ) , better predictive accuracy , and substantially reduced computation time when compared to using @xmath124 ( the largest @xmath66 for each curve ) , regardless of whether half - normal or half - cauchy prior is used .",
    "the reduced computation time is due to tighter concentration of the posterior mass which aids the sampling greatly .",
    "the half - cauchy prior performs better than the half - normal prior with small @xmath66 values ( both for classification and regression datasets ) , because the thick tails allow @xmath3 to get much larger values than @xmath81 , but for too small values of @xmath66 , even the half - cauchy can lead to bad results ( corn - oil  corn - starch ) . on the other hand ,",
    "when @xmath66 happens to be well chosen , the tighter half - normal prior can yield a simpler model ( smaller @xmath6 ) with equally good or even better predictive accuracy ( classification datasets ) . for any reasonably selected prior",
    ", the horseshoe consistently outperforms lasso in a pairwise comparison , but note that for ovarian and colon , @xmath124 ( largest @xmath66 ) actually yields worse results . in terms of classification accuracy , the differences are smaller , but in regression , the horseshoe also performs better when measured by the mean squared error ( not shown ) .",
    "a clear advantage for lasso , on the other hand , is that it is hugely faster , with computation time of only a few seconds for these problems .",
    "this paper has discussed the prior choice for the global shrinkage parameter in the horseshoe prior for sparse bayesian regression and classification .",
    "we have shown that the previous default choices are often dubious based on their tendency to favor solutions with too many parameters unshrunk .",
    "the experiments show that for many datasets , one can obtain clear improvements  in terms of better parameter estimates , prediction accuracy and computation time  by coming up even with a crude guess for the number of relevant variables and transforming this knowledge into a prior for @xmath3 using our proposed framework .",
    "the results also show that there is no globally optimal prior choice that would perform best for all problems , which emphasizes the relevance of the prior choice . as a new default choice for regression ,",
    "we recommend @xmath120 , where @xmath81 is computed from   using the prior guess @xmath66 for the number of relevant variables . for logistic regression , an approximately equivalent choice is obtained by plugging in @xmath106 ( see sec .",
    "[ sec : nongaussian_lik ] ) .",
    "this choice seems to perform well unless @xmath66 is very far from the optimal .",
    "if the results still indicate that more regularization is needed , we recommend investigating the imposed prior on @xmath6 as discussed in section  [ sec : meff_and_tau ] , and changing @xmath5 so that the prior for @xmath6 corresponds to our beliefs as closely as possible .",
    "we thank andrew gelman , jonah gabry and eero siivola for helpful comments to improve the manuscript .",
    "we also acknowledge the computational resources provided by the aalto science - it project .",
    "_ posterior approximation_using the second order taylor expansion for the log likelihood terms @xmath125 ( where @xmath126 and @xmath96 denotes a possible dispersion parameter ) , we can approximate the posterior as ( ch .  16.2 , * ? ? ?",
    "* ) @xmath127 where @xmath128 denote the location and variance of the gaussian pseudo - observations .",
    "the derivatives are calculated w.r.t .",
    "@xmath129 at the posterior mode @xmath130 .",
    "using these , the posterior ( given the hyperparameters ) is approximately @xmath92 where @xmath93 , @xmath94 and @xmath95 ( assuming the first inverse exists ) . _ _ logistic regression__consider the logistic regression model @xmath131 the second derivative for the @xmath98th log - likelihood term is given by @xmath132 if we now plug in the derivatives @xmath133 after a few lines of straightforward algebra , we are left with @xmath134 this is a strictly negative function with minimum at @xmath135 , which occurs when @xmath136 .",
    "thus also @xmath137 is minimized at @xmath136 . in other words ,",
    "those points that lie on the classification boundary are the most informative ones , and the pseudo - variance for these points is @xmath138 this result serves as a useful reference value as discussed in section  [ sec : nongaussian_lik ] .",
    "the following shows the stan code for the linear gaussian model . in the code , both @xmath3 and @xmath15 are given half-@xmath139 priors with the degrees of freedom and the scale defined by the user ( the scale can be adjusted only for @xmath3 , the local parameters @xmath15 have unit scale ) . setting nu_local",
    "@xmath140 corresponds to the horseshoe .",
    "nu_global @xmath141 gives @xmath3 a half - cauchy prior , whereas fixing nu_global to some large value ( say 100 ) would give @xmath3 practically a half - normal prior .",
    "the scale for @xmath3 is scale_global*sigma , so if we want to set this to be @xmath142 ( eq .  ) , we should set ` scale_global ` @xmath143 .",
    "the code for the logistic regression model is very similar , we simply remove the lines related to the noise deviation sigma , and change the observation model and the type of the target variable data y. the scale for @xmath3 is now simply scale_global .",
    "thus , to follow our recommendation , we set scale_global @xmath144 ( eq .  ) , by plugging in @xmath145 ( sec .",
    "[ sec : nongaussian_lik ] ) ."
  ],
  "abstract_text": [
    "<S> the horseshoe prior has proven to be a noteworthy alternative for sparse bayesian estimation , but as shown in this paper , the results can be sensitive to the prior choice for the global shrinkage hyperparameter . </S>",
    "<S> we argue that the previous default choices are dubious due to their tendency to favor solutions with more unshrunk coefficients than we typically expect a priori . </S>",
    "<S> this can lead to bad results if this parameter is not strongly identified by data . </S>",
    "<S> we derive the relationship between the global parameter and the effective number of nonzeros in the coefficient vector , and show an easy and intuitive way of setting up the prior for the global parameter based on our prior beliefs about the number of nonzero coefficients in the model . </S>",
    "<S> the results on real world data show that one can benefit greatly  in terms of improved parameter estimates , prediction accuracy , and reduced computation time  from transforming even a crude guess for the number of nonzero coefficients into the prior for the global parameter using our framework . </S>"
  ]
}