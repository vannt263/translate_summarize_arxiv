{
  "article_text": [
    "when attempting to model biological learning , what factors should we take into account , and what sort of problems should we expect our models to solve ? one of the things which per bak always emphasized was that it was not enough simply to learn one task fast : biological neural dynamics had to be able to _ adapt _ , to _ _ un__learn patterns of behaviour that were no longer working and find new ones .",
    "for example , the important early work on ` reinforcement learning ' by barto and colleagues  @xcite , which produced much more biologically plausible learning rules than those previously considered , still foundered on this problem , with networks having to be completely reset in order to learn a new problem .",
    "particular progress in this regard was made by the work of dmitris stassinopoulos , in collaboration with preben alstrm  @xcite and per himself  @xcite .",
    "however , it was a few years later that an especially elegant model was developed by per in collaboration with dante chialvo  @xcite .",
    "this model , which i rather cheekily dubbed the ` minibrain'@xcite , addressed the problem of adaptation by assuming that learning was by only _ long - term synaptic depression _",
    "( ltd ) , the weakening of connections .",
    "synapses involved in bad decisions were suppressed , but only enough to render them inactive . meanwhile , because synapses were not strengthened or reinforced in any way  there was a complete absence of long - term potentiation ( ltp)the strengths of active connections remained barely greater than those of the inactive ones .",
    "thus , the network could easily switch to using a different set of connections if the need arose .",
    "it was suggested that it was the absence of any strengthening of connections in the model that was the key to its adaptive ability . in the present work",
    "i illustrate this by investigating the adaptive ability of the minibrain when simple forms of ltp are included , compared to the original model and the ` selective punishment ' extension later proposed  @xcite .",
    "for simplicity i consider here only the basic feedforward minibrain : 3 layers of neurons ( ` input ' , intermediary and ` output ' ) of size @xmath0 , @xmath1 and @xmath2 respectively .",
    "each neuron in the input layer has a one - way connection to every neuron in the intermediary layer , and similarly each intermediary neuron has a connection to every output .",
    "each connection is assigned a strength value , initially evenly distributed in the interval @xmath3 $ ] ( here @xmath4 ) .",
    "activity propagates according to _ extremal dynamics _ :",
    "if we stimulate a neuron , the signal travels along the single strongest outgoing connection , and the neuron at the end of that connection then fires , and so on until an output neuron fires .",
    "should this output be incorrect , a negative feedback signal is sent to the system and the connections responsible are punished by having their strengths reduced by a random amount in the interval @xmath5 $ ] ( here @xmath6 ) . learning efficiency is measured by the total number @xmath7 of such signals required for complete learning . depending on a control",
    "parameter @xmath8 , two phases of behaviour are identifiable  @xcite : for @xmath9 the network is in the _ disordered _ phase where complete learning is impossible and @xmath10 . for",
    "@xmath11 complete learning becomes possible with @xmath12 . in the present work , @xmath13 , @xmath14 and @xmath15 , with the @xmath1 value picked to ensure the network is in the ordered phase so complete learning",
    "will always be possible .",
    "the simplest manner of including ltp is symmetric with the negative feedback : in the event of a successful decision , the connections responsible can be rewarded by having their strengths increased by a random amount in the interval @xmath16 $ ] .",
    "how does this affect the network s adaptive ability ?",
    "adaptive ability of the minibrain model when unbounded ltp is included , compared to the traditional version with ltd only .",
    "( a ) at each adaptation , one input - output association is randomly reselected ( ` slow change ' ) .",
    "the inset shows more clearly the results for smaller @xmath17 .",
    "note the astonishing increase in the required value of @xmath7 for @xmath18 : only for the smallest values , @xmath19 , is this bounded , and then performance is still significantly worse than for the ltd - only network .",
    "( b ) the network is required to adapt successively back and forth between two different input - output maps ( the ` flip - flop ' problem ) .",
    "@xmath20 is bounded in all cases , but nevertheless @xmath21 ( ltd only ) provides the best performance .",
    "data averaged over 128 realizations.,title=\"fig:\",scaledwidth=49.0% ] adaptive ability of the minibrain model when unbounded ltp is included , compared to the traditional version with ltd only .",
    "( a ) at each adaptation , one input - output association is randomly reselected ( ` slow change ' ) .",
    "the inset shows more clearly the results for smaller @xmath17 .",
    "note the astonishing increase in the required value of @xmath7 for @xmath18 : only for the smallest values , @xmath19 , is this bounded , and then performance is still significantly worse than for the ltd - only network .",
    "( b ) the network is required to adapt successively back and forth between two different input - output maps ( the ` flip - flop ' problem ) .",
    "@xmath20 is bounded in all cases , but nevertheless @xmath21 ( ltd only ) provides the best performance .",
    "data averaged over 128 realizations.,title=\"fig:\",scaledwidth=49.0% ]    suppose that we present a network with an input - output map to learn and , each time learning is completed , randomly reroll one of the input - output associations .",
    "we can call this the ` slow change ' problem .",
    "naturally we want to know what value of @xmath7 the network will require to adapt to the new maps , the number of applications of negative feedback required for complete learning , as a measure of learning efficiency .",
    "ltp in this context is not so much a learning mechanism as an ` anti - forgetting ' mechanism . ] .",
    "we stimulate each of the inputs in turn and apply ltp or ltd as necessary ; learning is deemed complete when we can run through all the inputs without error .",
    "[ fig : ltd - ltp_unl]a shows how the average @xmath20 varies with successive adaptations .",
    "most of the networks with @xmath22 very quickly become hopelessly addicted , requiring huge amounts of negative feedback to adapt to new input - output maps ; this is escaped only with the smallest values of @xmath18 .",
    "indeed , we can observe two distinct phases of behaviour : for @xmath23 , @xmath20 is bounded ( though always worse than the @xmath21 case ) , while for @xmath24 we have the real addiction with @xmath20 growing exponentially .",
    "we can explain this as follows .",
    "consider one input neuron .",
    "on average , @xmath0 adaptations will pass before its associated output is reselected .",
    "since it is receiving positive feedback all the while , the amount of potentiation given to its active outgoing connections will be proportional to @xmath0 .",
    "thus , if the ratio @xmath25 is greater than @xmath26 , there will be a divergence between the strengths of the active and inactive connections , leading to the observed addiction . more generally , for any @xmath18 , it is possible to think of a rate of change slow enough that addiction will result .",
    "[ fig : ltd - ltp_unl]b shows the results for a different problem , the ` flip - flop ' problem .",
    "this time , the network starts by having to learn the map @xmath27 , @xmath28 , @xmath29 ,  , and , once this has been learned , the map required is switched to @xmath30 , @xmath31 , @xmath32 ,  ; and we continue switching back and forth between these two inverse input - output maps . again , the learning process consists of repeatedly running through the cycle of inputs until the complete cycle can be run through without error .",
    "the slowness of change of the input - output map is no longer an issue , since _ all _ of the input - output mappings are changed at each adaptation .",
    "thus , as one should expect , the values of @xmath20 required to adapt remain bounded for all values of @xmath17 . nevertheless , performance is still observably worse in all cases than the @xmath21 case with ltd only .",
    "collapse of data in fig .",
    "[ fig : ltd - ltp_unl]b for the flip - flop problem and unbounded ltp , according to the prediction of equation  ( [ eq : ff - unl]).,scaledwidth=60.0% ]    to explain this , consider again a single input .",
    "once it has been correctly wired up to its associated output , it will have a window of time ( while the rest of the network is still learning ) in which its active outgoing synapses will be potentiated . the average for this time window",
    "will be controlled by the system size , i.e. a constant @xmath33 for any given network , and the total divergence between active and inactive connections will be proportional to @xmath17 ; thus the gain in the amount of negative feedback required to adapt will be given by @xmath25 . mathematically speaking ,",
    "we have , @xmath34 which is confirmed by the data collapse achieved in fig .",
    "[ fig : ltd - ltp_ff - collapse ] .",
    "a method to avoid addiction while maintaining potentiation was proposed by parisi  @xcite with respect to the hopfield neural network model . by placing an upper bound on synaptic strength",
    ", he was able to construct a ` memory which forgets ' and thus avoid the state of total confusion observed if the network were overloaded .",
    "this can be easily applied to the minibrain model , requiring synaptic strengths to be bounded in the interval @xmath35 $ ] , with @xmath36 . in this bounded case , should potentiation cause a synaptic weight @xmath37 to go over the limit , we simply reset it to @xmath38 .",
    "adaptive ability when bounded ltp is included .",
    "( a ) slow change problem .",
    "addiction is prevented but for @xmath39 , adaptive ability is worse than in the ltd - only ( @xmath21 ) version .",
    "( b ) flip - flop problem .",
    "@xmath20 increases to a maximum value determined by @xmath17 ( see fig .  [",
    "fig : ltd - ltp_unl]b ) .",
    "data averaged over 128 realizations.,title=\"fig:\",scaledwidth=49.0% ] adaptive ability when bounded ltp is included .",
    "( a ) slow change problem .",
    "addiction is prevented but for @xmath39 , adaptive ability is worse than in the ltd - only ( @xmath21 ) version .",
    "( b ) flip - flop problem .",
    "@xmath20 increases to a maximum value determined by @xmath17 ( see fig .  [",
    "fig : ltd - ltp_unl]b ) .",
    "data averaged over 128 realizations.,title=\"fig:\",scaledwidth=49.0% ]    fig .",
    "[ fig : ltd - ltp_cap]a shows the performance in the slow change problem of different networks with @xmath40 and varying values of @xmath38 , as compared to a network with @xmath21 . while the presence of the bound @xmath38 has prevented the runaway addiction seen in fig .",
    "[ fig : ltd - ltp_unl]a , there is no improvement over the simple ltd - only case . in general , the network performs worse , and it is only with @xmath41 that the network matches the performance of the standard ltd - only minibrain .",
    "this is natural when one thinks that inactive connections will always have strength of @xmath42 whereas active connections will have strength @xmath43 .",
    "by contrast a curious behaviour is observed in the ` flip - flop ' problem of switching back and forth between two different ( non - overlapping ) input - output maps ( fig .",
    "[ fig : ltd - ltp_cap]b ) . in all cases",
    "the value of @xmath20 increases with successive adaptations to a maximum value controlled , not by @xmath38 , but by @xmath17 ( see also fig .  [",
    "fig : ltd - ltp_unl]b ) .",
    "how can we explain this ?",
    "each time the input - output map switches , the system must suppress the active connections and then search among all connections for the required correct outputs .",
    "what makes this different from the slow change problem is that here the majority of possible input - output connections are _ always _ incorrect .",
    "therefore , the majority of connections will be continually weakened , never strengthened , and the gap between the average synapse strength and the maximum synapse strength @xmath38 will diverge .",
    "this is equivalent to continually increasing the value of @xmath38 , meaning that in the long run the system will behave as if this limit does not exist , reproducing the behaviour observed in the case of _ unbounded _ ltp .",
    "finally , let us consider the case of _ selective punishment_. here a synapse involved in a successful decision becomes permanently marked as ` good ' ; should it later be punished , it is by an amount in the interval @xmath44 $ ] with @xmath45 .",
    "thus , a previously good connection that has been suppressed is easier to reactivate than a connection that has never been good .    as fig .",
    "[ fig : ltd - ltp_sp]a shows , this makes no significant difference to the network s ability to adapt in the slow change problem .",
    "should @xmath1 take a smaller value , the effects of the selective punishment become more pronounced and adaptation is initially slower , but this effect vanishes as the network gains ` good ' connections to all possible outputs .    adaptive ability with ltd only , but with selective punishment of previously successful connections .",
    "( a ) slow change problem .",
    "no significant difference is observed for different values of @xmath46 .",
    "( b ) flip - flop problem .",
    "when lesser rates of punishment are applied to ` good ' connections , @xmath20 _ decreases _ with successive adaptations : the system has a memory of previously good responses .",
    "data averaged over 128 realizations.,title=\"fig:\",scaledwidth=49.0% ] adaptive ability with ltd only , but with selective punishment of previously successful connections .",
    "( a ) slow change problem .",
    "no significant difference is observed for different values of @xmath46 .",
    "( b ) flip - flop problem .",
    "when lesser rates of punishment are applied to ` good ' connections , @xmath20 _ decreases _ with successive adaptations : the system has a memory of previously good responses .",
    "data averaged over 128 realizations.,title=\"fig:\",scaledwidth=49.0% ]    however , in the second of our two problems  switching back and forth between two distinct input - output maps  selective punishment proves a considerable benefit ( fig .",
    "[ fig : ltd - ltp_sp]b ) . while for @xmath47",
    "the value of @xmath20 remains constant with a value of @xmath48 , as one would expect , values of @xmath49 see a _ decrease _ in @xmath20 with successive adaptations , towards a minimum value much lower than without the selective punishment .",
    "recall that the selective punishment favours previously - good connections over others .",
    "since in this case the number of good responses for each input is small by comparison to the total number of responses , this has the effect of drastically cutting learning times .",
    "perhaps the key result of the present work has been to observe that , in the setup considered here , strengthening of synapses _ always _ carries within it the potential for divergence between the strengths of active and inactive connections .",
    "this divergence is governed not merely by the level of potentiation but also by the system size , increasing as the network becomes larger .",
    "the minibrain is a ` toy ' model , but it is nevertheless instructive to consider it in the light of biological results .",
    "both ltp and ltd are well - observed in biological neuronal systems but their precise functions remain unclear @xcite .",
    "a variety of different points of view can be found in the literature , with a number of authors explicitly endorsing a selectionist picture of neuronal dynamics  @xcite where learning is by either elimination or depression of connections .",
    "thinking along these lines one might want to seek other means of positive feedback than ltp , such as the ` synaptic forgiveness ' proposed by klemm et al .",
    "@xcite .",
    "other authors have suggested that learning may result from a balance of ltp and ltd with a global feedback mechanism to prevent runaway strengthening or weakening of synapses @xcite .",
    "a modification to the minibrain along these lines has recently been proposed by bosman et al .",
    "the present results suggest that such a global mechanism may not just be useful , but _ vital _ , if ltp is to be an effective part of learning .",
    "many thanks to dante chialvo and maya paczuski for helpful comments and advice , and for a great deal of personal and professional support over the last two years . thanks also to yi - cheng zhang and mogens hgh jensen for making possible my sabbatical at the nbi where this work was completed .",
    "i acknowledge the support of a grant from the swiss national science foundation ( no .",
    "20 - 61470.00 ) and a marie curie fellowship ( hpmt - ct-2001 - 00402 ) .",
    "there are really no words adequate to express the debt of thanks and friendship i owe per bak , whose immense kindness and support , both professional and personal , made possible my whole scientific career ."
  ],
  "abstract_text": [
    "<S> one of the key points addressed by per bak in his models of brain function was that biological neural systems must be able not just to learn , but also to _ </S>",
    "<S> adapt_to quickly change their behaviour in response to a changing environment . </S>",
    "<S> i discuss this in the context of various simple learning rules and adaptive problems , centred around the chialvo - bak ` minibrain ' model [ neurosci . </S>",
    "<S> 90 ( 1999 ) 11371148 ] .    adaptive learning , neural networks , feedback mechanisms , biological learning 07.05.mh , 84.35.+i , 87.18.sn , 87.19.la </S>"
  ]
}