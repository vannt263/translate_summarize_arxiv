{
  "article_text": [
    "the problem of inferring interactions couplings in complex systems arises from the huge quantity of empirical data that are being made available in many fields of science and from the difficulty of making systematic measurements on interactions . in biology , for example",
    ", empirical data on neurons populations , small molecules , proteins and genetic interactions , have largely outgrown the understanding of the underlying system mechanisms . in all these cases the inverse problem , whose aim is to infer some effective model from the empirical data with just partial a priori knowledge , is of course extremely relevant .    statistical physics , with its set of well - understood theoretical models , has the crucial role to provide complex , but clear - cut benchmarks ,",
    "whose direct solution is known and that can therefore be used to develop and test new inference methods .    in a nutshell , the equilibrium approach to inverse problems consists inferring some information about a system defined through an energy function @xmath0 starting from a set of sampled equilibrium configurations .",
    "suppose @xmath1 i.i.d sampled configurations @xmath2 generated by a boltzmann distribution of an unknown energy function @xmath3 , @xmath4 are given .",
    "the posterior distribution of @xmath3 ( also called likelihood ) is given by @xmath5 , where @xmath6 represents the average of @xmath3 over the given sample configurations and @xmath7 the prior knowledge about @xmath3 .",
    "the parameter @xmath1 plays the role of an inverse temperature : when @xmath1 is very large , @xmath8 peaks on the maximums ( with respect to @xmath3 ) of the  log - likelihood  @xmath9 .",
    "the problem of identifying the maximum of @xmath10 can be thought of as an optimization problem , normally very difficult both because the space of @xmath3 is large and because @xmath11 is very difficult to estimate by itself on a candidate solution .",
    "several methodological advances and stimulating preliminary applications in neuroscience have been put forward in the last few years @xcite .",
    "still the field presents several major conceptual and methodological open problems related to both the efficiency and the accuracy of the methods .",
    "one problem which we consider here is how to perform inference when data are not coming from a uniform sampling over the equilibrium configurations of a system but rather they are taken from a subset of all the attractive states .",
    "this case arises for instance when we consider systems with multiple attractors and we want to reconstruct the interactions couplings from measurements coming from a subset of the attractors .    in what follows ,",
    "we take as model system the hopfield model over random graphs in its memory phase ( i.e. with multiple equilibrium states ) and show how the interaction couplings can be inferred from data taken from a subset of memories ( states ) .",
    "this will be done by employing the bethe equations ( normally used in the ergodic phase where they are asymptotically exact ) by taking advantage of a certain property of their multiple fixed points in the non - ergodic phase .",
    "the method can be used to infer both couplings and external local fields .",
    "we also show how from the inference method one can derive a simple unsupervised learning protocol which is able to learn patterns in presence of week and highly fluctuating input signals , without ever reaching a spin glass like saturation regime in which all the memories are lost .",
    "the technique that we will discuss is based on the so called cavity method and leads to a distributive algorithmic implementation generically known as message  passing scheme .",
    "the paper is organized as follows .",
    "first , in section [ s2 ] we define the problem , and make connections with related works .",
    "section [ s3 ] is concerned with the inference problem in non - ergodic regimes , for which a simple algorithmic approach is presented . in section [ s4 ]",
    "we apply the technique to the finite connectivity hopfield model in the memory phase .",
    "section [ s5 ] shows how the approach can be turned into an unsupervised learning protocol .",
    "conclusions and perspectives are given in section [ s6 ] .",
    "the hopfield model is a simple neural network model with pair - wise interactions which behaves as an attractor associative memory @xcite .",
    "its phase diagram is known exactly when memories are random patterns and the model is defined over either fully connected or sparse graphs @xcite .",
    "reconstructing interactions in the hopfield model from partial data thus represents a natural benchmark problem for tools which pretend to be applied to data coming from multi electrode measurements from large collections of neurons .",
    "the underlying idea is that a statistically consistent interacting model ( like the hopfield model ) inferred from the data could capture some aspects of the system which are not easy to grasp from the raw data @xcite .",
    "here we limit our analysis to artificial data .    in the hopfield model",
    "the couplings @xmath12 are given by the covariance matrix of a set of random patterns which represent the memories to be stored in the system .",
    "we will use the model to generate data through sampling and we will aim at inferring the couplings .",
    "the structure of the phase space of the hopfield model at sufficiently low temperature and for a not too large number of patterns is divided into clusters of configurations which are highly correlated with the patterns .",
    "we will proceed by sampling configurations from a subset of clusters and try to infer the interactions .",
    "the simple observation that we want to exploit is the fact that fluctuations within single clusters are heavily influenced by the existence of other clusters and thus contain information about the total system .",
    "we consider a system of @xmath13 binary neurons @xmath14 ( or ising spins ) interacting over a random regular graph of degree @xmath15 ; that is every node has a fixed number @xmath15 of neighbors which are selected randomly .",
    "the connectivity pattern is defined by the elements @xmath16 of the adjacency matrix .",
    "the ( symmetric ) interactions between two neighboring neurons are given by the hebb rule , i.e. @xmath17 , where @xmath18 are the patterns to be memorized and @xmath19 is their number . at finite temperature @xmath20 , we simulate the system by a glauber dynamics @xcite ; starting from an initial configuration , the spins are chosen in a random sequential way and flipped with the following transition probability @xmath21 where @xmath22 is the local field experienced by spin @xmath23 and @xmath24 is an external field .",
    "we use @xmath25 to denote the set of neighbors interacting with @xmath23 .",
    "the process satisfies detailed balance and at equilibrium the probability of steady state configurations @xmath26 is given by the gibbs measure @xmath27}e^{\\beta \\sum_i \\theta_i \\sigma_i+\\beta \\sum_{i < j } j_{ij}\\sigma_i \\sigma_j},\\ ] ] where @xmath28 $ ] is a normalization constant , or partition function . in the memory phase , the system will explore configurations close to a pattern , provided that the initial configuration lies in the basin of attraction of that pattern . in the following we use the wording patterns , basins of attraction or states equivalently . in a given state the average activity ( or magnetization ) and correlations are denoted by @xmath29 and @xmath30 where the averages are taken with the gibbs measure inside that state .",
    "informally , a gibbs state corresponds to a stationary state of the system , and so defined by the average macroscopic quantities in that state .",
    "suppose that starting from some random initial configuration we observe the system for a long time , measuring @xmath1 configurations .",
    "the standard way to infer interactions couplings @xmath31 , and external fields @xmath32 is by maximizing the log - likelihood of @xmath33 , given the experimental data @xcite , namely @xmath34,\\end{aligned}\\ ] ] where @xmath35 and @xmath36 are the experimental magnetizations and correlations and @xmath37 is the free energy .",
    "one can exploit the concavity of the log - likelihood and use a gradient ascent algorithm to find the unique parameters maximizing the function .",
    "however , this needs an efficient algorithm to compute derivatives of the free energy @xmath38 $ ] , which in general is a difficult task .",
    "a well known technique which can be used for not too big systems is of course the monte carlo method ( see e.g. @xcite ) .",
    "though under certain limiting assumptions , there exist good approximation techniques which are efficient , namely mean field , small - correlation and large - field expansions @xcite .",
    "in this paper we resort to the mean - field cavity method , or belief propagation ( bp ) , to compute the log - likelihood ( see e.g. @xcite ) .",
    "this technique is closely related to the thouless - anderson - palmer ( tap ) approximation in spin glass literature @xcite .",
    "the approximation is exact on tree graphs and asymptotically correct as long as the graph is locally tree - like or the correlations between variables are sufficiently weak . in spin glass jargon ,",
    "the approximation works well in the so called replica symmetric phase .    in the bp approach ,",
    "the marginals of variables and their joint probability distribution ( which is assumed to take a factorized form where only pair correlations are kept ) are estimated by solving a set of self - consistency functional equations , by exchanging messages along the edge of the interaction graph ( see ref .",
    "@xcite for comprehensive review ) . a message ( typically called `` cavity belief '' )",
    "@xmath39 is the probability that spin @xmath23 takes state @xmath40 ignoring the interaction with its neighbor @xmath41 , i.e. in a cavity graph .",
    "we call this probability distribution a cavity message . assuming a tree interaction graph we can write an equation for @xmath39 relating it to other cavity messages @xmath42 sent to @xmath23 : @xmath43 as in cavity graphs the neighboring variables are independent of each other .",
    "these are bp equations and can be used even in loopy graphs to estimate the local marginals .",
    "the equations are solved by starting from random initial values for the cavity messages and updating them in some random sequential order till a fixed point is reached . upon convergence",
    "the cavity messages are used to obtain the local marginals or `` beliefs '' : @xmath44 these marginals are enough to compute the magnetizations @xmath45 and correlations @xmath46 and thus can be used for maximizing the log - likelihood by updating the parameters as @xmath47 with @xmath48 and positive . repeating this procedure for sufficient times leads to an estimate of the unknown parameters . assuming that the external fields are absent , the inference error can be written as : @xmath49 a more accurate estimate of the correlations can be obtained by exploiting the fluctuation - response theorem @xmath50 .",
    "this method , called susceptibility propagation @xcite , uses derivatives of cavity messages ( cavity susceptibilities ) .",
    "its time complexity grows as @xmath51 , to be compared with the @xmath52 complexity of bp equations .    in this paper",
    "we will work exclusively with the bp estimate which is simple and accurate enough for our studies .",
    "actually , if one is interested only on correlations along the edges of a sparse graph , the bp estimation would be as good as the one obtained by susceptibility propagation .",
    "the reader can find more on the susceptibility propagation in @xcite .",
    "in an ergodic phase a system visits all configuration space . sampling for a long time",
    "is well represented by the measure in ( [ gibbs ] ) .    in a non - ergodic phase , as happens for the hopfield model in the memory phase , the steady state of a system is determined by the initial conditions .",
    "starting from a configuration close to pattern @xmath53 , the system spends most of its time ( depending on the size of system ) in that state .",
    "we indicate the probability measure which describes such a situation by @xmath54 , that is the gibbs measure restricted to state @xmath53 . if configurations are sampled from one state , then the expression for the log - likelihood in ( [ logl ] )",
    "should be corrected by replacing @xmath55 with @xmath56 , the free energy of state @xmath53 .",
    "still the log - likelihood is a concave function of its arguments and so there is a unique solution to this problem .",
    "it is well known that the bethe approximation is asymptotically exact in the ergodic phase ( @xcite ) . in this case , the gibbs weight can be approximately expressed in terms of one- and two- point marginals @xmath57 , @xmath58 as follows : @xmath59 the above equation is exact only asymptotically ( on a replica - symmetric system ) ; it can be used for inference in at least two ways : the simplest one is by replacing @xmath57 and @xmath58 in the above expression by their experimental estimation ( given as input of the inference process ) , equating ( [ eq : pbethe ] ) to ( [ gibbs ] ) and solving for @xmath31 and @xmath32 .",
    "this is known as the `` independent pairs '' approximation .",
    "a second one , often more precise but computationally more involved , is to search for a set of @xmath60 and a corresponding @xmath60-fixed point of bp equations , such that the bethe estimation @xmath61 , @xmath62 of the two- and one - point marginals match the experimental input as accurately as possible .    in a non - ergodic phase ,",
    "it is known however that bp equations typically do not converge or have multiple fixed points .",
    "this is normally attributed to the fact that the bp hypothesis of decorrelation of cavity marginals fails to be true .",
    "when a bp fixed point is attained , it is believed to approximate marginals inside a single state ( and not the full gibbs probability ) , as the decorrelation hypothesis are satisfied once statistics are restricted to this state @xcite .",
    "the fact that bp solutions correspond to restriction to subsets of the original measure may suggest that there is little hope in exploiting ( [ eq : pbethe ] ) on such systems .",
    "fortunately , this is not the case . for every finite system , and every bp fixed point @xmath53 the following holds ,",
    "@xmath63 } \\prod_i \\pi^\\alpha_i(\\sigma_i )      \\prod_{i < j}\\frac{\\pi^\\alpha_{ij}(\\sigma_i,\\sigma_j)}{\\pi^\\alpha_i(\\sigma_i)\\pi^\\alpha_j(\\sigma_j)}. \\label{eq : pbethe2}\\ ] ] a proof of a more general statement will be given in appendix [ app - bp ] .",
    "as in the ergodic case , ( [ eq : pbethe2 ] ) can be exploited in at least two ways : one is by replacing @xmath64 and @xmath65 by their experimental estimation inside a state and solving for @xmath60 the identity between ( [ eq : pbethe2 ] ) and ( [ gibbs ] ) , exactly like in the independent pairs approximation , as if one just forgets that the samples come from a single ergodic component .",
    "a second one is by inducing bp equations to converge on fixed points corresponding to appropriate ergodic components . in this paper",
    "we will take the latter option .",
    "please notice that the second method , as an algorithm , is more flexible with respect to the first one ; indeed , there is no reason to have a bp fixed point for any set of experimental data , especially when the number of samples is not too large .",
    "it means that matching exactly the data with those of a bp fixed point is not always possible .",
    "therefore , a better strategy would be to find a good bp solution which is close enough to the experimental data .    ignoring the information that our samples come from a single ergodic component would result in a large inference error due to the maximization of the wrong likelihood .",
    "as an example , we take a tree graph with ising spins interacting through random couplings @xmath66 , in zero external fields @xmath67 .",
    "choose an arbitrary pattern @xmath68 and fix a fraction @xmath69 of the boundary spins to the values in @xmath68 .",
    "for @xmath70 the system would be in paramagnetic phase for any finite @xmath71 , therefore the average overlap of the internal spins with the pattern would be zero .",
    "on the other hand , for @xmath72 and low temperatures the overlap would be greater than zero , as expected from a localized gibbs state around pattern @xmath68 . in this case the observed magnetizations are nonzero and without any information about the boundary condition we may attribute these magnetizations to external fields which in turn result to a large inference error in the couplings .",
    "equivalently , we could put the boundary spins free but restrict the spin configurations to a subspace , for instance a sphere of radius @xmath73 centered at pattern @xmath68 in the configuration space @xmath74 .",
    "that is , the system follows the following measure : @xmath75 where @xmath76 is an indicator function which selects configurations in the subspace @xmath74 . by the bp approximation",
    "we can compute the magnetizations @xmath77 and the correlations @xmath78 , see appendix [ app - bpd ] for more details",
    ". taking these as experimental data , we may perform the inference by assuming that our data represent the whole configuration space .",
    "this again would result to a large inference error ( for the same reason mentioned before ) whereas taking into account that the system is limited to @xmath74 , we are able to infer the right parameters by maximizing the correct likelihood ; i.e. replacing the total free energy @xmath55 in the log - likelihood with @xmath79 , the free energy associated to subspace @xmath74 .    in figure",
    "@xmath80 we display the inference error obtained by ignoring the prior information in the above two cases .",
    "notice that in principle the error would be zero if we knew @xmath74 and the boundary condition .",
    "as it is seen in the figure , the error remains nonzero when the boundary spins are fixed ( @xmath72 ) even if sampling is performed over the whole space .",
    "inference error on a cayley tree when the leaves are free ( @xmath70 ) or fixed ( @xmath72 ) to random configuration @xmath68 .",
    "the data come from subspace @xmath74 ( a sphere of radius @xmath73 centered at @xmath81 ) . in the inference algorithm",
    "we ignore the boundary condition and that samples are restricted to @xmath74 .",
    "the internal nodes have degree @xmath82 and size of the tree is @xmath83.,width=377 ]",
    "the hopfield model can be found in three different phases . for large temperatures",
    "the system is in the paramagnetic phase where , in the absence of external fields , magnetizations @xmath45 and overlaps @xmath84 are zero . if the number of patterns is smaller than the critical value @xmath85 , for small temperatures the system enters the memory phase where the overlap between the patterns and the configurations belonging to states selected by the initial conditions can be nonzero . for @xmath86 , the hopfield model at low temperature",
    "enters in a spin glass phase , where the overlaps are typically zero . in fully connected graphs @xmath87 and in random poissonian graphs @xmath88 where @xmath89 is the average degree @xcite .",
    "take the hopfield model with zero external fields and in the memory phase .",
    "we measure samples from a glauber dynamics which starts from a configuration close to a pattern @xmath90",
    ". the system will stay for a long time in the state @xmath90 and is thus well described by the restricted gibbs measure @xmath91 . in a configuration @xmath26 , the local field seen by neuron @xmath23 is @xmath92 . if @xmath90 corresponds to the retrieved pattern , the first term ( signal ) would have the dominant contribution to @xmath93 . the last term ( noise )",
    "is a contribution of the other patterns to the local field . to exploit this information",
    ", we look for a set of couplings that result to a gibbs state equivalent to the observed state of the system . one way to do",
    "this is by introducing an auxiliary external field pointing to the experimental magnetizations , i.e. @xmath94 , for a positive @xmath95 ; we may set the couplings @xmath12 at the beginning to zero and compute our estimate of the correlations @xmath46 by the bp algorithm .",
    "this can be used to update the couplings by a small amount in the direction that maximizes the likelihood , as in ( [ update - j ] ) ( we do not update the external fields which for simplicity are assumed to be zero ) .",
    "this updating is repeated iteratively , decreasing @xmath95 by a small amount in each step .",
    "the procedure ends when @xmath95 reaches the value zero .",
    "the auxiliary field is introduced only to induce convergence of the equations towards a fixed point giving statistics inside a particular state .",
    "figure [ f2 ] compares the inference error obtained with the above procedure for several values of temperature in the memory phase . for the parameters in the figure , the inferred couplings from one basin were enough to recover the other two patterns . in the figure",
    "we also see how the error decreases by taking larger number of samples from the system .",
    "inference error versus number of samples for different temperatures .",
    "the data are extracted from one pure state of the hopfield model in the memory phase ( @xmath96 ) .",
    "size of the system is @xmath97 , each spin interacts with @xmath98 other randomly selected spins , and number of stored patterns is @xmath99 . in the inference algorithm we use @xmath100.,width=377 ]    in general we may have samples from different states of a system .",
    "let us assume that in the hopfield model we stored @xmath19 patterns by the hebb rule but the samples are from @xmath101 basins .",
    "the estimated correlations @xmath102 in any state @xmath103 should be as close as possible to the experimental values @xmath104 .",
    "a natural generalization of the previous algorithm is the following : as before we introduce external fields @xmath105 for each state @xmath106 . at fixed positive @xmath95",
    "we compute the estimated bp correlations for different states .",
    "each of these estimations can be used to update the couplings as in the single state case",
    ". specifically , this amounts to make a single additive update to the couplings by the average vector @xmath107 given by @xmath108 , where @xmath109 and @xmath110 . indeed , the addends of @xmath111 will be typically linearly independent , so @xmath112 will imply @xmath113 for @xmath114 .",
    "we then decrease @xmath95 and do the bp computation and update steps .",
    "again we have to repeat these steps until the external field goes to zero .",
    "figures [ f3 ] and [ f4 ] show how the inference error changes with sampling from different states .",
    "notice that if we had an algorithm that returns exact correlations , an infinite number of samples from one state would be enough to infer the right interactions in the thermodynamic limit .",
    "however , given that we are limited by the number of samples , the learning process is more efficient if this number is taken from different states instead of just one .",
    "evolution of the inference error with update iterations .",
    "the data obtained by sampling from one or several pure states of the hopfield model in the memory phase ( @xmath96 ) .",
    "the total number of samples in each case is @xmath115 .",
    "size of the system is @xmath97 , each spin interacts with @xmath98 other randomly selected spins , and number of stored patterns is @xmath99 . in the inference algorithm we use @xmath100.,width=377 ]     inference error and number of states which are stable and highly correlated with the patterns after sampling from @xmath101 pure states of the hopfield model in the memory phase ( @xmath96 ) .",
    "size of the system is @xmath97 , each spin interacts with @xmath116 other randomly selected spins , and number of stored patterns is @xmath117 . in the inference algorithm we use @xmath100 and number of samples",
    "is @xmath118.,width=377 ]",
    "hebbian learning is a stylized way of representing learning processes . among the many oversimplifications that it involves",
    "there is the fact that patterns are assumed to be presented to the networks through very strong biasing signals . on the contrary it is of biological interest to consider the opposite limit where only weak signals are allowed and retrieval takes place with sizable amount of errors . in the spin language we are thus interested in the case in which the system is only slightly biased toward the patterns during the learning phase . in what follows",
    "we show that one can `` invert '' the inference process discussed in the previous sections and define a local learning rule which copes efficiently with this problem .    as first step",
    "we consider a learning protocol in which the patterns are presented sequentially and in random order to the system by applying an external field in direction of the pattern , that is a field @xmath119 with @xmath120 .",
    "we assume that initially all couplings are zero .",
    "depending on the strength of the field , the system will be forced to explore configurations at different overlaps with the presented pattern @xmath106 .",
    "a small @xmath95 corresponds to a weak or noisy learning whereas for large @xmath95 the system has to remain very close to the pattern .",
    "what is a small or large @xmath95 , of course depends on the temperature and strength of the couplings .",
    "here we assumed the couplings are initially zero , so @xmath121 defines the boundary between weak and strong fields .",
    "the learning algorithm should indeed force the system to follow a behavior that is suggested by the auxiliary field .",
    "therefore , it seems reasonable if we try to match the correlations in the two cases : in absence and presence of the field .",
    "notice to the similarities and differences with the first part of the study . as before we are to update the couplings according to deviations in the correlations .",
    "but , here the auxiliary field is necessary for the learning ; without that the couplings would not be updated anymore .",
    "moreover , it is obvious that we can not match exactly the correlations in absence and presence of an external field .",
    "we just push the system for a while towards one of the patterns to reach a stationary state in which all the patterns are remembered .    for any @xmath95 we can compute the correlations @xmath122 by either sampling from the glauber dynamics or by directly running bp , with external fields @xmath119 . at the same time",
    "we can compute correlations @xmath102 by the bp algorithm in zero external fields and with initial messages corresponding to pattern @xmath106 .",
    "then we try to find couplings which match the correlations in the two cases , namely we update the couplings by a quantity @xmath123 .",
    "the process is repeated for all couplings and for @xmath124 iterations with the same pattern @xmath106 .",
    "next we switch to some other randomly selected pattern @xmath90 and the whole process is repeated for @xmath125 learning steps .",
    "notice that here @xmath95 is fixed from the beginning .",
    "the above learning protocol displays a range of interesting phenomena .",
    "firstly one notices that for @xmath126 ( i.e. for very large external fields ) and @xmath127 ( i.e. for very high temperature or isolated neurons ) the above learning results to the hebb couplings of the hopfield model . in figure [ f5 ]",
    "we compare the histogram of learned couplings for small and large @xmath95 with the hebbian ones .     comparing the histogram of hebbian couplings with that of learned couplings for small and large values of the external field .",
    "size of the system is @xmath97 , each spin interacts with @xmath128 other randomly selected spins . here",
    "we are to store @xmath117 random and uncorrelated patterns . in the learning algorithm",
    "we use @xmath96 and @xmath129.,width=377 ]    the number of patterns @xmath130 which are highly correlated with stable configurations depends on the strength of external fields .",
    "we consider that pattern @xmath106 is `` learned '' if there is a gibbs state with nonzero overlap @xmath131 that is definitely larger than the other ones @xmath132 .",
    "figures [ f6 ] and [ f7 ] show how these quantities evolve during the learning process and by increasing the magnitude of external filed .",
    "evolution of the average overlap and fraction of successfully learned patterns in the learning algorithm .",
    "size of the system is @xmath97 , each spin interacts with @xmath128 other randomly selected spins . in the learning algorithm we set @xmath133 , @xmath96 , @xmath129 , and number of patterns that are to store is @xmath117.,width=377 ]     average number of successfully learned patterns in the learning algorithm for different values of @xmath95 .",
    "size of the system is @xmath97 , each spin interacts with @xmath134 other randomly selected spins .",
    "the learning algorithm works at @xmath135 , and number of patterns that are to store is @xmath136 .",
    "the average is taken over @xmath137 realizations of the patterns.,width=377 ]    for small @xmath19 nearly all patterns are learned , whereas , for larger @xmath19 some patterns are missing .",
    "a large number of patters can thus be learned at the price of smaller overlaps and weaker states .",
    "that is , the average overlap in successfully learned patterns decreases continuously by increasing @xmath19 , approaching the paramagnetic limit . in figure [ f8 ]",
    "we compare this behavior with that of hebb couplings .",
    "as the figure shows , there is a main qualitative difference between hebbian learning of the hofield model and the protocol discussed here . in the former case when the number of stored patterns exceeds some critical value the systems enters in a spin glass phase where all memories are lost and the bp algorithm does not converge anymore . on the contrary , in our case many patterns can be stored without ever entering the spin glass phase ( for a wide range of choices of @xmath95 ) .",
    "the bp algorithm always converges , possibly to a wrong fixed point if the corresponding pattern is not stored .",
    "average number of successfully learned patterns in the learning algorithm and hebb rule versus @xmath19 , the number of patterns that are to store .",
    "the inset shows the average overlap .",
    "size of the system is @xmath97 , each spin interacts with @xmath134 other randomly selected spins .",
    "the learning algorithm works at @xmath138 . the average is taken over @xmath137 realizations of the patterns.,width=377 ]      population dynamics",
    "is usually used to obtain the asymptotic and average behavior of quantities that obey a set of deterministic or stochastic equations @xcite .",
    "for instance , to obtain the phase diagram of the hopfield model with population dynamics one introduces a population of @xmath139 messages representing the bp cavity messages in a reference state , e.g. pattern @xmath140 @xcite .",
    "then one updates the messages in the population according to the bp equations : at each time step a randomly selected cavity message is replaced with a new one computed by @xmath141 randomly selected ones appearing on the r.h.s . of the bp equations .",
    "in each update , one generates the random couplings @xmath142 by sampling the other @xmath143 random patterns .",
    "after a sufficiently large number of updates , one can compute the average overlap with the condensed pattern @xmath90 to check if the system is in a memory phase .",
    "the stability of condensed state would depend on the stability of the above dynamics with respect to small noises in the cavity messages .",
    "if @xmath139 is large enough , one obtains the phase diagram of hopfield model in the thermodynamic limit averaged over the ensemble of random regular graphs and patterns . we used the above population dynamics to obtain the phase diagram of the hopfield model on random regular graphs ,",
    "see figure [ f9 ] .",
    "the phase diagram of hopfield model on random regular graphs of degree @xmath98 obtained with population dynamics ( @xmath144 ) .",
    "horizontal axes is number of patterns @xmath19 and vertical axes is temperature @xmath20 .",
    "the paramagnetic , memory and spin glass phases are labeled with @xmath145 and @xmath146 , respectively.,width=377 ]    in order to study the new learning protocol we need a more sophisticated population dynamics .",
    "the reason is that in contrast to hebb couplings , we do not know in advance the learned couplings . in appendix [ app - pop ]",
    "we explain in more details the population dynamics that we use to analyze the learning process studied in this paper . the algorithm is based on @xmath19 populations of bp messages and one population of couplings .",
    "these populations represent the probability distributions of bp messages in different states and couplings over the interaction graph . for a fixed set of patterns",
    "@xmath147 we update the populations according to the bp equations and the learning rule , to reach a steady state .",
    "figure [ f10 ] displays the histogram of couplings obtained in this way . in the figure we compare two cases of bounded and unbounded couplings . in the first case",
    "the couplings should have a magnitude less than or equal to @xmath148 whereas in the second case they are free to take larger values .",
    "we observe a clear difference between the two cases ; when @xmath95 is small , the couplings are nearly clipped in the bounded case whereas the unbounded couplings go beyond @xmath149 . however , in both cases there is some structure in the range of small couplings . increasing the magnitude of @xmath95 we get more and more structured couplings .",
    "for very large fields they are similar to the hebb couplings . for small fields",
    "the histogram of the couplings is very different from the hebb one , though the sign of the learned and the hebbian couplings is the same .",
    "there are a few comments to mention here ; in the population dynamics we do not have a fixed graph structure and to distinguish @xmath19 patterns from each other we have to fix them at the beginning of the algorithm .",
    "moreover , we have to modify the bp equations to ensure that populations are representing the given patterns , see appendix [ app - pop ] . and",
    "finally the outcome would be an average over the ensemble of random regular graphs , for a fixed set of patterns .",
    "having the stationary population of couplings , one can check the stability of each state by checking the stability of the bp equations at the corresponding fixed point .",
    "the maximum capacity that we obtain in this way for the learned couplings is the same as the hebb one whereas on single instances we could store much larger number of patterns .",
    "the reason why we do not observe this phenomenon in the population dynamics resides in the way that we are checking stability ; the fixed patterns should be stable in the ensemble of random regular graphs .",
    "in other words , checking for stability in the population dynamics is stronger than checking it in a specific graph .",
    "the main result of our analysis consists in showing that the distribution of the couplings arising from the bp learning protocol is definitely different from the hebbian one .",
    "the histogram of learned couplings obtained by the population dynamics in random regular graphs of degree @xmath128 .",
    "number of patterns that are to store is @xmath117 . in the upper panel",
    "we compare the two cases of learning with bounded and unbounded couplings for a small external field . in the lower panel",
    "we compare the hebb rule with the learning algorithm for a large external field . in the algorithm we use @xmath150 , @xmath151 , and @xmath152.,width=377 ]",
    "we studied the finite connectivity inverse hopfield problem at low temperature , where the data are sampled from a non - ergodic regime .",
    "we showed that the information contained in the fluctuations within single pure states can be used to infer the correct interactions .",
    "we also used these findings to design a simple learning protocol which is able to store patterns learned under noisy conditions .",
    "surprisingly enough it was possible to show that by demanding a small though finite overlap with the patterns it is possible to store a large number of patterns without ever reaching a spin glass phase .",
    "the learning process avoids the spin glass phase by decreasing the overlaps , as the number of patterns increases . a separate analysis which is similar to the one presented in ref .",
    "@xcite ( and not reported here ) shows that the equations can be heavily simplified without loosing their main learning capabilities .    in this paper we focused on a simple model of neural networks with symmetric couplings .",
    "it would be interesting to study more realistic models like the integrate and fire model of neurons with general asymmetric couplings .",
    "moreover , instead of random unbiased patterns one may consider sparse patterns which are more relevant in the realm of neural networks .",
    "the arguments presented in this paper can also be relevant to problem of inferring a dynamical model for a system by observing its dynamics . in this case , a system is defined solely by its evolution equations and one can not rely on the boltzmann equilibrium distribution . still it is possible to try to infer the model by writing the likelihood for the model parameters given the data and given the underlying dynamical stochastic process .",
    "a mean - field approach has been recently described in @xcite .",
    "we actually checked this approach in our problem and observed qualitatively the same behavior as the static approach .",
    "in fact , which method is best heavily depends on the type of data which are available .",
    "the work of was partially supported by a _",
    "programma neuroscienze _ grant by the compagnia di san paolo and the ec grant 265496 .",
    "a @xmath153 limit version of this result ( except the determination of the value of the constant @xmath154 ) appeared in @xcite .",
    "this result is valid for general ( non - zero ) interactions . for a family of `` potentials '' @xmath155 , where we denote by @xmath156 the subvector of @xmath157 given by @xmath158 .",
    "we will use the shorthand @xmath159 or @xmath160 to mean @xmath161 .    _",
    "proposition_. given a factorized probability function @xmath162 and a bp fixed point @xmath163 and plaquette marginals @xmath164 and single marginals @xmath165 for every @xmath160 , then @xmath166    _ proof_. using the fact that @xmath167 and @xmath168 , we obtain @xmath169 , then using the definitions :    @xmath170    this proves that a fixed point can be interpreted as a form of reparametrization of the original potentials .",
    "in fact , a sort of converse also holds :    _ proposition _ : if @xmath171 satisfies a bethe - type expression    @xmath172    with @xmath173 for every @xmath159 .",
    "then there exists a bp fixed point @xmath163 such that @xmath174 .",
    "_ proof _ : choose any configuration @xmath175 .",
    "we will use the following notation : @xmath176 , and @xmath177 . define @xmath178 , normalized appropriately .",
    "afterwards , we can define @xmath179 .    by definition of @xmath180 we have @xmath181 . similarly , but using ( [ eq : bethe3 ] ) , and noting by @xmath182 , we have also @xmath183 .",
    "then @xmath184 , and thus @xmath185 .",
    "this also implies that @xmath186 , proving that the first bp equation is satisfied .    by definition of @xmath180 , @xmath187 where @xmath188 . moreover using ( [ eq : bethe3 ] )",
    ", we can conclude that also @xmath189 where @xmath190 this implies that @xmath191 .",
    "but we also have that @xmath192 , so @xmath174 as desired .",
    "now @xmath193 by hypothesis , so @xmath194 and this proves that the second bp equation is also satisfied .",
    "consider the ising model on a tree graph of size @xmath13 with couplings @xmath31 and external fields @xmath32 .",
    "suppose that we are given a reference point @xmath68 in the configuration space @xmath195 and the following measure @xmath196 where @xmath74 is a sphere of radius @xmath73 centered at @xmath68 .",
    "by distance of two configurations we mean the hamming distance , i.e. number of spins which are different in the two configurations .",
    "the aim is to compute thermodynamic quantities like average magnetizations and correlations in an efficient way .",
    "we do this by means of the bethe approximation and so bp algorithm .",
    "first we express the global constraint @xmath197 as a set of local constraints by introducing messages @xmath198 that each node sends for its neighbors . for a given configuration @xmath26",
    ", @xmath199 denotes the distance of @xmath26 from @xmath68 in the cavity graph @xmath200 which includes @xmath23 and all nodes connected to @xmath41 through @xmath23 . with these new variables we can write bp equations as @xmath201 where @xmath202 is an indicator function to check the constrains on @xmath203 and @xmath204 . starting from random initial values for the bp messages we update them according to the above equation .",
    "after convergence the local marginals read @xmath205 where in @xmath206 we check if @xmath207 .",
    "these marginals will be used to compute the average magnetizations and correlations .",
    "notice that when the graph is not a tree we need to pass the messages @xmath198 only along the edges of a spanning tree ( or chain ) which is selected and fixed at the beginning of the algorithm .",
    "consider @xmath19 patterns @xmath208 , where @xmath209 and @xmath210 goes from @xmath148 to @xmath211 , which is equivalent to the size of system .",
    "the patterns , learning rate @xmath212 and parameter @xmath95 are fixed at the beginning of the algorithm . to each patten",
    "we assign a population of messages @xmath213 where @xmath214 ( @xmath15 is the node degree ) .",
    "these are to represent the normalized bp messages that we use in the learning algorithm .",
    "besides this we have also a population of couplings @xmath215 .",
    "notice to the maximum we are taking in the last step .",
    "this is to ensure that bp messages in population @xmath106 are related to pattern @xmath223 .",
    "we do these updates for @xmath224 iterations , where in each iteration all members of a population are updated in a random sequential way ."
  ],
  "abstract_text": [
    "<S> we discuss how inference can be performed when data are sampled from the non - ergodic phase of systems with multiple attractors . </S>",
    "<S> we take as model system the finite connectivity hopfield model in the memory phase and suggest a cavity method approach to reconstruct the couplings when the data are separately sampled from few attractor states . </S>",
    "<S> we also show how the inference results can be converted into a learning protocol for neural networks in which patterns are presented through weak external fields . </S>",
    "<S> the protocol is simple and fully local , and is able to store patterns with a finite overlap with the input patterns without ever reaching a spin glass phase where all memories are lost . </S>"
  ]
}