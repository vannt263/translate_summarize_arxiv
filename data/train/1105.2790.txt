{
  "article_text": [
    "a common goal in machine learning is to design a device able to reproduce a given system , namely to estimate the probability distribution of its possible states @xcite . when a satisfactory model of the system is not available , and its underlying principles are not known , this goal can be achieved by the observation of a large number of samples @xcite . a well studied example is the visual world , the problem of estimating the probability of all possible visual stimuli @xcite .",
    "a fundamental ability for the survival of living organisms is to predict which stimuli will be encountered and which are more or less likely to occur . on this purpose ,",
    "the brain is believed to develop an internal model of the visual world , to estimate the probability and respond to the occurrence of various events @xcite,@xcite .",
    "ising - type neural networks have been widely used as generative models of simple systems @xcite,@xcite .",
    "those models update the synaptic weights between neurons according to a specific learning rule , depending on the neural activity driven by a given set of observations ; after learning , the network is able to generate a sequence of states whose probabilities match those of the observations .",
    "popular examples of ising models , characterized by a quadratic energy function and a boltzmann distribution of states , are the hopfield model @xcite@xcite and boltzmann machines ( bm ) @xcite .",
    "boltzmann machines ( bm ) have been designed to capture the complex statistics of arbitrary systems by dividing neurons in two subsets , visible and hidden units : marginalizing the boltzmann distribution over the hidden units allows the bm to reproduce , through the visible units , arbitrarily complex distributions of states , by learning the appropriate synaptic weights @xcite .",
    "state of the art feature detectors and classifiers implement a specific type of bm , the restricted boltzmann machine ( rbm ) , because of its efficient learning algorithms @xcite .",
    "the rbm is characterized by a bipartite topology in which hidden and visible units are coupled , but there is no interaction within either set of visible or hidden units @xcite .",
    "left panel : schematic representation of a hybrid boltzmann machine ( hbm ) where the hidden units are analog ( @xmath5 variables ) and the visible units are digital ( @xmath6 variables ) .",
    "the two sets of hidden units , @xmath7 and @xmath8 , represent two feature sets that are both connected to the layer of visible units @xmath6 .",
    "the layers of hidden and visible units are reciprocally connected , but there are no intra - layer connections , thus forming a bipartite topology .",
    "right panel : schematic representation of the equivalent hopfield neural network built upon the visible units only , with an internal fully connected structure.,title=\"fig : \" ]   left panel : schematic representation of a hybrid boltzmann machine ( hbm ) where the hidden units are analog ( @xmath5 variables ) and the visible units are digital ( @xmath6 variables ) .",
    "the two sets of hidden units , @xmath7 and @xmath8 , represent two feature sets that are both connected to the layer of visible units @xmath6 .",
    "the layers of hidden and visible units are reciprocally connected , but there are no intra - layer connections , thus forming a bipartite topology .",
    "right panel : schematic representation of the equivalent hopfield neural network built upon the visible units only , with an internal fully connected structure.,title=\"fig : \" ]    all neurons of rbm s are binary , both the visible and the hidden units .",
    "the analog equivalent of rbm , the restricted diffusion networks , have all analog units and have been described in @xcite@xcite . here",
    "we study the case of a `` hybrid '' boltzmann machine ( hbm ) , in which the hidden units are analog and the visible units are binary ( fig.@xmath9 left ) . we show that the hbm , when marginalized over the hidden units , is equivalent to a hopfield network ( fig.@xmath9 right ) , where the @xmath0 visible units are the neurons and the @xmath1 hidden units are the learned patterns .",
    "although the hopfield network can generate probability distributions in a limited space , it has been widely studied for its associative and retrieval properties .",
    "the exact mapping proven here introduces a new way to simulate hopfield networks , and allows a novel interpretation of the spin glass transition , which translates into an optimal criterion for selecting the relative size of the hidden and visible layers in the hbm .",
    "we use the method of stochastic stability to study the thermodynamics of the system in the case of analog synapses .",
    "this method has been previously described in @xcite , @xcite , and offers an alternative approach to the replica trick for studying ising - type neural networks , including the hopfield model and the hbm .",
    "we analyze the model with two non - interacting sets of hidden units in the hbm , which corresponds to two sets of uncorrelated patterns in the hopfield network , and study the thermodynamics with the assumption of replica symmetry .",
    "we extend the theory to cope with two sets of interconnected hidden layers , coresponding to sets of correlated patterns , and we show that their interaction acts as a noise source for retrieval .",
    "we define a `` hybrid '' boltzmann machine ( hbm , see fig .",
    "@xmath9 left ) as a network in which the activity of units in the visible layer is discrete , @xmath10 ( digital layer ) , and the activity in the hidden layer is continuous ( analog layer ) .",
    "the layers of hidden and visible units are reciprocally connected , but there are no intra - layer connections , thus forming a bipartite topology .",
    "we assume that the layer of hidden units is further divided into two sets , both described by continuous variables , @xmath11 .",
    "we will consider the case of interacting hidden units ( connections between @xmath7 and @xmath8 ) in the next section . in order to maintain a parsimonious notation , in this section we consider a single hidden layer , e.g. only the layer defined by the variables @xmath7 .",
    "the synaptic connections between units in the two layers are fixed and symmetric , and are defined by the synaptic matrix @xmath12 .",
    "the input to unit @xmath13 in the visible ( digital ) layer is the sum of the activities in the hidden ( analog ) layer weighted by the synaptic matrix , i.e. @xmath14 . the input to unit @xmath15 in the hidden ( analog ) layer is the sum of the activities in the visible ( digital ) layer , weighted by the synaptic matrix , i.e. @xmath16 . in the following ,",
    "we denote by @xmath7 the set of all hidden @xmath17 variables , and by @xmath6 the set of all visible @xmath18 variables .",
    "the dynamics of the activity is different in the two layers ; in the analog layer it changes continuously in time , while in the digital layer it changes in discrete steps .",
    "the activity in the hidden ( analog ) layer follows the stochastic differential equation @xmath19 where @xmath20 is a white gaussian noise with zero mean and covariance @xmath21 .",
    "the parameter @xmath22 quantifies the timescale of the dynamics , and the parameter @xmath23 determines the strength of the fluctuations .",
    "the first term in the right hand side is a leakage term , the second term is the input signal and the third term is a noise source . since noise is uncorrelated between different hidden units , they evolve independently .",
    "eq.([langevin ] ) describes an ornstein - uhlembeck diffusion process @xcite and , for fixed values of @xmath6 , the equilibrium distribution of @xmath15 is a gaussian distribution centered around the input signal , which is equal to    @xmath24\\ ] ]    in order for this equilibrium distribution to hold , the activity of digital units @xmath6 must be constant , while in fact it depends on time .",
    "however , we assume that the timescale of diffusion @xmath22 is much faster than the rate at which the digital units are updated .",
    "therefore , a different equilibrium distribution for @xmath7 , characterized by different values of @xmath6 , holds between each subsequent update of @xmath6 . since hidden units are independent , their joint distribution is the product of individual distributions , i.e. @xmath25 .    the activity in the visible ( digital ) layer follows a standard glauber dynamics for ising - type systems @xcite . at a specified sequence of time intervals ( much larger than @xmath22 ) , the activity of units in the digital layer is updated randomly according to a probability that depends on their input . while updating the digital units @xmath6 , the analog variables @xmath7 are fixed , namely the update of digital units is instantaneous .",
    "the activity of a unit @xmath13 is independent on other units , and the probability is a logistic function of its input , i.e.    @xmath26}{\\exp[\\beta \\sum_\\mu\\xi_i^\\mu z_\\mu]+\\exp[-\\beta \\sum_\\mu\\xi_i^\\mu z_\\mu]}\\ ] ]    note that this distribution is normalized , namely @xmath27 . since visible units are independent ,",
    "their joint distribution is the product of individual distributions , i.e. @xmath28 .    given the conditional distributions of either layers , eqs.([cond1],[glauber ] ) , we can determine their joint distribution , @xmath29 , and the marginal distributions @xmath30 and @xmath31 , apart from a normalization factor .",
    "we use bayes rule , @xmath32 , and we use the fact that marginal distributions depend on single layer variables .",
    "the result is , for the joint distribution    @xmath33    the marginal distribution of the visible units is equal to    @xmath34\\ ] ]    as explained in more detail in the next section , this probability distribution is equal to the distribution of a hopfield network , where the synaptic weights of the hopfield network are given by the expression in round brackets .",
    "the stored patterns of the hopfield model corresponds to the synaptic weights of the hbm , described by the @xmath35 variables , and the number of patterns corresponds to the number @xmath1 of hidden units .",
    "therefore , we have shown that the hbm and hopfield network admit the same probability distribution , once the hidden variables of the hbm are marginalized , and the hbm and hopfield network are statistically equivalent .",
    "in other words , a configuration @xmath6 in the hopfield network has the same probability as the same configuration @xmath6 in the hbm , when averaged over the hidden configurations @xmath7 .",
    "retrieval in the hopfield network corresponds to the case in which the hbm learns to reproduce a specific pattern of neural activation .",
    "the maximum number of patterns @xmath1 that can be retrieved in a hopfield network is known @xcite , and is equal to @xmath36 . if the number of patterns exceeds this limit , the network is not able to retrieve any of them . on the other hand ,",
    "if the hbm has a very large number @xmath1 of hidden variables , this provokes over - fitting in learning the observed patterns , and the hbm is not able to reproduce the statistics of the observed system .",
    "the correspondence between hopfield network and hbm allows recognizing that the maximum number of hidden variables in the hbm is @xmath36 .",
    "we check this prediction by numerical simulations of the hbm .",
    "we pick each element of the synaptic matrix @xmath12 independently from a bernouilli distribution , @xmath37 or @xmath38 with @xmath39 probability ( the scaling with @xmath0 is imposed for comparison to the original hopfield model ) .",
    "we set the number of neurons in the visible layer as @xmath40 , the timescale of dynamics of hidden units is @xmath41 , and we use @xmath22 as a reference time unit . in each simulation , we update the visible units every @xmath42 and we run the simulation for @xmath43 , therefore performing one thousand updates of the visible units .",
    "we simulate the dynamics of hidden units by standard numerical methods for stochastic differential equations and using a time step of 0.01 t . in different simulations we vary the values of the noise amplitude by manipulating @xmath44 , and the number of hidden units @xmath45 .",
    "we observe the overlap of the activity of visible units with each one of the pattern @xmath46 by computing @xmath47 , such that overlap equal to one for some value of @xmath46 implies that visible units precisely align to that pattern @xmath46 . in each simulation , we initialize the hidden units at random and the visible units exactly aligned to one of the patterns .",
    "fig.2 shows the results of simulations , the dynamics of the overlap of visible units with all patterns for different values of the parameters @xmath23 and @xmath1 . for high noise , @xmath48 ,",
    "no retrieval is possible and all overlaps are near zero regardless of the number of hidden units @xmath1 . for intermediate noise ,",
    "@xmath49 , retrieval is possible provided that the number of hidden units is not too large .",
    "the prediction of the hopfield network is that retrieval is lost at about @xmath50 @xcite , accurately matching our findings . for low noise , @xmath51 , retrieval",
    "is maintained up to large values of @xmath1 . in the low noise regime ,",
    "the hopfield network can retrieve a number of patterns near its maximum , i.e. @xmath52 , which again matches well with the results of our simulations .",
    "dynamics of the overlap of visible units with all patterns for different values of the parameters @xmath23 and @xmath1 .",
    "simulations run for @xmath53 units of time , which corresponds to @xmath53 updates of the visible units .",
    "thick blue line in each panel shows the overlap of visible units with the pattern imposed by the initial condition , other lines show the overlaps with all other patterns .",
    "no retrieval is observed ( overlap@xmath54 ) for high noise , @xmath48 , while for intermediate @xmath49 and low noise @xmath51 retrieval is possible ( overlap@xmath55 ) for a small number of hidden units ( patterns ) @xmath1 .",
    "results of simulations match with the theory of hopfield networks .",
    ", scaledwidth=70.0% ]",
    "in canonical statistical mechanics , a system is described by the probability distribution of each one of its possible states . in the hbm ,",
    "a given state is associated with its probability according to the boltzmann distribution .",
    "this distribution is expressed by eq.([seiuncane ] ) , which we rewrite while reintroducing the variables @xmath8 dropped in previous section and by defining the hamiltonian function @xmath56 we denote by @xmath35 the set of all @xmath57 variables , and by @xmath58 the set of all @xmath59 variables , where @xmath60 is the synaptic matrix for the connections with the @xmath8 layer .",
    "the boltzmann distribution depends on the parameters @xmath61 , and its expression includes the normalization factor @xmath62 :    @xmath63z(\\beta,\\xi,\\eta)^{-1}\\ ] ]    the partition function @xmath62 corresponds to the normalization factor of the boltzmann distribution , and is defined as @xmath64 using this definition of the partition function , it is straightforward to show that the bolzmann distribution , eq.([boltzdist ] ) , is normalized . in order to marginalize the hidden variables , we use the following identity , the gaussian integral :    @xmath65 } = \\sqrt{\\frac{2\\pi}{\\beta}}\\exp{\\left(\\beta\\frac{a^2}{2}\\right)}.\\ ] ]    using this identity , we marginalize the analog variables @xmath7 and @xmath8 in eq.([partitionz ] ) , and we obtain    @xmath66    where we define the following hamiltonian : @xmath67 this is the hamiltonian of a hopfield neural network .",
    "this result connects the two hamiltonians of the hopfield network and the boltzmann machine and states that thermodynamics obtained by the first cost function , eq.([hamilton1 ] ) , is the same as the one obtained by the second one , eq.([monopartito ] ) .",
    "this offers a connection between retrieval through free energy minimization in the hopfield network and learning through log - likelihood estimation in the hbm @xcite@xcite .",
    "note that observable quantities stemmed from hbm are equivalent in distribution , and not pointwise , to the corresponding ones in the hopfield network .",
    "next , we calculate the free energy , which allows determining the value of all relevant quantities and the different phases of the system .",
    "the thermodynamic approach consists in averaging all observable quantities over both the noise and the configurations of the system .",
    "therefore , we define two different types of averaging , the average @xmath68 over the state configurations @xmath69 , and the average @xmath70 over the synaptic weights ( quenched noise ) @xmath71 . note that a given hbm is defined by a fixed and constant value of the synaptic weights @xmath71 .",
    "however , those synaptic weights are taken at random from a given distribution , and different realizations of the synaptic weights correspond to different hbm s . since we are interested in determining the average behavior of a `` typical '' hbm , we average the relevant quantities over the distribution of synaptic weights .",
    "the average @xmath68 of a given observable @xmath72 under the boltzmann distribution is defined as @xmath73 the average @xmath70 of a given observable @xmath74 over the distribution of synaptic weights is defined as    @xmath75=\\int d\\mu(\\xi)\\int d\\mu(\\eta)f(\\xi,\\eta),\\ ] ]    where @xmath46 is the standard gaussian measure , @xmath76 .",
    "note that the standard hopfield network is built with random binary patterns @xmath35 , while we use gaussian patterns here : despite retrieval with the former choice has been extensively studied , we have chosen the latter in order to show a novel technique , stochastic stability , for studying the related thermodynamics . for finite @xmath0 ,",
    "this is known to be equivalent to the former case , despite for infinite neurons a complete picture of the quality of the retrieval is still under discussion .",
    "we define the free energy as @xmath77.\\ ] ] since the free energy is proportional to the logarithm of the partition function , and due to the additive properties of the logarithm , @xmath78 , we neglect the factor @xmath79 in @xmath80 ( see eq .",
    "( [ parthopf ] ) ) , as it gives a negligible contribution to the free energy in the thermodynamic limit .",
    "we also neglect the factor in the bolzmann average @xmath68 as it appears both at the numerator and denominator and therefore it cancels out .",
    "thermodynamics can be described by the standard gaussian measure .    in the hbm , parameters @xmath1 and @xmath81",
    "determine the number of neurons in the hidden layers , while in the hopfield model they represent the number of patterns stored in the network , or the number of stable states that can be retrieved .",
    "we consider the `` high storage '' regime , in which the number of stored patterns is linearly increasing with the number of neurons @xcite . in hbm , this corresponds to the case in which the sizes of the hidden and visible layers are comparable .",
    "their relative size is quantified by defining two control parameters @xmath82 as @xmath83 we further introduce the order parameters @xmath84 , called overlaps , as @xmath85 these objects describe the correlations between two different realizations of the system ( two different replicas @xmath86 ) .",
    "we also define the averages of these overlaps with respect to both state configurations and synaptic weights ( quenched noise ) . since the overlaps involve two realizations of the system ( @xmath87 ) , the boltzmann average is performed over both configurations . with some abuse of notation ,",
    "we use the symbol @xmath68 to also represent the boltzmann average over two - system configurations .",
    "therefore , the average overlaps are defined as    @xmath88    the goal of next section is to find an expression for the free energy in terms of these order parameters . while all configurations of the system are possible , only a subset of them has a significant probability to occur . in canonical thermodynamics ,",
    "those states are described by the minima of the free energy with respect to the order parameters .",
    "the free energy is the difference among the energy and the entropy , and its minimization corresponds to energy minimization and entropy maximization .      by definition of hbm ,",
    "we assume that no external field acts on the network ; inputs to all neurons are generated internally by other neurons .",
    "the overall stimulus felt by an element of a given layer is the sum , synaptically weighted , of the activity of neurons in the other layers .",
    "note that neurons are connected in loops , because a neuron receiving input from a layer also projects back to same layer .",
    "therefore , the hbm is a recurrent network , and this makes the calculation of the free energy complicated .",
    "however , the free energy can be calculated in specific cases by using a novel technique that has been developed in @xcite , which extends the stochastic stability developed for the analysis of spin glasses @xcite .",
    "this technique introduces an external field acting on the system which `` imitates '' the internal , recurrently generated input , by reproducing its average statistics .",
    "while the external , fictitious input does not reproduce the statistics of order two and higher , it represent correctly the averages .",
    "these external inputs are denoted as @xmath89 ( one for each neuron in each layer ) and are distributed following the standard gaussian distribution @xmath90 $ ] .    in order to recover the second order statistics",
    ", the free energy is interpolated smoothly between the case in which all inputs are external , and all high order statistics is missing , and the case in which all inputs are internal , describing the original hbm .",
    "we use the interpolating parameter @xmath91 $ ] , such that for @xmath92 the inputs are all external and the calculation straightforward , while for @xmath93 the full hbm is recovered .",
    "therefore , we define the interpolating free energy as @xmath94}\\\\ & \\cdot&\\exp{\\sqrt{t}\\big(\\beta \\sum_{i,\\mu}\\sigma_i\\xi_i^\\mu z_\\mu -\\sum_{i,\\nu}\\sigma_i\\eta_i^\\nu \\tau_\\nu\\big)}\\nonumber\\\\ & \\cdot&\\exp{\\sqrt{1-t}\\big(a\\sum_{i=1}^{n}\\tilde{\\eta}_{i}\\sigma_{i}+b\\sum_{\\mu=1}^{p}\\tilde{\\eta}_{\\mu}z_{\\mu}+ c\\sum_{\\nu=1}^{k}\\tilde{\\eta}_{\\nu}\\tau_{\\nu}\\big)}\\exp{\\big[(1-t)\\big(\\frac{h}{2}\\sum_{\\mu=1}^{p}z_{\\mu}^{2}+ \\frac{\\epsilon}{2}\\sum_{\\nu=1}^{k}\\tau_{\\nu}^{2}\\big)\\big]}.\\nonumber\\end{aligned}\\ ] ] in addition to the fictitious fields @xmath89 , we have introduced the auxiliary parameters @xmath95 , which serve to weight the external fields .",
    "we also introduced an additional leakage ( second order ) term , parameterized by @xmath96 and @xmath97 .",
    "those parameters are chosen once for all in appendix @xmath9 and @xmath98 in order to separate the contribution of mean and fluctuations of the order parameters in the final expression of the free energy .",
    "this technique is called multiple layer stochastic stability because each of the three layers are perturbed by external fictitious inputs to simplify the expression of the free energy .",
    "the free energy at @xmath92 is characterized by one - body terms and is calculated in appendix @xmath9 .",
    "the result is eq.([interpo0 ] ) and is equal to    @xmath99    in order to derive the expression of the free energy for the hbm , namely for @xmath93 , we use the sum rule    @xmath100    therefore , we need to compute the derivative of the interpolating free energy in order to recover the free energy of the hbm ( @xmath93 ) .",
    "we calculate the derivative in appendix @xmath98 ( eq.([strilla ] ) ) , and the result is    @xmath101    where the function @xmath102 is the source of the fluctuations of the order parameters , and is equal to    @xmath103\\rangle.\\ ] ]    in the following , we neglect the contribution of fluctuations , therefore we set @xmath104 . the integral in eq.([porcodio ] ) is calculated by substituting the derivative in eq.([seiuncojone ] ) with @xmath104 and , since the latter does not depend on @xmath105 , it can be integrated simply multiplying by one .",
    "further , we substitute the expression of the free energy at @xmath92 , eq.([cristo ] ) , and we obtain the final expression for the free energy of the hbm ( @xmath93 ) .",
    "the resulting expression is called @xmath106 , since it does not include fluctuations of the overlaps , and this corresponds to the replica symmetric ( @xmath107 ) solution in statistical mechanics .",
    "@xmath108    in appendix @xmath109 , we derive the free energy in the case in which an additional external input is applied to the hbm , in order to force the retrieval of stored patterns . in the next section ,",
    "we minimize the free energy with respect to the order parameters , in order to study the phases of the system .",
    "we minimize the free energy ( [ cocaina ] ) with respect to the order parameters @xmath110 .",
    "this is accomplished by imposing the following equations @xmath111 this gives the following system of integro - differential equations to be simultaneously satisfied @xmath112 note that , since the two hidden layers act symmetrically on the visible layer , in the sense that the synaptic weights are distributed identically , one of the above equations is redundant and the minimization condition is summarized by the following two equations @xmath113 these equations describe a minimum of the free energy , as can be checked by calculating the second - order derivatives of the free energy and verifying that the hessian has a positive determinant .",
    "the minima of free energy in the case of imposed retrieval are discussed in appendix @xmath109 .",
    "next , we study the phase transitions of the system by looking at divergences of the rescaled order parameters .",
    "if the overlap @xmath114 is zero , then all neurons in the visible layer are uncorrelated , implying that all neurons have random activity and the system has no structure .",
    "the value of parameters for which the transition to @xmath115 occurs corresponds to the case in which the fluctuations of @xmath116 diverge , and this defines the critical region .",
    "to evaluate the critical region , we study for which values of the parameters @xmath117 the squared order parameter @xmath118 diverges .",
    "this is obtained by expanding the hyperbolic tangent in eq.([crack ] ) to the second order , which gives a meromorphic expression for the overlap .",
    "this expression diverges at the critical region , which is characterized by @xmath119    the above equations are consistent with and generalize the results obtained in @xcite .",
    "since the hidden layers are not connected , and @xmath5 are conditionally independent , they are equivalent to a single hidden layer of @xmath120 neurons .",
    "therefore , the equivalent hopfield network stores @xmath120 independent patterns .",
    "the case of interacting ( correlated ) patterns is studied in the next section .      in this section",
    "we study the case in which the two hidden layers are connected by mild interactions .",
    "when the hidden units in the two separate layers interact , the performance of the network may change .",
    "we study this case for small interaction strengths , within a mean field approximation , in order to be able to obtain approximate results .",
    "we show that the two hidden layers act reciprocally as an additional noise source affecting the retrieval of stored patterns in the visible layer , i.e. the retrieval of the activities @xmath6 .",
    "we introduce the interacting energy of the hbm , denoted by @xmath121 , where @xmath122 stands for `` interacting layers '' : @xmath123 where the last term accounts for the interaction between hidden layers , and its strength is controlled by the parameter @xmath97 , which is assumed to be small .    the rigorous analysis of this model is complicated and still under investigation @xcite .",
    "however , for small @xmath97 , exact bounds can be obtained by first - order expansion .",
    "we will proceed as follows : first we marginalize over one layer ( either @xmath8 or @xmath7 ) and we find an expression of the interacting partition function depending on the two remaining ones . then , because of the symmetry between the hidden layers , we perform the same operation marginalizing the interacting partition function with respect to the other hidden layer .",
    "last , we sum the two expression and divide the result by two : this should represent the average behavior of the neural network , whose properties are then discussed .",
    "the interacting partition function @xmath124 , associated to the energy ( [ inter ] ) , can be written as @xmath125 we start integrating over the @xmath8 variables , and we find @xmath126 where the effective inputs @xmath127 and @xmath128 are given by @xmath129 next , we use the mean field approximation by which @xmath130 .",
    "therefore , we can bound the expression above with the partition function @xmath131 if we perform the same procedure , integrating first on @xmath7 and then on @xmath8 , we obtain the specular result @xmath132 to obtain the final equation for the partition function , we sum the two hamiltonians and divide by two , to find @xmath133 retaining only the first order terms in @xmath97 , we obtain an equivalent hamiltonian for a hbm where the hidden layers interact .",
    "this corresponds to a hopfield model with an additional noise source , characterized by the hamiltonian @xmath134 + \\sum_{\\mu}^{\\gamma n } \\xi_i^{\\mu}\\xi_j^{\\mu } [ 1 - \\epsilon \\beta^2 \\alpha/4 ] \\big).\\ ] ] note that for @xmath135 we recover the standard hopfield model .",
    "the effect of the additional noise source on retrieval of patterns corresponding to one layer depends on the load of the other layer : the larger number of neurons in one layer , the larger the perturbation on the retrieval of the other layer .",
    "we demonstrate an exact mapping between the hopfield network and a specific type of boltzmann machine , the hybrid boltzmann machine ( hbm ) , in which the hidden layer is analog and the visible layer is digital .",
    "this type of structure is novel , since previous studies have investigated the cases in which both types of layers are either analog or digital .",
    "the thermodynamic equivalence demonstrated in our study paves the way to a novel procedure for simulating large hopfield networks : in particular , hopfield networks require updating @xmath0 neurons and storing @xmath2 synapses , while hbm require updating @xmath3 neurons and storing only @xmath4 synapses , where @xmath1 is the number of stored patterns .",
    "in addition , the well known phase transition of the hopfield model has a counterpart in the hbm . in boltzmann machines ,",
    "the ratio between the sizes of the hidden and visible layers is arbitrary and needs to be adjusted in order to obtain the optimal generative model of the observed data .",
    "if the number of hidden units is too small , the generative model is over - constrained and is not able to learn , while if it is too big then the model `` overlearns '' ( overfits ) the observed data and is not able to generalize @xcite .",
    "interestingly , these two extrema correspond in the hopfield model to , respectively , the low storage phase , in which only a few patterns can be represented , and the spin glass phase , in which there is an exponentially increasing number of stable states .",
    "therefore , the corresponding phase transition in the hbm can be understood as the optimal trade - off between flexibility and generality , thus effectively representing a statistical regularization procedure @xcite .",
    "furthermore we showed that , if hidden layers are disconnected , the corresponding patterns contribute linearly to the capacity of the hopfield network . therefore , conditional independence among layers corresponds to linearity of the energy function .",
    "instead , if the hidden layers interact , we show that they affect retrieval by acting as an effective noise source .",
    "although the replica trick has represented a breakthrough for studying the thermodynamics of the hopfield model , we argue that the `` natural '' mathematical backbone required for studying the thermodynamics of the boltzmann machine is the stochastic stability , whose implementation is tractable .",
    "our work further contributes on connecting scientific communities quite far apart , such as the mathematical physicists studying spin glasses ( see i.e. @xcite ) and the computer scientists studying machine learning and artificial intelligence ( see i.e. @xcite ) .",
    "the strategy outlined in this research article belongs to the study supported by the italian ministry for education and research ( firb grant number @xmath136 ) and by sapienza universit di roma .",
    "adriano barra is partially funded by gnfm ( gruppo nazionale per la fisica matematica ) .",
    "adriano barra is grateful to elena agliari and francesco guerra for useful discussions .",
    "in this appendix , we calculate the interpolating free energy @xmath137 for @xmath92 .",
    "this calculation involves only one - body terms and is equal to @xmath138 due to the additive properties of the logarithm ( i.e. @xmath139 ) and the one - body factorization within each layer , the equation above can be rewritten as a sum of three separate terms , one for each layer : @xmath140 we show in appendix @xmath98 shows that the following choice of the parameters substantially simplifies the calculations , @xmath141 . using these values of the parameters , and performing the integrals and sums in the above expression , we find @xmath142",
    "in this section we focus on the @xmath105-derivative of @xmath137 .",
    "since the interpolating parameter @xmath105 appears seven times in the exponential , this derivative includes seven different terms .",
    "their derivation is long but straightforward , here we report the result for each of the seven terms @xmath143 pasting the various terms together we obtain @xmath144 we are left with the freedom of choosing the most convenient parameters ; we see that with the particular choice @xmath145 we can express the whole derivative as the source of the overlap fluctuations @xmath146\\rangle +   \\frac{\\beta}{2}(\\bar{q}-1)(\\alpha\\bar{p}+\\gamma\\bar{r})-\\frac{\\beta(\\alpha+\\gamma)}{2}.\\ ] ] the first term in the right hand side represents the fluctuations of each order parameter around its average ( i.e. @xmath110 ) , and we neglect this term within a replica symmetric approach .",
    "the second term includes only averages and does not depend on @xmath105 .",
    "its integration in @xmath105 on the interval @xmath147 coincides with multiplication by one .",
    "in this section , we calculate the free energy in presence of an external field designed to force retrieval of the stored patterns .",
    "when the stored patterns are gaussians , and in the thermodynamic limit , retrieval is not a spontaneous emergent feature of the network .",
    "however , it is possible to force retrieval by adding a proper lagrange multiplier in the interpolating free energy as @xmath148 , where @xmath149 is the mattis magnetization of the first condensed pattern ( we chose the first because there is full permutational invariance among patterns ) and @xmath150 is its replica symmetric approximation .    in analogy with the calculation performed in section @xmath151 , we find the following expression @xmath152\\rangle\\nonumber\\\\ & = \\frac{\\beta}{2}\\int_{0}^{1}dt\\langle ( m_{1}-m)^{2}\\rangle+a^{rs}(\\bar{p},\\bar{q},\\bar{r},m;\\alpha,\\beta,\\gamma),\\end{aligned}\\ ] ] fluctuations of @xmath153 around @xmath150 are now present .",
    "the final replica symmetric free energy can be written as @xmath154 we have to minimize the free energy ( [ rsfe ] ) with respect to the replica symmetric order parameters @xmath155 , namely we impose that @xmath156 this gives the following system of integrodifferential equations to be simultaneously satisfied @xmath157 which can be solved numerically ."
  ],
  "abstract_text": [
    "<S> a specific type of neural network , the restricted boltzmann machine ( rbm ) , is implemented for classification and feature detection in machine learning . </S>",
    "<S> rbm is characterized by separate layers of visible and hidden units , which are able to learn efficiently a generative model of the observed data . </S>",
    "<S> we study a `` hybrid '' version of rbm s , in which hidden units are analog and visible units are binary , and we show that thermodynamics of visible units are equivalent to those of a hopfield network , in which the @xmath0 visible units are the neurons and the @xmath1 hidden units are the learned patterns . </S>",
    "<S> we apply the method of stochastic stability to derive the thermodynamics of the model , by considering a formal extension of this technique to the case of multiple sets of stored patterns , which may act as a benchmark for the study of correlated sets . </S>",
    "<S> our results imply that simulating the dynamics of a hopfield network , requiring the update of @xmath0 neurons and the storage of @xmath2 synapses , can be accomplished by a hybrid boltzmann machine , requiring the update of @xmath3 neurons but the storage of only @xmath4 synapses . </S>",
    "<S> in addition , the well known glass transition of the hopfield network has a counterpart in the boltzmann machine : it corresponds to an optimum criterion for selecting the relative sizes of the hidden and visible layers , resolving the trade - off between flexibility and generality of the model . </S>",
    "<S> the low storage phase of the hopfield model corresponds to few hidden units and hence a overly constrained rbm , while the spin - glass phase ( too many hidden units ) corresponds to unconstrained rbm prone to overfitting of the observed data . </S>"
  ]
}