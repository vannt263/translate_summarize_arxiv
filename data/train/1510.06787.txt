{
  "article_text": [
    "the abundance of statistical surveys and censuses from the past years invites new enhanced methods for studying various aspects of the composition and dynamics of populations .",
    "gathered in different forms , as ( repeated ) cross - sectional or longitudinal data , they provide information on large , independent or overlapping , sets of subjects observed at several points in time .",
    "the first presents a snapshot of the population for quantitative and comparative analysis , while the latter tracks selected individuals , facilitating cohort and causal inferences .",
    "the cross - sectional data is often regarded inferior to the longitudinal one as it does not capture mechanisms underpinning observed effects . at the same time , however , it is oblivious to such problems as attrition , conditioning or response bias , while its much cheaper and faster collection procedure does not raise concerns about the confidentiality and data protection legislation . for these reasons , it is tempting to search for ways of employing it in the longitudinal analysis .    making inferences about the population dynamics on the basis of severed longitudinal information gleaned from cross - sectional data requires suitable theoretical approach and modelling tools .",
    "several methods proposed , e.g.  @xcite , are essentially based on regression techniques . in this paper",
    "we present a cross - sectional markov ( csm ) model for the transition analysis of survey data exploiting information from time series of cross - sectional samples . while sharing the attractive features of classic markov models , it avoids the practical problems associated with longitudinal data , and  due to its focus on population transfer rates between discrete states",
    " it is particularly well adapted to microsimulation modelling .",
    "the presented framework provides a set of versatile and robust tools for the analysis and forecasting of data trends with applications in various disciplines , including epidemiology , economics , marketing and political sciences .",
    "the following sections introduce the csm model , providing its detailed mathematical description and placing it within the context of statistics and information theory .",
    "we adapt it to analyse ageing cohorts and processes with memory , and demonstrate a regularisation method helping to avoid fallacious inferences . the framework is extended to describe incomplete longitudinal data , giving the possibility of fully exploiting all available information , e.g.  from aggregated surveys of different types .",
    "we also outline the model selection procedure required in its empirical applications as an integral part of any statistical data modelling .",
    "finally , we test the developed methods and illustrate them by examples using actual demographic statistics , obtaining new interesting results .",
    "we begin by expounding the mathematical background and derivation of the csm model in sec .  [",
    "sec : csm ] .",
    "sections  [ sec : bayesian - analysis ] and  [ sec : mle ] discuss it within the context of popular methods of statistical inference : bayesian analysis and maximum likelihood estimation . in sec .",
    "[ sec : kl - divergence ] we give the information - theoretical interpretation of the maximum likelihood estimation used to determine the csm model parameters .",
    "sections  [ sec : ageing_cohorts ] and  [ sec : memory ] , respectively , present the time - inhomogeneous extension for simulations of ageing cohorts and the process memory . finally , in sec .",
    "[ sec : model - selection ] we describe model selection procedure to be used in the csm framework applications .",
    "we analyse a time series of observations of a certain characteristic ( such as a risk factor , an exposure or a disease ) in a studied population .",
    "the frequency and pattern of the characteristic in a surveyed representative sample is described by the distribution of a categorical variable @xmath0 .",
    "the variable can take @xmath1 distinct values @xmath2 corresponding to different categories of the characteristic ( e.g.  ranges of risk factor values or stages of a disease ) .",
    "the observations @xmath3 are made at constant time intervals @xmath4 . at each time point @xmath5 , the sample of size @xmath6 consists of groups of @xmath7 individuals assigned to category @xmath8 , @xmath9 ; if data are missing for any @xmath5 then @xmath10 .",
    "thus , the empirical distribution of @xmath11 is given by @xmath12 .",
    "our goal is to estimate the true probability @xmath13 and its confidence intervals , as well as extrapolate the obtained result beyond the surveyed period .",
    "when the survey follows the same individuals ( a cohort ) over time , as in longitudinal studies , a common approach is to use a ( discrete - time ) markov model .",
    "it describes directly the stochastic dynamics of @xmath14 , which is the value of variable @xmath0 for an observed individual @xmath15 at time @xmath5 .",
    "the probability of @xmath16 conditioned on the value of @xmath17 at time @xmath18 is given by a constant @xmath19 transition matrix @xmath20 with elements @xmath21 satisfying the constraints @xmath22 $ ] and @xmath23 : @xmath24 from the above it immediately follows that @xmath25 i.e. ,  @xmath26 in vector notation , where @xmath27 $ ] and @xmath28 .    while eq .",
    "involves tracking the same individual over time , eq . is conspicuously free from this assumption .",
    "it transforms the distribution of variable @xmath0 in a cross - sectional sample collected at time @xmath18 into the distribution of this variable in another such sample collected at time @xmath5 , using the transition matrix @xmath20 . by rearranging its terms , we can trace changes in the frequency of property @xmath8 between consecutive observations : @xmath29 therefore , eq . can be the basis of a more robust dynamical model , which we propose in this article . due to its mathematical construction",
    ", we call it the cross - sectional markov model .",
    "it facilitates the description of temporal trends of investigated characteristics in ageing cohorts and ( groups of ) the population based on repeated cross - sectional data .",
    "the csm model parameters : transition matrix @xmath20 and initial distribution @xmath30 , are determined by fitting @xmath31 ( where @xmath32 is the @xmath5-th power of matrix @xmath20 ) to the observed distributions @xmath33 . in its extended version , the model utilises information collected in any form , including aggregates of cross - sectional and ( incomplete ) longitudinal data .",
    "it is achieved by maximising the log - likelihood of the data over @xmath20 and @xmath30 . the detailed mathematical reasoning and procedures",
    "are described in the following sections .",
    "estimation of standard markov model parameters amounts to measuring the initial distribution of an investigated variable and counting frequencies of transitions between its states at consecutive time steps .",
    "this straightforward procedure owes to the fact that continuous longitudinal trajectories provide full information about the dynamics of the observed process .",
    "in contrast , repeated cross - sectional data do not capture the individual transitions . to understand the implications of this difference for the csm model , we perform a bayesian analysis of the model parameters .",
    "we will demonstrate that the missing longitudinal information in repeated cross - sectional data results in a more complicated form of posterior distributions of @xmath30 and @xmath20 .",
    "this creates additional challenges in their estimation and calculations of confidence intervals .    within the bayesian paradigm",
    ", the probability distribution of @xmath30 and @xmath34 is inferred from observed data . before making any observations , we assume that all their values are equally probable , i.e.",
    ",  their prior probability density function is @xmath35 , which carries maximum entropy and therefore least extraneous information .",
    "next , by conditioning it on the observation results we obtain the posterior distribution with the density @xmath36 representing best our state of knowledge about the distribution of model parameters given our prior knowledge and the data .    in a longitudinal study , we track changes of the investigated characteristic in a group of the same individuals over a period of time . from bayes theorem",
    ", we update the posterior with the value of variable @xmath17 for each individual @xmath15 added to the sample : @xmath37 and at each time step @xmath5 of his or her longitudinal trajectory : @xmath38 where @xmath39 is the previously observed data . from the above updating rules it follows that the posterior joint density of the standard markov model parameters is a product of dirichlet distribution densities : @xmath40 where @xmath41 is an @xmath1-dimensional dirichlet density with a parameter vector @xmath42 ( i.e. ,  @xmath43 is the number of individuals in category @xmath8 at time 0 and @xmath44 is the overall number of transitions from category @xmath8 to @xmath45 ) , @xmath46 @xmath47 @xmath48 and @xmath49 is the @xmath8-th column vector of @xmath20",
    ". such bayesian estimation of @xmath50 for longitudinal data can be easily implemented numerically .    in the case of cross - sectional data , the same non - informative prior",
    "is updated by observations of the form @xmath51 , leading to the posterior density @xmath52 for example , for @xmath53 we obtain @xmath54 .",
    "thus , the posterior density of model parameters conditioned on the observed cross - sectional data is not a product of dirichlet densities , like in the longitudinal case , but a mixture of such products : @xmath55 where @xmath56 indexes all possible @xmath57 trajectories realising the observed cross - sectional distributions and @xmath58 , @xmath59 .",
    "consequently , the number of components of @xmath50 for cross - sectional data grows exponentially with the sample size and number of time steps , making its direct calculation impossible in practice .",
    "the posterior distributions densities   and   are also characterised by different covariance structures . to demonstrate it ,",
    "let @xmath60 for @xmath61 be defined as @xmath62 for longitudinal data , vectors @xmath60 and @xmath63 are statistically independent for @xmath64 , due to the product structure of the posterior  .",
    "however , for the cross - sectional posterior   we obtain @xmath65 = \\sum_{q } \\beta_q \\mathbb{e}_q [ ( { \\boldsymbol{\\mathbf{x}}}_j)_k ] \\mathbb{e}_q [ ( { \\boldsymbol{\\mathbf{x}}}_{j'})_{k ' } ] $ ] , where @xmath66 denotes the expectation value calculated using @xmath67 and @xmath68 = \\sum_{q } \\beta_q \\mathbb{e}_q [ ( { \\boldsymbol{\\mathbf{x}}}_j)_k ] $ ] .",
    "hence , @xmath65",
    "\\neq \\mathbb{e } [ ( { \\boldsymbol{\\mathbf{x}}}_j)_k ] \\mathbb{e } [ ( { \\boldsymbol{\\mathbf{x}}}_{j'})_{k ' } ] $ ] for @xmath64 , which indicates that @xmath60 and @xmath63 are not statistically independent .",
    "this makes the analytic calculation of confidence intervals for estimated probabilities more difficult , unless one resorts to the `` delta method '' approach ( postulating that the likelihood function is a multivariate gaussian and expanding @xmath69 up to linear terms in @xmath30 and @xmath20 ) , which can significantly underestimate their width . in general ,",
    "due to its complex structure the posterior density for cross - sectional data is broader than for longitudinal data , leading to wider confidence intervals .    for the above reasons ( impracticable calculation of the posterior distribution and correlation of model parameters for cross - sectional data ) we will use the maximum - likelihood method to obtain @xmath70 and @xmath71 , point estimates of @xmath20 and @xmath30 ( the next section ) , and bootstrapping for their confidence intervals ( appendix  [ sec : bootstrapping ] ) .      given the difficulties in calculating the distribution of the csm model parameters in the bayesian framework , we attempt instead to find their `` best '' values @xmath70 and @xmath71 by maximising the log - likelihood , @xmath72 : = \\ln p ( \\ { { \\boldsymbol{\\mathbf{n}}}_t \\ } | { \\boldsymbol{\\mathbf{p}}}_0,\\mpi ) $ ] . for cross - sectional data we obtain @xmath73 = \\ln \\prod_{t=0}^{t-1 } \\prod_{i=0}^{n_{t } - 1 } p(x_i = k_{it }",
    "| { \\boldsymbol{\\mathbf{p}}}_0,\\mpi ) = \\sum_{t=0}^{t-1 } n_{t } \\sum_{k=0}^{n-1 } { { \\tilde{p}}}_{kt } \\ln p_{kt } \\ .",
    "\\label{eq : ll - cs } \\ ] ] from the bayes theorem , @xmath74 , since @xmath75 ) . assuming a `` flat '' prior like in sec .",
    "[ sec : bayesian - analysis ] , we obtain @xmath76 . thus , the maximum likelihood estimator of the model parameters is equivalent to their maximum posterior density estimator , bringing the maximum likelihood estimation procedure closer to the bayesian analysis discussed in the previous section . in specific situations",
    "the csm model may not be able to uniquely recover the transition matrix because of the lack of information about individual transitions in repeated cross - sectional data , facing the threat of ecological inference fallacy  @xcite .",
    "for example , for @xmath77 and a constant time series @xmath78 , both @xmath79 and @xmath80 ( maximum and minimum correlation between @xmath11 and @xmath81 , respectively ) are perfect solutions of eq . , and so is their convex combination . to steer the model estimation procedure towards a particular outcome",
    ", one can subtract from @xmath82 a regularisation term @xmath83 , where @xmath84 is the regularisation strength and @xmath85 $ ] specifies the type of required solution ( from @xmath79 for @xmath86 to @xmath80 for @xmath87 ) .",
    "while realistic time - dependent data usually sufficiently constrains the solution space for @xmath70 , some form of regularisation may be required at times , e.g.  for purely cross - sectional data exhibiting simple temporal trends .",
    "this generally involves the loss of rigorous probabilistic interpretation of the model as it no longer maximises a proper log - likelihood .",
    "the csm framework can be extended to allow analysis of incomplete longitudinal data ( distorted by attrition or non - adherence ) .",
    "they can be represented as a set @xmath88 , consisting of @xmath89 independent trajectories indexed by @xmath15 , i.e. , vectors @xmath90 of @xmath91 consecutive categories measured in @xmath92 points in time",
    "@xmath93 , where @xmath94 and @xmath95 . this notation enables us to describe trajectories starting and ending at different times and having gaps .",
    "the likelihood of observing a trajectory @xmath96^{\\tau_i}$ ] given @xmath30 and @xmath20 is @xmath97 hence , the log - likelihood of the whole dataset @xmath98 is @xmath99 = \\sum_{i=0}^{q-1 } \\left [ \\ln [ ( \\mpi^{t_{i,0 } } { \\boldsymbol{\\mathbf{p}}}_0)_{k_{i,0 } } ] + \\sum_{s=1}^{\\tau_i - 1 } \\ln [ \\left(\\mpi^{t_{i , s } - t_{i , s-1}}\\right)_{k_{i , s } , k_{i , s-1 } } ] \\right ] .\\ ] ] ( it is easy to notice that for complete longitudinal trajectories , i.e. ,  @xmath100 @xmath101 for all @xmath15 , the above result reduces to a simple product of dirichlet distributions discussed in sec .  [ sec : bayesian - analysis ] ) .",
    "therefore , to model an aggregate of cross - sectional and longitudinal data , we maximise the sum of likelihood functions   and  , @xmath102 , over @xmath20 and @xmath30 .",
    "numerically , it requires solving a highly non - linear optimisation problem with the following constraints : @xmath103 and @xmath104 $ ] . in doing this , it is useful to know that although the cross - sectional part can be described by eq . for @xmath105 ,",
    "it is more efficient to treat it as a separate term using eq ..      to better understand the relation between the csm model and other popular approaches to modelling repeated cross - sectional data ( such as regression methods ) , it is helpful to couch it in terms of information theory . within this framework",
    "we represent our procedure of finding the model parameters as a fitting of estimated probability distributions @xmath106 to observed ones @xmath33 .",
    "the norm of the estimation error which is being minimised by the calibration procedure is the kullback ",
    "leibler divergence  @xcite , @xmath107 this important information - theoretical measure corresponds to the amount of information lost when replacing the probabilities indicated directly by the data with our model estimates .",
    "hence , by minimising @xmath108 over @xmath20 and @xmath30 we ensure that our model exploits the maximum information from the data , without making any distributional assumptions about the error  @xcite .",
    "it is easy to show that maximising likelihood for cross - sectional data , @xmath82 , over @xmath30 and @xmath20 is equivalent to minimising the weighted sum of kullback  leibler divergences @xmath108 , @xmath109    similarly , in the simple case of complete longitudinal trajectories with equal lengths @xmath110 , maximising their likelihood @xmath111 is equivalent to minimising the kullback ",
    "leibler divergence of two distributions over the space of all possible trajectories of length @xmath112 . to demonstrate this ,",
    "let @xmath113 be the number of observations of an identical trajectory @xmath114 in the dataset @xmath98 .",
    "then , @xmath115 is the distribution of different trajectories , which we want to approximate , an equivalent of @xmath116 in eq .. the kullback  leibler divergence to be minimised is @xmath117 where @xmath118 is given by eq . , which confirms our earlier assertion .",
    "the first term in this formula is equal to minus log - likelihood of @xmath98 given by eq .. it is worth noting that if we observe just a small subset of all possible @xmath119 trajectories ( e.g.  very few individuals with long trajectories ) , minimising @xmath120 will require concentrating the mass of the probability distribution @xmath121 in a small subregion of the available configuration space , making optimisation of model parameters more difficult numerically than in the case of cross - sectional data .",
    "the csm model can be adjusted to analyse trends of characteristics in ageing cohorts based solely ( or partly ) on the repeated cross - sectional data .",
    "figure  [ fig : cohortsim ] presents a schematic solution of this problem .",
    "it requires the estimation of csm transition matrices @xmath70 and initial distributions @xmath71 for each age group and in all time periods covered by the available data .",
    "the age brackets are numbered from 1 to @xmath122 and assumed to have the same length @xmath45 for simplicity .",
    "beginning with the initial state for the first bracket , we apply the transition matrix @xmath123 obtained for this bracket @xmath45 times , thus increasing the age of the cohort .",
    "this gives the first @xmath45 years of the csm model fit @xmath124 .",
    "next , we switch to the second age bracket and apply the transition matrix @xmath125 to the current distribution @xmath126 ; we perform the operation @xmath45 times obtaining the fit for @xmath45 subsequent years .",
    "we repeat the procedure until the last age bracket , at which point we keep on applying the last transition matrix @xmath127 until the end of the desired extrapolation period .    [",
    "fig : cohortsim ]      in discrete - time markov models , the memory of the process is introduced by conditioning the next state of the variable @xmath0 on not just the current one , but also one or more preceding states .",
    "for example , to model a one - step memory we can replace eq . with @xmath128 with constraints @xmath129 and @xmath130 $ ] .",
    "a similar procedure can be introduced to the csm framework by estimating the joint distribution of @xmath11 and @xmath81 based on repeated cross - sectional data . for this purpose",
    ", we define a random variable @xmath131 , noting that @xmath132 and @xmath133 are always correlated since both depend on @xmath81 .",
    "the distribution of @xmath132 is denoted by @xmath134 ; its dynamics are governed by a transition matrix @xmath135 , the counterpart of @xmath20 .",
    "hence , @xmath135 has the form @xmath136 , where @xmath137 satisfies the same constraints as in eq .. it follows that @xmath138 . since we do not observe the process @xmath132 directly , we have to estimate the initial state @xmath139 and transition matrix @xmath135 based on @xmath33 .",
    "it requires reducing the dimension of the distributions @xmath140 from @xmath141 to @xmath1 to the end that @xmath142 $ ] , where the reduction operator is defined as @xmath143_k : = \\sum_{l=0}^{n-1 } q_{(k , l),t } \\ .\\ ] ] the estimates @xmath144 and @xmath145 are found by minimising the total weighted kullback  leibler distance , @xmath146)\\ ] ]",
    "the above procedure easily extends to models with longer memory and has a straightforward numerical implementation .    in the general case of incomplete longitudinal data ,",
    "introducing the memory length @xmath147 leads to a more complicated form of the likelihood function  . let @xmath148^{\\lambda+1}$ ] be the set of values of @xmath149 , containing all possible continuous sequences of states of variable @xmath11 spanning the memory length @xmath147 .",
    "the transition matrix @xmath20 is indexed by such a sequence @xmath150 ( representing a trajectory over the time interval @xmath151 $ ] ) and a state @xmath152 $ ] at @xmath153 , that is @xmath154 .",
    "we define @xmath155 as the set of only those sequences for which the event @xmath156 is not contradicted by the observed trajectory @xmath90 . for example",
    ", we model a variable with @xmath157 states and memory @xmath158 based on three consecutive longitudinal measurements , one of which is missing : @xmath159 ; then @xmath160 .",
    "the final form of the likelihood function for @xmath90 starting at @xmath161 and ending at @xmath162 is @xmath163 where @xmath164 . a fully - specified trajectory ( without gaps )",
    "will always have only a single element in every set @xmath165 for @xmath166 .",
    "the error estimation by bootstrapping is performed identically as in the case of zero memory .",
    "an integral part of all statistical work with data is choosing a suitable model for their analysis .",
    "this section invokes the most popular model selection techniques and incorporates them in the proposed framework .",
    "they will enable us to quantitatively compare the performance of the csm model variants and other methods .",
    "on this basis , we will decide which model best explains the mechanisms underlying the data , providing the most accurate and stable predictions .",
    "assuming that the calibration procedure converges numerically , one can always improve the fit by increasing the model complexity ( e.g.  extending the csm memory length ) .",
    "however , indiscriminately adding new model parameters leads to overfitting . to strike a balance between these two factors ,",
    "we calculate akaike information criterion ( aic )  @xcite @xmath167 and bayesian information criterion ( bic )  @xcite @xmath168 where @xmath8 is the number of model degrees of freedom and @xmath122 is the sample size ( i.e. ,  the number of collected surveys for cross - sectional and observed trajectories for longitudinal data ) .. additionally , since typical applications of the csm model concern large @xmath122 , we neglect the commonly used small - sample correction to aic . ]",
    "both criteria are constructed as a penalty for the complexity of a candidate model minus twice the maximised log - likelihood @xmath169 value , but they are derived from different mathematical perspectives .",
    "aic intends to minimise the kullback ",
    "leibler distance between the true data - generating probability density and that predicted by the candidate model , while bic seeks for the model with maximum posterior probability given the data .",
    "consequently , they do not always select the same `` best '' candidate .",
    "in particular , bic assures consistency ( for very large sample size it will choose the correct model with probability approaching 1 ) , while aic aims at optimality ( as more benevolent to complexity , it leads to a lower variance of the estimated model parameters , especially if their true values are close to those from oversimplified models )  @xcite .",
    "the difference between the criteria becomes evident with growing sample size : aic allows additional model parameters to describe the new data ( increasing the predictive accuracy if it represents new information , or overfitting if it totes mostly noise and outliers ) , while bic is more stringent and favours smaller ( with fewer parameters ) models .",
    "while the information criteria focus on accuracy and parsimony , cross - validation analysis is a natural and practical way of assessing the predictive performance and robustness of the model  @xcite . as a realisation of out - of - sample testing",
    ", it does not rely on analytical approximations but exact algorithms and provides a stronger check against overfitting .",
    "we will use its most common variants : leave - one - out ( loocv ) and @xmath8-fold cross - validation ( @xmath8fcv ) , which under certain conditions behave similarly to aic and bic , respectively  @xcite , as well as time series cross - validation ( tscv ) .",
    "the application of the above techniques is straightforward when working with cross - sectional data . in the case of loocv ,",
    "given a time sequence of @xmath112 observations @xmath170 , for each @xmath171 we calibrate the csm model to all data points but @xmath33 and next use the obtained model to calculate the approximate value of the omitted point , @xmath172 .",
    "the kullback ",
    "leibler divergence @xmath173 measures the error with which the model recovers @xmath33 .",
    "the sum of @xmath174 over @xmath5 , @xmath175 , defines a measure of the model error which additionally discourages overfitting .",
    "the @xmath8fcv consists in randomly partitioning , one or several times , the sequence of observations into @xmath8 subsets of equal size and performing the leave - one - out analysis on the subsets , adding the errors up for all folds and averaging the sums over the partitionings . since in each fold",
    "we leave out more data points , @xmath8fcv is a stronger test for overfitting than loocv .",
    "both loocv and @xmath8fcv are sufficient for testing the model s approximation and forecasting abilities , however they may fail at the latter in the setting of highly autocorrelated data  @xcite . to assess the csm model performance in forecasting highly correlated time series we need a specialised method such as tscv  @xcite . in this approach , we fit the model to first @xmath176 time points @xmath177 , where @xmath178 , and use it to predict the probability distribution for @xmath176 , @xmath179 .",
    "the total tscv error is the sum of kullback  leibler divergences @xmath180 over @xmath181 .",
    "when working with data containing longitudinal information , each individual trajectory ( complete or with gaps ) is considered to be an independent observation .",
    "hence , we perform @xmath8fcv by dividing the set of observed trajectories @xmath98 ( see sec .  [",
    "sec : mle ] ) into @xmath8 subsets . due to the multitude of trajectories , we are able to perform only a limited number of iterations of the procedure , namely 30 .",
    "we perform tscv by truncating the trajectories to test the stability of extrapolation results .",
    "the error of extrapolation from @xmath182 to @xmath176 is measured as a difference of log - likelihoods of trajectories truncated at @xmath182 and @xmath176 , calculated using the model fitted to trajectories truncated at @xmath182 .    finally , a relevant measure of",
    "how suitable and robust the model is for analysing a particular dataset is whether the extrapolated trends it produces behave reasonably and stably .",
    "in this section we test the proposed framework using synthetic data ( sec .  [",
    "sec : synth ] ) and demonstrate its practical applications to a selection of repeated cross - sectional and incomplete longitudinal samples collected from real - life observational studies ( secs .  [ sec : bmi][sec : longit - bmi ] ) .",
    "we compare different variants of the csm model ( with memory and regularisation ) and estimate confidence intervals for extrapolated results .",
    "our analysis includes the popular multinomial logistic regression ( mlr )  @xcite using the same , maximum likelihood estimator .",
    "the model selection procedure enables us to choose the best model , representing the most trustworthy and reliable statistical description and prediction tool for the investigated problem .",
    "all models were implemented in c++11 , employing open source optimisation library nlopt  @xcite , automatic differentiation library sacado  @xcite and linear algebra library eigen  @xcite .      to test the csm framework",
    ", we use synthetic data consisting in a set of 1000 longitudinal trajectories , each of length 30 , generated by a 3-dimensional markov process with memory length equal 1 .",
    "the initial state is @xmath183 and the transition matrix is described by table  [ tab : synth - pi ] .",
    ".transition matrix coefficients used to generate synthetic data , @xmath184 . [ cols= \"",
    "< , < , < , < , < \" , ]     ; csm(0)@xmath185 and the observed longitudinal transitions ( the marker area is proportional to the category count @xmath6 ) have been shown for comparison . ]",
    "another valuable information produced by the csm framework by simply applying the bayes theorem is the variability of bmi in the population .",
    "figure  [ fig : csm_2 ] displays the joint probabilities of belonging to particular bmi categories at the current and the preceding time steps , calculated using csm(2)@xmath186 .",
    "panels ` a'`c ' indicate whether a person remains in the same or moves to a different category within the following 12 years .",
    "the calculated trends match the values derived directly from the data by counting transitions in all available continuous two - point fragments of observed trajectories .",
    "the summary of the results is presented in panel ` d ' : over 80% of the population stays in the same category , while the rest tends to experience a bmi increase rather than a decrease in the short period .",
    "we also indicate respective csm(0)@xmath185 results to demonstrate that only calibration to longitudinal data can accurately reproduce the joint probability trends .",
    "compared to the observed longitudinal transitions ( the marker area is proportional to the category count @xmath6 ) . ]",
    "figure  [ fig : csm_3 ] presents a similar analysis for joint probabilities of belonging to particular bmi categories at three contiguous time steps , facilitating the analysis of long - term ( 34 years ) bmi changes .",
    "we can distinguish three stable patterns : the bmi remains unchanged for the majority of the population ( diagonal panels ) in concordance with the short term predictions ; most persons who have moved to a higher bmi category remain in it ( above - diagonal panels ) ; over a half of those who have managed to reduce their bmi experience the `` yo - yo '' effect , i.e. ,  the cyclical loss and gain of weight ( below - diagonal panels ) . the described csm(2)@xmath186 results are compared with the longitudinal data by counting transitions in all available continuous three - point fragments of observed trajectories . since their number is much smaller than in the previous case ( see  fig .",
    "[ fig : histogram_csm]c ) , some disagreement is likely .",
    "the csm framework unifies all available data facilitating trend analysis and forecasts of bmi tendencies in the long period .",
    "the presented csm framework can utilise any type of available information , from cross - sectional to longitudinal data and their aggregates , for comprehensive studies of trends in groups and cohorts of the population .",
    "its mathematical structure based on classic markov models has a clear dynamical interpretation , providing an insight into mechanisms generating the observed process and making it particularly well adapted to microsimulation modelling .",
    "the employed maximum likelihood estimation procedure is compatible with popular model scoring methods , while the efficient numerical implementation facilitates an extensive cross - validation of the model results and an accurate estimation of confidence intervals by bootstrapping .",
    "the versatility of the csm approach enables us to analyse simple dependencies , as well as complex trends , obtaining realistic projections and steady states , while avoiding ecological fallacy and overfitting .",
    "the provided examples of the model applications to real world data yield new and interesting results . in particular , the combined results on bmi trends in the uk population and its birth cohorts based on cross - sectional data show that the excessive weight problem affects all generations equally , suggesting a common driving factor .",
    "the rich , shaped by historical policies , trend of marijuana use among american teenagers has been recovered assuming a 3-year - long memory of the process and shown to have achieved its steady state of about 20% of users .",
    "we have described the interesting dynamics of bmi changes behind the obesity growth in the us population concealed in incomplete longitudinal data , involving e.g.  ` yo - yo ' effect , which can not be recovered from the longitudinal information alone . in all above problems ,",
    "several variants of the csm model have been compared to the mlr method and chosen by the model selection procedure as best candidates to describe the analysed dataset .",
    "we would like to thank dr krzysztof bartoszek for valuable comments .",
    "bootstrapping is a very general method of calculating confidence intervals for quantities estimated from statistical data  @xcite . in its simplest form ,",
    "we construct an empirical distribution of the quantity @xmath89 estimated from a sample @xmath187 by drawing randomly with replacement the values from @xmath187 and constructing a new sample @xmath188 , from which the new value of @xmath89 can be computed . after doing it sufficient number of times",
    "we can provide e.g.  95% confidence intervals for @xmath89 based on its empirical distribution built by bootstrapping . a more advanced version , parametric bootstrapping , first estimates the distribution of the data based on the sample @xmath187 ( e.g.  using bayesian inference ) and then draws from this distribution .    in our case ,",
    "our datasets are sets of surveys collected each year , or observed longitudinal trajectories . to calculate confidence intervals for the csm estimations we adapt the bootstrapping procedure so that it respects the temporal trends present in the data .",
    "the role of the quantity @xmath89 can be played by the transition matrix @xmath70 , initial state @xmath189 or an extrapolated distribution @xmath190 . to obtain a resampled dataset",
    ", we proceed as follows :    \\1 . for each time @xmath5 in which we have a non - zero number @xmath6 of cross - sectional surveys , estimate the distribution of @xmath11 using a flat dirichlet prior ; i.e. ,  the posterior distribution for @xmath191 will be @xmath192 with @xmath193 .    \\2 . for each time",
    "@xmath5 we draw from @xmath192 a new distribution @xmath194 and then draw from it a set of @xmath6 new values of @xmath11 ( resampled surveys ) . from it",
    "we calculate ( by counting the numbers of occurrences @xmath195 of values",
    "@xmath196 ) the resampled distributions @xmath197 .",
    "\\3 . given a set of @xmath89 longitudinal trajectories observed over a number of time periods ,",
    "we simply draw from it with replacement a new set of @xmath89 trajectories .",
    "( we use such a simple procedure instead of estimating a dirichlet distribution because the dimension of the space of all longitudinal trajectories can be very large . )",
    "this approach respects the observed trends in the data and is free from assumptions about the error distribution or analytical simplifications ( such as commonly used quadratic expansion of the likelihood function mentioned in sec.[sec : bayesian - analysis ] ) .    having generated a new input set @xmath197 , we maximise the log - likelihood and obtain new transition matrix @xmath198 and initial state @xmath199 , and consequently a new extrapolated trend @xmath200 . to obtain the @xmath201 confidence intervals for each @xmath202 , we sort the bootstrapped trends @xmath200 by their total kullback ",
    "leibler divergence from @xmath190 , @xmath203 ( where @xmath204 is the number of extrapolated periods ) , and remove the furthest @xmath205 trends .",
    "the upper and lower confidence bounds for @xmath206 are the maximum and minimum of the remaining values of @xmath207 .",
    "this calculation gives us in particular the confidence intervals for the initial state @xmath71 .",
    "the above confidence level scheme can be applied to the estimated transition matrix @xmath70 , if we treat its columns as probability distributions and sort the bootstrapped matrices @xmath198 by their total kullback ",
    "leibler divergence from @xmath70 ( summing over columns ) .",
    "y.  meng , a.  brennan , r.  purshouse , d.  hill - mcmanus , c.  angus , j.  holmes , and p.s .",
    "estimation of own and cross price elasticities of alcohol demand in the uk - a pseudo - panel approach using the living costs and food survey 2001009 . , 34:96  103 , 2014 ."
  ],
  "abstract_text": [
    "<S> we present a stochastic model of population dynamics exploiting cross - sectional data in trend analysis and forecasts for groups and cohorts of a population . while sharing the convenient features of classic markov models , it alleviates the practical problems experienced in longitudinal studies . </S>",
    "<S> based on statistical and information - theoretical analysis , we adopt a maximum likelihood estimation procedure to determine the model parameters , in this way facilitating the use of a range of model selection methods . </S>",
    "<S> their application to several synthetic and empirical datasets shows that the proposed framework is robust , stable and superior to a regression - based approach . </S>",
    "<S> we develop extensions for simulations of memory of the process , distinguishing its short and long - term trends , as well as helping to avoid the ecological fallacy . </S>",
    "<S> the presented model illustrations yield new and interesting results , such as an even rate of weight gain across generations of the u.k .  </S>",
    "<S> population , suggesting a common driving factor , and `` yo - yo '' dieting in the u.s .  data . </S>"
  ]
}