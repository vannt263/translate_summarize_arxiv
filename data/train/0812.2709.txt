{
  "article_text": [
    "this note describes coding and decoding strategies for discrete - time additive memoryless gaussian - noise ( damgn ) channels with ideal feedback .",
    "it was shown by shannon @xcite in 1961 that feedback does not increase the capacity of memoryless channels , and was shown by pinsker @xcite in 1968 that fixed - length block codes on gaussian - noise channels with feedback can not exceed the sphere packing bound if the energy per codeword is bounded independently of the noise realization .",
    "it is clear , however , that reliable communication can be simplified by the use of feedback , as illustrated by standard automatic repeat strategies at the data link control layer .",
    "there is a substantial literature ( for example @xcite , @xcite , @xcite ) on using variable - length strategies to substantially improve the rate of exponential decay of error probability with _ expected _ coding constraint length .",
    "these strategies essentially use the feedback to coordinate postponement of the final decision when the noise would otherwise cause errors .",
    "thus small error probabilities can be achieved through the use of occasional long delays , while keeping the expected delay small .",
    "for damgn channels an additional mechanism for using feedback exists whereby the transmitter can transmit unusually large amplitude signals when it observes that the receiver is in danger of making a decoding error . the power ( _ i.e. _ , the expected squared amplitude ) can be kept small because these large amplitude signals are rarely required . in 1966 ,",
    "schalkwijk and kailath @xcite used this mechanism in a fixed - length block - coding scheme for infinite bandwidth gaussian noise channels with ideal feedback .",
    "they demonstrated the surprising result that the resulting probability of decoding error decreases as a second order exponential , the @xmath0th order exponent function @xmath1 is defined as @xmath2 with @xmath0 repetitions of exp .",
    "a function @xmath3 is said to decrease as a @xmath0th order exponential if for some constant @xmath4 and all sufficiently large @xmath5 , @xmath6 .",
    "] in the code constraint length at all transmission rates less than capacity .",
    "schalkwijk @xcite extended this result to the finite bandwidth case , _",
    "i.e. _ , damgn channels . later ,",
    "kramer @xcite ( for the infinite bandwidth case ) and zigangirov @xcite ( for the finite bandwidth case ) showed that the above doubly exponential bounds could be replaced by @xmath0th order exponential bounds for any @xmath7 in the limit of arbitrarily large block lengths . later encoding schemes inspired by the schalkwijk and kailath approach have been developed for multi - user communication with damgn @xcite , @xcite , @xcite , @xcite , @xcite , secure communication with damgn @xcite and point to point communication for gaussian noise channels with memory @xcite .",
    "the purpose of this paper is three - fold .",
    "first , the existing results for damgn channels with ideal feedback are made more transparent by expressing them in terms of a 1956 paper by elias on transmitting a single signal from a gaussian source via multiple uses of a damgn channel with feedback .",
    "second , using an approach similar to that of zigangirov in @xcite , we strengthen the results of @xcite and @xcite , showing that error probability can be made to decrease with blocklength @xmath8 at least with an exponential order @xmath9 for given coefficients @xmath10 and @xmath11 .",
    "third , a lower bound is derived .",
    "this lower bound decreases with an exponential order in @xmath8 equal to @xmath12 where @xmath13 is the same as in the upper bound and @xmath14 is a sublinear function ] of the block length @xmath8 .",
    "neither this paper nor the earlier results in @xcite , @xcite , @xcite , and @xcite are intended to be practical . indeed , these second and higher order exponents require unbounded amplitudes ( see @xcite , @xcite , @xcite ) .",
    "also kim et al @xcite have recently shown that if the feedback is ideal except for additive gaussian noise , then the error probability decreases only as a single exponential in block length , although the exponent increases with increasing signal - to - noise ratio in the feedback channel .",
    "thus our purpose here is simply to provide increased understanding of the ideal conditions assumed .",
    "we first review the elias result @xcite and use it to get an almost trivial derivation of the schalkwijk and kailath results .",
    "the derivation yields an exact expression for error probability , optimized over a class of algorithms including those in @xcite , @xcite .",
    "the linear processing inherent in that class of algorithms is relaxed to obtain error probabilities that decrease with block length @xmath8 at a rate much faster than an exponential order of 2 .",
    "finally a lower bound to the probability of decoding error is derived .",
    "this lower bound is first derived for the case of two codewords and is then generalized to arbitrary rates less than capacity .",
    "let @xmath15 represent @xmath16 successive inputs to a discrete - time additive memoryless gaussian noise ( damgn ) channel with ideal feedback .",
    "that is , the channel outputs @xmath17 satisfy @xmath18 where @xmath19 is an @xmath8-tuple of statistically independent gaussian random variables , each with zero mean and variance @xmath20 , denoted @xmath21 .",
    "the channel inputs are constrained to some given average power constraint @xmath22 in the sense that the inputs must satisfy the second - moment constraint @xmath23 .\\ ] ] without loss of generality , we take @xmath24 . thus @xmath22 is both a power constraint and a signal - to - noise ratio constraint .",
    "a discrete - time channel is said to have ideal feedback if each output @xmath25 , is made known to the transmitter in time to generate input @xmath26 ( see figure [ figelias ] ) .",
    "let @xmath27 be the random source symbol to be communicated via this @xmath8-tuple of channel uses .",
    "then each channel input @xmath28 is some function @xmath29 of the source and previous outputs .",
    "assume ( as usual ) that @xmath27 is statistically independent of @xmath30 .",
    "( 64,16)(0,0 ) ( -5,4)(8,6)@xmath27 ( 1,7)(1,0)5 ( 6,4)(19,6)@xmath31 ( 25,7)(1,0)15 ( 42,7)(1,0)16 ( 45,5.5)(8,6)@xmath32 ( 47 , 7)(0,-1)6 ( 47 , 1)(-1 , 0)35 ( 12 , 1)(0,1)3 ( 28.5,5.5)(8,6)@xmath33 ( 41,7)(41,12)(0,-1)4 ( 41,7.5)(0,-1)1(40.5,7)(1,0)1 ( 58,4)(10,6)decoder ( 37,10.5)(9.5,6)@xmath34 ( 73,4)(6,6)[l]@xmath35 ( 68,7)(1,0)4    elias @xcite was interested in the situation where @xmath36 is a gaussian random variable rather than a discrete message . for @xmath37 , the rate - distortion bound ( with a mean - square distortion measure ) is achieved without coding or feedback . for @xmath16 , attempts to map @xmath27 into an @xmath8 dimensional channel input in the absence of feedback involve non - linear or twisted modulation techniques that are ugly at best . using the ideal feedback , however , elias constructed a simple and elegant procedure for using the @xmath8 channel symbols to send @xmath27 in such a way as to meet the rate - distortion bound with equality .",
    "let @xmath38 $ ] be an arbitrary choice of energy , _",
    "i.e. _ , second moment , for each @xmath39 , @xmath40 .",
    "it will be shown shortly that the optimal choice for @xmath41 , subject to ( [ pc ] ) , is @xmath42 for @xmath40 .",
    "elias s strategy starts by choosing the first transmitted signal @xmath43 to be a linear scaling of the source variable @xmath27 , scaled to meet the second - moment constraint , _ i.e. _ , @xmath44 at the receiver , the minimum mean - square error ( mmse ) estimate of @xmath43 is @xmath45 =   \\frac{s_1 y_1}{1+s_1}$ ] , and the error in that estimate is @xmath46 .",
    "it is more convenient to keep track of the mmse estimate of @xmath27 and the error @xmath47 in that estimate .",
    "since @xmath27 and @xmath43 are the same except for the scale factor @xmath48 , these are given by @xmath49   & = \\frac{\\sigma_1\\sqrt{s_1}\\,y_1}{1 + s_1}\\\\ \\label{estimate1b } u_2&= u_1 - { \\textsf{e}}[u_1|y_1 ]   \\end{aligned}\\ ] ] where @xmath50 and @xmath51 .",
    "using the feedback , the transmitter can calculate the error term @xmath47 at time @xmath52 .",
    "elias s strategy is to use @xmath47 as the source signal ( without a second - moment constraint ) for the second transmission .",
    "this unconstrained signal @xmath47 is then linearly scaled to meet the second moment constraint @xmath53 for the second transmission .",
    "thus the second transmitted signal @xmath54 is given by @xmath55 we use this notational device throughout , referring to the unconstrained source signal to be sent at time @xmath39 by @xmath56 and to the linear scaling of @xmath56 , scaled to meet the second moment constraint @xmath57 , as @xmath28 .",
    "the receiver calculates the mmse estimate @xmath58 =   \\frac{\\sigma_2\\sqrt{s_2}\\,y_2}{1 + s_2}$ ] and the transmitter then calculates the error in this estimate , @xmath59 $ ] .",
    "note that @xmath60\\\\ & = u_3 + { \\textsf{e}}[u_2|y_2 ] + { \\textsf{e}}[u_1|y_1 ] .\\end{aligned}\\ ] ] thus @xmath61 can be viewed as the error arising from estimating @xmath27 by @xmath62 + { \\textsf{e}}[u_2|y_2]$ ] .",
    "the receiver continues to update its estimate of @xmath27 on subsequent channel uses , and the transmitter continues to transmit linearly scaled versions of the current estimation error .",
    "then the general expressions are as follows : @xmath63    & =   \\frac{\\sigma_i\\sqrt{s_i}\\,y_i}{1 + s_i};\\\\ \\label{estimateic } u_{i+1 }   & =   u_i - { \\textsf{e}}[u_i|y_i ] .   \\end{aligned}\\ ] ] where @xmath64 and @xmath65 .",
    "iterating on equation ( [ estimateic ] ) from @xmath66 to @xmath8 yields @xmath67.\\ ] ] similarly , iterating on @xmath68 , we get @xmath69    this says that the error arising from estimating @xmath27 by @xmath70 $ ] is @xmath71 .",
    "this is valid for any ( non - negative ) choice of @xmath41 , and this is minimized , subject to @xmath72 , by @xmath73 for @xmath40 . with this optimal assignment ,",
    "the mean square estimation error in @xmath27 after @xmath8 channel uses is @xmath74 we now show that this is the minimum mean - square error over all ways of using the channel .",
    "the rate - distortion function for this gaussian source with a squared - difference distortion measure is well known to be @xmath75 this is the minimum mutual information , over all channels , required to achieve a mean - square error ( distortion ) equal to @xmath76 . for @xmath77",
    ", @xmath78 is @xmath79 , which is the capacity of this channel over @xmath8 uses ( it was shown by shannon @xcite that feedback does not increase the capacity of memoryless channels ) .",
    "thus the elias scheme actually meets the rate - distortion bound with equality , and no other coding system , no matter how complex , can achieve a smaller mean - square error . note that ( [ iterv1 ] ) is also valid in the degenerate case @xmath37",
    "what is surprising about this result is not so much that it meets the rate - distortion bound , but rather that the mean - square estimation error goes down geometrically with @xmath8 .",
    "it is this property that leads directly to the doubly exponential error probability of the schalkwijk - kailath scheme .",
    "the schalkwijk and kailath ( sk ) scheme will now be defined in terms of the elias scheme , still assuming the discrete - time channel model of figure [ figelias ] and the power constraint of ( [ pc ] ) .",
    "the source is a set of @xmath80 equiprobable symbols , denoted by @xmath81 .",
    "the channel uses will now be numbered from @xmath82 to @xmath83 , since the use at time 0 will be quite distinct from the others .",
    "the source signal , @xmath84 is a standard @xmath80-pam modulation of the source symbol .",
    "that is , for each symbol @xmath85 , @xmath86 , from the source alphabet , @xmath85 is mapped into the signal @xmath87 where @xmath88 . thus the @xmath80 signals in @xmath84",
    "are symmetric around 0 with unit spacing .",
    "assuming equiprobable symbols , the second moment @xmath89 of @xmath84 is @xmath90 .",
    "the initial channel input @xmath91 is a linear scaling of @xmath84 , scaled to have an energy @xmath92 to be determined later .",
    "thus @xmath91 is an @xmath80-pam encoding , with signal separation @xmath93 . @xmath94",
    "the received signal @xmath95 is fed back to the transmitter , which , knowing @xmath91 , determines @xmath96 . in the following @xmath83 channel uses ,",
    "the elias scheme is used to send the gaussian random variable @xmath96 to the receiver , thus reducing the effect of the noise on the original transmission .",
    "after the @xmath83 transmissions to convey @xmath96 , the receiver combines its estimate of @xmath96 with @xmath97 to get an estimate of @xmath91 , from which the @xmath80-ary signal is detected .",
    "specifically , the transmitted and received signals for times @xmath98 are given by equations ( [ estimateia ] ) , ( [ estimateib ] ) and ( [ estimateic ] ) . at time 1 , the unconstrained signal @xmath27 is @xmath96 and @xmath99 = 1 $ ] .",
    "thus the transmitted signal @xmath43 is given by @xmath100 , where the second moment @xmath101 is to be selected later .",
    "we choose @xmath102 for @xmath103 for optimized use of the elias scheme , and thus the power constraint in ( [ pc ] ) becomes @xmath104 . at the end of transmission @xmath83 ,",
    "the receiver s estimate of @xmath96 from @xmath105 is given by ( [ finala ] ) as @xmath106 = \\sum_{i=1}^{n-1 } { \\textsf{e}}[u_i\\mid y_i].\\ ] ] the error in this estimate , @xmath107 $ ] , is a zero - mean gaussian random variable with variance @xmath108 , where @xmath109 is given by ( [ iterv1 ] ) to be @xmath110 since @xmath111 and @xmath112 + u_n$ ] we have @xmath113 = x_0 + u_n\\ ] ] where @xmath114    note that @xmath115 is a function of the noise vector @xmath116 and is thus statistically independent is independent of @xmath117 $ ] , and , second , that @xmath118 $ ] is a sufficient statistic for @xmath91 based on @xmath119 , ( i.e. @xmath120=\\pr[x_0\\mid \\tilde{y}]$ ] ) .",
    "thus this detection strategy is not as ad hoc as it might initially seem .",
    "] of @xmath91 .",
    "thus , detecting @xmath91 from @xmath121 $ ] ( which is known at the receiver . )",
    "is the simplest of classical detection problems , namely that of detecting an @xmath80-pam signal @xmath91 from the signal plus an independent gaussian noise variable @xmath122 . using maximum likelihood detection",
    ", an error occurs only if @xmath122 exceeds half the distance between signal points , _",
    "i.e. _ , if @xmath123 .",
    "since the variance of @xmath122 is @xmath124 , the probability of error is given by in ( [ pe1 ] ) arises because the largest and smallest signals each have only one nearest neighbor , whereas all other signals have two nearest neighbors . ]",
    "@xmath125 where @xmath126 and @xmath127 is the complementary distribution function of @xmath128 , _",
    "i.e. _ , @xmath129 choosing @xmath92 and @xmath101 , subject @xmath130 , to maximize @xmath131 ( and thus minimize @xmath132 ) , we get @xmath133 .",
    "that is , if @xmath134 is less than 1 , all the energy is used to send @xmath91 and the feedback is unused .",
    "we assume @xmath135 in what follows , since for any given @xmath136 this holds for large enough @xmath8 . in this case",
    ", @xmath92 is one unit larger than @xmath101 , leading to @xmath137 substituting ( [ enerdis ] ) into ( [ pe1 ] ) , @xmath138 where @xmath139    this is an _ exact _ expression for error probability , optimized over energy distribution , and using @xmath80-pam followed by the elias scheme and ml detection .",
    "it can be simplified as an upper bound by replacing the coefficient @xmath140 by 1 .",
    "also , since @xmath141 is a decreasing function of its argument , @xmath132 can be further upper bounded by replacing @xmath142 by @xmath143 .",
    "thus , @xmath144 where @xmath145    for large @xmath80 , which is the case of interest , the above bound is very tight and is essentially an equality , as first derived by schalkwijk rather than as estimating @xmath96 . ] in eq .",
    "12 of @xcite . recalling that @xmath146 we can further lower bound @xmath131 ( thus upper bounding @xmath132 ) . substituting @xmath147 and @xmath148",
    "we get @xmath149 \\exp(n(c(s)-r))\\ ] ] the term in brackets is decreasing in @xmath8 .",
    "thus , @xmath150 using this together with equations ( [ pe3a ] ) and ( [ pe4 ] ) we get , @xmath151 or more simply yet , @xmath152).\\ ] ] note that for @xmath153 , @xmath132 decreases as a second order exponential in @xmath8 .    in summary , then , we see that the use of standard @xmath80-pam at time 0 , followed by the elias algorithm over the next @xmath83 transmissions , followed by ml detection , gives rise to a probability of error @xmath132 that decreases as a second - order exponential for all @xmath153 .",
    "also @xmath132 satisfies ( [ pe4a ] ) and ( [ pe4aa ] ) for all @xmath154 .",
    "although @xmath132 decreases as a second - order exponential with this algorithm , the algorithm does not minimize @xmath132 over all algorithms using ideal feedback .",
    "the use of standard @xmath80-pam at time 0 could be replaced by pam with non - equal spacing of the signal points for a modest reduction in @xmath132 .",
    "also , as shown in the next section , allowing transmissions 1 to @xmath83 to make use of the discrete nature of @xmath91 allows for a major reduction in @xmath132.-pam , @xcite starts with a random ensemble of non - equally - spaced @xmath80-pam codes ingeniously arranged to form a gaussian random variable .",
    "the elias scheme is then used , starting with this gaussian random variable .",
    "thus the algorithm in @xcite has different constraints than those above .",
    "it turns out to have an insignificantly larger @xmath132 ( over this phase ) than the algorithm here for @xmath22 greater than @xmath155 $ ] and an insignificantly smaller @xmath132 otherwise . ]",
    "the algorithm above , however , does have the property that it is optimal among schemes in which , first , standard pam is used at time 0 and , second , for each @xmath39 , @xmath103 , @xmath28 is a linear function of @xmath96 and @xmath156 .",
    "the reason for this is that @xmath96 and @xmath157 are then jointly gaussian and the elias scheme minimizes the mean square error in @xmath96 and thus also minimizes @xmath132 .",
    "translating these results to a continuous time formulation where the channel is used 2w times per second , .",
    "this is a harmless and universally used abuse of the word bandwidth for channels without feedback , and refers to the ability to satisfy the nyquist criterion with arbitrarily little power sent out of band .",
    "it is more problematic with feedback , since it assumes that the sum of the propagation delay , the duration of the transmit pulse , the duration of the matched filter at the receiver , and the corresponding quantities for the feedback , is at most @xmath158 .",
    "even allowing for a small fraction of out - of - band energy , this requires considerably more than bandwidth @xmath159 . ] the capacity ( in nats per second ) is @xmath160 . letting @xmath161 and letting @xmath162 be the rate in nats per second",
    ", this formula becomes @xmath163 \\right).\\ ] ] let @xmath164 be the continuous - time power constraint , so that @xmath165 . in the broadband limit as @xmath166 for fixed @xmath167 , @xmath168 .",
    "since ( [ pe5 ] ) applies for all @xmath169 , we can simply go to the broadband limit , @xmath170 . since the algorithm is basically a discrete time algorithm",
    ", however , it makes more sense to view the infinite bandwidth limit as a limit in which the number of available degrees of freedom @xmath8 increases faster than linearly with the constraint time @xmath171 . in this case , the signal - to - noise ratio per degree of freedom , @xmath172 goes to 0 with increasing @xmath171 . rewriting @xmath131 in ( [ pe3a ] ) for this case , @xmath173\\\\ & \\ge & \\sqrt{3}\\exp\\left[\\frac{\\mathcal p t}{2 } - \\frac{1}{2 } -   \\frac{\\mathcal p^2t^2}{4n}-tr_\\infty\\right ] , \\label{bign}\\end{aligned}\\ ] ] where the inequality @xmath174 was used .",
    "note that if @xmath8 increases quadratically with @xmath171 , then the term @xmath175 is simply a constant which becomes negligible as the coefficient on the quadratic becomes large .",
    "for example , if @xmath176 , then this term is at most @xmath177 and ( [ bign ] ) simplifies to @xmath178 \\qquad \\mbox{for}\\quad n \\ge 6\\mathcal p^2t^2 . \\label{pe8}\\ ] ] this is essentially the same as the broadband sk result ( see the final equation in @xcite ) .",
    "the result in @xcite used @xmath179 degrees of freedom , but chose the subsequent energy levels to be decreasing harmonically , thus slightly weakening the coefficient of the result .",
    "the broadband result is quite insensitive to the energy levels used for each degree of freedom in ( [ pe1 ] ) by @xmath180 $ ] , each term of which can be lower bounded by the inequality @xmath181 .",
    "] , so long as @xmath92 is close to 1 and the other @xmath57 are close to 0 .",
    "this partly explains why the harmonic choice of energy levels in @xcite comes reasonably close to the optimum result .",
    "in the previous section , elias s scheme was used to allow the receiver to estimate the noise @xmath96 originally added to the pam signal at time @xmath82 .",
    "this gave rise to an equivalent observation , @xmath182 $ ] with attenuated noise @xmath122 as given in ( [ geonoise ] ) .",
    "the geometric attenuation of @xmath183 $ ] with @xmath8 is the reason why the error probability in the schalkwijk and kailath ( sk ) @xcite scheme decreases as a second order exponential in time .    in this section ,",
    "we explore an alternative strategy that is again based on the use of @xmath80-pam at time 0 , but is quite different from the sk strategy at times 1 to @xmath83 .",
    "the analysis is restricted to situations in which the signal - to - noise ratio ( snr ) at time 0 is so large that the distance between successive pam signal points in @xmath91 is large relative to the standard deviation of the noise . in this high snr regime , a simpler and more effective strategy than the elias scheme suggests itself ( see figure [ det ] ) .",
    "this new strategy is limited to the high snr regime , but section [ sectwophase ] develops a two - phase scheme that uses the sk strategy for the first part of the block , and switches to this new strategy when the snr is sufficiently large .    in this new strategy for the high snr regime , the receiver makes a tentative ml decision @xmath184 at time 0 . as seen in the figure ,",
    "that decision is correct unless the noise exceeds half the distance @xmath185 to either the signal value on the right or the left of the sample value @xmath87 of @xmath84 .",
    "each of these two events has probability @xmath186 .",
    "( 76,18)(-35,-5 ) ( -35,0)(1,0)80 ( 0,0)(0,1)11.5 ( 28,0)(0,1)11.5 ( -28,0)(0,1)11.5 ( 14,0)(0,1)5 ( -14,0)(0,1)5 ( 42,0)(0,1)5 ( 0,10)(2.1078,10)(6,6.065 ) ( 6,6.065)(9.582,2.444)(12,1.353 ) ( 12,1.353)(14.292,.317)(19.2,.111 ) ( 0,10)(-2.1078,10)(-6,6.065 ) ( -6,6.065)(-9.582,2.444)(-12,1.353 ) ( -12,1.353)(-14.292,.317)(-19.2,.111 ) ( -2,-4.5)(4,4)[l]@xmath187 ( 26,-4.5)(4,4)[l]@xmath188 ( -31,-4.5)(4,4)[l]@xmath189 ( 4.7,7.0)(4,4)[l]@xmath190 ( 2.5,0.2)(4,4)[l]@xmath191 ( 31,1.0)(4,4)[l]@xmath192 ( 0,3)(1,0)14 ( 0,3)(-1,0)14 ( 28,2)(1,0)14 ( 28,2)(-1,0)14    the transmitter uses the feedback to calculate @xmath184 and chooses the next signal @xmath27 ( in the absence of a second - moment constraint ) to be a shifted version of the original @xmath80-pam signal , shifted so that @xmath193 where @xmath85 is the original message symbol being transmitted . in other words , @xmath27 is the integer - valued error in the receiver s tentative decision @xmath194 of @xmath84 .",
    "the corresponding transmitted signal @xmath43 is essentially given by @xmath195}$ ] , where @xmath101 is the energy allocated to @xmath43 .",
    "we now give an approximate explanation of why this strategy makes sense and how the subsequent transmissions are chosen .",
    "this is followed by a precise analysis . temporarily ignoring the case where either @xmath196 or @xmath197 ( _ i.e. _ , where @xmath87 has only one neighbor ) , @xmath27 is @xmath82 with probability @xmath198 .",
    "the probability that @xmath199 is two or more is essentially negligible , so @xmath200 with a probability approximately equal to @xmath201 .",
    "thus @xmath202 \\approx 2q(d_0/2);\\qquad \\quad x_1 \\approx   \\frac{u_1\\sqrt{s_1}}{\\sqrt{2q(d_0/2})}\\ ] ] this means that @xmath43 is not only a shifted version of @xmath91 , but ( since @xmath93 ) is also scaled up by a factor that is exponential in @xmath92 when @xmath92 is sufficiently large .",
    "thus the separation between adjacent signal points in @xmath43 is exponentially increasing with @xmath92 .",
    "this also means that when @xmath43 is transmitted , the situation is roughly the same as that in figure [ det ] , except that the distance between signal points is increased by a factor exponential in @xmath92 .",
    "thus a tentative decision at time 1 will have an error probability that decreases as a second order exponential in @xmath92 .",
    "repeating the same procedure at time 2 will then give rise to a third order exponential in @xmath92 , etc .",
    "we now turn to a precise analysis and description of the algorithm at times 1 to @xmath83 .",
    "the following lemma provides an upper bound to the second moment of @xmath27 , which was approximated in ( [ approxu ] ) .",
    "[ lem : pam1 ] for any @xmath203 , let @xmath204 be a @xmath76-quantization of a normal random variable @xmath205 in the sense that for each integer @xmath206 , if @xmath207 $ ] , then @xmath208 .",
    "then @xmath209 $ ] is upper bounded by @xmath210\\leq \\tfrac{1.6}{d } \\exp [ -\\tfrac{d^2}{8}]\\ ] ]    note from figure [ det ] that , aside from a slight exception described below , @xmath193 is the same as the @xmath211-quantization of @xmath96 where @xmath93 .",
    "the slight exception is that @xmath184 should always lie between @xmath212 and @xmath80 .",
    "if @xmath213 , then @xmath214 , whereas the @xmath211-quantization takes on a larger integer value .",
    "there is a similar limit for @xmath215 .",
    "this reduces the magnitude of @xmath27 in the above exceptional cases , and thus reduces the second moment .",
    "thus the bound in the lemma also applies to @xmath27 . for simplicity in what follows",
    ", we avoid this complication by assuming that the receiver allows @xmath184 to be larger than @xmath80 or smaller than 1 .",
    "this increases both the error probability and the energy over true ml tentative decisions , so the bounds also apply to the case with true ml tentative decisions .    from the definition of @xmath204",
    ", we see that @xmath216 if @xmath207 $ ] .",
    "thus , for @xmath217 , @xmath218 = q(d\\ell-\\frac{d}{2 } ) - q(d\\ell+\\frac{d}{2})\\ ] ] from symmetry , @xmath219 = \\pr[u=\\ell]$ ] , so the second moment of @xmath204 is given by @xmath220 & = 2\\sum_{\\ell=1}^\\infty \\ell^2\\left [ q(d\\ell-\\frac{d}{2 } ) -   q(d\\ell+\\frac{d}{2})\\right]\\\\ & = 2 q(d/2 ) + 2\\sum_{\\ell=2}^\\infty [ \\ell^2 - ( \\ell-1)^2 ]   \\left[q(d\\ell-\\frac{d}{2})\\right].\\end{aligned}\\ ] ] using the standard upper bound @xmath221 $ ] for @xmath222 , and recognizing that @xmath223 , this becomes @xmath224 & \\leq \\frac{4}{\\sqrt{2\\pi}\\,d}\\left\\{\\exp[-d^2/8 ] + \\sum_{\\ell=2}^\\infty \\exp[-(2\\ell-1)^2d^2/8]\\right\\ } \\\\ { \\nonumber } & = \\frac{4}{\\sqrt{2\\pi}\\,d}\\exp[-d^2/8]\\left\\{1 + \\sum_{\\ell=2}^\\infty \\exp[-4\\ell(\\ell-1)d^2/8]\\right\\}\\\\ { \\nonumber } & \\leq \\frac{4}{\\sqrt{2\\pi}\\,d}\\exp[-d^2/8]\\left\\{\\frac{1}{1-\\exp(-d^2)}\\right\\}\\\\ & \\leq \\frac{1.6}{d } \\exp[-\\tfrac{d^2}{8 } ] \\qquad\\mbox{for}\\,\\ , d\\ge 4 .",
    "\\label{gamn}\\end{aligned}\\ ] ]    we now define the rest of this new algorithm . we have defined the unconstrained signal @xmath27 at time 1 to be @xmath225 but have not specified the energy constraint to be used in amplifying @xmath27 to @xmath43 .",
    "the analysis is simplified by defining @xmath43 in terms of a specified scaling factor between @xmath27 and @xmath43 .",
    "the energy in @xmath43 is determined later by this scaling .",
    "in particular , let @xmath226 the peculiar expression for @xmath227 above looks less peculiar when expressed as @xmath228 .",
    "when @xmath229 is received , we can visualize the situation from figure [ det ] again , where now @xmath211 is replaced by @xmath227 .",
    "the signal set for @xmath43 is again a pam set but it now has signal spacing @xmath227 and is centered on the signal corresponding to the transmitted source symbol @xmath85 .",
    "the signals are no longer equally likely , but the analysis is simplified if a maximum likelihood tentative decision @xmath230 is again made .",
    "we see that @xmath231 where @xmath232 is the @xmath227-quantization of @xmath233 ( and where the receiver again allows @xmath230 to be an arbitrary integer ) .",
    "we can now state the algorithm for each time @xmath39 , @xmath234 .",
    "@xmath235 where @xmath236 is the @xmath237-quantization of @xmath238 .",
    "[ lem : highsnr ] for @xmath239 , the algorithm of ( [ newalg0])-([newalg3 ] ) satisfies the following for all alphabet sizes @xmath80 and all message symbols @xmath85 : @xmath240 & \\le & \\frac{12.8}{d_{i-1}}. \\\\",
    "\\label{newalg6 } \\sum_{i=1}^\\infty { \\textsf{e}}[x_i^2 ] & \\le & 5 .",
    "\\\\ \\label{newalg7 } \\pr(\\hat m_i \\ne m ) & \\le & 1/g_{i+1}(2),\\end{aligned}\\ ] ] where @xmath241 with @xmath39 exponentials .    from the definition of @xmath237 in ( [ newalg0 ] ) ,",
    "@xmath242 this establishes the first part of ( [ newalg4 ] ) and the inequality follows since @xmath239 and @xmath243 is increasing in @xmath5 .",
    "next , since @xmath244 , we can use ( [ newalg4 ] ) and lemma [ lem : pam1 ] to see that @xmath245   & = d_i^2 { \\textsf{e}}[u_i^2 ] \\\\ & = \\left(8 \\exp(\\frac{d_{i-1}^2}{8})\\right)\\left ( \\frac{1.6}{d_{i-1 } } \\exp(-\\frac{d_{i-1}^2}{8})\\right)\\\\ & \\le \\,\\frac{12.8}{d_{i-1}},\\end{aligned}\\ ] ] where we have canceled the exponential terms , establishing ( [ newalg5 ] ) .    to establish ( [ newalg6 ] ) , note that each @xmath237 is increasing as a function of @xmath211 , and thus each @xmath246 $ ] is upper bounded by taking @xmath239 to be 4 .",
    "then @xmath247 = 3.2 $ ] , @xmath248 = 1.6648 $ ] , and the other terms can be bounded in a geometric series with a sum less than 0.12 .    finally , to establish ( [ newalg7 ] ) , note that @xmath249\\\\ & \\mathop{\\le}^{(a ) } \\frac{1.6}{d_i}\\exp(-d_i^2/8 ) \\mathop{\\le}^{(b ) }   \\exp(-d_i^2/8)\\\\ & \\mathop{=}^{(c ) } 1/\\exp ( g_i(d_0 ^ 2/8 ) ) \\mathop{\\le}^{(d ) } 1/g_{i+1}(2),\\end{aligned}\\ ] ] where we have used lemma [ lem : pam1 ] in @xmath250 , the fact that @xmath251 in @xmath252 , and equation ( [ newalg4 ] ) in @xmath253 and @xmath254 .    we have now shown that , in this high snr regime , the error probability decreases with time @xmath39 as an @xmath39th order exponent .",
    "the constants involved , such as @xmath255 are somewhat ad hoc , and the details of the derivation are similarly ad hoc .",
    "what is happening , as stated before , is that by using pam centered on the receiver s current tentative decision , one can achieve rapidly expanding signal point separation with small energy .",
    "this is the critical idea driving this algorithm , and in essence this idea was used earlier by zigangirov @xcite",
    "we now combine the shalkwijk - kailath ( sk ) scheme of section [ secsk ] and the high snr scheme of section [ secpam ] into a two phase strategy .",
    "the first phase , of block length @xmath256 , uses the sk scheme . at time @xmath257 , the equivalent received signal @xmath258 $ ] , ( see ( [ geonoise ] ) ) , is used in an ml decoder to detect the original pam signal @xmath91 in the presence of additive gaussian noise of variance @xmath259 .",
    "note that if we scale the equivalent received signal , @xmath260 $ ] by a factor of @xmath261 so as to have an equivalent unit variance additive noise , we see that the distance between adjacent signal points in the normalized pam is @xmath262 where @xmath263 is given in ( [ pe1 ] ) .",
    "if @xmath256 is selected to be large enough to satisfy @xmath264 , then this detection at time @xmath257 satisfies the criterion assumed at time 0 of the high snr algorithm of section [ secpam ] . in other words ,",
    "the sk algorithm not only achieves the error probability calculated in section [ secsk ] , but also , if the block length of the sk phase @xmath256 is chosen to be large enough , it creates the initial condition for the high snr algorithm .",
    "that is , it provides the receiver and the transmitter at time @xmath257 with the output of a high signal - to - noise ratio pam .",
    "consequently not only is the tentative ml decision at time @xmath257 correct with moderately high probability , but also the probability of the distant neighbors of the decoded messages vanishes rapidly .",
    "the intuition behind this two - phase scheme is that the sk algorithm seems to be quite efficient when the signal points are so close ( relative to the noise ) that the discrete nature of the signal is not of great benefit .",
    "when the sk scheme is used enough times , however , the signal points becomes far apart relative to the noise , and the discrete nature of the signal becomes important .",
    "the increased effective distance between the signal points of the original pam also makes the high snr scheme , feasible .",
    "thus the two - phase strategy switches to the high snr scheme at this point and the high snr scheme drives the error probability to 0 as an @xmath265 order exponential .",
    "we now turn to the detailed analysis of this two - phase scheme .",
    "note that 5 units of energy must be reserved for phase 2 of the algorithm , so the power constraint @xmath101 for the first phase of the algorithm is @xmath266 . for any fixed rate @xmath267 , we will find that the remaining @xmath268 time units is a linearly increasing function of @xmath8 and yields an error probability upper bounded by @xmath269 .",
    "for the finite - bandwidth case , we assume an overall block length @xmath270 , an overall power constraint @xmath22 , and an overall rate @xmath271 .",
    "the overall energy available for phase @xmath212 is at least @xmath272 , so the average power in phase @xmath212 is at least @xmath273 .",
    "we observed that the distance @xmath274 between adjacent signal points , assuming that signal and noise are normalized to unit noise variance , is twice the parameter @xmath263 given in ( [ pe3 ] ) . rewriting ( [ pe3 ] ) for the power constraint @xmath273 , @xmath275 where to get @xmath250 we assumed that @xmath276 . we can also show that the multiplicative term , @xmath277 , is a decreasing function of @xmath256 satisfying @xmath278 this establishes ( [ peph2 ] ) . in order to satisfy @xmath279",
    ", it suffices for the right - hand side of ( [ peph2 ] ) to be greater than or equal to @xmath280 .",
    "letting @xmath281 , this condition can be rewritten as @xmath282 \\ge   \\tfrac{2 e^3}{\\sqrt{{3}}}.\\ ] ] define @xmath283 by @xmath284 this is a concave increasing function for @xmath285 and can be interpreted as the capacity of the given channel if the number of available degrees of freedom is reduced from @xmath8 to @xmath286 without changing the available energy per block , _",
    "i.e. _ , it can be interpreted as the capacity of a continuous time channel whose bandwidth has been reduced by a factor of @xmath287 .",
    "we can then rewrite ( [ fofnu ] ) as @xmath288 where @xmath289 .",
    "this is interpreted in figure [ figfofnu ] .",
    "( 76,18)(-8,-3.5 ) ( 0,0)(1,0)40 ( 30,0)(0,1)16 ( 17.3,11)(0,-1)11 ( 12.4,11)(0,-1)11 ( 0,0)(0,7.21)(15,11.88 ) ( 30,15)(20.97,13.74)(15,11.88 ) ( 0,5.66)(4.3,7)(30,15 )",
    "( 31,14.4)@xmath290 ( 31,6.4)@xmath291 ( 31,10.4)@xmath292 ( 12,-2.1)@xmath293 ( 30,11)(-1,0)17.6 ( 3,-2.1)@xmath294 ( 0,-2.1)@xmath82 ( 16.8,-2.1)@xmath295 ( 29.7,-2.1)@xmath212 ( 22,.6)@xmath287 ( 21,14.5)@xmath283 ( 30,7)(-1,0)25.7(4.3,7)(0,-1)7    the condition @xmath296 is satisfied by choosing @xmath297 for @xmath295 defined in figure [ figfofnu ] , _ i.e. _ , @xmath298thus the duration @xmath265 of phase 2 can be chosen to be @xmath299 - \\frac{\\beta(1-\\phi^{-1}(r))}{c - r}\\right\\rfloor.\\ ] ]    this shows that @xmath265 increases linearly with @xmath8 at rate @xmath300 for @xmath301 . as a result of lemma [ lem : highsnr ] the error probability is upper bounded as @xmath302 thus the probability of error is bounded by an exponential order that increases at a rate @xmath300 .",
    "we later derive a lower bound to error probability which has this same rate of increase for the exponential order of error probability .",
    "the broadband case is somewhat simpler since an unlimited number of degrees of freedom are available . for phase 1 , we start with equation ( [ pe6 ] ) , modified by the fact that @xmath303 units of energy must be reserved for phase 2 .",
    "@xmath304\\\\ \\notag & \\ge   2\\sqrt{3}\\exp\\left[\\frac{\\mathcal p t}{2 } - 3 - \\frac{\\mathcal   p^2t^2}{4n_1}-t r_\\infty\\right],\\end{aligned}\\ ] ] where , in order to get the inequality in the second step , we assumed that @xmath305 and used the identity @xmath306 . as in the broadband sk analysis , we assume that @xmath256 is increasing quadratically with increasing @xmath171 . then @xmath307 becomes just a constant . specifically if @xmath308 we get , @xmath309,\\ ] ] it follows that @xmath296 if @xmath310 if ( [ pe9ph2 ] ) is satisfied , then phase 2 can be carried out for arbitrarily large @xmath265 , with @xmath132 satisfying ( [ order3 ] ) . in principle",
    ", @xmath265 can be infinite , so @xmath132 becomes 0 whenever @xmath171 is large enough to satisfy([pe9ph2 ] ) .",
    "one might object that the transmitter sequence is not well defined with @xmath311 , but in fact it is , since at most a finite number of transmitted symbols can be nonzero .",
    "one might also object that it is impossible to obtain an infinite number of ideal feedback signals in finite time .",
    "this objection is certainly valid , but the entire idea of ideal feedback with infinite bandwidth is unrealistic .",
    "perhaps a more comfortable way to express this result is that 0 is the greatest lower bound to error probability when ( [ pe9ph2 ] ) is satisfied , _ i.e. _ , any desired error probability , no matter how small is achievable if the continuous - time block length @xmath171 satisfies ( [ pe9ph2 ] ) .",
    "the previous sections have derived upper bounds to the probability of decoding error for data transmission using particular block coding schemes with ideal feedback .",
    "these schemes are non - optimal , with the non - optimalities chosen both for analytical convenience and for algorithmic simplicity .",
    "it appears that the optimal strategy is quite complicated and probably not very interesting .",
    "for example , even with a block length @xmath37 , and a message set size @xmath312 , pam with equi - spaced messages is neither optimal in the sense of minimizing average error probability over the message set ( see exercise 6.3 of @xcite ) nor in the sense of minimizing the error probability of the worst message . aside from this rather unimportant non - optimality ,",
    "the sk scheme is also non - optimal in ignoring the discrete nature of the signal until the final decision .",
    "finally , the improved algorithm of section [ sectwophase ] is non - optimal both in using ml rather than maximum _ a posteriori _ probability ( map ) for the tentative decisions and in not optimizing the choice of signal points as a function of the prior received signals .    the most important open question , in light of the extraordinarily rapid decrease of error probability with block length for the finite bandwidth case , is whether any strictly positive lower bound to error probability exists for fixed block length @xmath8 . to demonstrate that there is such a positive lower bound we first derive a lower bound to error probability for the special case of a message set of size @xmath313 .",
    "then we generalize this to codes of arbitrary rate and show that for @xmath314 , the lower bound decreases as a @xmath0th order exponential where @xmath0 increases with the block length @xmath8 and has the form @xmath315 where the coefficient @xmath13 is the same as that in the upper bound in section [ sectwophase ] .",
    "it is more convenient in this section to number the successive signals from 1 to @xmath8 rather than @xmath82 to @xmath83 as in previous sections .",
    "although it is difficult to find and evaluate the entire optimal code , even for @xmath313 , it turns out to be easy to find the optimal encoding in the last step .",
    "thus , for each @xmath316 , we want to find the optimal choice of @xmath317 as a function of , first , the encoding functions @xmath318 , @xmath103 , and , second , the allocation of energy , @xmath319 $ ] for that @xmath316 .",
    "we will evaluate the error probability for such an optimal encoding at time @xmath8 and then relate it to the error probability that would have resulted from decoding at time @xmath83 .",
    "we will use this relation to develop a recursive lower bound to error probability at each time @xmath39 in terms of that at time @xmath320 .    for a given code function @xmath318 for @xmath321 , the conditional probability density$ ] where @xmath322 is the normal density @xmath323 .",
    "] of @xmath324 given @xmath325 or 2 is positive for all sample values for @xmath324 ; thus the corresponding conditional probabilities of hypotheses @xmath325 and @xmath326 are positive i.e. @xmath327 in particular , for @xmath328 , define @xmath329 for some given @xmath316 . finding the error probability @xmath330 is an elementary binary detection problem for the given @xmath316 .",
    "map detection , using the _ a priori _ probabilities @xmath331 and @xmath332 , minimizes the resulting error probability .    for a given sample value of @xmath316 , let @xmath333 and @xmath334 be the values of @xmath335 for @xmath336 and @xmath52 respectively",
    "let @xmath13 be half the distance between @xmath333 and @xmath334 , _",
    "i.e. _ , @xmath337 . the error probability @xmath338 depends on @xmath333 and @xmath334 only through @xmath13 . for a given @xmath339 , we choose @xmath333 and @xmath334 to satisfy @xmath340 = 0 $ ] , thus maximizing @xmath13 for the given @xmath339 .",
    "the variance of @xmath335 conditional on @xmath316 is given by @xmath341and since @xmath340 = 0 $ ] , this means that @xmath13 is related to @xmath339 by @xmath342 .",
    "now let @xmath343 .",
    "note that @xmath344 is the probability of error for a hypothetical map decoder detecting @xmath204 at time @xmath83 from @xmath316 .",
    "the error probability @xmath338 for the map decoder at the end of time @xmath8 is given by the classic result of binary map detection with _ a priori _ probabilities @xmath344 and @xmath345 , @xmath346 where @xmath347 and @xmath348 .",
    "this equation relates the error probability @xmath338 at the end of time @xmath8 to the error probability @xmath344 at the end of time @xmath83 , both conditional on @xmath316 .",
    "we are now going to view @xmath338 and @xmath344 as functions of @xmath316 , and thus as random variables .",
    "similarly @xmath349 can be any non - negative function of @xmath316 , subject to a constraint @xmath350 on its mean ; so we can view @xmath339 as an arbitrary non - negative random variable with mean @xmath350 . for each @xmath316 , @xmath351 and @xmath344 determine the value of @xmath13 ;",
    "thus @xmath13 is also a non - negative random variable .",
    "we are now going to lower bound the expected value of @xmath338 in such a way that the result is a function only of the expected value of @xmath344 and the expected value @xmath350 of @xmath339 .",
    "note that @xmath338 in ( [ exact ] ) can be lower bounded by ignoring the first term and replacing the second term with @xmath352 .",
    "thus ,    @xmath353    where the last step uses the facts that @xmath127 is a decreasing function of @xmath5 and that @xmath354 .",
    "@xmath355 & \\ge & { \\textsf{e}}[\\phi ] q\\left(\\frac{1}{{\\textsf{e}}[\\phi ] } { \\textsf{e}}\\left[\\phi \\sqrt{\\frac{\\tilde s}{2\\phi   } } \\right]\\right)\\\\ & = & \\nonumber { \\textsf{e}}[\\phi ] q\\left(\\frac{1}{\\sqrt{2}{\\textsf{e}}[\\phi ] } { \\textsf{e}}\\left[\\sqrt{\\phi \\tilde s}\\right]\\right)\\\\ \\label{ineq2 } & \\ge & { \\textsf{e}}[\\phi ] q\\left(\\frac{1}{\\sqrt{2}{\\textsf{e}}[\\phi ] } \\sqrt{{\\textsf{e}}[\\phi ] { \\textsf{e}}[\\tilde s]}\\right)\\\\ & = & { \\textsf{e}}[\\phi ] q\\left(\\sqrt{\\frac{s_n}{2{\\textsf{e}}[\\phi ] } } \\,\\right ) .",
    "\\label{ineq3}\\end{aligned}\\ ] ]    in ( [ ineq1 ] ) , we used jensen s inequality , based on the facts that @xmath127 is a convex function for @xmath356 and that @xmath357 $ ] is a probability distribution on @xmath316 . in ( [ ineq2 ] ) , we used the schwarz inequality along with the fact that @xmath127 is decreasing for @xmath356 .",
    "we now recognize that @xmath358 $ ] is simply the overall error probability at the end of time @xmath8 and @xmath359 $ ] is the overall error probability ( if a map decision were made ) at the end of time @xmath83 .",
    "thus we denote these quantities as @xmath360 and @xmath361 respectively , @xmath362 note that this lower bound is monotone increasing in @xmath361 .",
    "thus we can further lower bound @xmath360 by lower bounding @xmath361 .",
    "we can lower bound @xmath361 ( for a given @xmath363 and @xmath364 ) in exactly the same way , so that @xmath365 .",
    "these two bounds can be combined to implicitly bound @xmath366 in terms of @xmath363 , @xmath350 and @xmath364 .",
    "in fact , the same technique can be used for each @xmath367 , getting @xmath368 this gives us a recursive lower bound on @xmath366 for any given choice of @xmath369 subject to the power constraint @xmath370 .",
    "we have been unable to find a clean way to optimize this over the choice of @xmath41 , so as a very crude lower bound on @xmath360 , we upper bound each @xmath57 by @xmath134 . for convenience , multiply each side of ( [ lagrange5 ] ) by @xmath371 , @xmath372 at this point , we can see what is happening in this lower bound . as @xmath373 approaches 0 ,   @xmath374 . also @xmath375 approaches @xmath82 as @xmath376 .",
    "now we will lower bound the expression on the right hand side of ( [ lagrange7 ] ) .",
    "we can check numerically ) is satisfied for @xmath377 and verify that the right - hand side is decreasing faster than the left for @xmath378 . ]",
    "that for @xmath379 , @xmath380 furthermore @xmath381 is decreasing in @xmath5 for all @xmath222 , and thus @xmath382 substituting this into ( [ lagrange7 ] ) we get , @xmath383 applying this recursively for @xmath384 down to @xmath385 for any @xmath386 we get , @xmath387}.\\end{aligned}\\ ] ] where @xmath250 simply follows from the fact that @xmath388 .",
    "this bound holds for @xmath389 , giving an overall lower bound on error probability in terms of @xmath390 . in the usual case where the symbols are initially equiprobable , @xmath391 and @xmath392}.\\ ] ] note that this lower bound is an @xmath8th order exponential .",
    "although it is numerically much smaller than the upper bound in section [ sectwophase ] , it has the same general form .",
    "the intuitive interpretation is also similar . in going from block length @xmath83 to @xmath8 , with very small error probability at @xmath83 , the symbol of large _",
    "a priori _ probability is very close to 0 and the other symbol is approximately at @xmath393 .",
    "thus the error probability is decreased in one time unit by an exponential in @xmath361 , leading to an @xmath8th order exponential over @xmath8 time units .",
    "next consider feedback codes of arbitrary rate @xmath314 with sufficiently large blocklength @xmath8 and @xmath394 codewords .",
    "we derive a lower bound on error probability by splitting @xmath8 into an initial segment of length @xmath256 and a final segment of length @xmath395 .",
    "this segmentation is for bounding purposes only and does not restrict the feedback code .",
    "the error probability of a hypothetical map decoder at the end of the first segment , @xmath396 , can be lower bounded by a conventional use of the fano inequality .",
    "we will show how to use this error probability as the input of the lower bound for @xmath313 case derived in the previous subsection , i.e. , equation ( [ nkorder ] ) .",
    "there is still the question of allocating power between the two segments , and since we are deriving a lower bound , we simply assume that the entire available energy is available in the first segment , and can be reused in the second segment .",
    "we will find that the resulting lower bound has the same form as the upper bound in section [ sectwophase ] .",
    "using energy @xmath397 over the first segment corresponds to power @xmath398 , and since feedback does not increase the channel capacity , the average directed mutual information over the first segment is at most @xmath399 . reusing the definitions @xmath281 and @xmath400 from section [ sectwophase ] , @xmath401 the entropy of the source is @xmath402 , and thus the conditional entropy of the source given @xmath403 satisfies @xmath404   & \\le & h(u | { { \\textbf{\\textit{y}}}}_1^{n_1 } ) \\\\ \\notag & \\le & h(p_e(n_1 ) ) + p_e(n_1)nr\\\\ & \\le & \\ln 2 + p_e(n_1)nr , \\label{fano2}\\end{aligned}\\ ] ] where we have used the fano inequality and then bounded the binary entropy @xmath405 by @xmath406 .    to use ( [ fano2 ] ) as a lower bound on @xmath396 ,",
    "it is necessary for @xmath407 to be small enough that @xmath283 is substantially less than @xmath291 , and to be specific we choose @xmath287 to satisfy @xmath408 with this restriction , it can be seen from ( [ fano2 ] ) that @xmath409 figure [ figfofnu1 ] illustrates that the following choice of @xmath256 in ( [ boundn1a ] ) satisfies both equation ( [ boundn1 ] ) and equation ( [ fano ] ) .",
    "this uses the fact that @xmath283 is a monotonically increasing concave function of @xmath287 .",
    "@xmath410    ( 76,18)(-8,-2 ) ( 0,0)(1,0)40 ( 30,0)(0,1)16 ( 30,8.5)(-1,0)26.1 ( 3.9,8.5)(0,-1)8.5 ( 6.75,8.5)(0,-1)8.5 ( 30,10)(-1,0)20.2 ( 9.8,10)(0,-1)10 ( 0,0)(0,7.21)(15,11.88 ) ( 30,15)(20.97,13.74)(15,11.88 ) ( 0,7.575)(9.8,10)(30,15 ) ( 31,14.4)@xmath290 ( 31,9.7)@xmath291 ( 31,7.6)@xmath411 ( 6,-2.1)@xmath293 ( 9,-2.1)@xmath294 ( 0,-2.1)@xmath82 ( 3,-2.1)@xmath295 ( 29.7,-2.1)@xmath212 ( 20,.6)@xmath287 ( 18,14)@xmath283    [ figfofnu1 ]    the corresponding choice for @xmath265 is @xmath412 + \\frac{1-\\phi^{-1}(r)}{c - r } \\right\\rceil.\\ ] ] thus with this choice of @xmath413 , the error probability at the end of time @xmath256 satisfies ( [ fano ] )",
    ".    the straightforward approach at this point would be to generalize the recursive relationship in ( [ lagrange5 ] ) to arbitrary @xmath80 .",
    "this recursive relationship could then be used , starting at time @xmath414 and using each successively smaller @xmath39 until terminating the recursion at @xmath415 where ( [ fano ] ) can be used .",
    "it is simpler , however , since we have already derived ( [ lagrange5 ] ) for @xmath313 , to define a binary coding scheme from any given @xmath80-ary scheme in such a way that the binary results can be used to lower bound the @xmath80-ary results .",
    "this technique is similar to one used earlier in @xcite .",
    "let @xmath318 for @xmath40 be any given coding function for @xmath416 . that code is used to define a related binary code .",
    "in particular , for each received sequence @xmath403 over the first segment , we partition the message set @xmath417 into two subsets , @xmath418 and @xmath419 .",
    "the particular partition for each @xmath403 is defined later .",
    "this partitioning defines a binary random variable @xmath420 as follows , @xmath421 at the end of the transmission , the receiver will use its decoder to decide @xmath422 .",
    "we define the decoder for @xmath420 at time @xmath8 , using the decoder of @xmath204 as follows , @xmath423 note that with the above mentioned definitions , whenever the @xmath80-ary scheme decodes correctly , the related binary scheme does also , and thus the error probability @xmath424 for the @xmath80-ary scheme must be greater than or equal to the error probability @xmath360 of the related binary scheme .",
    "the binary scheme , however , is one way ( perhaps somewhat bizarre ) of transmitting a binary symbol , and thus it satisfies the results as in section [ binary ] , but rather is a randomized binary scheme .",
    "that is , for a given @xmath403 and a given choice of @xmath420 , the subsequent transmitted symbols @xmath28 are functions not only of @xmath420 and @xmath425 , but also of a random choice of @xmath204 conditional on @xmath420 .",
    "the basic conclusion of ( [ lagrange5 ] ) is then justified by averaging over both @xmath425 and the choice of @xmath204 conditional on @xmath420 . ] of section [ binary ] . in particular , for the binary scheme , the error probability @xmath360 at time @xmath8 is lower bounded by the error probability @xmath426 at time @xmath256 by ( [ nkorder ] ) , @xmath427}.\\ ] ] our final task is to relate the error probability @xmath426 at time @xmath256 for the binary scheme to the error probability @xmath396 in ( [ fano ] ) for the @xmath80-ary scheme . in order to do this , let @xmath428 be the probability of message @xmath85 conditional on the received first segment @xmath403 .",
    "the map error probability for an @xmath80-ary decision at time @xmath256 , conditional on @xmath403 , is @xmath429 where @xmath430 .",
    "thus @xmath396 , given in ( [ fano ] ) , is the mean of @xmath431 over @xmath403 .",
    "now @xmath426 is the mean , over @xmath403 , of the error probability of a hypothetical map decoder for @xmath420 at time @xmath256 conditional on @xmath403 , @xmath432 .",
    "this is the smaller of the a posteriori probabilities of the subsets @xmath433 , @xmath434 conditional on @xmath403 , _",
    "i.e. _ , @xmath435 the following lemma shows that by an appropriate choice of partition for each @xmath403 , this binary error probability is lower bounded by 1/2 the corresponding @xmath80-ary error probability .",
    "[ lem : con ] for any probability distribution @xmath436 on a message set @xmath437 with @xmath438 , let @xmath439 .",
    "then there is a partition of @xmath417 into two subsets , @xmath440 and @xmath441 such that @xmath442    order the messages in order of decreasing @xmath443 .",
    "assign the messages one by one in this order to the sets @xmath440 and @xmath444 .",
    "when assigning the @xmath0th most likely message , we calculate the total probability of the messages that have already been assigned to each set , and assign the @xmath0th message to the set which has the smaller probability mass . if the probability mass of the sets are the same we choose one of the sets arbitrarily .",
    "with such a procedure , the difference in the probabilities of the sets , as they evolve , never exceeds @xmath445 .",
    "after all messages have been assigned , let @xmath446 we have seen that @xmath447 . since @xmath448 , ( [ msplit ] ) follows .",
    "since the error probability for the binary scheme is now at least one half of that for the @xmath80-ary scheme for each @xmath403 , we can take the mean over @xmath403 , getting @xmath449 . combining this with ( [ nkorderb ] ) and ( [ fano ] ) @xmath450},\\ ] ] where @xmath265 is given in ( [ boundn2a ] ) .",
    "the exact terms in this expression are not particularly interesting because of the very weak bounds on energy at each channel use .",
    "what is interesting is that the order of exponent in both the upper bound of ( [ order3 ] ) and ( [ n2lin ] ) and the lower bound here are increasing linearly is proportional to @xmath451 , so that this bound does not quite decrease with the exponential order @xmath265 .",
    "it does , however , decrease with an exponential order @xmath452 , where @xmath453 increases with @xmath8 much more slowly than , say , @xmath454 .",
    "thus @xmath455 is asymptotically proportional to @xmath300 .",
    "] at the same rate @xmath300 .",
    "the sk data transmission scheme can be viewed as ordinary pam combined with the elias scheme for noise reduction .",
    "the sk scheme can also be improved by incorporating the pam structure into the transmission of the error in the receiver s estimate of the message , particularly during the latter stages . for the bandlimited version ,",
    "this leads to an error probability that decreases with an exponential order @xmath456 where @xmath457 and @xmath458 is a constant . in the broadband version ,",
    "the error probability is zero for sufficiently large finite constraint durations @xmath171 .",
    "a lower bound to error probability , valid for all @xmath314 was derived .",
    "this lower bound also decreases with an exponential order @xmath459 where again @xmath457 and @xmath14 is essentially a constant .",
    "is a sublinear function of @xmath8 , i.e. @xmath460 . ]",
    "it is interesting to observe that the strategy yielding the upper bound uses almost all the available energy in the first phase , using at most 5 units of energy in the second phase .",
    "the lower bound relaxed the energy constraint , allowing all the allowable energy to be used in the first phase and then to be used repeatedly in each time unit of the second phase .",
    "the fact that both bounds decrease with the same exponential order suggests that the energy available for the second phase is not of primary importance .",
    "an open theoretical question is the minimum overall energy under which the error probability for two code words can be zero in the infinite bandwidth case .",
    "p.  elias .",
    " channel capacity without coding .",
    "quarterly progress report , mit research laboratory of electronics , oct 15 1956 .",
    "also in _ lectures on communication system theory _ , e. baghdady , ed .",
    ", new york : mcgraw hill , 1961 .",
    "a.  sahai , s.c .",
    "draper , and m.  gastpar . boosting reliability over awgn networks with average power constraints and noiseless feedback . in _ information theory , 2005 .",
    "isit 2005 . proceedings .",
    "international symposium on _ , pages 402406 , sept . 2005 ."
  ],
  "abstract_text": [
    "<S> schalkwijk and kailath ( 1966 ) developed a class of block codes for gaussian channels with ideal feedback for which the probability of decoding error decreases as a second - order exponent in block length for rates below capacity . </S>",
    "<S> this well - known but surprising result is explained and simply derived here in terms of a result by elias ( 1956 ) concerning the minimum mean - square distortion achievable in transmitting a single gaussian random variable over multiple uses of the same gaussian channel . a simple modification of the schalkwijk - kailath scheme is then shown to have an error probability that decreases with an exponential _ order _ which is linearly increasing with block length . in the infinite bandwidth limit , this scheme produces zero error probability using bounded expected energy at all rates below capacity . a lower bound on error probability for the finite bandwidth case </S>",
    "<S> is then derived in which the error probability decreases with an exponential order which is linearly increasing in block length at the same rate as the upper bound . </S>"
  ]
}