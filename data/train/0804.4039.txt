{
  "article_text": [
    "it is clear that processors technology is undergoing a vigorous shaking - up to allow one processor socket to provide access to multiple logical cores .",
    "current technology already allows multiple processor cores to be contained inside a single processor module .",
    "such chip multiprocessors seem to overcome the thermal and power problems that limit the performance that single - processor chips can deliver .",
    "recently , researchers have proposed multiprocessor platforms where individual processors have different computation capabilities ( e.g. , see @xcite ) .",
    "such architectures are attractive because a few high - performance complex processors can provide good serial performance , and many low - performance simple processors can provide high parallel performance .",
    "such asymmetric platforms can also achieve energy - efficiency since the lower the processing speed , the lower the power consumption is  @xcite .",
    "reducing the energy consumption is an important issue not only for battery operated mobile computing devices but also in desktop computers and servers .",
    "as the number of chip multiprocessors is growing tremendously , the need for algorithmic solutions that efficiently use such platforms is increasing as well . in these platforms",
    "a key assumption is that processors may have different speeds and capabilities but that the speeds and capabilities do not change .",
    "we consider multiprocessor architectures @xmath2 , where @xmath3 is the speed of processor @xmath4 .",
    "the total processing capability of the platform is denoted by @xmath5 .",
    "one of the key challenges of asymmetric computing is the scheduling problem .",
    "given a parallel program of @xmath6 tasks represented as a dependence graph , the scheduling problem deals with mapping each task onto the available asymmetric resources in order to minimize the makespan , that is , the maximum completion time of the jobs . in this work",
    "we also look into how to reduce energy consumption without affecting the makespan of the schedule .",
    "energy efficiency for speed scaling of parallel processors , which is not assumed in this work , was considered in @xcite .",
    "our notion of a parallel program to be executed is a set of @xmath6 tasks represented by @xmath7 , a directed acyclic graph ( dag ) .",
    "the set @xmath8 represents @xmath9 simple tasks each of a _ unit _ processing time .",
    "if task @xmath10 precedes task @xmath11 ( denoted via @xmath12 ) , then @xmath11 can not start until @xmath10 is finished .",
    "the set @xmath13 of edges represents precedence constraints among the tasks .",
    "we assume that the whole dag is presented as an input to the multiprocessor architecture .",
    "our objective is to give schedules that complete the processing of the whole dag in small time . using terminology from scheduling theory , the problem is that of scheduling precedence - constrained tasks on related processors to minimize the makespan . in our model",
    "the _ speed asymmetry _ is the basic characteristic .",
    "we assume that the overhead of ( re)assigning processors to tasks of a parallel job to be executed is _ negligible_.    the special case in which the dag is just a collection of chains is of importance because general dags can be scheduled via a maximal chain decomposition technique of @xcite .",
    "let @xmath14 a program of @xmath15 chains of tasks to be processed .",
    "we denote the length of chain @xmath16 by @xmath17 , the count of the jobs in @xmath16 ; without loss of generality @xmath18 .",
    "clearly @xmath19 . in this case",
    "the problem is also known as _",
    "chains scheduling_. note that the decomposition technique of @xcite requires @xmath20 time and the maximal chain decomposition depends only on the jobs of the given instance and is independent of the machine environment .    because the problem is np - hard @xcite",
    "even when all processors have the same speed , the scheduling community has concentrated on developing approximation algorithms for the makespan .",
    "early papers introduce @xmath21-approximation algorithms @xcite , and more recent papers propose @xmath22-approximation algorithms @xcite .",
    "numerous asymmetric processor organizations have been proposed for power and performance efficiency , and have investigated the behavior of multi - programmed single threaded applications on them .",
    "@xcite investigate the impact of performance asymmetry in emerging multiprocessor architectures .",
    "they conduct an experimental methodology on a multiprocessor systems where individual processors have different performance .",
    "they report that the running times of commercial application may benefit from such performance asymmetry .",
    "previous research assumed the general case where multiprocessor platforms have @xmath0 distinct speeds . yet recent technological advances ( e.g. , see @xcite ) build systems of two essential speeds .",
    "unfortunately , in the scheduling literature , the case of just @xmath23 distinct processor speeds has not been given much attention .",
    "in fact , the best till now results of @xcite reduce instances of arbitrary ( but related ) speeds , to at most @xmath24 distinct speeds .",
    "then the same work gives schedules of a makespan at most @xmath25 times the optimal makespan , where @xmath25 is @xmath26 for general dags .",
    "we consider architectures of chip multiprocessors consisting of @xmath27 processors , with @xmath28 fast processors of speed @xmath29 and of @xmath30 slow processors of speed @xmath31 where the energy consumption per unit time is a convex function of the processor speed .",
    "thus , our model is a special case of the uniformly related machines case , with only two _ distinct _ speeds .",
    "in fact , the notion of distinct speeds used in @xcite and @xcite allows several speeds for our model , but not differing much from each other .",
    "so for the case of 2 speeds , considered here , this gives a @xmath32-factor approximation for general dags .",
    "our goal here is to improve on this and under this simple model provide schedules with better makespan .",
    "we also focus on the special case where the multiprocessor system is composed of a single fast processor and multiple slow processors , like the one designed in @xcite .",
    "note that @xcite has recently worked on a different model that assumes asynchronous processors with time varying speeds .",
    "asymmetric platforms can achieve energy - efficiency since the lower the processing speed , the lower the power consumption is  @xcite . reducing energy consumption is important for battery operated mobile computing devices but also for desktop computers and servers . to examine the energy usage of multiprocessor systems we adopt the model of @xcite :",
    "the energy consumption per unit time is a convex function of the processor speed . in particular , the energy consumption of processor @xmath33 is proportional to @xmath34 , where @xmath35 is a constant . clearly by increasing the makespan of a schedule we can reduce the energy usage .",
    "we design the _ preemptive algorithm `` save - energy '' _ ( see alg.[algo : saveenergy ] ) that post processes a schedule of tasks to processors in order to improve the energy efficiency by reassigning tasks to processors of slower speed .",
    "we assume no restrictions in the number of speeds of the processors and rearrange tasks so that the makespan is not affected .",
    "this reduces the energy consumption since in our model the energy spent to process a task is proportional to the speed of the processor to the power of @xmath36 ( where @xmath37 ) . in this sense",
    ", our algorithm will optimize a given schedule so that maximum energy efficiency is achieved .",
    "split schedule in intervals @xmath38 , where @xmath39 $ ] sort times in ascending order .",
    "we start by sorting the processors according to the processing capability @xmath41 so that @xmath42 .",
    "we then split time in intervals @xmath38 , where @xmath39 $ ] , where @xmath43 is such that between these intervals there is not any preemption , no task completes and no changes are made to the precedence constraints",
    ". furthermore we denote @xmath44 if at @xmath38 we use @xmath45 and @xmath46 otherwise .",
    "so the total energy consuption of the schedule is @xmath47 .",
    "if @xmath13 is the optimal energy consumption of a schedule ( i.e. , no further energy savings can be achieved ) , the following holds : there does not exist any @xmath48 , where @xmath49 $ ] ,",
    "so that a list @xmath50 initially assigned to speed @xmath51 at time @xmath52 can be rescheduled to @xmath38 with speed @xmath53 and reduce energy .",
    "suppose that we can reduce the energy @xmath13 of the schedule .",
    "we obtain a contradiction .",
    "we can assume without any loss of generality that there exists at time @xmath52 a core @xmath54 that processes a list at speed @xmath51 and there is a @xmath38 so that we can reschedule it to processor @xmath55 with speed @xmath56 .",
    "this is so because if @xmath57 we will not have energy reduction .",
    "therefore since @xmath48 exists then the new energy @xmath58 must be lower than @xmath13 .",
    "there exist only three cases when we try to reschedule a list @xmath50 from @xmath52 to @xmath38 from @xmath51 to @xmath59 where @xmath56 : +    * _ the process of @xmath50 at @xmath52 fits exactly to @xmath38_. this is the case when @xmath60 . in this case @xmath61 .",
    "but this violates the requirement @xmath62 since @xmath63 because @xmath64 ( recall that @xmath65 ) . + * _ the process of @xmath50 at @xmath52 fits to @xmath38 and there remains time at @xmath38_. this is the case when @xmath66 .",
    "again we reach a contradiction since the new energy is the same with the previous case since @xmath66 and there exists @xmath67 so that @xmath68 . +",
    "* _ the process of @xmath50 at @xmath52 does not fit completely to @xmath38_. this is the case when @xmath69 .",
    "now we can not move all the processing of @xmath50 from @xmath52 to @xmath38 .",
    "so there exists @xmath70 so that @xmath71 .",
    "so the processing splits in two , at time @xmath52 for @xmath72 and completely to @xmath38 .",
    "the energy we save is @xmath73 because @xmath74 which proves the theorem .",
    "if the processing of list @xmath50 at @xmath52 at speed @xmath51 fits completely to @xmath38 to two different speeds or more , we save more energy if we reschedule the list to the speed which is closer to @xmath75{\\frac{1}{\\alpha}}\\cdot c(u ) $ ] , and when @xmath36 is @xmath23 it simplifies to @xmath76",
    ".    the whole processing of @xmath50 must not change .",
    "so @xmath77 is the time that the list will remain on speed @xmath51 and can be calculated by the equation @xmath78 .",
    "so the energy that we spend if we do not use @xmath59 is @xmath79 and if we use @xmath59 is @xmath80 .",
    "so @xmath81 and because @xmath82 we always save energy if we reschedule any task to a lower speed .",
    "the minimum energy occurs when the differential equals to zero . that happens when @xmath83{\\frac{1}{\\alpha}}\\cdot c(u)$ ] .",
    "now if the fragment of the list can be reassigned to a further smaller @xmath45 we obtain an even smaller energy schedule .",
    "thus we try to fill all the holes starting from lower speeds and going upwards , in order to prevent total fragmentation of the whole schedule and obtain a schedule of nearly optimal energy consumption on the condition of unharmed makespan .",
    "the algorithm  save - energy \" clearly does not increase the makespan since it does not delay the processing of any task , instead there may be even a reduction of the makespan . the new hole has size @xmath84 of the previous size and in every execution , a hole that can be filled goes to a faster processor .    in arbitrary dags",
    "the problem is that due to precedence constraints we can not swap two time intervals . to overcome this problem we proceed as follows : we define the supported set @xmath85 to be all the tasks that have been completed until time @xmath52 as well as those currently running and those who are ready to run . between two intervals that have the same we can swap , or reschedule any assignment so we run the above algorithm between all of these marked time intervals distinctly to create local optimums . in this case",
    "the complexity of the algorithm reduces to @xmath86 where @xmath87 is the time between two time intervals with different while in list of tasks the time complexity is @xmath88 .",
    "we note that in general dags the best scheduling algorithms for distinct speeds produces an @xmath89-approximation ( where @xmath0 is the number of essential speeds ) . in cases where schedules are far from tight , the energy reduction that can be achieved in high .",
    "we continue by providing some arguments for using asymmetric multiprocessors in terms of time efficiency .",
    "we show that preemptive scheduling in an asymmetric multiprocessor platform achieves the same or better optimal makespan than in a symmetric multiprocessor platform .",
    "the basic characteristic of our approach is _",
    "speed asymmetry_. we assume that the overhead of ( re)assigning processors to tasks of a parallel job to be executed is _",
    "negligible_.    [ listoptimal]given any list @xmath90 of @xmath15 chains of tasks to be scheduled on preemptive machines , an asymmetric multiprocessor system will always have a better or equal optimal makespan than a symmetric one , given that both have the same average speed ( @xmath91 ) and the same total number of processors ( @xmath27 ) .",
    "the equality holds if during the whole schedule all processors are busy .",
    "again we start by sorting the processors according to the processing capability @xmath41 so that @xmath42 .",
    "we then split time in intervals @xmath38 , where @xmath92 $ ] , so that between these @xmath27 intervals there is not any preemption , no task completes and no changes are made to the precedence constraints .",
    "this is feasible since the optimal schedule is feasible and has finite preemptions .",
    "let @xmath93 the optimal schedule for the symmetric multiprocessor system . now consider the interval @xmath94 where all processors process a list and divide it in @xmath27 time intervals .",
    "we assign each list to each of the @xmath27 asymmetric processors that are active , so that a task is assigned sequentially to all processors in the original schedule of @xmath93 .",
    "so each task will be processed by any processor for @xmath95 time .",
    "thus every task will have been processed during @xmath96 with an average speed of @xmath97 , which is the speed of every symmetric processor .",
    "thus given an optimal schedule for the symmetric system we can produce one that has at most the same makespan on the asymmetric set of processors .",
    "the above is true when all processors are processing a list , at all times .",
    "then the processing in both cases is the same .",
    "of course there are instances of sets of lists that can not be made to have all processors running at all times .",
    "in such schedules the optimal makespan on the asymmetric platform is better . recall that we have sorted all speeds . since the system is asymmetric it must have at least @xmath23 speeds .",
    "if at any time of @xmath93 we process less lists than processors , following the analysis above , we will have to divide the time in @xmath98 ( denoted by @xmath99 ) .",
    "so during time - interval @xmath94 the processing of any list that is processed on symmetric systems will be @xmath100 . while for the asymmetric system , the processing speed for the same time - interval will be @xmath101 .",
    "note that sum in the second equation is bigger than that of the first .",
    "that is valid because we use only the fastest processors .",
    "more formally @xmath102 .",
    "so we produced a schedule that has a better makespan than @xmath93 .",
    "in other words , if during the optimal schedule for a symmetric system there exists at least one interval where a processor is idle , we can produce an optimal schedule for the asymmetric multiprocessors platform with smaller makespan .",
    "[ dagoptimal]given any dag @xmath103of tasks to be scheduled on preemptive machines , an asymmetric multiprocessor system will always have a better or equal optimal makespan than a symmetric one , provided that both have the same average speed ( @xmath91 ) and the same total number of processors ( @xmath27 ) .",
    "the equality holds if during the whole schedule all processors are busy .",
    "we proceed as above .",
    "the difference is that we split time in @xmath104 that have the following property : between any of these times @xmath94 there is not any preemption on processors or completion of a _ list _ or support for any _ list _ that we could not process at @xmath52 due to precedence - constraints .",
    "when all processors are processing a list , at all times , the processing in both asymmetric and symmetric systems is the same , i.e. , @xmath105 .",
    "of course there are dags that can not be made to have all processors running at all times due to precedence - constraints or due to lack of tasks . in such dags",
    "the optimal makespan on the assymetric system is better than that of the symmetric one . if at any time of @xmath93 we process less lists than processors , following the analysis of theorem [ listoptimal ] we have that on the symmetric system the total processing will be @xmath106 while the processing speed during the same interval on the asymmetric one will be @xmath107 which is better",
    "we now focus on the case where the multiprocessor system is composed of a single fast processor and multiple slow ones , like the one designed in @xcite .",
    "consider that the fast processor has speed @xmath108 and the remaining @xmath109 processors have speed @xmath31 . in the sequel preemption of tasks",
    "is not allowed .",
    "we design the _ non - preemptive algorithm `` remnants '' _ ( see alg.[algo : remnants ] ) that always gives schedules with makespan @xmath110 .",
    "we greedily assign the fast processor first in each round .",
    "then we try to maximize parallelism using the slow processors . in the beginning of round @xmath33",
    "we denote @xmath111 the _ suffix _ of list @xmath112 not yet done .",
    "let @xmath113 .",
    "for @xmath6 tasks , the algorithm can be implemented to run in @xmath114 time .",
    "the slow processors , whose  list \" is taken by the speedy processor in round @xmath33 , can be reassigned to free remnants .",
    "remark in the speed assignment produced by `` remnants '' we can even _ name _ the processors assigned to tasks ( in contrast of general speed assignment methods , see e.g. , @xcite ) .",
    "thus the actual scheduling of tasks is much more easy and of reduced overhead .",
    "@xmath115    as an example , consider a system with 3 processors ( @xmath116 ) where the speedy processor has @xmath117 .",
    "in other words , we have a fast processor and two slow ones .",
    "we wish to schedule 4 lists , where @xmath118 and @xmath119 .",
    "the `` remnants '' algorithm produces the following assignment with a makespan of @xmath120 :    ( l11 ) at ( 0,0 ) [ level distance=8 mm ] child[- > ] node[minimum size=0.35cm , circle , draw ] ( l12 ) child[- > ] node[minimum size=0.35cm , circle , draw ] ( l13 ) ; ( l11 ) node[above=6pt ] @xmath121 ;    ( l21 ) at ( 2,0 ) [ level distance=8 mm ] child[- > ] node[minimum size=0.35cm , circle , draw ] ( l22 ) child[- > ] node[minimum size=0.35cm , circle , draw ] ( l23 ) ; ( l21 ) node[above=6pt ] @xmath122 ;    ( l31 ) at ( 4,0 ) [ level distance=8 mm ] child[- > ] node[minimum size=0.35cm , circle , draw ] ( l32 ) ; ( l31 ) node[above=6pt ] @xmath123 ;    ( l41 ) at ( 6,0 ) [ level distance=8 mm ] child[- > ] node[minimum size=0.35cm , circle , draw ] ( l42 ) ; ( l41 ) node[above=6pt ] @xmath124 ;    ( l41 ) node[right=24pt , red ] round 1 ; ( l11 ) node[left=8pt ] @xmath108 ; ( l12 ) node[left=8pt ] @xmath108 ; ( l13 ) node[left=8pt ] @xmath108 ; ( l21 ) node[left=8pt ] @xmath108 ; ( l31 ) node[left=8pt ] @xmath31 ; ( l41 ) node[left=8pt ] @xmath31 ; ( -1,-2 ) .. controls ( 1,-2 ) .. ( 0.9,-0.8 ) .. controls ( 1,-0.3 ) .. ( 9,-0.3 ) ;    ( l42 ) node[right=24pt , red ] round 2 ; ( l22 ) node[left=8pt ] @xmath108 ; ( l23 ) node[left=8pt ] @xmath108 ; ( l32 ) node[left=8pt ] @xmath108 ; ( l42 ) node[left=8pt ] @xmath108 ; ( 1,-2 ) .. controls ( 2.7,-2 ) .. ( 2.9,-1.5 ) .. controls ( 3.2,-1.2 ) .. ( 9,-1.2 ) ;    notice that the slow processors , whose  list \" is taken by the speedy processor in round @xmath33 , can be reassigned to free remnants ( one per free remnant ) .",
    "so our assignment tries to use all available parallelism per round .    now consider the case where the fast processor has @xmath125 , that is , it runs slower than the processor of the above example .",
    "for the same lists of tasks , the algorithm now produces a schedule with a makespan of @xmath126 :    ( l11 ) at ( 0,0 ) [ level distance=8 mm ] child[- > ] node[minimum size=0.35cm , circle , draw ] ( l12 ) child[- > ] node[minimum size=0.35cm , circle , draw ] ( l13 ) ; ( l11 ) node[above=6pt ] @xmath121 ;    ( l21 ) at ( 2,0 ) [ level distance=8 mm ] child[- > ] node[minimum size=0.35cm , circle , draw ] ( l22 ) child[- > ] node[minimum size=0.35cm , circle , draw ] ( l23 ) ; ( l21 ) node[above=6pt ] @xmath122 ;    ( l31 ) at ( 4,0 ) [ level distance=8 mm ] child[- > ] node[minimum size=0.35cm , circle , draw ] ( l32 ) ; ( l31 ) node[above=6pt ] @xmath123 ;    ( l41 ) at ( 6,0 ) [ level distance=8 mm ] child[- > ] node[minimum size=0.35cm , circle , draw ] ( l42 ) ; ( l41 ) node[above=6pt ] @xmath124 ;    ( l31 ) node[above=8pt , right=7pt , red ] round 1 ; ( l11 ) node[left=8pt ] @xmath108 ; ( l12 ) node[left=8pt ] @xmath108 ; ( l13 ) node[left=8pt ] @xmath108 ; ( l21 ) node[left=8pt ] @xmath31 ; ( l31 ) node[left=8pt ] @xmath31 ; ( -1,-2 ) .. controls ( 0.8,-2 ) .. ( 1.0,-0.7 ) .. controls ( 1.5,-0.3 ) .. ( 3.8,-0.3 ) .. controls ( 4.2,-0.3 ) .. ( 5,-0 ) ;    ( l41 ) node[right=24pt , red ] round 2 ; ( l22 ) node[left=8pt ] @xmath108 ; ( l23 ) node[left=8pt ] @xmath108 ; ( l32 ) node[left=8pt ] @xmath108 ; ( l41 ) node[left=8pt ] @xmath31 ; ( 1,-2 ) .. controls ( 3.5,-2 ) .. ( 4.2,-1.3 ) .. controls ( 4.2,-1.3 ) .. ( 5,-0.7 ) .. controls ( 6,-0.3 ) .. ( 8.5,-0.3 ) ;    ( l42 ) node[right=24pt , red ] round 3 ; ( l42 ) node[left=8pt ] @xmath108 ; ( 5.5,-1.4 ) .. controls ( 5.5,-1.4 ) .. ( 8.5,-1.1 ) ;    notice that for this configuration , the following schedule produces a makespan of @xmath23 :    ( l11 ) at ( 0,0 ) [ level distance=8 mm ] child[- > ] node[minimum size=0.35cm , circle , draw ] ( l12 ) child[- > ] node[minimum size=0.35cm , circle , draw ] ( l13 ) ; ( l11 ) node[above=6pt ] @xmath121 ;    ( l21 ) at ( 2,0 ) [ level distance=8 mm ] child[- > ] node[minimum size=0.35cm , circle , draw ] ( l22 ) child[- > ] node[minimum size=0.35cm , circle , draw ] ( l23 ) ; ( l21 ) node[above=6pt ] @xmath122 ;    ( l31 ) at ( 4,0 ) [ level distance=8 mm ] child[- > ] node[minimum size=0.35cm , circle , draw ] ( l32 ) ; ( l31 ) node[above=6pt ] @xmath123 ;    ( l41 ) at ( 6,0 ) [ level distance=8 mm ] child[- > ] node[minimum size=0.35cm , circle , draw ] ( l42 ) ; ( l41 ) node[above=6pt ] @xmath124 ;    ( l11 ) node[left=8pt ] @xmath108 ; ( l12 ) node[left=8pt ] @xmath108 ; ( l13 ) node[left=8pt ] @xmath108 ; ( l21 ) node[left=8pt ] @xmath108 ; ( l22 ) node[left=8pt ] @xmath108 ; ( l23 ) node[left=8pt ] @xmath108 ; ( l31 ) node[left=8pt ] @xmath31 ; ( l32 ) node[left=8pt ] @xmath31 ; ( l41 ) node[left=8pt ] @xmath31 ; ( l42 ) node[left=8pt ] @xmath31 ;    in the following theorem we show that the performance of remnants is actually very close to optimal , in the sense of arguing that the above counter - example is essentially the only one .    [ theorem4 ] for any set of lists @xmath90 and multiprocessor platform with one fast processor of speed @xmath108 and @xmath109 slow processors of speed @xmath31 , if @xmath127 is the makespan of algorithm remnants then @xmath110 .",
    "we apply here the construction of graham , as it was modified by @xcite , which we use in order to see if @xmath127 can be improved .",
    "let @xmath128 a task that completes _",
    "last _ in remnants .",
    "without loss of generality , from the way remnant works , we can always assume that @xmath129 was executed by the speedy processor .",
    "we consider now the logical chain ending with @xmath128 as follows : iteratively define @xmath130 as a predecessor of @xmath131 that completes last of all predecessors of @xmath131 in remnants . in this chain ( a )",
    "either all its tasks were done at speed @xmath132 ( in which case and since the fast processors works all the time , the makespan @xmath127 of remnants is _ optimal _ ) , or ( b ) there is a task @xmath133 at distance at most @xmath134 from @xmath135 that was done by speed @xmath31 in remnants . in the later case ,",
    "if @xmath136 is the start time of @xmath135 , this means that before @xmath136 all speed @xmath31 processors are busy , else @xmath135 could be have scheduled earlier .    * if there is no other task in the chain done at speed @xmath31 and before @xmath135 then again @xmath127 is _ optimal _ since before @xmath135 all processors of all speeds are busy .",
    "* let @xmath137 be another task in the chain done at speed @xmath31 and @xmath138",
    ". then @xmath137 must be an immediate predecessor of @xmath135 in a chain ( because of the way remnants work ) and , during the execution of @xmath137 , speed @xmath108 is busy but there could be some processor of speed @xmath31 available .",
    "define @xmath139 similarly ( tasks of the last chain , all done in speed @xmath31 and @xmath140 , @xmath141 ) .",
    "this can go up to the chain s start , which could have been done earlier by another speed @xmath31 processor and this _ is the only task _ that could be done by an available processor , _ just one step before_. so , the makespan @xmath127 of algorithm remnants can be compressed by only one task , and become optimal .",
    "but then @xmath142 ( i.e. , it is the start of the last list that has no predecessor and which could go at speed @xmath31 together with nodes in the previous list ) .      in this section",
    "we relax the limitations to asymmetry .",
    "we work on the more general case of having @xmath28 fast processors of speed @xmath108 and @xmath30 slow processors of speed @xmath31 .",
    "note that we still have two distinct speeds and preemption of tasks is not allowed .",
    "we follow the basic ideas of @xcite and specialize the general lower bounds on makespan for the more general case . clearly , the maximum _ rate _ at which the multiprocessor system of limitted asymmetry can process tasks is @xmath143 , which is achieved if and only if all machines are busy . therefore to finish all @xmath6 tasks",
    "requires time at least @xmath144 .",
    "now let @xmath145 where @xmath146 and @xmath147 are the individual processor speeds from the fast to the slow .",
    "it follows that , @xmath148 _ the interesting case is when @xmath149_. so , we assume @xmath149 and let @xmath150 .",
    "thus    @xmath151 by @xcite then    let @xmath152 the optimal makespan of @xmath15 chains",
    ". then @xmath153 .",
    "since the average load is also a lower bound for preemptive schedules we get    [ corollary1]@xmath154 is also a lower bound for preemptive schedules .    as for the case where we have only one fast processor , i.e. @xmath155 , in each step , at most @xmath156 tasks can be done since no two processors can work in parallel on the same list .",
    "this gives @xmath157 .",
    "of course the bound @xmath158 still also holds .    for a natural variant of list scheduling where no preemption takes place ,",
    "called speed - based list scheduling , developed in @xcite , is constrained to schedule according to the speed assignments of the jobs . in classical list scheduling , whenever a machine is free the first available job from the list is scheduled on it . in this method , an available task is scheduled on a free machine provided that the speed of the free machine matches the speed assignment of the task .",
    "the speed assignments of tasks have to be done in a clever way for good schedules . in the sequel ,",
    "let @xmath159 where @xmath160 is the number of tasks assigned to speed @xmath108 .",
    "let @xmath161 . finally , for each chain @xmath16 and each task",
    "@xmath162 with @xmath163 being the speed assigned to @xmath11 , compute @xmath164 and let @xmath165 .",
    "the proof of the following theorem follows from an easy generalization of graham s analysis of list scheduling .",
    "[ theorem1]for any speed assignment ( @xmath166 or @xmath31 ) to tasks @xmath167 , the non - preemptive speed - based list scheduling method produces a schedule of makespan @xmath168 .",
    "based on the above specializations , we wish to provide a non - preemptive schedule ( i.e. , speed assignment ) that achieves good makespan .",
    "we either assign tasks to speed @xmath108 or to speed @xmath31 so that @xmath169 is not too large .",
    "let , for task @xmath11 :    @xmath170    and @xmath171 since each task @xmath11 must be assigned to some speed we get @xmath172 in time @xmath173 , the fast processors can complete @xmath174 tasks and the slow processors can complete @xmath175 tasks .",
    "so @xmath176 and @xmath177 let @xmath38 be the completion time of task @xmath11 @xmath178 if @xmath179 then clearly @xmath180 also @xmath181 and @xmath182 based on the above constraints , consider the following mixed integer program :    * mip : * @xmath183 under ( [ equ:1 ] ) to ( [ equ:7 ] )    mip s optimal solution is clearly a lower bound on @xmath152 .",
    "note that @xmath184 and @xmath185 . also note that since @xmath186 by ( [ equ:5 ] ) and thus also @xmath187 , by adding times on each chain .",
    "so , if we could solve mip then we would get a schedule of makespan @xmath188 , by theorem [ theorem1 ] .",
    "suppose we relax ( [ equ:7 ] ) as follows : @xmath189\\qquad j=1\\ldots n\\ ] ] consider the following linear program :    * lp : * @xmath183 under ( [ equ:1 ] ) to ( [ equ:7 ] ) and ( [ equ:7- ] )    this lp can be solved in polynomial time and its optimal solution @xmath190 , where @xmath167 , gives an optimal @xmath191 , also @xmath192 ( because @xmath193 ) .",
    "we now use randomized rounding , to get a speed assignment @xmath194 @xmath195 let @xmath196 be the makespan of @xmath194 . since @xmath197 . but note that @xmath198 so @xmath199 by ( [ equ:2],[equ:3 ] ) and , for each list @xmath16 @xmath200 i.e. , @xmath201 .",
    "so we get the following theorem :    [ theorem2]our speed assignment @xmath194 gives a non - preemptive schedule of expected makespan at most @xmath202    our mip formulation also holds for general dags and 2 speeds , when all tasks are of unit length . since theorem [ theorem1 ] of @xcite and the lower bound of @xcite also holds for general dags , we get :    [ corollary 2]our speed assignment @xmath194 , for general dags of unit tasks gives a non - preemptive schedule of expected makespan at most @xmath202 .",
    "we continue by making some special consideration for lists of tasks , that is we think about dags that are decomposed in sets of lists .",
    "then , @xmath194 can be greedily improved since all tasks are of unit processing time , as follows . after doing the assignment experiment for the nodes of a list @xmath16 and",
    "get @xmath203 nodes on the fast processors and @xmath204 nodes on slow processors .",
    "we then reassign the first @xmath203 nodes of @xmath16 to the fast processors and the remaining nodes of @xmath16 to the slow processors .",
    "clearly this does not change any of the expectations of @xmath205 , @xmath206 and @xmath207 .",
    "let @xmath208 be this modified ( improved ) schedule .    also",
    ", because all tasks are equilenght ( unit processing time ) , any reordering of them in the same list will not change the optimal solution of lp .",
    "but then , for each list @xmath16 and for each task @xmath162 , @xmath209 is the same ( call it @xmath210 ) , and the same holds for @xmath211 .",
    "then the processing time of @xmath16 is just @xmath212 where @xmath213 is as the bernoulli @xmath214 .    in the sequel ,",
    "let @xmath215 , for some @xmath216 and let @xmath217 , where @xmath218 .",
    "then from @xmath208 we produce the speed assignment @xmath219 as follows :    since for the makespan @xmath220 of @xmath208 we have @xmath221 we get @xmath222 but @xmath223 thus @xmath224 however , in @xmath219 , the probability that @xmath225 , where @xmath226 is a constant @xmath227 , is at most @xmath228 ( by chernoff bounds ) , i.e. , at most @xmath229 .",
    "this implies that it is enough to repeat the randomized assignment of speeds at most a polynomial number of times and get a schedule of actual makespan at most @xmath230 .",
    "so , we get our next theorem :    [ theorem3 ] when each list has length @xmath231 ( where @xmath232 ) and @xmath233 ( where @xmath218 ) then we get a ( deterministic ) schedule of actual makespan at most @xmath234 in expected polynomial time .",
    "processors technology is undergoing a vigorous shaking - up to enable low - cost multiprocessor platforms where individual processors have different computation capabilities . we examined the energy consumption of such asymmetric arcitectures .",
    "we presented the preemptive algorithm ",
    "save - energy \" that post processes a schedule of tasks to reduce the energy usage without any deterioration of the makespan .",
    "then we examined the time efficiency of such asymmetric architectures .",
    "we shown that preemptive scheduling in an asymmetric multiprocessor platform can achieve the same or better optimal makespan than in a symmetric multiprocessor platform .",
    "motivited by real multiprocessor systems developed in @xcite , we investigated the special case where the system is composed of a single fast processor and multiple slow processors .",
    "we say that these architectures have limited asymmetry .",
    "interestingly , alghough the problem of scheduling has been studied extensively in the field of parallel computing and scheduling theory , it was considered for the general case where multiprocessor platforms have @xmath0 distinct speeds .",
    "our work attempts to bridge between the assumptions in these fields and recent advances in multiprocessor systems technology . in our simple , yet realistic , model where @xmath235 , we presented the non - preemptive algorithm `` remnants '' that achieves almost optimal makespan .",
    "we then generalized the limited asymmetry to systems that have more than one fast processors while @xmath235 .",
    "we refined the scheduling policy of @xcite and give a non - preemptive speed based list randomized scheduling of dags that has a makespan @xmath127 whose expectation @xmath236 .",
    "this improves the previous best factor ( 6 for two speeds ) .",
    "we then shown how to convert the schedule into a deterministic one ( in polynomial _ expected _ time ) in the case of long lists .",
    "susanne albers , fabian mller , and swen schmelzer .",
    "speed scaling on parallel processors . in _",
    "spaa 07 : proceedings of the nineteenth annual acm symposium on parallel algorithms and architectures _ , pages 289298 , new york , ny , usa , 2007 .",
    "saisanthosh balakrishnan , ravi rajwar , michael upton , and konrad  k. lai .",
    "the impact of performance asymmetry in emerging multicore architectures . in _",
    "32st international symposium on computer architecture ( isca ) _ , pages 506517 .",
    "ieee computer society , 2005 .",
    "michael  a. bender and cynthia  a. phillips . scheduling dags on asynchronous processors . in _ spaa 07 : proceedings of the nineteenth annual acm symposium on parallel algorithms and architectures _ , pages 3545 , new york , ny , usa , 2007 .",
    "acm .",
    "jianjun guo , kui dai , and zhiying wang . a heterogeneous multi - core processor architecture for high performance computing . in _ advances in computer systems architecture ( acsa )",
    ", pages 359365 , 2006 .",
    "lecture notes in computer science ( lncs 4186 ) ."
  ],
  "abstract_text": [
    "<S> in this work we study the problem of scheduling tasks with dependencies in multiprocessor architectures where processors have different speeds . </S>",
    "<S> we present the preemptive algorithm  </S>",
    "<S> save - energy \" that given a schedule of tasks it post processes it to improve the energy efficiency without any deterioration of the makespan . in terms of time efficiency , we show that preemptive scheduling in an asymmetric system can achieve the same or better optimal makespan than in a symmetric system . </S>",
    "<S> motivited by real multiprocessor systems , we investigate architectures that exhibit limited asymmetry : there are two essentially different speeds . </S>",
    "<S> interestingly , this special case has not been studied in the field of parallel computing and scheduling theory ; only the general case was studied where processors have @xmath0 essentially different speeds . </S>",
    "<S> we present the non - preemptive algorithm `` remnants '' that achieves almost optimal makespan . </S>",
    "<S> we provide a refined analysis of a recent scheduling method . based on this analysis </S>",
    "<S> , we specialize the scheduling policy and provide an algorithm of @xmath1 expected approximation factor . </S>",
    "<S> note that this improves the previous best factor ( 6 for two speeds ) . </S>",
    "<S> we believe that our work will convince researchers to revisit this well studied scheduling problem for these simple , yet realistic , asymmetric multiprocessor architectures . </S>"
  ]
}