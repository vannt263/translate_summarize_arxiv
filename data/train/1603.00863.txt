{
  "article_text": [
    "the area of optimization received enormous attention in recent years due to the rapid progress in computer technology , development of user - friendly software , the advances in scientific computing provided , and most of all , the remarkable interference of mathematical programming in crucial decision - making problems .",
    "one - dimensional optimization or simply line search optimization is a branch of optimization that is most indispensable , as it forms the backbone of nonlinear programming algorithms .",
    "in particular , it is typical to perform line search optimization in each stage of multivariate algorithms to determine the best length along a certain search direction ; thus , the efficiency of multivariate algorithms largely depends on it . even in constructing high - order numerical quadratures",
    ", line search optimization emerges in minimizing their truncation errors ; thus boosting their accuracy and allowing to obtain very accurate solutions to intricate boundary - value problems , integral equations , integro - differential equations , and optimal control problems in short times via stable and efficient numerical schemes ; cf . @xcite .",
    "many line search methods were presented in the literature in the past decades .",
    "some of the most popular line search methods include interpolation methods , fibonacci s method , golden section search method , secant method , newton s method , to mention a few ; cf .",
    "perhaps brent s method is considered one of the most popular and widely used line search methods nowadays .",
    "it is a robust version of the inverse parabolic interpolation method that makes the best use of both techniques , the inverse parabolic interpolation method and the golden section search method , and can be implemented in matlab software , for instance , using the ` fminbnd ' optimization solver .",
    "the method is a robust optimization algorithm that does not require derivatives ; yet it lacks the rapid convergence rate manifested in the derivative methods when they generally converge .    in @xmath0",
    ", @xcite gave a new approach for constructing an exact line search method using chebyshev polynomials , which have become increasingly important in scientific computing , from both theoretical and practical points of view .",
    "they introduced a fast line search method that is an adaptive version of newton s method .",
    "in particular , their method forces newton s iterative scheme to progress only in descent directions . moreover , the method replaces the classical finite - difference formulas for approximating the derivatives of the objective function by the more accurate step lengths along the chebyshev pseudospectral ( ps ) differentiation matrices ( cpsdms ) ; thus getting rid of the dependency of the iterative scheme on the choice of the step - size , which can significantly affect the quality of calculated derivatives approximations .",
    "although the method worked quite well on some test functions , where classical newton s method fail to converge , the method may still suffer from some drawbacks that we discuss thoroughly later in the next section .    in this article ,",
    "the question of how to construct an exact line search method based on ps methods is re - investigated and explored .",
    "we present for the first time a robust exact line - search method based on a full ps numerical scheme employing chebyshev polynomials and adopting two global strategies : ( i ) approximating the objective function by an accurate fourth - order chebyshev interpolant based on chebyshev - gauss - lobatto ( cgl ) points ; ( ii ) approximating the derivatives of the function using cpsdms .",
    "the first strategy actually plays a significant role in capturing a close profile to the objective function from which we can determine a close initial guess to the local minimum , since chebyshev polynomials as basis functions can represent smooth functions to arbitrarily high accuracy by retaining a finite number of terms .",
    "this approach also improves the performance of the adaptive newton s iterative scheme by starting from a sufficiently close estimate ; thus moving in a quadratic convergence rate .",
    "the second strategy yields accurate search directions by maintaining very accurate derivative approximations . the proposed method is also adaptive in the sense of searching only along descent directions , and avoids the raised drawbacks pertaining to the @xcite method .",
    "the rest of the article is organized as follows : in the next section , we revisit the @xcite method highlighting its strengths aspects and weaknesses . in section [ sec : tscpdm1 ] , we provide a modified explicit expression for higher - order cpsdms based on the successive differentiation of the chebyshev interpolant .",
    "we present the novel line search strategy in section [ sec : plsm ] using first - order and/or second - order information , and discuss its integration with multivariate nonlinear optimization algorithms in section [ eq : iwmnoa1 ] .",
    "furthermore , we provide a rigorous error and sensitivity analysis in sections [ sec : ea1 ] and [ sec : sa1 ] , respectively .",
    "section [ sec : numex1 ] verifies the accuracy and efficiency of the proposed method through an extensive sets of one- and multi - dimensional optimization test examples followed by some conclusions given in section [ conc ] .",
    "some useful background in addition to some useful pseudocodes for implementing the developed method are presented in appendices [ sec : p1 ] and [ appendix : ps1 ] , respectively .",
    "the @xcite line search method is an adaptive method that is considered an improvement over the standard newton s method and the secant method by forcing the iterative scheme to move in descent directions .",
    "the developed algorithm is considered unique as it exploits the peculiar convergence properties of spectral methods , the robustness of orthogonal polynomials , and the concept of ps differentiation matrices for the first time in a one - dimensional search optimization .",
    "in addition , the method avoids the use of classical finite difference formulas for approximating the derivatives of the objective function , which are often sensitive to the values of the step - size .",
    "the success in carrying out the above technique lies very much in the linearity property of the interpolation , differentiation , and evaluation operations .",
    "indeed , since all of these operations are linear , the process of obtaining approximations to the values of the derivative of a function at the interpolation points can be expressed as a matrix - vector multiplication . in particular , if we approximate the derivatives of a function @xmath1 by interpolating the function with an @xmath2th - degree polynomial @xmath3 at , say , the cgl points defined by eq .",
    ", then the values of the derivative @xmath4 at the same @xmath5 points can be expressed as a fixed linear combination of the given function values , and the whole relationship can be written in the following matrix form : @xmath6 setting @xmath7^t$ ] , as the vector consisting of values of @xmath1 at the @xmath5 interpolation points , @xmath8^t$ ] , as the values of the derivative at the cgl points , and @xmath9 , as the first - order differentiation matrix mapping @xmath10 , then formula can be written in the following simple form @xmath11 eq .",
    "is generally known as the ps differentiation rule , and it generally delivers better accuracy than standard finite - difference rules .",
    "we show later in the next section some modified formulas for calculating the elements of a general @xmath12th - order cpsdm , @xmath13 . further information on classical explicit expressions of the entries of differentiation matrices can be found in @xcite .",
    "a schematic figure showing the framework of @xcite line search method is shown in figure [ fig : ls2 ] . as can be observed from the figure ,",
    "the method starts by transforming the uncertainty interval @xmath14 $ ] into the domain @xmath15 $ ] to exploit the rapid convergence properties provided by chebyshev polynomials .",
    "the method then takes on a global approach in calculating the derivatives of the function at the cgl points , @xmath16 , using cpsdms .",
    "if the optimality conditions are not satisfied at any of these candidate points , the method then endeavors to locate the best point @xmath17 that minimizes the function , and decides whether to keep or expand the uncertainty interval .",
    "an update @xmath18 is then calculated using the descent direction property followed by updating row @xmath19 in both matrices @xmath20 and @xmath21 .",
    "the iterative scheme proceeds repeatedly until the stopping criterion is satisfied .",
    "although the method possesses many useful features over classical newton s method and the secant method ; cf .",
    "* section 8) , it may still suffer from the following drawbacks : ( i ) in spite of the fact that , @xmath20 and @xmath21 are constant matrices , the method requires the calculation of their entire elements beforehand .",
    "we show later that we can establish rapid convergence rates using only one row from each matrix in each iterate .",
    "( ii ) the method attempts to calculate the first and second derivatives of the function at each point @xmath22 , i = 0 , \\ldots , n$ ] .",
    "this could be expensive for large values of @xmath2 , especially if the local minimum @xmath23 is located outside the initial uncertainty interval @xmath24 $ ] ; cf .",
    "* table 6 ) , for instance .",
    "( iii ) the method takes on a global approach for approximating the derivatives of the objective function using cpsdms instead of the usual finite - difference formulas that are highly sensitive to the values of the step - size .",
    "however , the method locates a starting approximation by distributing the cgl points along the interval @xmath15 $ ] , and finding the point that best minimizes the value of the function among all other candidate points .",
    "we show later in section [ sec : plsm ] that we can significantly speed up this process by adopting another global approach based on approximating the function via a fourth - order chebyshev interpolant , and determining the starting approximation through finding the best root of the interpolant derivative using exact formulas .",
    "( iv ) during the implementation of the method , the new approximation to the local minimum , @xmath18 , may lie outside the interval @xmath15 $ ] in some unpleasant occasions ; thus the updated differentiation matrices may produce false approximations to the derivatives of the objective function .",
    "( v ) to maintain the adaptivity of the method , the authors proposed to flip the sign of the second derivative @xmath25 whenever @xmath26 , at some point followed by its multiplication with a random positive number @xmath27 .",
    "this operation may not be convenient in practice ; therefore , we need a more efficient approach to overcome this difficulty . ( vi )",
    "even if @xmath28 , at some point , the magnitudes of @xmath29 and @xmath25 may be too small slowing down the convergence rate of the method .",
    "this could happen for instance if the function has a multiple local minimum , or has a nearly flat profile about @xmath23 .",
    "( vii ) suppose that @xmath23 belongs to one side of the real line while the initial search interval @xmath24 $ ] is on the other side . according to the presented method",
    ", the search interval must shrink until it converges to the point zero , and the search procedure halts .",
    "such drawbacks motivate us to develop a more robust and efficient line search method",
    ".     \\to \\mathbb{r}$ ] , where @xmath16 are the cgl points , @xmath30 , are the corresponding points in @xmath24 $ ] , @xmath31 , @xmath32 is a relatively small positive number , @xmath33 , is a parameter preferably chosen as @xmath34 , where @xmath35 is the golden ratio , and @xmath36 , is the iteration counter , title=\"fig : \" ] +",
    "in the sequel , we derive a modified explicit expression for higher - order cpsdms to that stated in @xcite based on the successive differentiation of the chebyshev interpolant .",
    "the higher derivatives of chebyshev polynomials are expressed in standard polynomial form rather than as chebyshev polynomial series expansion .",
    "the relation between chebyshev polynomials and trigonometric functions is used to simplify the expressions obtained , and the periodicity of the cosine function is used to express it in terms of existing nodes .",
    "the following theorem gives rise to a modified useful form for evaluating the @xmath19th - derivative of chebyshev polynomials .",
    "[ thm1 ] the @xmath19th - derivative of the chebyshev polynomials is given by    align & 0,0 k < m,[eq : dercheb5 ] + & 1,k = m = 0,[eq : dercheb3 ] + & 2^k - 1m!,k = m 1,[eq : dercheb4 ] + & _ l = 0^k/2 _ l , k^(m)c_l^(k)x^k - 2l - m , k > m 0 x 0,[eq : dercheb1 ] + & _ k^(m ) ( ( k - _ , ) ) , k > m 0 x = 0,[eq : dercheb2 ]    where    align & 1,m = 0 , + & ( k - 2l - m + 1 ) ( k - 2l - m +",
    "2 ) _ m - 1,m 1 ,    [ eq : clkmain1 ]    align & 1,l = k = 0 , + & 2^k - 1,l = 0 k 1 , + & - c_l - 1^(k),l = 1 , ",
    ", k 1 ,    @xmath37 is the floor function of a real number @xmath38 ; @xmath39 is the pochhammer symbol , for all @xmath40 .",
    "the proof of eqs .",
    "- is straightforward using eqs . - .",
    "we prove eqs . and by mathematical induction .",
    "so consider the case where @xmath41 .",
    "for @xmath42 , we can rewrite eq . with a bit of manipulation in the form @xmath43 for @xmath44 , we have @xmath45 so the theorem is true for @xmath42 and @xmath46 .",
    "now , assume that the theorem is true for @xmath47 , then for @xmath48 , we have @xmath49 hence eq . is true for every positive integer @xmath19 . now consider the case @xmath50 .",
    "the proof of eq . for @xmath42 is trivial , so consider the case @xmath51 , where the proof is derived as follows : @xmath52 now assume that eq .",
    "is true for @xmath47 .",
    "we show that it is also true for @xmath48 . since @xmath53 then substituting eq . in eq .",
    "yields @xmath54 where @xmath55 . since @xmath56",
    "then by the general leibniz rule , @xmath57 eq . at @xmath58 is reduced to @xmath59 } } } \\right ) } \\right),\\end{aligned}\\ ] ] which completes the proof .    introducing the parameters",
    ", @xmath60 replaces formulas ( [ eq : clenshaw1 ] ) and ( [ coeff ] ) with @xmath61 @xmath62 where @xmath63 . substituting eq . into eq .",
    "yields @xmath64    the derivatives of the chebyshev interpolant @xmath65 of any order @xmath19 are then computed at the cgl points by differentiating such that @xmath66 where @xmath67 are the elements of the @xmath19th - order cpsdm . with the aid of theorem [ thm1 ]",
    ", we can now calculate the elements of the @xmath19th - order cpsdm using the following useful formula :    [ eq : cpsdmat1 ] @xmath68    using the following periodic property of the cosine function @xmath69 we can further rewrite eqs . as follows : @xmath70 where @xmath71 . to improve the accuracy of eqs .",
    ", we can use the negative sum trick , computing all the off - diagonal elements then applying the formula @xmath72 to compute the diagonal elements . applying the last trick",
    "gives the formula @xmath73 hence , the elements of the first- and second - order cpsdms are given by @xmath74 @xmath75 respectively .",
    "in this section we present a novel line search method that we shall call the chebyshev ps line search method ( cpslsm ) .",
    "the key idea behind our new approach is five - fold :    1 .   express the function as a linear combination of chebyshev polynomials , 2 .",
    "find its derivative , 3 .",
    "find the derivative roots , 4 .",
    "determine a local minimum in @xmath15 $ ] , and , 5 .   finally reverse the change of variables to obtain the approximate local minimum of the original function .    to describe the proposed method ,",
    "let us denote by @xmath76 the space of polynomials of degree at most @xmath2 , and suppose that we want to find a local minimum @xmath23 of a twice - continuously differentiable nonlinear single - variable function @xmath77 on a fixed interval @xmath24 $ ] to within a certain accuracy @xmath32 . using the change of variable , @xmath78 we transform the interval @xmath24 $ ] into @xmath79",
    ". we shall refer to a point @xmath38 corresponding to a candidate local minimum point @xmath80 according to eq . by the translated candidate local minimum point .",
    "now let @xmath81 , be the fourth - degree chebyshev interpolant of @xmath82 at the cgl points such that @xmath83 we can determine the chebyshev coefficients @xmath84 via the discrete chebyshev transform @xmath85,\\end{aligned}\\ ] ] where @xmath86 and @xmath87 we approximate the derivative of @xmath82 by the derivative of its interpolant @xmath88 , @xmath89 where the coefficients @xmath90 are akin to the coefficients of the original function , @xmath91 , by the following recursion @xcite @xmath92 @xmath93 we can calculate @xmath90 efficiently using algorithm [ sec : alg1chebcoeffder1 ] . now we collect the terms involving the same powers of @xmath38 in eq . to get the cubic algebraic equation @xmath94 where    [ eqs : subcoeffmaink1 ] @xmath95    let @xmath96 and @xmath97 denote a relatively small positive number and the machine precision that is approximately equals @xmath98 in double precision arithmetic , respectively . to find a local minimum of @xmath82",
    ", we consider the following three cases :    * case * @xmath99 : if @xmath100 , then @xmath101 is linear or nearly linear .",
    "notice also that @xmath102 can not be zero , since @xmath82 is nonlinear and formula is exact for all polynomials @xmath103 .",
    "this motivates us to simply calculate the root @xmath104 , and set @xmath105 if @xmath106 .",
    "if not , then we carry out one iteration of the golden section search method on the interval @xmath24 $ ] to determine a smaller interval @xmath107 $ ] with candidate local minimum @xmath108 .",
    "if the length of the new interval is below @xmath32 , we set @xmath109 , and stop .",
    "otherwise , we calculate the the first- and second - order derivatives of @xmath82 at the translated point @xmath110 in the interval @xmath15 $ ] defined by @xmath111 to this end , we construct the row cpsdms @xmath112 and @xmath113 of length @xmath114 , for some @xmath115 using the following formulas : @xmath116 @xmath117 respectively . the computation of the derivatives can be carried out easily by multiplying @xmath20 and @xmath21 with the vector of function values ; that is ,    [ eq : der12k1 ] @xmath118    where @xmath119^t ; \\bm{\\mathcal{f } '' } = \\left[{}_{{a_1}}^{{b_1}}{f''_0 } , \\ldots , { } _ { { a_1}}^{{b_1}}{f''_m}\\right]^t$ ] .",
    "notice here how the cpslsm deals adequately with drawback ( i ) of @xcite by calculating only one row from each of the cpsdms in each iterate . if @xmath120 , then newton s direction is a descent direction , and we follow the @xcite approach by updating @xmath110 according to the following formula @xmath121 at this stage , we consider the following scenarios :    ( i ) : :    if the stopping criterion @xmath122    is fulfilled , we set    @xmath123    and stop .",
    "( ii ) : :    if @xmath124 we repeat the    procedure again starting from the construction of a fourth - degree    chebyshev interpolant of @xmath125 using the cgl    points .",
    "( iii ) : :    if the magnitudes of both    @xmath126 and    @xmath127 are too small ,    which may appear as we mentioned earlier when the profile of the    function @xmath82 is too flat near the current point , or if the    function has a multiple local minimum , then the convergence rate of    the iterative scheme is no longer quadratic , but rather linear .",
    "we    therefore suggest here to apply brent s method .",
    "to reduce the length    of the search interval though , we consider the following two cases :    +    * if @xmath128 , then we carry out    brent s method on the interval    @xmath129 $ ] .",
    "notice that the direction from @xmath130 into    @xmath131 is a descent direction as shown by    @xcite .    * if @xmath132 , then we carry out    brent s method on the interval    @xmath133 $ ] .",
    "( iv ) : :    if none of the above three scenarios occur , we compute the first- and    second - order derivatives of the interpolant at    @xmath134 as discussed before , set    @xmath135 , and repeat the iterative    formula .",
    "if @xmath136 , we repeat the procedure again .",
    "+ * case * @xmath137 : if @xmath138 , then @xmath101 is quadratic such that the second derivative of the derivative interpolant is positive and its graph is concave up ( simply convex and shaped like a parabola open upward ) if @xmath139 ; otherwise , the second derivative of the derivative interpolant is negative and its graph is concave down ; that is , shaped like a parabola open downward .",
    "for both scenarios , we repeat the steps mentioned in * case * @xmath99 starting from the golden section search method .",
    "* case * @xmath140 : if @xmath141 , then the derivative of the chebyshev interpolant is cubic . to avoid overflows",
    ", we scale the coefficients of @xmath101 by dividing each coefficient with the coefficient of largest magnitude if the magnitude of any of the coefficients is larger than unity .",
    "this procedure ensures that @xmath142 the next step is divided into two subcases :    * subcase * @xmath143 : if any of the three roots @xmath144 of the cubic polynomial @xmath101 , is a complex number , or the magnitude of any of them is larger than unity , we perform one iteration of the golden section search method on the interval @xmath24 $ ] to determine a smaller interval @xmath107 $ ] with candidate local minimum @xmath108 . again , and as we showed before in * case * @xmath99 , if the length of the new interval is below @xmath32 , we set @xmath109 , and stop . otherwise , we calculate the translated point @xmath110 using eq . .",
    "* subcase * @xmath145 : if all of the roots are real , distinct , and lie within the interval @xmath79 $ ] , we find the root that minimizes the value of @xmath82 among all three roots ; that is , we calculate , @xmath146 we then update both rows of the cpsdms @xmath112 and @xmath113 using eqs . and , and calculate the first- and second- derivatives of the chebyshev interpolant using eqs . .",
    "if @xmath120 , then newton s direction is a descent direction , and we follow the same procedure presented in * case * @xmath99 except when @xmath147 . to update the uncertainty interval",
    "@xmath24 $ ] , we determine the second best root among all three roots ; that is , we determine , @xmath148 now if @xmath149 , we replace @xmath150 with @xmath151 . otherwise , we replace @xmath152 with @xmath151 .",
    "the method then proceeds repeatedly until it converges to the local minimum @xmath23 , or the number of iterations exceeds a preassigned value , say @xmath153 .",
    "it is noteworthy to mention that the three roots of the cubic polynomial , @xmath154 , can be exactly calculated using the trigonometric method due to franois vite ; cf .",
    "@xcite . in particular ,",
    "let @xmath155 and define , @xmath156 then the three roots can be easily computed using the following useful formulas @xmath157 where    @xmath158    if the three roots @xmath144 are real and distinct , then it can be shown that they satisfy the inequalities @xmath159 .    the tolerance @xmath160 is chosen to satisfy the stopping criterion with respect to the variable @xmath80 , since @xmath161 where @xmath162    to reduce the round - off errors in the calculation of @xmath134 through eq . , we prefer to scale the vector of function values @xmath163 if any of its elements is large .",
    "that is , we choose a maximum value @xmath164 , and set @xmath165 if @xmath166 .",
    "this procedure does not alter the value of @xmath134 , since the scaling of @xmath163 is canceled out through division .",
    "the derivative of the chebyshev interpolant , @xmath154 , has all three simple zeros in @xmath167 if @xmath168 , has all its zeros in @xmath167 ; cf .",
    "@xcite .",
    "notice how the cpslsm handles drawback ( ii ) of @xcite by simply estimating an initial guess within the uncertainty interval with the aid of a chebyshev interpolant instead of calculating the first and second derivatives of the objective function at a population of candidate points ; thus reducing the required calculations significantly , especially if several uncertainty intervals were encountered during the implementation of the algorithm due to the presence of the local minimum outside the initial uncertainty interval .",
    "moreover , the cpslsm handles drawback ( iii ) of @xcite by taking advantage of approximate derivative information derived from the constructed chebyshev interpolant instead of estimating an initial guess by comparing the objective function values at the cgl population points that are distributed along the interval @xmath169 $ ] ; thus significantly accelerating the implementation of the algorithm .",
    "drawback ( iv ) is resolved by constraining all of the translated candidate local minima to lie within the chebyshev polynomials feasible domain @xmath15 $ ] at all iterations .",
    "drawback ( v ) is treated efficiently by combining the popular numerical optimization algorithms : the golden - section algorithm and newton s iterative scheme endowed with cpsdms .",
    "brent s method is integrated within the cpslsm to overcome drawback ( vi ) .",
    "the cpslsm can be implemented efficiently using algorithms [ sec1:alg : cpslsm2][sec1:alg : chebyshevnewton ] in appendix [ appendix : ps1 ] .",
    "it is important to mention that the proposed cpslsm can easily work if the uncertainty interval is not known a priori . in this case",
    "the user inputs any initial interval , say @xmath170 $ ] .",
    "we can then divide the interval into some @xmath171 uniform subintervals using @xmath172 equally - spaced nodes @xmath173 .",
    "we then evaluate the function @xmath82 at those points and find the point @xmath174 that minimizes @xmath82 such that @xmath175 now we have the following three cases :    * if @xmath176 , then we set @xmath177 and @xmath178 , and return . * if @xmath179 , then we divide @xmath180 by @xmath181 if @xmath182 , where @xmath35 is the golden ratio and @xmath183 is the iteration number as shown by @xcite .",
    "however , to avoid drawback ( vii ) in section [ sec : teahomr ] , we replace the calculated @xmath184 with @xmath185 if @xmath186 .",
    "otherwise , we multiply @xmath180 by @xmath181 . in both cases we set @xmath187 , and repeat the search procedure . * if @xmath188 , then we multiply @xmath189 by @xmath181 if @xmath190 .",
    "otherwise , we divide @xmath189 by @xmath181 and replace the calculated @xmath189 with @xmath191 if @xmath192 . in both cases we set @xmath193 , and repeat the search procedure .",
    "the above procedure avoids drawback ( vii ) , and proceeds repeatedly until an uncertainty interval @xmath14 $ ] is located , or the number of iterations exceeds @xmath153 .",
    "suppose that we want to find a local minimum @xmath23 of a differentiable nonlinear single - variable function @xmath77 on a fixed interval @xmath24 $ ] to within a certain accuracy @xmath32 .",
    "moreover , suppose that the second - order information is not available .",
    "we can slightly modify the method presented in section [ sec : plsm ] to work in this case .",
    "in particular , in * case * @xmath99 , we carry out one iteration of the golden section search method on the interval @xmath24 $ ] to determine a smaller interval @xmath107 $ ] with two candidate local minima @xmath194 and @xmath195 : @xmath196 .",
    "if the length of the new interval is below @xmath32 , we set @xmath197 , and stop .",
    "otherwise , we calculate the first - order derivatives of @xmath82 at the two points @xmath110 and @xmath134 defined by eqs . .",
    "we then calculate @xmath198 where @xmath199 and @xmath200 are the first - order cpsdms corresponding to the points @xmath110 and @xmath134 , respectively . if @xmath201 , we follow @xcite approach , and calculate the secant search direction @xmath202 then update @xmath134 according to the following formula , @xmath203 again , we consider the following course of events :    ( i ) : :    if the stopping criterion    @xmath204    is fulfilled , we set    @xmath205    and stop .",
    "( ii ) : :    if @xmath206 we repeat the    procedure again starting from the construction of a fourth - degree    chebyshev interpolant of @xmath125 at the cgl points .",
    "( iii ) : :    the third scenario appears when the magnitudes of both    @xmath207 and @xmath208 are too small .",
    "we then apply    brent s method as described by the following two cases :    +    * if @xmath209 , then we carry out    brent s method on the interval    @xmath210 $ ] .    * if @xmath211 , then we carry out    brent s method on the interval    @xmath212 $ ] .",
    "( iv ) : :    if none of the above three scenarios appear , we replace    @xmath213 by    @xmath214 , calculate the    first - order derivative of the interpolant at @xmath215 ,    update @xmath216 , set    @xmath217 , and repeat the iterative    formula .    finally ,",
    "if @xmath218 , we repeat the procedure again .    in * case",
    "* @xmath140 ( * subcase * @xmath143 ) , we apply one iteration of the golden section search method on the interval @xmath24 $ ] to determine a smaller interval @xmath107 $ ] with candidate local minima @xmath194 and @xmath219 .",
    "if the length of the new interval is below @xmath32 , we set @xmath197 , and stop .",
    "otherwise , we calculate the two points @xmath110 and @xmath134 using eqs . .",
    "for * subcase * @xmath145 , we find the best root , @xmath131 , that minimizes the value of @xmath82 among all three roots .",
    "then suppose that @xmath220 , for some @xmath221 . to calculate the secant search direction , we need another point @xmath130 within the interval @xmath15 $ ] .",
    "this can be easily resolved by making use of the useful inequalities @xmath159 . in particular , we proceed as follows :    * if @xmath222 , then @xmath223 , and we set @xmath224 . * if @xmath225 , then @xmath226 , and we set @xmath227 . * if @xmath228 , then @xmath229 , and we set @xmath230 .",
    "this procedure is then followed by updating both rows of the cpsdms , @xmath231 and @xmath232 , and calculating the first - order derivatives of the chebyshev interpolant at the two points @xmath233 .",
    "we then update @xmath234 using eq . .",
    "if @xmath201 , then the secant direction is a descent direction , and we follow the same procedure discussed before .",
    "the method then proceeds repeatedly until it converges to the local minimum @xmath23 , or the number of iterations exceeds the preassigned value @xmath153 .",
    "notice that the convergence rate of the cpslsm using first - order information only is expected to be slower than its partner using second - order information , since it performs the secant iterative formula as one of its ingredients rather than newton s iterative scheme ; thus the convergence rate degenerates from quadratic to superlinear .",
    "it is inadvisable to assign the value of @xmath130 to the second best root that minimizes the value of @xmath82 , since the function profile could be changing rapidly near @xmath235 ; thus @xmath208 yields a poor approximation to the second derivative of @xmath82 .",
    "in fact , applying this procedure on the rapidly varying function @xmath236 ( see section [ sec : numex1 ] ) near @xmath237 using the cpslsm gives the poor approximate solution @xmath238 with @xmath239 .",
    "the proposed cpslsm can be integrated easily with multivariate nonlinear optimization algorithms .",
    "consider , for instance , one of the most popular quasi - newton algorithms for solving unconstrained nonlinear optimization problems widely known as broyden - fletcher - goldfarb - shanno ( bfgs ) algorithm .",
    "algorithm [ sec1:alg : mbfgsa1 ] implements a modified bfgs method endowed with a line search method , where a scaling of the search direction vector , @xmath240 , is applied at each iteration whenever its size exceeds a prescribed value @xmath241 .",
    "this step is required to avoid multiplication with large numbers ; thus maintaining the stability of the numerical optimization scheme .",
    "practically , the initial approximate hessian matrix @xmath242 can be initialized with the identity matrix @xmath243 , so that the first step is equivalent to a gradient descent , but further steps are more and more refined by @xmath244 . to update the search direction at each iterate",
    ", we can easily calculate the approximate inverse hessian matrix , @xmath245 , for each @xmath246 , by applying the sherman - morrison formula @xcite giving @xmath247 where @xmath248 instead of typically solving the linear system @xmath249 since @xmath240 is a descent direction at each iteration , we need to adjust the cpslsm to search for an approximate local minimum @xmath250 in @xmath251 . to this end , we choose a relatively small positive number , say @xmath252 , and allow the initial uncertainty interval @xmath253 : b > \\hat \\varepsilon$ ] , to expand as discussed before , but only rightward the real line of numbers .",
    "a major step in implementing the proposed cpslsm lies in the interpolation of the objective function @xmath82 using the cgl points .",
    "in fact , the exactness of formula for all polynomials @xmath103 allows for faster convergence rates than the standard quadratic and cubic interpolation methods . from another point of view",
    ", the cgl points have a number of pleasing advantages as one of the most commonly used node distribution in spectral methods and numerical discretizations .",
    "they include the two endpoints , @xmath254 and @xmath46 , so they cover the whole search interval @xmath15 $ ] .",
    "moreover , it is well known that the lebesgue constant gives an idea of how good the interpolant of a function is in comparison with the best polynomial approximation of the function . using theorem 3.4 in @xcite",
    ", we can easily deduce that the lebesgue constant , @xmath255 , for interpolation using the cgl set @xmath256 , is uniformly bounded by those obtained using the gauss nodal set that is close to that of the optimal canonical nodal set . in particular , @xmath255 is bounded by @xmath257 where @xmath258 , represents euler s constant , and @xmath259 .      in this section",
    "we address the effect of round - off errors encountered in the calculation of the elements @xmath260 and @xmath261 given by @xmath262 respectively , since they are the major elements with regard to their values .",
    "accordingly , they bear the major error responsibility comparing to other elements .",
    "so let @xmath263 , be the round - off unity in the double - precision floating - point system , and assume that @xmath264 are the exact cgl points , @xmath265 are the computed values , and @xmath266 are the corresponding round - off errors such that @xmath267 with @xmath268 . if we denote the exact elements of @xmath20 and @xmath21 by @xmath269 and @xmath270 , respectively , then @xmath271 moreover , @xmath272    it is noteworthy to mention here that the round - off error in the calculation of @xmath273 from the classical chebyshev differentiation matrix is of order @xmath274 ; cf .",
    "hence , formulas are better numerically",
    ". moreover , the upper bounds and are in agreement with those obtained by @xcite .",
    "the following theorem highlights the conditioning of a given root @xmath275 with respect to a given coefficient @xmath276 .",
    "[ thm:1 ] let @xmath154 be the cubic polynomial defined by eq .",
    "after scaling the coefficients @xmath276 , such that condition is satisfied .",
    "suppose also that @xmath277 is a simple root of @xmath154 , for some @xmath278 .",
    "then the relative condition number , @xmath279 , of @xmath277 with respect to @xmath280 is given by @xmath281 moreover , @xmath279 is bounded by the following inequality @xmath282    suppose that the @xmath283th coefficient @xmath280 of @xmath284 , is perturbed by an infinitesimal quantity @xmath285 , so that the change in the polynomial is @xmath286 .",
    "suppose also that @xmath287 denotes the perturbation in the @xmath288th root @xmath277 of @xmath154 .",
    "then by the mean value theorem @xmath289 the condition number of finding @xmath277 with respect to perturbations in the single coefficient @xmath280 is therefore @xmath290 @xmath291 from which eq . and inequality follow .",
    "in the following two sections we show our numerical experiments for solving two sets of one- and multi - dimensional optimization test problems .",
    "all numerical experiments were conducted on a personal laptop equipped with an intel(r ) core(tm ) i7 - 2670qm cpu with 2.20ghz speed running on a windows 10 64-bit operating system , and the numerical results were obtained using matlab software v. r2014b ( 8.4.0.150421 ) .      in this section , we first apply the cpslsm using second - order information on the seven test functions @xmath292 , considered earlier by @xcite , in addition to the following five test functions @xmath293 the plots of the test functions are shown in figure [ funplots ] . the exact local minima and their corresponding optimal function values obtained using mathematica 9 software accurate to @xmath294 significant digits precision are shown in table [ sec : numerical : tab : tf1 ] .",
    "all of the results are presented against the widely used matlab ` fminbnd ' optimization solver to assess the accuracy and efficiency of the current work .",
    "we present the number of correct digits @xmath295 , obtained for each test function , where @xmath296 is the approximate solution obtained using the competing line search solvers , the cpslsm and the fminbnd solver .",
    "the cpslsm was carried out using @xmath297 , and the magnitudes of both @xmath126 and @xmath127 were considered too small if their values fell below @xmath298 . the fminbnd solver was implemented with the termination tolerance ` tolx ' set at @xmath299 .",
    "the starting uncertainty intervals @xmath300 , for the considered test functions are listed in respective order as follows : @xmath301 , i_2 = [ 0 , 20 ] , i_3 = [ 1 , 5 ] , i_4 = [ 0 , 5 ] , i_5 = [ 1 , 20 ] , i_6 = [ 0.5 , 5 ] , i_7 = [ -10 , 10 ] , i_8 = [ 0 , 10 ] , i_9 = [ -5 , 5 ] , i_{10 } = [ -2 , 2 ] , i_{11 } = [ 0 , 10]$ ] , and @xmath302 $ ] .",
    "figure [ cdn ] shows the @xmath303 obtained using the cpslsm and the fminbnd solver for each test function .",
    "clearly , the cpslsm establishes more accuracy than the fminbnd solver in general with the ability to exceed the required precision in the majority of the tests .",
    "moreover , the cpslsm was able to find the exact local minimum for @xmath236 .",
    "the experiments conducted on the test functions @xmath304 and @xmath305 are even more interesting , because they manifest the adaptivity of the cpslsm to locate a search interval bracketing the solution when the latter does not lie within the starting uncertainty interval .",
    "moreover , the cpslsm is able to determine highly accurate approximate solutions even when the starting uncertainty intervals are far away from the desired solutions .",
    "the proposed method is therefore recommended as a general purpose line search method . on the other hand ,",
    "the fminbnd solver was stuck in the starting search intervals , and failed to locate the solutions . for test function @xmath306 ,",
    "we observe a gain in accuracy in favor of the fminbnd solver .",
    "notice though that the approximate solution @xmath307 , obtained using the cpslsm in this case yields the function value @xmath308 that is accurate to @xmath309 significant digits .",
    "both methods yield the same accuracy for the test function @xmath310 .",
    "figure [ iter1 ] further shows the number of iterations @xmath183 required by both methods to locate the approximate minima given the stated tolerance @xmath32 .",
    "the figure conspicuously shows the power of the novel optimization scheme observed in the rapid convergence rate , as the cpslsm requires about half the iterations number required by the fminbnd solver for several test functions .",
    "the gap is even much wider for test functions @xmath311 and @xmath312 .         for the cpslsm and the fminbnd solver ]        figures",
    "[ cdn1 ] and [ iter11storder ] show the cd@xmath313 and the number of iterations required by the cpslsm using only first - order information versus the fminbnd solver . here",
    "we notice that the obtained cd@xmath313 values using the cpslsm are almost identical with the values obtained using second - order information with a slight increase in the number of iterations required for some test functions as expected .     for the cpslsm using first - order information and the fminbnd solver ]          the functions listed below are some of the common functions and data sets used for testing multi - dimensional unconstrained optimization algorithms :    * _ sphere function _ : @xmath314 the global minimum function value is @xmath315 , obtained at @xmath316^t$ ] . *",
    "_ bohachevsky function _ : @xmath317 the global minimum function value is @xmath315 , obtained at @xmath318^t$ ] . *",
    "_ booth function _ : @xmath319 the global minimum function value is @xmath315 , obtained at @xmath320^t$ ] . *",
    "_ three - hump camel function _ : @xmath321 the global minimum function value is @xmath315 , obtained at @xmath318^t$ ] . * _ powell function _ : @xmath322 the global minimum function value is @xmath315 , obtained at @xmath316^t$ ] . * _ goldstein - price function _ : @xmath323 the global minimum function value is @xmath324 , obtained at @xmath325^t$ ] . * _ styblinski - tang function _ : @xmath326 the global minimum function value is @xmath327 , obtained at @xmath328^t$ ] .",
    "* _ easom function _ : @xmath329 the global minimum function value is @xmath330 , obtained at @xmath331^t$ ] .",
    "table [ sec : numerical : tab : tf2 ] shows a comparison between the modified bfgs method endowed with matlab `` fminbnd '' line search solver ( mbfgsfminbnd ) and the modified bfgs method integrated with the present cpslsm ( mbfgscpslsm ) for the multi - dimensional test functions @xmath332 .",
    "the modified bfgs method was performed using algorithm [ sec1:alg : mbfgsa1 ] with @xmath333 , and @xmath334 .",
    "the gradients of the objective functions were approximated using central difference approximations with step - size @xmath335 .",
    "both fminbnd method and cpslsm were initiated using the uncertainty interval @xmath336 $ ] with @xmath337 .",
    "the maximum number of iterations allowed for each line search method was @xmath338 .",
    "each line search method was considered successful at each iterate @xmath183 if the change in the approximate step length @xmath250 is below @xmath339 .",
    "the cpslsm was carried out using @xmath340 , and @xmath341 .",
    "moreover , both mbfgsfminbnd and mbfgscpslsm were stopped whenever @xmath342 or @xmath343 table [ sec : numerical : tab : tf2 ] clearly manifests the power of the cpslsm , where the running time and computational cost of the modified bfgs method can be significantly reduced .",
    "while there is a widespread belief in the optimization community that an exact line search method is unnecessary , as what we may need is a large step - size which can lead to a sufficient descent in the objective function , the current work largely adheres to the use of adaptive ps exact line searches based on chebyshev polynomials .",
    "the presented cpslsm is a novel exact line search method that enjoys many useful virtues : ( i ) the initial guesses of the solution in each new search interval are calculated accurately and efficiently using a high - order chebyshev ps method .",
    "( ii ) the function gradient values in each iteration are calculated accurately and efficiently using cpsdms . on the other hand ,",
    "typical line search methods in the literature endeavor to approximate such values using finite difference approximations that are highly dependent on the choice of the step - size  a usual step frequently shared by the optimization community .",
    "( iii ) the method is adaptive in the sense of locating a search interval bracketing the solution when the latter does not lie within the starting uncertainty interval .",
    "( iv ) it is able to determine highly accurate approximate solutions even when the starting uncertainty intervals are far away from the desired solution .",
    "( v ) the accurate approximation to the objective function , quadratic convergence rate to the local minimum , and the relatively inexpensive computation of the derivatives of the function using cpsdms are some of the many useful features possessed by the method .",
    "( vi ) the cpslsm can be efficiently integrated with the current state of the art multivariate optimization methods .",
    "in addition , the presented numerical comparisons with the popular rival brent s method verify further the effectiveness of the proposed cpslsm .",
    "in general , the cpslsm is a new competitive method that adds more power to the arsenal of line search methods by significantly reducing the running time and computational cost of multivariate optimization algorithms .",
    "in this section , we present some useful results from approximation theory . the first kind chebyshev polynomial ( or simply the chebyshev polynomial ) of degree @xmath2 , @xmath344 ,",
    "is given by the explicit formula @xmath345,\\ ] ] using trigonometric functions , or in the following explicit polynomial form @xcite : @xmath346 where @xmath347 the chebyshev polynomials can be generated by the three - term recurrence relation @xmath348 starting with @xmath349 and @xmath350 .",
    "they are orthogonal in the interval @xmath79 $ ] with respect to the weight function @xmath351 , and their orthogonality relation is given by @xmath352 where @xmath353 , and @xmath354 is the kronecker delta function defined by @xmath355 the roots ( aka chebyshev - gauss points ) of @xmath344 are given by @xmath356 and the extrema ( aka cgl points ) are defined by @xmath357 the derivative of @xmath344 can be obtained in terms of chebyshev polynomials as follows @xcite : @xmath358    @xcite showed that a continuous function @xmath1 with bounded variation on @xmath79 $ ] can be approximated by the truncated series @xmath359 where @xmath360 @xmath361 , are the cgl points defined by eq . , @xmath362 , and the summation symbol with double primes denotes a sum with both the first and last terms halved . for a smooth function @xmath82 , the chebyshev series exhibits exponential convergence faster than any finite power of @xmath363 @xcite .",
    "positive integer number @xmath19 ; twice - continuously differentiable nonlinear single - variable objective function @xmath82 ; maximum function value @xmath164 ; uncertainty interval endpoints @xmath364 ; relatively small positive numbers @xmath365 ; maximum number of iterations @xmath153 .",
    "the local minimum @xmath366 $ ] . .",
    "objective function @xmath82 ; initial guess @xmath373 ; an approximate hessian matrix @xmath242 ; maximum number of iterations @xmath153 ; maximum direction size @xmath241 .",
    "@xmath374 ; calculate @xmath375 , and the gradient vector @xmath376 .",
    "@xmath377 if the convergence criterion is satisfied .",
    "@xmath378 perform a line search to find an acceptable step - size @xmath379 in the direction @xmath380 .",
    "@xmath382 . calculate @xmath383 , and set @xmath384 if the convergence criterion is satisfied . @xmath385 .",
    "@xmath386 @xmath387 @xmath388 .                      elgindy kt ( 2016 ) high - order numerical solution of second - order one - dimensional hyperbolic telegraph equation using a shifted gegenbauer pseudospectral method .",
    "numerical methods for partial differential equations 32(1):307349 .",
    "elgindy kt , smith - miles ka ( 2013 ) fast , accurate , and small - scale direct trajectory optimization using a gegenbauer transcription method .",
    "journal of computational and applied mathematics 251(0):93116 .",
    "elgindy kt , smith - miles ka ( 2013 ) solving boundary value problems , integral , and integro - differential equations using gegenbauer integration matrices .",
    "journal of computational and applied mathematics 237(1):307325 .",
    "sherman j , morrison wj ( 1949 ) adjustment of an inverse matrix corresponding to changes in the elements of a given column or a given row of the original matrix . in : annals of mathematical statistics ,",
    "inst mathematical statistics ims business office - suite 7 , 3401 investment blvd , hayward , ca 94545 , vol  20 , pp 621621 ."
  ],
  "abstract_text": [
    "<S> this paper presents for the first time a robust exact line - search method based on a full pseudospectral ( ps ) numerical scheme employing orthogonal polynomials . </S>",
    "<S> the proposed method takes on an adaptive search procedure and combines the superior accuracy of chebyshev ps approximations with the high - order approximations obtained through chebyshev ps differentiation matrices ( cpsdms ) . </S>",
    "<S> in addition , the method exhibits quadratic convergence rate by enforcing an adaptive newton search iterative scheme . </S>",
    "<S> a rigorous error analysis of the proposed method is presented along with a detailed set of pseudocodes for the established computational algorithms . </S>",
    "<S> several numerical experiments are conducted on one- and multi - dimensional optimization test problems to illustrate the advantages of the proposed strategy . </S>"
  ]
}