{
  "article_text": [
    "classic methods of linear dimensionality reduction assume that data approximately follows some low - dimensional linear subspace and aim at finding an optimal projection onto that subspace , i.e. principle component analysis ( pca ) @xcite and random projection @xcite .",
    "nonlinear dimensionality reduction , also referred to as _ manifold learning _ , is a generalization of those linear techniques that aims at fitting a nonlinear low dimensional structure .",
    "such manifold learning methods as isometric feature mapping @xcite , locally linear embedding @xcite , and laplacian eigenmap @xcite are widely used as methods of nonlinear dimensionality reduction in machine learning , either to reduce the computational cost of working in higher - dimensional spaces , or to learn or approximate a manifold more favourable to subsequent learning tasks such as classification or regression .",
    "these algorithms seek to determine a nonlinear lower dimensional space by preserving various geometric properties of the input .",
    "however , it is not clear which of these properties would be more beneficial to the later discrimination stage .",
    "since they are typically unsupervised techniques , they present a certain risk for the later classification or regression task : the lower - dimensional space found may not be the most helpful one for the second supervised learning stage and , in fact , in some cases could be harmful .",
    "how should we design manifold construction techniques to benefit most the subsequent supervised learning stage ?    as shown by figure",
    "[ fig : example ] , simply optimizing geometric properties may be detrimental the subsequent learning stage .",
    "to solve this problem , we consider an alternative scenario where the manifold construction step is not carried out _",
    "blindly_. we couple the task of nonlinear dimensionality reduction with the subsequent supervised learning stage . to do so , we make use of the known remarkable result that all of the manifold learning techniques already mentioned and many others are specific instances of the generic kernel pca ( kpca ) algorithm for different choices of the kernel function @xcite . more generally , all these methods can be thought of first mapping input vectors into a reproducing kernel hilbert space and then conducting a low - rank projection within that space .",
    "thus , our goal is to both learn a mapping as well as a projection taken from a parametric family as well as a hypothesis which is found in the low - dimensional space .",
    "the main purpose of this paper is precisely to derive learning guarantees for this scenario , which we coin as _ coupled nonlinear dimensionality reduction _ , and to use those guaranteed as guidelines in the design of algorithms .    in practice",
    ", a user will often use a handful of different kernel functions and choose the one that is most effective according to measurements on a validation dataset . instead ,",
    "in this work , we argue that a more effective method is to allow a learning algorithm itself to choose a kernel function from a parametrized class .",
    "the idea of automatically selecting a kernel function has been explored in context of learning algorithms such as support vector machines ( svm ) @xcite and kernel ridge regression ( krr ) @xcite ( see @xcite and references therein for a more complete survey ) . to define the feature mapping , we will consider kernel families that consist of linear combinations of fixed _ base kernel functions_. such linear families have been analyzed extensively in the literature @xcite , however , mainly in the context of kernelized learning algorithms rather than dimensionality reduction techniques .",
    "similarly , to define the projection , we will make use of the top-@xmath3 eigenspace of a covariance operator that is defined as the linear combination of the covariances operators of the weighted base kernels .",
    "while some recent work has considered kernel learning in the setting of dimensionality reduction @xcite , to the best of our knowledge there has been no theoretical analysis or theoretical justification for the proposed algorithms . in this work",
    ", we provide the necessary theoretical analysis .",
    "as mentioned above , within the setting of machine learning , dimensionality reduction is primarily used as a pre - processing step before regression / classification .",
    "for example , the recent work @xcite illustrates the benefit of dimensionality reduction as preprocessing step by comparing the risk of ols regression on reduced data to the risk of ridge regression on full data .",
    "they conclude that the risk of pca - ols is at most a constant factor of the risk of ridge regression , but can often be much less , as shown empirically .    perhaps unsurprisingly ,",
    "several empirical investigations have shown that tuning a dimensionality reduction algorithm in a coupled fashion , i.e.  taking into account the learning algorithm that will use the reduced features , results in considerably better performance on the learning task @xcite , @xcite . despite this",
    ", the vast majority of existing theoretical analyses of dimensionality reduction techniques ( even with fixed kernel functions ) do not directly consider the learning algorithm that it will be used in conjunction with , and instead focus on the optimization of surrogate metrics such as maximizing the variance of the projected features @xcite .",
    "one exception is the work of @xcite , which provides a generalization guarantee for hypotheses generated by first conducting kpca with a fixed kernel and then coupling with a regression model that minimizes squared loss .",
    "there is also a recent work of @xcite , which derives generalization bounds based on rademacher complexity for learning lipschitz functions in a general metric space .",
    "they show that the intrinsic dimension of data significantly influences learning guarantees by bounding the corresponding rademacher complexity in terms of dimension of underlying manifold and the distortion of training set relative to that manifold .    in our setting",
    ", we consider hypotheses which include both the kpca dimensionality reduction step , with a _ learned _ linear combination kernel , as well as a linear model which uses the reduced features for a supervised learning task .",
    "although the hypothesis set we analyze is most naturally associated to a `` coupled nonlinear dimensionality reduction '' algorithm , which jointly selects both a kernel for nonlinear projection as well as a linear parameter vector for a supervised learning task , we note that this hypothesis set also encompasses algorithms that proceed in two stages , i.e.  by first selecting a manifold and then learning a linear model on it .",
    "the results of this paper are organized as follows : in the following section we outline the learning scenario , including the hypothesis class , regularization constraints as well as define notation .",
    "section  [ sec : generalization ] contains our main result , which is an upper bound on the sample rademacher complexity of the proposed hypothesis class that also implies an upper bound on the generalization ability of the hypothesis class . in section  [ sec : lower bound ] we show a lower bound on the sample rademacher complexity as well as other quantities , which demonstrates a necessary dependence on several crucial quantities and helps to validate the design of the suggested hypothesis class . in section",
    "[ sec : discussion ] we provide a short discussion of the implications of our theoretical results , which leads us to section  [ sec : algorithm ] , where develop an algorithm for the coupled fitting of a kernel and a separation function .",
    "here , we describe the learning scenario of supervised dimensionality reduction . let @xmath4 denote the input space .",
    "we assume that the learner receives a labeled sample of size @xmath1 , @xmath5 , drawn i.i.d .",
    "according to some distribution @xmath6 over @xmath7 , as well as an unlabeled sample @xmath8 of size @xmath9 , typically with @xmath10 , drawn i.i.d .  according to the marginal distribution @xmath11 over @xmath4 .",
    "we assume that the learner has access to @xmath12 positive - definite symmetric ( pds ) kernels @xmath13 . instead of requiring the learner to commit to a specific kernel @xmath14 defining kpca with a solution subsequently used by a classification algorithm",
    ", we consider a case where the learner can define a dimensionality reduction solution defined based on @xmath15 , where the non - negative mixture weights @xmath16 are chosen to minimize the error of the classifier using the result of the dimensionality reduction .    given @xmath12 positive - definite symmetric ( pds ) kernels @xmath17 and a vector @xmath18 with non - negative coordinates ,",
    "consider a set of weighted kernel functions @xmath19 , where each @xmath20 has its reproducing space @xmath21 .",
    "the unlabeled sample @xmath22 is used to define the empirical covariance operator of each weighted base kernel @xmath20 , denoted as @xmath23 .",
    "given a set of covariance operators @xmath24 we define an operator @xmath25 that acts on the sum of reproducing spaces @xmath26 .",
    "let @xmath27 denote the rank-@xmath3 projection over the eigenspace of @xmath28 that corresponds to the top-@xmath3 eigenvalues of @xmath28 denoted as @xmath29 .",
    "satisfies @xmath30 in order to simplify the presentation .",
    "note , assumption this is always satisfied if the eigenvalues are simple .",
    "] let @xmath31 denote the feature mapping associated to @xmath32 , specifically for each @xmath33 , we have the function @xmath34 .",
    "define @xmath35 as @xmath36 .",
    "this parametrized projection is used to define rank-@xmath3 feature vectors ( functions ) @xmath37 .    from this point",
    "onward , in to avoid intricate notation , we will not not explicitly indicate the dependence of @xmath38 , @xmath39 and @xmath40 on @xmath41 and instead use @xmath42 , @xmath43 and @xmath44 .",
    "similarly , we will refer to @xmath45 as @xmath46 and @xmath47 as @xmath48 .",
    "we will also use the shorthand @xmath49 ( resp .",
    "@xmath50 ) instead of @xmath51 ( resp .",
    "@xmath52 ) .",
    "once a projection is defined , the labeled sample @xmath53 is used to learn a linear hypothesis @xmath54 with bounded norm , @xmath55 , in the subspace of the projected features @xmath56 .",
    "the two steps just described , dimensionality reduction by projection @xmath49 and supervised learning of @xmath57 , may be coupled so that the best choice of weights @xmath41 is made for the subsequent learning of @xmath57 .",
    "( see figure  [ fig : coupled ] ) .",
    "to do so , given constants @xmath2 and @xmath58 , we select vector @xmath41 out of a convex set @xmath59 @xmath60 we will show that the choice of this convex regularization set is crucial in guaranteeing the generalization ability our hypothesis class .",
    "the vector @xmath41 is upper bounded by an @xmath61-norm inequality @xmath62 ( standard from learning kernels literature ) but also by an inequality @xmath63 , where @xmath64 is the semi - norm defined as the ky fan @xmath3-norm of @xmath43 @xcite : @xmath65 the use of the semi - norm @xmath66 in this context is key since @xmath67 appears as the relevant quantity both in our generalization bounds and in our lower bounds . the lower bound constraint on @xmath41 , @xmath68",
    ", will imply an upper bound on the eigengap of the induced covariance operator , which is a fundamental quantity that influences the concentration of eigenspaces .",
    "in fact in section  [ sec : lower bound ] we give a simple example that demonstrates the dependence on the eigengap is tight , and also implies the necessity of the lower bound regularization .",
    "thus , the hypothesis set @xmath69 defined by our supervised dimensionality reduction set - up is defined as follows : @xmath70 in the analysis that follows , we will also make use of normalized kernel matrices which , given a sample @xmath53 of size @xmath1 and kernel function @xmath14 , are defined as @xmath71_{i , j } = \\frac{1}{m } k(x_i , x_j)$ ] .",
    "since the kernel matrix is normalized we have that @xmath72 , where @xmath73 is the sample covariance operator associated to @xmath74 ( see @xcite proposition  9.2 ) .",
    "we also define the unscaled sample kernel matrix @xmath75_{i , j}=k(x_{i},x_{j})$ ] .",
    "we will assume that @xmath43 admits at least @xmath3 non - zero eigenvalues and will similarly assume that the set of kernel matrices @xmath76 of @xmath74 associated to the sample @xmath22 or @xmath53 for any @xmath77 $ ] contains at least one matrix with rank at least @xmath3 .",
    "furthermore , we assume that the base kernels @xmath74 , @xmath78 $ ] , satisfy the condition @xmath79 for all @xmath80 , which is guaranteed to hold for all normalized kernels .",
    "finally , we assume the base kernels are _ linearly independent with respect to the union of the samples @xmath53 and @xmath22_.    [ [ def : lin - indep - kernels ] ] definition 1 .",
    "linearly independent kernels + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath81 be @xmath12 pds kernels and let @xmath82 be a sample of size @xmath1 . for any @xmath78",
    "$ ] , let @xmath83 denote the rkhs associated to @xmath74 and @xmath84 the subspace of @xmath83 spanned by the set of functions @xmath85 .",
    "then , @xmath81 are said to be _ linearly independent with respect to the sample @xmath53 _ if , for any @xmath78 $ ] , no non - zero function in @xmath84 can be expressed as a linear combination of the functions in @xmath86 .",
    "linear independence typically holds in practice , e.g. , for polynomial and gaussian kernels on @xmath87 . as an example , let @xmath88 and define the sample @xmath89 .",
    "define two base kernels : gaussian @xmath90 and linear @xmath91 .",
    "then @xmath92 , i.e.  @xmath93 is an exponential function @xmath94 with parameter @xmath95 and argument @xmath96 . in the same manner @xmath97 .",
    "thus , @xmath98 is the span of exponential functions @xmath99 and @xmath100 is the span of linear functions @xmath101 . clearly , no exponential function can be represented as a linear combination of linear functions and likewise , in general , no linear function is represented as a ( finite ) linear combination of exponential functions .",
    "thus , the base kernels @xmath102 and @xmath103 are linearly independent with respect to sample @xmath53 as in definition  [ def : lin - indep - kernels ] .",
    "therefore @xmath104 in the reproducing space of @xmath105 and defined @xmath106",
    ". such decomposition of @xmath107 into a direct sum allows to characterize the eigenfunctions of @xmath108 : they consist of the eigenfunctions of @xmath109 and @xmath110 .",
    "the eigenfunctions of each restricted operator are orthogonal to each other .",
    "note that the orthogonality of eigenfunctions does not necessarily imply the orthogonality of the eigenvectors of sample kernel matrices @xmath111 and @xmath112 .",
    "more generally , the support of the base kernels can be straightforwardly modified to ensure that this condition is satisfied .",
    "it follows from construction @xmath113 and the results of ( * ? ? ?",
    "* section 6 ) that when base kernels are linearly independent with respect to sample @xmath53 , then @xmath84 are orthogonal subspaces of @xmath42 , thus we can define @xmath114 , which will be extremely useful in decomposing the spectra of operators @xmath108 .",
    "linearly independent base kernels imply that @xmath108 has at most @xmath115 nonzero eigenvalues of the form @xmath116 , which is an explicit representation of the eigenvalues of @xmath108 in terms of @xmath41 .",
    "in this section we outline the main steps taken in deriving a generalization bound as well as analyze the bound and discuss its implications .",
    "proofs that are not included in this section can be found in the appendix .",
    "the main result of this section is to derive an upper bound on the sample rademacher complexity @xmath117 of hypothesis class @xmath69 .",
    "the sample rademacher complexity of @xmath69 is defined as @xmath118 $ ] , where @xmath119 are i.i.d .",
    "random variables taking values + 1 and -1 with equal probabilities .",
    "once that is done we can then directly invoke the result of @xcite and @xcite , which states that with probability at least @xmath120 over the draw of sample @xmath121 and for all @xmath122 the generalization error @xmath123 is bounded by @xmath124 where , given @xmath125 is the true label of @xmath80 , @xmath126 $ ] and @xmath127 is the fraction of points in @xmath53 with classification margin less than @xmath128 .",
    "note , in our setting @xmath69 is parametrized by @xmath57 and @xmath41 and that we can consider the supremum over these two parameters separately . finding the supremum over @xmath57 can be done in a standard manner , using cauchy - schwarz , @xmath129 now it remains to compute @xmath130 , which is the more challenging expression .",
    "first of all , it will be more convenient to work with a projection defined with respect to @xmath131 instead of @xmath43 , since we are projecting instances from sample @xmath53 .",
    "similarly , we will find it useful to control a norm @xmath132 instead of @xmath133 .",
    "both of these issues can be addressed by using concentration inequalities to bound the difference of the projections @xmath49 and @xmath134 @xcite as well as the difference of the operators @xmath43 and @xmath108 @xcite .",
    "for that we extend a constraint set @xmath135 to a larger set @xmath136 : @xmath137 where @xmath138 . in the following lemma we bound the transition from @xmath139 to @xmath140 .",
    "[ lemma : approximation of supremum ] let @xmath73 be the sample covariance operator of kernel @xmath20 with a reproducing space @xmath83 . define @xmath108 ( resp .",
    "@xmath43 ) as @xmath141 and @xmath134 ( resp .",
    "@xmath49 ) be the orthogonal projection onto the eigenspace of @xmath142 for @xmath143 $ ] .",
    "then with probability at least @xmath120 for any @xmath144 @xmath145 where @xmath146 } \\big(\\lambda_{r}(c_k)-\\lambda_{r+1}(c_k)\\big)$ ] , @xmath147 is the true covariance operator of kernel @xmath74 and @xmath138 .    now using lemma  [ lemma : approximation of supremum ] and letting @xmath148 we find that , with high probability , the rademacher complexity of @xmath69 is bounded by    @xmath149   \\ , .",
    "\\end{split}\\end{aligned}\\ ] ]    we will distribute the supremum and bound each of the two terms separately . in the case of the second term , @xmath150 , we will upper bound the supremum over @xmath151 with the supremum over a larger set constrained only by @xmath152 .",
    "this leads us to the expression @xmath153 $ ] , which is exactly equal to the rademacher complexity of learning kernels for classification without projection .",
    "this complexity term can be bounded using theorem 2 of @xcite , which gives the following : @xmath154 \\leq \\frac{\\sqrt { \\eta_{0 } e \\lceil \\log{p } \\rceil } } { \\sqrt{m } } \\,,\\ ] ] where @xmath155 .",
    "now it remains to bound the expectation of @xmath156 .",
    "[ lemma : supremum ] let @xmath147 and @xmath73 be the true and sample covariance operator of kernel @xmath74 . define @xmath108 ( resp .",
    "@xmath43 ) as @xmath141 and let @xmath134 ( resp .",
    "@xmath49 ) be the orthogonal projection onto the eigenspace of @xmath142 ( resp .",
    "@xmath157 ) for @xmath143 $ ] . then with probability at least @xmath120 .",
    "@xmath158 \\leq   \\\\        \\frac{1}{\\sqrt{m}}\\sqrt{2\\big({{\\lambda_{(r)}}}+       { \\kappa}\\big)\\log{(2 p \\textbf { } m ) } }        \\,,\\end{gathered}\\ ] ] where @xmath138 .",
    "the term @xmath159 is naturally bound using the constraint on @xmath160 since it involves projection onto eigenspace of @xmath108 and @xmath161 controls its spectrum",
    ". therefore we will reduce the problem to the supremum over @xmath162 , where @xmath163 .    by lemma  [ lemma : inner_product_form ] ( see appendix ) we have that @xmath164 , where @xmath165 is a vector with entries @xmath166 and @xmath167 is a vector with entries @xmath168 such that @xmath169 .",
    "an indexing set @xmath170 is a set of pairs @xmath171 that correspond to largest @xmath3 eigenvalues of @xmath172 . in order",
    "remove the dependence of the indexing set on the identity of the top eigenvalues , we upper bound the expression over the choice of all size-@xmath3 sets : @xmath173 where @xmath174 indicates the supremum over all indexing sets @xmath175 of size @xmath3 .",
    "then , by the dual norm property we have @xmath176 thus , @xmath177 is bounded by the following :    @xmath178    by massart s lemma @xcite @xmath179\\leq \\sqrt{2 \\log{(2 p m ) } } \\,.\\ ] ] this follows since the norm of @xmath180 is bounded by @xmath181 and the cardinality of the set which the maximum is taken over is bounded by @xmath182 .",
    "combining all intermediate results brings us to the bound @xmath183 \\leq \\\\        \\frac{1}{\\sqrt{m } } \\sqrt{2 \\epsilon \\log{(2 p \\textbf { } m)}},\\end{gathered}\\ ] ] and the final result is obtained by letting @xmath163 .",
    "< 1.5em - 1.5em plus0em minus0.5em height0.75em width0.5em depth0.25em    thus , after combining lemma  [ lemma : supremum ] and  [ lemma : approximation of supremum ] above we derive an upper bound on the expectation in , which gives us a bound on the sample rademacher complexity . that bound is presented in the following theorem .",
    "[ theorem : rademacher_complexity ] let hypothesis set @xmath69 be defined as in .",
    "then for any sample @xmath53 of size @xmath184 drawn i.i.d.according to some distribution @xmath6 over @xmath7 such that @xmath185 the empirical rademacher complexity of the hypothesis set @xmath69 can be bounded as follows with probability at least @xmath120 , @xmath186 where @xmath187 , @xmath138 and @xmath188 .    if we consider only parameters @xmath12 and @xmath2 , then the rademacher complexity bound in   is @xmath189 .",
    "the learning scenario and regularization in standard learning kernels @xcite differs from ours , thus we will make a few adjustments that will allow us to compare those two bounds in a most coherent way , particularly , we will let @xmath190 and express @xmath2 in terms of unscaled sample kernel matrices . @xmath191",
    "that results in rademacher complexity of @xmath192 , while the standard learning kernels bound is @xmath193}\\sum_{j=1}^{m}\\lambda_{j}({{\\mathbf{k}}}_k)\\log{(p)}}\\big)$ ] . here",
    ", @xmath194 is the largest @xmath3-long sum of eigenvalues that can be picked from any base kernel matrix , while @xmath195}\\sum_{j=1}^{m}\\lambda_{j}({{\\mathbf{k}}}_k)$ ] is the largest @xmath196 long sum of eigenvalues that can be picked only from one base kernel matrix .",
    "thus , if @xmath3 is sufficiently smaller than @xmath1 and given a certain choice of base kernels , learning kernels in the supervised dimensionality reduction problem will enjoy a tighter rademacher complexity than standard learning kernels .    finally , plugging in the upper bound from theorem  [ theorem : rademacher_complexity ] into results in the generalization bound of @xmath69 .",
    "note that the confidence term in   changes from @xmath197 to @xmath198 , because rademacher complexity is bounded with high probability . to the best of our knowledge ,",
    "this is the first generalization guarantee provided for the use of projection in the reproducing space with a learned kernel in a supervised learning setting .",
    "[ theorem : generalization_bound ] let hypothesis set @xmath69 be defined as in . then with probability at least @xmath120 over the draw of sample @xmath121 and for all @xmath122 the generalization error @xmath123 is bounded by @xmath199 where @xmath127 is the fraction of points in @xmath53 with classification margin less than @xmath128 .",
    "we note that @xcite and @xcite provide generalization bounds for supervised dimensionality reduction , however their learning scenarios are different from ours , particularly in a sense that they do not learn a mapping and projection for dimensionality reduction jointly with a discrimination function on reduced data",
    ". nevertheless , their generalization bounds are comparable to ours in the special case @xmath200 .",
    "the analysis of @xcite is done for general metric spaces , while in the particular example of euclidean space they show that generalization bound is @xmath201 , where @xmath202 is the dimension of underlying data manifold and @xmath203 is the average distance of training set to that manifold .",
    "thus , while our bound has the same rate with respect to @xmath1 as that of @xcite , it is based on ky - fan semi - norm regularization @xmath2 , while the bound of @xcite is based on the assumption that data approximately follows some low dimensional manifold with dimension @xmath202 .",
    "the generalization bound in @xcite is @xmath204 , however while we fix the number of eigenvalues for dimensionality reduction at @xmath3 , their bound requires selecting all eigenvalues above a threshold @xmath205 . by applying this threshold",
    "they have control over the eigenspectrum of covariance operator , while we control the spectrum by the ky - fan regularization .",
    "in this section we show a lower bound on the rademacher complexity of the hypothesis class @xmath69 defined in .",
    "this lower bound demonstrates the central role that the ky fan @xmath3-norm regularization , @xmath206 , plays in controlling the complexity of the hypothesis class .",
    "furthermore , it demonstrates the tightness of the upper bound presented in the previous section in terms the number of training samples @xmath1 .",
    "we additionally give a small example that demonstrates the necessity of the eigengap term which appears in lemma  [ lemma : approximation of supremum ] and which motivates the additional regularization term @xmath207 that is used to bound the eigengap in the proof of lemma  [ lemma : approximation of supremum ] .",
    "[ theorem : lower_bound ] for any choice of @xmath1 , @xmath3 there exists samples @xmath53 and @xmath22 , a setting of the regularization parameter @xmath2 , as well as a choice of base kernels @xmath208 that guarantees @xmath209    first we let @xmath53 and @xmath22 be any two samples , both of size @xmath1 , such that the @xmath22 is simply an unlabeled version of @xmath53 .",
    "now , assume @xmath200 and the sample kernel matrix @xmath111 of kernel @xmath102 has exactly @xmath3 distinct non - zero simple eigenvalues .",
    "finally , select @xmath2 such that @xmath210 .    as calculated in section [ sec : generalization ] , @xmath211 and in this particular scenario @xmath212 , thus the empirical rademacher complexity simplifies to @xmath213 $ ] , where the projection can be written directly in terms of the sample @xmath53 and the @xmath61 constraint on @xmath41 is not needed since it is satisfied by the ky fan @xmath3-norm constraint when @xmath214 .",
    "now , let @xmath215 denote the top @xmath3 eigenfunctions of @xmath108 , then following the steps from lemma  [ lemma : inner_product_form ] we can express the norm of projection as    @xmath216    where @xmath217 is the eigenpair of normalized sample kernel matrix @xmath218 .",
    "the expression is furthermore simplified by introducing the vectors @xmath165 with entries @xmath219 and @xmath167 with entries @xmath220 .",
    "note that here , unlike in the general statement of lemma  [ lemma : inner_product_form ] , the choice of @xmath3 entries that appear in @xmath221 and @xmath222 are not effected by the value of @xmath41 , since there are in fact only @xmath3 non - zero eigenvalues total by construction ( i.e. there is one base kernel of rank @xmath3 ) .",
    "the choice of @xmath41 , however , still affects the scale of the @xmath3 eigenvalues .    by the monotonicity of the square - root function and using the definition of @xmath223 as well as the dual norm we have @xmath224",
    "thus , the rademacher complexity is reduced to @xmath225 } ( { { \\mathbf{v}}}^{\\top}_{1,j } { { \\boldsymbol \\sigma}})^2 } \\big ] \\\\   & = \\sqrt{\\frac{{{\\lambda_{(r)}}}}{m } } \\operatorname*{\\rm e}_{{{\\boldsymbol \\sigma } } } \\big [ \\max_{j \\in [ 1,r]}|{{\\mathbf{v}}}^{\\top}_{1,j } { { \\boldsymbol \\sigma}}|\\big ] \\ , .",
    "\\end{split}\\end{aligned}\\ ] ] finally , we use jensen s inequality and khintchine s inequality to show @xmath226}|{{\\mathbf{v}}}^{\\top}_{1,j } { { \\boldsymbol \\sigma}}|\\big ]    & \\geq \\max_{j \\in [ 1,r ] } \\operatorname*{\\rm e}_{{{\\boldsymbol \\sigma } } } \\big [ |{{\\mathbf{v}}}^{\\top}_{1,j } { { \\boldsymbol \\sigma}}|\\big ] \\\\    & \\geq \\max_{j \\in [ 1,r ] } 2^{-1/2 } \\| { { \\mathbf{v}}}_{1,j } \\| = 2^{-1/2},\\end{aligned}\\ ] ] where the tight constant @xmath227 used in khintchine s inequality can be found in chapter ii of @xcite .",
    "plugging this constant back into equation   completes the theorem .",
    "< 1.5em - 1.5em plus0em minus0.5em height0.75em width0.5em depth0.25em    the lower bound demonstrates the effect of the regularization parameter @xmath2 as well as the tightness of the upper bound in terms of @xmath1 .",
    "while theorem  [ theorem : lower_bound ] has shown the necessity of the ky fan @xmath3-norm constraint , we will now give a small example that illustrates the difference of projections ( for example as seen in lemma  [ lemma : approximation of supremum ] ) must necessary depend on the eigengap quantity .",
    "this in turn motivates the regularization @xmath228 which ensures that eigengap is not arbitrarily small , since otherwise if @xmath41 goes to zero , then @xmath229 also goes to zero .",
    "the fact that the eigengap is essential for the concentration of projections has been known in the matrix perturbation theory literature @xcite .",
    "the following proposition gives an example which shows that the dependence on the eigengap is tight .",
    "there exists operators @xmath230 and @xmath231 such that @xmath232 where @xmath233 ( resp .",
    "@xmath234 ) is the orthogonal projection onto the top @xmath3 eigenspace of @xmath230 ( resp .",
    "@xmath231 ) .    consider @xmath235 and @xmath230 and @xmath231 defined as follows @xmath236 and @xmath237 , thus @xmath238 , which implies that @xmath239 .",
    "also , the eigengap is equal to @xmath240 .",
    "now note that @xmath241 is the projection onto @xmath242 and @xmath243 is the projection onto @xmath244 .",
    "since @xmath245 and @xmath246 are orthogonal , this implies @xmath247 . on the other hand , @xmath248 , which complete the proof .",
    "< 1.5em - 1.5em plus0em minus0.5em height0.75em width0.5em depth0.25em    assume that operator @xmath249 is defined together with a positive @xmath250 , then the stability of the r - eigenspace of @xmath251 is determined by @xmath252 .",
    "when we have operators @xmath253 , with identical spectra such that @xmath254 for each @xmath255 in @xmath256 $ ] , but each of them acting on mutually orthogonal subspaces , the stability of the r - eigenspace of @xmath257 is determined by @xmath258 .",
    "this example clearly shows that @xmath259 controls the stability of eigenspace when operators act on orthogonal subspaces , which directly applies to linearly independent kernels .",
    "here , we briefly discuss the results presented .",
    "let us first emphasize that our choice of the hypothesis class @xmath69 ( section  [ sec : learning scenario ] ) is strongly justified a posteriori by the learning guarantees we presented : both our upper and lower bounds on the rademacher complexity ( sections  [ sec : generalization ] and [ sec : lower bound ] ) suggest controlling the quantities present in the definition of @xmath69 .",
    "the regularization parameters we provide can be tuned to directly bound each of these crucial quantities and thereby limit the risk of over fitting .",
    "second , we observe that the hypothesis class @xmath69 clearly motivates the design of a single - stage coupled algorithm . such an algorithm would be based on structural risk minimization ( srm ) and seek to minimize the empirical error over increasingly complex hypothesis sets , by varying the parameters @xmath2 and @xmath58 , to trade - off empirical error and model complexity . although the design and evaluation of such an algorithm is beyond the scope of this paper , we note that existing literature has empirically evaluated both learning kernels with kpca in an unsupervised ( two - stage ) fashion @xcite and applied dimensionality reduction ( single - stage training ) with a fixed kernel function @xcite , @xcite . while these existing algorithms do not directly consider the hypothesis class we motivated , they can , in certain cases , still select a hypothesis function that is found in our class . in particular , our learning guarantees remain applicable to hypotheses chosen in a two - stage manner , as long as the regularization constraints are satisfied .",
    "similarly the case @xmath260 which corresponds to the standard fixed - kernel supervised learning scenario is covered by our analysis .",
    "we note that even in such cases , the bounds that we provide would be the first to guarantee the generalization ability of the algorithm via bounding the sample rademacher complexity .",
    "in this section we obtain a computational expression for @xmath261 , where @xmath262 and @xmath122 .",
    "moreover , we formulate a minimization problem for training @xmath41 and @xmath57 as well as discuss ways to efficiently solve it by breaking into a series of convex sub - problems .    for the clarity of presentation",
    "we assume @xmath49 is full rank and provide the expression for @xmath263 in the following lemma .",
    "[ lemma : compute_h ] let @xmath264 , then for every @xmath265 there exist real numbers @xmath266,j\\in[1,m]}$ ] such that    @xmath267_{n}\\ ] ]    where @xmath268    with the following constraints @xmath269    @xmath270    @xmath271    @xmath272    @xmath273    first , to obtain the computational expression for @xmath274 , for the moment imagine that @xmath49 is of full rank , then if @xmath275 are the coordinates of @xmath276 in the span of eigenfunctions of @xmath43 and @xmath277 are the coordinates of @xmath278 in that span , we will have @xmath279 . now , when we go back to the original scenario of rank @xmath3 projection @xmath49 , we introduce choice variables @xmath280 , where @xmath281 if the @xmath282-th eigenfunction is chosen for projection and @xmath283 otherwise .",
    "thus , the expression for @xmath261 becomes @xmath284    the assumption of linearly independent kernels allows us to break the sum above into @xmath12 components for each base kernel",
    ". we will do it by keeping two indices @xmath285 $ ] and @xmath286 $ ] , which gives @xmath287 .",
    "observe that @xmath288 and the steps in the proof of lemma  [ lemma : inner_product_form ] show that @xmath289_{n}\\ ] ] moreover , varying @xmath276 in @xmath42 for the purpose of our algorithm is equivalent to varying its coordinates @xmath290 , thus we will use variables @xmath291 instead of them with the constraint @xmath292 . given all this analysis , the computational expression for @xmath261 becomes    the optimization variables are @xmath291 and @xmath293 , while @xmath294 is determined by @xmath41 .",
    "< 1.5em - 1.5em plus0em minus0.5em height0.75em width0.5em depth0.25em    when we define @xmath295 as vector with entries @xmath291 and have some convex loss function over the training sample @xmath296 , the optimization problem is @xmath297    subject to @xmath298    @xmath299    note that the loss function includes the complicated term @xmath300_{n}\\,.\\ ] ]    for conciseness , define @xmath301_{n } \\,,\\ ] ] which allows us to write clearly @xmath302 .",
    "we will make a substitution @xmath303 . that substitution changes constraint @xmath304 to @xmath305 , which is a convex set in @xmath57 and @xmath41 for @xmath306 .",
    "this reduces the problem to the following constrained optimization problem @xmath307subject to @xmath308 and @xmath309    there are at least two possible ways to relax the problem .",
    "first , we can relax @xmath310 to no longer be a function of @xmath41 and make @xmath311 a discrete optimization variable instead with an additional constraint @xmath312 .",
    "the second option is even weaker : we can let @xmath294 be a continuous variable in the interval @xmath313 $ ] and keep the constraint @xmath312 .",
    "investigation of those relaxation steps is a direction for further research .",
    "we presented a new analysis and generalization guarantees for the scenario of coupled nonlinear dimensionality reduction with a learner kernel .",
    "the hypothesis class is designed with regularization constraints that are directly motivated by the generalization guarantee , which we show lower bounds for as well .",
    "our analysis invites the design of learning algorithms for selecting hypotheses from this specifically tailored class , either in a two - stage or a single - stage manner .",
    "anthony , m. and bartlett , p. ( 1999 ) . .",
    "cambridge university press .",
    "aronszajn , n. ( 1950 ) .",
    "theory of reproducing kernels . , pages 337404 .",
    "bartlett , p.  l. and mendelson , s. ( 2003 ) .",
    "rademacher and gaussian complexities : risk bounds and structural results . , 3:463482 .",
    "belkin , m. and niyogi , p. ( 2001 ) .",
    "laplacian eigenmaps and spectral techniques for embedding and clustering . in _ nips _ , volume  14 , pages 585591 .",
    "bhatia , r. ( 1997 ) . .",
    "springer .",
    "blanchard , g. and zwald , l. ( 2008 ) .",
    "finite - dimensional projection for classification and statistical learning .",
    ", 54(9):41694182 .",
    "cortes , c. , mohri , m. , and rostamizadeh , a. ( 2009 ) .",
    "l 2 regularization for learning kernels . in _ proceedings of the twenty - fifth conference on uncertainty in artificial intelligence _ , pages 109116 .",
    "cortes , c. , mohri , m. , and rostamizadeh , a. ( 2010 ) .",
    "generalization bounds for learning kernels . in _ proceedings of the 27th international conference on machine learning ( icml-10 ) _ , pages 247254 .",
    "dhillon , p.  s. , foster , d.  p. , kakade , s.  m. , and ungar , l.  h. ( 2013 ) .",
    "a risk comparison of ordinary least squares vs ridge regression .",
    ", 14(1):15051511 .",
    "fukumizu , k. , bach , f.  r. , and jordan , m.  i. ( 2004 ) . dimensionality reduction for supervised learning with reproducing kernel hilbert spaces .",
    ", 5:7399 .",
    "gnen , m. ( 2014 ) .",
    "coupled dimensionality reduction and classification for supervised and semi - supervised multilabel learning .",
    ", 38:132141 .",
    "gnen , m. and alpaydin , e. ( 2011 ) .",
    "multiple kernel learning algorithms .",
    ", 12:22112268 .",
    "gottlieb , l .- a . , kontorovich , a. , and krauthgamer , r. ( 2013 ) . adaptive metric dimensionality reduction . in _ algorithmic learning theory _ , pages 279293 .",
    "springer .",
    "ham , j. , lee , d.  d. , mika , s. , and schlkopf , b. ( 2004 ) . a kernel view of the dimensionality reduction of manifolds . in",
    "_ proceedings of the twenty - first international conference on machine learning _",
    ", page  47 .",
    "hegde , c. , wakin , m. , and baraniuk , r. ( 2008 ) .",
    "random projections for manifold learning . in _ advances in neural information processing systems _ , pages 641648 .",
    "kloft , m. , brefeld , u. , sonnenburg , s. , and zien , a. ( 2011 ) .",
    "lp - norm multiple kernel learning .",
    ", 12:953997 .",
    "koltchinskii , v. and panchenko , d. ( 2002 ) . empirical margin distributions and bounding the generalization error of combined classifiers . , pages 150 .",
    "lanckriet , g.  r. , cristianini , n. , bartlett , p. , ghaoui , l.  e. , and jordan , m.  i. ( 2004 ) .",
    "learning the kernel matrix with semidefinite programming .",
    ", 5:2772 .",
    "lin , y .- y . ,",
    "liu , t .- l . , and fuh , c .- s .",
    "multiple kernel learning for dimensionality reduction .",
    ", 33(6):11471160 .",
    "massart , p. ( 2000 ) .",
    "some applications of concentration inequalities to statistics .",
    ", 9(2):245303 .",
    "mohri , m. , rostamizadeh , a. , and talwalkar , a. ( 2012 ) . .",
    "mit press .",
    "mosci , s. , rosasco , l. , and verri , a. ( 2007 ) .",
    "dimensionality reduction and generalization . in _ proceedings of the 24th international conference on machine learning _ ,",
    "pages 657664 .",
    "nazarov , f.  l. and podkorytov , a.  n. ( 2000 ) .",
    "ball , haagerup , and distribution functions . in _ complex analysis , operators , and related topics _ , pages 247267 .",
    "springer .",
    "pearson , k. ( 1901 ) .",
    "liii . on lines and planes of closest fit to systems of points in space .",
    ", 2(11):559572 .",
    "rosasco , l. , belkin , m. , and vito , e.  d. ( 2010 ) .",
    "on learning with integral operators .",
    ", 11:905934 .",
    "roweis , s.  t. and saul , l.  k. ( 2000 ) .",
    "nonlinear dimensionality reduction by locally linear embedding .",
    ", 290(5500):23232326 .",
    "shawe - taylor , j. and cristianini , n. ( 2003 ) .",
    "estimating the moments of a random vector with applications . in _ proceedings of the gretsi 2003 conference _ ,",
    "pages 4752 .",
    "stewart , g. and sun , j. ( 1990 ) . .",
    "academic press .",
    "tenenbaum , j.  b. , de  silva , v. , and langford , j.  c. ( 2000 ) . a global geometric framework for nonlinear dimensionality reduction .",
    ", 290(5500):23192323 .",
    "zhuang , j. , wang , j. , hoi , c.  h. , and lan , x. ( 2011 ) .",
    "unsupervised multiple kernel learning .",
    ", 20:129144 .",
    "zwald , l. and blanchard , g. ( 2005 ) . on the convergence of eigenspaces in kernel principal component analysis . in",
    "_ proceedings of nips _ , pages 16491656 .",
    "for the first part of the proof , let @xmath314 be the true covariance operator of kernel @xmath315 .",
    "since for each @xmath255 both @xmath73 and @xmath46 approach @xmath314 with high probability , we will show a concentration bound on their difference that holds uniformly over @xmath316 $ ] as well as @xmath22 and @xmath53 . using union bound for probabilities and lemma  1 from @xcite ( equivalently corollary  5 from @xcite ) with probability at least @xmath120 for",
    "all @xmath285 $ ] , @xmath317 \\leq 2\\mu_{k}m_{\\delta } / \\sqrt{m}\\,,\\ ] ] where @xmath318 .",
    "we used @xmath319 to obtain the bound .    by triangle inequality and decomposition over orthogonal subspaces of @xmath320 , the norm @xmath321 is bounded by @xmath322 . by theorem  3 , which stems from the convergence of power series used in the proof . here , for the simplicity of presentation",
    "we multiply their bound by 4 , which relaxes such a requirement ] from @xcite for each @xmath285 $ ] , @xmath323 and a similar statement holds for projeciton with respect to sample @xmath22 .    we will use @xmath324 to make the bound in   be @xmath325 and we will decompose @xmath326 , where @xmath327 is the true covariance operator of kernel @xmath328 and @xmath329}\\big(\\lambda_{r}(c_k)-\\lambda_{r+1}(c_k)\\big)$ ] . now @xmath330 is the uniform bound on the norm of projections in  . summing up @xmath331 over @xmath255 and applying the uniform bound @xmath332 , which holds for both samples @xmath22 and @xmath53 , we conclude @xmath333    for the last part we use a simple series of inequalities to get @xmath334 which is in turn bounded by @xmath335 using hoffman - wielandt inequality .",
    "now , @xmath336 is simply bounded by @xmath337 .",
    "if we apply the uniform bound from   in the form @xmath338 , we get that with probability at least @xmath120 @xmath339    putting together the two main bounds , we have that : @xmath340 < 1.5em - 1.5em plus0em minus0.5em height0.75em width0.5em depth0.25em",
    "[ lemma : inner_product_form ] let @xmath73 be the sample covariance operator of kernel @xmath20 . define @xmath108 ( resp .",
    "@xmath43 ) as @xmath141 and @xmath134 ( resp .",
    "@xmath49 ) be the orthogonal projection onto the eigenspace of @xmath142 for @xmath143 $ ] .",
    "let @xmath341 be the eigenvector corresponding to @xmath342 . if @xmath170 is an indexing set that contains pairs @xmath171 that correspond to largest @xmath3 eigenvalues from the set @xmath172 , then @xmath343 where @xmath165 is a vector with entries @xmath166 , and @xmath167 is a vector with entries @xmath168 such that @xmath169 .    by properties of orthogonal projection , @xmath344 can be expressed as @xmath345 , where @xmath346 is the orthogonal projection onto the eigenfunction that corresponds to @xmath142 . recall from section  [ sec : learning scenario ] that when base kernels are linearly independent with respect to sample @xmath53 , then @xmath347 , which implies that for every @xmath348 $ ] there exists a @xmath171 such that the eigenfunction of @xmath142 is equal to the eigenfunction of @xmath349 .",
    "note that @xmath350 may not be equal to @xmath282 since @xmath41 influences the ordering of eigenvalues .",
    "thus , we define the indexing set @xmath351 that contains the @xmath3 pairs of indices @xmath171 that correspond to the @xmath3 largest eigenvalues in @xmath352 for particular value of @xmath41 .    for any @xmath353 we can express @xmath354 , where @xmath355 is the eigenfunction corresponding to @xmath356 . since @xmath347",
    "we can express the norm of projection as @xmath357 , where @xmath358 is an eigenfunction of @xmath73 with eigenvalue @xmath359 .",
    "when @xmath360 observe that @xmath358 belongs to orthogonal component @xmath84 , therefore it suffices to take inner product in @xmath84 , which by the reproducing property is equal to @xmath361 . by (",
    "* equation 18 ) @xmath361 takes the form @xmath362_{i } \\,,\\ ] ] which results in the following series of equalities @xmath363_{i}\\frac{1}{m}k_k(x_{n},x_{i } )   \\\\ & = \\sqrt{m } \\sqrt{\\frac { \\mu_k}{\\lambda_{j}({\\overline{{\\mathbf{k}}}}_k ) } } \\lambda_{j}({\\overline{{\\mathbf{k}}}}_k ) \\sum_{n = 1}^m \\sigma_n\\big[{{\\mathbf{v}}}_{k , j } \\big]_{n } \\\\ & = \\sqrt{m}\\sqrt { \\mu_k\\lambda_{j}({\\overline{{\\mathbf{k}}}}_k ) } { { \\mathbf{v}}}_{k , j}^{\\top}{{\\boldsymbol \\sigma}}\\,.\\end{aligned}\\ ] ] squaring the terms above and summing them up , we arrive at @xmath364 which complete the proof .",
    "< 1.5em - 1.5em plus0em minus0.5em height0.75em width0.5em depth0.25em"
  ],
  "abstract_text": [
    "<S> in this paper we introduce and analyze the learning scenario of _ coupled nonlinear dimensionality reduction _ , which combines two major steps of machine learning pipeline : projection onto a manifold and subsequent supervised learning . </S>",
    "<S> first , we present new generalization bounds for this scenario and , second , we introduce an algorithm that follows from these bounds . </S>",
    "<S> the generalization error bound is based on a careful analysis of the empirical rademacher complexity of the relevant hypothesis set . in particular </S>",
    "<S> , we show an upper bound on the rademacher complexity that is in @xmath0 , where @xmath1 is the sample size and @xmath2 the upper bound on the ky - fan @xmath3-norm of the associated kernel matrix . </S>",
    "<S> we give both upper and lower bound guarantees in terms of that ky - fan @xmath3-norm , which strongly justifies the definition of our hypothesis set . to the best of our knowledge , </S>",
    "<S> these are the first learning guarantees for the problem of coupled dimensionality reduction . </S>",
    "<S> our analysis and learning guarantees further apply to several special cases , such as that of using a fixed kernel with supervised dimensionality reduction or that of unsupervised learning of a kernel for dimensionality reduction followed by a supervised learning algorithm . based on theoretical analysis </S>",
    "<S> , we suggest a structural risk minimization algorithm consisting of the coupled fitting of a low dimensional manifold and a separation function on that manifold . </S>"
  ]
}