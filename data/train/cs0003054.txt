{
  "article_text": [
    "for solving new , more difficult search problems , scientists need better search heuristics and/or more powerful resources .",
    "the need for hundreds or even thousands of processors is justified in the case of branch - and - bound search algorithms by problems that could not be solved after months of execution on tens of processors @xcite .    rarely , however , are thousands of processors assembled in a single location and available for a single problem .",
    "thus , techniques are needed that would allow us to aggregate processors at many different internet - connected locations .",
    "these processors are likely often to be required for other purposes ; hence their availability will be episodic , and any algorithm designed to take advantage of these resources must be opportunistic . furthermore , the internet environment is likely to be unreliable and heterogeneous .",
    "various groups have demonstrated the feasibility of using internet - connected computers for solving embarrassingly parallel problems @xcite . in our work",
    ", we investigate the feasibility of applying internet - connected resources to more tightly coupled problems , in which a centralized scheme is not computationally efficient .",
    "our approach is to develop specialized algorithms that incorporate scalability and reliability mechanisms .    for providing reliable services over unreliable architectures ,",
    "researchers usually choose one of the following approaches : ( 1 ) embed fault - tolerance mechanisms within the middleware software layer , as in isis @xcite or corba transaction service , or as in systems like condor @xcite or legion @xcite ; or ( 2 ) embed fault - tolerance mechanisms within algorithms .",
    "the former approach is more general .",
    "successful results in this domain guarantee communication and hardware reliability to a large number of applications .",
    "but its generality imposes problems that sometimes turn out to be unsolvable @xcite or very expensive .",
    "the latter alternative is applicable to specific problem classes and is therefore less general . but",
    "exploiting the characteristics of a class of problems may ease the design of fault - tolerance mechanisms , yielding simpler and more efficient algorithms .",
    "note that middleware can still be of assistance in this case , by providing appropriate fault - detection services @xcite .    in our work ,",
    "we focus on problem - specific fault - tolerance mechanism .",
    "specifically , we propose a fault - tolerant , totally distributed branch - and - bound algorithm designed for unreliable architectures , with a dynamically variable number of resources .",
    "the description of the branch - and - bound problem ( section [ sec : bb ] ) and the target architecture ( section [ sec : architecture ] ) provide the motivation for our work .",
    "we describe our branch - and - bound algorithm in section [ sec : algorithm ] , focusing particularly on the fault - tolerance mechanism .",
    "related work ( section [ sec : related ] ) refers to other fault - tolerance techniques embedded in tree - based , distributed , asynchronous algorithms . for testing our solution ,",
    "we have developed a simulation framework , which is presented in section [ sec : simulation ] , along with the results obtained .",
    "we conclude with a discussion of what we learned from trying to solve this problem and how we intend to continue this work .",
    "the search for optimal solutions is one of the most important searching problems . since",
    "exhaustive search is often impracticable in np - hard problems , heuristics are employed to improve search performance .",
    "branch - and - bound ( which we will hereafter refer to as b&b ) is an intelligent search method often used for optimization problems .",
    "it uses a successive decomposition of the original problem into smaller disjoint subproblems , while reducing ( _ pruning _ ) the search space by recognizing unpromising problems before starting to solve them .",
    "a sequential b&b algorithm consists of a sequence of iterations in which four basic operators are applied over a list of problems , called a _ pool of active problems _ :    * * decompose*. splits a problem into a set of new subproblems . a problem that can not be split ( either because it has no solution or because a solution is found ) is _ fathomed_. a problem decomposed into new subproblems is _",
    "branched_. * * bound*. computes a bound value @xmath0 on the optimal solution of subproblem @xmath1 .",
    "this bound value will be used by _ select _ and _ eliminate _ operations . *",
    "* select*. selects which problem to branch from next , as a function of some heuristic priority function .",
    "selection may depend on bound values , such as in the _ best - first _ selection rule , or not , as in the case of _ depth - first _ or _ breadth - first _ rules .",
    "* * eliminate*. eliminates problems that can not lead to an optimal solution of the original problem ( i.e. , problems for which @xmath2 , where @xmath3 is the _ best known solution _ ) .",
    "successive decomposition operations create a tree of problems rooted in the original problem .",
    "the value of the best solution found thus far is used to recognize the unpromising problems and _ prune _ the tree .",
    "if the bound value of the current problem is not better than the best - known solution , then the problem is eliminated . otherwise , it is stored into the pool of active problems .",
    "the best - known solution is updated when a better feasible solution is found .",
    "the leaves of the tree are infeasible problems , or pruned problems , or problems that lead to locally optimal solutions .",
    "the size and shape of the tree strongly depends on the quality of the heuristic function for the selection rule .    in b&b algorithms",
    ", parallelism can be achieved in different ways @xcite .",
    "we consider the most general approach , in which the b&b tree is built in parallel by performing operations on different subproblems simultaneously .",
    "three design choices most influence the performance of parallel b&b algorithms : the choice of a synchronous or an asynchronous algorithm , the _ work sharing _",
    "mechanism , and the _ information sharing _",
    "synchronous vs. asynchronous design defines what processes do upon completion of a work unit  they wait for each other ( in the case of synchronous algorithms ) or not ( in asynchronous algorithms ) .",
    "work sharing is the method used to assign work to processes in order to fully and efficiently exploit available parallelism .",
    "information sharing refers to the methods used to publish and update the best - known solution .",
    "using an up - to - date best - known solution improves the efficiency of the selection and elimination rule and hence has an important effect on the size of the search space .",
    "many investigations of parallel b&b for distributed - memory systems have adopted a centralized approach in which a single manager maintains the tree and hands out tasks to workers @xcite . while clearly not scalable , this approach simplifies the management of information and multiple processes .",
    "scalability can be improved through a hierarchical organization of processes or by varying the size of work units , but the central manager remains an obstacle to both scalability and fault tolerance .",
    "reliability can be achieved through checkpointing , but this approach assumes that there exists at least one reliable process / machine , able to manage the failure recovery process .",
    "because of the highly variable number of resources in the architecture we consider , we need more flexibility than that offered by the centralized design . hence we chose a fully decentralized design .",
    "the only fully decentralized , fault - tolerant b&b algorithm for distributed - memory architectures is dib ( distributed implementation of backtracking ) @xcite .",
    "dib was designed for a wide range of tree - based applications , such as recursive backtrack , branch - and - bound , and alpha - beta pruning .",
    "it is a distributed , asynchronous algorithm that uses a dynamic load - balancing technique .",
    "its failure recovery mechanism is based on keeping track of which machine is responsible for each unsolved problem .",
    "each machine memorizes the problems for which it is responsible , as well as the machines to which it sent problems or from which it received problems .",
    "the completion of a problem is reported to the machine the problem came from .",
    "hence , each machine can determine whether the work for which it is responsible is still unsolved , and can redo that work in the case of failure .",
    "the target architecture for our algorithm is a collection of internet - connected computers",
    ". the distinctive characteristics of this environment , when compared with a conventional parallel computer , are as follows :    * * scale*. the number of resources available can potentially be much larger than on a conventional parallel computer . *",
    "* dynamic availability*. the quantity of resources available may vary over time , as may the amount of computation delivered by a single resource . * * unreliability*. resources may become unreachable without notice because of system or network failures .",
    "* * communication characteristics*. latencies may be high , variable , and unpredictable ; bandwidth may be low , variable , and unpredictable .",
    "connectivity ( as measured , for example , by bisection bandwidth ) may be particularly low . * * heterogeneity*. resources may have varying physical characteristics ( for example , amount of memory , speed ) . * * lack of centralized control*. there is no central authority for quality control or operational management .",
    "the failure model we consider is crash @xcite , in which a processor fails by halting .",
    "once it halts , the processor remains in that state .",
    "the fact that a processor has failed may not be detectable by other processors .",
    "we make minimum assumptions about the system :     there is no bound on message delivery time .",
    " messages may be lost altogether .",
    " a network link does not duplicate , corrupt , or spontaneously create messages .",
    " the clock rate on each host is close to accurate ( we do not assume that the clocks are synchronized ) .",
    "this condition is assumed in many works in the fault - tolerance domain and it does not represent a _ practical _",
    "restriction @xcite .",
    "we propose a fully decentralized , asynchronous , fault - tolerant parallel b&b algorithm suited for the environment described above .",
    "asynchrony is required by the heterogeneity of the architecture and allowed by the b&b problem @xcite .",
    "each process maintains its local pool of problems to be solved .",
    "when the local pool is empty , the process sends work requests to other processes . a process that receives a work request and has enough problems in its pool removes some of those problems and sends them to the requester .",
    "this on - demand dynamic load - balancing scheme was chosen to reduce unnecessary communication .",
    "the fully decentralized scheme was preferred for better scalability and for greater reliability .",
    "the information sharing issue is solved by circulating the best - known solution among processes , embedded in the most frequently sent messages .",
    "processes update the local value for the best - known solution every time they receive it , and use it when the next decision is to be made .    for adapting this rather conventional b&b algorithm to the environment described above , we extend it with ( 1 ) a group membership protocol to allow dynamic variation in the number of resources and ( 2 ) a fault - tolerance mechanism .",
    "the novelty of this paper is the decentralized fault - tolerance mechanism that uses a tree - based encoding of the b&b subproblems .",
    "this strategy for problem encoding also offers a simple mechanism for termination detection , described in section [ sec : td ] . a brief description of the epidemic communication mechanism ( section [ sec : ec ] ) will help in understanding how the group membership protocol ( section [ sec : gmp ] ) and fault - tolerance mechanism ( section [ sec : ftm ] ) function .",
    "a comparison with dib , the decentralized b&b algorithm mentioned in section [ sec : related ] , concludes this section .",
    "epidemic communication @xcite allows temporary inconsistencies in shared data in exchange for low - overhead implementation .",
    "more specifically , information changes are spread gradually throughout the processes , without the overhead and communication costs typically used to achieve a high degree of consistency .    both our group membership and fault - tolerance mechanisms use epidemic communication .",
    "since these mechanisms do not require data consistency , epidemic communication is a convenient algorithm for spreading information .",
    "however , epidemic communication guarantees that eventually consistency is achieved ; that is , all processes will eventually see the same data when no more new information is brought into the system , independent of system failures @xcite . this observation is exploited for termination detection .",
    "the epidemic algorithms used are variants of the _ rumor - mongering _ algorithm ( analyzed in @xcite ) : when a site receives a new update ( _ rumor _ ) , it becomes `` infectious '' and is willing to share  it repeatedly chooses another member , to which it sends the rumor . upon receipt of a rumor",
    ", a member updates its local information and sends its own version after some time interval . in the membership protocol ,",
    "the rumor received is sent farther , without being processed . in the fault - tolerance mechanism , the rumor is stored for local processing , may be processed locally , and is spread infrequently .",
    "the group membership protocol is used for collecting and updating information about which resources participate in the computation at any given time .",
    "the impossibility of guaranteeing consistent views of group membership in asynchronous , unreliable systems was proven in @xcite .",
    "even in reliable systems , membership protocols are expensive , requiring several phases for consistency .",
    "a group is defined as a set of members .",
    "it is initialized when the first member enters the group and ceases to exist when the last member leaves .",
    "a process joins a group by finding one or more members of the group and leaves it either by leaving or by failing .",
    "we assume the existence of a fault - tolerant method by which processes can find other processes , such as broadcasting ( when applicable ) , known addresses of gossip servers ( described below ) , or a location service . for the moment , we assume that gossip servers exist .",
    "our membership protocol is inspired by the failure - detection mechanism based on epidemic communication presented in @xcite .",
    "other membership protocols based on epidemic communication @xcite are more elaborate and introduce constraints or costs that are not justified in our case .",
    "the membership protocol works as follows : when a new computer joins the group of resources , it sends its address to some known gossip servers .",
    "the gossip servers act as any other member of the group , except that at least one of them is guaranteed to be active at any given moment during the computation .",
    "this is a loose fault - tolerance constraint , easily achievable , without extra costs , by increasing the number of gossip servers in the system .",
    "the main task of these servers is to propagate information about the newly arrived members .",
    "each member process maintains a view of group membership .",
    "the view defines a set of processes that the member believes are part of the group at any given time .",
    "in addition , it contains specific information designed to log the members activity by keeping track of when it last heard of each ( known ) member , directly from it or through the gossip system .",
    "the parameters involved in this mechanism ( for example , the frequency of gossiping and the timeout period used to deduce failure of a passive member ) are chosen to keep communication and the probability of false membership information under some threshold values @xcite .    among the advantages of using this membership protocol",
    "are ( 1 ) scalability in network load with the size of the group , ( 2 ) tolerance to a small percentage of message loss or failed members , and ( 3 ) scalability in accuracy with the number of members .      for b&b algorithms ,",
    "the loss of a subproblem is unacceptable when the accuracy of the solution is important .",
    "our proposed fault - tolerance mechanism does not attempt to detect failures of computers and restore their data , but rather focuses on detecting missing results .",
    "given that the b&b tree of problems is dynamic , how is it possible to know the set of existing problems , so that , knowing the problems completed , one can infer the set of not - completed problems ?",
    "our solution exploits the fact that the subproblems dynamically generated by the b&b algorithm are nodes of a tree .",
    "each node can be uniquely represented by its position in the tree .",
    "if we encode the position of the nodes in the tree , we obtain a unique code for each subproblem .",
    "furthermore , given a set of nodes of the tree , we can easily find its complement , that is , the list of nodes of the tree that are not in the given set .      without loss of generality ,",
    "we assume that the branching factor for the search tree is 2 and that each branch is a decision on a condition variable .",
    "therefore , a subproblem is entirely described by a sequence of pairs @xmath4 where @xmath5 is a condition variable and @xmath6 is @xmath7 or @xmath8 , indicating the left or the right branch , respectively .",
    "we need to include condition variables in the subproblem encoding because the order in which condition variables are considered may vary over the tree .",
    "for example , the left subtree of a node that branches upon @xmath9 may consider @xmath5 first and therefore will generate the subproblems @xmath10 and @xmath11 , whereas the right subtree may branch upon @xmath12 first , producing the subproblems @xmath13 and @xmath14 .",
    "each pair @xmath4 introduces and assigns a condition to a new variable .",
    "that is what makes the codes ( subproblems ) self - contained : the code ( along with the initial data , which is provided by a gossip server when a process joins the computation ) is enough to initiate a problem on any processor .",
    "our failure - recovery mechanism allows each process to detect missing problems independently , based on local information about completed problems .",
    "we consider a subproblem _ solved _ after the branching operation has been performed on it .",
    "_ solved _ subproblems are not necessarily _ completed _ : we consider a subproblem to be _ completed _ if it is solved and either it is a leaf or both its children are completed ( see figure [ fig : completed ] ) .",
    "every process maintains a list of new locally completed subproblems and a table of the completed problems it knows about .",
    "when a problem is completed , it is included in the local list .",
    "when @xmath15 problems ( codes ) are in the list or the list has not been updated for a long time , it is sent to @xmath16 of the other members as a _",
    "work report _ message .",
    "when a member receives a work report , it stores the report in its table .",
    "occasionally , in order to inform new members of the current state of the execution and to increase the degree of consistency , a member sends its table of completed problems to a randomly chosen member .",
    "the size and the number of the problem codes vary with the shape and number of nodes of the b&b tree . the deeper the node in the tree , the larger the size of its code ; the more nodes in the tree , the larger the number of codes .",
    "since the completion of a parent node implies the completion of its children , communication costs can be reduced by compressing work report messages , via the recursive replacement of pairs of sibling codes with the code of their parent , and the deletion of codes whose ancestors are also in the list .",
    "simulations performed on real b&b trees confirmed that the compression rate is better when processors are sufficiently loaded : the taller the subtree completed locally , the larger the number of codes that do not need to be sent .",
    "failure recovery is achieved as follows .",
    "when a member runs out of work and an attempt to get work through the load - balancing mechanism fails , it chooses an uncompleted problem ( by complementing the code of a solved problem whose sibling is not solved ) and solves it .",
    "the mechanism `` repairs '' system failures due to , for example , a computer that failed before sending work reports or work reports that were lost before reaching any machine . note that this mechanism also works in the case of temporary network partitions .",
    "this simple , fully distributed mechanism can lead to redundant work in two situations : ( a ) the lag in updating information can lead to faulty presumptions on failure ; and ( b ) the lack of coordination among processors permits multiple members to work on the same problem .",
    "the former case can be fixed easily by interrupting the redundant work when information is updated .",
    "the costs of the latter situation can be reduced by employing more sophisticated methods for choosing work , such as using the location of the last problem completed locally .",
    "notice , however , that redundant computation may be inevitable .",
    "if information about completed problems is spread uniformly , then the loss of a percentage of members may not lead to information loss : if the information about the problems reported to be completed still exists in the system , they will not have to be redone .",
    "the problem encoding used for implementing the fault - tolerance mechanism also has the advantage of implicitly solving the termination detection problem .",
    "when successive code compressions of local lists and tables lead to the code of the root problem , termination is detected .",
    "since none of the communication mechanisms used guarantees data consistency , it is possible that some members do not have enough information to detect termination .",
    "that is why , before termination , each member that detected the termination will have to send one more work report , that is , the code of the root problem , to all members from its local membership list .",
    "both dib and the algorithm we propose are decentralized and fault - tolerant algorithms that work on a dynamic , tree - like search space .",
    "both algorithms implement low - cost , simple fault - tolerance protocols for the price of potentially redundant work .",
    "however , the two algorithms have different failure - recovery mechanisms and react differently in the case of failure .",
    "dib uses a hierarchical structure for failure detection and recovery that imposes the need for a reliable or duplicated node for the root of this hierarchy .",
    "moreover , the failure of a node affects not only the problems solved locally and not reported as solved yet , but also the problems given to other nodes , whose completion can not be reported ( and therefore considered ) anymore .    in our algorithm ,",
    "all processes are equally responsible for the behavior of the system in case of failure .",
    "our simulation studies confirm that the failure of all processes but one still allows the problem to be correctly solved .",
    "the mechanism is also reliable in the case of faulty network links or temporary network partitions .",
    "however , the homogeneity involved in our algorithm has a communication cost : information about the completion of a problem is eventually spread to all processes , directly ( by reporting the code of the problem ) or indirectly ( by reporting the completion of one of its ancestors ) .",
    "performance comparisons of dib and our algorithm are of limited interest for two reasons . because dib was designed for a wide range of applications , such as recursive backtrack , alpha - beta search and branch - and - bound , its speedup is `` excellent for exhaustive traversal and quite good for branch - and - bound '' @xcite . furthermore , speedup results are given for maximum 16 processors , while we are interested in many more resources .",
    "we use simulations rather than a real implementation to evaluate our algorithm , as the use of simulation techniques provides great flexibility in testing a wide range of b&b strategies in a variety of internet - like environments .",
    "the goals of our experimental work are as follows : ( 1 ) to verify reliability and evaluate the overall performance of the algorithm , focusing on the costs introduced by the fault - tolerance mechanism ; and ( 2 ) to evaluate scalability for different problem classes and environments .",
    "our work to date has focused primarily on the first of these two issues .",
    "we studied algorithm reliability by testing various failure scenarios .",
    "the costs introduced by our fault - tolerance mechanism are communication costs , storage space , tree contraction time , and redundant work .",
    "because we avoid centralized control by spreading information throughout the system , communication costs may be significant .",
    "redundant work may increase when communication conditions are poor ( messages are delayed or lost ) or when work load is low .",
    "storage space may become a serious concern for large problems because the algorithm permits ( and benefits from ) the replication of data .",
    "however , the results we obtained encourage us to continue our research in this direction .",
    "we used parsec @xcite to develop our simulation system .",
    "parsec is a c - based simulation language for sequential and parallel execution of discrete - event simulation models .",
    "processes are modeled by objects ; interactions among objects are modeled by time stamped message exchanges .",
    "our simulation system incorporates a detailed representation of load balancing , failure recovery , and termination detection mechanisms .",
    "we do not include yet the membership protocol : hence , the pool of resources is predetermined and varies only with failures .",
    "each process , after it has solved a b&b subproblem , checks to see whether any messages are pending .",
    "if it received a work request , it satisfies the request if there are enough problems in its active pool .",
    "if it received a work report , it merges that report with its local information on completed problems and contracts the result .",
    "the simulation was configured so that it could be driven either by _ real _ ( precomputed ) b&b trees or by random trees . for real problems",
    ", we tested our algorithm on a set of _ basic trees _ that we obtained from an instrumented b&b code .",
    "basic trees are trees generated by executing a branch - and - bound algorithm without eliminating the unpromising nodes .    for each node in the tree , we have the following information : ( 1 ) the node identifier , ( 2 ) its bound value , ( 3 ) the time needed for computing the bound value and expanding the node or determining infeasibility , and ( 4 ) a value specifying whether the bound value is a feasible solution .",
    "the bound values are used for pruning the test tree and obtaining the b&b tree , and for computing the optimal solution .",
    "the time value is used for simulating the execution time needed for the bounding operation .",
    "notice that the time values determine the granularity of the subproblems . during our experiments",
    ", we tuned this granularity by multiplying all time values by a constant factor , and we studied how granularity affects the overall performance of the b&b algorithm .",
    "running simulations on basic trees leaves enough room for generating different b&b trees , depending on communication characteristics ( for example , up - to - date information about the best - known solution influences pruning decision ) and on the number of processors ( because the number of nodes expanded may vary with the number of processors ) .",
    "note that the basic branch - and - bound operation _",
    "decompose _ is recorded within the basic tree structure .    because the amount of communication and storage space depends on the shape and the size of the tree",
    ", testing trees resulted from solving real problems provides better accuracy .",
    "however , creation of basic trees is computationally infeasible for anything but small problems . but for testing reliability , and later scalability",
    ", the number of nodes is the only important feature of the test tree .",
    "therefore , we enriched our set of test trees with randomly created trees of various sizes and tested them without eliminating the unpromising nodes .      our simulator measured execution time , communication costs , and storage space .",
    "we tested the algorithm on relatively small problems ( up to tens of thousands of nodes expanded ) , with no optimization efforts : work reports are sent to randomly chosen resources , without eliminating redundant messages .",
    "when out of work , resources ask randomly chosen resources for work , without using previous experience to increase performance .       ms . for messages of size @xmath17 bytes.,width=480 ]",
    "figure [ fig : hist_p0201 ] shows results obtained for a small problem ( approximately 3500 nodes expanded ) with average granularity of 0.01 seconds per node . for this problem ,",
    "the overhead introduced by the algorithm reaches 36% for 8 processors .",
    "this is determined by three factors : ( 1 ) the relatively high communication costs considered ( @xmath18 milliseconds for messages of size @xmath17 bytes ) ; ( 2 ) the cost of the dynamic load balancing mechanism for a network of workstations @xcite ; and ( 3 ) the small granularity of the subproblems .",
    "we will see that for a larger problem ( table [ tlb : trim4 ] ) the overhead is much lower ( 15.58% of the total execution for 100 processors , from which 13.67% are load balancing costs , 0.78% communication time and 1.13% list contraction time ) .",
    "furthermore , this overhead can be controlled by tuning various execution parameters .",
    "for example , less frequent termination verification leads to lower list contraction costs but may increase idle time . sending work reports more rarely may decrease communication time and list contraction costs but may increase termination detection time , because of lack of information .",
    "if the failure recovery mechanism is activated ( decides that a problem was lost and recreates it ) less often , the overhead introduced ( list contraction and redundant work costs ) is lower , but recovery in case of failure is also slower .",
    "the tests we performed on larger problems ( total uniprocessor execution time of around 75 hours ) show that communication and storage space costs remain negligible ( table [ tlb : trim4 ] ) .",
    "we find that good performance is achieved on up to 100 processors .",
    "these preliminary results encourage us to continue evaluating our algorithm on larger problems , with larger number of resources .",
    ".simulated execution of a real problem ( approximately 79,600 nodes expanded ) . in this problem ,",
    "average cost per node is 3.47 sec .",
    "communication costs are modeled as @xmath19 ms . for messages of size @xmath17 bytes . [ cols= \"",
    "> , > , > , > , > , > , > \" , ]     .,title=\"fig:\",width=307 ] .,title=\"fig:\",width=307 ]    communication per processor increases with the number of processors because the number of work reports sent per processor increases : since the work load is lower , and therefore processes are idle longer periods of time , they suspect termination and send more work reports .",
    "storage space is measured for the entire system .",
    "the results obtained43 mb storage space for 100 machines  are promising .",
    "a normal trend would be that the amount of time spent on list contraction increases with the number of processors , since the number of messages circulated within the system increases and the receiving of a work report message requires a list contraction procedure .",
    "but because this depends on how the subproblems are assigned on processors , a lucky configuration may lead to unexpected good results ( as for 100 processors , table [ tlb : trim4 ] ) .    the amount of redundant work performed is another interesting measure of our algorithm that remains to be evaluated",
    ". however , this amount can be reduced by tuning parameters ( for example , how soon failure is suspected after a machine unsuccessfully tries to get work ) or by designing more sophisticated methods for picking up unsolved problems .    when varying problem granularity ( by multiplying the time needed to solve a problem with some constant values ) , we observed the following ( not unexpected ) behavior : the number of nodes expanded may vary , because the information of the best - known solution is computed at different moments .",
    "load balance is better when granularity is coarser .",
    "communication increases unnecessarily because work reports are sent at fixed time intervals .",
    "this last observation taught us that for scalability , we need to design an adaptive mechanism for deciding how often work reports should be sent , based on information collected at runtime : for example , information about execution time per subproblem and frequency of messages received .      because our termination detection mechanism operates by detecting",
    "that all expanded problems have been completed , it is straightforward to verify that our fault - tolerance algorithm is working correctly  we simply verify that termination is detected . for visualizing the behavior of the algorithm , we used jumpshot , a graphical visualization tool for ` clog ` log file format .",
    "we used the mpe library developed by the mpich team at argonne national laboratory for logging the execution profile .",
    "figures [ fig : misc02_nf ] and [ fig : misc02_wf ] are snapshots of the execution of the algorithm on a very small problem .",
    "figure [ fig : misc02_nf ] shows the behavior of the algorithm in the absence of failures .",
    "the same problem is presented in figure [ fig : misc02_wf ] , where two of the three processors fail at about 85% of the execution time .",
    "the only processor available after this moment is able to solve the problem and terminate .",
    "we presented a failure - recovery mechanism suited for a tree - like problem space .",
    "this mechanism and a low - cost group membership protocol are the ingredients that transform a rather conventional parallel branch - and - bound algorithm into a scalable , reliable , more powerful algorithm , able to exploit the computational power of hundreds of internet - connected resources .",
    "scalability is achieved through a fully distributed design .",
    "the algorithm is fault tolerant under our assumptions and can execute and terminate correctly even if only a single resource remains available .",
    "we solved the difficult problems of fault tolerance and termination detection in distributed environments by exploiting problem - specific features , specifically the tree structure of the problem space .",
    "while the mechanism we propose is not applicable to all distributed computations , we believe that a large class of problems can benefit from it .",
    "we have used simulation studies to explore the behavior of our algorithm .",
    "initial results on relatively small problems and up to 100 processors are promising : performance is good despite the lack of optimization .",
    "communication costs are reasonable , storage space costs are negligible . however , we need results on a much larger number of processors .",
    "we plan to introduce the group membership protocol into our simulations and to test the algorithm under various network conditions .",
    "an interesting issue to study is how the network characteristics influence the performance of the algorithm in general and the costs introduced by the failure - recovery mechanism in particular .",
    "also , in order to accurately analyze scalability issues , we plan to design a flexible scheme for adapting parameters to runtime informations , such as total execution time and execution time per problem .",
    "n. alon , a. barak , u. manber .",
    "1987 . on disseminating information reliably without broadcasting .",
    "_ proceedings of the 7th international conference on distributed computing systems _ ieee computer society press , 7481 .",
    "t. chandra , v. hadzilacos , s. toueg , b. charron - bost .",
    "1996 . on the impossibility of group membership .",
    "_ podc 96 .",
    "proceedings of the 15th annual acm symposium on principles of distributed computing _ , 322330 .",
    "a. demers , d. greene , c. hauser , w. irish , j. larson , s. shenker , h. sturgis , s. swinehart , d. terry . 1988 .",
    "epidemic algorithms for replicated database maintenance .",
    "_ operating systems review _ , 22(1):831 .",
    "p. hahn , w. hightower , t. johnson , m. guignard - spielberg , c. roucairol .",
    "tree elaboration strategies in branch - and - bound algorithms for assigning the quadratic assignment problem , _",
    "6th siam conference on optimization_. a. lenstra .",
    "factoring integers using the web and the number field sieve . tech .",
    "report , bellcore .",
    "m. litzkow , t. tannenbaum , j. basney , m. livny .",
    "1997 . checkpoint and migration of unix processes in the condor distributed processing system .",
    "technical report # 1346 , university of wisconsin - madison , computer sciences department .",
    "p. stelling , i. foster , c. kesselman , c. lee , g. von laszewski .",
    "1998 . a fault detection service for wide area distributed computations .",
    "7th ieee symp . on high performance",
    "distributed computing _ , 268278 ."
  ],
  "abstract_text": [
    "<S> the idle computers on a local area , campus area , or even wide area network represent a significant computational resource  one that is , however , also unreliable , heterogeneous , and opportunistic . </S>",
    "<S> this type of resource has been used effectively for embarrassingly parallel problems but not for more tightly coupled problems . </S>",
    "<S> we describe an algorithm that allows branch - and - bound problems to be solved in such environments . in designing this algorithm , we faced two challenges : ( 1 ) scalability , to effectively exploit the variably sized pools of resources available , and ( 2 ) fault tolerance , to ensure the reliability of services . </S>",
    "<S> we achieve scalability through a fully decentralized algorithm , by using a membership protocol for managing dynamically available resources . </S>",
    "<S> however , this fully decentralized design makes achieving reliability even more challenging . </S>",
    "<S> we guarantee fault tolerance in the sense that the loss of up to all but one resource will not affect the quality of the solution . for propagating information efficiently , </S>",
    "<S> we use epidemic communication for both the membership protocol and the fault - tolerance mechanism . </S>",
    "<S> we have developed a simulation framework that allows us to evaluate design alternatives . </S>",
    "<S> results obtained in this framework suggest that our techniques can execute scalably and reliably . </S>"
  ]
}