{
  "article_text": [
    "thanks to the advances of information technologies , large - scale data sets with a large number of variables or dimensions are commonly collected in many contemporary applications that arise in different fields of sciences , engineering and humanities .",
    "examples include marketing data in business , panel data in economics and finance , genomics data in heath sciences , and brain imaging data in neuroscience , among many others .",
    "the emergence of a large amount of information contained in high - dimensional data sets provides opportunities , as well as unprecedented challenges , for developing new statistical methods and theory .",
    "see , for example , @xcite and @xcite for insights and discussions on the statistical challenges associated with high dimensionality , and @xcite for a brief review of some recent developments in high - dimensional sparse modeling with variable selection .",
    "the approach of variable selection aims to effectively identify important variables and efficiently estimate their effects on a response variable of interest .    for the purpose of prediction and variable selection , it is important to understand and characterize the impacts of high dimensionality in finite samples .",
    "@xcite investigated this problem under the asymptotic framework of fixed sample size @xmath0 and diverging dimensionality @xmath1 , and revealed an interesting geometric representation of high dimension , low sample size data . when viewed in the diverging @xmath1-dimensional euclidean space ,",
    "the randomness in the data vectors can be asymptotically squeezed into random rotation , with the shape of the rescaled @xmath0-polyhedron approaching deterministic , modulo the orientation .",
    "such concentration phenomenon of random design matrix in high dimensions is also shared by the concentration property in @xcite ( see definition [ def1 ] ) , in the asymptotic setting of both @xmath0 and @xmath1 diverging .",
    "geometrically , this property means that the configuration of the @xmath0 sub - data vectors , modulo the orientation , becomes close to regular asymptotically .",
    "such a property is key to establishing the sure screening property , which means that all important variables are retained in the reduced feature space with asymptotic probability one , of the sure independence screening ( sis ) method introduced in @xcite .",
    "the sis uses the idea of independence learning by applying componentwise regression .",
    "techniques of independence learning have been widely used for variable ranking and screening .",
    "recent work on variable screening includes @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and @xcite , among others .",
    "the utility of these methods is characterized by the sure screening property . in particular",
    ", @xcite proved that the concentration property holds when the design matrix is generated from gaussian distribution , and conjectured that it may well hold for a wide class of elliptical distributions .",
    "@xcite presented some simulation studies investigating such a property for non - gaussian distributions .",
    "the first major contribution of our paper is to provide an affirmative answer to the conjecture posed in @xcite .    to ensure model identifiability and stability for reliable prediction and variable selection ,",
    "it is practically important to control the collinearity for sparse models .",
    "since it is well known that the level of collinearity among covariates typically increases with the model dimensionality , bounding the sparse model size can be effective in controlling model collinearity .",
    "such a bound is characterized by the concept of robust spark ( see definition [ def2 ] ) .",
    "another contribution of the paper is to establish a lower bound on the robust spark in the setting of large random design matrix generated from the family of elliptical distributions .",
    "in addition to the above probabilistic view of finite samples in high dimensions , we also present a nonprobabilistic high - dimensional geometric view . both views",
    "are concerned with how much information finite sample contains .",
    "a fundamental question is what the impact of high dimensionality on differentiating the subspaces spanned by different sets of predictors is .",
    "such a question is tied to the issue of model identifiability . in this paper",
    ", we intend to derive general bounds on dimensionality with some distance constraint on sparse models .",
    "the rest of the paper is organized as follows .",
    "section  [ sec2 ] establishes the concentration property and robust spark bound for large random design matrix generated from elliptical distributions .",
    "we investigate general bounds on dimensionality with distance constraint from a nonprobabilistic point of view in section  [ sec3 ] .",
    "section  [ sec4 ] presents two numerical examples to illustrate the theoretical results .",
    "we provide some discussions of our results and their implications in section  [ sec5 ] .",
    "all technical details are relegated to the .",
    "in this section , we focus on the case of large random design matrix observed in a high - dimensional problem , in which each column vector contains the information of a particular covariate . in high - dimensional sparse modeling ,",
    "a  common practice is to assume that only a faction of all covariates , the so - called true or important covariates , contribute to the regression or classification problem , whereas the other covariates , the so - called noise covariates , are simply noise information .",
    "the inclusion of noise covariates can deteriorate the performance of the estimated model due to the well - known phenomenon of noise accumulation in high - dimensional prediction [ @xcite ] . a crucial issue behind high - dimensional inference is to characterize the distance between the true underlying sparse model and other sparse models , under some discrepancy measure .",
    "intuitively , such a distance can become smaller as the dimensionality increases , making it more difficult to distinguish the true model from the others .",
    "therefore , it is a fundamental problem to characterize the impacts of high dimensionality in finite samples .",
    "we start with the task of dimensionality reduction , particularly variable screening , which is useful in analyzing ultra - high dimensional data sets . with the idea of independence learning",
    ", @xcite introduced the sis method to reduce the dimensionality of the feature space from the ultra - high scale to a moderate scale , such as below sample size .",
    "they introduced an asymptotic framework under which the sis enjoys the sure screening property even when the dimensionality can grow exponentially with the sample size ; see their theorem 1 .",
    "the sure screening property means that the true model is contained in the much reduced model after variable screening with asymptotic probability one .",
    "in particular , a key ingredient of their asymptotic analysis is the so - called concentration property in condition 2 of @xcite .",
    "they verified such property for random design matrix generated from gaussian distribution , and conjectured that it may also hold for a wide class of elliptical distributions . to show that sis is widely applicable , it is crucial to establish the concentration property for classes of non - gaussian distributions .",
    "the class of elliptical distributions , which is a wide family of distributions generalizing the multivariate normal distribution , has been broadly used in real applications .",
    "examples of nonnormal elliptical distributions include the laplace distribution , @xmath2-distribution , cauchy distribution , logistic distribution , and symmetric stable distribution .",
    "in particular , an important subclass of elliptical distributions is the family of mixtures of normal distributions .",
    "mixture distributions provide a useful tool for describing heterogeneous populations .",
    "elliptical distributions also play an important role in the theory of portfolio choice [ @xcite ] .",
    "this is due to important properties that any affine transformation of elliptically distributed random variables still has an elliptical distribution and each elliptical distribution is uniquely determined by its location and scale parameters .",
    "an implication in portfolio theory is that if all asset returns jointly follow an elliptical distribution , then all portfolios are characterized fully by their location and scale parameters .",
    "we refer to @xcite for a comprehensive account of elliptical distributions .",
    "assume that @xmath3 is a @xmath1-dimensional random covariate vector having an elliptical distribution @xmath4 with mean @xmath5 and nonsingular covariance matrix  @xmath6 , and that we have a sample @xmath7 of i.i.d .",
    "covariate vectors from this distribution .",
    "then we have an @xmath8 random design matrix @xmath9 . by the definition of elliptical distribution [ see @xcite or @xcite ] , the transformed @xmath1-dimensional random vector @xmath10 has a spherical distribution @xmath11 with mean @xmath5 and covariance matrix @xmath12 .",
    "similarly , we define the transformed covariate vectors and transformed random design matrix as @xmath13 where @xmath14",
    ". clearly , @xmath15 are @xmath0 i.i.d .",
    "copies of the transformed random covariate vector @xmath16 .",
    "we denote by @xmath17 and @xmath18 the largest and smallest eigenvalue of a given matrix , respectively . in high - dimensional problems ,",
    "we often face the situation of @xmath19 , so it is desirable to reduce the dimensionality of the feature space from @xmath1 to a moderate one such as below sample size @xmath0 .",
    "the sis is capable of doing so when the random design matrix @xmath20 satisfies the following property , as introduced in @xcite .",
    "[ def1 ] the random design matrix @xmath20 is said to satisfy the concentration property if there exist some positive constants @xmath21 such that the deviation probability bound @xmath22 holds for each @xmath23 submatrix @xmath24 of @xmath25 with @xmath26 and @xmath27 some positive constant .",
    "as mentioned in the , the above concentration property shows a similar concentration phenomenon of large random design matrix to that in @xcite .",
    "when the distribution @xmath28 of the covariate vector @xmath29 is @xmath1-variate gaussian , @xcite proved that the random design matrix @xmath20 satisfies the concentration property .",
    "we now consider a more general class of distributions including gaussian distributions , the family of elliptical distributions .",
    "assume that @xmath30 .",
    "then it follows from theorem 1.5.6 in @xcite that the @xmath1-variate spherical distribution @xmath11 has a density function with respect to the lebesgue measure that is spherically symmetric on @xmath31 .",
    "we will work with the family of log - concave spherical distributions on @xmath31 that satisfy the following two conditions .",
    "[ cond1 ] the density function @xmath32 of the @xmath1-variate log - concave spherical distribution @xmath11 satisfies that for some positive constant  @xmath33 , @xmath34 where @xmath35 denotes the hessian matrix and @xmath36 means that @xmath37 is positive semidefinite for any symmetric matrices @xmath38 and @xmath39 .",
    "[ cond2 ] there exists some positive constant @xmath40 such that @xmath41 .",
    "condition [ cond1 ] puts a constraint on the curvature of the log - density of distribution  @xmath11 , and condition [ cond2 ] requires that the mean @xmath42 needs to be bounded from below .",
    "clearly , log - concave spherical distributions satisfying conditions [ cond1][cond2 ] comprise a wide class containing gaussian distributions . as seen in lemma [ lem2 ] later , condition [ cond1 ] entails that the corresponding spherical distribution is light - tailed , which is important for variable screening . for heavy - tailed data sets , @xcite showed that effective variable selection with untransformed data requires slower growth of dimensionality .",
    "in particular , they exploited variable transformation methods to transform the original data into light - tailed data and demonstrated their effectiveness and advantages . so in the presence of heavy - tailed data",
    ", one may work with the assumption of elliptical distributions on the transformed data .",
    "the assumption of elliptical distributions is commonly used in dimension reduction and has also been used for variable screening .",
    "see , for example , @xcite .",
    "this assumption facilitates our technical analysis .",
    "similar results may hold for more general family of distributions by resorting to techniques in random matrix theory .",
    "some other variable screening methods such as in @xcite require no such an assumption .",
    "[ thm1 ] under conditions [ cond1][cond2 ] , the random design matrix @xmath20 satisfies the concentration property ( [ e002 ] ) .",
    "theorem [ thm1 ] shows that the concentration property holds not only for gaussian distributions , but also for a wide class of elliptical distributions , as conjectured by @xcite ( see their section  5.1 ) .",
    "this provides an affirmative answer to their conjecture , showing that the sis indeed enjoys the sure screening property for the random design matrix generated from a wide class of elliptical distributions .",
    "the proof of theorem [ thm1 ] relies on the following three lemmas that are of independent interest .",
    "[ lem1 ] under condition [ cond1 ] , each @xmath43-variate marginal distribution @xmath44 of @xmath11 with @xmath45 satisfies the logarithmic sobolev inequality @xmath46 for any smooth function @xmath47 on @xmath48 with @xmath49 , where @xmath50 and @xmath51 denotes the gradient of function @xmath47 .",
    "[ lem2 ] let @xmath52 be an arbitrary @xmath43-dimensional subvector of @xmath16 with .",
    "then we have :    under condition [ cond1 ] , it holds for any @xmath53 that",
    "@xmath54    it holds that @xmath55    [ lem3 ] assume that conditions [ cond1][cond2 ] hold , @xmath52 is a @xmath43-dimensional subvector of @xmath16 with @xmath56 , and @xmath57 .",
    "then there exist some positive constants @xmath58 , @xmath59 , and @xmath60 such that @xmath61    lemma [ lem1 ] shows that each marginal distribution of @xmath11 satisfies the logarithmic sobolev inequality , which is an important tool for proving the concentration probability inequality for measures .",
    "lemma [ lem2 ] establishes that for each @xmath43-dimensional subvector @xmath52 of @xmath16 , its @xmath62-norm @xmath63 concentrates around the mean @xmath64 with significant probability , which is in turn sandwiched between two quantities @xmath65 and @xmath66 .",
    "lemma [ lem3 ] demonstrates an interesting phenomenon of measure concentration in high dimensions .",
    "as is well known in high - dimensional sparse modeling , controlling the level of collinearity for sparse models is essential for model identifiability and stable estimation . for a given @xmath8 design matrix @xmath20",
    ", there may exist another @xmath1-vector @xmath67 that is different from the true regression coefficient vector @xmath68 such that @xmath69 is ( nearly ) identical to @xmath70 , when the dimensionality @xmath1 is large compared with the sample size @xmath0 .",
    "this indicates that model identifiability is generally not guaranteed in high dimensions when no additional constraint is imposed on the model parameter .",
    "in addition , the subdesign matrix corresponding to a sparse model should be well conditioned to ensure reliable estimation of model parameters and nice convergence rates as in such as the least - squares or maximum likelihood estimation .",
    "as an example , the covariance matrix of the least - squares estimator is proportional to the inverse gram matrix given by the design matrix .    since the collinearity among the covariates increases with the dimensionality as evident from the geometric point of view , a natural and effective way to ensure model identifiability and reduce model instability is to control the size of sparse models .",
    "such an idea has been adopted in @xcite for the problem of sparse recovery , which is the noiseless case of linear regression .",
    "in particular , they introduced the concept of spark as a bound on sparse model size to characterize model identifiability .",
    "the spark @xmath71 of the design matrix @xmath20 is defined as the smallest possible positive integer such that there exists a singular @xmath72 submatrix of @xmath20 .",
    "this concept plays an important role in the problem of sparse recovery ; see also @xcite .",
    "an implication is that the true model parameter vector @xmath68 is uniquely defined as long as @xmath73 , which provides a basic condition for model identifiability . for the problem of variable selection in the presence of noise , a stronger condition than provided by the spark",
    "is generally needed .",
    "for this purpose , the concept of spark was generalized in @xcite by introducing the concept of robust spark , as follows .",
    "[ def2 ] the robust spark @xmath74 of the @xmath8 design matrix @xmath20 is defined as the smallest possible positive integer such that there exists an @xmath75 submatrix of @xmath76 having a singular value less than a given positive constant @xmath77 .",
    "it is easy to see that the robust spark @xmath74 approaches the spark of @xmath20 as @xmath78 .",
    "the robust spark provides a natural bound on model size for effectively controlling the collinearity level of sparse models , which is referred to as the robust spark condition . for each sparse model with size @xmath79 , the corresponding @xmath80 submatrix of @xmath76 have all singular values bounded from below by @xmath77 .",
    "the robust spark @xmath74 is always a positive integer no larger than @xmath81 .",
    "it is practically important in high - dimensional sparse modeling to show that the robust spark can be some large number diverging with the sample size @xmath0 .",
    "we intend to build a lower bound on the robust spark for the case of random design matrix , following the setting in section  [ sec21 ] .",
    "[ thm2 ] assume that the rows of the @xmath8 random design matrix @xmath20 are i.i.d . as @xmath4 having mean @xmath5 and covariance matrix @xmath6 and satisfying conditions [ cond1][cond2 ] , with @xmath82 bounded from below by some positive constant . then with asymptotic probability one , @xmath83 for sufficiently small constant @xmath77 and some positive constant @xmath84 depending only on @xmath77 .",
    "theorem [ thm2 ] formally characterizes the order of the robust spark @xmath74 when the design matrix @xmath20 is generated from the family of elliptical distributions .",
    "we see that sparse linear models of size as large as of order @xmath85 can still be well separated from each other . on the other hand , when the true model size is beyond such an order , the true underlying sparse model may be indistinguishable from others in finite sample .",
    "theorem [ thm2 ] also justifies the range of the true sparse model size under which the problem of variable selection is meaningful .",
    "the deflation factor of @xmath86 represents the general price one has to pay for the search of important covariates in high dimensions .",
    "the concept of robust spark shares a similar spirit as the restricted eigenvalue condition on the design matrix in @xcite , in the sense that both are sparse eigenvalue type conditions . instead of constraining the sparse model size",
    ", the restricted eigenvalue condition uses an @xmath87-norm constraint on the parameter vector . as discussed in @xcite , the robust spark condition can be weaker than the restricted eigenvalue condition , since the @xmath88-norm constraint can define a smaller subset than the @xmath87-norm constraint .",
    "many other conditions have also been introduced to characterize the properties of variable selection methods such as the lasso .",
    "see , for example , @xcite for a comprehensive comparison and discussions on these conditions .    in particular , the robust spark condition is weaker than the partial orthogonality condition , which requires that true covariates and noise covariates are essentially uncorrelated , with absolute correlation of the order @xmath89 .",
    "in contrast , the robust spark condition can allow for much stronger correlation between true covariates and noise covariates .",
    "the robust spark condition can also be weaker than the irrepresentable condition .",
    "to see this , let us consider the simple example constructed in @xcite . in their example 1 , the irrepresentable condition becomes the constraint that the maximum absolute correlation between the response and all noise covariates is bounded from above by @xmath90 , where @xmath91 denotes the true model size .",
    "since the response is a linear combination of true covariates in that example , this indicates that the irrepresentable condition can be stronger than the robust spark condition when the true model size grows .",
    "we have provided in section  [ sec2 ] a probabilistic view of finite samples in high dimensions , with focus on large random design matrix generated from the family of elliptical distributions .",
    "it is also important to understand how the dimensionality plays an role in deterministic finite samples .",
    "for such a purpose , we take a high - dimensional geometric view of finite samples and derive general bounds on dimensionality using nonprobabilistic arguments . with a slight abuse of notation ,",
    "we now denote by @xmath92 an @xmath0-dimensional vector of observations from the @xmath93th covariate , and consider a collection of @xmath1 covariates @xmath94 .",
    "assume that each covariate vector @xmath92 is rescaled to have @xmath62-norm @xmath95 .",
    "then all vectors @xmath96 , @xmath97 , lie on the unit sphere @xmath98 in the @xmath0-dimensional euclidean space @xmath99 .",
    "we are interested in a natural question that how many variables there can be if the maximum collinearity of sparse models is controlled .    for each positive integer @xmath91 , denote by @xmath100 the set of all subspaces spanned by @xmath91 of covariates @xmath92 s .",
    "assume that @xmath91 is less than half of the spark @xmath71 of the @xmath8 design matrix @xmath101 .",
    "then each subspace in @xmath100 is @xmath91-dimensional and @xmath102 . to control the collinearity among the variables , it is desirable to bound the distances between @xmath91-dimensional subspaces in @xmath100 away from zero , under some discrepancy measure .",
    "when each pair of subspaces in @xmath100 has a positive distance , intuitively there can not be too many of them .",
    "the geometry of the space of all @xmath91-dimensional subspaces of @xmath103 is characterized by the grassmann manifold @xmath104 . to facilitate our presentation , we list in appendix [ sec31 ] some necessary background and terminology on the geometry and invariant measure of grassmann manifold . in particular",
    ", @xmath104 admits an invariant measure which under a change of variable and symmetrization can be represented as a probability measure @xmath105 on @xmath106^s$ ] with density given in ( [ 003 ] ) .    with the aid of the measure",
    "@xmath105 , we can calculate the volumes of various shapes of neighborhoods in the grassmann manifold , which are typically given in terms of the principal angles @xmath107 between an @xmath91-dimensional subspace of @xmath103 and a fixed @xmath91-dimensional subspace with generator matrix @xmath108 .",
    "the principal angles between subspaces are natural extensions of the concept of angle between lines .",
    "let @xmath109 be two subspaces in @xmath104 having a set of principal angles @xmath110 , with @xmath111 and corresponding @xmath91 pairs of unit vectors @xmath112 .",
    "if @xmath113 is spanned by @xmath91 of @xmath92 s , then putting @xmath114 and reversing the order give the canonical correlations @xmath115 and corresponding pairs of canonical variables @xmath116 , for the two groups of variables .",
    "there are three frequently used distances between subspaces @xmath117 and @xmath118 on the grassmann manifold @xmath104 : the geodesic distance @xmath119 [ @xcite ] , the chordal distance @xmath120 [ @xcite ] , and the maximum chordal distance [ @xcite ] @xmath121 in view of the probability measure @xmath105 in ( [ 003 ] ) , it seems natural to consider the latter two distances , which is indeed the case . to see this ,",
    "let @xmath122 be an @xmath123 orthonormal generator matrix for @xmath113 .",
    "then @xmath113 is uniquely determined by the projection matrix @xmath124 , which corresponds to the projection onto the @xmath91-dimensional subspace  @xmath113 .",
    "it is known that @xmath125 where and denote the frobenius norm and spectral norm ( or operator norm ) of a given matrix , respectively .",
    "these two matrix norms are commonly used in large covariance matrix estimation and other multivariate analysis problems .",
    "we now bound the size of the set @xmath126 of all subspaces spanned by @xmath91 of covariates @xmath92 s under some distance constraint , which in turn gives bounds on the dimensionality @xmath1 .",
    "the probability measure @xmath105 on @xmath106^s$ ] defined in ( [ 003 ] ) is a key ingredient in our analysis .",
    "when all the subspaces in @xmath127 have distance at least @xmath128 under any distance @xmath129 , it is easy to see that @xmath130 , where @xmath131 denotes a ball of radius @xmath132 in grassmann manifold @xmath104 under distance @xmath129 . in particular",
    ", we focus on the maximum chordal distance defined in ( [ 004 ] ) .",
    "equivalently , the maximum chordal distance constraint gives the maximum principal angle constraint .",
    "since the sample size @xmath0 is usually small or moderate in many contemporary applications , we adopt the asymptotic framework of @xmath133 as @xmath134 for deriving asymptotic bounds on the dimensionality @xmath1 .",
    "[ thm3 ] assume that all subspaces spanned by @xmath91 of covariates @xmath92 s have maximum chordal distance at least a fixed constant @xmath135 , and @xmath136 as @xmath137 .",
    "then we have @xmath138 where @xmath139 denotes asymptotic dominance .",
    "theorem [ thm3 ] gives a general asymptotic bound on the dimensionality @xmath1 under the maximum chordal distance constraint , or equivalently , the maximum principal angle constraint .",
    "we see that finite sample can allow for a large number of variables , in which sparse models with size much smaller than sample size @xmath0 can still be distinguishable from each other .",
    "the leading order in the bound for @xmath86 is proportional to sample size @xmath0 , with factors @xmath140 and @xmath141 .",
    "this result is reasonable because larger @xmath142 means bigger separation of all @xmath91-dimensional subspaces spanned by covariates @xmath92 s , and large @xmath143 means more such subspaces separated from each other , both cases leading to tighter constraint on the growth of dimensionality @xmath1 .",
    "it is interesting that there are only two terms @xmath144 and @xmath145 following the leading order in the above bound on dimensionality .",
    "the general bound on dimensionality with distance constraint in theorem [ thm3 ] also shares some similarity with the lower bound @xmath146 on the robust spark in theorem [ thm2 ] , although the former uses nonprobabilistic arguments with no distributional assumption and the latter applies probabilistic arguments .",
    "the robust spark provides a natural bound on sparse model size to control collinearity for sparse models .",
    "intuitively , when the dimensionality @xmath1 grows with the sample size @xmath0 , one expects tighter control on the robust spark through a deflation factor of @xmath86 .",
    "similarly , the upper bound on the logarithmic dimensionality @xmath86 in theorem [ thm3 ] decreases with the minimum maximum chordal distance @xmath147 between sparse models through the factor @xmath140 . as mentioned in section  [ sec22 ] , these sparse eigenvalue type conditions play an important role in characterizing the variable selection properties including the model selection consistency for various regularization methods . although the result in theorem [ thm3 ] can be viewed as the bound for the worst case scenario , it provides us caution and guidance on the growth of dimensionality in real applications , particularly when variable selection is an important goal in the studies .",
    "in general , the robust spark @xmath74 provides a stronger measure on collinearity than the maximum chordal distance . to see this ,",
    "assume that @xmath148 and let @xmath109 be two subspaces spanned by two different sets of @xmath91 of covariates @xmath92 s .",
    "then the maximum principal angle @xmath149 between @xmath117 and @xmath118 is the angle between two vectors @xmath150 , where @xmath151 is a linear combination of the corresponding set of covariate vectors @xmath96 for each @xmath152 .",
    "since the union of these two sets of covariates has cardinality bounded from above by @xmath153 , it follows from the definition of the robust spark that the angle @xmath149 between @xmath154 and @xmath155 is bounded from zero , which entails that the maximum chordal distance between @xmath117 and @xmath118 is also bounded from zero .",
    "conversely , when two @xmath91-dimensional subspaces @xmath117 and @xmath118 has the maximum chordal distance bounded from zero , the subdesign matrix corresponding to covariates in the sets can still be singular .",
    "we next consider a stronger distance constraint than in theorem [ thm3 ] , where in addition , all disjoint subspaces in @xmath100 have minimum principal angles at least @xmath156 for some @xmath157 $ ] , with @xmath142 given in theorem [ thm3 ] .",
    "such disjoint subspaces are spanned by disjoint sets of @xmath91 of covariates @xmath92 s . in this case , it is natural to expect a tighter bound on the dimensionality @xmath1 .",
    "[ thm4 ] assume that the conditions of theorem [ thm3 ] hold and all disjoint subspaces have minimum principal angles at least a fixed constant @xmath156 with @xmath157 $ ] .",
    "then we have @xmath158 n\\nonumber\\\\[-8pt]\\\\[-8pt ] & & { } + 2 \\log n + o(1),\\nonumber\\end{aligned}\\ ] ] where @xmath159 ( 1-\\gamma ) - 2^{-1}(1-\\delta_1 ^ 2)^{-1 } \\delta_1 ^ 2(1 - 2\\gamma)$ ] .    compared to the bound in theorem [ thm3 ] , theorem [ thm4 ]",
    "indeed provides a tighter bound on the dimensionality @xmath1 due to the additional distance constraint involving @xmath160 .",
    "we are interested in the asymptotic bound on the dimensionality when @xmath160 is near zero . in this case",
    ", we have @xmath161 .",
    "observe that @xmath162 and is of order @xmath163 .",
    "it is generally difficult to derive tight bounds over the whole ranges of @xmath160 and @xmath164 .",
    "this is essentially due to the challenge of obtaining a globally tight function bounding the function @xmath165 defined in ( [ 015 ] ) from above , while retaining analytical tractability of evaluating the resulting integral .",
    "we finally revisit the marginal correlation ranking , a widely used technique for analyzing large - scale data sets , from a nonprobabilistic point of view . given a sample of size @xmath0 ,",
    "the maximum correlation of noise covariates with the response variable can exceed the maximum correlation of true covariates with the response variable when the dimensionality @xmath1 is high . here",
    "the correlation between two @xmath0-vectors @xmath166 and @xmath167 is referred to as @xmath168 , where @xmath169 is the angle between them .",
    "it is important to understand the limit on the dimensionality @xmath1 under which the above undesired phenomenon can happen .",
    "[ thm5 ] let @xmath170 be the maximum absolute correlation between @xmath91 true predictors @xmath92 and response vector @xmath171 in @xmath99 and assume that all @xmath172 noise predictors @xmath92 have absolute correlations bounded by @xmath173 .",
    "then there exists a noise predictor having absolute correlation with @xmath171 larger than @xmath174 if @xmath175 \\ } ( n-1 ) + 2^{-1 } \\log n + o(1)$ ] .",
    "it is an interesting result that the above asymptotic bound on the dimensionality @xmath1 depends only on @xmath173 , and is independent of the specific value of @xmath170 .",
    "the condition on the dimensionality is sufficient but not necessary in general , since one can always add an additional noise predictor having absolute correlation with @xmath171 larger than @xmath174 .",
    "nevertheless , theorem [ thm5 ] gives us a general limit on dimensionality even when one believes that a majority of noise predictors have weak correlation with the response variable .",
    "meanwhile , we also see from theorem [ thm5 ] that the dimensionality @xmath1 generally needs to be large compared to the sample size @xmath0 such that a noise predictor may have the highest correlation with the response variable .",
    "this result is reflected in a common feature of many variable selection procedures including commonly used greedy algorithms , that is , initially selecting one predictor with the highest correlation with the response variable .",
    "see , for example , the lars algorithm in @xcite and the lla algorithm in @xcite .",
    "such a variable , which gives a sparse model with size one , commonly appears on the solution paths of many regularization methods for high - dimensional variable selection .     in different scenarios of distributions of @xmath20 with @xmath176 and @xmath177 and @xmath178 , based on 100 simulations . ]",
    "in this section we provide two simulation examples to illustrate the theoretical results in section  [ sec2 ] , obtained through probabilistic arguments .",
    "the first simulation example examines the concentration property for large random design matrix .",
    "let @xmath20 be an @xmath23 random design matrix with @xmath179 for some constant @xmath27 .",
    "we set @xmath177 and @xmath178 , and @xmath180 .",
    "we considered three scenarios of distributions : ( 1 ) each entry of @xmath20 is sampled independently from @xmath181 , ( 2 ) each entry of @xmath20 is sampled independently from the laplace distribution with mean 0 and variance 1 , and ( 3 ) each row of @xmath20 is sampled independently from the multivariate @xmath2-distribution with 10 degrees of freedom and then rescaled to have unit variances . in view of definition [ def1 ] ,",
    "the concentration property of @xmath20 is characterized by the distribution of the condition number of @xmath182 . in each case , 100 monte carlo simulations were used to obtain the distribution of such condition number .",
    "figure  [ fig1 ] depicts these distributions in different scenarios .",
    "we see that in scenarios 1 and 2 , the condition number concentrates in the range of relatively small numbers , indicating the associated concentration property as shown in theorem [ thm1 ] . in scenario 3 with multivariate @xmath2-distribution , one still observes the concentration phenomenon .",
    "however , since this distribution is relatively more heavy - tailed , we see that the distribution of the condition number becomes more spread out and shifts toward the range of large numbers .",
    "the second simulation example investigates the robust spark bound for large random design matrix @xmath20 .",
    "we adopted the same three scenarios of distributions as in the first simulation example , except that @xmath177 , and @xmath183 and @xmath184 . in light of theorem [ thm2 ]",
    ", we sampled randomly 1000 @xmath185 submatrices of @xmath186 each with @xmath187 columns and calculated the minimum of those 1000 smallest singular values .",
    "similarly , in each case 100 monte carlo simulations were used to obtain the distribution of such minimum singular value which is tied to the     each with @xmath188 columns , in different scenarios of distributions of @xmath20 with @xmath189 , and @xmath183 and @xmath184 , based on 100 simulations . ]",
    "robust spark bound of @xmath20 .",
    "these distributions are shown in figure  [ fig2 ] .",
    "in particular , we see that the distribution of the minimum singular value concentrates clearly away from zero in each of the three scenarios of distributions .",
    "these numerical results indicate that the robust spark of random design matrix can indeed be at least of order @xmath85 , as shown in theorem [ thm2 ] .",
    "we have investigated the impacts of high dimensionality in finite samples from two different perspectives : a probabilistic one and a nonprobabilistic one .",
    "an interesting concentration phenomenon for large random design matrix has been revealed , as shown previously in @xcite .",
    "we have shown that the concentration property , which is important in characterizing the sure screening property of the sis , holds for a wide class of elliptical distributions , as conjectured by @xcite .",
    "we have also established a lower bound on the robust spark which is important in ensuring model identifiability and stable estimation .",
    "the high - dimensional geometric view of finite samples has lead to general bounds on dimensionality with distance constraint on sparse models , using nonprobabilistic arguments .    both probabilistic and nonprobabilistic",
    "views provide understandings on how the dimensionality interacts with the sample size for large - scale data sets .",
    "characterizing the limit of the dimensionality with respect to the sample size is key to the success of high - dimensional inference goals such as prediction and variable selection .",
    "we have focused on the family of elliptical distributions .",
    "it would be interesting to consider a more general class of distributions for future research .",
    "for notational simplicity , we use @xmath190 to denote a generic positive constant , whose value may change from line to line .      by theorem 5.2 in @xcite , we know that condition [ cond1 ] entails the logarithmic sobolev inequality ( [ e004 ] ) when @xmath191 .",
    "it remains to prove the logarithmic sobolev inequality for any marginal distribution of @xmath11 .",
    "let @xmath192 and @xmath44 be a @xmath43-variate marginal distribution of @xmath193 . by the spherical symmetry of @xmath11 , without loss of generality",
    "we can assume that @xmath44 is concentrated on @xmath194 . for any smooth function @xmath195 on @xmath48 with @xmath196 ,",
    "define @xmath197 by @xmath198 clearly @xmath47 is a smooth function on @xmath31 and @xmath199,\\ ] ] which shows that @xmath200 in view of ( [ e005 ] ) , it follows from fubini s theorem that @xmath201 thus by ( [ e005 ] ) , ( [ e006 ] ) , and fubini s theorem , applying the logarithmic sobolev inequality ( [ e004 ] ) for @xmath11 to the smooth function @xmath47 yields @xmath202 which completes the proof .",
    "we first make a simple observation .",
    "the standard gaussian distributions are special cases of spherical distributions . recall that the @xmath43-variate standard gaussian distribution @xmath209 has density function @xmath210 , @xmath211 .",
    "thus it is easy to check that @xmath209 satisfies condition [ cond1 ] with @xmath212 .",
    "let @xmath213 .",
    "then it follows immediately from lemma [ lem2 ] that for any @xmath53 , @xmath214 note that @xmath215 by ( [ e008 ] ) , ( [ e010 ] ) , and ( [ e011 ] ) , we have for any @xmath216 , @xmath217\\\\[-8pt ] & \\leq&2 e^{-n r_1 ^ 2/2}\\nonumber\\end{aligned}\\ ] ] since @xmath218 .",
    "now we get back to @xmath52 .",
    "it follows from ( [ e007 ] ) and ( [ e008 ] ) in lemma [ lem2 ] and condition  [ cond2 ] that for any @xmath219 , @xmath220\\\\[-8pt ] & \\leq&2 e^{-c_2^{-1 } n r_2 ^ 2/2}\\nonumber\\end{aligned}\\ ] ] since @xmath218 .",
    "let @xmath221 then combining ( [ e012 ] ) and ( [ e013 ] ) along with bonferroni s inequality yields @xmath222 where @xmath223 .",
    "this completes the proof .      in section a.7",
    ", @xcite proved that gaussian distributions satisfy the concentration property ( [ e002 ] ) , that is , for @xmath224 .",
    "we now consider the general situation where @xmath0 rows of the @xmath8 random matrix @xmath225 are i.i.d .",
    "copies from the spherical distribution @xmath11 .",
    "fix an arbitrary @xmath226 submatrix @xmath24 of @xmath25 with @xmath227 , where @xmath228 .",
    "we aim to prove deviation inequality in ( [ e002 ] ) with different constants @xmath229 and @xmath230 .    by the spherical symmetry , without loss of generality",
    "we can assume that @xmath24 consists of the first @xmath231 columns of @xmath25 .",
    "let @xmath232 clearly , @xmath233 are @xmath0 i.i.d .",
    "copies of @xmath234 .",
    "take an @xmath235 random matrix @xmath236 which is independent of @xmath24 .",
    "then for each @xmath14 , @xmath237 has the @xmath231-variate standard gaussian distribution .",
    "it is well known that @xmath238 has the haar distribution on the unit sphere @xmath239 in @xmath240-dimensional euclidean space @xmath241 , that is , the uniform distribution on @xmath239 .",
    "since the distribution of @xmath234 is a marginal distribution of @xmath11 , the spherical symmetry of @xmath11 entails that of the distribution of @xmath234 .",
    "it follows easily from the assumption of @xmath242 that @xmath243 .",
    "thus by theorem 1.5.6 in @xcite , @xmath244 is uniformly distributed on @xmath245 and is independent of @xmath246 .",
    "this along with the above fact shows that for each @xmath14 , @xmath247 where we use the symbol @xmath248 to denote being identical in distribution .",
    "hereafter , for notational simplicity we do not distinguish @xmath249 and @xmath250 .",
    "define the @xmath251 diagonal matrix @xmath252 then we have @xmath253 which entails @xmath254 this shows that @xmath255 and @xmath256    as mentioned before , we have for some @xmath257 and @xmath258 , @xmath259 note that @xmath260 .",
    "thus by ( [ e009 ] ) in lemma [ lem3 ] , an application of bonferroni s inequality gives @xmath261 where @xmath262 , @xmath263 , and @xmath264 . therefore by bonferroni s inequality , combining ( [ e017 ] ) and ( [ e018 ] ) proves the deviation inequality in ( [ e002 ] ) by appropriately changing the constants @xmath257 and @xmath265 .",
    "this concludes the proof .      using the similar arguments as in the proof of theorem [ thm1 ]",
    ", we can prove that there exist some universal positive constants @xmath266 such that the deviation probability bound @xmath267 holds for each @xmath23 submatrix @xmath24 of @xmath25 with @xmath268 and @xmath269 some positive constant .",
    "this is because lemmas [ lem1][lem2 ] are free of the dimension @xmath43 of the marginal distribution , and lemma [ lem3 ] still holds with the choice of @xmath270 .",
    "we should also note that the deviation probability bound ( [ 028 ] ) holds when @xmath271 , which is entailed by the concentration property ( [ e002 ] ) proved in @xcite for gaussian distributions .",
    "for each set @xmath272 with @xmath273 , denote by @xmath274 the principal submatrix of @xmath6 corresponding to variables in @xmath275 , and @xmath276 a submatrix of the design matrix @xmath20 consisting of columns with indices in @xmath275 .",
    "it follows easily from the representation of elliptical distributions that @xmath277 has the same distribution as @xmath278 .",
    "since @xmath279 is bounded from below by some positive constant , we have @xmath280 where @xmath190 is some positive constant .",
    "therefore , combining the above results yields @xmath281 with a possibly different positive constant @xmath282 .",
    "note that the positive constants involved are universal ones . we choose a positive integer @xmath283 .",
    "then an application of the bonferroni inequality together with ( [ 029 ] ) gives @xmath284 as @xmath137 .",
    "this shows that with asymptotic probability one , the robust spark @xmath285 for any @xmath286 , which completes the proof .      for the maximum chordal distance @xmath287 , by noting that @xmath288",
    ", we have a simple representation of the neighborhood @xmath289^s\\dvtx   \\max_{i = 1}^s x_i \\leq\\delta^2\\}$ ] for @xmath290 .",
    "we need to calculate its volume under the probability measure @xmath105 given in ( [ 003 ] ) . in light of ( [ 003 ] ) , a change of variable @xmath291 gives @xmath292 where @xmath293 over @xmath106^s$ ] .",
    "observe that without the term @xmath47 in ( [ 005 ] ) , @xmath294 would become selberg s integral which is a generalization of the beta integral [ @xcite ] .",
    "we will evaluate this integral by sandwiching the function @xmath47 between two functions of the same form .",
    "since the function @xmath295 is increasing and convex on @xmath106 $ ] , it follows that @xmath296 , where @xmath297 and @xmath298 .",
    "this shows that @xmath299 thus , we obtain a useful representation of the volume of the neighborhood @xmath300 where @xmath301 with some @xmath302 $ ] is an integral given in the following lemma .",
    "[ lem4 ] for each @xmath303 , we have @xmath304^s } \\prod _ { i = 1}^s ( 1 + c y_i ) \\prod _ { 1 \\leq i < j \\leq s } |y_i - y_j| \\prod _",
    "{ i = 1}^s y_i^{\\alpha-1 } \\,d y_1 \\cdots d y_s \\nonumber \\\\ & = & 2^s \\pi^{-s/2 } \\sum_{m = 0}^s \\pmatrix{s \\cr m } c^m \\prod_{i = s - m}^{s-1 } \\frac{\\alpha+ 2^{-1}i}{\\alpha+ 2^{-1 } ( s +",
    "i + 1 ) } \\\\ & & { } \\times\\prod_{i = 0}^{s - 1 } \\frac{\\gamma(\\alpha+ 2^{-1}i ) \\gamma(1 + 2^{-1 } ( i + 1 ) ) \\gamma(1 + 2^{-1}i)}{\\gamma(\\alpha+ 2^{-1}(s + i + 1 ) ) } , \\nonumber\\end{aligned}\\ ] ] where the factor containing @xmath305 equals @xmath205 when @xmath306 .",
    "observe that the integrand in ( [ 007 ] ) is symmetric in @xmath307 .",
    "thus , an expansion of @xmath308 gives @xmath309^s } \\prod_{i = 1}^s ( 1 + c y_i ) \\prod_{1",
    "\\leq i < j \\leq s }    y_i^{\\alpha-1 } \\,d y_1 \\cdots d y_s \\\\ & & \\qquad = \\sum_{m = 0}^s \\pmatrix{s \\cr m } c^m \\int_{[0 , 1]^s } \\prod _ { i = 1}^m y_i \\prod _ { 1 \\leq i < j \\leq s } |y_i - y_j| \\prod _ { i = 1}^s y_i^{\\alpha -1 } \\,d y_1 \\cdots d y_s,\\end{aligned}\\ ] ] where @xmath310 when @xmath306 .",
    "the above integrals are exactly aomoto s extension of selberg s integral [ @xcite ] and can be calculated as @xmath309^s } \\prod_{i = 1}^m y_i \\prod_{1 \\leq i < j \\leq s } |y_i - y_j| \\prod_{i = 1}^s y_i^{\\alpha-1 } \\,d y_1 \\cdots d y_s \\\\ & & \\qquad = 2^s \\pi^{-s/2 } \\prod_{i = s - m}^{s-1 } \\frac{\\alpha+ 2^{-1}i}{\\alpha+ 2^{-1 } ( s +",
    "i + 1 ) } \\\\ & & \\qquad\\quad{}\\times\\prod_{i = 0}^{s - 1 } \\frac { \\gamma ( \\alpha+ 2^{-1}i ) \\gamma(1 + 2^{-1 } ( i + 1 ) ) \\gamma(1 + 2^{-1}i)}{\\gamma(\\alpha+ 2^{-1}(s + i + 1))},\\end{aligned}\\ ] ] where the factor containing @xmath305 equals @xmath205 when @xmath306 .",
    "this completes the proof of lemma [ lem4 ] .",
    "let us continue with the proof of theorem [ thm3 ] . by assumption , @xmath311 as @xmath137 , so @xmath312 .",
    "applying stirling s formula for large factorials gives @xmath313 .",
    "thus by omitting @xmath314 and smaller order terms , @xmath315 \\sim ( \\log\\delta ) \\gamma(1-\\gamma)n^2 - \\gamma n \\log n.\\ ] ] using stirling s formula for the gamma function @xmath316 as @xmath317 and noting that @xmath318 and @xmath319 , we derive @xmath320 which entails that @xmath321\\\\[-8pt ] & & \\qquad\\sim-(\\log\\pi ) \\gamma^2 n^2/2 - \\gamma n ( \\log n)/2.\\nonumber\\end{aligned}\\ ] ] similarly , it follows from the identities @xmath322 and @xmath323 that @xmath324 this shows that @xmath325\\\\[-8pt ] & & \\qquad\\sim(\\log\\pi ) \\gamma^2",
    "n^2/2 + \\gamma n ( \\log n)/2.\\nonumber\\end{aligned}\\ ] ]    it remains to consider the last term .",
    "note that @xmath326 which entails that @xmath327 \\nonumber\\\\[-8pt]\\\\[-8pt ] & = & o(n).\\nonumber\\end{aligned}\\ ] ] thus combining ( [ 008 ] ) and ( [ 010])([012 ] ) yields @xmath328 since all the subspaces in @xmath100 have maximum chordal distance at least @xmath147 , it holds that @xmath329 .",
    "hence by ( [ 013 ] ) , @xmath330 where @xmath139 denotes asymptotic dominance .",
    "it is easy to derive @xmath331 .",
    "these two results lead to the claimed bound on @xmath86 , which concludes the proof .",
    "let us fix an arbitrary subset @xmath332 and denote by @xmath333 the set of @xmath91-subspaces spanned by @xmath91 of the remaining @xmath334 @xmath92 s . by assumption",
    ", @xmath335 lies in a neighborhood in the grassmann manifold @xmath104 that is characterized by the set @xmath336^s\\dvtx",
    "\\min_{i = 1}^s x_i \\geq\\delta_1 ^ 2\\}$ ] , since @xmath337 . in view of ( [ 003 ] ) , a change of variable @xmath338 gives @xmath339\\\\[-8pt ] & & { } \\times\\prod_{1 \\leq i < j \\leq s } |y_i - y_j| \\prod_{i = 1}^s y_i^{-1/2 } \\,d y_1 \\cdots d y_s,\\nonumber\\end{aligned}\\ ] ] where @xmath340^{\\alpha-1}$ ] over @xmath106^s$ ] . clearly",
    "@xmath341^{\\alpha-1 } \\geq(1 - y_i)^{\\alpha-1}$ ] for @xmath342 , which together with ( [ 003 ] ) and ( [ 015 ] ) entails that @xmath343 is a lower bound on the integral @xmath344 .",
    "however , we need an upper bound on it .",
    "the idea is to bound the function @xmath165 by an exponential function .",
    "we are more interested in the asymptotic behavior of @xmath345 when @xmath160 is near zero . using the inequality @xmath346",
    ", we derive @xmath347 \\leq\\bigl(1- \\delta_1 ^ 2\\bigr ) e^{(1-\\delta_1 ^ 2)^{-1 } \\delta_1 ^ 2 } e^{-y}.\\ ] ] this leads to @xmath348^{s(\\alpha-1 ) } \\prod_{i = 1}^s e^{-(\\alpha-1 ) y_i}.\\ ] ] thus we have @xmath349 where @xmath350^s } \\prod_{1 \\leq i",
    "< j \\leq s } |y_i - y_j| \\prod_{i = 1}^s y_i^{-1/2 } e^{-(\\alpha-1 ) y_i } \\,d y_1 \\cdots d y_s$ ] .",
    "since @xmath351 as @xmath352 , a change of variable @xmath353 gives @xmath354^s } \\prod_{1 \\leq i < j",
    "\\leq s } |z_i - z_j| \\prod_{i = 1}^s z_i^{-1/2 } e^{-z_i } \\,d z_1 \\cdots d z_s \\\\ & \\leq & ( \\alpha-1)^{-s^2/2 } \\int_{[0 , \\infty)^s } \\prod _ { 1 \\leq i < j \\leq s } |z_i - z_j| \\prod _",
    "{ i = 1}^s z_i^{-1/2 } e^{-z_i } \\,d z_1 \\cdots d z_s.\\end{aligned}\\ ] ] note that the last integral is a selberg type integral related to the laguerre polynomials [ @xcite ] , which can be calculated exactly .",
    "this along with the identities @xmath322 and @xmath355 yields @xmath356    by assumption , @xmath357 as @xmath137 .",
    "it is easy to show that @xmath358 \\sim - c_{\\delta_1 } \\gamma n^2 + o(n),\\ ] ] where @xmath159 ( 1-\\gamma ) - 2^{-1}(1-\\delta_1 ^ 2)^{-1 } \\delta_1 ^ 2(1 - 2\\gamma)$ ] .",
    "it remains to consider the term @xmath359 . by ( [ 002 ] ) , we have @xmath360^{s/2 } ( 2e)^{-s^2/2},\\end{aligned}\\ ] ] where we used stirling s formula for the gamma function in the last step .",
    "it follows from @xmath357 that @xmath361 and an application of stirling s formula for large factorials gives @xmath362^{s/2 } ( 2e)^{-s^2/2 } & \\sim & \\biggl(\\frac{n-2}{n - s-2 } \\biggr)^{s(n - s-3/2)/2 } \\bigl[(n-2)/\\bigl(2e^2 \\bigr)\\bigr]^{s^2/2 } \\\\",
    "& \\lesssim & ( 1-\\gamma)^{-\\gamma(1-\\gamma)n^2/2 } \\bigl[n/\\bigl(2e^2\\bigr ) \\bigr]^{\\gamma^2 n^2/2}.\\end{aligned}\\ ] ] note that @xmath363^{-\\gamma ^2n^2/2}$ ] . combining these results together",
    "yields @xmath364 \\\\ & \\lesssim & -\\bigl[2\\gamma+\\log(1 - 2\\gamma)\\bigr ] \\gamma n^2/2 . \\nonumber\\end{aligned}\\ ] ] it follows from ( [ 016])([018 ] ) that @xmath365 \\gamma n^2/2 + o(n).\\ ] ]    finally , we are ready to derive a bound on the dimensionality @xmath1 .",
    "since @xmath333 lies in a neighborhood in @xmath366 characterized by the set @xmath367 and all the subspaces in @xmath333 have maximum chordal distance at least @xmath142 , it holds that @xmath368 . aided by ( [ 013 ] ) and ( [ 019 ] ) , a similar argument as in the proof of theorem [ thm3 ] gives the claimed bound on @xmath369 .",
    "this completes the proof .      to prove the conclusion , we use the terminology introduced in section  [ sec3 ] .",
    "note that the @xmath0-vectors @xmath92 and @xmath171 can be viewed as elements of grassmannian manifold @xmath370 , which consists of all one - dimensional subspaces of @xmath103 .",
    "the absolute correlation between two @xmath0-vectors is given by @xmath371 , where @xmath372 $ ] is the principal angle between the two corresponding one - dimensional subspaces .",
    "we use the parametrization with local coordinate @xmath149 at the one - dimensional subspace @xmath373 spanned by @xmath171 .",
    "then the uniform distribution on the grassmann manifold @xmath370 can be expressed in local coordinate @xmath149 and gives a probability measure @xmath105 in ( [ 003 ] ) with @xmath374 on @xmath106 $ ] through a change of variable @xmath375 , where @xmath376 in this case .",
    "consider the maximum chordal distance on @xmath370 , which is defined as @xmath377 .",
    "for any @xmath378 , denote by @xmath379 a ball of radius @xmath2 centered at @xmath380 in @xmath381 under the maximum chordal distance , that is , @xmath382\\dvtx",
    "x_1 = \\sin^2 \\theta_1 \\leq t^2\\}$ ] in local coordinate .",
    "we need to calculate the volumes of @xmath383 with @xmath384 and @xmath385 , the complement of @xmath386 with @xmath387 , under the measure @xmath105 .    in view of ( [ 003 ] ) , we have @xmath388 \\bigr ) = k_{n , 1 } \\int_0^{t^2 } x_1^{(n-3)/2 } ( 1-x_1)^{-1/2 } \\,dx_1,\\ ] ] where @xmath389 with @xmath390 the area of the unit sphere @xmath391 .",
    "it follows from stirling s formula for the gamma function that @xmath392 it remains to evaluate the integral in ( [ 024 ] ) .",
    "note that @xmath393 is bounded between @xmath205 and @xmath394 on @xmath395 $ ] for @xmath2 bounded away from @xmath205 .",
    "thus , we have @xmath396 where both sides have the same asymptotic order . combining ( [ 024])([026 ] ) yields @xmath397 with @xmath398 , and @xmath399 for @xmath2 bounded away from @xmath205 .",
    "since all the @xmath400 noise predictors @xmath92 have absolute correlations bounded by @xmath173 , we have @xmath401 if there exists no noise predictor that has absolute correlation with @xmath171 larger than @xmath170 .",
    "the right - hand side of ( [ 027 ] ) is @xmath402/\\nu(b_{t_1 , d_m})$ ] , which is less than and has the same asymptotic order as @xmath403^{(n-1)/2 } \\sim(\\pi/2)^{1/2 } n^{1/2 } [ 4/(1-\\delta ^2)]^{(n-1)/2}$ ] .",
    "this together with ( [ 027 ] ) concludes the proof .",
    "we briefly introduce some necessary background and terminology on the geometry and invariant measure of grassmann manifold .",
    "let @xmath117 and @xmath118 be two @xmath91-dimensional subspaces of @xmath103 and @xmath404 be the geodesic distance on @xmath98 , that is , the distance induced by the euclidean metric on @xmath103 .",
    "it was shown by @xcite that as @xmath154 and @xmath155 vary over @xmath405 and @xmath406 , respectively , @xmath407 has a set of @xmath91 critical values @xmath408 with @xmath409 , corresponding to @xmath91 pairs of unit vectors @xmath112 , @xmath410 .",
    "each critical value @xmath107 is exactly the angle between @xmath411 and @xmath412 , and @xmath411 is orthogonal to @xmath413 and @xmath414 if @xmath415 .",
    "the principal angles @xmath107 are unique and if none of them are equal , the principle vectors @xmath112 are unique up to a simultaneous direction reversal . in general , the dimensions of @xmath117 and @xmath118 can be different , in which case @xmath91 should be their minimum .",
    "all @xmath91-dimensional subspaces of @xmath103 form a space , the so - called grassmann manifold @xmath104 .",
    "it is a compact riemannian homogeneous space , of dimension @xmath416 , isomorphic to @xmath417 , where @xmath418 denotes the orthogonal group of order @xmath93 .",
    "it is well known that @xmath104 admits an invariant measure @xmath419 .",
    "it can be constructed by viewing @xmath104 as @xmath420 , where @xmath421 denotes the stiefel manifold of all orthonormal @xmath91-frames ( i.e. , sets of @xmath91 orthonormal vectors ) in  @xmath103 . by deriving the exterior differential forms on those manifolds [ @xcite ]",
    ", @xmath422 can be expressed in local coordinates , at the @xmath91-dimensional subspace with generator matrix @xmath108 , as a product of three independent densities @xmath423 , where @xmath424 over @xmath425 , and @xmath426 and @xmath427 are independent of parameters @xmath428 .",
    "the normalization constant is given by @xmath429 where @xmath390 is the area of the unit sphere @xmath430 .",
    "a change of variable @xmath431 and symmetrization in ( [ 001 ] ) yield a probability measure @xmath105 on @xmath432^s$ ] with density @xmath433 where @xmath434 and @xmath435 .",
    "the author sincerely thanks the co - editor , associate editor and two referees for their valuable comments that improved significantly the paper ."
  ],
  "abstract_text": [
    "<S> high - dimensional data sets are commonly collected in many contemporary applications arising in various fields of scientific research . </S>",
    "<S> we present two views of finite samples in high dimensions : a probabilistic one and a nonprobabilistic one . with the probabilistic view </S>",
    "<S> , we establish the concentration property and robust spark bound for large random design matrix generated from elliptical distributions , with the former related to the sure screening property and the latter related to sparse model identifiability . an interesting concentration phenomenon in high dimensions </S>",
    "<S> is revealed . with the nonprobabilistic view </S>",
    "<S> , we derive general bounds on dimensionality with some distance constraint on sparse models . </S>",
    "<S> these results provide new insights into the impacts of high dimensionality in finite samples . </S>"
  ]
}