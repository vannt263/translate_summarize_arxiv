{
  "article_text": [
    "linear network coding @xcite is a promising new approach to information dissemination over networks .",
    "the fact that packets may be linearly combined at intermediate nodes affords , in many useful scenarios , higher rates than conventional routing approaches . if the linear combinations are chosen in a random , distributed fashion , then random linear network coding @xcite not only maintains most of the benefits of linear network coding , but also affords a remarkable simplicity of design that is practically very appealing .",
    "however , linear network coding has the intrinsic drawback of being extremely sensitive to error propagation . due to packet mixing , a single corrupt packet has the potential to contaminate all packets received by a destination node .",
    "the problem is better understood by looking at a matrix model for ( single - source ) linear network coding , given by @xmath4 all matrices are over a finite field . here",
    ", @xmath5 is an @xmath6 matrix whose rows are packets transmitted by the source node , @xmath7 is an @xmath8 matrix whose rows are the packets received by a ( specific ) destination node , and @xmath9 is a @xmath10 matrix whose rows are the additive error packets injected at some network links .",
    "the matrices @xmath11 and @xmath12 are transfer matrices that describe the linear transformations incurred by packets on route to the destination .",
    "such linear transformations are responsible for the ( unconventional ) phenomenon of error propagation .",
    "there has been an increasing amount of research on error control for network coding , with results naturally depending on the specific channel model used , i.e. , the joint statistics of @xmath11 , @xmath12 and @xmath9 given @xmath5 . under a worst - case ( or adversarial ) error model , the work in @xcite ( together with @xcite ) has obtained the maximum achievable rate for a wide range of conditions . if @xmath11 is square ( @xmath13 ) and nonsingular , and @xmath14 , then the maximum information rate that can be achieved in a single use of the channel is exactly @xmath15 packets when @xmath11 is known at the receiver , and approximately @xmath16 packets when @xmath11 is unknown .",
    "these approaches are inherently pessimistic and share many similarities with classical coding theory .",
    "recently , montanari and urbanke @xcite brought the problem to the realm of information theory by considering a probabilistic error model .",
    "their model assumes , as above , that @xmath11 is invertible and @xmath14 ; in addition , they assume that the matrix @xmath17 is chosen uniformly at random among all @xmath6 matrices of rank @xmath2 .",
    "for such a model and , under the assumption that the transmitted matrix @xmath5 must contain an @xmath18 identity submatrix as a header , they compute the maximal mutual information in the limit of large matrix size  approximately @xmath19 packets per channel use .",
    "they also present an iterative coding scheme with decoding complexity @xmath20 that asymptotically achieves this rate .",
    "the present paper is motivated by @xcite , and by the challenge of computing or approximating the actual channel capacity ( i.e. , without any prior assumption on the input distribution ) for any channel parameters ( i.e. , not necessarily in the limit of large matrix size ) .",
    "our contributions can be summarized as follows :    * assuming that the matrix @xmath11 is a constant known to the receiver , we compute the exact channel capacity for any channel parameters . we also present a simple coding scheme that asymptotically achieves capacity in the limit of large field or matrix size . * assuming that the matrix @xmath11 is chosen uniformly at random among all nonsingular matrices , we compute upper and lower bounds on the channel capacity for any channel parameters .",
    "these bounds are shown to converge asymptotically in the limit of large field or matrix size .",
    "we also present a simple coding scheme that asymptotically achieves capacity in both limiting cases .",
    "the scheme has decoding complexity @xmath3 and a probability of error that decays exponentially fast both in the packet length and in the field size in bits . * we present several extensions of our results for situations where",
    "the matrices @xmath11 , @xmath12 and @xmath9 may be chosen according to more general probability distributions .",
    "a main assumption that underlies this paper ( even the extensions mentioned above ) is that the transfer matrix @xmath11 is always invertible .",
    "one might question whether this assumption is realistic for actual network coding systems .",
    "for instance , if the field size is small , then random network coding may not produce a nonsingular @xmath11 with high probability .",
    "we believe , however , that removing this assumption complicates the analysis without offering much insight . under an _ end - to - end coding _",
    "( or _ layered _ ) approach , there is a clear separation between the network coding protocol  which induces a matrix channel  and the error control techniques applied at the source and destination nodes .",
    "in this case , it is reasonable to assume that network coding system will be designed to be _ feasible _",
    "( i.e. , able to deliver @xmath5 to all destinations ) when no errors occur in the network .",
    "indeed , a main premise of linear network coding is that the field size is sufficiently large in order to allow a feasible network code .",
    "thus , the results of this paper may be seen as conditional on the network coding layer being successful in its task .",
    "the remainder of this paper is organized as follows . in section  [ sec : matrix - channels ] , we provide general considerations on the type of channels studied in this paper . in section  [ sec :",
    "mmc ] , we address a special case of ( [ eq : basic - channel - model ] ) where @xmath11 is random and @xmath21 , which may be seen as a model for random network coding without errors . in section  [ sec : amc ] , we address a special case of ( [ eq : basic - channel - model ] ) where @xmath11 is the identity matrix .",
    "this channel may be seen as a model for network coding with errors when @xmath11 is known at the receiver , since the receiver can always compute @xmath22 . the complete channel with a random",
    ", unknown @xmath11 is addressed section  [ sec : ammc ] , where we make crucial use of the results and intuition developed in the previous sections .",
    "section  [ sec : extensions ] discusses possible extensions of our results , and section  [ sec : conclusion ] presents our conclusions .",
    "we will make use of the following notation .",
    "let @xmath23 be the finite field with @xmath24 elements .",
    "we use @xmath25 to denote the set of all @xmath6 matrices over @xmath23 and @xmath26 to denote the set of all @xmath6 matrices of rank @xmath2 over @xmath23 .",
    "we shall write simply @xmath27 when the field @xmath23 is clear from the context .",
    "we also use the notation @xmath28 for the set of all full - rank @xmath6 matrices .",
    "the @xmath6 all - zero matrix and the @xmath18 identity matrix are denoted by @xmath29 and @xmath30 , respectively , where the subscripts may be omitted when there is no risk of confusion .",
    "the reduced row echelon ( rre ) form of a matrix @xmath31 will be denoted by @xmath32 .",
    "for clarity and consistency of notation , we recall a few definitions from information theory @xcite .",
    "a discrete channel @xmath33 consists of an input alphabet @xmath34 , an output alphabet @xmath35 , and a conditional probability distribution @xmath36 relating the channel input @xmath37 and the channel output @xmath38 .",
    "an @xmath39 code for a channel @xmath33 consists of an encoding function @xmath40 and a decoding function @xmath41 , where @xmath42 denotes a decoding failure .",
    "it is understood that an @xmath39 code is applied to the @xmath43th extension of the discrete memoryless channel @xmath33 . a rate @xmath44 ( in bits ) is said to be achievable if there exists a sequence of @xmath45 codes such that decoding is unsuccessful ( either an error or a failure occurs ) with probability arbitrarily small as @xmath46 .",
    "the capacity of the channel is the supremum of all achievable rates .",
    "it is well - known that the capacity is given by @xmath47 where @xmath48 denotes the input distribution .    here , we are interested in matrix channels , i.e. , channels for which both the input and output variables are matrices . in particular , we are interested in a family of additive matrix channels given by the channel law @xmath49 where @xmath50 , @xmath51 , @xmath52 , @xmath53 , and @xmath5 , @xmath54 and @xmath9 are statistically independent . since the capacity of a matrix channel naturally scales with @xmath55 , we also define a _ normalized capacity _",
    "@xmath56    in the following ,",
    "we assume that statistics of @xmath11 , @xmath12 and @xmath9 are given for all @xmath57 . in this case",
    ", we may denote a matrix channel simply by the tuple @xmath58 , and we may also indicate this dependency in both @xmath59 and @xmath60 .",
    "we now define two limiting forms of a matrix channel ( strictly speaking , of a sequence of matrix channels ) . the first form , which we call the _ infinite - field - size channel _ , is obtained by taking @xmath61 .",
    "the capacity of this channel is given by @xmath62 represented in @xmath24-ary units per channel use .",
    "the second form , which we call the _ infinite - rank channel _",
    ", is obtained by setting @xmath63 and @xmath64 , and taking @xmath65 .",
    "the normalized capacity of this channel is given by @xmath66 represented in @xmath24-ary units per transmitted @xmath24-ary symbol .",
    "we will hereafter assume that logarithms are taken to the base @xmath24 and omit the factor @xmath67 from the above expressions .",
    "note that , to achieve the capacity of an infinite - field - size channel ( similarly for an infinite - rank channel ) , one should find a two - dimensional family of codes : namely , a sequence of codes with increasing block length @xmath43 for each @xmath24 , as @xmath61 ( or for each @xmath1 , as @xmath65 ) .",
    "we will simplify our task here by considering only codes with block length @xmath68 , which we call _ one - shot codes_. we will show , however , that these codes can achieve the capacity of both the infinite - field - size and the infinite - rank channels , at least for the classes of channels considered here . in other words , one - shot codes are asymptotically optimal as either @xmath61 or @xmath65 .    for completeness",
    ", we define also two more versions of the channel : the _ infinite - packet - length channel _ , obtained by fixing @xmath24 , @xmath2 and @xmath0 , and letting @xmath65 , and the _ infinite - batch - size channel _",
    ", obtained by fixing @xmath24 , @xmath2 and @xmath1 , and letting @xmath69 . these channels are discussed in section  [ ssec : other - infinite - channels ] .",
    "it is important to note that a @xmath70 channel is not the same as the @xmath43-extension of a @xmath58 channel .",
    "for instance , the @xmath71-extension of a @xmath58 channel has channel law @xmath72 where @xmath73 , and @xmath74 and @xmath75 correspond to independent realizations of a @xmath58 channel .",
    "this is not the same as the channel law for a @xmath76 channel , @xmath77 since @xmath78 may not be equal to @xmath79 .    to the best of our knowledge ,",
    "the @xmath43-extension of a @xmath58 channel has not been considered in previous works , with the exception of @xcite .",
    "for instance , @xcite and @xcite consider only limiting forms of a @xmath58 channel .",
    "although both models are referred to simply as `` random linear network coding , '' the model implied by the results in @xcite is in fact an infinite - rank channel , while the model implied by the results in @xcite is an infinite - packet - length - infinite - field - size channel .",
    "we now proceed to investigating special cases of ( [ eq : prob - basic - channel - law ] ) , by considering specific statistics for @xmath11 , @xmath12 and @xmath9 .",
    "we define the _ multiplicative matrix channel _ ( mmc ) by the channel law @xmath80 where @xmath81 is chosen uniformly at random among all @xmath18 nonsingular matrices , and independently from @xmath5 .",
    "note that the mmc is a @xmath82 channel .      in order to find the capacity of this channel",
    ", we will first solve a more general problem .",
    "[ prop : mmc - group - capacity ] let @xmath83 be a finite group that acts on a finite set @xmath84 . consider a channel with input variable @xmath85 and output variable @xmath86 given by @xmath87 , where @xmath88 is drawn uniformly at random and independently from @xmath5 .",
    "the capacity of this channel , in bits per channel use , is given by @xmath89 where @xmath90 is the number of equivalence classes of @xmath84 under the action of @xmath83 .",
    "any complete set of representatives of the equivalence classes is a capacity - achieving code .    for each @xmath91 ,",
    "let @xmath92 denote the orbit of @xmath93 under the action of @xmath83 . recall that @xmath94 for all @xmath95 and all @xmath91 , that is , the orbits form equivalence classes .    for @xmath95 ,",
    "let @xmath96 .",
    "by a few manipulations , it is easy to show that @xmath97 for all @xmath98 .",
    "since @xmath11 has a uniform distribution , it follows that @xmath99 = 1/|\\mathcal{g}(x)|$ ] , for all @xmath95 .    for any @xmath91 , consider the same channel but with the input alphabet restricted to @xmath100 .",
    "note that the output alphabet will also be restricted to @xmath100 .",
    "this is a @xmath101-ary channel with uniform transition probabilities ; thus , the capacity of this channel is 0 .",
    "now , the overall channel can be considered as a sum ( union of alphabets ) of all the restricted channels .",
    "the capacity of a sum of @xmath31 channels with capacities @xmath102 , @xmath103 , is known to be @xmath104 bits .",
    "thus , the capacity of the overall channel is @xmath105 bits , where @xmath106 is the number of orbits . a capacity - achieving code ( with block length 1 )",
    "may be obtained by simply selecting one representative from each equivalence class .",
    "proposition  [ prop : mmc - group - capacity ] shows that in a channel induced by a group action , where the group elements are selected uniformly at random , the receiver can not distinguish between transmitted elements that belong to the same equivalence class .",
    "thus , the transmitter can only communicate the choice of a particular equivalence class .",
    "returning to our original problem , we have @xmath107 and @xmath108 ( the general linear group @xmath109 ) .",
    "the equivalence classes of @xmath84 under the action of @xmath83 are the sets of matrices that share the same row space .",
    "thus , we can identify each equivalence class with a subspace of @xmath110 of dimension at most @xmath0 .",
    "let the gaussian coefficient @xmath111}{0pt}{}{m}{k}}_q = { \\prod_{i=0}^{k-1 } ( q^m - q^i)}/{(q^k - q^i)}\\ ] ] denote the number of @xmath112-dimensional subspaces of @xmath110 .",
    "we have the following corollary of proposition  [ prop : mmc - group - capacity ] .",
    "[ cor : mmc - capacity ] the capacity of the mmc , in @xmath24-ary units per channel use , is given by @xmath113}{0pt}{}{m}{k}}_q.\\ ] ] a capacity - achieving code @xmath114 can be obtained by ensuring that each @xmath112-dimensional subspace of @xmath110 , @xmath115 , is the row space of some unique @xmath116 .",
    "note that corollary  [ cor : mmc - capacity ] reinforces the idea introduced in @xcite that , in order to communicate under random network coding , the transmitter should encode information in the choice of a subspace .",
    "we now compute the capacity for the two limiting forms of the channel , as discussed in section  [ sec : matrix - channels ] .",
    "we have the following result .",
    "[ prop : mmc - capacity - limit ] let @xmath117 and assume @xmath118 .",
    "then @xmath119    first , observe that @xmath120}{0pt}{}{m}{n^*}}_q < \\sum_{k=0}^n { \\genfrac{[}{]}{0pt}{}{m}{k}}_q < ( n+1 ) { \\genfrac{[}{]}{0pt}{}{m}{n^*}}_q\\ ] ] where @xmath121 .",
    "using the fact that @xcite @xmath122}{0pt}{}{m}{k}}_q < 4 q^{(m - k)k}\\ ] ]",
    "it follows that @xmath123 the last term on the right vanishes on both limiting cases .    the case @xmath124",
    "can also be readily obtained but is less interesting since , in practice , the packet length @xmath1 will be much larger than the number of packets @xmath0 .    note that an expression similar to ( [ eq : mmc - capacity - general ] ) has been found in @xcite under a different assumption on the transfer matrix ( namely , that @xmath11 is uniform on @xmath125 .",
    "it is interesting to note that , also in that case , the same conclusion can be reached about the sufficiency of transmitting subspaces @xcite .    an intuitive way to interpret ( [ eq : mmc - capacity - limit - q ] )",
    "is the following : out of the @xmath55 symbols obtained by the receiver , @xmath126 of these symbols are used to describe @xmath11 , while the remaining ones are used to communicate @xmath5 .",
    "it is interesting to note that ( [ eq : mmc - capacity - limit - q ] ) precisely matches ( [ eq : mmc - capacity - limit - m ] ) after normalizing by the total number of transmitted symbols , @xmath55 .",
    "both limiting capacity expressions ( [ eq : mmc - capacity - limit - q ] ) and ( [ eq : mmc - capacity - limit - m ] ) can be achieved using a simple coding scheme where an @xmath127 data matrix @xmath128 is concatenated on the left with an @xmath18 identity matrix @xmath129 , yielding a transmitted matrix @xmath130 .",
    "the first @xmath0 symbols of each transmitted packet may be interpreted as pilot symbols used to perform `` channel sounding '' .",
    "note that this is simply the standard way of using random network coding @xcite .",
    "we define the _ additive matrix channel _ ( amc ) according to @xmath131 where @xmath132 is chosen uniformly at random among all @xmath6 matrices of rank @xmath2 , independently from @xmath5 .",
    "note that the amc is a @xmath58 channel with @xmath133 and @xmath134 uniformly distributed , and @xmath135 .",
    "the capacity of the amc is computed in the next proposition .",
    "[ prop : amc - capacity ] the capacity of the amc is given by @xmath136 for @xmath117 and @xmath137 , we have the limiting expressions @xmath138    to compute the capacity , we expand the mutual information @xmath139 where the last equality holds because @xmath5 and @xmath140 are independent . note that @xmath141 , and the maximum is achieved when @xmath7 is uniform . since @xmath142 does not depend on the input distribution , we can maximize @xmath143 by choosing , e.g. , a uniform @xmath48 .",
    "the entropy of @xmath140 is given by @xmath144 .",
    "the number of @xmath6 matrices of rank @xmath2 is given by @xcite @xmath145}{0pt}{}{m}{t}}_q \\label{eq : number - matrices - rank } \\\\ & = q^{(n+m - t)t}\\prod_{i=0}^{t-1}\\frac{(1 - q^{i - n})(1 - q^{i - m})}{(1 - q^{i - t})}. \\label{eq : number - matrices - rank-2}\\end{aligned}\\ ] ]    thus , @xmath146 the limiting expressions ( [ eq : amc - capacity - limit - q ] ) and ( [ eq : amc - capacity - limit - m ] ) follow immediately from the equation above .",
    "the expression ( [ eq : amc - capacity - limit - m ] ) , which gives the capacity of the infinite - rank amc , has been previously obtained in @xcite for a channel that is equivalent to the amc .",
    "our proof is a simple extension of the proof in @xcite .    as can be seen from ( [ eq : number - matrices - rank-2 ] ) , an @xmath6 matrix of rank @xmath2 can be specified with approximately @xmath147 symbols .",
    "thus , the capacity ( [ eq : amc - capacity - limit - q ] ) can be interpreted as the number of symbols conveyed by @xmath7 minus the number of symbols needed to describe @xmath140 .",
    "note that , as in section  [ sec : mmc ] , the normalized capacities of the infinite - field - size amc and the infinite - rank amc are the same .",
    "an intuitive explanation might be the fact that , for the two channels , both the number of bits per row and the number of bits per column tend to infinity .",
    "in contrast , the normalized capacity is different when only one of these quantities grows while the other is fixed .",
    "this is the case of the infinite - packet - length amc and the infinite - batch - size amc , which are studied in section  [ ssec : other - infinite - channels ] .",
    "we now present an efficient coding scheme that achieves ( [ eq : amc - capacity - limit - q ] ) and ( [ eq : amc - capacity - limit - m ] ) . the scheme is based on an `` error trapping '' strategy .",
    "let @xmath148 be a data matrix , where @xmath149 .",
    "a codeword @xmath5 is formed by adding all - zero rows and columns to @xmath128 so that @xmath150 these all - zero rows and columns may be interpreted as the `` error traps . '' clearly , the rate of this scheme is @xmath151 .",
    "since the noise matrix @xmath140 has rank @xmath2 , we can write it as @xmath152 where @xmath153 , @xmath154 , @xmath155 and @xmath156 . the received matrix @xmath7 is then given by @xmath157    we define an error trapping failure to be the event that @xmath158 .",
    "intuitively , this corresponds to the situation where either the row space or the column space of the error matrix has not been `` trapped '' .    for now , assume that the error trapping is successful , i.e. , @xmath159 .",
    "consider the submatrix corresponding to the first @xmath160 columns of @xmath7 .",
    "since @xmath161 , the rows of @xmath162 are completely spanned by the rows of @xmath163 .",
    "thus , there exists some matrix @xmath164 such that @xmath165 .",
    "but @xmath166 implies that @xmath167 , since @xmath168 has full row rank .",
    "it follows that @xmath169 note also that @xmath170 .",
    "thus , @xmath171 from which the data matrix @xmath128 can be readily obtained .",
    "the complexity of the scheme is computed as follows . in order to obtain @xmath164",
    ", it suffices to perform gaussian elimination on the left @xmath172 submatrix of @xmath7 , for a cost of @xmath173 operations .",
    "the data matrix can be extracted by multiplying @xmath164 with the top right @xmath174 submatrix of @xmath7 , which can be accomplished in @xmath175 operations .",
    "thus , the overall complexity of the scheme is @xmath176 operations in @xmath23 .",
    "note that @xmath163 is available at the receiver as the top - left submatrix of @xmath7 .",
    "moreover , the rank of @xmath163 is already computed during the gaussian elimination step of the decoding . thus , the event that the error trapping fails can be readily detected at the receiver , which can then declare a decoding failure .",
    "it follows that the error probability of the scheme is zero .",
    "let us now compute the probability of decoding failure .",
    "consider , for instance , @xmath177 $ ] , where @xmath178 is a full - rank matrix chosen uniformly at random",
    ". an equivalent way of generating @xmath9 is to first generate the entries of a matrix @xmath179 uniformly at random , and then discard @xmath31 if it is not full - rank .",
    "thus , we want to compute @xmath180 $ ] , where @xmath181 corresponds to the first @xmath160 columns of @xmath31 .",
    "this probability is @xmath182}{p[\\operatorname{\\sf rank\\hspace{0.1em}}m = t ] } = \\frac{q^{mt}\\prod_{i=0}^{t-1 } ( q^v - q^i)}{q^{vt}\\prod_{i=0}^{t-1 } ( q^m - q^i ) } \\nonumber \\\\ & > \\prod_{i=0}^{t-1 } ( 1 - q^{i - v } ) \\geq ( 1 - q^{t-1-v})^t \\geq 1 - \\frac{t}{q^{1+v - t}}. \\nonumber\\end{aligned}\\ ] ] the same analysis holds for @xmath183 $ ] . by the union bound",
    ", it follows that the probability of failure satisfies @xmath184    [ prop : amc - coding - achievability ] the coding scheme described above can achieve both capacity expressions ( [ eq : amc - capacity - limit - q ] ) and ( [ eq : amc - capacity - limit - m ] ) .    from ( [ eq : amc - failure - prob ] ) , we see that achieving either of the limiting capacities amounts to setting a suitable @xmath160 .",
    "to achieve ( [ eq : amc - capacity - limit - q ] ) , we set @xmath185 and let @xmath24 grow .",
    "the resulting code will have the correct rate , namely , @xmath186 in @xmath24-ary units , while the probability of failure will decrease exponentially with the field size in bits .",
    "alternatively , to achieve ( [ eq : amc - capacity - limit - m ] ) , we can choose some small @xmath187 and set @xmath188 , where both @xmath137 and @xmath117 are assumed fixed . by letting @xmath1 grow ,",
    "we obtain a probability of failure that decreases exponentially with @xmath1 . the ( normalized ) gap to capacity of the resulting code",
    "will be @xmath189 which can be made as small as we wish .",
    "consider a @xmath58 channel with @xmath81 , @xmath133 and @xmath134 uniformly distributed and independent from other variables .",
    "since @xmath11 is invertible , we can rewrite ( [ eq : prob - basic - channel - law ] ) as @xmath190 now , since @xmath191 acts transitively on @xmath192 , the channel law ( [ eq : ammc - model ] ) is equivalent to @xmath193 where @xmath81 and @xmath132 are chosen uniformly at random and independently from any other variables .",
    "we call ( [ eq : ammc - model-2 ] ) _ the additive - multiplicative matrix channel _ ( ammc )",
    ".      one of the main results of this section is the following theorem , which provides an upper bound on the capacity of the ammc .",
    "[ thm : ammc - capacity - upper - bound ] for @xmath194 , the capacity of the ammc is upper bounded by @xmath195    let @xmath196 . by expanding @xmath197 , and using the fact that @xmath5 , @xmath198 and @xmath7 form a markov chain , in that order , we have @xmath199 where ( [ eq : proof - ammc - mutual-1 ] ) follows since @xmath5 and @xmath140 are independent .",
    "we now compute an upper bound on @xmath200 .",
    "let @xmath201 and write @xmath202 , where @xmath203 and @xmath204 .",
    "note that @xmath205 where @xmath206 . since @xmath207 is full - rank , it must contain an invertible @xmath208 submatrix . by reordering columns if necessary ,",
    "assume that the left @xmath208 submatrix of @xmath207 is invertible .",
    "write @xmath209 , @xmath210 and @xmath211 , where @xmath212 , @xmath213 and @xmath214 have @xmath44 columns , and @xmath215 , @xmath216 and @xmath217 have @xmath218 columns",
    ". we have @xmath219 it follows that @xmath217 can be computed if @xmath214 is known .",
    "thus , @xmath220 where ( [ eq : proof - ammc - bound-1 ] ) follows since @xmath214 may possibly be any @xmath18 matrix with rank @xmath221 .",
    "applying this result in ( [ eq : proof - ammc - mutual-2 ] ) , and using ( [ eq : bound - sum - gc ] ) and ( [ eq : number - matrices - rank ] ) , we have @xmath222}{0pt}{}{m}{n } } + \\log_q ( t+1)\\frac{|{\\mathcal{t}}_{n\\times t}|{\\genfrac{[}{]}{0pt}{}{n}{t}}}{|{\\mathcal{t}}_{n\\times t}|{\\genfrac{[}{]}{0pt}{}{m}{t } } } \\nonumber \\\\ & \\leq \\log_q ( n+1)(t+1 ) { \\genfrac{[}{]}{0pt}{}{m - t}{n - t } } \\label{eq : proof - ammc - mutual-3 } \\\\    & \\leq ( m - n)(n - t ) + \\log_q 4(1+n)(1+t ) .",
    "\\nonumber\\end{aligned}\\ ] ] where ( [ eq : proof - ammc - mutual-3 ] ) follows from @xmath223}{0pt}{}{m}{n } } { \\genfrac{[}{]}{0pt}{}{n}{t } } = { \\genfrac{[}{]}{0pt}{}{m}{t } } { \\genfrac{[}{]}{0pt}{}{m - t}{n - t}}$ ] , for @xmath224 .",
    "we now develop a connection with the subspace approach of @xcite that will be useful to obtain a lower bound on the capacity . from section  [ sec : mmc ] , we know that , in a multiplicative matrix channel , the receiver can only distinguish between transmitted subspaces .",
    "thus , we can equivalently express @xmath225 where @xmath226 and @xmath227 denote the row spaces of @xmath5 and @xmath7 , respectively .    using this interpretation ,",
    "we can obtain the following lower bound on capacity .",
    "[ thm : ammc - capacity - lower - bound ] assume @xmath228 . for any @xmath229",
    ", we have @xmath230    in order to prove theorem  [ thm : ammc - capacity - lower - bound ] , we need a few lemmas .",
    "[ lem : prob - low - rank ] let @xmath231 be a matrix of rank @xmath112 , and let @xmath232 be a random matrix chosen uniformly among all matrices of rank @xmath2 . if @xmath233 , then @xmath234 < \\frac{2t}{q^{\\min\\{n , m\\}-k - t+1}}.\\ ] ]    write @xmath235 , where @xmath236 and @xmath237 are full - rank matrices .",
    "we can generate @xmath140 as @xmath238 , where @xmath239 and @xmath240 are chosen uniformly at random and independently from each other .",
    "then we have @xmath241 note that @xmath242 if and only if the column spaces of @xmath243 and @xmath244 intersect trivially _ and _ the row spaces of @xmath245 and @xmath246 intersect trivially .",
    "let @xmath247 and @xmath248 denote the probabilities of these two events , respectively . by a simple counting argument ,",
    "we have @xmath249 similarly , we have @xmath250 .",
    "thus , @xmath251 & < \\frac{t}{q^{n - k - t+1}}+\\frac{t}{q^{m - k - t+1 } } \\nonumber \\\\ & \\leq \\frac{2t}{q^{\\min\\{n , m\\}-k - t+1}}. \\nonumber\\end{aligned}\\ ] ]    for @xmath252 , let @xmath253 denote the set of all @xmath0-dimensional subspaces of @xmath110 that contain a subspace @xmath254 .    [ lem : sphere - top - shell - size ] @xmath255}{0pt}{}{m - k}{n - k}}_q\\ ] ] where @xmath256 .    by the fourth isomorphism theorem @xcite , there is a bijection between @xmath253 and the set of all @xmath257-dimensional subspaces of the quotient space @xmath258 .",
    "since @xmath259 , the result follows",
    ".    we can now give a proof of theorem  [ thm : ammc - capacity - lower - bound ] .",
    "assume that @xmath5 is selected from @xmath260 , where @xmath261 and @xmath229 .",
    "define a random variable @xmath262 as @xmath263 note that @xmath264 when @xmath265 .    by lemma  [",
    "lem : sphere - top - shell - size ] and ( [ eq : gauss - coeff - bound ] ) , we have @xmath266 where @xmath267 .",
    "choosing @xmath5 uniformly from @xmath260 , we can also make @xmath227 uniform within a given dimension ; in particular , @xmath268}{0pt}{}{m}{n'}}_q \\geq ( m - n')n'.\\ ] ] it follows that @xmath269    now , using lemma  [ lem : prob - low - rank ] , we obtain @xmath270i(\\mathcal{x};\\mathcal{y}|{q}=1 ) \\nonumber \\\\ & \\geq i(\\mathcal{x};\\mathcal{y}|{q}=1 ) - p[{q}=0]nm \\nonumber \\\\ & \\geq ( m - n)(n - t-\\epsilon t ) - \\log_q 4 - \\frac{2tnm}{q^{\\epsilon t+1}}. \\nonumber\\end{aligned}\\ ] ]    note that , differently from the results of previous sections , theorems  [ thm : ammc - capacity - upper - bound ] and [ thm : ammc - capacity - lower - bound ] provide only upper and lower bounds on the channel capacity . nevertheless , it is still possible to compute exact expressions for the capacity of the ammc in certain limiting cases .",
    "[ cor : ammc - capacity - limit ] for @xmath271 and @xmath137 , we have @xmath272    the fact that the values in ( [ eq : ammc - capacity - limit - q ] ) and ( [ eq : ammc - capacity - limit - m ] ) are upper bounds follows immediately from theorem  [ thm : ammc - capacity - upper - bound ] .",
    "the fact that ( [ eq : ammc - capacity - limit - q ] ) is a lower bound follows immediately from theorem  [ thm : ammc - capacity - lower - bound ] by setting @xmath273 . to obtain ( [ eq : ammc - capacity - limit - m ] ) from theorem  [ thm : ammc - capacity - lower - bound ] , it suffices to choose @xmath274 such that @xmath275 grows sublinearly with @xmath1 , e.g. , @xmath276 .    once again , note that ( [ eq : ammc - capacity - limit - q ] ) agrees with ( [ eq : ammc - capacity - limit - m ] ) if we consider the normalized capacity .    differently from the mmc and",
    "the amc , successful decoding in the ammc does not ( necessarily ) allow recovery of all sources of channel uncertainty  in this case , the matrices @xmath11 and @xmath140 . in general , for every observable @xmath277 pair , there are many valid @xmath11 and @xmath140 such that @xmath278 .",
    "such coupling between @xmath11 and @xmath140 is reflected in extra term @xmath200 in ( [ eq : proof - ammc - mutual-1 ] ) , which provides an additional rate of roughly @xmath279 as compared to the straightforward lower bound @xmath280 .    in @xcite , the problem of communicating over the ammc",
    "was addressed assuming a specific form of transmission matrices that contained an @xmath18 identity header .",
    "note that , if we consider such a header as being part of the channel ( i.e. , beyond the control of the code designer ) then , with high probability , the resulting channel becomes equivalent to the amc ( see @xcite for details ) .",
    "however , as a coding strategy for the ammc , using such an @xmath18 identity header results in a suboptimal input distribution , as the mutual information achieved is strictly smaller than the capacity .",
    "indeed , the capacity - achieving distribution used in theorem  [ thm : ammc - capacity - lower - bound ] and corollary  [ cor : ammc - capacity - limit ] corresponds to transmission matrices of rank @xmath281 .",
    "this result shows that , for the ammc , using headers is neither fundamental nor asymptotically optimal .",
    "we now propose an efficient coding scheme that can asymptotically achieve ( [ eq : ammc - capacity - limit - q ] ) and ( [ eq : ammc - capacity - limit - m ] ) .",
    "the scheme is based on a combination of channel sounding and error trapping strategies .    for a data matrix @xmath282 , where @xmath149 , let the corresponding codeword be @xmath283 note that the all - zero matrices provide the error traps , while the identity matrix corresponds to the pilot symbols . clearly , the rate of this scheme is @xmath284 .",
    "write the noise matrix @xmath140 as @xmath285 where @xmath153 , @xmath154 , @xmath155 , @xmath286 and @xmath287 .",
    "the auxiliary matrix @xmath198 is then given by @xmath288    similarly as in section  [ sec : amc ] , we define that the error trapping is successful if @xmath161 .",
    "assume that this is the case .",
    "from section  [ sec : amc ] , there exists some matrix @xmath289 such that @xmath290 note further that @xmath291 for some @xmath292 in rre form and some @xmath293 .",
    "it follows that @xmath294 where the bottom @xmath295 rows are all - zeros .",
    "since @xmath11 is invertible , we have @xmath296 , from which @xmath128 can be readily obtained .",
    "thus , decoding amounts to performing gauss - jordan elimination on @xmath7 .",
    "it follows that the complexity of the scheme is @xmath297 operations in @xmath23 .",
    "the probability that the error trapping is not successful , i.e. , @xmath158 , was computed in section  [ sec : amc ] .",
    "let @xmath298 correspond to the first @xmath0 columns of @xmath7 .",
    "note that @xmath161 if and only if @xmath299 . thus , when the error trapping is not successful , the receiver can easily detect this event by looking at @xmath300 and then declare a decoding failure .",
    "it follows that the scheme has zero error probability and probability of failure given by ( [ eq : amc - failure - prob ] ) .",
    "[ thm : ammc - coding - achievability ] the proposed coding scheme can asymptotically achieve ( [ eq : ammc - capacity - limit - q ] ) and ( [ eq : ammc - capacity - limit - m ] ) .    using ( [ eq : amc - failure - prob ] ) and the same argument as in the proof of proposition  [ prop : amc - coding - achievability ] , we can set a suitable @xmath160 in order to achieve arbitrarily low gap to capacity while maintaining an arbitrary low probability of failure , for both cases where @xmath61 or @xmath65 .",
    "in this section , we discuss possible extensions of the results and models presented in the previous sections .      as discussed in section  [ sec : ammc ] , the ammc is equivalent to a channel of the form ( [ eq : prob - basic - channel - law ] ) where @xmath81 and @xmath133 are chosen uniformly at random and independently from each other .",
    "suppose now that the channel is the same , except for the fact that @xmath11 and @xmath12 are not independent .",
    "it should be clear that the capacity of the channel can not be smaller than that of the ammc .",
    "for instance , one can always convert this channel into an ammc by employing randomization at the source .",
    "( this is , in fact , a natural procedure in any random network coding system . )",
    "let @xmath301 , where @xmath289 is chosen uniformly at random and independent from any other variables .",
    "then @xmath302 is uniform on @xmath191 and independent from @xmath12 .",
    "thus , the channel given by @xmath303 is an ammc .",
    "note that our coding scheme does not rely on any particular statistics of @xmath11 given @xmath5 and @xmath140 ( except the assumption that @xmath11 is invertible ) and therefore works unchanged in this case .",
    "the model for the ammc assumes that the transfer matrix @xmath81 is chosen uniformly at random . in a realistic network coding system",
    ", the transfer matrix may be a function of both the network code and the network topology , and therefore may not have a uniform distribution .",
    "consider the case where @xmath11 is chosen according to an arbitrary probability distribution on @xmath191 .",
    "it should be clear that the capacity can only increase as compared with the ammc , since less `` randomness '' is introduced in the channel .",
    "the best possible situation is to have a constant @xmath11 , in which case the channel becomes exactly an amc .",
    "again , note that our coding scheme for the ammc is still applicable in this case .      when expressed in the form ( [ eq : prob - basic - channel - law ] ) , the models for both the amc and the ammc assume that the matrix @xmath9 is uniformly distributed on @xmath192 .",
    "in particular , each error packet is uniformly distributed on @xmath304 . in a realistic situation , however , it may be the case that error packets of low weight are more likely to occur . consider a model identical to the amc or the ammc except for the fact that the matrix @xmath9 is chosen according to an arbitrary probability distribution on @xmath305 . once again , it should be clear that the capacity can only increase .",
    "note that the exact capacity in proposition  [ prop : amc - capacity ] and the upper bound of theorem  [ thm : ammc - capacity - upper - bound ] can be easily modified to account for this case ( by replacing @xmath306 with the entropy of @xmath140 ) .",
    "although our coding scheme in principle does not hold in this more general case , we can easily convert the channel into an amc or ammc by applying a random transformation at the source ( and its inverse at the destination ) .",
    "let @xmath307 , where @xmath308 is chosen uniformly at random and independent from any other variables .",
    "then @xmath309 where @xmath310 .",
    "since @xmath311 acts ( by right multiplication ) transitively on @xmath305 , we have that @xmath312 is uniform on @xmath305 .",
    "thus , we obtain precisely an ammc ( or amc ) and the assumptions of our coding scheme hold .",
    "note , however , that , depending on the error model , the capacity may be much larger than what can be achieved by the scheme described above .",
    "for instance , if the rows of @xmath9 are constrained to have weight at most @xmath313 ( otherwise chosen , say , uniformly at random ) , then the capacity would increase by approximately @xmath314 , which might be a substantial amount if @xmath313 is small .",
    "the model we considered for the amc and the ammc assumes an error matrix @xmath140 whose rank is known and equal to @xmath2 .",
    "it is useful to consider the case where @xmath315 is allowed to vary , while still bounded by @xmath2 .",
    "more precisely , we assume that @xmath140 is chosen uniformly at random from @xmath316 , where @xmath317 is a random variable with probability distribution @xmath318 = p_r$ ] .    since @xmath319",
    "we conclude that the capacities of the amc and the ammc may be reduced by at most @xmath320 .",
    "this loss is asymptotically negligible for large @xmath24 and/or large @xmath1 , so the expressions ( [ eq : amc - capacity - limit - q ] ) , ( [ eq : amc - capacity - limit - m ] ) , ( [ eq : ammc - capacity - limit - q ] ) and ( [ eq : ammc - capacity - limit - m ] ) remain unchanged .",
    "the steps for decoding and computing the probability of error trapping failure also remain the same , provided we replace @xmath2 by @xmath44 .",
    "the only difference is that now decoding errors may occur . more precisely ,",
    "suppose that @xmath321 .",
    "a necessary condition for success is that @xmath322 .",
    "if this condition is not satisfied , then a decoding failure is declared .",
    "however , if the condition is true , then the decoder can not determine whether @xmath323 ( an error trapping success ) or @xmath324 ( an error trapping failure ) , and must proceed assuming the former case .",
    "if the latter case turns out to be true , we would have an undetected error .",
    "thus , for this model , the expression ( [ eq : amc - failure - prob ] ) gives a bound on the probability that decoding is not successful , i.e. , that either an error or a failure occurs .",
    "we now extend our results to the infinite - packet - length amc and ammc and the infinite - batch - size amc .",
    "( note that , as pointed out in section  [ sec : mmc ] , there is little justification to consider an infinite - batch - size ammc . ) from the proof of propositon  [ prop : amc - capacity ] and the proof of corollary  [ cor : ammc - capacity - limit ] , it is straightforward to see that @xmath325    it is _ not _ straightforward , however , to obtain capacity - achieving schemes for these channels .",
    "the schemes described in sections  [ sec : amc ] and  [ sec : ammc ] for the infinite - rank amc and ammc , respectively , use an error trap whose size ( in terms of columns _ and _ rows ) grows proportionally with @xmath1 ( or @xmath0 ) . while this is necessary for achieving vanishingly small error probability",
    ", it also implies that these schemes are not suitable for the infinite - packet - length channel ( where @xmath65 but not @xmath0 ) or the infinite - batch - size channel ( where @xmath69 but not @xmath1 ) .    in these situations , the proposed schemes can be adapted by replacing the data matrix and part of the error trap with a _ maximum - rank - distance _ ( mrd ) code @xcite .",
    "consider first an infinite - packet - length amc .",
    "let the transmitted matrix be given by @xmath326 where @xmath327 is a codeword of a matrix code @xmath328 .",
    "if ( column ) error trapping is successful then , under the terminology of @xcite , the decoding problem for @xmath328 amounts to the correction of @xmath2 _ erasures_. it is known that , for @xmath329 , an mrd code @xmath330 with rate @xmath331 can correct exactly @xmath2 erasures ( with zero probability of error ) @xcite .",
    "thus , decoding fails if and only if column trapping fails .",
    "similarly , for an infinite - batch - size amc , let the transmitted matrix be given by @xmath332 where @xmath333 is a codeword of a matrix code @xmath328 .",
    "if ( row ) error trapping is successful then , under the terminology of @xcite , the decoding problem for @xmath328 amounts to the correction of @xmath2 _ deviations_. it is known that , for @xmath334 , an mrd code @xmath335 with rate @xmath336 can correct exactly @xmath2 deviations ( with zero probability of error ) @xcite .",
    "thus , decoding fails if and only if row trapping fails .",
    "finally , for the infinite - packet - length ammc , it is sufficient to prepend to ( [ eq : amc - mrd - error - trap ] ) an identity matrix , i.e. , @xmath337 the same reasoning as for the infinite - packet - length amc applies here , and the decoder in @xcite is also applicable in this case .    for more details on the decoding of an mrd code combined with an error trap ,",
    "we refer the reader to @xcite .",
    "the decoding complexity is in @xmath338 and @xmath339 ( whichever is smaller ) @xcite .    in all cases ,",
    "the schemes have probability of error upper bounded by @xmath340 and therefore are capacity - achieving .",
    "we have considered the problem of reliable communication over certain additive matrix channels inspired by network coding .",
    "these channels provide a reasonable model for both coherent and random network coding systems subject to random packet errors . in particular , for an additive - multiplicative matrix channel",
    ", we have obtained upper and lower bounds on capacity for any channel parameters and asymptotic capacity expressions in the limit of large field size and/or large matrix size ; roughly speaking , we need to use @xmath2 redundant packets in order to be able to correct up to @xmath2 injected error packets .",
    "we have also presented a simple coding scheme that achieves capacity in these limiting cases while requiring a significantly low decoding complexity ; in fact , decoding amounts simply to performing gauss - jordan elimination , which is already the standard decoding procedure for random network coding . compared to previous work on correction of adversarial errors ( where approximately @xmath341 redundant packets are required ) ,",
    "the results of this paper show an improvement of @xmath2 redundant packets that can be used to transport data , if errors occur according to a probabilistic model .",
    "several questions remain open and may serve as an interesting avenue for future research :    * our results for the ammc assume that the transfer matrix @xmath11 is always nonsingular",
    ". it may be useful to consider a model where @xmath342 is a random variable .",
    "note that , in this case , one can not expect to achieve reliable ( and efficient ) communication with a one - shot code , as the channel realization would be unknown at the transmitter .",
    "thus , in order to achieve capacity under such a model ( even with arbitrarily large @xmath24 or @xmath1 ) , it is strictly necessary to consider multi - shot codes . * as pointed out in section  [ ssec : nonuniform - packet - errors ] , our proposed coding scheme may not be even close to optimal when packet errors occur according to a nonuniform probability model . especially in the case of low - weight errors",
    ", it is an important question how to approach capacity with a low - complexity coding scheme",
    ". it might also be interesting to know whether one - shot codes are still useful in this case . *",
    "another important assumption of this paper is the bounded number of @xmath343 packet errors .",
    "what if @xmath2 is unbounded ( although with a low number of errors being more likely than a high number ) ? while the capacity of such a channel may not be too hard to approximate ( given the results of this paper ) , finding a low - complexity coding scheme seems a very challenging problem .",
    "we would like to thank the associate editor and the anonymous reviewers for their helpful comments .",
    "t.  ho , m.  mdard , r.  koetter , d.  r. karger , m.  effros , j.  shi , and b.  leong , `` a random linear network coding approach to multicast , '' _ ieee trans .",
    "inf . theory _ , vol .",
    "52 , no .",
    "44134430 , oct . 2006 .",
    "s.  jaggi , m.  langberg , s.  katti , t.  ho , d.  katabi , m.  mdard , and m.  effros , `` resilient network coding in the presence of byzantine adversaries , '' _ ieee trans .",
    "inf . theory _",
    "54 , no .  6 , pp . 25962603 , jun .",
    "2008 .",
    "danilo silva ( s06 ) received the b.sc .",
    "degree from the federal university of pernambuco , recife , brazil , in 2002 , the m.sc .",
    "degree from the pontifical catholic university of rio de janeiro ( puc - rio ) , rio de janeiro , brazil , in 2005 , and the ph.d .",
    "degree from the university of toronto , toronto , canada , in 2009 , all in electrical engineering .",
    "frank r. kschischang ( s83m91sm00f06 ) received the b.a.sc .",
    "degree ( with honors ) from the university of british columbia , vancouver , bc , canada , in 1985 and the m.a.sc . and ph.d",
    ". degrees from the university of toronto , toronto , on , canada , in 1988 and 1991 , respectively , all in electrical engineering .",
    "he is a professor of electrical and computer engineering and canada research chair in communication algorithms at the university of toronto , where he has been a faculty member since 1991 . during 19971998 , he was a visiting scientist at the massachusetts institute of technology , cambridge , and in 2005 he was a visiting professor at the eth , zrich , switzerland .",
    "his research interests are focused on the area of channel coding techniques .",
    "kschischang was the recipient of the ontario premier s research excellence award . from 1997 to 2000 , he served as an associate editor for coding theory for the ieee transactions on information theory .",
    "he also served as technical program co - chair for the 2004 ieee international symposium on information theory ( isit ) , chicago , il , and as general co - chair for isit 2008 , toronto .",
    "ralf ktter ( s92-m95-sm06-f09 ) received a diploma in electrical engineering from the technical university darmstadt , germany in 1990 and a ph.d .",
    "degree from the department of electrical engineering at linkping university , sweden .    from 1996 to 1997 , he was a visiting scientist at the ibm almaden research laboratory in san jose , ca .",
    "he was a visiting assistant professor at the university of illinois at urbana - champaign and a visiting scientist at cnrs in sophia - antipolis , france , from 1997 to 1998 .",
    "in the years 1999 - 2006 he was member of the faculty of the university of illinois at urbana - champaign , where his research interests included coding and information theory and their application to communication systems . in 2006",
    "he joined the faculty of the technische universitt mnchen , munich , germany , as the head of the institute for communications engineering .",
    "he served as an associate editor for both the ieee transactions on communications and the ieee transactions on information theory .",
    "he received an ibm invention achievement award in 1997 , an nsf career award in 2000 , an ibm partnership award in 2001 , and a 2006 xerox award for faculty research .",
    "he is co - recipient of the 2004 information theory society best paper award , of the 2004 ieee signal processing magazine best paper award , and of the 2009 joint communications society and information theory society best paper award .",
    "he received the vodafone innovationspreis in 2008 ."
  ],
  "abstract_text": [
    "<S> this paper is motivated by the problem of error control in network coding when errors are introduced in a random fashion ( rather than chosen by an adversary ) . </S>",
    "<S> an additive - multiplicative matrix channel is considered as a model for random network coding . </S>",
    "<S> the model assumes that @xmath0 packets of length @xmath1 are transmitted over the network , and up to @xmath2 erroneous packets are randomly chosen and injected into the network . </S>",
    "<S> upper and lower bounds on capacity are obtained for any channel parameters , and asymptotic expressions are provided in the limit of large field or matrix size . </S>",
    "<S> a simple coding scheme is presented that achieves capacity in both limiting cases . </S>",
    "<S> the scheme has decoding complexity @xmath3 and a probability of error that decreases exponentially both in the packet length and in the field size in bits . </S>",
    "<S> extensions of these results for coherent network coding are also presented .    </S>",
    "<S> error correction , error trapping , matrix channels , network coding , one - shot codes , probabilistic error model . </S>"
  ]
}