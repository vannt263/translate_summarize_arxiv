{
  "article_text": [
    "when building devices for quantum information processing one has to take changing environment conditions and device imperfections into account .",
    "it is therefore necessary to include adaptive mechanisms that characterize and calibrate the device from within .",
    "furthermore , it is desirable for these devices to obtain a certain degree of autonomy in maintaining their functional state despite detrimental environment influences , in particular , when they are assembled to a larger quantum information processing infrastructure . in the attempt to miniaturize current implementations of quantum devices",
    ", we will reach the point where these devices will be of microscopic scale and require short reaction times .",
    "for such microscopic systems we can no longer assume that their internal controllers are full - fledged universal computers that can carry out arbitrary programs .",
    "instead , controllers will be small physical systems that are specialized for their respective purpose with a program that emerges from the controller s analog dynamics .    in this paper",
    "we explore the applicability of a controller in form of an intelligent learning agent that has access to a _ projective simulator _  @xcite . within this agent framework ,",
    "the aim is to demonstrate adaptive calibration and compensation strategies against stray external fields when carrying out quantum information tasks .",
    "the agent shall thereby implement a simple form of adaptive error avoidance and implicit parameter estimation .",
    "algorithms from machine learning have been used to find strategies for parameter estimation , and optimal strategies for parameter estimation are known for specific cases , see e.g.  @xcite . here , however , we focus on strategies that arise naturally from the adaptive dynamics of the underlying physical system , for which we choose a projective simulator .",
    "the projective simulator is a platform that has been proposed as a physical model for reinforcement learning  @xcite , and it effectively reproduces input  output  reward correlations from an internal adaptive stochastic process . with the restriction to this particular system , one can not hope for the best possible strategy to emerge while keeping the rules governing the dynamics reasonably simple and computational overhead low .",
    "both requirements are necessary to allow for an actual physical realization . as an additional feature ,",
    "the projective simulator offers a natural route to quantization as indicated in  @xcite and thereby a way to intelligent agents that benefit from internal quantum dynamics , as demonstrated in the reflective quantum projective simulator  @xcite .",
    "agent quantization is not explored further in the present work as we focus on the application of a classical agent to quantum information processing first . for recent comprehensive reviews in the domain of quantum physics and artificial intelligence or machine learning",
    "see @xcite and @xcite .    as illustration of our method of adaptive quantum information processing",
    "we study grover s quantum search algorithm @xcite in the paradigm of measurement - based quantum computation .",
    "grover s algorithm provides a fast way to find a marked item in an unsorted database with @xmath0 elements .",
    "in particular , it provides a quadratic speed - up with @xmath1 database look - ups over a search by means of a classical computer with @xmath2 look - ups .",
    "first proof - of - principle implementations of grover s algorithm with nuclear magnetic resonance techniques @xcite and entangled photons @xcite employed the circuit model of quantum computation , where individual unitary quantum logic gates are applied to a register of qubits to process information .",
    "measurement - based quantum computation ( mbqc ) @xcite is a different paradigm of quantum computation , where the computation is carried out by measuring single qubits of an initially highly entangled resource state  @xcite .",
    "the first experimental demonstration of mbqc in a system of entangled photons  @xcite ( and with trapped ions @xcite ) also demonstrated the grover algorithm in its smallest realization with a database of 4 entries ( 2-qubits ) by using a 4-qubit cluster state as computation resource .",
    "as preparation for the full measurement - based algorithm we first study a basic setting .",
    "we situate a quantum system , a single qubit , in an unknown external magnetic field .",
    "an artificial agent , the controller , is endowed with a projective simulator and the ability to measure the quantum system and thereby prepare quantum states .",
    "we hardwire the learning process , i.e. , the update rule in the reinforcement learning process of the projective simulator , such that the agent effectively carries out the following tasks : ( i ) adapt measurement directions to changes of the external magnetic field , and dynamically improve the sensing resolution .",
    "( ii ) learn to adapt simultaneously for multiple measurement directions needed for general mbqc - algorithms in a feedback scheme .",
    "( iii ) carry out a quantum information task , the grover algorithm  @xcite in the setting of measurement - based quantum computation , with unknown stray magnetic fields .",
    "this provides a completely worked - out example , starting from the physical system that generates the actions of an adaptive `` intelligent '' agent , here a projective simulator , to a controller tailored to a specific quantum information task , e.g.  measurement - based grover s search algorithm .",
    "first , we describe an approach that allows the projective simulator to effectively obtain a notion of the strength of an external magnetic field and hence carry out a primitive form of parameter estimation .",
    "however , there is a conceptual difference between our approach and parameter estimation .",
    "after the agent has learned , the information on the strength of the magnetic field will _ not _ be available as a number that the agent gives as an output . instead ,",
    "this information is only indirectly incorporated into the dynamics and decision patterns of the agent , and it can be exploited to _ do _",
    "certain things that are adapted to the external field .",
    "therefore , we will analyze the learning process of the agent from two different perspectives : from an _ operational _ perspective we characterize how well the agent adapts its actions to the external field , and from an _ informational _ perspective we quantify how much of the information about the external field is really contained in the parameters that define the dynamics of the agent .",
    "we start with a detailed description of the setting , that is , of the agent and its interaction with the measurement apparatus , the dynamics of the projective simulator , and an analysis of the learning process .          in the present setting the magnetic field direction is promised to be fixed along the @xmath3-axis , and the agent needs to estimate its strength @xmath4 .",
    "the following steps are visualized in figure  [ fig : agentsetup ] .",
    "the agent starts by preparing a single qubit in the state @xmath5 , which in the presence of the field evolves according to the hamiltonian @xmath6 , where the frequency @xmath7 is proportional to the magnetic field strength @xmath4 .",
    "after some fixed time interval @xmath8 , the initial state has evolved into @xmath9 up to a global phase , with @xmath10 . estimating the field strength @xmath4 amounts to estimating @xmath11 and obtaining information about the angle @xmath12 between this state and the initial state in the equatorial plane of the bloch sphere . in a linear optics setup",
    "@xcite , the unknown angle @xmath12 would correspond to an unknown phase shifter in the beam line .",
    "the agent measures the qubit in the unknown state @xmath11 in various directions and incorporates the measurement outcomes to change its choice of measurement directions .",
    "the measurements applied by the agent are in general described by povms  @xcite . for simplicity",
    ", we will restrict our analysis to projective measurements .",
    "we shall comment on the general case at the end of the paper .",
    "the challenge is to effectively realize a probability distribution for the unknown angle @xmath12 without explicitly performing computations and analyzing the measurement data .",
    "rather it should emerge dynamically as the result of a feedback loop by reinforcing certain actions on the quantum system .",
    "therefore , we choose an approach where the internals of the agent are wired such that it tries to optimize the direction of a measurement . in the optimal case @xmath11 is the @xmath13 eigenstate of this measurement .",
    "qubit observables whose eigenstates with eigenvalues @xmath14 lie in the equator of the bloch sphere are given by @xmath15 where @xmath16 is of the form with angle @xmath17 .",
    "both eigenstates lie on opposite sides of the equator .",
    "the probability to obtain the measurement outcome @xmath14 is @xmath18 that is , the closer the angles @xmath17 and @xmath12 the higher is the probability to obtain the @xmath13 measurement outcome . to simplify notation",
    "we often consider the projector onto the @xmath13 eigenstate @xmath19 instead of the observable @xmath20 , and measurements of the projector with outcomes 1 and 0 . for qubits , measuring @xmath21 gives the same statistics of measurement outcomes and resulting states as measuring the observable @xmath20 because there is a unique state orthogonal to @xmath16 .",
    "the projective simulator inside the agent employs an adaptive stochastic process that is modeled by a random walk of an excitation in a network of so - called `` clips ''  @xcite . for",
    "now the clip network takes the form of a directed weighted graph depicted in figure  [ fig : network_1to4 ] .",
    "the random walk starts at the only `` percept clip '' , which is excited by an internal trigger of the agent with a time interval @xmath8 after the qubit has been prepared ( cf .",
    "figure  [ fig : agentsetup ] ) .",
    "the excitation propagates in the network according to the weights of the links that connect the percept clip to the action clips .",
    "once the excitation reaches an action clip , the corresponding action is performed and the process inside the projective simulator is finished .",
    "a single action is a measurement of a certain @xmath21 at the qubit . if the measurement outcome is @xmath13 it is fed back as reward to the agent to re - enforce and strengthen the link between the percept clip and the last action clip .",
    "the process is repeated for the next measurement .",
    "the probabilities to select certain measurements , however , change as a result of previous measurement outcomes .",
    "this makes measurements with angles @xmath17 closer to @xmath12 more likely .",
    "these probabilities in effect represent a coarse - grained , discrete probability distribution over angles @xmath12 .",
    "-clip , which then undergoes random walk dynamics according to the weights of the links .",
    "the action clip where the excitation arrives determines the measurement direction . ]        in detail , each link in the clip network carries a weight @xmath22 .",
    "the probability to jump from the percept clip `` @xmath23 '' to the action clip corresponding to @xmath21 is given by the normalized weight of all edges from the percept clip , that is , @xmath24 at the beginning of the learning process all weights are initialized with @xmath25 . after the measurement of @xmath21 in the @xmath26-th round , the measurement outcome ( 0 or 1 ) is rescaled by a factor @xmath27 and fed back into the projective simulator as a reward @xmath28 to the transition with weight @xmath22 . regardless of whether or not a transition has been taken , all weights are damped by a small amount with rate @xmath29 .",
    "after the @xmath26-th round , in which @xmath17 was the measurement angle , all weights are changed according to the following update rule :",
    "@xmath30 as a result , the projective simulator converges to a state ( set of @xmath31-values ) that increases the chances of obtaining @xmath13 measurement outcomes and thereby increases the probability to measure in directions close to @xmath12 . from the perspective of the projective simulator",
    "only an outcome @xmath13 denotes success because the action that led to this outcome will be reinforced .",
    "this `` subjective '' success probability is @xmath32 an action that leads to a reward ( measurement result @xmath13 ) is also the correct action from an operational point of view .",
    "the transition probabilities @xmath33 provide an internal representation of a discretized probability distribution for the angle @xmath12 .",
    "the change of @xmath34 as a function of the number of rounds ( measurements on the quibt ) is depicted in figure  [ fig : learning_single_phi ] for several examples of @xmath12 .",
    "the results in figure  [ fig : learning_single_phi ] show that the agents learns to obtain rewards more often and thus obtains information about the state @xmath11 and thereby about  @xmath4 .",
    "in our example we start with 4 projectors at angles every @xmath35 , which corresponds to the projectors onto the eigenstates of the observables that are given by the pauli matrices @xmath36 and @xmath37 .",
    "if @xmath38 , measurements of @xmath39 will always give outcome @xmath13 and hence be rewarded .",
    "the two adjacent projectors at @xmath40 and @xmath41 are rewarded in half of the measurements , and measurements in the direction @xmath42 are never rewarded .",
    "in this situation the projective simulator builds a strong link to @xmath39 , somewhat less strong links to @xmath43 and @xmath44 and leaves the link for @xmath45 at its initial value . the coarse - grained discrete probability distribution for @xmath12 is consequently peaked at @xmath38 and  within statistical fluctuations ",
    "symmetric around this direction ( figure  [ fig : learning_single_phi ] top left inset ) .",
    "if @xmath12 is between two of the projectors , say @xmath46 , measurements of @xmath39 and @xmath43 will only be rewarded with only @xmath47 probability , and measurements of the opposite projectors with @xmath48 probability .",
    "the distribution of measurement probabilities will also be symmetric around the direction @xmath49 but less pronounced as shown by a broader distribution in figure  [ fig : learning_single_phi ] ( bottom left inset ) . a broad distribution for measuring in the direction @xmath17 results in a lower success probability for angles that have a large distance to all projectors , e.g. , @xmath50 .",
    "at this point a smaller damping rate @xmath29 and a larger multiplier of the rewards @xmath27 both lead to a larger value of rewarded transitions in the steady - state and hence to a larger success probability and a probability distribution that is more peaked . at the same time",
    "increasing both @xmath27 and @xmath29 speeds - up the learning process leading to learning curves with a steeper initial rise .",
    "note , however , that extremal cases with too large rewards or too weak damping favor situations in which the agent prefers actions that just by luck led to a reward in the past although they are not highly rewarded on average .",
    "un - learning such an initial `` misunderstanding '' and building a probability distribution that reflects the actual probabilities of being rewarded may take a long time .",
    "this aspect leads to larger fluctuations in the success probability of an ensemble of agents and a slower final convergence .    _ asymptotic success probability._for the asymptotes of the success probability we can find a first - order approximation by assuming a steady state of the transition probabilities @xmath51 and the respective @xmath31-values .",
    "the resulting steady - state success probability is @xmath52 .",
    "when coarse - graining over many measurements the time average of the reward for each action is given by @xmath53 , and the steady - state probability to measure in direction @xmath17 is @xmath54 . with these assumptions",
    "the update rule turns into a set of coupled equations for the steady - state values @xmath55 ,",
    "@xmath56 in which the loss terms given by the damping @xmath29 and the gain terms given by the time - averaged reward are in equilibrium .",
    "this set of nonlinear equations can be solved numerically and yields a very good approximation for the ensemble average as seen in figure  [ fig : learning_single_phi ] .",
    "the asymptotic value obtained in this approximation only depends on the ratio @xmath57 .    :",
    "states are summarized as the position of the resulting vectors @xmath58 for each agent @xmath59 in the complex plane ( equatorial plane ) .",
    "the degeneracy with respect to the expected reward for @xmath60 provides a manifold of equally successful states , which is populated by the ensemble .",
    "blue dot on the unit circle gives the angle of the ensemble average , the black dot is the angle @xmath12 . ]     ( left ) the action pair @xmath39 and @xmath43 is degenerate with respect to the expected reward . for @xmath61 ( middle ) , i.e. , not exactly between two projectors , the agent measures more often into the direction @xmath62 .",
    "fluctuations in the measurement probabilities do not necessarily show in the success probability . for comparison ,",
    "the ensemble averages of 1000 agents after 1000 measurements are given as dashed lines .",
    "larger rewards @xmath27 and damping @xmath29 ( both rescaled by a factor 10 ) decrease the timescale of the fluctuations while maintaining approximately the same time average ( right ) .",
    "the agent jumps between different preferred action and stays for extended times . ]    _ time average vs.  ensemble average._the fluctuations of the probabilities to choose certain actions ( see insets in figure  [ fig : learning_single_phi ] ) show that even after 1000 iterations of the update rule not all agents have converged to a single state ( figure  [ fig : ensembleaverage ] ) .",
    "many steady states occur if there is a whole manifold that is rewarded equally , that is , when two or more actions have the same expected reward .",
    "for example , for @xmath60 both actions @xmath39 and @xmath43 have equal chances of being rewarded and thus the there is no preference of either action as long as one of them is carried out .",
    "actions that have the same expected reward span a subspace for which the sum of the probabilities for doing these actions is approximately constant in the steady state , however , their relative ratio is not .",
    "the states of the whole ensemble of agents fills this degenerate subspace of action probabilities .",
    "the ensemble average yields an approximation of @xmath12 .",
    "although the ensemble has learned , i.e. , the success probability has converged , the dynamics of each individual is not necessarily converged to a single state where it remains . in the course of time the state of a single agent explores the whole degenerate reward manifold while keeping the success probability constant as we numerically illustrate in figure  [ fig : timeaverage ] .",
    "we find that the time average of a single agent for long times equals the ensemble average because the state of the single agent assumes all the different steady states that an ensemble produced after short time as in figure  [ fig : ensembleaverage ] .",
    "hence , to obtain an ensemble average a snapshot after a relatively short time is sufficient .",
    "however , if there is a degenerate space in the reward scheme , the state of a single agent at a fixed time gives only an imprecise estimate of @xmath12 , and even its time average does when considered only for a short time .",
    "a larger damping parameter @xmath29 and higher rewards @xmath27 facilitate a faster exploration of the degenerate reward manifold and thus provide a better time average for a single agent for shorter times .",
    "for @xmath60 in figure  [ fig : timeaverage](right ) , the agent selects either @xmath62 or @xmath35 for an extended time and then suddenly switches between these equally rewarded choices .",
    "this jumping behavior occurs for large reward and damping , whereas for smaller values ( left ) also equal probabilities occur for longer durations .",
    "the way that the agent uses the rewards to change its actions to do measurements more often along angles that are close to @xmath12 , is a way of representing information about @xmath12 .",
    "we regard this probability distribution of actions along the discrete set of angles as a probability distribution of @xmath12 @xcite , and compare it to standard computational analysis procedures employed in state and parameter estimation . by its actions and the returned rewards the agent effectively samples the reward distribution @xmath63 .",
    "the same data , namely the measurement direction and outcome , however , can also be used in a bayesian update rule to explicitly build a probability distribution @xmath64 , or the data can be used to reconstruct the state @xmath11 via state tomography .",
    "we compare the angular distribution of actions that the agent maintains to the angular distribution that a bayesian update would produce , and also to a simple state tomography by estimating expectation values from the same measurement data .",
    "a simple form of state tomography can be done by calculating the expectation values @xmath65 and @xmath66 from the measurement results of the four projective measurements .",
    "together with the initial assumption @xmath67 , these expectation values give an approximation of the state s bloch vector .",
    "our four measurement directions @xmath68 give the same measurements as the pauli matrices with expectation values @xmath69 where expectation values of the observables can be related to those of the projectors by @xmath70 for a total of @xmath71 measurements , of which @xmath72 are done in direction @xmath17 , with individual measurement outcomes @xmath73 for observable @xmath20 , the expectation values can be approximated with the mean @xmath74 the resulting bloch vector with coordinates @xmath75 provides an angle with the @xmath76-axis and thereby an estimate of  @xmath12 .    in a bayesian analysis",
    ", we update an initially flat prior distribution @xmath77 with the information obtained from each measurement . after each measurement , the distribution is updated with result @xmath78 for measurement in direction @xmath79 , e.g. , for the first update @xmath80 where we include the knowledge of quantum mechanics and the statistics of measurement outcomes for the underlying system with @xmath81 given by .",
    "after @xmath82 measurements the resulting probability distribution is @xmath83 with normalization @xmath84 . for an efficient update and a compact representation of the conditioned probability distribution",
    "we expand it in a fourier series , which has at most @xmath82 higher harmonics , and construct an recursive update rule for the expansion coefficients following the approach in @xcite for parameter estimation with a single fixed observable but variable time delays . for our choice of measurement directions ,",
    "with @xmath17 being a multiple of @xmath35 , the fourier expansion generally contains @xmath85 and @xmath86 terms .",
    "the recursive update rules for the expansion coefficients are given in the appendix .    .",
    "the data points represent the measurement data of 10 agents from 1500 measurements each , and they give an estimate of the angle @xmath12 as the mean of the 10 distributions .",
    "the 10 data points are supplemented by a black error bar , which indicates their circular mean and circular standard deviation . for the projective simulator ,",
    "the blue error bar indicates the circular mean and circular standard deviation of the discrete probability distribution over the 4 actions , after averaging over all 10 agents . for most examples of @xmath12",
    "the projective simulators generate a distribution of the mean angles that coincide with @xmath12 except for @xmath61 , where similar as in figure  [ fig : ensembleaverage ] a bias towards the nearest available projector ( @xmath62 ) occurs . ]    to compare the estimates of @xmath12 by these three approaches , we fix the angle @xmath12 , and let 10 agents with a projective simulator do 1500 measurements each and according to the dynamics arising from using the projective simulator . after these 1500 measurement each agent has built a probability distribution of actions @xmath33 , we take the mean of each distribution as the estimate of @xmath12 .",
    "figure  [ fig : distributioncomparison ] shows these estimates as the blue data points . because the probability distributions for @xmath12 that we obtain from the @xmath33 have support only on 4 angles , which are uniformly and discretely spaced on the circle , each distribution has a large variance .",
    "we average the distributions of all 10 agents and give the mean and circular standard deviation of the resulting distribution as the blue error bar in figure  [ fig : distributioncomparison ] for comparison .",
    "clearly , when @xmath12 is close to one of the possible choices of @xmath17 , the projective simulator captures @xmath12 accurately , but for values of @xmath61 or @xmath49 the estimates are biased towards one of the @xmath17 as in figure  [ fig : ensembleaverage ] . for @xmath60 the angular means",
    "are widely spread , and their distribution has a large variance , which is reminiscent of the distributions given in the insets in figure  [ fig : learning_single_phi ] and the distribution of means of a large ensemble in figure  [ fig : ensembleaverage ] .",
    "the estimates for @xmath12 obtained from the expectation values of the pauli matrices , i.e. , the simple state tomography , are calculated from the same measurement record for each agent and are given by the orange data points in figure  [ fig : distributioncomparison ] . for the bayesian update scheme",
    ", we construct the conditional probability distributions for @xmath12 , again from the same measurement record that the projective simulator generated .",
    "all of the resulting distributions assume an approximate gaussian shape with a narrow peak ( @xmath87 ) .",
    "the means are given as red data points in figure  [ fig : distributioncomparison ] .",
    "both approaches can estimate @xmath12 correctly within the error bars .",
    "surprisingly , for @xmath12 along one of the @xmath17 , the estimates from the expectation values spread more than in the other two approaches .",
    "the reason is that in these cases the projective simulator samples most of measurements along a single direction and only few for the other observable , which causes a rather large uncertainty in one of the coordinates .",
    "although state tomography and bayesian estimation perform generally equally good or better than the projective simulator , the big conceptual difference between these approaches is that very little knowledge of quantum physics and measurement statistics is build into the projective simulator .",
    "the projective simulator does not assume that the rewards originate from measurement probabilities of a quantum state and , therefore , it is `` model free '' .",
    "the update rule causes a learning dynamics that drive the agent to measure more often into directions that give a + 1 measurement outcome and thereby implicitly align measurement directions with @xmath12 .",
    "even when no optimal measurement direction is available the agent learns how to deal with a system such that reward is most likely to occur . in principle",
    ", it could even adapt to artificial situations , where measurements along the @xmath76-axis always give a @xmath13 outcome and measurements along @xmath88 always give the outcome @xmath89 , something which can not be explained by measuring a qubit in a defined fixed state .",
    "therefore , it is not surprising that methods that make use of additional information , namely measurement probabilities predicted by quantum physics , can extract more information about @xmath12 from the measurement results . given that an agent with a projective simulator lacks this additional information it does comparably well , and , conceivably , it can be improved by changing the update rule to incorporate more knowledge about the underlying quantum physics .",
    "for example , a positive measurement result and reward in one direction can be combined with a negative reward into the opposite direction , or , for each measurement result the reward is distributed according to how close all potential actions are to the rewarded one .",
    "an important feature of the projective simulator is its ability to forget and thus to adapt to a changed situation .",
    "this ability distinguishes the present setting from schemes of parameter estimation , where the unknown parameter is assumed to be constant .",
    "for example , for a changing parameter standard bayesian updating can not be applied because past information needs to be disregarded and only recent information should be considered for estimating the current parameter .",
    "the projective simulator , in contrast , keeps track of an integrated average of past rewards for each action and is endowed with an element , the damping quantified by @xmath29 , to forget these rewards .",
    "the agent has the ability to completely change its behavior regardless of what has been rewarded earlier and irrespective of its earlier state .",
    "we shall consider two of such relearning scenarios in the following .",
    "we analyze the relearning by means of two quantities , the asymptotic success probability and the learning time it takes the agent to adapt .     leading to a shift from @xmath12 to @xmath90 .",
    "an ensemble of 1000 agents first learns with @xmath12 for @xmath91 measurements , after which the angle changes to @xmath90 for another @xmath91 measurements , @xmath92 , @xmath93 .",
    "* top : * examples of learning curves for 4 different switches . @xmath12 and @xmath90 are given in the same color as the corresponding learning curve ( @xmath94 for blue and @xmath95 for the other examples ) . *",
    "middle : * the asymptotic success probability ( analytically obtained ) shifts due to the change in angles . *",
    "bottom : * the relearning time @xmath96 to reach 90% of the asymptotic success probability after the field has changed also gives rise to a periodically repeating pattern and shows a structure commensurate with the choice of projectors in intervals of @xmath35 . ]    _ relearning after a switched field._ changes of @xmath4 result in a different @xmath12 and require the agent to adapt and relearn .",
    "for a single sudden change in @xmath4 the angle @xmath12 changes only once at a certain time to a new angle @xmath90 . depending on the values of @xmath12 and @xmath90 the agents shows a rich landscape of relearning patterns as illustrated in figure  [ fig : relearning ] .    after the switch the asymptotic success probability is always that of the new @xmath90 and may lie above or below the success probability of the old @xmath12 ( figure  [ fig : relearning ] top ) .",
    "the change in @xmath97 is illustrated in figure  [ fig : relearning ] ( middle ) .",
    "the sudden drop or increase in success right after the change of the angle and the time to reach a success probability depends strongly on the relation of the two angles and how much of the internal state ( @xmath31-values ) needs to be changed to reach the new state .",
    "these effects in the relearning time appear in addition to the known effects of changing the reward scaling @xmath27 and damping rate @xmath29  @xcite .",
    "a summary of the relearning times and change in asymptotic efficiencies is given in figure  [ fig : relearning ] ( middle and bottom ) .    _ time - dependent fields._an important feature for applications is the agent s ability to adapt its actions to slowly changing external fields .",
    "an agent s state is the result of a dynamical equilibrium between rewarded actions in the past and forgetting this information on a time scale given by @xmath29 .",
    "therefore , the speed at which an agent can adapt is limited by the speed with which it can modify its internal state",
    ". the agent can adapt to a change in the reward landscape caused by a changing field as long as it has enough time to sample the modified reward landscape and modify its internal state accordingly , which depends on @xmath27 and the timescale given by  @xmath29 .",
    "figure  [ fig : timedependentfields ] shows two examples .",
    "the first example ( left ) is a setting with a fast oscillating field , i.e. , one with @xmath98 as a function of the measurement round  @xmath26 , where only the time average is learned because the agent effectively takes samples from the entire reward landscape .",
    "the state vector converges to angle @xmath38 and reaches almost unit length .",
    "the second example ( right ) shows a setting with a linearly increasing magnetic field , giving rise to @xmath99 . as @xmath12 moves anticlockwise on the unit circle as a function of the measurement round",
    ", the agent can keep up as quantified by @xmath100 with a state trajectory that also moves counterclockwise albeit with a slight delay and the length of the state vector is longer , i.e. , the field is learned better , for a slower rate of change .     as averaged over an ensemble of 1000 agents .",
    "the state vector of the ensemble starts in the origin . *",
    "left : * for fast oscillating fields @xmath101 with @xmath102 ( red ) and @xmath103 ( blue ) the agents adapt to the average angle @xmath38 over 5000 measurements .",
    "data points are explicitly indicated for the first 20 measurements and joined by a line . *",
    "right : * linearly drifting fields @xmath104 can be learned by the agent the better the slower they change on the timescale of the learning time : @xmath105 ( blue ) shown for 10000 measurements , and @xmath106 ( red ) , @xmath107 ( orange ) shown for 4000 measurements each .",
    "the ensemble follows the field and the state trajectory converges to a limiting cycle . ]",
    "the choice of projectors that the agent can measure affects the agent s success in two ways : on one hand it fixes the available angles and thereby ability to measure the correct angle .",
    "a finer grained sample of measurement angles is beneficial because it will contain an angle that is closer to the actual angle and allow for almost perfect measurements .",
    "it also avoids efficiency minima due to the coarse - graining as they appear in figure  [ fig : learning_single_phi ] ( top , right ) . a finer resolution of measurement angles , however , will introduce many angles that are almost equally successful and are hard to distinguish by their average reward .",
    "on the other hand , the choice of measurement angles fixes the discrete support on which the probability distribution for @xmath12 can be built , which contains the information on the angle @xmath12 .",
    "a drawback of a coarse - grained support is the arising large variance in the distribution .",
    "a fine - grained support , however , needs lots of sampling to evaluate each individual point in the distribution .",
    "measurement directions initially available to an agent ( blue ) , uniformly spaced on the circle , and with additional bisection compositions ( red ) .",
    "data points are angular averages , and the vertical region denotes the maximum and minimum of the success probability .",
    "these data summarize the angular dependence of @xmath97 as depicted for several examples in the inset .",
    "the reason for a decrease of @xmath97 with an increase in measurement directions is due to damping . ]",
    "as there are advantages and disadvantages to the number of measurement directions , we ask if there is an optimal number of fixed projectors . in order to distinguish directions on a circle ,",
    "at least three directions are needed .",
    "( for just two directions that are equally successful , it is not possible to decide which of the two angles between those two directions is the correct one . )",
    "we have calculated the asymptotic success probability for an agent that has access to @xmath108 measurement angles that are equidistantly spaced on the unit circle , i.e. , it can measure @xmath109 observables of the form with two eigenstates each that are opposite on the equator of the bloch sphere .",
    "the angular dependence of the success probability in figure  [ fig : initialprojectorscomposition ] ( inset ) shows maxima at angles that are measurement directions and minima between two neighboring angles . as more",
    "observables are added , the worst cases in between two neighboring projectors improve , but at the same time also the optimal cases decrease because the optimal angle is not chosen as frequently due to slightly better neighboring angles that are also rewarded more often .",
    "the success probability averaged over all angles first increases and then decreases as summarized in figure  [ fig : initialprojectorscomposition ] . in the limit of large @xmath109",
    "the success probability converges to 50% because of the constant damping @xmath29 . in the example case with @xmath92 and @xmath93",
    "we can give these recommendations : for optimizing the best case , the number of 2 projectors is optimal , for the best worst case success probability , 8 projectors are best , and for the best average success probability 6 projectors are the best initial choice .    a strategy to mitigate the decrease in overall efficiency for a more refined angular resolution is _ composition _ , which is one of the original features of projective simulation  @xcite . with composition",
    "the projective simulator is endowed with the ability to generate new clips based on the composition of already existing ones . for parameter estimation",
    "the projective simulator can insert new clips with new measurement directions only where additional resolution is needed .",
    "the composition mechanism is an additional dynamical element in the projective simulator .",
    "based on the state of the projective simulator it is triggered and inserts a new clip based on existing ones .",
    "these new elements , i.e. , the trigger mechanism , constructing the new clip , and how the new clip is inserted into the network must be specified and leave room for arbitrarily complicated rules .",
    "we will restrict to the simplest mechanisms , which will also draw some intuition from actual conceivable physical dynamics .",
    "_ bisecting composition._the first composition mechanism simply operates by bisection and refining the resolution in the relevant regions . after the agent has learned with its initial set of projectors , the two actions clips with the largest @xmath31-values are selected and used to compose a new clip between the two . in situations with angles @xmath60 or @xmath110 the action clips with @xmath62 and @xmath35",
    "will have the largest @xmath31-values and give rise to the creation of a new clip with @xmath50 , which improves the resolution of the discretization in the first quadrant . the success probability before and after one such composition is depicted in the inset in figure  [ fig : initialprojectorscomposition ] as light blue and red curve , respectively .",
    "for the angle @xmath60 the success probability is increased from a minimum of 83.4% to a maximum with 96.2% without adding unnecessary projectors in the remaining quadrants , which would lower @xmath97 to 93.2% of the curve with 8 projectors .",
    "when always adding a single additional angle in the middle of the quadrant in which @xmath12 lies , worst case scenarios for @xmath97 appear only for angles like @xmath110 and @xmath111 with 93.32% , which still is a slight improvement over the coarse graining with only 4 projectors ( 93.26% ) .",
    "for @xmath61 the composition at @xmath50 is helpful but suboptimal . for angles at the projectors , e.g. @xmath38 ,",
    "an additional composition is harmful and decreases @xmath97 from 97.1% to 96.1% .",
    "a single composition that doubles the angular resolution in one quadrant is qualitatively similar to 8 initial measurement angles , but with a higher success probability .",
    "a second composition step that adds another projector with an angle of odd multiples of @xmath110 , effectively reproduces the resolution of 16 initial angles but only in one octant of the unit circle .",
    "it improves the worst cases at the cost of a slightly reduced overall success probability .",
    "even more bisections will further increase the angular resolution but reduce the overall success to the point that they are counterproductive .",
    "although a bisecting composition is very simple approach , it provides the advantage of a larger number in initial projectors while avoiding a large penalty in overall efficiency due to a large action space with the same parameters .    _ composition with the glow mechanism._the second mechanism departs from the strict bisection strategy of the first mechanism .",
    "the agent reaches an optimal success probability if it can measure along the direction @xmath112 .",
    "the bisection strategy only approximates @xmath12 and sometimes introduces unnecessarily many angles , e.g. , for @xmath61 the additional angle @xmath50 has to be built first .",
    "we overcome this disadvantage by a better use of the information provided by the measurement results to estimate which new projector angle should be inserted as addition action .",
    "we employ a variant of the `` edge glow mechanism ''  @xcite to compose a single new action clip in the following way .",
    "we assign a second degree of freedom to each edge called `` glow '' and denote it by @xmath113 . instead of updating the @xmath31-values with the reward according to , we first accumulate rewards in the @xmath113 according to the following update rule : @xmath114 with initial values @xmath115 .",
    "the change in the update rule for @xmath31 effectively amounts to setting @xmath116 and @xmath117 .",
    "the behavior of the agent remains unchanged as the @xmath31-values remain at their initial values @xmath25 , i.e. , the agent measures equally often in all available directions .",
    "however , since there is no bias in the frequency of available measurement direction , the accumulated rewards in the respective @xmath113 provide a measure of the average reward for each direction .",
    "once the agent sampled enough measurement results , e.g. , when the first @xmath113 surpasses the threshold @xmath118 , a new action clip is composed and inserted into the projective simulator . the new measurement direction @xmath119 is composed from all @xmath17 and weighted by the @xmath113 : @xmath120 and we set the new @xmath121 .    in order to prevent that a direction is inserted that is already present , the agent first checks that @xmath119 is sufficiently different from all already existing @xmath17 , e.g. , by inserting @xmath119 only if it differs from @xmath17 by more than @xmath122 circular standard deviations of the circular distribution given by the @xmath113 .",
    "if @xmath119 is too close to one @xmath17 , the @xmath22 of this @xmath17 is instead strengthened and set equal to the sum of all @xmath113 , and no new clip is inserted . after the composition , we continue with the usual update rule for the @xmath31-values .    .",
    "the @xmath31-values are only updated with @xmath27 and @xmath29 after the composition . for @xmath12 coinciding with an existing @xmath17 only 4 agents compose a new angle @xmath119 , which is more than @xmath123 away from an existing @xmath17 , whereas all agents compose angles for the other examples of @xmath12 .",
    "the position of the step depends on the choice of the threshold for composition , here @xmath124 , which is chosen for large statistics but can be decreased without much penalty in the asymptotic efficiency albeit at the cost of slightly less accurate composed angles . ]",
    "the learning curves for the this form of glow composition are shown in figure  [ fig : glow ] . starting with 4 angles and a @xmath118 ,",
    "at least 2000 measurements have to be done on average before the first composition can occur .",
    "this threshold can be decreased leading to a faster composition , albeit at worse statistics , which result in inaccuracies of the composed angles .",
    "inaccurate compositions , however , impact the success probability only to a small extent because it decreases with the cosine of the angular difference between @xmath12 and the composed angle .    in the direction of an existing angle , e.g. , @xmath38 or @xmath35 ,",
    "the first amplifications of the respective @xmath17 occurs starting with 2000 measurements . for the direction @xmath60 ,",
    "more measurement need to be done on average to reach @xmath125 or @xmath126 because these direction are not rewarded with certainty , and composition occurs on average later , with @xmath60 being one of the four latest instances . after the composition",
    "the success probability jumps from 50% to about 99% .",
    "since the newly set @xmath31-value for the best measurement direction is larger than the steady - state value for our choice of @xmath92 and @xmath93 , the success probability decreases slightly to approach @xmath97 from above .    in an ensemble of 1000 agents only 4 compose an angle when @xmath38 or @xmath35 , whereas all do a composition for @xmath61 or @xmath49 . in our numerical experiment ,",
    "the distribution of composed angles @xmath119 is sharply peaked around @xmath12 with a @xmath87 .    by using the glow mechanism to obtain an effective average reward for each measurement direction , and then composing a mean angle from the reward distribution , the agent effectively creates a weighted sum of directions .",
    "it thereby embodies a method similar to the estimation of expectation values done in state tomography .",
    "so far we have demonstrated how an agent equipped with a suitable projective simulator can align a single measurement direction , e.g. , @xmath36 for the state @xmath127 , with an initially unknown state @xmath11 , which emerged from @xmath127 due to a magnetic field . since one of the aims is to employ the agent as a means to carry out measurement - based quantum computation ( mbqc )  @xcite in an unknown external field , all measurement directions that are required to run a specific algorithm in mbqc need to be adapted to this unknown stray field .",
    "we therefore need to extend the projective simulator to learn several measurement directions , which shall be given as the respective input .",
    "ultimately , the agent would translate the measurement directions necessary for the algorithm to the reference frame that rotates due to the magnetic field .",
    "we modify the inital agent setup depicted in figure  [ fig : agentsetup ] in the following way .",
    "the step that prepares the defined initial state @xmath127 is removed and the qubit is simply left in the state that is prepared by the previous measurement .",
    "the projective simulator now receives as an input not just a trigger event , which activated the @xmath23-clip , but now it receives the previous measurement direction and the obtained measurement result as a percept .",
    "the initial state and percept can be chosen arbitrarily , e.g. , at random , as they do not matter in the subsequent feedback loop .     and obtainable measurement outcome ( 0 or 1 ) .",
    "percepts that correspond to the same state prepared by the previous measurement are colored equally . ]    for the new scheme , we also extend the clip network of the projective simulator to 8 percept clips , which represent all combinations of previous measurement direction and obtained reward , as depicted in figure  [ fig : network_8to4 ] .",
    "effectively , the extended clip network consists of 8 copies of the previous simple clip network , which are activated according to the actions and results of the previous time step .",
    "the agent enters a feedback cycle , where measured directions and outcomes are fed back to the agent .",
    "the information about which state preparation method was used is available to the agent as percept , and thereby it indirectly receives a hint about which state has been prepared . given each prepared state , which then evolves to acquire an additional shift in the angle by @xmath12 , the agent learns which measurement direction most likely matches this rotated initial state .",
    "to give an example , consider the test qubit in the initial state @xmath128 , which evolves into @xmath11 .",
    "the agent measures this state , say along @xmath40 , and obtains result @xmath129 .",
    "it thereby prepares the test qubit in state @xmath130 , which again evolves for time @xmath8 into @xmath131 for the next measurement .",
    "this next measurement is chosen according to the @xmath31-values of edges originating from the percept clip `` @xmath132 '' to each of the four actions .    .",
    "plotted are conditioned success probability , i.e. , given a percept , what is the probability of obtaining a + 1 measurement outcome , where each curve corresponds to one percept , solid lines represent outcome-@xmath129 preparation methods , dashed lines those for outcome  @xmath133 .",
    "color codings are the same as for the clip network .",
    "curves are averages over 100 agents , with @xmath92 and @xmath93 . for the last time",
    "step the clip network with @xmath31-values encoded in the thickness of the edges are given in the inset .",
    "colors and clips match those of figure  [ fig : network_8to4 ] . ]",
    "the clip network is now much larger than before and the agent needs more measurements to update all the connections until the @xmath31-values converge into those of the steady state .",
    "naively , we can expect an 8-fold increase , however , since the agent converges to a state in which measurements that give outcomes @xmath129 are preferred , learning the right measurements for a outcome-@xmath133 preparation is delayed .",
    "this learning behavior is shown in figure  [ fig : completepslearning ] , where outcome-@xmath129 preparations converge early and outcome-@xmath133 preparations later , which in turn also delays the overall convergence .",
    "naturally , the training of the whole network is faster in situations where the @xmath133 outcomes occur more often , e.g. , for @xmath60 , or in situations that lead to different measurement directions , e.g. , for @xmath134 .",
    "as the agent encounters situations with different percepts , the number of time steps in between two successive activations of the same percept is now increased on average .",
    "this leads to a qualitative and quantitative change in the learning curves as compared to the previous simple agent with only one percept .",
    "the number of times that the damping reduces the @xmath31-value of each edge would increase and lead to a reduced efficiency , because the agent forgets too quickly in between rewards . to maintain high @xmath31-values for rewarded transitions we could adjust @xmath29 to a lower value , but we choose to simply restrict the application of the update rule , and the application of the damping in particular , to a subgraph of the clip network , namely , only those edges that are connected to the activated percept clip .",
    "thereby we maintain the quantitative behavior of the simple clip network used in the previous sections .",
    "percepts give the preparation procedure of the test qubit and thereby effectively encode information about which state has been prepared .",
    "a closer inspection reveals that each state is represented twice because is can be prepared in two ways , e.g. , @xmath127 can be prepared by a measurement of @xmath39 with outcome 1 or by @xmath45 with outcome 0 .",
    "preparation procedures that result in the same prepared states are highlighted with the same color of the percept clip in figures  [ fig : network_8to4 ] and [ fig : completepslearning ] .",
    "this redundancy increases the learning times because the same behavior has to be learned twice .",
    "the clip network could be optimized with an additional intermediate layer that first maps preparation methods to states , which may be learned first without a stray field , and then the prepared states to best measurement directions in a stray field .",
    "once the agent has adapted its measurement directions to the unknown external field with a test qubit , it can be used as a translator between intended measurement directions and their corresponding directions in the rotated reference frame .",
    "this application of a trained agent works as follows . after a training period , we fix all the @xmath31-values .",
    "instead on the test qubit , the agent now acts on the qubit that needs to be measured along a certain direction according to a mbqc scheme , for example .",
    "we then excite a percept of the agent that corresponds to the direction of the intended measurement direction in zero field .",
    "the agent then chooses most likely the measurement direction that corresponds to this measurement in the rotated frame , i.e. , the measurement that takes the field into account .",
    "we first briefly repeat the mbqc variant of the grover search algorithm for a database with 4 elements  @xcite and adapt it to our notation and use of projective measurements @xmath21 .",
    "the initial resource state is a cluster state of 4 qubits in ring form , i.e. , starting from the state @xmath135 we apply a controlled phase gate between the qubit pairs 12 , 23 , 34 , 41 , and obtain @xmath136 a database with 4 entries ( i.e. , with elements @xmath137 , @xmath138 , @xmath139 , and @xmath140 ) only requires a single grover step to find the marked element .",
    "the algorithm starts by doing this one necessary query to the database and thereby marks the database entry that is to be found .",
    "a measurement of the projectors @xmath39 or @xmath45 on qubits 1 and 4 realizes the specific database , where each pair of measurement directions @xmath137 , @xmath141 , @xmath142 , and @xmath143 corresponds to marking the database element @xmath137 , @xmath138 , @xmath139 , and @xmath140 , respectively . for each of the two measurements of @xmath39 or @xmath45 both measurement results @xmath144 or @xmath129 appear with probability @xmath145 .",
    "therefore , the results alone do not allow us to infer the measurement directions and thereby the marked element . in the problem setting of the algorithm",
    "the choice of measurement directions is hidden . only from the measurement results of qubits 1 and 4 , and from the measurements done on the remaining two qubits",
    ", we should infer the marked element . on the remaining qubits we therefore measure the observable @xmath39 ,",
    "whose measurement outcome depends on the measurement directions on qubits 1 and 4 , and is correlated to the previous two outcomes .",
    "finally , the calculation of @xmath146 , i.e. , addition of the measurement outcomes modulo 2 , reveals the two bits of the marked element with certainty .",
    "although , at the present point the mbqc version of grover s algorithm appears to merely uncover ( anti-)correlations between measurement directions , there is an explicit mapping between the quantum circuit of gover s algorithm on one hand , and the circuit for creating and measuring the cluster state on the other  @xcite .     along the equator of the bloch sphere .",
    "data is obtained in independent runs with marked element @xmath137 for all fields giving rise to angles @xmath12 between 0 and @xmath147 in steps of @xmath148 .",
    "noisy data is the fraction of an ensemble of agents that identifies the marked element correctly when performing all four measurements in grover s algorithm .",
    "red ( analytical ) and orange ( numerical , 3000 agents ) curves give the success of the grover search ( all four measurements ) without taking into account the field in the measurement direction .",
    "the light blue curve gives the success for an ensemble of 1000 agents that each have a perfectly trained projective simulator with 4 measurement directions , which has learned the external magnetic field before doing the measurements for the grover search algorithm .",
    "the dark blue curve is an ensemble of 1000 agents that employs the glow mechanism to build a measurement direction that is adapted to the external magnetic field before using it to perform the grover search . ]",
    "if the initial state is placed in an unknown external field pointing along the @xmath3-direction , the state @xmath149 is transformed into @xmath150 with the local unitary rotations @xmath151 .",
    "if we recall that @xmath152 it is straightforward to see that the measurement protocol of the grover algorithm will no longer give the correct marked element because the external field effectively shifts the measurement directions by the angle @xmath153 with respect to the original measurement directions . as a result the probability to identify the correct marked element , i.e. , the success probability of the algorithm , is periodically modulated by @xmath12 and a straightforward calculation gives @xmath154 for @xmath12 being a multiple of @xmath155 the algorithm works perfectly because the local rotations align the qubits reference frames again with the @xmath76-axis .",
    "the grover algorithm is invariant under the inversion of all measurement directions ( i.e. , changing all directions @xmath133 to @xmath155 and vice versa ) . in the worst case , for @xmath12 being an odd multiple of @xmath35 ,",
    "the chance of identifying the right element is 1/4 , as the measurements of the intended grover search actually reveal no useful information because they are unbiased with respect to the required measurement direction . in figure  [ fig : grover ] , the analytic results for identifying the correct marked element @xmath137 in the rotated state match the trials with 1000 agents that simply measure all 4 qubits the direction @xmath62 and then try to identify the marked element from the obtained measurement results .    for testing the agent with a projective simulator we restrict to a realization with a single marked element , namely @xmath137 , which can be implemented with measurements along the @xmath76-axis , all in the direction @xmath62 .",
    "the agent first learns with a test qubit exposed to the external field and adapts to the field strength .",
    "we then fix these obtained @xmath31-values and use the agent without update rule to carry out the four measurements on the cluster state , one after the other , according to its available measurement directions and internal probabilities .",
    "the first example is an agent that has only 4 fixed measurement directions available ( @xmath17 being a multiple of @xmath35 ) , which we first train to achieve optimal success probability with the test qubit .",
    "the optimal performance is reached in the limit @xmath156 , which amounts to @xmath157 for the single @xmath17 that is closest to @xmath12 and all others zero .",
    "the light blue curve in figure  [ fig : grover ] shows the fraction of the agent ensemble that identifies the marked element with this projective simulator correctly .",
    "the grover search is recovered perfectly for fields with @xmath12 being a multiple of @xmath35 , which can be matched exactly by the available measurement directions .",
    "the second example is an agent that first learns with a test qubit in the external field with composition according to the glow mechanism .",
    "that is , after 2000 measurements on average , the agent composes a new measurement direction or strengthens an existing one that matches @xmath12 .",
    "the @xmath31-values after the composition remain fixed and the agent measures the cluster state according to the available measurement directions and probabilities .",
    "the dark blue curve in figure  [ fig : grover ] illustrates that an ensemble of this kind of agent is highly successful in doing the grover search for all angles @xmath12 . the shortfall from a perfect performance (",
    "the average success probability is 99.0% with a standard deviation of 0.3% ) originates in the slight deviations of the composed angle from @xmath12 and the non - zero probability to chose the remaining non - optimal measurement directions .",
    "we presented an autonomous adaptive system that is able to perform quantum information processing in changing environments .",
    "the controller is a learning agent endowed with a projective simulator that adapts measurement directions in a setup of measurement - based quantum computation by reinforcement learning .",
    "our approach thus combines elements from embodied artificial intelligence with the purpose of carrying out robust quantum information processing .    in an exemplary setup of adapting measurement directions to an unknown stray magnetic field in a fixed direction ,",
    "we have characterized the learning process of the projective simulator and its adaption to time - varying fields using numerical studies .",
    "we found that an agent using projective simulation is able to adapt to such unknown stray fields .",
    "we provide analytical estimates of its success probability in limiting cases of the non - linear learning process . in our scenario",
    "the agent may adapt the measurement direction by drawing from a initially provided set of fixed measurement directions .",
    "we have characterized the performance of the agent for different sets of available measurement directions and we explored composition mechanisms to create new and better measurement directions on the fly , together with the corresponding internal structure in the projective simulator .",
    "strategies with composed measurement directions surpass strategies with fixed sets of directions in both learning speed and resulting efficiency . as a demonstration of adaptive quantum information processing",
    ", the agent successfully carries out a measurement - based version of grover s search algorithm in the presence of a detrimental unknown external magnetic field",
    ".    the present approach can be readily extended and improved in several directions as indicated in the respective sections in the paper .",
    "first and foremost , the agent effectively develops and embodies rules to cope and operate with quantum mechanical systems , which are seeded by the specific form the update rule together with the reward scheme , and the composition mechanisms .",
    "both of these elements start from simple primitives , e.g.  `` prefer a specific measurement if it more likely results in a + 1 measurement outcome '' for the update rule , and give rise to a sensible and sufficient behavior in our problem setting .",
    "both can be improved by effectively incorporating more information about the quantum mechanical nature of the underlying problem domain , however , at the expense of more complicated update and composition rules .",
    "errors or imperfect measurements can be straightforwardly incorporated into the present scheme by using povms instead of projective measurement , or by adding a classical noise , e.g. bit flips , to the measurement outcomes .",
    "such errors lead to a diluted information about which measurement directions are correct and give @xmath13 measurement outcomes . in the presence of errors ,",
    "spurious rewards appear for wrong measurement directions and the average reward for correct measurement directions is reduced .",
    "both effectively diminish the contrast in the reward landscape , which is equivalent to a lower reward scaling factor @xmath27 .",
    "we expect that the agent will still be able to learn in such situations , but it will take longer to do so and reach a lower asymptotic success probability .",
    "the latter can partly be recovered by adjusting @xmath27 and @xmath29 , however , an increase of the learning time over a noiseless scenario will remain .    the long - term goal of this investigation is to develop integrated and autonomous schemes for measurement - based quantum information processing that can adapt to changing environments . in our scheme ,",
    "learning is not realized by feedback from some external macroscopic sensor , e.g. a magnetometer , but it uses only information drawn from measurements on qubits , which are also the operations that drive the processing of the quantum information . in this sense",
    "our approach is related to recent work on intelligent quantum error correction  @xcite .",
    "the approach that we have presented in the present paper can be generalized and integrated into a scheme of universal measurement - based quantum computation , where measurements of stabilizer operators of a cluster state are used both for the correction of errors on the resource state and , at the same time , for the adaption of measurement directions that drive the quantum computation .",
    "this will be reported elsewhere .",
    "we note that the projective simulator does not assume that rewards originate from measurement probabilities of a quantum state and , therefore , it is `` model free '' .",
    "this also opens the path to study foundational questions such as , to what extent can a machine effectively learn the rules of quantum mechanics through simple reinforcement processes .",
    "we thank wolfgang dr for initial discussions on this topic and vedran dunjko for comments on the manuscript .",
    "we acknowledge support from the austrian science fund ( fwf ) through the sfb foqus : f4012 , and the templeton world charity foundation grant twcf0078/ab46 .",
    "499 briegel , h. j. & de las cuevas , g. projective simulation for artificial intelligence .",
    "* 2 * , 400 ( 2012 ) .",
    "mautner , j. , makmal , a. , manzano , d. , tiersch , m. & briegel , h. j. projective simulation for classical learning agents : a comprehensive investigation .",
    "_ new gen .",
    "comp . _ * 33 * , 69114 ( 2015 ) .",
    "melnikov , a. a. , makmal , a. & briegel , h. j. projective simulation applied to the grid - world and the mountain - car problem .",
    "_ preprint _",
    "arxiv:1405.5459 [ cs.ai ] ( 2014 ) .",
    "melnikov , a. a. , makmal , a. , dunjko , v. & briegel , h. j. projective simulation with generalization , _ preprint _ arxiv:1504.02247 [ cs.ai ] ( 2015 )",
    ".    hentschel , a. & sanders , b. c. machine learning for precise quantum measurement .",
    "lett . _ * 104 * , 063603 ( 2010 ) .",
    "hentschel , a. & sanders , b. c. efficient algorithm for optimizing adaptive quantum metrology processes .",
    "lett . _ * 107 * , 233601 ( 2011 ) .",
    "lovett , n. b. , crosnier , c. , perarnau - llobet , m. , & sanders , b. c. differential evolution for many - particle adaptive quantum metrology . _ phys .",
    "lett . _ * 110 * , 220501 ( 2013 ) .",
    "sergeevich , a. , chandran , a. , combes , j. , bartlett , s. d. & wiseman , h. m. characterization of a qubit hamiltonian using adaptive measurements in a fixed basis .",
    "a _ * 84 * , 052315 ( 2011 ) .",
    "granade , c. e. , ferrie , c. , wiebe , n. & cory , d. g. robust online hamiltonian learning .",
    "_ new j. phys . _ * 14 * , 103013 ( 2012 ) .",
    "hayes , a. j. f. & berry , d. w. swarm optimization for adaptive phase measurements with low visibility .",
    "rev . a _ * 89 * , 013838 ( 2014 ) .",
    "sutton , r. s. & barto , a. g. _ reinforcement learning : an introduction 1st edn _ ( mit press , 1998 ) .",
    "russel , s. j. & norvig , p. _ artificial intelligence  a modern approach 2nd edn _",
    "( prentice hall , 2003 ) .",
    "paparo , g. d. , dunjko , v. , makmal , a. , martin - delgado , m. a. & briegel , h. j. quantum speedup for active learning agents .",
    "x _ * 4 * , 031002 ( 2014 ) .",
    "dunjko , v. , friis , n. & briegel , h. j. quantum - enhanced deliberation of learning agents using trapped ions .",
    "_ new j. phys . _ * 17 * , 023006 ( 2015 ) .",
    "wittek , p. _ quantum machine learning : what quantum computing means to data mining_. ( academic press , 2014 ) .",
    "schuld , m. , sinayskiy , i. & petruccione , f. an introduction to quantum machine learning .",
    "_ preprint _",
    "arxiv:1409.3097 [ quant - ph ] ( 2014 ) .",
    "grover , l. k. a fast quantum mechanical algorithm for database search in _ proceedings of the 28th annual symposium on the theory of computing _ , 212219 , ( acm press , 1996 ) .",
    "grover , l. k. quantum mechanics helps in search for a needle in a haystack .",
    "lett . _ * 79 * , 325328 ( 1997 ) .",
    "chuang , i. l. , gershenfeld , n. & kubinec , m. experimental implementation of fast quantum searching .",
    "lett . _ * 80 * , 34083411 ( 1998 ) .",
    "jones , j. a. , mosca , m. & hansen , r. h. implementation of a quantum search algorithm on a quantum computer .",
    "_ nature _ * 393 * , 344346 ( 1998 ) .",
    "kwiat , p. g. , mitchell , j. r. , schwindt , p. d. d. & white , a. g. grover s search algorithm : an optical approach .",
    "opt . _ * 47 * , 257266 ( 2000 ) .",
    "raussendorf , r. & briegel , h. j. a one - way quantum computer .",
    "lett . _ * 86 * , 51885191 ( 2001 ) .",
    "raussendorf , r. , browne , d. e. & briegel , h. j. measurement - based quantum computation on cluster states .",
    "rev . a _ * 68 * , 022312 ( 2003 )",
    ".    briegel , h. j. & raussendorf , r. persistent entanglement in arrays of interacting particles , _ phys .",
    "* 86 * , 910913 ( 2001 ) .",
    "walther , p. et al .",
    "experimental one - way quantum computing .",
    "_ nature _ * 434 * , 169176 ( 2005 ) .",
    "prevedel r. et al .",
    "high - speed linear optics quantum computing using active feed - forward . _ nature _ * 445 * , 6569 ( 2007 ) .",
    "lanyon , b. p. et al .",
    "measurement - based quantum computation with trapped ions .",
    "lett . _ * 111 * , 210501 ( 2013 ) .",
    "knill , e. , laflamme , r. & milburn , g. j. a scheme for efficient quantum computation with linear optics .",
    "_ nature _ * 409 * , 4652 ( 2001 ) .",
    "helstrom , c. w. _ quantum detection and estimation theory_. ( academic press , 1976 ) .",
    "fisher , n. i. , lewis , t. & embleton , b. j. j. _ statistical analysis of spherical data_. ( cambridge university press , 1987 ) .",
    "mardia , k. v. & jupp , p. e. _ directional statistics_. wiley series in probability and statistics ( john wiley & sons ltd , 2000 ) .",
    "combes , c. et al .",
    "in - situ characterization of quantum devices with error correction .",
    "_ preprint _",
    "arxiv:1405.5656 [ quant - ph ] ( 2014 ) .",
    "the angular probability distribution for @xmath12 given the @xmath82 measurement outcomes @xmath158 in directions @xmath79 , which are multiples of @xmath35 , is given by @xmath159 with normalization @xmath84 , and can be expanded into a fourier sum @xmath160 where the normalization is solely contained in the coefficient @xmath161 .",
    "updating this probability distribution with the next measurement result @xmath162 amounts to multiplication with the factor @xmath163 , which we again expand into a fourier sum . comparing the coefficients we obtain the following recursion relations for the @xmath164 and @xmath165 :    @xmath166 \\\\",
    "s_{m+1}(q ) & = \\frac{s_m(q)}{2 } + \\frac{r_{m+1}}{4 } \\big [          \\sin(\\alpha_{m+1 } ) \\big ( c_m(q-1 ) - c_m(q+1 ) \\big ) +          \\cos(\\alpha_{m+1 } ) \\big ( s_m(q-1 ) + s_m(q+1 ) \\big )          \\big],\\end{aligned}\\ ] ]      the advantage of the fourier representation is that circular moments of the probability distribution can be straightforwardly calculated : @xmath170 the first circular moment @xmath171 gives rise to the mean angle @xmath172 and the circular standard deviation @xmath173 @xcite ."
  ],
  "abstract_text": [
    "<S> quantum information processing devices need to be robust and stable against external noise and internal imperfections to ensure correct operation . in a setting of measurement - based quantum computation , </S>",
    "<S> we explore how an intelligent agent endowed with a projective simulator can act as controller to adapt measurement directions to an external stray field of unknown magnitude in a fixed direction . </S>",
    "<S> we assess the agent s learning behavior in static and time - varying fields and explore composition strategies in the projective simulator to improve the agent s performance . </S>",
    "<S> we demonstrate the applicability by correcting for stray fields in a measurement - based algorithm for grover s search . </S>",
    "<S> thereby , we lay out a path for adaptive controllers based on intelligent agents for quantum information tasks . </S>"
  ]
}