{
  "article_text": [
    "wide field of view ( fisheye ) camera has received increasing attention over the past few years with its broad applications in surveillance , robotics , intelligent vehicles , immersive virtual environment construction , etc .",
    "for example , nissan motors developed a visual system that consists of four fisheye cameras mounted on the four sides of the vehicle .",
    "they together cover the entire 360@xmath0 surrounding scene and allow drivers to examine all the visual blind spots that may cause danger . in surveillance ,",
    "ip fisheye camera has become extremely prevalent for its wide cover range and easy axcessibility .",
    "samsung provides a product with over 5 megpixel and 360@xmath0 fov , which is equipped in an alarm system performing intelligent motion detection , audio detection , and tampering detection .",
    "the supporting de - warping software allows users to undistort any subregion in the captured image .",
    "recently , ricoh unveilled its first personal 360@xmath0 fisheye camera  ricoh theta .",
    "two fisheye cameras are embedded on both front and back sides , to capture the entire scene with one click .",
    "then the two captured images are stitched together to provide a dynamic 360@xmath0 view with adjustable perspective controlled by the user . with this portable and handy device",
    ", our project aims to reconstruct the 3d scene using the captured spherical images . one important advantage of using this camera is that we are no longer required to set up multiple traditional cameras at different locations and directions to cover the entire scene . as a tradeoff , traditional camera model with perspective projection",
    "can not be directly applied since fisheye camera has large radial distortion , especially near the border . to establish a one - to - one mapping between the 180@xmath0 scene and a circular image",
    ", we created a model based on spherical projection . based on this model",
    ", we can develop the epipolar geometry for fisheye cameras and solve the triangulation problem with least - square method .",
    "we used manually selected points to calculate the fundamental matrix , then applied it as a filter to prune the sift [ [ lowe ] ] matching result , at last augmented the point correspondences for reconstruction . on the other hand",
    ", we also tried dense reconstruction by first doing image rectification and then calculating the disparity map .    in sec .",
    "[ relatedwork ] , we will briefly review previous work about 3d reconstruction with fisheye camera . then in sec . [ model ]",
    "we will jump into the details of our camera model , revised epipolar geometry and data augmentation .",
    "extension to multicamera registration and dense reconstruction will also be illustrated . in sec .",
    "[ experiment ] , we first show the reconstruction result using hand - picked points , then we show the sift augmented result . next , we give the disparity map and dense reconstruction result . finally , we will show a snapshot of our gui and provide the source code package for users to taste .",
    "perspective camera model is the most popular camera model in 3d reconstruction .",
    "however , it is limited for its narrow field of view . on the other hand , fisheye cameras which can capture spherical images",
    "have been paid more attention to during recent years .",
    "the major advantage is the wide fov and thus more information it can incorporate from the environment .    shah and aggarwal [ [ shah ] ] presented an autonomous mobile robot navigation system in an indoor environment using two calibrated fisheye sensors .",
    "micusik et al .",
    "[ [ micusik ] ] proposed a 3d reconstruction of the surrounding scene with two or more uncalibrated fisheye images .",
    "li [ [ li ] ] drew 3d reconstruction by computing spherical disparity maps using binocular fisheye camera , which first calibrated the binocular camera to rectify the captured images and then used the correlation - based stereo to acquire the dense 3d representation of some simple environment .",
    "herrera et al .",
    "[ [ herrera ] ] and moreau et al .",
    "[ [ moreau ] ] placed the camera upwards and retrieved the environment information from the images .",
    "they computed disparity maps without image rectification step .",
    "* proposed the camera model and epipolar geometry for fisheye camera . *",
    "designed a method to estimate camera rotation and position from point correspondences in multiple images .",
    "* implemented sift feature extraction and matching algorithm through equirectangular - to - cube mapping .",
    "* proposed sparse & dense 3d reconstruction algorithm from multiple images . *",
    "developed a graphical user interface to interactively show multiple correlated 360@xmath0 images .",
    "in this section , we go over the mathematical model behind this project .",
    "it mainly consists of four parts , the fisheye camera model , epipolar geometry , multicamera registration and image rectification for dense reconstruction .",
    "the fisheye camera model is based on spherical projection .",
    "suppose there is a sphere of radius @xmath1 and a point @xmath2 in space , as shown in fig .",
    "[ cammodel ] .",
    "first , @xmath2 is projected to @xmath3 which is the intersection of the sphere surface with the line defined by sphere center @xmath4 and point @xmath2 .",
    "this defines a mapping between spatial points to points on the sphere surface .",
    "then , these points are vertically projected onto the image plane as @xmath3 is projected to @xmath5 , which results in a circular image . in mathematical term , let @xmath6^t$ ] , then we have @xmath7^t$ ] .",
    "the relation between @xmath2 and @xmath3 is @xmath8 where @xmath9 and @xmath10 .",
    "the vertical projection reduces the @xmath11 component to 0 , and we get @xmath12^t$ ] . here",
    ", we let @xmath13 which means we project onto a unit sphere .",
    "the raw images acquired by ricoh theta , shown in fig .",
    "[ fig:2view ] are in equirectangular form with resolution 1024@xmath142048 , i.e. the @xmath15 image coordinates represent the longitude and the latitude on the unit sphere .",
    "@xmath16 @xmath17 @xmath18^t\\ ] ]      now , assume there are two cameras centered at @xmath19 and @xmath20 , as shown in fig .",
    "[ epipolar ] .",
    "there is a point @xmath2 in 3d space .",
    "then , for camera 1 , the projection on spherical surface is @xmath21 ; for camera 2 , the projection on spherical surface ( in world coordinates ) is @xmath22 . without loss of generality , assume the reference system of camera 1 is the same as the world reference system , and the rotation and translation between camera 1 and camera 2 is @xmath23 and @xmath20 .",
    "then , we have @xmath24 , where @xmath25 and @xmath26 are the coordinates of @xmath27 and @xmath28 in their cameras reference system .    notice , now we have five coplanar points : camera centers @xmath29 , @xmath30 and @xmath2 .",
    "thus , we have the constraint @xmath31 .",
    "i.e. @xmath32 .",
    "substitute @xmath33 with @xmath25 and @xmath26 , we get , @xmath34\\cdot r^{-1 } z_{p,2 } = 0\\ ] ] define @xmath35\\cdot r^{-1}$ ] as the fundamental matrix for fisheye camera pair , we have constraint @xmath36 .",
    "now , we can use the eight - points algorithm or ransac to solve for @xmath37 . once we get the fundamental matrix",
    ", we can calculate the epipoles in the two cameras by solving @xmath38 , @xmath39 .",
    "recall that the definition of epipoles is @xmath40 , @xmath41 , which gives @xmath42 .",
    "then the rotation matrix @xmath23 can be derived as , + @xmath43_{\\times } + [ v]_{\\times}^2\\frac{1-c}{s^2}\\ ] ] where @xmath44 , @xmath45 , @xmath46 .",
    "here we assume the euclidean distance between @xmath47 and @xmath48 is 1 , i.e. @xmath49 .",
    "now , we can triangulate @xmath2 using parameters @xmath25 , @xmath26 , @xmath50 , and @xmath23 .",
    "we define the line passing through @xmath47 and @xmath25 as @xmath51 , where @xmath52 ; the line passing through @xmath48 and @xmath26 as @xmath53 , @xmath54 .",
    "the goal of triangulation is to find the minimal distance between the two lines",
    ". we can formulate this into a least square problem , + @xmath55 where the optimal solution is given by , @xmath56 = { \\left ( { { a^t}a } \\right)^ { - 1}}{a^t}{e_1},\\quad a = \\left [ { \\begin{array}{*{20}{c } }      { { z_{p,1 } } } & { - { r^ { - 1}}{z_{p,2 } } }      \\end{array } } \\right]\\ ] ] once we get the optimal parameter @xmath57 , @xmath58 . the minimal distance is known to be achieved between @xmath59 and @xmath60 , then @xmath2 can be assigned as the their middle points : @xmath61      in order to calculate @xmath37 , we must have enough point correspondences in multiple images . in our methods , we manually selected around 45 pairs of corresponding points .",
    "we also attempted to automatically estimate @xmath37 by applying ransac with constraint @xmath62 on sift matching results , to find the best estimation of @xmath37 .",
    "however , sift matching is not invariant to radial distortion and the matching results have unacceptable outliers , thus the estimation of @xmath37 is not robust enough .",
    "instead , we estimated @xmath37 by using hand - picked points , and in turn use @xmath37 to filter the sift matches and extend our point pairs pool .",
    "next , we extend the discussion to multi - view scenario . from the section above we can obtain the fundamental matrix and epipoles for each pair of cameras , but we can no longer assume the euclidean distance between camera centers is 1 .",
    "now we want to estimate the rotation matrix and camera position for each camera",
    ". this can be done in a two - step process .",
    "first , we estimate the rotation for each camera .",
    "assume we have @xmath63 cameras , for each pair of camera @xmath64 and @xmath65 , we can calculate the epipoles @xmath66 and @xmath67 , which denotes @xmath68 on image @xmath64 and @xmath69 on image @xmath65 , respectively . here",
    ", we assume the cameras all lie on the same horizonal plane , which is a very good approximation of how we took pictures .",
    "therefore , the rotation of each camera can be represented by an angle @xmath70 .",
    "the relation between @xmath70 and rotation matrix @xmath71 is @xmath72\\ ] ]    the epipole direction in world coordinate is @xmath73 @xmath74 @xmath75    the last line should be obvious as they both denote the direction of the line segment defined by @xmath69 and @xmath68 .",
    "the @xmath76 sign indicates the two - fold ambiguity in calculating @xmath66 from the fundamental matrix .",
    "now we need to minimize the objective function , @xmath77 which is a convex optimization problem , and we solve it by newton s method .",
    "next , we estimate the position of each camera . as we have the direction of each line segment",
    "@xmath78 , this is a triangulation problem . a naive way to solve",
    "this problem is to choose two cameras , e.g. @xmath47 and @xmath48 , and set the euclidean distance between them to be 1 .",
    "then for each camera other than @xmath47 and @xmath48 , its position can be triangulated from the direction of @xmath79 and @xmath80 .",
    "we can repeat the procedure with different choice of baseline to check the consistency .",
    "we can also feed the result into another gradient descent program to adjust the camera positions using all directions @xmath81 obtained .",
    "now that we have recovered the rotation and translation of each camera , the object points can be triangulated in a similar way as described in sec .",
    "[ epigeo ] . in the multiview case ,",
    "we assign @xmath82 as the distance between object point and camera center @xmath69 , and minimize the mean squared distance among the @xmath63 points obtained from each camera image .",
    "@xmath83 @xmath84 @xmath85      now , we want to go one step further from sparse reconstruction to dense reconstruction . in order to achieve a dense reconstruction ,",
    "we need to rectify the image pairs so that their epipolar lines are horizontal and all corresponding points have the same vertical coordinate on the image . as we know , the epipolar lines in spherical images are circles which intersect with the epipoles . therefore ,",
    "if we rotate the camera reference such that the @xmath11 axis align with the epipole and the @xmath86,@xmath87 axis are parallel , and map the sphere onto equirectangular image , then epipolar lines would be vertical lines in equirectangular images , as show in fig .",
    "[ rectpipe ] . by exchanging the horizontal and vertical coordinates , the image pairs will be rectified . from the rectified image pairs",
    "we can calculate a disparity map @xmath88 , which is the distance ( in pixels ) between corresponding points on the image pair . as",
    "the images are equirectangular , @xmath88 is the angle @xmath89 by a constant . assume the corresponding points are @xmath90 and @xmath91 respectively , and @xmath92 , then @xmath93 , @xmath94 , where @xmath95 . from that we can calculate the 3d coordinates of point @xmath2 .",
    "in this section , we show the reconstruction results for a 2-cameras settings .",
    "the two raw images are shown in fig .",
    "[ fig:2view ] .          in order to implement the eight - point algorithm to compute the fundamental matrix",
    ", we manually labeled ground truth point correspondences on circular images , shown in fig .",
    "[ fig : ptscorr ] .",
    "for each view , we labeled around 45 pairs of corresponding points , which are typically on the ceiling or on the walls , thus easy to recognize .",
    "there are also several points around the desk , such as the corner of the computer .",
    "we also tried to extract point correspondences using sift , then estimate @xmath37 automatically using ransac .",
    "however , due to the large amount of outliers , this approach is not robust enough .",
    "therefore , we proposed another pipeline  use the ground truth @xmath37 to filter sift matching results , and add those correspondences into our correspondences pool to achieve a denser reconstruction result . in our project",
    ", we used the sift implementation in vlfeat toolbox [ [ vlfeat ] ] for extracting features and performing point matching .",
    "we applied point mathcing on both raw images and cubic images achieved by cube mapping [ [ greene ] ] .",
    "[ cubesift ] shows a rough matching result using cubic images .          using the ground truth fundamental matrix @xmath37 , we calculated @xmath96 and @xmath23 .",
    "then , we used the triangulation method we proposed in sec .",
    "[ epigeo ] to recover points position in 3d space .",
    "the reconstruction result is shown in fig .",
    "[ fig:3drec ] .",
    "now , we show the reconstruction result using pictures captured at 6 different loacations , which is equivalent to 6 cameras .",
    "we manually select 12 corresponding points on each of the 6 images . for each pair of image",
    "we calculate the fundamental matrix @xmath37 and epipoles @xmath50 , @xmath97 .",
    "then , we calculated the rotation and position of each camera .",
    "the position of the cameras is shown in fig .",
    "[ campos ] .",
    "once the rotation and position of each camera is obtained , we can triangulate the corresponding points as well as rectify each pair of images . the 3d reconstruction for the 12 points",
    "is shown in fig .",
    "the result matches well with the ground truth .",
    "after rectification , the corresponding points are at the same longitude with each other .",
    "so after transforming the raw image into longitude - latitude image , we can use the traditional method to find the corresponding pairs in the images .",
    "the calculated disparity map is shown in fig . [",
    "rectified ] , together with the two rectified images .",
    "the brighter part means smaller disparity and the darker part indicates larger disparity . as we can see , the image have roughly presented the deapth information .",
    "while since the rectified images still have distortion , the disparity map may have noise .",
    "the reconstruction result is shown in fig .",
    "[ denserec ] .",
    "although the result looks a little messy , we can see the closet are reconstructed fairly well .              a graphical user interface ( gui )",
    "is developed using the 6-view dataset .",
    "you can run ` demos.m ` to see the demonstration .",
    "[ gui ] gives a brief illustration of the 6 views obtained by user control .",
    "in this project we implemented 3d reconstruction algorithm for multiple spherical images .",
    "we obtained our data using ricoh theta fullview fisheye camera .",
    "we used both manually selected points and sift matching points to estimate fundamental matrix for each pair of images .",
    "then , we calculated epipoles , the rotation and the position of each camera . based on these information we implemented sparse 3d reconstruction , the result matches well with the ground truth .",
    "we also developed a user interface to enable users to interactively view multiple correlated 360@xmath0 images .",
    "our project is an important step towards building virtual tour from large number of fullview images .",
    "there are two things we want to improve in the future .",
    "the first is to enhance the algorithm of generating disparity map .",
    "the second is the robustness of sift matching in various datasets .",
    "currently the performance of sift matching fluctuates between different image sets . in outdoor images ,",
    "sift matching performance tends to deteriorate , the reason could be that camera centers are too far apart thus image pairs differ too much , or that buildings tend to have repetitive features like arches , windows , etc .",
    "we could improve the image capturing behaviours and select more appropriate scenes to get a better performance .",
    "[ shah ] shah , shishir , and j. k. aggarwal .",
    "`` intrinsic parameter calibration procedure for a ( high - distortion ) fish - eye lens camera with distortion model and accuracy estimation . ''",
    "pattern recognition 29.11 ( 1996 ) : 1775 - 1788 .",
    "[ micusik ] micusik , branislav , and tomas pajdla .",
    "`` autocalibration & 3d reconstruction with non - central catadioptric cameras . '' computer vision and pattern recognition , 2004 .",
    "cvpr 2004 .",
    "proceedings of the 2004 ieee computer society conference on .",
    "1 . ieee , 2004 .",
    "[ moreau ] moreau , julien , sebastien ambellouis , and yassine ruichek .",
    "`` 3d reconstruction of urban environments based on fisheye stereovision . ''",
    "signal image technology and internet based systems ( sitis ) , 2012 eighth international conference on .",
    "ieee , 2012 .",
    "[ fujiki ] fujiki , jun , akihiko torii , and shotaro akaho .",
    "`` epipolar geometry via rectification of spherical images . ''",
    "computer vision / computer graphics collaboration techniques .",
    "springer berlin heidelberg , 2007 .",
    "461 - 471 ."
  ],
  "abstract_text": [
    "<S> in this report , we proposed a 3d reconstruction method for the full - view fisheye camera . </S>",
    "<S> the camera we used is ricoh theta , fig . </S>",
    "<S> [ ricoh ] , which captures spherical images and has a wide field of view ( fov ) . </S>",
    "<S> the conventional stereo apporach based on perspective camera model can not be directly applied and instead we used a spherical camera model to depict the relation between 3d point and its corresponding observation in the image . </S>",
    "<S> we implemented a system that can reconstruct the 3d scene using captures from two or more cameras . </S>",
    "<S> a gui is also created to allow users to control the view perspective and obtain a better intuition of how the scene is rebuilt . </S>",
    "<S> experiments showed that our reconstruction results well preserved the structure of the scene in the real world . </S>"
  ]
}