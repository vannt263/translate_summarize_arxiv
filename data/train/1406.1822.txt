{
  "article_text": [
    "the central problem of this paper is computational complexity in a setting where the number of classes @xmath0 for multiclass prediction is very large .",
    "such problems occur in natural language ( which translation is best ? ) , search ( what result is best ? ) , and detection ( who is that ? ) tasks . almost all machine learning algorithms ( with the exception of decision trees ) have running times for multiclass classification which are @xmath1 with a canonical example being one - against - all classifiers  @xcite .    in this setting ,",
    "the most efficient possible accurate approach is given by information theory  @xcite .",
    "in essence , any multiclass classification algorithm must uniquely specify the bits of all labels that it predicts correctly on .",
    "consequently , kraft s inequality ( @xcite equation 5.6 ) implies that the expected _ computational _ complexity of predicting correctly is @xmath2 per example where @xmath3 is the shannon entropy of the label . for the worst case distribution on @xmath0 classes , this implies @xmath4 computation is required .",
    "hence , our goal is achieving @xmath5 computational time per example for both training and testing , while effectively using online learning algorithms to minimize passes over the data .",
    "the goal of logarithmic ( in @xmath0 ) complexity naturally motivates approaches that construct a logarithmic depth hierarchy over the labels , with one label per leaf . while this hierarchy is sometimes available through prior knowledge , in many scenarios it needs to be learned as well .",
    "this naturally leads to a _",
    "partition _ problem which arises at each node in the hierarchy .",
    "the partition problem is finding a classifier : @xmath6 which divides examples into two subsets with a purer set of labels than the original set .",
    "definitions of purity vary , but canonical examples are the number of labels remaining in each subset , or softer notions such as the average shannon entropy of the class labels . despite resulting in a classifier ,",
    "this problem is fundamentally different from standard binary classification . to see this , note that replacing @xmath7 with @xmath8 is very bad for binary classification , but has no impact on the quality of a partition .",
    "the partition problem is fundamentally non - convex for symmetric classes since the average @xmath9 of @xmath7 and @xmath8 is a poor partition ( the always-@xmath10 function places all points on the same side ) .    the choice of partition matters in problem dependent ways .",
    "for example , consider examples on a line with label @xmath11 at position @xmath11 and threshold classifiers . in this case , trying to partition class labels @xmath12 from class label @xmath13 results in poor performance .",
    "the partition problem is typically solved for decision tree learning via an enumerate - and - test approach amongst a small set of possible classifiers ( see e.g.  @xcite ) . in the multiclass setting",
    ", it is desirable to achieve substantial error reduction for each node in the tree which motivates using a richer set of classifiers in the nodes to minimize the number of nodes , and thereby decrease the computational complexity .",
    "the main theoretical contribution of this work is to establish a boosting algorithm for learning trees with @xmath14 nodes and @xmath15 depth , thereby addressing the goal of logarithmic time train and test complexity .",
    "our main theoretical result , presented in section  [ sec : boosting ] , generalizes a binary boosting - by - decision - tree theorem  @xcite to multiclass boosting . as in all boosting results , performance is critically dependent on the quality of the _ weak learner _ , supporting intuition that we need sufficiently rich partitioners at nodes .",
    "the approach uses a new objective for decision tree learning , which we optimize at each node of the tree .",
    "the objective and its theoretical properties are presented in section  [ sec : framework ] .",
    "l0.5    a complete system with multiple partitions could be constructed top down ( as the boosting theorem ) or bottom up ( as filter tree  @xcite ) .",
    "a bottom up partition process appears impossible with representational constraints as shown in section  [ sec : bottom - up ] in the supplementary material so we focus on top - down tree creation .",
    "whenever there are representational constraints on partitions ( such as linear classifiers ) , finding a strong partition function requires an efficient search over this set of classifiers .",
    "efficient searches over large function classes are routinely performed via gradient descent techniques for supervised learning , so they seem like a natural candidate . in existing literature , examples for doing this",
    "exist when the problem is indeed binary , or when there is a prespecified hierarchy over the labels and we just need to find partitioners aligned with that hierarchy .",
    "neither of these cases applies  we have multiple labels and want to dynamically create the choice of partition , rather than assuming that one was handed to us .",
    "does there exist a purity criterion amenable to a gradient descent approach ?",
    "the precise objective studied in theory fails this test due to its discrete nature , and even natural approximations are challenging to tractably optimize under computational constraints . as a result",
    ", we use the theoretical objective as a motivation and construct a new logarithmic online multiclass tree ( lomtree ) algorithm for empirical evaluation .",
    "creating a tree in an online fashion creates a new class of problems .",
    "what if some node is initially created but eventually proves useless because no examples go to it ? at best this results in a wasteful solution , while in practice it starves other parts of the tree which need representational complexity . to deal with this , we design an efficient process for recycling orphan nodes into locations where they are needed , and prove that the number of times a node is recycled is at most logarithmic in the number of examples .",
    "the algorithm is described in section  [ sec : alg ] and analyzed in section  [ sec : swap - bound ] .    and",
    "is it effective ? given the inherent non - convexity of the partition problem this is unavoidably an empirical question which we answer on a range of datasets varying from 26 to 105k classes in section  [ sec : experiments ] .",
    "we find that under constrained training times , this approach is quite effective compared to all baselines while dominating other @xmath15 train time approaches .",
    "what s new ? to the best of our knowledge , the splitting criterion , the boosting statement , the lomtree algorithm , the swapping guarantee , and the experimental results are all new here .",
    "only a few authors address logarithmic time training .",
    "the filter tree  @xcite addresses consistent ( and robust ) multiclass classification , showing that it is possible in the statistical limit .",
    "the filter tree does not address the partition problem as we do here which as shown in our experimental section is often helpful .",
    "the partition finding problem is addressed in the conditional probability tree  @xcite , but that paper addresses conditional probability estimation .",
    "conditional probability estimation can be converted into multiclass prediction  @xcite , but doing so is not a logarithmic time operation .",
    "quite a few authors have addressed logarithmic testing time while allowing training time to be @xmath14 or worse .",
    "while these approaches are intractable on our larger scale problems , we describe them here for context .",
    "the partition problem can be addressed by recursively applying spectral clustering on a confusion graph  @xcite ( other clustering approaches include  @xcite ) .",
    "empirically , this approach has been found to sometimes lead to badly imbalanced splits  @xcite . in the context of ranking ,",
    "another approach uses @xmath0-means hierarchical clustering to recover the label sets for a given partition  @xcite .",
    "the more recent work  @xcite on the multiclass classification problem addresses it via sparse output coding by tuning high - cardinality multiclass categorization into a bit - by - bit decoding problem .",
    "the authors decouple the learning processes of coding matrix and bit predictors and use probabilistic decoding to decode the optimal class label .",
    "the authors however specify a class similarity which is @xmath16 to compute ( see section @xmath17 in  @xcite ) , and hence this approach is in a different complexity class than ours ( this is also born out experimentally ) .",
    "the variant of the popular error correcting output code scheme for solving multi - label prediction problems with large output spaces under the assumption of output sparsity was also considered in  @xcite .",
    "their approach in general requires @xmath14 running time to decode since , in essence , the fit of each label to the predictions must be checked and there are @xmath1 labels .",
    "another approach  @xcite proposes iterative least - squares - style algorithms for multi - class ( and multi - label ) prediction with relatively large number of examples and data dimensions , and the work of  @xcite focusing in particular on the cost - sensitive multiclass classification .",
    "both approaches however have @xmath1 training time .",
    "decision trees are naturally structured to allow logarithmic time prediction .",
    "traditional decision trees often have difficulties with a large number of classes because their splitting criteria are not well - suited to the large class setting .",
    "however , newer approaches  @xcite have addressed this effectively at significant scales in the context of multilabel classification ( multilabel learning , with missing labels , is also addressed in  @xcite ) .",
    "more specifically , the first work  @xcite performs brute force optimization of a multilabel variant of the gini index defined over the set of positive labels in the node and assumes label independence during random forest construction .",
    "their method makes fast predictions , however has high training costs  @xcite .",
    "the second work  @xcite optimizes a rank sensitive loss function ( discounted cumulative gain ) . additionally , a well - known problem with hierarchical classification is that the performance significantly deteriorates lower in the hierarchy  @xcite which some authors solve by biasing the training distribution to reduce error propagation while simultaneously combining bottom - up and top - down approaches during training  @xcite .",
    "the reduction approach we use for optimizing partitions implicitly optimizes a differential objective . a non - reductive approach to this",
    "has been tried previously  @xcite on other objectives yielding good results in a different context .",
    "in this section we describe the essential elements of the approach , and outline the theoretical properties of the resulting framework .",
    "we begin with high - level ideas .",
    "we employ a hierarchical approach for learning a multiclass decision tree structure , training this structure in a _ top - down _ fashion .",
    "we assume that we receive examples @xmath18 , with labels @xmath19 .",
    "we also assume access to a hypothesis class @xmath20 where each @xmath21 is a binary classifier , @xmath22 .",
    "the overall objective is to learn a tree of depth @xmath15 , where each node in the tree consists of a classifier from @xmath20 .",
    "the classifiers are trained in such a way that @xmath23 ( @xmath24 denotes the classifier in node @xmath25 of the tree whenever it is clear from the context that we consider a fixed tree node . ] ) means that the example @xmath26 is sent to the right subtree of node @xmath25 , while @xmath27 sends @xmath26 to the left subtree .",
    "when we reach a leaf , we predict according to the label with the highest frequency amongst the examples reaching that leaf .    in the interest of computational complexity , we want to encourage the number of examples going to the left and right to be _ fairly balanced_. for good statistical accuracy ,",
    "we want to send examples of class @xmath11 almost exclusively to either the left or the right subtree , thereby refining the _ purity _ of the class distributions at subsequent levels in the tree .",
    "the _ purity _ of a tree node is therefore a measure of whether the examples of each class reaching the node are then mostly sent to its one child node ( pure split ) or otherwise to both children ( impure split ) .",
    "the formal definitions of _ balancedness _ and _ purity _ are introduced in section  [ sec : objective ] . an objective expressing both criteria , satisfy this requirement ( for the entropy - based criteria see  @xcite , for our criterion see lemma  [ lemma : maximal ] ) . ] and resulting theoretical properties are illustrated in the following sections .",
    "a key consideration in picking this objective is that we want to effectively optimize it over hypotheses @xmath21 , while streaming over examples in an online fashion .",
    "this seems unsuitable with some of the more standard decision tree objectives such as shannon or gini entropy , which leads us to design a new objective . at the same time",
    ", we show in section  [ sec : boosting ] that under suitable assumptions , optimizing the objective also leads to effective reduction of the average shannon entropy over the entire tree .",
    "we now define a criterion to measure the quality of a hypothesis @xmath21 in creating partitions at a fixed node @xmath25 in the tree .",
    "let @xmath28 denotes the proportion of label @xmath11 amongst the examples reaching this node .",
    "let @xmath29 and @xmath30 denote the fraction of examples reaching @xmath25 for which @xmath31 , marginally and conditional on class @xmath11 respectively .",
    "then we define the objective : @xmath32 we aim to _ maximize the objective @xmath33 _ to obtain high quality partitions .",
    "intuitively , the objective encourages the fraction of examples going to the right from class @xmath11 to be substantially different from the background fraction for each class @xmath11 . as a concrete simple scenario , if @xmath34 for some hypothesis @xmath35 , then the objective prefers @xmath36 to be as close to 0 or 1 as possible for each class @xmath11 , leading to pure partitions .",
    "we now make these intuitions more formal .",
    "the hypothesis @xmath37 induces a pure split if @xmath38 where @xmath39 , and @xmath40 is called the _",
    "purity factor_.    in particular , a partition is called _ maximally pure _ if @xmath41 , meaning that each class is sent exclusively to the left or the right .",
    "we now define a similar definition for the balancedness of a split .",
    "the hypothesis @xmath37 induces a balanced split if @xmath42 where @xmath43 $ ] , and @xmath44 is called the _ balancing factor_.    a partition is called _ maximally balanced _ if @xmath45 , meaning that an equal number of examples are sent to the left and right children of the partition .",
    "the balancing factor and the purity factor are related as shown in lemma  [ lemma : obj - to - purity ] ( the proofs of lemma  [ lemma : obj - to - purity ] and the following lemma ( lemma  [ lemma : maximal ] ) are deferred to the supplementary material ) .    for any hypothesis @xmath35 , and any distribution over examples @xmath46 , the purity factor @xmath40 and the balancing factor @xmath44 satisfy @xmath47 .",
    "[ lemma : obj - to - purity ]    a partition is called _ maximally pure and balanced _ if it satisfies both @xmath48 and @xmath45 .",
    "we see that @xmath49 for a hypothesis @xmath35 inducing a maximally pure and balanced partition as captured in the next lemma .",
    "of course we do not expect to have hypotheses producing maximally pure and balanced splits in practice .",
    "for any hypothesis @xmath50 , the objective @xmath33 satisfies @xmath51 $ ] . furthermore , if @xmath35 induces a maximally pure and balanced partition then @xmath49 .",
    "[ lemma : maximal ]      the above section helps us understand the quality of an individual split produced by effectively maximizing @xmath33 .",
    "we next reason about the quality of the entire tree as we add more and more nodes .",
    "we measure the quality of trees using the average entropy over all the leaves in the tree , and track the decrease of this entropy as a function of the number of nodes .",
    "our analysis extends the theoretical analysis in  @xcite , originally developed to show the boosting properties of the decision trees for binary classification problems , to the multiclass classification setting .",
    "given a tree @xmath52 , we consider the entropy function @xmath53 as the measure of the quality of tree : @xmath54 where @xmath55 s are the probabilities that a randomly chosen data point @xmath26 drawn from @xmath56 , where @xmath56 is a fixed target distribution over @xmath57 , has label @xmath11 given that @xmath26 reaches node @xmath58 , @xmath59 denotes the set of all tree leaves , @xmath60 denotes the number of internal tree nodes , and @xmath61 is the weight of leaf @xmath58 defined as the probability a randomly chosen @xmath26 drawn from @xmath56 reaches leaf @xmath58 ( note that @xmath62 ) .",
    "we next state the main theoretical result of this paper ( it is captured in theorem  [ thm : main ] ) .",
    "we adopt the _ weak learning _ framework . the _ weak hypothesis assumption _ , captured in definition",
    "[ def : wha ] , posits that each node of the tree @xmath52 has a hypothesis @xmath35 in its hypothesis class @xmath63 which guarantees simultaneously a `` weak  purity and a ' ' weak  balancedness of the split on any distribution @xmath56 over @xmath57 . under this assumption",
    ", one can use the new decision tree approach to drive the error below any threshold .",
    "let @xmath64 denote any node of the tree @xmath52 , and let @xmath65 and @xmath66 .",
    "furthermore , let @xmath67 be such that for all @xmath64 , @xmath68 $ ] .",
    "we say that the _ weak hypothesis assumption _ is satisfied when for any distribution @xmath56 over @xmath57 at each node @xmath64 of the tree @xmath52 there exists a hypothesis @xmath69 such that @xmath70 .",
    "[ def : wha ]    under the weak hypothesis assumption , for any @xmath71 $ ] , to obtain @xmath72 it suffices to make @xmath73 splits .",
    "[ thm : main ]    we defer the proof of theorem  [ thm : main ] to the supplementary material and provide its sketch now .",
    "the analysis studies a tree construction algorithm where we recursively find the leaf node with the highest weight , and choose to split it into two children .",
    "let @xmath25 be the heaviest leaf at time @xmath60 .",
    "consider splitting it to two children .",
    "the contribution of node @xmath25 to the tree entropy changes after it splits .",
    "this change ( entropy reduction ) corresponds to a gap in the jensen s inequality applied to the concave function , and thus can further be lower - bounded ( we use the fact that shannon entropy is strongly concave with respect to @xmath74-norm ( see e.g. , example 2.5 in shalev - shwartz  @xcite ) ) .",
    "the obtained lower - bound turns out to depend proportionally on @xmath75 .",
    "this implies that the larger the objective @xmath76 is at time @xmath60 , the larger the entropy reduction ends up being , which further reinforces intuitions to maximize @xmath77 . in general",
    ", it might not be possible to find any hypothesis with a large enough objective @xmath76 to guarantee sufficient progress at this point so we appeal to a _ weak learning assumption_. this assumption can be used to further lower - bound the entropy reduction and prove theorem  [ thm : main ] .",
    "[ cols=\"<\",options=\"header \" , ]     [ tab : testerr ]    the third hypothesis is weakly consistent with the empirical results .",
    "the time advantage of _ lomtree _ comes with some loss of statistical accuracy with respect to _ oaa _ where _ oaa _ is tractable .",
    "we conclude that _ lomtree _ significantly closes the gap between other logarithmic time methods and _ oaa _ , making it a plausible approach in computationally constrained large-@xmath0 applications .",
    "the lomtree algorithm reduces the multiclass problem to a set of binary problems organized in a tree structure where the partition in every tree node is done by optimizing a new partition criterion online .",
    "the criterion guarantees pure and balanced splits leading to logarithmic training and testing time for the tree classifier . we provide theoretical justification for our approach via a boosting statement and",
    "empirically evaluate it on multiple multiclass datasets .",
    "empirically , we find that this is the best available logarithmic time approach for multiclass classification problems .",
    "we would like to thank alekh agarwal , dean foster , robert schapire and matus telgarsky for valuable discussions .",
    "the most natural bottom - up construction for creating partitions is not viable as will be now shown by an example .",
    "bottom - up construction techniques start by pairing labels , either randomly or arbitrarily , and then building a predictor of whether the class label is left or right conditioned on the class label being one of the paired labels . in order to construct a full tree ,",
    "this operation must compose , pairing trees with size @xmath13 to create trees of size @xmath78 . here , we show that the straightforward approach to composition fails .",
    "suppose we have a one dimensional feature space with examples of class label @xmath11 having feature value @xmath11 and we work with threshold predictors .",
    "suppose we have 4 classes @xmath79 , and we happen to pair @xmath80 and @xmath81 .",
    "it is easy to build a linear predictor for each of these splits .",
    "the next step is building a predictor for @xmath80 vs @xmath81 which is impossible because all thresholds in @xmath82 , @xmath83 , and @xmath84 err on two labels while thresholds on @xmath85 and @xmath86 err on one label .",
    "we start from deriving an upper - bound on @xmath33 . for the ease of notation let @xmath87 .",
    "thus @xmath88 where @xmath89 .",
    "let @xmath90 and recall the purity factor @xmath91 and the balancing factor @xmath92 .",
    "without loss of generality let @xmath93 .",
    "furthermore , let @xmath94 @xmath95 first notice that @xmath96 therefore @xmath97 note that @xmath98 and therefore @xmath99 furthermore , since @xmath100 we further write that @xmath101 by equation  [ eq : medium ] , it can be further rewritten as @xmath102 since @xmath103 s are bounded by @xmath104 we obtain @xmath105 thus : @xmath106",
    "we first show that @xmath107 $ ] .",
    "we start from deriving an upper - bound on @xmath33 , where @xmath37 is some hypothesis in the hypothesis class . for the ease of notation let @xmath87 .",
    "thus @xmath108 where @xmath89 .",
    "the objective @xmath33 is certainly maximized on the extremes of the @xmath109 $ ] interval .",
    "the upper - bound on @xmath33 can be thus obtained by setting some of the @xmath110 s to @xmath111 s and remaining ones to @xmath10 s . to be more precise , let @xmath112",
    "therefore it follows that @xmath113\\\\ & = &   2\\left[\\sum_{i \\in l_1 } \\pi_i - ( \\sum_{i \\in l_1 } \\pi_i)^2 + ( 1 -   \\sum_{i \\in l_1 } \\pi_i ) \\sum_{i \\in l_1 } \\pi_i\\right]\\\\   & = & 4\\left[\\sum_{i \\in l_1 } \\pi_i - ( \\sum_{i \\in l_1 } \\pi_i)^2\\right]\\end{aligned}\\ ] ]    let @xmath114 thus @xmath115    since @xmath116 $ ] , it is straightforward that @xmath117 $ ] and thus @xmath107 $ ] .",
    "we now proceed to prove the main statement of lemma  [ lemma : maximal ] , if @xmath35 induces a maximally pure and balanced partition then @xmath49 . since @xmath35 is maximally balanced , @xmath34 .",
    "simultaneously , since @xmath35 is maximally pure @xmath118 . substituting that into equation  [ eq : objform ] yields that @xmath49 .",
    "the analysis studies a tree construction algorithm where we recursively find the leaf node with the highest weight , and choose to split it into two children .",
    "consider the tree constructed over @xmath60 steps where in each step we take one leaf node and split it into two .",
    "let @xmath25 be the heaviest node at time @xmath60 and its weight @xmath119 be denoted by @xmath120 for brevity .",
    "consider splitting this leaf to two children @xmath121 and @xmath122 .",
    "for the ease of notation let @xmath123 and @xmath124 . also for the ease of notation let @xmath125 and @xmath126 . let @xmath28 be the shorthand for @xmath127 and @xmath35 be the shorthand for @xmath24 . recall that @xmath128 and @xmath129 .",
    "also notice that @xmath130 and @xmath131 .",
    "let @xmath132 be the @xmath0-element vector with @xmath133 entry equal to @xmath28 .",
    "furthermore let @xmath134 .",
    "before the split the contribution of node @xmath25 to @xmath53 was @xmath135 .",
    "let @xmath136 and @xmath137 be the probabilities that a randomly chosen @xmath26 drawn from @xmath56 has label @xmath11 given that @xmath26 reaches nodes @xmath121 and @xmath122 respectively . for brevity ,",
    "let @xmath138 be denoted by @xmath139 and @xmath140 be denoted by @xmath141 .",
    "furthermore let @xmath142 be the @xmath0-element vector with @xmath133 entry equal to @xmath139 and let @xmath143 be the @xmath0-element vector with @xmath133 entry equal to @xmath141 .",
    "notice that @xmath144 .",
    "after the split the contribution of the same , now internal , node @xmath25 changes to @xmath145 .",
    "we denote the difference between them as @xmath146 and thus @xmath147 .",
    "\\label{eqn : ent - decrease } \\vspace{-0.02in}\\ ] ] we aim to lower - bound @xmath146 .",
    "the entropy reduction of equation  [ eqn : ent - decrease ]  @xcite corresponds to a gap in the jensen s inequality applied to the concave function @xmath148 .",
    "this leads to the lower - bound on @xmath146 given in lemma  [ lem : lower - bound ] ( the lemma is proven in section  [ sec : lower - boundproof ] in the supplementary material ) .",
    "the entropy reduction @xmath146 of equation  [ eqn : ent - decrease ] can be lower - bounded as follows @xmath149 [ lem : lower - bound ]    lemma  [ lem : lower - bound ] implies that the larger the objective @xmath33 is at time @xmath60 , the larger the entropy reduction ends up being , which further reinforces intuitions to maximize @xmath77 . in general , it might not be possible to find any hypothesis with a large enough objective @xmath33 to guarantee sufficient progress at this point so we appeal to a _ weak learning assumption_. this assumption can be used to further lower - bound @xmath146 .",
    "the lower - bound can then be used ( details are in section  [ sec : maindetails ] in the supplementary material ) to obtain the main theoretical statement of the paper captured in theorem  [ thm : main ] .    from the definition of @xmath150",
    "it follows that @xmath151 .",
    "also note that the _ weak hypothesis assumption _",
    "guarantees @xmath152 , which applied to the lower - bound on @xmath146 captured in lemma  [ lem : lower - bound ] yields @xmath153 let @xmath154",
    ". then @xmath155 .",
    "thus we obtain the recurrence inequality @xmath156   \\vspace{-0.02in}\\ ] ] one can now compute the minimum number of splits required to reduce @xmath53 below @xmath40 , where @xmath71 $ ] . applying the proof technique from  @xcite ( the proof of theorem 10 )",
    "gives the final statement of theorem  [ thm : main ] .",
    "without loss of generality assume that @xmath157 .",
    "as mentioned before , the entropy reduction @xmath146 corresponds to a gap in the jensen s inequality applied to the concave function @xmath158 . also recall that shannon entropy is strongly concave with respect to @xmath74-norm ( see e.g. , example 2.5 in shalev - shwartz  @xcite ) . as a specific consequence ( see e.g. theorem 2.1.9 in nesterov  @xcite ) we obtain @xmath159 where the last equality results from the definition of @xmath160 .",
    "note that the following holds @xmath161 , where recall that @xmath120 is the weight of the heaviest leaf in the tree , i.e. the leaf with the highest weight , at round @xmath60 .",
    "this leaf is selected to the currently considered split  @xcite . in particular ,",
    "the lower - bound on @xmath120 is the consequence of the following @xmath162 where @xmath163 .",
    "thus @xmath164 which when substituted to equation  [ eq : subst ] gives the final statement of the lemma .",
    "we bound the number of swaps that any node makes .",
    "consider @xmath165 and let @xmath166 be the node that is about to split and @xmath167 be the orphan node that will be recycled ( thus @xmath168 ) .",
    "the condition in equation  [ eq : swap_cond ] implies that the swap is done if @xmath169 .",
    "algorithm  [ alg : ott ] makes @xmath167 a child of @xmath166 during the swap and sets its counter to @xmath170 .",
    "then @xmath171 gets updated . since the value of @xmath172",
    "at least doubles after a swap and all counters are bounded by the number of examples @xmath25 , the node can be involved in at most @xmath173 swaps .",
    "consider the objective function as given in equation  [ eqn : objective ] @xmath174 recall that @xmath57 denotes the set of all examples and let @xmath175 denote the set of examples in class @xmath11 .",
    "also let @xmath176 denote the cardinality of set @xmath57 and let @xmath177 denote the cardinality of set @xmath175 .",
    "then we can re - write the objective as @xmath178 - \\mathbb{e}_{x}[\\mathds{1}(h(x ) > 0|i ) ]    \\right| \\nonumber\\\\ & = & 2\\mathbb{e}_i[\\left| \\mathbb{e}_x[\\mathds{1}(h(x ) > 0 ) ] - \\mathbb{e}_{x}[\\mathds{1}(h(x ) > 0|i ) ]    \\right| ] .",
    "\\nonumber\\end{aligned}\\ ] ]",
    "figure  [ fig : root_example ] shows the toy example of the behavior of lomtree algorithm for the first few data points . without loss of generality we consider the root node ( exactly the same actions would be performed in any other tree node ) .",
    "notice that the algorithm achieves simultaneously balanced and pure split of classes reaching the considered node .",
    "@xmath179 denotes the expectation @xmath180 $ ] , and @xmath181 denote the expectations @xmath182 $ ] , @xmath183 $ ] , @xmath184 $ ] , and @xmath185 $ ] . for simplicity",
    "we assume score @xmath186 can only be either @xmath111 ( if the example is sent to the right ) or @xmath187 ( if the example is sent to the left ) .",
    "the figure should be read as follows ( we explain how to read first few illustrations ) :    1 .",
    "root is initialized .",
    "expectation @xmath179 is initialized to @xmath10 .",
    "2 .   the first example @xmath188 comes with label @xmath111 ( we denote it as @xmath189 ) .",
    "@xmath190 is initialized to @xmath10 .",
    "the difference between @xmath179 and @xmath190 is computed : @xmath191 .",
    "the difference is non - positive thus the example is sent to the right child of the root , which is now being created ( the left child is created along with the right child as we always create both children of any node simultaneously ) .",
    "3 .   expectations @xmath179 and @xmath190 get updated .",
    "it is shown that root and its right child saw an example of class @xmath111 .",
    "the second example @xmath192 comes with label @xmath13 ( we denote it as @xmath193 ) .",
    "@xmath194 is initialized to @xmath10 .",
    "the difference between @xmath179 and @xmath194 is computed : @xmath195 .",
    "the difference is positive thus the example is sent to the left child of the root .",
    "expectations @xmath179 and @xmath194 get updated . it is shown that root saw examples of class @xmath111 and @xmath13 , whereas its resp . left and right child saw example of class resp . @xmath13 and @xmath111 .",
    "@xmath196    \\a ) b ) c ) + d ) e ) f ) + g ) h ) i ) + j ) k )",
    "below we provide the details of the datasets that we were using for the experiments in section  [ sec : experiments ] :    * _ isolet _ : downloaded from http://www.cs.huji.ac.il/~shais/datasets/classificationdatasets.html * _ sector _ and _ aloi _ : downloaded from http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html * _ imagenet _",
    "@xcite : features extracted according to http://www.di.ens.fr/willow/research/cnn/ , dataset obtained from the authors . *",
    "@xcite : obtained from paul bennett .",
    "our version has significantly more classes than reported in the cited paper because we use the entire dataset ."
  ],
  "abstract_text": [
    "<S> we study the problem of multiclass classification with an extremely large number of classes ( @xmath0 ) , with the goal of obtaining train and test time complexity logarithmic in the number of classes . </S>",
    "<S> we develop top - down tree construction approaches for constructing logarithmic depth trees . on the theoretical front </S>",
    "<S> , we formulate a new objective function , which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure ( in terms of class labels ) and balanced . </S>",
    "<S> we demonstrate that under favorable conditions , we can construct logarithmic depth trees that have leaves with low label entropy . </S>",
    "<S> however , the objective function at the nodes is challenging to optimize computationally . </S>",
    "<S> we address the empirical problem with a new online decision tree construction procedure . </S>",
    "<S> experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches , which makes it a plausible method in computationally constrained large-@xmath0 applications . </S>"
  ]
}