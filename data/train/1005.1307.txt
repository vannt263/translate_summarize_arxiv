{
  "article_text": [
    "consider the regression model @xmath1 , where @xmath2 for @xmath3 , and conditionally on the regressors @xmath4 s , @xmath5 are i.i.d .",
    "@xmath6 with @xmath7 . suppose that we are interested in knowing whether the true regression curve @xmath8 is nondecreasing on some sub - interval of @xmath0 $ ] .",
    "@xcite considered the nonparametric test based on the maximum difference between the cumulative sum diagram of the observations and its concave majorant , multiplied by @xmath9 and divided by any consistent estimator of @xmath10 .",
    "the intuition behind this test is as follows : under the null hypothesis that @xmath11 is decreasing on @xmath0 $ ] , the function @xmath12 , @xmath13 $ ] , is concave , and hence the cumulative sum diagram of the data must be very close  to its concave majorant as @xmath14 .",
    "@xcite showed that asymptotically , the type i error of the test attains its maximum when @xmath8 is constant on @xmath0 $ ] .",
    "then , under this least favorable case , the test statistic converges weakly to the maximum difference between a standard brownian motion on @xmath0 $ ] starting at 0 and its concave majorant .",
    "if @xmath15 , @xmath16 denote a standard brownian motion on @xmath0 $ ] starting at 0 and @xmath17 its concave majorant on @xmath0 $ ] , then durot s test statistic converges weakly to @xmath18 } \\left ( \\widehat b(t ) - b(t)\\right).\\end{aligned}\\ ] ] a similar testing problem for densities was considered by @xcite .",
    "the test can be based on the maximum difference between the empirical distribution and its concave majorant , multiplied by @xmath9 .",
    "when the true density is uniform on @xmath0 $ ] , this maximum difference converges weakly to the distribution of @xmath19 as in the regression setting above .",
    "as proved in proposition 4 ( iii ) in @xcite , one interesting property of the distribution of @xmath19 is that we can replace @xmath20 in ( [ m ] ) by a standard brownian _",
    "bridge _ ; i.e , the distribution of @xmath19 is also that of the maximum difference between a standard brownian bridge and its concave majorant . furthermore , the random variable @xmath19 can be given under a more useful form .",
    "let @xmath21 denote the maximum of a brownian excursion and @xmath22 an infinite sequence of independent random variables distributed as @xmath21 .",
    "if @xmath23 is an infinite sequence of independent uniform random variables on @xmath0 $ ] and @xmath24 the corresponding uniform stick - breaking process ; i.e. , @xmath25 then @xcite proved in theorem 1 that @xmath26 absolute continuity of @xmath19 is an immediate corollary of ( [ maintheo ] ) .",
    "two other corollaries will follow from the same equality in distribution giving formulae for @xmath27 , the cdf of @xmath19 . if @xmath28 is the cdf of @xmath21 , then @xmath29 , \\",
    "\\ \\forall \\ x > 0\\end{aligned}\\ ] ]",
    "the expectation in the formula above is taken with respect to the stick - breaking process , and @xmath28 is known to be given by @xmath30 see e.g. @xcite and @xcite .    a second formula , which follows from proposition 7 and theorem 8 of @xcite , gives @xmath27 as a function of the inverse of a laplace transform .",
    "let @xmath31 denote the modified bessel function of the second kind and order @xmath32 , and @xmath33 the function defined by @xmath34 then , @xmath35 where @xmath36 denotes the value of the inverse of laplace transform of @xmath37 at @xmath38 .",
    "we describe in section 2 and 3 the implementation of two variants based on a monte carlo approach and a gaver - stehfest algorithm for approximating the inverse of laplace transform .",
    "if the monte carlo ( mc ) methods are easy to implement , they both require a very large number of simulations in order to obtain the same precision as the deterministic gaver - stehfest ( gs ) algorithm . for @xmath39 ,",
    "gs is able to approximate very accurately the cumulative distribution of @xmath19 at @xmath40 using a multiple precision library .",
    "for values @xmath40 below what it seems to be a cut - off point for both methods , it is difficult to get a precise approximation for the distribution of @xmath19 .",
    "this problem does not affect the calculation of the upper quantiles which is one of the main motivations of the work .",
    "although the mc approach is not as efficient as the gs algorithm , it seemed natural to describe it in the sequel .",
    "we only report the numerical results of the gs algorithm , however .",
    "tables [ disfun1 ] , [ disfun2 ] and [ disfun3 ] below give approximated values of the distribution function of @xmath19 on a grid of real numbers @xmath40 such that @xmath41 with a regular mesh equal to 0.01 .",
    "the approximation was performed with a precision ensuring up to 60 significant digits .",
    "a table of quantiles of order @xmath42 is given as well .",
    "this table can be compared to the monte carlo approximated quantiles obtained by @xcite .",
    "all the code used in the numerical computations in this paper is available at http://www.ceremade.dauphine.fr/~fadoua / bf2010_code/.",
    "we consider two different mc - based algorithms .",
    "they have the advantage of being very easy to understand and implement .",
    "the first approach is straightforwardly based on the expression of the distribution function of @xmath19 given in ( [ expmontecarlo ] ) . because of the infinite product in ( [ expmontecarlo ] ) , a first approximation due to the truncation of the product is introduced .",
    "control of the error due to this approximation is important in order to obtain a good theoretical estimator .",
    "let @xmath43 be some finite integer and consider the problem of estimating @xmath44 , \\ \\",
    "\\forall \\ x > 0.\\ ] ]    for @xmath45 and a given @xmath46 ,",
    "the following lemma gives a lower bound for @xmath47 so that @xmath48    [ lemmaapproxj ] the approximation error satisfies ( [ approxj ] ) if @xmath49    * proof . * see appendix .    for @xmath45 and a given @xmath50 , we draw @xmath51 independent copies @xmath52 for @xmath53 to estimate @xmath54 where @xmath55 as given in lemma [ approxj ] .",
    "the resulting monte carlo estimator is @xmath56 the computation of the distribution function @xmath28 imposes yet another approximation due to the fact that it is defined through an infinite series . the number of terms in the approximating finite sum needs to be larger for smaller values of @xmath40 .",
    "now by the central limit theorem , we have @xmath57 with @xmath58.\\ ] ] let @xmath59 be the @xmath60- quantile of a standard normal for some small @xmath61 . then , for @xmath51 large enough the event @xmath62\\end{aligned}\\ ] ] occurs with probability @xmath63 . combining both the deterministic and monte carlo approximations and noting that @xmath64 $ ]",
    ", it follows that @xmath65\\end{aligned}\\ ] ] occurs with at least probability @xmath66 .",
    "hence , to ensure an error of order @xmath67 , the sample size @xmath51 should be chosen of order @xmath68 . therefore , very large sample sizes are needed to get accurate results . to give an order of magnitude , table [ j0c ] shows several values of @xmath69 and @xmath51 corresponding to desired precision targets .",
    "all the values are computed for @xmath70 , where 0.33 appears to be the numerical limit of what we can compute without violating the basic properties of a distribution function .",
    "this point will be brought up again in the next section .",
    "note that the main purpose of table [ j0c ] is to give an idea about how @xmath69 and @xmath51 behave as functions of the precision .",
    "for instance , a precision of order @xmath71 is useless if the goal is to compute an approximation of the value distribution function of @xmath19 at @xmath72 since it is of order @xmath73 as found with the gs algorithm .",
    "we use the above mc approach to estimate the distribution function of @xmath19 for @xmath41 as well as the upper quantiles .",
    "the algorithm is implemented in c. this method turns out to be very slow for large sample sizes .",
    "moderate sample sizes ( of order @xmath74 ) do not give the desired accuracy for small @xmath40 .",
    "the estimates of the distribution function for large @xmath40 ( of order 0.80 and above ) as well as the upper quantiles match with those obtained by gs algorithm ( see next section ) .    in the same vein , one can consider a second variant of mc .",
    "it is mainly based on the following result due to kennedy 1976 ( see corollary on page 372 ) : @xmath75 } { b^{\\mbox{$\\scriptstyle{\\rm br}$}}}(t ) - \\inf_{t \\in [ 0,1 ] } { b^{\\mbox{$\\scriptstyle{\\rm br}$}}}(t)\\ ] ] where @xmath76 is a brownian bridge on length 1 . now using the well - known donsker approximation ,",
    "the distribution of @xmath21 can be approximated for large @xmath77 by the distribution of the random variable @xmath78 } \\sqrt{n }   ( \\mathbb g_n(t ) - t )",
    "- \\inf_{t \\in [ 0,1 ] } \\sqrt{n }   ( \\mathbb g_n(t ) - t)\\ ] ] where @xmath79 is the uniform empirical process based on @xmath77 independent uniform random variables @xmath80 in @xmath0 $ ] .",
    "using the fact that @xmath81 is a constant function between the order statistics @xmath82 , it can be easily shown that @xmath83 now the formula in ( [ maintheo ] ) yields the weak approximation @xmath84 where @xmath85 are independent random variables distributed as @xmath86 , and @xmath47 is a positive integer that should be chosen large enough to have the truncation error under control as done above .",
    "the distribution function of @xmath19 can be estimated empirically by generating @xmath51 independent random variables @xmath87 with the same distribution as @xmath88 .",
    "if this second variant of mc has the drawback of adding another error due to the stochastic approximation of @xmath28 by that of @xmath86 , it gives the possibility to generate samples with a distribution close to that of @xmath19 for @xmath47 , @xmath77 and @xmath51 large enough .",
    "we will not pursue here the calculation of the approximation error as a function of @xmath47 , @xmath77 and @xmath51 , which have to be very large to achieve high precision .",
    "the plot in figure [ mcf ] shows an estimation of @xmath27 using the first mc method with @xmath89 and @xmath90 . if the values are not accurate for small @xmath40 , the plot gives nevertheless a good idea about the true shape of @xmath27 .",
    "this is confirmed by the approximation results we obtain with the numerical inversion of laplace transform .",
    "the trajectory of 1000 independent random variables with the same distribution of @xmath91 for @xmath92 an @xmath93 is shown in figure [ simu1000 ] .",
    "the sample was extracted from a larger one of size 10,000 with an empirical mean and standard deviation equal to @xmath94 and @xmath95 respectively .    if the mc approach gives a first idea of the support and shape of the distribution of @xmath19 , it is not satisfactory in terms of efficiency and precision . as we show in the next section ,",
    "the gs algorithm is a much better choice in both respects .",
    "the gaver - stehfest ( gs ) algorithm is one of several algorithms of numerical inversion of laplace transform . for an excellent description of these algorithms ,",
    "see @xcite .",
    "the gs algorithm is different from other inversion procedures in that it involves only real numbers , but it also requires a very high numerical precision as we explain below ( also see  @xcite , p. 415 ) .",
    "if @xmath96 is the laplace transform of some function @xmath11 defined on @xmath97 , then gs approximation of @xmath11 is given by @xmath98 where @xmath99 is an integer in @xmath100 and @xmath101 under the assumption that the inverse of laplace transform @xmath11 has all its singularity points in @xmath102 $ ] and that is infinitely differentiable on @xmath103 , an extensive computation study carried out by @xcite has shown that @xmath104 if the function @xmath11 is bounded by 1 say , then the approximation in ( [ approxinvlt ] ) for well - behaved functions ( in the sense given above ) coincides with the truth up to significant @xmath105 digits .",
    "hence , the bigger @xmath99 is , the better is the approximation .",
    "however , for large values of @xmath99 , the binomial coefficients in @xmath106 become extremely large and require high numerical precision .",
    "such a facility is typically provided by a multiple precision ( mp ) numerical library or is built - in in some programming languages .    for a given integer @xmath107 , let @xmath108 denote the gs approximation of @xmath27 . from the formula of @xmath27 in ( [ expinvlt ] ) and ( [ approxinvlt ] )",
    ", it is easily seen that @xmath109 where @xmath33 is the same function defined by the infinite product in ( [ g ] ) .    for @xmath45 and a given @xmath50 we approximate @xmath33 by the product of the first @xmath77 terms , where @xmath77 is a positive integer depending on @xmath40 and @xmath67 .",
    "define @xmath110 the truncated version of @xmath33 .",
    "this truncation induces an additional error which we need to control .",
    "in fact , in computing the gaver - stehfest approximation of the distribution function @xmath27 , we actually replace @xmath111 in ( [ fk ] ) by @xmath112 the following shows that the error due to replacing @xmath113 by @xmath114 does not exceed a given threshold @xmath50 provided than @xmath77 is large enough .    [ approxg ] for @xmath50 , we have @xmath115 if @xmath116 where @xmath117    * proof .",
    "*  see appendix .    from lemma [ approxg ]",
    "it follows that @xmath118 the second term in the left side is known to be of order @xmath119 , and hence the approximation is of the same order if @xmath67 is chosen to be @xmath120 , and of order @xmath67 if the latter dominates and @xmath77 is chosen to be larger or equal than @xmath121 given in ( [ lowern ] ) .",
    "we implement the multiple precision calculation of @xmath122 in c++ using two open - source libraries for arbitrary precision computation : the gnu multiple precision arithmetic library ( see @xcite ) and the multiple precision floating - point reliable library ( mpfr ) ; see @xcite .",
    "gmp is an optimized library written in c with assembly code for common inner loops .",
    "mpfr is built on top of gmp and adds support for common floating - point operations such as @xmath123 .    to approximate the bessel functions in ( [ g ] ) , we use bessel routines from the alglib library based on piecewise rational and chebyshev polynomial approximations .",
    "we use a precision of 4000 bits to represent multiple precision floating - point numbers .",
    "however , the provided aglib bessel approximations only guaranty a maximal error of order @xmath124 . as a proof - of - concept",
    ", we have also implemented the same algorithm using a much slower but more accurate numerical library in python . for small values of @xmath40 such as 0.30 , 0.31 , and 0.32 , and unlike with the c library , we obtain results consistent with the monotonicity and positivity of a cumulative distribution function . for @xmath125 , @xmath126",
    ", the python code gives the following approximations @xmath127 for @xmath128 , @xmath129 for @xmath128 and @xmath130 for @xmath131 .",
    "computing @xmath132 $ ] takes about 6 hours ( 90 seconds per function evaluation ) on a 2ghz single - processor machine .",
    "the computation is dominated by the evaluation of @xmath33 in ( [ g ] ) .",
    "the coefficients @xmath133 need to be computed only once .",
    "tables [ disfun1 ] , [ disfun2 ] and [ disfun3 ] give the approximated values of @xmath27 on a grid starting at 0.33 and ending at 2.54 with a regular mesh chosen to be equal 0.01 .    finally , computing the upper quantiles of order is crucial when using the kolomogorov type monotonicity test based on the maximal distance between the empirical cumulative sum diagram ( resp .",
    "the empirical distribution ) in the regression estimation setting ( resp .",
    "the density estimation setting ) , see @xcite and @xcite .",
    "the gs algorithm can be easily used to approximate the upper quantiles of order @xmath134 .",
    "note that these quantiles are between 1.33 and 1.72 ( see table [ disfun2 ] ) .",
    "for each quantile , we used a binary search and stopped when the difference between the gs approximation of @xmath27 at the point and the targeted probability falls below a given threshold ( @xmath135 in the results we report ) .",
    "the results are shown in table [ quantiles ] .",
    "this table is to be compared with the one published by @xcite who obtained the quantiles for the same probabilities using a monte carlo approach .    in this paper , monte carlo and a numerical inversion of the laplace transform",
    "were used to estimate the distribution function and upper quantiles of @xmath19 , the maximal difference between a brownian motion on @xmath0 $ ] ( or a brownian bridge of length 1 ) and its concave majorant .",
    "this random variable determines the asymptotic critical region of a nonparametric test for monotonicity of a density or regression curve .",
    "we find the numerical inversion of laplace transform , based here on the gaver - stehfest algorithm , to be much more accurate and faster than the monte carlo method .",
    "numerical inversion of laplace transform was then very well adapted to this problem .",
    "however , it would not have been possible to use such an efficient method if a laplace transform representation of the distribution of @xmath19 was not available , see @xcite .",
    "finally , we would like to draw the reader s attention to the earlier computational work of @xcite on chernoff s distribution .",
    "the latter appears as the limit distribution of the grenander estimator ; that is the maximum likelihood estimator of a decreasing density on @xmath103 . in their work , @xcite have also used a mathematical characterization of chernoff s distribution .",
    "this allowed for a very efficient and fast approximation procedure which also outperformed monte carlo estimation .",
    "[ simu1000 ]    .order of the lower bound @xmath69 and sample size @xmath51 . [ cols=\"^,^,^ \" , ]",
    "the following facts will be used in the proof of lemma [ lemmaapproxj ] .",
    "the first identity can be proved recursively . for @xmath136 , we have @xmath137 .",
    "suppose that @xmath138 for all @xmath139 .",
    "it is easy to check that @xmath140 by independence of @xmath141 and @xmath142 , we can write @xmath143 and the identity is proved for all @xmath144 .    for the second inequality",
    ", we will use the fact that for a given @xmath145 @xmath146 consider the function @xmath147 the study of variations of @xmath37 shows that @xmath37 is increasing on @xmath148 $ ] and decreasing on @xmath149 with with @xmath150 , @xmath151 and @xmath152 now , the function @xmath153 is decreasing on @xmath154 with @xmath155 , and hence @xmath156 .",
    "it follows that @xmath157 and the inequality in ( [ ineqinterm ] ) is proved .",
    "it follows that @xmath158 to show that @xmath159 for all @xmath160 , it is enough to show that @xmath161 or equivalently @xmath162 the preceding inequality can be proved as follows .",
    "define the function @xmath163 we will show now that @xmath164 for all @xmath165 . for @xmath165 , we have @xmath166 where @xmath167.\\end{aligned}\\ ] ] it is easy to show that @xmath168 is increasing on @xmath169 $ ] and hence @xmath170 .",
    "it follows that the function @xmath171 is increasing on @xmath172 .",
    "since @xmath173 , the inequality @xmath174 follows .",
    "@xmath175       define @xmath176 we have @xmath177 \\\\                      & \\le & e[\\delta_j ] \\\\                      &   =   &   e\\left[\\delta_j 1_{\\delta_j \\le \\epsilon}\\right ] +    e\\left[\\delta_j 1_{\\delta_j > \\epsilon}\\right ] \\\\                      & \\le & \\epsilon + p(\\delta_j > \\epsilon).\\end{aligned}\\ ] ] let @xmath178 be the event @xmath179 and @xmath180 its complement .",
    "we can write @xmath181 using lemma a.1 ( ii ) and the chebyshev inequality , we get @xmath182 and @xmath183 hence , @xmath184 to have this approximation error smaller than @xmath185 , it suffices to take @xmath186 if @xmath187 we can take @xmath188 and lemma [ approxj ] is proved .",
    "@xmath175      the modified bessel function of the second kind @xmath189 is known to converge to 0 as @xmath190 .",
    "moreover we have @xmath191 and @xmath192 see lemma a.2 . for @xmath193 , define @xmath194 so that @xmath195 . also , for @xmath196 let @xmath197 so that @xmath198 .",
    "we have @xmath199 where @xmath200 .",
    "let us write again @xmath201 for the gaver - stehfest approximation of the inverse of laplace transform of @xmath202 .",
    "the corresponding approximation error due to truncating @xmath33 is given by @xmath203 by ( [ fondineq ] ) , we can write @xmath204 where @xmath205 .",
    "now , @xmath206 and so @xmath207 for @xmath208 . the coefficients @xmath106 can be loosely bounded using the following upper bounds for binomial coefficients @xmath209 for @xmath208 , we have @xmath210 so that @xmath211 hence , if we impose that @xmath212 , then it is enough to choose @xmath77 such that @xmath213      * proof .",
    "*  let us recall some well - known facts about modified bessel functions of the second kind .",
    "@xmath215 see e.g. abramowitz and stegun 1964 .",
    "note first that by ( [ propk1 ] ) , the inequality stated in the lemma is equivalent to @xmath216 from ( [ propk1 ] ) , ( [ propk2 ] ) and ( [ propk3 ] ) , it follows that @xmath217 let us write @xmath218 .",
    "suppose now that there exists @xmath45 such that @xmath219 .",
    "this would imply that there exists @xmath220 such that @xmath221 and @xmath222 .",
    "now , using ( [ propk5 ] ) and ( [ propk6 ] ) it follows that @xmath223 hence , @xmath224 satisfies @xmath225 it follows that @xmath226 since @xmath227 and @xmath228 for all @xmath45 , we must have @xmath229 .",
    "but if @xmath229 , then the previous inequality implies @xmath230 which is impossible by ( [ propk7 ] ) ."
  ],
  "abstract_text": [
    "<S> in this paper , we describe two computational methods for calculating the cumulative distribution function and the upper quantiles of the maximal difference between a brownian bridge and its concave majorant . </S>",
    "<S> the first method has two different variants that are both based on a monte carlo approach , whereas the second uses the gaver - stehfest ( gs ) algorithm for numerical inversion of laplace transform . </S>",
    "<S> if the former method is straightforward to implement , it is very much outperformed by the gs algorithm , which provides a very accurate approximation of the cumulative distribution as well as its upper quantiles . </S>",
    "<S> our numerical work has a direct application in statistics : the maximal difference between a brownian bridge and its concave majorant arises in connection with a nonparametric test for monotonicity of a density or regression curve on @xmath0 $ ] . </S>",
    "<S> our results can be used to construct very accurate rejection region for this test at a given asymptotic level . </S>"
  ]
}