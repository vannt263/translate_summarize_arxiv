{
  "article_text": [
    "continuous advancement in vlsi technologies has resulted in extremely small transistor sizes and highly complex microprocessors . however , on - chip interconnects responsible for on - chip communication have been improved only moderately .",
    "this leads to the `` paradox '' that local information processing is done very efficiently , but communicating information between on - chip units is a major challenge .",
    "this work focuses on an emergent issue expected to challenge circuit development in future technologies : information communication and processing is associated with energy dissipation into heat which raises the temperature of the transmitter / receiver or processing devices ; moreover , the intrinsic device noise level depends strongly and increasingly on the temperature .",
    "therefore , the total physical structure can be modeled as a communication channel whose noise level is data dependent .",
    "this channel was studied at low transmit power levels in @xcite where it was shown that in the low power limit the heating effect is beneficial . in this paper , we focus on the high transmit power case . when the allowed transmit power is large , then there is a trade - off between optimizing the present transmission and minimizing the interference to future transmissions . indeed , increasing the transmission power may help to overcome the present ambient noise , but it also heats up the chip and thus increases the noise variance in future receptions .",
    "_ prima facie _ it is not clear that , as we increase the allowed transmit power , the capacity tends to infinity .",
    "this paper studies conditions under which the capacity is bounded in the transmit power .",
    "we consider the communication system depicted in figure  [ fig1 ] .",
    "the message @xmath0 to be transmitted over the channel is assumed to be uniformly distributed over the set @xmath1 for some positive integer @xmath2 .",
    "the encoder maps the message to the length-@xmath3 sequence @xmath4 , where @xmath3 is called the _ block - length_. thus , in the absence of feedback , the sequence @xmath5 is a function of the message @xmath0 , i.e. , @xmath6 for some mapping @xmath7 . here , @xmath8 stands for @xmath9 , and @xmath10 denotes the set of real numbers .",
    "if there is a feedback link , then @xmath11 , @xmath12 is a function of the message @xmath0 and , additionally , of the past channel output symbols @xmath13 , i.e. , @xmath14 for some mapping @xmath15 . the receiver guesses the transmitted message @xmath0 based on the @xmath3 channel output symbols @xmath16 , i.e. , @xmath17 for some mapping @xmath18 .",
    "[ cc][cc]transmitter [ cc][cc]channel [ cc][cc]receiver [ cc][cc]delay [ b][b]@xmath0 [ b][b]@xmath19 [ b][b]@xmath11 [ b][b]@xmath20 [ b][b]@xmath13    let @xmath21 denote the set of positive integers .",
    "the channel output @xmath22 at time @xmath23 corresponding to the channel inputs @xmath24 is given by @xmath25 where @xmath26 are independent and identically distributed ( iid ) , zero - mean , unit - variance random variables , drawn independently of @xmath0 , and being of finite fourth moment and of finite differential entropy @xmath27 the most interesting case is when @xmath26 are iid , zero - mean , unit - variance gaussian random variables , and the reader is encouraged to focus on this case .",
    "the coefficients @xmath28 in are non - negative and bounded , i.e. , @xmath29 note that this channel is not stationary as the variance of the additive noise depends on the time - index @xmath30 .",
    "we study the above channel under an average - power constraint on the inputs , i.e. , @xmath31 and we define the signal - to - noise ratio ( snr ) as @xmath32      let the _ rate _",
    "@xmath33 ( in nats per channel use ) be defined as @xmath34 where @xmath35 denotes the natural logarithm function .",
    "a rate is said to be _ achievable _ if there exists a sequence of mappings @xmath36 ( without feedback ) or @xmath37 ( with feedback ) and @xmath38 such that the error probability @xmath39 vanishes as @xmath3 tends to infinity .",
    "the _ capacity _",
    "@xmath40 is the supremum of all achievable rates .",
    "we denote by @xmath41 the capacity under the input constraint when there is no feedback , and we add the subscript `` fb '' to indicate that there is a feedback link .",
    "clearly , @xmath42 as we can always ignore the feedback link .    for the above channel the _ capacities per unit cost _ which are defined as @xcite @xmath43 were studied in @xcite under the additional assumptions that @xmath26 are iid , zero - mean , unit - variance gaussian random variables , and that the coefficients fulfill @xmath44 it was shown that",
    ", irrespective of whether feedback is available or not , the capacity per unit cost is given by @xmath45    in this paper , we focus on the high snr case . specifically , we explore the question whether the capacity is bounded or unbounded in the snr .",
    "we show that whether the capacity is bounded or not depends highly on the decay rate of the coefficients @xmath28 .",
    "this is stated precisely in the following theorem .",
    "consider the above channel model .",
    "then ,    llcl i ) & _ > 0 & & _ > 0 c_fb ( ) < [ eq : main1 ] + ii ) & _ = 0 & & _ >",
    "0 c ( ) = [ eq : main2 ]    where we define , for any @xmath46 , @xmath47 and @xmath48 .",
    "[ thm : main ]    for example , when @xmath28 is a geometric sequence , i.e. , for @xmath49 , then the capacity is bounded .",
    "part i ) of theorem  [ thm : main ] holds also when @xmath50 has an infinite fourth moment . in part",
    "ii ) of theorem  [ thm : main ] , the condition on the left - hand side ( lhs ) of can be replaced by the weaker condition @xmath51    a proof of theorem  [ thm : main ] is given in the next section . in section  [ sec : discussion ] we address the case where neither the lhs of nor the lhs of holds , i.e. , @xmath52 and @xmath53 .",
    "we show that in this case the capacity can be bounded or unbounded .",
    "in this section we provide a proof of theorem  [ thm : main ] .",
    "part i ) is proven in the next subsection , while the proof of part ii ) can be found in the subsequent subsection .      in order to show that @xmath54 implies that the feedback capacity @xmath55 is bounded , we derive a capacity upper bound which is , like in ( * ? ? ?",
    "8.12 ) , based on fano s inequality and on an upper bound on @xmath56 . to simplify notation",
    ", we define @xmath57 .",
    "we first note that , due to , we can find an @xmath58 and a @xmath49 so that @xmath59 we continue with the chain rule for mutual information @xcite    lcl + & = & _ k=1^_0 i(m;y_k|y_1^k-1)+_k=_0 + 1^n i(m;y_k|y_1^k-1).[eq : firstsum ]    each term in the first sum on the right - hand side ( rhs ) of is upper - bounded by    lcl + & & h(y_k ) - h(y_k|y_1^k-1,m ) + & = & h(y_k)- - h(u_k ) + & & ( 2e(1+_=1^k _ k- ) ) - h(u_k ) + & & ( 2e(1 + ( _  ^+_0 _  ) _",
    "= 1^k ) ) - h(u_k ) + & & ( 2e(1+(_ ^+_0 _",
    ")n ) ) -h(u_k)[eq : firstterm ]    where @xmath60 denotes the set of non - negative integers .",
    "recall that @xmath61 is assumed to be finite . here",
    ", the first inequality follows because conditioning can not increase entropy ; the following equality follows because @xmath62 is a function of @xmath63 and from the behavior of entropy under translation and scaling ( * ? ? ?",
    "9.6.3 & 9.6.4 ) in conjunction with the fact that @xmath50 is independent of @xmath64 ; the subsequent inequality follows from the entropy maximizing property of gaussian random variables ( * ? ? ?",
    "9.6.5 ) and by lower - bounding ; the next inequality by upper - bounding each coefficient , @xmath65 ; and the last inequality follows from @xmath66 , @xmath67 which is a consequence of the power constraint and of the fact that @xmath68 , @xmath69 .",
    "the terms in the second sum on the rhs of are upper - bounded using the general upper bound for mutual information ( * ? ? ?",
    "5.1 ) @xmath70 where @xmath71 denotes relative entropy , @xmath72 is the channel law , @xmath73 is the distribution on the channel input @xmath74 , and @xmath75 is any distribution on the output alphabet .",
    "thus , any choice of output distribution @xmath75 yields an upper bound on the mutual information .    for @xmath76",
    "we upper - bound @xmath77 for a given @xmath78 by choosing @xmath75 to be of a cauchy distribution whose density is given by @xmath79 where we choose the scale parameter @xmath80 to be then the density of the cauchy distribution is undefined .",
    "however , this event is of zero probability and has therefore no impact on the mutual information @xmath81 . ]",
    "@xmath82 and @xmath83 with @xmath49 and @xmath84 given by .",
    "note that together with the assumption that the coefficients @xmath28 are bounded implies that @xmath85 applying to yields    lcl + & & + ( y_k-_0 ^ 2 ) + + & & - h(y_k|m , y_1^k-1=y_1^k-1 ) ,    and we thus obtain , averaging over @xmath13 ,    lcl + & & - h(y_k|y_1^k-1,m ) + + & & + - -.[eq : ub1 ]    we evaluate the terms on the rhs of individually .",
    "we begin with @xmath86 where we use the same steps as in .",
    "the next term is upper - bounded by    lcl + & = & + & & + & = & + & & [ eq:2 ]    where , conditional on @xmath87 , @xmath88 here , the first inequality follows from jensen s inequality applied to the concave function @xmath89 , @xmath90 ; and the second inequality follows from .",
    "similarly , we use jensen s inequality along with to upper - bound    lcl + & & + & & 2 + .[eq:3 ]    in order to lower - bound @xmath91 we need the following lemma :    [ lemma : expectedlog ] let @xmath74 be a random variable of density @xmath92 , @xmath93 .",
    "then , for any @xmath94 and @xmath95 we have @xmath96 where @xmath97 denotes the indicator function ; @xmath98 is defined as @xmath99 and where @xmath100 tends to zero as @xmath101 .",
    "a proof can be found in ( * ? ? ?",
    "* lemma 6.7 ) .",
    "we write the expectation as    lcl + & = &    and lower - bound the conditional expectation for a given @xmath102 by    lcl + & = & _ x_1^k-_0 - 1 ^ 2 - 2 + & & _ x_1^k-_0 - 1 ^ 2 -2 ( , ) - h^-(u_k)+ ^2 [ eq : bla1 ]    for some @xmath94 and @xmath103 . here",
    ", the inequality follows by splitting the conditional expectation into the two expectations given in ( on the top of the next page ) and by upper - bounding then the first term on the rhs of using lemma  [ lemma : expectedlog ] and the second term by @xmath104 .",
    "lcl & = & + & & + [ eq : condexpect ]    ' '' ''    averaging over @xmath105 yields    lcl & & + & & - 2(,)-h^-(u_k)+^2.[eq:4 ]    note that , since @xmath50 is of unit variance , together with ( * ? ? ?",
    "* lemma 6.4 ) implies that @xmath106 is finite .    turning back to the upper bound we obtain from , , , and    lcl + & & 2 + + h^-(u_k ) + & & + 2(,)-^2- + & & - + + + & & - - h(u_k ) + & & + & & - + [ eq : ub2 ]    where @xmath107 is a finite constant , and where the last inequality in follows because for any @xmath108 we have @xmath109 .",
    "note that @xmath110 does not depend on @xmath30 as @xmath26 are iid .",
    "turning back to the evaluation of the second sum on the rhs of we use that for any sequences @xmath111 and @xmath112    lcl + & = & _ k = n-2_0 + 1^n ( a_k - b_k - n+3_0 ) + _ k=_0 + 1^n-2_0(a_k - b_k+2_0 ) .",
    "[ eq : sums ]    for @xmath113 we have    lcl + & & ( 1 + ( _ ^+_0 _ )",
    "n ) [ eq : wholesum1 ]    which follows by lower - bounding the denominator by @xmath114 , and by using then jensen s inequality together with the last two inequalities in . thus ,",
    "applying and to yields    lcl + & & + ( 1 + ( _ ^+_0 _ ) n ) + & & + _ k=_0 + 1^n-2_0 + & & + ( 1 + ( _ ^+_0 _ ) n ) + & & + _ k=_0 + 1^n-2_0 + & & - + & & + ( 1 + ( _ ^+_0 _ )",
    "n ) + & & - [ eq : ub3 ]    where the second inequality follows by adding @xmath115 to the expectation and by upper - bounding then @xmath116 , @xmath117 ; and the last inequality follows because for any given @xmath118 we have @xmath119 .    combining , , and",
    "we obtain    lcl + & & - + ( 2e ) - h(u_k ) + & & + ( 1 + ( _ ^+_0 _ ) n )    which converges to @xmath120 as we let @xmath3 go to infinity . with this",
    ", we have shown that @xmath121 implies that the capacity @xmath55 is bounded .",
    "we shall show that @xmath122 implies that the capacity @xmath41 in the absence of feedback is unbounded in the snr .",
    "part ii ) of theorem  [ thm : main ] follows then by noting that @xmath123    we prove the claim by proposing a coding scheme that achieves an unbounded rate .",
    "we first note that implies that for any @xmath124 we can find an @xmath125 so that @xmath126    if there exists an @xmath125 so that @xmath127 , @xmath128 , then we can achieve the ( unbounded ) rate @xmath129 by a coding scheme where the channel inputs @xmath130 are iid , zero - mean gaussian random variables of variance @xmath131 , and where the other inputs are deterministically zero . indeed , by waiting @xmath132 time - steps , the chip s temperature cools down to the ambient one so that the noise variance is independent of the previous channel inputs and we can achieve  after appropriate normalization  the capacity of the additive white gaussian noise ( awgn ) channel @xcite .    for the more general case we propose the following encoding and decoding scheme .",
    "let @xmath133 , @xmath134 denote the codeword sent out by the transmitter that corresponds to the message @xmath135 .",
    "we choose some period @xmath136 and generate the components @xmath137 , @xmath134 , @xmath138 ( where @xmath139 denotes the floor function ) independently of each other according to a zero - mean gaussian law of variance @xmath140 .",
    "the other components are set to zero .",
    ", @xmath141 converges to @xmath142 in probability as @xmath3 tends to infinity ; this guarantees that the probability that a codeword does not satisfy the power constraint vanishes as @xmath3 tends to infinity . ]",
    "the receiver uses a _",
    "nearest neighbor decoder _ in order to guess @xmath0 based on the received sequence of channel outputs @xmath143 .",
    "thus , it computes @xmath144 for any @xmath145 and decides on the message that satisfies @xmath146 where ties are resolved with a fair coin flip . here , @xmath147 denotes the euclidean norm , and @xmath148 and @xmath149 denote the respective vectors @xmath150 and @xmath151 .",
    "we are interested in the average probability of error @xmath152 , averaged over all codewords in the codebook , and averaged over all codebooks . due to the symmetry of the codebook construction ,",
    "the probability of error corresponding to the @xmath153-th message @xmath154 does not depend on @xmath153 , and we thus conclude that @xmath155 .",
    "we further note that @xmath156 where @xmath157 which is , conditional on @xmath158 , equal to @xmath159 . in order analyze",
    "we need the following lemma .",
    "[ lemma : typical ] consider the channel described in section  [ sub : channelmodel ] , and assume that the coefficients @xmath28 satisfy .",
    "further assume that @xmath130 are iid , zero - mean gaussian random variables of variance @xmath140 .",
    "let the set @xmath160 be defined as    lcl _ & & \\ { ( , ) ^n",
    "/ l^n / l : + & & | ^2-(^2++^(l ) ) | < , + & & | ^2-(^2+^(l ) ) | < } [ eq : setdef ]    with @xmath161 being defined as @xmath162 .",
    "then , @xmath163 for any @xmath164 .",
    "first note that , since @xmath50 has a finite fourth moment , our choice of input distribution implies that @xmath165 .",
    "this along with yields that the variances @xmath166 and @xmath167 vanish as @xmath3 tends to infinity .",
    "the lemma follows then by computing @xmath168 and @xmath169 and by chebyshev s inequality ( * ? ? ?",
    "5.4 ) .    in order to upper - bound the rhs of we proceed along the lines of @xcite , @xcite .",
    "we have    lcl + [ eq : typical1 ]    and it follows from lemma  [ lemma : typical ] that the first term on the rhs of vanishes as @xmath3 tends to infinity .",
    "note that since the codewords are independent of each other , conditional on @xmath158 , the distribution of @xmath170 , @xmath171 does not depend on @xmath172 .",
    "we upper - bound the second term on the rhs of by analyzing @xmath173 for each @xmath171 and by applying then the union of events bound .    for @xmath174 and @xmath171",
    "we have    lcl + & & \\{-s n / l ( ^2+^(l)+ ) + + & & - n / l ( 1 - 2s ) } , s<0 [ eq : typical2 ]    which follows by upper - bounding @xmath175 by @xmath176 and from the chernoff bound ( * ? ? ?",
    "5.4 ) . using that , for @xmath177 , @xmath178 it follows from the union of events bound and that goes to zero as @xmath3 tends to infinity if for some @xmath179    lcl r & < & ( ^2+^(l ) + ) + ( 1 - 2s ) + & & - .    thus , choosing @xmath180 yields that any rate below    l - + ( 1 + ) + + [ eq : ach1 ]    is achievable .",
    "as @xmath140 tends to infinity this converges to for a distribution on the channel inputs under which @xmath130 are iid , zero - mean , variance-@xmath131 gaussian random variables while the other inputs are deterministically zero .",
    "however , as the channel is not stationary , it is _ prima facie _ not clear whether there is a coding theorem associated with this quantity . ]",
    "@xmath181    it remains to show that given we can make @xmath161 arbitrarily small .",
    "indeed , implies that @xmath182 and can therefore be further lower - bounded by @xmath183 letting @xmath132 tend to infinity yields then that we can achieve any rate below @xmath184 .",
    "as this can be made arbitrarily large by choosing @xmath185 sufficiently small , we conclude that @xmath186 implies that the capacity is unbounded .",
    "theorem  [ thm : main ] resolves the question whether capacity is bounded or unbounded in the snr when the coefficients satisfy either @xmath187 or @xmath188 .",
    "we next address the case where neither condition holds , i.e. , @xmath189 example  [ ex:1 ] exhibits a sequence @xmath28 satisfying for which the capacity is bounded , and example  [ ex:2 ] provides a sequence @xmath28 satisfying for which the capacity is unbounded .    [ ex:1 ] consider the sequence @xmath28 where all coefficients with an even index are @xmath190 and all coefficients with an odd index are zero .",
    "it satisfies because @xmath191 and @xmath192 .",
    "thus , at even times , the output @xmath193 , @xmath194 only depends on the `` even '' inputs @xmath195 , while at odd times , the output @xmath196 , @xmath197 only depends on the `` odd '' inputs @xmath198 . by proceeding along the lines of the proof of part i ) of theorem  [ thm : main ]",
    "while choosing in @xmath199 , it can be shown that the capacity of this channel is bounded .",
    "[ ex:2 ] consider the sequence @xmath28 where @xmath200 , where all coefficients with an odd index are @xmath190 , and where all other coefficients ( whose index is an even positive integer ) are zero .",
    "( again , we have @xmath201 and @xmath202 . )",
    "using gaussian inputs of power @xmath203 at even times while setting the inputs to be zero at odd times , and measuring the channel outputs only at even times , reduces the channel to a memoryless additive noise channel and demonstrates the achievability of @xcite @xmath204 which is unbounded in the snr .",
    "a.  lapidoth and s.  m. moser , `` capacity bounds via duality with applications to multiple - antenna systems on flat fading channels , '' _ ieee trans .",
    "inform . theory _",
    "49 , no .",
    "10 , pp . 24262467 , oct ."
  ],
  "abstract_text": [
    "<S> this paper studies on - chip communication with non - ideal heat sinks . </S>",
    "<S> a channel model is proposed where the variance of the additive noise depends on the weighted sum of the past channel input powers . </S>",
    "<S> it is shown that , depending on the weights , the capacity can be either bounded or unbounded in the input power . a necessary condition and a sufficient condition for the capacity to be bounded are presented . </S>"
  ]
}