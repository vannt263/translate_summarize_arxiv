{
  "article_text": [
    "embedding words as real - valued vectors enables the application of neural network ( nn ) based approaches for several supervised natural language processing ( nlp ) tasks , such as sentiment analysis , named entity recognition , semantic role labelling etc . @xcite .",
    "the real - valued vectors are fed into the input layers of nns designed for various supervised tasks .",
    "the word embeddings themselves are obtained by unsupervised pre - training from a large corpus of text , by making use of recurrent neural networks ( rnns ) @xcite .",
    "the specific technique of word embeddings , known as _ word2vec _ , uses the core idea of _ negative sampling _ which massively increases the efficiency of the embedding process so as to make it scalable for large document collections @xcite .",
    "the main idea behind _ negative sampling _ is to design an objective function that seeks to maximize the similarity of the vector representation of the current word with another word sampled from the context of it , i.e. from a word window of a preset length around the current word .",
    "the objective function also seeks to minimize the similarity of the vector representation of the current word with another word drawn randomly from outside its context ; hence the name _ negative sampling _",
    "@xcite .",
    "a useful feature of word vectors pre - trained using word2vec is that vector addition ( component wise addition ) of two or more word vectors results in a vector that is close to the semantic meaning of the constituent words .",
    "this property of the embeddings has been used to address the proportional analogy task in nlp , such as predicting that ` berlin ' is the capital of ` germany ' given that ` paris ' is the capital of ` france ' , since @xmath1 is close to the vector @xmath2 .",
    "word embedding thus encapsulates the useful information about the context of a word and effectively represents the semantic information of a word .",
    "it is thus tempting to use the real - valued vector representations of words to represent documents and queries in information retrieval ( ir ) ir and defining similarity measures between them .",
    "however , a major difficulty in applying word vector embeddings in ir is to be able to devise an effective strategy for obtaining representations of compound units of text ( in comparison to the atomic words ) , such as passages and documents for the purpose of indexing . adding word vectors for deriving composed meanings has been shown to be beneficial for small units of text , e.g. for predicting query intent in search sessions @xcite .",
    "however , this approach of adding the constituent word vectors of a document to obtain the vector representation of the whole document is not likely to be useful , because the notion of compositionality of the word vectors works well when applied over a relatively small number of words .",
    "a vector addition for composition does not scale well for a larger unit of text , such as passages or full documents , because of the broad context present within a whole document .",
    "a possible solution is to use an extension of ` word2vec ' , commonly called ` doc2vec ' , to obtain distributed representations of arbitrary large units of text , such as paragraphs or documents , where in addition to learning the vector representations of words , the rnn also learns of the vector representation of the current document with a modified objective function @xcite .",
    "unfortunately , training ` doc2vec ' on a collection of a large number of documents ( as often encountered in ir ) is practically intractable both in terms of speed and memory resources .",
    "we address this issue from a different perspective . instead of striving for a suitable method for obtaining a single vector representation of a large document of text and a query ,",
    "so as to use the inner product similarity between them for scoring documents , we seek to develop a suitable similarity ( or inverse distance ) metric which makes use of the each individual embedded word vector in a document and a query .",
    "more specifically , we represent each document ( and a query ) as a set of word vectors , and use a standard notion of similarity measure between these sets . typically , such set based distance measures , such as the single - link , complete - link , average - link similarities etc .",
    "are used in hierarchical agglomerative clustering ( hac ) algorithms @xcite . generally speaking ,",
    "these similarities are computed as functions of the individual similarities between each constituent word vector pair of a document and a query set .",
    "we then combine this word - vector based similarity measure with a standard text based ir similarity measure in order to rank the retrieved documents in response to a query .",
    "the remainder of the paper is organized as follows . in section [ sec : representation ] , we discuss how documents and queries are represented as sets of word vectors so as to compute the similarities between them .",
    "section [ sec : indexing ] describes the index construction procedure to store and make use of the additional word vectors .",
    "section [ sec : eval ] evaluates our proposed approach of document ranking with the word vector based similarity .",
    "finally , section [ sec : concl ] concludes the paper with directions for future work .",
    "the ` bag - of - words ' ( bow ) representation of a document within a collection treats the document as a sparse vector in the term space comprised of all terms in the collection vocabulary .",
    "the vector representation of each word , obtained with a word embedding approach , makes provision to view a document as a ` bag - of - vectors ' ( bov ) rather than as a bow . with this viewpoint , one may imagine that a document is a set of words with one or more clusters of words , each cluster broadly representing a topic of the document .",
    "to model this behaviour , it can be assumed that a document is a probability density function that generates the observed document terms .    in particular , it is convenient to represent this density function as a mixture of gaussians of @xmath3 dimensions so that each word vector ( of @xmath3 dimensions ) is an observed sample drawn from this mixture distribution . with the observed document words ,",
    "this probability can be estimated with an expectation maximization ( em ) algorithm , e.g. k - means clustering of the observed document vectors .",
    "the observed query vectors during retrieval can be considered to be points that are also sampled from this mixture density representation of a document .",
    "documents can then be ranked by the posterior likelihood values of generating query vectors from their mixture density representations .",
    "more specifically , let the bow representation of a document @xmath4 be @xmath5 , where @xmath6 is the number of unique words in @xmath4 and @xmath7 is the @xmath8 word .",
    "the bov representation of @xmath4 is the set @xmath9 , where @xmath10 is the vector representation of the word @xmath7 .",
    "let each vector representation @xmath11 be associated with a latent variable @xmath12 , which denotes the topic or concept of a term and is an integer between @xmath13 and @xmath14 , where @xmath14 , being a parameter , is the total number of topics or the number of gaussians in the mixture distribution .",
    "these latent variables , @xmath12s , can be estimated by an em based clustering algorithm such as k - means , where after the convergence of k - means on the set @xmath15 , each @xmath12 represents the cluster i d of each constituent vector @xmath11 .",
    "let the points @xmath16 represent the @xmath14 cluster centres as obtained by the k - means algorithm .",
    "the posterior likelihood of the query to be sampled from the @xmath14 mixture model of gaussians , centred around the @xmath17 centroids , can then be estimated by the average distance of the observed query points , @xmath18 , from the centroids of the clusters . @xmath19 in equation [ eq : avgsim ] , @xmath20 denotes the inner product between the query word vector @xmath21 and the @xmath22 centroid vector @xmath17 .",
    "this measure is commonly known as the _ centroid similarity _ or _",
    "average inter - similarity _ in the literature , where it finds application to measure how similar two sets are during hierarchical agglomerative clustering ( hac ) @xcite .",
    "although there are multiple set - based similarity measures , such as the single - link , complete - link similarities etc . , we report the average - link similarity in our experiments because it produced the best results .",
    "a likely reason for this metric to produce the best results is that it is not largely affected by the presence of outliers like single - link or complete - link algorithms ( for more details , see the discussion in chapter 17 of the book @xcite ) .    intuitively speaking ,",
    "the notion of set - based similarity is able to make use of the semantic distances between the constituent terms of a document and a given query so as to improve the rankings of retrieved documents .",
    "note that a standard text - based retrieval model can only make use of the term overlap statistics between documents and queries , and can not utilize the semantic distances between constituent terms for scoring documents .",
    "we now demonstrate the key idea of the usefulness of the set - based similarity of the constituent word vectors of documents and queries with illustrative examples .",
    "consider the documents of figure [ fig : case1 ] , where for illustrative purposes , we assume that the @xmath23 , i.e. , each word is embedded in a two dimensional space . the individual word vectors of a document are shown with blue dots , whereas the query points are shown with red triangles .",
    "note that the document in figure [ fig : case1_a ] has one cluster and all the three query points are relatively close to the centroid of this cluster .",
    "in contrast , the document in figure [ fig : case1_b ] is comprised of two clusters ( one blue and the other red ) . in this case , the query terms are relatively far away from the central theme of the document , i.e. the position of the centroid vector of the predominant topic of the document , i.e the centroid of the blue words .",
    "this indicates that the posterior query likelihood for the document in figure [ fig : case1_b ] is lower than that of figure [ fig : case1_a ] .",
    "this means that the document of figure [ fig : case1_a ] will be ranked higher for the example query than the document on the right . intuitively speaking ,",
    "the closer the query terms are to the clusters of the constituent word vectors of a document , the higher is the similarity of the document with the query .",
    "figure [ fig : case2 ] demonstrates two example cases where @xmath24 distinct clusters of terms ( topics ) can be seen .",
    "the query likelihood for the document in figure [ fig : case2_a ] is lower than that of the document in figure [ fig : case2_b ] .",
    "this is because the observed query terms in figure [ fig : case2_b ] are close to the cluster centres on the bottom - left and bottom - right , whereas for the example scenario in figure [ fig : case2_a ] , the query point @xmath25 is relatively far away from all the centroids .",
    "this has the implication that the information need aspect of the query term @xmath25 is not adequately expressed in the contents of the document in figure [ fig : case2_a ] .",
    "equation [ eq : avgsim ] gives a new way to compute query likelihoods in comparison to the standard method of computing @xmath26 using the bow representation model using the standard language modeling ( lm ) @xcite .",
    "in lm , the prior probability of generating a query @xmath27 from a document @xmath4 is given by a multinomial sampling probability of obtaining a term @xmath21 from @xmath4 with prior belief @xmath28 ( a parameter for lm ) , coupled with a probability of sampling the term @xmath21 from the collection with prior belief @xmath29 .",
    "let this probability be represented by @xmath30 as shown in equation [ eq : lm ] .",
    "@xmath31 in order to combine the standard lm query likelihood probability @xmath30 of equation [ eq : lm ] with the query likelihood estimated using the mixture model density estimate of the word vectors , as shown in equation [ eq : avgsim ] , we introduce another indicator binary random variable to denote the individual believes of the text based similarity and the word vector based similarity .",
    "if the probability of this indicator variable is denoted by @xmath32 , we obtain the combined query likelihood as a mixture model of the two respective query likelihoods , one for the text and the other for the embedded word vectors . @xmath33",
    "in our approach , as discussed in section [ sec : representation ] , the embedded representation of the constituent terms of a document are used for estimating the similarity between a document and a query in combination with the bow based lm similarity . in this section ,",
    "we discuss how can the constituent word vector representations be stored in the index so that the cluster centres for each document can be computed efficiently during retrieval time in order to compute its similarity with the query .",
    "[ cols=\"<,<,^,^,^,<,^,^,^,^,^ \" , ]     ) , title=\"fig : \" ] ) , title=\"fig : \" ] + ) , title=\"fig : \" ] ) , title=\"fig : \" ]",
    "in this paper , we have proposed a novel approach to represent queries and documents as sets of embedded word vectors .",
    "a document is represented as a probability density function of a mixture of gaussians .",
    "the posterior query likelihood is estimated by the average distance of the query points from the centroids of these gaussians . this word vector based query likelihood",
    "is then combined with the standard lm based query likelihood for document ranking .",
    "experiments on standard text collections showed that the combined similarity measure almost always outperforms ( often significantly ) the lm ( text based ) similarity measure .",
    "as a possible future work , we would like to apply our proposed representation of queries and documents for relevance feedback and query expansion .",
    "we would also like to explore various notions of distance measures ( e.g. @xcite ) .",
    "it would also be interesting to see the performance of this set - based representation obtained with a multi - sense word embedding method , such as @xcite .",
    "this research is supported by science foundation ireland ( sfi ) as a part of the adapt centre at dcu ( grant no : 13/rc/2106 ) and by a grant under the sfi isca india consortium ."
  ],
  "abstract_text": [
    "<S> a major difficulty in applying word vector embeddings in information retrieval is in devising an effective and efficient strategy for obtaining representations of compound units of text , such as whole documents , ( in comparison to the atomic words ) , for the purpose of indexing and scoring documents . instead of striving for a suitable method to obtain a single vector representation of a large document of text </S>",
    "<S> , we aim to develop a similarity metric that makes use of the similarities between the individual embedded word vectors in a document and a query . </S>",
    "<S> more specifically , we represent a document and a query as sets of word vectors , and use a standard notion of similarity measure between these sets , computed as a function of the similarities between each constituent word pair from these sets . </S>",
    "<S> we then make use of this similarity measure in combination with standard information retrieval based similarities for document ranking . </S>",
    "<S> the results of our initial experimental investigations show that our proposed method improves map by up to @xmath0 , in comparison to standard text - based language model similarity , on the trec 6 , 7 , 8 and robust ad - hoc test collections .    </S>",
    "<S> < ccs2012 > < </S>",
    "<S> concept > </S>",
    "<S> < concept_id>10002951.10003317.10003318.10003321</concept_id > < concept_desc > information systems  content analysis and feature selection</concept_desc > </S>",
    "<S> < concept_significance>500</concept_significance > </S>",
    "<S> < /concept > < /ccs2012 > </S>"
  ]
}