{
  "article_text": [
    "in this paper we further investigate the new classification rules introduced in @xcite with a built - in reject option in the standard binary classification setting , where we observe independent realizations @xmath1 , @xmath2 , of a random pair @xmath3 in @xmath4 ( here , @xmath5 is an arbitrary space ) .",
    "a discriminant function @xmath6 classifies an observation @xmath7 into one of two classes , labeled @xmath8 or @xmath9 . viewing @xmath10 as a proxy value of the conditional probability @xmath11 we are less confident for small values of @xmath12 , corresponding to @xmath13 near 1@xmath142 .",
    "our strategy is to report @xmath15 if @xmath12 exceeds some prescribed threshold @xmath16 and withhold decision otherwise . assuming that the cost of making a wrong decision is 1 and that of withholding a decision is @xmath17 , the appropriate risk function is @xmath18 = { \\mathbb{p}}\\ { y f(x ) < -\\tau\\ } + d { \\mathbb{p}}\\ { | y f(x ) | \\le\\tau\\}\\label{risk_orig}\\ ] ] with the discontinuous loss function @xmath19 since we always reject if @xmath20 and never reject if @xmath21 ( see @xcite ) , we take @xmath22 in what follows without loss of generality . although the minimizer of this risk is not unique , all such minimizers correspond to the unique classification rule that assigns @xmath23 or withhold decision , depending on which of @xmath24 , @xmath25 or @xmath17 is smallest",
    ". the smallest risk is @xmath26 $ ] and we may interpret the cost @xmath17 as the largest conditional probability of misclassification that is considered tolerable .    in practice ,",
    "minimization of the empirical counterpart @xmath27 of @xmath28 over a large class of functions @xmath29 is computationally not feasible .",
    "for this reason , we could replace the loss function @xmath30 by a convex surrogate loss function and consider discriminant functions @xmath29 of the form @xmath31 based on a set of known functions @xmath32 and coefficients @xmath33 , @xmath34 .",
    "following @xcite , we will consider the generalized hinge loss @xmath35 with slope @xmath36 .",
    "observe that @xmath37 is piecewise linear , so that minimization of the empirical risk @xmath38 can be solved by a tractable linear program .",
    "crucial for the choice of @xmath37 is that it is classification calibrated : the unique minimizer @xmath39 of @xmath40 $ ] also minimizes the risk @xmath41 $ ] over all measurable @xmath6 for all @xmath42 ; see , for example , @xcite .    at this point",
    "it is important to note that truncating the minimizer @xmath43 of the hinge - loss - based risk @xmath44 does not yield the optimal rule for any positive threshold @xmath16 .",
    "this is the reason why we generalize the hinge loss instead .",
    "in addition to the generalized hinge loss , there are also other choices of the surrogate loss function and corresponding truncation value @xmath16 that are classification calibrated .",
    "the treatment for the generalized hinge loss differs considerably from that for other losses , such as the logistic , exponential and quadratic loss , which are smoother .",
    "we refer to @xcite for a detailed discussion .",
    "observe that @xmath45 for all @xmath46 and , subsequently , @xmath47 \\le{\\mathbb{e } } [ \\phi(y f(x ) ) ] $ ] .",
    "it is shown in @xcite that a similar relationship remains true for the excess risks , that is , the inequality @xmath48 - { \\mathbb{e } } [ \\ell(y f_0(x ) ) ] \\le { \\mathbb{e } } [ \\phi(y f(x ) ) ] - { \\mathbb{e } } [ \\phi(y f_0(x ) ) ] \\ ] ] holds for all @xmath49 .",
    "this property is useful for deriving oracle inequalities in terms of the @xmath30-risk since minimization of ( [ emp ] ) produces oracle inequalities in terms of the @xmath50-risk rather than the @xmath30-risk directly .    of particular interest here",
    "is the case where the number of basis functions , @xmath51 , is large when compared with the sample size @xmath52 .",
    "usually , the minimization of the empirical risk @xmath53 is computed under a restriction on the quadratic term @xmath54 . here",
    ", we opt instead for an @xmath0-type restriction @xmath55 and estimate @xmath56 by @xmath57 , where @xmath58 and @xmath59 is a tuning parameter .",
    "the choice of an @xmath0 penalty reflects our preference for sparse solutions , which is desirable when @xmath51 is large .    in the remainder of this paper",
    ", we study the properties of @xmath60 and its population counterpart , @xmath61 we establish oracle inequalities for @xmath62 and @xmath60 in sections [ sec2 ] and [ sec3 ] , respectively . the results that we obtain are similar in spirit to those from @xcite .",
    "however , @xcite do not discuss properties of @xmath62 , and our results in section [ sec2 ] obtained here extend those proved by @xcite in the context of twice differentiable loss functions .",
    "furthermore , the oracle inequalities for the penalized empirical risk minimizer @xmath60 in section [ sec3 ] are much sharper than earlier results from  @xcite for @xmath63 and @xcite for @xmath64 .",
    "in particular , the new inequality reveals that the rate of convergence of the excess risk of @xmath65 can be even faster than @xmath66 if the optimal discriminant function @xmath67 can be written as a linear combination of the @xmath68 s in the dictionary .",
    "moreover , we relax the condition on the dictionary and do not require that the parameter @xmath69 is bounded .",
    "we emphasize that our results hold , in particular , for @xmath64 , the case of support vector machines without a  reject option , and generalize and extend the results obtained in @xcite .",
    "in addition , novel empirical bounds on the error and reject rate are given . to demonstrate the feasibility of the @xmath0-regularized support vector machine with a reject option , in section [ sec4 ] we formulate @xmath70 as a solution of a  linear program and report some numerical experiments .",
    "some technical lemmas and a maximal inequality for a weighted empirical process are collected in the .",
    "we begin by studying @xmath62 , the population version of @xmath60 . recall that @xmath62 is defined by @xmath71 in particular",
    ", @xmath72 minimizes the risk @xmath73 over @xmath74 . by definition , we find that @xmath75 holds for all @xmath76 .",
    "this inequality applied to @xmath77 has the following consequences .",
    "[ prop1 ] let @xmath78 be the support of @xmath72 .",
    "if @xmath79 as @xmath80 , then @xmath81 as @xmath80 .",
    "@xmath82 for all @xmath59 .",
    "@xmath83 .    after applying inequality ( [ def ] ) to @xmath77 and using the fact that @xmath84 , we get @xmath85 which implies ( a ) . the second claim follows from @xmath86 for the proof of part ( c ) , we first observe that @xmath87 is equivalent to @xmath88 next , we note that the term on the left equals @xmath89 and we bound the term on the right by @xmath90 using the triangle inequality .",
    "this proves part ( c ) .",
    "this result gives a simple condition for @xmath91 and shows that the @xmath0 norm of the solution @xmath62 is always smaller than the @xmath0 norm of @xmath72 .",
    "similar properties are established by  @xcite for minimizers of twice differentiable loss functions @xmath50 and @xmath92 norms for @xmath93 .",
    "in contrast , we consider here a non - differentiable loss function @xmath50 and @xmath94 .",
    "our target is a sparse vector @xmath95 with risk @xmath96 close to @xmath97 .",
    "before we make this precise , we need to introduce a few concepts depending on the behavior of @xmath98 near @xmath17 and @xmath99 , and the set of functions @xmath68 .",
    "the classification complexity is defined as the largest number @xmath100 such that , for some @xmath101 and all @xmath102 , @xmath103    this notion of complexity is a generalization of tsybakov s margin condition @xcite for @xmath64 .",
    "the behavior of @xmath98 is obviously not relevant in the interval @xmath104 , only at the endpoints @xmath105 and @xmath99 .",
    "the inequality always holds for @xmath106 and @xmath107 .",
    "in contrast , @xmath108 describes the easiest classification situation where we essentially require that @xmath98 stays away from @xmath17 and @xmath99 with probability one .",
    "if @xmath98 has a density in the neighborhood of @xmath17 and @xmath99 , then we have that @xmath109 .",
    "let @xmath95 , @xmath110 and @xmath111 be the @xmath112 matrix with entries @xmath113 $ ] with @xmath114 . for @xmath115 , the support of @xmath116 ,",
    "we define @xmath117    the condition @xmath118 is a restrictive eigenvalue condition on the gram matrix @xmath111 of the type introduced in @xcite in the context of linear regression . using similar reasoning as in @xcite , page  1714 ,",
    "it is implied by the local mutual coherence condition used in @xcite .",
    "we are now in position to state an oracle inequality for the excess risk , @xmath119 of the regularized minimizer @xmath62 and the @xmath0-distance between the vectors @xmath62 and @xmath116 .",
    "[ theorie ] let @xmath120 be the classification complexity , and @xmath116 be such that @xmath121 and @xmath122 .",
    "then , for any @xmath123 with @xmath124 and @xmath125 , we have @xmath126\\\\ [ -8pt ] & & \\quad\\le 3 \\delta r_\\phi({\\sf f}_\\theta)+ 6 \\ { 4a ( 2d)^{\\alpha } \\}^{1/(2+\\alpha ) } \\| { \\sf f}_{\\theta } - f_0\\|_{\\infty } ( \\kappa^{-2 } r^2 \\| \\theta\\| _ { \\ell _ 0 } ) ^{(1+\\alpha)/(2+\\alpha)}.\\nonumber\\end{aligned}\\ ] ]    set @xmath127 .",
    "let @xmath128 be the support of @xmath116 .",
    "it is straightforward to derive from proposition [ prop1 ] that @xmath129 and , subsequently , that @xmath130 the first inequality , combined with the assumption @xmath122 , yields @xmath131 using the notation @xmath132 $ ] and @xmath133 . by lemma [ t ] in appendix [ appendixa ] ,",
    "we find that @xmath134 for @xmath135 and @xmath136 .",
    "after we plug this bound into the right - hand side of the previous display , we find that @xmath137 next , we apply young s algebraic inequality , @xmath138 to the last two terms on the right - hand side , with @xmath139 and @xmath140 to get @xmath141 since @xmath142 we deduce , after invoking ( [ reg2.4 ] ) , that @xmath143 and the conclusion follows .",
    "it is interesting to see that the bound ( [ theorem : oracle.th ] ) crucially depends on the classification complexity parameter @xmath120 and @xmath144 . in particular , if @xmath67 can itself be represented as a linear combination of the basis functions , then @xmath145 . in this case , provided that @xmath146 , theorem [ theorie ] implies that @xmath147 in other words , we have the following corollary .    if @xmath145 and @xmath146 , then @xmath148 for any @xmath149",
    "in this section we study the estimate @xmath150 . in",
    "what follows , we will simplify notation so as not to show dependence of @xmath151 on @xmath152 whenever no confusion occurs . again",
    ", we emphasize that our results hold , in particular , for @xmath64 , the case of a support vector machine without a reject option .",
    "note that the inequality @xmath153 applied to the vector of zeros @xmath154 implies that @xmath155 this means that we can restrict our analysis to the set @xmath156 the aim of this section is to show that @xmath157 is close to @xmath62 for a judiciously chosen tuning parameter  @xmath152 .    [ cons ] if , for some @xmath158 , @xmath159 then for all @xmath160 , with probability larger than @xmath161 , @xmath162 and , moreover , @xmath163    write @xmath164 .",
    "let @xmath165 and define @xmath166 by propositions [ lemma : mcd ] and [ lb ] in appendix [ appendixb ] , @xmath167 for the choice @xmath152 given in ( [ r ] ) .",
    "rewriting the inequality ( [ lambdahat ] ) , we find that @xmath168 thus , on the event @xmath169 , after adding @xmath170 to both sides , we obtain @xmath171 which proves the first claim . adding @xmath172 to both sides easily yields the second claim .",
    "a direct consequence of theorem [ cons ] is the following corollary which states that in the sparse setting where @xmath173 , the estimator @xmath150 behaves like the penalized minimizer @xmath62 in terms of their risk .",
    "[ cor1 ] suppose that @xmath173 as @xmath174 for @xmath152 satisfying ( [ r ] ) .",
    "then , with probability at least @xmath161 , @xmath175 as @xmath174 . in particular , when taking @xmath176 , we have @xmath177 and @xmath178 .    we combine the basic property ( [ lambdahat ] ) applied to @xmath179 and theorem [ cons ] , and we find that on the event @xmath169 , @xmath180 the result then follows from @xmath181 .",
    "we emphasize that the above results do not impose any restrictions on the dictionary @xmath182 .",
    "if we are willing to make assumptions on the gram matrix @xmath111 , then we obtain a more refined result .",
    "[ empirical ] for all @xmath152 satisfying ( [ r ] ) and @xmath160 such that @xmath183 and @xmath184 for some ( small ) @xmath185 depending on @xmath186 , @xmath120 , @xmath187 and @xmath17 , we have , for some @xmath188 depending on @xmath185 , that @xmath189 holds with probability at least @xmath161 .",
    "recall that @xmath190 .",
    "we may assume without loss of generality that @xmath191 holds , since otherwise the statement holds trivially .",
    "consequently , on the event @xmath169 , using ( [ wat ] ) and ( [ een ] ) , we get @xmath192 so that @xmath193 , where @xmath194 is the support of @xmath116 . on the other hand ,",
    "@xmath195 the remainder of the proof follows that of theorem [ theorie ] , with @xmath196 .",
    "this result differs from @xcite ( and @xcite for the case @xmath64 ) in the appearance of the norm @xmath197 on the right - hand side of the ( oracle ) inequality .",
    "this implies that for @xmath198 and for some sparse @xmath199 satisfying the conditions of theorem [ empirical ] , we can expect fast rates , regardless of the classification complexity ! another important difference with both papers",
    "is that no restriction is imposed on the sup - norm of @xmath200 .",
    "such a condition is unnatural as @xmath201 may overrule the restriction that the penalty term @xmath202 imposes .",
    "+ we now consider bounds on the error and reject rates without an additional test sample .",
    "we write @xmath203 for any @xmath204 .",
    "the misclassification and rejection rate can be bounded above as follows .",
    "[ theorem : errorrate ] if @xmath205 then , with probability at least @xmath161 , we have @xmath206 + n^{-p},\\\\ { \\mathbb{p}}\\ { | { \\sf f}_{{\\hat}{\\lambda}}(x ) |\\le\\tau\\ } & \\le & \\min_{\\gamma>0 } [ { \\mathbb{p}}_n\\{| { \\sf f}_{{\\hat}{\\lambda}}(x)| \\le\\tau+ \\gamma\\ } + r(\\gamma ) \\|{\\hat}{\\lambda}\\|_{\\ell_1 } ] + n^{-p}.\\end{aligned}\\ ] ]    set @xmath207 the following inequalities then hold uniformly in @xmath69 : @xmath208 where @xmath209 with @xmath210 given by @xmath211 .",
    "we can invoke propositions [ lemma : mcd ] and [ lb ] to complete the proof of the first claim .",
    "the proof of the second claim uses the reasoning above , with the only modification being that @xmath212 is now given by @xmath213 the rest of the reasoning is unchanged .",
    "we now demonstrate the practical merits of @xmath60 via a couple of numerical experiments .",
    "we begin by noting that the computation of @xmath60 can be conveniently formulated as a linear program .",
    "let @xmath214 be the slack variables such that @xmath215 clearly the minimum @xmath216 that satisfies these constraints is @xmath217 .",
    "we also introduce slack variables @xmath218 , @xmath219 to represent @xmath220 , that is , @xmath221 using the slack variables , @xmath60 can be given as the solution of the linear program @xmath222\\end{aligned}\\ ] ] subject to @xmath223 to illustrate the merits of @xmath157 , we implement the method described above and first apply it to a set of simulated examples . to fix ideas , we set @xmath224 or , equivalently , @xmath225 . for each run , @xmath226",
    "positive instances ( @xmath227 ) and @xmath226 negative instances ( @xmath228 ) were generated .",
    "two hundred ( @xmath229 ) features ( @xmath68 s ) were simulated from a multivariate normal distribution . for positive instances ,",
    "the mean was set to @xmath230 whereas for the negative instances , the mean was set to @xmath231 . in both cases , the covariance matrix was the identity matrix .",
    "the operating characteristics of the method are demonstrated in figure [ fig : sim ] . on the left - hand side ,",
    "the misclassification rate ( @xmath232 ) , rejection rate ( @xmath233 ) and associated @xmath30-risk of the @xmath0-regularized generalized hinge loss ( @xmath234 ) are plotted as functions of the tuning parameter @xmath152 for a typical simulation .",
    "the results are to be compared with the usual @xmath0-regularized support vector machines where no rejection option is allowed .",
    "since there is no rejection , the misclassification rate for the usual support vector machines coincides with its @xmath30-risk .",
    "it is evident that by incorporating the rejection option , @xmath157 yields a smaller @xmath30-risk , provided that both methods are optimally tuned .",
    "to further investigate the merits of allowing the rejection option , we repeated the experiment 200 times .",
    "the excess risk @xmath235 of both the usual support vector machine and the proposed method are summarized in the plot on the right - hand side .",
    "it further confirms the advantage of @xmath157 .    .",
    "the left - hand panel shows the three criteria as functions of the tuning parameter @xmath152 for the support vector machine ( svm ) with rejection option for a typical run .",
    "also included is the misclassification rate for the usual svm .",
    "it is evident that svm with rejection option enjoys lower misclassification rate by withholding decision for `` hard - to - classify '' cases .",
    "the right - hand panel compares the excess @xmath30-risk for svm with or without rejection option .",
    "the box plots of the excess risk are produced based on 200 runs .",
    "this again confirms that svm with rejection option leads to improved performance in terms of the @xmath30 loss . ]    to further demonstrate the merits of the method , we apply it to the mixture data example considered in @xcite .",
    "the training data consist of 200 data points generated from a pair of two - dimensional mixture densities .",
    "similarly to @xcite , we consider a dictionary of gaussian radial basis functions @xmath236 where the locations @xmath237 are placed on a  @xmath238 equally spaced lattice . to fix ideas , we consider the case where @xmath224 .",
    "the optimal classification rule will classify an observation as @xmath9 if the corresponding conditional probability @xmath239 is greater than @xmath240 and as @xmath8 if the conditional probability is less than @xmath241 .",
    "when the conditional probability is between @xmath241 and @xmath240 , we withhold the decision .",
    "the corresponding decision boundaries are given in the right - hand panel of figure [ fig : mix ] .",
    "it is known that the usual svm only targets the decision boundary identified with @xmath239 and can not be used to recover the optimal decision boundaries given here ; see , for instance , @xcite for further discussion of this issue . in contrast , the svm with rejection option is devised specifically for this purpose . to this end , we ran the svm with rejection option with @xmath225 and @xmath242 as discussed earlier .",
    "the tuning parameter @xmath152 was selected by tenfold cross - validation .",
    "the left - hand panel of figure [ fig : mix ] gives the estimated decision boundaries .",
    "it is clear from the plot that svm with rejection option successfully captured the main characteristics of the underlying probabilities .",
    "the main difference between the two sets of decision boundaries occurs in regions where no observations are available . as a result , the svm with rejection option opted for withholding a decision .     and",
    "light green regions to classification @xmath228 . areas where a decision is withheld are not shaded .",
    "the solid black line in the left - hand panel is the level set for @xmath243 .",
    "the solid black line in the right - hand panel is the level set for @xmath244 . ]",
    "[ appendix ]",
    "the next lemma is a technical result that links the excess risk @xmath246 to the @xmath245 norm : @xmath247 replaces the suboptimal bound @xmath248 in @xcite .",
    "[ t ] let @xmath249 be as in definition 2.2 . then",
    ", for all @xmath76 , @xmath250    let @xmath6 be arbitrary and set @xmath251 then @xcite , lemma 9 , states that @xmath252\\ ] ] with @xmath253 using ( a.1 ) , for any set @xmath254 , @xmath255\\\\ & & \\quad\\ge t { \\mathbb{e}}\\bigl [ \\rho_\\eta(f , f_0)(x ) i_{\\ { |\\eta(x)-(1-d)|\\ge t\\ } } i_{\\ { x\\in e\\ } } \\bigr ] \\\\ & & \\quad= t { \\mathbb{e}}\\bigl [ \\rho_\\eta(f , f_0)(x ) i_{\\ { x\\in e\\ } } \\bigr ] - t { \\mathbb{e}}\\bigl [ \\rho_\\eta(f , f_0)(x ) i_{\\ { |\\eta(x)-(1-d)|",
    "< t , x\\in e\\ } } \\bigr]\\\\ & & \\quad\\ge t { \\mathbb{e}}\\bigl [ \\rho_\\eta(f , f_0)(x ) i_{\\ { x\\in e\\ } } - \\|f - f_0\\|_{\\infty } a t^\\alpha\\bigr].\\end{aligned}\\ ] ] similarly , @xmath256 \\ge t { \\mathbb{e}}\\bigl [ \\rho_\\eta(f , f_0)(x ) i_{\\ { x\\in e\\ } } - \\|f - f_0\\|_{\\infty } a t^\\alpha\\bigr],\\ ] ] and we obtain @xmath257\\\\ & = & d^{-1 } t { \\mathbb{e } } [ \\rho_\\eta({\\sf f}_\\lambda , f_0)(x ) -2\\|{\\sf f}_\\lambda - f_0\\|_{\\infty } at^\\alpha].\\end{aligned}\\ ] ] plugging @xmath258 } { 4 a \\| { \\sf f}_\\lambda - f_0\\|_{\\infty } } \\biggr)^{1/\\alpha}\\ ] ] into the preceding expression , we obtain @xmath259 ) ^{(1+\\alpha)/\\alpha } } { 2d(4a\\| { \\sf f}_\\lambda - f_0\\|_{\\infty } ) ^{1/\\alpha } } .\\ ] ] since @xmath260 \\le\\| { \\sf f}_\\lambda - f_0\\|_{\\infty}{\\mathbb{e } } [ \\omega(x ) |{\\sf f}_\\lambda(x)-f_0(x)|],\\ ] ] we get , for all @xmath69 , @xmath261)^{(1+\\alpha)/\\alpha}}{2d(4a \\|{\\sf f}_\\lambda - f_0\\|_{\\infty } ) ^{1/\\alpha } } \\\\ & \\ge & \\frac{(\\| { \\sf f}_\\lambda - f_0\\| ^2)^{(1+\\alpha)/\\alpha}}{2d(4a ) ^{1/\\alpha } \\|{\\sf f}_\\lambda - f_0\\|_{\\infty}^{(2+\\alpha)/\\alpha } } .\\end{aligned}\\ ] ] the claim follows .    if @xmath262 , then @xmath263 .",
    "hence , if we restrict the parameters @xmath69 such that @xmath200 are bounded by 1 , then we can impose the restricted eigenvalue condition on the matrix with entries @xmath264 $ ] instead of @xmath265 $ ] .",
    "recall that @xmath266 and let @xmath160 and @xmath267 .",
    "let @xmath268 be a convex function with lipschitz constant @xmath269 and define the risks @xmath270,\\\\ { \\hat}r_\\varphi({\\sf f}_\\lambda ) & = & \\frac1n \\sum_{i=1}^n \\varphi(y_i { \\sf f}_\\lambda(x_i)).\\end{aligned}\\ ] ]    finally , let @xmath267 and set @xmath271 we prove a maximal inequality for @xmath272 which slightly generalizes the result obtained in  @xcite .",
    "[ lemma : mcd ] let @xmath273 and set @xmath274 + c_\\varphi c_f \\sqrt { \\frac{2\\log ( 1/\\delta)}{n}}.\\ ] ] then , @xmath275    first , observe that changing a pair @xmath1 in @xmath276 changes it by at most @xmath277 .",
    "the result follows immediately after applying mcdiarmid s exponential inequality @xcite , theorem 2.2 , page 8 .",
    "we now control the expectation of @xmath272 .    [ lb ] set @xmath278 .",
    "then , @xmath279 \\le 9 c_\\varphi c_f",
    "\\sqrt{\\frac{2\\log2(m\\vee n)}{n } } + \\frac{2j c_\\varphi c_f } { \\sqrt{2(m\\vee n)}}.\\ ] ]    let @xmath280 be independent rademacher variables , taking the values @xmath281 , each with probability 1@xmath142 , independent of the data @xmath282 .",
    "set @xmath283    a standard symmetrization trick @xcite , page 18 , shows that @xmath284 & \\le&2 { \\mathbb{e}}\\biggl [ \\sup _ { \\lambda\\in\\lambda } \\frac { |{\\hat}r_\\varphi^0({\\sf f}_\\lambda)- { \\hat}{r}_\\varphi^0({\\sf f}_\\theta ) |}{\\|\\lambda-\\theta\\|_{\\ell_1 } + { \\varepsilon } } \\biggr]\\\\ & \\le & 2{\\mathbb{e}}\\biggl [ \\sup_{\\|\\lambda-\\theta\\|_{\\ell_1}\\le{\\varepsilon } } \\frac { |{\\hat}r_\\varphi^0({\\sf f}_\\lambda)- { \\hat}{r}_\\varphi^0({\\sf f}_\\theta )    { \\mathbb{e}}\\biggl[\\sup_{{\\varepsilon}\\le\\|\\lambda-\\theta\\|_{\\ell_1}\\le1/r } \\frac { |{\\hat}r_\\varphi^0({\\sf f}_\\lambda)- { \\hat}{r}_\\varphi^0(\\theta ) |}{\\| \\lambda-\\theta\\|_{\\ell_1 } + { \\varepsilon}}\\biggr ] \\\\ & = & ( i ) + ( \\mathit{ii}).\\end{aligned}\\ ] ] the first term @xmath285\\\\ & = & { \\mathbb{e}}\\biggl [ \\sup_{\\|\\lambda-\\theta\\|_{\\ell_1}\\le{\\varepsilon } } \\frac { 1}{\\| \\lambda-\\theta\\|_{\\ell_1 } + { \\varepsilon } } \\biggl| \\frac1n \\sum_{i=1}^n \\ { \\varphi ( y_i { \\sf f}_\\lambda(x_i ) ) - \\varphi(y_i { \\sf f}_{\\theta } ( x_i))\\ } \\biggr| \\biggr]\\end{aligned}\\ ] ] can be bounded using the contraction principle for rademacher processes ; see @xcite , pages 112113 . for this , we observe that the function @xmath286 is lipschitz with lipschitz constant @xmath269 and @xmath287 .",
    "consequently , @xmath288 \\\\ & \\le & 2\\frac{c_\\varphi}{{\\varepsilon } } { \\mathbb{e}}\\biggl [ \\sup_{\\|\\lambda-\\theta\\|_{\\ell_1 } \\le{\\varepsilon } } \\|\\lambda -\\theta \\|_{\\ell_1 } \\max_{1\\le j\\le m } \\biggl| \\frac1n \\sum_{i=1}^n \\sigma _ i y_i f_j(x_i ) \\biggr| \\biggr ] \\\\ &",
    "\\le&2 c_\\varphi{\\mathbb{e}}\\biggl[\\max_{1\\le j\\le m } \\biggl| \\frac1n \\sum _ { i=1}^n \\sigma_i y_i f_j(x_i ) \\biggr| \\biggr]\\\\ & \\le&2 c_\\varphi c_f \\frac{\\sqrt{2\\log(2m)}}{\\sqrt{n}}.\\end{aligned}\\ ] ] the last maximal inequality can be found in @xcite , lemma 2.2 , page 7 , which uses the fact that the variables @xmath289 are sub - gaussian , @xmath290 \\le\\exp(n s^2 c_f^2 /2)\\ ] ] for all @xmath291 , which follows , in turn , from @xcite , lemma 2.1 , page 5",
    ".    the second term ( ii ) requires a peeling argument @xcite , page 70 . since @xmath292",
    "almost surely , we can use the bound @xmath293 observe that for any @xmath294 , @xmath295 now , set @xmath296 and the same considerations leading to the final bound of ( i ) above yield @xmath297 \\le2^j { \\varepsilon}c_\\phi c_f \\frac { \\sqrt{2 \\log ( 2m)}}{\\sqrt{n}}\\ ] ] and for @xmath298 , we obtain @xmath299 \\ge 2^{j-2 } { \\varepsilon}\\zeta-{\\mathbb{e } } [ z_j ] \\}.\\ ] ] a change of a single pair @xmath1 changes @xmath300 by at most @xmath301 , so that another application of the bounded differences inequality @xcite , theorem 2.2 , page 8 , gives , by taking @xmath302 the final bound @xmath303 \\ge2^{j-2 } { \\varepsilon}\\zeta -{\\mathbb{e } } [ z_j ] \\}\\\\ & & \\quad\\le \\sum_{j=1}^{j } { \\mathbb{p}}\\biggl\\ { z_j -{\\mathbb{e } } [ z_j ] \\ge t\\cdot2^{j } c_\\varphi c_f { \\varepsilon}\\frac{\\sqrt{2\\log(2m\\vee 2n)}}{\\sqrt{n } } \\biggr\\}\\\\ & & \\quad\\le j \\exp\\biggl\\{-2 \\frac{t^2 ( c_\\varphi c_f 2^j { \\varepsilon})^2 2\\log ( 2m\\vee2n)}{(2c_\\phi c_f 2^j { \\varepsilon})^2 } \\biggr\\}\\\\ & & \\quad= j ( 2 m\\vee2n)^{-t^2 } < j /\\sqrt { 2m\\vee2n}.\\end{aligned}\\ ] ] finally , we invoke ( [ ong ] ) to complete the proof of proposition [ lemma : mcd ] .",
    "the research of marten wegkamp was supported in part by nsf grant dms-0706829 .",
    "the research of ming yuan was supported in part by nsf grant dms-08 - 46234 and nih grant r01gm076274 - 01 ."
  ],
  "abstract_text": [
    "<S> this paper studies @xmath0 regularization with high - dimensional features for support vector machines with a  built - in reject option ( meaning that the decision of classifying an observation can be withheld at a cost lower than that of misclassification ) . the procedure can be conveniently implemented as a linear program and computed using standard software . </S>",
    "<S> we prove that the minimizer of the penalized population risk favors sparse solutions and show that the behavior of the empirical risk minimizer mimics that of the population risk minimizer . </S>",
    "<S> we also introduce a notion of classification complexity and prove that our minimizers adapt to the unknown complexity . using a novel oracle inequality for the excess risk , we identify situations where fast rates of convergence occur . </S>"
  ]
}