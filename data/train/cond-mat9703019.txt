{
  "article_text": [
    "the problem of learning is one of the most interesting aspects of feed - forward neural networks @xcite .",
    "recent activities in the theory of learning have gradually shifted toward the issue of on - line learning . in the on - line learning scenario ,",
    "the student is trained only by the most recent example which is never referred to again .",
    "in contrast , in the off - line ( or batch ) learning scheme , the student is given a set of examples repeatedly and memorizes these examples so as to minimize the global cost function .",
    "therefore , the on - line learning has several advantages over the off - line method .",
    "for example , it is not necessary for the student to memorize the whole set of examples , which saves a lot of memory space .",
    "in addition , theoretical analysis of on - line learning is usually much less complicated than that of off - line learning which often makes use of the replica method . in many of the studies of learning",
    ", authors assume that the teacher and student networks have the same structures .",
    "the problem is called learnable in these cases . however , in the real world we find innumerable unlearnable problems where the student is not able to perfectly reproduce the output of teacher in principle .",
    "it is therefore both important and interesting to devote our efforts to the study of learning unlearnable rules .",
    "if the teacher and student have the same structure , a natural strategy of learning is to modify the weight vector of student @xmath0 so that this approaches teacher s weight @xmath1 as quickly as possible .",
    "however , if the teacher and student have different structures , the student trained to satisfy @xmath2 sometimes can not generalize the unlearnable rule better than the student with @xmath3 .",
    "several years ago , watkin and rau @xcite investigated the off - line learning of unlearnable rule where the teacher is a perceptron with a non - monotonic transfer function while the student is a simple perceptron .",
    "they discussed the case where the number of examples is of order unity and therefore did not derive the asymptotic form of the generalization error in the limit of large number of training examples .",
    "furthermore , as they used the replica method under the replica symmetric ansatz , the result may be unstable against replica symmetry breaking .",
    "for such a type of non - monotonic transfer function , a lot of interesting phenomena have been reported .",
    "for example , the critical loading rate of the model of hopfield type @xcite or the optimal storage capacity of perceptron @xcite is known to increase dramatically by non - monotonicity .",
    "it is also worth noting that perceptrons with the non - monotonic transfer function can be regarded as a toy model of a multilayer perceptron , a parity machine @xcite . in this context , inoue , nishimori and",
    "kabashima @xcite recently investigated the problem of on - line learning of unlearnable rules where the teacher is a non - monotonic perceptron : the output of the teacher is @xmath4 $ ] , where @xmath5 is the input potential of the teacher @xmath6 , with @xmath7 being a training example , and the student is a simple perceptron . for this system , difficulties of learning for the student can be controlled by the width @xmath8 of the reversed wedge . if @xmath9 or @xmath10 , the student can learn the rule perfectly and the generalization error decays to zero as @xmath11 for the conventional perceptron learning algorithm and @xmath12 for the hebbian learning algorithm , where @xmath13 is the number of presented examples , @xmath14 , divided by the number of input nodes , @xmath15 . for finite @xmath8 , the student can not generalize perfectly and the generalization error converges exponentially to a non - vanishing @xmath8-dependent value . in this paper",
    "we investigate the generalization ability of student trained by the on - line adatron learning algorithm with examples generated by the above - mentioned non - monotonic rule .",
    "the adatron learning is a powerful method for learnable rules both in on - line and off - line modes in the sense that this algorithm gives a fast decay , proportional to @xmath16 , of the generalization error @xcite , in contrast to the @xmath17 and @xmath18 decays of the perceptron and hebbian algorithms .",
    "we investigate the performance of the adatron learning algorithm in the unlearnable situation and discuss the asymptotic behavior of the generalization error .",
    "this paper is organized as follows . in the next section ,",
    "we explain the generic properties of the generalization error for our system and formulate the on - line adatron learning .",
    "some of the results of our previous paper @xcite are collected here concerning the perceptron and hebbian learning algorithms which are to be compared with the adatron learning .",
    "section iii deals with the conventional adatron learning both for learnable and unlearnable rules . in sec .",
    "iv we investigate the effect of optimization of the learning rate . in sec .",
    "v the issue of optimization is treated from a different point of view where we do not use the parameter @xmath8 , which is unknown to the student , in the learning rate . in last section",
    "we summarize our results and discuss several future problems .",
    "let us first fix the notation .",
    "the input signal comes from @xmath15 input nodes and is represented by an @xmath15-dimensional vector @xmath19 .",
    "the components of @xmath19 are randomly drawn from a uniform distribution and then @xmath19 is normalized to unity .",
    "synaptic connections from input nodes to the student perceptron are also expressed by an @xmath15-dimensional vector @xmath20 which is not normalized .",
    "the teacher receives the same input signal @xmath19 through the normalized synaptic weight vector @xmath21 .",
    "the generalization error is @xmath22 , where @xmath23 is the student output with the internal potential @xmath24 and @xmath25 stands for the average over the distribution function @xmath26 . \\label{dist1}\\ ] ] here @xmath27 stands for the overlap between the teacher and student weight vectors , @xmath28 .",
    "this distribution has been derived from randomness of @xmath7 and is valid in the limit @xmath29 .    the generalization error @xmath30 is easily calculated as a function of @xmath27 as follows @xcite @xmath31 where @xmath32 with @xmath33 .",
    "it is important that this expression is independent of specific learning algorithm .",
    "minimization of @xmath34 with respect to @xmath27 gives the theoretical lower bound , or the best possible value , of the generalization error for given @xmath8 . in fig .",
    "1 we show @xmath34 for several values of @xmath8 .",
    "this figure indicates that the generalization error goes to zero if the student is trained so that the overlap @xmath27 becomes 1 for @xmath9 and @xmath35 for @xmath10 .",
    "if the parameter @xmath8 is larger than some critical value @xmath36 , @xmath34 decreases monotonically from @xmath37 to @xmath38 as @xmath27 increases from @xmath39 to @xmath37 .",
    "when @xmath8 is smaller than @xmath40 , a local minimum appears at @xmath41 , but the global minimum is still at @xmath42 as long as @xmath8 is larger than @xmath43 .",
    "if @xmath8 is less than @xmath44 , the global minimum is found at @xmath45 , not at @xmath42 .",
    "this situation is depicted in figs . 2 and 3 where we show the optimal overlap @xmath27 giving the smallest value of @xmath34 and the corresponding best possible value of the generalization error as functions of @xmath8 .",
    "from these two figures , we see that the optimal overlap which gives the theoretical lower bound shows a first - order phase transition at @xmath46 .",
    "therefore , our efforts should be directed to finding the best strategy which gives the best possible value of the generalization error for a wide range of the parameter @xmath8 .",
    "it may be useful to review some of the results of , inoue , nishimori and kabashima @xcite who studied the present problem under the perceptron and hebbian algorithms .",
    "for the conventional perceptron learning , the generalization error decays to zero as @xmath11 if the rule is learnable ( @xmath9 ) , whereas it converges to a non - vanishing value @xmath47 , where @xmath48 , exponentially for the unlearnable case .",
    "this value of @xmath34 is larger than the best possible value as seen in fig .",
    "3 . introduction of optimization processes of the learning rate improves the performance significantly in the sense that the generalization error then converges to the best possible value when @xmath49 .",
    "for the conventional hebbian learning , the generalization error decays to the theoretical lower bound as @xmath12 not only in the learnable limit @xmath50 but for a finite range of @xmath8 , @xmath51 .",
    "however , for @xmath52 , the generalization error does not converge to the optimal value .",
    "the on - line training dynamics of the adatron algorithm is @xmath53 where @xmath54 stands for the number of presented patterns and @xmath55 is the leaning rate . it is straightforward to obtain the recursion equations for the overlap @xmath56 and the length of the student weight vector @xmath57 . in the limit @xmath29 ,",
    "these two dynamical quantities become self - averaging with respect to the random training data @xmath19 . for continuous time @xmath58 in the limit",
    "@xmath29 , @xmath59 with @xmath13 kept finite , the evolutions of @xmath27 and @xmath60 are given by the following differential equations @xcite : @xmath61 @xmath62 where @xmath63 with @xmath64 and @xmath65\\hspace{1.0in}\\nonumber \\\\ \\mbox{}\\hspace{.4in}+\\sqrt{\\frac{2}{\\pi}}\\ , ra ( \\sqrt{1-r^{2}}){\\delta } \\left [ 1 - 2h\\left ( \\frac{ra}{\\sqrt{1-r^{2}}}\\right ) \\right ] + re_{\\rm ad}. \\label{gg}\\end{aligned}\\ ] ] equations ( [ dlda ] ) and ( [ drda ] ) determine the learning process . in the rest of the present section",
    "we restrict ourselves to the case of @xmath66 corresponding to the conventional adatron learning .",
    "we first consider the case of @xmath67 and @xmath9 , the learnable rule .",
    "we investigate the asymptotic behavior of the generalization error when @xmath27 approaches 1 , @xmath68 , @xmath69 and @xmath70 , a constant . from eqs .",
    "( [ ead ] ) and ( [ gg ] ) , we find @xmath71 and @xmath72 with @xmath73",
    ". then eq .",
    "( [ drda ] ) is solved as @xmath74 with @xmath75 using this equation and eq .",
    "( [ ge ] ) , we obtain the asymptotic form of the generalization error as @xmath76 the above expression of the generalization error depends on @xmath77 , the asymptotic value of @xmath60 , through @xmath78 .",
    "apparently @xmath77 is a function of the initial value of @xmath60 as shown in fig .",
    "a special case is @xmath79 in which case @xmath60 does not change as learning proceeds as is apparent from eq .",
    "( [ dlda ] ) as well as from fig .",
    "such a constant-@xmath60 problem was studied by biehl and riegler @xcite who concluded @xmath80 for the adatron algorithm . our formula ( [ eg2 ] )",
    "reproduces this result when @xmath79 . if one takes @xmath77 as an adjustable parameter , it is possible to minimize @xmath81 by maximizing @xmath78 in the denominator of eq .",
    "( [ eg2 ] ) .",
    "the smallest value of @xmath81 is achieved when @xmath82 , yielding @xmath83 which is smaller than eq .",
    "( [ ge3 ] ) for a fixed @xmath60 .",
    "we therefore have found that the asymptotic behavior of the generalization error depends upon whether or not the student weight vector is normalized and that a better result is obtained for the un - normalized case .",
    "we plot the generalization error for the present learnable case with the initial value of @xmath84 in fig .",
    "we see that the hebbian learning has the highest generalization ability and the adatron learning shows the slowest decay among the three algorithms in the initial stage of learning .",
    "however , as the number of presented patterns increases , the adatron algorithm eventually achieves the smallest value of the generalization error . in this sense",
    "the adatron learning algorithm is the most efficient learning strategy among the three in the case of the learnable rule .      for unlearnable case , there can exist only one fixed point @xmath85",
    "this reason is , for finite @xmath8 , @xmath86 appearing in eq .",
    "( [ dlda ] ) does not vanish in the limit of large @xmath87 and @xmath86 has a finite value for @xmath88 . for this finite @xmath86 ,",
    "the above differential equation has only one fixed point @xmath85 .",
    "in contrast , for the learnable case , @xmath86 behaves as @xmath71 in the limit of @xmath89 and thus @xmath90 becomes zero irrespective of @xmath60 asymptotically .",
    "we plot trajectories in the @xmath27-@xmath60 plane for @xmath91 in fig . 6 and the corresponding generalization error is plotted in fig . 7 as an example . from fig .",
    "6 , we see that the destination of @xmath60 is @xmath92 for all initial conditions .",
    "figure 7 tells us that for the unlearnable case @xmath91 , the adatron learning has the lowest generalization ability among the three .",
    "we should notice that the generalization error decays to its asymptotic value , the residual error @xmath93 , as @xmath94 for the hebbian learning and decays exponentially for perceptron learning @xcite .",
    "the residual error of the hebbian learning @xmath95 is also the best possible value of the generalization error for @xmath96 as seen in fig .",
    "3 . in fig . 8 we also plot the generalization error of the adatron algorithm for several values of @xmath8 . for the adatron learning of the unlearnable case , the generalization error converges to a non - optimal value @xmath97 exponentially . for all unlearnable cases ,",
    "the @xmath27-@xmath60 flow is attracted into the fixed point @xmath98 , where @xmath99 is obtained from @xmath100 the solution @xmath99 of the above equation is not the optimal value because the optimal value of the present learning system is @xmath101 for @xmath49 and @xmath102 for @xmath103 @xcite . from figs .",
    "3 and 7 , we see that the residual error @xmath93 of the adatron learning is larger than that of the conventional perceptron learning .",
    "therefore , we conclude that if the student learns from the unlearnable rules , the on - line adatron algorithm becomes the worst strategy among three learning algorithms as we discussed above although for the learnable case , the on - line adatron learning is a sophisticated algorithm and the generalization error decays to zero as quickly as the off - line learning @xcite .",
    "in the previous section , we saw that the on - line adatron learning fails to get the best possible value of the generalization error for the unlearnable case and its residual error @xmath93 is larger than that of the conventional perceptron learning or hebbian learning .",
    "we show that it is possible to overcome this difficulty .",
    "we now consider an optimization the learning rate @xmath55 @xcite .",
    "this optimization procedure is different from the technique of kinouchi and caticha @xcite . as the optimal value of @xmath27 which gives the best possible value of the generalization error is @xmath101 for @xmath49",
    ", we determine @xmath55 so that @xmath27 is accelerated to become @xmath37 . in order to determine @xmath104 using the above strategy",
    ", we maximize the right hand side of eq .",
    "( [ drda ] ) with respect to @xmath55 and obtain @xmath105 .",
    "using this optimal learning rate , eqs .",
    "( [ dlda ] ) and ( [ drda ] ) are rewritten as follows @xmath106 @xmath107 for the learnable case , we obtain the asymptotic form of the generalization error from eqs .",
    "( [ dlda2 ] ) and ( [ drda2 ] ) by the same relation @xmath68 , @xmath69 as we used for the case of @xmath66 as @xmath108 this is the same asymptotic behavior as that obtained by optimizing the initial value of @xmath60 as we saw in the previous section .",
    "next we investigate the unlearnable case .",
    "the asymptotic forms of @xmath86 and @xmath109 in the limit of @xmath89 are obtained as @xmath110 and @xmath111 then we get the asymptotic solution of eq .",
    "( [ drda2 ] ) with respect to @xmath112 , @xmath68 , as @xmath113 as the asymptotic behavior of @xmath34 is obtained as @xmath114 @xcite , we find the generalization error in the limit of @xmath89 as follows @xmath115 where @xmath116 is the best possible value of the generalization error for @xmath49 .",
    "therefore , our strategy to optimize the learning rate succeeds in training the student to obtain the optimal overlap @xmath42 for @xmath117 . for the perceptron learning",
    ", this type of optimization failed to reach the theoretical lower bound of the generalization error for @xmath8 exactly at @xmath118 in which case the generalization error is @xmath119 , equivalent to a random guess because for @xmath120 optimal learning rate vanishes @xcite .",
    "in contrast , for the adatron learning , the optimal learning rate has a non - zero value even at @xmath120 . in this sense ,",
    "the on - line adatron learning with optimal learning rate is superior to the perceptron learning .",
    "in the previous section , we were able to get the theoretical lower bound of the generalization error for @xmath96 by introducing the optimal learning rate @xmath121 .",
    "however , as the optimal learning rate @xmath121 contains a parameter @xmath8 unknown to the student , the above result can be regarded only as a lower bound of the generalization error .",
    "the reason is that the student can get information only about teacher s output and no knowledge of @xmath8 or @xmath122 . in realistic situations ,",
    "the student does not know @xmath8 or @xmath5 and therefore has a larger value of the generalization error . in this section",
    ", we construct a learning algorithm without the unknown parameter @xmath8 using the asymptotic form of the optimal learning rate",
    ".      for the learnable case , the optimal learning rate is estimated in the limit of @xmath89 as @xmath123 this asymptotic form of the optimal learning rate depends on @xmath87 only through the length @xmath60 of student s weight vector .",
    "we therefore adopt @xmath55 proportional to @xmath60 , @xmath124 , also in the case of the parameter - free optimization and adjust the parameter @xmath125 so that the student obtains the best generalization ability . substituting this expression into the differential equation ( [ drda ] ) for @xmath27 and using @xmath68 with @xmath69 , we get @xmath126 where we have set @xmath127 this leads to @xmath128",
    ". then , the generalization error is obtained from @xmath129 as @xmath130 in order to minimize @xmath30 , we maximize @xmath131 with respect to @xmath125 .",
    "the optimal choice of @xmath125 in this sense is @xmath132 and we find in such a case @xmath133 this is the same asymptotic form as the previous @xmath8-dependent result ( [ ge5 ] ) .",
    "next we consider the unlearnable case .",
    "the asymptotic form of the learning rate we derived in the previous section for the unlearnable case is @xmath134 where we used eq .",
    "( [ asep ] ) to obtain the right - most equality and we set the @xmath8-dependent prefactor of @xmath60 as @xmath125 . using this learning rate ( [ asg2 ] ) and the asymptotic forms of @xmath135 and @xmath136 as @xmath137 and @xmath138 in the limit of @xmath89 , we obtain the differential equation with respect to @xmath112 from eq .",
    "( [ drda ] ) as follows @xmath139 \\frac{{\\eta}^{2}}{{\\alpha}^{2 } } -{\\eta } \\frac{4a}{\\sqrt{2\\pi } } { \\delta}\\frac{\\varepsilon}{\\alpha}. \\label{difeq}\\ ] ] this differential equation can be solved analytically as @xmath140 where @xmath141 is a constant determined by the initial condition .",
    "therefore , if we choose @xmath142 to satisfy @xmath143 , the generalization error converges to the optimal value @xmath116 as @xmath144 in order to obtain the best generalization ability , we minimize the prefactor of @xmath145 in the second term of eq .",
    "( [ type1 ] ) and obtain @xmath146 for this @xmath125 , the condition @xmath147 is satisfied . in general ,",
    "if we take @xmath125 independent of @xmath8 , the condition @xmath147 is not always satisfied .",
    "the quantity @xmath148 takes the maximum value @xmath149 at @xmath150 .",
    "therefore , whatever value of @xmath8 we choose , we can not obtain the @xmath12 convergence if the product of this maximum value @xmath149 and @xmath125 is not larger than unity .",
    "this means that @xmath125 should satisfy @xmath151 for the first term of eq .",
    "( [ sol ] ) dominate asymptotically , yielding eq .",
    "( [ type1 ] ) , for a non - vanishing range of @xmath8 .",
    "in contrast , if we choose @xmath125 to satisfy @xmath152 , the generalization error is dominated by the second term of eq .",
    "( [ sol ] ) and behaves as @xmath153 in this case , the generalization error converges less quickly than ( [ type1 ] ) .",
    "for example , if we choose @xmath154 , we find that the condition @xmath155 can not be satisfied by any @xmath8 and the generalization error converges as in eq .",
    "( [ type2 ] ) .",
    "if we set @xmath156 ( @xmath157 ) as another example , the asymptotic form of the generalization error is either eq . ( [ type1 ] ) or eq . ( [ type2 ] ) depending on the value of @xmath8 .",
    "we have investigated the generalization abilities of a simple perceptron trained by the teacher who is also a simple perceptron but has a non - monotonic transfer function using the on - line adatron algorithm . for the learnable case ( @xmath9 ) ,",
    "if we fix the length of the student weight vector as @xmath158 , the generalization error converges to zero as @xmath159 as biehl and riegler reported @xcite .",
    "however , if we allow the time development of the length of student weight vector , the asymptotic behavior of the generalization error shows dependence on the initial value of @xmath60 . when the student starts the training process from the optimal length of weight vector @xmath60",
    ", we can obtain the generalization error @xmath160 which is a little faster than @xmath161 .",
    "as the student is able to know the length of its own weight vector in principle , we can get the better generalization ability @xmath160 by a heuristic search of the optimal initial value of @xmath60 . on the other hand ,",
    "if the width @xmath8 of the reversed wedge has a finite value , the generalization error converges exponentially to a non - optimal @xmath8-dependent value .",
    "in addition , these residual errors are larger than those of the conventional perceptron learning for the whole range of @xmath8 .",
    "therefore we conclude that , although the adatron learning is powerful for the learnable case @xcite including the situation in which the input vector is structured @xcite , it is not necessarily suitable for learning of the non - monotonic input - output relations .",
    "next we introduced the learning rate and optimized it .",
    "for the learnable case , the generalization error converges to zero as @xmath162 which is as fast as the result obtained by selecting the optimal initial condition for the case of non - optimization , @xmath66 . for this learnable case",
    ", the asymptotic form of the optimal leaning rate is @xmath163 .",
    "therefore , for the on - line adatron learning , it seems that the length of the student weight vector plays an important role to obtain a better generalization ability .",
    "if the task is unlearnable , the generalization error under optimized learning rate converges to the theoretical lower bound @xmath116 as @xmath164 for @xmath117 . using this strategy",
    ", we can get the optimal residual error for @xmath8 even exactly at @xmath40 for which the optimized perceptron learning failed to obtain the optimal residual error @xcite .",
    "we also investigated the generalization ability using a parameter - free learning rate .",
    "when the task is learnable , we assumed @xmath165 and optimized the prefactor @xmath125 . as a result , we obtained @xmath166 which is the same asymptotic form as the parameter - dependent case .",
    "therefore , we can obtain this generalization ability by a heuristic choice of @xmath125 ; we may choose the best @xmath125 by trial and error .",
    "on the other hand , for the unlearnable case , we used the asymptotic form of the @xmath8-dependent learning rate in the limit of @xmath89 , @xmath167 , and optimized the coefficient @xmath125 .",
    "the generalization error then converges to @xmath116 as @xmath12 for @xmath155 . if @xmath168 , the generalization error decays to @xmath116 as @xmath169 , where the exponent @xmath170 is smaller than @xmath92 because @xmath168 .",
    "similar slowing down of the convergence rate of the generalization error by tuning a control parameter was also reported by kabashima and shinomoto in the problem of learning of two - dimensional blurred dichotomy @xcite . in conclusion",
    ", we could overcome the difficulty of the adatron learning of unlearnable problems by optimizing the learning rate and the generalization error was shown to converge to the best possible value as long as the width @xmath8 of reversed wedge satisfies @xmath49 .",
    "for the parameter region @xmath103 , this approach does not work well because the optimal value of @xmath27 is @xmath171 instead of @xmath37 ; our optimization is designed to accelerate the increase to @xmath27 toward @xmath37 . in this paper",
    ", we could construct a learning strategy suitable to achieve the @xmath8-dependent optimal value @xmath116 for @xmath49 .",
    "however , for @xmath103 , it is a very difficult but challenging future problem to get the optimal value by improving the conventional adatron learning .",
    "the authors would like to thank dr .",
    "yoshiyuki kabashima for helpful suggestions and comments .",
    "one of the authors ( j. i. ) thanks dr .",
    "siegfried b@xmath172s for several useful comments .",
    "j. a. hertz , a. krogh and r. g. palmer , _ introduction to the theory of neural computation _",
    "( addison - wesley , redwood city , 1991 ) .",
    "t. h. watkin , a. rau and m. biehl , rev .",
    "65*,499 ( 1993 ) .",
    "m. opper and w. kinzel , in _ physics of neural networks iii _ , eds .",
    "e. domany , j. l. van hemmen and k. schulten ( springer , berlin , 1995 ) .",
    "t. h. watkin and a. rau , phys .",
    "a * 45 * , 4111 ( 1992 )",
    ". m. morita , s. yoshizawa and k. nakano , trans .",
    "ieice * j73-d - ii * , 242 ( 1990 ) ( in japanese ) .",
    "h. nishimori and i. opris , neural networks * 6 * , 1061 ( 1993 ) .",
    "j. inoue , j. phys .",
    "a : math . gen .",
    "* 29 * , 4815 ( 1996 ) .",
    "g. boffetta , r. monasson and r. zecchina , j. phys .",
    "a : math . gen . * 26 * , l507 ( 1993 ) .",
    "r. monasson and d. okane , europhys . lett . * 27 * , 85 ( 1994 ) .",
    "j. inoue , h. nishimori and y. kabashima , ( unpublished ) .",
    "m. biehl and p. riegler , europhys .",
    "* 28 * , 525 ( 1994 ) .",
    "j. k. anlauf and m. biehl , europhys . lett . * 10 * , 687 ( 1989 ) .",
    "p. riegler , m. biehl , s. a. solla and c. marangi , in _ proc .",
    "of italian workshop on neural nets vii _ ( 1995 ) .",
    "m. opper , w. kinzel , j. kleinz and r. nehl , j. phys .",
    "a : math . gen .",
    "* 23 * , l581(1990 ) .",
    "o. kinouchi and n. caticha , j. phys .",
    "a : math . gen .",
    "* 26 * , 6161 ( 1993 ) .",
    "y. kabashima and s. shinomoto , neural comp .",
    "* 7 * , 158 ( 1995 ) ."
  ],
  "abstract_text": [
    "<S> we study the on - line adatron learning of linearly non - separable rules by a simple perceptron . </S>",
    "<S> training examples are provided by a perceptron with a non - monotonic transfer function which reduces to the usual monotonic relation in a certain limit . </S>",
    "<S> we find that , although the on - line adatron learning is a powerful algorithm for the learnable rule , it does not give the best possible generalization error for unlearnable problems . </S>",
    "<S> optimization of the learning rate is shown to greatly improve the performance of the adatron algorithm , leading to the best possible generalization error for a wide range of the parameter which controls the shape of the transfer function .    </S>",
    "<S> pacs numbers : 87.10.+e </S>"
  ]
}