{
  "article_text": [
    "low - level vision is the estimation of depth , motion , shape , and other physical scene properties from visual measurements .",
    "since it is ill - posed , methods often employ a _ local model _ that is expected to apply piecewise across the scene , and that restricts the variation of scene values within each applicable piece or region . slanted planes for binocular disparity , affine optical flows , and families of smooth shapes for surface normals are common examples .",
    "the restriction on scene variability in applicable regions allows image cues to be aggregated spatially across each region , thereby reducing the ambiguity that exists point - wise .",
    "the fundamental challenge lies in identifying  automatically from the image input  the sizes and shapes of the aggregation regions that are right for each part of a scene .",
    "regions that are too small do not sufficiently reduce the underlying ambiguity , while those that are too big or the wrong shape span abrupt scene changes that violate the local model and make estimates unreliable ( , fig .",
    "[ fig : fwork ]  ( a ) ) .",
    "we introduce a computational framework to address this challenge . called the _ consensus framework _ , we apply it to the binocular stereo problem while also presenting it generally as a way to attack a variety of low - level tasks .",
    "the framework explicitly considers a large set of dense , overlapping regions of many sizes that redundantly cover the image plane ( fig .",
    "[ fig : fwork ]  ( b ) ) .",
    "it simultaneously determines which regions are inliers to the local model ( binary variables ) and , for each inlying region , the correct coordinates in the local model space for that region ( continuous variables ) .",
    "estimation is cast as optimizing an objective that requires each inlying region to be supported by its local image data while also having scene estimates that are consistent with its overlapping neighbor regions .",
    "the output of the framework  the inlier statuses of all regions and the local estimates from the inliers  offers a rich , multi - scale representation of the physical scene .",
    "this includes spatial grouping information , a global scene map , and a point - wise measure of confidence , all of which are desirable when seeking to combine multiple low - level cues or integrate higher - level processes .",
    "compared to traditional approaches based on markov random fields ( mrfs ) , the consensus framework reasons in a much larger variable space , and more critically , with orders of magnitude more links between variables .",
    "this is because it enforces simultaneous consistency between the thousands of regions that overlap any single pixel .",
    "despite this complexity , two properties make estimation not only feasible , but efficient .",
    "first , since the dense region - set embodies an over - complete scene representation  with many more internal variables than values in the output scene map  good solutions can often be reached by a simple alternating algorithm similar to expectation - maximization .",
    "second , we show analytically that when the regions are organized hierarchically by scale ( fig .",
    "[ fig : fwork ]  ( c ) ) , each region only needs to sum information from its parents and children ( fig .",
    "[ fig : fwork ]  ( d ) ) .",
    "this leads to a significant reduction in computation because the hierarchical connections constitute only a minuscule fraction of the total links that exist in the consensus objective .",
    "the estimation architecture ends up being composed of a large network of computational units , one for each region .",
    "regardless of its region s scale , each unit carries out identical operations at each iteration , and these operations happen in parallel at each scale . by sharing information through sparse connections between parents and children ,",
    "the units collaborate to produce a consistent scene representation over the image plane . from an implementation perspective",
    ", this structure allows estimation to be trivially parallelized across multiple cores and single instruction multiple data ( simd ) channels .",
    "experiments on the binocular stereo problem show that the consensus framework achieves greater accuracy on the kitti benchmark  @xcite than comparable state - of - the - art variational and mrf approaches .",
    "there are many techniques for low - level vision problems like binocular stereo , optical flow , and shape - from - shading .",
    "while they vary greatly in the way they derive information point - wise from image cues , their mechanisms for spatial aggregation tend to follow one of three different paradigms .",
    "the simplest paradigm is purely local  a single support region is explicitly defined around each pixel  @xcite .",
    "these regions are typically determined using intensity and texture information , either independently for each pixel or jointly for all pixels via segmentation , and they succeed when color and texture boundaries are well aligned with boundaries in the latent scene map .",
    "variational methods form another category .",
    "estimation involves minimizing a per - pixel data cost along with a spatial regularization term that penalizes large derivatives in the scene map  @xcite .",
    "the derivative filters are designed to measure deviations from some implied local model , and the penalty is chosen to promote piecewise adherence while still being convex .",
    "some variational methods employ multi - scale reasoning , through sequential coarse - to - fine optimization  @xcite or simultaneous penalization of derivatives at multiple scales  @xcite .",
    "the third dominant paradigm are mrf - based methods  @xcite .",
    "these explicitly encode piece - wise adherence to the local model ( as opposed to the convex penalties in variational methods , which do so implicitly ) , by making hard decisions about the local model being valid across an edge or clique .",
    "since they often consider continuous label spaces and non - submodular smoothness terms , these methods tend to rely on expensive approximate algorithms for optimization .",
    "computation can be reduced by defining graphs on super - pixels instead of pixels  @xcite , and this does not substantially reduce accuracy as long as the super - pixel boundaries happen to be well aligned with scene boundaries .",
    "the consensus framework is different from traditional , single - scale mrf techniques because it is defined on overlapping regions at multiple scales .",
    "it is also different from multi - scale mrf formulations that have been used for segmentation  @xcite , where parent nodes encode semantic context for co - occurring labels of their children . in consensus , all regions at all scales are self - similar .",
    "they all make direct predictions about pixel - level scene values , and they all use the same local model .",
    "consensus is inspired by our previous work on shape - from - shading  @xcite .",
    "the objective in that paper can be seen as a special case of the one proposed here . here ,",
    "we introduce an encoding of local models that broadens the approach to a variety of low - level vision tasks .",
    "we also show that this encoding , when combined with a hierarchical organization that can be applied to a broad class of region sets , dramatically reduces computational expense by sharing information between parent and child regions in the hierarchy .",
    "we use an alternating algorithm to minimize our objective .",
    "this is similar to `` divide and concur '' optimization algorithms like the _ alternating direction method of multipliers _ ( admm )  @xcite that modify an objective to create multiple copies of a variable ",
    "one for each term in the original objective that includes that variable  and then enforce consistency between these copies .",
    "our consensus objective resembles these modified , split objectives .",
    "a crucial part of our approach is the hierarchical organization of regions across scales , which makes the aggregation steps in the alternating minimization tractable .",
    "it is worth noting the approach of @xcite here , which also uses an efficient data - structure for message aggregation during mean - field inference in a densely connected graph .",
    "we begin with a formal description of the three main components of the proposed framework .",
    "first , there is the global scene map .",
    "this is a function @xmath0 on the two - dimensional image plane , with @xmath1 indexing discrete spatial locations .",
    "@xmath2 may be scalar - valued ( @xmath3 ) for properties such as stereo disparity , or vector - valued for properties such as motion and 3d surface orientation .",
    "second , there is a dense set @xmath4 of overlapping regions @xmath5 within the image plane , each one a collection of locations @xmath6 .",
    "set @xmath4 has regions at @xmath7 different scales , and symbol @xmath8 represents the subset of regions at scale @xmath9 . by convention ,",
    "larger values of @xmath10 correspond to larger regions .",
    "moreover , the regions can be organized hierarchically : for every region @xmath11 at scale @xmath12 , it is possible to select a set @xmath13 of non - overlapping `` child regions '' from scales smaller than @xmath10 , such that @xmath14 can be written exactly as their disjoint union .",
    "[ fig : fwork ]  ( c , d ) shows an example of such a region set for a one - dimensional image plane , where each @xmath8 is the set of overlapping regions of length @xmath15 , and each @xmath16 is the union of two children from @xmath17 .",
    "the final component is the local model .",
    "it is expected to apply piecewise across most of the scene , and it restricts accordingly the allowable choices for scene values within any region @xmath14 .",
    "it is encoded in a mathematical form that encompasses all sorts of local models proposed in computer vision  @xcite , while also enabling the system to exploit the computational redundancy inherent to a hierarchy of regions .",
    "the local model says that scene values within any region @xmath14 must satisfy : @xmath18 where @xmath19 is some pre - defined matrix - valued function on the image plane , and @xmath20 is a variable associated with region @xmath14 .",
    "algebraically , this restricts local scene values to an @xmath21-dimensional linear sub - space , regardless of region size ; and as a consequence of using a common @xmath22 , local scene estimates from two overlapping regions @xmath14 and @xmath23 agree whenever @xmath24 . here",
    "are some examples of functions @xmath22 and their corresponding physical interpretations : @xmath25 , d = 1 , m = 3,\\\\&\\mbox{(disparity of locally - planar surfaces),}\\vspace{1em}\\\\ u(n ) = & \\left[\\begin{array}{c}\\partial/\\partial x\\\\\\partial/\\partial y\\end{array}\\right][x^2~~y^2~~xy~~x~~y ] , d = 2 , m = 5,\\\\&\\mbox{(normals of locally - quadratic surfaces),}\\vspace{1em}\\\\ u(n)=&\\left[\\begin{array}{cc}x~~y~~1&0\\\\0&x~~y~~1\\end{array}\\right ] , d = 2 , m = 6,\\\\ & \\mbox{(flow vectors for locally - affine motion ) . } \\end{array}\\ ] ]    with the three components in hand , estimation requires determining : a ) which regions @xmath5 are inliers with respect to the local model ; and b ) for all inlying regions , values of the per - region variables @xmath26 that are supported by the image data and consistent with each other .",
    "inliers are indicated by a binary variable @xmath27 associated with each patch .",
    "once determined , the values of @xmath28 together provide a rich and over - complete representation of the physical scene . at each point @xmath6 ,",
    "local grouping information is available through the subset @xmath29 of ( potentially thousands of ) inlying regions covering that point : @xmath30 an estimate @xmath31 of the global scene map is induced as the point - wise average , or _ consensus _ , of the local estimates from inlying regions :",
    "@xmath32 the count @xmath33 represents the _ degree of consensus _ at each point , and provides a point - wise measure of confidence in the estimate @xmath31 .",
    "estimation is then cast as a minimization of the following cost over variables @xmath28 : @xmath34 .",
    "\\label{eq : maincost}\\end{aligned}\\ ] ] the first term applies a cost @xmath35 for declaring region @xmath14 an outlier , in line with intuition that the local model is often valid .",
    "the second term scores local variables @xmath26 in each inlying region using _ data cost _ @xmath36 , typically measuring the ability of restricted local scene estimates @xmath37 to explain the relevant image data .",
    "both @xmath35 and @xmath36 can optionally be augmented to encode prior information about the scene or context from semantic visual processes .",
    "the final @xmath38-weighted term promotes consistency between overlapping regions by penalizing , at every point , the variance of the scene predictions from inlying regions that cover it .",
    "to minimize , we re - write the consistency term in terms of the global scene map @xmath39 , creating a related cost @xmath40 : @xmath41 where the two costs are equal when @xmath39 is set to the consensus , @xmath42 . in this new cost ,",
    "the per - region summations are quadratic functions of variables @xmath26 : @xmath43 with each @xmath44 a pre - computed @xmath45 matrix permanently associated with region @xmath14 ; and each @xmath46 an @xmath21-vector and a scalar , respectively , derived from @xmath39 as : @xmath47    cost @xmath40 is minimized iteratively , with each iteration having two steps .",
    "the first step is a minimization over region variables @xmath28 with @xmath39 fixed .",
    "conveniently , this can be done independently  and in parallel  for each region since there are no cross - region terms in @xmath40 when @xmath39 is fixed .",
    "these independent minimizations are achieved by setting @xmath48,\\ ] ] and then , @xmath49 > \\tau_p,\\\\         1 , & \\mbox{otherwise . }         \\end{array}\\right.\\ ] ] in other words , the best model - based explanation is found for each region @xmath14 , and then the region is declared outlier if the error - of - fit exceeds the outlier cost @xmath35 .    the second step at each iteration is a minimization over @xmath39 with region variables fixed at their new values .",
    "this is achieved simply by setting @xmath50 as per , and it is thus guaranteed that @xmath51 at the end of every iteration . consequently , beginning with any initial estimate of the scene map @xmath39 , each iteration decreases the value of @xmath40 , and therefore of @xmath52 , which converges to a ( local ) minimum whenever @xmath53 have finite lower bounds .    convergence to a good local minimum is promoted by beginning the iterations with a smaller value for the consistency weight @xmath38 , and then increasing it to its final value across the initial iterations .",
    "interestingly , this induces a temporal coarse - to - fine refinement of the scene map during the optimization . early - on ,",
    "smaller @xmath38 values allow more inlying regions , causing the consensus to be smoothed across larger areas .",
    "as @xmath38 increases , more regions that span scene discontinuities become outliers , and the consensus exhibits progressively finer detail .",
    "the computational cost of this optimization depends on the complexities of the three parts of every iteration :    1 .   computing intermediate regional consistency terms \\{@xmath46 } from @xmath39 as per . 2 .   updating @xmath54 for every region @xmath14 as per , .",
    "3 .   setting @xmath55 as per .",
    "the complexity of the second part depends on the forms of functions @xmath36 , but it usually scales well because it involves parallel optimization over @xmath21-dimensional domains ( regardless of region size ) .",
    "the first and third parts would scale poorly if implemented naively , but this can be averted by exploiting the hierarchical structure of @xmath4 .",
    "first , consider the computation of @xmath56 from @xmath39 .",
    "using the fact that every region @xmath14 is partitioned by its child regions @xmath57 , we can write @xmath58 and similarly , @xmath59 .",
    "this reduces the number of additions significantly  from the size of the region @xmath14 to just the number of its children . to ensure that values of @xmath60 are available ,",
    "calculations of @xmath56 are scheduled in an upward sweep through the hierarchy , using explicit summation over @xmath6 for regions at scale @xmath61 , and the cheaper right - most expression of for progressively larger scales .",
    "the hierarchical structure can also be leveraged to efficiently compute the consensus @xmath31 from the current values of the region variables @xmath62 .",
    "note that for every region @xmath63 at scale @xmath12 , there is one and only one child region in @xmath13 that also includes @xmath6 .",
    "for the simple case with only two scales ( @xmath64 ) , we see that the summation of local estimates from inlying regions can be simplified to @xmath65 where @xmath66 denotes the set of _ parents _ for any region @xmath14 at the largest scale , @xmath67 . ] . in the more general case with @xmath7 scales ,",
    "we recursively define augmented variables @xmath68 for every region @xmath14 as @xmath69 which can be computed by a _ downward _",
    "sweep through the pyramid .",
    "then , it is easy to see that the numerator and denominator of the expression for @xmath70 in are given by @xmath71 thus , instead of computing summations over all overlapping regions at all scales for each location @xmath6 , the consensus can be computed using summations over the augmented variables @xmath72 of regions at just the smallest scale .",
    "the gains from using these recursive computations is substantial , and can be interpreted as reducing the _ effective _ connectivity of the framework to just the sparse set of hierarchical links . for the network in fig .",
    "[ fig : fwork ] ( c , d ) , it represents a reduction , in the number of required summations for and , from @xmath73 to @xmath74 .",
    "moreover , while the recursion requires different scales to be processed sequentially , note that the computations in and can still be carried out for all regions @xmath11 at each scale @xmath10 in parallel .",
    "therefore , as visualized in fig .",
    "[ fig : fwork ]",
    "( d ) , computation happens in a distributed architecture , requires the identical operations of , , , and at each region , with operations at each scale happening in parallel and information being passed through hierarchical links between scales  all of which arises naturally as an efficient way to optimize a well - defined mathematical objective . a complete listing of the algorithm is included in the supplementary material .",
    "in this section , we describe the application of the consensus framework to the task of stereo estimation , and its evaluation on the kitti benchmark  @xcite .",
    "we reason with a planar local model ( , @xmath75 $ ] ) , and define our region set @xmath4 to be a two - dimensional equivalent of the pyramid in fig .",
    "[ fig : fwork](b , c ) , with five scales consisting of all overlapping patches of sizes @xmath76 , @xmath77 , @xmath78 , @xmath79 , and @xmath80 , where patches at all but the finest scale can be partitioned into four children from the next lower scale .",
    "we follow the approach of @xcite in defining our data cost @xmath36 . in particular",
    ", we use the implementation from yamaguchi et al .",
    "@xcite , that implements semi - global matching ( sgm )  @xcite with a cost based on absolute differences of gradients and the census transform  @xcite , to compute an initial set of approximate disparity estimates @xmath81 at a semi - dense set of locations @xmath82 . the data costs for every region @xmath14 are then defined as : @xmath83 where @xmath84 if @xmath85 , @xmath86 if there is a discontinuity in @xmath87 around @xmath6 , and @xmath88 otherwise .",
    "note that since each @xmath36 above is also a quadratic function of @xmath89 , the minimizations in simply involve solving a @xmath90 linear system for each region ( and moreover , we can re - use the results of ldl decompositions across iterations when @xmath38 is constant ) .",
    "the scalar outlier costs @xmath91 for various patches are defined to be proportional to their size @xmath92 as : @xmath93 here , @xmath94 measures if @xmath14 s children are a better fit to a different parent based on intensity variance , and is the count of the number of patches that share a child with @xmath14 and whose intensity variance is lower than that of @xmath14 . through cross - validation , we set @xmath95 and the consistency weight @xmath96 , and as described in sec .  [",
    "sec : body2 ] , we begin the iterations with a lower value of @xmath97 , and increase it by a factor of eight every six iterations till it reaches its final value .        finally , we add a way to incorporate reasoning about occlusions . while one could achieve this encoding using more sophisticated definitions of @xmath36 or @xmath35 , we find that a much simpler approach suffices .",
    "we use the fact that the pixels missing in @xmath98 correspond to those that have failed a left - right consistency check , and are likely occluded . using the data costs",
    "defined as above , the consensus framework usually yields a scene map where disparity values in occluded regions , in the absence of any input data , are interpolated between the occluded and occluding planes either smoothly , or with a discontinuity at an arbitrary location within the region . in order to incorporate the intuition that occluded pixels are likely to be part of the occluded background , we run the alternating minimization for fifty iterations",
    "; then set the values of the consensus @xmath2 , at the potentially occluded points in @xmath99 , to the lower of their current value and that of the background pixel on the same epipolar line ( , the nearest pixel on the same horizontal line in @xmath98 ) ; and then run the minimization for another thirty iterations .    a reference implementation ( available on the project page ) , designed to make use of thread - based parallelism and simd instruction sets , takes an average of only six seconds ( 1.5 seconds for the initial sgm step ) on a @xmath100 image , when running on a cpu with six cores .",
    "moreover , since the computations in the framework have a degree of parallelism roughly equal to the resolution of the input image , we expect execution time will continue to decrease with access to larger numbers of cores .",
    "we evaluate the proposed algorithm on the kitti benchmark  @xcite which contains a total of 389 grayscale image pairs of rural road scenes , captured using an autonomous driving platform equipped with a pair of high - resolution cameras .",
    "a velodyne laser scanner provides ground truth at a subset of pixels in each scene .",
    "this ground truth is made available for a subset of 194 image pairs ",
    "the _ training set_and withheld for the remaining image pairs that form the _ testing set_. a website associated with the database tracks the performance of stereo algorithms on the testing set . note that while the benchmark also contains temporally - adjacent stereo frames that allow simultaneous reasoning about optical flow and stereo , we ignore those extra frames and consider the pure stereo problem here .",
    "figure  [ fig : results ] visualizes various aspects of the internal representation of our framework on convergence , for three scenes in the kitti training set .",
    "the top row shows the consensus global disparity map , and rows 26 visualizes a regularly - spaced subset of in the inlier statuses @xmath101 .",
    "row 7 provides another view of variables @xmath101 , by explicitly showing some of the `` support regions '' formed as the union of all patches in @xmath29 , for various pixels @xmath6 .",
    "these regions by - and - large group together points whose disparity values would be well - explained by a slanted plane model . as expected , there is significant variation in the size and shapes of the support regions across each scene , matching the scale of the underlying scene structures .",
    "this highlights the distinction from superpixel - based mrf approaches  @xcite , which require choosing a single scale for the entire scene .",
    "also note that for many pairs of points that do not directly lie in each others support regions , the regions themselves have significant overlap . through such overlap ,",
    "the consensus estimate at a point has benefited from aggregation across regions that are larger than the union of the set of patches that include it .    the final row in fig .",
    "[ fig : results ] visualizes the degree of consensus @xmath102 at all points ( blue saturation ) , simultaneously with locations of erroneous estimates ( red ) .",
    "we see that many of the errors occur around object boundaries and near small scene structures , which are also points where @xmath102 is low .",
    "we quantify this observation in fig .  [",
    "fig : jnerr ] , and find that average estimation error drops rapidly as we discard points with the lowest values of @xmath102 .",
    "this an other benefit of the rich internal representation : in addition to providing a global scene estimate , it also provides a natural measure of point - wise confidence in this estimate .",
    "next , we visualize the progression of the alternating minimization algorithm . figure  [ fig : temporal ] shows the evolution of the consensus @xmath70 across iterations . in the early iterations as the consistency weight @xmath38",
    "is increased toward its final value , the map undergoes a coarse - to - fine refinement , with image boundaries becoming sharper . after the fiftieth iteration",
    ", the occlusion correction is applied to propagate disparity estimates from background planes into potentially - occluded regions .",
    "since this is a simple correction applied independently along each epipolar line , it introduces inconsistencies within occluded regions .",
    "the scene map therefore benefits from further refinement through another thirty iterations of minimizing the consensus cost , yielding a final estimate that is more accurate .",
    "the supplementary material includes additional results showing the evolution of the objective for different initial values and update schedules for @xmath38 .    .",
    "blue curve shows percentage of points with @xmath102 above different thresholds , and red curve their corresponding error rate , in terms of percentage with error @xmath103 px .",
    "these are computed over all pixels with ground truth data available , across all images in the kitti training set . ]    [ cols=\"^,^,^,^,^ \" , ]      : same matching cost    table  [ tab : res ] compares the consensus framework with other state - of - the - art stereo algorithms in terms of various error quantiles on the kitti testing set .",
    "the most direct comparisons of our results are with those of @xcite , since these methods all use the same approach to derive their data costs ( census transform and gradient - based matching with sgm ) .",
    "these only differ  from us , and from each other  in their approach to spatial aggregation .",
    "the consensus framework outperforms all of these methods on all error metrics , while also having a low execution time .",
    "table  [ tab : res ] also reports the performance of the recently released mc - cnn  @xcite algorithm , which computes point - wise matching costs using a multi - layer convolutional neural network .",
    "this produces lower error values than all other methods , including ours , in exchange for greater computation ( the method takes 100 seconds on a gpu with 2880 cuda cores ) .",
    "this is encouraging , because improved pixel - wise data costs like this one can be directly substituted into the consensus framework to enhance accuracy .",
    "finally , we demonstrate the benefit of the pixel - wise confidence measure in our framework by reporting a second set of results in table  [ tab : res ] .",
    "this is simply produced by discarding a small number of pixels with the lowest degree of consensus @xmath102 ( those with degree less than @xmath104 out of the maximum possible value of @xmath105 ) .",
    "this second set of error quantiles  computed now on the high - confidence set with only @xmath106 fewer pixels  are the smallest of all methods . in this mode",
    ", the proposed algorithm efficiently produces very reliable disparity estimates , at all but a small fraction of locations .",
    "this also suggests a strategy for leveraging sophisticated matching strategies such as @xcite when execution time is a bottleneck ( such as for automated driving applications)one where the more expensive matching costs are computed only for the small number of low - confidence pixels .",
    "in this paper , we introduced a framework for low - level visual estimation with local scene models that reasons with a large overlapping , multi - scale set of regions , to determine which of them are outliers , and which of them can generate model - based scene value estimates while being consistent with each other . despite the larger variable space , and the greater complexity of the consensus objective",
    ", we showed that optimization can be carried out efficiently by recognizing that the regions can be organized hierarchically .",
    "an evaluation on stereo estimation found that the framework outperforms existing approaches to spatial reasoning .    an important direction of future research lies in applying the framework to problems involving estimating different physical properties of the same scene ( such as material and shape ) , with different piecewise local models for each , when the aggregation regions of one property suggest , but do not determine , those of the other .    on a different note , many properties of the consensus framework  multi - scale collaboration , implementation as a distributed architecture of computational units carrying out the same operations , coarse - to - fine evolution of the scene map , mimic behavior observed in biological systems  @xcite . it would be interesting to explore these links systematically  to investigate whether the framework , or some variation of it , can serve as a faithful model for biological processing ; as well as whether insights from biology can be used to further improve the framework .",
    "yx and tz acknowledge support from the nsf under award no.iis-1212928 .    * supplementary material *   +",
    "we present here a summary of the inference algorithm for reference .",
    "it takes as input the following elements :    1 .   sets of regions @xmath107 at @xmath7 different scales .",
    "2 .   for each region @xmath108 , a set @xmath13 consisting of non - overlapping child regions that partition @xmath14 , and are from scales smaller than @xmath10 ( @xmath109 ) .",
    "3 .   the data cost functions @xmath36 and scalar outlier costs @xmath35 for every region @xmath14 .",
    "4 .   the value of the consistency weight @xmath38 . additionally , its value @xmath110 to be used at the beginning of the iterations , the factor @xmath111 by which it is to be increased at every @xmath112 iterations . 5 .",
    "an initial estimate @xmath113 of the scene value map .    given these elements , the algorithm to minimize the cost function @xmath52",
    "is reproduced below :    ' '' ''    _ # initialization _ + set @xmath114 for all @xmath6 .",
    "+ in parallel , * for * all @xmath115 : +  set @xmath116 . + * end ; * + * for * @xmath117 +  in parallel , * for * all @xmath11 : +  set @xmath118 .",
    "+  * end ; * + * end ; * +   + _ # main iterations _ + * for * iters = @xmath119maxiters +  _ # upsweep",
    "_ +  in parallel , * for * all @xmath115 : +  set @xmath120 +  * end ; * +  * for * @xmath117 +  in parallel , * for * all @xmath11 : +  set @xmath121 as per ( 10 )",
    ". +  * end ; * +  * end ; * +   +  _ # minimize _ +  in parallel , * for * all @xmath5 : +  set @xmath122 as per ( 8) and ( 9 ) .",
    "+  * end ; * +   +  _ # downsweep _",
    "+  * for * @xmath123 +  in parallel , * for * all @xmath11 : +  set @xmath124 as per ( 11 ) .",
    "+  * end ; * +  * end ; * +  in parallel , * for * all @xmath6 : +  set @xmath2 as per ( 12 ) . +  * end ; * +   +  _ # update @xmath125 _ +  set @xmath126min@xmath127 * if * mod(iters,@xmath112 ) = 0 . + * end ; * +    ' '' ''    .",
    "and update schedules @xmath128 ( see appendix a ) for @xmath129 . the consensus cost shown is computed with the true value of the consistency weight @xmath38 ( even for the iterations when minimization is done with lower values @xmath129 ) , and the occlusion - based correction step is omitted.,scaledwidth=88.0% ]    as described in sec .  [",
    "sec : body2 ] , to avoid poor local minima and promote convergence to a good solution with a low cost , we use a lower value @xmath129 of the consistency weight in the early iterations of the alternating minimization method , and increase it slowly to the desired weight @xmath38 .",
    "figure [ fig : cevol ] illustrates the effect of different schedules for @xmath129 on convergence for a typical example . in fig .",
    "[ fig : cevol ]  ( a ) , we show the evolution of the objective starting with different values of @xmath130 , and increasing it by a constant factor of @xmath131 at every iteration , and keeping it fixed after it reaches @xmath132 . we see that the direct alternating minimization case ( @xmath133 ) decreases the consensus cost sharply in the first few iterations , but then stagnates at a local minima with a relatively high cost .",
    "as we lower the starting value of @xmath110 , the cost has higher values and decreases more gradually in the initial iterations , but continues to decrease over a larger number of iterations and eventually converges to a better solution with a lower cost .",
    "figure  [ fig : cevol ]  ( b ) explores the effect of a higher rate @xmath134 of increasing @xmath129 .",
    "we see that like with a lower starting value for @xmath110 , a slower rate @xmath134 leads to convergence to a better solution , albeit more gradually .",
    "in addition to requiring more iterations to converge , another computational penalty of changing @xmath129 across iterations is that it requires re - doing any pre - computations that depend on the consistency weight . for our stereo algorithm , minimizing the sum of the data and consistency costs involves solving a @xmath90 linear system for each region , and changing the value of @xmath129 requires re - doing the ldl decompositions of the system matrices .",
    "since this is expensive , it is desirable to avoid changing the value of @xmath129 at every iteration . in fig .",
    "[ fig : cevol ]  ( c ) , we consider different cases with the same value of @xmath110 while jointly setting the increase factor @xmath134 , and the interval @xmath112 at which it is applied , so that the total number of iterations taken for @xmath129 to reach its final value @xmath38 remains the same ( , by applying a higher rate at larger intervals ) .",
    "we see that choosing higher intervals leads to a `` stair - casing '' effect in the evolution of the objective , but the solution it converges to is only worse by a relatively small margin . we find this to be an acceptable trade - off between convergence to a low - cost solution and limiting computational expense , and use the parameters @xmath135 in our stereo implementation ."
  ],
  "abstract_text": [
    "<S> we introduce a multi - scale framework for low - level vision , where the goal is estimating physical scene values from image data  such as depth from stereo image pairs . </S>",
    "<S> the framework uses a dense , overlapping set of image regions at multiple scales and a `` local model , '' such as a slanted - plane model for stereo disparity , that is expected to be valid piecewise across the visual field . </S>",
    "<S> estimation is cast as optimization over a dichotomous mixture of variables , simultaneously determining which regions are inliers with respect to the local model ( binary variables ) and the correct co - ordinates in the local model space for each inlying region ( continuous variables ) . when the regions are organized into a multi - scale hierarchy </S>",
    "<S> , optimization can occur in an efficient and parallel architecture , where distributed computational units iteratively perform calculations and share information through sparse connections between parents and children . </S>",
    "<S> the framework performs well on a standard benchmark for binocular stereo , and it produces a distributional scene representation that is appropriate for combining with higher - level reasoning and other low - level cues .    </S>",
    "<S> = 1 </S>"
  ]
}