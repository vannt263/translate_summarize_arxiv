{
  "article_text": [
    "the expressive power of neural networks is achieved through depth .",
    "there is mounting empirical evidence that for a given budget of resources ( e.g.  neurons ) , the deeper one goes , the better the eventual performance will be .",
    "however , existing theoretical arguments that support this empirical finding are limited .",
    "there have been many attempts to theoretically analyze function spaces generated by network architectures , and their dependency on network depth and size .",
    "the prominent approach for justifying the power of depth is to show that deep networks can efficiently express functions that would require shallow networks to have super - polynomial size .",
    "we refer to such scenarios as instances of _ depth efficiency_. unfortunately , existing results dealing with depth efficiency ( e.g.  @xcite ) typically apply to specific network architectures that do not resemble ones commonly used in practice .",
    "in particular , none of these results apply to convolutional networks ( @xcite ) , which represent the most empirically successful and widely used deep learning architecture to date .",
    "a further limitation of current results is that they merely show _ existence _ of depth efficiency ( i.e.  of functions that are efficiently realizable with a certain depth but can not be efficiently realized with shallower depths ) , without providing any information as to how frequent this property is .",
    "these shortcomings of current theory are the ones that motivated our work .",
    "the architectural features that specialize convolutional networks compared to classic feed - forward fully - connected networks are threefold .",
    "the first feature , _ locality _ , refers to the connection of a neuron only to neighboring neurons in the preceding layer , as opposed to having the entire layer drive it . in the context of image processing ( the most common application of convolutional networks ) ,",
    "locality is believed to reflect the inherent compositional structure of data    the closer pixels are in an image , the more likely they are to be correlated .",
    "the second architectural feature of convolutional networks is _ sharing _ , which means that different neurons in the same layer , connected to different neighborhoods in the preceding layer , share the same weights .",
    "sharing , which together with locality gives rise to convolution , is motivated by the fact that in natural images , the semantic meaning of a pattern often does not depend on its location ( i.e.  two identical patterns appearing in different locations of an image often convey the same semantic content ) .",
    "finally , the third architectural idea of convolutional networks is _ pooling _ , which is essentially an operator that decimates layers , replacing neural activations in a spatial window by a single value ( e.g.  their maximum or average ) . in the context of images , pooling induces invariance to translations ( which often do not affect semantic content ) , and in addition",
    "is believed to create a hierarchy of abstraction in the patterns neurons respond to .",
    "the three architectural elements of locality , sharing and pooling , which have facilitated the great success of convolutional networks , are all lacking in existing theoretical studies of depth efficiency .    in this paper we introduce",
    "a _ convolutional arithmetic circuit _ architecture that incorporates locality , sharing and pooling .",
    "arithmetic circuits ( also known as sum - product networks ,  @xcite ) are networks with two types of nodes : sum nodes , which compute a weighted sum of their inputs , and product nodes , computing the product of their inputs .",
    "we use sum nodes to implement convolutions ( locality with sharing ) , and product nodes to realize pooling .",
    "the models we arrive at may be viewed as convolutional networks with product pooling and linear point - wise activation .",
    "they are attractive on three accounts .",
    "first , as discussed in app .",
    "[ app : simnets ] , convolutional arithmetic circuits are equivalent to simnets , a new deep learning architecture that has recently demonstrated promising empirical results on various image recognition benchmarks  ( @xcite ) .",
    "second , as we show in sec .",
    "[ sec : cac ] , convolutional arithmetic circuits are realizations of hierarchical tensor decompositions ( see  @xcite ) , opening the door to various mathematical and algorithmic tools for their analysis and implementation .",
    "third , the depth efficiency of convolutional arithmetic circuits , which we analyze in sec .",
    "[ sec : theorems ] , was shown in the subsequent work of  @xcite to be superior to the depth efficiency of the popular convolutional rectifier networks , namely convolutional networks with rectified linear ( relu ) activation and max or average pooling .",
    "employing machinery from measure theory and matrix algebra , made available through their connection to hierarchical tensor decompositions , we prove a number of fundamental results concerning the depth efficiency of our convolutional arithmetic circuits . our main theoretical result ( thm .",
    "[ thm : fundamental ] and corollary  [ corollary : fundamental ] ) states that _ besides a negligible ( zero measure ) set , all functions that can be realized by a deep network of polynomial size , require exponential size in order to be realized , or even approximated , by a shallow network_. when translated to the viewpoint of tensor decompositions , this implies that _ almost all _ tensors realized by hierarchical tucker ( ht ) decomposition ( @xcite ) can not be efficiently realized by the classic cp ( rank-1 ) decomposition . to the best of our knowledge",
    ", this result is unknown to the tensor analysis community , in which the advantage of ht over cp is typically demonstrated through _",
    "specific examples _ of tensors that can be efficiently realized by the former and not by the latter . following our main result",
    ", we present a generalization ( thm .",
    "[ thm : generalized ] and corollary  [ corollary : generalized ] ) that compares networks of arbitrary depths , showing that the amount of resources one has to pay in order to maintain representational power while trimming down layers of a network grows double exponentially w.r.t . the number of layers cut off .",
    "we also characterize cases in which dropping a single layer bears an exponential price .",
    "the remainder of the paper is organized as follows . in sec .",
    "[ sec : preliminaries ] we briefly review notations and mathematical background required in order to follow our work . this",
    "is followed by sec .",
    "[ sec : cac ] , which presents our convolutional arithmetic circuits and establishes their equivalence with tensor decompositions .",
    "our theoretical analysis is covered in sec .",
    "[ sec : theorems ] . finally , sec .",
    "[ sec : discussion ] concludes . in order to keep the manuscript at a reasonable length",
    ", we defer our detailed survey of related work to app .",
    "[ app : related_work ] , covering works on the depth efficiency of boolean circuits , arithmetic circuits and neural networks , as well as different applications of tensor analysis in the field of deep learning .",
    "we begin by establishing notational conventions that will be used throughout the paper . we denote vectors using bold typeface , e.g. @xmath0 .",
    "the coordinates of such a vector are referenced with regular typeface and a subscript , e.g. @xmath1 .",
    "this is not to be confused with _",
    "bold _ typeface and a subscript , e.g. @xmath2 , which represents a vector that belongs to some sequence .",
    "tensors ( multi - dimensional arrays ) are denoted by the letters `` a '' and `` b '' in calligraphic typeface , e.g. @xmath3 .",
    "a specific entry in a tensor will be referenced with subscripts , e.g. @xmath4 .",
    "superscripts will be used to denote individual objects within a collection .",
    "for example , @xmath5 stands for vector @xmath6 and @xmath7 stands for tensor @xmath8 . in cases where the collection of interest is indexed by multiple coordinates",
    ", we will have multiple superscripts referencing individual objects , e.g. @xmath9 will stand for vector @xmath10 .",
    "as shorthand for the cartesian product of the euclidean space @xmath11 with itself @xmath12 times , we will use the notation @xmath13 . finally , for a positive integer @xmath14 we use the shorthand @xmath15 $ ] to denote the set @xmath16 .",
    "we now turn to establish a baseline , i.e.  to present basic definitions and results , in the broad and comprehensive field of tensor analysis .",
    "we list here only the essentials required in order to follow the paper , referring the interested reader to  @xcite for a more complete introduction to the field . the most straightforward way to view a tensor is simply as a multi - dimensional array : @xmath17 where @xmath18,d_i\\in[m_i]$ ] . the number of indexing entries in the array , which are also called _ modes _ , is referred to as the _ order _ of the tensor .",
    "the term _ dimension _ stands for the number of values an index can take in a particular mode .",
    "for example , the tensor @xmath19 appearing above has order @xmath12 and dimension @xmath20 in mode @xmath6 , @xmath18 $ ] .",
    "the space of all possible configurations @xmath19 can take is called a _ tensor space _ and is denoted , quite naturally , by @xmath21 .",
    "a central operator in tensor analysis is the _ tensor product _",
    ", denoted @xmath22 . this operator intakes two tensors @xmath19 and @xmath23 of orders @xmath24 and @xmath25 respectively , and returns a tensor @xmath26 of order @xmath27 , defined by : @xmath28 .",
    "notice that in the case @xmath29 , the tensor product reduces to an outer product between vectors .",
    "specifically , @xmath30    the tensor product between @xmath31 and @xmath32 , is no other than the rank-1 matrix @xmath33 . in this context",
    ", we will often use the shorthand @xmath34 to denote the joint tensor product @xmath35 .",
    "tensors of the form @xmath34 are called _ pure _ or _ elementary _ , and are regarded as having _ rank-1 _ ( assuming @xmath36 ) .",
    "it is not difficult to see that any tensor can be expressed as a sum of rank-1 tensors : a= _",
    "z=1^z v^(1)_z v^(n)_z , v_z^(i ) r^m_i [ eq : cp_decomp_def ] a representation as above is called a candecomp / parafac decomposition of @xmath19 , or in short , a _",
    "cp decomposition_. the _ cp - rank _ of @xmath19 is defined as the minimum number of terms in a cp decomposition , i.e. as the minimal @xmath37 for which eq .",
    "[ eq : cp_decomp_def ] can hold .",
    "notice that for a tensor of order 2 , i.e. a matrix , this definition of cp - rank coincides with that of standard matrix rank .",
    "a _ symmetric tensor _ is one that is invariant to permutations of its indices .",
    "formally , a tensor @xmath19 of order @xmath12 which is symmetric will have equal dimension @xmath38 in all modes , and for every permutation @xmath39\\to[n]$ ] and indices @xmath40 $ ] , the following equality will hold : @xmath41 . note that for a vector @xmath42 , the tensor @xmath43 is symmetric .",
    "moreover , every symmetric tensor may be expressed as a linear combination of such ( symmetric rank-1 ) tensors : @xmath44 .",
    "this is referred to as a _",
    "symmetric cp decomposition _ , and the _",
    "symmetric cp - rank _ is the minimal @xmath37 for which such a decomposition exists .",
    "since a symmetric cp decomposition is in particular a standard cp decomposition , the symmetric cp - rank of a symmetric tensor is always greater or equal to its standard cp - rank .",
    "note that for the case of symmetric matrices ( order-@xmath45 tensors ) the symmetric cp - rank and the original cp - rank are always equal .",
    "a repeating concept in this paper is that of _ measure zero_. more broadly , our analysis is framed in measure theoretical terms . while an introduction to the field is beyond the scope of the paper ( the interested reader is referred to  @xcite ) , it is possible to intuitively grasp the ideas that form the basis to our claims .",
    "when dealing with subsets of a euclidean space , the standard and most natural measure in a sense is called the _",
    "lebesgue measure_. this is the only measure we consider in our analysis .",
    "a set of ( lebesgue ) measure zero can be thought of as having zero `` volume '' in the space of interest .",
    "for example , the interval between @xmath46 and @xmath47 has zero measure as a subset of the 2d plane , but has positive measure as a subset of the 1d @xmath48-axis . an alternative way to view a zero measure set @xmath49 follows the property that",
    "if one draws a random point in space by some continuous distribution , the probability of that point hitting @xmath49 is necessarily zero .",
    "a related term that will be used throughout the paper is _ almost everywhere _ , which refers to an entire space excluding , at most , a set of zero measure .",
    "we consider the task of classifying an instance @xmath50 , @xmath51 , into one of the categories @xmath52 .",
    "representing instances as collections of vectors is natural in many applications . in the case of image processing for example",
    ", @xmath53 may correspond to an image , and @xmath54 may correspond to vector arrangements of ( possibly overlapping ) patches around pixels . as customary ,",
    "classification is carried out through maximization of per - label score functions @xmath55 , i.e.  the predicted label for the instance @xmath53 will be the index @xmath56 for which the score value @xmath57 is maximal .",
    "our attention is thus directed to functions over the instance space @xmath58 .",
    "we define our hypotheses space through the following representation of score functions : h_y(x_1,  ,x_n)=_d_1  d_n=1^ma_d_1, ",
    ",d_n^y_i=1^n f__d_i(x_i ) [ eq : score ] @xmath59 are referred to as _ representation functions _ , selected from a parametric family @xmath60 .",
    "natural choices for this family are wavelets , radial basis functions ( _ gaussians _ ) , and affine functions followed by point - wise activation ( _ neurons _ ) .",
    "the _ coefficient tensor _",
    "@xmath7 has order @xmath12 and dimension @xmath38 in each mode .",
    "its entries correspond to a basis of @xmath61 point - wise product functions @xmath62}$ ] .",
    "we will often consider fixed linearly independent representation functions @xmath63 . in this case",
    "the point - wise product functions are linearly independent as well ( see app .",
    "[ app : hypo_space : preliminaries ] ) , and we have a one to one correspondence between score functions and coefficient tensors . to keep the manuscript concise , we defer the derivation of our hypotheses space ( eq .  [ eq : score ] ) to app .",
    "[ app : hypo_space ] , noting here that it arises naturally from the notion of tensor products between @xmath64  spaces .",
    "our eventual aim is to realize score functions @xmath65 with a layered network architecture . as a first step along this path ,",
    "we notice that @xmath66 is fully determined by the activations of the @xmath38 representation functions @xmath63 on the @xmath12 input vectors @xmath67 . in other words ,",
    "given @xmath68,i\\in[n]}$ ] , the score @xmath66 is independent of the input .",
    "it is thus natural to consider the computation of these @xmath69 numbers as the first layer of our networks .",
    "this layer , referred to as the _ representation layer _",
    ", may be conceived as a convolutional operator with @xmath38 channels , each corresponding to a different function applied to all input vectors ( see fig .",
    "[ fig : cp_model ] ) .",
    "once we have constrained our score functions to have the structure depicted in eq .",
    "[ eq : score ] , learning a classifier reduces to estimation of the parameters @xmath70 , and the coefficient tensors @xmath71 .",
    "the computational challenge is that the latter tensors are of order @xmath12 ( and dimension @xmath38 in each mode ) , having an exponential number of entries ( @xmath61 each ) . in the next subsections we utilize tensor decompositions ( factorizations ) to address this computational challenge , and",
    "show how they are naturally realized by convolutional arithmetic circuits .",
    "the most straightforward way to factorize a tensor is through a cp ( rank-1 ) decomposition ( see sec .  [",
    "sec : preliminaries ] ) . consider a joint cp decomposition for the coefficient tensors @xmath72 : a^y = _",
    "z=1^z a_z^y a^z,1 a^z , n [ eq : cp_decomp ] where @xmath73 for @xmath56 ( @xmath74 stands for entry @xmath75 of @xmath76 ) , and @xmath77 for @xmath18,z\\in[z]$ ] .",
    "the decomposition is joint in the sense that the same vectors @xmath78 are shared across all classes @xmath8 .",
    "clearly , if we set @xmath79 this model is universal , i.e. any tensors @xmath71 may be represented .    substituting our cp decomposition ( eq .  [ eq : cp_decomp ] ) into the expression for the score functions in eq .",
    "[ eq : score ] , we obtain : @xmath80 from this we conclude that the network illustrated in fig .",
    "[ fig : cp_model ] implements a classifier ( score functions ) under the cp decomposition in eq .",
    "[ eq : cp_decomp ] .",
    "we refer to this network as _",
    "cp model_. the network consists of a representation layer followed by a single hidden layer , which in turn is followed by the output .",
    "the hidden layer begins with a _",
    "@xmath81 conv _ operator , which is simply a 3d convolution with @xmath37 channels and receptive field @xmath81 .",
    "the convolution may operate without coefficient sharing , i.e.  the filters that generate feature maps by sliding across the previous layer may have different coefficients at different spatial locations .",
    "this is often referred to in the deep learning community as a locally - connected operator ( see  @xcite ) . to obtain a standard convolutional operator , simply enforce coefficient sharing by constraining the vectors @xmath78 in the cp decomposition ( eq .  [ eq : cp_decomp ] ) to be equal to each other for different values of  @xmath6 ( this setting is discussed in sec .",
    "[ sec : shared ] ) .",
    "following conv operator , the hidden layer includes global product pooling .",
    "feature maps generated by conv are reduced to singletons through multiplication of their entries , creating a vector of dimension @xmath37 .",
    "this vector is then mapped into the @xmath82 network outputs through a final dense linear layer .    to recap , cp model ( fig .",
    "[ fig : cp_model ] ) is a shallow ( single hidden layer ) convolutional arithmetic circuit that realizes the cp decomposition ( eq .",
    "[ eq : cp_decomp ] ) .",
    "it is universal , i.e.  it can realize any coefficient tensors with large enough size ( @xmath37 ) .",
    "unfortunately , since the cp - rank of a generic tensor is exponential in its order ( see  @xcite ) , the size required for cp model to be universal is exponential ( @xmath37 exponential in @xmath12 ) .          in this subsection",
    "we present a deep network that corresponds to the recently introduced hierarchical tucker tensor decomposition ( @xcite ) , which we refer to in short as _",
    "ht decomposition_. the network , dubbed _",
    ", is universal .",
    "specifically , any set of tensors @xmath7 represented by cp model can be represented by ht model with only a polynomial penalty in terms of resources . the advantage of ht model , as we show in sec .",
    "[ sec : theorems ] , is that in almost all cases it generates tensors that require an exponential size in order to be realized , or even approximated , by cp model .",
    "put differently , if one draws the weights of ht model by some continuous distribution , with probability one , the resulting tensors can not be approximated by a polynomial cp model .",
    "informally , this implies that ht model is exponentially more expressive than cp model .",
    "ht model is based on the hierarchical tensor decomposition in eq .",
    "[ eq : ht_decomp ] , which is a special case of the ht decomposition as presented in  @xcite ( in the latter s terminology , we restrict the matrices @xmath83 to be diagonal ) .",
    "our construction and theoretical results apply to the general ht decomposition as well , with the specialization done merely to bring forth a network that resembles current convolutional networks . to be diagonal , pooling operations would involve entries from different channels . ]",
    "^1,j , & = & _ = 1^r_0 a_^1,j , a^0,2j-1 , a^0,2j , + & & + ^l , j , & = & _ = 1^r_l-1 a_^l , j , _ _ + & & + ^l-1,j , & = & _ = 1^r_l-2 a_^l-1,j , _ _ + a^y & = &",
    "_ = 1^r_l-1 a_^l , y _ _ [ eq : ht_decomp ]    the decomposition in eq .",
    "[ eq : ht_decomp ] recursively constructs the coefficient tensors @xmath84}$ ] by assembling vectors @xmath85,\\gamma\\in[r_0]}$ ] into tensors @xmath86,j\\in[n/2^l],\\gamma\\in[r_l]}$ ] in an incremental fashion .",
    "the index @xmath87 stands for the level in the decomposition , @xmath88 represents the `` location '' within level @xmath87 , and @xmath89 corresponds to the individual tensor in level @xmath87 and location @xmath88 .",
    "@xmath90 is referred to as _ level-@xmath87 rank _ , and is defined to be the number of tensors in each location of level @xmath87 ( we denote for completeness @xmath91 ) .",
    "the tensor @xmath92 has order @xmath93 , and we assume for simplicity that @xmath12    the order of @xmath7 , is a power of @xmath45 ( this is merely a technical assumption also made in  @xcite , it does not limit the generality of our analysis ) .",
    "the parameters of the decomposition are the final level weights @xmath94}$ ] , the intermediate levels weights @xmath95,j\\in[n/2^l],\\gamma\\in[r_l]}$ ] , and the first level vectors @xmath96,\\gamma\\in[r_0]}$ ] .",
    "this totals at @xmath97 individual parameters , and if we assume equal ranks @xmath98 , the number of parameters becomes @xmath99 .",
    "the hierarchical decomposition ( eq .  [ eq : ht_decomp ] ) is universal , i.e.  with large enough ranks @xmath90 it can represent any tensors . moreover , it is a super - set of the cp decomposition ( eq .  [ eq : cp_decomp ] ) .",
    "that is to say , all tensors representable by a cp decomposition having @xmath37 components are also representable by a hierarchical decomposition with ranks @xmath100 . with cp",
    "s basis vectors , the last level weights with cp s per - class weights , and the intermediate levels weights with indicator vectors . ]",
    "note that this comes with a polynomial penalty    the number of parameters increases from @xmath101 in the cp decomposition , to @xmath102 in the hierarchical decomposition .",
    "however , as we show in sec .",
    "[ sec : theorems ] , the gain in expressive power is exponential .    plugging the expression for @xmath7 in our hierarchical decomposition ( eq .",
    "[ eq : ht_decomp ] ) into the score function @xmath65 given in eq .",
    "[ eq : score ] , we obtain the network displayed in fig .",
    "[ fig : ht_model ]    ht model .",
    "this network includes a representation layer followed by @xmath103 hidden layers which in turn are followed by the output . as in the shallow cp model ( fig .",
    "[ fig : cp_model ] ) , the hidden layers consist of @xmath81 conv operators followed by product pooling .",
    "the difference is that instead of a single hidden layer collapsing the entire spatial structure through global pooling , hidden layers now pool over size-@xmath45 windows , decimating feature maps by a factor of two ( no overlaps ) .",
    "after @xmath103 such layers feature maps are reduced to singletons , and we arrive at a 1d structure with @xmath104 nodes .",
    "this is then mapped into @xmath82 network outputs through a final dense linear layer .",
    "we note that the network s size-@xmath45 pooling windows ( and the resulting number of hidden layers @xmath103 ) correspond to the fact that our hierarchical decomposition ( eq .  [ eq : ht_decomp ] ) is based on a full binary tree over modes , i.e.  it combines ( through tensor product ) two tensors at a time .",
    "we focus on this setting solely for simplicity of presentation , and since it is the one presented in  @xcite . our analysis ( sec .",
    "[ sec : theorems ] ) could easily be adapted to hierarchical decompositions based on other trees ( taking tensor products between more than two tensors at a time ) , and that would correspond to networks with different pooling window sizes and resulting depths .",
    "ht model ( fig .",
    "[ fig : ht_model ] ) is conceptually divided into two parts .",
    "the first is the representation layer , transforming input vectors @xmath67 into @xmath105 real - valued scalars @xmath106,d\\in[m]}$ ] .",
    "the second and main part of the network , which we view as an `` inference '' engine , is the convolutional arithmetic circuit that takes the @xmath105 measurements produced by the representation layer , and accordingly computes @xmath82 class scores at the output layer .    to recap",
    ", we have now a deep network ( fig .  [ fig : ht_model ] ) , which we refer to as ht model , that computes the score functions @xmath65 ( eq .  [ eq : score ] ) with coefficient tensors @xmath7 hierarchically decomposed as in eq .",
    "[ eq : ht_decomp ] .",
    "the network is universal in the sense that with enough channels @xmath90 , any tensors may be represented .",
    "moreover , the model is a super - set of the shallow cp model presented in sec .  [",
    "sec : cp_model ] .",
    "the question of depth efficiency now naturally arises .",
    "in particular , we would like to know if there are functions that may be represented by a polynomially sized deep ht model , yet require exponential size from the shallow cp model .",
    "the answer , as described in sec .",
    "[ sec : theorems ] , is that almost all functions realizable by ht model meet this property . in other words ,",
    "the set of functions realizable by a polynomial cp model has _ measure zero _ in the space of functions realizable by a given polynomial ht model .      the @xmath81 conv operator in our networks ( see fig .  [",
    "fig : cp_model ] and  [ fig : ht_model ] ) implements a local linear transformation with coefficients generally being location - dependent . in the special case where coefficients do not depend on location ,",
    "i.e. remain fixed across space , the local linear transformation becomes a standard convolution .",
    "we refer to this setting as coefficient _",
    "sharing_. sharing is a widely used structural constraint , one of the pillars behind the successful convolutional network architecture . in the context of image processing ( prominent application of convolutional networks )",
    ", sharing is motivated by the observation that in natural images , the semantic content of a pattern often does not depend on its location . in this subsection",
    "we explore the effect of sharing on the expressiveness of our networks , or more specifically , on the coefficient tensors @xmath7 they can represent .    for cp model ,",
    "coefficient sharing amounts to setting @xmath107 in the cp decomposition ( eq .  [ eq : cp_decomp ] ) , transforming the latter to a symmetric cp decomposition : @xmath108 cp model with sharing is not universal ( not all tensors @xmath7 are representable , no matter how large @xmath37 is allowed to be )    it can only represent symmetric tensors .    in the case of ht model , sharing amounts to applying the following constraints on the hierarchical decomposition in eq .",
    "[ eq : ht_decomp ] : @xmath109 for every @xmath110 and @xmath111 .",
    "note that in this case universality is lost as well , but nonetheless generated tensors are not limited to be symmetric , already demonstrating an expressive advantage of deep models over shallow ones . in sec .",
    "[ sec : theorems ] we take this further by showing that the shared ht model is exponentially more expressive than cp model , even if the latter is not constrained by sharing .",
    "the first contribution of this paper , presented in sec .",
    "[ sec : cac ] , is the equivalence between deep learning architectures successfully employed in practice , and tensor decompositions .",
    "namely , we showed that convolutional arithmetic circuits as in fig .",
    "[ fig : ht_model ] , which are in fact simnets that have demonstrated promising empirical performance ( see app .",
    "[ app : simnets ] ) , may be formulated as hierarchical tensor decompositions . as a second contribution",
    ", we make use of the established link between arithmetic circuits and tensor decompositions , combining theoretical tools from these two worlds , to prove results that are of interest to both deep learning and tensor analysis communities .",
    "this is the focus of the current section .",
    "the fundamental theoretical result proven in this paper is the following :    [ thm : fundamental ]",
    "let @xmath7 be a tensor of order @xmath12 and dimension @xmath38 in each mode , generated by the recursive formulas in eq .",
    "[ eq : ht_decomp ] .",
    "define @xmath112 , and consider the space of all possible configurations for the parameters of the composition    @xmath113 . in this space",
    ", the generated tensor @xmath7 will have cp - rank of at least @xmath114 almost everywhere ( w.r.t .",
    "lebesgue measure ) . put differently , the configurations for which the cp - rank of @xmath7 is less than @xmath114 form a set of measure zero .",
    "the exact same result holds if we constrain the composition to be `` shared '' , i.e. set @xmath115 and consider the space of @xmath116 configurations .    from the perspective of deep learning , thm .",
    "[ thm : fundamental ] leads to the following corollary :    [ corollary : fundamental ] given linearly independent representation functions @xmath117}$ ] , randomizing the weights of ht model ( sec .",
    "[ sec : ht_model ] ) by a continuous distribution induces score functions @xmath65 that with probability one , can not be approximated arbitrarily well ( in @xmath64 sense ) by a cp model ( sec .",
    "[ sec : cp_model ] ) with less than @xmath118 hidden channels .",
    "this result holds even if we constrain ht model with weight sharing ( sec .",
    "[ sec : shared ] ) while leaving cp model in its general form .",
    "that is to say , _ besides a negligible set , all functions that can be realized by a polynomially sized ht model ( with or without weight sharing ) , require exponential size in order to be realized , or even approximated , by cp model_. most of the previous works relating to depth efficiency ( see app .",
    "[ app : related_work ] ) merely show _ existence _ of functions that separate depths ( i.e.  that are efficiently realizable by a deep network yet require super - polynomial size from shallow networks ) .",
    "corollary  [ corollary : fundamental ] on the other hand establishes depth efficiency for _ almost all _ functions that a deep network can implement .",
    "equally importantly , it applies to deep learning architectures that are being successfully employed in practice ( simnets  ",
    "see app .",
    "[ app : simnets ] ) .    adopting the viewpoint of tensor analysis , thm .",
    "[ thm : fundamental ] states that besides a negligible set , all tensors realized by ht ( hierarchical tucker ) decomposition can not be represented by the classic cp ( rank-1 ) decomposition if the latter has less than an exponential number of terms . , the decomposition in eq .",
    "[ eq : ht_decomp ] to which thm .",
    "[ thm : fundamental ] applies is actually a special case of ht decomposition as introduced in  @xcite .",
    "however , the theorem and its proof can easily be adapted to account for the general case .",
    "we focus on the special case merely because it corresponds to convolutional arithmetic circuit architectures used in practice . ] to the best of our knowledge , this result has never been proved in the tensor analysis community . in the original paper introducing ht decomposition  ( @xcite ) , as a motivating example , the authors present a specific tensor that is efficiently realizable by ht decomposition while requiring an exponential number of terms from cp decomposition .",
    "our result strengthens this motivation considerably , showing that it is not just one specific tensor that favors ht over cp , but rather , almost all tensors realizable by ht exhibit this preference . taking into account that any tensor realized by cp",
    "can also be realized by ht with only a polynomial penalty in the number of parameters ( see sec .",
    "[ sec : ht_model ] ) , this implies that in an asymptotic sense , ht decomposition is exponentially more efficient than cp decomposition .      the complete proofs of thm .",
    "[ thm : fundamental ] and corollary  [ corollary : fundamental ] are given in app .",
    "[ app : proofs ] .",
    "we provide here an outline of the main tools employed and arguments made along these proofs .    to prove thm .",
    "[ thm : fundamental ] we combine approaches from the worlds of circuit complexity and tensor decompositions .",
    "the first class of machinery we employ is _ matrix algebra _ , which has proven to be a powerful source of tools for analyzing the complexity of circuits .",
    "for example , arithmetic circuits have been analyzed through what is called the partial derivative matrix ( see  @xcite ) , and for boolean circuits a widely used tool is the communication matrix ( see  @xcite ) .",
    "we gain access to matrix algebra by arranging tensors that take part in the cp and ht decompositions as matrices , a process often referred to as _",
    "matricization_. with matricization , the tensor product translates to the kronecker product , and the properties of the latter become readily available .",
    "the second tool - set we make use of is _ measure theory _ , which prevails in the study of tensor decompositions , but is much less frequent in analyses of circuit complexity . in order to frame a problem in measure theoretical terms ,",
    "one obviously needs to define a measure space of interest . for tensor decompositions ,",
    "the straightforward space to focus on is that of the decomposition variables . for general circuits on the other hand ,",
    "it is often unclear if defining a measure space is at all appropriate .",
    "however , when circuits are considered in the context of machine learning they are usually parameterized , and defining a measure space on top of these parameters is an effective approach for studying the prevalence of various properties in hypotheses spaces .",
    "our proof of thm .",
    "[ thm : fundamental ] traverses through the following path .",
    "we begin by showing that matricizing a rank-1 tensor produces a rank-1 matrix .",
    "this implies that the matricization of a tensor generated by a cp decomposition with @xmath37 terms has rank at most @xmath37 .",
    "we then turn to show that the matricization of a tensor generated by the ht decomposition in eq .",
    "[ eq : ht_decomp ] has rank at least @xmath119 almost everywhere .",
    "this is done through induction over the levels of the decomposition ( @xmath120 ) .",
    "for the first level ( @xmath121 ) , we use a combination of measure theoretical and linear algebraic arguments to show that the generated matrices have maximal rank ( @xmath122 ) almost everywhere . for the induction step , the facts that under matricization tensor product translates into kronecker product , and that the latter increases ranks multiplicatively , denotes the kronecker product , then for any matrices @xmath123 and @xmath124 : @xmath125 . ] imply that matricization ranks in the current level are generally equal to those in the previous level squared .",
    "measure theoretical claims are then made to ensure that this indeed takes place almost everywhere .    to prove corollary  [ corollary : fundamental ] based on thm .",
    "[ thm : fundamental ] , we need to show that the inability of cp model to realize a tensor generated by ht model , implies that the former can not approximate score functions produced by the latter . in general , the set of tensors expressible by a cp decomposition is not topologically closed , which implies that a - priori , it may be that cp model can approximate tensors generated by ht model even though it can not realize them .",
    "however , since the proof of thm .",
    "[ thm : fundamental ] was achieved through separation of matrix rank , distances are indeed positive and cp model can not approximate ht model s tensors almost always . to translate from tensors to score functions , we simply note that in a finite - dimensional hilbert space convergence in norm implies convergence in coefficients under any basis .",
    "therefore , in the space of score functions ( eq .  [",
    "eq : score ] ) convergence in norm implies convergence in coefficients under the basis @xmath126}$ ] .",
    "that is to say , it implies convergence in coefficient tensors .",
    "[ thm : fundamental ] and corollary  [ corollary : fundamental ] compare the expressive power of the deep ht model ( sec .  [ sec : ht_model ] ) to that of the shallow cp model ( sec .",
    "[ sec : cp_model ] ) .",
    "one may argue that such an analysis is lacking , as it does not convey information regarding the importance of each individual layer .",
    "in particular , it does not shed light on the advantage of very deep networks , which at present provide state of the art recognition accuracy , compared to networks of more moderate depth . for this purpose",
    "we present a generalization , specifying the amount of resources one has to pay in order to maintain representational power while layers are incrementally cut off from a deep network .",
    "for conciseness we defer this analysis to app .",
    "[ app : general_thm ] , and merely state here our final conclusions .",
    "we find that the representational penalty is double exponential w.r.t .",
    "the number of layers removed .",
    "in addition , there are certain cases where the removal of even a single layer leads to an exponential inflation , falling in line with the suggestion of  @xcite .",
    "in this work we address a fundamental issue in deep learning    the expressive efficiency of depth .",
    "there have been many attempts to theoretically analyze this question , but from a practical machine learning perspective , existing results are limited .",
    "most of the results apply to very specific types of networks that do not resemble ones used in practice , and none of the results account for the locality - sharing - pooling paradigm which forms the basis for convolutional networks    the most successful deep learning architecture to date .",
    "in addition , current analyses merely show _ existence _ of depth efficiency , i.e. of functions that are efficiently realizable by deep networks but not by shallow ones .",
    "the practical implications of such findings are arguably slight , as a - priori , it may be that only a small fraction of the functions realizable by deep networks enjoy depth efficiency , and for all the rest shallow networks suffice .",
    "our aim in this paper was to develop a theory that facilitates an analysis of depth efficiency for networks that incorporate the widely used structural ingredients of locality , sharing and pooling .",
    "we consider the task of classification into one of a finite set of categories @xmath127 .",
    "our instance space is defined to be the cartesian product of @xmath12 vector spaces , in compliance with the common practice of representing natural data through ordered local structures ( e.g. images through patches ) .",
    "each of the @xmath12 vectors that compose an instance is represented by a descriptor of length @xmath38 , generated by running the vector through @xmath38 `` representation '' functions .",
    "as customary , classification is achieved through maximization of score functions @xmath65 , one for every category @xmath56 .",
    "each score function is a linear combination over the @xmath61 possible products that may be formed by taking one descriptor entry from every input vector .",
    "the coefficients for these linear combinations conveniently reside in tensors @xmath7 of order @xmath12 and dimension @xmath38 along each axis .",
    "we construct networks that compute score functions @xmath65 by decomposing ( factorizing ) the coefficient tensors @xmath7 .",
    "the resulting networks are convolutional arithmetic circuits that incorporate locality , sharing and pooling , and operate on the @xmath105 descriptor entries generated from the input .",
    "we show that a shallow ( single hidden layer ) network realizes the classic cp ( rank-1 ) tensor decomposition , whereas a deep network with @xmath128 hidden layers realizes the recently introduced hierarchical tucker ( ht ) decomposition  ( @xcite ) .",
    "our fundamental result , presented in thm .  [ thm : fundamental ] and corollary  [ corollary : fundamental ] , states that randomizing the weights of a deep network by some continuous distribution will lead , _ with probability one _ , to score functions that can not be approximated by a shallow network if the latter s size is not exponential ( in @xmath12 ) .",
    "we extend this result ( thm .",
    "[ thm : generalized ] and corollary  [ corollary : generalized ] ) by deriving analogous claims that compare two networks of any depths , not just deep vs. shallow .    to further highlight the connection between our networks and ones used in practice ,",
    "we show ( app .  [ app : simnets ] ) that translating convolution and product pooling computations to log - space ( for numerical stability ) gives rise to simnets    a recently proposed deep learning architecture which has been shown to produce state of the art accuracy in computationally limited settings  ( @xcite ) .",
    "besides the central line of our work discussed above , the construction and theory presented in this paper shed light on various conjectures and practices employed by the deep learning community .",
    "first , with respect to the _ pooling _ operation , our analysis points to the possibility that perhaps it has more to do with factorization of computed functions than it does with translation invariance .",
    "this may serve as an explanation for the fact that pooling windows in state of the art convolutional networks are typically very small ( see for example  @xcite ) , often much smaller than the radius of translation one would like to be invariant to . indeed , in our framework , as we show in app .",
    "[ app : general_thm ] , pooling over large windows and trimming down a network s depth may bring to an exponential decrease in expressive efficiency .",
    "the second point our theory sheds light on is _ sharing_. as discussed in sec .",
    "[ sec : shared ] , introducing weight sharing to a shallow network ( cp model ) considerably limits its expressive power .",
    "the network can only represent symmetric tensors , which in turn means that it is location invariant w.r.t .",
    "input vectors ( patches ) . in the case of a deep network ( ht model )",
    "the limitation posed by sharing is not as strict .",
    "generated tensors need not be symmetric , implying that the network is capable of modeling location    a crucial ability in almost any real - world task .",
    "the above findings suggest that the sharing constraint is increasingly limiting as a network gets shallower , to the point where it causes complete ignorance to location .",
    "this could serve as an argument supporting the empirical success of deep convolutional networks    they bind together the statistical and computational advantages of sharing with many layers that mitigate its expressive limitations .",
    "lastly , our construction advocates _ locality _ , or more specifically , @xmath81 receptive fields .",
    "recent convolutional networks providing state of the art recognition performance ( e.g.  @xcite ) make extensive use of @xmath81 linear transformations , proving them to be very successful in practice . in view of our model ,",
    "such @xmath81 operators factorize tensors while providing universality with a minimal number of parameters .",
    "it seems reasonable to conjecture that for this task of factorizing coefficient tensors , larger receptive fields are not significantly helpful , as they lead to redundancy which may deteriorate performance in presence of limited training data .",
    "investigation of this conjecture is left for future work .",
    "amnon shashua would like to thank tomaso poggio and shai s. shwartz for illuminating discussions during the preparation of this manuscript .",
    "we would also like to thank tomer galanti , tamir hazan and lior wolf for commenting on draft versions of the paper .",
    "the work is partly funded by intel grant icri - ci no . 9 - 2012 - 6133 and by isf center grant 1790/12 .",
    "nadav cohen is supported by a google fellowship in machine learning .",
    "in sec .  [ sec : theorems ] we presented our fundamental theorem of network capacity ( thm .",
    "[ thm : fundamental ] and corollary  [ corollary : fundamental ] ) , showing that besides a negligible set , all functions that can be realized by a polynomially sized ht model ( with or without weight sharing ) , require exponential size in order to be realized , or even approximated , by cp model . in terms of network depth , cp and ht models represent the extremes    the former has only a single hidden layer achieved through global pooling , whereas the latter has @xmath103 hidden layers achieved through minimal ( size-@xmath45 ) pooling windows .",
    "it is of interest to generalize the fundamental result by establishing a comparison between networks of intermediate depths .",
    "this is the focus of the current appendix .",
    "we begin by defining a truncated version of the hierarchical tensor decomposition presented in eq .",
    "[ eq : ht_decomp ] : ^1,j , & = & _ = 1^r_0 a_^1,j , a^0,2j-1 , a^0,2j , + & & + ^l , j , & = & _ = 1^r_l-1 a_^l , j , _ _ + & & + a&= & _",
    "= 1^r_l_c - 1 a^l_c _ _ j=1 ^ 2^l - l_c+1 _ [ eq : ht_decomp_trunc ] the only difference between this decomposition and the original is that instead of completing the full process with @xmath129 levels , we stop after @xmath130 . at this point",
    "remaining tensors are binded together to form the final order-@xmath12 tensor .",
    "the corresponding network will simply include a premature global pooling stage that shrinks feature maps to @xmath81 , and then a final linear layer that performs classification .",
    "as before , we consider a shared version of the decomposition in which @xmath115 .",
    "notice that this construction realizes a continuum between cp and ht models , which correspond to the extreme cases @xmath131 and @xmath132 respectively .    the following theorem , a generalization of thm .",
    "[ thm : fundamental ] , compares a truncated decomposition having @xmath133 levels , to one with @xmath134 levels that implements the same tensor , quantifying the penalty in terms of parameters :    [ thm : generalized ] let @xmath135 and @xmath136 be tensors of order @xmath12 and dimension @xmath38 in each mode , generated by the truncated recursive formulas in eq .",
    "[ eq : ht_decomp_trunc ] , with @xmath133 and @xmath137 levels respectively .",
    "denote by @xmath138 and @xmath139 the composition ranks of @xmath135 and @xmath136 respectively .",
    "assuming w.l.o.g .",
    "that @xmath140 , we define @xmath141 , and consider the space of all possible configurations for the parameters of @xmath135 s composition    @xmath142 . in this space , almost everywhere ( w.r.t . lebesgue measure ) , the generated tensor @xmath135 requires that @xmath143 if one wishes that @xmath136 be equal to @xmath135 . put differently , the configurations for which @xmath135 can be realized by @xmath136 with @xmath144 form a set of measure zero .",
    "the exact same result holds if we constrain the composition of @xmath135 to be `` shared '' , i.e. set @xmath145 and consider the space of @xmath146 configurations .    in analogy with corollary  [ corollary : fundamental ] , we obtain the following generalization :    [ corollary : generalized ] suppose we are given linearly independent representation functions @xmath63 , and consider two networks that correspond to the truncated hierarchical tensor decomposition in eq .",
    "[ eq : ht_decomp_trunc ] , with @xmath133 and @xmath137 hidden layers respectively .",
    "assume w.l.o.g . that @xmath147 , i.e. that network  1 is deeper than network  2 , and define @xmath148 to be the minimal number of channels across the representation layer and the first @xmath137 hidden layers of network  1 .",
    "then , if we randomize the weights of network  1 by a continuous distribution , we obtain , with probability one , score functions @xmath65 that can not be approximated arbitrarily well ( in @xmath64 sense ) by network  2 if the latter has less than @xmath149 channels in its last hidden layer .",
    "the result holds even if we constrain network  1 with weight sharing while leaving network  2 in its general form .",
    "proofs of thm .",
    "[ thm : generalized ] and corollary  [ corollary : generalized ] are given in app .",
    "[ app : proofs ] .",
    "hereafter , we briefly discuss some of their implications . first",
    ", notice that we indeed obtain a generalization of the fundamental theorem of network capacity ( thm .",
    "[ thm : fundamental ] and corollary  [ corollary : fundamental ] ) , which corresponds to the extreme case @xmath150 and @xmath151 .",
    "second , note that for the baseline case of @xmath150 , i.e. a full - depth network has generated the target score function , approximating this with a truncated network draws a price that grows _",
    "double exponentially _ w.r.t . the number of missing layers .",
    "third , and most intriguingly , we see that when @xmath133 is considerably smaller than @xmath152 , i.e. when a significantly truncated network is sufficient to model our problem , cutting off even a single layer leads to an exponential price , and this price is _ independent _ of @xmath133 .",
    "such scenarios of exponential penalty for trimming down a single layer were discussed in  @xcite , but only in the context of specific functions realized by networks that do not resemble ones used in practice ( see  @xcite for an example of such result ) .",
    "we prove this in a much broader , more practical setting , showing that for convolutional arithmetic circuit ( simnet  ",
    "see app .",
    "[ app : simnets ] ) architectures , almost any function realized by a significantly truncated network will exhibit this behavior .",
    "the issue relates to empirical practice , supporting the common methodology of designing networks that go as deep as possible .",
    "specifically , it encourages extending network depth by pooling over small regions , avoiding significant spatial decimation that brings network termination closer .",
    "we conclude this appendix by stressing once more that our construction and theoretical approach are not limited to the models covered by our theorems ( cp model , ht model , truncated ht model ) .",
    "these are merely exemplars deemed most appropriate for initial analysis .",
    "the fundamental and generalized theorems of network capacity are similar in spirit , and analogous theorems for networks with different pooling window sizes and depths ( corresponding to different tensor decompositions ) may easily be derived .",
    "our proof of thm .",
    "[ thm : fundamental ] and  [ thm : generalized ] relies on basic knowledge in measure theory , or more specifically , lebesgue measure spaces .",
    "we do not provide here a comprehensive background on this field ( the interested reader is referred to  @xcite ) , but rather supplement the brief discussion given in sec .  [",
    "sec : preliminaries ] , with a list of facts we will be using which are not necessarily intuitive :    * a union of countably ( or finitely ) many sets of zero measure is itself a set of zero measure . *",
    "if @xmath153 is a polynomial over @xmath154 variables that is not identically zero , the set of points in @xmath155 in which it vanishes has zero measure ( see  @xcite for a short proof of this ) . *",
    "if @xmath156 has zero measure , then @xmath157 , and every set contained within , have zero measure as well .    in the above , and in the entirety of this paper ,",
    "the only measure spaces we consider are euclidean spaces equipped with lebesgue measure .",
    "thus when we say that a set of @xmath154-dimensional points has zero measure , we mean that its lebesgue measure in the @xmath154-dimensional euclidean space is zero .    moving on to some preliminaries from matrix and tensor theory , we denote by @xmath158 $ ] the _ matricization _ of an order-@xmath12 tensor @xmath19 ( for simplicity , @xmath12 is assumed to be even ) , where rows correspond to odd modes and columns correspond to even modes .",
    "namely , if @xmath159 , the matrix @xmath158 $ ] has @xmath160 rows and @xmath161 columns , rearranging the entries of the tensor such that @xmath162 is stored in row index @xmath163 and column index @xmath164 . to distinguish from the tensor product operation @xmath22 , we denote the kronecker product between matrices by @xmath165 . specifically , for two matrices @xmath166 and @xmath167 , @xmath168 is the matrix in @xmath169 that holds @xmath170 in row index @xmath171 and column index @xmath172 .",
    "the basic relation that binds together tensor product , matricization and kronecker product is @xmath173=[{{\\mathcal a}}]\\odot[{{\\mathcal b}}]$ ] , where @xmath19 and @xmath23 are tensors of even orders .",
    "two additional facts we will make use of are that the matricization is a linear operator ( i.e. for scalars @xmath174 and tensors with the same size @xmath175 : @xmath176=\\sum_{i=1}^r\\alpha_i[{{\\mathcal a}}_i]$ ] ) , and less trivially , that for any matrices @xmath123 and @xmath124 , the rank of @xmath168 is equal to @xmath177 ( see  @xcite for a proof ) .",
    "these two facts , along with the basic relation laid out above , lead to the conclusion that : @xmath178=\\prod_{i=1}^{\\nicefrac{2^l}{2 } } rank\\overbrace{\\left[{{\\mathbf v}}^{(z)}_{2i-1}\\otimes{{\\mathbf v}}^{(z)}_{2i}\\right]}^{{{\\mathbf v}}^{(z)}_{2i-1}{{\\mathbf v}}^{(z)\\top}_{2i}}=1\\ ] ] and thus : @xmath179   = rank\\sum_{z=1}^z\\lambda_z\\left[{{\\mathbf v}}^{(z)}_1\\otimes\\cdots\\otimes{{\\mathbf v}}^{(z)}_{2^l}\\right ]   \\leq \\sum_{z=1}^z rank\\left[{{\\mathbf v}}^{(z)}_1\\otimes\\cdots\\otimes{{\\mathbf v}}^{(z)}_{2^l}\\right]=z\\ ] ] in words , an order-@xmath180 tensor given by a cp - decomposition ( see sec .  [",
    "sec : preliminaries ] ) with @xmath37 terms , has matricization with rank at most @xmath37 .",
    "thus , _ to prove that a certain order-@xmath180 tensor has cp - rank of at least @xmath181 , it suffices to show that its matricization has rank of at least @xmath181_.    we now state and prove two lemmas that will be needed for our proofs of thm .",
    "[ thm : fundamental ] and  [ thm : generalized ] .",
    "[ lemma : base ] let @xmath182 , and define the following mapping taking @xmath183 to three matrices : @xmath184 , @xmath185 and @xmath186 .",
    "@xmath187 simply holds the first @xmath188 elements of @xmath189 , @xmath190 holds the following @xmath188 elements of @xmath189 , and @xmath191 is a diagonal matrix that holds the last @xmath12 elements of @xmath189 on its diagonal .",
    "define the product matrix @xmath192 , and consider the set of points @xmath189 for which the rank of @xmath193 is different from @xmath194 .",
    "this set of points has zero measure .",
    "the result will also hold if the points @xmath189 reside in @xmath195 , and the same elements are used to assign @xmath187 and @xmath190 ( @xmath196 ) .",
    "obviously @xmath197 for all @xmath189 , so it remains to show that @xmath198 for all @xmath189 but a set of zero measure .",
    "let @xmath199 be the top - left @xmath200 sub - matrix of @xmath193 .",
    "if @xmath199 is non - singular then of course @xmath198 as required .",
    "it thus suffices to show that the set of points @xmath189 for which @xmath201 has zero measure .",
    "now , @xmath202 is a polynomial in the entries of @xmath189 , and so it either vanishes on a set of zero measure , or it is the zero polynomial ( see  @xcite ) .",
    "all that is left is to disqualify the latter option , and that can be done by finding a specific point @xmath203 for which @xmath204 . indeed",
    ", we may choose @xmath203 such that @xmath205 is the identity matrix and @xmath206 hold @xmath207 on their main diagonal and @xmath208 otherwise .",
    "this selection implies that @xmath209 is the identity matrix , and in particular @xmath204 .",
    "[ lemma : step ] assume we have @xmath153 continuous mappings from @xmath155 to @xmath210 taking the point @xmath211 to the matrices @xmath212 .",
    "assume that under these mappings , the points @xmath211 for which every @xmath213 $ ] satisfies @xmath214 form a set of zero measure .",
    "define a mapping from @xmath215 to @xmath216 given by @xmath217 .",
    "then , the points @xmath218 for which @xmath219 form a set of zero measure .",
    "denote @xmath220 .",
    "we would like to show that this set has zero measure .",
    "we first note that since @xmath221 is a continuous mapping , and the set of matrices @xmath222 which have rank less than @xmath148 is closed , @xmath49 is a closed set and in particular measurable .",
    "our strategy for computing its measure will be as follows .",
    "for every @xmath223 we define the marginal set @xmath224 .",
    "we will show that for every @xmath211 but a set of zero measure , the measure of @xmath225 is zero .",
    "an application of fubini s theorem will then prove the desired result .",
    "let @xmath226 be the set of points @xmath223 for which @xmath227:rank(a_i({{\\mathbf y } } ) ) < r$ ] . by assumption",
    ", @xmath226 has zero measure .",
    "we now show that for @xmath228 , the measure of @xmath229 is zero . by the definition of @xmath226 there exists",
    "an @xmath213 $ ] such that @xmath230 .",
    ", we assume that @xmath231 , and that the top - left @xmath200 sub - matrix of @xmath232 is non - singular . regarding @xmath233 as fixed , the determinant of the top - left @xmath200 sub - matrix of @xmath234 is a polynomial in the elements of @xmath189 .",
    "it is not the zero polynomial , as setting @xmath235 yields @xmath236 , and the determinant of the latter s top - left @xmath200 sub - matrix is non - zero . as a non - zero polynomial ,",
    "the determinant of the top - left @xmath200 sub - matrix of @xmath234 vanishes only on a set of zero measure  ( @xcite ) .",
    "this implies that indeed the measure of @xmath229 is zero .",
    "we introduce a few notations towards our application of fubini s theorem .",
    "first , the symbol @xmath237 will be used to represent indicator functions , e.g. @xmath238 is the function from @xmath215 to @xmath239 that receives @xmath207 on @xmath49 and @xmath208 elsewhere .",
    "second , we use a subscript of @xmath240 to indicate that the corresponding set is intersected with the hyper - rectangle of radius @xmath241 .",
    "for example , @xmath242 stands for the intersection between @xmath49 and @xmath243^{p+d}$ ] , and @xmath244 stands for the intersection between @xmath155 and @xmath243^d$ ] ( which is equal to the latter ) .",
    "all the sets we consider are measurable , and those with subscript @xmath241 have finite measure",
    ". we may thus apply fubini s theorem to get : @xmath245 recall that the set @xmath246 has zero measure , and for every @xmath247 the measure of @xmath248 is zero .",
    "this implies that both integrals in the last expression vanish , and thus @xmath249 .",
    "finally , we use the monotone convergence theorem to compute @xmath250 : @xmath251 this shows that indeed our set of interest @xmath49 has zero measure .    with all preliminaries and lemmas in place , we turn to prove thm .",
    "[ thm : fundamental ] , establishing an exponential efficiency of ht decomposition ( eq .  [ eq : ht_decomp ] ) over cp decomposition ( eq .  [ eq : cp_decomp ] ) .",
    "we begin with the case of an `` unshared '' composition , i.e. the one given in eq .",
    "[ eq : ht_decomp ] ( as opposed to the `` shared '' setting of @xmath115 ) . denoting for convenience @xmath252 and @xmath253",
    ", we will show by induction over @xmath254 that almost everywhere ( at all points but a set of zero measure ) w.r.t .",
    "@xmath113 , all cp - ranks of the tensors @xmath255,\\gamma\\in[r_l]}$ ] are at least @xmath256 . in accordance with our discussion in the beginning of this subsection , it suffices to consider the matricizations @xmath257 $ ] , and show that these all have ranks greater or equal to @xmath256 almost everywhere .    for the case",
    "we have : @xmath258 denote by @xmath259 the matrix with columns @xmath260 , by @xmath261 the matrix with columns @xmath262 , and by @xmath263 the diagonal matrix with @xmath264 on its diagonal .",
    "then , we may write @xmath265=adb^\\top$ ] , and according to lemma  [ lemma : base ] the rank of @xmath265 $ ] equals @xmath112 almost everywhere w.r.t . @xmath266 . to see that this holds almost everywhere w.r.t .",
    "@xmath113 , one should merely recall that for any dimensions @xmath267 , if the set @xmath156 has zero measure , so does any subset of @xmath157 .",
    "a finite union of zero measure sets has zero measure , thus the fact that @xmath268=r$ ] holds almost everywhere individually for any @xmath269 $ ] and @xmath270 $ ] , implies that it holds almost everywhere jointly for all @xmath88 and @xmath89 .",
    "this proves our inductive hypothesis ( unshared case ) for @xmath121 .",
    "assume now that almost everywhere @xmath271 \\geq r^{\\nicefrac{2^{l-1}}{2}}$ ] for all @xmath272 $ ] and @xmath273 $ ] .",
    "for some specific choice of @xmath274 $ ] and @xmath275 $ ] we have : @xmath276 = \\sum_{\\alpha=1}^{r_{l-1 } } a_\\alpha^{l , j,\\gamma } [ \\phi^{l-1,2j-1,\\alpha } ] \\odot [ \\phi^{l-1,2j,\\alpha}]\\ ] ] denote @xmath277 \\odot [ \\phi^{l-1,2j,\\alpha}]$ ] for @xmath278 . by our inductive assumption , and by the general property @xmath279 , we have that almost everywhere the ranks of all matrices @xmath280 are at least @xmath281 . writing @xmath257 = \\sum_{\\alpha=1}^{r_{l-1 } } a_\\alpha^{l , j,\\gamma } \\cdot m_\\alpha$ ] , and noticing that @xmath282 do not depend on @xmath9 , we turn our attention to lemma  [ lemma : step ] .",
    "the lemma tells us that @xmath283 \\geq r^{\\nicefrac{2^l}{2}}$ ] almost everywhere . since a finite union of zero measure sets has zero measure , we conclude that almost everywhere @xmath283 \\geq r^{\\nicefrac{2^l}{2}}$ ] holds jointly for all @xmath274 $ ] and @xmath275 $ ]",
    "this completes the proof of the theorem in the unshared case .    proving the theorem in the shared case",
    "may be done in the exact same way , except that for @xmath121 one needs the version of lemma  [ lemma : base ] for which @xmath187 and @xmath190 are equal .",
    "we now head on to prove thm .",
    "[ thm : generalized ] , which is a generalization of thm .",
    "[ thm : fundamental ] .",
    "the proof will be similar in nature to that of thm .",
    "[ thm : fundamental ] , yet slightly more technical . in short ,",
    "the idea is to show that in the generic case , expressing @xmath135 as a sum of tensor products between tensors of order @xmath284 requires at least @xmath285 terms .",
    "since @xmath136 is expressed as a sum of @xmath286 such terms , demanding @xmath287 implies @xmath288 .    to gain technical advantage and utilize known results from matrix theory ( as we did when proving thm .",
    "[ thm : fundamental ] ) , we introduce a new tensor `` squeezing '' operator @xmath289 .",
    "for @xmath290 , @xmath291 is an operator that receives a tensor with order divisible by @xmath292 , and returns the tensor obtained by merging together the latter s modes in groups of size @xmath292 .",
    "specifically , when applied to the tensor @xmath293 ( @xmath294 ) , @xmath291 returns a tensor of order @xmath295 which holds @xmath296 in the location defined by the following index for every mode @xmath297 $ ] : @xmath298 .",
    "notice that when applied to a tensor of order @xmath292 , @xmath291 returns a vector .",
    "also note that if @xmath19 and @xmath23 are tensors with orders divisible by @xmath292 , and @xmath299 is a scalar , we have the desirable properties :    * @xmath300 * @xmath301    for the sake of our proof we are interested in the case @xmath302 , and denote for brevity @xmath303 .",
    "as stated above , we would like to show that in the generic case , expressing @xmath135 as @xmath304 , where @xmath305 are tensors of order @xmath284 , implies @xmath306 .",
    "applying @xmath289 to both sides of such a decomposition gives : @xmath307 , where @xmath308 are now vectors .",
    "thus , to prove thm .",
    "[ thm : generalized ] it suffices to show that in the generic case , the cp - rank of @xmath309 is at least @xmath285 , or alternatively , that the rank of the matricization @xmath310 $ ] is at least @xmath285 .",
    "this will be our strategy in the following proof :    in accordance with the above discussion , it suffices to show that in the generic case @xmath311",
    "\\geq r^{\\nicefrac{n}{2^{l_2}}}$ ] . to ease the path for the reader , we reformulate the problem using slightly simpler notations .",
    "we have an order-@xmath12 tensor @xmath19 with dimension @xmath38 in each mode , generated as follows : ^1,j , & = & _ = 1^r_0 a_^1,j , a^0,2j-1 , a^0,2j , + & & + ^l , j , & = & _ = 1^r_l-1 a_^l , j , _ _ + & & + a&= & _ = 1^r_l_1 - 1 a^l_1,1,1__j=1 ^ 2^l - l_1 + 1 _ where :    * @xmath312 * @xmath313 * @xmath314 for @xmath315 $ ] and @xmath316 $ ] * @xmath317 for @xmath318 $ ] , @xmath274 $ ] and @xmath275 $ ] * @xmath319    let @xmath137 be a positive integer smaller than @xmath133 , and let @xmath289 be the tensor squeezing operator that merges groups of @xmath284 modes .",
    "define @xmath320 . with @xmath321 $ ] being the matricization operator defined in the beginning of the appendix ,",
    "our task is to prove that @xmath322 \\geq r^{\\nicefrac{n}{2^{l_2}}}$ ] almost everywhere w.r.t .",
    "we also consider the case of shared parameters  ",
    "@xmath115 , where we would like to show that the same condition holds almost everywhere w.r.t .",
    "@xmath116 .",
    "our strategy for proving the claim is inductive .",
    "we show that for @xmath323 , almost everywhere it holds that for all @xmath88 and all @xmath89 : @xmath324 \\geq r^{2^{l - l_2}}$ ] .",
    "we then treat the special case of @xmath325 , showing that indeed @xmath322 \\geq r^{\\nicefrac{n}{2^{l_2}}}$ ] .",
    "we begin with the setting of unshared parameters ( @xmath9 ) , and afterwards attend the scenario of shared parameters ( @xmath326 ) as well .",
    "our first task is to treat the case @xmath327 , i.e. show that @xmath328 \\geq r$ ] almost everywhere jointly for all @xmath88 and all @xmath89 ( there is actually no need for the matricization @xmath321 $ ] here , as @xmath329 are already matrices ) . since a union of",
    "finitely many zero measure sets has zero measure , it suffices to show that this condition holds almost everywhere when specific @xmath88 and @xmath89 are chosen .",
    "denote by @xmath330 a vector holding @xmath207 in entry @xmath6 and @xmath208 elsewhere , by @xmath331 a vector of zeros , and by @xmath237 a vector of ones .",
    "suppose that for every @xmath88 we assign @xmath332 to be @xmath333 when @xmath334 and @xmath331 otherwise .",
    "suppose also that for all @xmath335 and all @xmath88 we set @xmath9 to be @xmath336 when @xmath337 and @xmath331 otherwise .",
    "finally , assume we set @xmath338 for all @xmath88 and all @xmath89 .",
    "these settings imply that for every @xmath88 , when @xmath337 we have @xmath339 , i.e. the tensor @xmath340 holds @xmath207 in location @xmath341 and @xmath208 elsewhere . if @xmath342 then @xmath340 is the zero tensor .",
    "we conclude from this that there are indices @xmath343 such that @xmath344 for @xmath337 , and that for @xmath342 we have @xmath345 .",
    "we may thus write : @xmath346 now , since @xmath347 are different from each other , the matrix @xmath329 has rank @xmath148 .",
    "this however does not prove our inductive hypothesis for @xmath327 .",
    "we merely showed a specific parameter assignment for which it holds , and we need to show that it is met almost everywhere .",
    "to do so , we consider an @xmath200 sub - matrix of @xmath329 which is non - singular under the specific parameter assignment we defined .",
    "the determinant of this sub - matrix is a polynomial in the elements of @xmath113 which we know does not vanish with the specific assignments defined .",
    "thus , this polynomial vanishes at subset of @xmath113 having zero measure ( see  @xcite ) . that is to say , the sub - matrix of @xmath329 has rank @xmath148 almost everywhere , and thus @xmath329 has rank at least @xmath148 almost everywhere .",
    "this completes our treatment of the case @xmath327 .",
    "we now turn to prove the propagation of our inductive hypothesis .",
    "let @xmath348 , and assume that our inductive hypothesis holds for @xmath349 .",
    "specifically , assume that almost everywhere w.r.t .",
    "@xmath113 , we have that @xmath350 \\geq r^{2^{l-1-l_2}}$ ] jointly for all @xmath351 $ ] and all @xmath352 $ ] .",
    "we would like to show that almost everywhere , @xmath324",
    "\\geq r^{2^{l - l_2}}$ ] jointly for all @xmath274 $ ] and all @xmath275 $ ] . again",
    ", the fact that a finite union of zero measure sets has zero measure implies that we may prove the condition for specific @xmath274 $ ] and @xmath275 $ ] . applying the squeezing operator @xmath289 followed by matricization @xmath321 $ ] to the recursive expression for @xmath92 , we get : & = & = + & = & _",
    "= 1^r_l-1 a_^l , j , [ ( ^l-1,2j-1 , ) ] for @xmath278 , denote the matrix @xmath353 \\odot [ \\varphi(\\phi^{l-1,2j,\\alpha})]$ ] by @xmath280 . the fact that the kronecker product multiplies ranks , along with our inductive assumption , imply that almost everywhere @xmath354 .",
    "noting that the matrices @xmath280 do not depend on @xmath9 , we apply lemma  [ lemma : step ] and conclude that almost everywhere @xmath324 \\geq r^{2^{l - l_2}}$ ] , which completes the prove of the inductive propagation .",
    "next , we treat the special case @xmath325 .",
    "we assume now that almost everywhere @xmath355 \\geq r^{2^{l_1 - 1-l_2}}$ ] jointly for all @xmath88 and all @xmath89 .",
    "again , we apply the squeezing operator @xmath289 followed by matricization @xmath321 $ ] , this time to both sides of the expression for @xmath19 : @xmath356   = \\sum_{\\alpha=1}^{r_{l_1 - 1 } } a^{l_1,1,1}_\\alpha \\mathop{\\odot}_{j=1}^{2^{l - l_1 + 1 } } [ \\varphi(\\phi^{l_1 - 1,j,\\alpha})]\\ ] ] as before , denote @xmath357 $ ] for @xmath358 . using again the multiplicative rank property of the kronecker product along with our inductive assumption",
    ", we get that almost everywhere @xmath359 . noticing that @xmath360}$ ]",
    "do not depend on @xmath361 , we apply lemma  [ lemma : step ] for the last time and get that almost everywhere ( w.r.t .",
    "@xmath113 ) , the rank of @xmath362 $ ] is at least @xmath363 .",
    "this completes our proof in the case of unshared parameters .",
    "proving the theorem in the case of shared parameters ( @xmath364 ) can be done in the exact same way as above .",
    "in fact , all one has to do is omit the references to @xmath88 and the proof will apply . notice in particular that the specific parameter assignment we defined to handle @xmath327 was completely symmetric , i.e. it did not include any dependence on @xmath88 .",
    "corollaries  [ corollary : fundamental ] and  [ corollary : generalized ] are a direct continuation of thm .",
    "[ thm : fundamental ] and  [ thm : generalized ] respectively . in the theorems",
    ", we have shown that almost all coefficient tensors generated by a deep network can not be realized by a shallow network if the latter does not meet a certain minimal size requirement .",
    "the corollaries take this further , by stating that given linearly independent representation functions @xmath63 , not only is efficient realization of coefficient tensors generally impossible , but also efficient approximation of score functions . to prove this extra step , we recall from the proofs of thm .  [ thm : fundamental ] and  [ thm : generalized ] ( app .",
    "[ app : proofs : thm ] ) that in order to show separation between the coefficient tensor of a deep network and that of a shallow network , we relied on matricization rank .",
    "specifically , we derived constants @xmath365 , @xmath366 , such that the matricization of a deep network s coefficient tensor had rank greater or equal to @xmath367 , whereas the matricization of a shallow network s coefficient tensor had rank smaller or equal to @xmath368 . given this observation , corollaries  [ corollary : fundamental ] and  [ corollary : generalized ] readily follow from lemma  [ lemma : approx ] below ( the lemma relies on basic concepts and results from the topic of @xmath64 hilbert spaces  ",
    "see app .",
    "[ app : hypo_space : preliminaries ] for a brief discussion on the matter ) .",
    "[ lemma : approx ] let @xmath369 be a set of linearly independent functions , and denote by @xmath370 the ( euclidean ) space of tensors with order @xmath12 and dimension @xmath38 in each mode . for a given tensor @xmath371 , denote by @xmath372 the function in @xmath373 defined by : @xmath374",
    "let @xmath375 be a family of tensors , and @xmath376 be a certain target tensor that lies outside the family .",
    "assume that for all @xmath377 we have @xmath378)<rank([{{\\mathcal a}}^*])$ ] , where @xmath321 $ ] is the matricization operator defined in app .",
    "[ app : proofs : thm ] .",
    "then , the distance in @xmath373 between @xmath379 and @xmath380 is strictly positive , i.e. there exists an @xmath381 such that : @xmath382    the fact that @xmath383}$ ] are linearly independent in @xmath384 implies that the product functions @xmath385}$ ] are linearly independent in @xmath373 ( see app .",
    "[ app : hypo_space : preliminaries ] ) .",
    "let @xmath386 be a sequence of functions that lie in the span of @xmath387}$ ] , and for every @xmath388 denote by @xmath389 the coefficient tensor of @xmath390 under this basis , i.e. @xmath391 is defined by : @xmath392 assume that @xmath386 converges to @xmath379 in @xmath373 : @xmath393 in a finite - dimensional hilbert space , convergence in norm implies convergence in representation coefficients under any preselected basis .",
    "we thus have : @xmath394}:{{\\mathcal a}}_{d_1,\\ldots , d_n}^{(t ) } \\xrightarrow{t\\to\\infty } { { \\mathcal a}}_{d_1,\\ldots , d_n}^*\\ ] ] this means in particular that in the tensor space @xmath370 , @xmath376 lies in the closure of @xmath395 .",
    "accordingly , in order to show that the distance in @xmath373 between @xmath379 and @xmath380 is strictly positive , it suffices to show that the distance in @xmath370 between @xmath376 and @xmath396 is strictly positive , or equivalently , that the distance between the matrix @xmath397 $ ] and the family of matrices @xmath398\\}_{\\lambda\\in\\lambda}$ ] is strictly positive .",
    "this however is a direct implication of the assumption @xmath399)<rank([{{\\mathcal a}}^*])$ ] .",
    "in order to keep the body of the paper at a reasonable length , the presentation of our hypotheses space ( eq .  [ eq : score ] ) in sec .  [ sec : cac ] did not provide the grounds for its definition . in this appendix",
    "we derive the hypotheses space step by step . after establishing basic preliminaries on the topic of @xmath64 spaces ,",
    "we utilize the notion of tensor products between such spaces to reach a universal representation as in eq .",
    "[ eq : score ] but with @xmath400 .",
    "we then make use of empirical studies characterizing the statistics of natural images , to argue that in practice a moderate value of  @xmath38 ( @xmath401 ) suffices .",
    "when dealing with functions over scalars , vectors or collections of vectors , we consider @xmath64 spaces , or more formally , the hilbert spaces of lebesgue measurable square - integrable real functions equipped with standard ( point - wise ) addition and scalar multiplication , as well as the inner - product defined by integral over point - wise multiplication .",
    "the topic of @xmath64 function spaces lies at the heart of functional analysis , and requires basic knowledge in measure theory .",
    "we present here the bare necessities required to follow this appendix , referring the interested reader to  @xcite for a more comprehensive introduction .    for our purposes , it suffices to view an @xmath64 space as a vector space of all functions @xmath403 satisfying @xmath404 .",
    "this vector space is infinite dimensional , and a set of functions @xmath405 is referred to as _ total _ if the closure of its span covers the entire space , i.e. if for any function @xmath406 and @xmath381 , there exist functions @xmath407 and coefficients @xmath408 such that @xmath409 .",
    "@xmath410 is regarded as _",
    "linearly independent _",
    "if all of its finite subsets are linearly independent , i.e. for any @xmath407 , @xmath411 , and @xmath408 , if @xmath412 then @xmath413 . a non - trivial result states that @xmath64 spaces in general must contain total and linearly independent sets , and moreover , for any @xmath414 , @xmath384 contains a _",
    "countable _ set of this type .",
    "it seems reasonable to draw an analogy between total and linearly independent sets in @xmath64 space , and bases in a finite dimensional vector space .",
    "while this analogy is indeed appropriate from our perspective , total and linearly independent sets are not to be confused with _",
    "bases _ for @xmath64 spaces , which are typically defined to be orthonormal .",
    "it can be shown ( see for example  @xcite ) that for any natural numbers @xmath415 and @xmath12 , if @xmath416 is a total or a linearly independent set in @xmath384 , then @xmath417 , the induced point - wise product functions on @xmath13 , form a set which is total or linearly independent , respectively , in @xmath373 . as we now briefly outline , this result actually emerges from a deep relation between tensor products and hilbert spaces .",
    "the definitions given in sec .",
    "[ sec : preliminaries ] for a tensor , tensor space , and tensor product , are actually concrete special cases of much deeper , abstract algebraic concepts .",
    "a more formal line of presentation considers multiple vector spaces @xmath418 , and defines their tensor product space @xmath419 to be a specific quotient space of the space freely generated by their cartesian product set . for every combination of vectors",
    "@xmath420 , @xmath18 $ ] , there exists a corresponding element @xmath421 in the tensor product space , and moreover , elements of this form span the entire space . if @xmath418 are hilbert spaces , it is possible to equip @xmath419 with a natural inner - product operation , thereby turning it too into a hilbert space .",
    "it may then be shown that if the sets @xmath422 , @xmath18 $ ] , are total or linearly independent , elements of the form @xmath423 are total or linearly independent , respectively , in @xmath419 .",
    "finally , when the underlying hilbert spaces are @xmath384 , the point - wise product mapping @xmath424 from the tensor product space @xmath425 to @xmath373 , induces an isomorphism of hilbert spaces .",
    "recall from sec .",
    "[ sec : cac ] that our instance space is defined as @xmath426 , in accordance with the common practice of representing natural data through ordered local structures ( for example images are often represented through small patches around their pixels ) .",
    "we classify instances into categories @xmath427 via maximization of per - label score functions @xmath428 .",
    "our hypotheses space @xmath429 is defined to be the subset of @xmath373 from which score functions may be taken .    in app .",
    "[ app : hypo_space : preliminaries ] we stated that if @xmath416 is a total set in @xmath384 , i.e. if every function in @xmath384 can be arbitrarily well approximated by a linear combination of a finite subset of @xmath416 , then the point - wise products @xmath430 form a total set in @xmath373 .",
    "accordingly , in a universal hypotheses space @xmath431 , any score function @xmath65 may be arbitrarily well approximated by finite linear combinations of such point - wise products .",
    "a possible formulation of this would be as follows .",
    "assume we are interested in @xmath432-approximation of the score function @xmath65 , and consider a formal tensor @xmath7 having @xmath12 modes and a countable infinite dimension in each mode @xmath18 $ ] , indexed by @xmath433 .",
    "then , there exists such a tensor , with all but a finite number of entries set to zero , for which : h_y(x_1,  ,x_n ) _ d_1  d_nn a_d_1, ",
    ",d_n^y_i=1^n f_d_i(x_i ) [ eq : score_infinity ]    given that the set of functions @xmath434 is total , eq .",
    "[ eq : score_infinity ] defines a universal hypotheses space .",
    "there are many possibilities for choosing a total set of functions .",
    "wavelets are perhaps the most obvious choice , and were indeed used in a deep network setting by  @xcite .",
    "the special case of gabor wavelets has been claimed to induce features that resemble representations in the visual cortex  ( @xcite ) .",
    "two options we pay special attention to due to their importance in practice are :    * _ gaussians _ ( with diagonal covariance ) : f_(x ) = ( x;,diag(^2 ) ) [ eq : rep_gaussians ] where @xmath435 . *",
    "_ neurons _ : f_(x ) = ( x^w+b ) [ eq : rep_neurons ] where @xmath436 and @xmath437 is a point - wise non - linear activation such as threshold @xmath438}$ ] , rectified linear unit ( relu ) @xmath439 or sigmoid @xmath440 .    in both cases ,",
    "there is an underlying parametric family of functions @xmath441 of which a countable total subset may be chosen .",
    "the fact that gaussians as above are total in @xmath384 has been proven in  @xcite , and is a direct corollary of the stone - weierstrass theorem .",
    "to achieve countability , simply consider gaussians with rational parameters ( mean and variances ) . in practice , the choice of gaussians ( with diagonal covariance ) give rises to a `` similarity '' operator as described by the simnet architecture  ( @xcite ) .",
    "for the case of neurons we must restrict the domain @xmath11 to some bounded set , otherwise the functions are not integrable .",
    "this however is not a limitation in practice , and indeed neurons are widely used across many application domains .",
    "the fact that neurons are total has been proven in  @xcite and  @xcite for threshold and sigmoid activations .",
    "more generally , it has been proven in  @xcite for a wide class of activation functions , including linear combinations of relu .",
    "see  @xcite for a survey of such results . for countability",
    ", we may again restrict parameters ( weights and bias ) to be rational .    in the case of gaussians and neurons",
    ", we argue that a _ finite _ set of functions suffices , i.e. that it is possible to choose @xmath442 that will suffice in order to represent score functions required for natural tasks .",
    "moreover , we claim that @xmath38 need not be large ( e.g. on the order of  100 ) .",
    "our argument relies on statistical properties of natural images , and is fully detailed in app .",
    "[ app : hypo_space : finite_bases ] .",
    "it implies that under proper choice of @xmath383}$ ] , the finite set of point - wise product functions @xmath443}$ ] spans the score functions of interest , and we may define for each label @xmath8 a tensor @xmath7 of order @xmath12 and dimension @xmath38 in each mode , such that : h_y(x_1,  ,x_n)=_d_1,  ,d_n=1^ma_d_1,  ,d_n^y_i=1^n f__d_i(x_i ) which is exactly the hypotheses space presented in sec .",
    "[ sec : cac ] . notice that if @xmath444}{\\subset}l^2({{\\mathbb r}}^s)$ ] are linearly independent ( there is no reason to choose them otherwise ) , then so are the product functions @xmath445}{\\subset}l^2\\left(({{\\mathbb r}}^s)^n\\right)$ ] ( see app .",
    "[ app : hypo_space : preliminaries ] ) , and a score function @xmath65 uniquely determines the coefficient tensor @xmath7 .",
    "in other words , two score functions @xmath446 and @xmath447 are identical if and only if their coefficient tensors @xmath448 and @xmath449 are the same .      in app .",
    "[ app : hypo_space : construction ] we laid out the framework of classifying instances in the space @xmath450 into labels @xmath52 via maximization of per - label score functions @xmath451 : @xmath452 where @xmath66 is of the form : h_y(x_1,  ,x_n)=_d_1,  ,d_n=1^m",
    ",d_n^y_i=1^n f__d_i(x_i ) and @xmath453}$ ] are selected from a parametric family of functions @xmath454 .",
    "for universality , i.e.  for the ability of score functions @xmath65 to approximate any function in @xmath455 as @xmath400 , we required that it be possible to choose a countable subset of @xmath410 that is total in @xmath384 .",
    "we noted that the families of gaussians ( eq .  [ eq : rep_gaussians ] ) and neurons ( eq .  [ eq : rep_neurons ] ) meet this requirement .    in this subsection",
    "we formalize our argument that a _",
    "finite _ value for @xmath38 is sufficient when @xmath456 represents natural data , and in particular , natural images .",
    "based on empirical studies characterizing the statistical properties of natural images , and in compliance with the number of channels in a typical convolutional network layer , we find that @xmath38 on the order of  100 typically suffices .",
    "let @xmath457 be a distribution of labeled instances @xmath458 over @xmath459 ( we use bar notation to distinguish the label @xmath460 from the running index @xmath8 ) , and @xmath461 be the induced marginal distribution of instances @xmath53 over @xmath456 .",
    "we would like to show , given particular assumptions on @xmath457 , that there exist functions @xmath462 and tensors @xmath463 of order @xmath12 and dimension @xmath38 in each mode , such that the score functions @xmath65 defined in eq .",
    "[ eq : score ] achieve low classification error : l_d^0 - 1 ( h_1,  ,h_y ) : = e_(x,|y)~d [ eq : class_error ] @xmath464}$ ] here stands for the indicator function , taking the value @xmath207 when its argument is true , and @xmath208 otherwise .",
    "let @xmath465 be a set of `` ground truth '' score functions for which optimal prediction is achieved , or more specifically , for which the expected hinge - loss ( upper bounds the 0 - 1 loss ) is minimal : @xmath466 where : l_d^hinge ( h_1, ",
    ",h_y):=e_(x,|y)~d[eq : expected_hinge_loss ] our strategy will be to select score functions @xmath65 of the format given in eq .",
    "[ eq : score ] , that approximate @xmath467 in the sense of low expected maximal absolute difference : e:=e_x~d_x[eq : score_approx_error ] we refer to @xmath468 as the _ score approximation error _ obtained by @xmath65 .",
    "the 0 - 1 loss of @xmath65 with respect to the labeled example @xmath469 is bounded as follows : & & _ yy \\{+h_y(x ) } -h_|y(x ) + & & = _ yy \\{+h_y^*(x)+h_y(x)-h_y^*(x)}-h_|y^*(x)+h_|y^*(x)-h_|y(x ) + & & _ yy \\{+h_y^*(x)}-h_|y^*(x)+_yy \\{h_y(x)-h_y^*(x)}+h_|y^*(x)-h_|y(x ) + & & _ yy \\{+h_y^*(x)}-h_|y^*(x)+2_yy \\{_y(x)-h_y^*(x ) } taking expectation of the first and last terms above with respect to @xmath470 , and recalling the definitions given in eq .",
    "[ eq : class_error ] ,  [ eq : expected_hinge_loss ] and  [ eq : score_approx_error ] , we get : @xmath471 in words , the classification error of the score functions @xmath65 is bounded by the optimal expected hinge - loss plus a term equal to twice their score approximation error .",
    "recall that we did not constrain the optimal score functions @xmath467 in any way .",
    "thus , assuming a label is deterministic given an instance , the optimal expected hinge - loss is essentially zero , and the classification error of @xmath65 is dominated by their score approximation error @xmath468 ( eq .  [ eq : score_approx_error ] ) .",
    "our problem thus translates to showing that @xmath65 can be selected such that @xmath468 is small .    at this point",
    "we introduce our main assumption on the distribution @xmath457 , or more specifically , on the marginal distribution of instances @xmath461 . according to various studies , in natural settings",
    ", the marginal distribution of individual vectors in @xmath456 , e.g.  of small patches in images , may be relatively well captured by a gaussian mixture model ( _ gmm _ ) with a moderate number ( on the order of  100 or less ) of distinct components .",
    "for example , it was shown in  @xcite that natural image patches of size @xmath472 , @xmath473 , @xmath474 or  @xmath475 , can essentially be modeled by gmms with @xmath476 components ( adding more components barely improved the log - likelihood ) .",
    "this complies with the common belief that a moderate number of low - level templates suffices in order to model the vast majority of local image patches . following this line ,",
    "we model the marginal distribution of @xmath477 with a gmm having @xmath38 components with means @xmath478 .",
    "we assume that the components are well localized , i.e. that their standard deviations are small compared to the distances between means , and also compared to the variation of the target functions @xmath479 . in the context of images for example ,",
    "the latter two assumptions imply that a local patch can be unambiguously assigned to a template , and that the assignment of patches to templates determines the class of an image . returning to general instances @xmath53 ,",
    "their probability mass will be concentrated in distinct regions of the space @xmath456 , in which for every @xmath18 $ ] , the vector @xmath477 lies near @xmath480 for some @xmath481 $ ] .",
    "the score functions @xmath479 are approximately constant in each such region .",
    "it is important to stress here that we do _ not _ assume statistical independence of @xmath477 s , only that their possible values can be quantized into @xmath38 templates @xmath482 .",
    "under our idealized assumptions on @xmath461 , the expectation in the score approximation error @xmath468 can be discretized as follows : e:=e_x~d_x= _ c_1, ",
    ",c_n=1^m p_c_1,  ,c_n",
    "_ yy_y(m_c_1,  ,c_n)-h_y^*(m_c_1,  ,c_n ) [ eq : score_approx_error_discrete ] where @xmath483 and @xmath484 stands for the probability that @xmath477 lies near @xmath480 for every @xmath18 $ ] ( @xmath485 ) .",
    "we now turn to show that @xmath63 can be chosen to separate gmm components , i.e. such that for every @xmath486 $ ] , @xmath487 if and only if @xmath488 . if the functions @xmath489 are gaussians ( eq .",
    "[ eq : rep_gaussians ] ) , we can simply set the mean of @xmath490 to @xmath491 , and its standard deviations to be low enough such that the function effectively vanishes at @xmath492 when @xmath493 .",
    "if @xmath489 are neurons ( eq .  [ eq : rep_neurons ] ) , an additional requirement is needed , namely that the gmm component means @xmath494 be linearly separable . in other words",
    ", we require that for every @xmath495 $ ] , there exist @xmath496 and @xmath497 for which @xmath498 is positive if @xmath488 and negative otherwise .",
    "this may seem like a strict assumption at first glance , but notice that the dimension @xmath415 is often as large , or even larger , then the number of components @xmath38 .",
    "in addition , if input vectors @xmath477 are normalized to unit length ( a standard practice with image patches for example ) , @xmath494 will also be normalized , and thus linear separability is trivially met .",
    "assuming we have linear separability , one may set @xmath499 , and for threshold or relu activations we indeed get @xmath500 . with sigmoid activations , we may need to scale @xmath501 so that @xmath502 when @xmath503 , and that would ensure that in this case @xmath504 effectively vanishes .    assuming we have chosen @xmath63 to separate gmm components , and plugging - in the format of @xmath65 given in eq .",
    "[ eq : score ] , we get the following convenient form for @xmath505 : @xmath506 assigning the coefficient tensors through the following rule : @xmath507 implies : @xmath508 for every @xmath56 and @xmath509 $ ] . plugging this into eq .",
    "[ eq : score_approx_error_discrete ] , we get a score approximation error of zero .    to recap ,",
    "we have shown that when the parametric functions @xmath489 are gaussians ( eq .",
    "[ eq : rep_gaussians ] ) or neurons ( eq .  [ eq : rep_neurons ] ) , not only are the score functions @xmath65 given in eq .",
    "[ eq : score ] universal when @xmath400 ( see app .",
    "[ app : hypo_space : construction ] ) , but they can also achieve zero classification error ( eq .  [ eq : class_error ] ) with a moderate value of @xmath38 ( on the order of  100 ) if the underlying data distribution @xmath457 is `` natural '' . in this context , @xmath457 is regarded as natural if it satisfies two conditions .",
    "the first , which is rather mild , requires that a label be completely determined by the instance .",
    "for example , an image will belong to one category with probability one , and to the rest of the categories with probability zero .",
    "the second condition , which is far more restrictive , states that input vectors composing an instance can be quantized into a moderate number ( @xmath38 ) of templates .",
    "the assumption that natural images exhibit this property is based on various empirical studies where it is shown to hold approximately .",
    "since it does not hold exactly , our analysis is approximate , and its implication in practice is that the classification error introduced by constraining score functions to have the format given in eq .",
    "[ eq : score ] , is negligible compared to other sources of error ( factorization of the coefficient tensors , finiteness of training data and difficulty in optimization ) .",
    "the classic approach for theoretically analyzing the power of depth focused on investigation of the computational complexity of _",
    "boolean circuits_. an early result , known as the `` exponential efficiency of depth '' , may be summarized as follows : for every integer @xmath14 , there are boolean functions that can be computed by a circuit comprising alternating layers of and and or gates which has depth @xmath14 and polynomial size , yet if one limits the depth to @xmath510 or less , an exponentially large circuit is required .",
    "see  @xcite for a formal statement of this classic result .",
    "recently , @xcite  have established a somewhat stronger result , showing cases where not only are polynomially wide shallow boolean circuits incapable of exact realization , but also of approximation ( i.e. of agreeing with the target function on more than a specified fraction of input combinations ) .",
    "other classical results are related to _ threshold circuits _ , a class of models more similar to contemporary neural networks than boolean circuits .",
    "namely , they can be viewed as neural networks where each neuron computes a weighted sum of its inputs ( possibly including bias ) , followed by threshold activation ( @xmath511 $ ] ) . for threshold circuits ,",
    "the main known result in our context is the existence of functions that separate depth 3 from depth 2 ( see  @xcite for a statement relating to exact realization , and the techniques in  @xcite for extension to approximation ) .",
    "more recent studies focus on _ arithmetic circuits _  ( @xcite ) , whose nodes typically compute either a weighted sum or a product of their inputs ( besides their role in studying expressiveness , deep networks of this class have been shown to support provably optimal training  @xcite ) .",
    "a  special case of this are the sum - product networks ( _ spns _ ) presented in  @xcite .",
    "spns are a class of deep generative models designed to efficiently compute probability density functions .",
    "their summation weights are typically constrained to be non - negative ( such an arithmetic circuit is called _ monotone _ ) , and in addition , in order for them to be valid ( i.e. to be able to compute probability density functions ) , additional architectural constraints are needed ( e.g. decomposability and completeness ) .",
    "the most widely known theoretical arguments regarding the efficiency of depth in spns were given in  @xcite . in this work ,",
    "two specific families of spns were considered , both comprising alternating sum and product layers    a family @xmath410 whose nodes form a full binary tree , and a family @xmath512 with @xmath241 nodes per layer ( excluding the output ) , each connected to @xmath513 nodes in the preceding layer .",
    "the authors show that functions implemented by these networks require an exponential number of nodes in order to be realized by shallow ( single hidden - layer networks )",
    ". the limitations of this work are twofold .",
    "first , as the authors note themselves , it only analyzes the ability of shallow networks to realize _ exactly _ functions generated by deep networks , and does not provide any result relating to approximation .",
    "second , the specific spn families considered in this work are not universal hypothesis classes and do not resemble networks used in practice .",
    "recently , @xcite  proved that there exist functions which can be efficiently computed by decomposable and complete ( d&c ) spns of depth @xmath514 , yet require a d&c spn of depth @xmath154 or less to have super - polynomial size for exact realization .",
    "this analysis only treats approximation in the limited case of separating depth 4 from depth 3 ( d&c ) spns .",
    "additionally , it only deals with specific separating functions , and does not convey information regarding how frequent these are . in other words , according to this analysis , it may be that almost all functions generated by deep networks _ can _ be efficiently realized by shallow networks , and there are only few pathological functions for which this does not hold .",
    "a further limitation of this analysis is that for general @xmath154 , the separation between depths @xmath514 and @xmath154 is based on a multilinear circuit result by  @xcite , that translates into a network that once again does not follow the common practices of deep learning .",
    "there have been recent attempts to analyze the efficiency of network depth in other settings as well .",
    "the most commonly used type of neural networks these days includes neurons that compute a weighted sum of their inputs ( with bias ) followed by rectified linear unit ( relu ) activation ( @xmath515 ) . @xcite  and  @xcite study the number of linear regions that may be expressed by such networks as a function of their depth and width , thereby showing existence of functions separating deep from shallow ( depth 2 ) networks .",
    "@xcite  shows a simple construction of a depth @xmath154 width 2 relu network that operates on one - dimensional inputs , realizing a function that can not be approximated by relu networks of depth @xmath516 and width polynomial in @xmath154 .",
    "@xcite  provides functions expressible by relu networks of depth 3 and polynomial width , which can only be approximated by a depth 2 network if the latter s width is exponential .",
    "the result in this paper applies not only to relu activation , but also to the standard sigmoid ( @xmath440 ) , and more generally , to any universal activation ( see assumption 1 in  @xcite ) .",
    "@xcite  also considers different types of activations , studying the topological complexity ( through betti numbers ) of decision regions as a function of network depth , width and activation type . the results in this paper",
    "establish the existence of deep vs. shallow separating functions only for the case of polynomial activation .",
    "while the above works do address more conventional neural networks , they do not account for the structure of convolutional networks    the most successful deep learning architectures to date , and more importantly , they too prove only existence of _ some _ separating functions , without providing any insight as to how frequent these are .",
    "we are not the first to incorporate ideas from the field of tensor analysis into deep learning .",
    "@xcite ,  @xcite ,  @xcite ,  and  @xcite all proposed different neural network architectures that include tensor - based elements , and exhibit various advantages in terms of expressiveness and/or ease of training . in  @xcite ,",
    "an alternative algorithm for training neural networks is proposed , based on tensor decomposition and fourier analysis , with proven generalization bounds . in  @xcite ,  @xcite ,  @xcite and  @xcite , algorithms for tensor decompositions are used to estimate parameters of different graphical models .",
    "notably ,  @xcite uses the relatively new hierarchical tucker decomposition ( @xcite ) that we employ in our work , with certain similarities in the formulations .",
    "the works differ considerably in their objectives though : while  @xcite focuses on the proposal of a new training algorithm , our purpose in this work is to analyze the expressive efficiency of networks and how that depends on depth .",
    "recently ,  @xcite modeled the filters in a convolutional network as four dimensional tensors , and used the cp decomposition to construct an efficient and accurate approximation . another work that draws a connection between tensor analysis and deep learning is the recent study presented in  @xcite",
    "this work shows that with sufficiently large neural networks , no matter how training is initialized , there exists a local optimum that is accessible with gradient descent , and this local optimum is approximately equivalent to the global optimum in terms of objective value .",
    "a practical issue one faces when implementing arithmetic circuits is the numerical instability of the product operation    a product node with a large number of inputs is easily susceptible to numerical overflow or underflow .",
    "a common solution to this is to perform the computations in log - space , i.e. instead of computing activations we compute their @xmath517 .",
    "this requires the activations to be non - negative to begin with , and alters the sum and product operations as follows .",
    "a product simply turns into a sum , as @xmath518 .",
    "a sum becomes what is known as _ log - sum - exp _ or _",
    "softmax _ : @xmath519 .",
    "turning to our networks , the requirement that all activations be non - negative does not limit their universality .",
    "the reason for this is that the functions @xmath489 are non - negative in both cases of interest    gaussians ( eq .  [ eq : rep_gaussians ] ) and neurons ( eq .  [ eq : rep_neurons ] ) . in addition , one can always add a common offset to all coefficient tensors @xmath7 , ensuring they are positive without affecting classification .",
    "non - negative decompositions ( i.e. decompositions with all weights holding non - negative values ) can then be found , leading all network activations to be non - negative . in general , non",
    "- negative tensor decompositions may be less efficient than unconstrained decompositions , as there are cases where a non - negative tensor supports an unconstrained decomposition that is smaller than its minimal non - negative decomposition .",
    "nevertheless , as we shall soon see , these non - negative decompositions translate into a proven architecture , which was demonstrated to achieve comparable performance to state of the art convolutional networks , thus in practice the deterioration in efficiency does not seem to be significant .    navely implementing cp or ht model ( fig .  [",
    "fig : cp_model ] or  [ fig : ht_model ] respectively ) in log - space translates to @xmath517 activation following the locally connected linear transformations ( convolutions if coefficients are shared , see sec .  [ sec : shared ] ) , to product pooling turning into sum pooling , and to @xmath520 activation following the pooling .",
    "however , applying @xmath520 and @xmath517 activations as just described , without proper handling of the inputs to each computational layer , would not result in a numerically stable computation . directly .",
    "this however can be easily corrected by defining @xmath521 , and computing @xmath522 .",
    "the result is identical , but now we only exponentiate negative numbers ( no overflow ) , with at least one of these numbers equal to zero ( no underflow ) . ]",
    "the simnet architecture  ( @xcite ) naturally brings forth a numerically stable implementation of our networks .",
    "the architecture is based on two ingredients    a flexible similarity measure and the _ mex _ operator : @xmath523 the similarity layer , capable of computing both the common convolutional operator as well as weighted @xmath524  norm , may realize the representation by computing @xmath525 , whereas mex can naturally implement both log - sum - exp and sum - pooling ( @xmath526 ) in a numerically stable manner .    not only are simnets capable of correctly and efficiently implementing our networks , but they have already been demonstrated  ( @xcite ) to perform as well as state of the art convolutional networks on several image recognition benchmarks , and outperform them when computational resources are limited ."
  ],
  "abstract_text": [
    "<S> it has long been conjectured that hypotheses spaces suitable for data that is compositional in nature , such as text or images , may be more efficiently represented with deep hierarchical networks than with shallow ones . despite the vast empirical evidence supporting this belief , theoretical justifications to date </S>",
    "<S> are limited . in particular </S>",
    "<S> , they do not account for the locality , sharing and pooling constructs of convolutional networks , the most successful deep learning architecture to date . in this work </S>",
    "<S> we derive a deep network architecture based on arithmetic circuits that inherently employs locality , sharing and pooling . </S>",
    "<S> an equivalence between the networks and hierarchical tensor factorizations is established . </S>",
    "<S> we show that a shallow network corresponds to cp ( rank-1 ) decomposition , whereas a deep network corresponds to hierarchical tucker decomposition . using tools from measure theory and matrix algebra </S>",
    "<S> , we prove that besides a negligible set , all functions that can be implemented by a deep network of polynomial size , require exponential size in order to be realized ( or even approximated ) by a shallow network . </S>",
    "<S> since log - space computation transforms our networks into simnets , the result applies directly to a deep learning architecture demonstrating promising empirical performance . </S>",
    "<S> the construction and theory developed in this paper shed new light on various practices and ideas employed by the deep learning community .    _ </S>",
    "<S> deep learning _ , _ expressive power _ , </S>",
    "<S> _ arithmetic circuits _ , _ tensor decompositions _ </S>"
  ]
}