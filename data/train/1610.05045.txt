{
  "article_text": [
    "networks are mathematical representation of interactions among the components of a system and can be modeled by graphs . a graph g=(v , e ) consists of a collection of vertices v , corresponding to the individual units of the observed system , and a collection of edges e , indicating some relation between pairs of vertices .    graphs modeling real systems , i.e. social , biological , and technological networks , display non trivial topological features .",
    "indeed they present the properties that define a complex network : big inhomogeneities , a broad degree distribution and distribution of edges locally inhomogeneous . in the study of complex networks ,",
    "a network is said to have a community structure if the vertices can be divided in @xmath0 groups , such that nodes belonging to the same group are densely connected and the number of edges between nodes of different groups is minimal .",
    "the problem of community detection ( graph partitioning ) has been widely studied by researchers in a variety of fields , including statistics , physics , biology , social and computer science in the last 15 years .",
    "finding communities within an arbitrary complex network can be a computationally difficult task .",
    "the number of communities , if any , within the network is typically unknown and the communities are often of unequal size and/or density . despite these difficulties , however , several methods for community finding have been developed and employed with varying levels of success , see @xcite , @xcite , @xcite , @xcite , @xcite and @xcite for reviews .",
    "our work focusses on the problem of testing the robustness of the recovered partition of a given community detection method . in the following we",
    "provide a brief review of the state of the art of the literature addressing this problem .",
    "although the huge work developed for community detection and its applications , the question of the significance of results still remains open .",
    "our proposal represents a first attempt to statistically define the robustness of a clustering and hence can not be directly compared to any of the following described methodologies .",
    "[ [ state - of - the - art ] ] state of the art + + + + + + + + + + + + + + + +    the modularity @xmath1 of newman and girvan @xcite was the first attempt to give an answer to this question .",
    "it is defined as the fraction of the edges that fall within the given groups minus the expected such fraction if edges were distributed at random and is based on the idea that a random graph is not expected to have a cluster structure . however , as pointed out in @xcite and @xcite , there is an important limit .",
    "precisely , networks with a strong community structure have high modularity , on the contrary high modularity does not imply networks with a community structure .",
    "other authors , see @xcite , @xcite , suggested the use of a z - score to compare the maximum modularity of a graph to the maximum attainable modularity in purely random graphs of the same size and expected degree sequence .",
    "the problem is that the distribution of the null model , though peaked , is not gaussian , causing false positives and false negatives .",
    "a different approach was developed in @xcite , where the authors studied how canonical ensembles of network partitions depend on temperature to assess the significance and nature of the community structure obtained by algorithms that optimize the modularity . in this case",
    "@xmath2 plays the role of energy , i.e. at temperature @xmath3 , the statistical weight of a given partition in the ensemble is proportional to @xmath4 .",
    "typically , as the temperature increases , there is a transition from low entropy / high @xmath1 partitions ( significant cluster structure ) to high entropy / low @xmath1 ( random partitions ) .",
    "if there is strong community structure , the transition is sharp . the peak is broader for networks with weaker community structure , as there are more reasonable alternative partitions with intermediate values of @xmath1 , and so the transition occurs over a broader range of temperature .",
    "they also introduced an order parameter to measure the similarity of the sampled partitions at a given temperature , i.e. whether there is just a single partition with high @xmath1 or a number of competing partitions . therefore , it is a useful tool to detect false positives .",
    "however the methodology is computationally onerous and can not be easily generalized to other optimization methods .    in @xcite",
    "the authors introduced the notion of entropy of graph ensembles to assess the relevance of additional information about the nodes of a network using the information that comes from the topology of the network itself .",
    "the indicator of clustering significance @xmath5 they introduce can also reveal statistical regularities that shed light on possible mechanisms underlying the network stability and formation .",
    "in @xcite the authors presented the order statistics local optimisation method ( @xmath6 ) , a technique based on the local optimization of a fitness function , the c - score @xcite , expressing the statistical significance of a cluster with respect to random fluctuations . given a subgraph @xmath7 in a graph @xmath8 , the c - score measures the probability that the number of links connecting a node to nodes in @xmath7 , where @xmath7 is embedded within a random graph , is higher than or equal to the value seen in the original graph @xmath8 .",
    "this score permits to rank all the vertices external to @xmath7 ( in increasing order of the c - score ) , having at least one connection with @xmath7 , and to calculate its order statistic distribution @xmath9 .",
    "the minimum of @xmath9 is the random variable whose cumulative is the score of the community @xmath7 . to asses",
    "its significance a threshold parameter @xmath10 is fixed .",
    "the procedure is iterated to analyse the full network .",
    "the novelty of this approach is the local estimate of the significance , i.e. of single communities , not of partitions ; on the contrary a serious limit is due to the lack of a data driven procedure to estimate @xmath10 , indeed the authors fix its value to 0.1 .",
    "recently , @xcite proposed a testing based community detection procedure called extraction of statistically significant communities ( essc ) .",
    "the essc procedure measures the statistical significance of connections between a single vertex and a set of vertices in undirected networks under a null distribution derived from the configuration model @xcite .",
    "given an observed network @xmath11 with @xmath12 vertices and a vertex set @xmath13 , they introduce the statistics @xmath14 , measuring the number of edges between a vertex @xmath15 and @xmath13 in the random model @xmath16 , and show that @xmath17 is approximately binomial as @xmath18 in the total variation distance between two probability mass functions .",
    "this permits to obtain the p - values of the null distribution using the binomial approximation and gives origin to an iterative deterministic procedure that recovers robust communities .",
    "the technique has some similarities with oslom , indeed both are extraction methods and use the configuration model as reference distribution , but differentiates because the probabilities have a closed form .",
    "another group of techniques was proposed in @xcite,@xcite , @xcite , and their conceiving was completely different from previous described methodologies .",
    "indeed they introduce a stochastic component in the network by perturbing the graph structure , measure the effect of the perturbation and compare it with the corresponding value for a null model graph .",
    "the basic idea is that a significant partition should not be altered by small modifications , as long as the modification is not too extensive .",
    "an interesting feature of these methods is their independence from the community detection technique adopted .    in this paper",
    "we present a methodology able to clearly detect if the community structure found by some algorithms is statistically significant or is a result of chance , merely due to edge positions in the network .",
    "given a community detection method and a network of interest , our proposal examines the stability of the partition recovered against random perturbations of the original graph structure . to address this issue , following ideas from @xcite ,",
    "we specify a perturbation strategy and a null model to build some procedures based on variation of information as stability measure . given a this measure we address the question of evaluating its the significance .",
    "this permits to build the variation of information curve as a function of the perturbation percentage and to compare it with the corresponding null model curve using analysis tools set up for functional data analysis .",
    "the rest of the paper is organised as follows . in section [ vi ]",
    "we introduce the proposed procedures based on variation of information and the functional data analysis techniques , including their detailed description .",
    "section [ results ] shows the results achieved applying our methodology on simulated and real datasets .",
    "conclusions and ideas for future research are drawn in section [ discussion ] .",
    "variation of information ( vi ) is an information theoretic criterion for comparing two partitions , or clusterings , of the same data set @xcite .",
    "it is a metric and measures the amount of information lost and gained in changing from clustering @xmath19 to clustering @xmath20 .",
    "the criterion makes no assumptions about how the clusterings were generated and applies to both soft and hard clusterings .",
    "given a dataset @xmath21 and two clusterings @xmath19 and @xmath20 of @xmath21 , with @xmath22 and @xmath23 non empty clusters , respectively , vi is defined as @xmath24 where @xmath25 is the entropy associated with clustering @xmath19 @xmath26 and @xmath27 is the mutual information between @xmath19 and @xmath20 , i.e the information that one clustering has about the other @xmath28 @xmath29 is the probability of a point being in cluster @xmath30 and @xmath31 is the probability that a point belongs to @xmath30 in clustering @xmath19 and to @xmath32 in @xmath20 .",
    "another equivalent expression for vi is @xmath33 the first term measures the amount of information about @xmath19 that we loose , while the second measures the amount of information about @xmath20 that we gain , when going from clustering @xmath19 to clustering @xmath20 .",
    "vi metric is the basis of the hypothesis testing procedures we propose to establish the statistical significance of a recovered community structure in a complex network .",
    "our original idea is to generate two different curves based on the vi measure and to statistically test their difference .",
    "the first curve @xmath34 is obtained computing vi between the partition of our original network and the partition of different perturbed version of our original network .",
    "the second curve @xmath35 is obtained computing vi between the partition of a null random network and the partition of different perturbed version of such null network .",
    "the comparison between the two vi curves turns the question about the significance of the retrieved community structure into the study of the stability / robustness of the recovered partition against perturbations .",
    "we expect that it must be robust to small perturbations , because if  small changes \" in the network imply a completely different partition of the data , it means that the found communities are not trustworthy , and this can not be due to the failure of the chosen algorithm for the community detection . indeed the proposed testing procedure is independent from the clustering algorithm and it is easy to check if such a behaviour is due to it",
    ".    to understand well this point we must consider the behaviour of the vi curve for networks having a real community structure and those having a very poor community structure . in the first case the vi curve starts at 0 , when the perturbation level @xmath36 is 0@xmath37 ( unperturbed graph ) , rises rapidly ( perturbation level between 0@xmath37 and 40@xmath37 ) , then levels off when @xmath38 ; in the second case the vi curve immediately grows up to a certain value and levels off that value , meaning that whatever partition has been found , at each level of perturbation , it has the same robustness of a random graph .",
    "obviously the set up of a testing procedure is more necessary for all cases where the community structure is moderate or weak and the behaviour of the vi curve could be similar to that of a random graph .",
    "the choice of the null random network is more delicate , because we expect that it has the same structure of our original graph but with completely random edges .",
    "this is why our choice relapses on the configuration model @xcite associated with the degree sequence of the observed graph @xmath39 with vertex set @xmath40 , i.e. cm(@xmath41 ) .",
    "the cm(@xmath41 ) is a probability measure on the family of multi - graphs with vertex set v and degree sequence @xmath41 that reflects , within the constraints of the degree sequence , a random assignment of edges between vertices .",
    "the generative form is simple : one can simply cut all the edges in the network , so every node still retains its degree by the number of half - edges or stubs emanating from it .",
    "the result will be an even number of half - edges .",
    "to create new networks with the same degree , one simply needs to randomly pair all the half - edges , creating the new edges in the network .",
    "the configuration model generates every possible graph with the given degree distribution with equal probability . note that it naturally creates networks with multiple edges between nodes and self - connections between nodes .",
    "if such networks are unacceptable , one can reject those samples and try the algorithm again , repeating until one obtains a network without multiple or self - connections .",
    "the perturbation strategy adopted is described in section [ permute ] .",
    "the basic steps of our method are the following :    [ procedure ]    1 .",
    "[ first ] find a partition @xmath19 of the given network , with vertex set @xmath40 and degree sequence @xmath39 , by some chosen method @xmath42 ; 2 .",
    "[ second ] given a perturbation level @xmath43 $ ] , perturb the network shuffling its edges by the percentage @xmath36 preserving the original graphs degree distribution ; 3 .   [ third ] using the same method @xmath42 find a partition @xmath20 for the perturbed network and compute the @xmath44 to compare @xmath19 and @xmath20 ; 4 .",
    "[ fourth ] repeat steps [ second]-[third ] at different perturbation levels @xmath45 $ ] so to obtain the @xmath34 curve .",
    "( note that @xmath46 corresponds to the original unperturbed graph , @xmath47 corresponds to the maximal perturbation level ( random graph ) ) ; 5 .",
    "[ fifth ] repeat steps [ first]-[fourth ] starting in step [ first ] from the cm(@xmath41 ) ( null model ) so to get the @xmath35 curve ; 6 .",
    "[ six ] compare the @xmath34 and @xmath35 curves as functions of @xmath36 , statistically testing _",
    "`` the difference '' _ between the two curves .",
    "note that the variation of @xmath36 from 0 to 1 induces an intrinsic order to the data structure as in temporal data and can be treated as time point . moreover , as it will be described in section [ permute ] , we generate many perturbed graphs ( i.e. @xmath48 ) for each different level of @xmath36 and these are considered as replicates per time points in our strategy .",
    "+ step [ six ] of the above procedure is achieved by a functional data analysis approaches aiming to test if the two groups of curves represent  the same process \" or ",
    "different processes \" . the testing procedure we rely on is based on a tool set up for time course microarray data , namely gaussian process ( gp ) regression @xcite .",
    "aim of the gp regression in the context of gene expression data is to identify differentially expressed genes in a one - sample time course microarray experiment , i.e. to detect if the profile has a significant underlying signal or the observations are just random fluctuations . in this case",
    "we reformulate the testing problem working on @xmath49 , as described in section [ sec : gp ] .    in order to show that our approach is robust with respect to the testing procedure used in step [ six ]",
    ", we also display the overall results achieved when using other two approaches in step [ six ] , described respectively in section [ sec : fpc ] and section [ sec : ift ] .",
    "indeed , we can look at the two measured @xmath50 curves as independent realisations of two underlying processes say @xmath51 and @xmath52 observed with noise on a finite grid of points @xmath53 $ ] and to test the null hypothesis @xmath54 versus the alternative hypothesis @xmath55 where @xmath56 means that the processes on either side have the same distribution .    then , as described in section [ sec : fpc ] , taking advantage of the karhunen- love expansion we explore the methodology developed in @xcite based on functional principal components analysis ( fpca ) to test ( [ h0_fadtest ] ) .    on the contrary ,",
    "the approach described in section [ sec : ift ] addresses a domain - selective inferential procedure , providing an interval - wise non parametric functional testing @xcite , able not only to assess ( [ h0_fadtest ] ) , but also to point out specific differences",
    ".    we will briefly describe gp regression , fpca and interval - wise functional testing in the following sections .",
    "we would like to point out that our overall procedure provides a workflow to validate a community structure under different perspectives that can be investigated in dependence of the specific real problem dealt with .",
    "the description of the three different testing procedure is functional to the understanding of our overall procedure and in particular how we exploit the theory underling each single methodology to compare the curves @xmath34 and @xmath35 .",
    "hence we will summarise the three testing procedures to highlight the key connection to our testing problem .",
    "we choose to provide a review of each single methodology to provide awareness of the differences between the three procedures and their link to our testing problem . even if addressing the same problem they are not equivalent .",
    "we refer to the original papers for any theoretical property of such testing procedures , including type @xmath57 error study .",
    "we want to stress that the original contribution of our proposal is summarised in the six steps procedure depicted in the above frame .      in this section",
    "we briefly summarise the methodology proposed in @xcite , where the authors present an approach to estimate the continuous trajectory of gene expression time - series from microarray through gp regression .",
    "briefly we recall that a gaussian process is the natural generalisation of a multivariate gaussian distribution to a gaussian distribution over a specific family of functions .",
    "more precisely , as defined in @xcite , a gaussian process is a _ collection of random variables , any finite number of which have a joint gaussian distribution _ and is completely specified by its mean function and its covariance function .",
    "if we define the mean function @xmath58 and the covariance function @xmath59 of a real process @xmath60 as : @xmath61,\\\\ k(x , x\\textprime ) & = e[(f(x ) - m(x)(f(x\\textprime ) - m(x\\textprime))]\\end{aligned}\\ ] ]    then we can write the gp as @xmath62 the random variables @xmath63 represent the value of the function @xmath60 at time locations @xmath64 , being @xmath60 the true trajectory / profile of the gene . assuming @xmath65 , where @xmath66 are projection basis functions , with prior @xmath67 , we have @xmath68=\\phi(x)^te[\\mathbf{w}]=0 \\label{gpbayes_mean}\\\\ & & e[f(x)f(x)\\textprime]=\\sigma_{\\mathbf{w}}^2\\phi(x)^t\\phi(x ) \\label{gpbayes_cov}\\\\ & & f(x ) \\sim \\mathcal{gp}(0,\\sigma_{\\mathbf{w}}^2\\phi(x)^t\\phi(x)).\\end{aligned}\\ ] ] since observations are noisy , i.e. @xmath69 , with @xmath70 , assuming that the noise @xmath71 and using eq .",
    "( [ gpbayes_mean])-([gpbayes_cov ] ) , the marginal likelihood @xmath72 becomes @xmath73 with @xmath74 .    in this framework",
    "the hypothesis testing problem can be reformulated , over the perturbation interval @xmath75 $ ] , as : @xmath76 against @xmath77    the marginal likelihood derived from eq .",
    "( [ gpmarginal ] ) , enables then to compare or rank different models by calculating the bayes factor ( bf ) .",
    "more specifically the bf is approximated with a log - ratio of marginal likelihoods of two gps , each one representing the hypothesis of differential ( the profile has a significant underlying signal ) and non differential expression ( there is no underlying signal in the profile , just random noise ) .",
    "the significance of the profiles is then assessed based on the bf .      in this section",
    "we briefly summarise the approach proposed in @xcite to test the hypothesis ( [ h0_fadtest ] ) when the observed data are realisations of the curves at finite grids and possibly corrupted by noise .",
    "their motivating application is a diffusion tensor imaging study , where the objective is to compare white matter track profiles between healthy individuals and multiple sclerosis patients .",
    "they introduce a novel framework based on functional principal component analysis ( @xmath78 ) of an appropriate mixture process , referred to as marginal @xmath78 . the statistical framework for this problem assumes to observe data arising from two groups , namely @xmath79 and @xmath80 , where @xmath81 , a compact interval that in our case is t = [ 0,1 ] ( time plays the role of perturbation level @xmath36 ) .",
    "it is assumed that the @xmath82 and the @xmath83 are independent realisations of two underlying processes observed with noise on a finite grid of points :    @xmath84    where @xmath85 and @xmath86 are independent and square integrable random functions over @xmath3 , for some underlying ( latent ) random processes @xmath51 and @xmath52 .",
    "it is assumed that @xmath51 and @xmath52 are second - order stochastic processes with mean functions assumed to be continuous and covariance functions assumed to be continuous and positive semidefinite , both being unknown .",
    "the measurement errors @xmath87 and @xmath88 are independent and identically distributed ( @xmath89 ) , with zero mean and variances @xmath90 and @xmath91 , respectively .",
    "the authors exploit the truncated karhunen - lo@xmath92ve expansion of the mixture process @xmath93 of @xmath94 and @xmath95 with mixture probabilities @xmath36 and @xmath96 .",
    "let @xmath97 a binary random variable taking values in @xmath98 with @xmath99 , then @xmath100 $ ] and @xmath101 $ ] .",
    "let us consider the truncated karhunen - lo@xmath92ve expansion of @xmath93 and define @xmath102 , @xmath103 , testing hypothesis ( [ h0_fadtest ] ) reduce to testing if the @xmath104 scores @xmath105 and @xmath106 have the same distribution : @xmath107 in practice the authors consider @xmath22 null hypothesis given the finite truncation level and propose a multiple two - sample univariate test , the @xmath108-@xmath109 ( @xmath110 ) statistic @xcite , combined with a multiple - comparison adjustment .",
    "the authors propose a bonferroni correction , a procedure which controls the probability of erroneously rejecting even one of the true null hypotheses , the family wise error rate ( fwer ) . in this case hypothesis ( [ h0k ] )",
    "is rejected if @xmath111 where @xmath112 is the p - value that is obtained by using the chosen univariate two - sample test for each @xmath113 .",
    "the false discovery rate ( fdr ) , suggested in @xcite is a different point of view for how the errors in multiple testing could be considered .",
    "the fdr is the expected proportion of erroneous rejections among all rejections . if all tested hypotheses are true , controlling the fdr controls the traditional fwer .",
    "but when many of the tested hypotheses are rejected , indicating that many hypotheses are not true , the error from a single erroneous rejection is not always as crucial for drawing conclusions from the family tested , and the proportion of errors is controlled instead . using the individual testing statistics proposed in @xcite we will therefore adopt this fdr approach to adjust our tests for multiplicity .",
    "note that this procedure is designed for a more general framework in which the two curves @xmath50 and @xmath34 can be observed at different time points ( i.e. @xmath114 $ ] ) .      in the following",
    "we will briefly review the interval - wise functional testing procedure ( @xmath115 ) proposed by @xcite , where the authors develop a non - parametric domain - selective inferential methodology for functional data embedded in the @xmath116 space ( where @xmath3 is any limited open interval of @xmath117 ) to test ( [ h0_fadtest ] ) .",
    "their technique is not only able to assess the equality in distribution between functional populations , but also to point out specific differences .",
    "they propose a procedure based on the following three steps :    1 .",
    "basis expansion : functional data are projected on a functional basis ( i.e. fourier or b - splines expansion ) ; 2 .",
    "interval - wise testing : statistical tests are performed on each interval of basis coefficients ; 3 .   multiple correction : for each component of the basis expansion , an adjusted p - value is computed from the p - values of the tests performed in the previous step .",
    "more in detail , let us assume to observe two independent samples of sizes @xmath118 and @xmath119 of independent random functions on a separable hilbert space @xmath120 , @xmath121 , @xmath122 .    in the first step ,",
    "data are projected on a finite - dimension subspace generated by a reduced basis @xmath123 , where integer @xmath36 represents the dimension .",
    "it follows that each of the @xmath124 units can be represented by means of the corresponding p coefficients @xmath125 , @xmath126 ; moreover , for each @xmath127 , @xmath128 , @xmath129 and @xmath130 , @xmath131 are independent , and @xmath132 and @xmath133 where @xmath134 and @xmath135 denote the ( unknown ) distributions of the @xmath127th basis coefficient in the two populations .    in the second step ,",
    "the authors build a family of multivariate tests for @xmath136 @xmath137 and @xmath138 is a vector of successive indexes in @xmath139 .",
    "in addition they add the multivariate tests on the complementary sets of each interval , i.e. , they do also test each hypothesis @xmath140 .",
    "the tests are performed by the nonparametric combination procedure ( npc ) , see @xcite , that constructs multivariate permutation tests by means of combining univariate - synchronized permutation tests .    in the third step",
    "they obtain the adjusted p - value for the @xmath127th component @xmath141 by computing the maximum over all p - values of interval - wise tests whose null hypothesis implies @xmath142 : @xmath143 and prove that , if we reject the @xmath127th adjusted p - value @xmath144 , then , for any interval @xmath138 s.t .",
    "@xmath142 is true @xmath145 , the probability of rejecting any @xmath142 is lower or equal to @xmath146 .",
    "this property reads interval - wise control of the fwer .      mimicking the approach proposed by @xcite and @xcite , we restrict our perturbed networks to having the same numbers of vertices and edges as the original unperturbed network , hence only the positions of the edges change . in other words",
    "we apply a degree preserving randomization .",
    "our perturbation strategy relies on the @xmath147 function belonging to the @xmath148 package @xmath149 , using the option @xmath150 .",
    "moreover , we expect that a network perturbed by only a small amount has just a few edges moved in different communities , while a maximally perturbed network produces completely random clusters . in @xcite the perturbation strategy is achieved by removing each edge with a certain probability @xmath146 and replacing it with another edge between a pair of vertex @xmath151 chosen at random with a probability proportional to the degree of @xmath152 and @xmath153 .",
    "our perturbation strategy consists in randomly rewiring a percentage @xmath36 of edges while preserving the original graph s degree distribution .",
    "the rewiring algorithm indeed chooses two arbitrary edges in each step ( e.g. @xmath154 and @xmath155 ) and substitutes them with @xmath156 and @xmath157 , if they do not already exists in the graph .",
    "the algorithm does not create multiple edges .",
    "a null percentage of permutation @xmath46 corresponds to the original unperturbed graph , while @xmath47 corresponds to the maximal perturbation level .",
    "varying the percentage @xmath36 from @xmath158 ( original graph ) to @xmath159 ( maximal perturbation ) , many perturbed graph are generated and compared to the partition on the original graph by means of @xmath50 .",
    "indeed we generated @xmath48 perturbed graph for each different level of @xmath53 $ ] .",
    "then , from each of the obtained graphs , we generated other @xmath48 graphs rewiring @xmath160 of edges each time .",
    "hence resulting in 100 graphs for each level of @xmath53 $ ] . in our setting",
    "we chose @xmath161 levels of @xmath36 .",
    "the overall procedure proposed in the present paper was implemented in @xmath148 and validated both on simulated and real networks as will be described in the following sections [ sd ] and [ rd ] . in figure",
    "[ map ] we provide a flow chart that summarises our procedure . for each of the analysed networks ( either simulated or real ) we performed the first step of the overall procedure described in section [ vi ] , using some tools embedded in the @xmath148 package @xmath149 .",
    "we chose @xmath149 because it provides an implementation of graph algorithms able to fast identifying community structures in large graphs .",
    "in particular we used two community extraction functions , one based on a greedy optimisation of the modularity ( @xmath162 ) and another based on a multi - level optimisation of the modularity ( @xmath163 ) .",
    "more specifically @xmath162 implements the hierarchical agglomeration algorithm for detecting community structure described in @xcite and @xmath163 is based on the hierarchical approach proposed in @xcite .",
    "both these methods enables for an automatic definition of the optimal number of communities , are specific for large networks and are based on the optimisation of the modularity .",
    "they are briefly summarised in the following .    as regards the testing methodologies we used the bioconductor package gprege available at + https://www.bioconductor.org/packages/release/bioc/html/gprege.html for the gp regression , the r code from the professor staicu s web - site + http://www4.stat.ncsu.edu/~staicu/ for the functional principal component test and the r package fdatest available at + https://cran.r-project.org/web/packages/fdatest/index.html for the interval - wise functional test .",
    "@xmath164 @xmath165 is the modularity optimization algorithm introduced by clauset , newman and moore @xcite .",
    "this method is essentially a fast implementation of a previous technique proposed by newman @xcite . starting from a set of isolated nodes ,",
    "the links of the original graph are iteratively added such to produce the largest possible increase of the modularity .",
    "adding a first edge to the set of disconnected vertices reduces the number of groups forming a new partition of the graph .",
    "the edge is chosen such that this partition gives the maximum increase ( minimum decrease ) of modularity with respect to the previous configuration .",
    "all other edges are added based on the same principle . at each iteration step , the variation of modularity given by the merger of any two communities of the running partition is computed and the best merger chosen .",
    "the fast version of clauset , newman and moore , which uses more efficient data structures , has a complexity of @xmath166 on sparse graphs .",
    "@xmath167 method is the fast modularity optimization by blondel et al .",
    "this technique consists of two steps , executed alternatively .",
    "initially , each node is in its own community . in step 1 ,",
    "nodes are considered one by one , and each one is placed in the neighbouring community ( including its own ) that maximizes the modularity gain .",
    "this is repeated until no node is moved ( the obtained decomposition provides therefore a local optimization of newman - girvan modularity ) . after a partition is identified in this way , in step 2 communities",
    "are replaced by super - nodes , yielding a smaller weighted network where two super - nodes are connected if there is at least an edge between vertices of the corresponding communities .",
    "the two steps of the algorithm are then repeated until modularity ( which is always computed with respect to the original graph ) does not increase any further .    as pointed out in @xcite",
    ", this method offers a fair compromise between the accuracy of the estimate of the modularity maximum , which is better than that delivered by greedy techniques like the one by clauset et al . above , and computational complexity , which is essentially linear in the number of links of the graph .      in order to show the ability of our method to validate a network clustering",
    ", we applied it to modular random network graphs generated using the model implemented in @xcite .",
    "the model generates undirected , simple , connected graphs with prescribed degree sequences and a specified level of community structure , while maintaining a graph structure that is otherwise as random ( uncorrelated ) as possible over a broad range of distributions of network degree and community size . the model in @xcite",
    "is specified by the network size , the average network degree , the number of modules , the modularity , the degree distribution and the module size distribution .",
    "the generated graph results also to be as random as possible , to contain no self loops ( edges connecting a node to itself ) , multi - edges ( multiple edges between a pair of nodes ) , isolate nodes ( nodes with no edges ) , or disconnected components ( see @xcite for details ) .",
    "specifically , we generated a modular random graph for each level of modularity q = 0 , 0.2 , 0.4 , 0.6 , 0.8 , using a power law for degree distribution and for module size distribution , with size=2000 , number of modules=10 and average degree=10 .",
    "for each graph , the corresponding null model was generated using the configuration model .",
    "+ the application of the overall procedure on the simulated datasets is summarised in tables [ bfsimul ] and [ fadsimu ] and in figures [ fig : vifastsimu ] and [ fig : vilouvainsimu ] , respectively .",
    "the application of the gaussian processes approach described in section [ sec : gp ] to the simulated networks is summarised in table [ bfsimul ] .",
    ".gp bayes factor on the simulated networks with modularity q @xmath168 after clustering via fast greedy and louvain [ cols=\"<,^,^\",options=\"header \" , ]      the application of the interval - wise testing procedure described in section [ sec : ift ] to the real datasets after clustering via @xmath169 or @xmath170 are depicted respectively in figures [ fig : vifastreal ] and [ fig : vilouvainreal ] .    in each figure",
    ", panels @xmath171 show the vi curves for the null model ( @xmath35 ) and for the actual model ( @xmath34 ) . in all the cases the two curves appear to be very close for",
    "high perturbation values and depart from each other as perturbation level approaches zero . in panels",
    "@xmath172 this is quantified locally by a specific adjusted p - value in each sub - interval .",
    "also in this case significant p - values are falling under the horizontal red line corresponding to the critical value of 0.05 .",
    "as expected , either using @xmath173 or @xmath174 @xmath165 as clustering methods yields to similar results conclusion .",
    "as already observed for the synthetic datasets , if we strongly perturb a network ( @xmath175 , i.e. we rewire more than @xmath176 of edges ) it approaches a random network , indeed the two @xmath50 curves become very close , and the p - value could survive the threshold .",
    "+   +     +   +     +   +     +",
    "in this paper we propose an effective procedure to evaluate the robustness of a clustering . given a community detection method and a network of interest , our methodology enables to clearly detect if the community structure found by some algorithms is statistically significant or is a result of chance , permitting to examine the stability of the partition recovered .",
    "as suggested in @xcite , we specify a perturbation strategy and a null model to build a set of procedures based on vi as stability measure .",
    "this enables to build the vi curve as a function of the perturbation percentage and to compare it with the corresponding null model curve in the functional data analysis framework .",
    "we point out that our methodology could also be used to compare different clustering methodologies , indeed given two clusterings on the same network , we could test the agreement between the two recovered partitions via the direct comparison of the corresponding vi curves as defined by our procedure in section [ vi ] .",
    "for example , all the three procedures we used point out that @xmath170 method is able to recover a non random clustering also at low modularity ( @xmath177 ) , the other way around @xmath174 @xmath165 needs a more defined structure ( @xmath178 ) .    however , it is out of the scope of the present paper the comparative evaluation of different community extraction methods .",
    "the two methodologies @xmath170 and @xmath174 @xmath165 were indeed only instrumental to the exemplification of our procedure . both of them were selected at this stage as they both enables for an automatic definition of the optimal number of communities and are based on the optimisation of the modularity , that plays a key role in describing community structures .",
    "an interesting and straightforward extension of the current paper would be using a different clustering stability measure , for example the @xmath179 @xmath180 @xmath181 measure proposed in @xcite .",
    "this would also lead to a comparison of the performance of different measures for community structure comparison .",
    "harenberg s. , bello g. , gjeltema l. , ranshous s. , harlalka j. , seay r. , padmanabhan k. and samatova n. ( 2014 ) . community detection in large - scale networks : a survey and empirical evaluation .",
    "_ wires comput stat_. * 6 * 426439 .",
    "( 2014 ) .",
    "pomann , g .-",
    "staicu , a .- m . and",
    "ghosh , s. a two - sample distribution - free test for functional data with application to a diffusion tensor imaging study of multiple sclerosis .",
    "_ journal of the royal statistical society : series c ( applied statistics)_. * 65 * 395414 .",
    "doi : 10.1111/rssc.12130 .",
    "( 2016 ) .",
    "shiwei s. , lunjiang l. , nan z. , guojie l. and runsheng c. topological structure analysis of the protein - protein interaction network in budding yeast .",
    "_ nucleic acids research_. * 31(9 ) * 24432450 .",
    "( 2003 )          wilson j.d . , wang s. , mucha p.j .",
    ", bhamidi s. and nobel a.b . a testing based extraction algorithm for identifying significant communities in networks . _",
    "the annals of applied statistics_. * 8,3 * 18531891 . mr3271356 ."
  ],
  "abstract_text": [
    "<S> the large amount of work on community detection and its applications leaves unaddressed one important question : the statistical validation of the results . in this paper </S>",
    "<S> we present a methodology able to clearly detect if the community structure found by some algorithms is statistically significant or is a result of chance , merely due to edge positions in the network . given a community detection method and a network of interest </S>",
    "<S> , our proposal examines the stability of the partition recovered against random perturbations of the original graph structure . to address this issue </S>",
    "<S> , we specify a perturbation strategy and a null model to build a set of procedures based on a special measure of clustering distance , namely variation of information , using tools set up for functional data analysis . </S>",
    "<S> the procedures determine whether the obtained clustering departs significantly from the null model . </S>",
    "<S> this strongly supports the robustness against perturbation of the algorithm used to identify the community structure . </S>",
    "<S> we show the results obtained with the proposed technique on simulated and real datasets . _ </S>",
    "<S> keywords : _ community detection , networks , variation of information , multiple testing    # 1    1    1    0    1    * validation of community robustness * </S>"
  ]
}