{
  "article_text": [
    "astronomy is afloat with data and an estimate shows that by 2020 more than 60 pb of archived data will be electronically accessible to astronomers .",
    "but the complete analysis of the whole accumulated data distributed globally is challenging , not only due to the volume of data but also the communication cost .",
    "in general , the computational model used in astronomy is to download the data from archives to a central site and is analyzed in the local machines .",
    "hence , the network bandwidth limitations will be a hindrance for scientific discoveries and even analyzing this pb - scale on local machines in a centralized manner is challenging [ 1][2][3][28 ] .",
    "understanding the data collection rate ( e.g. large synoptic survey telescope ( lsst ) will generate @xmath0 30 tera bytes of data every night [ 4 ] ) , the centralize technique will not suffice for comprehensive co - analysis to exploit the potential of distributed archived data [ 5 ] . in this virtual observatory ( vo )",
    "is a step towards the problem , however , it does not provide the data mining model [ 6 ] . in this adding",
    "the distributed data mining layer to the vo can be the solution [ 12 ] in which the knowledge can be downloaded by the astronomers instead the raw data , and thereafter astronomers can either reconstruct the data back from the downloaded knowledge or use the knowledge directly for further analysis .",
    "astronomical data are mostly high dimensions .",
    "hence , reducing the dimensions of interrelated data will reduce the downloading cost of end users . to reduce the dimension of data , a technique called principal component analysis ( pca )",
    "are used in many fields [ 8 ] .",
    "it reduces the dimensionality of the data set of interrelated variables with retaining the variation present in the data [ 9 ] [ 10 ] .",
    "the technique is linear as its components are linear combinations of the original variables , but non - linearity is preserved in the dataset . in this paper , we use distributed load balancing principal component analysis ( dlpca ) which is a distributed version of normal pca to reduce the transmission ( cost incur for distributing the data among the computational nodes ) and downloading cost significantly from globally distributed observatories .",
    "the algorithm is scalable and also optimally distribute the computational load among the available resources .    for the experimental analysis",
    ", we took the fundamental plane ( fp ) , gadotti and complex mfeat datasets and use java agent development framework ( jade ) , which simplifies the implementation of multi - agent systems through a middle - ware and complies the foundation for intelligent physical agents specifications .",
    "the experimental analysis of our approach is done by creating the multiple agents .",
    "the local principal components are computed and communicated among them for computing the global principal component .",
    "the paper is organized as follows . in next section , we discuss the related work on the analysis of globally distributed astronomical data .",
    "section 3 briefly describe the mathematics of the pca to provide an intuitive feeling of it . in section 4 we present our algorithm for the communication efficient and scalable distributed data mining using dlpca . in section 5",
    "we present the experimental results .",
    "section 6 discusses the computational cost , load balancing and scalability of our approach .",
    "finally , section 7 contains the conclusion and future direction of the paper .",
    "big data analysis are primarily based on distribute data mining ( ddm ) to processes the heterogeneous data from databases located at different places . in literature ,",
    "various ddm techniques have been proposed for the analysis of heterogeneous data sets .",
    "these techniques differs from the centralized data mining in which the analysis is done after downloading the data to one single location .",
    "distributed data mining based on pca can be done in two ways ; either distributing the data horizontally or vertically [ 11][12 ] . a distributed pca algorithm based on the integration of local covariance matrices for the distributed databases which are horizontally partitioned",
    "is given by qi et.al [ 13 ] .",
    "if the data is vertically distributed then it is necessary that all the considered sites are associated with a unique way of matching the distributed data [ 12 ] , for e.g in astronomy , it is done using right ascension and declination ( ra , dec ) of the objects .",
    "a randomized pca is discussed by nathan halko et al . [ 14 ] , for the datasets which are too large to store in the random access memory(ram ) .",
    "astronomical research communities do data mining for large datasets e.g. f - mass [ 15 ] , the auton astro - statistics projects [ 16 ] . however , this project does not fully based on distribute data mining .",
    "a project called grid based data mining for astronomy ( grist ) [ 17 ] was the first attempts for large scale data mining in astronomy .",
    "projects in virtual observatories such as japanese virtual observatory ( jvo ) , us national virtual observatory ( nvo ) , european virtual observatory ( euro - vo ) and international virtual observatory ( ivoa ) , basically integrate and federate archive systems dispersed in a grid by standardizing xml schema , data access layer , and query language of archival data [ 17 ] . in this nvo has developed an information technology infrastructure enabling easy and robust access to distributed astronomical archives , from which users can search and gather data from multiple archives with basic statistical analysis and visualization functions .",
    "giannella et.al [ 12 ] describe the architecture of a system called distributed exploration of massive astronomy catalogs ( demac ) for distributed data mining of large astronomical catalogs .",
    "the system is designed to sit on top of the existing national virtual observatory environment to provide tools for distributed data mining without downloading the data to a centralized server .",
    "srivastava et .",
    "al [ 18 ] proposed a distributed and multi - threaded automated hierarchical density data in astronomy : from the pipeline to the virtual observatory clustering algorithm to produce computationally efficient high - quality clusters and scalable from 1 - 1024 compute - cores . for massive astronomical data analysis",
    "distributed cpu / gpu architecture is proposed to handle the data in peta scales [ 19 ] . recently a cloud based data mining system canfar+skytree is proposed at canadian astronomy data centre [ 20 ] .",
    "kargupata et .",
    "al [ 21 ] proposed the solutions for distributed clustering using collective principal component analysis .",
    "their work is mainly focused to obtain a good estimation of the global covariance matrix with a trade - off between communication cost and information loss .",
    "further , yue . et .",
    "al [ 22 ] proposed a better ddm than kargupata [ 21 ] in which one can achieve a better accuracy with the same communication cost .",
    "our proposed algorithm is communication efficient and scalable ddm based on pca to further reduce the communication cost with better accuracy which neither needs to send the local datasets to a central site nor require to reconstruct the local data for calculating the global principal components .",
    "principal component analysis is a simple non - parametric method to reduce the dimension of the datasets of possibly correlated variables into a smaller number of uncorrelated variables .",
    "the first component has maximum variability and each remaining components contains rest of the variabilities .",
    "it is abundantly used in many fields viz .",
    "astronomy , computer graphics etc . in this section",
    ", we briefly describe the mathematics of the pca to provide an intuitive feeling of it . for details mathematical illustrations the springer series in statistics  principal component analysis 2e \" by i.t .",
    "jollife is a good source [ 23 ] .",
    "suppose we have a dataset as @xmath1_{n \\times m } =   ( x_o , x_1 , x_2 , x_3 , ......... x_{l-1})\\ ] ] where @xmath2 is a @xmath3 matrix and @xmath4 is the number of columns in @xmath5    the covariance matrix of the data can be computed as    @xmath6    where , @xmath7 is the mean of the @xmath8 and @xmath9 column of the @xmath2 matrix",
    ".    the obtained covariance matrix will be a square matrix and symmetric .",
    "if the two columns data are completely uncorrelated , then the covariance will be zero .",
    "however , there may be a non - linear dependency between two variables that have zero covariance .",
    "as covariance matrix is symmetric , hence its eigenvalues and eigenvectors can be obtained by solving the equations @xmath10    where , @xmath11 is the eigenvector of eigenvalue @xmath12 and @xmath13 is the identity matrix of the same order of @xmath14 .",
    "now a matrix @xmath15 can be made consists of eigenvectors of the covariances matrices .",
    "the computed eigenvectors are ordered according to its significance . to construct the reduced dataset",
    "the least significant eigenvectors are left out and computed as + reduced dataset ( @xmath16 matrix ) = original dataset ( say @xmath17 matrix ) - mean @xmath18 reduced eigenvector matrix ( say @xmath19 matrix ) + original dataset can be computed as follows + original dataset ( @xmath17 matrix ) = reduced dataset ( @xmath16 matrix ) @xmath18 ( reduced eigenvector matrix)@xmath20 ( @xmath21 matrix)@xmath22 original mean .",
    "+   + where @xmath23 is the number of rows , @xmath24 is the number of columns and @xmath25 is the reduced number of columns .",
    "our approach is basically a communication efficient and scalable ddm for the analysis of astronomical data .",
    "the algorithm is described below in eight steps .",
    "a.   let the data is vertically partitioned among the @xmath25-sites as @xmath1_{n \\times m } =   ( x_o , x_1 , x_2 , x_3 , ......... x_{l-1})\\ ] ] where data @xmath2 is a @xmath3 matrix resides at the site @xmath26 and @xmath27 .",
    "b.   normalize locally all the columns data of each sites @xmath26 .",
    "c.   compute the covariance between all the columns of every considered sites , represented as + @xmath28 + where @xmath4 is the number of columns of the site @xmath29 .",
    "+ d.   locally find the eigenvectors and eigenvalues from the covariance matrix of each site .",
    "e.   compute the projected data from the dominant local principal components of all sites @xmath26 and send the data as follows * if the total number of sites is even , say @xmath30 , then send each @xmath26 column data to @xmath31 , where * * @xmath32 for all @xmath33 and @xmath34 , and * * @xmath35 , for all @xmath36 and @xmath37 . *",
    "if the total number of sites is odd , say @xmath38 , then send @xmath26 to @xmath31 , where * * @xmath39 for all @xmath40 and @xmath34 . + the above steps optimally balance the computational load among the available nodes . for eg .",
    "fig . 1 & fig .",
    "2 depict for even and odd number of nodes respectively .",
    "f.   compute global covariance matrix from the projected data as follows + @xmath41 .",
    "+ where @xmath42 and @xmath43 are the reduced number of columns in the @xmath44 and @xmath45 site respectively . g.   using the global eigenvectors project the data on global principal component axis .",
    "h.   now user can download the global eigenvectors , local eigenvectors and the computed global dominant projected data and can reconstruct the data from it for the scientific discoveries as discussed in the previous section .            *",
    "input : * data @xmath2 of all the sites @xmath26 + * output : * global pc s    compute @xmath46 mean of all columns of @xmath2 data    compute the covariance matrix @xmath47 where , @xmath7 is the mean of the @xmath8 and @xmath9 column of the the @xmath2 matrix .",
    "compute eigenvectors @xmath48 where , @xmath49 is the eigenvector of eigenvalue @xmath12 and @xmath13 is the identity matrix of the same order of @xmath14    compute principal components @xmath50 = [ @xmath51^t$ ] * @xmath52^t$ ] ] @xmath20 k = j+s and send @xmath50 of @xmath26 to @xmath31 k=(j+s)%2r and send @xmath50 of @xmath26 to @xmath31 k=(j+s)%(2r+1 ) and send @xmath50 of @xmath26 to @xmath31 compute the cross covariances @xmath41 . and",
    "then global covariance matrix @xmath53 compute the global eigenvectors @xmath54 where , @xmath55 is the eigenvector of eigenvalue @xmath12 and @xmath13 is the identity matrix of the same order of @xmath56 and project the data on global pc s",
    "experimental analysis of the proposed algorithm are done with the fundamental plane data ( 3 columns and 224 rows ) [ 24 ] , gadotti data ( 7 columns and 946 rows ) [ 25 ] and the complex mfeat data ( distributed in 6 files having 649 columns and 2000 rows ) [ 26 ] .",
    "the analysis is focused on two major aspects of the distributed computational environment :    * reduction in transmission cost among the computational nodes by wisely applying the concept of pca and the results are compared with qe et .",
    "al and yue et .",
    "al [ 13 ] [ 22 ] * reduction in downloading cost to the end user , so that the hindrance of the network bandwidth can be minimized for the scientific discoveries .      in astronomy",
    "finding the correlation between the observed quantities plays an important role because it can explain the formations / evolutions of these astronomical objects and can also give a method to measure different quantities .",
    "the fundamental plane ( fp ) is a linear relationship between the effective radius ( @xmath57 ) , the average surface brightness within the effective radius ( @xmath58 ) and the velocity dispersion ( @xmath59 ) of normal elliptical galaxies .",
    "hence , from the measured quantities viz . @xmath58 and @xmath59",
    ", one can find the approximated value of @xmath57 , a difficult task in observational astronomy .    to test our approach and the developed code , we recomputed the known fp data [ 24 ] , by computing all the three pcs and cross verified with the online iucaa vo observatory [ 27 ] and found that its lie in the same plane ( fig.3 ) . also , the computed pcs by our method is same as the given in the iucaa vo observatory .",
    "we reconstructed the fp data with two dominant pc and found that it almost same as the original data ( fig.4 ) , hence reduces the downloading cost @xmath033% .",
    "in subsequent subsection , we discuss the errors between the dominant local pcs and reduced global pcs computed with our algorithm by taking gadotti data .              to study the error in the global pcs and the reduction in the downloading cost for the end users , we took gadotti data which consists of 930 rows and seven columns .",
    "for the analysis , we computed the global pcs from the different combination of the considered local pcs.(table 1 ) the estimated error between the actual global pcs and computed global pcs by dlpca are shown in figure ( fig.5 ) .",
    "we found that the error in the global pcs reduces significantly and after ( 2,3 ) combination the error is negligible .",
    "[ @xmath60 for ( 2,3 ) and @xmath61 for @xmath62 .",
    "therefore , it will suffice to reconstruct the original data by taking only the five local dominant pcs which will reduce the downloading cost by @xmath63 . the complete trade - off in error of the taken pcs and the downloading cost",
    "is shown in the fig.5 . from analysis",
    "we find that the global pc1 error is less compared to other pcs , this is basically due to mean of the first column data is very high compared to other six column data ( the mean of the respective columns data are 20.195 , 0.070 , 1.484 , 0.072 , 2.999 , 0.473 , 0.4728    [ ]    [ !",
    "htb ]       to study the performance of our approach to a complex data , we took mfeat data which consists of 2000 rows and are distributed in six data files as follows [ 26 ] :    1 .",
    "mfeat - fac : 216 profile correlations ; 2 .",
    "mfeat - fou : 76 fourier coefficients of the character ; 3 .",
    "mfeat - kar : 64 karhunen  love coefficients ; 4 .",
    "mfeat - mor : 6 morphological features ; 5 .",
    "mfeat - pix : 240 pixel averages in 2 x 3 windows ; 6 .",
    "mfeat - zer : 47 zernike moments .",
    "following our method described in section 4.1 we computed the transmission cost versus the accuracy i.e. angle between the actual and global dominant pcs ( fig .",
    "12 -15 ) . for the purpose , we first computed the pcs variance of all the six datasets ( fig .",
    "6 - 11 ) and studied the various combinations of the dominant pcs ( table 2 ) to find the best combination among them based on their variances .    from the computed pc variance",
    ", we observed that after top 15 , 6 , 10 , 1 , 20 , 10 pcs of fac , fou , kar , mor , pix , zer respectively the variance in pcs are almost negligible .",
    "hence , following our approach we calculated the transmission cost versus the angle between the actual and four global dominant pcs and compared with qi et .",
    "12 - 15 ) .",
    "the transmission cost among the computational nodes is calculated as follows .",
    "@xmath64 + where @xmath65 is the number of transfers of @xmath66 computational node data to other computational nodes [ fig.1 & fig.2 ] , @xmath4 is the number of reduced columns of the @xmath66 site and @xmath67 is the number of rows of @xmath66 site which is common to all the sites .",
    "our algorithm outperforms qi et .",
    "al . in the transmission cost",
    "where as the accuracy is less than 0.1% else it is more or less same .",
    "however for pc4 transmission cost with the accuracy is always less than qi.et .",
    "al . [ 13 ]    if the local pc s are not distributed among the computational nodes then our approach outperforms ( fig .",
    "17 - 19 ) qi et .",
    "al . with the exception of pc1 ( fig .",
    "16 ) when compared with yue .",
    "the less accuracy compared to yue .",
    "al may be due to the complex nature of the data .",
    "[ ]                                                            [ ]",
    "we assumed that the data are distributed ( vertically partitioned ) , represented as @xmath68 , where @xmath67 and @xmath4 are the numbers of rows and columns of the @xmath44 site .",
    "at every site , the first column contains the same source locations ( ra , dec ) .",
    "+    for simplicity , let us assume that all the sites have same computational resources and @xmath69 be the time required to compute the covariance between two columns of any considered site .",
    "therefore , total time requires for computing the covariances between columns of all the considered sites can be written as    @xmath70    now , the total computational cost for cross site covariances of all the sites can be given as    @xmath71    where , @xmath72 are the reduced number of columns of the @xmath44 and @xmath45 site .",
    "now the communication / transmission cost among the nodes is @xmath73 + where @xmath65 is the number of transfers of jth computational node data to other computational nodes [ fig.1 & fig.2 ] , @xmath4 is the number of reduced columns of jth site and @xmath67 is the number of rows of jth site which is common to all the sites , @xmath74 is the communication cost to send one column data from on site to another",
    ".    therefore , the total computational cost of dlpca is    @xmath75    by considering @xmath76 as unit cost    @xmath77    @xmath78 where , @xmath79",
    "we proposed a communication efficient and scalable ddm using dlpca for downloading the astronomical data by the end user stored at different observatories .",
    "the algorithm uses distributed load balancing pca to reduce the transmission cost among the computational nodes and downloading cost with negligible loss in the information . in our approach",
    "the number of transfers of the pcs is half of the number of sites ( section 4 , fig . 1 and 2 ) i.e.",
    "it is not required that all the sites should have the pcs of all the other sites but in qi .",
    "the local pcs are sent to one centralized site and then the global pc s are estimated and yue .",
    "approach does not address the load balance among the sites .",
    "also , our approach is scalable i.e. one can easily add any number of new observatory data.the computational load for the computation of cross - site covariances is optimally distributed among the computational resources of the observatories . if @xmath80 is the number of final columns to be downloaded by the end user then our downloading cost is @xmath81 by neglecting the cost of global and local eigenvectors .",
    "the results also show that transmission cost between the computational nodes is less than the approach of yue et .",
    "al [ 22 ] .",
    "our experimental test data analysis shows that downloading cost will be reduced by @xmath0 33% for fp data , @xmath0 27% for gadotti data and",
    "@xmath0 90% for mfeat data with a good accuracy of the reconstructed data .",
    "the reduction in cost will be more depends on how much end users can afford the loss in information and the volume of data he / she supposes to download .",
    "dlpca is not only applicable to astronomical data but non - astronomical data also .",
    "the experimental analysis is done using fundamental plane , gadotti which are astronomical and mfeat which is non - astronomical . in future",
    ", we will do analysis taking in account of latency , bandwidth , processing speed and memory efficiency . in this",
    ", we understand that the local computation of the cross - site covariance can be made more efficient by using alternate computational frameworks like general purpose graphics processing unit .",
    "we are thankful to prof .",
    "kembhavi , prof .",
    "dipankar bhatacharya , iucaa , pune and r.k .",
    "roul , jessica periera bits , pilani , k.k .",
    "birla goa campus for the useful discussions and valuable suggestions .",
    "the authors are extremely thankful to the anonymous referee for his hard work in making the detailed suggestions .",
    "00            kanishka bhaduri , kamalika das , kirk borne et al .",
    "scalable , asynchronous , distributed eigen - monitoring of astronomy data streams _ proceedings of the 2009 siam international conference on data mining_. pp 247 - 258 .",
    "yan - xia zhang , a - li luo , yong - heng zhao .",
    "outlier detection in astronomical data .",
    "_ in proc .",
    "spie 5493 , optimizing scientific return for astronomy through information technologies _ ,",
    "521 ( september 16 , 2004 ) .",
    "hairong qi , tsei - wei wang , j douglas birdwell , global principal component analysis for dimensionality reduction in distributed data mining university of tennessee knoxville , crc press , 2003 , p. 324 - 337 .",
    "yue - fei guo , xiaodong lin , zhouu teng , xiangyang xue , jianping fan , a coariance - free iterative algorithm for distributed principal component analysis on vertically partitioned data , pattern recognition ( 3 ) : 1211 - 1219 ( 12 ) .",
    "gadotti , dimitri a. _ structural properties of pseudo - bulges , classical bulges and elliptical galaxies : a sloan digital sky survey perspective _ , monthly notices of the royal astronomical society , volume 393 , issue 4 , pp",
    ". 1531 - 1552 .",
    "daniel grosu , anthony t. chronopoulos ming - ying leung _ load balancing in distributed systems : an approach using cooperative games _ parallel and distributed processing symposium . , proceedings international , ipdps 2002"
  ],
  "abstract_text": [
    "<S> in 2020 , @xmath0 60pb of archived data will be accessible to the astronomers . but to analyze such a paramount data will be a challenging task . </S>",
    "<S> this is basically due to the computational model used to download the data from complex geographically distributed archives to a central site and then analyzing it in the local systems . </S>",
    "<S> because the data has to be downloaded to the central site , the network bw limitation will be a hindrance for the scientific discoveries . </S>",
    "<S> also analyzing this pb - scale on local machines in a centralized manner is challenging . in this virtual observatory </S>",
    "<S> is a step towards this problem , however , it does not provide the data mining model . adding </S>",
    "<S> the distributed data mining layer to the vo can be the solution in which the knowledge can be downloaded by the astronomers instead the raw data and thereafter astronomers can either reconstruct the data back from the downloaded knowledge or use the knowledge directly for further analysis.therefore , in this paper , we present distributed load balancing principal component analysis for optimally distributing the computation among the available nodes to minimize the transmission cost and downloading cost for the end user . </S>",
    "<S> the experimental analysis is done with fundamental plane(fp ) data , gadotti data and complex mfeat data . in terms of transmission cost , </S>",
    "<S> our approach performs better than qi . </S>",
    "<S> et al . and yue.et al . </S>",
    "<S> the analysis shows that with the complex mfeat data @xmath0 90% downloading cost can be reduced for the end user with the negligible loss in accuracy .    : _ distributed data mining , astronomical data , pca , load balancing _ </S>"
  ]
}