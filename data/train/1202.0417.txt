{
  "article_text": [
    "consider communication over a channel which has a general probabilistic structure .",
    "in other words , the infinite length output @xmath0 depends on the infinite length input @xmath1 through an arbitrary vector - wise probability function @xmath2 , which is unknown to the transmitter and the receiver .",
    "particular cases of such a channel include any unknown functional relation between the input and output sequences , as well as arbitrarily varying channels @xcite and channels with an individual state sequence considered in @xcite@xcite@xcite . in the current paper , an attempt is made to keep the model as general as possible , i.e. minimize any assumptions on @xmath3 , except for causality .",
    "without feedback , communication over such a channel is limited , as the communication rate , and the codebook would have to be selected in advance .",
    "therefore , the existence of a reliable feedback link is assumed .",
    "two traditional models which relate to particular cases of the current problem are the the arbitrarily varying channel ( avc ) model @xcite and the compound finite state channel ( compound - fsc ) model @xcite . in the avc model ,",
    "the channel is assumed to be controlled by a sequence of states which is arbitrary and unknown to the transmitter and the receiver . in the compound channel model ,",
    "the channel is assumed to be arbitrarily selected from a family of possible channels . in both models ,",
    "the capacity is the maximum rate of reliable communication that can be guaranteed .",
    "both models do not give a satisfying answer to the current problem .",
    "the fundamental reason is that these models focus on capacity , i.e. before knowing the channel , one is required to find a rate of reliable transmission which can be guaranteed a - priori .",
    "clearly , if the channel is completely general , the compound / avc capacity is zero , as it is possible , for example , that a channel with zero capacity will be selected . in both models",
    "mentioned , constraints on the family of channels , or on the possible state sequences need to be defined , and these constraints do not seem suitable for natural channels .",
    "in addition to this fundamental gap , the models considered under the avc and compound - fsc frameworks are quite limited , in a way that does not seem to capture the possible complexity of an unknown natural channel .",
    "for example , most papers on avc consider only memoryless channels , and the compound - fsc is stationary .    using feedback , the communication rate can be adapted , so that one does not have to commit to a communication rate a - priori .",
    "several works by us and other authors considered the gains from such adaptation using particular cases of this channel @xcite@xcite@xcite@xcite . the first question to ask is , how the target communication rate should be defined ?",
    "the sought rate @xmath4 can be a function of the channel , but should be universally attainable without prior knowledge of the channel , and should have an operational meaning . put simply , one would like to have a `` universal modem '' which can be connected over any channel , and would attain rates , which , may not be optimal , but would at least be justifiable and will not make one regret for not modeling the channel and using a modem optimized for the channel .    in a previous paper @xcite ,",
    "the problem of determining such a communication rate was addressed .",
    "this problem is similar to the source coding problem of setting a compression rate for an individual sequence . following the spirit of the `` finite state compressibility '' of lempel and ziv @xcite , we proposed to set as a target , the best rate that can be reliably attained by a system employing finite block encoding ( successively ) over the infinite channel .",
    "the supremum of these rates is termed the iterative - finite - block ( ifb ) capacity and denoted @xmath5 .",
    "as in the universal source coding problem , due to the richness of the model family , there is a large gap between the performance that can be attained universally and the performance that can be attained when knowing the specific model , and without any other constraints .",
    "this gap requires limiting the abilities of the reference system  ( require that it be a finite state , or a finite block system ) . in general , the shannon capacity @xcite of the channel , @xmath6 is not attainable universally with feedback , when the channel is unknown .",
    "this is exemplified in @xcite through the simple example of the modulo - additive channel with an unknown noise sequence , where the shannon capacity of each channel individually is positive ( the logarithm of the alphabet size ) , while the maximum reliable communication rate that can be guaranteed a - priori is zero .",
    "this motivates considering the ifb capacity as an alternate goal .",
    "when the channel is stationary and ergodic , then the ifb capacity equals the shannon capacity .",
    "there are many ways @xcite to extend the concept of ifb capacity , but for the sake of simplicity this paper focuses on this basic model .",
    "it is easy to see that the ifb capacity is not universally achievable for completely general models .",
    "the counter example in @xcite is of a family consisting of only two binary channels , termed `` password '' channels , where the first input bit @xmath7 determines whether the channel becomes `` good '' or `` bad '' for eternity , and where the values of @xmath7 matching each state are opposite in the two channels .",
    "there is no way for the universal system to correctly guess @xmath7 with high probability .",
    "the conclusion is that the ifb capacity is not universally attainable for some channels with infinite memory . on the other hand ,",
    "the ifb capacity was shown to be asymptotically attainable for the class of modulo - additive channels with an individual , unknown noise sequence . in this case , it was further shown , that the ifb capacity is related to the finite state compressibility of the noise sequence , and the scheme attaining it uses the lempel - ziv source encoder @xcite to generate decoding metrics .",
    "the result in @xcite relies crucially on two properties of the modulo additive channel :    1 .",
    "the channel is memoryless with respect to the input @xmath8 ( i.e. current behavior is not affected by previous values of the input ) .",
    "the capacity achieving prior is fixed for any noise sequence .    to avoid these assumptions it is required to address the memory of the channel and the setting of the communication prior .",
    "the second limitation , raises the question , how the input distribution should be adapted , if the channel changes arbitrarily over time ?",
    "this question was the center of @xcite , where universal prediction methods were used to set the communication prior .",
    "the focus of that paper is on channels which are memoryless in the input , and therefore can be defined by an unknown sequence of memoryless channels @xmath9 .",
    "it is shown there that the capacity of the time - averaged channel @xmath10 can be universally attained using feedback and common randomness , and that this value is the maximum rate that can be achieved universally and does not depend on the order of the channels in the sequence .",
    "the notion of universality used in @xcite is different and weaker than the ifb universality , since the rate is only compared with other rates that could have been universally attained .    in the current paper ,",
    "ideas from @xcite and @xcite are combined to generalize the previous results .",
    "it is shown that the ifb capacity is asymptotically universally attainable for any channel with a fading memory , i.e. where the effect of the channel history on the far future is vanishing . in this sense ,",
    "the two assumptions used in the previous paper @xcite are avoided as much as possible , and minimum assumptions are made on the channel .",
    "considering channels where memory of the past is not necessarily fading , it may still be possible to communicate universally over the channel , if it is not maliciously designed like the password channel described above .",
    "the advantage of the ifb reference class which enables it to win over any universal system is its ability to determine such a codebook that will not only enable reliable transmission , but will also keep the channel in a favorable state , whereas the universal system does not know , at least initially , the long term effects of certain input symbols or distributions . an alternative formulation is proposed , where the reference system is crippled , so that it can not enjoy the ability to shape the past : the encoder and decoder operate over finite blocks , however the error probability is required to be small in the worst case channel state ( history ) prior to each block , and average over blocks .",
    "this models a situation where the reference encoder and decoder are `` thrown '' each time into a different location in time , where the past state might have been arbitrary .",
    "it is not required to have good performance in each of these events , but only on average .",
    "this alternative reference system is termed `` arbitrary - finite - block '' ( afb ) and the same universal system is shown to asymptotically approach the respective afb capacity , without requiring that the channel memory is fading .",
    "this reference class is less natural than the ifb , yet it enables releasing constraints on the channel .",
    "although the current result is purely theoretical , it supplies motivation for using competitive universality in communication .",
    "vectors are denoted by boldface letters .",
    "sub - vectors are defined by superscripts and subscripts : @xmath11 $ ] and equals the empty string if @xmath12 , and @xmath13 . for a vector or random variable @xmath14 , @xmath15 }",
    "\\defeq \\vr x_{(i-1)k + 1}^{(i-1)k + k}$ ] denotes the @xmath16-th block of length @xmath17 in the vector . for brevity , vectors with similar ranges",
    "are sometimes joined together , for example , the notation @xmath18 is used instead of @xmath19 . exponents and logs are base 2 .",
    "random variables are distinguished from their sample values by capital letters .",
    "@xmath20 denotes the mutual information obtained when using a prior @xmath21 over a channel @xmath22 , i.e. it is the mutual information @xmath23 between two random variables with the joint probability @xmath24 .",
    "@xmath25 denotes the channel capacity @xmath26 .",
    "most of the definitions below are identical to the ones in the previous paper @xcite and are repeated here for completeness .",
    "let @xmath27 and @xmath28 be infinite sequences denoting the input and the output respectively , where each letter is chosen from over the alphabets @xmath29 . throughout the current paper",
    "the input and output alphabets are assumed to be finite .",
    "a channel @xmath3 is defined through the probabilistic relations @xmath30 for @xmath31 .",
    "a finite length output sequence is considered in order to make the probability well defined .",
    "sometimes , this probability will be informally referred to as @xmath32 , and should be understood as the sequence of the above distributions , or their limit for @xmath33 .",
    "[ def : causal_ch ] the channel defined by @xmath34 is termed _ causal _ if for all @xmath35 : @xmath36    all the definitions below ( including ifb / afb capacity ) pertain to causal channels .",
    "[ def : fading_memory_ch ] the channel is termed a _ fading memory channel _ if for any @xmath37 there exists @xmath38 and a sequence of causal conditional vector distribution functions @xmath39 , such that for all @xmath35 and @xmath40 : @xmath41 where the @xmath42 norm is calculated over @xmath43 , and defined @xmath44    ( 263.62 , 124.72)(0,0 ) ( 0,0 ) an illustration of the fading memory condition ( definition  [ def : fading_memory_ch]).,title=\"fig : \" ] ( 5.67,87.99)@xmath14 ( 111.97,113.50)n - l ( 157.32,113.50)n ( 5.67,59.64)@xmath45 ( 42.52,21.37)- condition required at both sides of ( 42.52,7.20)- conditioning weakly affects probability ( 42.52,35.55)- part on which probability is evaluated in    ( 263.62 , 124.72)(0,0 ) ( 0,0 ) an illustration of the fading memory condition ( definition  [ def : fading_memory_ch]).,title=\"fig : \" ] ( 5.67,87.99)@xmath14 ( 111.97,113.50)n - l ( 157.32,113.50)n ( 5.67,59.64)@xmath45 ( 42.52,21.37)- condition required at both sides of ( 42.52,7.20)- conditioning weakly affects probability ( 42.52,35.55)- part on which probability is evaluated in    the difference between the two sides of is that @xmath46 does not include @xmath47 and so the fading memory condition asserts that the dependence of the conditional distribution of future outputs , on the channel state at the far past , decays .",
    "see fig.[fig : illustration_fading_mem ] .",
    "notice that the conditional distribution @xmath48 is completely defined by the channel , since it is conditioned on the entire input @xmath1 . on the other hand , the conditional distribution @xmath49 may depend also on the input distribution ( through the unspecified symbols @xmath50 ) .",
    "therefore , the distribution @xmath46 in definition  [ def : fading_memory_ch ] is not identical to @xmath49 .",
    "on the other hand , proposition  [ prop : fading_memory_properties ] shows that @xmath49 obtained with any input distribution yields a legitimate @xmath46 .",
    "the fading memory condition does not imply stationarity or ergodicity .",
    "the memoryless arbitrary varying channel model considered in @xcite is fading memory , and so are the fsc @xcite or compound - fsc models @xcite , if the underlying fsc is indecomposable . in section  [ sec : fading_mem_example ] an example of a non - homogeneous finite state channel with fading memory is presented .",
    "the following definitions lead to the definition of ifb and afb capacity .",
    "[ def : e_and_d ] a finite length encoder @xmath51 with block length @xmath17 and a rate @xmath52 is a mapping @xmath53 from a set of @xmath54 messages to a set of input sequences @xmath55 .",
    "a respective finite length decoder @xmath56 is a mapping @xmath57 from the set of output sequences to the set of messages .",
    "[ def : mean_eps ] the _ average error probability in iterative mapping _ of the @xmath17 length encoder @xmath51 and decoder @xmath56 to @xmath58 blocks over the channel @xmath3 is defined as follows : @xmath58 messages are chosen as i.i.d .",
    "uniformly distributed random variables @xmath59 .",
    "the channel input is set to @xmath15 } = e(\\msg_i ) , i \\in   \\{1,\\ldots , m\\}$ ] , and the decoded message is @xmath60})$ ] where @xmath45 is the channel output .",
    "the iterative mapping is illustrated in fig.[fig : iterative_mapping ] .",
    "the average error probability is @xmath61 .",
    "[ def : mean_eps_afb ] the _ average error probability in arbitrary mapping _ of the @xmath17 length encoder @xmath51 and decoder @xmath56 to @xmath58 blocks over the channel @xmath3 is defined as follows : the message @xmath62 is chosen as a uniformly distributed random variable @xmath63 .",
    "for each block separately , let : @xmath64 } ) \\neq \\msg \\big| \\vr x_i^{[k ] } = e(\\msg ) , ( \\vr x \\vr y)_1^{(i-1)k } \\right\\}\\end{gathered}\\ ] ] the average error probability is @xmath65 .",
    "[ def : ifb_rate ] a rate @xmath52 is _ iterated - finite - block ( ifb ) / arbitrary - finite - block ( afb ) achievable _ over the channel @xmath3 , if for any @xmath66 there exist @xmath67 such that for any @xmath68 there exist an encoder @xmath51 and a decoder @xmath56 with block length @xmath17 and rate @xmath52 for which the average error probability in iterative / arbitrary mapping ( resp . ) of @xmath69 to @xmath58 blocks is at most @xmath70 .    note that this is equivalent to stating that the @xmath71 of the average error probability with respect to @xmath58 is at most @xmath70 .",
    "[ def : ifb_capacity ] the _ ifb / afb capacity _ of the channel @xmath3 is the supremum of the set of ifb / afb achievable rates , and is denoted @xmath5/@xmath72 ( resp . ) .",
    "notice that by definition , the afb average error probability is at least as large as the ifb average error probability , and as a result , the afb capacity is smaller than , or equal to the ifb capacity .",
    "( 140 , 60 ) ( 10,55)(30,0)5(0,-1)5 ( 12,53)@xmath73 ( 42,53)@xmath74 ( 72,53)@xmath75 ( 102,53)@xmath76 ( 132,53)@xmath77 ( 0,50)(30,0)5(1,0)20(20,50)(30,0)5(0,-1)10 ( 20,40)(30,0)5(-1,0)20(0,40)(30,0)5(0,1)10 ( 3,44)(30,0)5encoder ( 1,40)(2,0)10(0,-1)5 ( 31,40)(2,0)10(0,-1)5 ( 61,40)(2,0)10(0,-1)5 ( 91,40)(2,0)10(0,-1)5 ( 121,40)(2,0)10(0,-1)5 ( 0,35)(1,0)140(140,35)(0,-1)10(140,25)(-1,0)140(0,25)(0,1)10 ( 60,29)channel ( 1,30)@xmath78 ( 125,30)@xmath79 ( 1,25)(2,0)10(0,-1)5 ( 31,25)(2,0)10(0,-1)5 ( 61,25)(2,0)10(0,-1)5 ( 91,25)(2,0)10(0,-1)5 ( 121,25)(2,0)10(0,-1)5 ( 0,20)(30,0)5(1,0)20(20,20)(30,0)5(0,-1)10 ( 20,10)(30,0)5(-1,0)20(0,10)(30,0)5(0,1)10 ( 3,14)(30,0)5decoder ( 10,10)(30,0)5(0,-1)5 ( 12,5)@xmath80 ( 42,5)@xmath81 ( 72,5)@xmath82 ( 102,5)@xmath83 ( 132,5)@xmath84      ( 140 , 30 ) ( 23,16)encoder(20,10)(1,0)20(40,10)(0,1)15(40,25)(-1,0)20(20,25)(0,-1)15 ( 63,25)channel ( 58,18)@xmath85 ( 55,14)(1,0)30(85,14)(0,1)15(85,29)(-1,0)30(55,29)(0,-1)15 ( 103,16)decoder(100,10)(1,0)20(120,10)(0,1)15(120,25)(-1,0)20(100,25)(0,-1)15 ( 0,17.5)(1,0)20(6,18.5)@xmath86(0,14)(message ) ( 40,21.5)(1,0)15(42,22.5)@xmath87 ( 85,21.5)(1,0)15(87,22.5)@xmath88 ( 100,12)(-1,0)60(55,7)@xmath89 ( feedback ) ( 120,21.5)(1,0)20(125,22.5)@xmath52 ( rate ) ( 120,15)(1,0)20(125,16)@xmath90 ( message ) ( 30,5)(0,1)5(30,0)@xmath91 ( common randomness ) ( 110,5)(0,1)5(110,0)@xmath91    in the following , the properties of the adaptive system with feedback , and ifb / afb - universality are defined . a randomized rate - adaptive block encoder and decoder pair for block length @xmath35 with feedback",
    "is defined as in our previous paper @xcite : the encoder is presented with a message expressed by an infinite bit sequence , and following the reception of @xmath35 symbols , the decoder announces the achieved rate @xmath52 , and decodes the first @xmath92 bits .",
    "an error means any of these bits differs from the bits of the original message sequence .",
    "both encoder and decoder have access to a random variable @xmath91 ( the common randomness ) distributed over a chosen alphabet .",
    "the system is illustrated in fig .",
    "[ fig : system_adaptive ] .",
    "the following definition states formally the notion of ifb / afb - universality for rate adaptive systems :    [ def : ifbafb_universality ] with respect to a set of channels @xmath93 ( not necessarily finite or countable ) , a rate - adaptive communication system ( possibly using feedback and common randomness ) is called _ ifb / afb universal _ if for every channel in the family and any @xmath94 there is @xmath35 large enough so that when the system is operated over @xmath35 channel uses , then in probability @xmath95 , the message is correctly decoded and the rate is at least @xmath96 or @xmath97 ( resp . ) .      [ theorem : universal_w_memory_achievability ] for any @xmath66 there exists a sequence of adaptive rate systems with feedback and common randomness for growing values of @xmath98 , such that with probability of at least @xmath95 the message is received correctly with a rate of : @xmath99 \\geq \\max \\left [ \\cifb - \\delta^{{\\scriptscriptstyle \\mathrm{ifb}}}_n , \\cafb - \\delta^{{\\scriptscriptstyle \\mathrm{afb}}}_n \\right ] , \\ ] ] over @xmath98 symbols , where @xmath100 for any causal channel , and @xmath101 for any causal fading memory channel . furthermore , this rate can be attained with any positive rate of the feedback link .",
    "this implies that the system is ifb universal over the set of causal fading memory channels , and afb universal over the set of causal channels , according to definition  [ def : ifbafb_universality ] .",
    "the system does not depend on the channel , but the convergence rate of @xmath102 does .",
    "in @xcite we described a communication scheme for adapting the prior over an arbitrarily varying channel which is memoryless in the input . combining theorem  3 and lemma  9 of @xcite yields :    [ lemma : prior_prediction_channel_with_memory ] for every @xmath103 there exists @xmath104 and a constant @xmath105 , such that for any @xmath106 there is an adaptive rate system with feedback and common randomness , such that for any channel @xmath107 :    1 .",
    "the probability of error is at most @xmath70 2 .",
    "the rate satisfies @xmath108 with probability at least @xmath109 3 .   @xmath110    where @xmath111    the universal communication scheme for attaining the claims of theorem  [ theorem : universal_w_memory_achievability ] is as follows : the infinite time is divided into epochs of increasing length , numbered @xmath112 .",
    "in the first epoch , the scheme of @xcite is operated over @xmath113 symbols . in the second epoch ,",
    "the channel inputs and outputs are joined into pairs , i.e. super - symbols of dimension @xmath114 , and the scheme is operated over @xmath115 such super - symbols . in epoch @xmath58",
    ", the scheme is operated over @xmath116 super - symbols of dimension @xmath117 .",
    "since all @xmath116 are finite , the dimension of the super - symbols used grows indefinitely .",
    "the scheme of lemma  [ lemma : prior_prediction_channel_with_memory ] is a finite horizon scheme , i.e. @xmath35 has to be set in advance , there is no guarantee on the rate at the middle of an epoch . due to this technical limitation ,",
    "the universal scheme proposed here is also of a finite horizon , i.e. we specify in advance the symbol @xmath98 in which the system s performance is to be measured .",
    "it is clear from the construction of the scheme that this limitation is minor .",
    "the parameters of the scheme are chosen as follows .",
    "let @xmath66 be a parameter of choice .",
    "choose any @xmath118 , and let @xmath119 .",
    "the length of the @xmath58-th epoch , @xmath116 , is chosen such that :    1 .",
    "it is equal to or larger than the value of @xmath104 given by lemma  [ lemma : prior_prediction_channel_with_memory ] where the parameters @xmath120 of the lemma are both chosen to be equal to @xmath121 .",
    "the value of @xmath122 given by lemma  [ lemma : prior_prediction_channel_with_memory ] for @xmath123 is not larger than the chosen value for @xmath122 .",
    "if the end of the next epoch @xmath124 would occur beyond symbol @xmath98 , then the current epoch @xmath116 is extended to reach symbol @xmath98 .",
    "the last requirement makes sure that there is no more than a constant loss from capacity per epoch , while the dimension of the super - symbol of each epoch is growing , and therefore the loss normalized by the number of symbols tends to @xmath125 .",
    "the values of @xmath126 and @xmath70 guarantee that the overall probability of error is not larger than @xmath127 and similarly the overall probability that at any epoch the rate falls below the rate guaranteed by the lemma is at most @xmath128 . this way",
    "the overall probability of error or falling below the guaranteed rate is at most @xmath70 .",
    "note that the epoch durations @xmath116 are fixed and do not depend on the message or received signal .",
    "see fig.[fig : illustration_univ_epochs ] .",
    "the scheme does not need to know the ifb / afb block length , rate and error probability , and the exact relation between @xmath129 given by the fading memory condition .",
    "its only parameters are the input and output alphabets , the number of symbols @xmath98 , and @xmath70 .    the claim of theorem  [ theorem : universal_w_memory_achievability ] , that any positive feedback rate is sufficient , simply follows from the fact @xcite that this is true for the scheme of lemma  [ lemma : prior_prediction_channel_with_memory ] .",
    "( 247.08 , 102.14)(0,0 ) ( 0,0 ) an illustration of the division into epochs and super - symbols in the universal scheme.,title=\"fig : \" ] ( 14.89,38.46 ) ( 19.17,88.70 ) ( 76.72,72.82)super - symbols ( 48.94,88.70 ) ( 108.46,88.70 ) ( 185.85,88.70 ) ( 128.31,11.31)@xmath130 ( 31.08,31.15 ) ( 88.62,29.17 ) ( 175.93,31.15 )    ( 247.08 , 102.14)(0,0 ) ( 0,0 ) an illustration of the division into epochs and super - symbols in the universal scheme.,title=\"fig : \" ] ( 14.89,38.46 ) ( 19.17,88.70 ) ( 76.72,72.82)super - symbols ( 48.94,88.70 ) ( 108.46,88.70 ) ( 185.85,88.70 ) ( 128.31,11.31)@xmath130 ( 31.08,31.15 ) ( 88.62,29.17 ) ( 175.93,31.15 )",
    "the proof relies on the following principles :    * because the ifb / afb system uses the same encoding and decoding over multiple blocks , its rate and error probability can be related ( bounded ) using the capacity of a `` collapsed '' channel generated by randomly drawing the location on which the data is transmitted ( block or super - symbol ) * the operation of the reference system is considered in a channel composed of super - imposed super - symbols , to obtain a lower bound for the capacity of this averaged channel .",
    "* for the fading memory case , choosing a small @xmath131 , and throwing away the first @xmath38 symbols of each super - symbol , the obtained channel has a weak dependence on the state before the super - symbol .",
    "this enables relating the channel behavior when the reference system and alternatively the universal system , operate on it .",
    "* overheads related to alignment between the reference system blocks and the universal system super - symbols , the channel memory @xmath38 , and other overheads related to the proof technique , vanish as the super - symbol length @xmath132 tends to infinity .",
    "this implies that the system starts being effective only when @xmath133 .",
    "following is the outline of the proof .",
    "the value @xmath134 appearing in the definition of @xmath135 is the probability of a certain output symbol to appear given a certain input symbol at time @xmath16 , where the history of the channel @xmath136 attains the specific value that occurred during the universal system s operation .",
    "@xmath134 is a random variable and depends both on the channel and on the universal communication system behavior . as a result ,",
    "the rate @xmath137 guaranteed by lemma  [ lemma : prior_prediction_channel_with_memory ] is also a random variable and depends on the joint input - output distribution induced by the universal communication scheme .",
    "this rate is termed `` subjective '' since it would be different had a different scheme operated on the same channel .",
    "the baseline for comparison with the reference system is the `` pessimistic average channel capacity '' , obtained by replacing the history @xmath136 by an arbitrary state , and taking the worst - case state sequence ( worst case history ) , i.e. the one that yields the minimum capacity .",
    "the rate attained by the universal system ( for a particular state sequence ) would be at least as large . for super - symbols",
    ", the averaged channel relates to the joint distribution over the super - symbol , where the state @xmath138 refers to the input and output sequences before the start of the super - symbol .",
    "the universal system is shown to asymptotically attain a rate which is at least the weighted average of the pessimistic average channel capacities measured over the epochs ( proposition  [ prop : scheme_asymp_guarantee ] ) .",
    "next , the reference system with block size @xmath17 is compared to the universal system during epoch @xmath58 , where the super - symbol length is @xmath139 .",
    "consider a set of super - symbols in hops of @xmath17 ( @xmath140 ) .",
    "since the number of symbols between the start of two successive super - symbols in each of these `` alignment '' sets divides by @xmath17 , in each of these super - symbols , the reference system s blocks and the super - symbols align , i.e. the ifb / afb blocks begin at the same location with respect to the beginning of the super - symbol  ( see fig.[fig : block_and_supersymbol_alignment ] ) .",
    "therefore , there is an equivalence between the average error probability of the reference system over these super - symbols , and the error probability that would be attained for the `` collapsed '' channel , generated by randomly and uniformly drawing one of the super - symbols in the set and operating the reference system over this channel .",
    "due to this equivalence , the reference system s rate , for a given average error probability , is limited by the capacity of the `` collapsed '' channel .    for the ifb case ,",
    "this `` collapsed ''",
    "channel is induced not only by the channel law , but also by the behavior of the reference system in previous blocks .",
    "when replacing the collapsed channel with a similar channel , where the history @xmath138 before each super - symbol is forced to a specific value , then due to the fading memory assumption , from some point in the block onward , the two channels become similar ( in @xmath42 sense ) . due to this similarity",
    ", the increase in error probability , when exchanging the original `` collapsed channel '' with the new one , is small  ( lemma  [ lemma : l1_error_deterioration ] ) .",
    "the new channel is not `` subjective '' , i.e. it is only a function of the channel @xmath3 and not of the system operating over it . for the afb case ,",
    "this transition is not needed , as the desired relation stems immediately from the definition .    using a variant of fano s inequality ,",
    "the rate of the ifb / afb system is related to the capacity of the pessimistic average channel measured over each of the @xmath17 alignment sets of super - symbols  .",
    "the pessimistic average channel over the epoch , is the average of the @xmath17 average channels measured over the alignment sets .",
    "averaging @xmath17 channels may induce a loss of at most @xmath141 in capacity  ( lemma  [ lemma : mixing_capacities ] ) .",
    "this results in a bound on the pessimistic average channel during each epoch , as a function of the ifb / afb capacity , and the ifb / afb error probability during the epoch .",
    "note that at this stage , the error probability of the reference system can not be dismissed as being small , since only the average ( over growing intervals in time ) is guaranteed to be small .",
    "taking the weighted average of the pessimistic capacities over the epochs enables relating the rate of the universal system to the rate and the average error probability of the reference system , where the latter tends to zero .",
    "all overheads , such as the ones related to alignment of the blocks to the super - symbols , the time it takes the channel memory to fade , the @xmath141 penalty for mixing @xmath17 channels , vanish asymptotically as the super - symbol length increases indefinitely with time .",
    "additional notation required for the proof is defined below .",
    "the proof compares a situation where the reference ( ifb ) system operates on the channel to the universal system operating on the same channel .",
    "although the channels are the same , the joint distribution of the input and the output is different due to the different encoders . the channel input and outputs when the universal system operates are denoted by @xmath142 , while @xmath143 denote the channel inputs and outputs when the reference system operates .",
    "since both systems operate on the same channel the conditional distribution is the same , i.e. @xmath144    the following symbols have constant meaning throughout the proof .",
    "@xmath58 denotes the index an epoch , and @xmath132 denotes the dimension of the super - symbol , which is a function of @xmath58 ( @xmath139 ) .",
    "@xmath17 denotes the block length of the reference system .",
    "@xmath98 denotes the overall number of symbols and @xmath145 denotes the overall number of epochs .",
    "the following simple conclusions follow from the definitions of the causal and the fading memory channel .    regarding definition  [ def : causal_ch ] of a causal channel , note that the same holds for marginal distributions of @xmath146 ( e.g. the distribution of @xmath147 ) as is easily shown by summation over .",
    "another consequence of definition  [ def : causal_ch ] is that for @xmath148 , the following conditional distribution can also be given as a function of a finite input : @xmath149    two simple consequences of definition  [ def : fading_memory_ch ] ( fading memory channel ) are given below .",
    "the proof is simple and deferred to appendix  [ sec : proof_fading_memory_properties ] .",
    "[ prop : fading_memory_properties ] for a causal fading memory channel , the following holds    1 .   if holds for a certain @xmath58 , then it holds for any smaller @xmath40 .",
    "this implies that only needs to be established for @xmath58 `` large enough '' .",
    "2 .   for any input distribution and for any @xmath150 , @xmath151 in other words",
    ", the property applies when @xmath46 is replaced with the true probability @xmath152 , obtained with any input distribution .",
    "the rate @xmath137 is subjective in the sense that it depends on the joint input - output distribution induced by the universal communication scheme , and would be different had a different scheme operated on the same channel .    in the following",
    ", a lower rate is defined , but such that is a function of the channel alone .",
    "let @xmath153}$ ] denote the subjective average channel over @xmath35 super - symbols of dimension @xmath132 : @xmath154}(\\vr y^q",
    "| \\vr x^q ) = \\frac{1}{n } \\sum_{i=1}^n \\pr \\left ( \\vr y_i^{[q ] } = \\vr y | \\vr x_i^{[q ] } = \\vr x , ( \\vr x \\vr y)^{(i-1)q } \\right ) .\\ ] ] this channel is termed subjective since it depends on the specific input - output distribution induced by the universal scheme when operating on the channel ( which is different , in general , from the joint distribution induced by a reference system ) .",
    "furthermore , since @xmath155 } = \\vr y | \\vr x_i^{[q ] } = \\vr x , ( \\vr x \\vr y)^{(i-1)q } \\right)$ ] is a random variable depending on the history @xmath138 , also @xmath153}$ ] is a random variable , whose distribution depends on the joint distribution induced by the scheme .",
    "also note that while the conditioning on @xmath138 represent what truly happened ( as a random variable ) , this channel is not an empirical channel , since the probability @xmath156 above represents what would have happened , hypothetically at the output , if one forced the input @xmath157 } = \\vr x$ ] .",
    "let @xmath158}$ ] denote the state before super - symbol @xmath16 , @xmath158 } = ( \\vr x \\vr y)^{(i-1)q } $ ] . as the channel is not a finite state channel , the alphabet size of @xmath158}$ ] increases with @xmath16 .",
    "consider the average channel when the history @xmath159 obtains a specific value @xmath160 : @xmath161}(\\vr y^q|\\vr x^q ; \\{s_i\\}_{i=1}^n ) = \\\\ & \\qquad \\frac{1}{n } \\sum_{i=1}^n \\pr \\left ( \\vr y_i^{[q ] } = \\vr y | \\vr x_i^{[q ] } = \\vr x , s_i^{[q ] } = s_i \\right ) .",
    "\\end{split}\\ ] ] in other words , for fixed input and output , this is the average probability to see the specific output given the specific input when the channel had been in a specific state .",
    "this is no longer a random variable , but a function of @xmath162 .",
    "the pessimistic average channel capacity is defined as the worst capacity of @xmath163}(\\cdot|\\cdot ; \\{s_i\\}_{i=1}^n)$ ] for any state sequence . @xmath164 } = \\inf_{\\{s_i\\}_{i=1}^n } c \\left ( \\overline w^{[q]}(\\vr y|\\vr x ; \\{s_i\\}_{i=1}^n ) \\right ) .\\ ] ] note that in taking the minimum , does not require that the state sequence satisfies the natural constraint given by the recursion @xmath165 , i.e. it is allowed to include so - called `` contradictions '' . by definition , this rate lower bounds the capacity of the subjective averaged channel : @xmath166}(\\vr y^q |",
    "\\vr x^q ) \\right ) & = c \\left ( \\overline w^{[q]}(\\vr y|\\vr x ;",
    "\\{s_i\\}_{i=1}^n ) \\right ) \\big|_{s_i = s_i } \\\\ & \\geq \\inf_{\\{s_i\\}_{i=1}^n } c \\left ( \\overline w^{[q]}(\\vr y|\\vr x ; \\{s_i\\}_{i=1}^n ) \\right ) \\\\ & = c_{{\\scriptscriptstyle \\mathrm{pma}}}^{[q ] } .",
    "\\end{split}\\ ] ]    since in each epoch the universal scheme asymptotically attains the capacity of @xmath153}$ ] ( measured over the epoch ) it also attains @xmath167}$ ] .",
    "the next proposition maintains that if it is guaranteed that the normalized pessimistic capacity , is asymptotically on average above some rate @xmath168 then the scheme will asymptotically approach the rate @xmath168 .",
    "the pessimistic capacity with super - symbol @xmath132 measured over epoch @xmath58 is denoted @xmath169}$ ] .",
    "[ prop : scheme_asymp_guarantee ] if for each epoch @xmath58 with super - symbol length @xmath170 the pessimistic capacity satisfies : @xmath171 } \\geq c_m - \\delta_m , \\ ] ] where @xmath172 , then the rate the universal scheme attains over @xmath98 symbols and @xmath145 epochs satisfies and the message is correctly decoded with probability at least @xmath95 .",
    "@xmath173 \\geq \\overline c - \\tilde { \\delta}_n , \\ ] ] where @xmath174 is the average of @xmath175 weighted by the relative epoch durations and @xmath176 .",
    "_ proof : _ by its construction and lemma  [ lemma : prior_prediction_channel_with_memory ] , in epoch @xmath58 , with probability at least @xmath177 , the scheme attains the following rate , per super - symbol : @xmath178}(\\vr y^q | \\vr",
    "x^q ) \\right ) - \\delta_c \\stackrel{\\eqref{eq:138}}{\\geq } c_{{\\scriptscriptstyle \\mathrm{pma}}}^{[q , m ] } - \\delta_c \\geq q ( c_m - \\delta_m ) .\\ ] ] the number of bits sent during this epoch is at least @xmath179 .",
    "let @xmath145 denote the number of epochs until time @xmath98 , where @xmath180 .",
    "with probability at least @xmath95 ( recall : @xmath181 ) , there is no decoding error and the rate up to time @xmath98 is at least : @xmath182 & \\geq \\frac{\\sum_{m=1}^{m } r_{m } \\cdot n_{m}}{n } \\\\ & \\stackrel{\\eqref{eq:268}}{\\geq } \\frac{1}{n } \\sum_{m=1}^{m } \\left ( 2^{m-1 } ( c_m - \\delta_m ) -",
    "\\delta_c \\right ) \\cdot n_{m } \\\\",
    "& = \\overline c - \\frac{1}{n } \\sum_{m=1}^{m } \\left (   \\delta_m + 2^{-m+1 } \\delta_c \\right ) \\cdot 2^{m-1 } \\cdot n_{m } \\\\&= \\overline c - \\delta_m ' . \\end{split}\\ ] ] where @xmath183 .",
    "the last step stems from the following simple lemma ( see appendix  [ sec : proof_of_summation_lemma ] ) :    [ lemma : summation_lemma ] for a positive , monotonic non - decreasing sequence @xmath184 and @xmath185 , @xmath186 .",
    "furthermore , the convergence is uniform over the values of @xmath187 .",
    "note that because the last epoch stretches to time @xmath98 , the coefficients @xmath188 vary as @xmath98 is increased .",
    "however , according to the lemma , it only matters that they remain monotonic and that the number of coefficients grows with @xmath98 .",
    "the next subsections relate @xmath169}$ ] to the rate obtained by the reference system for a certain error probability .",
    "the proof for the ifb and afb cases is quite similar . for the purpose of clarity ,",
    "the proof below focuses on the more complex ifb case , and at the end the modifications required for the afb case are discussed .",
    "consider the reference system composed of an encoder and a decoder operating over block size @xmath17 , and the universal system in epoch @xmath58 , with super - symbol length @xmath139 .",
    "for simplicity , as long as a single epoch is concerned , the symbols and super - symbols of the epoch are denoted by indices starting from @xmath189 ( i.e. @xmath190 or @xmath191 respectively ) . in the following , the properties of the ifb system ( such as rate and error probability )",
    "are linked to a channel averaged over super - symbols .",
    "we begin by considering the channel from the ifb system s point of view . @xmath192",
    "denote the input and output vectors during the epoch , where the joint distribution depends on the joint behavior of the ifb encoder and the channel .",
    "consider the set of super - symbols with index @xmath193 for @xmath194 , i.e. the set of super - symbols in hops of @xmath17 ( the reference block size ) starting from the @xmath195-th super - symbol .",
    "@xmath196 are not necessarily of the same size . in each of the super - symbols in a set @xmath196 , the reference system s blocks begin at the same location with respect to the beginning of the super - symbol ( see fig.[fig : block_and_supersymbol_alignment ] ) .",
    "the sets @xmath196 are termed `` alignment sets '' .",
    "( 248.00 , 210.33)(0,0 ) ( 0,0 ) the alignment of reference system blocks in the universal system s super - symbols .",
    "the large dark rectangles are the supersymbols of length @xmath132 , with the triangles denoting the first @xmath38 symbols .",
    "the light rectangles are the reference system blocks of length @xmath17 where here @xmath197 .",
    "there are three alignment sets @xmath198 . in the example , @xmath199 for @xmath200 .",
    "the blocks that are not accounted for in @xmath201 are marked with an x. the error probability @xmath202 refers to the same reference system block over different super - symbols in the alignment set .",
    "the collapsed channel is averaged across an alignment set.,title=\"fig : \" ] ( 126.31,166.66 ) ( 156.31,151.69 ) ( 67.65,128.60)@xmath203 ( 133.98,128.60)@xmath204 ( 202.01,128.60)@xmath205 ( 67.65,77.57)@xmath206 ( 133.98,77.57)@xmath207 ( 202.01,77.57)@xmath208 ( 31.06,158.15 ) ( 38.27,32.65)time ( 123.30,17.34 ) ( 204.94,30.95 ) ( 159.02,19.04)average error probability over ( 159.02,7.14)block @xmath16 of subset @xmath196 ( 56.97,199.33 ) ( 125.01,199.33 ) ( 193.04,199.33 ) ( 67.18,44.56 )    ( 248.00 , 210.33)(0,0 ) ( 0,0 ) the alignment of reference system blocks in the universal system s super - symbols .",
    "the large dark rectangles are the supersymbols of length @xmath132 , with the triangles denoting the first @xmath38 symbols .",
    "the light rectangles are the reference system blocks of length @xmath17 where here @xmath197 .",
    "there are three alignment sets @xmath198 . in the example , @xmath199 for @xmath200 .",
    "the blocks that are not accounted for in @xmath201 are marked with an x. the error probability @xmath202 refers to the same reference system block over different super - symbols in the alignment set .",
    "the collapsed channel is averaged across an alignment set.,title=\"fig : \" ] ( 126.31,166.66 ) ( 156.31,151.69 ) ( 67.65,128.60)@xmath203 ( 133.98,128.60)@xmath204 ( 202.01,128.60)@xmath205 ( 67.65,77.57)@xmath206 ( 133.98,77.57)@xmath207 ( 202.01,77.57)@xmath208 ( 31.06,158.15 ) ( 38.27,32.65)time ( 123.30,17.34 ) ( 204.94,30.95 ) ( 159.02,19.04)average error probability over ( 159.02,7.14)block @xmath16 of subset @xmath196 ( 56.97,199.33 ) ( 125.01,199.33 ) ( 193.04,199.33 ) ( 67.18,44.56 )    to use the fading memory assumption , the reference system performance is considered only over symbols @xmath38 through @xmath132 out of the @xmath132 symbols in the super - symbol . in each subset @xmath196 , consider the blocks which completely overlap with symbols @xmath38 through @xmath132 .",
    "the number of such blocks per super - symbol in the set @xmath196 is denoted @xmath201 .",
    "the number of symbols in epoch @xmath58 which are not included in any of these blocks ( for any @xmath196 ) is denoted @xmath209 , where by the above definitions : @xmath210 i.e. @xmath209 equals the total number of symbols in the epoch , minus the number of symbols covered per super - symbol , summed over the subsets .",
    "@xmath209 can be bounded from above by considering that no more than @xmath211 blocks may fully or partially overlap with the first @xmath212 symbols of any super - symbol ( see fig.[fig : blocks_lost_in_alignment ] ) , and therefore at most @xmath213 symbols per super - symbol are lost , hence @xmath214 this calculation accounts correctly for the special cases of the first and the last super - symbols in the epoch , as one may wrap the epoch around its tail , and",
    "imagine that the end of the epoch is cyclically connected to its beginning .",
    "it is convenient to normalize @xmath209 and the number of symbols in each set @xmath196 by the total number of symbols in the epoch , and look at the relative sizes : @xmath215    @xmath216    ( 225.64 , 91.29)(0,0 ) ( 0,0 ) at worst , @xmath211 blocks may fully or partially overlap with the first @xmath212 symbols of any super - symbol.,title=\"fig : \" ] ( 112.82,78.95 ) ( 58.39,26.23 ) ( 95.81,26.23 ) ( 168.95,26.23 ) ( 131.53,26.23 ) ( 22.68,26.23 ) ( 201.26,26.23 )    ( 225.64 , 91.29)(0,0 ) ( 0,0 ) at worst , @xmath211 blocks may fully or partially overlap with the first @xmath212 symbols of any super - symbol.,title=\"fig : \" ] ( 112.82,78.95 ) ( 58.39,26.23 ) ( 95.81,26.23 ) ( 168.95,26.23 ) ( 131.53,26.23 ) ( 22.68,26.23 ) ( 201.26,26.23 )    considering a specific alignment set @xmath196 , because the reference system s operation is fixed during these blocks , its average error probability can be related to the mutual information of the averaged ( collapsed ) channel .",
    "denote by @xmath217 } , \\tilde { \\vr y}_{i}^{[q]}$ ] the channel input and output of the reference system during the @xmath16-th super - symbol , and by @xmath218})_{l}^{q}$ ] the output during symbols @xmath38 to @xmath132 of the super - symbol .",
    "let @xmath219 denote a random variable generated by a uniform selection over @xmath220 of @xmath221 } , ( \\tilde { \\vr y}_{i}^{[q]})_{l}^{q})$ ] , in other words , @xmath222 } , ( \\tilde { \\vr y}_{u}^{[q]})_{l}^{q } ) , \\qquad u \\sim",
    "\\unif(b_j ) .\\ ] ]    the joint distribution of @xmath223 is : @xmath224 } = \\vr x , ( \\tilde { \\vr y}_{i}^{[q]})_{l}^{q } = \\vr y \\right\\ } .\\end{gathered}\\ ] ] because the reference system induces the same input distribution in all the super - symbols in @xmath196 , i.e. @xmath225 } = \\vr x \\right\\}$ ] is constant for all @xmath220 , the marginal distribution of @xmath226 equals the per - block distribution ( for all @xmath220 ) @xmath227 } = \\vr x \\right\\ } = \\pr   \\left\\ { \\tilde { \\vr x}_{i}^{[q ] } = \\vr x \\right\\ } , \\ ] ] and hence the conditional distribution is : @xmath228 } = \\vr x , ( \\tilde { \\vr y}_{u}^{[q]})_{l}^{q } = \\vr y \\right\\}}{\\pr   \\left\\ { \\tilde { \\vr x}_{i}^{[q ] } = \\vr x \\right\\ } } \\\\&= \\frac{1}{|b_j| } \\sum_{i \\in b_j } \\pr   \\left\\ { ( \\tilde { \\vr y}_{i}^{[q]})_{l}^{q } = \\vr y \\big| \\tilde { \\vr x}_{i}^{[q ] } = \\vr x \\right\\ } .",
    "\\end{split}\\ ] ]    denote the reference system rate by @xmath229 .",
    "consider employing the reference system over a super - symbol selected randomly and uniformly over @xmath196 , where the message is encoded in the @xmath201 blocks which are contained in symbols @xmath38 through @xmath132 of the super - symbol .",
    "denote the average error probability which is attained for the @xmath16-th block ( averaged over the channel and over all super - symbols in @xmath196 ) by @xmath202 .",
    "the average error probability over the @xmath201 blocks is denoted @xmath230 .",
    "it is convenient to define the average error rate of the ifb system in the epoch @xmath58 , @xmath231 as the sum of error probabilities over all blocks that begin in the epoch , normalized by the approximate number of blocks in the epoch @xmath232 .",
    "this error probability can be bounded as : @xmath233 where the inequality is because the summation on the right side only accounts for the error probability over the blocks that are fully contained within symbols @xmath38 to @xmath132 of any super - symbol .",
    "the random variables @xmath219 are induced by the channel and the behavior of the reference system .",
    "not only is the distribution of @xmath226 determined by the codebook distribution of the reference encoder , but the channel behavior determining @xmath234 is potentially affected by the input distribution induced by the reference encoder on all previous symbols . to account for the fading memory of the channel , and relate these variables to the ones seen by the universal system ,",
    "let us consider alternative random variable , representing an alternative , specific , channel state at the beginning of the super - symbol .",
    "for a given state sequence @xmath235 , consider the random variable @xmath236 which depends on @xmath226 through the following conditional distribution : @xmath237})_{l}^{q } = \\vr y | \\tilde { \\vr x}_{i}^{[q ] } = \\vr x , ( \\tilde { \\vr x } \\tilde { \\vr y})^{(i-1)q } = s_{i } \\right\\}\\end{gathered}\\ ] ] because the current numbering refers only to epoch @xmath58 , the notation @xmath238 formally refers to the input and output of the channel ( with the reference system ) during the epoch @xmath58 .",
    "however the meaning of @xmath238 should be understood as the input and output of the channel from the beginning of time ( potentially before epoch @xmath58 ) .",
    "also , considering that the reference encoder may not be able to emit all possible input sequences , the meaning of conditioning on @xmath239 should be understood as if the encoder was disconnected and an input value was forced into the channel . because the probability on the rhs of is conditioned on the entire past of @xmath239 , and the channel is causal",
    ", this probability does not depend on the reference system , but only on the channel .",
    "therefore , the same probability would be attained for the random variables @xmath240 representing the inputs and outputs of the channel when the universal system is applied : @xmath241})_{l}^{q } = \\vr y | { \\vr x}_{i}^{[q ] } = \\vr x , ( { \\vr x } { \\vr y})^{(i-1)q } = s_{i } \\right\\}\\end{gathered}\\ ] ]    using the fading memory assumption , it is shown below , that the error probability obtained when applying the @xmath201 blocks of the reference system to the channel defined by is not significantly worse than its performance over the channel of .",
    "let @xmath242})_{l}^{q } = \\vr y | \\tilde { \\vr x}_{i}^{[q ] } = \\vr x   \\right\\ } \\\\ & \\qquad - \\pr \\left\\ { ( \\tilde { \\vr y}_{i}^{[q]})_{l}^{q } = \\vr y | \\tilde { \\vr x}_{i}^{[q ] } = \\vr x , ( \\tilde { \\vr x } \\tilde { \\vr y})^{(i-1)q } = s_{i } \\right\\ } .",
    "\\end{split}\\ ] ] because the channel is assumed to be causal and fading memory , using proposition  [ prop : fading_memory_properties ] , for any @xmath37 there exists @xmath38 such that : @xmath243 and therefore by the triangle inequality , the difference between the two channels is bounded by : @xmath244 from the @xmath42 bound on the difference between the conditional probabilities , a bound on the increase in the error probability is easily derived as follows :    [ lemma : l1_error_deterioration ] let the error probability of a given encoder and decoder over the vector channel @xmath245 be @xmath246 for @xmath247 .",
    "if for all @xmath14 , @xmath248 then @xmath249    _ proof : _ denote by @xmath51 the event of error .",
    "then , @xmath250 where the probability of error given @xmath251 does not depend on the channel ( and for a deterministic encoder and decoder it is either @xmath125 or @xmath189 depending on whether @xmath45 belongs to the decision region of @xmath14 ) . as a result ,",
    "@xmath252    note that the lemma applies to any event whose probability is fixed as function of @xmath240 . in the following",
    ", it will be applied to the event of an error in each of the blocks separately ( while the channel is the channel over the super - symbol defined in ) .      a lower bound on the capacity of the channel @xmath253",
    "is obtained by using the fact the ifb system delivers a certain rate with a small block error probability .",
    "following is a variation of fano s inequality , which takes into account that the errors are block errors rather than full message errors .",
    "denote by @xmath254 the indicator associated with the event of error in the @xmath16-th block out of @xmath201 blocks , and @xmath255 $ ] the probability of error on this block ( over all super - symbols in @xmath196 ) , when the reference decoder is applied to the channel output @xmath236 . by applying lemma  [ lemma : l1_error_deterioration ] to the event of an error in the @xmath16-th block ,",
    "@xmath256 is obtained ( where @xmath202 is the error probability of the same block under the original channel ) . the average error probability over the blocks",
    "is denoted @xmath257 . whenever @xmath258 , then given the channel output , @xmath259 bits of the input become known , whereas when @xmath260 these bits are unknown and have entropy at most @xmath259 .",
    "denote by @xmath62 the transmitted message ( a sequence of @xmath261 bits ) and by @xmath262 the decoded message .",
    "the derivation below uses the fact conditioning reduces entropy , and the concavity of the binary entropy function @xmath263 : @xmath264   \\cdot k \\cdot r_{{\\scriptscriptstyle \\mathrm{ifb } } }   + \\sum_i h_b(\\epsilon_{ij } ' ) \\\\ & = \\sum_i \\epsilon_{ij } '   \\cdot k \\cdot r_{{\\scriptscriptstyle \\mathrm{ifb } } }   + \\sum_i h_b(\\epsilon_{ij } ' ) \\\\ & \\leq n_b(j ) \\overline \\epsilon_{j } '   \\cdot k \\cdot r_{{\\scriptscriptstyle \\mathrm{ifb } } }   + n_b(j ) h_b(\\overline \\epsilon_{j } ' ) \\\\ & = k_j \\overline \\epsilon_{j } ' + n_b(j ) h_b(\\overline \\epsilon_{j } ' ) .",
    "\\end{split}\\ ] ] using the information processing inequality , the capacity of the channel is lower bounded as follows : @xmath265 define @xmath266 as the monotone continuation of @xmath263 .",
    "@xmath267 is non decreasing and concave . then using @xmath268 : @xmath269      to connect the capacity above to the pessimistic averaged channel , the bound on the capacity of each average channel over a set @xmath196 needs to be linked with the capacity over the averaged channel over the sets .",
    "for this purpose the following simple lemma is used :    [ lemma : mixing_capacities ] let @xmath270 be a series of channels , and @xmath271 a probability distribution over the channels .",
    "then @xmath272    the right inequality is based on convexity of the mutual information with respect to the channel and the left inequality is based on the fact the difference between knowing and not knowing the index @xmath16 at the channel output is at most the entropy of this information . the simple proof is deferred to appendix  [ sec : proof_lemma_mixing_capacities ] .",
    "the averaged channel over the epoch with a specific state sequence is : @xmath273}(\\vr y^q|\\vr x^q ; \\vr s ) \\\\ = \\frac{1}{n_m }",
    "\\sum_{i=1}^{n_m } \\pr \\left\\ { \\vr y_{i}^{[q ] }   = \\vr y | { \\vr x}_{i}^{[q ] } = \\vr x , ( { \\vr x } { \\vr y})^{(i-1)q } = s_{i } \\right\\}\\end{gathered}\\ ] ] this channel s capacity is at least as large of the capacity of the next channel , where the first @xmath212 outputs are removed : @xmath274 \\setminus l-1}(\\vr y^{q - l+1}|\\vr x^q ; \\vr s ) \\\\ & = \\frac{1}{n_m } \\sum_{i=1}^{n_m } \\pr \\left\\ { ( \\vr y_{i}^{[q]})_{l}^{q } = \\vr y | { \\vr x}_{i}^{[q ] } = \\vr x , ( { \\vr x } { \\vr y})^{(i-1)q } = s_{i } \\right\\ } \\\\ & = \\frac{1}{n_m } \\sum_{j=1}^k \\sum_{i \\in b_j } \\pr \\left\\ { ( \\vr y_{i}^{[q]})_{l}^{q } = \\vr y | { \\vr x}_{i}^{[q ] } = \\vr x , ( { \\vr x } { \\vr y})^{(i-1)q } = s_{i } \\right\\ } \\\\ &",
    "\\stackrel{\\eqref{eq:463u}}{= } \\sum_{j=1}^k \\frac{|b_j|}{n_m } \\pr \\left\\ { \\tilde { \\vr y}_{s , j } = \\vr y | \\tilde { \\vr x}_{c , j } = \\vr x \\right\\ } . \\end{split}\\ ] ]    the lemma implies that @xmath275}(\\cdot | \\cdot ; \\vr s ) \\right ) \\\\ & \\geq c \\left(\\overline w^{[q]\\setminus l-1}(\\cdot |   \\cdot ; \\vr s ) \\right ) \\\\ & = c \\left ( \\sum_{j=1}^k \\frac{|b_j|}{n_m } \\pr \\left\\ { \\tilde { \\vr y}_{s , j } = \\vr y | \\tilde { \\vr x}_{c , j } = \\vr x \\right\\ } \\right ) \\\\ & \\stackrel{\\text{lemma~\\ref{lemma : mixing_capacities}}}{\\geq } \\underbrace{\\sum_{j=1}^k \\frac{|b_j|}{n_m }   c \\left ( \\pr \\left\\ { \\tilde { \\vr y}_{s , j } = \\vr y | \\tilde { \\vr x}_{c , j } = \\vr x \\right\\ } \\right)}_{c_{avg } } \\\\ & \\qquad - h \\left ( \\left\\ { \\frac{|b_j|}{n_m } \\right\\}_{j=1}^k   \\right ) . \\end{split}\\ ] ] the last term is upper bounded by @xmath276 and the first term @xmath277 is bounded by and substituting @xmath261 : @xmath278 combining , , dividing by @xmath132 and taking infimum over @xmath279 yields : @xmath280 } & = \\inf_{\\vr s } c \\left(\\overline w^{[q]}(\\cdot | \\cdot ; \\vr s ) \\right ) \\\\ & \\geq r_{{\\scriptscriptstyle \\mathrm{ifb } } } - r_{{\\scriptscriptstyle \\mathrm{ifb } } } \\cdot ( \\lambda_0 +   \\epsifb^{(m ) } + 2h ) \\\\ & \\qquad - \\frac{1}{k } h_b^\\nearrow \\left(\\frac{1}{(1-\\lambda_0 ) } \\epsifb^{(m ) } + 2h \\right )   - \\frac{\\log k}{q } . \\end{split}\\ ] ] for any @xmath281 @xmath282 can be made arbitrarily small by taking @xmath58 large enough ( equivalently @xmath132 large enough ) .",
    "taking @xmath58 large enough such that @xmath283 .",
    "thus , for @xmath58 large enough : @xmath284 } & \\geq r_{{\\scriptscriptstyle \\mathrm{ifb } } } - r_{{\\scriptscriptstyle \\mathrm{ifb } } } \\cdot ( \\lambda_0 + \\epsifb^{(m ) } + 2h ) \\\\ & \\qquad - \\frac{1}{k } h_b^\\nearrow \\left(2 \\epsifb^{(m ) } + 2h \\right ) -",
    "\\frac{\\log k}{q } .",
    "\\end{split}\\ ] ] alternatively , for all @xmath58 : @xmath285 } \\geq r_{{\\scriptscriptstyle \\mathrm{ifb } } } - \\delta_{1}^{(k , r_{{\\scriptscriptstyle \\mathrm{ifb}}})}(2 \\epsifb^{(m ) } + 2h ) - \\delta_{2m}^{(k , r_{{\\scriptscriptstyle \\mathrm{ifb } } } ) } , \\ ] ] where @xmath286 and @xmath287 is defined as the remainder , i.e. @xmath229 minus the rhs of minus @xmath288 , and by , for large enough @xmath58 , @xmath289 @xmath290 is concave in @xmath291 , tends to @xmath125 with @xmath292 and decreases with @xmath17 .",
    "@xmath293 tends to @xmath125 with @xmath58 .",
    "now , multiple epochs are considered , and proposition  [ prop : scheme_asymp_guarantee ] is applied to bound the rate of the universal scheme .",
    "suppose that over the @xmath98 symbols ( and @xmath145 epochs ) of the system s operation , the ifb system achieves rate @xmath229 with an average error probability @xmath294 .",
    "the definition of @xmath231 ( above ) results in @xmath295 where @xmath296 , the number of ifb blocks that begin in any symbol of the system s operation is upper bounded by @xmath297 and so @xmath298    choose @xmath299 and determine the respective @xmath38 to satisfy the fading memory property ( note that this choice is for the purpose of analysis , and the scheme itself is not aware of these values ) . applying proposition  [ prop : scheme_asymp_guarantee ] with @xmath300 and @xmath301 , there exists @xmath176 such that @xmath302 & \\geq \\overline c - \\delta_n \\\\ & = r_{{\\scriptscriptstyle \\mathrm{ifb } } } - \\frac{1}{n } \\sum_{m=1}^m 2^{m-1 } n_m \\delta_{1}^{(k , r_{{\\scriptscriptstyle \\mathrm{ifb}}})}(2 \\epsifb^{(m ) } + 2h ) \\\\ & \\qquad - \\tilde \\delta_n \\\\ & \\geq r_{{\\scriptscriptstyle \\mathrm{ifb } } } - \\delta_{1}^{(k , r_{{\\scriptscriptstyle \\mathrm{ifb } } } ) } \\left(\\frac{1}{n } \\sum_{m=1}^m 2^{m-1 } n_m   ( 2 \\epsifb^{(m ) } + 2h ) \\right ) \\\\ & \\qquad - \\tilde \\delta_n \\\\ & = r_{{\\scriptscriptstyle \\mathrm{ifb } } } - \\delta_{1}^{(k , r_{{\\scriptscriptstyle \\mathrm{ifb } } } ) } \\left(2 \\epsifb + 2h \\right ) - \\tilde \\delta_n \\\\ & = r_{{\\scriptscriptstyle \\mathrm{ifb } } } - \\delta_{1}^{(k , r_{{\\scriptscriptstyle \\mathrm{ifb } } } ) } \\left(4 \\epsifb \\right ) - \\tilde \\delta_n , \\end{split}\\ ] ] where the inequality is due to the concavity of @xmath290 . for an arbitrarily small @xmath303 , choose @xmath304 . by the definition of the ifb capacity ,",
    "there is a @xmath17 large enough and @xmath98 large enough so that @xmath294 can be made arbitrarily small .",
    "therefore @xmath305 can be made arbitrarily small ( note that it decreases with @xmath17 ) while @xmath306 .",
    "therefore for large enough @xmath98 , the rhs of can be made arbitrarily close to @xmath307 .",
    "this proves the ifb universality of the proposed universal system .",
    "the proof for the afb case is similar . in this section",
    "the required modifications are discussed .",
    "the same definitions of section  [ sec : proof_ifb_single_epoch ] are used for the alignment sets ( up to equation  ) , except @xmath38 is set to @xmath308 .",
    "the definition of @xmath219 is not needed , as the performance of the afb system is directly related to the constrained - state channel whose output is @xmath236 . for the arbitrary sequence of states @xmath160 ,",
    "define the channel @xmath309 according to .",
    "this channel implies that the channel history is forced to @xmath160 at the beginning of each super - symbol . in each alignment",
    "set @xmath196 the @xmath201 blocks of the afb system are mapped to this averaged channel .",
    "clearly , the error probability of the afb system when the state is forced to some value at the beginning of the super - symbol , is not worse than the error probability in arbitrary mapping defined in definition  [ def : mean_eps_afb ] , where the state is forced to its worst - case value just before the relevant block .",
    "formally , let @xmath310 denote an indicator of the event of error in the @xmath311-th block of the @xmath16-th supersymbol , a block which begins at symbol @xmath312 of the supersymbol , then when mapping to the channel where only the initial state is forced , the error probability is : @xmath313",
    "\\\\ = \\e \\left [ \\e \\left [ e_l \\big| \\substack{(\\vr x \\vr y)^{(i-1)q } = s_i , \\\\",
    "( \\vr x \\vr y)_{(i-1)q+1}^{n_l-1 } } \\right ] \\big| ( \\vr x \\vr y)^{(i-1)q } = s_i \\right]\\end{gathered}\\ ] ] where the iterated expectation law is applied .",
    "the internal expectation is by definition upper bounded by @xmath314 , the error probability in arbitrary mapping ( definition  [ def : mean_eps_afb ] ) over the same block , and therefore the error probability with the current mapping is upper bounded by the error probability in arbitrary mapping .",
    "denote as before by @xmath202 the average error probability over the @xmath16-th blocks in the @xmath196 alignment set , when the afb system is mapped to the channel @xmath309 , and the average error probability over the @xmath201 blocks by @xmath230 , now holds with respect to the average error in arbitrary mapping over the epoch , where now the inequality stems not only from the fact that not all errors are accounted for , but in addition because the error probabilities @xmath202 , @xmath315 are upper bounded by the respective errors obtained by arbitrary mapping .",
    "the transition to a modified channel ( section  [ sec : proof_modified_channel ] ) is not required in this case and the proof is continued with the value @xmath316 .",
    "the rest of the proof proceeds as before ( sections  [ sec : proof_capacity_lower_bound],[sec : proof_pma_lower_bound],[sec : proof_conclusion_ifb ] ) , where @xmath317 and @xmath318 replace @xmath319 and @xmath229 , except that in @xmath131 is chosen to be @xmath125 rather than equal @xmath320 .",
    "this concludes the proof of theorem  [ theorem : universal_w_memory_achievability ] .",
    "in the definition of a fading memory channel ( definition  [ def : fading_memory_ch ] ) , the overall probability of @xmath45 over the infinite future ( from @xmath35 to @xmath321 ) is required to be close in @xmath42 sense to a distribution that does not depend on the past .",
    "this raises the question how strict is the requirement and whether it is satisfied by broad family of channels .",
    "below , an example is given of a family of finite state channels with non - homogeneous transition probabilities that , under the assumption that there is a non - zero probability to arrive from any state to any state , satisfies the fading memory requirement .",
    "consider a finite state channel where the state at each moment in time @xmath322 belongs to the finite set @xmath323 .",
    "the probability of each output letter is given as a time - varying function of the input letter and the current state @xmath324 , and the state sequence is a non - homogeneous markov chain which depends on the input via @xmath325 .",
    "the joint probability is therefore @xmath326    if the markov chain determining the state transitions is such that , eventually , it is possible to move from any state to any state , then it is termed a indecomposable markov chain .",
    "similarly , for constant state transition and channel probabilities @xmath327 , gallager @xcite defined the resulting finite state channel as indecomposable if the memory of the initial state fades with time ( eq .",
    "( 4.6.26 ) there ) . here , for simplicity",
    ", a stricter condition is assumed : that it is possible to move from any state to any state within one step and with a certain , non - vanishing probability @xmath328 , i.e. that @xmath329 it appears that this condition can be relaxed and the results can be generalized to indecomposable markov chains , under the assumption that there is some minimum probability to arrive from any state to any state with a finite number of steps , by simply treating a block of symbols as a new super - symbol .",
    "however for simplicity let us focus on this type of channels , which is also quite general .",
    "[ prop : fading_mem_example ] any channel with the structure defined above is a causal fading memory channel .",
    "specifically , the @xmath42 distance in definition  [ def : fading_memory_ch ] is @xmath330 , i.e. fades exponentially with @xmath38 .",
    "the rest of this section is devoted to the proof of this proposition .",
    "the transition probability may be written alternatively as follows : @xmath331 where @xmath332 . due to the condition",
    ", the remainder @xmath333 is non negative , and by summing both sides of over @xmath322 it is easily seen that @xmath333 is a legitimate probability distribution .",
    "this motivates the following formulation : consider a sequence of i.i.d .",
    "bernully random variables @xmath334 , which are drawn independently of @xmath1 and of previous @xmath322-s .",
    "the next state is determined as follows . if @xmath335 then the next state is determined by @xmath336 .",
    "otherwise , it is selected uniformly with equal probabilities .",
    "this results in the same conditional probability @xmath337 due to .",
    "the fading memory property stems from the observation that whenever @xmath338 , the memory of the past disappears , and that over a long enough interval , the probability for such an event approaches one .    due to the independence of @xmath339 in the sequence @xmath14 and the previous states ,",
    "it is also independent of the past of @xmath45 , hence @xmath340 the distribution conditioned on @xmath341 is : @xmath342    focusing on the last term , it can be shown that it has a weak dependence on @xmath343 .",
    "given @xmath344 , the sequence @xmath322 remains a markov chain , therefore for any @xmath345 @xmath346 if @xmath347 then the first term is constant and independent of @xmath348 and therefore @xmath349 does not depend on @xmath343 .",
    "the same is trivially true for @xmath350 .",
    "the probability that none of @xmath341 would be @xmath189 is @xmath351 .",
    "whenever any of @xmath341 is @xmath189 , because the last term in is independent of @xmath343 , the sum in breaks into two independent sums and @xmath352 does not depend on @xmath47 .",
    "therefore , considering the summation in , it can be written as : @xmath353 where the probabilities @xmath354 are generated by splitting the sum to the single component that depends on @xmath355 and the other components that do not , and normalizing each part . from the @xmath42 distance",
    "can be bounded : @xmath356",
    "although the current result is pleasing in terms of the asymptotical rate , it is theoretical in at least two senses related to asymptotical convergence rate . first , as the `` finite state compressibility '' , the definition of the ifb capacity relies on the order of limits ",
    "i.e. one first examines the performance of a finite - block code on the _ infinite _ channel and only then lets the block length go to infinity .",
    "the second sense is that the scheme proposed here only attempts to attain the asymptotical result , and does not endeavor to be efficient in terms of convergence rate .",
    "the best convergence rate , and more efficient schemes are left for further study .",
    "there are several reasons for the scheme s inefficiency .",
    "one is the use of a single super - symbol length . due to alignment issues with the reference system s blocks this requires the supersymbol length @xmath132 to exceed the block length @xmath17 significantly .",
    "it seems better to enhance the methods of @xcite for learning communication priors over several possible @xmath17-s simultaneously .",
    "another cause for inefficiency is the fact each epoch stands on its own and the information learned from the past is reset .",
    "furthermore , in the asymptotical case one can always assume that @xmath132 eventually becomes larger than @xmath38 , the channel s effective memory length .",
    "however , in a more efficient scheme it may be desired , instead of wasting @xmath38 symbols of each super - symbol , to attempt learning and adapting to a conditional distribution which includes also the past ( e.g. estimate the average over @xmath16 of @xmath357 } | \\vr x_{i}^{[q ] } , \\vr x_{(i-1)q - l}^{(i-1)q})$ ] and set the prior accordingly ) . the rate of convergence of the prior prediction scheme of @xcite used as basis for the current universal scheme may be improved as well . on the other hand ,",
    "it seems inevitable that the overheads related to learning the decoding rule and the prior would grow at a rate which is at least linear in the super - alphabet size , i.e. exponential in the super - symbol length .",
    "a possible direction for improving asymptotical convergence rate is modifying the comparison class or the channel model . as an example , it seems , comparing the results of @xcite and @xcite regarding convergence rates , it is observed that the overheads related to learning the prior are larger than overheads of universal decoding , for the same block lengths .",
    "the difficulty of finding an input distribution to attain the ifb capacity may be exemplified by the following channel .",
    "starting with an arbitrary ifb encoder of @xmath145 codewords over block length @xmath17 , the channel is constructed to favor this encoder . for each block of @xmath17 symbols ,",
    "if the input @xmath15]}$ ] is one of the codewords , then the output @xmath359}$ ] is a deterministic sequence , and otherwise , the output is chosen randomly .",
    "this channel is a fading memory channel so the current results hold with respect to it . in order to achieve the ifb capacity ( @xmath360 ) over this channel ,",
    "the universal system is required to `` guess '' most of the codewords in the reference encoder s codebook . in view of this , one may consider as reference , encoders and decoder which operate over a block of a certain size , however their codebook distributions are close to i.i.d . or have more constrained structures ( as practical codes do ) .",
    "also the channel assumed in this paper is very general , and the penalty for this generality it does not appear in the asymptotical rates .",
    "however it surely induces a penalty in the rate of convergence .",
    "probably , the ability to efficiently learn and utilize channel behavior would come from identifying similarities and repetitive behavior of channel occurrence , rather than slow increase of the super - symbol size as done here .",
    "it seems that even for improved schemes , the transmission lengths @xmath98 required to obtain a small redundancy with respect to the performance of a reference system with blocks of size @xmath17 , would scale exponentially with @xmath17 . in view of the fact that competing systems have @xmath17-s which are at least 100 - 1000 symbols ,",
    "this raises the question : can such schemes ever become practical ?    it is natural to compare the universal communication problem with the case of universal compression using the lz algorithm , especially in view of the theoretical and practical success of this algorithm .",
    "the result @xcite showing lz asymptotically beats every finite state machine , supplies motivation for the algorithm from an engineering perspective , since all digital computation machines are eventually finite state machines .",
    "however , as in the current case , this is only theoretical .",
    "considering that a state machine with a state memory of @xmath17 bits can simply memorize an individual sequence of @xmath361 bits , then the length of the sequence is required to be larger than this value in order to surpass the performance of a @xmath17-bit state machine . bit , and if it does , the remainder of the sequence can be encoded in any uniquely decodable way ( e.g. quoting the place of deviation and the remainder of the sequence ) . ] in fact , lempel and ziv s bound ( * ? ? ? * eq.(14 ) ) would require the length of the sequence @xmath35 to scale faster than the squared number of states ( @xmath362 ) in order for the redundancy @xmath363 ( * ? ? ?",
    "* eq.(10 ) ) to vanish . in spite of this impractical asymptotical result ,",
    "the lz algorithm and newer algorithms that improve over it , work well .",
    "the reason is probably related to the fact the sequences encountered in practice are relatively simple and can be modeled by small state machines .",
    "to summarize , in the case of lz universal source coding there is a combination of an elegant scheme on one hand , a competitive universality result which is rather theoretical ( if competent competitors are considered ) , and good performance for simple models and for practical scenarios . in the communication setting presented here , we currently only have the second property , i.e. a theoretical competitive universality result .",
    "however , it does not seem unfeasible to improve the results and obtain also good convergence rates for simple models ( and perhaps a more elegant scheme ) .",
    "another aspect related to convergence rate is the amount of time and data which are reasonable for training .",
    "suppose it takes the universal system a month of communication in order to achieve optimal performance .",
    "initially this could seem a lot of time .",
    "however , the alternative process , of manually studying the channel model , coming up with simplified mathematical models , and designing systems optimized for these models , would usually take years , and will not necessarily end up with a system better matched to the actual channel . therefore it is reasonable to allow a significant amount of time for training",
    ".    there are additional practical aspects that do not exist in universal source coding .",
    "for example , one issue with the current definitions is that in competing against _",
    "static _ coding systems the universal system does not take advantage of time variations in the channel , at least not explicitly .",
    "this is not only a matter of obtaining better rates : as an example , even a small frequency offset between the oscillators of the transmitter and the receiver may turn ifb capacity into @xmath125 , as a static decoder is not able to track and correct it . on the other hand ,",
    "if we consider the tracking mechanism as something external to the encoder / decoder , this raises the question how to perform these tasks over an unknown channel .",
    "this means that models have to be improved before these systems become practical .      in the definition of fading memory ( definition  [ def : fading_memory_ch ] ) there is a conditioning on @xmath364 which is required to have a small effect .",
    "it appears , at least intuitively , that the conditioning on @xmath45 is redundant , and may be done without .",
    "after all , what the universal system does not know and the reference system does , is the effect of possible _ inputs_. therefore the definition of fading memory as @xmath365 instead of the current definition : @xmath366 seems more plausible .",
    "the first definition can be thought of as fading memory in the wide sense , or input only , while the current definition is more narrow . to give an example , consider the channel where a coin is tossed at the beginning of time ( irrespective of any input ) and chooses between two channels memoryless in the input , which will last to eternity .",
    "this channel is fading memory according to but not according to and definition  [ def : fading_memory_ch ] .",
    "it is easy to see that although this channel is ruled out by the current fading - memory requirement , it does not pose a problem for competitive universality .",
    "because the ifb system is required to deliver a given rate at a vanishing error probability , it will eventually tune to the worst channel .",
    "therefore , the universal system should not have a problem to exceed the ifb system s performance .",
    "note that in spite of the fact the channel is given as a single conditional probability , it is beneficial to treat it as an arbitrary choice between the two channels ( seemingly a worst channel , as an arbitrary choice is worse than a probabilistic one ) , and see that the ifb system would attain either the ifb capacity of the good channel or the ifb capacity of the bad channel , according to whichever was drawn .",
    "this conditioning on @xmath367 appears also in the definition of the afb capacity ( through the definition of error probability in arbitrary mapping ) .",
    "it seems unfair that we `` punish '' the afb system by considering the worst channel state , or history @xmath364 ( where @xmath368 is controlled by the channel ) , and instead it would have been sufficient and more plausible to consider the worst case input @xmath369 .    technically speaking , the conditioning on @xmath367 stemmed from the analysis of the rate of the universal scheme in ( * ? ? ?",
    "* lemma  9 ) , and is required in order to generate the martingale property which is used in the convergence analysis .",
    "once the condition appears in @xmath135 it is required everywhere .",
    "it appears that removing this conditioning would require taking several steps back compared to the techniques developed here and in @xcite .",
    "an example is that the `` collapsed channel capacity '' is no longer a useful bound : considering the example channel above , the collapsed channel is the average ( across the `` coin toss '' ) of the per - block averaged channels , whereas in order to show universality one needs to bound the reference system by the capacity of the worst channel ( over the `` coin toss '' ) .",
    "for example , if the time - averaged channels over blocks of size @xmath17 are @xmath370 and @xmath371 , and the coin is fair , then the collapsed channel capacity is @xmath372 , while the rate that can be guaranteed by the universal system is related to @xmath373 . to solve this problem ,",
    "the information density should be considered instead of the mutual information ( its average ) , and the probability of the information density to fall below the rate of the ifb system should be used as a tighter bound for error probability ( * ? ? ? * thm.4,5 ) .",
    "this may require the universal system to base its decisions on the information density .",
    "communication over an unknown causal vector channel was considered , where the channel may include memory , and may change is behavior in an arbitrary way over time .",
    "it was demonstrated , that there exists a universal system with feedback , which without knowing the channel , asymptotically attains rates meeting or exceeding the rates of any finite block encoding system operating on the same channel , where the latter system may be designed with prior knowledge of the channel .",
    "the result holds for a finite block system mapped iteratively to sequential blocks , under a condition of fading - memory in the channel , and alternatively for any channel , but where the competing finite block system is required to start - off anywhere from an arbitrary channel state .",
    "compared to other models of unknown channels where there is an explicit model , here the assumptions on the channel are minimized .",
    "this general channel model includes as special cases many models previously considered .",
    "this result marks the theoretical possibility of having a system which is not designed based on a channel model , made up by engineers , but rather learns the actual channel and automatically adapts to it .",
    "there are many theoretical and practical issues to resolve before such systems would be practical .",
    "however , similarly to the world of source coding , there is hope that universal systems would be implemented one day , and perhaps beat systems optimized under channel model assumptions .          defining for brevity @xmath377 , and using the triangle inequality @xmath378 and causality , yields : @xmath379 p_z(\\vr z ) \\bigg\\|_1 \\\\ & \\leq h + \\sum_{\\vr z }   \\big\\| \\pr(\\vr y_n^m | \\vr x_{n - l}^m ( \\vr x \\vr y)^{n - l-1 } = \\vr z ) \\\\ & \\qquad - p_n(\\vr y_n^m | \\vr x_{n - l}^m ) \\big\\|_1 \\cdot p_z(\\vr z ) \\\\ &",
    "\\leq h + \\sum_{\\vr z }   h \\cdot p_z(\\vr z ) = 2h .",
    "\\end{split}\\ ] ] regarding the last inequality , note that due to causality , conditioning on @xmath380 or @xmath381 is the same ( see section  [ sec : proof_ch_preliminaries ] ) .",
    "let @xmath382 be the channel input , @xmath383 the channel output , @xmath384 the channel index and @xmath21 an input distribution .",
    "the joint distribution is defined by @xmath385 . then @xmath386 and @xmath387 on one hand , due to the convexity of the mutual information with respect to the channel @xmath388 maximizing with respect to @xmath21 yields the right inequality of . on the other hand , @xmath389 maximizing with respect to @xmath21 yields the left inequality of .      choose an @xmath70 and find @xmath98 large enough so that for @xmath390 @xmath391 , then for @xmath390 : @xmath392 by taking @xmath35 large enough , the first term can be made arbitrarily small , and therefore the rhs can be made arbitrarily small for @xmath35 large enough .",
    "y.  lomnitz and m.  feder .",
    "( 2010 , dec . )",
    "universal communication over modulo - additive channels with an individual noise sequence . arxiv:1012.2751v1 [ cs.it ] .",
    "[ online ] .",
    "available : http://arxiv.org/abs/1012.2751"
  ],
  "abstract_text": [
    "<S> consider communication over a channel whose probabilistic model is completely unknown vector - wise and is not assumed to be stationary . </S>",
    "<S> communication over such channels is challenging because knowing the past does not indicate anything about the future . </S>",
    "<S> the existence of reliable feedback and common randomness is assumed . in a previous paper </S>",
    "<S> it was shown that the shannon capacity can not be attained , in general , if the channel is not known . </S>",
    "<S> an alternative notion of `` capacity '' was defined , which is the maximum rate of reliable communication by any block - coding system used over consecutive blocks . </S>",
    "<S> this rate was shown to be achievable for the modulo - additive channel with an individual , unknown noise sequence , and not achievable for some channels with memory . in this paper </S>",
    "<S> this `` capacity '' is shown to be achievable for general channel models possibly including memory , as long as this memory fades with time . </S>",
    "<S> in other words , there exists a system with feedback and common randomness that without knowledge of the channel , asymptotically performs as well as any block - coding system , which may be designed knowing the channel . for non - fading memory channels a weaker type of `` capacity '' </S>",
    "<S> is shown to be achievable . </S>"
  ]
}