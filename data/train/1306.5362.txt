{
  "article_text": [
    "one popular method for dealing with large - scale data sets is sampling . in this approach ,",
    "one first chooses a small portion of the full data , and then one uses this sample as a surrogate to carry out computations of interest for the full data .",
    "for example , one might randomly sample a small number of rows from an input matrix and use those rows to construct a low - rank approximation to the original matrix , or one might randomly sample a small number of constraints or variables in a regression problem and then perform a regression computation on the subproblem thereby defined .",
    "for many problems , it is very easy to construct `` worst - case '' input for which _ uniform _ random sampling will perform very poorly",
    ". motivated by this , there has been a great deal of work on developing algorithms for matrix - based machine learning and data analysis problems that construct the random sample in a _ nonuniform _",
    "data - dependent fashion  @xcite .    of particular interest here is when that data - dependent sampling process selects rows or columns from the input matrix according to a probability distribution that depends on the empirical statistical leverage scores of that matrix .",
    "this recently - developed approach of _ algorithmic leveraging _ has been applied to matrix - based problems that are of interest in large - scale data analysis , e.g. , least - squares approximation  @xcite , least absolute deviations regression  @xcite , and low - rank matrix approximation  @xcite .",
    "typically , the leverage scores are computed approximately  @xcite , or otherwise a random projection  @xcite is used to precondition by approximately uniformizing them  @xcite .",
    "a detailed discussion of this approach can be found in the recent review monograph on randomized algorithms for matrices and matrix - based data problems  @xcite .",
    "this algorithmic leveraging paradigm has already yielded impressive algorithmic benefits : by preconditioning with a high - quality numerical implementation of a hadamard - based random projection , the blendenpik code of  @xcite `` beats lapack s direct dense least - squares solver by a large margin on essentially any dense tall matrix ; '' the lsrn algorithm of @xcite preconditions with a high - quality numerical implementation of a normal random projection in order to solve large over - constrained least - squares problems on clusters with high communication cost , e.g. , on amazon elastic cloud compute clusters ; the solution to the @xmath0 regression or least absolute deviations problem as well as to quantile regression problems can be approximated for problems with billions of constraints  @xcite ; and cur - based low - rank matrix approximations  @xcite have been used for structure extraction in dna snp matrices of size thousands of individuals by hundreds of thousands of snps  @xcite . in spite of these impressive _ algorithmic _ results , none of this recent work on leveraging or leverage - based sampling addresses _ statistical _ aspects of this approach .",
    "this is in spite of the central role of statistical leverage , a traditional concept from regression diagnostics  @xcite .    in this paper , we bridge that gap by providing the first statistical analysis of the algorithmic leveraging paradigm .",
    "we do so in the context of parameter estimation in fitting linear regression models for large - scale data  where , by `` large - scale , '' we mean that the data define a high - dimensional problem in terms of sample size @xmath1 , as opposed to the dimension @xmath2 of the parameter space .",
    "although @xmath3 is the classical regime in theoretical statistics , it is a relatively new phenomenon that in practice we routinely see a sample size @xmath1 in the hundreds of thousands or millions or more .",
    "this is a size regime where sampling methods such as algorithmic leveraging are indispensable to meet computational constraints .",
    "our main theoretical contribution is to provide an analytic framework for evaluating the statistical properties of algorithmic leveraging .",
    "this involves performing a taylor series analysis around the ordinary least - squares solution to approximate the subsampling estimators as linear combinations of random sampling matrices . within this framework ,",
    "we consider biases and variances , both conditioned as well as not conditioned on the data , for several versions of the basic algorithmic leveraging procedure .",
    "we show that both leverage - based sampling and uniform sampling are unbiased to leading order ; and that while leverage - based sampling improves the `` size - scale '' of the variance , relative to uniform sampling , the presence of very small leverage scores can inflate the variance considerably .",
    "it is well - known that , from the algorithmic perspective of worst - case analysis , leverage - based sampling provides uniformly superior worst - case algorithmic results , when compared with uniform sampling .",
    "however , our statistical analysis here reveals that from the statistical perspective of bias and variance , neither leverage - based sampling nor uniform sampling dominates the  other .    based on these theoretical results ,",
    "we propose and analyze two new leveraging algorithms designed to improve upon vanilla leveraging and uniform sampling algorithms in terms of bias and variance .",
    "the first of these ( denoted slev below ) involves increasing the probability of low - leverage samples , and thus it also has the effect of `` shrinking '' the effect of large leverage scores .",
    "the second of these ( denoted levunw below ) constructs an unweighted version of the leverage - subsampled problem ; and thus for a given data set it involves solving a biased subproblem . in both cases ,",
    "we obtain the algorithmic benefits of leverage - based sampling , while achieving improved statistical performance .",
    "our main empirical contribution is to provide a detailed evaluation of the statistical properties of these algorithmic leveraging estimators on both synthetic and real data sets .",
    "these empirical results indicate that our theory is a good predictor of practical performance for both existing algorithms and our two new leveraging algorithms as well as that our two new algorithms lead to improved performance .",
    "in addition , we show that using shrinked leverage scores typically leads to improved conditional and unconditional biases and variances ; and that solving a biased subproblem typically yields improved unconditional biases and variances . by using a recently - developed algorithm of  @xcite to compute fast approximations to the statistical leverage scores , we also demonstrate a regime for large data where our shrinked leveraging procedure is better algorithmically , in the sense of computing an answer",
    "more quickly than the usual black - box least - squares solver , as well as statistically , in the sense of having smaller mean squared error than nave uniform sampling . depending on whether one is interested in results unconditional on the data ( which is more traditional from a statistical perspective ) or conditional on the data ( which is more natural from an algorithmic perspective ) , we recommend the use of slev or levunw , respectively , in the future .",
    "the remainder of this article is organized as follows .",
    "we will start in section  [ sxn : background ] with a brief review of linear models , the algorithmic leveraging approach , and related work .",
    "then , in section  [ sxn : theory ] , we will present our main theoretical results for bias and variance of leveraging estimators .",
    "this will be followed in sections  [ sxn : empirical ] and  [ sxn : moreempirical ] by a detailed empirical evaluation on a wide range of synthetic and several real data sets .",
    "then , in section  [ sxn : conc ] , we will conclude with a brief discussion of our results in a broader context .",
    "appendix  [ sxn : interlude ] will describe our results from the perspective of asymptotic relative efficiency and will consider several toy data sets that illustrate various aspects of algorithmic leveraging ; and appendix  [ sxn : app - proofs ] will provide the proofs of our main theoretical  results .",
    "in this section , we will provide a brief review of relevant background , including our notation for linear models , an overview of the algorithmic leveraging approach , and a review of related work in statistics and computer science .",
    "we start with relevant background and notation . given an @xmath4 matrix @xmath5 and an @xmath1-dimensional vector @xmath6 , the least - squares ( ls )",
    "problem is to  solve @xmath7 where @xmath8 represents the euclidean norm on @xmath9 .",
    "of interest is both a vector exactly or approximately optimizing problem  ( [ eqn : ls ] ) , as well as the value of the objective function at the optimum .",
    "using one of several related methods  @xcite , this ls problem can be solved exactly in @xmath10 time ( but , as we will discuss in section  [ sxn : background - al ] , it can be solved approximately in @xmath11 time as @xmath12 means that for every positive constant @xmath13 there exists a constant @xmath14 such that @xmath15 , for all @xmath16 . informally , this means that @xmath17 grows more slowly than @xmath18 .",
    "thus , if the running time of an algorithm is @xmath11 time , then it is asymptotically faster than any ( arbitrarily small ) constant times @xmath19 . ] ) .",
    "for example , ls can be solved using the singular value decomposition ( svd ) : the so - called _ thin svd _ of @xmath5 can be written as @xmath20 , where @xmath21 is an @xmath4 orthogonal matrix whose columns contain the left singular vectors of @xmath5 , @xmath22 is an @xmath23 orthogonal matrix whose columns contain the right singular vectors of @xmath5 , and the @xmath23 matrix @xmath24 , where @xmath25 , @xmath26 , are the singular values of @xmath5 . in this case , @xmath27 .",
    "we consider the use of ls for parameter estimation in a gaussian linear regression model .",
    "consider the model @xmath28 where @xmath6 is an @xmath29 response vector , @xmath5 is an @xmath4 _ fixed _ predictor or design matrix , @xmath30 is a @xmath31 coefficient vector , and the noise vector @xmath32 . in this case , the unknown coefficient @xmath30 can be estimated via maximum - likelihood estimation as @xmath33 in which case the predicted response vector is @xmath34 , where @xmath35 is the so - called hat matrix , which is of interest in classical regression diagnostics  @xcite .",
    "the @xmath36 diagonal element of @xmath37 , @xmath38 , where @xmath39 is the @xmath36 row of @xmath5 , is the _ statistical leverage _ of @xmath36 observation or sample .",
    "the statistical leverage scores have been used historically to quantify the extent to which an observation is an outlier  @xcite , and they will be important for our main results below . since @xmath37 can alternatively be expressed as @xmath40 , where @xmath21 is any orthogonal basis for the column space of @xmath5 , e.g. , the @xmath41 matrix from a qr decomposition or the matrix of left singular vectors from the thin svd , the leverage of the @xmath36 observation can also be expressed  as @xmath42 where @xmath43 is the @xmath36 row of @xmath21 .",
    "using eqn .",
    "( [ eqn : lev - scores ] ) , the exact computation of @xmath44 , for @xmath45 $ ] , requires @xmath10 time  @xcite ( but , as we will discuss in section  [ sxn : background - al ] , they can be approximated in @xmath11 time ) .    for an estimate @xmath46 of @xmath47 ,",
    "the mse ( mean squared error ) associated with the prediction error is defined to be @xmath48}\\\\\\notag     & = \\frac{1}{n}\\trace{{\\mbox{}{\\bf{var}}\\left[x\\hat{{\\boldsymbol}{\\beta}}\\right ] } }      + \\frac{1}{n}{\\mbox{}{\\bf{e}}\\left[(x\\hat{{\\boldsymbol}{\\beta}}- x{{\\boldsymbol}{\\beta}}_0)^t(x\\hat{{\\boldsymbol}{\\beta}}- x{{\\boldsymbol}{\\beta}}_0)\\right]}\\\\\\notag     & = \\frac{1}{n}\\trace{{\\mbox{}{\\bf{var}}\\left[x\\hat{{\\boldsymbol}{\\beta}}\\right ] } }      + \\frac{1}{n}[\\textbf{bias}(x\\hat{{\\boldsymbol}{\\beta}})]^t[\\textbf{bias}(x\\hat{{\\boldsymbol}{\\beta}})]\\end{aligned}\\ ] ] where @xmath49 is the true value of @xmath50 .",
    "the mse provides a benchmark to compare the different subsampling estimators , and we will be interested in both the bias and variance components .      here , we will review relevant work on random sampling algorithms for computing approximate solutions to the general overconstrained ls problem  @xcite .",
    "these algorithms choose ( in general , nonuniformly ) a subsample of the data , e.g. , a small number of rows of @xmath5 and the corresponding elements of @xmath6 , and then they perform ( typically weighted ) ls on the subsample .",
    "importantly , these algorithms make no assumptions on the input data @xmath5 and @xmath6 , except that  @xmath51 .",
    "a prototypical example of this approach is given by the following meta - algorithm  @xcite , which we call ` subsamplels ` , and which takes as input an @xmath4 matrix @xmath5 , where @xmath51 , a vector @xmath6 , and a probability distribution @xmath52 , and which returns as output an approximate solution @xmath53 , which is an estimate of @xmath54 of eqn .",
    "( [ eqn : lsq ] ) .    * randomly sample @xmath55 constraints , i.e. , rows of @xmath5 and the corresponding elements of @xmath6 , using @xmath52 as an importance sampling distribution . *",
    "rescale each sampled row / element by @xmath56 to form a weighted ls subproblem . *",
    "solve the weighted ls subproblem , formally given in eqn .",
    "( [ lsq - sample ] ) below , and then return the solution @xmath53 .    it is convenient to describe ` subsamplels ` in terms of a random `` sampling matrix '' @xmath57 and a random diagonal `` rescaling matrix '' ( or `` reweighting matrix '' ) @xmath58 , in the following manner .",
    "if we draw @xmath59 samples ( rows or constraints or data points ) with replacement , then define an @xmath60 sampling matrix , @xmath57 , where each of the @xmath59 rows of @xmath57 has one non - zero element indicating which row of @xmath5 ( and element of @xmath6 ) is chosen in a given random trial .",
    "that is , if the @xmath61 data unit ( or observation ) in the original data set is chosen in the @xmath36 random trial , then the @xmath36 row of @xmath57 equals @xmath62 ; and thus @xmath57 is a random matrix that describes the process of sampling _ with _ replacement . as an example of applying this sampling matrix , when the sample size @xmath63 and the subsample size @xmath64 , then premultiplying  by @xmath65 represents choosing the second , fourth , and fourth data points or samples .",
    "the resulting subsample of @xmath59 data points can be denoted as @xmath66 , where @xmath67 and @xmath68 . in this case",
    ", an @xmath69 diagonal rescaling matrix @xmath58 can be defined so that @xmath36 diagonal element of @xmath58 equals @xmath70 if the @xmath61 data point is chosen in the @xmath36 random trial ( meaning , in particular , that every diagonal element of @xmath58 equals @xmath71 for uniform sampling ) . with this notation ,",
    "` subsamplels ` constructs and solves the _ weighted ls estimator _ : @xmath72    since ` subsamplels ` samples constraints and not variables , the dimensionality of the vector @xmath53 that solves the ( still overconstrained , but smaller ) weighted ls subproblem is the same as that of the vector @xmath54 that solves the original ls problem .",
    "the former may thus be taken as an approximation of the latter , where , of course , the quality of the approximation depends critically on the choice of @xmath52 .",
    "there are several distributions that have been considered previously  @xcite .    *",
    "* uniform subsampling .",
    "* let @xmath73 , for all @xmath45 $ ] , i.e. , draw the sample uniformly at random .",
    "* * leverage - based subsampling .",
    "* let @xmath74 be the normalized statistical leverage scores of eqn .",
    "( [ eqn : lev - scores ] ) , i.e. , draw the sample according to an importance sampling distribution that is proportional to the statistical leverage scores of the data matrix @xmath5 .",
    "although uniform subsampling ( with or without replacement ) is very simple to implement , it is easy to construct examples where it will perform very poorly ( e.g. , see below or see  @xcite ) . on the other hand",
    ", it has been shown that , for a parameter @xmath75 $ ] to be tuned , if @xmath76 then the following relative - error bounds hold : @xmath77 where @xmath78 is the condition number of @xmath5 and where @xmath79 is a parameter defining the amount of the mass of @xmath6 inside the column space of @xmath5  @xcite .",
    "due to the crucial role of the statistical leverage scores in eqn .",
    "( [ eqn : approx - lev - score - probs ] ) , we refer to algorithms of the form of ` subsamplels ` as the _ algorithmic leveraging _ approach to approximating ls approximation .",
    "several versions of the ` subsamplels ` algorithm are of particular interest to us in this paper .",
    "we start with two versions that have been studied in the past .    * * uniform sampling estimator ( unif ) * is the estimator resulting from _ uniform subsampling _ and _ weighted ls estimation _ ,",
    "i.e. , where eqn .",
    "( [ lsq - sample ] ) is solved , where both the sampling and rescaling / reweighting are done with the uniform sampling probabilities .",
    "( note that when the weights are uniform , then the weighted ls estimator of eqn .",
    "( [ lsq - sample ] ) leads to the same solution as same as the unweighted ls estimator of eqn .",
    "( [ unlsq - sample ] ) below . )",
    "this version corresponds to vanilla uniform sampling , and it s solution will be denoted by @xmath80 . * * basic leveraging estimator ( lev ) * is the estimator resulting from _ exact leverage - based sampling _ and _ weighted ls estimation _ , i.e. , where eqn .",
    "( [ lsq - sample ] ) is solved , where both the sampling and rescaling / reweighting are done with the leverage - based sampling probabilities given in eqn .",
    "( [ eqn : approx - lev - score - probs ] ) .",
    "this is the basic algorithmic leveraging algorithm that was originally proposed in  @xcite , where the exact empirical statistical leverage scores of @xmath5 were first used to construct the subsample and reweight the subproblem , and it s solution will be denoted by @xmath81 .    motivated by our statistical analysis ( to come later in the paper )",
    ", we will introduce two variants of ` subsamplels ` ; since these are new to this paper , we also describe them here .    *",
    "* shrinked leveraging estimator ( slev ) * is the estimator resulting from a _ shrinked leverage - based sampling _ and _ weighted ls estimation_. by shrinked leverage - based sampling , we mean that we will sample according to a distribution that is a convex combination of a leverage score distribution and the uniform distribution , thereby obtaining the benefits of each ; and the rescaling / reweighting is done according to the same distribution .",
    "that is , if @xmath82 denotes a distribution defined by the normalized leverage scores and @xmath83 denotes the uniform distribution , then the sampling and reweighting probabilities for slev are of the  form @xmath84 where @xmath85 .",
    "thus , with slev , eqn .",
    "( [ lsq - sample ] ) is solved , where both the sampling and rescaling / reweighting are done with the probabilities given in eqn .",
    "( [ eqn : slev_probs ] ) .",
    "this estimator will be denoted by @xmath86 , and to our knowledge it has not been explicitly considered previously . * * unweighted leveraging estimator ( levunw ) * is the estimator resulting from a _ leverage - based sampling _ and _ unweighted ls estimation_. that is , after the samples have been selected with leverage - based sampling probabilities , rather than solving the unweighted ls estimator of ( [ lsq - sample ] ) , we will compute the solution of the _ unweighted ls estimator _ : @xmath87 whereas the previous estimators all follow the basic framework of sampling and rescaling / reweighting according to the same distribution ( which is used in worst - case analysis to control the properties of both eigenvalues and eigenvectors and provide unbiased estimates of certain quantities within the analysis  @xcite ) , with levunw they are essentially done according to two different distributions  the reason being that not rescaling leads to the same solution as rescaling with the uniform distribution .",
    "this estimator will be denoted by @xmath88 , and to our knowledge it has not been considered previously .",
    "these methods can all be used to estimate the coefficient vector @xmath47 , and we will analyze  both theoretically and empirically  their statistical properties in terms of bias and variance .      although it is not our main focus ,",
    "the running time for leverage - based sampling algorithms is of interest .",
    "the running times of these algorithms depend on both the time to construct the probability distribution , @xmath52 , and the time to solve the subsampled problem . for unif ,",
    "the former is trivial and the latter depends on the size of the subproblem . for estimators that depend on the exact or approximate ( recall the flexibility in eqn .",
    "( [ eqn : approx - lev - score - probs ] ) provided by @xmath89 ) leverage scores , the running time is dominated by the exact or approximate computation of those scores .",
    "a nave algorithm involves using a qr decomposition or the thin svd of @xmath5 to obtain the exact leverage scores .",
    "unfortunately , this exact algorithm takes @xmath10 time and is thus no faster than solving the original ls problem exactly . of greater interest",
    "is the algorithm of  @xcite that computes relative - error approximations to all of the leverage scores of @xmath5 in @xmath11 time .    in more detail , given as input an arbitrary @xmath4 matrix @xmath5 , with @xmath51 , and an error parameter @xmath90 , the main algorithm of  @xcite ( described also in section  [ sxn : empirical - fast ] below ) computes numbers @xmath91 , for all @xmath92 , that are relative - error approximations to the leverage scores @xmath44 , in the sense that @xmath93 , for all @xmath92 .",
    "this algorithm runs in roughly @xmath94 time , to simplify this expression , suppose that @xmath95 and treat @xmath13 as a constant ; then , the asymptotic running time is @xmath96 which for appropriate parameter settings is @xmath11 time  @xcite .",
    "given the numbers @xmath91 , for all @xmath92 , we can let @xmath97 , which then yields probabilities of the form of eqn .",
    "( [ eqn : approx - lev - score - probs ] ) with ( say ) @xmath98 or @xmath99 .",
    "thus , we can use these @xmath100 in place of @xmath44 in belv , slev , or levunw , thus providing a way to implement these procedures in @xmath11 time .",
    "the running time of the relative - error approximation algorithm of  @xcite depends on the time needed to premultiply @xmath5 by a randomized hadamard transform ( i.e. , a `` structured '' random projection ) . recently ,",
    "high - quality numerical implementations of such random projections have been provided ; see , e.g. , blendenpik  @xcite , as well as lsrn  @xcite , which extends these implementations to large - scale parallel environments .",
    "these implementations demonstrate that , for matrices as small as several thousand by several hundred , leverage - based algorithms such as lev and slev can be better in terms of running time than the computation of qr decompositions or the svd with , e.g. , lapack .",
    "see  @xcite for details , and see  @xcite for the application of these methods to the fast computation of leverage scores .",
    "below , we will evaluate an implementation of a variant of the main algorithm of  @xcite in the software environment r.      our leverage - based methods for estimating @xmath47 are related to resampling methods such as the bootstrap @xcite , and many of these resampling methods enjoy desirable asymptotic properties @xcite .",
    "resampling methods in linear models were studied extensively in @xcite and are related to the jackknife @xcite .",
    "they usually produce resamples at a similar size to that of the full data , whereas algorithmic leveraging is primarily interested in constructing subproblems that are much smaller than the full data .",
    "in addition , the goal of resampling is traditionally to perform statistical inference and not to improve the running time of an algorithm , except in the very recent work @xcite .",
    "additional related work in statistics includes  @xcite .",
    "in this section , we develop analytic methods to study the biases and variances of the subsampling estimators described in section  [ sxn : background - al ] . analyzing these subsampling methods",
    "is challenging for at least the following two reasons : first , there are two layers of randomness in the estimators , i.e. , the randomness inherent in the linear regression model as well as random subsampling of a particular sample from the linear model ; and second , the estimators depends on random subsampling through the inverse of random sampling matrix , which is a nonlinear function . to ease the analysis",
    ", we will employ a taylor series analysis to approximate the subsampling estimators as linear combinations of random sampling matrices , and we will consider biases and variances both conditioned as well as not conditioned on the data",
    ". here is a brief outline of the main results of this section .",
    "* we will start in section  [ sxn : theory - weighted ] with bias and variance results for weighted ls estimators for general sampling / reweighting probabilities .",
    "this will involve viewing the solution of the subsampled ls problem as a function of the vector of sampling / reweighting probabilities and performing a taylor series expansion of the solution to the subsampled ls problem around the expected value ( where the expectation is taken with respect to the random choices of the algorithm ) of that vector .",
    "* then , in section  [ sxn : theory - levunif ] , we will specialize these results to leverage - based sampling and uniform sampling , describing their complementary properties .",
    "we will see that , in terms of bias and variance , neither lev nor unif is uniformly better than the other .",
    "in particular , lev has variance whose size - scale is better than the size - scale of unif ; but unif does not have leverage scores in the denominator of its variance expressions , as does lev , and thus the variance of unif is not inflated on inputs that have very small leverage scores . * finally , in section",
    "[ sxn : theory - novel ] , we will propose and analyze two new leveraging algorithms that will address deficiencies of lev and unif in two different ways .",
    "the first , slev , constructs a smaller ls problem with `` shrinked '' leverage scores that are constructed as a convex combination of leverage score probabilities and uniform probabilities ; and the second , levunw , uses leverage - based sampling probabilities to construct and solve an unweighted or biased ls problem .",
    "we start with the bias and variance of the traditional weighted sampling estimator @xmath101 , given in eqn .",
    "( [ lsq - wls ] ) below .",
    "recall that this estimator actually refers to a parameterized family of estimators , parameterized by the sampling / rescaling probabilities .",
    "the estimate obtained by solving the weighted ls problem of  ( [ lsq - sample ] ) can be represented  as @xmath102 where @xmath103 is an @xmath104 diagonal random matrix , i.e. , all off - diagonal elements are zeros , and where both @xmath105 and @xmath58 are defined in terms of the sampling / rescaling probabilities .",
    "( in particular , @xmath106 describes the probability distribution with which to draw the sample _ and _ with which to reweigh the subsample , where both are done according to the same distribution .",
    "thus , this section does _ not _ apply to levunw ; see section  [ sxn : theory - novel - levunw ] for the extension to levunw . )",
    "although our results hold more generally , we are most interested in unif , lev , and slev , as described in section  [ sxn : background - al ] .",
    "clearly , the vector @xmath101 can be regarded as a function of the random weight vector @xmath107 , denoted as @xmath108 , where @xmath109 are diagonal entries of @xmath106 .",
    "since we are performing random sampling with replacement , it is easy to see that @xmath107 has a scaled multinomial distribution , @xmath110 } = \\frac{r!}{k_1 ! k_2!\\ldots , k_n!}\\pi_1^{k_1}\\pi_2^{k_2}\\cdots \\pi_n^{k_n }   , \\ ] ] and thus it can easily be shown that @xmath111}={\\boldsymbol}{1}$ ] . by setting @xmath112 , the vector around which we will perform our taylor series expansion , to be the all - ones vector , i.e. , @xmath113 , then @xmath114 can be expanded around the full sample ordinary ls estimate @xmath54 , i.e. , @xmath115 . from this , we can establish the following lemma , the proof of which may be found in section  [ sxn : app - proofs - lev - taylor ] .",
    "[ lem : taylor ] let @xmath101 be the output of the ` subsamplels ` algorithm , obtained by solving the weighted ls problem of  ( [ lsq - sample ] ) .",
    "then , a taylor expansion of @xmath101 around the point @xmath113 yields @xmath116 where @xmath117 is the ls residual vector , and where @xmath118 is the taylor expansion remainder",
    ".    * remark .",
    "* the significance of lemma  [ lem : taylor ] is that , to leading order , the vector @xmath119 that encodes information about the sampling process and subproblem construction enters the estimator of @xmath101 linearly . the additional error",
    ", @xmath118 depends strongly on the details of the sampling process , and in particular will be very different for unif , lev , and slev .",
    "* remark . *",
    "our approximations hold when the taylor series expansion is valid , i.e. , when @xmath118 is `` small , '' e.g. , @xmath120 , where @xmath121 means `` little o '' with high probability over the randomness in the random vector @xmath119 .",
    "although we will evaluate the quality of our approximations empirically in sections  [ sxn : empirical ] and  [ sxn : moreempirical ] , we currently do _ not _ have a precise theoretical characterization of when this holds . here",
    ", we simply make two observations .",
    "first , this expression will fail to hold if rank is lost in the sampling process .",
    "this is because in general there will be a bias due to failing to capture information in the dimensions that are not represented in the sample .",
    "( recall that one may use the moore - penrose generalized inverse for inverting rank - deficient matrices . )",
    "second , this expression will tend to hold better as the subsample size @xmath59 is increased .",
    "however , for a fixed value of @xmath59 , the linear approximation regime will be larger when the sample is constructed using information in the leverage scores  since , among other things , using leverage scores in the sampling process is designed to preserve the rank of the subsampled problem  @xcite .",
    "a detailed discussion of this last point is available in  @xcite ; and these observations will be confirmed empirically in section  [ sxn : moreempirical ] .",
    "* remark . * since , essentially , levunw involves sampling and reweighting according to two _ different _ distributions , the analogous expression for levunw will be somewhat different , as will be discussed in lemma  [ lem : unwl - taylor ] in section  [ sxn : theory - novel ] .",
    "given lemma  [ lem : taylor ] , we can establish the following lemma , which provides expressions for the conditional and unconditional expectations and variances for the weighted sampling estimators .",
    "the first two expressions in the lemma are conditioned on the data vector @xmath6 on @xmath122 and @xmath123 refers to performing expectations and variances with respect to ( just ) the random weight vector @xmath119 and not the data . ] ; and the last two expressions in the lemma provide similar results , except that they are not conditioned on the data vector @xmath6 .",
    "the proof of this lemma appears in section  [ sxn : app - proofs - lev - bv ] .",
    "[ lem : bv ] the conditional expectation and conditional variance for the traditional algorithmic leveraging procedure , i.e. , when the subproblem solved is a weighted ls problem of the form  ( [ lsq - sample ] ) , are given by : @xmath124 }     \\hspace{-4 mm }     & = & \\hspace{-3 mm } \\hat{{\\boldsymbol}{\\beta}}_{ols }     +     { \\mbox{}{\\bf{e}_{w}}\\left[r_{w}\\right ] }   ; \\\\ \\label{eqn : slev - wls - var } { \\mbox{}{\\bf{var}_{w}}\\left[\\tilde{{\\boldsymbol}{\\beta}}_{w } |{\\boldsymbol}{y } \\right ] }     \\hspace{-4 mm }     & = & \\hspace{-3 mm } ( x^{t}x)^{-1}x^{t }       \\left[{\\mbox{}{diag}\\left\\{\\hat{{\\boldsymbol}{e}}\\right\\}}{\\mbox{}{diag}\\left\\{\\frac{1}{r{\\boldsymbol}{\\pi}}\\right\\ } } { \\mbox{}{diag}\\left\\{\\hat{{\\boldsymbol}{e}}\\right\\ } } \\right ]       x(x^{t}x)^{-1 }     \\hspace{-1 mm }     +      \\hspace{-1 mm }     { \\mbox{}{\\bf{var}_{w}}\\left[r_{w}\\right ] }   , \\end{aligned}\\ ] ] where @xmath106 specifies the probability distribution used in the sampling and rescaling steps . the unconditional expectation and unconditional variance for the traditional algorithmic leveraging procedure",
    "are given by : @xmath125 }     \\hspace{-4 mm }     & = & \\hspace{-3 mm } { \\boldsymbol}{\\beta}_0 + { \\mbox{}{\\bf{e}}\\left[r_{w}\\right ] }   ;   \\\\ \\label{eqn : slev - wls - var - r - d } { \\mbox{}{\\bf{var}}\\left[\\tilde{{\\boldsymbol}{\\beta}}_{w}\\right ] }     \\hspace{-4 mm }     & = & \\hspace{-3 mm } \\sigma^2(x^{t}x)^{-1 } + \\frac{\\sigma^2}{r}(x^{t}x)^{-1}x^{t}{\\mbox{}{diag}\\left\\{\\frac{(1-h_{ii})^2}{\\pi_{i}}\\right\\ } } x(x^{t}x)^{-1 } + { \\mbox{}{\\bf{var}}\\left[r_{w}\\right ] }   .\\end{aligned}\\ ] ]    * remark .",
    "( [ eqn : slev - cond - bias ] ) states that , when the @xmath126}$ ] term is negligible , i.e. , when the linear approximation is valid , then , conditioning on the observed data @xmath6 , the estimate @xmath101 is approximately unbiased , relative to the full sample ordinarily ls estimate @xmath54 ; and eqn .",
    "( [ eqn : slev - wls - bias ] ) states that , when the @xmath127}$ ] term is negligible , then the estimate @xmath101 is approximately unbiased , relative to the `` true '' value @xmath30 of the parameter vector @xmath47 .",
    "that is , given a particular data set @xmath128 , the conditional expectation result of eqn .",
    "( [ eqn : slev - cond - bias ] ) states that the leveraging estimators can approximate well @xmath54 ; and , as a statistical inference procedure for arbitrary data sets , the unconditional expectation result of eqn .",
    "( [ eqn : slev - wls - bias ] ) states that the leveraging estimators can infer well @xmath30",
    ".    * remark . * both the conditional variance of  eqn .",
    "( [ eqn : slev - wls - var ] ) and the ( second term of the ) unconditional variance of  eqn .",
    "( [ eqn : slev - wls - var - r - d ] ) are inversely proportional to the subsample size @xmath59 ; and both contain a sandwich - type expression , the middle of which depends on how the leverage scores interact with the sampling probabilities .",
    "moreover , the first term of the unconditional variance , @xmath129 , equals the variance of the ordinary ls estimator ; this implies , e.g. , that the unconditional variance of eqn .",
    "( [ eqn : slev - wls - var - r - d ] ) is larger than the variance of the ordinary ls estimator , which is consistent with the gauss - markov theorem .      here , we specialize lemma  [ lem : bv ] by stating two lemmas that provide the conditional and unconditional expectation and variance for lev and unif , and we will discuss the relative merits of each procedure .",
    "the proofs of these two lemmas are immediate , given the proof of lemma  [ lem : bv ] .",
    "thus , we omit the proofs , and instead discuss properties of the expressions that are of interest in our empirical evaluation .",
    "our main conclusion here is that lemma  [ lem : lev - bv ] and lemma  [ lem : unif - bv ] highlight that the statistical properties of the algorithmic leveraging method can be quite different than the algorithmic properties .",
    "prior work has adopted an _ algorithmic perspective _ that has focused on providing worst - case running time bounds for arbitrary input matrices . from this algorithmic perspective , leverage - based sampling ( i.e. , explicitly or implicitly biasing toward high - leverage components , as is done in particular with the lev procedure ) provides uniformly superior worst - case algorithmic results , when compared with unif  @xcite .",
    "our analysis here reveals that , from a _ statistical perspective _ where one is interested in the bias and variance properties of the estimators , the situation is considerably more subtle .",
    "in particular , a key conclusion from lemmas  [ lem : lev - bv ] and  [ lem : unif - bv ] is that , with respect to their variance or mse , neither lev nor unif is uniformly superior for all input .",
    "we start with the bias and variance of the leverage subsampling estimator @xmath81 .",
    "[ lem : lev - bv ] the conditional expectation and conditional variance for the lev procedure are given  by : @xmath130 } \\hspace{-4 mm }     & = & \\hspace{-3 mm } \\hat{{\\boldsymbol}{\\beta}}_{ols}+{\\mbox{}{\\bf{e}_{w}}\\left[r_{lev}\\right ] }   ; \\\\",
    "{ \\mbox{}{\\bf{var}_{w}}\\left[\\tilde{{\\boldsymbol}{\\beta}}_{lev } |{\\boldsymbol}{y } \\right ] } \\hspace{-4 mm }     & = & \\hspace{-3 mm } \\frac{p}{r}(x^{t}x)^{-1}x^{t }         \\left[{\\mbox{}{diag}\\left\\{\\hat{{\\boldsymbol}{e}}\\right\\}}{\\mbox{}{diag}\\left\\{\\frac{1}{h_{ii}}\\right\\ } } { \\mbox{}{diag}\\left\\{\\hat{{\\boldsymbol}{e}}\\right\\ } } \\right ]         x(x^{t}x)^{-1 }       \\hspace{-1mm}+\\hspace{-1 mm } { \\mbox{}{\\bf{var}_{w}}\\left[r_{lev}\\right]}.\\end{aligned}\\ ] ] the unconditional expectation and unconditional variance for the lev procedure are given by : @xmath131 }     \\hspace{-3 mm }     & = & \\hspace{-2 mm } { \\boldsymbol}{\\beta}_0 + { \\mbox{}{\\bf{e}}\\left[r_{lev}\\right ] }   ;   \\\\ \\nonumber { \\mbox{}{\\bf{var}}\\left[\\tilde{{\\boldsymbol}{\\beta}}_{lev}\\right ] }     \\hspace{-3 mm }     & = & \\hspace{-2 mm } \\sigma^2(x^{t}x)^{-1 } + \\frac{p\\sigma^2}{r}(x^{t}x)^{-1}x^{t}{\\mbox{}{diag}\\left\\{\\frac{(1-h_{ii})^2}{h_{ii}}\\right\\ } } x(x^{t}x)^{-1 } \\\\",
    "& + & { \\mbox{}{\\bf{var}}\\left[r_{lev}\\right ] } .",
    "\\label{lsq - wls - var - r - d}\\end{aligned}\\ ] ]    * remark . *",
    "two points are worth making .",
    "first , the variance expressions for lev depend on the size ( i.e. , the number of columns and rows ) of the @xmath4 matrix @xmath5 and the number of samples @xmath59 as @xmath132 .",
    "this variance size - scale many be made to be very small if @xmath133 .",
    "second , the sandwich - type expression depends on the leverage scores as @xmath134 , implying that the variances could be inflated to arbitrarily large values by very small leverage scores .",
    "both of these observations will be confirmed empirically in section  [ sxn : empirical ] .",
    "we next turn to the bias and variance of the uniform subsampling estimator @xmath80 .",
    "[ lem : unif - bv ] the conditional expectation and conditional variance for the unif procedure are given by : @xmath135 }     \\hspace{-3 mm }     & = & \\hspace{-2 mm } \\hat{{\\boldsymbol}{\\beta}}_{ols } + { \\mbox{}{\\bf{e}_{w}}\\left[r_{unif}\\right ] } \\\\ \\label{uniform - cvar } { \\mbox{}{\\bf{var}_{w}}\\left[\\tilde{{\\boldsymbol}{\\beta}}_{unif } |{\\boldsymbol}{y}\\right ] }     \\hspace{-3 mm }     & = & \\hspace{-2 mm } \\frac{n}{r } ( x^{t}x)^{-1}x^{t } \\left [ { \\mbox{}{diag}\\left\\{\\hat{{\\boldsymbol}{e}}\\right\\}}{\\mbox{}{diag}\\left\\{\\hat{{\\boldsymbol}{e}}\\right\\ } } \\right ] x(x^{t}x)^{-1 } + { \\mbox{}{\\bf{var}_{w}}\\left[r_{unif}\\right ] } .\\end{aligned}\\ ] ] the unconditional expectation and unconditional variance for the unif procedure are given by : @xmath136 }     \\hspace{-3 mm }     & = & \\hspace{-2 mm } { \\boldsymbol}{\\beta}_0 + { \\mbox{}{\\bf{e}}\\left[r_{unif}\\right ] }   ; \\\\",
    "\\nonumber { \\mbox{}{\\bf{var}}\\left[\\tilde{{\\boldsymbol}{\\beta}}_{unif}\\right ] }     \\hspace{-3 mm }     & = & \\hspace{-2 mm } \\sigma^2(x^{t}x)^{-1 }     + \\frac{n}{r } \\sigma^2(x^{t}x)^{-1}x^{t } { \\mbox{}{diag}\\left\\ { ( 1-h_{ii})^2 \\right\\ } } x(x^{t}x)^{-1 }   \\\\ \\label{uniform - var1 }     & + & { \\mbox{}{\\bf{var}}\\left[r_{unif}\\right ] }    .\\end{aligned}\\ ] ]    * remark . * two points are worth making .",
    "first , the variance expressions for unif depend on the size ( i.e. , the number of columns and rows ) of the @xmath4 matrix @xmath5 and the number of samples @xmath59 as @xmath137 .",
    "since this variance size - scale is very large , e.g. , compared to the @xmath132 from lev , these variance expressions will be large unless @xmath59 is nearly equal to @xmath1 .",
    "second , the sandwich - type expression is not inflated by very small leverage  scores",
    ".    * remark . * apart from a factor @xmath137 , the conditional variance for unif , as given in eqn .",
    "( [ uniform - cvar ] ) , is the same as hinkley s weighted jackknife variance estimator  @xcite .      in view of lemmas  [",
    "lem : lev - bv ] and  [ lem : unif - bv ] , we consider several ways to take advantage of the complementary strengths of the lev and unif procedures .",
    "recall that we would like to sample with respect to probabilities that are `` near '' those defined by the empirical statistical leverage scores .",
    "we at least want to identify large leverage scores to preserve rank .",
    "this helps ensure that the linear regime of the taylor expansion is large , and it also helps ensure that the scale of the variance is @xmath132 and not @xmath137 .",
    "but we would like to avoid rescaling by @xmath134 when certain leverage scores are extremely small , thereby avoiding inflated variance estimates .",
    "consider first the slev procedure .",
    "as described in section  [ sxn : background - al ] , this involves sampling and reweighting with respect to a distribution that is a convex combination of the empirical leverage score distribution and the uniform distribution .",
    "that is , let @xmath82 denote a distribution defined by the normalized leverage scores ( i.e. , @xmath138 , or @xmath82 is constructed from the output of the algorithm of  @xcite that computes relative - error approximations to the leverage scores ) , and let @xmath83 denote the uniform distribution ( i.e. , @xmath139 , for all @xmath45 $ ] ) ; then the sampling probabilities for the slev procedure are of the form @xmath140 where @xmath85 .    since slev involves solving a weighted ls problem of the form of eqn .",
    "( [ lsq - sample ] ) , expressions of the form provided by lemma  [ lem : bv ] hold immediately . in particular , slev enjoys approximate unbiasedness , in the same sense that the lev and unif procedures do .",
    "the particular expressions for the higher order terms can be easily derived , but they are much messier and less transparent than the bounds provided by lemmas  [ lem : lev - bv ] and  [ lem : unif - bv ] for lev and unif , respectively .",
    "thus , rather than presenting them , we simply point out several aspects of the slev procedure that should be immediate , given our earlier theoretical discussion .",
    "first , note that @xmath141 , with equality obtained when @xmath142 .",
    "thus , assuming that @xmath143 is not extremely small , e.g. , @xmath144 , then none of the slev sampling probabilities is too small , and thus the variance of the slev estimator does not get inflated too much , as it could with the lev estimator .",
    "second , assuming that @xmath143 is not too large , e.g. , @xmath144 , then eqn .  ( [ eqn : approx - lev - score - probs ] ) is satisfied with @xmath145 , and thus the amount of oversampling that is required , relative to the lev procedure , is not much , e.g. , @xmath146 . in this case",
    ", the variance of the slev procedure has a scale of @xmath132 , as opposed to @xmath137 scale of unif , assuming that @xmath59 is increased by that @xmath146 .",
    "third , since eqn .",
    "( [ eqn : slev - probs ] ) is still required to be a probability distribution , combining the leverage score distribution with the uniform distribution has the effect of not only increasing the very small scores , but it also has the effect of performing shrinkage on the very large scores .",
    "finally , all of these observations also hold if , rather that using the exact leverage score distribution ( which recall takes @xmath10 time to compute ) , we instead use approximate leverage scores , as computed with the fast algorithm of  @xcite . for this reason",
    ", this approximate version of the slev procedure is the most promising for very large - scale applications .",
    "consider next the levunw procedure .",
    "as described in section  [ sxn : background - al ] , this estimator is different than the previous estimators , in that the sampling and reweighting are done according to different distributions .",
    "( since levunw does _ not _ sample and reweight according to the same probability distribution , our previous analysis does not apply . )",
    "thus , we shall examine the bias and variance of the unweighted leveraging estimator @xmath88 .",
    "to do so , we first use a taylor series expansion to get the following lemma , the proof of which may be found in section  [ sxn : app - proofs - unwl - taylor ] .",
    "[ lem : unwl - taylor ] let @xmath88 be the output of the modified ` subsamplels ` algorithm , obtained by solving the unweighted ls problem of  ( [ unlsq - sample ] ) .",
    "then , a taylor expansion of @xmath88 around the point @xmath147 yields @xmath148 where @xmath149 is the full sample weighted ls estimator , @xmath150 is the ls residual vector , @xmath151 , and @xmath152 is the taylor expansion remainder",
    ".    * remark .",
    "* this lemma is analogous to lemma  [ lem : taylor ] . since the sampling and reweighting are performed according to different distributions , however , the point about which the taylor expansion is performed , as well as the prefactors of the linear term , are somewhat different .",
    "in particular , here we expand around the point @xmath147 since @xmath111}=r{\\boldsymbol}{\\pi}$ ] when no reweighting takes place .",
    "given this taylor expansion lemma , we can now establish the following lemma for the mean and variance of levunw , both conditioned and unconditioned on the data @xmath6 .",
    "the proof of the following lemma may be found in section  [ sxn : app - proofs - unwl - bv ] .",
    "[ lem : unwl - bv ] the conditional expectation and conditional variance for the levunw procedure are given by : @xmath153 }     & = & \\hat{{\\boldsymbol}{\\beta}}_{wls}+ { \\mbox{}{\\bf{e}_{w}}\\left[r_{levunw}\\right ] }   ;   \\\\ { \\mbox{}{\\bf{var}_{w}}\\left[\\tilde{{\\boldsymbol}{\\beta}}_{levunw}|{\\boldsymbol}{y } \\right ] }     & = & ( x^{t}w_0x)^{-1}x^{t}{\\mbox{}{diag}\\left\\{\\hat{{\\boldsymbol}{e}}_w\\right\\}}w_0{\\mbox{}{diag}\\left\\{\\hat{{\\boldsymbol}{e}}_w\\right\\ } } x(x^{t}w_0x)^{-1 } \\\\",
    "& + &   { \\mbox{}{\\bf{var}_{w}}\\left[r_{levunw}\\right]}.\\end{aligned}\\ ] ] where @xmath154 , and where @xmath149 is the full sample weighted ls estimator .",
    "the unconditional expectation and unconditional variance for the levunw procedure are given by : @xmath155 }     & = & { \\boldsymbol}{\\beta}_0 + { \\mbox{}{\\bf{e}}\\left[r_{levunw}\\right ] }   ; \\\\",
    "\\nonumber { \\mbox{}{\\bf{var}}\\left[\\tilde{{\\boldsymbol}{\\beta}}_{levunw}\\right ] }     & = & \\sigma^2(x^{t}w_0x)^{-1}x^{t}w_0 ^ 2x(x^{t}w_0x)^{-1 }    \\\\ \\nonumber     & + & { \\sigma^2}(x^{t}w_0x)^{-1}x^{t}{\\mbox{}{diag}\\left\\{i - p_{x , w_0}\\right\\}}w_0 { \\mbox{}{diag}\\left\\{i - p_{x , w_0}\\right\\ } } x(x^{t}w_0 x)^{-1 } \\\\ \\label{levnoweightvar11 }     & + & { \\mbox{}{\\bf{var}}\\left[r_{levunw}\\right]}\\end{aligned}\\ ] ] where @xmath156 .    * remark . * the two expectation results in this lemma state : ( i ) ,",
    "when @xmath157}$ ] is negligible , then , conditioning on the observed data @xmath6 , the estimator @xmath88 is approximately unbiased , relative to the full sample _ weighted _ ls estimator @xmath158 ; and ( ii ) , when @xmath159}$ ] is negligible , then the estimator @xmath88 is approximately unbiased , relative to the `` true '' value @xmath30 of the parameter vector @xmath47 .",
    "that is , if we apply levunw to a given data set @xmath14 times , then the average of the @xmath14 levunw estimates are _ not _ centered at the ls estimate , but instead are centered roughly at the weighted least squares estimate ; while if we generate many data sets from the true model and apply levunw to these data sets , then the average of these estimates is roughly centered around true value @xmath30 .    * remark . * as expected , when the leverage scores are all the same , the variance in eqn .",
    "( [ levnoweightvar11 ] ) is the same as the variance of uniform random sampling .",
    "this is expected since , when reweighting with respect to the uniform distribution , one does not change the problem being solved , and thus the solutions to the weighted and unweighted ls problems are identical . more generally , the variance is not inflated by very small leverage scores , as it is with lev .",
    "for example , the conditional variance expression is also a sandwich - type expression , the center of which is @xmath160 , which is not inflated by very small leverage scores .",
    "in this section , we describe the main part of our empirical analysis of the behavior of the biases and variances of the subsampling estimators described in section  [ sxn : background - al ] .",
    "additional empirical results will be presented in section  [ sxn : moreempirical ] . in these two sections",
    ", we will consider both synthetic data as well as real data that have been chosen to illustrate the extreme properties of the subsampling methods in realistic settings .",
    "we will use the mse as a benchmark to compare the different subsampling estimators ; but since we are interested in both the bias and variance properties of our estimates , we will present results for both the bias and variance separately .    here is a brief outline of the main results of this section .    * in section  [ sxn : empirical - synthetic ] , we will describe our synthetic data . these data are drawn from three standard distributions , and they are designed to provide relatively - realistic synthetic examples where leverages scores are fairly uniform , moderately nonuniform , or very nonuniform . * then , in section  [ sxn : empirical - levvunif ] , we will summarize our results for the unconditional bias and variance for lev and unif , when applied to the synthetic data . * then , in section  [ sxn : empirical - shrinkedunweight ] , we will summarize our results for the unconditional bias and variance of slev and levunw .",
    "this will illustrate that both slev and levunw can overcome some of the problems associated with lev and unif . *",
    "finally , in section  [ sxn : empirical - conditional ] , we will present our results for the conditional bias and variance of slev and levunw ( as well as lev and unif ) . in particular",
    ", this will show that levunw can incur substantial bias , relative to the other methods , when conditioning on a given data set .",
    "we consider synthetic data of 1000 runs generated from @xmath161 , where @xmath162 , where several different values of @xmath1 and @xmath2 , leading to both `` very rectangular '' and `` moderately rectangular '' matrices @xmath5 , are considered .",
    "the design matrix @xmath5 is generated from one of three different classes of distributions introduced below .",
    "these three distributions were chosen since the first has nearly uniform leverage scores , the second has mildly non - uniform leverage scores , and the third has very non - uniform leverage  scores .",
    "* * nearly uniform leverage scores ( ga ) .",
    "* we generated an @xmath4 matrix @xmath5 from multivariate normal @xmath163 , where the @xmath164th element of @xmath165 , and where we set @xmath166 .",
    "( referred to as ga data . ) * * moderately nonuniform leverage scores ( @xmath167 ) .",
    "* we generated @xmath5 from multivariate @xmath168-distribution with @xmath169 degree of freedom and covariance matrix @xmath170 as before .",
    "( referred to as @xmath167 data . ) * * very nonuniform leverage scores ( @xmath171 ) .",
    "* we generated @xmath5 from multivariate @xmath168-distribution with @xmath172 degree of freedom and covariance matrix @xmath170 as before .",
    "( referred to as @xmath171 data . )",
    "see table  [ tab : synthetic - summary - stats ] for a summary of the parameters for the synthetic data we considered and for basic summary statistics for the leverage scores probabilities ( i.e. , the leverage scores that have been normalized to sum to @xmath172 by dividing by @xmath2 ) of these data matrices .",
    "the results reported in table  [ tab : synthetic - summary - stats ] are for leverage score statistics for a single fixed data matrix @xmath5 generated in the above manner ( for each of the @xmath169 procedures and for each value of @xmath1 and @xmath2 ) , but we have confirmed that similar results hold for other matrices @xmath5 generated in the same  manner .",
    "[ tab : synthetic - summary - stats ]    lrrrrrrrrrr dstbn & @xmath1 & @xmath2 & min & median & max & mean & std.dev .",
    "& @xmath173 & @xmath174 & @xmath175 +   + ga & 1k & 10 & 1.96e-4 & 9.24e-4 & 2.66e-3 & 1.00e-3 & 4.49e-4 & 13.5 & 2.66 & 2.88 + ga & 1k & 50 & 4.79e-4 & 9.90e-4 & 1.74e-3 & 1.00e-3 & 1.95e-4 & 3.63 & 1.74 & 1.76 + ga & 1k & 100 & 6.65e-4 & 9.94e-4 & 1.56e-3 & 1.00e-3 & 1.33e-4 & 2.35 & 1.56 & 1.57 + ga & 5k & 10 & 1.45e-5 & 1.88e-4 & 6.16e-4 & 2.00e-4 & 8.97e-5 & 42.4 & 3.08 & 3.28 + ga & 5k & 50 & 9.02e-5 & 1.98e-4 & 3.64e-4 & 2.00e-4 & 3.92e-5 & 4.03 & 1.82 & 1.84 + ga & 5k & 250 & 1.39e-4 & 1.99e-4 & 2.68e-4 & 2.00e-4 & 1.73e-5 & 1.92 & 1.34 & 1.34 + ga & 5k & 500 & 1.54e-4 & 2.00e-4 & 2.48e-4 & 2.00e-4 & 1.20e-5 & 1.61 & 1.24 & 1.24 + @xmath167 & 1k & 10 & 2.64e-5 & 4.09e-4 & 5.63e-2 & 1.00e-3 & 2.77e-3 & 2.13e+3 & 56.3 & 138 + @xmath167 & 1k & 50 & 6.57e-5 & 5.21e-4 & 1.95e-2 & 1.00e-3 & 1.71e-3 & 297 & 19.5 & 37.5 + @xmath167 & 1k & 100 & 7.26e-5 & 6.39e-4 & 9.04e-3 & 1.00e-3 & 1.06e-3 & 125 & 9.04 & 14.1 + @xmath167 & 5k & 10 & 5.23e-6 & 7.73e-5 & 5.85e-2 & 2.00e-4 & 9.66e-4 & 1.12e+4 & 293 & 757 + @xmath167 & 5k & 50 & 9.60e-6 & 9.84e-5 & 1.52e-2 & 2.00e-4 & 4.64e-4 & 1.58e+3 & 76.0 & 154 + @xmath167 & 5k & 250 & 1.20e-5 & 1.14e-4 & 3.56e-3 & 2.00e-4 & 2.77e-4 & 296 & 17.8 & 31.2 + @xmath167 & 5k & 500 & 1.72e-5 & 1.29e-4 & 1.87e-3 & 2.00e-4 & 2.09e-4 & 108 & 9.34 & 14.5 + @xmath171 & 1k & 10 & 4.91e-8 & 4.52e-6 & 9.69e-2 & 1.00e-3 & 8.40e-3 & 1.97e+6 & 96.9 & 2.14e+4 + @xmath171 & 1k & 50 & 2.24e-6 & 6.18e-5 & 2.00e-2 & 1.00e-3 & 3.07e-3 & 8.93e+3 & 20 & 323 + @xmath171 & 1k & 100 & 4.81e-6 & 1.66e-4 & 9.99e-3 & 1.00e-3 & 2.08e-3 & 2.08e+3 & 9.99 & 60.1 + @xmath171 & 5k & 10 & 5.00e-9 & 6.18e-7 & 9.00e-2 & 2.00e-4 & 3.00e-3 & 1.80e+7 & 450 & 1.46e+5 + @xmath171 & 5k & 50 & 4.10e-8 & 2.71e-6 & 2.00e-2 & 2.00e-4 & 1.39e-3 & 4.88e+5 & 99.9 & 7.37e+3 + @xmath171 & 5k & 250 & 3.28e-7 & 1.50e-5 & 4.00e-3 & 2.00e-4 & 6.11e-4 & 1.22e+4 & 20 & 267 + @xmath171 & 5k & 500 &",
    "1.04e-6 & 2.79e-5 & 2.00e-3 & 2.00e-4 & 4.24e-4 & 1.91e+3 & 10 & 71.6 +   +    several observations are worth making about the summaries presented in table  [ tab : synthetic - summary - stats ] .",
    "first , and as expected , the gaussian data tend to have the most uniform leverage scores , the @xmath167 data are intermediate , and the @xmath171 data have the most nonuniform leverage scores , as measured by both the standard deviation of the scores as well as the ratio of maximum to minimum leverage score .",
    "second , the standard deviation of the leverage score distribution is substantially less sensitive to nonuniformities in the leverage scores than is the ratio of the maximum to minimum leverage score ( or the maximum to the mean / median score , although all four measures exhibit the same qualitative trends ) .",
    "although we have not pursued it , this suggests that these latter measures will be more informative as to when leverage - based sampling might be necessary in a particular application .",
    "third , in all these cases , the variability trends are caused both by the large ( in particular , the maximum ) leverage scores increasing as well as the small ( in particular , the minimum ) leverage score decreasing .",
    "fourth , within a given type of distribution ( i.e. , ga or @xmath167 or @xmath171 ) , leverage scores are more nonuniform when the matrix @xmath5 is more rectangular , and this is true both when @xmath1 is held fixed and when @xmath2 is held fixed .      here",
    ", we will describe the properties of lev versus unif for synthetic data .",
    "see figures  [ fig : simuline1 ] , [ fig : simuline2 ] , and  [ fig : simuline3 ] for the results on data matrices with @xmath176 and @xmath177 , @xmath178 , and @xmath179 , respectively .",
    "( the results for data matrices for @xmath180 and other values of @xmath1 are similar . ) in each case , we generated a single matrix from that distribution ( which we then fixed to generate the @xmath6 vectors ) and @xmath181 was set to be the all - ones vector ; and then we ran the sampling process multiple times , typically ca .",
    "@xmath182 times , in order to obtain reliable estimates for the biases and variances . in each of the figures  [ fig : simuline1 ] , [ fig : simuline2 ] , and  [ fig : simuline3 ] , the top panel is the variance , the bottom panel is the squared bias ; for both the bias and variance , we have plotted the results in log - scale ; and , in each figure , the first column is the ga model , the middle column is the @xmath167 model , and the right column is the @xmath171 model .",
    "the simulation results corroborate what we have learned from our theoretical analysis , and there are several things worth noting .",
    "first , in general the squared bias is much less than the variance , even for the @xmath171 data , suggesting that the solution is approximately unbiased , at least for the values of @xmath59 plotted here , in the sense quantified in lemmas  [ lem : lev - bv ] and  [ lem : unif - bv ] .",
    "second , lev and unif perform very similarly for ga , somewhat less similarly for @xmath167 , and quite differently for @xmath171 , consistent with the results in table  [ tab : synthetic - summary - stats ] that indicate that the leverage scores are very uniform for ga and very nonuniform for @xmath171 .",
    "in addition , when they are different , lev tends to perform better than unif , i.e. , have a lower mse for a fixed sampling complexity .",
    "third , as the subsample size increases , the squared bias and variance tend to decrease monotonically . in particular , the variance tends to decrease roughly as @xmath183 , where @xmath59 is the size of the subsample , in agreement with lemmas  [ lem : lev - bv ] and  [ lem : unif - bv ] .",
    "moreover , the decrease for unif is much slower , in a manner more consistent with the leading term of @xmath137 in eqn .",
    "( [ uniform - var1 ] ) , than is the decrease for lev , which by eqn .",
    "( [ lsq - wls - var - r - d ] ) has leading term @xmath132 .",
    "fourth , for all three models , both the bias and variance tend to increase when the matrix is less rectangular , e.g. , as @xmath2 increases @xmath184 to @xmath179 for @xmath176 .",
    "all in all , lev is comparable to or outperforms unif , especially when the leverage scores are nonuniform .      here",
    ", we will describe how our proposed slev and levunw procedures can both lead to improvements relative to lev and unif . recall that lev can lead to large mse by inflating very small leverage scores .",
    "the slev procedure deals with this by considering a convex combination of the uniform distribution and the leverage score distribution , thereby providing a lower bound on the leverage scores ; and the levunw procedure deals with this by not rescaling the subproblem to be solved .",
    "consider figures  [ fig : simual1 ] , [ fig : simual2 ] , and  [ fig : simual3 ] , which present the variance and bias for synthetic data matrices ( for ga , @xmath167 , and @xmath171 data ) of size @xmath4 , where @xmath176 and @xmath177 , @xmath178 , and @xmath179 , respectively . in each case ,",
    "lev , slev for three different values of the convex combination parameter @xmath185 , and levunw were considered .",
    "several observations are worth making .",
    "first of all , for ga data ( left panel in these figures ) , all the results tend to be quite similar ; but for @xmath167 data ( middle panel ) and even more so for @xmath171 data ( right panel ) , differences appear .",
    "second , slev with @xmath186 , i.e. , when slev consists mostly of the uniform distribution , is notably worse in a manner similarly as with unif .",
    "moreover , there is a gradual decrease in both bias and variance for our proposed slev as @xmath185 is increased ; and when @xmath187 slev is slightly better than lev . finally , our proposed levunw often has the smallest bias and variance over a wide range of subsample sizes for both @xmath167 and @xmath171 , although the effect is not major . all in all , these observations are consistent with our main theoretical  results .",
    "consider next figure  [ fig : simualev1 ] .",
    "this figure examines the optimal convex combination choice for @xmath185 in slev , and @xmath185 is the x - axis in all the plots .",
    "different column panels in figure  [ fig : simualev1 ] correspond to different subsample sizes @xmath59 .",
    "recall that there are two conflicting goals for slev : adding @xmath188 to the small leverage scores will avoid substantially inflating the variance of the resulting estimate by samples with extremely small leverage scores ; and doing so will lead to larger sample size @xmath59 in order to obtain bounds of the form eqns .",
    "( [ eqn : ls - bound - eq1 ] ) and  ( [ eqn : ls - bound - eq2 ] ) .",
    "figure  [ fig : simualev1 ] plots the variance and bias for @xmath171 data for a range of parameter values and for a range of subsample sizes .",
    "in general , one sees that using slev to increase the probability of choosing small leverage components with @xmath185 around @xmath189 ( and relatedly shrinking the effect of large leverage components ) has a beneficial effect on bias as well as variance .",
    "this is particularly true in two cases : first , when the matrix is very rectangular , e.g. , when the @xmath177 , which is consistent with the leverage score statistics from table  [ tab : synthetic - summary - stats ] ; and second , when the subsample size @xmath59 is larger , as the results for @xmath190 are much choppier ( and for @xmath191 , they are still choppier ) . as a rule of thumb , these plots suggest that choosing @xmath192 , and thus using @xmath193 as the importance sampling probabilities , strikes a balance between needing more samples and avoiding variance inflation",
    ".    one can also see in figure  [ fig : simualev1 ] the grey lines , dots , and dashes , which correspond to levunw for the corresponding values of @xmath2 , that levunw consistently has smaller variances than slev for all values of @xmath185 .",
    "we should emphasize , though , that these are _ unconditional _ biases and variances . since levunw is approximately unbiased relative to the full sample _",
    "weighted _ ls estimate @xmath158 , however , there is a large bias away from the full sample _ unweighted _ ls estimate @xmath54 .",
    "this suggests that levunw may be used when the primary goal is to infer the true @xmath30 ; but that when the primary goal is rather to approximate the full sample unweighted ls estimate , or when _ conditional _ biases and variances are of interest , then slev may be more appropriate .",
    "we will discuss this in greater detail in section  [ sxn : empirical - conditional ] next .      here",
    ", we will describe the properties of the _ conditional _ bias and variance under various subsampling estimators .",
    "these will provide a more direct comparison with eqns .",
    "( [ eqn : slev - cond - bias ] ) and  ( [ eqn : slev - wls - var ] ) from lemma  [ lem : bv ] and the corresponding bounds from lemma  [ lem : unwl - bv ] .",
    "these will also provide a more direct comparison with previous work that has adopted an algorithmic perspective on algorithmic leveraging  @xcite .",
    "consider figure  [ fig : simuline2c ] , which presents our main empirical results for conditional biases and variances .",
    "as before , matrices were generated from ga , @xmath167 and @xmath171 ; and we calculated the empirical bias and variance of unif , lev , slev with @xmath192 , and levunw  in all cases , conditional on the empirical data @xmath6 .",
    "several observations are worth making .",
    "first , for ga the variances are all very similar the same ; and the biases are also , with the exception of levunw .",
    "this is expected , since by the conditional expectation bounds from lemma  [ lem : unwl - bv ] , levunw is approximately unbiased , relative to the full sample _ weighted _ ls estimate @xmath158and thus there should be a large bias away from the full sample unweighted ls estimate .",
    "second , for @xmath167 and even more prominently for @xmath171 , the variance of levunw is less than that for the other estimators .",
    "third , when the leverage scores are very nonuniform , as with @xmath171 , the relative merits of unif versus levunw depend on the subsample size @xmath59 .",
    "in particular , the bias of levunw is larger than that of even unif for very aggressive downsampling ; but it is substantially less than unif for moderate to large sample sizes .    based on these and our other results , our default recommendation is to use slev ( with either exact or approximate leverage scores ) with @xmath194 : it is no more than slightly worse than levunw when considering unconditional biases and variances , and it can be much better than levunw when considering conditional biases and variances .",
    "in this section , we provide additional empirical results ( of a more specialized nature than those presented in section  [ sxn : empirical ] ) . here is a brief outline of the main results of this section .    * in section  [ sxn : empirical - singular ] , we will consider the synthetic data , and we will describe what happens when the subsampled problem looses rank .",
    "this can happen if one is _",
    "extremely _ aggressive in downsampling with slev ; but it is much more common with unif , even if one samples many constraints . in both cases , the behavior of bias and variance is very different than when rank is preserved .",
    "* then , in section  [ sxn : empirical - fast ] , we will summarize our results on synthetic data when the leverage scores are computed approximately with the fast approximation algorithm of  @xcite . among other things",
    ", we will describe the running time of this algorithm , illustrating that it can solve larger problems than can be solved with traditional deterministic methods ; and we will evaluate the unconditional bias and variance of slev when this algorithm is used to approximate the leverage scores . * finally , in section",
    "[ sxn : empirical - real ] , we will consider real data , and we will present our results for the conditional bias and variance for two data sets that are drawn from our previous work in two genetics applications .",
    "one of these has very uniform leverage scores , and the other has moderately nonuniform leverage scores ; and our results from the synthetic data hold also in these realistic applications .      here , we will describe the properties of lev versus unif for situations in which rank is lost in the construction of the subproblem .",
    "that is , in some cases , the subsampled matrix , @xmath195 , may have column rank that is smaller than the rank of the original matrix @xmath5 , and this leads to a singular @xmath196 . of course , the ls solution of the subproblem can still be solved , but there will be a `` bias '' due to the dimensions that are not represented in the subsample .",
    "( we use the moore - penrose generalized inverse to compute the estimators when rank is lost in the construction of the subproblem . ) before describing these results , recall that algorithmic leveraging ( in particular , lev , but it holds for slev as well ) guarantees that this will _ not _ happen in the following sense : if roughly @xmath197 rows of @xmath5 are sampled using an importance sampling distribution that approximates the leverage scores in the sense of eqn .",
    "( [ eqn : approx - lev - score - probs ] ) , then with very high probability the matrix @xmath195 does not loose rank  @xcite . indeed , this observation is crucial from the algorithmic perspective , i.e. , in order to obtain relative - error bounds of the form of eqns .",
    "( [ eqn : ls - bound - eq1 ] ) and  ( [ eqn : ls - bound - eq2 ] ) , and thus it was central to the development of algorithmic leveraging . on the other hand ,",
    "if one downsamples more aggressively , e.g. , if one samples only , say , @xmath198 or @xmath199 rows , or if one uses uniform sampling when the leverage scores are very nonuniform , then it is possible to loose rank . here , we examine the statistical consequences of this .",
    "we have observed this phenomenon with the synthetic data for both unif as well as for leverage - based sampling procedures ; but the properties are somewhat different depending on the sampling procedure . to illustrate both of these with a single synthetic example",
    ", we first generated a @xmath200 matrix from multivariate @xmath168-distribution with @xmath169 ( or @xmath201 or @xmath172 , denoted @xmath167 , @xmath202 , and @xmath171 , respectively ) degrees of freedom and covariance matrix @xmath203 ; we then calculated the leverage scores of all rows ; and finally we formed the matrix @xmath5 was by keeping the @xmath178 rows with highest leverage scores and replicating @xmath204 times the row with the smallest leverage score .",
    "( this is a somewhat more realistic version of the toy * worst - case matrix * that is described in section  [ sxn : interlude - toy ] . )",
    "we then applied lev and unif to the data sets with different subsample sizes , as we did for the results summarized in section  [ sxn : empirical - levvunif ] .",
    "our results are summarized in figure  [ fig : simurepeata ] and  [ fig : simurepeat ] .    the top row of figure  [ fig : simurepeata ] plots the fraction of singular @xmath205 , out of @xmath206 trials , for both lev and unif ; from left to right , results for @xmath167 , @xmath202 , and @xmath171 are shown .",
    "several points are worth emphasizing .",
    "first , both lev and unif loose rank if the downsampling is sufficiently aggressive .",
    "second , for lev , as long as one chooses more than roughly @xmath207 ( or less for @xmath202 and @xmath171 ) , i.e. , the ratio @xmath208 is at least roughly @xmath201 , then rank is _ not _ lost ; but for uniform sampling , one must sample a _ much _ larger fraction of the data .",
    "in particular , when fewer than @xmath209 samples are drawn then nearly all of the subproblems constructed with the unif procedure are singular , and it is not until more than @xmath210 that nearly all of the subproblems are not singular .",
    "although these particular numbers depend on the particular data , needing to draw many more samples with unif than with lev in order to preserve rank is a very general phenomenon .",
    "the middle row of figure  [ fig : simurepeata ] shows the boxplots of rank for the subproblem for lev for those @xmath206 tries ; and the bottom row shows the boxplots of the rank of the subproblem for unif for those @xmath206 tries .",
    "note the unusual scale on the x - axis designed to highlight the lost rank data for both lev as well as unif .",
    "these boxplots illustrate the sigmoidal distribution of ranks that obtained by unif as a function of the number of samples and the less severe beginning of the sigmoid for lev ; and they also show that when subproblems are singular , then often many dimensions fail to be captured .",
    "all in all , lev outperforms unif , especially when the leverage scores are nonuniform .",
    "figure  [ fig : simurepeat ] illustrates the variance and bias of the corresponding estimators . in particular , the upper panels plot the logarithm of variances ; the middle panels plot the same quantities , except that it is zoomed - in on the x - axis ; and the lower panels plot the logarithm of squared bias . as before ,",
    "the left / middle / right panels present results for the @xmath167/@xmath202/@xmath171 data , respectively .",
    "the behavior here is very different that that shown in figures  [ fig : simuline1 ] , [ fig : simuline2 ] , and  [ fig : simuline3 ] ; and several observations are worth making .",
    "first , for all three models and for both lev and unif , when the downsampling is very aggressive , e.g , @xmath211 or @xmath212 , then the bias is comparable to the variance .",
    "that is , since the sampling process has lost dimensions , the linear approximation implicit in our taylor expansion is violated .",
    "second , both bias and variance are worse for @xmath171 than for @xmath202 than for @xmath167 , which is consistent with table  [ tab : synthetic - summary - stats ] , but the effect is minor ; and the bias and variance are generally much worse for unif than for lev .",
    "third , as @xmath59 increases , the variance for unif increases , hits a maximum and then decreases ; and at the same time the bias for unif gradually decreases . upon examining the original data ,",
    "the reason that there is very little variance initially is that most of the subsamples have rank @xmath172 or @xmath201 ; then the variance increases as the dimensionality of the subsamples increases ; and then the variance decreases due to the @xmath183 scaling , as we saw in the plots in section  [ sxn : empirical - levvunif ] .",
    "fourth , as @xmath59 increases , both the variance and bias of lev decrease , as we saw in section  [ sxn : empirical - levvunif ] ; but in the aggressive downsampling regime , i.e. , when @xmath59 is very small , the variance of lev is particularly `` choppy , '' and is actually worse than that of unif , perhaps also due to rank deficiency issues .      here , we will describe using the fast randomized algorithm from  @xcite to compute approximations to the leverage scores of @xmath5 , to be used in place of the exact leverage scores in lev , slev , and levunw .",
    "to start , we provide a brief description of the algorithm of  @xcite , which takes as input an arbitrary @xmath4 matrix @xmath5 .",
    "* generate an @xmath213 random matrix @xmath214 and a @xmath215 random matrix @xmath216 .",
    "* let @xmath217 be the @xmath217 matrix from a qr decomposition of @xmath218 .",
    "* compute and return the leverage scores of the matrix @xmath219 .",
    "for appropriate choices of @xmath220 and @xmath221 , if one chooses @xmath214 to be a hadamard - based random projection matrix , then this algorithm runs in @xmath11 time , and it returns @xmath222 approximations to all the leverage scores of @xmath5  @xcite . in addition , with a high - quality implementation of the hadamard - based random projection , this algorithm runs faster than traditional deterministic algorithms based on lapack for matrices as small as several thousand by several hundred  @xcite .",
    "we have implemented in the software environment r two variants of this fast algorithm of @xcite , and we have compared it with qr - based deterministic algorithms also supported in r for computing the leverage scores exactly . in particular ,",
    "the following results were obtained on a pc with intel core i7 processor and 6 gbytes ram running windows 7 , on which we used the software package r , version 2.15.2 . in the following ,",
    "we refer to the above algorithm as bfast ( the binary fast algorithm ) when ( up to normalization ) each element of @xmath214 and @xmath216 is generated i.i.d . from @xmath223 with equal sampling probabilities",
    "; and we refer to the above algorithm as gfast ( the gaussian fast algorithm ) when each element of @xmath214 is generated i.i.d . from a gaussian distribution with mean zero and variance @xmath224 and each element of @xmath216",
    "is generated i.i.d . from a gaussian distribution with mean zero and variance @xmath225 .",
    "( in particular , note that here we do not consider hadamard - based projections for @xmath214 or more sophisticated parallel and distributed implementations of these algorithms  @xcite . )    to illustrate the behavior of this algorithm as a function of its parameters , we considered synthetic data where the @xmath226 design matrix @xmath5 is generated from @xmath171 distribution .",
    "all the other parameters are set to be the same as before , except @xmath227 , for @xmath228 , and @xmath229 .",
    "we then applied bfast and gfast with varying @xmath220 and @xmath221 to the data .",
    "in particular , we set @xmath230 , where @xmath231 , and we set @xmath232 , for @xmath233 , where @xmath234",
    ". see figure  [ fig : fastcor ] , which presents both a summary of the correlation between the approximate and exact leverage scores as well as a summary of the running time for computing the approximate leverage scores , as @xmath220 and @xmath221 are varied for both bfast and gfast .",
    "we can see that the correlations between approximated and exact leverage scores are not very sensitive to varying @xmath220 , whereas the running time increases roughly linearly for increasing @xmath220 . in contrast , the correlations between approximated and exact leverage scores increases rapidly for increasing @xmath221 , whereas the running time does not increase much when @xmath221 increases .",
    "these observations suggest that we may use a combination of small @xmath220 and large @xmath221 to achieve high - quality approximation and short running  time .",
    "next , we examine the running time of the approximation algorithms for computing the leverage scores . our results for running times are summarized in figure  [ fig : fasttime ] . in that figure",
    ", we plot the running time as sample size @xmath1 and predictor size @xmath2 are varied for bfast and gfast .",
    "we can see that when the sample size is very small , the computation time of the fast algorithms is slightly worse than that of the exact algorithm .",
    "( this phenomenon is primarily since the fast algorithm requires additional projection and matrix multiplication steps , which dominate the running time for very small matrices . ) on the other hand , when the sample size is larger than ca .",
    "@xmath235 , the computation time of the fast approximation algorithms becomes slightly less expensive than that of exact algorithm .",
    "much more significantly , when the sample size is larger than roughly @xmath236 , the exact algorithm requires more memory than our standard r environment can provide , and thus it fails to run at all .",
    "in contrast , the fast algorithms can work with sample size up to roughly  @xmath237 .",
    "that is , the use of this randomized algorithm to approximate the leverage scores permits us to work with data that are roughly @xmath238 times larger in @xmath1 or @xmath2 , even when a simple vanilla implementation is provided in the r environment .",
    "( if one is interested in much larger inputs , e.g. , with @xmath239 or more , then one should probably not work within r and instead use hadamard - based random projections for @xmath214 and/or the use of more sophisticated methods , such as those described in  @xcite ; here we simply evaluate an implementation of these methods in r. ) the reason that bfast and gfast can run for much larger input is likely that the computational bottleneck for the exact algorithm is a qr decomposition , while the computational bottleneck for the fast randomized algorithms is the matrix - matrix multiplication step .",
    "finally , we evaluate the bias and variance of lev , slev and levunw estimates where the leverage scores are calculated using exact algorithm , bfast , and gfast . in figure",
    "[ fig : fastbv2 ] , we plot the variance and squared bias for @xmath167 data sets .",
    "( we have observed similar but slightly smoother results for the gaussian data sets and similar but slightly choppier results for the @xmath171 data sets . )",
    "observe that the variances of lev estimates where the leverage scores are calculated using exact algorithm , bfast , and gfast are almost identical ; and this observation is also true for slev and levunw estimates .",
    "all in all , using the fast approximation algorithm of  @xcite to compute approximations to the leverage scores for use in lev , slev , and levunw leads to improved algorithmic performance , while achieving nearly identical statistical results as lev , slev , and levunw when the exact leverage scores are  used .      here",
    ", we provide an illustration of our methods on two real data sets drawn from two problems in genetics with which we have prior experience  @xcite .",
    "the first data set has relatively uniform leverage scores , while the second data set has somewhat more nonuniform leverage scores .",
    "these two examples simply illustrate that observations we made on the synthetic data also hold for more realistic data that we have studied previously . for more information on the application of these ideas in genetics ,",
    "see previous work on pca - correlated snps  @xcite .      in order to illustrate how our methods perform on a real data set with nearly uniform leverage scores",
    ", we consider an rna - seq data set containing @xmath240 read counts from embryonic mouse stem cells @xcite . recall that rna - seq is becoming the major tool for transcriptome analysis ; it produces digital signals by obtaining tens of millions of short reads ; and after being mapped to the genome , rna - seq data can be summarized by a sequence of short - read counts .",
    "recent work found that short - read counts have significant sequence bias  @xcite . here , we consider a simplified linear model of @xcite for correcting sequence bias in rna - seq .",
    "let @xmath241 denote the counts of reads that are mapped to the genome starting at the @xmath242th nucleotide of the @xmath243th gene , where @xmath244 and @xmath245 .",
    "we assume that the log transformed count of reads , @xmath246 , depends on @xmath247 nucleotides in the neighborhood , denoted as @xmath248 through the following linear model : @xmath249 where @xmath250 , where @xmath251 is used as the baseline level , @xmath185 is the grand mean , @xmath252 equals to 1 if the @xmath253th nucleotide of the surrounding sequence is @xmath254 , and @xmath255 otherwise , @xmath256 is the coefficient of the effect of nucleotide @xmath254 occurring in the @xmath253th position , and @xmath257 .",
    "this linear model uses @xmath258 parameters to model the sequence bias of read counts . for @xmath240 , model - fitting via ls",
    "is time - consuming .",
    "coefficient estimates were obtained using three subsampling algorithms for seven different subsample sizes : @xmath259 .",
    "we compare the estimates using the sample bias and variances ; and , for each subsample size , we repeat our sampling @xmath179 times to get @xmath179 estimates .",
    "( at each subsample size , we take one hundred subsamples and calculate all the estimates ; we then calculate the bias of the estimates with respect to the full sample least squares estimate and their variance . )",
    "see figure  [ fig : real1 ] for a summary of our results . in the left panel of figure  [ fig : real1 ] , we plot the histogram of the leverage score sampling probabilities .",
    "observe that the distribution is quite uniform , suggesting that leverage - based sampling methods will perform similarly to uniform sampling . to demonstrate this , the middle and right panels of figure  [ fig : real1 ] present the ( conditional ) empirical variances and biases of each of the four estimates , for seven different subsample sizes .",
    "observe that lev , levunw , slev , and unif all have comparable sample variances .",
    "when the subsample size is very small , all four methods have comparable sample bias ; but when the subsample size is larger , then levunw has a slightly larger bias than the other three estimates .      in order to illustrate how our methods perform on real data with moderately nonuniform leverage scores , we consider a microarray data set that was presented in  @xcite ( and also considered in  @xcite ) for @xmath260 cancer patients with respect to @xmath261 genes . here",
    ", we randomly select one patient s gene expression as the response @xmath6 and use the remaining patients gene expressions as the predictors ( so @xmath262 ) ; and we predict the selected patient s gene expression using other patients gene expressions through a linear model .",
    "we fit the linear model using subsampling algorithms with nine different subsample sizes .",
    "see figure  [ fig : real2 ] for a summary of our results . in the left panel of figure  [ fig : real2 ] , we plot the histogram of the leverage score sampling probabilities .",
    "observe that the distribution is highly skewed and quite a number of probabilities are significantly larger than the average probability .",
    "thus , one might expect that leveraging estimates will have an advantage over the uniform sampling estimate . to demonstrate this , the middle and right panels of figure  [ fig : real2 ] present the ( conditional ) empirical variances and biases of each of the four estimates , for nine different subsample sizes .",
    "observe that slev and lev have smaller sample variance than levunw and that unif consistently has the largest variance .",
    "interestingly , since levunw is approximately unbiased to the weighted least squares estimate , here we observe that levunw has by far the largest bias and that the bias does not decrease as the subsample size increases . in addition , when the subsample size is less than 2000 , the biases of lev , slev and unif are comparable ; but when the subsample size is greater than 2000 , lev and slev have slightly smaller bias than  unif .",
    "algorithmic leveraging  a recently - popular framework for solving large least - squares regression and other related matrix problems via sampling based on the empirical statistical leverage scores of the data  has been shown to have many desirable _ algorithmic _ properties . in this paper",
    ", we have adopted a _ statistical _ perspective on algorithmic leveraging , and we have demonstrated how this leads to improved performance of this paradigm on real and synthetic data . in particular , from the algorithmic perspective of worst - case analysis , leverage - based sampling provides uniformly superior worst - case algorithmic results , when compared with uniform sampling .",
    "our statistical analysis , however , reveals that , from the statistical perspective of bias and variance , neither leverage - based sampling nor uniform sampling dominates the other .",
    "based on this , we have developed new statistically - inspired leveraging algorithms that achieve improved statistical performance , while maintaining the algorithmic benefits of the usual leverage - based method .",
    "our empirical evaluation demonstrates that our theory is a good predictor of the practical performance of both existing as well as our newly - proposed leverage - based algorithms .",
    "in addition , our empirical evaluation demonstrates that , by using a recently - developed algorithm to approximate the leverage scores , we can compute improved approximate solutions for much larger least - squares problems than we can compute the exact solutions with traditional deterministic  algorithms .",
    "finally , we should note that , while our results are straightforward and intuitive , obtaining them was not easy , in large part due to seemingly - minor differences between problem formulations in statistics , computer science , machine learning , and numerical linear algebra .",
    "now that we have `` bridged the gap '' by providing a statistical perspective on a recently - popular algorithmic framework , we expect that one can ask even more refined statistical questions of this and other related algorithmic frameworks for large - scale computation .",
    "in this section , we will relate our analytic methods to the notion of asymptotic relative efficiency , and we will consider several toy data sets that illustrate various aspects of algorithmic leveraging . although the results of this section are not used elsewhere , and thus some readers may prefer skip this section , we include it in order to relate our approach to ideas that may be more familiar to certain readers .      here ,",
    "we present an asymptotic analysis comparing unif with lev , slev , and levunw in terms of their relative efficiency . recall that one natural way to compare two procedures",
    "is to compare the sample sizes at which the two procedures meet a given standard of performance .",
    "one such standard is efficiency , which addresses how `` spread out '' about @xmath30 is the estimator . in this case , the smaller the variance , the more `` efficient '' is the estimator @xcite . since @xmath30 is a @xmath2-dimensional vector , to determine the relative efficiency of two estimators , we consider the linear combination of @xmath30 , i.e. , @xmath263 , where @xmath264 is the linear combination coefficient . in somewhat more detail , when @xmath46 and @xmath265 are two one - dimensional estimates , their relative efficiency can be defined  as @xmath266 and when @xmath46 and @xmath265 are two @xmath2-dimensional estimates , we can take their linear combinations @xmath267 and @xmath268 , where @xmath264 is the linear combination coefficient vector , and define their relative efficiency  as @xmath269    in order to discuss asymptotic relative efficiency , we start with the following seemingly - technical observation .",
    "a @xmath270 matrix a is said to be @xmath271 if and only if every element of @xmath272 satisfies @xmath273 for @xmath274 .",
    "[ ass : ass1 ] @xmath275 is positive definite and @xmath276",
    ".    * remark . *",
    "assuming @xmath277 is nonsingular , for a ls estimator @xmath54 to converge to true value @xmath30 in probability , it is sufficient and necessary that @xmath278 as @xmath12  @xcite .    * remark .",
    "* although we have stated this as an assumption , one typically assumes an @xmath1-dependence for @xmath279  @xcite .",
    "since the form of the @xmath1-dependence is unspecified , we can alternatively view assumption  [ ass : ass1 ] as a definition of @xmath279 .",
    "the usual assumption that is made ( typically for analytical convenience ) is that @xmath280  @xcite",
    ". we will provide examples of toy data for which @xmath280 , as well as examples for which @xmath281 . in light of our empirical results in section  [ sxn : empirical ] and the empirical observation that leverage scores are often very nonuniform  @xcite , it is an interesting question to ask whether the common assumption that @xmath280 is too restrictive , e.g. , whether it excludes interesting matrices @xmath5 with very heterogeneous leveraging  scores .",
    "under assumption  [ ass : ass1 ] , i.e. , that @xmath282 is asymptotically parameterized as @xmath276 , we have the following three results to compare the leveraging estimators and the uniform sampling estimator .",
    "the expressions in these three lemmas are complicated ; and , since they are expressed in terms of @xmath279 , they are not easy to evaluate on real or synthetic data .",
    "( it is partly for this reason that our empirical evaluation is in terms of the bias and variance of the subsampling estimators . )",
    "we start by stating a lemma characterizing the relative efficiency of lev and unif ; the proof of this lemma may be found in appendix  [ sxn : app - proofs - ratio - lev ] .",
    "[ variance - ratio ] to leading order , the asymptotic relative efficiency of @xmath283 and @xmath284 is @xmath285 where the residual variance is ignored .",
    "next , we state a lemma characterizing the relative efficiency of slev and unif ; the proof of this lemma is similar to that of lemma  [ variance - ratio ] and is thus omitted .    [",
    "thm : slev - variance - ratio ] to leading order , the asymptotic relative efficiency of @xmath286 and @xmath284 is @xmath287 where the residual variance is ignored .",
    "finally , we state a lemma characterizing the relative efficiency of levunw and unif ; the proof of this lemma may be found in appendix  [ sxn : app - proofs - ratio - unweighted ] .",
    "[ levvariance - ratio ] to leading order , the asymptotic relative efficiency of @xmath288 and @xmath284 is @xmath289 where the residual variance is ignored and @xmath290 .    of course , in an analogous manner , one could derive expressions for the asymptotic relative efficiencies @xmath291 , @xmath292 , and @xmath293 .      here",
    ", we will consider several toy data sets that illustrate various aspects of algorithmic leveraging , including various extreme cases of the method . while some of these toy data may seem artificial or contrived",
    ", they will highlight properties that manifest themselves in less extreme forms in the more realistic data in section  [ sxn : empirical ] .",
    "since the leverage score structure of the matrix @xmath5 is crucial for the behavior of the method , we will focus primarily on that structure . to do so , consider the two extreme cases . at one extreme , when the leverage scores are all equal , i.e. , @xmath294 , for all @xmath295 $ ] , the first two variance terms in eqn .",
    "( [ uniform - var1 ] ) are equal to the first two variance terms in eqn .",
    "( [ lsq - wls - var - r - d ] ) . in this case",
    ", lev simply reduces to unif . at the other extreme",
    ", the leverage scores can be very nonuniform  e.g . , there can be a small number of leverage scores that are much larger than the rest and/or there can be some leverage scores that are much smaller than the mean score . dealing with these two cases properly is crucial for the method of algorithmic leveraging , but these two cases highlight important differences between the more common algorithmic perspective and our more novel statistical perspective .",
    "the former problem ( of a small number of very large leverage scores ) is of particular importance from an algorithmic perspective .",
    "the reason is that in that case one wants to compare the output of the sampling algorithm with the optimum based on the empirical data ( as opposed to the `` ground truth '' solution ) .",
    "thus , dealing with large leverage scores was a main issue in the development of the leveraging paradigm  @xcite .",
    "on the other hand , the latter problem ( of some very small leverage scores ) is also an important concern if we are interested in statistical properties of algorithmic leveraging . to see why , consider ,",
    "e.g. , the extreme case that a few data points have very very small leverage scores , e.g. @xmath296 for some @xmath243 . in this case , e.g. , the second variance term in eqn .",
    "( [ lsq - wls - var - r - d ] ) will be much larger than the second variance term in eqn .",
    "( [ uniform - var1 ] ) .    in light of this discussion , here are several toy examples to consider",
    ". we will start with several examples where @xmath297 that illustrate things in the simplest setting .    *",
    "* example 1a : sample mean . *",
    "let @xmath1 be arbitrary , @xmath297 , and let the @xmath4 matrix @xmath5 be such that @xmath298 , for all @xmath45 $ ] , i.e. , let @xmath5 be the all - ones vector .",
    "in this case , @xmath299 and @xmath300 , for all @xmath45 $ ] , i.e. , the leverage scores are uniform , and thus algorithmic leveraging reduces to uniform sampling .",
    "also , in this case , @xmath301 in assumption  [ ass : ass1 ] .",
    "all three asymptotic efficiencies are equal to @xmath302 . *",
    "* example 1b : simple linear combination .",
    "* let @xmath1 be arbitrary , @xmath297 , and let the @xmath4 matrix @xmath5 be such that @xmath303 , for all @xmath45 $ ] , either uniformly at random , or such that @xmath304 if @xmath243 is odd and @xmath305 if @xmath243 is even .",
    "in this case , @xmath299 and @xmath300 , for all @xmath45 $ ] , i.e. , the leverage scores are uniform ; and , in addition , @xmath301 in assumption  [ ass : ass1 ] . for all four estimators ,",
    "all four unconditional variances are equal to @xmath306 . in addition , for all four estimators , all three relative efficiencies are equal to @xmath302 . *",
    "* example 2 : `` inflated '' regression line through origin .",
    "* let @xmath1 be arbitrary , @xmath297 , and let the @xmath4 matrix @xmath5 be such that @xmath307 , i.e. , they are evenly spaced and increase without limit with increasing @xmath243 .",
    "( we thus refer the @xmath5 as `` inflated . '' ) in this case , @xmath308 and the leverage scores equal @xmath309 i.e. , the leverage scores @xmath44 are very nonuniform .",
    "this is illustrated in the left panel of figure  [ fig : theorem ] .",
    "also , in this case , @xmath310 in assumption  [ ass : ass1 ] .",
    "it is easy to see that the first variance components of unif , lev , slev are the same , i.e. , they equal @xmath311 it is also easy to see that variances of lev , slev and unif are dominated by their second variance component .",
    "the leading terms of the second variance component of lev and unif are the same , and we expect to see the similar performance based on their variance . the leading term of the second variance component of slev is smaller than that of lev and unif ; and thus slev has smaller variance than lev and unif .",
    "simple calculation shows that levunw has a smaller leading term for the second variance component than those of lev , unif and slev .",
    "* * example 3 : `` in - fill '' regression line through origin .",
    "* let @xmath1 be arbitrary , @xmath297 , and let the @xmath4 matrix @xmath5 be such that @xmath312 .",
    "this is different than the evenly spaced data points in the `` inflated '' toy example since the unevenly spaced data points this this example get denser in the interval @xmath313 $ ] .",
    "the asymptotic properties of such design matrix are so - called `` in - fill '' asymptotics  @xcite . in this case , @xmath314 where @xmath315 is the @xmath61 derivative of digamma function , and the leverage scores equal @xmath316 i.e. , the leverage scores @xmath44 are very nonuniform .",
    "this is illustrated in the middle panel of figure  [ fig : theorem ] .",
    "also , in this case , @xmath317 in assumption  [ ass : ass1 ] .    to obtain an improved understanding of these examples , consider the first two panels of figures  [ fig : theorem ] and  [ fig : theorem2 ] .",
    "figure  [ fig : theorem ] shows the sampling probabilities for the inflated regression line and the in - fill regression line .",
    "both the inflated regression line and the in - fill regression line have very nonuniform leverage scores , and by construction there is a natural ordering such that the leverage scores increase or decrease respectively . for the inflated regression line , the minimum , mean , and maximum leverage scores are @xmath318 , @xmath224 , and @xmath319 , respectively ; and for the in - fill regression line , the minimum , mean , and maximum leverage scores are @xmath320 , @xmath224 , and @xmath321 , respectively . for reference , note that for the sample mean ( as well as for the simple linear combination ) all of the the leverage scores are equal to @xmath224 , which equals @xmath322 for the value of @xmath323 used in figure  [ fig : theorem ] .",
    "figure  [ fig : theorem2 ] illustrates the theoretical variances for the same examples for particular values of @xmath324 and @xmath59 .",
    "in particular , observe that for the inflated regression line , all three sampling methods tend to have smaller variance as @xmath1 is increased for a fixed value of @xmath2 .",
    "this is intuitive , and it is a common phenomenon that we observe in most of the synthetic and real data sets .",
    "the property of the in - fill regression line where the variances are roughly flat ( actually , they increase slightly ) is more uncommon , but it illustrates that other possibilities exist .",
    "the reason is that leverage scores of most data points are relatively homogeneous ( as long as @xmath243 is greater than @xmath325 , the leverage score of @xmath243th observation is less than mean @xmath224 but greater than @xmath326 ) .",
    "when subsample size @xmath59 is reasonably large , we have high probabilities to sample these data points , whose sample probabilities inflate the variance .",
    "these curves also illustrate that lev and unif can be better or worse with respect to each other , depending on the problem parameters ; and that slev and levunw can be better than either , for certain parameter values .    from these examples",
    ", we can see that the variance for the leveraging estimate can be inflated by very small leverage scores .",
    "that is , since the variances involve terms that depend on the inverse of @xmath44 , they can be large if @xmath44 is very small . here",
    ", we note that the common practice of adding an intercept , i.e. , a sample mean or all - ones vector _ tends _ to uniformize the leverage scores .",
    "that is , in statistical model building applications , we usually have intercept  which is an all - ones vector , called the sample mean above  in the model , i.e. , the first column of @xmath5 is @xmath327 vector ; and , in this case , the @xmath44s are bounded below by @xmath224 and above by @xmath328  @xcite .",
    "this is also illustrated in figure  [ fig : theorem ] , which shows the the leverage scores for when an intercept is included .",
    "interestingly , for the inflated regression line , the scores for elements that originally had very small score actually increase to be on par with the largest scores . in our experience , it is much more common for the small leverage scores to simply be increased a bit , as is illustrated with the modified scores for the in - fill regression line .",
    "we continue the toy examples with an example for @xmath329 ; this is the simplest case that allows us to look at what is behind assumption  [ ass : ass1 ] .",
    "* * example 4 : regression surface through origin .",
    "* let @xmath329 and @xmath330 be even .",
    "let the elements of @xmath5 be defined as @xmath331 , and @xmath332 . in this case ,",
    "@xmath333 and the leverage scores equal @xmath334 here , @xmath301 in assumption  [ ass : ass1 ] , and the largest leverage score does _ not _ converge to zero .    to see the leverage scores and the ( theoretically - determined ) variance for the regression surface of example 4 ,",
    "see the third panel of figures  [ fig : theorem ] and  [ fig : theorem2 ] .",
    "in particular , the third panel of figure  [ fig : theorem ] demonstrates what we saw with the @xmath297 examples , i.e. , that adding an intercept tends to increase the small leverage scores ; and figure  [ fig : theorem2 ] illustrates that the variances of all four estimates are getting close as sample size @xmath1 becomes larger .    * remark . *",
    "it is worth noting that @xcite showed @xmath301 in assumption  [ ass : ass1 ] implies that @xmath335 . in his proof ,",
    "miller essentially assumed that @xmath336 , @xmath337 is a single sequence .",
    "example 4 shows that miller s theorem does not hold for triangular array ( with one pattern for even numbered observations and the other pattern for odd numbered observations ) @xcite .",
    "finally , we consider several toy data sets with larger values of @xmath2 . in this case",
    ", there starts to be a nontrivial interaction between the singular value structure and the singular vector structure of the matrix @xmath5 .    *",
    "* example 5 : truncated hadamard matrix . *",
    "an @xmath4 matrix consisting of @xmath2 columns from a hadamard matrix ( which is an orthogonal matrix ) has uniform leverage scores  all are equal .",
    "similarly , for an @xmath4 matrix with entries i.i.d . from gaussian distribution  that is , unless the aspect ratio of the matrix is extremely rectangular , e.g. , @xmath297 , the leverage scores of a random gaussian matrix are very close to uniform .",
    "( in particular , as our empirical results demonstrate , using nonuniform sampling probabilities is not necessary for data generated from gaussian random matrices . ) * * example 6 : truncated identity matrix . *",
    "an @xmath4 matrix consisting of the first @xmath2 columns from an identity matrix ( which is an orthogonal matrix ) has very nonuniform leverage scores  the first @xmath2 are large , and the remainder are zero .",
    "( since one could presumably remove the all - zeros rows , this example might seem trivial , but it is useful as a worst - case thought experiment . ) * * example 7 : worst - case matrix . * an @xmath4 matrix consisting of @xmath338 rows all pointing in the same direction and @xmath172 row pointing in some other direction .",
    "this has one leverage score ",
    "the one corresponding to the row pointing in the other direction  that is large , and the rest are mediumly - small .",
    "( this is an even better worst - case matrix than example 6 ; and in the main text we have an even less trivial example of this . )",
    "example 5 is `` nice '' from an algorithmic perspective and , as seen in section  [ sxn : empirical ] , from a statistical perspective as well .",
    "since they have nonuniform leverage scores ; example 6 and example 7 are worse from an algorithmic perspective .",
    "as our empirical results will demonstrate , they are also problematic from a statistical perspective , but for slightly different reasons .",
    "in this section , we will provide proofs of several of our main results .        by performing a taylor expansion of @xmath108 around the point @xmath113",
    ", we have @xmath339 where the second order remainder @xmath120 when @xmath119 is close to @xmath112 . by setting @xmath112 as the all - one vector , i.e. , @xmath113",
    ", @xmath340 is expanded around the full sample ordinary ls estimate @xmath54 , i.e. , @xmath115 .",
    "that is , @xmath341 by differentiation by parts , we obtain @xmath342}{\\partial { \\boldsymbol}{w}^{t}}\\notag\\\\ & = ( { \\boldsymbol}{1}\\otimes ( x^{t}{\\mbox{}{diag}\\left\\{{\\boldsymbol}{w}\\right\\}}x)^{-1})\\frac{\\partial \\text{vec}[x^{t}{\\mbox{}{diag}\\left\\{{\\boldsymbol}{w}\\right\\}}{\\boldsymbol}{y}]}{\\partial { \\boldsymbol}{w}^{t}}\\label{lsq - wls - taylor-2a1}\\\\ & + ( { \\boldsymbol}{y}^{t}{\\mbox{}{diag}\\left\\{{\\boldsymbol}{w}\\right\\}}x\\otimes i_{p})\\frac{\\partial \\text{vec}[(x^{t}{\\mbox{}{diag}\\left\\{{\\boldsymbol}{w}\\right\\}}x)^{-1}]}{\\partial { \\boldsymbol}{w}^{t}}\\label{lsq - wls - taylor-2b1}\\end{aligned}\\ ] ] where @xmath343 is vec operator , which stacks the columns of a matrix into a vector , and @xmath344 is the kronecker product .",
    "the kronecker product is defined as follows : suppose @xmath345 is an @xmath346 matrix and @xmath347 is a @xmath348 matrix ; then , @xmath349 is a @xmath350 matrix , comprising @xmath351 rows and @xmath1 columns of @xmath348 blocks , the @xmath352th of which is @xmath353 .",
    "to simplify ( [ lsq - wls - taylor-2a1 ] ) , note that is easy to show that ( [ lsq - wls - taylor-2a1 ] ) can be seen as @xmath354}{\\partial { \\boldsymbol}{w}^{t}}.\\ ] ] to simplify ( [ lsq - wls - taylor-2b1 ] ) , we need the following two results of matrix differentiation , @xmath355}{\\partial ( \\text{vec}x)^{t } } & = -(x^{-1})^{t}\\otimes x^{-1 } , \\mbox { and}\\notag\\\\ \\frac{\\partial \\text{vec}[awb]}{\\partial { \\boldsymbol}{w}^{t } } & = ( b^{t}\\otimes a)\\frac{\\partial \\text{vec}[w]}{\\partial { \\boldsymbol}{w}^{t}},\\end{aligned}\\ ] ] where the details on these two results can be found on page 366 - 367 of  @xcite . by combining the two results in ( [ harville ] ) , by the chain rule , we have @xmath356}{\\partial { \\boldsymbol}{w}^{t } } \\\\     & = & \\frac{\\partial \\text{vec}[(x^{t}{\\mbox{}{diag}\\left\\{{\\boldsymbol}{w}\\right\\}}x)^{-1}]}{\\partial \\text{vec}[(x^{t}{\\mbox{}{diag}\\left\\{{\\boldsymbol}{w}\\right\\}}x)]^{t}}\\frac{\\partial \\text{vec}[(x^{t}{\\mbox{}{diag}\\left\\{{\\boldsymbol}{w}\\right\\}}x)]}{\\partial { \\boldsymbol}{w}^{t}}\\\\     & = & -(x^{t}{\\mbox{}{diag}\\left\\{{\\boldsymbol}{w}\\right\\}}x)^{-1 } \\otimes ( x^{t}{\\mbox{}{diag}\\left\\{{\\boldsymbol}{w}\\right\\}}x)^{-1}(x^{t}\\otimes x^{t})\\frac{\\partial \\text{vec}[{\\mbox{}{diag}\\left\\{{\\boldsymbol}{w}\\right\\}}]}{\\partial { \\boldsymbol}{w}^{t}}\\end{aligned}\\ ] ] by simple but tedious algebra , ( [ lsq - wls - taylor-2a1 ] ) and ( [ lsq - wls - taylor-2b1 ] ) give rise to @xmath357}{\\partial { \\boldsymbol}{w}^{t}}\\\\ = \\{({\\boldsymbol}{y}-x\\tilde{{\\boldsymbol}{\\beta}}_{w}({\\boldsymbol}{w}))^{t}\\otimes ( x^{t}{\\mbox{}{diag}\\left\\{{\\boldsymbol}{w}\\right\\}}x)^{-1}x^{t}\\}\\frac{\\partial \\text{vec}[{\\mbox{}{diag}\\left\\{{\\boldsymbol}{w}\\right\\}}]}{\\partial{\\boldsymbol}{w}^{t}}\\end{gathered}\\ ] ] by combining these results , we thus have , @xmath358 where @xmath117 is the ls residual vector , @xmath359 is a length @xmath1 vector with @xmath36 element equal to one and all other elements equal to zero , from which the lemma follows .",
    "we start by establishing the conditional result . since @xmath111}={\\boldsymbol}{1}$ ] , it is straightforward to calculate conditional expectation of @xmath101 . then , it is easy to see that @xmath360 }      & = \\frac{1}{r \\pi_i}-\\frac{1}{r } \\quad \\text{for}\\quad",
    "i = j\\\\     & = -\\frac{1}{r }   \\quad\\text{for}\\quad i \\neq j   .\\end{aligned}\\ ] ] we rewrite it in matrix form , @xmath361}={\\mbox{}{\\bf{e}}\\left[({\\boldsymbol}{w}-{\\boldsymbol}{1})({\\boldsymbol}{w}-{\\boldsymbol}{1})^t\\right]}={\\mbox{}{diag}\\left\\{\\frac{1}{r{\\boldsymbol}{\\pi}}\\right\\}}-\\frac{1}{r}j_n   , \\ ] ] where @xmath362 and @xmath363 is a @xmath364 matrix of ones .",
    "some additional algebra yields that the variance of @xmath101 is @xmath365 } & = { \\mbox{}{\\bf{var}}\\left [ ( x^{t}x)^{-1}x^{t}{\\mbox{}{diag}\\left\\{\\hat{{\\boldsymbol}{e}}\\right\\ } } ( { \\boldsymbol}{w}-{\\boldsymbol}{1})|{\\boldsymbol}{y}\\right ] } + { \\mbox{}{\\bf{var}_{w}}\\left[r_{w}\\right]}\\\\   & = ( x^{t}x)^{-1}x^{t}{\\mbox{}{diag}\\left\\{\\hat{{\\boldsymbol}{e}}\\right\\}}({\\mbox{}{diag}\\left\\{\\frac{1}{r{\\boldsymbol}{\\pi}}\\right\\}}-\\frac{1}{r}j_n ) { \\mbox{}{diag}\\left\\{\\hat{{\\boldsymbol}{e}}\\right\\ } } x(x^{t}x)^{-1 }   + { \\mbox{}{\\bf{var}_{w}}\\left[r_{w}\\right]}\\\\   & = ( x^{t}x)^{-1}x^{t}[{\\mbox{}{diag}\\left\\{\\hat{{\\boldsymbol}{e}}\\right\\}}{\\mbox{}{diag}\\left\\{\\frac{1}{r{\\boldsymbol}{\\pi}}\\right\\ } } { \\mbox{}{diag}\\left\\{\\hat{{\\boldsymbol}{e}}\\right\\}}]x(x^{t}x)^{-1 }    + { \\mbox{}{\\bf{var}}\\left[r_{w}\\right]}\\\\   & = ( x^{t}x)^{-1}x^{t}{\\mbox{}{diag}\\left\\{\\frac{1}{r{\\boldsymbol}{\\pi}}\\hat{{\\boldsymbol}{e}}^2\\right\\}}x(x^{t}x)^{-1 }    +   { \\mbox{}{\\bf{var}_{w}}\\left[r_{w}\\right]}.\\end{aligned}\\ ] ] setting @xmath366 in above equations , we thus prove the conditional result .",
    "we next establish the unconditional result as follows .",
    "if we take one more expectation of the expectation result in lemma [ lem : lev - bv ] with respect to the response , then we have the expectation result . by rule of double expectations",
    ", we have the variance of @xmath101 result , from which the lemma follows .      first note that the unweighted leveraging estimate @xmath88 can be written as @xmath367 where @xmath368 , and where @xmath369 has a multinomial distribution @xmath370 .",
    "the proof of this lemma is analogous to the proof of lemma  [ lem : taylor ] ; and so here we provide only some details on the differences . by employing a taylor expansion",
    ", we have @xmath371 where @xmath372 . following the proof of the previous lemma",
    ", we have  that @xmath373 where @xmath374 , @xmath375 , @xmath376 is the weighted ls residual vector , @xmath359 is a length @xmath1 vector with @xmath36 element equal to one and all other elements equal to zero . from this",
    "the lemma follows .      by taking the conditional expectation of taylor expansion of the levunw estimate @xmath88 in lemma  [ lem : unwl - taylor ]",
    ", we have that @xmath377 }    = \\hat{{\\boldsymbol}{\\beta}}_{wls } + ( x^{t}w_0 x)^{-1}x^{t}{\\mbox{}{diag}\\left\\{\\hat{{\\boldsymbol}{e}}_w\\right\\ } } { \\mbox{}{\\bf{e}_{w}}\\left[{\\boldsymbol}{w}-r{\\boldsymbol}{\\pi}\\right ] } + { \\mbox{}{\\bf{e}_{w}}\\left[r_{levunw}\\right ] }   .\\end{aligned}\\ ] ] since @xmath378}=r{\\boldsymbol}{\\pi}$ ] , the conditional expectation is thus obtained .",
    "since @xmath369 is multinomial distributed , we have @xmath379}={\\mbox{}{\\bf{e}}\\left[({\\boldsymbol}{w}_{levunw}-r{\\boldsymbol}{\\pi})({\\boldsymbol}{w}_{levunw}-r{\\boldsymbol}{\\pi})^t\\right]}={\\mbox{}{diag}\\left\\{r{\\boldsymbol}{\\pi}\\right\\}}-{r}{\\boldsymbol}{\\pi}{\\boldsymbol}{\\pi}^{t }   .\\ ] ] some algebra yields that the conditional variance of @xmath88 is @xmath380 } \\\\     & = & { \\mbox{}{\\bf{var}_{w}}\\left [ ( x^{t}w_0 x)^{-1}x^{t}{\\mbox{}{diag}\\left\\{\\hat{{\\boldsymbol}{e}}_w\\right\\}}({\\boldsymbol}{w}_{levunw}-r{\\boldsymbol}{\\pi})|{\\boldsymbol}{y } \\right ] } + { \\mbox{}{\\bf{var}_{w}}\\left[r_{levunw}\\right]}\\\\     & = & ( x^{t}w_0x)^{-1}x^{t}{\\mbox{}{diag}\\left\\{\\hat{{\\boldsymbol}{e}}_w\\right\\}}w_0{\\mbox{}{diag}\\left\\{\\hat{{\\boldsymbol}{e}}_w\\right\\ } } x(x^{t}w_0 x)^{-1 }    +   { \\mbox{}{\\bf{var}_{w}}\\left[r_{levunw}\\right]}.\\end{aligned}\\ ] ] finally , note that @xmath381}= ( x^{t}w_0x)^{-1}xw_0{\\mbox{}{\\bf{e}}\\left[{\\boldsymbol}{y}\\right]}= ( x^{t}w_0x)^{-1}xw_0x{\\boldsymbol}{\\beta}_{0}= { \\boldsymbol}{\\beta}_{0}.\\ ] ] from this the lemma follows .      since @xmath382",
    ", we shall the derive the asymptotic order of @xmath383 .",
    "the second variance component of @xmath81 in ( [ lsq - wls - var - r - d ] ) is seen to be @xmath384 where cauchy - schwartz inequality has been used .",
    "next , we show that @xmath385 to see this , observe that @xmath386 thus , the second variance component of @xmath81 in ( [ lsq - wls - var - r - d ] ) is of the order of @xmath387 analogously , the second variance component of @xmath80 in ( [ uniform - var1 ] ) is of the order of @xmath388 the lemma then follows immediately .",
    "it is easy to see that @xmath389 .",
    "the second variance component of @xmath88 in ( [ levnoweightvar11 ] ) is seen to be @xmath390 where cauchy - schwartz inequality has used .",
    "next , we show that @xmath391 to see this , observe that @xmath392 thus , the second variance component of @xmath88 in ( [ levnoweightvar11 ] ) is of the order of @xmath393 the lemma then follows immediately .",
    "k.  l. clarkson , p.  drineas , m.  magdon - ismail , m.  w. mahoney , x.  meng , and d.  p. woodruff .",
    "the fast cauchy transform and faster robust linear regression .",
    "technical report .",
    "preprint : arxiv:1207.4684 ( 2012 ) .",
    "p.  drineas , m.  w. mahoney , and s.  muthukrishnan .",
    "sampling algorithms for @xmath394 regression and applications . in _ proceedings of the 17th annual acm - siam symposium on discrete algorithms _ , pages 11271136 , 2006 .",
    "t.  nielsen , r.  b. west , s.  c. linn , o.  alter , m.  a. knowling , j.  oconnell , s.  zhu , m.  fero , g.  sherlock , j.  r. pollack , p.  o. brown , d.  botstein , and m.  van  de rijn .",
    "molecular characterisation of soft tissue tumours : a gene expression study .",
    ", 359(9314):13011307 , 2002 ."
  ],
  "abstract_text": [
    "<S> one popular method for dealing with large - scale data sets is sampling . </S>",
    "<S> for example , by using the empirical statistical leverage scores as an importance sampling distribution , the method of _ algorithmic leveraging _ samples and rescales rows / columns of data matrices to reduce the data size before performing computations on the subproblem . </S>",
    "<S> this method has been successful in improving computational efficiency of algorithms for matrix problems such as least - squares approximation , least absolute deviations approximation , and low - rank matrix approximation . </S>",
    "<S> existing work has focused on algorithmic issues such as worst - case running times and numerical issues associated with providing high - quality implementations , but none of it addresses statistical aspects of this method .    in this paper </S>",
    "<S> , we provide a simple yet effective framework to evaluate the statistical properties of algorithmic leveraging in the context of estimating parameters in a linear regression model with a fixed number of predictors . in particular , for several versions of leverage - based sampling , </S>",
    "<S> we derive results for the bias and variance , both conditional and unconditional on the observed data . </S>",
    "<S> we show that from the statistical perspective of bias and variance , neither leverage - based sampling nor uniform sampling dominates the other . </S>",
    "<S> this result is particularly striking , given the well - known result that , from the algorithmic perspective of worst - case analysis , leverage - based sampling provides uniformly superior worst - case algorithmic results , when compared with uniform sampling .    </S>",
    "<S> based on these theoretical results , we propose and analyze two new leveraging algorithms : one constructs a smaller least - squares problem with `` shrinked '' leverage scores ( slev ) , and the other solves a smaller and unweighted ( or biased ) least - squares problem ( levunw ) . </S>",
    "<S> a detailed empirical evaluation of existing leverage - based methods as well as these two new methods is carried out on both synthetic and real data sets . </S>",
    "<S> the empirical results indicate that our theory is a good predictor of practical performance of existing and new leverage - based algorithms and that the new algorithms achieve improved performance . </S>",
    "<S> for example , with the same computation reduction as in the original algorithmic leveraging approach , our proposed slev typically leads to improved biases and variances both unconditionally and conditionally ( on the observed data ) , and our proposed levunw typically yields improved unconditional biases and variances . </S>"
  ]
}