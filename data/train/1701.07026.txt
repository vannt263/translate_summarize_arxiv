{
  "article_text": [
    "according to the standard hierarchical structure formation theory based on the @xmath0cdm cosmology model , density perturbations grow into small haloes where the first stars and galaxies form ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "these astrophysical sources produce ultraviolet  ( uv ) ionizing photons that escape into the intergalactic medium  ( igm ) which is eventually reionized .",
    "the epoch when this process occurs is called the `` epoch of reionization ''  ( eor ) .",
    "the detailed astrophysics of the eor is currently poorly understood because no observation has yet probed the early and middle stages of the eor although current observations have provided fruitful information on late stage of the eor .",
    "for example , the absorption spectra of high-@xmath1 quasars indicate that reionization was complete by @xmath2 ( e.g. * ? ? ? * ) and the number density of ly@xmath3 emitter galaxies at @xmath4 implies that the neutral hydrogen fraction increases at @xmath5 ( e.g. * ? ? ? * ) . on cosmological scales",
    ", reionization induces thomson scattering of cmb photons off free electrons .",
    "the optical depth of thomson scattering measured by planck is @xmath6 which corresponds , using an instantaneous reionization toy model , to a reionization at @xmath7  @xcite .    to further improve our understanding of the eor , observations targeted on the cosmological redshifted 21 cm signal from the eor are on - going .",
    "the 21 cm signal emission is due to the hyperfine structure of neutral hydrogen atoms and is expected to be a powerful tool to probe the neutral igm , yielding both astrophysical and cosmological information such as matter density fluctuations and the ionization state and thermal history of the igm at high redshift  @xcite .",
    "recently , some first - generation radio interferometers have been attempting to detect statistically the 21 cm signal from the eor , such as the murchison wide field array ( mwa )  @xcite , the low frequency array ( lofar )  @xcite and the precision array for probing the epoch of reionization ( paper )  @xcite .",
    "these observational efforts have resulted in upper limits of the 21 cm power spectrum and on the ionized state of the igm at @xmath8   ( e.g. * ? ? ? * ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "furthermore , future instruments such as the square kilometre array ( ska )  @xcite and hydrogen epoch of reionization array ( hera )  @xcite are designed to detect the 21 cm signal power spectrum with higher signal to noise ratio and at higher redshift , during the cosmic dawn .",
    "the ska should also be able to image the signal in 3d , which requires high sensitivity on small enough scale ( that is sufficient collecting area in a large enough core ) .",
    "so we expect a wealth of 21 cm data in the near future .",
    "then we face the fundamental question : what we can learn from the data ? from theoretical and numerical works",
    ", we have already some insights on the process of reionization . for instance , considering galaxies in relatively massive host haloes results in larger and more uniform ionized bubbles and imprints a peak at larger scales in the 21 cm power spectrum ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "on the other hand , abundant and small minihaloes , which serve as absorption systems by self - shielding from ionizing photons , result in small and disjointed ionized regions  ( e.g. * ? ? ? * ) . to extract information on the eor from the observed 21 cm signal",
    ", we need to be able compute the 21 cm signal from the basic physics of reionization ( e.g. * ? ? ?",
    "the most self - consistent method is to run numerical simulations .",
    "there are currently two approaches .",
    "the first is to implement the full radiation hydrodynamics ( rhd ) .",
    "this type of simulations is relevant if small scales , where feedback effects such as photo - heating play an important role , are resolved ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "the second approach is to run large scale simulations where radiative transfer  ( rt ) is computed in post - processing  ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "this type of simulations are able to account for large scales fluctuations in the signal but they require sub - grid modelling to treat processes on galaxy scales ( feedback , ionizing photons escape fraction , etc . ) .",
    "although numerical study based on simulations is the most consistent method , it implies a large computational cost .",
    "this disadvantage is somewhat alleviated by using a semi - numerical approach instead of rhd or rt simulations . in most semi - numerical simulations ,",
    "the production of ionizing photons is calculated based on the excursion set formalism , using analytically derived halo mass functions , and density fluctuations are obtained with linear perturbation theory  @xcite .",
    "however , the growth of hii regions is simply evaluated by considering the balance between the production of ionizing photons and the number of neutral hydrogen atoms @xcite .",
    "recently , the effect of recombination is also taken into account ( e.g. * ? ? ?",
    "the results produced by the semi - numerical approach shows good agreement with those by rt simulations on large scale ( @xmath9 mpc )  @xcite .    to maximize the scientific return of the upcoming observations it is important to establish systematic procedures to derive constraints on the eor modeling parameters from the observed 21-cm data",
    "recently , several works have studied how such constraints can be obtained by exploring the eor parameter space with semi - numerical simulations . for example , fisher analysis  @xcite and bayesian parameter inference such as the markov chain monte carlo  ( mcmc ) approach  @xcite have been applied .    in this work , we suggest a new approach for parameter reconstruction based on a machine learning method .",
    "machine learning is one of the hot topics in data science as a method to deal with big data .",
    "it is currently applied to many fields such as pattern recognition or search engine .",
    "the main purpose of machine learning is to find approximate functions that , given the input produce the desired outputs .",
    "this is achieved by  learning \" from training datasets with known inputs and outputs ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "recently , machine learning methods have been applied in the field of astronomy .",
    "for example , learning from huge galaxy image catalogs helps with morphological classification of galaxies  @xcite .",
    "machine learning can also help with the selection and classification of transients  @xcite .",
    "applying machine learning to a large sample of spectroscopic and photometric galaxies data can improve the accuracy of estimates of photometric redshifts  . using simulated gravitational wave data with noise as learning sample ,",
    "machine learning can help us search for gravitational wave signals from noisy real data  @xcite . in the context of cosmology , machine learning",
    "is used to model galaxy formation  @xcite or to make templates of nonlinear matter power spectrum  @xcite .",
    "closer to our field of interest , there is a study applying a machine learning method to estimate the escape fraction of ionizing photons during the eor  @xcite . in this study , they show how machine learning can estimate the lyman continuum escape fraction by using mock spectroscopic simulation data . in our work , using a simple astrophysical parameterization of the eor , we apply artificial neural network ( ann ) , which is one of the machine learning methods , to reconstruct the parameter values from the 21 cm power spectrum data .",
    "this paper is organised as follows . in section [ sec:21 cm ]",
    ", we introduce the cosmological 21 cm signal and the eor parameterization we focus on . in section [ sec : ann ] , we describe our artificial neural network and test the impact of its chosen architecture . in section [ sec : result ] we show our main results , and we give a summary and discussion in section  [ sec : summary ] . throughout this paper , we employ the best fit cosmological parameters obtained by .",
    "the brightness temperature for the @xmath10 cm signal is given by ( e.g. * ? ? ?",
    "* ) :    @xmath11 .",
    "\\label{eq : brightness}\\end{aligned}\\ ] ]    here , @xmath12 and @xmath13 represent the local spin temperature of the igm and the cmb temperature , respectively .",
    "@xmath14 is the local optical depth in the 21 cm rest frame frequency @xmath15 , @xmath16 is the local neutral fraction of the hydrogen gas , @xmath17 is the evolved matter overdensity , @xmath18 is the local gradient of the gas velocity along the line of sight and @xmath19 is the hubble parameter .",
    "all quantities are evaluated at redshift @xmath20 .",
    "as we can see , the 21 cm signal includes both astrophysical and cosmological information .",
    "thus , we can hope to use the 21 cm signal to disentangle and quantify them @xcite .",
    "let us now introduce the power spectrum of the 21 cm fluctuations .",
    "we define the 21 cm power spectrum as @xmath21 in our context , we use the _ dimensional _ 21 cm power spectrum , @xmath22.the 21 cm ps would describe the statistical properties of the 21 cm fluctuations perfectly if they were a gaussian random field .",
    "however , the 21 cm fluctuations are expected to deviate from a gaussian behaviour because of astrophysical effects such as ionization and x - ray heating .",
    "thus it is useful to compute higher order statistics and one - point statistics such as the bispectrum , the variance and the skewness to estimate non - gaussian features in the 21 cm fluctuations  .    in order to generate the 21 cm ps for a given set of astrophysical parameters",
    ", we use the publicly available code * 21cmfast *  @xcite .",
    "this code is based on a semi - numerical model of cosmic reionization and thermal history of the igm .",
    "it quickly generates maps and ps of the brightness temperature , matter density , velocity , spin temperature and ionization fraction at designated redshifts .",
    "we performed simulations in a @xmath23 @xmath24 comoving box with @xmath25 grid cells for a wide range of eor parameters described in the next section . in our calculation , we use the 21 cm power spectrum in the range @xmath26 divided into 14 bins .",
    "it is common to characterize eor models with parameters and then examine the effect of changing the parameters on the 21 cm signal .",
    "we employ three key parameters which are often used .",
    "let us briefly define these three parameters : + 1 .",
    "@xmath27 , _ the ionizing _ _ efficiency _ : @xmath27 is the combination of several parameters related to ionizing photons escaping from high redshift galaxies and is defined as @xmath28 @xcite . here",
    ", @xmath29 is the fraction of ionizing photons escaping from galaxies into the igm and @xmath30 is the fraction of baryons locked into stars .",
    "these parameters are extremely uncertain at high redshift  @xcite .",
    "@xmath31 is the number of ionizing photons produced per baryon in stars and @xmath32 is the mean recombination rate per baryon . in our calculation , we explore a range of @xmath33 .",
    "@xmath34 , _ the minimum _ _ virial _ _ temperature _ _ of _ _ haloes _ _ producing _ _ ionizing _ _ photons _ : @xmath34 parameterizes the minimum mass of haloes producing ionizing photons during the eor .",
    "typically , @xmath35 is chosen to be @xmath36 , corresponding to the temperature above which atomic cooling becomes effective .",
    "@xmath35 parameterizes the physics of star formation in high redshift galaxies . in haloes with virial temperature @xmath37",
    "atomic cooling is sufficient to trigger gravothermal instability and thus star formation .",
    "however , star formation is quenched if agn or supernovae feedback is effective and the igm is heated up .",
    "this leads to effective minimum virial temperature larger than @xmath38[k ] . in haloes with viral temperature @xmath39 ,",
    "hydrogen molecule cooling is necessary .",
    "however , if stars begin to form in a halo , radiative feedback such as the photodissociation of @xmath40 by lyman - werner photons may become effective and prevent the gas from cooling @xcite .",
    "conversely , positive feedback , such as the enhancement of @xmath40 molecules formation due to an increase in the free electrons density , tends to push the minimum virial temperature to lower value because cooling becomes more effective .",
    "thus @xmath35 parameterizes the uncertainty in the efficiency of radiative feedback . in our work ,",
    "we explore @xmath35 ranging from @xmath41 to @xmath42 .",
    "@xmath43 , _ the mean free path of ionizing photons _ : the propagation of ionizing photons through the ionized igm strongly depends on the presence of absorption systems and the sizes of ionized regions are determined by the balance between sinks and sources of ionizing photons ( e.g. * ? ? ?",
    "this process is modelled by the maximum mean free path of ionizing photons , @xmath43@xcite . physically",
    ", the mean free path of ionizing photons corresponds to the typical distance traveled by photons within ionized regions before they are abosorbed and is determined by the number density and the optical depth of lyman - limit systems . in our calculation",
    ", we explore @xmath43 from 10 mpc to 60 mpc .",
    "in this section , we introduce artificial neural networks ( ann ) .",
    "anns are one of the machine learning methods and are a mathematical model inspired by the natural neuron network in our brain .",
    "the main purpose of anns is to construct approximate functions which associate input data with output data . in order to construct such a function",
    ", the ann has to learn from `` _ training data _ '' .",
    "the architecture of a simple class of ann consists of three layers : the input layer , the hidden layer and the output layer .",
    "each of them has a number of neurons as shown in fig.[fig : fig1 ] . in a more general case",
    ", we could choose the number of hidden layers and the number of neurons at each layer arbitrarily .",
    "in our study , we use 1 hidden layer . note that it is mathematically proven that neural networks with only 1 hidden layer can approximate any function with any accuracy if we use a large enough number of neurons @xcite .",
    "let us briefly describe the architecture of our ann .",
    "the input data @xmath44 is fed to the @xmath45-th neurons in the input layer .",
    "each neuron in the input layer is connected to the @xmath46-th neuron in the hidden layer and a weight @xmath47 is associated with the connection .",
    "the input to the @xmath46-th neuron in the hidden layer @xmath48 is a linear combination of all the input neurons with weight @xmath47 :    @xmath49    here , @xmath50 is the number of input data . in the hidden layer ,",
    "the @xmath46-th neuron is activated by an activation function @xmath51 such as its output is @xmath52 . generally , the activation function is a nonlinear function .",
    "we use the sigmoid function @xmath53 .",
    "the properties of the sigmoid function are such that it saturates and returns a constant output when the absolute value of the input is large and that it is a smooth and differentiable function .",
    "thanks to the nonlinear activation function , a trained ann can express any function .    in the output layer ,",
    "we compute linear combinations of the activated outputs of the neurons in the hidden layer with weights @xmath54 and obtain the output vector :    @xmath55    here @xmath56 is the number of neurons in the hidden layer .",
    "note that we do not activate the output value .",
    "the aim of training the ann is to find a set of weights that ensures the output vectors produced by the ann for a set of input vectors is sufficiently close to the desired output vectors .",
    "once we adjust the weights to reach this goal on a training sample , we can make predictions for output vectors for arbitrary input vectors outside of the training sample ( for example , new observational data ) .",
    "a popular algorithm to compute the trained weights is the `` _ back propagation algorithm _ ''",
    "we will describe this algorithm briefly in the following section .      in this section",
    ", we present the back propagation algorithm for the 1 hidden layer case .",
    "we also show the back propagation algorithm for multiple hidden layers case in appendix .    in order to quantify",
    "how well the output obtained by the ann approximates the desired output for the training data set , we define the ( total ) cost function as :    @xmath57 , \\label{eq : cost}\\ ] ]    where @xmath58 is the number of training input vectors and @xmath59 is the number of neurons at the output layer . @xmath60 and @xmath61 are outputs of the ann and the ( desired ) training output data , respectively . our purpose is to find the weight set that minimises the cost function . in order to find this weights set , we need to compute the partial derivative of @xmath62 with respect to the individual weights @xmath63 and find the local minimum of @xmath62 using gradient descent .",
    "the weights are updated by gradient descent following the formula :    @xmath64    here , @xmath65 is a learning coefficient which controls how fast the weights are updated .",
    "we used @xmath66 .",
    "we only need to calculate the derivative of the cost function for each training input vector and then sum over all input vectors as shown in eq.[eq : gradient ] .",
    "first , let us consider the derivative with respect to the weights between output layer and hidden layer . in this case ( @xmath67=2 ) , we can simply calculate the derivative of @xmath62 as    @xmath68    next , we calculate the derivative of @xmath62 with respect to the weights between the hidden layer and the input layer . in this case",
    "( @xmath67=1 ) , the derivative of @xmath62 is    @xmath69    in the second line , we use the chain rule for derivative because @xmath62 depends on the activated neuron @xmath70 in the hidden layer only through the output neuron @xmath60 . here , @xmath71 denotes the derivative of the activation function with respect to @xmath72 .",
    "using eqs.([eq : gradient ] ) , ( [ eq : derivative1 ] ) and ( [ eq : derivative2 ] ) , we can iterate on the gradient descent until the outputs obtained by the ann converge to the desired outputs ( minimum of the cost function ) .",
    "the back - propagation algorithm can be summarised as follows :    1 .   starting with random weights ,",
    "compute the output of the ann using eq.([eq : hidden ] ) and eq.([eq : output ] ) for all input vectors in the training set ( _ forward propagation _ ) + 2 .",
    "compute the cost function + 3 .   compute the derivative of the cost function with respect to the weights between output layer and hidden layer with eq.([eq : derivative1 ] ) and then the derivative with respect to the weights between input layer and hidden layer with eq.([eq : derivative2 ] ) ( _ back - propagation _ ) .",
    "update the weights with eq.([eq : gradient ] ) .",
    "go back to ( i ) and iterate until the cost function converges to a minimum .",
    "in our case , we prepared 70 training data sets .",
    "each set consist of the 21 cm ps @xmath73 obtained with 21cmfast as input data and the corresponding eor parameters used in the simulation , @xmath74 as output data .",
    "the architecture of the ann is the following : ( _ i _ ) 14 neurons in the input layer , ( _ ii _ ) 14 neurons in the hidden layer , ( _ iii _ ) 3 neurons in the output layer .",
    "as we mentioned in section  [ sec:21cmps ] , we use 14  bins for the 21 cm ps , and we use 3 eor parameters .",
    "this is why the number of neurons in the input layer and in the output layers are 14 and 3 , respectively .",
    "note that the number of redshifts used to train will always match the number of redshifts being fit .",
    "to analyse in details how the ann performs we look at partial cost functions : we use the normalised root mean square error ( rmse ) defined by    @xmath75    here , @xmath76 ( with @xmath77 one of @xmath78 or , @xmath79 .",
    "@xmath80 and @xmath81 are the eor parameters evaluated by the ann and from the training data , respectively .",
    "( red ) , @xmath27 ( green ) and @xmath35 ( blue ) as functions of the number of iterations for the learning process , for a network with 14 neurons . ]",
    "( solid line ) , @xmath27 ( dashed line ) and @xmath35 ( dot - dashed line ) as functions of the number of neurons for @xmath82 iterations in the learning process . ]",
    "first , we study how the rmse depends on the number of iteration for each eor parameter .",
    "the result is plotted in fig.[fig : fig2 ] . in this figure , we fix the number of neurons in the hidden layer at 14 and perform calculation with @xmath83 iterations . after initial fluctuations ,",
    "the rmse decreases and converges for @xmath84 iterations .",
    "next , we show how the rmse depends on the number of neurons in the hidden layer after @xmath85 iterations .",
    "the rmse is only weakly dependent on the number of neurons in the explored range . in the following section , we perform @xmath82 iterations for the back - propagation algorithm with 14 neurons in the hidden layer unless specifically mentioned",
    "we apply the trained ann to 54 test datasets . as for the training datasets , these test datasets consist of the 21 cm ps and the corresponding eor parameters .",
    "we use the 21 cm ps data as input for the ann and obtain estimated values for the eor parameters .",
    "then , we compare the estimated values with the true values that were used to compute the ps in the 21cmfast simulation , and evaluate how the ann performs .",
    "first , we apply the ann to the 21 cm ps at a single redshift without including the effect thermal noise or sample variance . in figs.[fig : fig4 ] and [ fig : fig5 ] , we plot the eor parameters estimated with the ann from the 21 cm ps data against the true values . in fig.[fig : fig4 ] , we show the result at @xmath86 .",
    "as we can see , @xmath27 and @xmath35 estimated with the ann are in relatively good agreement with the true values . on the other hand , for @xmath43",
    ", the correct values are not recovered .",
    "this is because the 21 cm ps is not sensitive at @xmath86 to @xmath43 . on the other hand ,",
    "the 21 cm ps becomes sensitive to @xmath43 at lower redshift when reionization advances much more ( since @xmath43 physically expresses the maximum bubble size , the effect of @xmath43 on the 21 cm ps is remarkable mostly at lower redshift ) .",
    "thus the cost function is insensitive to changes in @xmath43 values ; the learning process of the ann is incomplete and systematic deviations remain .",
    "note that the reason why the estimated value of @xmath43 seems to be constant at @xmath87 is that @xmath43=30 occurred more often than other values in the training datasets .    in fig.[fig :",
    "fig5 ] we show the same plots obtained from the ps at @xmath88 .",
    "the agreement between recovered and expected values extends over a larger range than at @xmath1=12 , in particular for @xmath43 .",
    "indeed the 21 cm ps at @xmath88 is sensitive to @xmath43 and thus the ann learning process works better .    in the previous two cases , we considered the 21 cm ps without any source of noise .",
    "we will now consider both the contribution of thermal noise and sample variance .",
    "we model the thermal noise as a gaussian random field characterized by its power spectrum . in an annulus in fourier space with @xmath89 cells ,",
    "the thermal noise power spectrum is ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) : @xmath90 where @xmath77 is the angle between * k * and line of sight , @xmath91 is the effective area of antenna , @xmath92 is the observed wavelength and @xmath93 are factors converting @xmath94 space units into comoving wavenumber units and are determined by cosmology .",
    "@xmath95 is expressed by    @xmath96    where @xmath97 is the total system temperature , @xmath98 is the bandwidth and @xmath99 is the effective observing time .",
    "the spherically averaged noise can be obtained by summing over @xmath77 in a shell with radius @xmath56 shown in eq.[eq : noise_average ] .",
    "@xmath100^{2}\\right\\}^{-1/2}. \\label{eq : noise_average}\\ ] ]    we recommend to read mcquinn et al .",
    "( 2006 ) . in our case , we assume ska specification @xcite .",
    "we estimate the sample variance from 10 simulations using different realizations of the initial conditions .    in the general case , we would include the noise at the level of the visibilities and compute the resulting noisy power spectrum . assuming that the noise is a random gaussian field , we simply generate a noisy power spectrum by using the following formula :    @xmath101    where @xmath102 is the noisy power spectrum , @xmath103 is the power spectrum produced by the simulation for a given set of parameters generically labeled by the index j , and @xmath104 is _ a random draw _ from a gaussian probability distribution with variance equal to the thermal noise power spectrum or the sample variance . a different and independent draw is required for each value of @xmath56 . for the learning sample",
    "we include only the sample variance . for each parameter set ( 70 possible values for @xmath46 ) ,",
    "we generate 50 realizations of the noise ( labeled by @xmath105 ) .",
    "this means that we use 3500 different ps for the learning sample .",
    "this is necessary as we are not aware of a standard technique for directly including an uncertainty in the inputs of an ann . for the test datasets ,",
    "we add both thermal noise and sample variance to the 21 cm ps , using the same procedure .",
    "note that the result for the virial temperature is plotted in log scale .",
    "it is the same for the following figures . ]",
    "= 12 . in this case",
    ", we include both thermal noise and sample variance . ]    in fig.[fig : fig6 ] , we show the eor parameters found by the ann as functions of the values used in the simulations , using the 21 cm signal ps at @xmath88 , including both thermal noise and sample variance .",
    "the difference between fig.[fig : fig5 ] and fig.[fig : fig6 ] is not obvious at a glance . in order to quantify the difference",
    ", we compute the _",
    "mean chi - square value _ , @xmath106 :    @xmath107    @xmath80 is the @xmath43 , @xmath35 , @xmath27 reconstructed by the ann and @xmath108 is the value of the corresponding parameter used in the simulation . in table.[table : chi ] , we compare the @xmath106 values for each of the parameters , with and without noise .",
    "as we can see , the @xmath106 values for the 21 cm ps with noises are worse than those without noises .",
    "noise alters the efficiency of the learning process for the ann .",
    "[ my - label ]    [ cols=\"<,<,<,<,<,<\",options=\"header \" , ]",
    "here , we show the back propagation algorithm in the case of multiple hidden layers . by analogy with eq.([eq : derivative2 ] ) , we can express the derivative of @xmath109 with respect to the weight in the @xmath67-th layer , @xmath110 @xmath111 as    @xmath112    where @xmath113 .",
    "since the changes in @xmath114 are transmitted to @xmath109 through each @xmath115 in the ( @xmath67 + 1)-th layer , the derivative of @xmath109 with respect to @xmath114 can be expressed as    @xmath116    remember that @xmath115 can be expressed with the activation function @xmath51 as @xmath117 ,    @xmath118    here ,",
    "we define @xmath119 , then we can re - write eq.([eq : derivativel2 ] ) as    @xmath120    combining eq.([eq : delta ] ) with @xmath121 , the eq.([eq : derivativel ] ) can be re - written simply as    @xmath122    this form tells us that we can easily obtain the derivative of the cost function with respect to @xmath110 , which connects the neuron @xmath45 in the ( @xmath67 - 1 ) th layer to the neuron @xmath46 in the @xmath67 th layer , as the product of @xmath123 and @xmath124 . as shown in eq.([eq : delta ] ) , we start to calculate @xmath123 from the output layer ( @xmath67=l ) to the input layer .",
    "this is why this algorithm is called `` _ back propagation _ '' .",
    "if we use eq.([eq : cost ] ) as the cost function , we easily calculate @xmath125 .",
    "this work is benefited from a grant from the french anr funded project orage ( anr-14- ce33 - 0016 ) .",
    "we thank to g.mellema , a. fialkov , s. majumdar , s.giri and k. hasegawa for their useful comments and thank to s. yoshiura for providing the thermal noise data .",
    "abel , t. , bryan , g.  l. , & norman , m.  l.  2002 , science , 295 , 93 agarwal , s. , abdalla , f.  b. , feldman , h.  a. , lahav , o. , & thomas , s.  a.  2012 , , 424 , 1409 agarwal , s. , abdalla , f.  b. , feldman , h.  a. , lahav , o. , & thomas , s.  a.  2014 , , 439 , 2102          beardsley , a.  p. , hazelton , b.  j. , sullivan , i.  s. , et al .",
    "2016 , , 833 , 102 bernardi , g. , zwart , j.  t.  l. , price , d. , et al .",
    "2016 , , 461 , 2847 bloom , j.  s. , & richards , j.  w.  2012 , advances in machine learning and data mining for astronomy , 89 bromm , v. , coppi , p.  s. , & larson , r.  b.  2002 , , 564 , 23 bromm , v.  2013 , reports on progress in physics , 76 , 112901                  furlanetto .",
    "p. s and briggs .",
    "f , phys .  rept .",
    "* 433 * ( 2006 ) 181 [ astro - ph/0608032 ] .",
    "deboer , d.  r. , parsons , a.  r. , aguirre , j.  e. , et al .",
    "2016 , arxiv:1606.07473 deboer , d.  r. , parsons , a.  r. , aguirre , j.  e. , et al .",
    "2016 , arxiv:1606.07473    fialkov , a. , barkana , r. , visbal , e. , tseliakhovich , d. , & hirata , c.  m.  2013 , , 432 , 2909 fialkov , a. , cohen , a. , barkana , r. , & silk , j.  2017 , , 464 , 3498 folkes , s.  r. , lahav , o. , & maddox , s.  j.  1996 , , 283 , 651      greig , b. , & mesinger , a.  2015 , , 449 , 4246 greig , b. , mesinger , a. , & koopmans , l.  v.  e.  2015 , arxiv:1509.03312 greig , b. , mesinger , a. , & pober , j.  c.  2016 , , 455 , 4295 gnedin , n.  y. , kravtsov , a.  v. , & chen , h .- w .",
    "2008 , , 672 , 765 - 775          hornik , k. , stinchcombe , m . ,",
    "white , h. , _ neural networks _ vol.2 pp359 - 366 , 1989 jacobs , d.  c. , pober , j.  c. , parsons , a.  r. , et al .",
    "2015 , , 801 , 51 jensen , h. , zackrisson , e. , pelckmans , k. , et al .",
    "2016 , , 827 , 5    kamdar , h.  m. , turk , m.  j. , & brunner , r.  j.  2016 , , 455 , 642 kamdar , h.  m. , turk , m.  j. , & brunner , r.  j.  2016 , , 457 , 1162 kim , k. , harry , i.  w. , hodge , k.  a. , et al .  2015 , classical and quantum gravity , 32 , 245002                  mellema , g. , koopmans , l.  v.  e. , abdalla , f.  a. , et al .  2013 , experimental astronomy , 36 , 235 mesinger .",
    "a and furlanetto .",
    "s , arxiv:0704.0946 [ astro - ph ] .",
    ". a , furlanetto .",
    "r,2011 , mnras , 411 , 955 mesinger , a. , ferrara , a. , & spiegel , d.  s.  2013 , , 431 , 621                                          yoshida .",
    "n , omukai .",
    "k , hernquist .",
    "l , & t.  abel  2006 , apj , 652 , 6 yoshiura.s , shimabukuro.h , takahashi.k , et al .",
    "2015 , mon .  not .",
    "451 , 4785 zahn , o. , lidz , a. , mcquinn , m. , et al .",
    "2007 , , 654 , 12 zahn , o. , mesinger , a. , mcquinn , m. , et al .",
    "2011 , , 414 , 727"
  ],
  "abstract_text": [
    "<S> the 21 cm signal from the epoch of reionization should be observed within the next decade . </S>",
    "<S> while a simple statistical detection is expected with ska pathfinders , the ska will hopefully produce a full 3d mapping of the signal . to extract from the observed data constraints on the parameters describing the underlying astrophysical processes , </S>",
    "<S> inversion methods must be developed . </S>",
    "<S> for example , the markov chain monte carlo method has been successfully applied . here </S>",
    "<S> we test another possible inversion method : artificial neural networks ( ann ) . </S>",
    "<S> we produce a training set which consists of 70 individual sample . </S>",
    "<S> each sample is made of the 21 cm power spectrum at different redshifts produced with the 21cmfast code plus the value of three parameters used in the semi - numerical simulations that describe astrophysical processes . using this set we train the network to minimize the error between the parameter values it produces as an output and the true values . </S>",
    "<S> we explore the impact of the architecture of the network on the quality of the training . </S>",
    "<S> then we test the trained network on the new set of 54 test samples with different values of the parameters . </S>",
    "<S> we find that the quality of the parameter reconstruction depends on the sensitivity of the power spectrum to the different parameters at a given redshift , that including thermal noise and sample variance decreases the quality of the reconstruction and that using the power spectrum at several redshifts as an input to the ann improves the quality of the reconstruction . </S>",
    "<S> we conclude that anns are a viable inversion method whose main strength is that they require a sparse exploration of the parameter space and thus should be usable with full numerical simulations .    </S>",
    "<S> [ firstpage ]    cosmology : theory  intergalactic medium  epoch of reionization  21 cm line </S>"
  ]
}