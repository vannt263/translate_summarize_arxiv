{
  "article_text": [
    "progress in science is the result of an interplay between model building and the testing of models with experimental data . in this paper",
    ", we discuss model evaluation and focus primarily on situations where a statement is desired on the validity of a model without explicit reference to other models .",
    "we introduce different discrepancy variables for this purpose and define @xmath0-values based on these .",
    "we then study the usefulness of the @xmath0-values for passing judgments on models with a few simple examples reflecting commonly encountered analysis tasks .    in the ideal case , it is possible to calculate the degree - of - belief in a model based on the data .",
    "this option is only available when a complete set of models and their prior probabilities can be defined .",
    "however , the conditions necessary for this ideal case are usually not met in practice .",
    "we nevertheless often want to make some statement concerning the validity of the model(s ) .",
    "we then are left with using probabilities of data outcomes assuming the model to try to make some judgments .",
    "these probabilities can be determined deductively since the model is assumed , and therefore frequencies of possible outcomes can be produced within the context of the model .",
    "these can then be used to produce frequency distributions of discrepancy variables , and @xmath0-values ( one - sided probabilities for the discrepancy variables ) can be calculated using the distributions and the observed values .",
    "the use of @xmath0-values has been widely discussed in the literature  @xcite and many authors have commented that @xmath0-values are frequently misused in claiming support for models  @xcite .",
    "we give a bayesian argumentation for the use of @xmath0-values to make judgments on model validity , and it is in this bayesian sense that we will use @xmath0-values .",
    "we start with a review of model testing in a bayesian approach when an exhaustive set of models is available and a coherent probability analysis is possible .",
    "we then move to situations where this is not the case , and review some possible choices of discrepancy variables for goodness - of - fit ( gof ) tests .",
    "next , we define the @xmath0-values for the discrepancy variables and evaluate their usefulness for several example data sets .",
    "note that we omit a discussion of bayes factors since these require the definition of at least two models for evaluation .",
    "assume we have a complete set of models available for describing the data , such that we are sure the data can be described by one of the models available .",
    "the models can be used to calculate ` direct probabilities ' ; i.e. , relative frequencies of possible outcomes of the results were one to reproduce the experiment many times under identical conditions .",
    "the probability of a model , @xmath4 , is denoted by @xmath5 , with @xmath6 while the probability densities of the model parameters , @xmath7 , are typically continuous functions . in the bayesian approach ,",
    "the quantities @xmath5 and @xmath8 are treated as probabilities , although they are not frequency distributions and are more accurately described as ` degrees - of - belief ' ( dob ) .",
    "this dob is updated by comparing data with the predictions of the models .",
    "@xmath9 represents complete certainty that @xmath10 is the model which describes the data , and @xmath11 represents completely certainty that @xmath10 is not the correct model .",
    "the procedure for updating our dob using experimental data is @xmath12 where the index on @xmath13 represents a ` state - of - knowledge ' .",
    "the posterior probability density function , @xmath14 , is usually written simply as @xmath13 , and the prior is written as @xmath15 .",
    "the posterior describes the state of knowledge _ after _ the experiment is analyzed .",
    "the quantity @xmath16 represents a probability of getting the data @xmath17 given the model and parameter values , and can usually be defined in a number of ways ( see for example section  [ sec : expo ] ) .",
    "normalizing [ eq : bayes ] , and using @xmath18 yields @xmath19 this is the classic equation due to bayes and laplace  @xcite .",
    "models can be compared and the dob in a model can be obtained using @xmath20 this evaluation requires the specification of a full set of models and a definition of the prior beliefs such that @xmath21 it is very sensitive to the definitions of the priors when the data are not very selective .",
    "an example where this approach was used is given in  @xcite , and is reviewed here .",
    "the analysis is to be performed on an observed energy spectrum , for which we can form a background - only hypothesis , or a background+signal hypothesis .",
    "an example is the search for neutrinoless double beta decay .    in the following ,",
    "@xmath22 denotes the hypothesis that the observed spectrum is due to background only ; the negation , interpreted here as the hypothesis that the signal process contributes to the spectrum , is labeled @xmath23 .",
    "the posterior probabilities for @xmath22 and @xmath23 can be calculated using    @xmath24    and    @xmath25    the probability @xmath26 is    @xmath27    the probabilities for a given spectrum can be calculated based on assumptions on the signal strength and background shape as described in  @xcite .",
    "evidence for a signal or a discovery is then decided based on the resulting value for @xmath28 .    in this analysis",
    ", it was assumed that the observed spectrum must come from either the background model or a combination of the background and double beta decay signal .",
    "the probability of each case is evaluated and conclusions are drawn from these probabilities .",
    "in most cases , we analyze data without having an exhaustive set of models available , but nevertheless want to reach conclusions on how well the models account for the data .",
    "this information can be used , for example , to guide the search for new models . in the example given above , it is possible that there are unknown sources of background for which predictions are not possible before the experiment is performed .",
    "the quantities @xmath29 and @xmath30 could be individually examined and , if both are on the small side of the expected distribution , doubts concerning the completeness of our set of models could arise .      for a given model",
    ", we can define one or more discrepancy variable(s ) and calculate the expected frequency distribution of this discrepancy variable .",
    "if the discrepancy variable is well chosen , then the distribution for a ` good ' model should look significantly different than for a ` bad ' model .",
    "finding the discrepancy variable in the region populated by incorrect models then gives us cause to think our model is not adequate .",
    "a @xmath0-value is the probability that , in a future experiment , the discrepancy variable will have a larger value ( indicating greater deviation of the data from the model ) than the value observed , assuming that the model is correct and all experimental effects are perfectly known .",
    "in other words , not only is the model the correct one to describe the physical situation , but correct distribution functions are used to represent data fluctuations away from the ` true values ' .",
    "we will focus on gof tests for the underlying model , but it should be clear that incorrect formulations of the data fluctuations will bias the @xmath0-value distributions to lower ( if the data fluctuations are underestimated ) or higher ( if the data fluctuations are overestimated ) values .    in general , any discrepancy variable which can be calculated for the observations can be used to define a @xmath0-value .",
    "we use @xmath31 and @xmath32 to denote discrepancy variables evaluated with a possible set of observations @xmath33 for given model and parameter values , and for the observed data , @xmath34 , respectively . to simplify the notation",
    ", we will occasionally drop the arguments on @xmath35 and use @xmath36 to denote the value of the discrepancy variable found from the data set at hand .",
    "@xmath35 can be interpreted as a random variable ( e.g. , possible @xmath37 values for a given model ) , whereas @xmath36 has a fixed value ( e.g. , the observed @xmath37 derived from the data set at hand ) .",
    "assuming that smaller values of @xmath35 imply better agreement between the data and model predictions , the definition of @xmath0 ( for continuous distributions of @xmath35 ) is written as : @xmath38    the quantity @xmath0 is the ` tail - area ' probability to have found a result with @xmath39 , assuming that the model @xmath4 and the parameters @xmath7 are valid .",
    "if the modeling is correct ( including that of the data fluctuations ) , @xmath0 will have a flat probability distribution between @xmath40 $ ] . for discrete distributions of @xmath35 ,",
    "the integral is replaced by a sum , the @xmath0-value distribution is no longer continuous , and the cumulative distribution for @xmath0 will be step - like .",
    "if the existing data are used to modify the parameter values , the extracted @xmath0-value will be biased to higher values .",
    "the amount of bias will depend on many aspects , including the number of data points , the number of parameters , and the priors . we can remove the bias for the number of fitted parameters in @xmath41 fits by evaluating the probability of @xmath42 for @xmath43 degrees - of - freedom , @xmath44 , where @xmath45 is the number of data points and @xmath46 is the number of parameters fitted  @xcite , when    * the data fluctuations are gaussian and independent of the parameters , * the function to be compared to the data depends linearly on the parameters , and * the parameters are chosen such that @xmath37 is at its global minimum .    in general ,",
    "the bias introduced by the number of fitted parameters becomes small if @xmath47 .",
    "@xmath0-values can not be turned into probabilistic statements about the model being correct without priors , and statements of ` support ' for a model directly from the @xmath0-value behave ` incoherently '  @xcite .",
    "furthermore , approximations used for the distributions of the discrepancy variables , biases introduced when model parameters are fitted and difficulties in extracting reliable information from numerical algorithms used to evaluate the discrepancy variable ( see section  [ sec : comparisonpvalues ] ) further complicate their use .",
    "@xmath0-values should therefore be handled with care .",
    "nevertheless , we discuss the use of @xmath0-values to make judgments about the models at hand .",
    "the judgment will be based on a sequence of considerations of the type :    * the @xmath0-value distribution for a good model is expected to be ( reasonably ) flat between @xmath40 $ ] ; * the @xmath0-values for bad models usually have sharply falling distributions starting at @xmath48 ; * small @xmath0-values are worrisome ; if we know that other models can be reasonably constructed which would have higher @xmath0-values , then a small @xmath0-value for the model under consideration indicates that we may have picked a poor model ; * if the @xmath0-value is not too small , then our model is adequate to describe the existing data .",
    "we contend that the use of @xmath0-values for evaluation of models as just described is essentially bayesian in character . following the arguments given above ,",
    "assume that the @xmath0-value probability density for a good model , @xmath49 , is flat , @xmath50 and that for poor models , @xmath51 , can be represented by @xmath52 where @xmath53 so that the distribution is strongly peaked at @xmath54 and approximately normalized to @xmath55 .",
    "the dob assigned to model @xmath49 after finding a @xmath0-value @xmath0 is then @xmath56 if we take all models to have similar prior dobs , then @xmath57 in the limit @xmath58 , we have @xmath59 while for @xmath60 @xmath61    although this formulation in principle allows for a ranking of models , the vague nature of this procedure indicates that any model which can be constructed to yield a reasonable @xmath0-value should be retained .",
    "a further consideration is that the correct distributions for the data fluctuations are often not known ( due to the vague nature of systematic uncertainties ) and best guesses are used .",
    "this will generally also lead to non - flat @xmath0-value distributions for good models .",
    "scientific prejudices ( occam s razor , elegance or esthetics , etc . ) will influence the decision and act as a guide in selecting the ` best ' model in cases where several good models are available .        for uncorrelated data assumed to follow gaussian probability distributions relative to the model predictions ,",
    "the @xmath41 value is a natural discrepancy variable for a gof test : @xmath62 where the @xmath45 data points are given by @xmath63 , and the prediction of the model for @xmath64 is @xmath65 .",
    "the modeling of the data fluctuations uses fixed standard deviations @xmath66 .    if the parameters of the model are fitted to the data by minimizing @xmath37 and there are @xmath46 parameters we replace @xmath7 in the formula above with @xmath67 .",
    "the @xmath41 probability distribution is evaluated for @xmath43 ` degrees - of - freedom ' . in the special case",
    "where @xmath68 is linear in the parameters , this procedure again yields a flat @xmath0-value distribution between @xmath40 $ ] .      the standard @xmath41 test does not take into account clustering of data below or above expectations . to detect clusters the ordered set of @xmath45 observations",
    "@xmath63 is partitioned into subsets containing the success and failure runs ( defined as sequences of consecutive @xmath64 above or below the expectation from the model , @xmath65 , respectively ) .",
    "several discrepancy variables based on success runs can be found in the literature  @xcite but these do not take into account the size of the deviation , @xmath69 .",
    "recently , a discrepancy variable for runs incorporating this extra information was proposed for ordered data with gaussian fluctuations  @xcite .",
    "let @xmath70 denote the subset of the observations of the @xmath71 success run .",
    "the weight of the @xmath71 success run is then taken to be @xmath72 where the sum over @xmath73 covers the @xmath74 and @xmath75 is the length of the run . the discrepancy variable is the largest weight of any run @xmath76    the explicit distribution of @xmath77 , used to define the @xmath0-value , @xmath78 , is given in  @xcite for the case when @xmath79 are fully specified ( no fitting ) .",
    "a similar discrepancy variable can be defined for failure measurements , @xmath80 .    to illustrate the definition we present a simple example .",
    "suppose @xmath81 observations at @xmath82 positions @xmath83 with standardized residuals @xmath84 given by @xmath85 then there are two success runs @xmath86 , @xmath87 and we find @xmath88 due to the second run .",
    "similarly , for the single failure run , @xmath89 .      in the case of binned , poisson distributed data , the quantity @xmath90 can be used as a discrepancy variable where @xmath91 is the number of bins , the @xmath92 are the numbers of measured events and @xmath93 are the model expectations for event bin @xmath73 ; for an expectation @xmath94 the expected variance is @xmath94 .",
    "this pearson @xmath41 statistic  @xcite was originally proposed for multinomial data but has found wide use in analyzing poisson distributed data . rather than using the probability distribution of this discrepancy variable directly ,",
    "@xmath95 is often used as an approximation for @xmath96 .",
    "the distribution of @xmath97 has been shown to asymptotically reach a @xmath41 distribution for multinomially distributed data , giving some justification for this procedure .",
    "practically , this is the case for data with a large number of entries @xmath98 in all bins .",
    "when parameters are first estimated from a fit to the data , the parameter values which minimize @xmath97 are used to calculate the @xmath94 , and the @xmath0-value is evaluated using @xmath99 .",
    "it is often seen that the _ expected _ weight , @xmath100 , is replaced with the _ observed _ weight , @xmath98 , in the denominator ( neyman @xmath41  @xcite ) .",
    "the discrepancy variable is then @xmath101 again , rather than using the probability distribution of this discrepancy variable directly , it is assumed that @xmath102 has a distribution which approximates a @xmath37 distribution with @xmath91 degrees of freedom and @xmath95 is used as an approximation for @xmath103 . in cases where @xmath104 , practitioners of this approach set @xmath105 to avoid divergence . sometimes bins with @xmath106 are ignored , which can lead to very misleading results since finding @xmath106 is valuable information .",
    "when parameters are first estimated from a fit to the data , the parameter values which minimize @xmath102 are used , and the @xmath0-value is evaluated using @xmath99 .    note",
    "that in both of these cases , we do not expect flat @xmath0-value distributions since only approximations are used for @xmath107 .",
    "the deviations from flatness are expected to be greatest when small event numbers are present in the data sets .",
    "another option for a poisson model is based on the ( log of ) the likelihood ratio  @xcite ( sometimes referred to as the cash statistic  @xcite ) @xmath108 \\;\\;. \\label{eq : poisson likelihood ratio}\\ ] ] in bins where @xmath106 , the last term is set to @xmath54 .",
    "again , rather than using the probability distribution of this discrepancy variable directly , since @xmath109 has a distribution which approximates a @xmath37 distribution with @xmath91 degrees of freedom for large @xmath94 , @xmath95 is used as an approximation for @xmath110 .",
    "the validity of this approximation is critically discussed in  @xcite . when parameters are first estimated from a fit to the data , the parameter values which maximize the likelihood are used , and the @xmath0-value is evaluated using @xmath99 .",
    "note that the distribution of @xmath109 asymptotically converges faster to the @xmath41 distribution than the distribution of @xmath97 ( see @xcite ) .",
    "any probability of the data can be chosen as the discrepancy variable : @xmath111 in this case , larger values of @xmath112 imply better agreement with the data .",
    "the probability @xmath113 can be used to extract the probability for @xmath112 as @xmath114 an example of how this is done numerically for poisson distributed data and using @xmath115 is given in the appendix .",
    "once we have @xmath116 we can then evaluate @xmath117 where @xmath118 is the value observed with the data set at hand .    in a model with gaussian uncertainties where we use a product of gaussian densities , @xmath119 where @xmath120",
    ", then taking @xmath121 is equivalent to the usual @xmath41 test .",
    "if the model parameters are first fitted using the data , we propose the following correction for the number of fitted parameters :    * calculate the @xmath0-value taking @xmath122 , but assuming a simple hypothesis ( no fitted parameters ) ; * calculate the @xmath41 value which corresponds to this @xmath0-value using the inverse @xmath37 distribution corresponding to @xmath45 degrees of freedom ; * recalculate the @xmath0-value using the @xmath41 value and @xmath43 degrees of freedom .",
    "this procedure is valid for the case where we have gaussian uncertainties , but is ad - hoc for other cases .",
    "we test its usefulness below .",
    "care must be taken in using @xmath112 as a discrepancy variable , particularly when it is written as the product of individual probability densities .",
    "the overall shape of the distribution is potentially not tested , and large @xmath0-values can be produced with incorrect model choices .",
    "an example of this is given below in section [ sec : expo ] .",
    "rather than using the parameters at the mode of the posterior , it is also possible to define a @xmath0-value by averaging over the parameter values according to a probability distribution  @xcite . for the posterior - predictive case  @xcite , assuming we are using a probability of the data as discrepancy variable , @xmath123 p(\\vec{\\theta}|\\vec{d},m )   d\\vec{\\theta } \\;\\ ; .\\ ] ]    while the partial posterior - predictive @xmath0-value  @xcite has the desirable property of a flat distribution on @xmath40 $ ] , at least in the large sample limit as @xmath124 , it is not known in general how to compute it for realistic problems .",
    "furthermore , the numerical effort required to evaluate the double integral in quickly becomes prohibitive .",
    "therefore , these @xmath0-value definitions are not considered here despite their appeal from a bayesian perspective .",
    "johnson  @xcite proposed a modification of the @xmath41 definition to take into account the posterior probability density for the parameters of a model .",
    "rather than evaluating the probability of the data at fixed values of the parameters , the parameters are given values according to their probability density after evaluating the data .",
    "johnson s statistic , @xmath125 , is expected to behave asymptotically as a @xmath41 distribution with @xmath126 degrees of freedom , where @xmath91 is a number of bins to be defined , regardless of the number of parameters .",
    "imagine the data is given by a vector of values @xmath33 and these values are expected to deviate from the model prediction according to individual probabilities @xmath127 .",
    "the johnson prescription is to define @xmath91 bins , where bin @xmath73 contains a probability @xmath128 .",
    "intervals @xmath129 are defined for each data point @xmath130 via @xmath131 the intervals @xmath129 cover the full range of possible values for each @xmath132 and are ordered so that they include increasing values of @xmath82 .",
    "the definition of the intervals depends on the values of the parameters @xmath7 and vary for each data point @xmath130 .",
    "the parameter values are to be sampled from the full posterior probability @xmath133 . for each @xmath7 , a discrepancy variable @xmath125",
    "is calculated according to @xmath134 where @xmath92 is the actual number of data points falling within the intervals @xmath129 and @xmath45 is the total number of data points in the data set . in @xcite",
    "it is shown that asymptotically @xmath125 is distributed as a @xmath41 variable with @xmath126 degrees of freedom , regardless of the number of fitted parameters @xmath46 .",
    "hence the @xmath0-value is calculated using @xmath135 .    for data where the @xmath33 follow continuous probability densities the bins",
    "are usually chosen to have equal probabilities .",
    "the number of bins to be chosen is given by a rule of thumb due to mann / wald @xcite with a modification for small number of bins such that there are at least three : @xmath136 this means by default we have @xmath137 , @xmath138 , @xmath139 .    in case",
    "the values of @xmath33 follow a discrete distribution it is usually not possible to have equal probabilities for all @xmath128 . in this case , a randomization procedure is used to allocate data points to bins .",
    "in the following , we test the usefulness of the different @xmath0-values given above by looking at their distributions for specific examples motivated from common situations faced in experimental physics .",
    "we first consider a data set which consists of a background known to be smoothly rising and , in addition to the background , a possible signal .",
    "this could correspond for example to an enhancement in a mass spectrum from the presence of a new resonance .",
    "the width of the resonance is not known , so that a wide range of widths must be allowed for . also , the shape of the background is not well known .",
    "we do not have an exhaustive set of models to compare and want to look at gof s for models individually to make decisions .",
    "in this example , we will first assume that distributions of the data relative to expectations are modeled with gaussian distributions .",
    "we then consider the same problem with small event numbers , so that poisson statistics are appropriate , and again test our different @xmath0-value definitions .",
    "these examples were also discussed in  @xcite .",
    "finally , we consider the case of testing an exponential decay law on a sample of measured decay times .        a typical data set is shown in fig .",
    "[ fig : sample - data - set ] .",
    "it is generated from the function@xmath140 with parameter values ( @xmath141 , @xmath142 , @xmath143 , @xmath144 , @xmath145 , @xmath146 ) .",
    "the @xmath64 are generated from @xmath147 as @xmath148 where @xmath149 is sampled according to @xmath150 .     with gaussian fluctuations .",
    "the fits of the four models are superposed on the data.,width=336 ]    the @xmath82 domain is @xmath151 $ ] and two cases were considered : @xmath152 data points evenly sampled in the range , and @xmath153 data points evenly sampled in the range . the experimental resolution",
    "( gaussian with width @xmath154 ) is assumed known and correct .",
    "ensembles consisting of 10,000 data sets with @xmath152 or @xmath153 data points each were generated and four different models were fitted to the data .",
    "table  [ tab : models ] summarizes the parameters available in each model and the range over which they are allowed to vary . in all models ,",
    "flat priors were assumed for all parameters for ease of comparison between results .",
    ".summary of the models fitted to the data , along with the ranges allowed for the parameters . [ cols=\"^,<,^,^,^,^,^ \" , ]     most common @xmath0-values use approximations for the distribution of the discrepancy variable .",
    "this leads to non - flat distributions of the @xmath0-value even when no parameters are fitted , and these deviations can be severe in some cases .",
    "we discussed the use of the probability of the data itself as a discrepancy variable , and showed that it is flat in the case of no fitted parameters and poisson distributed data .",
    "this was due to the use of the correct probability distribution for the discrepancy variable .",
    "the algorithm described in the appendix can be used to generate the correct @xmath0-value distribution for any discrepancy variable for poisson distributed data .    for composite models , where parameters of the model are fit to the data before a @xmath0-value",
    "is calculated , we find that @xmath0-value distributions can depend strongly on the technical approach used to fit the data .",
    "using gradient - based approaches to find minima or maxima requires considerable attention from the user and generally fine tuning of fit ranges and starting values .",
    "the fine - tuning will certainly affect the @xmath0-value distributions , making their use more difficult . setting range limits",
    "effectively means defining a prior , and impacts @xmath0-value distributions . first fitting the data with a markov chain monte carlo before using migrad gave considerably better results , in the sense that the @xmath0-value distributions extracted in this way followed expectations .",
    "we therefore recommend using a mcmc to map out the parameter space as a start to the fitting procedure in situations where giving good starting values for a gradient - based fit is difficult .",
    "the peak probability for a poisson distribution @xmath155 occurs for @xmath156 .",
    "if the probability distribution of a data set is modeled as a product of poisson terms , then the highest probability is given for @xmath157 .",
    "we use this to define the starting point for a markov chain , and move the bin contents up or down ( chosen randomly ) at each iteration . for each attempted change in the bin content",
    ", we apply the usual metropolis test  @xcite .",
    "the probability @xmath158 is easily updated at each change .",
    "e.g. , if the result in bin @xmath73 increases from @xmath92 to @xmath159 , then the probability changes by @xmath160 a large number of experiments can be quickly simulated and the @xmath0-value extracted .",
    "see , e.g. , m. j. bayarri and j. o. berger , ` p values for composite null models ' , jour .",
    "assoc . , * 95 * ( 2000 ) 1127 ; m. j. bayarri and j. o. berger , ` the interplay of bayesian and frequentist analysis ' , statistic .",
    ", * 19 * ( 2004 ) 58 ; i. j. good , ` the bayes / non - bayes compromise : a brief review ' , jour . am . stat . assoc . , * 87 * ( 1992 ) 597 ; j. de la horra and m. t. rodriguez - bernal , ` posterior predictive @xmath0-values : what they are and what they are not ' , sociedad de estadstica e investigacin operativa test * 10 * ( 2001 ) 75 ; j. i. marden , ` hypothesis testing : from p values to bayes factors ' , jour",
    "* 95 * ( 2000 ) 1316 ; t. sellke , m.j .",
    "bayarri and j.o .",
    "berger , ` calibration of p values for testing precise null hypotheses ' , the american statistician , * 55 * ( 2001 ) 62 .",
    "see , e.g. , e.  t. jaynes , ` probability theory - the logic of science ' , g.  l. bretthorst ( ed . ) cambridge university press ( 2003 ) ; a. gelman , j.  b. carlin , h.  s. stern , d.  b. rubin ` bayesian data analysis ' , second edition ( texts in statistical science ) chapman & hall , london ( 2004 ) ; p. gregory , ` bayesian logical data analysis for the physics sciences ' , cambridge university press , ( 2005 ) ; g. dagostini , ` bayesian reasoning in data analysis - a critical introduction ' , world scientific , 2003 .",
    "see , e.g. , e.  j. burr and g. cane , ` longest run of consecutive observations having a specified attribute ' , biometrika , * 48 * ( 1961 ) 461 ; m. muselli , ` simple expressions for success run distributions in bernoulli trials ' , stat .",
    "* 31 * ( 1996 ) 121 ; j.  c. fu and w.  y.  w. lou , ` distribution theory of runs and patterns and its applications ' , world scientific , 2003 , and references therein .",
    "k. pearson , ` on the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arised from random sampling ' , philosophical magazine , * 50 * ( 1900 ) 157 .",
    "j. neyman , ` contribution to the theory of the @xmath37 test ' , proceedings of the berkeley symposium on mathematical statistics and probability , berkeley and los angeles , university of california press ( 1949 ) 239 .",
    "r.  protassov _ et al . _ , ` statistics : handle with care , detecting multiple model components with the likelihood ratio test ' , the astrophysical journal , * 571 * ( 2002 ) 545 [ arxiv : astro - ph/0201547 ] . symposium on mathematical      m. j. bayarri and j. o. berger , ` p values for composite null models ' , jour .",
    "assoc . , * 95 * ( 2000 ) 1127 ; j. m. robins , a. v. d. vaart , and v. ventura , ` asymptotic distribution of p values in composite null models ' , jour . am",
    "* 95 * ( 2000 ) 1143    d. b. rubin , ` bayesian justifiable and relevant frequency calculations for the applied statistician ' , ann",
    ". statist .",
    "* 12 * ( 1984 ) 1151 ; x. l. meng , ` posterior predictive @xmath0-values ' , ann .",
    "* 22 * ( 1994 ) 1142 .",
    "f.  james and m.  roos , ` minuit : a system for function minimization and analysis of the parameter errors and correlations , ' comput .",
    "commun .",
    "* 10 * ( 1975 ) 343 .",
    "j.  heinrich , ` pitfalls of goodness - of - fit from likelihood ' , in the proceedings of phystat2003:_statistical problems in particle physics , astrophysics , and cosmology _ , menlo park , california , 8 - 11 sep 2003 [ arxiv : physics/0310167 ] . n. metropolis et al .",
    ", ` equation of state calculations by fast computing machines ' , j. chem .",
    "* 21 * ( 1953 ) 1087"
  ],
  "abstract_text": [
    "<S> deciding whether a model provides a good description of data is often based on a goodness - of - fit criterion summarized by a @xmath0-value . </S>",
    "<S> although there is considerable confusion concerning the meaning of @xmath0-values , leading to their misuse , they are nevertheless of practical importance in common data analysis tasks . </S>",
    "<S> we motivate their application using a bayesian argumentation . </S>",
    "<S> we then describe commonly and less commonly known discrepancy variables and how they are used to define @xmath0-values . the distribution of these </S>",
    "<S> are then extracted for examples modeled on typical data analysis tasks , and comments on their usefulness for determining goodness - of - fit are given .    </S>",
    "<S> date : +    * @xmath0-values for model evaluation *    f. beaujean@xmath1 , a. caldwell@xmath1 , d. kollr@xmath2 , k. krninger@xmath3 +    @xmath1max - planck - institut fr physik , mnchen , germany + @xmath2cern , geneva , switzerland + @xmath3georg - august - universitt , gttingen , germany </S>"
  ]
}