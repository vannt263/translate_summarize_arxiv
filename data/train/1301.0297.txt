{
  "article_text": [
    "distributed source coding ( dsc ) studies compression of statistically dependent sources which do not communicate with each other @xcite . the wyner - ziv coding problem @xcite , a special case of _ lossy _ dsc , considers lossy data compression with side information at the decoder .",
    "the current approach to the dsc of continuous - valued sources is to first convert them to discrete - valued sources and then apply _ lossless _ ( slepian - wolf ) coder @xcite .",
    "similarly , a practical wyner - ziv encoder consists of a quantizer and slepian - wolf encoder .",
    "there are , hence , _ quantization _ and _ binning _ losses for the source coder . despite this",
    ", rate - distortion theory promises that block codes of sufficiently large length are asymptotically optimal , and they can be seen as vector quantizers followed by fixed - length coders @xcite",
    ". therefore , practical _ slepian - wolf _ coders have been realized using different binary channel codes , e.g. , ldpc and turbo codes @xcite .",
    "these codes , however , are out of the question if low delay is imposed on the system as they may introduce excessive delay when the desired probability of error is very low .    in this paper , we establish a new framework for the wyner - ziv coding and distributed lossy source coding , in general .",
    "we propose to first compress the continuous - valued sources and then quantize them , as opposed to the conventional approach .",
    "the new framework is compared against the existing one in fig .",
    "[ fig : realdsc ] .",
    "it introduces the use of _ real - number codes _ ( see , e.g. , @xcite ) , to represent correlated sources with fewer samples , in the real field . to do compression ,",
    "we generate syndrome or parity samples of the input sequence using a real - number channel code @xcite , similar to what is done to compress a binary sequence of data using binary channel codes .",
    "then , we quantize these syndrome or parity samples and transmit them .",
    "there are still coding ( binning ) and quantization losses ; however , since coding is performed before quantization , error correction is in the real field , and the quantization error can be corrected when two sources are completely correlated over a block of code . a second and more important advantage of this approach is the fact that the correlation channel model can be more realistic , as it captures dependency between continuous - valued sources rather than quantized sources . in the conventional approach , it is implicitly assumed that quantization of correlated signals results in correlated sequences in the binary domain ; this may not necessarily be precise due to the nonlinearity of the quantization operation . to avoid any loss due to the inaccuracy of correlation model",
    ", we exploit the correlation between the continuous - valued sources before quantization .",
    "the new approach is also capable of alleviating the quantization error .",
    "this is possible because coding precedes quantization .",
    "specifically , we use the _ bose - chaudhuri - hocquenghem _ ( bch ) dft codes for compression @xcite . owing to dft codes",
    ", the loss due to quantization can be decreased by a factor of @xmath0 for an @xmath1 code @xcite .",
    "additionally , reconstruction loss becomes zero if the two sources are perfectly correlated over one codeword .",
    "this is achieved in view of modeling the correlation between the two sources in the continuous domain .",
    "dft codes also can exploit the temporal correlation typically found in many sources .",
    "finally , the proposed scheme is more suitable for low - delay communications since , even by using short dft codes , a reconstruction error less than the quantization error is attainable .",
    "moreover , as another contribution of this paper , we use a single dft code both to compress and protect sources against channel variations .",
    "this extends the wyner - ziv coding to the case when errors occur during transmission and proposes joint source - channel coding ( jscc ) with side information at the decoder , within the realm of real - number codes .",
    "this scheme directly maps short source blocks into channel blocks , and thus it is well suited to low - delay coding . while the mse performance of distributed jscc systems with binary codes is limited to the quantization error level , the proposed scheme breaks through this limit .",
    "the paper is organized as follows . in section  [ sec : sys ] , we motivate the new framework for lossy dsc . in section  [ sec : dft ] , we study the encoding and decoding of dft codes , and we adapt the subspace error localization to the slepian - wolf coder . then in section  [ sec : wz ] , we present the encoder and decoder for the wyner - ziv coding based on dft codes , both for the syndrome and parity approaches .",
    "the system is extended to the noisy channel case in section  [ sec : djscc ] .",
    "section  [ sec : sum ] presents the simulation results .",
    "section  [ sec : con ] provides our concluding remarks .    for notation , we use upper - case and boldface lower - case letters for vectors , boldface upper - case letters for matrices , @xmath2 for transpose , @xmath3 for conjugate transpose , @xmath4 for conjugate , and @xmath5 for the trace . the dimensions of matrices are indicated by one and two subscripts , when required .",
    "similar to error correction in finite fields , the basic idea of error correcting codes in the _ real field _ is to insert redundancy to a message vector to convert it to a longer vector , called the codeword .",
    "nevertheless , the insertion of redundancy is done in the real field , i.e. , before quantization and entropy coding @xcite .",
    "one main advantage of _ soft redundancy _ ( real field codes ) over _ hard redundancy _ ( binary field codes ) is that by using soft redundancy one can go beyond the quantization error level and thus reconstruct continuous - valued signals more accurately .",
    "we introduce the use of real - number codes in lossy compression of correlated signals .",
    "the proposed system is depicted in fig .",
    "[ fig : realdsc ] .",
    "although it consists of the same blocks as the existing wyner - ziv coding scheme @xcite , the order of these blocks is changed here .",
    "therefore binning is performed before quantization ; we use dft codes for this purpose @xcite .",
    "this change in the order of binning and quantization blocks brings some advantages to lossy dsc , as described in the following :      in the existing framework for lossy dsc , correlation between two sources is modeled after quantization , i.e. , in the binary domain .",
    "more precisely , correlation between quantized sources is usually modeled as a binary symmetric channel ( bsc ) , mostly with known crossover probability @xcite .",
    "admittedly though , due to nonlinearity of the quantization operation , correlation between the quantized signals is not known accurately even if it is known in the continuous domain .",
    "this motivates us to investigate a method that exploits correlation between continuous - valued sources to perform dsc .      in lossy data compression with side information at the decoder , soft redundancy , added by dft codes ,",
    "can be used to correct both quantization errors and ( correlation ) channel errors .",
    "thus , the loss due to quantization can be recovered , at least partly if not wholly .",
    "in fact , if the two sources are exactly the same over a codeword , the quantization error can be compensated for .",
    "that is , perfect reconstruction is attainable over the corresponding samples .",
    "the loss due to the quantization error is decreased by a factor of code rate even if correlation is not perfect , i.e. , when ( correlation ) channel errors exist .",
    "this is because dft codes are _ tight _ frames ; hence , they minimize the mse @xcite .",
    "if communication is subject to low - delay constraints , we can not use turbo or ldpc codes , as their performance is not satisfactory for short code length .",
    "low - delay coding has recently drawn a lot of attention ; it is done by mapping short source blocks into channel blocks @xcite . whether low - delay requirement exists or not depends on the specific applications . however , even in the applications that low - delay transmission is not imperative , it is sometimes useful to consider low - dimensional systems for their low computational complexity .",
    "accurate modeling of the correlation between the sources plays a crucial role in the efficiency of the dsc systems .",
    "most of the previous works assume that the correlation between continuous - valued sources can be modeled by an i.i.d .",
    "bsc in the binary field .",
    "this is not , however , exact because of the nonlinearity of quantization operation . besides",
    ", independent errors in the sample domain are not translated to independent errors in the bit domain @xcite .",
    "these issues can be avoided if we exploit the correlation between the continuous - valued sources before quantization .",
    "the correlation between the analog sources @xmath6 and @xmath7 , in general , can be defined by @xmath8 where @xmath9 is a real - valued random variable .",
    "particularly , the above model represents some well - known models motivated in video coding and sensor networks .",
    "let @xmath10 in which @xmath11 and @xmath12 .",
    "then , for @xmath13 or @xmath14 the gaussian correlation is obtained , which is broadly used in the sensor networks literature .",
    "further , for @xmath15 the gaussian - bernoulli - gaussian ( gbg ) and for @xmath16 , @xmath17 the gaussian - erasure ( ge ) models are realized .",
    "the latter two models are more suitable for video applications @xcite .",
    "in this section , we study a class of real - number codes that are employed for binning throughout this paper , investigate some properties of their syndrome , and adapt their decoding algorithm to the slepian - wolf coding .",
    "these codes are a family of bose - chaudhuri - hocquenghem ( bch ) codes in the real field whose parity - check matrix @xmath18 and generator matrix @xmath19 are defined based on the dft matrix ; they are , thus , known as bch - dft codes , or simply dft codes .",
    "bch - dft codes@xcite are linear block codes over the _ real _ or _ complex _ fields .",
    "similar to other bch codes , the spectrum of any codeword is zero in a block of @xmath20 cyclically adjacent components , where @xmath21 is the designed distance of that code @xcite .",
    "the error correction capability of the code is , hence , given by @xmath22 .",
    "in this paper , we only consider real bch - dft codes , i.e. , the bch - dft codes whose generator matrix has real entries only .",
    "an @xmath23 real bch - dft code is defined by its generator and parity - check matrices .",
    "the generator matrix is given by @xmath24    in which @xmath25 and @xmath26 respectively are the dft and idft matrices of size @xmath27 and @xmath28 , and is an @xmath29 matrix defined as @xmath30 where @xmath31 and @xmath27 can not be simultaneously even for a real dft code @xcite , one can show that @xmath32 .",
    "] , @xmath33 , and the sizes of zero blocks are such that @xmath34 is an @xmath35 matrix @xcite . then , for any @xmath36 , this enforces the spectrum of the codeword @xmath37 to have @xmath38 consecutive zeros , which is required for any bch code @xcite .",
    "the parity - check matrix @xmath18 , on the other hand , is constructed by using the @xmath38 columns of @xmath26 corresponding to the @xmath38 zero rows of @xmath39 .",
    "therefore , by virtue of the unitary property of @xmath26 , @xmath18 is the null space of @xmath19 , i.e. , @xmath40    in the rest of this paper , we use the term dft code in lieu of real bch - dft code .      before introducing the decoding algorithm",
    ", we define some notation and basic concepts .",
    "let @xmath41 be the received vector ( a noisy version of @xmath42 ) , where @xmath42 is a codeword generated by .",
    "suppose that @xmath43 is an error vector with @xmath44 nonzero elements at positions @xmath45 ; the magnitude of error at position @xmath46 is @xmath47 .",
    "then , we can compute @xmath48 where @xmath49^t$ ] is a complex vector with @xmath50 in which @xmath32 as defined in , @xmath51 , and @xmath52 next , we define the syndrome matrix @xmath53 , \\label{eq : syndmatrix }   \\end{aligned}\\ ] ] for @xmath54 @xcite . also , we define the covariance matrix as @xmath55    for decoding , we use the extension of the well - known peterson - gorenstein - zierler ( pgz ) algorithm to the real field @xcite . this algorithm , aimed at detecting , localizing , and calculating the errors , works based on the syndrome of error .",
    "we summarize the main steps of this algorithm , adapted for a dft code of length @xmath28 , in the following .    *",
    "* error detection : * determine the number of errors @xmath44 by constructing a syndrome matrix and finding its rank . *",
    "* error localization : * find the coefficients @xmath56 of the _ error - locating polynomial _ @xmath57 whose roots @xmath58 are used to determine error locations ; the errors are then in the locations @xmath59 such that @xmath60 and @xmath61 .",
    "* * error calculation : * finally , calculate the error magnitudes by solving a set of linear equations whose constants coefficients are powers of @xmath62 .    in practice however , the received vector is distorted because of quantization .",
    "let @xmath63 and @xmath64 denote the quantized codeword and quantization noise so that @xmath65 .",
    "therefore , @xmath66 and its syndrome is no longer equal to the syndrome of error because @xmath67 where @xmath68 and @xmath69^t$ ] is the quantization error .",
    "the distorted syndrome samples can be written as @xmath70 the distorted syndrome matrix @xmath71 and the corresponding covariance matrix @xmath72 are defined similar to and but for the distorted syndrome samples .    while the exact value of the error is determined neglecting quantization",
    ", the decoding becomes an _ estimation _ problem in the presence of quantization .",
    "then , it is imperative to modify the pgz algorithm to decode the errors reliably @xcite . in the remainder of this section",
    ", we discuss this problem and also improve the error detection and localization , by introducing a slightly different version of the existing methods .      for a given dft code",
    ", we first fix an empirical threshold @xmath73 based on eigendecomposition of @xmath74 when the codewords are error - free , i.e. , when only the quantization error exist .",
    "this threshold is on the magnitude of eigenvalues , rather than the determinant of @xmath74 .",
    "let @xmath75 denote the largest eigenvalue of @xmath74 for @xmath76 .",
    "we find @xmath73 such that , for a desired probability of correct detection @xmath77 , @xmath78 in practice , when errors can occur , we estimate the number of errors by the number of eigenvalues of @xmath74 greater than @xmath73 .",
    "this one step estimation is better than the original estimation in the pgz algorithm @xcite , where the last row and column of @xmath79 are removed until coming up with a non - singular matrix .",
    "the improvement comes from incorporating all syndrome samples , rather than some of them , for decision making .",
    "the _ subspace _ or _ coding - theoretic _ error localizations @xcite can be used to find the coefficients @xmath56 of the error - locating polynomial .",
    "the subspace approach is , however , more general than the coding - theoretic approach in the sense that it can use up to @xmath80 degrees of freedom to localize @xmath44 errors , compared to just one degree of freedom in the coding - theoretic approach .",
    "hence , it improves the error localization in the presence of the quantization noise .",
    "we apply the subspace error localization both to the syndrome- and parity - based dsc ( see section  [ sec : wz ] ) , similar to that in channel coding @xcite . to this end",
    ", we eigen - decompose the covariance matrix @xmath72 for @xmath81 , the best result is achieved for @xmath76 @xcite .",
    "for this @xmath82 , the size of @xmath71 is either @xmath83 or @xmath84 .",
    "] it results in two orthogonal subspaces , the _ error _ and _ noise _ subspaces .",
    "there are @xmath85 vectors in the noise subspace ; we use them to localize errors using the music algorithm @xcite .",
    "however , one should note that the way we compute the syndrome in dsc is different from that in channel coding which was presented in and ; this will be elaborated in section  [ sec : wz ] .",
    "this last step is rather simple .",
    "let @xmath86 be the matrix consisting of the columns of corresponding to error indices .",
    "the errors magnitude @xmath87^t $ ] can be determined by solving @xmath88 in a _",
    "least squares _ sense , for example .",
    "this completes the error correction algorithm by calculating the error vector .",
    "dft codes by construction are capable of decreasing the quantization error .",
    "when there is no error , an @xmath23 dft code brings down the mse below the quantization error level with a factor of @xmath89 @xcite .",
    "this is also shown to be valid for channel errors , as long as channel can be modeled as an additive noise . to appreciate this",
    ", one can consider the generator matrix of a dft code as _",
    "analysis frame operator _ of a tight frame @xcite ; it is known that frames are resilient to any additive noise , and tight frames reduce the mse @xmath90 times @xcite .",
    "hence , dft codes can result in a mse even less than the quantization error level whereas the mse in a binary code is obviously lower - bounded by the quantization error level .",
    "in this section , we use dft codes to do wyner - ziv coding in the real field .",
    "this is accomplished by using dft codes for binning and transmitting compressed signal , in the form of syndrome or parity samples , in a digital communication system .",
    "let @xmath91 be a sequence of real random variables @xmath92 , and @xmath93 be a noisy version of @xmath91 such that @xmath94 , where @xmath95 is continuous , i.i.d . , and independent of @xmath96 , as described in .",
    "the lower - case letters @xmath97 , @xmath98 , and @xmath99 , respectively , are used to show the realization of the random variables @xmath6 , @xmath7 , and @xmath9",
    ". since @xmath43 is continuous , this model precisely captures any variation of @xmath100 , so it can model correlation between @xmath91 and @xmath93 accurately .",
    "this correlation model is important , for example , in video coders that exploit wyner - ziv concepts , e.g. , when the decoder builds side information via extrapolation of previously decoded frames or interpolation of key frames @xcite . in this paper",
    ", we use the ge model with @xmath101 in where @xmath43 is a sparse vector inserting up to @xmath102 errors in each codeword .            given , to compress an arbitrary sequence of data samples ,",
    "we multiply it with to find the corresponding syndrome samples @xmath103 . the syndrome is then quantized ( @xmath104@xmath105@xmath104@xmath106 ) and transmitted over a noiseless digital communication system , as shown in fig .",
    "[ fig : wzsynd ] . note that @xmath107 , @xmath103 are both complex vectors of length @xmath38 .",
    "thus , it seems that to transmit each sample we need to send two real numbers , one for the real part and one for the imaginary part , which halves the compression ratio .",
    "however , we observe that the syndrome of a dft code is symmetric , as stated below .",
    "the syndrome of an @xmath1 dft code satisfies @xmath108 for @xmath109 and @xmath20 .",
    "[ lem1 ]    see appendix  [ sec : app1 ] .",
    "the above lemma implies that , for any @xmath110 , it suffices to know the first @xmath111 syndrome samples .",
    "we know that syndromes are complex numbers in general ; however , from @xmath112 it is clear that if @xmath110 is an odd number , @xmath113 is real for @xmath114 .",
    "therefore , for an @xmath1 code with odd @xmath27 , transmitting @xmath38 real numbers suffices .",
    "this results in a compression ratio of @xmath115 for the slepian - wolf encoder .",
    "yet , one can check that for even @xmath27 , we have to transmit @xmath116 real samples , which incurs a slight loss in compression .",
    "it is however negligible for large @xmath28 .",
    "the decoder estimates the input sequence from the received syndrome and side information @xmath117 . to this end",
    ", it needs to evaluate the syndrome of ( correlation ) channel errors .",
    "this can be simply done by subtracting the received syndrome from the syndrome of the side information .",
    "then , neglecting the quantization error , we obtain , @xmath118 and @xmath119 can be used to precisely estimate the error vector , as described in section [ sec : dec ] . in practice , however , the decoder knows @xmath104@xmath105@xmath104@xmath106 rather than .",
    "therefore , only a distorted syndrome of error is available , i.e. , @xmath120 hence , using the pgz algorithm , error correction is accomplished based on .",
    "note that , having computed the syndrome of error , decoding algorithm in a dsc using dft codes is exactly the same as that in the channel coding problem .",
    "this is different from dsc techniques in the binary field which usually require a slight modification in the corresponding channel coding algorithm to be customized for dsc .",
    "the syndrome - based wyner - ziv coding is straightforward , but it is not clear how we can use it for noisy transmission . in the sequel ,",
    "we explore an alternative approached , namely parity - based approach , to the wyner - ziv coding .",
    "to compress @xmath100 , the encoder generates the corresponding parity sequence @xmath121 with @xmath38 samples .",
    "the parity is then quantized and transmitted , as shown in fig .",
    "[ fig : wzparity ] , instead of transmitting the input data . to this end",
    ", we need to find a systematic generator matrix @xmath122 , as @xmath19 in is not in the systematic form .",
    "a first approach is to find @xmath123 and build @xmath122 based on that @xcite .",
    "another , simpler , way is to obtain a systematic generator matrix directly from @xmath19 .",
    "let @xmath124 be a square matrix of size @xmath27 composed of arbitrary rows of @xmath19 .",
    "we see that @xmath124 is invertible because using any @xmath125 submatrix of @xmath19 can be represented as product of a vandermonde matrix and the dft matrix @xmath126 .",
    "this is also proven using a different approach in @xcite , where it is shown that any subframe of @xmath19 is a frame , and its rank is equal to @xmath27 .",
    "hence , a systematic generator matrix is given by @xmath127 besides , from @xmath128 , it is clear that @xmath129 therefore , we do not need to calculate @xmath123 , and the same parity - check matrix @xmath18 can be used for decoding in the parity approach .",
    "it is also obvious that @xmath122 is a real matrix .",
    "the question that remains to be answered is whether @xmath122 corresponds to a bch code . to generate a bch code ,",
    "@xmath122 must have @xmath38 consecutive zeros in the transform domain .",
    "the fourier transform of this matrix @xmath130 satisfies the required condition because @xmath131 , the fourier transform of original matrix , satisfies that .",
    "it should be emphasized that one can arbitrarily choose the rows of @xmath124 in ; this results in @xmath132 systematic generator matrix for an @xmath1 dft code .",
    "although any of those systematic codes can be used for encoding , the dynamic range of the generated parity samples depends on their relative position of the chosen rows @xcite . in (",
    "* theorem  7 ) , we have proved that when using these systematic frames for error correction , the mean - squared reconstruction error is minimized when the systematic rows are chosen as evenly spaced as possible . in the extreme scenario , where the systematic rows are equally spaced ,",
    "the systematic frame is also _",
    "tight_. this is realized only when @xmath28 is an integer multiple of @xmath27 .",
    "such a frame lends itself well to minimize reconstruction error @xcite .",
    "finally , seeing that parity samples are real numbers , using an @xmath23 dft code , a compression ratio of @xmath133 is achieved .",
    "obviously , a compression ratio of @xmath115 is achievable if we use a @xmath134 dft code .",
    "a parity decoder estimates the input sequence from the received parity and side information @xmath117 .",
    "similar to the syndrome approach , at the decoder , we need to find the syndrome of ( correlation ) channel errors .",
    "to do so , we append the parity to the side information and form a vector of length @xmath28 whose syndrome , neglecting quantization , is equal to the syndrome of error .",
    "that is , @xmath135   = \\left [ \\begin{array}{cc } \\bm{x }    \\\\",
    "\\bm{p }   \\end{array}\\right ]   + \\left [ \\begin{array}{cc } \\bm{e }    \\\\   \\bm{0}\\end{array}\\right]= \\bm{g}_{\\mathrm{sys}}\\bm{x}+\\bm{e } ' ,   \\end{aligned}\\ ] ] and @xmath136^t $ ] .",
    "hence , @xmath137 similarly , when quantization is involved ( @xmath138 ) , we get @xmath139   = \\bm{z } +   \\left [ \\begin{array}{cc } \\bm{0 }    \\\\ \\bm{q}\\end{array}\\right]= \\bm{g}_{\\mathrm{sys}}\\bm{x}+\\bm{e}'+\\bm{q } ' ,   \\end{aligned}\\ ] ] and @xmath140 where , @xmath141^t $ ] , and @xmath142 .",
    "therefore , we obtain a distorted version of error syndrome . in both cases ,",
    "the rest of the algorithm , which is based on the syndrome of error , is similar to that in the channel coding problem using dft codes , as explained in section  [ sec : decloc ] .",
    "error localization algorithm for the parity - based dsc @xcite can be further improved using the fact that parity samples are error - free . as parity samples are transmitted over a noiseless channel , the error locations , in the codewords ,",
    "are restricted to the systematic samples .",
    "therefore , we can exclude the set of roots corresponding to the location of the parity samples .",
    "we call this _ adapted _ error localization .",
    "furthermore , it makes sense to use a code with evenly - spaced parity samples so as to optimize the location of error - free and error - prone samples in the codewords .",
    "such a code maximizes the distance between the error - prone roots of the code ; hence , it helps decrease the probability of incorrect decision .",
    "as it was shown earlier , using an @xmath23 code the compression ratio in the syndrome and parity approaches , respectively , is @xmath115 and @xmath133 .",
    "hence , for a given code , the parity approach is @xmath143 times less efficient than the syndrome approach .",
    "conversely , we can find two different codes that result in same compression ratio , say @xmath115 .",
    "we mentioned that in the parity approach , a @xmath134 code can be used for this matter , whereas an @xmath23 dft code gives the desired compression ratio in the syndrome approach .",
    "thus , for a given compression ratio the syndrome approach implies a code with smaller rate compared to the code required in the parity approach .      from frame theory , we know that dft frames are tight , and an @xmath1 tight frame reduces the quantization error with a factor of @xmath144 @xcite .",
    "this result is extended to errors , given that channel can be modeled by an additive noise @xcite .",
    "the mse performance of systematic dft frames also linearly depends on the code rate , though they are not necessarily tight @xcite .",
    "therefore , for codes with the same error correction capability , the lower the code rate the better the error correction performance .",
    "this implies a better performance for syndrome - based dsc .",
    "further , a @xmath134 code has @xmath38 roots more than an @xmath23 code on the unit circle ; hence , the roots are closer to each other and the probability of incorrect localization of errors increases . additionally , from rate - distortion theory we know that the rate required to transmit a gaussian source logarithmically increases with the source variance @xcite .",
    "thus , in a system that uses a real - number code for encoding , since coding is performed before quantization , the variance of transmitted sequence depends on the behavior of the encoding matrix . in the syndrome - based dsc we transmit @xmath145 .",
    "one can check that @xmath146 @xcite .",
    "unlike that , in the parity - based dsc , the variance of the parity samples is larger than that of the inputs .",
    "more precisely , in an @xmath1 systematic dft code , if @xmath147 , then @xmath148 where @xmath149 @xcite .",
    "since we can write @xmath150^t$ ] , we have @xmath151 from ( * ? ? ?",
    "* theorem 7 ) , we know that the smallest @xmath152 for a given dft code is achieved when the parity samples , in the corresponding codewords , are located as  evenly \" as possible . considering the above arguments , one may expect the syndrome - based approach to perform better than the parity - based one , for a given code or fixed compression ratio .",
    "this is verified numerically in section [ sec : sum ] .",
    "the parity - based dsc , however , has other advantages .",
    "for example , by puncturing some parity samples _ rate - adaptive _ dsc , in the real field , is realized .",
    "besides , it can be easily extended to distributed joint source - channel coding , as explained in the following section .",
    "the concept of lossy dsc and wyner - ziv coding using dft codes was explained both for the syndrome and parity approaches in section  [ sec : wz ] , where syndrome or parity samples are quantized and transmitted over a _ noiseless _ channel .",
    "this implies _ separate _ source and channel coding .",
    "although simple , the _ separation theorem _ is based on several assumptions , such as the source and channel coders not being constrained in terms of complexity and delay , which do not hold in many situations .",
    "it breaks down , for example , for non - ergodic channels and real - time communication . in such cases",
    ", it makes sense to integrate the design of the source and channel coder systems , because _ joint source - channel coding _ ( jscc ) can perform better given a fixed complexity and/or delay constraints .",
    "likewise , distributed jscc ( djscc ) has been shown to outperform separate distributed source and channel coding in some practical cases @xcite .",
    "djscc has been addressed in @xcite , using different binary codes .    in this section ,",
    "we extent the parity - based wyner - ziv coding of analog sources to the case where errors in the transmission are allowed .",
    "thus , we introduce distributed jssc of analog correlated sources in the analog domain . to do this",
    ", we use a single dft code both to _ compress _ @xmath100 and _ protect _ it against channel variations ; this gives rise to a new framework for djscc , in which quantization is performed after doing jscc in the analog domain .",
    "this scheme directly maps short source blocks into channel blocks , and thus it is well suited to low - delay coding .          to compress and protect @xmath100 ,",
    "the encoder generates the parity sequence @xmath121 of @xmath38 samples , with respect to a good systematic dft code .",
    "the parity is then quantized and transmitted over a noisy channel , as shown in fig .",
    "[ fig : djscc ] . to keep the dynamic range of parity samples as small as possible",
    ", we make use of optimal systematic dft codes , proposed in @xcite .",
    "this increases the efficiency of the system for a fixed number of quantization levels . using an @xmath23 dft code a total compression ration of @xmath115",
    "is achieved . obviously , if @xmath153 compression is possible .",
    "however , since there is little redundancy the end - to - end distortion could be high .",
    "conversely , a code with @xmath154 expands input sequence by adding _ soft redundancy _ to protect it in a noisy channel .",
    "let @xmath155 be the received parity vector which is distorted by quantization error @xmath64 ( @xmath156 ) and channel error @xmath157 .",
    "also , let @xmath158 denote side information where @xmath159 represents the error due to the `` virtual '' correlation channel .",
    "the objective of the decoder is to estimate the input sequence from the received parity and side information .",
    "although we only need to determine @xmath159 , effectively it is required to find both @xmath159 and @xmath157 . from an error correction point of view , this is equal to finding the error vector @xmath160^t$ ] that affects the codeword @xmath161^t$ ] . hence , to find the syndrome of error at the decoder , we append the parity @xmath162 to the side information @xmath117 and form @xmath163 , a valid codeword perturbed by quantization and channel errors , @xmath164   + \\left [ \\begin{array}{cc } \\bm{e}_v    \\\\",
    "\\bm{e}_c \\end{array}\\right ] +   \\left [ \\begin{array}{cc } \\bm{0 }    \\\\ \\bm{q}\\end{array}\\right]= \\bm{g}_{\\mathrm{sys}}\\bm{x}+\\bm{e}+\\bm{q } ' .",
    "\\end{aligned}\\ ] ] multiplying both sides by @xmath18 , we obtain @xmath165 where @xmath166 and @xmath142 .",
    "again , we use the ge model with @xmath101 in to generate @xmath43 .",
    "it should be emphasized that for @xmath167 , error vector can be determined exactly , as long as the number of errors is not greater than @xmath102 . in practice , quantization is also involved , and we obtain only a distorted version of error syndrome . knowing the syndrome of error",
    ", we use the error detection and localization algorithm , explained in section  [ sec : dec ] , to find and correct error .",
    "we evaluate the performance of the proposed systems using a gauss - markov source with mean zero , variance one , and correlation coefficient 0.9 ; the effective range of the input sequences is thus @xmath168 $ ] .",
    "the sources are binned using a @xmath169 dft code . the compressed vector , either syndrome or parity , is then quantized with a 6-bit uniform quantizer with step size @xmath170 and transmitted over a noiseless communication media .",
    "the  virtual \" correlation channel inserts @xmath102 errors , generated by @xmath171 .",
    "the decoder detects , localizes , and decodes errors .",
    "we compare the mse between transmitted and reconstructed codewords , to measurers end - to - end distortion . in all simulations ,",
    "we use @xmath172 input blocks for each channel - error - to - quantization - noise ratio ( ceqnr ) , defined as @xmath173 and @xmath174 .",
    "we vary the ceqnr and plot the resulting mse .",
    "the results are presented in fig .",
    "[ fig : syndq ] and compared against the quantization error level in the existing lossy dsc methods .",
    "dft code in fig . 2 , 3 .",
    "for both schemes , the virtual correlation channel inserts one error at each input block . ]",
    "it can be observed that , in the syndrome approach the mse is lower than the quantization error level in all ceqnrs .",
    "analogously , in the parity approach , the mse is less than that level except for a small range of ceqnr ; further , it slightly improves when we use the adapted parity approach .",
    "note that in lossy dsc using binary codes , the mse can be equal to the quantization error level only if the bit error rate ( ber ) for the slepian - wolf coder is zero .",
    "this ideal case is not necessarily attainable even if capacity - approaching codes are used  @xcite .",
    "the performance of our systems considerably improves when ceqnr is very high .",
    "the improvement is due to a better error localization , since the higher the ceqnr , the better the error localization , as shown in fig .",
    "[ fig : poe ] . at very low ceqnrs , although error localization is poor , the mse is still very low .",
    "this is because , compared to the quantization error , the errors are so small that the algorithm mostly does not detect ( and thus localize ) any error .",
    "instead , it may occasionally localize and correct quantization errors .",
    "note that , even if no errors are localized and corrected , the mse is still very small as the errors are negligible at very low ceqnrs . additionally , recall that the mse is always reduced with a factor of @xmath144 , in an @xmath1 dft code @xcite .",
    "it should be added that , for error detection , we use the algorithm proposed in section  [ subsec : pd ] ; for this purpose , we first find the threshold corresponding to @xmath175 ; it leads to @xmath176 .",
    "the corresponding plots are not included , for brevity ; even so , similar plots are studied in the remainder of this section .",
    "since @xmath177 , the coding theoretic and subspace error localizations give the same results ; hence , we employ one of them in this set of plots . for the parity approach , we also plot the adapted error localization , introduced in section  [ subsec : pardec ] , as well . ]    in terms of compression , the efficiency of parity approach to the syndrome approach is @xmath178 , as discussed earlier in section  [ comp ] . despite this , the performance of the parity approach is not as good as that of the syndrome approach .",
    "one reason is that in this simulation @xmath179 of samples are corrupted in the parity approach while this figure is @xmath180 for the syndrome approach .",
    "more importantly , the dynamic range of the parity samples , generated by , could be much higher than that of the syndrome samples ; this range depends on the position of the parity samples in the codewords .",
    "a wider range implies more quantization levels to achieve the same accuracy . for the @xmath181 code , in the best case we get @xmath182 since @xmath183 in .",
    "this is realized for a codeword pattern @xmath184 , where `` @xmath185 s '' and `` @xmath186 s '' represent data and parity samples , respectively .",
    "next , we evaluate the performance of the jscc with side information at the decoder , illustrated in fig .  [",
    "fig : djscc ] . by using a systematic @xmath187 dft code ,",
    "we generate , quantize , and transmit parity samples over a noisy channel .",
    "note that , for this code the best systematic code @xcite achieves the lower bound in ; i.e. , it results in @xmath188 .",
    "the correlation channel and transmission channel altogether insert up to @xmath102 errors generated by @xmath171 .",
    "simulation results are plotted in fig .",
    "[ fig : simulations ] .",
    "first , based on fig .",
    "[ fig : subfig1 ] , the threshold @xmath189 is fixed for @xmath175 .",
    "next , this is used to estimate @xmath44 in fig .",
    "[ fig : subfig2 ] .",
    "the estimated number is subsequently used to find the location of errors , both for the pgz and subspace methods , in fig .",
    "[ fig : subfig3 ] .",
    "then , the output of fig .",
    "[ fig : subfig3 ] , for the subspace method , is fed to the last step to find the magnitude of errors and correct them . finally , in fig .",
    "[ fig : subfig4 ] , the mse is compared against the quantization error level , the ideal case in the lossy source coding based on binary codes . to put our results in perspective",
    ", we also calculate the mse assuming perfect error localization ; it gives @xmath190 , @xmath191 , and @xmath192 respectively for 0 , 1 , and 2 errors , in every ceqnr .",
    "this implies that there is still room to improve the mse performance of the proposed system , given a better solution for the error localization .",
    "it also shows the performance gap between this dft code and binary codes in the ideal case .",
    "it should be noted that capacity - approaching channel codes may introduce significant delay if one strives to approach the capacity of the channel with very a low probability of error .",
    "therefore , those are out of the question for delay - sensitive systems . in that case , it would be best to use channel codes of low rate and focus on achieving a very low probability of error .",
    "the system we introduced is a low - delay system which works well with reasonably high - rate codes .",
    "finally , by puncturing some parity samples , rate - adaptive schemes are realized for the proposed djscc and parity - based dsc .",
    "rate - adaptive systems are popular in the transmission of non - ergodic data , like video @xcite .",
    "we have introduced a new framework for the distributed lossy source coding , in general , and the wyner - ziv coding , in particular . the idea is to do binning before quantizing the continuous - valued signal , as opposed to the conventional approach where binning is done after quantization . by doing binning in the real field",
    ", the virtual correlation channel can be modeled more accurately and the quantization error can be compensated for when there is no error . in the new paradigm , wyner - ziv coding",
    "is realized by cascading a slepian - wolf encoder with a quantizer .",
    "we employ real bch - dft codes to do the slepian - wolf in the real field . at the decoder , by introducing both syndrome - based and parity - based systems ,",
    "we adapt the pgz decoding algorithm of channel coding to dsc .",
    "the extension of the parity - based wyner - ziv coding to joint source - channel coding with side information at the decoder is straightforward .",
    "this scheme directly maps short source blocks into channel blocks , and thus it is appropriate for low - delay coding . from simulation results ,",
    "we conclude that our systems can improve the reconstruction error even using short codes , so they can become viable in real - world scenarios where low - delay communication is required .",
    "we have also adapted the subspace error localization in this context and improved it for the parity - based scheme .",
    "this reasonably improves the error localization and leads to a better mean - squared reconstruction error .",
    "we should point out that , a more accurate algorithm for error localization is a key to further improve the reconstruction error .",
    "particularly , error localization becomes more challenging when codeword length increases , as the roots of error locator polynomial , which are over the unit circle , get closer .",
    "the proof is straightforward ; we show this for odd @xmath27 and leave the other case to the reader . since @xmath193 and @xmath194 , using , for odd @xmath27 we can write @xmath195 note that @xmath196 , for any @xmath197 .",
    "f.  marvasti , m.  hasan , m.  echhart , and s.  talebi , `` efficient algorithms for burst error recovery using fft and other transform kernels , '' _ ieee transactions on signal processing _ , vol .",
    "47 , pp .  10651075 , april 1999 .",
    "g.  takos and c.  n. hadjicostis , `` determination of the number of errors in dft codes subject to low - level quantization noise , '' _ ieee transactions on signal processing _ , vol .",
    "56 , pp .  10431054 , march 2008 .",
    "m.  vaezi and f.  labeau , `` least squares solution for error correction on the real field using quantized dft codes , '' in _ proc . the 20th european signal processing conference ( eusipco )",
    "_ , pp .  25612565 , 2012 .",
    "f.  bassi , m.  kieffer , and c.  weidmann , `` source coding with intermittent and degraded side information at the decoder , '' in _ proc .",
    "ieee international conference on acoustics , speech and signal processing ( icassp ) _ , pp .  29412944 , 2008 .",
    "m.  vaezi and f.  labeau , `` systematic dft frames : principle , eigenvalues structure , and applications , '' submitted to _ ieee transactions on signal processing_. [ online ] .",
    "available : http://arxiv.org/pdf/1207.6146.pdf .",
    "a.  d. liveris , z.  xiong , and c.  n. georghiades , `` joint source - channel coding of binary sources with side information at the decoder using ira codes , '' in _ proc .",
    "ieee workshop on multimedia signal processing _ , pp .  5356 , 2002 .",
    "j.  garcia - frias and z.  xiong , `` distributed source and joint source - channel coding : from theory to practice , '' in _ proc .",
    "ieee international conference on acoustic , speech and signal processing ( icassp ) _ , pp .  10931096 , 2005 ."
  ],
  "abstract_text": [
    "<S> we show how _ real - number codes _ can be used to compress correlated sources and establish a new framework for _ distributed lossy source coding _ , in which we quantize compressed sources instead of compressing quantized sources . </S>",
    "<S> this change in the order of _ binning _ and _ quantization _ blocks makes it possible to model correlation between continuous - valued sources more realistically and compensate for the quantization error when the sources are completely correlated . </S>",
    "<S> we focus on the asymmetric case , i.e. , lossy source coding with side information at the decoder , also known as _ wyner - ziv coding_. the encoding and decoding procedures are described in detail for discrete fourier transform ( dft ) codes , both for _ syndrome- _ and _ parity - based _ approaches . </S>",
    "<S> we also extend the parity - based approach to the case where the transmission channel is noisy and perform distributed _ joint source - channel coding _ in this context . </S>",
    "<S> the proposed system is well suited for _ low - delay _ communications . </S>",
    "<S> furthermore , the mean - squared reconstruction error ( mse ) is shown to be less than or close to the quantization error level , the ideal case in coding based on binary codes .    </S>",
    "<S> wyner - ziv coding , distributed source coding , joint source - channel coding , real - number codes , bch - dft codes , syndrome , parity , low - delay . </S>"
  ]
}