{
  "article_text": [
    "the demands of fitting models to large data - sets have exploded over the last decade .",
    "increasingly complex data sets are available , which has placed demands on statisticians to develop realistic models to represent these data .",
    "inevitably , for many classes of models , this places a further emphasis on being able to fit such models accurately and in a reasonable time - frame .    in this article",
    ", we consider fast bayesian statistical inference for dirichlet process mixture ( dpm ) models @xcite . this particular class of models have proven to be popular in the literature as a tool for both clustering and density estimation and there are a wide variety of elegant mcmc and sequential monte carlo algorithms ; see e.g.  @xcite .",
    "such algorithms provide exact inference from dpm models , but can be very computationally demanding when trying to analyze extremely large data - sets and even more , exact statistical inference from mixtures is notoriously difficult ; see @xcite . as mentioned above , this issue often leads to researchers resorting to approximate inference to browse or interrogate the data , so as to refine model specifications for an exact analysis ; one particular important and interesting method in this direction is the sugs algorithm .",
    "the sugs algorithm is a procedure for fast approximate fitting of dpm models .",
    "it relies on an approximation of the exact posterior distribution on the allocation of data to components and the component specific parameters , by sequentially adding data - points to the model .",
    "these data are allocated to a given mixture component and this allocation is frozen and taken as the truth when updating the posterior for new data points .",
    "the method can be sensitive to the ordering of the data , but @xcite provide procedures to select this ordering in a systematic way . in this paper",
    ", we reinterpret the sugs algorithm within a variational bayes framework .",
    "this allows one to derive different approximations of the posterior distribution .",
    "in particular , our interpretation does not mean that one needs to allocate data to a cluster and , instead , provides a probability distribution on these allocations ; this is done at a minor increase in computational cost .",
    "the advantage of this generalization , which we call vsugs , is apparent when one fits the approximation to data whose components are close in some sense . in this scenario , we have consistently found ( and as illustrated in section [ sec : simos ] ) that vsugs outperforms sugs in a variety senses ; this is important as , when the mixture components are highly separated , such initial browsing or interrogation of the data is less important .",
    "moreover , our variational approximation provides a lower - bound on the log - marginal likelihood ; we empirically find that this can be used as a technique for model selection ( e.g.  selecting the order in which the data arrive ) which did not seemingly work well in @xcite .",
    "this article is structured as follows .",
    "we begin with a motivating example in section [ sec : motivating ] . in section [ sec : dpm ]",
    "we give a basic summary of dpm models . in section [ sec : sugs ] we discuss sugs and in section [ sec : vsugs ] our generalization vsugs . in section [ sec : simos ] we give numerical examples ; both a simulation study and real data analyses associated to flow cytometry data and snp data via a three - class dirichlet process mixture model . in section [ sec : summ ] we conclude the article , discussing avenues for future work .",
    "a motivating application for us is the problem of genotyping single nucleotide polymorphisms ( snps ) from snp genotyping microarray data @xcite .",
    "figure [ fig : genotyping_example ] illustrates an example dataset .",
    "the statistical problem is to characterise the three genotype classes @xmath2 , @xmath3 and @xmath4 and classify each data point into one of these three classes .",
    "this is a straightforward three - way classification problem that can be approached using hierarchical mixture modelling where , for example , the class - conditional densities are modelled using multivariate normal or student @xmath5-distributions .",
    "although , these models work well in practice , it is clear that the class - conditional densities are not normal ( or student ) .",
    "we can obtain increased accuracy through the use of semi - parametric models for the class - conditional densities using dirichlet process mixtures .",
    "however , the size of the data sets presents a massive challenge for this type of modelling approach . for a single experiment ( individual )",
    ", modern genotyping microarray produces 300,000 - 5,000,000 two - dimensional measurements .",
    "each study may consist of hundreds to thousands of individuals .",
    "the data sizes here prohibit the use of monte carlo inference and motivate approximate approaches that are able to scale to the size of problems encountered .",
    "consider a dirichlet process mixture model of the form , for @xmath6 : @xmath7 where @xmath8 , @xmath9 are observation specific parameters , @xmath10 is a conditional probability which admits a density @xmath11 w.r.t .",
    "a single dominating @xmath12finite measure for each @xmath13 ( which is often lebesgue ) , @xmath14 is an unknown mixing distribution , and @xmath15 indicates that the prior for @xmath14 is a dirichlet process @xcite with precision parameter @xmath16 and base measure @xmath17 . @xmath17 is known and we also consider @xmath18 to be fixed .",
    "we remark that , in connection to subsequent methodology to be presented , @xcite consider a way of handling unknown @xmath18 which can also be used in all the extensions we consider but for simplicity we do not consider this below .",
    "a well known property of the dirichlet process is that a distribution drawn from it will put all its mass on a countable set of points .",
    "following the notation of @xcite we will write @xmath19 for the set of distinct values in the sequence @xmath20 where points in @xmath21 are labelled according to their order of appearance in @xmath22 .",
    "next , let @xmath23 if @xmath24 and write @xmath25 . using the plya urn characterization of the dirichlet process @xcite we can rewrite ( [ dpmmodel ] ) in the form @xmath26 where @xmath27 , @xmath28 is the probability density associated to @xmath17 and @xmath29 is as follows , ( writing @xmath30 , with the convention @xmath31 is the null vector ) , for @xmath32 @xmath33 where @xmath34 , @xmath35 , and @xmath36 is the maximum value in @xmath37 ( i.e.  the number of components  seen \" in the data up to to time @xmath38 ) .",
    "this representation of the model where the unknown measure @xmath14 is integrated out is important for many monte carlo sampling schemes for fitting dp mixtures @xcite .",
    "later we will work with a truncated dirichlet process mixture model , which is often convenient for computations @xcite .",
    "such truncations are based on the stick breaking representation of the dirichlet process @xcite .",
    "suppose we limit the number of distinct values appearing in the sequence @xmath39 to an upper truncation limit @xmath40 . then generalizing the plya urn representation we can consider the truncated dirichlet process with @xmath41 where now @xmath42 and @xmath29 is defined recursively by @xmath34 and similarly to ( [ polyaurn1 ] ) ( see , for example , ( * ? ? ? * section 2.2.2 ) ) for @xmath43 @xmath44    we note that truncations have also been used in the context of variational approximations for dirichlet process mixtures @xcite although these authors consider the truncation point as a variational parameter without truncating the original model .",
    "the algorithm of @xcite , although related to ours , is not a sequential algorithm however - here we are interested in very fast sequential algorithms related to the sugs method of @xcite .",
    "@xcite compare their approach with a variety of other fast computational methodologies for dirichlet process mixtures , and show that their algorithm is competitive with other fast approximation methodologies . in this work we focus only on comparing our new approach with the original sugs algorithm , and refer the reader to @xcite for information about the relative performance of alternative approximations to sugs",
    "sugs is a recursive algorithm that takes at time @xmath38 an estimate @xmath45 of @xmath37 and an approximation of @xmath46 and produces an estimate @xmath47 of @xmath48 and an approximation of @xmath49 . to start the recursion we use @xmath50 and @xmath51 here and in what follows",
    "a term @xmath52 in the conditioning means @xmath53 and similarly for @xmath47 .",
    "suppose at time @xmath38 we have an estimate @xmath45 of @xmath37 .",
    "consider the posterior distribution @xmath54 and approximate this by @xmath55 . that is to say , we initially consider our approximation of @xmath54 , @xmath56 , as @xmath57 using this approximation and integrating out @xmath21 , @xmath58 the sugs algorithm sets : @xmath59 \\label{eq : hat_delta}\\ ] ] and then one replaces @xmath60 by @xmath61 to form the approximation : @xmath62 in this approximation the components @xmath63 are independent for different @xmath64 , the posterior for @xmath63 for @xmath65 is unchanged from time @xmath38 , and the posterior for @xmath66 is updated by assuming that @xmath53 so that @xmath67 represents an observation from this mixture component .",
    "if the mixture components are from the exponential family and conjugate priors are used , the densities such as @xmath68 can be calculated in closed form and sufficient statistics are updated recursively , leading to a very efficient update .",
    "@xcite consider a number of further innovations in their algorithm .",
    "first , since the fitting algorithm is sequential and there is a dependence of the fit on the ordering of the data , they suggest running their algorithm for different random orderings and then choosing the best according to a pseudo - likelihood criterion . secondly , for model comparison they suggest the approximation @xmath69 and show that this crude approximation can be useful for tasks such as comparison of parametric and nonparametric models .",
    "thirdly , they suggest a way of dealing with unknown @xmath18 in the dirichlet process prior .",
    "here we suggest a simple improvement of the sugs algorithm which we call vsugs ( variational sugs ) . we begin with a brief introduction to variational bayes ( vb ) methods .",
    "suppose we have a parameter @xmath70 and data @xmath71 , @xmath72 is the prior density , @xmath73 the likelihood and @xmath74 denotes the posterior density w.r.t .",
    "lebesgue measure ( which we use for presentational purposes only ) . in vb",
    "@xcite we split @xmath75 into blocks @xmath76 , @xmath77 , with @xmath78 and seek to find a good approximation to @xmath79 of the form @xmath80 where each @xmath81 is a probability density w.r.t .  the appropriate dimensional lebesgue measure .",
    "given known probability densities for @xmath81 , @xmath82 , the optimal choice for @xmath83 for minimizing the kullback - leibler divergence @xmath84 is @xmath85 \\ }   \\label{graddescent}\\end{aligned}\\ ] ] where @xmath86 $ ] denotes expectation w.r.t .",
    "hence there is a gradient descent algorithm for minimizing based on choosing initial values for the factors in @xmath88 and then iteratively updating each term according to . minimizing ( [ kld ] ) is equivalent to maximizing @xmath89 and is a lower bound on the log marginal likelihood @xmath90 where @xmath91 .",
    "@xmath92 is a key quantity in bayesian model selection and the lower bound is tight , @xmath93 , when @xmath94 . generally @xmath95 is often used as an approximation to @xmath90 for model selection in the vb framework .      as we have seen in the sugs algorithm recursively",
    "approximates @xmath96 . considering this in a variational framework ,",
    "suppose we have an approximation to the posterior @xmath97 of the form @xmath98 in the above expression , we omit certain conditionings ( as will become apparent below ) to reduce the subsequent notational burdens .",
    "we will suggest a way to update this approximation of @xmath96 , using variational ideas .",
    "the approximation will be of the form @xmath99 .",
    "we start the recursion with @xmath100 , @xmath101 , @xmath102 , @xmath103 .",
    "the idea is to make a particular fixed choice at time @xmath104 for @xmath105 and not to revisit that choice at future times .",
    "that is , the solution to the ( partial ) variational optimization at time @xmath38 is used to initialize the optimization at time @xmath104 .",
    "the original sugs algorithm chooses @xmath106 where @xmath52 is defined in but it is possible to make a better choice than this without sacrificing the attractive computational properties of the original sugs algorithm . in particular , at time @xmath104 , we set @xmath107 , @xmath108 and choose @xmath109 for @xmath110 where @xmath111 is a truncation point for the number of mixture components and @xmath112/t)}{\\alpha+i-1 } & \\mbox{$j= ( i-1)\\wedge t+1$}. \\end{array}\\right.\\ ] ] @xmath113 will be chosen as an approximation to @xmath114 . to provide some intuition for this selection of @xmath113 , we remark that @xmath115 next , note that @xmath116=\\mathbb{e}[p(\\delta_i|\\delta_{1:i-1})|y_{1:i-1}]\\ ] ] where the expectation is w.r.t .",
    "this suggests approximating @xmath118 by taking the expectation in the above expression with respect to the variational posterior @xmath119 .",
    "although this approximation is still not easy to work with , if we we condition on @xmath120 in ( [ polyaurn2 ] ) and then take the expectation with respect to @xmath119 , we get @xmath121 .",
    "so @xmath121 is an approximation to @xmath118 in ( [ brule ] ) and the term @xmath122 in simply approximates the integral in by replacing @xmath123 with the variational posterior @xmath124 .",
    "as noted earlier , the original sugs algorithm can be placed in our framework by using @xmath106 , a  hard \" rather than  soft \" allocation to clusters which tends to result in greater under - estimation of uncertainty than in our approach .    next , using @xmath125 as the prior for @xmath63 at time @xmath104 for processing the data point @xmath67 , the optimal choice for @xmath126 is , via @xmath127 where @xmath128 denotes expectation w.r.t .",
    "\\{@xmath129 and @xmath130 .",
    "again , with the choice @xmath106 , exponential family mixture components and conjugate priors this reduces to the sugs update . if the mixture components are normal then the generalized update above can be done in closed form  we will give details of this below .",
    "note the attractive form of the above update .",
    "the likelihood contribution from the @xmath131 observation is split among different mixture components @xmath64 according to the weight @xmath113 rather than assuming the most likely allocation as in the original sugs algorithm .",
    "we can also use the variational lower bound ( [ lowerbd ] ) to approximate the marginal likelihood more accurately than with @xmath132 in the sugs algorithm .",
    "details of this are given in the next section for the case of normal mixture components .",
    "@xcite consider the case of dp mixtures of normals in detail . in this case , @xmath133 where @xmath134 is the mean for the @xmath104th component and @xmath135 is the precision and we have @xmath136 .",
    "they consider a normal inverse - gamma prior for @xmath63 , @xmath137 where @xmath138 is a normal density with associated distribution with mean @xmath139 and variance @xmath140 and @xmath139 and @xmath141 are known hyperparameters , and @xmath142 is a gamma density with known parameters @xmath143 and @xmath144 .    our vsugs algorithm results in @xmath145 being normal inverse - gamma also , @xmath146 where @xmath147 is the normal density with mean @xmath148 and variance @xmath149 , and @xmath150 is a gamma density with parameters @xmath151 and @xmath152 . the parameters @xmath148 , @xmath153 , @xmath151 and @xmath152 are updated recursively by ( c.f .",
    "@xcite ) @xmath154 to calculate the terms @xmath113 in the vsugs algorithm , we also need to evaluate the integral @xmath155 in the normal case with the priors we have chosen this integral evaluates to a @xmath5 density , @xmath156 @xmath157 where @xmath158 denotes a @xmath5 density for @xmath71 with @xmath159 degrees of freedom , location parameter @xmath160 and scale parameter @xmath161 .",
    "an approximate variational lower bound on @xmath162 can also be computed recursively .",
    "we can think of the posterior at stage @xmath38 as the prior to be updated by the likelihood contribution for the @xmath104th observation : @xmath163 approximating @xmath118 by @xmath121 as we did previously and approximating @xmath46 by @xmath164 and calculating the lower bound ( [ lowerbd ] ) using these priors for the likelihood contribution @xmath165 gives @xmath166 where @xmath167 denotes the digamma function .",
    "_ setup . _ we generated 100 data sets for each setting and all results are averaged over that for each data set . @xmath168 for @xmath169 with @xmath170 .",
    "examples are shown in figure [ fig : mixtures ] .",
    "we compared the relative errors of sugs to vsugs under different values of @xmath18 , i.e. @xmath171 . throughout @xmath172 for vsugs .",
    "we used 50 different ( but random ) orderings of the data and chose the ordering with the maximal variational lower - bound for vsugs in section [ sec : dpm_normal ] and the best ordering for sugs as in @xcite .",
    "we also used a standard collapsed gibbs sampling method @xcite for posterior inference on some of the datasets for comparison . to assess the performance in density estimation we compute the value of @xmath173 where @xmath174 , @xmath175 , are the estimated ( predictive ) and true density for the data , evaluated at data - point @xmath176 and examine the relative errors .    _ results . _ figure [ fig : dens_examples ] shows example predictive density estimates from sugs and vsugs .",
    "figure [ fig : relative_error](a ) shows that for large values of @xmath18 and closely spaced clusters @xmath177 , vsugs provides more accurate density estimates than sugs .",
    "however , for @xmath178 and @xmath179 , i.e.  well - separated clusters , the density estimates from sugs are relatively more accurate .",
    "the computation time for vsugs is constant for given truncation level @xmath111 as we use a fixed maximum number of mixture components .",
    "in contrast , the computation time required for sugs is variable and depends both on the data set and the order in which the data is processed .",
    "figure [ fig : relative_error](b ) considers the computation burden for the two methods .",
    "in particular , for large values of @xmath18 and more mixture components , sugs can be computationally quite demanding due to the excessive numbers of mixture components that are realised . whilst in practice , one might estimate @xmath18 , this value is not known and hence sugs could both be significantly less accurate and computationally more expensive in many situations .",
    "we compared the sugs and vsugs predictive densities with those obtained from collapsed gibbs sampling , we considered the case @xmath180 and show results in table [ tab : comparison_to_gibbs ] for different data sizes @xmath181 and the truncation parameter @xmath111 .",
    "using collapsed gibbs sampling as a  gold standard \" , we find that vsugs consistently provides better predictive density estimates .",
    "example computational times for @xmath182 were @xmath183 seconds for sugs , @xmath184 seconds for vsugs ( @xmath185 ) and @xmath186 seconds for collapsed gibbs sampling .",
    "we analyzed the flow cytometry data example , which has been studied thoroughly by @xcite .",
    "flow cytometers detect fluorescent reporter markers that typically correspond to specific cell surface or intracellular proteins on individual cells , and can assay millions of such cells in a fluid stream in minutes .",
    "these data points are associated with one ( or more ) components of a gaussian mixture model ( @xcite ) and are from human peripheral blood cells , with 6 marker measurements each : forward scatter ( measure of cell size ) , side scatter ( measure of cell granularity ) , cd4 ( marker for helper t cells ) , ifng+il-2 ( effector crytokines ) , cd8 ( a marker for cytotoxic t cells ) , cd3 ( marker for all t cells ) ; that is , the observations are 6 dimensional ( the priors are modified to normal - inverse wishart , which leads to a similar derivation of the vsugs algorithm as in section [ sec : dpm_normal ] , in this multivariate scenario ) .",
    "our objective is to compare the performance of vsugs to sugs and collapsed gibbs sampling for clustering and density estimation in this multivariate , large data setting .",
    "the size of the whole data is @xmath187 with @xmath188 dimensions and @xcite state the components of these data are centered closely . in the following simulations",
    ", we adopted a gamma prior for @xmath18 , i.e.  @xmath189 for the three approaches . when considering @xmath18 as unknown we use the approach to handling uncertainty in @xmath18 described in @xcite for all algorithms .",
    "the collapsed gibbs sampler was run for a 300 iteration burn - in followed by 1000 iterations .",
    "this low number is adopted due to the size and complexity of the data ; these type of data scenarios are exactly those which motivate the development of sugs and vsugs algorithms . for the vsugs approximation ,",
    "the truncation value @xmath111 is set to be @xmath190 ( we did not find significant differences in our results when @xmath111 is increased or decreased by around 10 ) .",
    "we chose the permutation of the order of the data for vsugs and sugs as in the previous example .",
    "_ results .",
    "_ we first compared the computation time for the three method with @xmath191 data points randomly choose from the whole data set .",
    "this process is repeated for @xmath192 times and we took the average value of the time cost .",
    "the analyses through collapsed gibbs sampling were completed in approximately @xmath193 seconds while approximately @xmath194 seconds and @xmath195 seconds were required for sugs and vsugs respectively .",
    "next , we choose another data sample of @xmath196 data points .",
    "we were interested in the performance of all approaches in clustering and density estimation ( i.e. the predictive density ) .",
    "the predictive density is calculated on the remaining @xmath197 data points ; the collapsed gibbs sampler analysis was repeated @xmath198 times .",
    "the performance of predictive density estimates obtained by the three approaches are shown in table [ tab : flow ] .",
    "the collapsed gibbs sampling method has the greatest predictive ability with vsugs showing greater predictive power than sugs .",
    "this illustrates that the vsugs approximation is performing better than sugs with regard to density estimation and provides an efficient way of detecting and drawing inferences about rare populations in the presence of very large datasets .",
    "figure [ fig : cyto ] shows that sugs has difficulty approximating the data density whilst our vsugs approach better approximates the density estimates by gibbs sampling .",
    "we now turn to our original motivating snp genotyping example and examined the use of vsugs and sugs for a hierarchical bayesian clustering problem .",
    "_ for our experiments , we considered a genotyping dataset that were considered in a recent comparison study @xcite .",
    "the study consists of @xmath188 different individuals , each individual was genotyped three times using the illumina humanhap650 genotyping array which produces approximately 650,000 two - dimensional measurements per sample .",
    "we normalised the data by taking @xmath199 transforms and performed quantile normalisation between the two channels to correct for allele - specific biases .",
    "_ model . _",
    "we clustered the data using a three - class bayesian mixture model : @xmath200 where @xmath201 is the multinomial distribution , we fixed @xmath202 and the class conditional density @xmath203 is given by a dirichlet process mixture of bivariate normal distributions ( one dpm for each genotype ) .",
    "we implemented the model using both the sugs and vsugs approaches to fit the dpms .    for comparison , we classified the genotyping data using a standard genotyping tool , genosnp @xcite which models the class - conditional densities using multivariate student-@xmath5 distribution and also performs inference using variational methods .",
    "we used majority vote over the three replicates per sample to obtain the _ true _ genotypes from the genosnp genotype calls .",
    "_ results .",
    "_ over the @xmath204 samples , the average concordance of our vsugs implementation was 99.45% compared to 98.90% for the sugs implementation .",
    "figure [ fig : genotyping_example ] illustrates genotyping performance for one particular sample .",
    "figure [ fig : genotyping_example](c ) indicates that , using genotype calls from genosnp as a reference , vsugs produced the highest concordance with the genosnp results across a range of genosnp call probability thresholds . for the snps with discordant genotype calls between genosnp and sugs / vsugs , we plotted the empirical distribution of the maximum genotype call probabilities for these snps .",
    "figure [ fig : genotyping_example](d ) shows that for vsugs the discordant genotype calls were associated with snps where the maximum genotype classification probability was around 0.5 . with sugs ,",
    "discordant calls have probabilities in excess of 0.5 .",
    "in this paper we have considered vsugs as a generalization of the sugs algorithm for fast inference from dpm models .",
    "we saw that when the components of the mixture appear to be close in some sense , vsugs seems to consistently outperform sugs with regards to density estimation and this improvement is also found by using our variational lower - bound for model selection .",
    "in addition , when @xmath18 grows , we have found vsugs performs significantly better , with less computation time .",
    "we have found that for real data examples , vsugs can detect features of the data which sugs can not .    in terms of extensions to our work , we are currently considering the development of vsugs for new models .",
    "in particular , we are developing the ideas for hierarchical mixture models and infinite hidden markov models .",
    "these initial experiments suggests that vsugs can prove to be a very efficient tool for fast , but approximate , inference from a wide class of statistical models .",
    "we thank ioanna manolopoulou for providing codes and data for the flow cytometry example . the second and fourth authors acknowledge support from the moe singapore ."
  ],
  "abstract_text": [
    "<S> in this article we propose an improvement on the sequential updating and greedy search ( sugs ) algorithm @xcite for fast fitting of dirichlet process mixture models . </S>",
    "<S> the sugs algorithm provides a means for very fast approximate bayesian inference for mixture data which is particularly of use when data sets are so large that many standard markov chain monte carlo ( mcmc ) algorithms can not be applied efficiently , or take a prohibitively long time to converge . </S>",
    "<S> in particular , these ideas are used to initially interrogate the data , and to refine models such that one can potentially apply exact data analysis later on . </S>",
    "<S> sugs relies upon sequentially allocating data to clusters and proceeding with an update of the posterior on the subsequent allocations and parameters which assumes this allocation is correct . </S>",
    "<S> our modification softens this approach , by providing a probability distribution over allocations , with a similar computational cost ; this approach has an interpretation as a variational bayes procedure and hence we term it variational sugs ( vsugs ) . </S>",
    "<S> it is shown in simulated examples that vsugs can out - perform , in terms of density estimation and classification , the original sugs algorithm in many scenarios . </S>",
    "<S> in addition , we present a data analysis for flow cytometry data , and snp data via a three - class dirichlet process mixture model illustrating the apparent improvement over sugs . </S>",
    "<S> + * key - words * : approximate bayesian inference ; mixture modelling ; variational bayes ; density estimation .    * </S>",
    "<S> a sequential algorithm for fast fitting of dirichlet process mixture models *    by david j. nott@xmath0 , xiaole zhang@xmath1 , christopher yau@xmath1 & ajay jasra@xmath0    @xmath0department of statistics & applied probability , national university of singapore , singapore , 117546 , sg . </S>",
    "<S> + e-mail:`standj@nus.edu.sg , staja@nus.edu.sg ` + @xmath1department of mathematics , imperial college london , london , sw7 2az , uk . </S>",
    "<S> + e-mail:`x.zhang11@imperial.ac.uk , c.yau@imperial.ac.uk ` </S>"
  ]
}