{
  "article_text": [
    "[ sec : one ]    krylov subspace methods may be considered the method of choice for solving large and sparse systems of linear equations arising from the discretization of ( systems of ) partial differential equations on modern parallel computers .",
    "this class of algorithms are iterative in nature . at every step @xmath0 , they compute the approximate solution @xmath1 of a linear system @xmath2 from the krylov subspace of dimension  @xmath0 @xmath3 according to different criteria for each given method .",
    "the computation requires matrix - vector products with the coefficient matrix @xmath4 plus vector operations , thus potentially reducing the cumbersome costs of sparse direct solvers on large problems , especially in terms of memory .",
    "all of the iterative krylov methods converge rapidly if @xmath4 is somehow close to the identity .",
    "therefore , it is common replacing the original system @xmath2 by @xmath5 or @xmath6 for a nonsingular matrix @xmath7 .",
    "systems  ( [ eq:2 ] ) and  ( [ eq:3 ] ) are referred to as _ left _ and _ right preconditioned _",
    "systems , respectively , and @xmath8 as the _ preconditioner matrix_. in the case @xmath8 is factorized as the product of two sparse matrices , @xmath9 , like in the hermitian and positive definite case , one might solve the modified linear system @xmath10 if one may choose @xmath8 so that @xmath11 , @xmath12 or @xmath13 approximate the identity , and linear systems with @xmath8 or with @xmath14 and @xmath15 as coefficient matrices are easy to invert , it is more efficient to apply a krylov subspace method to the modified linear system .",
    "optimal analytic preconditioners based on low order discretizations , nearby equations that are simple to solve , or similar ideas have been proposed in the literature for specific problems .",
    "however , the problem - specific approach is generally sensitive to the characteristics of the underlying operator and to the details of the geometry . in this study",
    ", we pursue an algebraic approach where the preconditioner @xmath8 is computed only from the coefficient matrix @xmath4 .",
    "although not optimal for any specific problem , algebraic methods are universally applicable , they can be adapted to different operators and to changes in the geometry by tuning a few parameters , and are well suited for solving irregular problems defined on unstructured grids .    roughly speaking ,",
    "most of the existing techniques can be divided into either implicit or explicit form .",
    "a preconditioner of _ implicit _ form is defined by any nonsingular matrix @xmath7 , and requires to solve an extra linear system with @xmath8 at each step of an iterative method . the most important example in this class is represented by the incomplete @xmath16 decomposition ( ilu ) , where @xmath8 is implicitly defined as @xmath17 , @xmath18 and @xmath19 being triangular matrices that approximate the exact @xmath20 and @xmath21 factors of @xmath4 according to a prescribed dropping strategy adopted during the gaussian elimination process .",
    "these methods are considered amongst the most reliable in a general setting .",
    "well known theoretical results on the existence and the stability of the factorization can be proved for the class of @xmath8-matrices  @xcite , and recent studies are involving more general matrices , both structured and unstructured .",
    "the quality of the factorization on difficult problems can be enhanced by using several techniques such as reordering , scaling , diagonal shifting , pivoting and condition estimators  ( see e.g.  @xcite ) . as a result of this active development , in the last years",
    "successful results are reported with ilu - type preconditioners in many areas that were of exclusive domain of direct solution methods like in circuits simulation , power system networks , chemical engineering plants modelling , graphs and other problems not governed by pdes , or in areas where direct methods have been traditionally preferred , like in structural analysis , semiconductor device modelling and computational fluid dynamics applications  ( see e.g.  @xcite ) .",
    "one problem with ilu - techniques is the severe degradation of performance observed on vector , parallel and gpus machines , mainly due to the sparse triangular solves  @xcite . in some cases ,",
    "reordering techniques may help to introduce nontrivial parallelism .",
    "however , parallel orderings may sometimes degrade the convergence rate , while more fill - in diminishes the overall parallelism of the solver  @xcite .    _ explicit _ preconditioning tries to mitigate such difficulties by approximating directly @xmath22 , as the product @xmath8 of sparse matrices , so that the preconditioning operation reduces to forming one ( or more ) sparse matrix - vector product , and consequently the application of the preconditioner may be easier to parallelize and numerically stable .",
    "some methods can also perform the construction phase in parallel  @xcite ; additionally , on certain indefinite problems with large nonsymmetric parts , the explicit approach can provide better results than ilu techniques ( see e.g.  @xcite ) .",
    "in practice , however , some questions need to be addressed",
    ". the computed matrix @xmath8 could be singular , and the construction cost is typically much higher than for ilu - type methods , especially for sequential runs .",
    "the main issue is the selection of the non - zero pattern of @xmath8 .",
    "the idea is to keep @xmath8 reasonably sparse while trying to capture the ` large ' entries of the inverse , which are expected to contribute the most to the quality of the preconditioner . on general problems",
    "it is difficult to determine the best structure for @xmath8 in advance , and the computational and storage costs required to achieve the same rate of convergence of preconditioners given in implicit form may be prohibitive in practice .    in this study , we present an algebraic multilevel solver for preconditioning general nonsymmetric linear systems which attempts to combine characteristics of both approaches . assuming that the matrix @xmath4 admits the factorization @xmath23 , with @xmath20 a unit lower and @xmath21 an upper triangular matrix , our method approximates the inverse factors @xmath24 and @xmath25 .",
    "sparsity in the approximate inverse factors is maximized by employing recursive combinatorial algorithms .",
    "robustness is enhanced by combining the factorization with recently developed overlapping strategies and by using efficient local solvers .",
    "the paper is organized as follows . in section  [ sec:3 ]",
    "we describe the proposed multilevel preconditioner . in section  [ sec:4 ]",
    "we show how to combine our preconditioner with overlapping strategies , and in section  [ sec:5 ] we assess its overall performance by showing several numerical experiments on realistic matrix problems , also against other state - of - the - art solvers .",
    "finally , in section  [ sec:6 ] we conclude the study with some remarks and perspectives for future work .",
    "let @xmath26 be a @xmath27 general linear system with nonsingular , possibly indefinite and nonsymmetric matrix @xmath28 , and vectors @xmath29 .",
    "we assume that @xmath4 admits for a triangular decomposition @xmath30 and we precondition system  ( [ eq:1 ] ) as @xmath31 with @xmath32 and @xmath33 , clearly preserving symmetry and/or positive definiteness of @xmath4 .",
    "this approach of preconditioning linear systems has been extensively investigated in a series of papers by kolotilina and yeremin  @xcite , who prescribed the nonzero pattern of the inverse factors @xmath34 and @xmath35 of @xmath4 in advance equal to the pattern of the lower and upper triangular part of @xmath36 , respectively , and determined the entries of @xmath34 and @xmath35 explicitly by solving linear equations involving the principal submatrices of @xmath4 ( _ the ` fsai ' preconditioner _ ) .",
    "chow suggested to use as pattern for the inverse factors the structure of the lower and upper triangular part of @xmath37 , where @xmath38 is a positive integer  @xcite .",
    "the larger @xmath38 , in general the higher the quality of the computed preconditioner , although the construction , storage and application costs tend to increase rapidly with @xmath38 . blocking and adaptive strategies have been recently studied to overcome these problems  @xcite .",
    "benzi and t23uma proposed to compute the entries of matrices @xmath34 and @xmath35 by means of a ( two - sided ) gram - schmidt orthogonalization process with respect to the bilinear form associated with @xmath4 , and to determine the best structure for the inverse factors dynamically , during the construction ( _ the ` ainv ' preconditioner _ ) .",
    "sparsity is preserved in the process by discarding elements having magnitude smaller than a given positive threshold  @xcite .    in this study",
    "we analyse multilevel mechanisms , recursive combinatorial algorithms and overlapping techniques , combined with efficient local solvers , to enhance robustness and reduce costs for the approximation of the inverse factors .",
    "we refer to the resulting preconditioner as ames  ( algebraic multilevel explicit solver ) .",
    "it is easier to describe the ames method by using graph notation , dividing the solution of system  ( [ eq:1 ] ) in five distinct phases :    1 .   a _ scale phase _",
    ", where the coefficient matrix @xmath4 is scaled by rows and columns so that the largest entry of the scaled matrix has magnitude smaller than one ; 2",
    ".   a _ preorder phase _",
    ", where the structure of @xmath4 is used to compute a suitable ordering that maximizes sparsity in the approximate inverse factors ; 3 .   an _ analysis phase _ , where the sparsity preserving ordering is analyzed and an efficient data structure is generated for the factorization ; 4 .",
    "a _ factorization phase _ , where the nonzero entries of the preconditioner are computed ; 5 .",
    "a _ solve phase _",
    ", where all the data structures are accessed for solving the linear system .",
    "below we describe each phase separately .",
    "we initially scale system  ( [ eq:1 ] ) by rows and columns as @xmath39 where the @xmath40 diagonal scaling matrices @xmath41 and @xmath42 have the form @xmath43 for simplicity , we still refer in this paper to the scaled system  ( [ eq : linscal ] ) as @xmath2 .",
    "we use standard notation of graph theory to describe this computational step .",
    "we denote by @xmath44 the undirected graph associated with the matrix @xmath45 first , @xmath44 is partitioned into @xmath38 non - overlapping subgraphs @xmath46 of roughly equal size by using the multilevel graph partitioning algorithms available in the metis package  @xcite . for each partition @xmath46",
    "we distinguish two disjoint sets of nodes ( or vertices ) :  _ interior nodes _ that are connected only to nodes in the same partition , and _ interface nodes _ that straddle between two different partitions ; the set of interior nodes of @xmath46 form a so called _ separable _ or _ independent cluster_. upon renumbering the vertices of @xmath47 one cluster after another , followed by the interface nodes as last , and permuting @xmath4 according to this new ordering , a block bordered linear system is obtained , with coefficient matrix of the form @xmath48 in  , each diagonal block @xmath49 corresponds to the interior nodes of @xmath46 , and the blocks @xmath50 and @xmath51 correspond to the interface nodes of @xmath46 ; the block @xmath52 is associated to the mutual interactions between the interface nodes . in our multilevel scheme",
    "we apply the same block downward arrow structure to the diagonal blocks @xmath49 of @xmath53 ; the procedure is repeated recursively until a maximum number of levels is reached , or until the blocks at the last level are sufficiently small to be easily factorized . as an example , in figure  [ fig : permuted](b ) we show the structure of the sparse matrix _ rdb2048 _ from tim davis matrix collection  @xcite after three reordering levels .    to reduce factorization costs ,",
    "a similar permutation is applied to the schur complement matrix @xmath54 as follows @xmath55      in the analysis phase , a suitable data structure for storing the linear system is defined , allocated and initialized .",
    "we use a tree structure to store the block bordered form  ( [ eq : perm ] ) of @xmath53 .",
    "the root is the whole graph @xmath47 , and the leaves at each level are the independent clusters of each subgraph .",
    "each node of the tree corresponds to one partition @xmath46 of @xmath44 , or equivalently to one block @xmath49 of matrix @xmath53 .",
    "the information stored at each node are the entries of the off - diagonal blocks @xmath56 and @xmath57 of @xmath58 father , and those of the block @xmath52 of @xmath49 after its permutation , except at the last level of the tree where we store the entire block @xmath49 .",
    "all these matrices are represented in the computer memory using a compressed sparse row storage format , except for blocks @xmath51 that are stored in compressed sparse column format .",
    "blocks @xmath50 and @xmath51 can be very sparse ; many of their rows and columns can be zero , and this leads to a significant saving of computation .      the approximate inverse factors @xmath59 and @xmath60 of  @xmath53 write in the following form @xmath61 where @xmath62 and @xmath63 are the triangular factors of the schur complement matrix @xmath64    some fill - in may occur in @xmath59 and @xmath60 during the factorization , but only within the nonzero blocks .",
    "this two - level reordering scheme was used in the context of factorised approximate inverse methods for the parallelization of the ainv preconditioner in  @xcite . differently from  @xcite",
    ", we apply the arrow structure  ( [ eq : perm ] ) recursively to the diagonal blocks and to the first level schur complement as well , to gain additional sparsity .",
    "the multilevel factorization algorithm requires to invert only the last level blocks and the small schur complements at each reordering level ; the blocks @xmath65 , @xmath66 do not need to be assembled explicitly , as they may be applied using eqn  ( [ eq : borders ] ) . for the _ rdb2048 _",
    "problem , in figure  [ fig : permuted](c ) we display in red the actual extra storage required by the exact multilevel inverse factorization in addition to matrix @xmath4 ; these represent only @xmath67 of the total nonzeros of @xmath4 . from the knowledge of the red entries ,",
    "the blue ones can be retrieved from eqn  ( [ eq : borders ] ) , using the off - diagonal blocks of @xmath4 .",
    "[ fig : permuted ]    we also permute the large schur complement at the first level into a block bordered structure , until we reach a maximal number of levels or a given minimal size . the last - level matrix is inverted inexactly .",
    "an inexact solver is also used to factorize the last - level blocks @xmath49 in  ( [ eq : schurbuild ] ) .      in the solve phase ,",
    "the multilevel factorization is applied at every iteration step of a krylov method for solving the linear system .",
    "notice that the inverse factorization of  @xmath53 may be written as @xmath68 where @xmath69 , and @xmath70 are the inverse factors of the schur complement matrix @xmath71 .    from eqn .",
    "( [ eq:2by2inv ] ) , we obtain the following expression for the exact inverse @xmath72 we can derive preconditioners from eqn .",
    "( [ eq : exinv ] ) by computing approximate solvers @xmath73 for @xmath74 and @xmath75 for @xmath76 .",
    "hence the preconditioner matrix @xmath8 will have the form @xmath77 and the preconditioning operation @xmath78 = m\\left [ { \\begin{array}{*{20}c }     { x_1 }   \\\\     { x_2 }   \\\\ \\end{array } } \\right]$ ] writes as algorithm  [ alg : prec ] . notice that algorithm  [ alg : prec ] is called recursively at lines  1 - 3 , as @xmath79 and @xmath80 also have a block bordered structure upon permutation .",
    "@xmath81 @xmath82=\\tilde s^{-1}[e \\cdot p_1 , x_2]$ ] @xmath83=\\tilde b^{-1}[f \\cdot p_2 , f \\cdot p_3]$ ]",
    "in  @xcite , grigori , nataf and qu have introduced an  _ overlapping technique _ to enhance the robustness of multilevel incomplete lu factorization preconditioning computed from matrices reordered in arrow form , e.g. using the nested dissection method by george  @xcite . the multilevel mechanism incorporated in the ames preconditioner described in the previous section",
    "is based on a nested dissection - like ordering , and thus it can easily accomodate for overlapping .",
    "we have tested this idea in our numerical experiments , and in this section we shortly describe the procedure adopted . the results of our experiments are reported in section  [ sec:5 ] .",
    "let @xmath86 be the graph of @xmath4 , @xmath87 denoting the set of vertices and @xmath88 the set of edges in @xmath89 .",
    "if the graph is directed , we denote an edge of @xmath56 issuing from vertex @xmath90 to vertex @xmath91 as @xmath92 ; @xmath90 is called a predecessor of @xmath91 , and @xmath91 a successor of @xmath90 .",
    "if the graph is undirected , we denote the edges of @xmath56 by non - ordered pairs @xmath93 ; @xmath90 is called a neighbour of @xmath91 . as in the previous section",
    ", we assume that @xmath47 is partitioned into @xmath38 independent non - overlapping subgraphs @xmath94 , @xmath95 , @xmath96 , and we call @xmath76 the set of separator nodes , straddling between two different partitions .",
    "goal of overlapping is to extend each independent set of @xmath89 by including its direct neighbours , similarly to the overlapping idea used in other domain decomposition methods , for example in the restricted additive schwarz method  @xcite .",
    "following  @xcite , we denote by @xmath97 the separator nodes that are successors of @xmath98 , @xmath99 and by @xmath100 the complete set of successor nodes of all the subdomains @xmath101 then @xmath98 is extended to the set @xmath102 as @xmath103 and the separator @xmath76 is extended to @xmath104 by adding the successors of nodes in @xmath100 , that is @xmath105    using this notation , the overlapped graph of @xmath4 , @xmath106 , is introduced as follows .",
    "first define the overlapped subgraph @xmath107 and the overlapped separator @xmath80 as , respectively , @xmath108 @xmath109 for simplicity we refer to @xmath110 as @xmath111 .",
    "then the vertex set @xmath112 of the overlapped graph @xmath113 is formed by the disjoint union of the @xmath114 s and of @xmath115 as @xmath116 recall that , given the union @xmath74 of a family of sets indexed by the index set @xmath117 , @xmath118 their disjoint union @xmath52 is defined as the set @xmath119 at this stage , it is useful to introduce the two projection operators @xmath120 and @xmath121 such that @xmath122 and @xmath123 with this notation , the set of edges of the overlapped graph @xmath113 is defined according to their projection onto the original graph as follows @xmath124 @xmath125    @xmath126 @xmath127    @xmath128 @xmath129    the following property , established in  @xcite , ensures an equivalence between the equations of the overlapped system and those of the original system .",
    "let @xmath89 be the associated directed graph of a given system of linear equations and @xmath90 be a vertex of @xmath87 .",
    "let @xmath113 be the overlapped graph , and let @xmath130 be a vertex of @xmath131 such that @xmath132 .",
    "for each edge @xmath133 , there is a unique @xmath134 such that we have both @xmath135 and @xmath136 .",
    "this property shows that there exists a bijection between the nonzeros of the equation corresponding to vertex @xmath90 in the original system and the nonzeros of the equation corresponding to its dual @xmath130 , where @xmath137 . from a matrix viewpoint , to each nonzero entry @xmath138 in the overlapped matrix there is a unique nonzero entry @xmath139 in the original matrix , where @xmath137 and @xmath140 .",
    "therefore there is a one - to - one correspondence between equations in the original system and those in the overlapped system . by solving the overlapped system ,",
    "we can automatically obtain the solution of the original system .",
    "we display a simple example from  @xcite to describe shortly how the overlapping procedure works in practice .",
    "we consider a @xmath141 matrix having the structure shown in figure  [ fig : smalloverlapping](a ) .",
    "the graph consists of two independent subgraphs @xmath142 , @xmath143 and one separator @xmath144 .",
    "we just pick the first subgraph and the separator set to explain .",
    "separator nodes that are successors of @xmath145 are the set @xmath146 and we have @xmath147 so that @xmath148 analogously , @xmath149 and @xmath150 next , we compute the overlapped separator set @xmath80 .",
    "the vertices of @xmath151 and @xmath152 directed by @xmath100 are @xmath153 , so @xmath154 and @xmath155    according to eqns .",
    "( [ edgei])-([edgesi ] ) , the edges of the overlapped subdomain @xmath156 are defined based on their projection onto the original graph . the first diagonal block of the overlapped matrix is formed by picking the @xmath157 rows and columns of the original matrix @xmath158 similarly for the other two diagonal blocks , and this is shown in figure  [ fig : smalloverlapping](b ) .    from  eqn .",
    "( [ edgeis ] ) , we can construct the edges from @xmath159 to @xmath160 .",
    "these are the nonzero entries of the overlapped interface block @xmath161 , adopting the same notation as in ( [ eq : perm ] ) .",
    "we pick the @xmath162 rows and @xmath163 columns of the original matrix , and we set the columns corresponding to the common vertexes of @xmath164 and @xmath165 to zeros . in our example",
    "this results in zeroing out the columns of @xmath166 indexed by @xmath167 , giving @xmath168 similar procedure is followed for the other blocks @xmath169 , @xmath170 .",
    "finally , the overlapped matrix has the form given in figure  [ fig : smalloverlapping](b ) .",
    "the block arrow structure of the original matrix is preserved .",
    "however , symmetry is lost and the sparsity pattern also changes significantly .",
    "@xmath171\\ ] ]    @xmath172\\ ] ]      it is interesting to analyse the effect that overlapping may produce on the ames method . according to ( [ hati ] ) and ( [ edgei ] ) , the size and the number of nonzeros in each subgraph is increased after overlapping . according to ( [ edgeis ] )",
    ", the interconnections between subdomains and separator are reduced in the overlapped system , as the original interconnectivities are all removed .",
    "the more nodes are added to the subgraphs , the richer they are in terms of information about the system matrix , and a larger performance improvement may be expected . in the overlapped system , each block @xmath173 has the following structure    @xmath174    from eqn .",
    "( [ iext ] ) we see that the set of neighboring nodes @xmath97 corresponds to the nonzero columns of the block @xmath51 , and the nonzero elements of @xmath51 are determined by the set of interconnections @xmath175 .",
    "the more dense and larger the blocks @xmath176 ( that is , the size of separator ) in the original matrix , the more nodes and interconnections are added to subdomains , and a larger reduction of the number of iterations can be achieved .",
    "the ames preconditioning algorithm described in section  [ sec:3 ] with one extra overlapping phase writes as follows :    1 .   a _ scale phase _ , where the matrix @xmath4 is scaled by rows and columns so that the largest entry of the scaled matrix has magnitude smaller than one ; 2 .",
    "a _ preorder phase _",
    ", where the structure of @xmath4 is used to compute a suitable ordering that can maximize sparsity in the approximate inverse factors ; 3 .   an _ overlap phase _ , which extends each block @xmath49 and the schur complement , and generates the overlapped matrix @xmath177 and the right - hand side vector @xmath178 ; 4 .",
    "an _ analysis phase _",
    ", where the sparsity preserving and overlapping orderings are analyzed and an efficient data structure is generated for the factorization ; 5 .   a _ factorization phase _ , where the entries of @xmath177 are processed to explicitly compute the approximate inverse factors ; 6 .   a _ solve phase _",
    ", that accesses all the data structures for solving the overlapped linear system .",
    "a _ restriction phase _ , that restricts the solution obtained from the overlapped system to the original system , and obtains the solution .",
    "in this section we present the results of our numerical experiments to illustrate the performance of the ames preconditioner , also against other state - of - the - art methods and software for solving general linear systems .",
    "the selected matrix problems are extracted from the public - domain matrix repository available at the university of florida  @xcite , and arise from various application fields .",
    "we present a summary of the characteristics of each linear system in table  [ tab : problems ] .",
    ".[tab : problems ] set and characteristics of the test matrix problems . [ cols=\"<,^,<,>\",options=\"header \" , ]      +",
    "in this paper a recursive multilevel implementation of factorized sparse approximate inverse preconditioners for krylov subspace methods was discussed .",
    "we used recursive combinatorial techniques and overlapping strategies as an attempt to remedy two typical drawbacks of explicit preconditioning , that are lack of robustness and high construction cost .",
    "the numerical experiments show that these strategies can improve the performance of conventional approximate inverse methods , yielding iterative solutions that can compete favourably against other popular solvers in use today .",
    "parallelism can be exploited at various levels in our method , alongside other code optimization .",
    "fine - grained blocking , filtering , postfiltering , adaptive pattern selection strategies have been shown to be promising approaches in other contexts  @xcite , and these can be considered also in our setting . in a distributed memory implementation , it will be natural to split the oct - tree by assigning the local problems to different processors .",
    "an efficient use of recursive combinatorial algorithms may reduce considerably the size of the schur complements , hence the amount of inter - node communications .",
    "memory demands , an important bottleneck of modern algorithms , are also limited , but this does not penalize much the overall numerical efficiency of the solver , as illustrated by the experiments of tables  [ tab : result1 ] and  [ tab : direct ] . overlapping does not destroy the sparsity structure of the matrix and can reduce further the interconnections between subdomains and separator set . hence it is worthwhile considering it in a parallel setting as well .",
    "however , the parallel implementation of a fully distributed schur complement formulation may not be trivial and will be considered in a separate study .",
    "the work of the first and of the third authors is funded by the ubbo emmius scholarship of the university of groningen .",
    "the work of the last author is supported by nsfc ( 61170309 ) , the fundamental research funds for the central universities ( zygx2013z005 ) .",
    "we are grateful to miroslav tma for motivating discussion and for supplying the ainv package for our numerical experiments .",
    "finally , we would like to thank the anonymous reviewers for their valuable and thorough comments that we believe helped improve significantly the presentation and the quality of the paper .",
    "m.  benzi , j.  marn , m.  m. t23uma , a two - level parallel preconditioner based on sparse approximate inverses , in : d.  kincaid , a.  elster ( eds . ) , iterative methods in scientific computation iv , imacs series , 1999 .",
    "b.  carpentieri , m.  bollhfer , symmetric inverse - based multilevel ilu preconditioning for solving dense complex non - hermitian systems in electromagnetics , progress in electromagnetics research 128 ( 2012 ) 5574 .",
    "b.  carpentieri , i.  duff , l.  giraud , g.  sylvand , combining fast multipole techniques and an approximate inverse preconditioner for large electromagnetism calculations , siam j. scientific computing 27  ( 3 ) ( 2005 ) 774792 .",
    "b.  carpentieri , j.  liao , m.  sosonkina , vbarms : a variable block algebraic recursive multilevel solver for sparse linear systems , journal of computational and applied mathematics 259 ( a ) ( 2014 ) 164173 .",
    "l.  grigori , f.  nataf , l.  qu , overlapping for preconditioners based on incomplete factorizations and nested arrow form , numerical linear algebra with applications 22  ( 1 ) ( 2015 ) 4875 .",
    "http://dx.doi.org/10.1002/nla.1937          t.  huckle , m.  sedlacek , smoothing and regularization with modified sparse approximate inverses , journal of electrical and computer engineering 2010 ( 2010 ) 116 .",
    "l.  y. kolotilina , a.  y. yeremin , a.  nikishin , factorized sparse approximate inverse preconditionings .",
    "iv : simple approaches to rising efficiency . , numerical linear algebra with applications 6 ( 1999 ) 515531 .",
    "l.  y. kolotilina , a.  y. yeremin , a.  nikishin , factorized sparse approximate inverse preconditionings .",
    "iii : iterative construction of preconditioners . ,",
    "journal of mathematical sciences 101 ( 2000 ) 32373254 , originally published in russian in _ zap .  nauchn .",
    "pomi _ , 248:17 - 48 , 1998 ."
  ],
  "abstract_text": [
    "<S> in this paper we introduce an algebraic recursive multilevel incomplete factorization preconditioner , based on a distributed schur complement formulation , for solving general linear systems . </S>",
    "<S> the novelty of the proposed method is to combine factorization techniques of both implicit and explicit type , recursive combinatorial algorithms , multilevel mechanisms and overlapping strategies to maximize sparsity in the inverse factors and consequently reduce the factorization costs . </S>",
    "<S> numerical experiments demonstrate the good potential of the proposed solver to precondition effectively general linear systems , also against other state - of - the - art iterative solvers of both implicit and explicit form .    </S>",
    "<S> * keywords : * linear systems ; iterative solvers ; preconditioners ; sparse approximate inverse methods ; multilevel reordering algorithms . </S>"
  ]
}