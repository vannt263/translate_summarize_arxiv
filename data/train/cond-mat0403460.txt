{
  "article_text": [
    "during recent years the blume - emery - griffiths ( beg ) model @xcite has been studied quite intensively in the context of neural networks , one of the reasons being that it was argued in @xcite that this model maximizes the mutual information content of three - state networks with hebbian - type learning rules . to know in more detail how the retrieval quality of the beg network compares with other three - state neuron models ,",
    "the thermodynamics of this model was studied and temperature - capacity phase diagrams were obtained @xcite . it was shown that the retrieval phase is systematically larger than that of other three - state models and that the critical capacity is about twice as large as that of the three - state neuron ising model @xcite .",
    "also the region of thermodynamic stability is much larger and , furthermore , the phase diagram itself is much richer with the presence of a stable quadrupolar state , carrying also retrieval information , at high temperatures .    it was also shown that this enhancement of the retrieval properties is not restricted to the use of the hebbian learning rule but that it is inherent to the model",
    "indeed , by studying the gardner optimal capacity @xcite in replica symmetric ( rs ) mean - field theory it was found recently @xcite that for the corresponding beg perceptron with , e.g. , zero embedding stability parameter and uniform patterns this capacity is @xmath0 .",
    "comparing with other three - state neuron perceptron models , we recall that for the @xmath1 ising perceptron the gardner optimal capacity can maximally reach @xmath2 @xcite , whereas for the @xmath1 clock and potts model both reach an optimal capacity of @xmath3 @xcite . at this point",
    "we have to remark that the @xmath1 ising perceptron and the beg perceptron have the same topology structure in the neurons , whereas the @xmath1 clock and potts models have different topologies . for the ising topology structure",
    "the beg - perceptron has the best performance .",
    "the interesting question remains whether and in how far these enhanced retrieval properties are robust against dilution .",
    "studying this question is the aim of the present work . besides the fact that the connectivity of biological networks is far from complete , diluted networks offer the possibility to study the robustness against malfunctioning of some of the connections .",
    "furthermore , in asymmetric architectures they reduce the internal feedback correlations of fully connected networks making a complete analytic description of the dynamics much easier @xcite , @xcite .",
    "finally , in the beg perceptron there are two sets of couplings , those referring to the three - state patterns themselves and those related to the active , i.e. , the non - zero patterns . by diluting both types of couplings simultaneously or diluting these couplings independently ,",
    "we can study , in particular , the influence of the active patterns on the gardner optimal capacity of the beg perceptron .",
    "these results can be obtained in closed analytic form .",
    "we remark that the type of dilution we study in this paper is such that the number of connections to a given site still increases with the size of the system . in the replica approach to capacity problems for these systems ,",
    "only order parameters with two replica indices appear .",
    "recently , the study of neural networks with finite connectivity , i.e. , where the number of connections to a given site remains finite in the thermodynamic limit has been started @xcite . there",
    ", functional order parameters have to be introduced .",
    "the paper is organized as follows . in sect .",
    "[ sec : definition ] we recall the beg model and briefly discuss some of its properties . in sect .",
    "[ sec : gardner ] we introduce the different kinds of dilution that one may study and report on the application of the gardner approach to these cases .",
    "we present the results for the optimal capacity in the rs approximation as a function of the pattern activity , the stability parameter and the degree of dilution . in sect .",
    "[ sec : distribution ] we discuss the results for the distribution of the couplings and in sect .  [ sec : stability ] we study the validity of the local stability criterion for the rs solution .",
    "the last section contains the conclusions .",
    "let us consider a neural network consisting of @xmath4 neurons which can take values @xmath5 from the discrete set @xmath6 .",
    "the @xmath7 patterns to be stored in this network are supposed to be a collection of independent and identically distributed random variables ( i.i.d.r.v . ) , @xmath8 , @xmath9 , taken from the set @xmath10 with a probability distribution @xmath11 with @xmath12 the activity of the patterns so that @xmath13 given the network configuration at time @xmath14 , @xmath15 , the following dynamics is considered .",
    "the configuration @xmath16 is chosen as input .",
    "the neurons are updated according to the stochastic parallel spin - flip dynamics defined by the transition probabilities @xmath17 }          { \\sum_{s \\in \\mathcal{s } } \\exp [ - \\beta \\epsilon_i                                     ( s|{\\mbox{\\boldmath $ { \\sigma}$}}_n(t))]}\\ , .",
    "\\label{eq : trans}\\end{aligned}\\ ] ] here the energy potential @xmath18 $ ] is defined by @xmath19 =            -sh_i({{\\mbox{\\boldmath $ { \\sigma}$}}}_n(t))-s^2\\theta_i({{\\mbox{\\boldmath $ { \\sigma}$}}}_n(t ) )                \\ , , \\label{eq : energy}\\ ] ] where the local fields in neuron @xmath20 , @xmath21 carry all the information @xmath22 at zero temperature the updating rule of this dynamics ( [ eq : trans])-([eq : energy ] ) is equivalent to the gain function formulation @xmath23 with @xmath24 and @xmath25 the heaviside and the sign function , respectively .    concerning the loading capacity of this model",
    ", the following results have appeared in the literature . for hebbian - type synaptic couplings @xmath26 and @xmath27 @xmath28 [ ( \\xi^\\mu_j)^2 -a ]       \\label{hamcoupk}\\end{aligned}\\ ] ]",
    "the long - time behavior is governed by the hamiltonian @xmath29 and the retrieval properties are enhanced @xcite in comparison to other three - state neuron models . in particular , the retrieval phase is systematically larger than that of other three - state models and the critical capacity is about twice as large as that of the three - state neuron ising model .",
    "moreover , depending on the value of the pattern activity a stable quadrupolar state carrying also non - zero retrieval information arises at high temperatures .",
    "however , an underlying reason why there is such an enlargement of the basin of attraction and hence of the retrieval properties of the network seems still to be absent .",
    "this enhancement of retrieval has also been found @xcite for the beg - perceptron @xmath30 with @xmath31 denoting the output , and where @xmath32 and @xmath33 are the local fields at the output created by the pattern @xmath34 @xmath35 with @xmath36 a set of couplings connecting the input with the output . in a rs analysis the gardner optimal capacity for this perceptron",
    "is calculated analytically and seen to be bigger than that of the @xmath1-ising perceptron  @xcite .",
    "we want to find out in how far these enhanced retrieval properties are robust against dilution .",
    "one of the questions we want to answer then is the following .",
    "let @xmath37 be an extensive set of @xmath38 patterns supposed to be fixed points of the dynamical rule where the local fields @xmath32 and @xmath33 are now given by @xmath39 the parameters @xmath40 and @xmath41 control the presence of the connections @xmath42 and @xmath43 .",
    "we want to find a set of couplings , @xmath44 , or equivalently , a beg - perceptron with an average dilution @xmath45 and @xmath46 @xmath47 that still fulfil the conditions .",
    "it is clear that for small values of the capacity @xmath48 more than one beg - perceptron storing these patterns can be found .",
    "the bigger the value of @xmath48 the more difficult this task becomes and a saturation limit , called gardner optimal capacity , is reached .    in the following we study dilution during learning , i.e. , annealed dilution , which can be realized in two different ways .",
    "the first one , called synchronous dilution , assumes that @xmath49 and , hence @xmath50 ; the second one , named asynchronous dilution , allows the @xmath51 s to be different .",
    "looking back at we see that the @xmath52 control the connections of the three - state patterns , while the @xmath53 control the connections related to the active , i.e. , the non - zero patterns .",
    "in fact , for hebbian learning in - the @xmath54-couplings control the fluctuations around these active patterns .",
    "therefore , by allowing synchronous or asynchronous dilution we can study the influence of the active patterns on the optimal capacity of the beg perceptron .",
    "to study the optimal capacity , we follow the entropy approach introduced by gardner @xcite .",
    "since the dynamical variables are continuous , entropy has only meaning relatively and we write the volume @xmath55 of all possible beg - perceptrons satisfying , without normalizing , as @xmath56 with @xmath57 the characteristic function given by @xmath58\\theta(-|h^\\mu|-\\theta^\\mu-\\kappa ) \\label{charateristic_function}\\end{aligned}\\ ] ] where @xmath59 is the embedding stability parameter .",
    "since we consider continuous couplings we need to introduce a modified spherical constraint @xmath60 from this spherical constraint we see that the couplings are not well normalized at those sites where @xmath51 is zero .",
    "one can solve this difficulty either by introducing an extra spherical constraint for the remaining couplings @xcite , either by restricting the trace over the couplings @xcite .",
    "we take the second solution and define the restricted trace as @xmath61 since we want to study typical features of the system the important quantity to average over is the entropy . employing replica techniques",
    "@xcite we express the entropy per neuron as @xmath62 where @xmath63 denotes the average over the pattern distribution and where @xmath64 is the @xmath65-th times replicated volume of solutions    @xmath66    \\big[\\prod_{\\alpha=1}^n\\delta\\big(\\sum_{i=1}^n             c^\\alpha_{i}(j^\\alpha_i)^2-cn\\big )            \\delta\\big(\\sum_{i=1}^n c^\\alpha_{i}(k^\\alpha_i)^2-cn\\big )           \\delta\\big(\\sum_{i=1}^n c^\\alpha_{i}-cn\\big)\\big ]         \\prod_{\\mu=1}^p\\prod_{\\alpha=1}^n               \\chi_{\\xi^\\mu_0}(h^\\mu_\\alpha,\\theta^\\mu_\\alpha;\\kappa ) .",
    "\\label{power_volume}\\ ] ]    the further analysis then proceeds in a standard way although the technical details are much more involved .",
    "a short account is given in appendix  a.    the results are described essentially in terms of three order parameters , the first one , @xmath67 , defined as the overlaps between two distinct replicas for the couplings @xmath42 , the second one , @xmath68 , a similar quantity for the couplings @xmath43 and the third one , @xmath69 , arising from the fact that the dynamics and , hence , also the characteristic function contains a second field @xmath70 , quadratic in the patterns ( see ) . in the rs approximation we are discussing here they are given by @xmath71 .",
    "the rs gardner optimal capacity is obtained when the overlap order parameters @xmath72 and @xmath73 go to @xmath74 .",
    "it is clear that these limits have to be taken simultaneously but , in general , their rate of convergence could be different .",
    "therefore , we introduce @xmath75 where @xmath76 is a new parameter which one also needs to extremize .",
    "we expect this parameter @xmath76 to depend on the pattern distribution through the activity @xmath12 .",
    "the result for the replica symmetric gardner optimal capacity @xmath77 then reads @xmath78 where the function @xmath79 is defined by @xmath80 stationarity with respect to @xmath81 then leads to @xmath82 . here",
    "the functions @xmath83 , @xmath84 are given by @xmath85 }     { \\big(\\cos^2\\varphi+\\gamma\\sin^2\\varphi\\big)^m}.\\ ] ]    the function @xmath86 in can be expressed as    @xmath87    with @xmath88 and @xmath89\\ , dx$ ] .",
    "the integration regions are the following ones @xmath90 and the corresponding integrands are given by @xmath91    with @xmath92 .    after inserting - in and extremizing",
    "numerically we find the results presented in fig .",
    "we plot the optimal capacity @xmath93 itself ( insets ) and its values normalized by the optimal capacity for no dilution , @xmath94 , as a function of the dilution @xmath95 for @xmath96 and several values of the activity @xmath12 .     for @xmath96 and several values of @xmath12 .",
    "top figure : the normalized optimal capacity ( solid line ) , its upper bound ( dashed - dotted line ) and its lower bound ( broken line ) ; the inset displays @xmath93 for @xmath97 from top to bottom .",
    "bottom figure : similar to the top figure for @xmath98 .,title=\"fig:\",scaledwidth=30.0% ] +   for @xmath96 and several values of @xmath12 .",
    "top figure : the normalized optimal capacity ( solid line ) , its upper bound ( dashed - dotted line ) and its lower bound ( broken line ) ; the inset displays @xmath93 for @xmath97 from top to bottom .",
    "bottom figure : similar to the top figure for @xmath98 .,title=\"fig:\",scaledwidth=30.0% ]    we see that different regions of activities lead to different results . for small activities @xmath99 and hence , many inactive neurons , the optimal capacity strongly increases for decreasing dilution .",
    "this seems to be in agreement with what is known in the literature for very small activities or so - called sparse coding ( see , e.g. , @xcite , @xcite , @xcite ) . when normalizing these results by @xmath100 we find that all the lines collapse into the full line . for large activities @xmath101 and , hence , many active states @xmath102 , the results for @xmath93 are only weakly dependent on the activity ( see inset ) but the results for the normalized optimal capacity do not collapse .",
    "furthermore , we see that the network is more robust against synchronous dilution for non - sparse coding , i.e. , for activities ranging in the interval @xmath103 $ ] : the dilution @xmath95 can decrease from @xmath74 to about @xmath104 before one sees a substantial decrease in the optimal capacity . comparing to the @xmath1 ising perceptron @xcite ,",
    "the effect of dilution , especially for larger activities is about the same .",
    "when @xmath105 ( @xmath106 ) , the functions @xmath107 can be explicitly integrated leading to @xmath108 and one recovers the optimal capacity found in the fully connected case @xcite .",
    "when the pattern activity @xmath12 goes to @xmath74 the system is forced into two possible states , as in the gardner model with dilution @xcite .",
    "since the overlap parameter @xmath73 becomes irrelevant in such a limit @xmath76 must go to infinity .",
    "the numerical solution does confirm this .",
    "furthermore , in this limit the functions @xmath107 become @xmath109 and hence @xmath110 these are precisely the gardner results with dilution @xcite after rescaling @xmath111 .",
    "we remark that in this case , and also for the @xmath112-ising type models @xcite , it is possible to rescale the optimal capacity as follows @xmath113 with @xmath114 . for the general beg - perceptron treated here such",
    "a scaling is not possible because the factor @xmath76 appears both in the numerator and denominator of eq .. it is possible , however , to derive the bound @xmath115 for @xmath116 with @xmath95 and @xmath81 related through @xmath117 .",
    "these bounds are shown in fig .  1 as the broken line ( lower bound ) and the dashed - dotted line ( upper bound ) .",
    "although the dependence on the dilution and other parameters is not that simple , we do find that the dependence on the embedding stability parameter @xmath59 is rather weak .",
    "in this case @xmath45 is different from @xmath46 allowing us to study the relative influence of the two sets of couplings .",
    "an analogous calculation as the one in subsection a can be done leading to the following result for the optimal capacity     for @xmath118 , @xmath119 and activity @xmath120 from top to bottom .",
    "the dashed - dotted line is for @xmath121 .",
    "bottom figure : the normalized capacity as a function of @xmath46 for @xmath122 , @xmath119 and pattern activity @xmath123 from bottom to top.,title=\"fig:\",scaledwidth=30.0% ] +   for @xmath118 , @xmath119 and activity @xmath120 from top to bottom .",
    "the dashed - dotted line is for @xmath121 .",
    "bottom figure : the normalized capacity as a function of @xmath46 for @xmath122 , @xmath119 and pattern activity @xmath123 from bottom to top.,title=\"fig:\",scaledwidth=30.0% ]    @xmath124    with @xmath125 to simplify notation we denote both @xmath126 and @xmath127 as @xmath81 in the sequel since there should be no confusion possible .",
    "stationarity with respect to @xmath81 leads to @xmath114 .",
    "we remark that when we take the dilution averages to be equal , i.e. , @xmath128 the dependence on the dilution in factorizes and we simply get an expression equivalent to @xmath129 for any value of the pattern activity @xmath12 and stability constant @xmath59 .    in order to understand the role of the different couplings in the learning process",
    ", we cut them independently and study the influence with varying activity .",
    "the results are presented in fig .  2 .",
    "we plot the optimal capacity normalized by its value for no dilution , @xmath94 as a function of the dilution of the @xmath130-couplings , @xmath45 , for @xmath118 , @xmath96 and several values of the activity @xmath12 ( top ) and , analogously ( bottom ) as a function of the dilution of the @xmath54-couplings .",
    "we find that when diluting the @xmath130-couplings , referring to the ternary patterns , and keeping all the @xmath54-couplings , related to the active patterns , the normalized capacity decreases as a function of the activity obtaining the gardner result for @xmath121 .",
    "when doing the reverse , the normalized capacity increases as a function of the activity .",
    "moreover , the network is more robust against @xmath54-dilution , especially for large activities .",
    "this seems to be quite natural since large activities means many active states @xmath102 such that cutting active patterns becomes relatively less important .",
    "we study the distribution of couplings @xmath131 inside @xmath55 in analogy with @xcite .",
    "this probability distribution can be splitted into two parts , the first one involving the @xmath132 inactive couplings and the second one , @xmath133 , representing the remaining @xmath134 active couplings . obviously , the first set of couplings is delta distributed so that we can write @xmath135 where the second set of couplings satisfies    @xmath136    in order to compute @xmath133 we follow @xcite by introducing replicas allowing us to lift the volume @xmath55 to the numerator .",
    "the calculations are standard but tedious . evaluating the expression within the rs approximation we get for synchronous dilution    @xmath137 \\theta\\left(\\frac{\\gamma a_{\\text{syn}}(u,\\gamma;c)}{c(1+\\gamma ) } \\big(j^2+\\frac{k^2}{\\gamma}\\big)-u^2\\right ) \\ , .",
    "\\label{distribution_couplings_syn}\\ ] ]    this distribution is a two - dimensional gaussian from which the middle section has been cut out , as represented by the heaviside function .",
    "this gap has an ellipsoidal shape because of the scaling factor @xmath138 accompanying the @xmath139 in the argument .",
    "it increases with increasing dilution to reach its maximum when @xmath95 tends to zero . in the limit",
    "@xmath140 this distribution reduces to    @xmath141 \\theta\\left(\\frac { a_{\\text{syn}}(u,\\infty;c)}{c}j^2-u^2\\right)\\,.\\end{aligned}\\ ] ]    we remark that this distribution is different from the one obtained in the gardner case ( i.e. , the @xmath142 limit ) because , although the @xmath54 couplings do not play any role for @xmath140 the spherical constraint is still present , no matter what the value of @xmath12 is .",
    "it is interesting to determine how this probability distribution behaves in the case of no dilution .",
    "then , the distribution for the couplings becomes gaussian without a gap , viz .",
    "@xmath143 this result is intuitively meaningful since the couplings are forced to obey only the spherical constraint without any restriction coming from the dilution variable .",
    "therefore , we find back the probability distribution for the couplings of the fully connected beg - perceptron .    for asynchronous dilution a similar treatment can be pursued and we find that the probability distribution for the couplings factorizes    @xmath144    these distributions are of a similar nature as the one for the standard diluted perceptron case @xcite .",
    "finally , we are interested in studying the local stability of the obtained solutions against rs fluctuations following @xcite . from the work on the non - diluted beg - perceptron @xcite we recall that in that case the solutions are unstable only for small activities and very small embedding constants @xmath59 .",
    "furthermore , we know that , in general , there are four transverse eigenvalues . in the case of asynchronous dilution",
    "these eigenvalues are given by the roots of the fourth degree characteristic polynomial @xmath145 where @xmath146 and @xmath147 read @xmath148_\\dag(x , e,\\widehat{q},\\psi_j)\\big]^2\\label{eq : stability1}\\\\ \\delta_{\\widehat{r}}&= & \\int d(x)\\big[\\frac{1}{\\widehat{r}}\\frac{\\partial^2}{\\partial x^2 } \\log[1]_\\dag(x , f,\\widehat{r},\\psi_k)\\big]^2\\label{eq : stability2 } \\label{b}\\end{aligned}\\ ] ] with @xmath149 , @xmath150 , @xmath151 and @xmath152 the conjugate variables appearing in the integral representations of the constraints , @xmath153 and @xmath154 the conjugate variables of the order parameters @xmath72 and @xmath73 , and with the short - hand notation @xmath155_\\dag(x , a , b , d)&=&1+\\sqrt{\\frac{2\\pi}{a - b } } \\exp\\big[-\\frac{bx^2}{2(a - b)}-\\frac{d}{2}\\big]\\ , .",
    "\\nonumber\\end{aligned}\\ ] ] similar expressions can be written down for @xmath156 , @xmath157 and @xmath158 but they are not needed for the argumentation .",
    "indeed , it is straightforward to check that as soon as dilution is allowed the solution becomes unstable in the saturation limit @xmath159 .",
    "the first derivative of @xmath160_\\dag(x , a , b , d)$ ] has a jump at @xmath161 proportional to @xmath81 , leading to a dirac delta contribution in the second derivative .",
    "the square in and forces the replicon eigenvalue to go to @xmath162 , similarly to what happens for the standard perceptron model as explained in @xcite .",
    "when @xmath106 , i.e. in the absence of dilution , there is no such delta contribution and we find back the results of @xcite .",
    "the same reasoning holds for synchronous dilution .",
    "in this work we have studied annealed dilution in the beg perceptron model . two types of dilution have been discussed , the first one being synchronous dilution , i.e. , simultaneous dilution of some of the couplings referring to the ternary patterns themselves and some of the couplings related to the active patterns , the second one being dilution of both these types of couplings independently , so - called asynchronous dilution .",
    "we have obtained an analytic formula for the replica symmetric gardner optimal capacity . for synchronous dilution",
    "we see that different regions of activities lead to different results . for small activities @xmath99",
    "the optimal capacity strongly increases for decreasing dilution but normalizing these results by its value for no dilution , the lines for different activities collapse . for large activities",
    "@xmath101 the optimal storage capacity is only weakly dependent on the activity but the results for the normalized optimal capacity do not collapse .",
    "furthermore , we see that the network is robust against synchronous dilution for non - sparse coding , i.e. , for activities ranging in the interval @xmath103 $ ] . for asynchronous dilution",
    "we find that diluting only the @xmath130-couplings , the normalized optimal capacity decreases as a function of the activity obtaining the gardner result for @xmath121 . when diluting the @xmath54-couplings , the normalized optimal capacity increases as a function of the activity .",
    "moreover , the network is more robust against @xmath54-dilution , especially for large activities .",
    "since the effects of dilution are of the same order as those in the @xmath1-ising model , these results also confirm the better retrieval properties found before for the beg model .",
    "we have studied the stability of the rs solution against rs breaking fluctuations by generalizing the de almeida - thouless analysis .",
    "we find that as soon as there is dilution the results are unstable .",
    "we are indebted to jort van mourik and nikos skantzos for critical and informative discussions .",
    "this work has been supported by the fund of scientific research , flanders - belgium .",
    "in this appendix we outline the main steps in the calculation of the entropy per neuron - .    after defining the order parameters @xmath163 introducing the conjugate order parameters @xmath164 , and enforcing the constraints and using the lagrange multipliers @xmath165 , @xmath166 and",
    "@xmath167 , we write @xmath168 as the following integral    @xmath169 \\big[\\prod_{\\alpha=1}^n\\frac{d l^{\\alpha}d\\widehat{l}^{\\alpha } }                      { 2\\pi /\\sqrt{cn}}\\frac{de^\\alpha}{4\\pi i }       \\frac{df^\\alpha}{4\\pi i}\\frac{d\\widehat{\\psi}^\\alpha}{4\\pi i}\\big ]      \\exp[{n(g_1+g_2+g_3)}]\\end{aligned}\\ ] ]    where we have defined the functions @xmath170 \\big[\\prod_{\\alpha=1}^n\\frac{d\\theta^\\alpha   d\\widehat{\\theta}^\\alpha}{2\\pi}\\big ] \\exp\\big[[i\\sum_{\\alpha=1}^n\\big ( h^\\alpha \\widehat{h}^\\alpha + \\theta^\\alpha \\widehat{\\theta}^\\alpha\\big ) -ia \\sum_{\\alpha=1}^n\\widehat{\\theta}^\\alpha l^\\alpha\\nonumber\\\\ & & -\\frac{a}{2 } \\sum_{\\alpha,\\beta=1}^n \\widehat{h}^\\alpha\\widehat{h}^\\beta q_{\\alpha\\beta } -\\frac{a(1-a)}{2 }   \\sum_{\\alpha,\\beta=1}^n\\widehat{\\theta}^\\alpha\\widehat{\\theta}^\\beta   r_{\\alpha\\beta}\\big\\}\\big]{\\big<\\hspace{-1.5mm}\\big<}\\prod_{\\alpha=1}^n \\chi_{\\xi_0}(h_\\alpha,\\theta_\\alpha;\\kappa){\\big>\\hspace{-1.5mm}\\big>}_{\\xi_0}\\\\ g_2&=&\\log\\prod_{\\alpha=1}^n\\text{tr}_{\\{c^\\alpha , j^\\alpha , k^\\alpha\\ } } \\exp\\big[-\\sum_{\\alpha<\\beta}\\widehat{q}_{\\alpha\\beta }   c^\\alpha j^{\\alpha}c^\\beta j^{\\beta}-    \\sum_{\\alpha<\\beta}\\widehat{r}_{\\alpha\\beta }   c^\\alpha k^{\\alpha}c^\\beta k^{\\beta}\\nonumber\\\\ & & -\\frac{1}{2}\\sum_{\\alpha=1}^n e^\\alpha c^\\alpha(j^\\alpha)^2 -\\frac{1}{2}\\sum_{\\alpha=1}^n f^\\alpha c^\\alpha(k^\\alpha)^2 -\\frac{1}{2}\\sum_{\\alpha=1}^n \\widehat{\\psi}^\\alpha c^\\alpha\\big]\\\\ g_{3}&=&c\\sum_{\\alpha<\\beta}\\widehat{q}_{\\alpha\\beta}q_{\\alpha\\beta } + c\\sum_{\\alpha<\\beta}\\widehat{r}_{\\alpha\\beta}r_{\\alpha\\beta } + \\frac{c}{2}\\sum_{\\alpha=1}^n\\big[e^\\alpha+f^\\alpha   + \\widehat{\\psi}^\\alpha\\big]\\,.\\end{aligned}\\ ] ]    we have already used that @xmath171 , @xmath172 at the saddle - point . in the thermodynamic limit @xmath173 the entropy",
    "is evaluated at the saddle - point for the order parameters , the conjugate ones and the lagrange multipliers @xmath165 , @xmath166 and @xmath167 . using the rs ansatz for the order parameters",
    "@xmath174 the functions @xmath175 can be simplified further and the entropy can be written as    @xmath176 + \\int_{-\\infty}^\\infty{\\cal d}(x , y)\\log[1]_\\dag(x , y ) + \\alpha\\int_{-\\infty}^\\infty{\\cal d}(h_0,\\theta_0-l ) { \\big<\\hspace{-1mm}\\big<}\\log [ 1]^{\\xi}_{\\star}(h_0,\\theta_0 ) { \\big>\\hspace{-1mm}\\big>}\\label{entropy_rs}\\ ] ]    with the short - hand notations @xmath177_\\dag(x , y)&=&1+\\frac{2\\pi}{\\sqrt{(e-\\widehat{q})(f-\\widehat{r } ) } } \\exp\\left[{-\\frac{x^2\\widehat{q}}{2(e-\\widehat{q } ) } -\\frac{y^2\\widehat{r}}{2(f-\\widehat{r})}-\\frac{\\psi}{2}}\\right ]        \\label{function1}\\\\ ~[1]^{\\xi}_{\\star}(h_0,\\theta_0)&=&\\int_{\\omega_{\\xi } }     \\frac{dh}{\\sqrt{2\\pi(1-q ) } } \\frac{d\\theta}{\\sqrt{2\\pi ( 1-r ) } }    \\exp\\left[{-\\frac{(h - h_0)^2}{2(1-q ) }      -\\frac{(\\theta-\\theta_0)^2}{2(1-r)}}\\right]\\label{function2}\\end{aligned}\\ ] ]      at this point we have two choices to proceed .",
    "either we solve numerically the saddle - point equations or we do an asymptotic expansion in the limit @xmath181 in the entropy ( or equivalently in the saddle - point equations for the parameters ) .",
    "the first approach has the advantage that we can study @xmath48 as a function of @xmath182 .",
    "but , since we are only interested in the optimal capacity , we opt for the asymptotic expansion . since",
    "the limits @xmath183 and @xmath184 must be taken simultaneously , we introduce a factor @xmath76 such that @xmath75 .",
    "then , a simple inspection of the function appearing in the expression of the entropy suggests that in the limit @xmath159 this function will diverge as @xmath185 . since this function",
    "is coupled to the capacity and we expect non - trivial results , the other terms in the entropy also have to diverge in such a way .",
    "this implies , for instance , that for the function @xmath160_\\dag(x , y)$ ] the terms @xmath186 , @xmath187 and @xmath188 appearing in its argument have to go to infinity as @xmath185 .",
    "the precise coefficients in front of this divergence are given by the saddle - point equations of the conjugated order - parameters .",
    "performimg this asymptotic expansion explicitly leads to the result in section [ sec : gardner ] .",
    "99 m. blume , v.j .",
    "emery , and r.b .",
    "griffiths , phys .",
    "rev . a * 4 * , 1071 ( 1971 ) ; m. blume , phys",
    ". rev . * 141 * , 517 ( 1966 ) ; h.w .",
    "capel , physica ( amsterdam ) * 32 * , 966 ( 1966 ) .",
    "dominguez carreta and e. korutcheva , phys .",
    "e * 62 * , 2620 ( 2000 ) .",
    "d. boll and t. verbeiren , j. phys . a : math .",
    "gen * 36 * , 295 , 2003 .",
    "d. boll , h. rieger , and g.m .",
    "shim , j. phys . a : math .",
    "gen * 27 * , 3411 , 1994 .",
    "e. gardner , j. phys .",
    "a : math . gen . * 21 * , 257 ( 1988 ) .",
    "d. boll , i. prez castillo , and g. m. shim , phys .",
    "e * 67 * , 036113 , 2003 . s. mertens , h. m. khler , and s. bs , j. phys .",
    "a * 24 * , 4941 ( 1991 ) .",
    "d. boll , p. dupont , and j. van mourik , europhys . lett . * 15 * , 893 ( 1991 ) . f. gerl and u. krey , j. phys .",
    "a * 27 * , 7353 ( 1994 ) . f. gerl , k. bauer , and u. krey , z. phys .",
    "b * 88 * , 339 ( 1992 )",
    ". b. derrida , e. gardner , and a. zippelius , europhys .",
    "lett * 4 * , 167 ( 1987 ) .",
    "e. gardner , j. phys .",
    "a : math . gen . *",
    "22 * , 1969 ( 1989 ) .",
    "b. wemmenhove and a. c. c. coolen j. phys .",
    "gen . * 63 * , 9617 ( 2003 ) .",
    "i. prez castillo and n. skantzos , cond - mat/0309655 m. bouten , a. engel , a. komoda , and r. serneels , j. phys .",
    "gen * 23*,4643 ( 1990 ) .",
    "d. boll and j. van mourik , j. phys . a : math .",
    ", * 27 * , 1151 ( 1997 ) .",
    "m. mzard , g. parisi , and m.a .",
    "virasoro , _ spin glass theory and beyond _ , singapore , world scientific ( 1987 ) .",
    "tsodyks , europhys . lett .",
    "* 7 * , 203 ( 1988 ) .",
    "perez - vicente , europhys . lett . * 10 * , 621 ( 1989 ) .",
    "h. horner , z. phys .",
    "b * 75 * , 133 ( 1989 ) .",
    "j. r. de almeida and d. thouless , j. phys .",
    "* 11 * , 983 ( 1978 ) .",
    "m. bouten , j. phys .",
    "a : math . gen . , * 27 * , 6021 ( 1994 ) ."
  ],
  "abstract_text": [
    "<S> the optimal capacity of a diluted blume - emery - griffiths neural network is studied as a function of the pattern activity and the embedding stability using the gardner entropy approach . </S>",
    "<S> annealed dilution is considered , cutting some of the couplings referring to the ternary patterns themselves and some of the couplings related to the active patterns , both simultaneously ( synchronous dilution ) or independently ( asynchronous dilution ) . through the de almeida - thouless criterion </S>",
    "<S> it is found that the replica - symmetric solution is locally unstable as soon as there is dilution . </S>",
    "<S> the distribution of the couplings shows the typical gap with a width depending on the amount of dilution , but this gap persists even in cases where a particular type of coupling plays no role in the learning process . </S>"
  ]
}