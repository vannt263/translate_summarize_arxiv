{
  "article_text": [
    "detection of sparse mixtures is an important problem that arises in many scientific applications such as signal processing @xcite , biostatistics @xcite , and astrophysics @xcite , where the goal is to determine the existence of a signal which only appears in a small fraction of the noisy data .",
    "for example , topological defects and doppler effects manifest themselves as non - gaussian convolution component in the cosmic microwave background ( cmb ) temperature fluctuations .",
    "detection of non - gaussian signatures are important to identify cosmological origins of many phenomena @xcite .",
    "another example is disease surveillance where it is critical to discover an outbreak when the infected population is small @xcite .",
    "the detection problem is of significant interest also because it is closely connected to a number of other important problems including estimation , screening , large - scale multiple testing , and classification .",
    "see , for example , @xcite , @xcite , @xcite , @xcite , and @xcite .",
    "one of the earliest work on sparse mixture detection dates back to dobrushin @xcite , who considered the following problem originating from multi - channel detection in radiolocation .",
    "let @xmath0 denote the rayleigh distribution with the density @xmath1 .",
    "let @xmath2 be independently distributed according to @xmath3 , representing the random voltages observed on the @xmath4 channels . in the absence of noise , @xmath5 s are all equal to one , the nominal value ; while in the presence of signal , exactly one of the @xmath5 s becomes a known value @xmath6 . denoting the uniform distribution on @xmath7 $ ] by @xmath8 , the goal is to test the following competing hypotheses @xmath9 , \\quad \\mbox{versus}\\quad h_{1}^{(n ) } :   \\alpha_i = 1 + ( \\alpha -1 ) { { \\mathbf{1}_{\\left\\{{i = j}\\right\\ } } } } , \\quad j \\sim u_n \\ , .",
    "\\label{eq : ht.dobrushin}\\ ] ] since the signal only appears once out of the @xmath4 samples , in order for the signal to be distinguishable from noise , it is necessary for the amplitude @xmath10 to grow with the sample size @xmath4 ( in fact , at least logarithmically ) . by proving that the log - likelihood ratio converges to a stable distribution in the large-@xmath4 limit ,",
    "dobrushin @xcite obtained sharp asymptotics of the smallest @xmath10 in order to achieve the desired false alarm and miss detection probabilities .",
    "similar results are obtained in the continuous - time gaussian setting by burnashev and begmatov @xcite .",
    "subsequent important work include ingster @xcite and donoho and jin @xcite , which focused on detecting a sparse binary vector in the presence of gaussian observation noise .",
    "the problem can be formulated as follows .",
    "given a random sample @xmath11 , one wishes to test the hypotheses @xmath12   \\quad \\mbox{versus}\\quad h_{1}^{(n ) } :   y_i \\ , { { \\stackrel{\\text{{i.i.d.\\xspace}}}{\\sim}}}\\ , ( 1-\\epsilon_n ) { { \\mathcal{n}}}(0,1 ) + \\epsilon_n { { \\mathcal{n}}}(\\mu_n , 1 ) ,   i \\in [ n ]          \\label{eq : ht.idj}\\ ] ] where the non - null proportion @xmath13 is calibrated according to @xmath14 and the non - null effect @xmath15 grows with the sample size according to @xmath16 equivalently , one can write @xmath17 where @xmath18 is the observation noise . under the null hypothesis ,",
    "the mean vector @xmath19 is equal to zero ; under the alternative , @xmath20 is a non - zero sparse binary vector with @xmath21 , where @xmath22 denotes the point mass at @xmath23 .",
    "_ the detection boundary _ , which gives the smallest possible signal strength , @xmath24 , such that reliable detection is possible , is given by the following function in terms of the sparsity parameter @xmath25 : @xmath26 see ingster @xcite and donoho and jin @xcite .",
    "therefore , the hypotheses in can be tested with vanishing probability of error if and only if the pair @xmath27 lies in the strict epigraph @xmath28 which is called the _",
    "detectable region_. furthermore , because the fraction of the non - zero mean is very small , most tests based on the empirical moments have no power in detection .",
    "donoho and jin @xcite proposed an adaptive testing procedure based on tukey s higher criticism statistic and showed that it attains the optimal detection boundary without requiring the knowledge of the unknown parameters @xmath27 .",
    "the above results have been generalized along various directions within the framework of two - component gaussian mixtures .",
    "jager and wellner @xcite proposed a family of goodness - of - fit tests based on the rnyidivergences @xcite , including the higher criticism test as a special case , which achieve the optimal detection boundary adaptively .",
    "the detection boundary with correlated noise was established in @xcite which also proposed a modified version of the higher criticism that achieves the corresponding optimal boundary . in a related setup ,",
    "@xcite considered detecting a signal with a known geometric shape in gaussian noise .",
    "minimax estimation of the non - null proportion @xmath13 was studied in cai , jin and low @xcite .",
    "the setup of @xcite and @xcite specifically focuses on the two - point gaussian mixtures .",
    "although @xcite and @xcite provide insightful results for sparse signal detection , the setting is highly restrictive and idealized . in particular , it has the limitation that the signal strength must be a constant under the alternative , i.e. , the mean vector @xmath20 takes constant value @xmath15 on its support . in many applications ,",
    "the signal itself varies among the non - null portion of the samples .",
    "a natural question is the following : what is the detection boundary if @xmath15 varies under the alternative , say with a distribution @xmath29 ? motivated by these considerations ,",
    "the following heteroscedastic gaussian mixture model was considered in cai , jeng and jin @xcite : @xmath30 in this case , ( * ? ? ?",
    "* theorems 2.1 and 2.2 ) showed that reliable detection is possible if and only if @xmath31 where @xmath32 is given by @xmath33 where @xmath34 .",
    "it was also shown that the optimal detection boundary can be achieved by a double - sided version of the higher criticism test .      although the setup in cai , jeng and jin @xcite is more general than that considered in @xcite and @xcite , it is still restricted to the two - component gaussian mixtures .",
    "in many applications such as the aforementioned multi - channel detection @xcite and astrophysical problems @xcite , the sparse signal may not be binary and the distribution may not be gaussian . in the present paper , we consider the problem of sparse mixture detection in a general framework where the distributions are not necessarily gaussian and the non - null effects are not necessarily a binary vector .",
    "more specifically , given a random sample @xmath35 , we wish to test the following hypotheses @xmath36 where @xmath37 is the null distribution and @xmath38 is a distribution modeling the statistical variations of the non - null effects . the non - null proportion @xmath39 is calibrated according to .",
    "in this paper we obtain an explicit formula for the fundamental limit of the general testing problem under mild technical conditions on the mixture .",
    "we also establish the adaptive optimality of the higher criticism procedure across all sparse mixtures satisfying certain mild regularity conditions .",
    "in particular , the general results obtained in this paper recover and extend all the previously known results mentioned earlier in a unified manner .",
    "the results also generalize the optimality and adaptivity of the higher criticism procedure far beyond the original equal - signal - strength gaussian setup in @xcite and the heteroscedastic extension in @xcite . in the most general case ,",
    "it turns out that the detectability of the sparse mixture is governed by the behavior of the log - likelihood ratio evaluated at an appropriate quantile of the null distribution .",
    "although our general approach does not rely on the gaussianity of the model , it is however instructive to begin by considering the special case of _ sparse normal mixture _ with @xmath40 , i.e. , @xmath41 it is of special interest to consider the",
    "_ convolution model _ , where @xmath42 is a standard normal mixture and @xmath43 denotes the convolution of two distributions . in this case",
    "the hypotheses can be equivalently expressed via the additive - noise model , where @xmath44 under the null and @xmath45 under the alternative .",
    "based on the noisy observation @xmath46 , the goal is to determine whether @xmath20 is the zero vector or a _ sparse _ vector , whose support size is approximately @xmath47 and non - zero entries are distributed according to @xmath29 .",
    "therefore , the distribution @xmath29 represents the prior knowledge of the signal .",
    "the case of @xmath29 being a point mass is treated in @xcite . the case of rademacher @xmath29 in covered in (",
    "* chapter 8) . the heteroscedastic case where @xmath29 is gaussian is considered in @xcite .",
    "these results can be recovered by particularizing the general conclusion in the present paper .",
    "moreover , our results also shed light on what governs detectability in gaussian noise when the signal does not necessarily have equal strength .",
    "for example , consider the classical setup where the signal strength @xmath15 is now a random variable .",
    "if we have @xmath48 for some random variable @xmath49 , then the resulting detectable region is given by the ingster - donoho - jin expression scaled by the @xmath50-norm of @xmath49 .",
    "on the other hand , it is also possible that certain distributions of @xmath15 induces different shapes of detectable region than .",
    "see sections [ sec : char ] and [ sec : ex.dilate ] for further discussions .",
    "the rest of the paper is organized as follows .",
    "states the setup , defines the fundamental limit of sparse mixture detection and reviews some previously known results .",
    "the main results of the paper are presented in sections [ sec : main ] and [ sec : hc ] , where we provide an explicit characterization of the optimal detection boundary under mild technical conditions .",
    "moreover , it is shown in that the higher criticism test achieves the optimal performance adaptively .",
    "particularizes the general result to various special cases to give explicit formulae of the fundamental limits .",
    "discussions of generalizations and open problems are presented in .",
    "the main theorems are proven in , while the proofs of the technical lemmas are relegated to the appendices .      throughout the paper ,",
    "@xmath51 and @xmath52 denote the cumulative distribution function ( cdf ) and the density of the standard normal distribution respectively .",
    "let @xmath53 .",
    "let @xmath54 denote the @xmath4-fold product measure of @xmath55 .",
    "we say @xmath55 is absolutely continuous with respect to @xmath56 , denoted by @xmath57 , if @xmath58 for any measurable set @xmath59 such that @xmath60 .",
    "we say @xmath55 is singular with respect to @xmath56 , denoted by @xmath61 , if there exists a measurable @xmath59 such that @xmath62 and @xmath60 .",
    "we denote @xmath63 if @xmath64 , @xmath65 if @xmath66 , @xmath67 if @xmath68 and @xmath69 if @xmath70 .",
    "these asymptotic notations extend naturally to probabilistic setups , denoted by @xmath71 , etc . , where limits are in the sense of convergence in probability .",
    "in this section we define the fundamental limits for testing the hypotheses in terms of the sparsity parameter @xmath25 .",
    "an equivalent characterization in terms of the hellinger distance is also given .",
    "it is easy to see that as the non - null proportion @xmath13 decreases , the signal is more sparse and the testing problem in becomes more difficult .",
    "recall that @xmath13 is given by where @xmath72 parametrizes the sparsity level .",
    "thus , the question of detectability boils down to characterizing the smallest ( resp .",
    "largest ) @xmath25 such that the hypotheses in can be distinguished with probability tending to one ( resp .",
    "zero ) , when the sample size @xmath4 is large .",
    "for testing between two probability measures @xmath55 and @xmath56 , denote the optimal sum of type - i and type - ii error probabilities by @xmath73 where the infimum is over all measurable sets @xmath59 . by the neyman - pearson lemma @xcite",
    ", @xmath74 is achieved by the likelihood ratio test : declare @xmath55 if and only if @xmath75 .",
    "moreover , @xmath74 can be expressed in terms of the _ total variation distance _ @xmath76 as @xmath77    for a fixed sequence @xmath78 , denote the total variation between the null and alternative by @xmath79 which takes values in the unit interval . in view of ,",
    "the fundamental limits of testing the hypothesis are defined as follows .",
    "@xmath80    if @xmath81 , the common value is denoted by @xmath82 .",
    "[ def : betas ]    as illustrated by , the operational meaning of @xmath83 and @xmath84 are as follows : for any @xmath85 , all sequences of tests have vanishing probability of success ; for any @xmath86 , there exists a sequence of tests with vanishing probability of error . in information - theoretic parlance , if @xmath87 , we say _ strong converse _ holds , in the sense that if @xmath88 , all tests fail with probability tending to one ; if @xmath89 , there exists a sequence of tests with vanishing error probability .     and regimes of ( in)distinguishability of the hypotheses in the large-@xmath4 limit .",
    "]    clearly , @xmath84 and @xmath83 only depend on the sequence @xmath90 .",
    "the following lemma , proved in , shows that it is always sufficient to restrict the range of @xmath25 to the unit interval .",
    "@xmath91    [ lmm : beta01 ]    \\(o ) at ( -1,0 ) ; ( bl ) at ( 2,0 ) ; ( bu ) at ( 3,0 ) ; ( one ) at ( 6,0 ) ; ( b ) at ( 7,0 ) ; ( d2 ) [ right of = b , node distance=.2 cm ] @xmath25 ; ; ; ; ; ( o )  ( b ) ; ( o ) circle[radius=1pt ] ; ( @xmath92 )  ( bu ) ; ( @xmath93 )  ( bl ) ; ( one ) circle[radius=1pt ] ; at ( .5,.3 ) @xmath94 ; at ( .5,-.3 ) @xmath95 ; at ( 4.5,.3 ) @xmath96 ; at ( 4.5,-.3 ) @xmath97 ;    in the gaussian mixture model with @xmath40 , if the sequence @xmath98 is parametrized by some parameter @xmath24 , the fundamental limit @xmath82 in is a function of @xmath24 , denoted by @xmath99 . for example , in the ingster - donoho - jin setup where @xmath100 , @xmath82 , denoted by @xmath101 , can be obtained by inverting : @xmath102 in terms of , the detectable region is given by the strict hypograph @xmath103 .",
    "the function @xmath101 , plotted in , plays an important role in our later derivations .",
    "similarly , for the heteroscedastic mixture , inverting gives @xmath104 as shown in , all the above results can be obtained in a unified manner as a consequence of the general results in .",
    "closely related to the total variation distance is the hellinger distance ( * ? ? ?",
    "* chapter 2 ) @xmath105 which takes values in the interval @xmath106 $ ] and satisfies the following relationship : @xmath107 therefore , the total variation distance converges to zero ( resp .",
    "one ) is equivalent to the squared hellinger distance converges to zero ( resp .",
    "we will be focusing on the hellinger distance partly due to the fact that it tensorizes nicely under the product measures : @xmath108    denote the hellinger distance between the null and the alternative by @xmath109 in view of  and , the fundamental limits @xmath84 and @xmath83 can be equivalently defined as follows in terms of the asymptotic squared hellinger distance : @xmath110",
    "in this section we characterize the detectable region explicitly by analyzing the exact asymptotics of the hellinger distance induced by the sequence of distributions @xmath78 .      this subsection",
    "we focus on the case of sparse normal mixture with @xmath111 and @xmath38 absolutely continuous .",
    "we will argue in that by performing the lebesgue decomposition on @xmath38 if necessary , we can reduce the general problem to the absolutely continuous case .",
    "we first note that the _ essential supremum _ of a measurable function @xmath112 with respect to a measure @xmath113 is defined as @xmath114 we omit mentioning @xmath113 if @xmath113 is the lebesgue measure .",
    "now we are ready to state the main result of this section .",
    "let @xmath40 .",
    "assume that @xmath38 has a density @xmath115 with respect to the lebesgue measure .",
    "denote the log - likelihood ratio by @xmath116 let @xmath117 be a measurable function and define @xmath118    1 .   if @xmath119 _ uniformly _ in @xmath120 , where @xmath121 on a set of positive lebesgue measure , then @xmath122 .",
    "2 .   if @xmath123 _ uniformly _ in @xmath120 , then @xmath124 .    consequently , if the limits in and agree and @xmath121 on a set of positive measure , then @xmath125 .",
    "[ thm : main ]    .    assuming the setup of",
    ", we ask the following question in the reverse direction : what kind of function @xmath10 can arise in equations and ?",
    "the following lemma ( proved in ) gives a necessary and sufficient condition for @xmath10 .",
    "however , in the special case of convolutional models , the function @xmath10 needs to satisfy more stringent conditions , which we also discuss below .",
    "suppose @xmath126 holds _ uniformly _ in @xmath120 for some measurable function @xmath117 .",
    "then @xmath127 in particular , @xmath128 lebesgue - a.e .",
    "conversely , for all measurable @xmath10 that satisfies , there exists a sequence of @xmath98 , such that holds .    additionally , if the model is convolutional , i.e. , @xmath129 , then @xmath10 is convex .",
    "[ lmm : alpha ]    in many applications , we want to know how fast the optimal error probability decays if @xmath25 lies in the detectable region .",
    "the following result gives the precise asymptotics for the hellinger distance , which also gives upper bounds on the total variation , in view of .",
    "assume that holds .",
    "for any @xmath130 , the exponent of the hellinger distance is given by @xmath131 where @xmath132 which satisfies @xmath133 ( resp .",
    "@xmath134 ) if and only if @xmath135 ( resp .",
    "@xmath136 ) .",
    "[ thm : ebeta ]    as an application of , the following result relates the fundamental limit @xmath82 of the convolutional models to the classical ingster - donoho - jin detection boundary :    let @xmath129 .",
    "assume that @xmath29 has a density @xmath137 which satisfies that @xmath138 uniformly in @xmath139 for some measurable @xmath140 .",
    "then @xmath141 where @xmath101 is the ingster - donoho - jin detection boundary defined in .",
    "[ cor : conv ]    it should be noted that the convolutional case of the normal mixture detection problem is briefly discussed in ( * ? ? ?",
    "* section 6.1 ) , where inner and outer bounds on the detection boundary are given but do not meet . here completely settles this question .",
    "see for more examples .",
    "we conclude this subsection with a few remarks on .    under the assumption that the function @xmath121 on a set of positive lebesgue measure , the formula shows that the fundamental limit @xmath82 lies in the very sparse regime ( @xmath142 ) .",
    "we discuss the two extremal cases as follows :    1 .",
    "_ weak signal _ : note that @xmath143 if and only if @xmath144 almost everywhere . in this case",
    "the non - null effect is too weak to be detected for any @xmath145 .",
    "one example is the zero - mean heteroscedastic case @xmath146 with @xmath147 .",
    "then we have @xmath148 .",
    "2 .   _ strong signal _ : note that @xmath149 if and only if there exists @xmath150 , such that @xmath151 and @xmath152 at this particular @xmath150 , the density of the signal satisfies @xmath153 , which implies that there exists significant mass beyond @xmath154 , the extremal value under the null hypothesis @xcite .",
    "this suggests the possibility of constructing test procedures based on the _ sample maximum_. indeed , to understand the implication of more quantitatively , let us look at an even weaker condition : there exists @xmath150 such that @xmath151 and @xmath155 which , as shown in , implies that @xmath149 .",
    "[ rmk : extreme ]    in general @xmath82 need not exist .",
    "based on , it is easy to construct a gaussian mixture where @xmath84 and @xmath83 do not coincide .",
    "for example , let @xmath156 and @xmath157 be two measurable functions which satisfy and give rise to different values of @xmath158 in , which we denote by @xmath159 . then there exist sequences of distributions @xmath160 and @xmath161 which satisfy for @xmath156 and @xmath157 respectively .",
    "now define @xmath98 by @xmath162 and @xmath163 . then by , we have @xmath164 .",
    "the detection boundary in @xcite is obtained by deriving the limiting distribution of the log - likelihood ratio which relies on the normality of the null hypothesis .",
    "in contrast , our approach is based on analyzing the sharp asymptotics of the hellinger distance .",
    "this method enables us to generalize the result of to sparse non - gaussian mixtures , where we even allow the null distribution @xmath37 to vary with the sample size @xmath4 .",
    "consider the hypothesis testing problem .",
    "let @xmath165 .",
    "denote by @xmath166 and @xmath167 the cdf and the quantile function of @xmath38 , respectively , i.e. , @xmath168 if the log - likelihood ratio @xmath169 satisfies @xmath170 as @xmath171 _ uniformly _ in @xmath172 for some measurable function @xmath173 . if @xmath174 on a set of positive lebesgue measure , then @xmath175 [ thm : ng ]    the function @xmath176 appearing in satisfies the same condition as in . comparing with",
    ", we see that the uniform convergence condition is naturally replaced by the uniform convergence of the log - likelihood ratio evaluated at the null quantile .",
    "using the fact that @xmath177 for all @xmath178 @xcite , which implies that @xmath179 uniformly as @xmath180 , we can recover from by setting @xmath181 .",
    "the results in and are obtained under the assumption that the non - null effect @xmath38 is absolutely continuous with respect to the null distribution @xmath37 .",
    "next we show that it does not lose generality to focus our attention on this case . using the hahn - lebesgue decomposition ( * ? ? ?",
    "* theorem 1.6.3 ) , we can write @xmath182 for some @xmath183 $ ] , where @xmath184 and @xmath185 . put @xmath186 which satisfies @xmath187",
    ". then @xmath188 by , @xmath189 therefore the asymptotic hellinger distance of the original problem is completely determined by @xmath190 and the square - hellinger distance @xmath191 , which is also of a _",
    "sparse mixture _ form , with @xmath192 replaced by @xmath193 given in .",
    "in particular , we note the following special cases :    1 .   if @xmath194 , then @xmath195 ( resp .",
    "@xmath196 ) if and only if @xmath197 ( resp .",
    "@xmath196 ) , which means that detectability of the original sparse mixture coincide with the new mixture .",
    "2 .   if @xmath198 , then @xmath199 , which means that the original sparse mixture can be detected reliably .",
    "in fact , a trivial optimal test is to reject the null hypothesis if there exists one sample lying in the support of the singular component @xmath200 .",
    "as discussed in , the fundamental limit @xmath82 of testing sparse normal mixtures can be achieved by the likelihood ratio test . however , in general the likelihood ratio test requires the knowledge of the alternative distribution , which is typically not accessible in practice . to overcome this limitation , it is desirable to construct _ adaptive _ testing procedures to achieve the optimal performance simultaneously for a collection of alternatives .",
    "this problem is also known as _",
    "universal hypothesis testing_. see , e.g. , @xcite and the references therein , for results on discrete alphabets .",
    "the basic idea of adaptive procedures usually involves comparing the empirical distribution of the data to the null distribution , which is assumed to be known .    for the problem of detecting sparse normal mixtures , it is especially relevant to construct adaptive procedures , since in practice the underlying sparsity level and the non - zero priors are usually unknown .",
    "toward this end , donoho and jin @xcite introduced an adaptive test based on tukey s _ higher criticism _ statistic .",
    "for the special case of , i.e. , @xmath201 , it is shown that the higher criticism test achieves the optimal detection boundary while being adaptive to the unknown non - null parameters @xmath27 . following the generalization by jager and wellner @xcite via rnyidivergence , next we explain briefly the gist of the higher criticism test .",
    "given the data @xmath202 , denote the empirical cdf by @xmath203 respectively .",
    "similar to the kolmogorov - smirnov statistic @xcite which computes the @xmath50-distance ( maximal absolute difference ) between the empirical cdf and the null cdf , the higher criticism statistic is the maximal pointwise @xmath204-divergence between the null and the empirical cdf .",
    "we first introduce a few auxiliary notations .",
    "recall that the @xmath204-divergence between two probability measures is defined as @xmath205 in particular , the binary @xmath204-divergence function ( i.e. , the @xmath204-divergence between bernoulli distributions ) is given by @xmath206 where @xmath207 denotes the bernoulli distribution with bias @xmath208 .",
    "the higher criticism statistic is defined by @xmath209 based on the statistics , the higher criticism test declares @xmath210 if and only if @xmath211 where @xmath212 is an arbitrary fixed constant . the next result shows that the higher criticism test achieves the fundamental limit @xmath82 characterized by while being adaptive to all sequences of distributions @xmath98 which satisfy the regularity condition .",
    "this result generalizes the adaptivity of the higher criticism procedure far beyond the original equal - signal - strength setup in @xcite and the heteroscedastic extension in @xcite .    under the same assumption of , for any @xmath88 , the sum of type - i and type - ii error of the higher criticism test vanishes as @xmath213 .",
    "[ thm : hc ]",
    "in this section we particularize the general result in to several interesting special cases to obtain explicit detection boundaries .",
    "we derive the classical detection boundary from for the equal - signal - strength setup , which is a convolutional model with signal distribution @xmath214 and @xmath15 in .",
    "the log - likelihood ratio is given by @xmath215 plugging in @xmath216 , we have @xmath217 .",
    "consequently , the condition is fulfilled uniformly in @xmath120 with @xmath218 straightforward calculation yields that @xmath219 applying , we obtain the desired expression for @xmath99 .    as a variation of , the symmetrized version of @xmath220 was considered in ( * ? ? ?",
    "* section 8.1.6 ) , whose detection boundary is shown to be identical to .",
    "indeed , for binary - valued signal distributed according to , we have @xmath221 which gives rise to @xmath222 comparing with and , we conclude that the detection boundary still applies .      generalizing both the unary and binary signal distributions in",
    ", we consider @xmath29 that is the distribution of the random variable @xmath223 where @xmath224 is a sequence of positive numbers and @xmath49 is distributed according to a fixed distribution @xmath55 , parameterizing the shape of the signal . in other words , @xmath29 is the dilation of @xmath55 by @xmath15 .",
    "we ask the following question : by choosing the sequence @xmath15 and the random variable @xmath49 , is it possible to have detection boundaries which are shaped differently than the classical ingster - donoho - jin detection boundary ?",
    "it turns out that for @xmath225 , the answer to the above question is negative .",
    "as the next theorem shows , the detection boundary is given by that of the classical setup rescaled by the @xmath50-norm of @xmath49 .",
    "note that and corresponds to @xmath226 and @xmath227 , respectively .",
    "consider the convolutional model @xmath129 , where @xmath29 is the distribution of @xmath228 . then @xmath229 [ cor : dilate ]    recall that @xmath230 denotes the ingster - donoho - jin detection boundary defined in . since the log - likelihood ratio is given by @xmath231}$ ]",
    ", we have @xmath232 }     \\nonumber \\\\ = & ~ { \\mathop{\\mathrm{ess \\ , sup}}}_x { \\left\\ { -x^2 + 2 u x \\right\\ } } \\log n ( 1+o(1 ) ) \\label{eq : alphadilate }    , \\end{aligned}\\ ] ] where we have applied and the essential supremum in is with respect to @xmath55 , the distribution of @xmath49 . therefore @xmath233 . applying yields the existence of @xmath82 , given by @xmath234 where follows from the facts that @xmath230 is increasing and that @xmath235",
    ".    tightens the bounds given at the end of ( * ? ? ?",
    "* section 6.1 ) based on the interval containing the signal support . from we see that the detection boundary coincides with the classical case with @xmath236 replaced by @xmath50-norm of @xmath49 . therefore , as far as the detection boundary is concerned , only the support of @xmath49 matters and the detection problem is driven by the maximal signal strength .",
    "in particular , for @xmath237 or non - compactly supported @xmath49 , we obtain the degenerate case @xmath149 ( see also about the strong - signal regime ) . however , it is possible that the density of @xmath49 plays a role in finer asymptotics of the testing problem , e.g. , the convergence rate of the error probability and the limiting distribution of the log - likelihood ratio at the detection boundary .",
    "one of the consequences of is the following : as long as @xmath225 , non - compactly supported @xmath49 results in the degenerate case of @xmath238 , since the signal is too strong to go undetected .",
    "however , this conclusion need not be true if @xmath15 behaves differently .",
    "we conclude this subsection by constructing a family of distributions of @xmath49 with unbounded support and an appropriately chosen sequence @xmath239 , such that the detection boundary is non - degenerate : let @xmath49 be distributed according to the following _ generalized gaussian _ ( subbotin ) distribution @xmath240 @xcite with shape parameter @xmath241 , whose density is @xmath242 put @xmath243 .",
    "then the density of @xmath244 is given by @xmath245 .",
    "hence @xmath246 which satisfies the condition with @xmath247 applying , we obtain the detection boundary @xmath82 ( a two - dimensional _ surface",
    "_ parametrized by @xmath248 shown in ) as follows @xmath249 where is the ingster - donoho - jin detection boundary .",
    "dilate.pdf ( 8,60)@xmath82 ( 102,3)@xmath24 ( 60,57)@xmath250 ( 60,46)@xmath251 ( 60,26)@xmath252 ( 60,15)@xmath253    equation can be further simplified for the following special cases .    * @xmath252 ( laplace ) : plugging into , straightforward computation yields @xmath254 * @xmath250 ( gaussian ) : in this case we have @xmath255 and @xmath256 .",
    "this is a special case of the heteroscedastic case in @xcite , which will be discussed in detail in .",
    "simplifying we obtain @xmath257 which coincides with .",
    "the heteroscedastic normal mixtures considered in corresponds to @xmath258 with @xmath15 given in and @xmath259 .",
    "in particular , if @xmath260 , @xmath38 is given by the convolution @xmath261 , where the gaussian component @xmath262 models the variation in the signal amplitude .    for any @xmath120 , @xmath263 where @xmath264 similar to the calculation in , we have . ] @xmath265 and @xmath266 note that @xmath267 if @xmath268 . assembling  and applying , we have @xmath269 solving the equation @xmath270 in @xmath24 yields the equivalent detection boundary in terms of @xmath24 . in the special case of @xmath271 , where the signal is distributed according to @xmath272 , we have @xmath273 therefore , as long as the signal variance exceeds that of the noise , reliable detection is possible in the very sparse regime @xmath145 , even if the average signal strength does not tend to infinity .",
    "we consider the detection boundary of the following generalized gaussian location mixture which was studied in ( * ? ?",
    "* section 5.2 ) : @xmath274 where @xmath240 is defined in , and @xmath275 . since @xmath276",
    "uniformly in @xmath277 , is fulfilled with @xmath278 .",
    "applying , we have @xmath279 it is easy to verify that agrees with the results in ( * ? ? ?",
    "* theorem 5.1 ) .",
    "similarly , the detection boundary for exponential-@xmath280 mixture in ( * ? ? ?",
    "* theorem 1.7 ) can also be derived from .",
    "we conclude the paper with a few discussions and open problems .      our main results in only concern the _ very sparse _ regime @xmath282 .",
    "this is because under the assumption in that @xmath121 on a set of positive lebesgue measure , we always have @xmath283 . one of the major distinctions between the very sparse and moderately sparse regimes is the effect of symmetrization . to illustrate this point ,",
    "consider the sparse normal mixture model .",
    "given any @xmath38 , replacing it by its symmetrized version @xmath284 always increases the difficulty of testing .",
    "this follows from the inequality @xmath285 , a consequence of the convexity of the squared hellinger distance and the symmetry of @xmath51 .",
    "a natural question is : does symmetrization always have an impact on the detection boundary ? in the very sparse regime , it turns out that under the regularity conditions imposed in , symmetrization does not affect the fundamental limit @xmath82 , because both @xmath38 and @xmath286 give rise to the same function @xmath10 .",
    "it is unclear whether @xmath84 and @xmath83 remain unchanged if an arbitrary sequence @xmath98 is symmetrized .",
    "however , in the moderately sparse regime , an asymmetric non - null effect can be much more detectable than its symmetrized version .",
    "for instance , direct calculation ( see for example ( * ? ? ?",
    "* section 2.2 ) ) shows that @xmath287 for @xmath288 , but @xmath289 for @xmath290 .",
    "moreover , unlike in the very sparse regime , moment - based tests can be powerful in the moderately sparse regime , which guarantee that @xmath291 .",
    "for instance , in the above examples @xmath288 or @xmath290 , the detection boundary can be obtained by thresholding the sample mean or sample variance respectively .",
    "more sophisticated moment - based tests such as the excess kurtosis tests have been studied in the context of sparse mixtures @xcite .",
    "it is unclear whether they are always optimal when @xmath292 .",
    "while establishes the adaptive optimality of the higher criticism test in the very sparse regime @xmath145 , the optimality of the higher criticism test in the moderately sparse case @xmath292 remains an open question .",
    "note that in the classical setup , it has been shown @xcite that the higher criticism test achieves adaptive optimality for @xmath293 $ ] and @xmath294 . in this case since @xmath295 , we have @xmath296 and thus does not apply .",
    "it is possible to obtain a counterpart of and an analogous expression for @xmath82 for the moderately sparse regime if one assumes a similar uniform approximation property of the log - likelihood ratio , for example , @xmath297 for some function @xmath10 .",
    "another interesting problem is to investigate the optimality of procedures introduced in @xcite based on rnyidivergence under the same setup of .",
    "laplace s method ( see , e.g. , ( * ? ? ?",
    "* section 2.4 ) ) is a technique for analyzing the asymptotics of integrals of the form @xmath298 when @xmath299 is large .",
    "the proof of uses the following first - order version of the laplace s method .",
    "since we are only interested in the exponent ( i.e. , the leading term ) , we do not use saddle - point approximation in the usual laplace s method and impose _ no _ regularity conditions on the function @xmath112 except for the finiteness of the integral .",
    "moreover , the exponent only depends on the _ essential supremum _ of @xmath112 with respect to @xmath300 , which is invariant if @xmath112 is modified on a @xmath300-negligible set .",
    "let @xmath301 be a measure space .",
    "let @xmath302 be measurable .",
    "assume that @xmath303 holds uniformly in @xmath304 for some measurable @xmath305 .",
    "if @xmath306 for some @xmath307 , then @xmath308 [ lmm : laplace ]    first we deal with the case of @xmath309 , which implies that @xmath310 for all @xmath311 . moreover , by chernoff bound , @xmath312 . by , for any @xmath313 , there exists @xmath314 such that @xmath315 for all @xmath304 and @xmath316 .",
    "therefore , @xmath317 for any @xmath318 and @xmath311 . then @xmath319 . by the arbitrariness of @xmath23 and @xmath320 , we have @xmath321 .",
    "next we assume that @xmath322 . by replacing @xmath112 with @xmath323",
    ", we can assume that @xmath324 without loss of any generality",
    ". then @xmath325 @xmath300-a.e .",
    "hence , by , @xmath326 holds for all @xmath316 . by the arbitrariness of @xmath320",
    ", we have @xmath327 for the lower bound , note that , by the definition of @xmath324 , @xmath328 for all @xmath212 .",
    "therefore , by , we have @xmath329 for any @xmath318 and @xmath212 . first sending @xmath330 then @xmath331 and @xmath332 , we have @xmath333 completing the proof of .",
    "let @xmath301 be a measure space .",
    "let @xmath302 be measurable .",
    "let @xmath334 satisfy @xmath335 @xmath300-a.e .",
    "for some @xmath336 .",
    "if @xmath337 holds uniformly in @xmath304 for some measurable @xmath338 , then @xmath339    conversely , if @xmath340 holds uniformly in @xmath304 for some measurable @xmath341 such that @xmath342 for some @xmath307 , then @xmath343 [ lmm : laplace2 ]    by , for any @xmath313 , there exists @xmath344 such that @xmath345 for all @xmath304 and @xmath346 . by definition of @xmath347 ,",
    "therefore , by , we have @xmath349 for all sufficiently large @xmath299 . first sending @xmath330",
    "then @xmath332 , we obtain the desired .",
    "next we assume that @xmath350 . by replacing @xmath112 with @xmath351",
    ", we can assume that @xmath352 without loss of any generality .",
    "then @xmath325 @xmath300-a.e . on @xmath59",
    "hence , by , @xmath353 holds for all @xmath316 . by the arbitrariness of @xmath320 ,",
    "we have @xmath354    the following lemma is useful for analyzing the asymptotics of hellinger distance :    1 .",
    "[ sqrt1 ] for any @xmath355 , the function @xmath356 is strictly convex on @xmath357 and strictly decreasing and increasing on @xmath358 $ ] and @xmath359 , respectively .",
    "[ sqrt2 ] for any @xmath360 , @xmath361    [ lmm : sqrt ]    1 .",
    "since @xmath362 is strictly concave , @xmath363 is strictly convex .",
    "solving for the stationary point yields the minimum at @xmath364 .",
    "first we consider @xmath365 .",
    "since @xmath366 is convex , @xmath367 is increasing .",
    "consequently , we have @xmath368 for all @xmath369 .",
    "+ next we consider @xmath370 . by the concavity of @xmath362 , @xmath371 is decreasing .",
    "hence @xmath372 for all @xmath373 $ ] . assembling the above two cases yields .",
    "the following lemmas are useful in proving :    let @xmath140 be measurable and @xmath113 be any measure on @xmath374 .",
    "the function @xmath375 defined by @xmath376 is decreasing and lower - semicontinuous , where the essential supremum is with respect to @xmath113 .",
    "[ lmm : gs ]    the monotonicity is obvious .",
    "we only prove lower - semicontinuity , which , in particular , also implies right - continuity .",
    "let @xmath377 . by definition of the essential supremum , for any @xmath378",
    ", we have @xmath379 . by the dominated convergence theorem , @xmath380 .",
    "hence there exists @xmath381 such that @xmath382 for all @xmath383 , which implies that @xmath384 for all @xmath383 . by the arbitrariness of @xmath378",
    ", we have @xmath385 , completing the proof of the lower semi - continuity .    under the conditions of , for any @xmath386 , @xmath387 [ lmm : gntail ]",
    "first assume that @xmath388 .",
    "then @xmath389 where the last equality follows from .",
    "the proof for @xmath390 is completely analogous .",
    "let @xmath391 . put @xmath392 .",
    "since @xmath393 by assumption , we also have @xmath394 .",
    "denote the likelihood ratio by @xmath395 .",
    "then @xmath396 _ ( direct part ) _ recall the notation @xmath158 defined in , which can be equivalently written as @xmath397 assuming , we show that @xmath122 by lower bounding the hellinger distance .",
    "to this end , fix an arbitrary @xmath212 .",
    "let @xmath398 .",
    "denote by @xmath399 the lebesgue measure on the real line . by definition of the essential supremum , @xmath400 since @xmath401 for all @xmath150 and @xmath402 , we must have @xmath403 since , by assumption , @xmath404 , there exists @xmath405 , such that @xmath406 by assumption , there exists @xmath407 such that @xmath408 holds for all @xmath120 and all @xmath409 . from , we have either @xmath410 or @xmath411",
    "next we discuss these two cases separately :    _ case i _ : assume .",
    "let @xmath412 the square hellinger distance can be lower bounded as follows : @xmath413 } \\label{eq : acc0}\\\\ \\geq & ~ { \\mathbb{e}\\left [ { \\left ( \\sqrt{1 + n^{-\\beta } ( \\exp(\\ell_n(u \\sqrt{2 \\log n } ) ) - 1 ) } - 1 \\right)}^2 { { \\mathbf{1}_{\\left\\{{|u| \\leq 1 , \\alpha(u)- \\frac{u^2}{2 } \\geq \\beta + \\delta - \\frac{1}{2 } , \\alpha(u ) \\geq 2 \\epsilon}\\right\\ } } } } \\right ] } \\nonumber \\\\ \\geq & ~ { \\mathbb{e}\\left [ { \\left ( \\sqrt{1 + n^{-\\beta } ( n^{\\alpha(u ) - \\epsilon } - 1)}-1 \\right)}^2 { { \\mathbf{1}_{\\left\\{{|u| \\leq 1 , \\alpha(u)- \\frac{u^2}{2 } \\geq \\beta + \\delta - \\frac{1}{2 } , \\alpha(u ) \\geq 2 \\epsilon}\\right\\ } } } } \\right ] } \\label{eq : acc2}\\\\ \\geq & ~ \\frac{(\\sqrt{2}-1)^2}{4 } { \\mathbb{e}\\left [   n^{(\\alpha(u ) - \\epsilon-\\beta ) \\wedge 2(\\alpha(u ) - \\epsilon-\\beta ) } { { \\mathbf{1}_{\\left\\{{|u| \\leq 1 , \\alpha(u)- \\frac{u^2}{2 } \\geq \\beta + \\delta - \\frac{1}{2 } , \\alpha(u ) \\geq 2 \\epsilon}\\right\\ } } } } \\right ] } \\label{eq : acc3}\\\\ = & ~ \\frac{(\\sqrt{2}-1)^2 \\sqrt{\\log n}}{4\\sqrt{\\pi } } \\int n^{(\\alpha(u ) - \\epsilon-\\beta ) \\wedge 2(\\alpha(u ) - \\epsilon-\\beta ) - u^2 } { { \\mathbf{1}_{\\left\\{{|u| \\leq 1 , \\alpha(u)- \\frac{u^2}{2 } \\geq \\beta + \\delta - \\frac{1}{2 } , \\alpha(u ) \\geq 2 \\epsilon}\\right\\ } } } }   { { \\rm d}}u   \\label{eq : acc4 } \\\\",
    "\\geq & ~ \\frac{(\\sqrt{2}-1)^2 \\sqrt{\\log n}}{4\\sqrt{\\pi } } \\lambda{\\left\\ { |u| \\leq 1 , \\alpha(u)- \\frac{u^2}{2 } \\geq \\beta + \\delta - \\frac{1}{2 } , \\alpha(u ) \\geq 2 \\epsilon \\right\\ } } n^{-1+\\frac{\\delta}{2 } }   \\label{eq : acc5}\\end{aligned}\\ ] ] where    * : by .",
    "* : by .[sqrt1 ] and .",
    "* : without loss of generality , we can assume that @xmath414 . then applying the lower bound in .[sqrt2 ] yields the desired inequality .",
    "* : we used the density of @xmath415 defined in . * : given that @xmath416 and @xmath417 , we have both @xmath418 and @xmath419 .",
    "_ case ii _ : now we assume .",
    "following analogous steps as in the previous case , we have @xmath420 where is due to the following : since @xmath151 and @xmath421 , we have both @xmath422 and @xmath423 .    combining and",
    "we conclude that @xmath424 . by the arbitrariness of @xmath212 and the alternative definition of @xmath83 in",
    ", the proof of @xmath122 is completed .    _",
    "( converse part ) _ fix an arbitrary @xmath212 .",
    "let @xmath425 we upper bound the hellinger integral as follows : first note that @xmath426 } + { \\mathbb{e}\\left [ { \\left ( \\sqrt{1 + n^{-\\beta } ( l_n - 1 ) } - 1 \\right)}^2 { {",
    "\\mathbf{1}_{\\left\\{{l_n \\leq 1}\\right\\ } } } } \\right]}.       \\label{eq : h3}\\ ] ] applying .[sqrt1 ] , we have @xmath427 } \\leq ( \\sqrt{1-n^{-\\beta}}-1)^2 \\leq n^{-2\\beta } = o(n^{-1 } ) ,       \\label{eq : h33}\\ ] ] since @xmath428 by .",
    "consequently , the asymptotics of the hellinger integral @xmath429 is dominated by the first term in , denoted by @xmath430 , which we analyze below using the laplace method .    by",
    ", there exists @xmath431 such that @xmath432 holds for all @xmath120 and all @xmath433 . then @xmath434 }    \\nonumber \\\\ = & ~ { \\mathbb{e}\\left [ { \\left ( \\sqrt{1 + n^{-\\beta } ( \\exp(\\ell_n(u \\sqrt{2 \\log n } ) ) - 1 ) } - 1 \\right)}^2 { { \\mathbf{1}_{\\left\\{{l_n \\geq 1}\\right\\ } } } } \\right ] } \\nonumber \\\\ \\leq & ~ { \\mathbb{e}\\left [ { \\left ( \\sqrt{1 + n^{-\\beta } ( n^{\\alpha(u ) + \\delta } - 1 ) } - 1 \\right)}^2 { { \\mathbf{1}_{\\left\\{{\\alpha(u ) \\geq -\\delta}\\right\\ } } } } \\right ] } \\label{eq : cv0}\\\\ \\leq & ~ { \\mathbb{e}\\left [ { \\left ( \\sqrt{1 + n^{\\alpha(u ) + \\delta-\\beta } } - 1 \\right)}^2   \\right ] } \\nonumber \\\\ \\leq & ~ { \\mathbb{e}\\left [ n^{(2(\\alpha(u ) + \\delta-\\beta ) ) \\wedge ( \\alpha(u ) + \\delta-\\beta ) } \\right ] } \\label{eq : cv1}\\\\ = & ~ \\sqrt{\\frac{\\log n}{\\pi } } \\int n^{(2(\\alpha(u ) + \\delta-\\beta))\\wedge ( \\alpha(u ) + \\delta-\\beta ) - u^2 } { { \\rm d}}u \\label{eq : cv2}\\end{aligned}\\ ] ] where and are due to and .[sqrt2 ] , respectively .",
    "next we apply to analyze the exponent of .",
    "first we verify the integrability condition : @xmath435 in view of . applying to",
    ", we have @xmath436 by , @xmath437 holds a.e .",
    "consequently , @xmath438 holds for almost every @xmath439 \\cup [ 1,\\infty)$ ] and @xmath440 holds for almost every @xmath441 $ ] . these conditions immediately imply that @xmath442 holds a.e . assembling and , we conclude that @xmath443 . by the arbitrariness of @xmath212 and the alternative definition of @xmath84 in",
    ", the proof of @xmath124 is completed .    in view of the proof of ,",
    "the desired readily follows from combining , , and .",
    "let @xmath391 .",
    "write the hellinger integral in terms of the cdf of the likelihood ratio as follows : @xmath444 }    \\nonumber \\\\ = & ~ \\int_{{{\\mathbb{r}}}_+ } { { \\mathbb{p}\\left\\ { { \\left ( \\sqrt{1 + n^{-\\beta } ( l_n(w ) - 1 ) } - 1 \\right)}^2 \\geq t \\right\\ } } } { { \\rm d}}t       \\nonumber \\\\ = & ~ \\int_{{{\\mathbb{r}}}_+ } { { \\mathbb{p}\\left\\ { \\sqrt{1 + n^{-\\beta } ( l_n(w ) - 1 ) } \\geq 1+\\sqrt{t }   \\right\\ } } } { { \\rm d}}t + \\int_0 ^ 1 { { \\mathbb{p}\\left\\ { \\sqrt{1 + n^{-\\beta } ( l_n(w ) - 1 ) } \\leq 1-\\sqrt{t }   \\right\\ } } } { { \\rm d}}t   \\nonumber \\\\ = & ~ \\underbrace{\\int_{{{\\mathbb{r}}}_+ } { { \\mathbb{p}\\left\\ { l_n(w ) \\geq 1 + n^{\\beta}((1+\\sqrt{t})^2 - 1 )   \\right\\ } } } { { \\rm d}}t}_{\\triangleq a_n } + \\underbrace{\\int_0 ^ 1 { { \\mathbb{p}\\left\\ { l_n(w ) \\leq 1 + n^{\\beta}((1-\\sqrt{t})^2 - 1 )   \\right\\ } } } { { \\rm d}}t}_{\\triangleq b_n }   \\label{eq : h2}\\end{aligned}\\ ] ] where @xmath445 due to the following reasoning : since @xmath446 , the integrand in @xmath447 is zero whenever @xmath448 , i.e. , @xmath449 .",
    "therefore @xmath450 .",
    "therefore @xmath429 is dominated by @xmath430 , which we analyze below : @xmath451    using the cdf of @xmath452 under the null hypothesis ,    therefore the exponent of @xmath430 coincides with the exponent of the integral @xmath453 , which can be analyzed via the laplace method .",
    "[ rmk : lncdf ]    put @xmath454 _",
    "( necessity ) _ since @xmath455 , it is sufficient to prove @xmath456 since @xmath457 , we have @xmath458 . by assumption , @xmath459 uniformly in @xmath150 .",
    "then for all @xmath212 , @xmath460 holds for sufficiently large @xmath4 .",
    "in particular , @xmath461 . for",
    "general @xmath462 , let @xmath463 , @xmath464 and @xmath465 .",
    "put @xmath466 and @xmath467 . then @xmath468 $ ] .",
    "hlder s inequality yields @xmath469 , which gives the desired .",
    "it then follows from that @xmath470 , i.e. , @xmath128 a.e .    _",
    "( sufficiency ) _ let @xmath10 be a measurable function satisfying .",
    "let @xmath38 be a probability measure with the density @xmath471 which is a legitimate density function in view of .",
    "then the log - likelihood ratio satisfies @xmath472 , which fulfills uniformly .    for convolutional models ,",
    "the convexity of @xmath10 is inherited from the geometric properties of the log - likelihood ratio in the normal location model : since @xmath473}}{\\varphi(y)}$ ] is convex for any random variable @xmath49 ( see , e.g. , ( * ? ? ?",
    "* property 3 ) and @xcite ) , we have @xmath474 for any @xmath373 $ ] and @xmath475 . dividing both sides by @xmath476 and sending @xmath171",
    ", we have @xmath477 .    since @xmath478",
    ", we have @xmath479 where the last equality follows from . plugging the above asymptotics into @xmath480",
    ", we see that is fulfilled uniformly in @xmath481 with @xmath482 applying , we obtain @xmath483 where the last step follows from the .",
    "let @xmath484 . put @xmath485 . since @xmath165 by assumption , we also have @xmath486 .",
    "denote the likelihood ratio ( radon - nikodym derivative ) by @xmath487 .",
    "then @xmath488    instead of introducing the random variable @xmath415 in for the gaussian case , we apply the quantile transformation to generate the distribution of @xmath489 : let @xmath415 be uniformly distributed on the unit interval .",
    "then @xmath490 which is exponentially distributed . putting @xmath491 we have @xmath492 set @xmath493 and @xmath494 , which satisfy @xmath495 for all sufficiently large @xmath4 .",
    "for the converse proof , we can write the square hellinger distance as an expectation with respect to @xmath496 : @xmath497 }   \\\\      & ~ + { \\mathbb{e}\\left [ { \\left ( \\sqrt{1 + n^{-\\beta } ( \\exp(\\ell_n(z_n(1-u ) ) ) - 1 ) } - 1 \\right)}^2 { { \\mathbf{1}_{\\left\\{{0 < u \\leq \\frac{1}{2}}\\right\\ } } } } \\right ] } .\\end{aligned}\\ ] ] analogous to , by truncating the log - likelihood ratio at zero , we can show that the hellinger distance is dominated by the following : @xmath498 } \\nonumber    \\\\      & ~ + { \\mathbb{e}\\left [ { \\left ( \\sqrt{1 + n^{-\\beta } ( \\exp(\\ell_n(z_n(1-u ) ) ) - 1 ) } - 1 \\right)}^2 { { \\mathbf{1}_{\\left\\{{0",
    "< u \\leq \\frac{1}{2 } , t_n(s_n ) \\geq 0}\\right\\ } } } } \\right ] } \\nonumber \\\\ = & ~ { \\mathbb{e}\\left [ { \\left ( \\sqrt{1 + n^{-\\beta } ( \\exp(r_n(s_n ) ) - 1 ) } - 1 \\right)}^2 { { \\mathbf{1}_{\\left\\{{s_n > \\log_n 2 , r_n(s_n ) \\geq 0}\\right\\ } } } } \\right ] } \\label{eq : jg1 } \\\\      & ~ + { \\mathbb{e}\\left [ { \\left ( \\sqrt{1 + n^{-\\beta } ( \\exp(t_n(s_n ) ) - 1 ) } - 1 \\right)}^2 { { \\mathbf{1}_{\\left\\{{s_n \\geq   \\log_n 2 , t_n(s_n ) \\geq 0}\\right\\ } } } } \\right ] } \\nonumber \\\\ \\leq & ~ { \\mathbb{e}\\left [ { \\left ( \\sqrt{1 + n^{-\\beta } ( n^{\\alpha_0(s_n)+\\delta } - 1 ) } - 1 \\right)}^2 + { \\left ( \\sqrt{1 + n^{-\\beta } ( n^{\\alpha_1(s_n)+\\delta } - 1 ) } - 1 \\right)}^2   \\right ] } \\label{eq : jg2 } \\\\ \\leq & ~ 2 \\ , { \\mathbb{e}\\left [ n^{2(\\alpha_0 \\vee \\alpha_1(u ) + \\delta-\\beta ) \\wedge ( \\alpha_0 \\vee \\alpha_1(u ) + \\delta-\\beta ) } \\right ] } \\\\ \\leq & ~ n^{-1-\\delta } \\label{eq : jg3 } \\end{aligned}\\ ] ] where follows from  , from  and from  . the direct part of the proof is completely analogous to that of by lower bounding the integral in .",
    "let @xmath499 , which is uniformly distributed on @xmath358 $ ] under the null hypothesis . with a change of variable",
    ", we have @xmath500 which satisfies that @xmath501 @xcite .",
    "therefore the type - i error probability of the test vanishes for any choice of @xmath212 .",
    "it remains to show that @xmath502 under the alternative . to this end , fix @xmath503 and put @xmath504 and @xmath505 . by",
    ", we have @xmath506 where @xmath507 is binomially distributed with sample size @xmath4 and success probability @xmath508 .",
    "therefore @xmath509 }   = \\sqrt{n } \\frac{\\rho_{n , s } - r_{n , s}}{\\sqrt{r_{n , s}(1-r_{n , s } ) } }   = n^{\\frac{1}{2}-\\beta } \\frac{{g_n}(\\sqrt{2s\\log n } ) - r_{n , s}}{\\sqrt{r_{n , s}(1-r_{n , s } ) } }   .",
    "\\label{eq : evn}\\ ] ] and @xmath510 by chebyshev s inequality , @xmath511 } \\right\\ } } } \\leq \\frac{4 \\ , { \\mathsf{var}}v_n(s)}{{\\mathbb{e}\\left [ v_n(s ) \\right]}^2 } = \\frac{4 \\rho_{n , s}(1- \\rho_{n , s})}{n ( \\rho_{n , s } - r_{n , s})^2}.\\ ] ] by , @xmath512 where @xmath513 . plugging into and yields @xmath509 } = n^{\\frac{1+s}{2}-\\beta+v(s)+o(1 ) }       \\label{eq : evns}\\ ] ] and @xmath511 } \\right\\ } } } \\leq n^{2\\beta - s-1 - 2v(s)+o(1)}+ n^{\\beta-1-v(s)+o(1)}.         \\label{eq : pvns}\\ ] ] suppose that @xmath514",
    ". then @xmath515 } = \\omega(\\sqrt{\\log \\log n})$ ] .",
    "moreover , we have @xmath516 and @xmath517 since @xmath518 . combining , and",
    ", we obtain @xmath519 that is , the type - ii error probability also vanishes .",
    "consequently , a sufficient condition for the higher criticism test to succeed is @xmath520 where follows from the following reasoning : by ( * ? ? ?",
    "* proposition 3.5 ) , the supremum and the essential supremum ( with respect to the lebesgue measure ) coincide for all lower semi - continuous functions .",
    "indeed , @xmath521 is lower semi - continuous by , and so is @xmath522 .",
    "it remains to show that the right - hand side of coincides with the expression of @xmath82 in .",
    "indeed , we have @xmath523 note that the second equality follows from interchanging the essential supremums : for any bi - measurable function @xmath524 , @xmath525 where the last essential supremum is with respect to the product measure . thus the proof of the theorem is completed .",
    "this appendix collects a few properties of total variation and hellinger distances for mixture distributions .",
    "let @xmath526 and @xmath527 .",
    "then @xmath528 which satisfies @xmath529    [ lmm : h2.mix ]    since @xmath527 , there exists a measurable set @xmath530 such that @xmath531 and @xmath532",
    ". then @xmath533 the inequalities in follow from and the facts that @xmath534 and @xmath535 .    for any probability measures",
    "@xmath536 , @xmath537 is decreasing on @xmath358 $ ] .",
    "[ lmm : hdec ]    fix @xmath538 .",
    "since @xmath539 , the convexity of @xmath540 yields @xmath541    we conclude this appendix by proving presented in :    by , the function @xmath542 is decreasing , which , in view of the characterization  , implies that @xmath543 .",
    "thus it only remains to establish the rightmost inequality in . to this end , we show that as soon as @xmath25 exceeds @xmath544 , @xmath545 becomes @xmath546 regardless of the choice of @xmath98 : fix @xmath547 . then @xmath548 where follows from the data - processing inequality , which is satisfied for all @xmath112-divergences @xcite , in particular , the total variation : @xmath549 , where @xmath550 is any probability transition kernel .",
    "while is sufficient for our purpose in proving , it is unclear whether the monotonicity carries over to @xmath551 , since product measures do not form a convex set .",
    "it is however easy to see that @xmath551 is decreasing , which follows from the proof of with @xmath552 replaced by @xmath553 .",
    "it is also clear that @xmath554 is decreasing in view of .",
    "[ rmk : hdec ]",
    "in this appendix we show that implies that @xmath555 , i.e. , for any @xmath556 , the hypotheses in can be tested reliably . without loss of generality ,",
    "we assume that @xmath557 .",
    "then @xmath558 we show that the total variation distance between the product measures converge to one .",
    "put @xmath559^n$ ] . in view of the first inequality in",
    ", the total variation distance can be lower bounded as follows : @xmath560 using , we have @xmath561 on the other hand , @xmath562 where the last equality is due to @xmath563 .",
    "therefore @xmath564 for any @xmath556 , which proves that @xmath555 .",
    "in fact , the above derivation also shows that the following _ maximum test _ achieves vanishing probability of error : declare @xmath210 if and only if @xmath565 .",
    "in general the maximum test is suboptimal .",
    "for example , in the classical setting where @xmath566 , ( * ? ? ?",
    "* theorem 1.3 ) shows that the maximum test does not attain the ingster - donoho - jin detection boundary for @xmath567 $ ] ."
  ],
  "abstract_text": [
    "<S> detection of sparse signals arises in a wide range of modern scientific studies . </S>",
    "<S> the focus so far has been mainly on gaussian mixture models . in this paper , we consider the detection problem under a general sparse mixture model and obtain an explicit expression for the detection boundary . </S>",
    "<S> it is shown that the fundamental limits of detection is governed by the behavior of the log - likelihood ratio evaluated at an appropriate quantile of the null distribution . </S>",
    "<S> we also establish the adaptive optimality of the higher criticism procedure across all sparse mixtures satisfying certain mild regularity conditions . </S>",
    "<S> in particular , the general results obtained in this paper recover and extend in a unified manner the previously known results on sparse detection far beyond the conventional gaussian model and other exponential families .    </S>",
    "<S> * keywords : * hypothesis testing , high - dimensional statistics , sparse mixture , higher criticism , adaptive tests , total variation , hellinger distance . </S>"
  ]
}