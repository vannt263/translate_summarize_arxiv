{
  "article_text": [
    "in regression and classification , an omnipresent challenge is the correct prediction in the presence of a huge amount @xmath2 of variables based on a small number @xmath3 of observations , and for any regularized method , one typically expects the performance to increase with increasing observations - to - variables ration @xmath0 . while this is true in the regions @xmath4 and @xmath5 , some estimators exhibit a peaking behavior for @xmath6 , leading to particularly low performance .",
    "as documented in the literature @xcite , this affects all methods that use the ( moore - penrose ) inverse of the sample covariance matrix ( see section [ sec : cov ] for more details ) .",
    "this leads e.g. to the peculiar effect that for linear discriminant analysis , the performance improves in the @xmath6 case if a set of uninformative variables is added to the model . in this note ,",
    "i show that this peaking phenomenon can also occur in scenarios where the moore - penrose inverse is not directly used for computing the model , but in cases where least - squares estimates are used for model selection .",
    "one particularly popular method is the lasso @xcite and its current implementation in the software @xmath7 . as illustrated in section [ sec : lasso ] , its parameterization of the penalty term in terms of a ration of the @xmath8-norm of the lasso solution and the least - squares solution leads to problems when using cross - validation for model selection .",
    "i present a solution in terms of a normalized penalty term .",
    "for a @xmath2-dimensional linear regression model @xmath9 the task is to estimate @xmath10 based on @xmath3 observations @xmath11 . as usual ,",
    "the centered and scaled observations are pooled into @xmath12 and @xmath13 .    in this note ,",
    "i study the performance of the lasso @xcite @xmath14 for a fixed dimensionality @xmath2 and for a varying number @xmath3 of observations .",
    "common sense tells us that the test error is approximately a decreasing function of the observations - to - variables ratio @xmath0 .",
    "however , in several empirical studies , i observe particularly poor results for the lasso in the transition case @xmath1 , leading to a prominent peak in the test error curve at @xmath6 .    in the remainder of this section ,",
    "i illustrate this unexpected behavior on a synthetic data set .",
    "i would like to stress that the peaking behavior is not due to particular choices in the simulation setup , but only depends on the ratio @xmath0 .",
    "i generate @xmath15 observations @xmath16 , where @xmath17 is drawn from a multivariate normal distribution with no collinearity . out of the @xmath18 true regression coefficients @xmath10 , a random subset of size @xmath19",
    "are non - zero and drawn from a univariate distribution on @xmath20 $ ] .",
    "the error term @xmath21 is normally distributed with variance such that the signal - to - noise - ratio is equal to @xmath22 . for the simulation , i sub - sample training sets of sizes @xmath23 .",
    "the sub - sampling is repeated @xmath24 times . on the training set of size @xmath3 , the optimal amount of penalization",
    "is chosen via @xmath24-fold cross - validation .",
    "the lasso solution is then computed on the whole training set of size @xmath3 , and the performance is evaluated by computing the mean squared error on an additional test set of size @xmath25 .",
    "as defined in equation ( [ eq : s]).,width=529,height=264 ]    i use the ` cv.lars ` function of the ` r ` package ` lars ` version @xmath26 @xcite to perform the experiments .",
    "the mean test error over the @xmath24 runs are displayed in the left panel of figure [ fig : peak_lasso ] .",
    "as expected , the test error decreases with the number of observations . for @xmath6",
    "however , there is a striking peak in the test error ( marked by the letter x ) , and the performance is much worse compared to the seemingly more complex scenario of @xmath27 .",
    "we also observe the peaking behavior in the case where @xmath6 in the cross - validation split ( marked by the letter o ) .",
    "the right panel of figure [ fig : peak_lasso ] displays the cross - validated penalty term of the lasso as a function of @xmath3 .",
    "note that in the ` cv.lars ` function , the amount of penalization is not parameterized by @xmath28 but by the more convenient quantity @xmath29\\,.\\end{aligned}\\ ] ]    values of @xmath30 close to @xmath31 correspond to a high value of @xmath32 , and hence to a large amount of penalization .",
    "the right panel of figure [ fig : peak_lasso ] shows that the peaking behavior also occurs for the amount of penalization , measured by @xmath30 .",
    "interestingly , the peak does not occur for @xmath6 , but in the case where the number of observations equals the number of variables in the cross - validation loops .",
    "this peculiar behavior is explained in the two following sections , and i also present a normalization procedure that solves this problem .",
    "it has been reported in the literature @xcite that the pseudo - inverse of the covariance matrix @xmath33 is a particularly bad estimate for the true precision matrix @xmath34 in the case @xmath35 .",
    "the rationale behind this effect is as follows .",
    "the moore - penrose - inverse of the empirical covariance matrix is @xmath36 in particular , in the small sample case , the smallest @xmath37 eigenvalues of the moore - penrose inverse are set to @xmath31 .",
    "this corresponds to cutting off directions with high frequency .",
    "while this introduces an additional bias , it tends to avoid the huge amount of variance that is due to the inversion of small but non - zero eigenvalues . in the transition case @xmath1 , all eigenvalues are @xmath38 ( with some of them very small ) and",
    "the mse is most prominent in this situation .",
    "the striking peaking behavior for @xmath6 is illustrated in e.g. @xcite . as a consequence , any statistical method that uses the pseudo - inverse of",
    "the covariance suffers from the peaking phenomenon .",
    "-norm of the least squares estimate as a function of the number of observations.,width=264,height=264 ]    consequently , the peaking behavior also occurs in ordinary least squares regression , as it uses the pseudo - inverse , @xmath39    this is illustrated in figure [ fig : peak_norm ] . on the training data of size @xmath40 , i compute the @xmath8-norm of least squares estimate .",
    "the figure displays the mean norm over all @xmath24 runs . for @xmath6 ,",
    "the norm is particularly high .",
    "note furthermore that except for @xmath6 , the curve is rather smooth , and small changes in the number of observations only lead to small changes in the @xmath8-norm of the estimate .",
    "this observation is the key to understanding the peaking behavior of the lasso . while for the estimation of the lasso coefficients itself , the pseudo - inverse of the covariance matrix does not occur , it is used for model selection , via the regularization parameter @xmath30 defined in equation ( [ eq : s ] ) .",
    "i elaborate on this in the next section .",
    "let me denote by @xmath41 the number of observations in the @xmath42 cross - validation splits , and by @xmath43 the optimal parameter chosen via cross - validation . as @xmath44 ,",
    "one expects the mse - optimal coefficients @xmath45 computed on a set of size @xmath3 and the mse - optimal coefficients @xmath46 based on a set of size @xmath41 to be similar , i.e.    @xmath47    now , if @xmath48 , then , in each of the @xmath42 cross - validation splits , the number of observations equals the number of dimensions . as the least squares estimate is prone to the peaking behavior ( recall figure [ fig : peak_norm ] ) , we observe @xmath49 this implies that even though the @xmath8-norms of the regression coefficients @xmath50are almost the same , their corresponding values of @xmath30 differ drastically . to put it the other way around :",
    "the optimal @xmath30 found on the cross - validation splits ( where @xmath48 ) is way too small , and it dramatically overestimates the amount of penalization .",
    "this explains the high test error in the case @xmath51 that is indicated by the letter o in figure [ fig : peak_lasso ] .    for @xmath6",
    ", the same argument applies .",
    "the optimal @xmath52 on the cross - validation splits ( where @xmath53 ) underestimates the amount of complexity in the @xmath6 case , which leads to the peak indicated by the letter x in figure [ fig : peak_lasso ] .    to illustrate that the peaking problem is indeed due to the parametrization ( [ eq : s ] )",
    ", i normalize the scaling parameter @xmath30 in the following way .",
    "let me denote by @xmath54 the average over all @xmath42 different @xmath8-norms of the least squares estimates obtained on the @xmath42 cross - validation splits .",
    "furthermore , @xmath55 is the @xmath8-norm of the least squares estimates on the complete training data of size @xmath3 .",
    "the normalized regularization parameter is @xmath56 note that the function ` lars ` returns the least squares solution , hence there are no additional computational costs .    to illustrate the effectiveness of the normalization ,",
    "i re - run the simulation experiments with cross - validation based on the normalized penalty parameter ( [ eq : snormal ] ) .",
    "this function - called ` mylars `  is implemented in the ` r`-package ` parcor ` version 0.1 @xcite .",
    "the results together with the results for the un - normalized parameter [ eq : s ] are displayed in figure [ fig : peak_lasso2 ] .",
    "( black solid line ) and @xmath57 ( blue jagged line ) as defined in equation ( [ eq : s ] ) and ( [ eq : snormal ] ) respectively.,width=529,height=264 ]",
    "the peaking phenomenon is well - documented in the literature , and it effects every estimator that uses the pseudo - inverse of the sample covariance matrix . as i illustrate in this note , this defect in the transition point @xmath1 can also occur in more subtle ways .",
    "for the lasso , the particular parameterization of the penalty term uses least - squares estimates , and it leads to difficulties in model selection .",
    "one can expect similar problems if one e.g. measures the fit of a model in terms of the total variance that it explains , and if the total variance is estimated using least squares . in this case , a normalization as proposed above is advisable .",
    "i observed the peaking phenomenon during the preparation of a paper with juliane schfer and anne - laure boulesteix on regularized estimation of gaussian graphical models @xcite .",
    "together with lukas meier , the three of us discussed the source of the peaking phenomenon in great detail .",
    "my colleagues ryota tomioka , gilles blanchard and benjamin blankertz provided additional material to the discussion and pointed to relevant literature ."
  ],
  "abstract_text": [
    "<S> i briefly report on some unexpected results that i obtained when optimizing the model parameters of the lasso . in simulations with varying observations - to - variables ratio @xmath0 , </S>",
    "<S> i typically observe a strong peak in the test error curve at the transition point @xmath1 . </S>",
    "<S> this peaking phenomenon is well - documented in scenarios that involve the inversion of the sample covariance matrix , and as i illustrate in this note , it is also the source of the peak for the lasso . </S>",
    "<S> the key problem is the parametrization of the lasso penalty  as e.g. in the current ` r ` package ` lars `  and i present a solution in terms of a normalized lasso parameter . </S>"
  ]
}