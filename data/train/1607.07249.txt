{
  "article_text": [
    "the semantic web  @xcite and its linked data  @xcite movement have brought us many great , interlinked and freely available machine readable rdf  @xcite datasets , often summarized in the linking open data cloud .",
    "being extracted from wikipedia and spanning many different domains , dbpedia  @xcite forms one of the most central and best interlinked of these datasets .    nevertheless , even with all this easily available data , using it is still very challenging : for a new question , one needs to know about the available datasets , which ones are best suited to answer the question , know about the way knowledge is modelled inside them and which vocabularies are used , before even attempting to formulate a suitable sparql query to return the desired information .",
    "the noise of real world datasets adds even more complexity to this . in this paper",
    "we present a graph pattern learning algorithm that can help to identify sparql queries for a relation @xmath4 between node pairs @xmath5 in a given knowledge graph @xmath6 is a set of rdf triples , typically accessible via a given sparql endpoint . ] , where @xmath7 is a source node and @xmath8 a target node .",
    "@xmath4 can for example be a simple relation such as `` given a capital @xmath7 return its country @xmath8 '' @xmath9 or a complex one such as `` given a stimulus @xmath7 return a response @xmath8 that a human would associate '' @xmath10 .    to learn queries for @xmath4 from @xmath6 , without any prior knowledge about the modelling of @xmath4 in @xmath6",
    ", we allow users to compile a ground truth set of example source - target - pairs @xmath11 as input for our algorithm .",
    "for example , for relation @xmath9 between capital cities and their countries , the user could generate a ground truth list @xmath12 ( , ) , ( , ) , ( , ) @xmath13 .",
    "given @xmath14 and the dbpedia sparql endpoint , our graph pattern learner then learns a set of graph patterns @xmath15 such as :    @xmath16 : \\ { }    @xmath17 : \\ { }    in this paper , a graph pattern @xmath18 is an instance of the infinite set of sparql basic graph patterns @xmath19 .",
    "each @xmath20 has a corresponding sparql ask and select query .",
    "we denote their execution against @xmath6 as @xmath21 and @xmath22 .",
    "the graph patterns can contain sparql variables , out of which we reserve and as special ones .",
    "a mapping @xmath23 can be used to bind variables in @xmath20 before execution .",
    "the resulting learned patterns can either be inspected or be used to predict targets by selecting all bindings for given a source node @xmath24 : @xmath25 for example , given the source node the pattern @xmath16 can be used to predict @xmath26 .    the remainder of this paper is structured as follows : we present related work in section  [ sec : relwork ] , before describing our graph pattern learner in detail in section  [ sec : gp_learner ] . in sections",
    "[ sec : visualisation ] and [ sec : prediction ] we will then briefly describe visualisation and prediction techniques before evaluating our approach in section  [ sec : eval ] .",
    "to the best of our knowledge , our algorithm is the first of its kind .",
    "it is unique in that it can learn a set of sparql graph patterns for a given input list of source - target - pairs directly from a given sparql endpoint .",
    "additionally , it can cope with scenarios in which there is not a single pattern that covers all source - target - pairs .",
    "many other algorithms exist , which learn vector space representations from knowledge graphs .",
    "an excellent overview of such algorithms can be found in @xcite .",
    "we are however not aware that any of these algorithms have the ability of returning a list of sparql graph patterns that cover an input list of source - target - pairs .",
    "there are other approaches that help formulating sparql queries , mostly in an interactive fashion such as relfinder @xcite or autosparql  @xcite .",
    "their focus however lies on finding relationships between a short list of entities ( not source - target - pairs ) or interactively formulating sparql queries for a list of entities of a single kind",
    ". they can not deal with entities of different kinds .",
    "sparql pattern learning , there is an approach for pattern based feature construction  @xcite that focuses on learning sparql patterns to use them as features for binary classification of entities .",
    "it can answer questions such as : does an entity belong to a predefined class ?",
    "in contrast to that , our approach focuses on learning patterns between a list of source - target - pairs for entity prediction : given a source entity predict target entities . to simulate target entity prediction for a single given source with binary classification",
    ", one would need to train @xmath27 classifiers , one for @xmath27 potential target entities . in the context of mining patterns for human associations and linked data , we previously focused on collecting datasets of semantic associations directly from humans @xcite , ranking existing facts according to association strengths @xcite and mapping the edinburgh associative thesaurus @xcite to dbpedia @xcite .",
    "none of these previous works directly focused on identifying existing patterns for human associations in existing datasets .",
    "the outline of our graph pattern learner is similar to the generic outline of evolutionary algorithms : it consists of individuals ( in our case sparql graph patterns @xmath28 ) , which are evaluated to calculate their fitness",
    ". the fitter an individual is , the higher its chance to survive and reach the next generation .",
    "the individuals of a generation are also referred to as population . in each generation",
    "there is a chance to mate and mutate for each of the individuals .",
    "a population can contain the same individual ( graph pattern ) several times , causing fitter individuals to have a higher chance to mate and mutate over several generations .    as mentioned in the introduction ,",
    "the training input of our algorithm is a list of ground truth source - target - pairs @xmath29 .",
    "due to size limitations , we will focus on the most important aspects of our algorithm in the following . for further detail",
    "please see our website where you can find the source - code , visualisation and other complementary material .      before describing the realisation of the components of our evolutionary learner , we want to introduce our concept of coverage .",
    "we say that a graph pattern @xmath30 covers , models or fulfils a source - target - pair @xmath31 if the evaluation of its sparql ask query returns true : @xmath32    our algorithm is not limited to learning a single best pattern for a list of ground truth pairs , but it can learn multiple patterns which together cover the list .",
    "we realise this by invoking our evolutionary algorithm in several _ runs_. in each run a full evolutionary algorithm is executed ( with all its generations ) . after each run",
    "the resulting patterns are added to a global list of results . in the following runs ,",
    "all ground truth pairs which are already covered by the patterns from previous runs become less rewarding for a newly learnt pattern to cover . over",
    "its runs our algorithm will thereby re - focus on the left - overs , which allows us to maximise the coverage of all ground truth pairs with good graph patterns .      in order to evaluate the fitness of a pattern , we define the following dimensions to capture what makes a pattern `` good '' .    *",
    "high _ recall _ : + a good pattern fulfils as many of the given ground truth pairs @xmath14 as possible : @xmath33 * high _ precision _ : + a good pattern should also be precise . for each individual ground truth",
    "pair @xmath34 we can define the precision as : @xmath35 the target @xmath36 should be in the returned result list and if possible nothing else . in other words , we are not searching for patterns that return thousands of potentially wrong target for a given source . over all ground truth pairs",
    ", we can define the average precision for @xmath20 via the inverse of the average result lengths : @xmath37 * high _ gain _ : + a pattern discovered in run @xmath38 is better if it covers those ground truth pairs @xmath39 that are nt covered with high precisions in previous runs ( @xmath40 already : @xmath41 similarly , the potentially remaining gain can be computed as : @xmath42 * no _ over - fitting _ : + while precision is to be maximised , a good pattern should not _ over - fit _ to a single source or target from the training input .",
    "* short _ pattern length _ and low _ variable count _ : + if all other considerations are similar , then a shorter pattern or one with less variables is preferable . note , that this is a low priority dimension . a good pattern is not restricted to a shortest path between and .",
    "good patterns can be longer and can have edges off the connecting path ( e.g. , see @xmath17 in section  [ sec : intro ] ) . *",
    "low execution _ time _ & _ timeout _ : + last but not least , to have any practical relevance , good patterns should be executable in a short _ time_. especially during the training phase , in which many queries are performed that take too long , we need to make sure to early terminate such queries on both , the graph pattern learner and the endpoint ( cf . section  [ sec : real_world_considerations ] ) . in case",
    "the query was aborted due to a _ timeout _ and only a partial result obtained , it should not be trusted .",
    "based on these considerations , we define the _ fitness _ of an individual graph pattern as a tuple of real numbers with the following optimization directions . when comparing the fitness of two patterns , the fitness tuples for now are compared lexicographically .    1 .",
    "* remains * ( max ) : remaining precision sum @xmath43 in the current run @xmath38 ( see section  [ sec : coverage ] ) .",
    "patterns found in earlier runs are considered better .",
    "* score * ( max ) : a derived attribute combining gain with a configurable multiplicative punishment for over - fitting patterns .",
    "* gain * ( max ) : the summed gained precision over the remains of the current run @xmath38 : @xmath44 . in case of timeouts or incomplete patterns",
    "the gain is set to 0 .",
    "* @xmath45-measure * ( max ) : @xmath45-measure for precision and recall of this pattern .",
    "* average result lengths * ( min ) : @xmath46 .",
    "* recall ( ground truth matches ) * ( max ) : @xmath47 . 7 .   * pattern length * ( min ) : the number of triples this pattern contains . 8 .",
    "* pattern variables * ( min ) : the number of variables this pattern contains . 9 .   * timeout * ( min ) : punishment term for timeouts ( 0.5 for a soft and 1.0 for a hard timeout ) ( see section  [ sec : real_world_considerations ] and gain ) . 10 . *",
    "query time * ( min ) : the evaluation time in seconds .",
    "this is particularly relevant since it hints at the real complexity of the pattern .",
    "i.e. , a pattern may objectively have a small number of triples and variables , but its evaluation could involve a large portion of the dataset .      in order to start any evolutionary algorithm an initial population needs to be generated .",
    "the main objective of the first population is to form a starting point from which the whole search space is reachable via mutations and mating over the generations . while the initial population is not meant to immediately solve the whole problem , a poorly chosen initial population results in a lot of wasted computation time .",
    "the starting point of our algorithm are single triple sparql bgp queries , consisting only of variables with at least a and variable , e.g. :    \\ { }    while having a small chance of survival ( direct evaluation would typically yield bad fitness ) , such patterns can re - combine ( see mating in section  [ sec : mating ] ) with other patterns to form good and complete patterns in later generations .    for prediction capabilities , we are searching graph patterns which connect and",
    ", our algorithm mostly fills the initial population with path patterns of varying lengths @xmath48 between and .",
    "initially such a path pattern purely consists of variables and is directed from source to target :    \\ { }    for example a pattern of desired length of @xmath49 looks like this :    \\ { }    as longer patterns are less desirable , they are generated with a lower probability .",
    "furthermore , we randomly flip each edge of the generated patterns , in order to explore edges in any direction .    in order to reduce the high complexity and noise introduced by patterns only consisting of variables",
    ", we built in a high chance to immediately subject them to the fix variable mutation ( see section  [ sec : mutation ] ) .      in each generation",
    "there is a configurable chance for two patterns to mate in order to exchange information . in our algorithm",
    "this is implemented in a way that mating always creates two children , having the benefit of keeping the amount of individuals the same .",
    "each child has a dominant and a recessive parent .",
    "the child will contain all triples that occur in both parents .",
    "additionally , there is a high chance to select each of the remaining triples from the dominant parent and a low chance to select each of the remaining triples from the recessive parent . by this",
    "the children have the same expected length as their parents .",
    "furthermore , as variables from the recessive parent could accidentally match variables already being in the child , and this can be beneficial or not , we add a 50  % chance to rename such variables before adding the triples .      besides mating , which exchanges information between two individuals ,",
    "information can also be gained by mutation .",
    "each individual in a population has a configurable chance to mutate by the following ( non exclusive ) mutation strategies .",
    "currently , all but one of the mutation operations can be performed on the pattern itself ( local ) without issuing any sparql queries .",
    "the mutation operations also have different effects on the pattern itself ( grow , shrink ) and on its result size ( harden , loosen ) .    *",
    "* introduce var * select a component ( node or edge ) and convert it into a variable ( loosen ) ( local ) * * split var * select a variable and randomly split it into 2 vars ( grow , loosen ) ( local ) * * merge var * select 2 variables and merge them ( shrink , harden ) ( local ) * * del triple * delete a triple statement ( shrink , loosen ) ( local ) * * expand node * select a node , and add a triple from its expansion ( grow , harden ) ( local for now ) * * add edge * select 2 nodes , add an edge in between if available ( grow , harden ) ( local for now ) * * increase dist * increase distance between source and target by moving one a hop away ( grow ) ( local ) * * simplify pattern * simplify the pattern , deleting unnecessary triples ( shrink ) ( local ) ( cf .",
    "section  [ sec : pattern_simplification ] ) * * fix var * select a variable and instantiate it with an iri , bnode or literal that can take its place ( harden ) ( sparql ) ( see below )    in a single generation sequential mutation ( by different strategies in the order as above ) is possible .",
    "we can generally say that introducing a variable loosens a pattern and fixing a variable hardens it .",
    "patterns which are too loose will generate a lot of candidates and take a long time to evaluate .",
    "patterns which are too hard will generate too few solutions , if any at all .",
    "very big patterns , even though very specific can also exceed reasonable query and evaluation times .",
    "unlike the other mutations , the fix var mutation is the only one which makes use of the underlying dataset via the sparql endpoint @xmath6 , in order to instantiate variables with an iri , bnode or literal .",
    "as it is one of the most important mutations and also because performing sparql queries is expensive , it can immediately return several mutated children .    for",
    "a given pattern @xmath20 we randomly select one of its variables ( excluding and ) .",
    "additionally , we sample up to a defined number of source - target - pairs from the ground truth which are not well covered yet ( high potential gain ) . for each of these sampled pairs @xmath50",
    "we issue a sparql select query of the form :    \\ { }    we collect the possible instantiations for , count them over all queries and randomly select ( with probabilities according to their frequencies ) up to a defined number of them .",
    "each of the selected instantiations forms a separate child by replacing in the current pattern .",
    "after each generation the next generation is formed by the surviving ( fittest ) individuals from @xmath27 tournaments of @xmath51 randomly sampled individuals from the previous generation .",
    "we also employ two techniques , to counter population degeneration in local maxima and make our algorithm robust ( even against non - optimal parameters ) :    * in each generation we re - introduce a small number of newly generated initial population patterns ( see section  [ sec : init_population ] ) . *",
    "each generation updates a hall of fame , which will preserve the best patterns ever encountered over the generations . in each generation",
    "a small number of the best of these all - time best patterns is re - introduced .      in the following",
    ", we will briefly discuss practical problems that we encountered and necessary optimizations we used to overcome them .",
    "we implemented our graph pattern learner with the help of the deap ( distributed evolutionary algorithms in python ) framework  @xcite .",
    "the single most important optimization of our algorithm lies in the reduction of the amount of issued queries by using batch queries .",
    "this mostly applies to the queries for fitness evaluation ( section  [ sec : fitness ] ) .",
    "it is a lot more efficient to run several sub - queries in one big query and to only transport the ground truth pairs to the endpoint once ( via ) , than to ask for each result separately .",
    "another mandatory optimization involves the use of timeouts and limits for all queries , even if they usually only return very few results in a short time .",
    "we found that a few run - away queries can quickly lead to congestion of the whole endpoint and block much simpler queries .",
    "timeouts are also especially useful as a reliable proxy to exclude too complicated graph patterns .",
    "even seemingly simple patterns can take a very long time to evaluate based on the underlying dataset and its distribution .",
    "apart from timeouts we use a filter which checks if mutants and children are actually desirable ( e.g. , length and variable count in boundaries , pattern is complete and connected ) , meaning fit to live , before evaluating them . if not , the respective parent takes their place in the new population , allowing for a much larger part of the population to be viable .",
    "two other crucial optimizations to reduce the overall run - time of the algorithm are parallelization and client side caching .",
    "evolutionary algorithms are easy to parallelize via parallel evaluation of all individuals , but in our case the sparql endpoint quickly becomes the bottleneck .",
    "ignoring the limits of the queried endpoint will resemble a denial of service attack . for most of our experiments we hence use an internal lod cache with exclusive access for our learning algorithm . in case the algorithm is run against public endpoints we suggest to only use a single thread in order not to disturb their service ( fair use ) .",
    "client side caching further helps to reduce the time spent on evaluating graph patterns , by only evaluating them once , should the same pattern be generated by different sequences of mutation and mating operations . to identify equivalent patterns despite different syntactic surface forms",
    ", we had to solve sparql bgp canonicalization ( -complete ) .",
    "we were able to to reduce the problem to rdf graph canonicalization and achieve good practical run - times with rgda1  @xcite .    in the context of caching ,",
    "one other important finding is that many sparql endpoints ( especially the widely used openlink virtuoso ) often return incomplete and thereby non - deterministic results by default .",
    "unlike many other search algorithms , an evolutionary algorithm has the benefit that it can cope well with such non - determinism .",
    "hence , when caching is used , it is helpful to reduce , but not completely remove redundant queries .",
    "last but not least , as our algorithm can create patterns that are unnecessarily complex , it is useful to simplify them .",
    "we developed a pattern simplification algorithm , which given a complicated graph pattern @xmath52 finds a minimal equivalent pattern @xmath53 with the same result set wrt .",
    "the and variables .",
    "the simplification algorithm removes unnecessary edges , such as redundant parallel variable edges , edges between and behind fixed nodes and unrestricting leaf branches .",
    "after presenting the main components of our evolutionary algorithm in the previous section , we will now briefly present an interactive visualisation .",
    "as the learning of our evolutionary algorithm can produce many graph patterns , the visualisation allows to quickly get an overview of the resulting patterns in different stages of the algorithm .",
    "pair from the ground truth training set .",
    "the darker its colour the higher the precision for the ground truth pair .",
    ", title=\"fig : \" ]   pair from the ground truth training set .",
    "the darker its colour the higher the precision for the ground truth pair .",
    ", title=\"fig : \" ]    [ fig : visualisation_coverage ]    figure  [ fig : visualisation_gp_results ]  ( left ) shows a screen shot of the visualisation of a single learned graph pattern . in the sidebar the user can select between individual generations , the results of a whole run or the overall results ( default ) to inspect the outcomes at various stages of the algorithm .",
    "afterwards , the individual result graph patterns can be selected .",
    "below these selection options the user can inspect statistics about the selected graph pattern including its fitness , a list of matching training ground truth pairs and the corresponding sparql select query for the pattern .",
    "links are provided to perform live queries on the sparql endpoint .    at each of the stages ,",
    "the user can also get an overview of the precision coverage of a single pattern ( as can be seen in figure  [ fig : visualisation_coverage ]  ( right ) ) or the accumulated coverage over all patterns .",
    "as already mentioned in the introduction , the learned patterns can be used to predict targets for a given source .",
    "the basic idea is to insert a given source @xmath24 in place of the variable in each of the learned patterns @xmath54 and execute a sparql select query over the variable ( c.f .",
    ", @xmath55 in section  [ sec : intro ] ) .      while interesting for manual exploration , for practical prediction purposes the amount of learned graph patterns",
    "can easily become too large by discovering many very similar patterns that are only differing in minor aspects .",
    "one realisation from visualising the resulting patterns @xmath20 , is that we can use their precision vectors wrt .",
    "the ground truth pairs to cluster graph patterns .",
    "the @xmath56-th component of @xmath57 is defined by the precision value corresponding to the @xmath56-th ground truth source - target - pair @xmath58 : @xmath59    we employ several standard clustering algorithms on @xmath57 and select the best patterns @xmath60 in each cluster as representatives to reduce the amount of queries . by default",
    "our algorithm applies all of these clustering techniques and then selects the one which minimises the precision loss at the desired number of queries to be performed during prediction . in our tests",
    "we could observe , that clustering ( e.g. , with hierarchical scaled euclidean ward clustering ) allows us to reduce the number of performed sparql queries to 100 for all practical purposes with a precision loss of less than @xmath61 .",
    "when used for prediction , each graph pattern @xmath20 creates an unordered list of possible target nodes @xmath62 for an inserted source node @xmath24 .",
    "we evaluated the following fusion strategies to combine and rank the returned target candidates @xmath63 ( higher fusion value means lower rank ) :    * * target occurrences * : a simple occurrence count of each of the targets over all graph patterns . * * scores * : sum of all graph pattern scores ( from the graph pattern s fitness ) for each returned target . * * f - measures * : sum of all graph pattern @xmath45-measures ( from the graph pattern s fitness ) for each returned target . * * gp precisions * : sum of all graph pattern precisions ( from the graph pattern s fitness ) for each returned target . *",
    "* precisions * : sum of the actual precisions per graph pattern in this prediction .    by default",
    "our algorithm will calculate them all , allowing the user to pick the best performing fusion strategy for their use - case .",
    "in order to evaluate our graph pattern learner , we performed several experiments which we will describe in the following .",
    "we ran our experiments against a local virtuoso 7.2 sparql endpoint containing over @xmath64 g triples , from many central datasets of the lod cloud , denoted as @xmath6 in the following .",
    "one of our claims is that our algorithm can learn good sparql queries for a relation @xmath4 represented by a set of ground truth source - target - pairs @xmath14 . in order to evaluate this",
    ", we started with simple relations such as `` given a capital @xmath7 return its country @xmath8 '' ( see @xmath9 in section  [ sec : intro ] ) .",
    "for each @xmath4 , we used a generating sparql query @xmath65 ( such as @xmath17 from section  [ sec : intro ] ) to generate @xmath66 , then executed our graph pattern learner @xmath15 and checked if @xmath65 was in the resulting patterns : @xmath67    the result of these experiments is that our algorithm is able to re - identify such simple , readily modelled relations @xmath4 in @xmath68 of our test cases ( typically within the first run , so the first 3 minutes ) . while this might sound astonishing , it is merely a test that our algorithm can perform the simplest of its tasks : if there is a single sparql bgp pattern @xmath20 that models the whole training list @xmath14 in @xmath6 , then our algorithm is quickly able to find it via the fix var mutation in section  [ sec : mutate_fix_var ] . due to the page limit , we omit further details and instead turn to a more complex relation in the next section .",
    "two additional claims are that our algorithm can learn a set of patterns , which cover a complex relation @xmath4 that is not readily modelled in @xmath6 , and that we can use the resulting patterns for prediction .",
    "hence , in the following we focus on one such complex relation @xmath10 : human associations . we will present some of the identified patterns and then evaluate the prediction quality .",
    "human associations are an important part of our thinking process .",
    "an _ association _ is the mental connection between two ideas : a _ stimulus _",
    "( e.g. , `` pupil '' ) and a _ response _",
    "( e.g. , `` eye '' ) .",
    "we call such associations _ strong associations _ if more than 20  % of people agree on the response .    in the following , we focus on a dataset of 727 strong human associations ( corresponding to @xmath69 k raw associations ) from the edinburgh associative thesaurus  @xcite that we previously already mapped to dbpedia entities  @xcite .",
    "the dataset contains stimulus - response - pairs such as ( , ) , ( , ) and ( , ) .",
    "we randomly split our 727 ground truth pairs into a training set @xmath70 of 655 and a test set @xmath71 of 72 pairs ( 10  % random split ) .",
    "all training , visualising and development has been performed on the training set in order to reduce the chance of over - fitting our algorithm to our ground truth .",
    "we ran the algorithm ( @xmath72 ) on @xmath6 with a population size of 200 , a maximum of 20 generations each in a maximum of 64 runs .",
    "the first 5 runs of our algorithm are typically completed within 3 , 6 , 9 , 13 and 15 minutes . in the first couple of minutes",
    "all of the very simple patterns that model a considerable fraction of the training set s pairs are found . with the mentioned settings",
    "the algorithm will terminate after around 3 hours .",
    "it finds roughly 530 graph patterns with a score > 2 ( cf .",
    "section  [ sec : fitness ] ) .",
    "due to the page limit , we will briefly mention only 3 notable patterns from the resulting learned patterns in this paper .",
    "we invite the reader to explore the full results online with the interactive visualisation presented in section  [ sec : visualisation ] .",
    "the three notable patterns we want to present here are :    \\ { }    \\ { }    \\ { }    the first two are intuitively understandable patterns which typically are amongst the top patterns .",
    "the first one shows that human associations often seem to be represented via http://purl.org/linguistics/gold/hypernym [ ] in dbpedia ( the response is often a hypernym ( broader term ) for the stimulus ) .",
    "the second one shows that associations often correspond to bidirectionally linked wikipedia articles .",
    "the third pattern represents a whole class of intra - dataset learning by making use of a connection of the to babelnet s http://www.w3.org/2004/02/skos/core#exactmatch [ ] .",
    "as human associations are not readily modelled in dbpedia , it is difficult to assess the quality of the learned patterns @xmath20 directly .",
    "hence , we evaluate the quality indirectly via their prediction quality on the test - set @xmath71 .    for each of the @xmath73 we generate a ranked target list @xmath74 $ ] of target predictions @xmath75 .",
    "the list is the result of one of the fusion variants ( cf .",
    "section  [ sec : fusion ] ) after clustering ( cf .",
    "section  [ sec : query_reduction ] ) .",
    "for evaluation , we can then check the rank @xmath76 of @xmath77 in @xmath78 ( lower ranks are better ) . if @xmath79 , we set @xmath80 .",
    "an example of a ranked target prediction list ( for the fusion method _ precisions _ ) for source @xmath81 is the ranked list : @xmath82 [ , , , , , , , , , ] . in this case",
    "the ground truth target @xmath83 is at rank @xmath84 .",
    "as we can see most of the results are relevant as associations to humans .",
    "nevertheless , for the purpose of our evaluation , we will only consider the single @xmath77 corresponding to a @xmath85 as relevant and all other @xmath75 as irrelevant . based on the ranked result lists",
    ", we can calculate the recall@k due to the fact that we only have 1 relevant target per result of any @xmath86 .",
    "] , mean average precision ( map ) and normalised discounted cumulative gain of the various fusion variants over the whole test set @xmath71 , as can be seen in table  [ tbl : eval ] and figure  [ fig : recall ] .",
    "we also calculate these metrics for several baselines , which try to predict the target nodes from the 1-neighbourhood ( bidirectionally , incoming or outgoing ) by selecting the neighbour with the highest pagerank , hits score , in- and out - degree  @xcite . as can be seen , all our fusion strategies significantly outperform the baselines .",
    ".recall@k , map and ndcg for our fusion variants and against baselines . [ cols=\"<,^,^,^,^,^,^,^,^\",options=\"header \" , ]     k over the different fusion variants and against baselines . ]",
    "[ sec : future_work ] in this paper we presented an evolutionary graph pattern learner . the algorithm can successfully learn a set of patterns for a given list of source - target - pairs from a sparql endpoint .",
    "the learned patterns can be used to predict targets for a given source .",
    "we use our algorithm to identify patterns in dbpedia for a dataset of human associations .",
    "the prediction quality of the learned patterns after fusion reaches a recall@xmath8710 of 63.9  % and map of 39.9  % , and significantly outperforms pagerank , hits and degree based baselines .",
    "the algorithm , the used datasets and the interactive visualisation of the results are available online .    in the future",
    ", we plan to enhance our algorithm to support literals in the input source - target - pairs , which will allow us to learn patterns directly from lists of textual inputs .",
    "further , we are investigating mutations , for example to introduce constraints .",
    "we also plan to investigate the effects of including negative samples ( currently we only use positive samples and treat everything else as negative ) .",
    "additionally , we plan to employ more advanced late fusion techniques , in order to learn when to trust the prediction of which pattern .",
    "as this idea is conceptually close to interpreting the learned patterns as a feature vector ( with understandable and executable patterns to generate target candidates ) , we plan to investigate combinations of our algorithm with approaches that learn vector space representations from knowledge graphs .",
    "bizer , c. , lehmann , j. , kobilarov , g. , auer , s. , becker , c. , cyganiak , r. , hellmann , s. : dbpedia - a crystallization point for the web of data .",
    "web semantics : science , services and agents on the world wide web 7(3 ) , 154165 ( 2009 )        hees , j. , khamis , m. , biedert , r. , abdennadher , s. , dengel , a. : collecting links between entities ranked by human association strengths . in : eswc . vol .",
    "7882 , pp .",
    "springer lncs , montpellier , france ( 2013 ) ,    hees , j. , roth - berghofer , t. , biedert , r. , adrian , b. , dengel , a. : betterrelations : using a game to rate linked data triples . in : ki 2011 : advances in artificial intelligence .",
    ". 134138 .",
    "springer ( 2011 )    hees , j. , roth - berghofer , t. , biedert , r. , adrian , b. , dengel , a. : betterrelations : collecting association strengths for linked data triples with a game . in : search computing ,",
    ". 223239 .",
    "springer lncs ( 2012 )          kiss , g.r . , armstrong , c. , milroy , r. , piper , j. : an associative thesaurus of english and its computer analysis . in : the computer and literary studies ,",
    ". 153165 .",
    "edinburgh university press , edinburgh , uk ( 1973 )"
  ],
  "abstract_text": [
    "<S> efficient usage of the knowledge provided by the linked data community is often hindered by the need for domain experts to formulate the right sparql queries to answer questions . </S>",
    "<S> for new questions they have to decide which datasets are suitable and in which terminology and modelling style to phrase the sparql query .    in this work we present an evolutionary algorithm to help with this challenging task . given a training list of source - target node - pair examples </S>",
    "<S> our algorithm can learn patterns ( sparql queries ) from a sparql endpoint . </S>",
    "<S> the learned patterns can be visualised to form the basis for further investigation , or they can be used to predict target nodes for new source nodes .    amongst others , </S>",
    "<S> we apply our algorithm to a dataset of several hundred human associations ( such as `` circle - square '' ) to find patterns for them in dbpedia . </S>",
    "<S> we show the scalability of the algorithm by running it against a sparql endpoint loaded with @xmath0 billion triples . </S>",
    "<S> further , we use the resulting sparql queries to mimic human associations with a mean average precision ( map ) of @xmath1 and a recall@xmath2 of @xmath3 . </S>"
  ]
}