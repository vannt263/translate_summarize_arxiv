{
  "article_text": [
    "robbins and monro ( @xcite ) introduced the stochastic approximation algorithm to solve the integration equation @xmath0 where @xmath1 is a parameter vector and @xmath2 , @xmath3 , is a density function depending on @xmath4 .",
    "the @xmath5 and @xmath6 denote the dimensions of @xmath4 and @xmath7 , respectively .",
    "the stochastic approximation algorithm is an iterative recursive algorithm , whose each iteration consists of two steps :      * generate @xmath8 , where @xmath9 indexes the iteration . *",
    "set @xmath10 , where @xmath11 is called the gain factor .",
    "the stochastic approximation algorithm is often studied by rewriting it as follows : @xmath12,\\ ] ] where @xmath13 corresponds to the mean effect of @xmath14 , and @xmath15 , @xmath16 is called the observation noise . in the literature of stochastic approximation",
    ", @xmath17 is also called the mean field function .",
    "it is well known that the optimal convergence rate of ( [ moteq1 ] ) can be achieved with @xmath18 , where @xmath19 , and @xmath20 denotes the zero point of @xmath17 . in this case , ( [ moteq1 ] ) is reduced to newton s algorithm .",
    "unfortunately , it is often impossible to use this algorithm , as the matrix @xmath21 is generally unknown .",
    "although an optimal convergence rate of @xmath22 can not be obtained in general , in a sequence of fundamental papers @xcite , @xcite and @xcite showed that the trajectory averaging estimator is asymptotically efficient ; that is , @xmath23 can converge in distribution to a normal random variable with mean @xmath20 and covariance matrix @xmath24 , where @xmath24 is the smallest possible covariance matrix in an appropriate sense .",
    "the trajectory averaging estimator requires @xmath25 to be relatively large , decreasing slower than @xmath26 .",
    "as discussed by @xcite , trajectory averaging is based on a paradoxical principle : a slow algorithm having less than optimal convergence rate must be averaged .",
    "recently , the trajectory averaging technique has been further explored in a variety of papers [ see , e.g. , @xcite , kushner and yang ( @xcite , @xcite ) , @xcite , wang , chong and kulkarni ( @xcite ) , @xcite , @xcite and @xcite ] with different assumptions for the observation noise . however , up to our knowledge , it has not yet been explored for stochastic approximation mcmc ( samcmc ) algorithms [ @xcite , @xcite , @xcite , @xcite , @xcite ] .",
    "the stochastic approximation mcmc algorithms refer to a class of stochastic approximation algorithms for which the sample is generated at each iteration via a markov transition kernel ; that is , @xmath27 is generated via a family of markov transition kernel @xmath28 controlled by @xmath29 .",
    "recently , the stochastic approximation mcmc algorithms have been used in statistics for solving maximum likelihood estimation problems [ younes ( @xcite , @xcite ) , @xcite , @xcite , @xcite ] , and for general simulation and optimizations [ @xcite , @xcite ] .",
    "it is worth to point out that in comparison with conventional mcmc algorithms , for example , the metropolis ",
    "hastings algorithm [ @xcite , @xcite ] , parallel tempering [ @xcite ] , and simulated tempering [ @xcite , @xcite ] , the stochastic approximation monte carlo ( samc ) algorithm [ @xcite ] has significant advantages in simulations of complex systems for which the energy landscape is rugged . as explained later ( in section [ sec3 ] ) , samc is essentially immune to the local trap problem due to its self - adaptive nature inherited from the stochastic approximation algorithm .",
    "samc has been successfully applied to many statistical problems , such as @xmath30-value evaluation for resampling - based tests [ @xcite ] , bayesian model selection [ @xcite , @xcite ] and spatial model estimation [ liang ( @xcite ) ] , among others .    in this paper",
    ", we explore the theory of trajectory averaging for stochastic approximation mcmc algorithms , motivated by their wide applications .",
    "although chen ( @xcite , @xcite ) considered the case where the observation noise can be state dependent , that is , the observation noise @xmath31 depends on @xmath32 , their results are not directly applicable to the stochastic approximation mcmc algorithms due to some reasons as explained in section [ sec5 ] . the theory established by @xcite",
    "can potentially be extended to the stochastic approximation mcmc algorithm , but , as mentioned in kushner and yin [ ( @xcite ) , page 375 ] the extension is not straightforward and more work needs to be done to deal with the complicated structure of the markov transition kernel . in this paper , we propose a novel decomposition of the observation noise for the stochastic approximation mcmc algorithms .",
    "based on the proposed decomposition , we show the trajectory averaging estimator is asymptotically efficient for the stochastic approximation mcmc algorithms , and then apply this result to the samc algorithm .",
    "these results are presented in lemma [ lem3 ] , theorems [ efftheorem ] and [ samcavetheorem ] , respectively .",
    "the application of the trajectory averaging technique to other stochastic approximation mcmc algorithms , for example , a stochastic approximation mle algorithm for missing data problems , is also considered in the paper .",
    "the remainder of this paper is organized as follows . in section [ sec2 ] , we present our main theoretical result that the trajectory averaging estimator is asymptotically efficient for the stochastic approximation mcmc algorithms . in section [ sec3 ] , we apply the trajectory averaging technique to the samc algorithm . in section [ sec4 ] , we apply the trajectory averaging technique to a stochastic approximation mle algorithm for missing data problems . in section [ sec5 ] ,",
    "we conclude the paper with a brief discussion .",
    "to show the convergence of the stochastic approximation algorithm , restrictive conditions on the observation noise and mean field function are required .",
    "for example , one often assumes the noise to be mutually independent or to be a martingale difference sequence , and imposes a sever restriction on the growth rate of the mean field function .",
    "these conditions are usually not satisfied in practice .",
    "see chen [ ( @xcite ) , chapter 1 ] for more discussions on this issue . to remove the growth rate restriction on the mean field function and to weaken the conditions imposed on noise , @xcite proposed a varying truncation version for the stochastic approximation algorithm .",
    "the convergence of the modified algorithm can be shown for a wide class of the mean filed function under a truly weak condition on noise ; see , for example , @xcite and @xcite .",
    "the latter gives a proof for the convergence of the modified algorithm with markov state - dependent noise under some conditions that are easy to verify .",
    "following @xcite , we consider the following varying truncation stochastic approximation mcmc algorithm .",
    "let @xmath33 be a sequence of compact subsets of @xmath34 such that @xmath35 where int(@xmath36 ) denotes the interior of set @xmath36 .",
    "let @xmath25 and @xmath37 be two monotone , nonincreasing , positive sequences .",
    "let @xmath38 be a subset of @xmath39 , and let @xmath40 be a measurable function which maps a point @xmath41 in @xmath42 to a random point in @xmath43 ; that is , both @xmath7 and @xmath4 will be reinitialized in @xmath44 .",
    "as shown in lemma [ lem3 ] , for the stochastic approximation mcmc algorithm , when the number of iterations becomes large , the observation noise @xmath45 can be decomposed as @xmath46 where @xmath47 forms a martingale difference sequence , and the expectation of the other two terms will go to zero in certain forms . in theorems [ contheorem ] and [ efftheorem ] , we show that @xmath48 leads to the asymptotic normality of the trajectory averaging estimator @xmath49 , and @xmath50 and @xmath51 can vanish or be ignored when the asymptotic distribution of @xmath52 is considered .",
    "let @xmath53 denote the number of truncations performed until iteration @xmath9 and @xmath54 .",
    "the varying truncation stochastic approximation mcmc algorithm starts with a random choice of @xmath55 in the space @xmath56 , and then iterates between the following steps :      * draw sample @xmath57 with a markov transition kernel , @xmath58 , which admits @xmath59 as the invariant distribution . *",
    "set @xmath60 . * if @xmath61 and @xmath62 , where @xmath63 denote the euclidean norm of the vector @xmath64 , then set @xmath65 and @xmath66 ; otherwise , set @xmath67 and @xmath68 .",
    "as depicted by the algorithm , the varying truncation mechanism works in an adaptive manner as follows : when the current estimate of the parameter wanders outside the active truncation set or when the difference between two successive estimates is greater than a time - dependent threshold , then the algorithm is reinitialized with a smaller initial value of the gain factor and a larger truncation set .",
    "this mechanism enables the algorithm to select an appropriate gain factor sequence and an appropriate starting point , and thus to confine the recursion to a compact set ; that is , the number of reinitializations is almost surely finite for every @xmath69 .",
    "this result is formally stated in theorem [ lem50 ] , which plays a crucial role for establishing asymptotic efficiency of the trajectory averaging estimator .    regarding the varying truncation scheme",
    ", one can naturally propose many variations .",
    "for example , one may not change the truncation set when only the condition @xmath61 is violated , and , instead of jumping forward in a unique gain factor sequence , one may start with a different gain factor sequence ( smaller than the previous one ) when the reinitialization occurs . in either case ,",
    "the proof for the theorems presented in section [ samcmctheory ] follows similarly .",
    "the asymptotic efficiency of @xmath49 can be analyzed under the following conditions .",
    "let @xmath70 denote the euclidean inner product .",
    "@xmath34 is an open set , the function @xmath71 is continuous , and there exists a continuously differentiable function @xmath72 such that :    there exists @xmath73 such that @xmath74    there exists @xmath75 such that @xmath76 is a compact set , where @xmath77 .    for any @xmath78 , @xmath79 .",
    "the closure of @xmath80 has an empty interior .",
    "this condition assumes the existence of a global lyapunov function @xmath81 for the mean field @xmath82 .",
    "if @xmath82 is a gradient field , that is , @xmath83 for some lower bounded real - valued and differentiable function @xmath84 , then @xmath81 can be set to @xmath85 , provided that @xmath85 is continuously differentiable .",
    "this is typical for stochastic optimization problems , for example , machine learning [ @xcite ] , where a continuously differentiable objective function @xmath84 is minimized .      1 .",
    "the mean field function @xmath17 is measurable and locally bounded .",
    "there exist a stable matrix @xmath21 ( i.e. , all eigenvalues of @xmath21 are with negative real parts ) , @xmath86 , @xmath87 $ ] , and a constant @xmath88 such that , for any @xmath89 , @xmath90 where @xmath91 is defined in ( [ solutionseteq ] )",
    ".    this condition constrains the behavior of the mean field function around the solution points .",
    "it makes the trajectory averaging estimator sensible both theoretically and practically . if @xmath17 is differentiable , the matrix @xmath21 can be chosen to be the partial derivative of @xmath17 , that is , @xmath92 .",
    "otherwise , certain approximation may be needed .      before giving details of this condition ,",
    "we first define some terms and notation .",
    "assume that a transition kernel @xmath93 is irreducible , aperiodic , and has a stationary distribution on a sample space denoted by @xmath39 .",
    "a set @xmath94 is said to be small if there exist a probability measure @xmath95 on @xmath39 , a positive integer @xmath96 and @xmath97 such that @xmath98 where @xmath99 is the borel set defined on @xmath39 .",
    "a function @xmath100 is said to be a drift function outside @xmath101 if there exist positive constants @xmath102 and @xmath103 such that @xmath104 where @xmath105 . for a function @xmath106 ,",
    "define the norm @xmath107 and define the set @xmath108 .",
    "given the terms and notation introduced above , the drift condition can be specified as follows .    for any given @xmath109 ,",
    "the transition kernel @xmath93 is irreducible and aperiodic .",
    "in addition , there exists a function @xmath100 and a constant @xmath110 such that for any compact subset @xmath111 :    there exist a set @xmath94 , an integer @xmath96 , constants @xmath112 , @xmath103 , @xmath113 , @xmath97 and a probability measure @xmath95 such that @xmath114    there exists a constant @xmath115 such that , for all @xmath116 , @xmath117    there exists a constant @xmath115 such that , for all @xmath118 , @xmath119    assumption ( a@xmath120)(i ) is classical in the literature of markov chain .",
    "it implies the existence of a stationary distribution @xmath2 for all @xmath121 and @xmath122-uniform ergodicity [ @xcite ] .",
    "assumption ( a@xmath120)(ii ) gives conditions on the bound of @xmath123 .",
    "this is a critical condition for the observation noise .",
    "as seen later in lemmas [ lem1 ] and [ lem3 ] , it directly leads to the boundedness of some terms decomposed from the observation noise .",
    "for some algorithms , for example , samc , for which @xmath123 is a bounded function , the drift function can be simply set as @xmath124 .      1",
    ".   the sequences @xmath25 and @xmath37 are nonincreasing , positive and satisfy the conditions : @xmath125\\\\[-8pt ] \\frac{a_{k+1}-a_k}{a_k}&=&o(a_{k+1}),\\qquad b_k = o\\bigl(a_k^{({1+\\tau})/{2}}\\bigr),\\nonumber\\end{aligned}\\ ] ] for some @xmath126 $ ] , @xmath127 and for some constants @xmath128 as defined in condition ( a@xmath120 ) , @xmath129    it follows from ( [ coneq2 ] ) that @xmath130}^k \\frac{a_i^{(1+\\tau)/2}}{\\sqrt{i } } = o(1),\\ ] ] where @xmath131 $ ] denotes the integer part of @xmath64 .",
    "since @xmath132 is nonincreasing , we have @xmath133}^k \\frac{1}{\\sqrt{i } } = o(1),\\ ] ] and thus @xmath134 , or @xmath135 for @xmath136 .",
    "for instance , @xmath137 for some constants @xmath138 and @xmath139 , then we can set @xmath140 for some constants @xmath141 and @xmath142 , which satisfies ( [ coneq1 ] ) and ( [ coneq0003 ] ) . under this setting ,",
    "the existence of @xmath143 is obvious .",
    "theorem [ lem50 ] concerns the convergence of the general stochastic approximation mcmc algorithm .",
    "the proof follows directly from theorems 5.4 , 5.5 and proposition 6.1 of @xcite .",
    "[ lem50 ] assume conditions and hold .",
    "let @xmath144 denote the iteration number at which the @xmath145th truncation occurs in the stochastic approximation mcmc simulation .",
    "let @xmath146 be such that @xmath147 and that @xmath148 , where @xmath149 is defined in .",
    "then there exists almost surely a number , denoted by @xmath150 , such that @xmath151 and @xmath152 ; that is , @xmath153 has no truncation for @xmath154 , or mathematically , @xmath155 in addition , we have @xmath156 for some point @xmath89 .    theorem [ contheorem ] concerns the asymptotic normality of @xmath49 .",
    "[ contheorem ] assume conditions and hold .",
    "let @xmath146 be such that @xmath147 and that @xmath148 , where @xmath149 is defined in .",
    "then @xmath157 for some point @xmath158 , where @xmath159 , @xmath19 is negative definite , @xmath160 , and @xmath161 is as defined in ( [ rev.eq1 ] ) .",
    "below we consider the asymptotic efficiency of @xmath49 .",
    "as already mentioned , the asymptotic efficiency of the trajectory averaging estimator has been studied by quite a few authors . @xcite gives the following definition for the asymptotic efficient estimator that can be resulted from a stochastic approximation algorithm .",
    "[ effdef ] consider the stochastic approximation algorithm ( [ moteq1 ] ) .",
    "let @xmath162 , given as a function of @xmath163 , be a sequence of estimators of @xmath20 .",
    "the algorithm @xmath162 is said to be asymptotically efficient if @xmath164 where @xmath165 , and @xmath166 is the asymptotic covariance matrix of @xmath167 .    as mentioned in @xcite",
    ", @xmath166 is the smallest possible limit covariance matrix that an estimator based on the stochastic approximation algorithm ( [ moteq1 ] ) can achieve .",
    "if @xmath168 and @xmath169 forms or asymptotically forms a martingale difference sequence , then we have @xmath170 . in the next theorem",
    ", we show that the asymptotic covariance matrix @xmath171 established in theorem  [ contheorem ] is the same as @xmath166 , and thus the trajectory averaging estimator @xmath52 is asymptotically efficient .",
    "[ efftheorem ] assume conditions and hold .",
    "let @xmath146 be such that @xmath147 and that @xmath148 , where @xmath149 is defined in .",
    "then @xmath49 is asymptotically efficient .",
    "as implied by theorem [ efftheorem ] , the convergence rate of @xmath52 , which is measured by the asymptotic covariance matrix @xmath172 , is independent of the choice of the gain factor sequence as long as the condition ( a@xmath173 ) is satisfied .",
    "the asymptotic efficiency of @xmath49 can also be interpreted in terms of fisher information theory .",
    "refer to pelletier [ ( @xcite ) , section 3 ] and the references therein for more discussions on this issue .",
    "trajectory averaging enables smoothing of the behavior of the algorithm but at the same time , it slows down the numerical convergence because it takes longer for the algorithm to forget the first iterates .",
    "an alternative idea would be to consider moving window averaging algorithms , see , for example , @xcite and kushner and yin ( @xcite ) , chapter 11 .",
    "extension of their results to the general stochastic approximation mcmc algorithm will be of great interest .",
    "suppose that we are interested in sampling from the following distribution @xmath174 where @xmath88 is an unknown constant , @xmath175 is the sample space .",
    "the basic idea of samc stems from the wang ",
    "landau algorithm [ @xcite , @xcite ] and can be briefly explained as follows .",
    "let @xmath176 denote a partition of @xmath39 , and let @xmath177 for @xmath178 .",
    "samc seeks to draw sample from the trial distribution @xmath179 where @xmath180 s are prespecified constants such that @xmath181 for all @xmath182 and , and @xmath183 if @xmath184 and 0 otherwise .",
    "for example , if the sample space is partitioned according to the energy function into the following subregions : @xmath185 , @xmath186 , where @xmath187 are the user - specified numbers , then sampling from @xmath188 would result in a random walk ( by viewing each subregion as a `` point '' ) in the space of energy with each subregion being sampled with probability @xmath180 . here",
    ", without loss of generality , we assume that each subregion is unempty ; that is , assuming @xmath189 for all @xmath190 . therefore , sampling from ( [ eq11 ] ) essentially avoids the local - trap problem suffered by the conventional mcmc algorithms .",
    "this is attractive , but @xmath191 s are unknown .",
    "samc provides a dynamic way to estimate @xmath191 s under the framework of the stochastic approximation mcmc algorithm .    in what follows",
    "we describe how @xmath192 can be estimated by samc .",
    "since @xmath188 is invariant with respect to a scale change of @xmath192 , it suffices to estimate @xmath193 by fixing @xmath194 to a known constant provided @xmath195 .",
    "let @xmath196 denote the working estimate of @xmath197 obtained at iteration @xmath9 , and let @xmath198 .",
    "why this reparameterization is used will be explained at the end of this subsection .",
    "let @xmath25 denote the gain factor sequence , and let @xmath33 denote a sequence of compact subsets of @xmath34 as defined in ( [ truncationseteq ] ) . for this algorithm ,",
    "@xmath199 can be chosen as follows .",
    "define @xmath200 where @xmath201 for @xmath202 , and @xmath203 .",
    "clearly , @xmath204 is continuous in @xmath4 , and @xmath205 for any @xmath206 forms a compact subset of @xmath34 .",
    "therefore , @xmath207 , @xmath208 , is an appropriate choice of @xmath33 . for the samc algorithm ,",
    "as seen below , @xmath209 is bounded by the constant @xmath210 , so we can set the drift function @xmath124 .",
    "hence , the initial sample @xmath211 can be drawn arbitrarily from @xmath212 , while leaving the condition @xmath213 holds . in summary , samc starts with an initial estimate of @xmath214 , and a random sample drawn arbitrarily from the space @xmath39 , and then iterates between the following steps .",
    "_ samc algorithm_.    a.   ( sampling . ) simulate a sample @xmath57 by a single mh update with the target distribution @xmath215 provided that @xmath216 is nonempty . in practice , @xmath216 can be replaced by any other unempty subregion . 1 .",
    "generate @xmath217 according to a proposal distribution @xmath218 .",
    "2 .   calculate the ratio @xmath219 where @xmath220 denotes the index of the subregion that the sample @xmath64 belongs to .",
    "3 .   accept the proposal with probability @xmath221 .",
    "if it is accepted , set @xmath222 ; otherwise , set @xmath223",
    ". b.   ( weight updating . ) set @xmath224 c.   ( varying truncation . ) if @xmath225 , then set @xmath226 and @xmath66 ; otherwise , set @xmath227 and @xmath68 , where @xmath53 and @xmath228 are as defined in section [ sec2 ] .",
    "samc sampling is driven by its self - adjusting mechanism , which , consequently , implies the superiority of samc in sample space exploration .",
    "the self - adjusting mechanism can be explained as follows : if a subregion is visited at , @xmath22 will be updated accordingly such that the probability that this subregion ( other subregions ) will be revisited at the next iterations will decrease ( increase ) .",
    "mathematically , if @xmath229 , then @xmath230 and @xmath231 for @xmath232 .",
    "note that the linear adjustment on @xmath4 transforms to a multiplying adjustment on @xmath192 .",
    "this also explains why samc works on the logarithm of @xmath192 .",
    "working on the logarithm enables @xmath192 to be adjusted quickly according to the distribution of the samples .",
    "otherwise , learning of @xmath192 would be very slow due to the linear nature of stochastic approximation .",
    "including @xmath180 in the transformation @xmath233 facilitates our computation , for example , the ratio @xmath234 in step ( a.2 ) .",
    "the self - adjusting mechanism has led to successful applications of samc for many hard computational problems , including phylogenetic tree reconstruction [ cheon and liang ( @xcite , @xcite ) ] , neural network training [ liang ( @xcite ) ] , bayesian network learning [ @xcite ] , among others .      to show that the trajectory averaging estimator is asymptotically efficient for samc",
    ", we assume the following conditions .    1 .",
    "the mh transition kernel used in the sampling step satisfies the drift condition ( a@xmath120 ) .    to ensure the drift condition to be satisfied , @xcite",
    "restrict the sample space @xmath39 to be a compact set , assume @xmath235 to be bounded away from 0 and @xmath236 , and choose the proposal distribution @xmath237 to satisfy the local positive condition : for every @xmath238 , there exist positive @xmath239 and @xmath240 such that @xmath241 if the compactness condition on @xmath39 is removed , we may need to impose some constraints on the tails of the target distribution @xmath235 and the proposal distribution @xmath237 as done by @xcite .    1 .",
    "the sequence @xmath25 satisfies the following conditions : @xmath242 for some @xmath126 $ ] .    for the samc algorithm , as previously discussed ,",
    "@xmath243 is bounded by the constant @xmath210 , so we can set @xmath124 and set @xmath244 to any a large number in condition ( a@xmath120 ) .",
    "furthermore , given a choice of @xmath135 for some @xmath245 , there always exists a sequence @xmath37 , for example , @xmath246 for some @xmath126 $ ] , such that the inequality @xmath247 holds for all iterations .",
    "hence , a specification of the sequence @xmath37 can be omitted for the samc algorithm .",
    "theorem [ samcconvergence ] concerns the convergence of samc . in the first part ,",
    "it states that @xmath248 is almost surely finite ; that is , @xmath249 can be included in a compact set almost surely . in the second part , it states the convergence of @xmath22 to a solution @xmath20 .",
    "we note that for samc , the same convergence result has been established by @xcite under ( c@xmath250 ) and a relaxed condition of ( c@xmath251 ) , where @xmath25 is allowed to decrease at a rate of @xmath26 .",
    "since the focus of this paper is on the asymptotic efficiency of @xmath52 , the convergence of @xmath249 is only stated under a slower decreasing rate of @xmath25 .",
    "we also note that for samc , we have assumed , without loss of generality , that all subregions are unempty . for the empty subregions ,",
    "no adaptation of @xmath249 occurs for the corresponding components in the run .",
    "therefore , the convergence of @xmath249 should only be measured for the components corresponding to the nonempty subregions .",
    "[ samcconvergence ] assume conditions and hold .",
    "then there exists ( a.s . ) a number , denoted by @xmath150 , such that @xmath151 , @xmath152 , and @xmath249 given by the samc algorithm has no truncation for @xmath154 , that is , @xmath252 and @xmath253 where @xmath254 , and @xmath255 .",
    "theorem [ samcavetheorem ] concerns the asymptotic normality and efficiency of @xmath49 .",
    "[ samcavetheorem ] assume conditions and .",
    "then @xmath49 is asymptotically efficient ; that is , @xmath256 where @xmath159 , @xmath19 is negative definite and @xmath257 .",
    "the above theorems address some theoretical issues of samc . for practical issues",
    ", please refer to @xcite , where issues , such as how to partition the sample space , how to choose the desired sampling distribution , and how to diagnose the convergence , have been discussed at length .",
    "an issue particularly related to the trajectory averaging estimator is the length of the burn - in period . to remove the effect of the early iterates , the following estimator : @xmath258 instead of @xmath49 , is often used in practice , where @xmath259 is the so - called length of the burn - in period .",
    "it is obvious that the choice of @xmath259 should be based on the diagnosis for the convergence of the simulation . just like monitoring convergence of mcmc simulations , monitoring convergence of samc simulations",
    "should be based on multiple runs [ @xcite ] . in practice , if only a single run was made , we suggest to look at the plot of @xmath260 to choose @xmath259 from where @xmath261 has been approximately stable . here , we denote by @xmath261 the sampling frequencies of the respective subregions realized by iteration @xmath9 .",
    "it follows from theorem [ samcconvergence ] that @xmath262 when the number of iterations , @xmath9 , becomes large .",
    "trajectory averaging can directly benefit one s inference in many applications of samc .",
    "a typical example is bayesian model selection , where the ratio @xmath263 just corresponds to the bayesian factor of two models if one partitions the sample space according to the model index and imposes an uniform prior on the model space as done in @xcite .",
    "another example is inference for the spatial models with intractable normalizing constants , for which @xcite has demonstrated how samc can be used to estimate the normalizing constants for these models and how the estimate can then be used for inference of the model parameters .",
    "an improved estimate of the normalizing constant function would definitely benefit one s inference for the model .",
    "consider the standard missing data problem :    * @xmath217 is the observed incomplete data .",
    "* @xmath264 is the complete data likelihood , that is , the likelihood of the complete data @xmath265 obtained by augmenting the observed data @xmath217 with the missing data  @xmath7 .",
    "the dependence of @xmath264 on @xmath217 is here implicit .",
    "* @xmath266 is the predictive distribution of the missing data @xmath7 given the observed data @xmath217 , that is , the predictive likelihood .",
    "our goal is to find the maximum likelihood estimator of @xmath4 .",
    "this problem has been considered by a few authors under the framework of stochastic approximation ; see , for example , @xcite , @xcite and @xcite . a basic algorithm proposed by @xcite for the problem",
    "can be written as @xmath267 where the missing data @xmath268 can be imputed using a mcmc algorithm , such as the metropolis  hastings algorithm . under standard regularity conditions ,",
    "we have @xmath269 = \\partial_{\\theta } l(\\theta),\\ ] ] where @xmath270 is the log - likelihood function of the incomplete data .    to show that the trajectory averaging estimator is asymptotically efficient for a varying truncation version of the algorithm ( [ mleeq1 ] ) , we assume ( a@xmath120 ) , ( a@xmath173 ) and some regularity conditions for the distribution @xmath264 .",
    "the conditions ( a@xmath250 ) and ( a@xmath251 ) can be easily verified with the following settings :    * the lyapunov function @xmath204 can be chosen as @xmath271 , where @xmath272 is chosen such that @xmath273 .",
    "thus , @xmath274 the set of stationary points of ( [ mleeq1 ] ) , @xmath275 , coincides with the set of the solutions @xmath276 .",
    "then the condition ( a@xmath250 ) can be verified by verifying that @xmath270 is continuously differentiable ( this is problem dependent ) . *",
    "the matrix @xmath21 trivially is the hessian matrix of @xmath270 .",
    "then ( a@xmath251 ) can be verified using the taylor expansion .    in summary , we have the following theorem .    [ mleefficiency ]",
    "assume conditions and hold .",
    "then the estimator @xmath49 generated by a varying truncation version of algorithm ( [ mleeq1 ] ) is asymptotically efficient .    in practice , to ensure the drift condition to be satisfied , we may follow @xcite to impose some constraints on the tails of the distribution @xmath277 and the proposal distribution @xmath237 . alternatively , we can follow @xcite to choose a proposal satisfying the local positive condition ( [ proposalcons ] ) and to restrict the sample space @xmath39 to be compact .",
    "for example , we may set @xmath39 to a huge space , say , @xmath278^{d_x}$ ] . as a practical matter",
    ", this is equivalent to setting @xmath279 .",
    "in this paper , we have shown that the trajectory averaging estimator is asymptotically efficient for a general stochastic approximation mcmc algorithm under mild conditions , and then applied this result to the stochastic approximation monte carlo algorithm and a stochastic approximation mle algorithm . the main difference between this work and the work published in the literature , for example , @xcite and @xcite ,",
    "are at the conditions on the observation noise . in the literature",
    ", it is usually assumed directly that the observation noise has the decomposition @xmath280 , where @xmath47 forms a martingale difference sequence and @xmath281 is a higher order term of @xmath282 . as shown in lemma [ lem3 ]",
    ", the stochastic approximation mcmc algorithm does not satisfy this decomposition .",
    "lemma [ lem1 ] is a partial restatement of proposition 6.1 of @xcite .",
    "[ lem1 ] assume condition holds .",
    "then the following results hold :    1 .   for any @xmath109",
    ", the markov kernel @xmath283 has a single stationary distribution @xmath284 .",
    "in addition , @xmath285 is measurable for all @xmath109 , @xmath286 .",
    "2 .   for any @xmath109",
    ", the poisson equation @xmath287 has a solution @xmath288 , where @xmath289 .",
    "there exist a function @xmath290 such that @xmath291 , and a constant @xmath292 $ ] such that for any compact subset @xmath293 , the following holds : @xmath294\\\\[-8pt ] \\mbox{\\textup{(iii ) } } & & \\sup_{(\\theta,\\theta')\\in\\mathcal{k}\\times\\mathcal{k } } \\| \\theta-\\theta ' \\| ^{-\\beta } \\bigl(\\|u(\\theta , x)-u(\\theta',x)\\|_v\\nonumber\\\\ & & \\hspace*{78.8pt}\\qquad{}+\\|p_{\\theta } u(\\theta , x)-p_{\\theta ' } u(\\theta',x)\\|_v \\bigr ) < \\infty.\\nonumber\\end{aligned}\\ ] ]    lemma [ lem1.1 ] is a restatement of proposition 5.1 of @xcite .",
    "[ lem1.1 ] assume conditions and hold .",
    "let @xmath146 be such that @xmath295 @xmath296 and that @xmath148 , where @xmath297 is defined in . then @xmath298 < \\infty $ ] , where @xmath128 is defined in condition and @xmath248 is defined in theorem [ lem50 ] .",
    "lemma [ lem2 ] is a restatement of corollary 2.1.10 of duflo ( @xcite ) , pages 46 and  47 .",
    "[ lem2 ] let @xmath299 be a zero - mean , square - integrable martingale array with differences @xmath300 , where @xmath301 denotes the @xmath145-field .",
    "suppose that the following assumptions apply :    the @xmath145-fields are nested : @xmath302 for @xmath303 , @xmath304 .",
    "@xmath305 in probability , where @xmath306 is a positive definite matrix .    for any @xmath307",
    ", @xmath308 \\rightarrow0 $ ] in probability .",
    "then @xmath309 in distribution .",
    "[ dfslln ] for @xmath310 , a sequence @xmath311 of random variables is said to be residually cesro @xmath312-integrable [ @xmath313 , in short ] if @xmath314 and @xmath315    lemma [ lemmaslln ] is a restatement of theorem 2.1 of chandra and goswami ( @xcite ) .",
    "[ lemmaslln ] let @xmath316 be a sequence of nonnegative random variables satisfying @xmath317 for all @xmath318 and let @xmath319 .",
    "if @xmath320 is @xmath313 for some @xmath321 , then @xmath322 \\rightarrow0\\qquad \\mbox{in probability}.\\ ] ]    [ lem3 ] assume conditions and hold .",
    "let @xmath146 be such that @xmath295 @xmath296 and that @xmath148 , where @xmath297 is defined in .",
    "if @xmath323 , which is defined in theorem [ lem50 ] , then there exist @xmath324-valued random processes @xmath325 , @xmath326 and @xmath327 defined on a probability space @xmath328 such that :    @xmath329 for @xmath154 .",
    "@xmath330 is a martingale difference sequence , and @xmath331 in distribution , where @xmath160 .",
    "@xmath332 , as @xmath333 .",
    "@xmath334 , as @xmath333 .",
    "\\(i ) let @xmath335 , and @xmath336\\nonumber\\\\ & & { } + \\frac{a_{k+2}-a_{k+1}}{a_{k+1 } } p_{\\theta_{k+1 } } u(\\theta_{k+1},x_{k+1 } ) , \\\\",
    "\\tilde{\\varsigma}_{k+1}&=&a_{k+1 } p_{\\theta_k } u(\\theta_k , x_k ) , \\nonumber\\\\ \\varsigma_{k+1}&=&\\frac{1}{a_{k+1 } } ( \\tilde{\\varsigma}_{k+1}-\\tilde { \\varsigma}_{k+2}).\\nonumber\\end{aligned}\\ ] ] it is easy to verify that ( i ) holds by noticing the poisson equation given in ( b@xmath251 ) .    by ( [ noisedecomeq ] )",
    ", we have @xmath337 where @xmath338 is a family of @xmath145-algebras satisfying @xmath339 and @xmath340 for all @xmath154 . hence , @xmath330 forms a martingale difference sequence .",
    "when @xmath323 , there exists a compact set @xmath341 such that @xmath342 for all @xmath343 .",
    "following from lemmas [ lem1 ] and [ lem1.1 ] , @xmath344 is @xmath161 is uniformly square integrable with respect to @xmath9 , and the martingale @xmath345 is square integrable for all @xmath346 .    by ( [ noisedecomeq ] ) , we have @xmath347\\nonumber \\\\ & & { } - p_{\\theta_k } u(\\theta_k , x_k ) p_{\\theta_k } u(\\theta_k , x_k)^t\\\\ & \\stackrel{\\triangle}{= } & l(\\theta_k , x_k).\\nonumber\\end{aligned}\\ ] ]    following from lemmas [ lem1 ] and [ lem1.1 ] , @xmath348 is uniformly integrable with respect to @xmath9 .",
    "hence , @xmath349 is rci(@xmath312 ) for any @xmath350 ( definition [ dfslln ] ) . since @xmath351 forms a martingale difference sequence , the correlation coefficient @xmath352 , @xmath353 for all @xmath354 . by lemma  [ lemmaslln ]",
    ", we have , as @xmath355 , @xmath356    now we show that @xmath357 also converges .",
    "it follows from ( a@xmath250 ) and ( b@xmath251 ) that @xmath358 is continuous in @xmath4 . by the convergence of @xmath22",
    ", we can conclude that @xmath359 converges to @xmath360 for any @xmath238 .",
    "following from lemmas [ lem1 ] , [ lem1.1 ] and lebesgue s dominated convergence theorem , @xmath357 converges to @xmath361 . combining with ( [ mcmccon ] )",
    ", we obtain @xmath362 since @xmath363 can be uniformly bounded by an integrable function @xmath364 , the lindeberg condition is satisfied , that is , @xmath365",
    "$ n \\rightarrow\\infty$}.\\ ] ] following from lemma [ lem2 ] , we have @xmath366 by identifying @xmath367 to @xmath300 , @xmath346 to @xmath368 , and @xmath369 to @xmath301 .    by condition ( a@xmath173 )",
    ", we have @xmath370 by ( [ noisedecomeq ] ) and ( [ boundeq1 ] ) , there exists a constant @xmath371 such that the following inequality holds : @xmath372 which implies , by ( [ boundeq1 ] ) , that there exists a constant @xmath373 such that @xmath374 since @xmath375 is square integrable , @xmath376 is uniformly integrable with respect to @xmath9 and there exists a constant @xmath377 such that @xmath378 where the last inequality follows from condition ( a@xmath173 ) .",
    "therefore , ( iii ) holds by kronecker s lemma .",
    "a straightforward calculation shows that @xmath379 by lemmas [ lem1 ] and [ lem1.1 ] , @xmath380 is uniformly bounded with respect to  @xmath9 .",
    "therefore , ( iv ) holds .    by theorem [ lem50 ] , we have @xmath381 to facilitate the theoretical analysis for the random process",
    "@xmath382 , we define a reduced random process : @xmath383 , where @xmath384 which is equivalent to set @xmath385 for all @xmath386 . for convenience",
    ", we also define @xmath387 it is easy to verify that @xmath388\\\\[-8pt ] & & { } + a_k \\bigl ( h(\\theta_k)-f(\\tilde{\\theta}_k-\\theta^ * ) \\bigr ) + a_k \\tilde{\\varepsilon}_{k+1}\\qquad \\forall k \\geq k_{\\sigma_s},\\nonumber\\end{aligned}\\ ] ] which implies @xmath389\\\\[-8pt ] & & { } + \\sum_{j = k_{\\sigma_s}}^k \\phi_{k , j+1 } a_j \\bigl ( h(\\theta_j)-f(\\tilde { \\theta}_j-\\theta ^ * ) \\bigr)\\qquad \\forall k \\geq k_{\\sigma_s},\\nonumber\\end{aligned}\\ ] ] where @xmath390 if @xmath391 , and @xmath392 , and @xmath393 denotes the identity matrix .    for @xmath394 specified in ( a@xmath251 ) and a deterministic integer @xmath259 , define the stopping time @xmath395 if @xmath396 and 0 if @xmath397 .",
    "define @xmath398 and let @xmath399 denote the indicator function ; @xmath400 if @xmath401 and 0 otherwise .",
    "therefore , for all @xmath402 , @xmath403 i_a(k+1 ) \\\\ & & \\qquad\\quad { } + \\biggl [ \\sum_{j = k_0}^k \\phi_{k , j+1 } a_j \\bigl ( h(\\theta_j)- f(\\tilde{\\theta}_j-\\theta^ * )",
    "\\bigr ) i_a(j ) \\biggr ] i_a(k+1).\\nonumber\\end{aligned}\\ ] ] including the terms @xmath404 in ( [ lem6eq1 ] ) facilitates our use of some results published in @xcite in the later proofs , but it does not change equality of ( [ lem6eq1 ] ) . note that if @xmath405 , then @xmath406 for all @xmath407 .",
    "[ lem4 ] the following estimate takes place : @xmath408 where @xmath409 denotes a magnitude that tends to zero as @xmath410 .",
    "let @xmath88 be a positive constant , then there exists another constant @xmath371 such that @xmath411    there exist constants @xmath412 and @xmath115 such that @xmath413    let @xmath414 .",
    "then @xmath415 is uniformly bounded with respect to both @xmath9 and @xmath416 for @xmath417 , and @xmath418    parts ( i ) and ( iv ) are a restatement of lemma 3.4.1 of @xcite .",
    "the proof of part ( ii ) can be found in the proof of lemma 3.3.2 of @xcite .",
    "the proof of part ( iii ) can be found in the proof of lemma 3.1.1 of @xcite .",
    "[ lem6 ] if conditions hold , then @xmath419 is uniformly bounded with respect to @xmath9 , where the set @xmath36 is as defined in ( [ setaeq ] ) .    by ( [ redproc ] ) and ( [ noisedecomeq ] )",
    ", we have @xmath420 following from ( b@xmath251 ) and lemma [ lem1.1 ] , it is easy to see that @xmath421 is uniformly bounded with respect to @xmath9 . hence , to prove the lemma , it suffices to prove that @xmath422 is uniformly bounded with respect to @xmath9 .    by ( [ redproc ] ) , ( a@xmath251 ) and ( b@xmath251 )",
    ", there exist constants @xmath371 and @xmath373 such that @xmath423\\\\[-8pt ] & & \\qquad \\leq\\|h(\\theta_j)-f(\\theta_j-\\theta^*)\\| i_a(j ) + c_2 a_j \\|p_{\\theta_{j-1 } } u(\\theta_{j-1 } , x_{j-1 } ) \\| \\nonumber\\\\ & & \\qquad \\leq c_1 \\|\\theta_j-\\theta^*\\|^{1+\\rho}+ c_2 a_j \\|p_{\\theta_{j-1 } } u(\\theta_{j-1 } , x_{j-1 } ) \\|.\\nonumber\\end{aligned}\\ ] ] in addition , we have @xmath424\\\\[-8pt ] & \\leq & 2 \\| \\theta_{k_0}-\\theta^ * \\|^2 i_a(k_0 ) + 2 e \\|\\tilde { \\varsigma } _ { k_0}\\|^2.\\nonumber\\end{aligned}\\ ] ] it is easy to see from ( [ boundeq1 ] ) and ( [ noisedecomeq ] ) that @xmath425 is square integrable .",
    "hence , following from ( [ setaeq ] ) , there exists a constant @xmath426 such that @xmath427    by ( [ lem6eq1 ] ) , ( [ lem4eq1 ] ) , ( [ lem6eq2 ] ) and ( [ eq??1 ] ) , and following chen [ ( @xcite ) , page 141 ] we have @xmath428 \\\\ \\hspace*{-5pt}&&\\qquad\\quad { } + \\frac{5 c_0 ^ 2}{a_{k+1 } } \\sum_{i = k_0}^k \\sum_{j = k_0}^k \\biggl [ \\exp \\biggl(-c\\hspace*{-0.5pt}\\sum _ { s = j+1}^k a_s \\biggr ) a_j \\exp\\biggl(-c\\hspace*{-0.5pt}\\sum_{s = i+1}^k a_s \\biggr ) a_i e \\| \\nu_{i+1 } \\nu_{j+1}^t\\| \\biggr ]",
    "\\\\ \\hspace*{-5pt}&&\\qquad\\quad { } + \\frac{5 c_0 ^ 2 c_2 ^ 2 } { a_{k+1 } } \\sum_{i = k_0}^k \\sum_{j = k_0}^k \\biggl [ \\exp \\biggl(-c\\sum_{s = j+1}^k a_s \\biggr ) a_j^2 \\exp\\biggl(-c\\hspace*{-0.5pt}\\sum_{s = i+1}^k a_s \\biggr)\\\\ \\hspace*{-5pt}&&\\hspace*{83.3pt}\\qquad\\quad{}\\times a_i^2 e \\|p_{\\theta_{i-1 } } u(\\theta_{i-1 } , x_{i-1 } ) ( p_{\\theta_{j-1 } } u(\\theta_{j-1 } , x_{j-1}))^t \\| \\biggr ] \\\\ % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % & + \\frac{5c_0 ^ 2 c_2 ^ 2}{a_{k+1 } } % [ \\sum_{j = k_0}^k\\exp(-c\\sum_{s = j+1}^k a_s ) a_j^2 % e \\|p_{\\theta_{j-1 } } u(\\theta_{j-1 } , x_{j-1})\\|^2 ] ^2 \\\\ % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % \\hspace*{-5pt}&&\\qquad\\quad { } + \\frac{5c_0 ^ 2 c_1 ^ 2}{a_{k+1 } } e \\biggl [ \\sum_{j = k_0}^k \\exp\\biggl(-c\\hspace*{-0.5pt}\\sum_{s = j+1}^k a_s \\biggr ) a_j \\| \\theta_j-\\theta^*\\|^{1+\\rho } i_a(j ) \\biggr]^2 \\\\ \\hspace*{-5pt}&&\\qquad\\stackrel{\\triangle}{= } i_1+i_2+i_3+i_4+i_5.\\end{aligned}\\ ] ]    by ( [ lem4eq0 ] ) , there exists a constant @xmath377 such that such that @xmath429 where @xmath430 as @xmath431 .",
    "this implies that @xmath432 if @xmath259 is large enough .",
    "hence , @xmath433 is bounded if @xmath259 is large enough .    by ( [ lem4eq0 ] ) and ( [ lem4eq00 ] ) , for large enough @xmath259",
    ", there exists a constant @xmath434 such that @xmath435 since @xmath436 forms a martingale difference sequence ( lemma [ lem3 ] ) , @xmath437 which implies that @xmath438 \\\\ & \\leq & 5 c_0 ^ 2 \\sup_i e\\|e_i\\|^2 \\sum_{i = k_0}^k \\biggl [ a_i^2 \\exp\\biggl(-2c\\sum_{s = j+1}^k a_s \\biggr ) \\biggr].\\end{aligned}\\ ] ] since @xmath439 is uniformly bounded by a function @xmath440 which is square integrable , @xmath441 is bounded by a constant .",
    "furthermore , by ( [ lem4eq00 ] ) , @xmath442 is uniformly bounded with respect to @xmath9 .    by ( [ noisedecomeq ] ) ,",
    "( [ boundeq1 ] ) and condition ( a@xmath173 ) , there exist a constant @xmath443 and a constant @xmath444 such that the following inequality holds : @xmath445 this , by ( b@xmath250 ) and the cauchy ",
    "schwarz inequality , further implies that there exists a constant @xmath446 such that @xmath447 therefore , there exists a constant @xmath448 such that @xmath449 \\\\",
    "& \\leq & 5c_0 ^ 2 c_5 \\sum_{i = k_0}^k \\sum_{j = k_0}^k \\biggl [ \\exp\\biggl(-\\frac{c}{2 } \\sum_{s = j+1}^k a_s \\biggr ) { a_j}^{{1}/{2}}\\\\ & & \\hspace*{67.5pt}{}\\times \\exp\\biggl(-\\frac{c}{2 } \\sum_{s = i+1}^k a_s \\biggr ) { a_i}^{{1}/{2 } } a_i^{({1+\\tau})/{2 } } a_j^{({1+\\tau})/{2 } } \\biggr ] \\\\ & = & 5 c_0 ^ 2 c_5 \\biggl\\ { \\sum_{j = k_0}^k \\biggl [ { a_j}^{1+{\\tau}/{2 } } \\exp\\biggl(-\\frac{c}{2 } \\sum_{s = j+1}^k a_s \\biggr ) \\biggr ] \\biggr\\}^2.\\end{aligned}\\ ] ] by ( [ lem4eq00 ] ) , @xmath450 is uniformly bounded with respect to @xmath9 .",
    "following from lemmas [ lem1 ] and [ lem1.1 ] , @xmath451 is uniformly bounded with respect to @xmath9 .",
    "therefore , there exists a constant @xmath452 such that @xmath453 \\\\",
    "& \\leq & 5 c_0 ^ 2 c_2 ^ 2 c_6 \\biggl\\ { \\sum_{j = k_0}^k \\biggl [ { a_j}^{{3}/{2 } } \\exp\\biggl(-\\frac{c}{2 } \\sum_{s = j+1}^k a_s \\biggr ) \\biggr ] \\biggr\\}^2.\\end{aligned}\\ ] ] by ( [ lem4eq00 ] ) , @xmath454 is uniformly bounded with respect to @xmath9 .    the proof for the uniform boundedness of @xmath455 can be found in the proof of lemma 3.4.3 of chen ( @xcite ) , pages 143 and 144 .",
    "[ lem7 ] if conditions hold , then as @xmath333 , @xmath456    by ( [ redproc ] ) and ( [ noisedecomeq ] ) , there exists a constant @xmath88 such that @xmath457 to prove the lemma , it suffices to prove that @xmath433 and @xmath442 both converge to zero in probability as @xmath333 .",
    "following from lemmas [ lem1 ] and [ lem1.1 ] , @xmath458 is uniformly bounded for all @xmath459 .",
    "this implies , by condition ( a@xmath173 ) , there exists a constant @xmath88 such that @xmath460 by kronecker s lemma , @xmath461 , and thus @xmath462 in probability .",
    "the convergence @xmath463 can be established as in chen [ ( @xcite ) , lemma 3.4.4 ] using the condition ( a@xmath251 ) and lemma [ lem6 ] .",
    "_ _ proof of theorem [ contheorem].__by theorem [ lem50 ] , @xmath22 converges to the zero point @xmath20 almost surely and @xmath155 consequently , we have , by ( [ redproc ] ) , @xmath464\\\\[-8pt ] & = & o(1)+\\frac{1}{\\sqrt{k } } \\sum_{i = k_{\\sigma_s}}^k ( \\tilde{\\theta } _ i-\\theta^ * ) -\\frac{1}{\\sqrt{k } } \\sum_{i = k_{\\sigma_s}}^k \\tilde{\\varsigma}_i,\\nonumber\\end{aligned}\\ ] ] where @xmath430 as @xmath333 .",
    "condition ( a@xmath173 ) implies @xmath465 by kronecker s lemma .",
    "following lemmas [ lem1 ] and [ lem1.1 ] , there exists a constant @xmath88 such that @xmath466 therefore , @xmath467 in probability as @xmath468 .    by ( [ redcedeqiter ] ) , ( [ proeq1 ] ) and ( [ proeq2 ] )",
    ", we have @xmath469\\\\[-8pt ] & & { } + \\frac{1}{\\sqrt{k } } \\sum_{i = k_{\\sigma_s}}^k \\sum_{j = k_{\\sigma_s}}^{i-1 } \\phi_{i-1,j+1 } a_j \\bigl(h(\\theta_j)-f(\\tilde{\\theta}_j-\\theta^ * ) \\bigr)\\nonumber\\\\ & \\stackrel{\\triangle}{= } & o_p(1)+ i_1+i_2+i_3,\\nonumber\\end{aligned}\\ ] ] where @xmath470 means @xmath471    by noticing that @xmath472 , we have @xmath473 and thus @xmath474 by the definition of @xmath415 given in lemma [ lem4](iv ) , we have @xmath475 which implies @xmath476 by lemma [ lem4 ] , @xmath415 is bounded .",
    "therefore , @xmath463 as @xmath333 .",
    "the above arguments also imply that there exists a constant @xmath477 such that @xmath478 by ( [ proeq5 ] ) , we have @xmath479 it then follows from lemma [ lem7 ] that @xmath450 converges to zero in probability as @xmath333 .",
    "now we consider @xmath442 . by ( [ redprocrev ] ) and ( [ proeq4 ] ) , @xmath480 since @xmath481 is a martingale difference sequence , @xmath482=0\\qquad \\forall i > j,\\ ] ] which implies that @xmath483 by the uniform boundedness of @xmath484 , ( [ lem4eq2 ] ) and the uniform boundedness of @xmath415 , there exists a constant @xmath371 such that @xmath485 therefore , @xmath486 in probability as @xmath468 .",
    "since @xmath415 is uniformly bounded with respect to both @xmath9 and @xmath416 , there exists a constant @xmath373 such that @xmath487 following from lemma [ lem3](iii ) , @xmath488 converges to zero in probability as @xmath468 .    by lemma [ lem3 ] , @xmath489 in distribution . combining with the convergence results of @xmath433 , @xmath450 , @xmath490 and @xmath488",
    ", we conclude the proof of the theorem .",
    "_ _ proof of theorem [ efftheorem].__since the order of @xmath491 is difficult to treat , we consider the following stochastic approximation mcmc algorithm : @xmath492 where @xmath493 and @xmath494 are as defined in ( [ redproc ] ) and ( [ redprocrev ] ) , respectively . following from lemma [ lem3](ii ) ,",
    "@xmath495 forms a sequence of asymptotically unbiased estimator of 0 .",
    "let @xmath496 . to establish that @xmath497 is an asymptotically efficient estimator of @xmath20",
    ", we will first show ( in step 1 ) @xmath498 where @xmath499 , @xmath19 and @xmath160 ; and then show ( in step 2 ) that the asymptotic covariance matrix of @xmath500 is equal to  @xmath171 .",
    "_ step _ 1 . by ( [ redprocrev ] )",
    ", we have @xmath501 by lemmas [ lem1 ] and [ lem1.1 ] , @xmath502 is uniformly bounded for @xmath154 and thus there exists a constant @xmath88 such that @xmath503 by kronecker s lemma and ( a@xmath173 ) , we have @xmath504 in probability . hence , @xmath505 @xmath506 and @xmath507 that is @xmath508 following from theorem [ contheorem ] and slutsky s theorem , ( [ reveffeq4 ] ) holds .",
    "_ step _ 2 .",
    "now we show the asymptotic covariance matrix of @xmath509 is equal to @xmath171 .",
    "consider @xmath510 \\biggl [ \\sum_{k=1}^n e(\\tilde{\\varepsilon}_k ) \\biggr]^t \\\\ & & \\qquad = ( i_1)+(i_2)+(i_3).\\end{aligned}\\ ] ]    by ( [ redprocrev ] ) , we have @xmath511 by ( [ nunormeq2 ] ) , @xmath512 for @xmath154 , where @xmath444 is defined in ( a@xmath173 ) .",
    "since @xmath513 is square integrable , there exists a constant @xmath88 such that @xmath514 which , by kronecker s lemma and ( a@xmath173 ) , implies @xmath515 as @xmath516 .",
    "following from lemmas [ lem1 ] and [ lem1.1 ] , @xmath517 is uniformly bounded with respect to @xmath9 .",
    "therefore , there exists a constant @xmath88 such that @xmath518 following from lemma [ lem3](iii ) , @xmath519 as @xmath520 .    by ( [ lem3proofeq1 ] ) , @xmath521 .",
    "since @xmath358 is continuous in @xmath4 , it follows from theorem [ lem50 ] that @xmath359 converges to @xmath360 for any @xmath238 .",
    "furthermore , following from lemma [ lem1.1 ] and lebesgue s dominated convergence theorem , we conclude that @xmath357 converges to @xmath361 , and thus @xmath522 summarizing the convergence results of @xmath523 , @xmath490 and @xmath488 , we conclude that @xmath524 as @xmath520 .    by ( [ redprocrev ] ) , for @xmath525 , @xmath526 and @xmath527",
    ", we have @xmath528\\\\[-8pt ] & = & e(\\nu_i \\nu_j^t),\\nonumber\\end{aligned}\\ ] ] where the last equality follows from the result that @xmath330 is a martingale difference sequence [ lemma [ lem3](ii ) ] . by ( [ nunormeq222 ] )",
    ", there exists a constant @xmath88 such that @xmath529 which implies that @xmath530 \\biggl [ \\frac{1}{\\sqrt{n } } \\sum_{j = k_{\\sigma_s}}^n a_j^{({1+\\tau } ) /{2 } } \\biggr].\\ ] ] by kronecker s lemma and ( a@xmath173 ) , @xmath531 and thus @xmath532 in summary of ( [ reveffeq2 ] ) and ( [ noveq2 ] ) , we have @xmath533    by ( [ nunormeq2 ] ) , there exists a constant @xmath88 such that @xmath534 by kronecker s lemma and ( a@xmath173 ) , we have @xmath535 by lemma [ lem1](i ) and ( ii ) , where it is shown that @xmath325 is a martingale difference sequence , we have @xmath536 \\biggl [ \\sum_{k=1}^n e(e_k + \\nu_k ) \\biggr]^t\\\\ & = & \\biggl [ \\frac{1}{\\sqrt{n } } \\sum_{k=1}^n e(\\nu_k ) \\biggr ] \\biggl [ \\frac{1}{\\sqrt{n } } \\sum_{k=1}^n e(\\nu_k ) \\biggr]^t.\\end{aligned}\\ ] ] following from ( [ noveq22 ] ) , we have @xmath537 as @xmath520 .",
    "summarizing the convergence results of @xmath538 , @xmath539 and @xmath540 , the asymptotic covariance matrix of @xmath541 is equal to @xmath171 . combining with ( [ reveffeq4 ] ) ,",
    "we conclude that @xmath542 is an asymptotically efficient estimator of @xmath20 .    since @xmath542 and @xmath49 have the same asymptotic distribution @xmath543",
    ", @xmath49 is also asymptotically efficient as an estimator of @xmath20 .",
    "this concludes the proof of theorem  [ efftheorem ] .",
    "the theorems can be proved using theorems [ lem50 ] and [ contheorem ] by showing that samc satisfies the conditions ( a@xmath250 ) and ( a@xmath251 ) , as ( a@xmath120 ) is assumed , and ( a@xmath173 ) and and the condition @xmath544 have been verified in the text .",
    "_ verification of _ ( a@xmath250 ) . to simplify notation , in the proof we drop the subscript @xmath9 , denoting @xmath545 by @xmath7 and denote @xmath546 by @xmath547 .",
    "since the invariant distribution of the mh kernel is @xmath2 , we have for any fixed  @xmath4 , @xmath548}-\\pi_i \\\\ & = & \\frac { s_i}{s}-\\pi_i\\nonumber\\end{aligned}\\ ] ] for @xmath549 , where @xmath550 and @xmath551 .",
    "therefore , @xmath552    it follows from ( [ app22 ] ) that @xmath17 is a continuous function of @xmath4 .",
    "let @xmath553 , and define @xmath554 as in ( [ vfunctioneq ] ) .",
    "as shown below , @xmath204 is continuously differentiable . since @xmath555 \\leq1 $ ] for all @xmath109",
    ", @xmath204 takes values in the interval @xmath556 .    solving the system of equations formed by ( [ app22 ] )",
    ", we have the single solution @xmath557 where @xmath558 .",
    "it is obvious that @xmath559 , and @xmath80 has an empty interior , where @xmath20 is specified in theorem [ samcconvergence ] .",
    "therefore , ( a@xmath250)(iv ) is satisfied .    given the continuity of @xmath204 , for any numbers @xmath560 , @xmath561 , and @xmath76 is a compact set , where @xmath562 denotes the interior of the set @xmath36 .",
    "therefore , ( a@xmath250)(i ) and ( a@xmath250)(ii ) are verified .    to verify the condition ( a@xmath250)(iii ) , we have the following calculations : @xmath563\\\\[-8pt ] \\frac{\\partial ( { s_i}/{s } ) } { \\partial\\theta^{(i)}}&=&-\\frac { s_i}{s}\\biggl(1-\\frac{s_i}{s}\\biggr ) , \\qquad \\frac{\\partial ( { s_i}/{s } ) } { \\partial\\theta^{(j)}}= \\frac{\\partial ( { s_j}/{s } ) } { \\partial\\theta^{(j)}}=\\frac{s_i s_j}{s^2}\\nonumber\\end{aligned}\\ ] ] for @xmath564 and @xmath565 .",
    "let @xmath566 , then we have @xmath567\\\\ & = & \\frac{1}{\\lambda(\\theta ) } \\biggl [ \\sum_{j=1}^{m-1 } \\biggl(\\frac{s_j}{s}-\\pi_j\\biggr ) \\frac{s_i s_j}{s^2}- \\biggl(\\frac{s_i}{s}-\\pi_i\\biggr ) \\frac { s_i}{s } \\biggr ] \\\\",
    "& = & \\frac{1}{\\lambda(\\theta ) } \\biggl [ b \\mu_{\\xi } \\frac{s_i}{s}-\\biggl(\\frac{s_i}{s}-\\pi_i\\biggr ) \\frac{s_i}{s } \\biggr]\\end{aligned}\\ ] ] for @xmath568 , where it is defined @xmath569 .",
    "thus , @xmath570 \\nonumber \\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\qquad= -\\frac{1}{\\lambda(\\theta ) } \\biggl [ b \\sum_{i=1}^{m-1 } \\biggl(\\frac{s_i}{s}-\\pi_i\\biggr)^2 \\frac{s_i}{bs}- b^2 \\mu _ { \\xi } ^2 \\biggr ] \\nonumber\\\\ % [ \\sum_{i=1}^m ( \\frac{s_i}{s}-\\pi_i ) \\frac{s_i}{s } ] ^2 \\ } \\\\ & & \\qquad= - \\frac{1}{\\lambda(\\theta ) } \\bigl(b \\sigma_{\\xi}^2 + b(1-b ) \\mu_{\\xi } ^2 \\bigr ) \\leq0,\\nonumber\\end{aligned}\\ ] ] where @xmath571 denotes the variance of the discrete distribution defined in the following table :    [ cols=\"<,^,^,^ \" , ]     this implies that the matrix @xmath21 is negative definite and thus stable .",
    "applying taylor s expansion to @xmath17 at the point @xmath20 , we have @xmath572 for some constants @xmath87 $ ] and @xmath115 , by noting that @xmath573 and that the second derivatives of @xmath17 are uniformly bounded with respect to @xmath4 .",
    "therefore , ( a@xmath251 ) is satisfied .",
    "the author thanks the editor , associate editor and the referee for their constructive comments which have led to significant improvement of this paper .",
    "geyer , c. j. ( 1991 ) .",
    "markov chain monte carlo maximum likelihood . in _ computing science and statistics : proceedings of the 23rd symposium on the interface _",
    "( e. m. keramigas , ed . ) 156163 . interface foundation , fairfax , va .",
    "tadi , v. ( 1997 ) .",
    "convergence of stochastic approximation under general noise and stability conditions . in : _ proceedings of the 36th ieee conference on decision and control _ * 3 * 22812286 .",
    "ieee systems society , san diego , ca .",
    "younes , l. ( 1999 ) . on the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates .",
    "_ stochastics stochastics rep . _ * 65 * 177228 .",
    "yu , k. and liang , f. ( 2009 ) .",
    "efficient @xmath574-value evaluation for resampling - based tests . technical report ,",
    "dept . statistics , texas a&m univ .",
    ", college station , tx ."
  ],
  "abstract_text": [
    "<S> the subject of stochastic approximation was founded by robbins and monro [ _ ann . </S>",
    "<S> math . </S>",
    "<S> statist . _ * 22 * ( @xcite ) 400407 ] . after five decades of continual development </S>",
    "<S> , it has developed into an important area in systems control and optimization , and it has also served as a prototype for the development of adaptive algorithms for on - line estimation and control of stochastic systems . </S>",
    "<S> recently , it has been used in statistics with markov chain monte carlo for solving maximum likelihood estimation problems and for general simulation and optimizations . in this paper , we first show that the trajectory averaging estimator is asymptotically efficient for the stochastic approximation mcmc ( samcmc ) algorithm under mild conditions , and then apply this result to the stochastic approximation monte carlo algorithm [ liang , liu and carroll _ j. amer . statist . </S>",
    "<S> assoc . _ </S>",
    "<S> * 102 * ( @xcite ) 305320 ] . </S>",
    "<S> the application of the trajectory averaging estimator to other stochastic approximation mcmc algorithms , for example , a stochastic approximation mle algorithm for missing data problems , is also considered in the paper .    .    . </S>"
  ]
}