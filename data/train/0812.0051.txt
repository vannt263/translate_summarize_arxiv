{
  "article_text": [
    "let @xmath2 be a random sample from an unknown density @xmath3 .",
    "a kernel density estimator of @xmath4 is @xmath5 where @xmath6 is a smoothing parameter , also known as the bandwidth , and @xmath7 is the kernel , which is generally chosen to be a unimodal probability density function that is symmetric about zero and has finite variance .",
    "a popular choice for @xmath7 is the gaussian kernel : @xmath8 . to distinguish between estimators with different kernels",
    ", we shall refer to estimator with given kernel @xmath7 as a _",
    "@xmath7-kernel estimator_. choosing an appropriate bandwidth is vital for the good performance of a kernel estimate .",
    "this paper is concerned with a new method of data - driven bandwidth selection that we call _ indirect cross - validation _ ( icv ) .",
    "many data - driven methods of bandwidth selection have been proposed .",
    "the two most widely used are least squares cross - validation , proposed independently by   and  , and the   plug - in method .",
    "plug - in produces more stable bandwidths than does cross - validation , and hence is the currently more popular method .",
    "nonetheless , an argument can be made for cross - validation since it requires fewer assumptions than plug - in and works well when the density is difficult to estimate ; see .",
    "a survey of bandwidth selection methods is given by  .",
    "a number of modifications of lscv has been proposed in an attempt to improve its performance .",
    "these include the biased method of  , a method of  , the trimmed of  , the modified of  , and the method of   based on kernel contrasts .",
    "the icv method is similar in spirit to one - sided cross - validation ( oscv ) , which is another modification of cross - validation proposed in the regression context by  . as in oscv",
    ", icv initially chooses the bandwidth of an @xmath9-kernel estimator using least squares cross - validation . multiplying the bandwidth chosen at this initial stage by a known constant results in a bandwidth ,",
    "call it @xmath10 , that is appropriate for use in a gaussian kernel estimator .    a popular means of judging a kernel estimator",
    "is the mean integrated squared error , i.e. , @xmath11 $ ] , where @xmath12 letting @xmath13 be the bandwidth that minimizes @xmath14 when the kernel is gaussian , we will show that the mean squared error of @xmath10 as an estimator of @xmath13 converges to 0 at a faster rate than that of the ordinary lscv bandwidth .",
    "we also describe an unexpected bonus associated with icv , namely that , unlike lscv , it is robust to rounded data .",
    "a fairly extensive simulation study and two data analyses confirm that icv performs better than ordinary cross - validation in finite samples .",
    "we begin with some notation and definitions that will be used subsequently . for an arbitrary function @xmath15 , define @xmath16 the lscv criterion is given by @xmath17 where , for @xmath18 , @xmath19 denotes a kernel estimator using all the original observations except for @xmath20 .",
    "when @xmath21 uses kernel @xmath7 , @xmath22 can be written as [ eq : lscv1 ] lscv(h)&=&r(k)+_ijk(t)k(t+)dt + & & -_ijk ( ) . it is well known that @xmath23 is an unbiased estimator of @xmath24 , and hence the minimizer of @xmath23 with respect to @xmath25 is denoted @xmath26 .      our aim is to choose the bandwidth of a _ second order _ kernel estimator .",
    "a second order kernel integrates to 1 , has first moment 0 , and finite , nonzero second moment . in principle",
    "our method can be used to choose the bandwidth of any second order kernel estimator , but in this article we restrict attention to @xmath27 , the gaussian kernel .",
    "it is well known that a @xmath28-kernel estimator has asymptotic mean integrated squared error ( mise ) within 5% of the minimum among all positive , second order kernel estimators .",
    "indirect cross - validation may be described as follows :    * select the bandwidth of an @xmath9-kernel estimator using least squares cross - val- idation , and call this bandwidth @xmath29 .",
    "the kernel @xmath9 is a second order kernel that is a linear combination of two gaussian kernels , and will be discussed in detail in section [ sec : wildk ] .",
    "* assuming that the underlying density @xmath3 has second derivative which is continuous and square integrable , the bandwidths @xmath30 and @xmath31 that asymptotically minimize the @xmath32 of @xmath28- and @xmath9-kernel estimators , respectively , are related as follows : [ eq : hnbn ] h_n=()^1/5b_ncb_n . *",
    "define the indirect cross - validation bandwidth by @xmath33 .",
    "importantly , the constant @xmath34 depends on no unknown parameters .",
    "expression ( [ eq : hnbn ] ) and existing cross - validation theory suggest that @xmath35 will at least converge to 1 in probability , where @xmath13 is the minimizer of @xmath32 for the @xmath28-kernel estimator .",
    "henceforth , we let @xmath26 denote the bandwidth that minimizes @xmath23 with @xmath27 .",
    "theory of   and   shows that the relative error @xmath36 converges to 0 at the rather disappointing rate of @xmath1 .",
    "in contrast , we will show that @xmath37 can converge to 0 at the rate @xmath0 .",
    "kernels @xmath9 that are sufficient for this result are discussed next .",
    "we consider the family of kernels @xmath38 , where , for all @xmath39 , @xmath40 note that the gaussian kernel is a special case of   when @xmath41 or @xmath42 .",
    "each member of @xmath43 is symmetric about 0 and such that @xmath44 .",
    "it follows that kernels in @xmath43 are second order , with the exception of those for which @xmath45 .",
    "the family @xmath43 can be partitioned into three families : @xmath46 , @xmath47 and @xmath48 .",
    "the first of these is @xmath49 .",
    "each kernel in @xmath50 has a negative dip centered at @xmath51 . for @xmath52 fixed , the smaller",
    "@xmath53 is , the more extreme the dip ; and for fixed @xmath53 , the larger @xmath52 is , the more extreme the dip .",
    "the kernels in @xmath54 are ones that `` cut - out - the - middle . ''",
    "the second family is @xmath55 .",
    "kernels in @xmath56 are densities which can be unimodal or bimodal . note that the gaussian kernel is a member of this family .",
    "the third sub - family is @xmath57 , each member of which has negative tails",
    ". examples of kernels in @xmath58 are shown in figure  [ fig : lnegative ] .",
    "kernels in @xmath54 and @xmath59 are not of the type usually used for estimating @xmath3 .",
    "nonetheless , a worthwhile question is `` why not use @xmath9 for both estimation of @xmath3 ? ''",
    "one could then bypass the step of rescaling @xmath60 and simply estimate @xmath3 by an @xmath9-kernel estimator with bandwidth @xmath60 .",
    "the ironic answer to this question is that the kernels in @xmath43 that are best for purposes are very inefficient for estimating @xmath3 .",
    "indeed , it turns out that an @xmath9-kernel estimator based on a sequence of icv - optimal kernels has @xmath32 that does not converge to 0 faster than @xmath61 . in contrast , the @xmath32 of the best @xmath28-kernel estimator tends to 0 like @xmath62 .",
    "these facts fit with other paradoxes , which include the fact that lscv outperforms other methods when the density is highly structured , , the improved performance of in multivariate density estimation , , and its improvement when the true density is not smooth , .",
    "one could paraphrase these phenomena as follows : `` the more difficult the function is to estimate , the better seems to perform . '' in our work , we have in essence made the function more difficult to estimate by using an inefficient kernel @xmath9 .",
    "more details on the @xmath32 of @xmath9-kernel estimators may be found in  .",
    "the theory presented in this section provides the underpinning for our methodology .",
    "we first state a theorem on the asymptotic distribution of @xmath10 , and then derive asymptotically optimal choices for the parameters @xmath52 and @xmath53 of the selection kernel .",
    "classical theory of   and   entails that the bias of an lscv bandwidth is asymptotically negligible in comparison to its standard deviation .",
    "we will show that the variance of an icv bandwidth can converge to 0 at a faster rate than that of an lscv bandwidth .",
    "this comes at the expense of a squared bias that is _ not _ negligible .",
    "however , we will show how to select @xmath52 and @xmath53 ( the parameters of the selection kernel ) so that the variance and squared bias are balanced and the resulting mean squared error tends to 0 at a faster rate than does that of the lscv bandwidth .",
    "the optimal rate of convergence of the relative error @xmath37 is @xmath0 , a substantial improvement over the infamous @xmath1 rate for lscv .    before stating our main result concerning the asymptotic distribution of @xmath10",
    ", we define some notation : @xmath63 @xmath64,\\ ] ] @xmath65 @xmath66,\\ ] ] @xmath67 note that to simplify notation , we have suppressed the fact that @xmath9 , @xmath68 and @xmath69 depend on the parameters @xmath52 and @xmath53 .",
    "an outline of the proof of the following theorem is given in the appendix .",
    "* theorem .",
    "*  _ assume that @xmath3 and its first five derivatives are continuous and bounded and that @xmath70 exists and is lipschitz continuous .",
    "suppose also that @xmath71 for any sequence of random variables @xmath72 such that @xmath73 , a.s .",
    "then , if @xmath74 and @xmath52 is fixed , @xmath75 as @xmath76 and @xmath77 , where @xmath78 converges in distribution to a standard normal random variable , [ eq : sd ] s_n= ( ) c _ , and [ eq : bias ] b_n=()^2/5d_. _    * remarks *    * assumption ( [ cond1 ] ) is only slightly stronger than assuming that @xmath79 converges in probability to 1 . to avoid making our paper overly technical we have chosen not to investigate sufficient conditions for ( [ cond1 ] )",
    ". however , this can be done using techniques as in   and  .",
    "* theorem 4.1 of   on asymptotic normality of lscv bandwidths is not immediately applicable to our setting for at least three reasons : the kernel @xmath9 is not positive , it does not have compact support , and , most importantly , it changes with @xmath80 via the parameter @xmath53 . *",
    "the assumption of six derivatives for @xmath3 is required for a precise quantification of the asymptotic bias of @xmath81 .",
    "our proof of asymptotic normality of @xmath82 only requires that @xmath3 be four times differentiable , which coincides with the conditions of theorem 4.1 in  . * the asymptotic bias @xmath83 is positive , implying that the icv bandwidth tends to be larger than the optimal bandwidth .",
    "this is consistent with our experience in numerous simulations .    in the next section",
    "we apply the results of our theorem to determine asymptotically optimal choices for @xmath52 and @xmath53 .",
    "the limiting distribution of @xmath84 has second moment @xmath85 , where @xmath86 and @xmath83 are defined by ( [ eq : sd ] ) and ( [ eq : bias ] ) .",
    "minimizing this expression with respect to @xmath53 yields the following asymptotically optimal choice for @xmath53 : [ eq : sigopt ] _",
    "n , opt = n^3/8()^5/4 ^ 5/8 .",
    "the corresponding asymptotically optimal mean squared error is [ eq : optmse ] mse_n , opt = n^-1/2c_d _ , which confirms our previous claim that the relative error of @xmath87 converges to 0 at the rate @xmath0 .",
    "the corresponding rates for lscv and the sheather - jones plug - in rule are @xmath1 and @xmath88 , respectively .",
    "because @xmath52 is not confounded with @xmath3 in @xmath89 , we may determine a single optimal value of @xmath52 that is independent of @xmath3 .",
    "the function @xmath90 of @xmath52 is minimized at @xmath91 .",
    "furthermore , small choices of @xmath52 lead to an arbitrarily large increase in mean squared error , while the mse at @xmath92 is only about 1.33 times that at the minimum .",
    "our theory to this point applies to kernels in @xmath48 , i.e. , kernels with negative tails .",
    "has developed similar theory for the case where @xmath93 , which corresponds to @xmath94 , i.e. , kernels that apply negative weights to the smallest spacings in the lscv criterion .",
    "interestingly , the same optimal rate of @xmath0 results from letting @xmath93 . however , when the optimal values of @xmath95 are used in the respective cases ( @xmath93 and @xmath77 ) , the limiting ratio of optimum mean squared errors is @xmath96 , with @xmath77 yielding the smaller error .",
    "our simulation studies confirm that using @xmath9 with large @xmath53 does lead to more accurate estimation of the optimal bandwidth .",
    "in order to have an idea of how good choices of @xmath52 and @xmath53 vary with @xmath80 and @xmath3 , we determined the minimizers of the asymptotic mean squared error of @xmath10 for various sample sizes and densities . in doing so , we considered a single expression for the asymptotic mean squared error that is valid for either large or small values of @xmath53 .",
    "furthermore , we use a slightly enhanced version of the asymptotic bias of @xmath10 .",
    "the first order bias of @xmath87 is @xmath97 , or @xmath98 , where [ eq : b_n ] b_n=()^1/5n^-1/5h_n=()^1/5n^-1/5 .",
    "now , the term @xmath99 is of smaller order asymptotically than @xmath100 and hence was deleted in the theory of section [ sec : theory ] . here",
    "we retain @xmath99 , and hence the @xmath52 that minimizes the mean squared error depends on both @xmath80 and @xmath3 .",
    "we considered the following five normal mixtures defined in the article by  :    [ cols= \" < , < \" , ]     interestingly , a high percentage of the defaulters have credit scores less than 620 , which many lenders consider the minimum score that qualifies for a loan ; see .      for this example we took five samples of size @xmath101 from the kurtotic unimodal density defined in  .",
    "first , we note that even the bandwidth that minimizes @xmath102 results in a density estimate that is much too wiggly in the tails . on the other hand , using local versions of either icv or lscv resulted in much better density estimates , with local icv producing in each case a visually better estimate than that produced by local lscv .    for the local lscv and icv methods we considered four values of @xmath103 ranging from 0.05 to 0.3 .",
    "a selection kernel with @xmath104 and @xmath105 was used in local icv .",
    "this @xmath95 choice performs well for global bandwidth selection when the density is unimodal , and hence seems reasonable for local bandwidth selection since locally the density should have relatively few features . for a given @xmath103 , the local icv and lscv bandwidths",
    "were found for @xmath106 , and were interpolated at other @xmath107 $ ] using a spline .",
    "average squared error ( ase ) was used to measure closeness of a local density estimate @xmath108 to the true density @xmath3 : @xmath109    figure  [ fig : loc_estimates ] shows results for one of the five samples .",
    "estimates corresponding to the smallest and the largest values of @xmath103 are provided . the local icv method",
    "performed similarly well for all values of @xmath103 considered , whereas all the local lscv estimates were very unsmooth , albeit with some improvement in smoothness as @xmath103 increased .     & + @xmath110&@xmath111 +   + & + @xmath112&@xmath113 +",
    "a widely held view is that kernel choice is not terribly important when it comes to estimation of the underlying curve . in this paper",
    "we have shown that kernel choice can have a dramatic effect on the properties of .",
    "cross - validating kernel estimates that use gaussian or other traditional kernels results in highly variable bandwidths , a result that has been well - known since at least 1987 .",
    "we have shown that certain kernels with low efficiency for estimating @xmath3 can produce cross - validation bandwidths whose relative error converges to 0 at a faster rate than that of gaussian - kernel cross - validation bandwidths .",
    "the kernels we have studied have the form @xmath114 , where @xmath28 is the standard normal density and @xmath52 and @xmath53 are positive constants .",
    "the interesting selection kernels in this class are of two types : unimodal , negative - tailed kernels and `` cut - out the middle kernels , '' i.e. , bimodal kernels that go negative between the modes .",
    "both types of kernels yield the rate improvement mentioned in the previous paragraph .",
    "however , the best negative - tailed kernels yield bandwidths with smaller asymptotic mean squared error than do the best `` cut - out - the - middle '' kernels .    a model for choosing",
    "the selection kernel parameters has been developed .",
    "use of this model makes our method completely automatic . a simulation study and examples",
    "reveal that use of this method leads to improved performance relative to ordinary lscv .",
    "to date we have considered only selection kernels that are a linear combination of two normal densities .",
    "it is entirely possible that another class of kernels would work even better . in particular",
    ", a question of at least theoretical interest is whether or not the convergence rate of @xmath0 for the relative bandwidth error can be improved upon .",
    "here we outline the proof of our theorem in section [ sec : theory ] .",
    "a much more detailed proof is available from the authors .",
    "we start by writing t_n(b_0)&=&t_n(b_ucv)+(b_0-b_ucv)t_n^(1)(b_0)+ ( b_0-b_ucv)^2t_n^(2)(b ) + & = & -nr(l)/2+(b_0-b_ucv)t_n^(1)(b_0)+ ( b_0-b_ucv)^2t_n^(2)(b ) , where @xmath72 is between @xmath115 and @xmath60 , and so @xmath116 using condition ( [ cond1 ] ) we may write the last equation as [ eq : delta ] ( b_ucv - b_0)=+o_p ( ) .    defining @xmath117 and @xmath118 , we have @xmath119 using the central limit theorem of  , it can be verified that @xmath120 computation of the first two moments of @xmath121 reveals that @xmath122 and so @xmath123    at this point we need the first two moments of @xmath124 .",
    "a fact that will be used frequently from this point on is that @xmath125 , @xmath126 . using our assumptions on the smoothness of @xmath3 , taylor series expansions , symmetry of @xmath68 about 0 and @xmath127 , @xmath128 recalling the definition of @xmath31 from ( [ eq : b_n ] )",
    ", we have [ eq : et ] _ n&=&-b_0 ^ 5_4r(f)+b_0 ^ 7_6r(f ) + & & + b_n^5_2l^2r(f)+o(n^2b_0 ^ 8 ^ 7 ) . let @xmath129 denote the mise of an @xmath9-kernel estimator with bandwidth @xmath130 . then @xmath131 $ ] , implying that [ eq : secord ] b_n^5=b_0 ^ 5 + 5b_0 ^ 4+o . using a second order approximation to @xmath132 and a first order approximation to @xmath133",
    ", we then have @xmath134 substitution of this expression for @xmath31 into ( [ eq : et ] ) and using the facts @xmath135 , @xmath136 and @xmath137 , it follows that @xmath138 . later in the proof",
    "we will see that this last result implies that the first order bias of @xmath10 is due only to the difference @xmath97 .",
    "tedious but straightforward calculations show that @xmath139 , where @xmath140 is as defined in section [ sec : mse ] .",
    "it is worth noting that @xmath141 , where @xmath142 and @xmath143 .",
    "one would expect from theorem 4.1 of   that the factor @xmath144 would appear in @xmath145 .",
    "indeed it does implicitly , since @xmath146 as @xmath77 .",
    "our point is that , when @xmath77 , the part of @xmath9 depending on @xmath53 is negligible in terms of its effect on @xmath144 and also @xmath147 .    to complete the proof write & = & + o_p + & = & + + o_p .",
    "applying the same approximation of @xmath115 that led to ( [ eq : secord ] ) , and the analogous one for @xmath13 , we have & = & b_n^2-h_n^2+o(b_n^2 ^ 2+h_n^2 ) + & = & n^-2/5+o(b_n^2 ^ 2 ) .",
    "it is easily verified that , as @xmath77 , @xmath148 , @xmath149 and @xmath150 , and hence @xmath151.\\ ] ] the proof is now complete upon combining all the previous results .",
    "the authors are grateful to david scott and george terrell for providing valuable insight about , and to three referees and an associate editor , whose comments led to a much improved final version of our paper .",
    "the research of savchuk and hart was supported in part by nsf grant dms-0604801 ."
  ],
  "abstract_text": [
    "<S> a new method of bandwidth selection for kernel density estimators is proposed . </S>",
    "<S> the method , termed _ </S>",
    "<S> indirect cross - validation _ , or icv , makes use of so - called _ selection _ kernels . </S>",
    "<S> least squares cross - validation ( lscv ) is used to select the bandwidth of a selection - kernel estimator , and this bandwidth is appropriately rescaled for use in a gaussian kernel estimator . </S>",
    "<S> the proposed selection kernels are linear combinations of two gaussian kernels , and need not be unimodal or positive . theory is developed showing that the relative error of icv bandwidths can converge to 0 at a rate of @xmath0 , which is substantially better than the @xmath1 rate of lscv . </S>",
    "<S> interestingly , the selection kernels that are best for purposes of bandwidth selection are very poor if used to actually estimate the density function . </S>",
    "<S> this property appears to be part of the larger and well - documented paradox to the effect that `` the harder the estimation problem , the better performs . '' </S>",
    "<S> the icv method uniformly outperforms lscv in a simulation study , a real data example , and a simulated example in which bandwidths are chosen locally .    </S>",
    "<S> key words : kernel density estimation ; bandwidth selection ; cross - validation ; local cross - validation . </S>"
  ]
}