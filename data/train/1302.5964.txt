{
  "article_text": [
    "information processing in the brain is carried out by a complex network of neurons communicating by sending reliable stereotypical electrical pulses known as action potentials , or spikes .",
    "thus , the information is encoded in a sequence of events over continuous time , and not in the amplitude of the signal as is common in signal processing applications ( see fig .",
    "[ fig : spiketrain ] ) . studying how information is represented and processed as spike trains  known as the neural coding problem  is one of the key challenges of neuroscience .",
    "we venture to say that the theory of how to represent information in continuous , infinite dimensional spaces is also far from being understood in the signal processing and machine learning communities . in light of the current signal processing focus in sparseness ,",
    "point processes ( that generate spike trains ) are very appealing , since a point process provides the natural limiting case of sparse priors that underlie compressive sensing , and it implements the ultimate sparse representation : the system only communicates when the information crosses some internal threshold .",
    "this strategy saves power , and provides naturally a sparse representation in time , so the costly step of finding alternative spaces to map the input data for sparseness is unnecessary .",
    "the problem is that the system becomes less observable , and therefore algorithms intended to predict , control or otherwise process the incoming information are less effective and much more cumbersome .",
    "the early attempts in the engineering literature to apply stochastic process theory to zero crossing analysis ( a simple way to create a point process ) started in the 40 s with rice at the bell labs , and found applications in frequency modulation ( fm ) and shot noise .",
    "the theory of point processes developed primarily in the statistics literature  @xcite and currently this theory is the most widely used approach to quantify spike trains in computational neuroscience as well as in all other areas of science and engineering .",
    "point processes are also important for machine learning , in particular for online learning that deals with data streams , because of the shortcomings of vector spaces to represent both unbounded data and the resulting inference structure obtained after processing .     that action potentials are detected .",
    ", scaledwidth=48.0% ]    since the spike train space is devoid of an algebra , it imposes many challenges to signal processing methods .",
    "we must then first establish a space for computation or a transformation to a space with the necessary properties .",
    "the approach explained here is to define a proper kernel function on spike trains to capture nonparametrically the temporal structure and the variability of the spike trains of interest .",
    "once a positive definite kernel is defined , it maps the spike trains into a hilbert space of functions which allows signal processing tools to be applied directly through the kernel trick .",
    "this methodology has the potential to enlarge the footprint of digital signal processing to objects that are non - numeric , i.e. , we can filter spike trains , decompose them in principal components , and perform inference , with exactly the same tools available for time series defined in @xmath0 .",
    "but more importantly , the use of kernels provides an opportunity for a myriad of advanced machine learning tools such as gaussian processes , and probability embedding to be applied to spike trains , opening up a new frontier for next generation spike train signal processing .",
    "the idea of a neural code is prevalent in the sensory and motor systems where the variables of interest are directly observable , although it is latent in all neuronal communication . in a sensory system",
    ", we would often like to understand how the sensory stimuli are encoded and transformed in each stage of neural processing .",
    "for example , visual stimuli excite a cascade of neurons in the visual pathway from photoreceptor neurons , and retinal ganglion cells in the eye to various areas of the visual cortex . by analyzing the spike trains",
    ", we can understand how certain aspects of a stimulus are processed and represented in these neurons  @xcite .",
    "the study of neural code often consists of    1 .",
    "identifying neurons that encode certain features of interest ( neuron identification ) , and 2 .   finding the functional relation between the feature and spike trains of the identified neurons ( neural encoding / decoding ) .",
    "a major challenge in neuron identification is the neural variability , or `` noise '' , in the system .",
    "for example , when a fixed stimulus is repeatedly presented to the system , the trial - to - trial variability of the neural response is often complex ( see fig .  [",
    "fig : ttv ] ) .",
    "therefore , to determine if a neuron encodes for the variable of interest @xmath1 , one can not compare simply single trial responses , but requires _ collections _ of responses that are samples from the stimulus - conditional distribution @xmath2 .",
    "fortunately , the tools from signal detection theory and hypothesis testing can be extended to stochastic spike trains via kernels to solve the neural identification problem ( section  [ sec : mmd ] ) .",
    "on the other hand , a better method for reading out the stimulus from the spike trains ( neural decoding ) can have major impact on a number of clinical and biomedical applications .",
    "for example , it can improve sensory prosthetics such as cochlear implants , which are widely used , and retinal and tactile prosthetics that are under active development  @xcite . in motor systems , identifying which neurons are involved in motor planning and control , and understanding how information is represented in spike trains is essential in building motor prosthetics  @xcite .",
    "similar approaches have been taken for various higher - level cognitive systems such as decision making , memory , and language  @xcite .",
    "spike train kernels provide alternative tools to the neural coding problem ( fig .",
    "[ fig : decoding ] ) .",
    "traditionally , the `` rate code '' has been the dominant idea in neuroscience  @xcite , and it has been repeatedly demonstrated experimentally .",
    "the rate code hypothesis states that the average spike count encodes all information underlying the stimulus , i.e. , that the spike timing is not useful for neural processing .",
    "contrary to the rate code hypothesis , there is also ample evidence for the so - called `` temporal code '' hypothesis which states that extra information is encoded in spike timings  @xcite . the neuroscience community , however , has largely relegated the possibility of a temporal code to a secondary role perhaps due to the large dimensionality of the neural code space and the limited ability of statistical methods that directly operate on spike trains and are powerful enough to discover new patterns .",
    "if the brain processes and communicates sensory data optimally amongst neurons , one natural solution is to utilize a representation that preserves as much of the information as possible  @xcite . along this line of reasoning , the timing hypothesis should be the preferred theory because it is the one that guarantees no loss of information and it solves the conundrum : in cases where it is impossible to use rates ( when the response time has to be minimized ) , spike times are preferred , but a representation that is sensitive to spike times also can easily represent rates by integration . practically the argument between rates and timing is also biased by the degree of sophistication of the mathematical tools required : it is difficult to quantify spike timings , while it is very easy to process rates , therefore there may have been many experimental observations that corroborate the spike timing hypothesis that were never published because researchers could not quantify appropriately their data .",
    "spike train kernels shine a new light into this controversy by providing a general framework for studying spike trains that can accommodate both hypotheses .",
    "we hope that , by focusing on what is common , the spike train kernel approach may kindle experimental research to show that different neuron classes are optimized for different time scales of processing , just like engineers design differently transistors for high speed cmos and sample and hold circuits .",
    "the practicality of signal processing is due to a clever exploitation of the linear model .",
    "unfortunately , not all the problems we want to solve are well approximated by the linear model .",
    "the hilbert space approach  @xcite , and more specifically the reproducing kernel hilbert space ( rkhs )  @xcite extend the linear model mathematics to nonlinear modeling in the input space .",
    "the methodology is principled because it provides a general way to handle different types of nonlinearity , the optimization is convex , and the methodology is still practical in terms of computational complexity .",
    "but in our opinion , the true importance of kernel methods for neural signal processing is their ability of map abstract objects to an hilbert space  a linear functional space equipped with an inner product .",
    "indeed , at the core of the above mentioned problems in neuroscience to quantify spike trains is the lack of standard algebraic operations such as linear projection and linear combination for spike trains .",
    "this mapping supplies the required structure for applying most signal processing tools , and also allows otherwise complex nonlinear computation .",
    "the theory of reproducing kernel hilbert spaces provides a foundation for the existence of a ( possibly ) infinite dimensional hilbert space  a feature space  associated with any positive definite function of two arguments called a kernel  @xcite .",
    "let the input space data @xmath3 be an object ",
    ", a point in @xmath4 , a graph , or a spike train  and a kernel @xmath5 be a real - valued bivariate function defined in the input space @xmath3 .",
    "the input sample @xmath6 is mapped to the rkhs as the function @xmath7 , therefore the kernel specifies the richness of the transformation .",
    "the kernel defines also the inner product of the hilbert space , i.e. the kernel @xmath8 provides the similarity in the rkhs of the functional images of any two samples @xmath1 and @xmath9 in the input space and encapsulates any prior knowledge about the input space structure .",
    "moreover , the inner product of two functions in the rkhs can be computed by a scalar kernel evaluation in the input space , i.e. @xmath10 .",
    "this property brings computational simplicity , therefore we have a principled framework that allows nonlinear signal processing with linear algorithms and makes working with functions practical .    for any finite set of points in the input space @xmath11 , the resulting matrix @xmath12 , @xmath13 must be symmetric and positive semi - definite for a proper kernel @xmath14 , i.e. , for any real vector @xmath15 , @xmath16 .",
    "given data in @xmath3 , the kernel matrix @xmath12 represents the inner product between each pair in the hilbert space .",
    "the kernel matrix plays a central role in kernel method algorithms , because for most algorithms , it contains all information required about the input .",
    "let us illustrate the importance of the kernel design with kernel mappings on the real line where we know the feature space explicitly .",
    "if we map @xmath17 to a three - dimensional feature vector @xmath18 \\in { { \\ensuremath{\\mathbb{r}}}}^3 $ ] , then linear regression in the feature space corresponds to a quadratic fit in @xmath0 .",
    "equivalently , this quadratic fit can be achieved by kernel least squares using the polynomial kernel @xmath19 without explicitly constructing the feature space  @xcite .",
    "this is because the least squares linear regression only requires operations provided by the hilbert space ( linear combination , and inner product ) and the polynomial kernel is the inner product of the feature space .",
    "the advantage of kernel method is avoiding the intermediate feature space representation especially when it is of high dimension .",
    "one popular kernel is the gaussian ( a.k.a .",
    "squared exponential ) kernel @xmath20 , which implicitly corresponds to an infinite dimensional feature space .",
    "it captures the local similarity in the real line ; @xmath1 and @xmath21 are assumed to be very similar if @xmath22 , and gradually becomes dissimilar as @xmath23 increases .",
    "however , the choice of @xmath24 is critical .",
    "as the kernel size parameter @xmath24 tends to zero , it approaches the trivial kernel , @xmath25 , and @xmath26 for @xmath27 , that still maps the input to an infinite dimensional space where every mapped input point is orthogonal to each other , and hence the feature space has no ability to generalize and basically acts as a lookup table . on the other hand ,",
    "if @xmath24 is larger than the dynamic range of the data , the gaussian kernel provides basically a constant mapping of the argument , therefore the feature space is unable to weight distances differently and looses the ability to discriminate distinct points .",
    "these two example kernels are extremes that do not bring any advantage . in practice",
    ", we use a kernel ( and kernel size ) that is in between , one that provides a rich feature space with proper smoothing such that the practitioner can use it to nonlinearly interpolate between data points ( e.g. , in the scale of @xmath24 for the gaussian kernel ) .",
    "in the previous section we discussed kernel methods on the real line , but the beauty of the theory is that it can be applied to other more abstract spaces .",
    "in fact , various discrete objects naturally represented as a collection such as graphs , sets , and strings have been successfully embedded in hilbert spaces through kernels  @xcite , opening the door for many traditional tools to be directly applied when a suitable kernel is used .",
    "the only difference with respect to the gaussian kernel is that we now need a way to define the kernel in a way that is relevant to measuring the similarity between spike trains .",
    "once this is done , we can replicate the same operations as explained for the gaussian kernel on the real line , i.e. , we can quantify similarity between spike trains and define a space where we can build signal processing models and perform inferences .    in the remainder of the section",
    ", we introduce several important spike train kernels and discuss their advantages and disadvantages .",
    "a trivial way of constructing a spike train kernel is by first mapping the spike trains into a finite and fixed dimensional euclidean space and using its inner product : @xmath28 .",
    "for example , simply counting the total number of spikes in each spike train maps spike trains into natural numbers which is a subset of the real line .",
    "the resulting kernel is called the _ count kernel_. count kernel completely ignores the timing , but it is useful because it encompasses the conventional rate coding analysis done by neuroscientists .",
    "for instance , the optimal least squares linear decoder is equivalent to the kernel least squares , and the test of mean rate difference is equivalent to mmd ( see section  [ sec : mmd ] ) with the count kernel .",
    "a nave extension of the count kernel is to bin the spike train by choosing a sequence of consecutive time windows ( fig .",
    "[ fig : binning ] ) .",
    "neighboring time bins corresponds to different dimensions , and hence the kernel does not bring any smoothing across the bin boundaries . in the limit of fine time binning , all information of the continuous representation is preserved in the binned representation at the expense of huge dimensionality . when combined with the euclidean inner product , binning in this regime is catastrophic because the inner product implements a look - up table , like the trivial kernel mentioned earlier . on the other hand ,",
    "when the bin size is larger , the temporal continuity within each time bin is respected , and some smoothing is provided , however , the resulting feature space is low dimensional , and temporal details in the spike trains can not be fully represented . for some applications , there is a sweet spot for the bin size that may perform well , since it can partially extract linear relations with respect to the rate code hypothesis  @xcite .",
    "the linear model on binned data , so popular in brain machine interfaces ( bmis )  @xcite is one example of this technique .",
    "although directly binning spike trains as objects in the euclidean space can be misleading , a better kernel can be constructed using this representation but different inner product .",
    "the first successful kernel for neuroscience is the _ spikernel _",
    "@xcite which falls in this category .",
    "it allows local time manipulation ( time warping ) enabling spike counts in neighboring bins to be matched , effectively smoothing over bin boundaries .",
    "it also weights different time bins according to the distance from the time of interest , which is a reasonable assumption based on the finite memory of neural systems .",
    "the spikernel has been successfully demonstrated to perform better than binned count kernel in the context of brain - machine interfaces ( decoding the motor signal from neural activity )  @xcite .",
    "in general , the spikernel performs robustly  @xcite , however , it fundamentally lacks the ability to control its temporal precision since it is tied to a binned representation .",
    "in addition , it should be noted that the spikernel is computationally expensive to evaluate , and it requires tuning five free parameters , including bin size .",
    "the relatively large number of free parameters delivers a flexible kernel , which is supported by its performance , but tuning these parameters requires an extensive optimization that hinders its appeal .",
    "as we have seen earlier , the binning transformation is lossy  many spike trains can be mapped to the same binned representation  and similar spike trains can be mapped to quite different representations .",
    "how can we avoid binning and preserve all information and create a positive definite kernel ?",
    "one solution is to use an infinite dimensional representation  @xcite .",
    "let @xmath29 be a finite energy impulse response of a linear filter over time ( possibly non - causal ) , and represent the spike train as a sum of dirac delta functions @xmath30 ( non - binned representation , see fig .",
    "[ fig : binning ] ) .",
    "each spike train can be uniquely transformed into a function via convolution with a non - trivial @xmath29 : @xmath31 the resulting transformed spike train @xmath32 is a function in @xmath33 space , which is a hilbert space on its own .",
    "the inner product in @xmath33 is defined as , @xmath34 by choosing a locally concentrated @xmath29 , spike trains with similar small jitter are mapped to similar functions ( fig .",
    "[ fig : linear]a )",
    ". therefore , it is continuous with respect to small temporal perturbations .",
    "the _ linear functional kernel _ is simply defined as , @xmath35 that is , the inner product of the two smoothed functional representations in @xmath33 ( fig .",
    "[ fig : linear ] ) .     in red .",
    "the spike timings of the first spike train are jittered to generate the second spike train , and their smoothed representation are similar in @xmath33 .",
    "( b ) demonstration of linearity of linear functional kernels using @xmath36 .",
    "the third spike train is a superposition of the first two spike trains , and so is the corresponding smoothed representation .",
    "a schematic of the three vectors in @xmath33 explicitly indicates their linearity .",
    ", scaledwidth=48.0% ]    note that can be rewritten with an explicit summation over all pairs of spikes , @xmath37 where @xmath38 .",
    "therefore , the kernel evaluation in the infinite dimensional space can be computed with @xmath39 function evaluations of @xmath40 where @xmath41 is the number of spikes in the spike train @xmath1 .",
    "a few special choices of @xmath29 are worth noting  @xcite . if the smoothing function has the form of a gaussian function , @xmath42 , then @xmath43 .",
    "more importantly , if the smoothing filter is a causal exponential decay @xmath44 for @xmath45 , then we obtain the following kernel : @xmath46 this spike train kernel has two distinct advantages .",
    "first , it has a neurophysiological interpretation , since the synaptic transfer function that transforms the spike trains to an analogue intracellular signal in the downstream neuron can be approximated as a first order dynamical system ( i.e. , first order iir filter )  @xcite .",
    "second , it can be computed in @xmath47 time  @xcite .",
    "the fast computation is due to properties of the double exponential function  @xcite .",
    "it is easy to see that the linear functional kernels are linear with respect to the superposition of spike trains . if @xmath48 , @xmath49 , and @xmath50 are spike trains represented as sum of delta functions , @xmath51 ( fig .",
    "[ fig : linear]b ) .",
    "therefore , the functions built on this space have the same constraint ; the function value for the superposition @xmath52 is the sum of the function value for @xmath48 and @xmath49 .",
    "note that the binned spike train kernels share this property of linearity with respect to superposition .",
    "we will see that this limitation can be a critical weakness .      to unleash the full potential of kernel methods , we need binless nonlinear spike train kernels .",
    "there are several ways to extend the linear functional kernels to be nonlinear  @xcite . here , we focus on building the schoenberg kernel since it provides a provably universal kernel .",
    "schoenberg kernels are derived from the radial basis functions , and takes the following form , @xmath53 where the function @xmath54 is completely monotone on @xmath55 but not a constant function  @xcite .",
    "examples of completely monotone functions are @xmath56 where @xmath57 and @xmath58 are constants  @xcite .",
    "we take the functional norm derived from the linear functional kernel , that is , @xmath59 next , we build radial basis kernels on top of the feature space induced by the linear functional kernel . therefore , in schoenberg kernels , the underlying linear functional kernel provides the smoothness in the space , and the radial basis function @xmath60 enforces the linear superposition to only hold locally .",
    "this combination guarantees the resulting kernel to be powerful for both neural identification and decoding applications .",
    "a typical choice is to use as @xmath61 with @xmath62 which results in the following form : @xmath63 this can be considered as an analogue of the widely used gaussian kernel for euclidean space .",
    "schoenberg kernels are universal , in the sense that they can asymptotically approximate arbitrary nonlinear function from spike trains to reals .",
    "they have an additional scale parameter @xmath24 which controls how much smoothing is applied to the space induced by the base kernel .",
    "so far we have introduced kernels that compute similarity between a pair of spike trains ( either from a single neuron at different times or from a pair of neurons ) .",
    "however , recent recording techniques allow simultaneous recording of up to a couple of hundreds of neurons . thus , we need kernels for a pair of _ sets of _ spike trains from many neurons .",
    "there are a couple of simple yet effective ways to extend single neuron kernels to multiple neuron kernels ( see  @xcite for combining kernels ) .",
    "first is to use a product kernel , @xmath64 where @xmath65 indexes over simultaneously recorded neurons .",
    "second is to use a direct sum kernel , @xmath66 where @xmath67 are weights for combining the effect of each neuron .",
    "the product kernel is the natural inner product of the product hilbert space , and the direct sum kernel is that of the direct sum hilbert space .",
    "the product kernel preserves the universality of elementary kernels ( e.g. , with schoenberg kernels ) , but if the effective dimension of the spike train manifold increases ( as in the case of less dependent spike trains and/or independent noise processes ) the number of spike trains required to `` fill '' the space increases for the same kernel size .",
    "hence , more smoothness may have to be incorporated ( imposed by kernel sizes ) , or exponentially more data may be required to estimate equivalently detailed nonlinear functions .",
    "the direct sum kernel does not preserve universality ; in fact , only additive functions over multiple neurons are spanned by those kernels . therefore ,",
    "unless such constrains are of interest , it is not useful for general neuroscience applications . in general , combining kernels increases the number of hyperparameters , making cross - validation less practical , hence we recommend empirical bayes methods for their estimation  @xcite .",
    "although it is possible to form a product kernel from the spikernel , it is not necessary to do so because the spikernel can be extended directly for multiple neurons by considering a vector of spike counts for each time bin  @xcite . in such construction ,",
    "the time warping is uniformly applied to all spike trains . since the time complexity is only additive for the number of neurons , for a large population recording , spikernel could be computationally advantageous .",
    "equipped with spike train kernels , we can now discuss application areas in neuroscience and neural engineering , each of which requires a different class of kernel methods .",
    "we discuss the problem of hypothesis testing first , followed by stationary neural code analysis using regression and online neural decoding with adaptive filtering .      due to the trial - to - trial variability of neural responses ( fig .",
    "[ fig : ttv ] ) the collection of responses to repeated stimuli can be considered as realizations of a random process . when realizations are spike trains , the corresponding mathematical object , the probability law over spike trains , is called a _",
    "point process_. it is often necessary to determine if two sets of responses given different experimental conditions are different  we want to know if the response carries any information about the experimental condition of interest .",
    "for example , some neurons in the visual cortex encode the stimulus color regardless of the motion , while some encode the directional motion regardless of the color .    in practice , a severe bias may be unwillingly included when searching for a neuron that encodes information about a certain feature especially in the context of _ in  vivo _ electrophysiology . in a typical",
    "setting , a trained electrophysiologist would listen to the firing pattern of each neuron and make a decision on the fly to record from the probed neuron , which tends to be the one with larger firing rate modulation .",
    "identifying neural selectivity has been widely done assuming that the information is only represented in the mean statistics ( firing rate ) .",
    "the conventional estimator is a histogram ( or a smoothed version ) , known as the peri - stimulus time histogram ( psth ) , which is obtained by binning the time , and averaging over trials .",
    "the difference in the mean statistics is then used for associated hypothesis testing  @xcite .",
    "another widely used test is the wilcoxon rank sum test on the spike counts  @xcite .",
    "it nonparametrically captures the difference in the distribution over counts , therefore , more than just the mean count is tested .",
    "since the count distribution must span across multiple values to be meaningful , it requires a large window that captures relatively many spikes , and it is difficult to apply it to multiple bins .",
    "thus , the count distribution can not capture the differences in the temporal structure .",
    "for these reasons , these widely used parametric statistical tests are fundamentally incapable of discriminating many features .",
    "instead , we submit that a class of nonparametric statistics is needed that can discriminate among many point process distributions , either in terms of higher order statistics or the temporal dimension .",
    "such a statistic is known generally as a _ divergence _",
    "one can define a divergence measure from kernels by utilizing a recent development known as _ probability embedding _ , which provides a general construction of divergence measures by representing the data s probability distribution as a point in the feature space  @xcite .",
    "the idea of probability embedding is very simple .",
    "use a kernel to map the samples to the feature space , and take the mean to represent the ( empirical ) probability distribution .",
    "this is possible because the feature space is a ( linear ) hilbert space . as long as one uses a powerful enough kernel , this simple process gives a unique representation of the empirical probability law that asymptotically converges to the true distribution . technically , it is sufficient to show that the kernel @xmath14 is _ strictly positive definite _ ( spd ) in the integral sense , that is , @xmath68 for any probability distribution @xmath69 of consideration on the input space  @xcite .",
    "interestingly , the mean of the point process in the hilbert space for the binned or linear functional kernel results in estimators for the firing rate function .",
    "the psth can be formed by using a binned spike train kernel , and a smoothed estimate of the intensity function can be produced by using a linear functional kernel . given a collection of spike trains ( with possible repeats ) , @xmath70 , the mean in the hilbert space corresponding to a linear functional kernel @xmath14 can be represented as a function over arbitrary spike train @xmath71 , @xmath72 since the kernel is linear for superposition , the mean does not depend on which spike train a particular spike came from .",
    "therefore , the mean does not capture any statistical structure between spikes within the same trial . as can be expected ,",
    "neither the binned kernel nor the linear functional kernels is spd  @xcite .    for spd kernels",
    ", the mean contains all information about the collection of spike trains .",
    "this is not surprising , given that unique spike trains mapped to the hilbert space are not only unique , but are mutually linearly independent ( the gram matrix is full rank ) .",
    "the mean `` remembers '' the set of spike trains that formed it , except for the ordering .",
    "what is important is that the mean in the hilbert space is a smoothed representation , and hence if the spike trains that consist the mean are similar , they are close in the hilbert space",
    ".    a divergence measure for empirical observations can be defined as the distance of the means for a pair of collections of observed spike trains in the hilbert space , @xmath73 where @xmath14 is a spd kernel .",
    "this divergence statistic @xmath74 is called _ maximum mean discrepancy _ ( mmd )  @xcite .",
    "when mmd is large , it is an indication that the two point processes are different . in the classical hypothesis testing framework ,",
    "we need the distribution of mmd under the null hypothesis which assumes that both collections originate from the same underlying random process . we can generate mmd values from the null distribution by mixing the samples from both conditions and resampling from the mixture  @xcite .",
    "the following simple procedure describes a typical hypothesis testing given two collections of spike trains @xmath75 and @xmath76 , and a test size",
    "@xmath57 :    1 .   compute the kernel matrix @xmath12 2 .",
    "compute @xmath77 3 .",
    "bootstrap randomly permuted indices of size @xmath78 and @xmath79 with replacement and recompute the statistic of the null distribution 4 .",
    "if @xmath80 is above the @xmath81 quantile of the bootstrapped null distribution , reject the null hypothesis , otherwise accept it .",
    "the smoothness of the probability embedding is controlled by the spike train kernel of choice , and hence it is important to choose a kernel that captures the natural similarity of spike trains well .",
    "this may come as a surprise since all spd kernel are asymptotically equivalent for mmd , that is , if the two underlying probability laws are different , any spd kernel can discriminate given a large enough sample . yet , the small sample power of the divergence test is greatly enhanced by encoding more prior information of the similarity into the spike train kernel .",
    "neural decoding searches for the detailed relationship between neural activity and the variable of interest ( fig .",
    "[ fig : decoding ] ) .",
    "successful decoding analysis often provides evidence ( or new hypothesis ) for specific coding schemes the neural system uses , functionally identifies the system , and moreover , it can be used to develop neural prosthetics and interfaces .",
    "depending on the modality of the target variable , neural decoding can be tackled by different tools .",
    "when the target variable is categorical ( finite number of possibilities ) , classification algorithms are suitable ; e.g. , object recognition and multiple choice decision are naturally categorical .",
    "if the target variable is continuous valued and fixed for each trial , but jumps from one value to another , then regression tools are appropriate .",
    "such trial - based experimental design is commonly used for studying the neural code . when a continuous target variable is varied through time , filtering and smoothing algorithms",
    "are appropriate .",
    "most primary sensory as well as motor features naturally fall in this category , and are of most practical use in neural engineering . here",
    "we will focus our discussion on regression and filtering . by regression ,",
    "we mean batch analysis , while by filtering , we refer to online ( real - time ) signal processing .    a traditional approach to map single or multiple spike trains to a continuous target variable is linear regression on binned spike trains with relatively large bin sizes  @xcite . again , the rational stems from the neuroscience literature which focuses primarily on the information carried by the mean firing rate , and little about the detailed temporal structure within each trial . despite their crudeness , linear models on binned spike trains perform reasonably for motor brain machine interfaces , because the time scale of behavior is at the hundred milliseconds scale .    for filtering , conventional linear filtering methods such as least mean",
    "squares , recursive least squares , and kalman filters are often used , and recurrent neural network approach for filtering is also worth mentioning  @xcite . in recent years , state - space based",
    "bayesian filtering approaches have been popular  @xcite .",
    "a state - space ( also known as latent - variable ) model combined with an encoding model from continuous observation to spike trains is inverted using the bayesian filtering framework .",
    "this method requires a good encoding model which has to be fit ahead of time , and is based on stationary assumptions during and between training and testing conditions . because of neural plasticity , in practice frequent retraining or sophisticated tracking is needed .",
    "( right ) .",
    "( volterra kernels @xmath82 and @xmath83 are not to be confused with the spike train kernel used for decoding . ) , scaledwidth=48.0% ]    gaussian process ( gp ) regression is a widely used nonparametric bayesian regression method  @xcite .",
    "for neural decoding , we assume a prior distribution over the functions from spike trains to @xmath0 .",
    "this prior is explicitly stated to be a gaussian process with covariance provided by a kernel ; the covariance of the function values evaluated at two spike trains @xmath1 and @xmath9 is given by @xmath84 . to be specific ,",
    "given a set of spike trains @xmath11 , the distribution of the function values from the prior is multivariate gaussian distributed @xmath85 \\sim \\mathcal{n}(0 , { \\ensuremath{\\mathbf{k}}})\\ ] ] where @xmath12 is the kernel matrix  .    using the gp prior",
    ", we can infer the posterior assuming a gaussian noise corrupted observation model .",
    "the prediction of the function evaluated at spike train @xmath71 is given by , @xmath86 where @xmath87 , where @xmath88 is the observation noise variance and @xmath89 is the desired signal vector corresponding to training data @xmath11 .",
    "there are several advantages of gp :    1 .",
    "the prediction coincides with kernel ridge regression ( regularized kernel least squares ) , but gp provides the posterior credible interval ( not to be confused with the frequentist confidence interval ) which indicates the uncertainty of the prediction under the model assumptions , 2 .   given a universal kernel",
    ", it can learn any nonlinear functional relation , and , 3 .",
    "hyperparameters ( kernel parameters and observation noise variance ) can be tuned in a principled way using empirical bayes procedure .    in figure",
    "[ fig : gp ] , we compare gp regression with linear functional , schoenberg , and spikernel in a synthetic example where a poisson spike train is mapped to a real - valued signal through a second order volterra system .",
    "the hyperparameters are learned through empirical bayes method where the marginal likelihood is maximized on the training set ( 400 points ) .",
    "the linear functional kernel of   does not perform well on mean prediction ( red trace ) because of the strong nonlinear component ( pairwise interaction of spike timings due to the second order volterra kernel ) , while the spikernel obtains a reasonable prediction , and schoenberg kernel of   achieves very high performance .",
    "the credible interval resulting from using the schoenberg kernel is the smallest , meaning the model is confident that the data is well described by the regression result .",
    "in contrast , the inferred credible interval for the spikernel is large , meaning at least some aspects of the data are not well described by the fit model .",
    "for closed loop applications , the system identification and prediction benefit from sequential processing where the system parameters are adapted with every new sample because neural systems are plastic and there are real time constraints in the experimental setup .",
    "therefore , adaptive filtering algorithms have been widely used in the brain - machine interface applications , and other neural prosthetics  @xcite .",
    "as stated , linear filtering algorithms such as least mean squares ( lms ) and recursive least squares ( rls ) algorithms as well as kalman filtering have been successful using the binned representation , but performance improvements are still needed .",
    "kernel adaptive filters ( kaf ) have been recently developed that kernelize the linear adaptive filtering algorithms  @xcite , inheriting their simple computational structure , and extending them to nonlinear transfer functions .",
    "similarly , kafs operate on a sample - by - sample basis , and can deal with non - stationary environments .        here",
    "we describe the simplest yet powerful kernel least mean squares ( klms ) algorithm , which has been successfully applied in neural engineering as inverse control in the context of somatosensory prosthetics  @xcite .",
    "klms is a nonlinear extension of the popular lms algorithm that can be derived as a stochastic gradient descent on the mean squared error but with an infinite dimensional weight vector .",
    "the filter mapping is a function of the same form as , where the @xmath57 s are proportional to the instantaneous errors : @xmath90 where @xmath91 is the estimated filter before the @xmath65-th observation , and @xmath92 is the learning rate  @xcite .",
    "thus , the filter input - output map is fully represented by pairs of real coefficient @xmath93 and observed spike train @xmath94 .",
    "the klms has been applied with advantages in nonlinear signal processing of continuous amplitude signals , mostly using the gaussian kernel , but one of the advantages of the rkhs approach is that the algorithm formulation is independent of the kernel .",
    "therefore , any of the spike train kernels presented in this paper can be directly used in the klms .",
    "we demonstrate online inverse modeling using klms in figure  [ fig : klms ]  @xcite .",
    "the goal is to reconstruct properties of the induced tactile stimulation ( here the derivative of the force ) from the generated multi - channel spikes in the thalamus ( vpl ) and primary sensory areas ( s1 ) upon stimulation . given the finite memory in the neural system , typically a moving window is used  for example , of 80 milliseconds slided 2 ms at a time over the spike data .",
    "we use the product kernel in this miso klms model , train a decoder on the tactile stimulus time series on a training set , and compare reconstruction performance with different spike train kernels on a test set .",
    "overall , a similar trend to previous examples is observed in this application where the schoenberg kernel outperforms the spikernel and the linear kernel , both in terms of faster convergence in the training set and better reconstruction of the stimulus in the test set .",
    "spike train kernels enable signal processing and machine learning of spike trains by providing a feature space for computation . in this article",
    ", we surveyed how positive definite functions can be used to quantify spike trains , at the rate or spike timing level , and implement important operations for neural decoding such as hypothesis testing using probability embedding , and estimation of continuous functional mappings from spike trains . as we have briefly demonstrated , spike train kernels provide a unifying framework for most of the techniques used in spike train processing , from the statistical rate descriptors , to the binned representations up to the full description ( injective mapping by a spd kernel ) of the spike train timing structure in infinitely dimensional functional spaces .",
    "the approach is therefore versatile and mathematically principled , extending the footprint of signal processing and machine learning to more abstract spaces .    among the spike train kernels ,",
    "we have promoted the binless schoenberg kernel , since , ( 1 ) it provides an injective mapping , ( 2 ) it can embed arbitrary stochasticity of neural responses as the sample mean in the rkhs , and ( 3 ) it is a universal kernel that can approximate arbitrary functions on spike trains .",
    "such arbitrarinesses in the point process and function classes are allowed because of the strongly nonparametric nature ( spd ) of the kernel mapping .      like other advanced signal processing methodologies ( e.g. , deep learning and nonparametric bayesian methods ) ,",
    "strictly positive definite spike train kernels make the results less interpretable due to the high dimensionality of the implicit feature space .",
    "weaker kernels only encapsulate explicitly certain designed features , for instance , the count kernel is only sensitive to the total spike count , and the linear functional kernel is mostly sensitive to firing rate profiles . although the stronger kernels can capture arbitrary features , they are not unique . therefore designing explicitly stronger spike train kernels",
    "is non - trivial , because it is hard to understand what spike train features they are emphasizing .",
    "there are several ways we can partially recover the intuition , although more research is needed in this direction : via visualization of spike trains in the feature space in the case of mmd - pca ( fig .",
    "[ fig : divergence ] ) , via sparse regression methods like relevant vector machine , or via kernel selection over a set of strongly interpretable weaker ( more parametric ) kernels .",
    "another caveat of decoding analysis is that successful decoding does not imply that the brain is using the information , it only signifies that the information exists in the collected neural signals .",
    "for example , in early sensory neurons like retinal ganglion cells or auditory fiber , we can often decode and reconstruct the sensory stimulus better than what the subject can report by behavior .",
    "therefore , we should be cautious not to over - interpret successful decoding .",
    "the last decade has been productive in terms of new kernels for many abstract ( non - euclidean ) spaces , but there is still room for improvement .",
    "we would like to have a spike train kernel that is powerful enough , i.e. , spd and universal , while at the same time , able to capture our prior knowledge about the similarity between spike trains .",
    "the variability of the neural system could provide hints to designing better spike train kernels .",
    "there are three practical aspects of designing a useful kernel : ( 1 ) the kernel should encode the prior knowledge of the input domain , and the problem at hand , ( 2 ) the kernel should be computationally tractable , perhaps with linear or less time complexity in the number of spikes ; and ( 3 ) the kernel should have no or very few parameters  in the latter case there should be simple intuition behind the parameters , and more importantly simple rules for setting their values .",
    "we have discussed two frameworks in this article , namely the binned kernels , and the functional kernels .",
    "binned kernels are either too simplistic , they ignore the temporal structure , or computationally too expensive , e.g. , spikernel . on the other hand ,",
    "some functional kernels are either overly sensitive to the mean rate , such as linear functional kernel , or involve parameters that are not easily visualized .",
    "a kernel with the right balance among these three properties remains to be found .",
    "it is safe to assume that we have only scratched the surface of this problem , and there remain many open avenues to be explored .",
    "two possible approaches are edit kernels , and generative kernels .",
    "edit kernels rely on the principle of adopting simple operations such as shifting , addition , and deletion to convert one spike train into another where each operation is assigned a cost , whereas generative kernels rely on the principle that two spike trains are similar if they originate from the same generative model .",
    "b.  s. wilson , c.  c. finley , d.  t. lawson , r.  d. wolford , d.  k. eddington , and w.  m. rabinowitz , `` better speech recognition with cochlear implants , '' _ nature _ , vol .",
    "6332 , pp . 236238 , jul .",
    "m.  s. humayun , j.  d. weiland , g.  y. fujii , r.  greenberg , r.  williamson , j.  little , b.  mech , v.  cimmarusti , g.  van  boemel , g.  dagnelie , and e.  de  juan , `` visual perception in a blind subject with a chronic microelectronic retinal prosthesis , '' _ vision research _ , vol .",
    "43 , no .",
    "25732581 , nov .",
    "2003 .",
    "m.  a.  l. nicolelis and m.  a. lebedev , `` principles of neural ensemble physiology underlying the operation of brain - machine interfaces , '' _ nature reviews neuroscience _ , vol .",
    "10 , no .  7 , pp .",
    "530540 , july 2009 .",
    "j.  k. chapin , k.  a. moxon , r.  s. markowitz , and m.  a.  l. nicolelis , `` real - time control of a robot arm using simultaneously recorded neurons in the motor cortex , '' _ nat neurosci _ , vol .  2 , no .  7 ,",
    "pp . 664670 , jul .",
    "t.  berger , m.  baudry , r.  brinton , j .- s .",
    "liaw , v.  marmarelis , a.  y. park , b.  sheu , and a.  tanguay , `` brain - implantable biomimetic electronics as the next era in neural prosthetics , '' _ proceedings of the ieee _ , vol .",
    "89 , no .  7 ,",
    "pp . 9931012 , 2001 .",
    "b.  schlkopf and a.  j. smola , _ learning with kernels : support vector machines , regularization , optimization , and beyond _ , ser .",
    "adaptive computation and machine learning.1em plus 0.5em minus 0.4emmit press , 2002 .",
    "m.  d. serruya , n.  g. hatsopoulos , l.  paninski , m.  r. fellows , and j.  p. donoghue , `` brain - machine interface : instant neural control of a movement signal , '' _ nature _ , vol .",
    "6877 , pp .",
    "141142 , mar .",
    "l.  shpigelman , y.  singer , r.  paz , and e.  vaadia , `` spikernels : predicting arm movements by embedding population spike rate patterns in inner - product spaces , '' _ neural computation _ , vol .",
    "17 , no .  3 , pp .",
    "671690 , march 2005 .",
    "a.  chen , `` fast kernel density independent component analysis , '' in _ proceedings of the 6th international conference on independent component analysis and blind signal separation _",
    "ica06.1em plus 0.5em minus 0.4emberlin , heidelberg : springer - verlag , 2006 , pp .",
    "i.  park and j.  c. prncipe , `` quantification of inter - trial non - stationarity in spike trains from periodically stimulated neural cultures , '' in _ ieee international conference on acoustics , speech , and signal processing ( icassp ) _",
    ", 2010 , pp .",
    "54425445 , special session on multivariate analysis of brain signals : methods and applications .      r.  schaback and h.  wendland , _ multivariate approximation and applications_.1em plus 0.5em minus 0.4emcambridge university press , 2001 , ch . characterization and construction",
    "of radial basis functions , pp .",
    "124 .",
    "j.  j. eggermont , a.  m. aertsen , and p.  i. johannesma , `` quantitative characterisation procedure for auditory neurons based on the spectro - temporal receptive field . ''",
    "_ hearing research _ , vol .  10 , no .  2 , pp . 167190 , may 1983",
    "s.  seth , i.  park , a.  j. brockmeier , m.  semework , j.  choi , j.  francis , and j.  c. prncipe , `` a novel family of non - parametric cumulative based divergences for point processes , '' in _ advances in neural information processing systems ( nips ) _ , 2010 , pp .",
    "21192127 .",
    "i.  park , s.  seth , m.  rao , and j.  c. principe , `` estimation of symmetric chi - square divergence for point processes , '' in _ acoustics , speech and signal processing ( icassp ) , 2011 ieee international conference on_.1em plus 0.5em minus 0.4emieee , may 2011 , pp .",
    "20162019 .",
    "a.  gretton , k.  borgwardt , m.  rasch , b.  schlkopf , and a.  smola , `` a kernel method for the two - sample - problem , '' in _ advances in neural information processing systems 19 _ , b.  schlkopf , j.  platt , and t.  hoffman , eds .",
    "1em plus 0.5em minus 0.4emcambridge , ma : mit press , 2007 , pp .",
    "513520 .",
    "a.  smola , a.  gretton , l.  song , and b.  schlkopf , `` a hilbert space embedding for distributions , '' in _",
    "alt 07 : proceedings of the 18th international conference on algorithmic learning theory_.1em plus 0.5em minus 0.4emberlin , heidelberg : springer - verlag , 2007 , pp .",
    "l.  song , b.  boots , s.  siddiqi , g.  gordon , and a.  smola , `` hilbert space embeddings of hidden markov models , '' in _ international conference on machine learning ( icml ) _ , j.  frnkranz and t.  joachims , eds.1em plus 0.5em minus 0.4emomnipress , 2010 .",
    "j.  w. xu , a.  r.  c. paiva , i.  park , and j.  c. prncipe , `` a reproducing kernel hilbert space framework for information - theoretic learning , '' _ signal processing , ieee transactions on _ , vol .",
    "56 , no .  12 , pp . 58915902 , 2008 .",
    "m.  m. churchland and s.  g. lisberger , `` shifts in the population response in the middle temporal visual area parallel perceptual and motor illusions produced by apparent motion , '' _ the journal of neuroscience _ , vol .",
    "21 , no .",
    "93879402 , dec . 2001 .",
    "a.  b.  a. graf , a.  kohn , m.  jazayeri , and j.  a. movshon , `` decoding the activity of neuronal populations in macaque primary visual cortex , '' _ nature neuroscience _ ,",
    "advance online publication , no .  2 ,",
    "239245 , jan . 2011 .",
    "j.  c. sanchez , d.  erdogmus , m.  a.  l. nicolelis , j.  wessberg , and j.  c. principe , `` interpreting spatial and temporal neural activity through a recurrent neural network brain - machine interface , '' _ neural systems and rehabilitation engineering , ieee transactions on _ , vol .  13 , no .  2 , pp .",
    "213219 , jun . 2005 .",
    "e.  n. brown , d.  p. nguyen , l.  m. frank , m.  a. wilson , and v.  solo , `` an analysis of neural receptive field plasticity by point process adaptive filtering , '' _ proceedings of the national academy of sciences _ , vol .",
    "98 , pp . 1226112266 , 2001 .",
    "w.  truccolo , u.  t. eden , m.  r. fellows , j.  p. donoghue , and e.  n. brown , `` a point process framework for relating neural spiking activity to spiking history , neural ensemble , and extrinsic covariate effects , '' _ journal of neurophysiology _ , vol .",
    "93 , no .  2 ,",
    "pp . 10741089 , february 2005 .",
    "y.  wang , a.  r.  c. paiva , j.  c. prncipe , and j.  c. sanchez , `` sequential monte carlo point process estimation of kinematics from neural spiking activity for brain machine interfaces , '' _ neural computation _ , vol .  21 , no .  10 , pp . 28942930 , oct .",
    "2009 .",
    "l.  li , j.  choi , j.  francis , j.  sanchez , and j.  principe , `` decoding stimuli from multi - source neural responses , '' in _ engineering in medicine and biology society ( embc ) , 2012 annual international conference of the ieee _",
    ", 2012 , pp .",
    "1331 1334 .",
    "l.  li , i.  m. park , a.  brockmeier , s.  seth , b.  chen , j.  francis , j.  c. sanchez , and j.  c. prncipe , `` adaptive inverse control of neural spatiotemporal spike patterns with a reproducing kernel hilbert space ( rkhs ) framework , '' _ ieee transactions on neural and rehabilitation systems engineering _ , ( in press ) ."
  ],
  "abstract_text": [
    "<S> over the last decade several positive definite kernels have been proposed to treat spike trains as objects in hilbert space . however , for the most part , such attempts still remain a mere curiosity for both computational neuroscientists and signal processing experts . </S>",
    "<S> this tutorial illustrates why kernel methods can , and have already started to , change the way spike trains are analyzed and processed . </S>",
    "<S> the presentation incorporates simple mathematical analogies and convincing practical examples in an attempt to show the yet unexplored potential of positive definite functions to quantify point processes . </S>",
    "<S> it also provides a detailed overview of the current state of the art and future challenges with the hope of engaging the readers in active participation .    at ( current page.south ) ; </S>"
  ]
}