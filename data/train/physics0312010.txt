{
  "article_text": [
    "complex systems pervade our daily life .",
    "they are difficult to study because they do nt exhibit simple cause - and - effect relationships and their interconnections are not easy to disentangle .",
    "game theory has demonstrated to be a very flexible tool to study complex systems .",
    "it coalesced in its _",
    "normal form_@xcite during the second world war with the work of von neumann and morgenstern @xcite who first applied it in economics .",
    "later , in the seventies , it was the turn of biology mainly with the work of j. maynard - smith @xcite , who shown that the game theory can be applied to various problems of evolution , and proposed the concept of evolutionary stable strategy ( ess ) , as an important concept for understanding biological phenomena .",
    "following rules dictated by game theory to attain an ess requires neither consciousness nor a brain .",
    "moreover , a recent experiment found that two variants of a rna virus seem to engage in two - player games @xcite .",
    "this opens a new perspective , perhaps the dynamic of very simple agents , of the kind we know in physics , can be modeled by game theory providing an alternative approach to physical problems .",
    "for instance , energies could be represented as payoffs and phenomena like phase transitions understood as many - agents games . as a particular application of this line of thought",
    "we have seen recently a proliferation of papers addressing the issue of _ quantum games _",
    "@xcite which might shed light on the hot issue of quantum computing .",
    "conversely , physics can be useful to understand the behavior of adaptive agents playing games used to model several complex systems in nature .",
    "for instance , in some interesting works szab _",
    "et al _ @xcite,@xcite applied the sophisticated techniques developed in non - equilibrium statistical physics to spatial evolutionary games .",
    "the most popular exponent of game theory is the _ prisoner s dilemma _ ( pd ) game introduced in the early fifties by m. flood and m. dresher @xcite to model the social behavior of `` selfish '' individuals - individuals which pursue exclusively their own self - benefit .",
    "the pd game is an example of a @xmath4 game in normal form : i ) there are 2 players , each confronting 2 choices - to cooperate ( c ) or to defect ( d)- , ii ) with a @xmath4 matrix specifying the payoffs of each player for the 4 possible outcomes : [ c , c],[c , d],[d , c ] and [ d , d ] means that the first player plays x and the second player plays y ( x an y = c or d ) . ] and iii ) each player makes his choice without knowing what the other will do .",
    "a player who plays c gets the `` reward '' @xmath5 or the `` sucker s payoff '' @xmath6 depending if the other player plays c or d respectively , while if he plays d he gets the `` temptation to defect '' @xmath7 or the `` punishment '' @xmath8 depending if the other player plays c or d respectively .",
    "these four payoffs obey the relations : @xmath9 and @xmath10 thus independently of what the other player does , by ( [ eq : inequal ] ) , defection d yields a higher payoff than cooperation c ( @xmath11 and @xmath12 ) and is the _",
    "dominant strategy_. the outcome [ d , d ] is thus called a nash equilibrium @xcite .",
    "the dilemma is that if both defect , both do worse than if both had cooperated ( @xmath13 ) . condition ( [ eq : dilemma2 ] ) is required in order that the average utilities for each agent of a cooperative pair ( @xmath5 ) are greater than the average utilities for a pair exploitative - exploiter ( ( @xmath14)/2 ) .",
    "changing the rank order of the payoffs - the inequalities ( [ eq : inequal])- gives rise to different games .",
    "a general taxonomy of @xmath4 games ( one - shot games involving two players with two actions each ) was constructed by rapoport and guyer @xcite .",
    "a general @xmath15 game is defined by a payoff matrix m@xmath16 with payoffs not necessarily obeying the conditions ( [ eq : inequal ] ) or ( [ eq : dilemma2 ] ) or @xmath8 to denote the payoffs in order to keep the pd standard notation . ] @xmath17 the payoff matrix gives the payoffs for _ row _ actions when confronting with _ column _ actions .",
    "apart from the pd game there are other some well studied games .",
    "for instance , when the damage from mutual defection in the pd is increased so that it finally exceeds the damage suffered by being exploited : @xmath18 the new game is called the _ chicken",
    "chicken is named after the car racing game .",
    "two cars drive towards each other for an apparent head - on collision .",
    "each player can swerve to avoid the crash ( cooperate ) or keep going ( defect ) .",
    "this game applies thus to situations such that mutual defection is the worst possible outcome ( hence an unstable equilibrium ) .",
    "when the reward of mutual cooperation in the chicken game is decreased so that it finally drops below the losses from being exploited : @xmath19 it transforms into the _ leader _ game .",
    "the name of the game stems from the following every day life situation : two car drivers want to enter a crowded one - way road from opposite sides , if a small gap occurs in the line of the passing cars , it is preferable that one of them take the lead and enter into the gap instead of that both wait until a large gap occurs and allows both to enter simultaneously .",
    "in fact , every payoff matrix , which at a first glance could seem unreasonable from the point of view of selfish individuals , can be applicable to describe real life situations in different realms or contexts .",
    "furthermore , `` unreasonable '' payoff matrices can be used by minorities of individuals which depart from the `` normal '' ones ( assumed to be neutral ) for instance , absolutely d individuals incapable of realizing any value to cooperation or absolutely c `` altruistic '' individuals ( more on this later ) .    in one - shot or non repeated games , where each player has a dominant strategy , as in the pd , then generally these strategies will be chosen",
    "the situation becomes more interesting when the games are played repeatedly . in these _ iterated games",
    "_ players can modify their behavior with time in order to maximize their utilities as they play _",
    "i.e. _ they can adopt different strategies . in order to escape from the non - cooperative nash equilibrium state of social dilemmas",
    "it is generally assumed either memory of previous interactions @xcite or features ( `` tags '' ) permitting cooperators and defectors to distinguish one another @xcite ; or spatial structure is required @xcite .",
    "recently , it was proposed @xcite a simple model of selfish agents without memory of past encounters , without tags and with no spatial structure playing an arbitrary @xmath4 game , defined by a general payoff matrix like ( [ eq : payoffmatrix ] ) . at a given time @xmath20 ,",
    "each of the @xmath21 agents , numbered by an index @xmath22 , has a probability @xmath23 of playing c ( @xmath24 of playing d ) .",
    "then a pair of agents are selected at random to play .",
    "all the players use the same measure of success to evaluate if they did well or badly in the game which is based on a comparison of their utilities @xmath25 with an estimate of the expected income @xmath26 and the arithmetic mean of payoffs @xmath27 .",
    "next , they update their @xmath23 in consonance , i.e. a player keeps his @xmath23 if he did well or modifies it if he did badly .",
    "our long term goal is to study the quantum and statistical versions of this model .",
    "that is , on one hand to compare the efficiency and properties of quantum strategies vs. the classical ones for this model in a spirit similar to that of ref .",
    "on the other hand , we are also interested in the effect of noise , for instance by introducing a metropolis monte - carlo temperature , and the existence of power laws in the space of payoffs that parameterize the game , of the type found in ref .",
    "@xcite and @xcite , for a spatial structured version of this model . before embarking on the quantum or statistical mechanics of this model ,",
    "the objective in this paper is to complete the study of the simplest non - spatial m - f version . in particular , to present an analytic derivation of the equilibrium states for any payoff matrix _",
    "i.e. _ for an arbitrary @xmath4 game using elemental calculus , both for the deterministic and stochastic versions . in the first case",
    "the calculation is elementary and serves as a guide to the more subtle computation of the stochastic model .",
    "these equilibrium states into which the systems self - organizes , which depend on the payoff matrix , are of three types : `` universal cooperation '' or `` all c '' , of intermediate level of cooperation and `` universal defection '' or `` all d '' with , respectively , @xmath2 = 1.0 , @xmath28 and 0.0 .",
    "we also consider the effect of mixing players using two different payoff matrices .",
    "specifically , a payoff matrix producing @xmath2=0.0 and the canonical payoff matrix are used to simulate , respectively , absolutely d or `` antisocial '' agents and `` normal '' agents .",
    "we consider two versions of the model introduced in ref . @xcite .",
    "first , a deterministic version , in which the agents are always in definite states either c or d _ i.e. _ `` black and white '' agents without `` gray tones '' .",
    "nevertheless , it is often remarked that this is clearly an over - simplification of the behavior of individuals.indeed , their levels of cooperation exhibit a continuous gamma of values .",
    "furthermore , completely deterministic algorithms fail to incorporate the stochastic component of human behavior .",
    "thus , we consider also a stochastic version , in which the agents only have probabilities for playing c. in other words , the variable @xmath29 , denoting the state or `` behavior '' of the agents , for the deterministic case takes only two values : @xmath29 = 1 ( c ) or 0 ( d ) while for the stochastic case @xmath29 is a real variable @xmath30 $ ] .",
    "the pairs of players are chosen randomly instead of being restricted to some neighborhood .",
    "the implicit assumptions behind this are that the population is sufficiently large and the system connectivity is high . in other words , the agents display high mobility or they can experiment interactions at a distance ( for example electronic transactions , etc . ) .",
    "this implies that @xmath21 the number of agents needs to be reasonably large .",
    "for instance , in the simulations presented in this work the population of agents will be fixed to @xmath31 .",
    "the update rule for the @xmath32 of the agents is based on comparison of their utilities with an estimate . the simplest estimate @xmath33 that agent number @xmath34 for his expected utilities in the game",
    "is provided by the utilities he would made by playing with himself , to get a better estimate of their expected utilities .",
    "however , the main results do not differ from the ones obtained with this simpler agents ] that is : @xmath35 where @xmath32 is the probability that in the game the agent k plays c. from equation ( [ eq : epsilon ] ) we see that the estimate for c - agents ( @xmath36 ) @xmath37 and d - agents ( @xmath38 ) @xmath39 are given by @xmath40    the measure of success we consider here is slightly different from the one considered in ref .",
    "@xcite : to measure his success each player compares his profit @xmath41 with the maximum between his _ estimate _ @xmath42 , given by ( [ eq : epsilon ] ) , and the arithmetic mean of the four payoffs given by @xmath43 . is to cover a wider range of situations than the ones permitted by the so - called _",
    "pavlov s _ rule .",
    "pavlov strategy consists in to stick to the former move if it earned one of the two highest payoff but to switch in the contrary case .",
    "the measure considered here reduces to it when @xmath44 . ]",
    "if @xmath45 ( @xmath46 ) @xmath47 the player assumes he is doing well ( badly ) and he keeps ( changes ) his @xmath48 as follows : if player @xmath34 did well he assumes his @xmath48 is adequate and he keeps it . on the other hand ,",
    "if he did badly he assumes his @xmath32 is inadequate and he changes it ( from c to d or from d to c in the deterministic version ) .",
    "we are interested in measuring the average probability of cooperation @xmath1 vs. time , and in particular in its value of equilibrium @xmath2 , after a transient which is equivalent to the final fraction of c - agents @xmath49 .",
    "for the deterministic case the values of @xmath2 are obtained by elementary calculus as follows .",
    "once equilibrium has been reached , the transitions from d to c , on average , must equal those from c to d. thus , the average probability of cooperation @xmath2 is obtained by equalizing the flux from c to d , @xmath50 , to the flux from d to c , @xmath51 .",
    "the players who play c either they get @xmath5 ( in [ c , c ] encounters ) or @xmath6 ( in [ c , d ] encounters ) , and their estimate is @xmath52 ; thus , according to the update rule , they change to d if @xmath53 or @xmath54 respectively . for a given average probability of cooperation @xmath1 , [ c , c ] encounters",
    "occur with probability @xmath55 and [ c , d ] encounters with probability @xmath56 .",
    "consequently , @xmath50 can be written as : @xmath57 with @xmath58 where @xmath59 is the step function given by :    @xmath60    on the other hand , the players who play d either they get @xmath7 ( in [ d , c ] encounters ) or @xmath8 ( in [ d , d ] encounters ) and their estimate is @xmath61 ; thus , according to the update rule , they change to c if @xmath62 or @xmath63 respectively .",
    "as [ d , c ] encounters occur with probability @xmath64 and [ d , d ] encounters with probability @xmath65 , @xmath50 can be written as : @xmath66 with @xmath67 in equilibrium @xmath68 and thus we get a set of second order algebraic equations for @xmath2 : @xmath69 as there are 2 possibilities for each coefficient @xmath70 , we have a total of @xmath71 different equations governing all the possible equilibrium states ( actually there are 15 since this includes the trivial equation @xmath72 ) .",
    "the roots$ ] ] of these equations are : + @xmath73    in addition , we have to take into account the case when : @xmath74 in this case we can see from ( [ eq : jcd ] ) and ( [ eq : jdc ] ) that @xmath75 _ identically _ , so we have that @xmath76 , ( being @xmath77 the initial mean probability ) , whatever the initial conditions are .",
    "+ for instance , for the canonical payoff matrix we have @xmath78 and @xmath79 , therefore we get @xmath80 with the root @xmath81 corresponding to the stable dynamic equilibrium in which the agents change their state in such a way that , on average , half of the transitions are from c to d and the other half from d to c.      in the case of a continuous probability of cooperation @xmath32 , the calculation is a little bit more subtle : now the estimate @xmath33 for the agent @xmath34 is not only r or p , as it happened in the discrete case , but it can take a continuum of values as the probability @xmath82 varies in the interval [ 0,1 ] . from now on we will use the estimate as given in ( [ eq : epsilon ] ) , but instead of a @xmath83 as a function of time we will use a generic @xmath26 that is a function of the cooperation probability ( and implicitly of time , off course ) , that is : @xmath84 so we have : @xmath85 + to calculate @xmath2 we begin by writing a balance equation for the probability @xmath23 . the agents will follow the same rule as before : they will keep their state if they are doing well ( in the sense explained earlier ) and otherwise they will change it",
    ". if two agents @xmath22 and @xmath86 play at time @xmath20 , with probabilities @xmath87 and @xmath88 respectively , then the change in the probability @xmath89 , provided he knows @xmath88 , would be given by : @xmath90   \\\\ \\vspace{2 mm }   & - c_{i}(t)[1-c_{j}(t)]\\,[1 - \\theta(s - \\epsilon^{r s t p}(c_{i}(t))\\,\\theta(s-\\mu ) ] \\\\",
    "\\vspace{2 mm }   & + [ 1-c_{i}(t)]c_{j}(t)\\,[1 - \\theta(t - \\epsilon^{r s t p}(c_{i}(t))\\,\\theta(t-\\mu ) ] \\\\    & + [ 1-c_{i}(t)][1-c_{j}(t)]\\,[1 - \\theta(p - \\epsilon^{r s t p}(c_{i}(t))\\ , \\theta(p-\\mu ) ] , \\end{array}\\ ] ] being @xmath91 the step function .",
    "the equation of evolution for @xmath92 is obtained by simply exchanging @xmath93 in equation ( [ eq : updateci ] ) . certainly , the assumption that each agent knows the probability of cooperation of his opponent is not realistic . later ,",
    "when we perform the simulations , we will introduce a procedure to estimate the opponent s probability ( more on this in section * v.b * ) + in ( [ eq : updateci ] ) if at time @xmath20 the payoff obtained by agent @xmath22 , @xmath94 or @xmath95 is less than @xmath96 , the first two terms in the rhs decrease the cooperation probability of agent @xmath22 , while the two last terms increase it .",
    "the terms give no contribution if the payoff @xmath97 is greater or equal than @xmath98 .",
    "+ we will use the canonical payoff matrix @xmath99 to illustrate how the above equation of evolution for @xmath23 works . in this case , the estimate function is , by ( [ eq : epsilon2 ] ) : @xmath100 thus it is easy to see that : @xmath101 \\\\ \\vspace{1 mm } \\theta(0-\\epsilon^{3 0 5 1}(c))\\,=\\,0 \\quad & \\forall \\,c\\ , \\in \\,[0,1 ] \\\\",
    "\\vspace{1 mm } \\theta(5-\\epsilon^{3 0 5 1}(c))\\,=\\,1 \\quad & \\forall \\,c\\ , \\in \\,[0,1 ] \\\\",
    "\\theta(1-\\epsilon^{3 0 5 1}(c))\\,=\\,0 \\quad & \\forall \\,c\\ , \\in \\,(0,1 ] .",
    "\\end{array}\\ ] ] + in addition we have for this case @xmath102 , thus : @xmath103 + we can then write , to a very good approximation ( we are assuming that the last line of ( [ eq : tita3051epsilon ] ) is valid for @xmath104 also ) : @xmath105+[1-c_{i}(t)][1-c_{j}(t ) ] \\\\    & = [ 1-c_{j}(t)][1 - 2c_{i}(t ) ] . \\end{array }",
    "\\quad\\forall i \\neq j\\ ] ] defining the mean probability of cooperation as @xmath106 + summing eq .",
    "( [ eq : updateci3051 ] ) over @xmath22 and @xmath86 leads to : @xmath107[1 - 2 c(t ) ] \\\\    & = 1 - 3 c(t ) + c(t)^{2 } , \\end{array}\\ ] ] + within an error of @xmath108 since ( [ eq : updateci3051 ] ) is valid @xmath109 but we are summing over all the @xmath21 agents . +   + thereof we can calculate the equilibrium mean probability of cooperation @xmath2 : @xmath110 obtaining the two roots : @xmath111 + being @xmath81 the stable solution . hence we obtain the same result that in the deterministic case + using analog reasoning for the general case , we can conclude that if @xmath112\\ ] ] or @xmath113 the results for the mean cooperation probability for the deterministic version and the stochastic version , are the same . + there is an easy way to evaluate @xmath114 in practice .",
    "it can be seen -see appendix- that + if @xmath115 + while , if @xmath116 + when there is a payoff @xmath97 such that @xmath117\\ ] ] things can change because agents who get @xmath97 update in general their probability of cooperation @xmath23 differently depending whether @xmath118 or @xmath119 .",
    "so as the probability takes different values in the interval @xmath120 $ ] , we have different equations of evolution , which somehow `` compete '' against each other in order to reach the equilibrium .",
    "the different equations that can appear are off course restricted to the ones generated by the coefficients @xmath70 as they appear in ( [ eq : eq - for - p ] ) .",
    "it is reasonable to expect then that the final equilibrium value for the mean probability will be somewhere in between the original equilibrium values for the equations competing .",
    "we will analyze some particular cases of this type in section * v.b * to illustrate this point . +",
    "although at first sight one may think that the universe of possibilities fulfilling condition ( [ eq : xinto ] ) is very vast , it happens that no more than three different balance equations can coexist .",
    "this can be seen as follows : from eqs .",
    "( [ eq : epsmax1 ] ) and ( [ eq : epsmax2 ] ) , @xmath121 , and besides we know that the estimate never could be greater than all the payoffs , so there is at least one @xmath97 such that @xmath122 .",
    "so this leaves us with only two payoffs that effectively can be between @xmath123 and @xmath114 , and this results in at most three balance equations playing in a given game .",
    "let us analyze now the situation where there are a mixing of agents using two different payoff matrices , each leading by separate to a different value of @xmath2 . for simplicity",
    "we consider the deterministic version but the results for the stochastic version are similar .",
    "we call `` antisocial '' individuals those for whom cooperation never pays and thus , although they can initially be in the c state , after playing they turn to state d and remain forever in this state",
    ". they can be represented by players using a payoff matrix that always update @xmath29 to 0 ; for instance m@xmath124 .",
    "notice that these individuals are basically different from those which use a payoff matrix fulfilling conditions ( [ eq : inequal ] ) and ( [ eq : dilemma2 ] ) who , even though they realize the value of cooperation i.e. @xmath125 and @xmath126 , often may be tempted to `` free ride '' in order to get a higher payoff .",
    "however , with the proposed mechanism -which implies a sort of indirect reciprocity- when d grows above 50 % it punishes , on average , this behavior more than c favoring thus a net flux from d to c. conversely , if c grows above 50 % it punish , on average , this behavior more than d favoring thus the opposite flux from c to d. in other words , small oscillations around @xmath127 occur .",
    "on the other hand , agents using @xmath128 are `` immune '' to the former regulating mechanism .",
    "let us analyze the effect they have on cooperation when they `` contaminate ''",
    "a population of neutral agents ( using the canonical payoff matrix ) . in short ,",
    "the two types of individuals play different games ( specified by different payoff matrices ) without knowing this fact , a situation which does not seem too far from real life .",
    "the asymptotic average probabilities of cooperation can be obtained by simple algebra combining the update rules for m@xmath129 and m@xmath124 .",
    "the computation is completely analogous the one which leads to ( [ eq : algebraic3051 ] ) .",
    "we have to calculate @xmath51 and @xmath50 as a function of the variable @xmath1 and the parameter @xmath3 and by equalizing them at equilibrium we get the equation for @xmath2 . to @xmath51 only contribute the fraction ( 1-@xmath3 ) of normal players using the canonical payoff matrix who play d against a player who also plays d ( normal or antisocial ) .",
    "that is , @xmath51 is given by @xmath130 on the other hand , contributions to @xmath50 come from one of these 3 types of encounters : i ) [ c , d ] no matter if agents are neutral or antisocial , ii ) [ c , c ] of two antisocial agents and iii ) [ c , c ] of a neutral and antisocial agent ( the neutral agent remains c and the antisocial , who started at @xmath131 playing c and has not played yet , changes from c to d ) .",
    "the respective weights of these 3 contributions are : @xmath56 , @xmath132 and @xmath133 .",
    "therefore , @xmath50 is given by @xmath134 in equilibrium @xmath135 and the following equation for @xmath2 arises : @xmath136 and solving it : @xmath137 we must take the roots with the `` - '' sign because those with `` + '' are greater than 1 for non null values of d. we thus get the following table for @xmath2 for different values of the parameter @xmath3 :    [ cols=\"<\",options=\"header \" , ]     @xmath138    as it can be seen from the data , for @xmath139 , that is , when condition ( [ eq : xin ] ) is valid , the results for the stochastic case are the same that they would be if we were working with the deterministic model .",
    "this is a consequence of the estimate ( [ eq : estcj ] ) together with conditions ( [ eq : truncation ] ) .",
    "+ for values of @xmath7 and @xmath6 greater than @xmath140 , for which condition ( [ eq : xin ] ) does not hold any more , we can observe what at first may seem a curiosity : for @xmath7 or @xmath6 near @xmath140 , the equilibrium values for the deterministic case are recovered as expected , but as we increase the values of @xmath7 or @xmath6 , the value of @xmath81 is approached . after a little thought , it is clear that this is also a consecuence of the estimation of ( [ eq : estcj ] ) , since it depends on the payoffs .",
    "it can be easily seen that in the case of @xmath141 : @xmath142 if we take then @xmath143 in eq .",
    "( [ eq : updateci ] ) , and remembering that @xmath144 implies that @xmath145 , we will obtain that @xmath81 . in an analogous way for @xmath146 : @xmath147 which toghether with eq .",
    "( [ eq : updateci ] ) again leads to @xmath81 .",
    "the encounters for which @xmath148 or @xmath7 are responsible for that the exact value @xmath81 is not attained .",
    "a similar analysis can be done when @xmath5 or @xmath149 .",
    "the proposed strategy , the combination of measure of success and update rule , produces cooperation for a wide variety of payoff matrices .      * a cooperative regime arises for payoff matrices representing _ `` social dilemmas '' _ like the canonical one . on the other hand spatial game algorithms like the one of ref .",
    "@xcite produce cooperative states ( @xmath150 ) in general for the case of a `` weak dilemma '' in which @xmath8 = @xmath6 = 0 or at most when @xmath8 is significantly below @xmath5 .. ] * payoff matrices with @xmath151 which , at least in principle , one would bet that favor d , actually generate equilibrium states with @xmath152 , provided that @xmath63 -see eqs .",
    "( [ eq : jcd])-([eq : eqfluxes ] ) . * any value of equilibrium average cooperation can be reached in principle , even in the case of the deterministic model , by the appropriate mixing of agents using 2 different payoff matrices .",
    "this is an interesting result that goes beyond the different existent social settings .",
    "for instance we have in mind situations in which one wants to design a device or mechanism with a given value of @xmath2 that optimizes its performance . * in this work we adopted a _",
    "mean field _ approach in which all the spatial correlations between agents were neglected .",
    "one virtue of this simplification is that it shows the model does not require that agents interact only with those within some geographical proximity in order to sustain cooperation .",
    "playing with fixed neighbors is sometimes considered as an important ingredient to successfully maintain the cooperative regime @xcite,@xcite .",
    "( additionally , the equilibrium points can be obtained by simple algebra . )    to conclude we mention different extensions and applications of this model as possible future work .",
    "we mentioned , at the beginning , `` statistical mechanic '' studies .",
    "for instance , by dividing the four payoffs between say the reward @xmath5 reduces the parameters to three : @xmath153 , @xmath154 and @xmath155 , and we are interested to analyze the dependence of @xmath2 on each one of these 3 parameters in the vicinity of a transition between two different values .",
    "it is also interesting to introduce noise in the system , by means of an inverse temperature parameter @xmath156 , in order to allow irrational choices .",
    "the player @xmath22 changes his strategy with a probability @xmath157 given by            finally , a test for the model against experimental data seems interesting . in the case of humans",
    "the experiments suggest , for a given class of games ( _ i.e. _ a definite rank in the order of the payoffs ) , a dependency of @xmath161 with the relative weights of @xmath162 and @xmath8 , which is not observed in the present model .",
    "therefore , we should change the update rule in such a way to capture this feature .",
    "work is also in progress in that direction .",
    "+      we will now show in detail the calculus for the maximum of the gain estimate function @xmath163 , restricted to the interval @xmath120 $ ] .",
    "first we have to know if the function has a maximum in the open interval @xmath164 .",
    "this can be done by noticing that , by ( [ eq : epsilon2 ] ) , for having negative concavity , we have the condition : @xmath165 by doing @xmath166 we find that the extremum of @xmath163 is attained at @xmath167 imposing @xmath168 , @xmath169 and using ( [ eq : epsmax ] ) for consistency , we obtain : @xmath170 notice that the sum of this two conditions is equivalent to condition ( [ eq : epsmax ] ) . in turn ,",
    "( [ eq : cond ] ) can be expressed as @xmath171 so this inequality resumes ( [ eq : epsmax ] ) and ( [ eq : cond ] ) . it can be seen that if ( [ eq : ineqrstp ] ) is fulfilled , @xmath172 always .",
    "+ so if condition ( [ eq : ineqrstp ] ) holds , the maximum of the function @xmath163 takes place in the interval @xmath164 and its value as a function of the parameters @xmath173 and @xmath8 is : @xmath174 on the other hand , if @xmath175 then @xmath176 since @xmath177 , @xmath178 ."
  ],
  "abstract_text": [
    "<S> we analyze , both analytically and numerically , the self - organization of a system of `` selfish '' adaptive agents playing an arbitrary iterated pairwise game ( defined by a 2@xmath02 payoff matrix ) . </S>",
    "<S> examples of possible games to play are : the _ prisoner s dilemma _ ( pd ) game , the _ chicken _ game , the _ hero _ game , etc . </S>",
    "<S> the agents have no memory , use strategies not based on direct reciprocity nor tags and are chosen at random _ </S>",
    "<S> i.e. _ geographical vicinity is neglected . </S>",
    "<S> they can play two possible strategies : cooperate ( c ) or defect ( d ) . </S>",
    "<S> the players measure their success by comparing their utilities with an estimate for the expected benefits and update their strategy following a simple rule .    </S>",
    "<S> two versions of the model are studied : 1 ) the deterministic version ( the agents are either in definite states c or d ) and 2 ) the stochastic version ( the agents have a probability @xmath1 of playing c ) .    using a general master equation we compute the equilibrium states into which the system self - organizes , characterized by their average probability of cooperation @xmath2 . depending on the payoff matrix , we show that @xmath2 can take five different values .    </S>",
    "<S> we also consider the mixing of agents using two different payoff matrices an show that any value of @xmath2 can be reached by tunning the proportions of agents using each payoff matrix . </S>",
    "<S> in particular , this can be used as a way to simulate the effect a fraction @xmath3 of `` antisocial '' individuals -incapable of realizing any value to cooperation- on the cooperative regime hold by a population of neutral or `` normal '' agents .    </S>",
    "<S> psfig.sty    pacs numbers : 89.75.-k , 87.23.ge , 89.65.gh , 89.75.fb </S>"
  ]
}