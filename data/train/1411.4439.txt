{
  "article_text": [
    "in finite temperature quantum chromodynamics ( qcd ) fluctuations of conserved charges , baryon number ( @xmath1 ) , electric charge ( @xmath2 ) and strangeness ( @xmath3 ) , are particular interesting observables .",
    "they can be measured in experiments at the relativistic heavy ion collider ( rhic ) and the large hadron collider ( lhc ) and have also been calculated in lattice qcd ( lqcd ) with increasing precision  @xcite .",
    "they are derived from generalized susceptibilities @xmath4 where @xmath5 denotes the partition function of the medium at temperature @xmath6 and volume @xmath7 .    in lqcd the required derivatives of @xmath5 w.r.t .",
    "the chemical potentials @xmath8 can be obtained by stochastically estimating traces over combinations of the inverse and derivatives of the fermion matrix @xmath9 with a sufficiently large number of random vectors @xmath10 , e.g. @xmath11 to control the errors we use 500 - 1500 random vectors on each gauge configuration .",
    "depending on the desired highest derivative degree this involves several inversion of the fermion matrix for each random vector .    for reasons of the numerical costs ,",
    "staggered fermions are the most common type of fermions for thermodynamic calculations on the lattice .",
    "we use the highly improved staggered fermion ( hisq ) action  @xcite . in terms of the smeared links @xmath12 and naik links",
    "@xmath13 the dslash operator reads @xmath14\\ .\\end{aligned}\\ ] ] here @xmath13 and @xmath12 are complex @xmath15 matrices and @xmath16 are complex @xmath17-dimensional vectors . within the inversion the application of the dslash operator typically consumes more than @xmath18 of the runtime .",
    "this part already has a low arithmetic intensity ( see tab .",
    "[ arithm_intens ] ) and the average arithmetic intensity is further decreased by the linear algebra operation in the conjugate gradient .",
    "thus , the achievable performance is clearly bound by the available memory bandwidth .",
    "given its massively parallel nature and the bandwidth hunger it is well suitable for accelerators .",
    "lattice qcd simulations make extensive use of gpus for several years now  @xcite .",
    "the mic architecture is also gaining more attraction and codes are being optimized  @xcite .",
    "a common optimization to reduce memory accesses in lqcd is to exploit available symmetries and reconstruct the gauge links from 8 or 12 floats instead of loading all 18 floats . for improved actions these symmetries",
    "are often broken and thus we can only reconstruct the naik links from @xmath19 or @xmath20 floats .    for our and many other applications a large number of inversions are performed on a single gauge configuration . in this case",
    ", one can exploit the constant gauge field by grouping the random vectors in small bundles , thus applying the dslash for multiple right - hand sides ( rhs ) at once : @xmath21 this increases the arithmetic intensity of the hisq dslash as the load of the gauge field occurs only once for the @xmath22 rhs .",
    ".[arithm_intens]the arithmetic intensity of the hisq dslash for different number of right - hand sides ( rhs ) using full or reduced 14 float storage ( r14 ) for the naik links . [ cols=\"^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     we summarized some technical data of the accelerators we use for our comparison in table  [ tab2 ] . in the following",
    "we will only discuss our implementation for the intelxeon phi .",
    "information about our gpu implementation can be found in  @xcite .",
    "the intelxeon phiis an in - order ` x`86 based many - core processor  @xcite . the coprocessor runs a linux @xmath8os and can have up to 61 cores combined via a bidirectional ring ( see fig .  [ mic_ring ] ) .",
    "therefore , the memory transfers are limited by concurrency reaching only @xmath23 on a 7120p running a stream triad benchmark  @xcite .",
    "each core has a private l1 data and instruction cache and a globally visible l2 cache . in the case of an local l2 cache",
    "miss , a core can cross - snoop another s core l2 cache and if the data is present avoid a direct memory access .",
    "visualization of the bidirectional ring on the die and the microarchitecture of one core showing the scalar processing unit ( spu ) , vector processing unit ( vpu ) and the cache hierarchy .",
    "the latter is kept fully coherent through global distributed tag directories ( td).,scaledwidth=87.0% ]    each core has thirty - two @xmath24 ` zmm ` vector registers and 4 hardware context threads . to fully utilize the many integrated core ( mic ) it is , especially for memory - bound applications , necessary to run with four threads per core",
    "this offers more flexibility to the processor to swap the context of a thread , which is currently stalled by a cache miss .",
    "the mic has its own simd instruction set extension ` imic ` with support for fused multiply - add and masked instructions .",
    "the latter allows to conditionally execute vector instructions on single elements of a vector register .",
    "the coprocessor can stream data directly into memory without reading the original content of an entire cache line , thus bypassing the cache and increasing the performance of algorithms where the memory footprint is too large for the cache .",
    "* implementation :",
    "* we have parallelized our program with openmp and vectorized it using low - level compiler functions called intrinsics .",
    "these are expanded inline and do not require explicit register management or instruction scheduling through the programmer as in pure assembly code .",
    "there are @xmath24 intrinsics data types for single- and double - precision accuracy as well as for integer values .",
    "more than 32 variables of a @xmath24 data type can be used simultaneously . with only 32 ` zmm ` registers available in hardware , the compiler is , in this case , forced to use `` spills '' , i.e.  temporarily storing the contents of a register into l1 cache , and reloading the register when the data is required later , thereby increasing memory bandwidth pressure and cache pollution . when using intrinsics the software has to be designed in a register aware manner ; only the explicit management of the registers is taken over by the compiler .",
    "we found that the compiler is only able to optimize code over small regions .",
    "thus , the order of intrinsics can have an influence on the achieved performance , thereby making optimizations more difficult .",
    "nonetheless , the use of intrinsics for the dslash kernel is lightweight requiring only a subset of 9 instructions . due to the different links needed for the nearest and third - nearest neighbor term we implemented both in separate kernels , thereby reducing cache pollution and simplifying cache reuse for the vectors . for the global sums inside the linear algebra kernels we use the openmp reduction clause . in order to avoid explicit barriers",
    ", each thread repeats the calculation of the coefficients necessary for the cg in a thread local variable .",
    "* site fusion : * one problem of using @xmath24 registers involving @xmath25 matrix - vector products is that one matrix / vector does not fit into an integer number of ` zmm ` registers without padding .     visualization of a fused matrix - vector product using the 16-fold vectorization scheme .",
    "each lane corresponds to one ` zmm ` register , which holds the same element of all 16 sites . ]    because of this , it is more efficient to process several matrix - vector products at the same time using a site fusion method .",
    "a naive single - precision implementation could be to create a `` struct of arrays '' ( soa ) object for 16 matrices as well as for 16 vectors .",
    "such a soa vector object requires 6 ` zmm ` registers when it is loaded from memory .",
    "one specific register then refers to the real or imaginary part of the same color component gathered from all 16 vectors , thus each vector register can be treated in a `` scalar way '' ( see fig .",
    "[ 16fold ] ) .",
    "these soa objects are stored in an array using a site ordering technique .",
    "our dslash kernel runs best with streaming through @xmath26-planes and is specifically adapted for inverting multiple right - hand sides .",
    "therefore , we use a 8-fold site fusion method , combining 8 sites of the same parity in @xmath27-direction , which makes the matrix - vector products less trivial and requires explicit in - register align / blend operations . by doing so",
    ", we reduce the register pressure by 50% compared to the naive 16-fold site fusion method , leaving more space for the intermediate result of the right - hand sides for each direction @xmath8 .",
    "this is why the 8-fold site fusion is @xmath28 faster compared to the 16-fold scheme at 4 rhs ( see fig .",
    "[ perf_changes ] ) . for one right - hand side",
    "this optimization is insignificant since the 16-fold matrix - vector product requires only 30 of the 32 in hardware available ` zmm ` registers .",
    "performance comparison of the 8-fold and 16-fold vectorization scheme measured on a 5110p with enabled ecc .",
    "the dashed lines correspond to kernels without software prefetching . ]",
    "* prefetching : * for indirect memory access , i.e.  the array index is a non - trivial calculation or loaded from memory , it is not possible for the compiler to insert software prefetches .",
    "the mic has a l2 hardware prefetcher which is able to recognize simple access pattern .",
    "we found that it does a good job for a linear memory access .",
    "thus , there is no need for software prefetching by hand inside the linear algebra operations of the cg .",
    "however , the access pattern of the dslash kernel is too complicated for the hardware prefetcher .",
    "therefore , it is required to insert software prefetches using intrinsics .",
    "the inverter runs @xmath29 faster with inserted software prefetches .",
    "we unroll the loop over all directions and prefetch always one @xmath8-term ahead . the first right - hand side vector and link of the first @xmath8-term",
    "are prefetched at the end of the previous site loop iteration .",
    "considering that there is no reuse of gauge links , it is more efficient to prefetch these into the non - temporal cache . for the vectors we use temporal prefetch hints .",
    "it is important to note that software prefeteches are dropped if they cause a page table walk and in order to counterbalance the increased tlb pressure from the multiple right - hand sides , we store , for each lattice site , all rhs contiguously in memory .",
    "this approach is 15% faster for large lattices compared to an implementation which stores each right - hand side in a separate array .",
    "for the xeon phiwe used the intelcompiler 14.0 and mpss 3.3 with disabled instruction cache snooping , huge pages and a balanced processor affinity .",
    "for the gpu part we used the nvidiacuda 6.0 toolkit .",
    "for the k40 we enabled gpu boost at the highest possible clock rate 875 mhz . in all benchmarks we left ecc enabled .    in the left panel of fig .",
    "[ fig4 ] we show the performance as a function of the number of rhs .",
    "the maximum number of rhs is limited by memory requirements .",
    "we observe roughly the behavior as expected from the increased arithmetic intensity .",
    "comparing the results using four right - hand sides to one right - hand side we find a speedup of roughly 2.05 for the full cg , very close to the increase of 2.16 in arithmetic intensity for the dslash . despite the linear algebra operations that limit the obtainable speedup",
    "this is still about 95% of the expected effect .",
    "independent of the number of rhs the ordering with decreasing performance is k40 , 7120p , 5110p , k20 with the relative performance roughly given by 1.4 , 1.2 , 1.1 , 1.0 ( normalized to k20 ) .      in the right panel",
    "we show the performance with 4 rhs as a function of the lattice size . ignoring the smallest size , the k40 is always superior while the 7120p is faster than the k20 .",
    "the @xmath31 lattice seems to be more expedient for the simt model of the gpu .",
    "* acknowledgments : * we would like to thank intelfor providing a xeon phisystem and especially jeongnim kim for supporting this work .",
    "we acknowledge support from nvidiathrough the cuda research center program .        ,",
    "b * 737 * , 210 ( 2014 ) ; phys .",
    "lett .   * 113 * , 072001 ( 2014 ) ; * * , ( ) ; * * , ( ) .",
    "e. follana _",
    "[ hpqcd and ukqcd collaborations ] , phys .  rev .",
    "d * 75 * , 054502 ( 2007 ) ; a. bazavov _",
    "[ milc collaboration ] , phys .",
    "d * 82 * , 074501 ( 2010 ) .",
    "see e.g. f.  t.  winter _",
    "_ , arxiv:1408.5925 [ hep - lat ] ; r.  babich _ et al .",
    "_ , arxiv:1109.2935 [ hep - lat ] ; m.  a.  clark _ et al .",
    "_ , comput .",
    "commun .",
    "* 181 * , 1517 ( 2010 ) ; g.  i.  egri _ et al . _ ,",
    "commun .   * 177 * , 631 ( 2007 ) ."
  ],
  "abstract_text": [
    "<S> lattice quantum chromodynamics simulations typically spend most of the runtime in inversions of the fermion matrix . </S>",
    "<S> this part is therefore frequently optimized for various hpc architectures . here </S>",
    "<S> we compare the performance of the intelxeon phito current kepler - based nvidiateslagpus running a conjugate gradient solver . by exposing more parallelism to the accelerator through inverting multiple vectors at the same time , </S>",
    "<S> we obtain a performance greater than @xmath0 on both architectures . </S>",
    "<S> this more than doubles the performance of the inversions . </S>",
    "<S> we also give a short overview of the knights corner architecture , discuss some details of the implementation and the effort required to obtain the achieved performance . </S>"
  ]
}