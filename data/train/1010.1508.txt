{
  "article_text": [
    "statistical information theory @xcite constitutes an essential tool for modern signal processing , computation , coding , and communication systems . its core philosophy hinges on the notions of information potential and the ability of systems to encode , transmit and decode information about one or more input parameters .    based in statistical estimation theory , fisher information ( fi ) @xcite on the other hand represents the sensitivity of statistical data to one or more input parameters .",
    "its inverse , the so - called cramr - rao bound ( crb ) , yields a useful lower bound on the variance of any statistical data - based estimation of those parameters .    in spite of the different essential motivations for the two families of information measures , mutual information ( mi ) and fi are closely related at least asympotically in the limit of a large number of conditionally independent measurements @xcite .",
    "a recent paper explores the validity of this asymptotic relationship when the number of measurements is not particularly large @xcite .",
    "the relation of mi to fi is essentially a local one that is valid only in the limit of either a narrow channel pdf , as in ref.@xcite , or a narrow input pdf , as we shall see in this paper . in the more general case , the mi , as i shall also show ,",
    "is related more naturally to the minimum mean squared error ( mmse ) of bayesian estimation . unlike previous work @xcite , on this topic , the new relation , a lower bound on the mi , is general and applicable to arbitrary channel and input statistics .",
    "it may be regarded as a global generalization of the more restrictive local relations between mi and fi .",
    "a number of additional correspondences between the mi and mmse are derived that apply when either more measurements , or channels , are added or multiple input parameters must be estimated at once . in the latter case",
    "if the input parameters are statistically independent , then each parameter serves as a nuisance for the other parameters that must , in general , reduce both the mi and the fidelity of estimation for each parameter .",
    "these local and global considerations on the fundamental relationship between mi and bayesian estimation error are the subject of this paper .",
    "let @xmath0 be an input parameter that is statistically distributed according to the probability density function ( pdf ) @xmath1 @xcite with mean @xmath2 and variance @xmath3 .",
    "let @xmath4 be an output variable , _",
    "e.g. , _ a measurement variable , that carries information about @xmath0 , and is distributed according to the pdf @xmath5 . for notational definiteness ,",
    "let us take these variables to be continuous over appropriate ranges of values , but the analysis of this section applies equally well to discrete random variables too , provided all integrals over such variables are regarded as discrete sums over the corresponding sample spaces .",
    "the communication channel , or the measurement system as the case may be , is described by means of the conditional pdf , @xmath6 . in spite of the notation ,",
    "there is no restriction placed on the number of output variables represented by the symbol . in other words",
    ", @xmath4 is in general a multi - dimensional output vector .",
    "although i shall for clarity assume initially that the input is one - dimensional , the generalization to multiple input parameters , as we shall see subsequently , is straightforward .",
    "the three pdfs are related according to the bayes rule , @xmath7 the mi is defined in terms of the various pdfs by three different entirely equivalent expressions , @xmath8 where for each pdf @xmath9 denotes the corresponding differential entropy defined by averaging the negative logarithm of the pdf over the joint pdf , @xmath10 , @xmath11 and so on .",
    "i shall always use the natural logarithm for the definition of entropies in this paper , as it yields the simplest form of the final results .",
    "all entropy and information measures are thus expressed in natural units , or nats .    by using definitions of form ( [ e3 ] ) in the second of the expressions ( [ e2 ] ) and using the bayes relation ( [ e1 ] )",
    ", we may express the mi as the average @xmath12.\\ ] ] by expanding @xmath13 in a taylor series of powers of the deviation @xmath14 , we may transform the logarithmic term in eq .",
    "( [ e4 ] ) , @xmath15,\\ ] ] where the @xmath16 dependent  moments \" of the @xmath0-pdf are defined as @xmath17 by subtracting @xmath2 , the mean value of @xmath0 , from both @xmath18 and @xmath16 inside the integrand in eq .",
    "( [ e6 ] ) and noting that linear deviations from the mean average to 0 , we may easily evaluate the first two @xmath16-dependent moments as @xmath19    we may now expand the logarithm ( [ e5 ] ) to second order in the deviations and note that @xmath20 for all @xmath21 . in view of this result ,",
    "the only contributing term to the second order is @xmath22 ^ 2 ( \\partial \\ln \\ , p(y|x)/\\partial x)^2 $ ] . substituting this term into eq .",
    "( [ e4 ] ) yields to the second order the following expression for the mi , @xmath23 : @xmath24 ^ 2 \\ , \\int dy\\ , p(y|x )   \\nonumber\\\\ & \\qquad\\times \\left[{\\partial \\ln \\ , p(y|x ) \\over",
    "\\partial x}\\right]^2\\nonumber\\\\ & = { 1\\over 2}\\int dx\\ ,   p(x)\\ , ( x-\\bar x)^2 \\ , j(y|x),\\end{aligned}\\ ] ] where @xmath25 is the fi defined locally at each value of @xmath0 as @xmath26 ^ 2.\\ ] ]    this is the first important result of the paper .",
    "its validity is guaranteed for sufficiently narrow priors for which the higher - order deviations about the input mean are negligible .",
    "note the non - local character of this second - order equality ( [ e9 ] ) : the mi is a squared - deviation - weighted average of the fi , the latter evaluated locally over the full sample space of @xmath0 .    for multiple - input , multiple - output ( mimo ) channels , the following multi - parameter analog of the second - order result ( [ e9 ] )",
    "is easily derived as well : @xmath27 where @xmath28 denotes the deviation of the @xmath29th component of the input vector @xmath30 from its mean value .",
    "it is also possible to extend relations ( [ e9 ] ) and ( [ e11 ] ) to the case of _ discrete _ random input parameters by replacing all integrals over @xmath16 to discrete sums over values in the sample space of @xmath0 , writing instead of eq .",
    "( [ e5 ] ) @xmath31\\right\\},\\ ] ] expanding the logarithm in a power series , and then noting that up to the second order it may be expressed as @xmath32\\right\\}^2\\nonumber\\\\ & \\leq { 1\\over 2}\\be_x\\left\\{\\left [ { p(y|x)-p(y|x)\\over p(y|x)}\\right]^2\\right\\},\\end{aligned}\\ ] ] where the inequality follows from a simple application of the cauchy - schwarz inequality .",
    "the subscript @xmath0 to the expectation - value symbol indicates that the expectation is taken relative to @xmath0 , keeping other variables fixed .",
    "an expectation of the rhs above , first over @xmath33 , given @xmath16 , and finally over @xmath16 yields the following upper bound on mi to the second order : @xmath34,\\ ] ] where @xmath35 defined by @xmath36 ^ 2\\bigg| x , x^\\prime\\right\\}\\ ] ] is the chapman - robbins information ( cri ) @xcite . for a fixed value of @xmath0 , the cri when minimized over all possible values that @xmath37 can take yields , via its reciprocal , the tightest lower bound on the error in estimating the discrete variable in the single - test - point optimization subspace .",
    "note that the upper bound ( [ e11c ] ) applies to mimo channels as well .",
    "the results of this section have a simple interpretation : for a narrow input pdf , the mi , like the fi and cri , is a local sensitivity based measure of information .",
    "the more sensitive the channel pdf  and thus the data  to the input , the larger all these information measures .",
    "the gaussian linear channel illustrates this point well .",
    "consider the gaussian linear channel in which @xmath0 and @xmath4 are related through a linear gain parameter @xmath38 , a linear bias @xmath39 , and an additive noise @xmath40 distributed according to a zero - mean gaussian pdf of variance @xmath41 : [ e12 ] y = ax+b+n ,  n~n(0,_n^2 ) .",
    "in this case , the fi of @xmath4 , given @xmath42 , is easily computed to be [ e12p ] j(y|x)=a^2_n^2 , independent of @xmath16 . in view of this result ,",
    "the second - order equality ( [ e9 ] ) becomes @xmath43 times the power snr , which is the ratio of @xmath44 times the @xmath0-variance and the noise variance , [ e13 ] i(x;y)= 12 a^2_x^2_n^2= 12 snr .",
    "note that the gaussian - channel result ( [ e13 ] ) is independent of the statistics of @xmath0 .",
    "it is also in agreement with the well known expression for the mi of a gaussian channel with a gaussian input pdf , @xmath45 when the latter is expanded to the lowest order in @xmath3 .    for input pdfs that have arbitrary width , a different relation between the estimation error and mi",
    "can be obtained .",
    "the precise relation in this case involves the minimum mean - squared error of bayesian estimation and provides a lower bound on the mi .",
    "i next derive this lower bound .",
    "a good bayesian estimation is one that reduces the mean squared error ( mse ) to a value below the variance of the input pdf , the so - called prior .",
    "the variance of the prior represents the maximum mse incurred by electing to use the mean of the prior as the trivial estimator when no information from data is availaible as , e.g. , in the limit of a vanishing snr .",
    "the mse of a bayesian estimator , @xmath46 , of @xmath0 is defined as [ e14 ] _",
    "x=\\ { [ x(y)-x]^2 } , where the statistical average is taken over the joint distribution of @xmath0 and @xmath4 .",
    "the estimator @xmath47 that minimizes the mse is called the minimum - mse estimator ( mmsee ) @xcite .",
    "it is easily shown to be the mean of @xmath0 , given @xmath4 , _",
    "i.e. , _ its _ posterior _ mean , [ e15 ] x_m(y)=(x|y ) = x p(x|y ) dx .",
    "its mean value is the mean of the prior , @xmath2 .",
    "the mse corresponding to the mmsee is the minimum mse ( mmse ) that provides the tightest possible lower bound for the mse of _ any _ bayesian estimator of @xmath0 .",
    "since @xmath48 ^ 2 = \\hat x^2(y)-2x\\hat x+x^2 $ ] , we may express the mse ( [ e14 ] ) for the mmsee , _",
    "i.e. , _ the mmse as @xmath49+\\be [ \\hat x_{m}^2(y)]\\nonumber\\\\ & = \\be(x^2)-\\be[\\hat x^2_{m } ] = \\sigma_x^2-\\sigma_{m}^2,\\end{aligned}\\ ] ] where the last two equalities are obtained by recognizing that @xmath50 is the mmsee , @xmath51 , and that @xmath0 and @xmath52 both have the same expectation .",
    "since variance is always non - negative , the last equality proves that the mmse can never exceed the prior variance .",
    "the conditional differential entropy , @xmath53 , sometimes called equivocation , may be expressed as a statistical average over the output , @xmath4 , [ e17 ] h(x|y ) = -where the argument of the @xmath4-average is the conditional entropy , given a fixed value of @xmath4 .",
    "but for a given variance , @xmath54 , of the pdf @xmath55 , its entropy is bounded above by the entropy of a gaussian pdf with the same variance @xcite , namely @xmath56 . as a result ,",
    "the conditional differential entropy ( [ e18 ] ) is bounded above as follows : @xmath57\\nonumber\\\\ & \\leq { 1\\over 2}\\ln(2\\pi e)+{1\\over 2}\\ln \\left[\\int dy \\ , p(y)\\,\\sigma_{x|y}^2\\right],\\end{aligned}\\ ] ] where the second inequality results from the convexity of the logarithm .    to see that the integral on the rhs of the second of the relations ( [ e18 ] ) evaluates to the mmse",
    ", we may note that in view of relation ( [ e15 ] ) @xmath58\\right\\}^2 p(x|y)\\ , dx\\nonumber\\\\   & = \\int \\left[{\\hat x}(y)-x\\right]^2 p(x|y ) \\",
    ", dx,\\end{aligned}\\ ] ] whose @xmath4-average is simply the mse for the mmsee estimator , namely the mmse .",
    "( to simplify notation here and in the rest of the paper , i have omitted the subscript @xmath59 from the mmse estimator . )",
    "putting results ( [ e18 ] ) and ( [ e19 ] ) together , we arrive at the following upper bound on equivocation : [ e20 ] h(x|y ) ( 2e ) and the corresponding lower bound on the mi ( [ e2 ] ) : [ e21 ] i(x;y ) h(x)-12 ( 2e ) .",
    "result ( [ e21 ] ) is the second major contribution of this paper .",
    "it demonstrates the precise inverse relationship between the _ minimum _ bayesian estimation error and the _ minimum _ statistical information that can be transmitted by the measurement channel . for an additive , linear gaussian channel with a gaussian input , both inequalities in eq .",
    "( [ e18 ] ) become equalities , the first because in this case @xmath60 is gaussian and the second because @xmath54 is independent of @xmath4 . consequently , for such channel and input , the inequality ( [ e21 ] ) is obeyed as an equality . indeed , since the mmse for this case is simply @xmath61 , while @xmath62 is @xmath63 , we have the well known result , @xmath64 , for mi , where @xmath65 is the power snr and @xmath38 is the linear gain factor of the gaussian channel . the derivative equality obtained in @xcite , [ e22 ]",
    "dd i(x;y ) = 12 ^ 2_x , is a simple , immediate consequence of this result specific to gaussian channels .",
    "for a non - gaussian channel , the lower bound ( [ e21 ] ) on the mi , @xmath23 , is in general not attainable .",
    "i now analyze the poisson channel with a negative - exponential prior to illustrate this fact .",
    "consider the linear poisson channel with linear gain ( or , scaling ) factor @xmath38 and linear bias @xmath39 , so the conditional mean of output @xmath4 , given input @xmath0 , is @xmath66 .",
    "the conditional poisson probability distribution ( pd ) over the discrete samples of @xmath4 , given @xmath0 , has the form [ e23 ] p(y|x ) = ( ax+b)^yy !",
    ",  y=0,1,2,  .",
    "if we take the prior pdf to be negative exponential with mean @xmath2 , [ e24 ] p(x ) = \\ {    ll 1|x(-x/|x ) & for   x0 + 0 & otherwise ,    .",
    "then by bayes theorem the unconditional @xmath4-pdf takes the form @xmath67\\exp(-x/\\bar x),\\nonumber\\\\ & y=0,1,2,\\ldots .\\end{aligned}\\ ] ] by a suitable scaling and shift of the integration variable , this integral may be expressed in terms of the incomplete gamma function , [ e26 ] ( y+1,u)=_u^dx ( -x ) x^y , as [ e26 ] p(y)=1y!(a|x)^y(a|x+1)^y+1(b / a|x)(y+1,b(a|x+1)/a|x ) .",
    "the following expression for the mean squared mmsee , @xmath68 $ ] , is a simple consequence of the definition ( [ e15 ] ) and the bayes theorem : [ e27 ] = _ y=0^ , where @xmath69 denotes the expression [ e28 ] k(y)=dx x p(x ) p(y|x ) . for the poisson channel and negative - exponential prior",
    ", @xmath69 may be expressed in terms of @xmath70 , since the latter has a similar expression as ( [ e28 ] ) with the only difference that the factor @xmath16 is missing from the integrand . to see this , we first write @xmath71 in expression ( [ e28 ] ) and then recognize that for the poisson channel pd given by eq .",
    "( [ e23 ] ) @xmath72 equals @xmath73 times @xmath74 .",
    "this yields the following useful form for @xmath69 : [ e29 ] k(y)= ( y+1)a p(y+1 ) - ba p(y ) . substituting this expression into eq .",
    "( [ e27 ] ) and noting that [ e30 ] _",
    "y=0^(y+1 ) p(y+1 ) = y= a|x + b ;  _ y=0^p(y)= 1 ; and @xmath75 for the ne prior ( [ e24 ] ) , we obtain the following expression for the mmse ( [ e16 ] ) : [ e31 ] = 2 ^ 2 + 2ba + b^2a^2 - 1a^2_y=0^. we can now numerically evaluate the mmse expression ( [ e31])in the general case of arbitrary @xmath38 and @xmath39 , but for the case of zero bias , @xmath76 , a simple analytical expression can be derived as we now show .      expression ( [ e26 ] ) for @xmath70 now greatly simplifies since the incomplete gamma function in that expression becomes complete , taking the value @xmath77 , and the sum in expression ( [ e31 ] ) may now be easily performed analytically , since @xmath78 is related to the sum @xmath79 by two successive applications of the differential operator , @xmath80 .",
    "this yields the following simple expression for the mmse when @xmath76 : [ e33 ] = ^21+a .",
    "this expression has the desired property of reducing to the prior variance , @xmath81 , in the limit of vanishing snr , @xmath82 , and of vanishing in the opposite limit , @xmath83 .",
    "the mi may also be evaluated for the poisson channel and negative exponential prior , most simply via the second of the expressions ( [ e2 ] ) . since @xmath84 , the conditional mean of @xmath85 , given @xmath16 ,",
    "is simply [ e34 ] -= -(ax+b)[(ax+b)-1 ] + _ y|x ( y ! ) .",
    "a subsequent average over the prior @xmath1 then yields the conditional ( discrete ) entropy @xmath86 , which when subtracted from the unconditional output entropy @xmath87 $ ] produces the following exact expression for the mi : @xmath88 \\nonumber\\\\ & -\\sum_{y=0}^\\infty p(y)\\ ,   \\ln [ p(y)\\ , y!].\\end{aligned}\\ ] ] this too can be evaluated numerically .",
    "the differential entropy of the negative exponential prior takes a simple analytical form , since @xmath89 whose mean , the differential entropy of @xmath0 , is simply @xmath90 : [ e36 ] h(x)= 1 + .",
    "use of this expression and the mmse ( [ e31 ] ) yields the lower bound ( [ e21 ] ) on the mi .",
    "i now compare this lower bound numerically with the exact value given by the expression ( [ e35 ] ) .",
    "mi vs. the normalized linear gain parameter , @xmath91 , for three different values of @xmath39 , as indicated .",
    "the solid curves refer to the exact result ( [ e35 ] ) , while the corresponding dashed curves refer to the lower bound ( [ e21 ] ) .",
    ", width=288 ]    in fig .  1 i display , as a function of the normalized linear gain parameter @xmath91 , the exact expression ( [ e35 ] ) ( solid curves ) along with the corresponding lower bound ( [ e21 ] ) ( dashed curves ) for three different values of @xmath39 , namely 0 , 50 , and 100 .",
    "the lower bound becomes tighter as the gain parameter increases in value , but typically it fails to provide a useful , nontrivial lower bound below a certain threshold value of the gain .",
    "indeed , as the exact expression for the lower bound in the case @xmath76 obtained from eqs .",
    "( [ e36 ] ) , ( [ e33 ] ) , and ( [ e21 ] ) , namely @xmath92\\end{aligned}\\ ] ] shows , the lower bound drops below the trivial lower bound of 0 for @xmath91 below @xmath93 .",
    "a similar but higher threshold below which the lower bound ( [ e21 ] ) ceases to be nontrivial is obtained when @xmath39 is non - zero .",
    "however , as @xmath39 increases this lower bound becomes increasingly tighter and thus more useful at sufficiently large values of the normalized gain , @xmath91 .",
    "mi vs. the linear bias parameter , @xmath39 , for three different values of gain @xmath91 , as indicated .",
    "the solid curves refer to the exact result ( [ e35 ] ) , while the corresponding dashed curves refer to the lower bound ( [ e21 ] ) .",
    ", width=288 ]    in fig .  2 , i plot the exact values and the corresponding lower - bound values for the mi as a function of the linear bias parameter , @xmath39 , for three different values of the gain parameter , @xmath91 .",
    "as @xmath39 increases , the mi decreases as expected since the sensitivity of data on the input variable @xmath0 is reduced . raising the linear gain raises the mi , as expected , for each @xmath39 value , as the previous figure shows .",
    "again , it is clear that the lower bound ( [ e21 ] ) is useful one for sufficiently large values of @xmath91 and @xmath39 .",
    "when mimo channels are involved , we may organize the input and output variables into two different column vectors , say @xmath94 and @xmath95 , where @xmath96 denotes a matrix transpose .",
    "the @xmath40-parameter analog of the upper bound ( [ e18 ] ) is simply @xmath97\\right\\},\\ ] ] where @xmath98 denotes the determinant of the positive semi - definite covariance matrix of @xmath99 , given @xmath100 .",
    "the determinant of such a matrix is a product of its @xmath40 non - negative eigenvalues , or simply the @xmath40th power of their geometric mean .",
    "since the latter can not exceed the arithmetic mean of these eigenvalues , which is @xmath101 times the trace of the matrix , and since the logarithm is a convex function , we have the following inequalities for @xmath53 : @xmath102 \\nonumber\\\\   & \\leq { n\\over 2 } \\ln(2\\pi e / n)\\ , + \\,{n\\over 2 } \\ln\\ , \\be\\left(\\tr\\ , \\bc_{x|y}\\right ) \\nonumber\\\\   & = { n\\over 2 } \\ln(2\\pi e \\mmse),\\end{aligned}\\ ] ] where @xmath103 here is the average minimum mse of a component - wise estimation of @xmath99 , @xmath104^t[\\bfx-\\hat \\bfx(\\bfy)]\\big|y\\right\\}\\nonumber\\\\ & = { 1\\over n}\\be_y [ \\tr\\,\\bc_{x|y } ] , \\end{aligned}\\ ] ] involving the mmsee @xmath105 for the mimo problem , @xmath106 correspondingly , the mi is lower bounded by [ m27 ] i ( ; ) h()-n2 ( 2e ) .",
    "note that in the mimo case each component of the mmsee minimizes the mse of the corresponding input parameter , the one it estimates .",
    "as such , the mmsee vector ( [ m26 ] ) as a whole also minimizes the average mse per component of the input vector .",
    "i now establish two additional important properties of the mmse not previously reported in the literature but which help strengthen the correspondences with information i have already discussed via relations ( [ e10 ] ) , ( [ e11 ] ) , ( [ e21 ] ) , and ( [ m27 ] ) .",
    "the first of these concerns the behavior of the mmse as additional measurements are made .",
    "it is well known @xcite that both mi and fi exhibit an additive property , namely @xmath107 which represents the fact that in general an additional measurement only increases information .",
    "the conditional information , either @xmath108 or @xmath109 , is a direct measure of the capacity of the measurement @xmath110 to improve information about @xmath0 , given that the measurement @xmath4 has already been made . since the estimation variance is lower bounded by the inverse of the fi , the two relations ( [ e38 ] ) represent a useful inverse relationship between mi and estimation error .",
    "but this fundamental relationship is at best a local one since , as i have argued before , the fi and its inverse , the cramr - rao lower bound on estimator variance , are local measures of information and estimation fidelity .",
    "i now show that mmse exhibits a similar behavior , which will serve to accord a general global character to this local inverse relationship between information and error .",
    "let us consider two measurements , @xmath4 and @xmath110 , of the input parameter @xmath0 .",
    "the joint mmse estimator , @xmath111 , has the following mean squared value : @xmath112&=\\be\\left[\\iint dx\\ , dx^\\prime x\\ , x^\\prime \\ ,",
    "p(x|y , z)\\ , p(x^\\prime|y , z)\\right]\\nonumber\\\\ = \\iint & dx\\,dx^\\prime x\\ , x^\\prime   \\iint dy \\",
    ", dz { p(x , y , z ) p(x^\\prime , y , z ) \\over p(y , z)},\\end{aligned}\\ ] ] where the bayes theorem was used to replace the _",
    "posterior _ probabilities in terms of the joint pdfs .    in terms of the integral , [ e40 ] k(z , y )",
    "dx x p(x , y , z ) , we may write the mean squared value of the joint mmse estimator ( [ e39 ] ) as @xmath113 ^ 2\\}.\\end{aligned}\\ ] ] the first equality follows from substituting the bayes relation , @xmath114 , and definition ( [ e40 ] ) into expression ( [ e39 ] ) and from the unit normalization of the pdf @xmath115 ; the second line follows from the cauchy - schwarz inequality ; the third line from a substitution of the definition ( [ e40 ] ) and the identity , @xmath116 ; and the fourth line from an interchange of the order of the integrals .",
    "since the last expression in inequality ( [ e41 ] ) is simply the mean squared value of the mmse estimator , @xmath46 , relative to the measurement @xmath4 alone , we have arrived at the desired result , @xmath117\\nonumber\\\\ & \\leq\\be(x^2)-\\be[\\hat x^2(y ) ] = \\mmse(y),\\end{aligned}\\ ] ] where we have used the fact that @xmath118 = \\be[\\hat x(y , z)]=e(x)$ ] to express the mse , @xmath119 ^ 2]\\}$ ] , as the difference of mean squared values of the prior and the estimator .",
    "note that for the inequality ( [ e42 ] ) to hold , the two measurements are not required to be conditionally independent , given the input @xmath0 .",
    "the second property of mmse relates to the case of multiple input parameters and how the error in the estimation of any one parameter is affected by the presence of the others . but to fully appreciate this property , we must place it in the context of statistical information processing to which i now turn .",
    "it is well known that the fidelity of estimation of a parameter , defined here as the smallness of the lower bound on the statistical variance of the estimator , decreases when other parameters are added to the problem .",
    "these added parameters , when not of interest , are known as nuisance parameters , and serve to reduce the fidelity , _",
    "i.e. , _ increase the variance , of estimation of the parameter of interest .",
    "the essence of this phenomenon is captured well by the fi matrix and its inverse whose diagonal elements provide the cramr - rao lower bounds on the variances of an unbiased estimation of the parameters @xcite .",
    "a similar result must hold in the context of statistical information theory as well .",
    "it must be possible to show that when the output variables @xmath4 depend on two input parameters , @xmath0 and @xmath120 , that are distributed independently , then the mi between @xmath0 and @xmath4 can not be larger than the mi obtained by computing the mi between @xmath0 and @xmath4 for a fixed value of @xmath120 first and then averaging it over the statistical distribution of the possible values of @xmath120 .",
    "the latter , averaged mi represents the information about @xmath0 successfully transmitted through the information channel when @xmath120 is held fixed in each instance , so the statistical dispersion of @xmath120 does not corrupt the data relative to their capacity to carry information about @xmath0 .",
    "i now prove this result .",
    "let us define @xmath121 as the mi in the case @xmath120 serves as a nuisance parameter , namely as @xmath122 where @xmath123 are defined as before .",
    "this expression for mi may also be written as the following average over all three variables : @xmath124\\right\\ } \\nonumber\\\\   & = -\\int p(x , y , u )    \\ln\\left[{p(x , y)\\over p(x ) p(y)}\\right ] dx\\ , dy\\ , du,\\end{aligned}\\ ] ] as the integral over @xmath125 only affects the joint density @xmath126 , reducing it to the marginal , @xmath10 .    in the absence of nuisance , which is indicated by a @xmath127 superscript ,",
    "the mi is the following @xmath128averaged conditional mi : @xmath129 dx\\ , dy\\ , du.\\ ] ] note that @xmath130 is the same as the more familiar conditional mi , @xmath131 , so the difference between @xmath121 and @xmath130 is equivalently that between @xmath23 and its conditional version , @xmath131 , which , as is well known @xcite , can be of either sign .    in view of jensen s inequality applied to the logarithm , the difference between the two mis , ( [ n16 ] ) and ( [ n17 ] ) , has a lower bound , @xmath132 dx\\ , dy\\ , du\\nonumber\\\\ & \\geq-\\ln\\ \\int p(x , y , u )   \\left[{p(x , y)\\ , p(x|u)\\ , p(y|u)\\over p(x , y|u)\\ , p(x)\\ , p(y)}\\right ] dx\\ , dy\\ , du\\nonumber\\\\ & = -\\ln\\   \\int   \\left[{p(u)\\ , p(x , y)\\ , p(x|u)\\ , p(y|u)\\over   p(x)\\ , p(y)}\\right ] dx\\ , dy\\ , du\\nonumber\\\\ & = -\\ln\\ \\int   \\left[{p(x|u)\\ , p(y , u)\\ , p(x , y)\\over p(x)\\ , p(y)}\\right ] dx\\ , dy\\ , du.\\end{aligned}\\ ] ] in obtaining the last two equalities above , i have used the bayes rule twice , first via the identity @xmath133 , and then via the identity @xmath134 .",
    "when the variables @xmath0 and @xmath120 are statistically independent , @xmath135 , the above inequality simplifies greatly to the form @xmath136 dx\\ , dy\\ , du\\nonumber\\\\   & = -\\ln\\,\\iint p(x , y ) dx\\ , dy\\ = -\\ln\\ 1 = 0,\\end{aligned}\\ ] ] where i used the fact that @xmath137 and the normalization of the joint pdf @xmath10 .",
    "this proves our assertion .",
    "note that since i have made no explicit use of the dimensionality of the input and output spaces in this proof , the result is valid for an arbitrary mimo chennel .",
    "a correspondence may be drawn with analogous results from statistical estimation theory using fi .",
    "one can consider two different estimation problems involving nuisance , one in which the nuisance is also estimated and another in which it is not , which must be treated separately .",
    "[ [ estimation - of - both - input - and - nuisance - parameters ] ] estimation of both input and nuisance parameters + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the fi matrix relative to @xmath0 and @xmath120 , when both are unknown , namely @xmath138 , may be expressed in terms of the fi matrix relative to @xmath0 , when @xmath120 is known , namely @xmath139 , in the following block form : @xmath140,\\ ] ] where the matrix block @xmath141 refers to the fi matrix relative to @xmath120 alone , when @xmath0 is known , and the off - diagonal blocks @xmath142 and @xmath143 , which are transposes of each other , refer to the cross - sensitivity of the data likelihood relative to @xmath0 and @xmath120 .",
    "the presence of the cross - sensitivity matrices , @xmath142 and @xmath143 , tends to increase the crbs since , as one may easily show @xcite that , e.g. , the @xmath144 block of @xmath145 has the form @xmath146 the matrix inequality following from the fact that @xmath147 is a positive matrix .",
    "for the bayesian case of priors on @xmath0 and @xmath120 , assumed for the moment to be uncorrelated , the fi matrices relative to these priors on @xmath0 and @xmath120 must be added to the blocks @xmath139 and @xmath141 , respectively , in expression ( [ f1 ] ) .",
    "adding these prior - information - based fi submatrices has , as expected , the opposite effect : it decreases the crbs on @xmath0 and @xmath120 , thus improving the fidelity of estimation .",
    "[ [ estimation - of - input - without - estimating - nuisance ] ] estimation of input without estimating nuisance + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this case , we must integrate over the statistical distribution of nuisance to obtain the needed pdfs from their nuisance - free counterparts .",
    "we have , in particular , @xmath148 where the input , output , and nuisance parameters have been organized into three respective column vectors , @xmath99 , @xmath100 , and @xmath149 .",
    "if we take the nuisance and input variables to be statistically uncorrelated , @xmath150 , then we may take the gradient of eq .",
    "( [ fn1 ] ) with respect to @xmath151 simply , @xmath152 the inner product of this gradient vector with an arbitrary vector , @xmath153 , of the same length generates a scalar quantity @xmath154 upon writing the integrand in eq .",
    "( [ fn3 ] ) as the bilinear product @xmath155 \\nonumber\\\\ & \\times [ p^{1/2}(\\bfu ) p^{1/2}(\\bfy|\\bfx,\\bfu)],\\end{aligned}\\ ] ] squaring both sides of that equation , and then using the cauchy - schwarz inequality , we arrive at the inequality @xmath156 ^ 2 & \\leq \\int d\\bfu\\ , p(\\bfu ) { 1\\over p(\\bfy|\\bfx,\\bfu)}[\\blambda^t \\bnabla_x p(\\bfy|\\bfx,\\bfu)]^2\\nonumber\\\\ & \\qquad\\times \\int p(\\bfu)\\ , p(\\bfy|\\bfx,\\bfu)\\ , d\\bfu . \\end{aligned}\\ ] ] since the last @xmath157-integral above evaluates simply to @xmath158 according to bayes rule , by dividing both sides by @xmath158 , integrating over @xmath159 , and finally averaging over @xmath99 , we obtain the desired inequality , @xmath160 where the fi matrices in the presence and absence of nuisance are defined as @xmath161 ^ 2 \\nonumber\\\\ & \\qquad \\times\\bnabla_x p(\\bfy|\\bfx)\\ , \\bnabla_x^t p(\\bfy|\\bfx )   \\nonumber\\\\ \\bj^{(-)}(\\bfy|\\bfx)&\\defeq \\int d\\bfu\\ , p(\\bfu ) \\iint d\\bfx\\ , d\\bfy \\ , p(\\bfx ) \\ , p(\\bfy|\\bfx,\\bfu ) \\nonumber\\\\ & \\times \\left[{1\\over p(\\bfy|\\bfx,\\bfu)}\\right]^2   \\bnabla_x p(\\bfy|\\bfx,\\bfu)\\ , \\bnabla_x^t p(\\bfy|\\bfx,\\bfu ) .",
    "\\end{aligned}\\ ] ] note that for statistically mutually independent input and nuisance parameters , the prior - based fi for the input paramaters is the same whether the nuisance parameters are present or absent .",
    "it then follows from the the non - negative - definiteness of the difference of the data - based fis , @xmath162 , implied by relation ( [ fn6 ] ) , that the corresponding difference between the sums of data - based and prior - based fis is also non - negative - definite .",
    "this result embodies the fact that in general nuisance parameters even when they are not estimated , if statistically independent of the input parameters of interest , degrade the fidelity with which the latter can be estimated @xcite .      for the more general case",
    "when @xmath0 and @xmath120 are statistically correlated , @xmath120 may indeed carry information about @xmath0 through their correlation , in which case the rhs of the inequality ( [ n18 ] ) may be negative allowing for @xmath163 to exceed @xmath164 . as i noted before , in this general case the inequality ( [ n18 ] ) can be of either sign .",
    "the corresponding result from statistical estimation theory is based on the fact that any information that @xmath120 has about @xmath0 through its correlations with it yields an additional fi submatrix , @xmath165 , to be added to the @xmath139 block in eq .",
    "( [ f1 ] ) .",
    "this submatrix represents information that @xmath120 carries about @xmath0 through the first - order sensitivity of @xmath166 on @xmath16 . unlike the coupling of @xmath120 to the data alone",
    ", such additional prior information can reduce the crbs on estimating @xmath0",
    ". when the nuisance parameters are _ not _ estimated , but are correlated with the input parameters of interest , the basic relation ( [ fn2 ] ) used to obtain the desired inequality ( [ fn6 ] ) is itself not valid .",
    "also , the prior - based fis are not necessarily the same with and without nuisance . as a result ,",
    "the data - based fi or prior - based fi or both may contain more information about the parameters of interest in the presence of nuisance than in its absence .      as a first example , let us consider a gaussian additive channel with additive noise @xmath40 , @xmath167 where @xmath0 , @xmath120 , and @xmath40 are all independently normally distributed as follows : @xmath168 thus the marginal pdf for @xmath4 as well as its various conditional pdfs are all gaussian too , @xmath169    in view of the variances given in relations ( [ n22 ] ) , we may easily write down the mis of interest here using the well known expression for the differential entropy for a gaussian additive channel @xcite , @xmath170 since @xmath171 , it follows that @xmath172 . for uncorrelated @xmath0 and @xmath173 ,",
    "the latter serves as a nuisance relative to information about the former in the sense that the terms @xmath174 and @xmath40 in the model ( [ n21a ] ) simply combine to yield an increased noise variance , @xmath175 , on the determination of @xmath0 from @xmath4 .",
    "but when the nuisance is removed by holding @xmath173 fixed in each measurement , the noise variance is lower at @xmath41 , leading to increased information about @xmath0 .",
    "as my second example , let us modify the gaussian channel represented by eqs .",
    "( [ n21a ] ) and ( [ n21b ] ) simply to include a correlation between @xmath0 and @xmath173 , so only the first of the relations in eq .",
    "( [ n21b ] ) is changed to the conditional relation @xmath176 while the remaining relations are unchanged . in the limit",
    "that @xmath177 , the variables @xmath0 and @xmath120 become uncorrelated as in the previous example .",
    "thus the largeness of @xmath178 in relation to @xmath179 may be regarded as the strength of the correlation between @xmath0 and @xmath173 .    in view of the relation ( [ n24 ] ) and the fact that @xmath180 , the marginal pdf @xmath1 is also gaussian .",
    "the conditional pdf , @xmath181 , for @xmath4 , given @xmath173 , may be computed by integrating @xmath182 over @xmath16 .",
    "the marginal pdf @xmath5 is then obtained by integrating @xmath183 over @xmath125 . using standard analysis involving gaussian integrals",
    ", we may derive the following marginal and conditional pdfs : @xmath184 where the @xmath4-variance may be expressed as @xmath185 having evaluated @xmath1 , we may now evaluate @xmath166 , via bayes rule , as the ratio @xmath186 , @xmath187\\nonumber\\\\ & = { 1\\over \\sqrt{2\\pi\\sigma_{u|x}^2}}\\exp\\left[-{\\left(u-\\bar{u}_{|x } \\right)^2\\over 2\\sigma_{u|x}^2}\\right],\\end{aligned}\\ ] ] where the conditional mean , @xmath188 , and variance , @xmath189 , are given by the expressions @xmath190    by multiplying @xmath166 with @xmath191 and integrating over @xmath125 ,",
    "we may obtain the last of the needed conditional pdfs , namely @xmath6 , @xmath192,\\ ] ] where the conditional mean and variance may be expressed as @xmath193    we are now in a position to write down both @xmath121 and @xmath194 by use of the gaussian - channel entropy formula in terms of the variances of @xmath5 , @xmath6 , @xmath181 , and @xmath195 , @xmath196 ; \\nonumber\\\\ i^{(-)}(x;y ) & = i(x;y|u)\\nonumber\\\\ & = { 1\\over 2 } \\ln \\left(1+{a^2\\sigma_{x|{u}}^2\\over \\sigma_n^2}\\right ) .",
    "\\end{aligned}\\ ] ] note that in the limit of @xmath197 , the two variables , @xmath0 and @xmath173 , are infinitely tightly coupled . in effect , @xmath198 , and the data @xmath4 carry no information about @xmath0 when @xmath173 is held fixed .",
    "this is seen in the relation ( [ n30 ] ) . by contrast , @xmath121 is finite in this limit , and thus trivially exceeds @xmath130 .",
    "the other limit , @xmath177 , returns us to the case of uncorrelated @xmath0 and @xmath120 variables for which the results ( [ n23 ] ) are recouped and @xmath130 exceeds @xmath121 .",
    "the more general cases in which neither of these limits is a good approximation are illustrated in fig .",
    "i plot here @xmath121 ( solid curves ) and @xmath130 ( dashed curves ) for the case of normalized @xmath173 variance , @xmath199 , equal to 5 .",
    "each variance is normalized the same way by multiplying it with @xmath200 .",
    "six different values of @xmath201 , which determines the largness of the conditional mean of @xmath0 , given @xmath173 , were used to generate the various @xmath163 curves . by contrast , @xmath164 is independent of @xmath201 , as seen from the single dashed curve on each plot .",
    "a number of observations can be made from these plots .",
    "first , for @xmath202 , the variables @xmath0 and @xmath173 are uncorrelated , so in this case the plot of mi in the presence of the nuisance variable , @xmath173 , lies below that for mi when uninfluenced by the nuisance variable .",
    "second , as @xmath201 increases , the coupling of @xmath0 and @xmath173 becomes increasingly less sensitive to the noise in @xmath173 .",
    "this means that when @xmath4 is measured , its value reveals more information about @xmath0 than when @xmath201 is smaller . when the nuisance is removed , _",
    "@xmath173 is held fixed , then a change of @xmath201 merely changes the mean value of @xmath0 , leaving its variance unchanged , which is the reason why @xmath164 depends neither on @xmath201 nor on @xmath203 .",
    "third , as the relative strength , @xmath204 , of the nuisance increases while @xmath201 is held fixed , @xmath163 increases initially since the data @xmath4 possess an increasing amount of information about @xmath0 through the latter s coupling to @xmath173 .",
    "however , increasing the strength of the @xmath174 term in eq .",
    "( [ n21a ] ) to large values leads to the data becoming more corrupted than helped by the nuisance , which leads to an eventual decrease of the mi .",
    "these two competing tendencies lead to a maximum for each curve ( left top and bottom ) , with the location of the maxima shifting to larger nuisance - parameter strength values with increasing @xmath201 .",
    "fourth , comparing the plots for the smaller vs. larger values of @xmath205 ( left panels ) , we see that the tighter the @xmath0-@xmath120 coupling the softer the degradation of mi@xmath206 with increasing strength of the nuisance parameter . finally , as seen from the right - hand panels of the figure , an infinitely tight coupling between @xmath0 and @xmath173 ( for @xmath207 ) yields , through the sensitivity of data to @xmath173 , information about @xmath0 as well .",
    "this information about @xmath0 degrades when @xmath208 increases to finite values , the more so the smaller the parameter @xmath201 .",
    "when multiple input parameters must all be estimated from the same measurement(s ) , one expects the mmse , like the crb , for estimating any of the parameters to be higher than if the others were not present .",
    "i prove this result next .",
    "let @xmath209 be two input parameters to be estimated from data @xmath4 .",
    "let @xmath210 be the joint prior on the inputs .",
    "the mmse estimator for @xmath0 in the absence of the nuisance @xmath120 can be defined in terms of the conditional mmse estimator , @xmath211 given @xmath212 .",
    "it is the mmse estimator of @xmath0 for a given value of @xmath120 .",
    "its mean squared value has an expression analogous to that found in ( [ e41 ] ) , @xmath213 where @xmath214 stands for the function @xmath215 we also used the bayes - rule identity , @xmath216 , to arrive at the last line of eq .",
    "( [ n32 ] ) .    a use of the cauchy - schwarz inequality in eq .",
    "( [ n32 ] ) shows that the mean squared value of the mmse estimator in the absence of nuisance has the lower bound @xmath217,\\end{aligned}\\ ] ] where a simple substitution of @xmath214 from eq .",
    "( [ n33 ] ) was used to obtain the second relation , bayes rule to obtain the third relation , and the definition of the mmse estimator @xmath46 as the posterior mean of @xmath0 , namely @xmath218 , to arrive at the final relation .",
    "since the mmse , as we have noted earlier , may be expressed simply as the mean squared value of @xmath0 minus the mean squared value of the mmse estimator , the desired inequality between the mmse without and with nuisance follows immediately , @xmath219    we deduce from this important result that the presence of the nuisance parameter can _ never _ lower the mmse below that obtained in its absence , _",
    "i.e. , _ when the nuisance has a known value , regardless of whether the priors on @xmath0 and the nuisance @xmath120 is statistically correlated or not .",
    "this seems to exclude the possibility that @xmath120 if suitably correlated with @xmath0 may serve , as we observed in sec .",
    "[ mi+- ] in the context of mi , as a source of additional information for @xmath0 .",
    "the answer to this apparent paradox may be found in the way mmse is defined . since given a value of the nuisance @xmath125 , the mmse estimator minimizes the mse relative to the corresponding conditional prior , @xmath220 , on @xmath0 and measurement pdf @xmath195 , the nuisance - averaged mmse is not characterizable as the mse for a single , nuisance - averaged mmse estimator .",
    "the mmse metric thus may not possess the same degree of specificity as the mi or fi metrics when the effect of nuisance must be quantified .",
    "we now illustrate the effect of nuisance on the mmse with our previous example of a gaussian channel for which some of the relevant pdfs are given in eqs .",
    "( [ n24])-([n28 ] ) .",
    "what we need are the mmse@xmath221 estimators , namely @xmath222 given by expression ( [ n31 ] ) , and @xmath46 by ( [ e15 ] ) .",
    "as is well known from the theory of mmse @xcite for gaussian priors and gaussian channel pdfs , each mmse estimator may be expressed as the inverse - variance - weighted sum of its prior and measurement based estimates , @xmath223,\\end{aligned}\\ ] ] where the multipiers @xmath224 are given by @xmath225 and the unconditional @xmath0-variance , @xmath3 , may be expressed as @xmath226 the various data and prior based estimates and variances used in arriving at the expressions ( [ n36 ] ) have been inferred from the mean values and variances of the pdfs given in eqs .",
    "( [ n24])-([n29 ] ) .",
    "[ [ mmse - in - the - absence - of - nuisance ] ] mmse in the absence of nuisance + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to compute the mean squared values of these estimators , we first subtract and add the appropriate mean values of @xmath4 from it in the expressions ( [ n36 ] ) and then use the fact that @xmath227=\\be[(\\delta y)^2]+q^2 $ ] , where @xmath228 is the deviation of @xmath4 from its mean and @xmath229 is any quantity independent of @xmath4 . for the mmse@xmath230 estimator , the mean we subtract and add is the conditional mean of @xmath4 , given @xmath125 , namely @xmath231 , so the following conditional squared mean value for it , given @xmath125 , results : @xmath232&=f^{(-)2}\\left[{a^2\\sigma_{x|u}^2+\\sigma_n^2\\over a^2\\sigma_n^4 } + \\alpha^2u^2/f^{(-)2}\\right]\\nonumber\\\\ & = { \\sigma_{x|u}^2/\\sigma_n^2\\over { 1\\over\\sigma_n^2}+{1\\over a^2\\sigma_{x|u}^2 } } + \\alpha^2u^2.\\end{aligned}\\ ] ] an averaging of this expression over @xmath125 with the help of the result @xmath233 then yields the required mean squared value of the mmse@xmath230 estimator . subtracting this squared mean value from @xmath234 , the latter being simply @xmath235 , generates , according to eq .",
    "( [ e16 ] ) , the mmse@xmath230 , @xmath236 where use was made of relation ( [ n38 ] ) in the second line .    [ [ mmse - in - the - presence - of - nuisance ] ] mmse in the presence of nuisance + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    subtracting and adding the mean value of @xmath4 , namely @xmath237 , from @xmath4 inside the expression ( [ n36 ] ) for the estimator @xmath46 and the squaring and averaging over @xmath4 generates the following mean squared value of the mmse estimator in the presence of nuisance : @xmath238&=f^{(+)2}{\\sigma_y^2\\over \\sigma_{y|x}^4 } \\left[{a\\sigma_x^2+\\alpha b\\sigma_u^2\\over\\sigma_x^2}\\right]^2\\nonumber\\\\ & + f^{(+)2}{\\alpha^2\\bar u^2\\over \\sigma_x^4 } \\left[{(a\\sigma_x^2+\\alpha b\\sigma_u^2)^2\\over\\sigma_{y|x}^2\\sigma_x^2}+1\\right]^2\\nonumber\\\\ & = \\sigma_y^2 \\left[{(a\\sigma_x^2+\\alpha b\\sigma_u^2)^2\\over\\sigma_{y|x}^2 + ( a\\sigma_x^2+\\alpha b\\sigma_u^2)^2/\\sigma_x^2}\\right]^2\\nonumber\\\\ & + \\alpha^2\\bar u^2,\\end{aligned}\\ ] ] where use was made of the definition ( [ n37 ] ) of @xmath239 to simplify both terms on the rhs . in view of relations ( [ n29 ] ) for the conditional mean and variance of @xmath4 , given @xmath42 , and the fact that all pdfs are gaussian",
    ", we may express the unconditional variance of @xmath4 , namely @xmath240 , as the sum of conditional variance , @xmath241 , given @xmath0 , and @xmath242 times @xmath3 .",
    "this observation greatly simplifies the preceding expression , @xmath243=\\alpha^2\\bar u^2 + { ( a\\sigma_x^2+\\alpha b\\sigma_u^2)^2\\over\\sigma_y^2}.\\ ] ]    the mmse now follows from subtracting expression ( [ n42 ] ) from @xmath244 , a result that can be simplified further in view of the relation between @xmath240 and @xmath241 that we just noted in the previous paragraph , @xmath245 by using relations ( [ n29 ] ) , ( [ n27 ] ) , the alternate form of @xmath240 given by relation ( [ n25 ] ) , and @xmath246 , we may express the mmse in the presence of nuisance in the more explicit form @xmath247    in this form , we may easily compare mmse@xmath206 to the corresponding result ( [ n40 ] ) for mmse in the absence of nuisance .",
    "a sequence of steps involving simple algebraic manipulations , followed by a use of the inequality , @xmath248 , easily confirms the general result proved earlier that the presence of nuisance parameters can never reduce the mmse for the estimation of the parameter of interest , @xmath249    this result is illustrated in fig .  4 where we plot both mmse@xmath250 , in units of @xmath251 , as functions of the variable @xmath252 for different values of the nuisance coupling parameter , @xmath253 .",
    "the reciprocal of @xmath254 is a measure of the strength of the statistical correlation between @xmath0 and nuisance @xmath120 , while @xmath255 represents the ability of nuisance to carry information about @xmath0 through its statistical correlations with @xmath0 .",
    "as @xmath254 becomes larger , the prior on @xmath0 becomes broader and the measurement becomes increasingly more dominant in controlling the mmse whether the nuisance is absent or present .",
    "but , as expected , mmse@xmath230 does not depend on the coupling parameter @xmath255 or the nuisance - parameter snr defined as snr@xmath256 . on the other hand",
    ", mmse@xmath206 decreases with increasing @xmath255 since the nuisance becomes increasingly more effective - and the data @xmath4 increasingly less so - in controlling the mse . with increasing snr@xmath257 , from 10 to 100 between the two panels of the figure",
    ", the nuisance causes an increased error in estimating @xmath0 , as its increased variance leads to an increased variance of the prior on @xmath0 .",
    "but in no event does the mmse in the presence of nuisance fall below the mmse without nuisance .",
    "the optimal condition under which the presence of nuisance does not degrade the mmse , _",
    "mmse@xmath206 = mmse@xmath230 , is achieved when @xmath258 , as seen from the figures and can also be easily shown analytically from the expressions ( [ n40 ] ) and ( [ n44 ] ) .",
    "in this paper i have derived a number of previously unknown relationships between mutual information and the minimum error of estimating a parameter from its measurements .",
    "a seoond order linear relation between mi and a prior - averaged , squared - deviation - weighted form of the fi accords added significance to the phrase  information \" when describing the latter even though its chief claim to this phrase has been in the sense of being the reciprocal of estimation error .    a second",
    ", more important relation between information and estimation error has been obtained in the fully bayesian context of minimum mean squared error .",
    "i have shown , in particular , that the shannon equivocation , @xmath53 , in the differential sense can not exceed @xmath259 , and hence the mi is bounded below by @xmath260 .",
    "both these results were generalized to the case of mimo channels .",
    "however , the mmse - based lower bound on mi is not easily extendable to the discrete case .",
    "( i exclude here the trivial construct of associating with the pdf @xmath1 of a continuous random parameter @xmath0 a discrete pd involving probabilities @xmath261 computed for finite bins , centered at regularly spaced points @xmath262 that are separated by an interval @xmath263 small compared to the scale over which @xmath1 varies significantly . )    if additional input variables other than those of interest to the estimation problem are present , in general they serve to compromise the fidelity with which the variables of interest may be estimated .",
    "the impact of such nuisance variables on estimator performance was elucidated here with formulations based separately on mi , fi , and mmse , and a number of important inequalities were derived that provide valuable insight into information and error - based metrics of performance .",
    "the mmse based description of the nuisance is particularly intriguing since it seems to predict a nearly counter - intuitive result that the presence of nuisance , can never improve performance , even when it is strongly coupled to the input and has vanishing variance , _",
    "i.e. , _ independent of its statistical correlations with the input .",
    "this may be a peculiarity of how mmse is defined , but surely deserves additional consideration .",
    "the author is pleased to acknowledge helpful contributions from s. narravula .",
    "funding support from the air force office of scientific research under grants fa9550 - 08 - 1 - 0151 and fa9550 - 09 - 1 - 0495 is gratefully acknowledged .",
    "99 c.  shannon ,  a mathematical theory of communication , \" bell syst .",
    "j. , * 27 * , pp .",
    "379 - 423 and 623 - 656 ( 1948 ) .",
    "t.  cover and j.  thomas , _ elements of information theory _ , wiley ( new york , 1991 ) .",
    "h.  van trees , _ detection , estimation , and modulation theory _ , wiley ( new york , 1968 ) .",
    "b. clarke and a. barron ,  information - theoretic asymptotics of bayes methods , \" ieee trans .",
    "th . , * 36 * , pp .",
    "453 - 471 ( 1990 ) .",
    "j. rissanen ,  fisher information and stochastic complexity , \" ieee trans .",
    "th . , * 42 * , pp .",
    "40 - 47 ( 1996 ) .",
    "n. brunel and j .- p .",
    "nadal ,  mutual information , fisher information , and population coding , \" neural computation , * 10 * , pp .",
    "1731 - 1757 ( 1998 ) . k. kang and h. sompolinsky ,  mutual information of population codes and distance measures in probability space , \" phys .",
    "lett . , * 86 * , pp .",
    "4958 - 4961 ( 2001 ) .",
    "e. challis , s. yarrow , and p. seris ,  fisher vs shannon information in populations of neurons , \" preprint ( 2008 ) .",
    "t. duncan ,  on the calculation of mutual information , \" siam j. appl . math . * 19 * , pp .",
    "215 - 220 ( 1970 ) .",
    "d. guo , s. shamai , and s. verdu ,  mutual information and minimum mean - square error in gaussian channels , \" ieee trans .",
    "th . * 51 * , pp .",
    "1261 - 1282 ( 2005 ) .",
    "e. mayer - wolf and m. zakai ,  some relations between mutual information and estmation error in wiener space , \" annals appl . prob .",
    "* 17 * , pp .",
    "1102 - 1116 ( 2007 ) .",
    "whenever possible , a random variable is denoted by an upper - case roman letter and the values it may take by the corresponding lower - case letter .",
    "also , for notational simplicity the same function label @xmath264 is used for the pdfs of different variables , even though the pdfs have , in general , different functional dependences on their arguments .",
    "d. chapman and h. robbins ,  minimum variance estimation without regularity assumptions , \" ann .",
    "* 22 * , pp .",
    "581 - 586 ( 1951 ) .",
    "s. kay , _ fundamentals of statistical signal processing : estimation theory _ ,",
    "prentice hall ( new jersey,1993 ) , chapters 10 and 11 .",
    "c.  rao , _ linear statistical inference and its applications _ , wiley ( new york , 1973 ) . a single - parameter , non - bayesian version of this result was proved in a. dandrea , u. mengali , and r. reggiannini ,  the modified cramr - rao bound and its application to synchronization problems , \" ieee trans .",
    "* 42 * , pp .",
    "1391 - 1399 ( 1994 ) ."
  ],
  "abstract_text": [
    "<S> i present several new relations between mutual information ( mi ) and statistical estimation error for a system that can be regarded simultaneously as a communication channel and as an estimator of an input parameter . </S>",
    "<S> i first derive a second - order result between mi and fisher information ( fi ) that is valid for sufficiently narrow priors , but arbitrary channels . </S>",
    "<S> a second relation furnishes a lower bound on the mi in terms of the minimum mean - squared error ( mmse ) on the bayesian estimation of the input parameter from the channel output , one that is valid for arbitrary channels and priors . </S>",
    "<S> the existence of such a lower bound , while extending previous work relating the mi to the fi that is valid only in the asymptotic and high - snr limits , elucidates further the fundamental connection between information and estimation theoretic measures of fidelity . </S>",
    "<S> the remaining relations i present are inequalities and correspondences among mi , fi , and mmse in the presence of nuisance parameters .    </S>",
    "<S> this work has been submitted to the ieee for possible publication . </S>",
    "<S> copyright may be transferred without notice , after which this version may no longer be accessible .    mutual information , mmse , bayesian estimation , fisher information , nuisance parameters </S>"
  ]
}