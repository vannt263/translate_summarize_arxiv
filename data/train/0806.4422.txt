{
  "article_text": [
    "the method of _ stable random projections_@xcite , as an efficient tool for computing pairwise distances in massive high - dimensional data , provides a promising mechanism to tackle some of the challenges in modern machine learning . in this paper",
    ", we provide an easy - to - implement algorithm for _ stable random projections _ which is both statistically accurate and computationally efficient .",
    "we denote a data matrix by @xmath3 , i.e. , @xmath4 data points in @xmath5 dimensions .",
    "data sets in modern applications exhibit important characteristics which impose tremendous challenges in machine learning @xcite :    * modern data sets with @xmath6 or even @xmath7 points are not uncommon in supervised learning , e.g. , in image / text classification , ranking algorithms for search engines , etc . in the unsupervised domain ( e.g. , web clustering , ads clickthroughs ,",
    "word / term associations ) , @xmath4 can be even much larger .",
    "* modern data sets are often of ultra high - dimensions ( @xmath5 ) , sometimes in the order of millions ( or even higher ) , e.g. , image , text , genome ( e.g. , snp ) , etc .",
    "for example , in image analysis , @xmath5 may be @xmath8 if using pixels as features , or @xmath9 million if using color histograms as features .",
    "* modern data sets are sometimes collected in a dynamic streaming fashion . *",
    "large - scale data are often heavy - tailed , e.g. , image and text data .",
    "some large - scale data are dense , such as image and genome data . even for data sets which are sparse , such as text",
    ", the absolute number of non - zeros may be still large .",
    "for example , if one queries  machine learning \" ( a not - too - common term ) in google.com , the total number of pagehits is about 3 million . in other words ,",
    "if one builds a term - doc matrix at web scale , although the matrix is sparse , most rows will contain large numbers ( e.g. , millions ) of non - zero entries .",
    "many learning algorithms require a similarity matrix computed from pairwise distances of the data matrix @xmath3 .",
    "examples include clustering , nearest neighbors , multidimensional scaling , and kernel svm ( support vector machines ) .",
    "the similarity matrix requires @xmath10 storage space and @xmath11 computing time .",
    "this study focuses on the @xmath0 distance ( @xmath12 ) .",
    "consider two vectors @xmath13 , @xmath14 ( e.g. , the leading two rows in @xmath15 ) , the @xmath0 distance between @xmath13 and @xmath16 is    @xmath17    note that , strictly speaking , the @xmath0 distance should be defined as @xmath18 .",
    "because the power operation @xmath19 is the same for all pairs , it often makes no difference whether we use @xmath18 or just @xmath20 ; and hence we focus on @xmath20 .    the radial basis kernel ( e.g. , for svm )",
    "is constructed from @xmath20 @xcite :    @xmath21    when @xmath22 , this is the gaussian radial basis kernel . here @xmath23 can be viewed as a _ tuning _ parameter .",
    "for example , in their histogram - based image classification project using svm , @xcite reported that @xmath24 and @xmath25 achieved good performance .",
    "for heavy - tailed data , tuning @xmath23 has the similar effect as term - weighting the original data , often a critical step in a lot of applications @xcite .    for popular kernel svm solvers including the _ sequential minimal optimization ( smo _ ) algorithm@xcite ,",
    "storing and computing kernels is the major bottleneck .",
    "three computational challenges were summarized in @xcite :    * * _ computing kernels is expensive _ * * * _ computing full kernel matrix is wasteful _ * efficient svm solvers often do not need to evaluate all pairwise kernels . * * _ kernel matrix does not fit in memory _ * storing the kernel matrix at the memory cost @xmath10 is challenging when @xmath26 , and is not realistic for @xmath27 , because @xmath28 consumes at least @xmath29 gbs memory .",
    "a popular strategy in large - scale learning is to evaluate distances * * on the fly**@xcite .",
    "that is , instead of loading the similarity matrix in memory at the cost of @xmath10 , one can load the original data matrix at the cost of @xmath30 and recompute pairwise distances on - demand .",
    "this strategy is apparently problematic when @xmath5 is not too small . for high - dimensional data , either loading the data matrix in memory is unrealistic or computing distances on - demand becomes too expensive .",
    "those challenges are not unique to kernel svm ; they are general issues in distanced - based learning algorithms .",
    "the method of _ stable random projections _ provides a promising scheme by reducing the dimension @xmath5 to a small @xmath31 ( e.g. , @xmath32 ) , to facilitate compact data storage and efficient distance computations .",
    "the basic procedure of _ stable random projections _ is to multiply @xmath3 by a random matrix @xmath33 ( @xmath34 ) , which is generated by sampling each entry @xmath35 i.i.d . from a symmetric stable distribution @xmath36 .",
    "the resultant matrix @xmath37 is much smaller than @xmath15 and hence it may fit in memory .",
    "suppose a stable random variable @xmath38 , where @xmath39 is the scale parameter .",
    "then its characteristic function ( fourier transform of the density function ) is    @xmath40    which does not have a closed - form inverse except for @xmath41 ( normal ) or @xmath42 ( cauchy ) .",
    "note that when @xmath41 , @xmath39 corresponds to `` @xmath43 '' ( not `` @xmath44 '' ) in a normal .",
    "corresponding to the leading two rows in @xmath15 , @xmath13 , @xmath14 , the leading two rows in @xmath45 are @xmath46 , @xmath47 .",
    "the entries of the difference ,    @xmath48    for @xmath49 to @xmath31 , are i.i.d .",
    "samples from a stable distribution with the scale parameter being the @xmath0 distance @xmath20 , due to properties of fourier transforms .",
    "for example , when @xmath41 , a weighted sum of i.i.d .",
    "standard normals is also normal with the scale parameter ( i.e. , variance ) being the sum of squares of all weights .",
    "once we obtain the stable samples , one can discard the original matrix @xmath15 and the remaining task is to estimate the scale parameter @xmath20 for each pair . + some applications of _ stable random projections _",
    "are summarized as follows :    * * _ computing all pairwise distances _ * the cost of computing all pairwise distances of @xmath3 , @xmath11 , is significantly reduced to @xmath50 . * * _ estimating @xmath0 distances online _",
    "* for @xmath26 , it is challenging or unrealistic to materialize all pairwise distances in @xmath15 .",
    "thus , in applications such as online learning , databases , search engines , and online recommendation systems , it is often more efficient if we store @xmath51 in the memory and estimate any distance _ on the fly _ if needed . estimating distances online is the standard strategy in large - scale kernel learning@xcite . with _ stable random projections _ , this simple strategy becomes effective in high - dimensional data .",
    "* * _ learning with dynamic streaming data _ * in reality , the data matrix may be updated overtime .",
    "in fact , with streaming data arriving at high - rate@xcite , the `` data matrix '' may be never stored and hence all operations ( such as clustering and classification ) must be conducted on the fly .",
    "the method of _ stable random projections _ provides a scheme to compute and update distances on the fly in one - pass of the data ; see relevant papers ( e.g. , @xcite ) for more details on this important and fast - developing subject . * * _ estimating entropy _ * the entropy distance @xmath52 is a useful statistic .",
    "a workshop in nips03 ( www.menem.com/~ilya/pages/nips03 ) focused on entropy estimation .",
    "a recent practical algorithm is simply using the difference between the @xmath53 and @xmath54 distances@xcite , where @xmath55 , @xmath56 , and the distances were estimated by _",
    "stable random projections_.    if one tunes the @xmath0 distances for many different @xmath23 ( e.g. , @xcite ) , then _",
    "stable random projections _ will be even more desirable as a cost - saving device .",
    "recall that the method of _ stable random projections _ boils down to a statistical estimation problem . that is , estimating the scale parameter @xmath20 from @xmath31 i.i.d .",
    "samples @xmath57 , @xmath49 to @xmath31 .",
    "we consider that a good estimator @xmath58 should have the following desirable properties :    * ( asymptotically ) unbiased and small variance . * computationally efficient . *",
    "exponential decrease of error ( tail ) probabilities .",
    "the _ arithmetic mean _",
    "estimator @xmath59 is good for @xmath60 . when @xmath61 , the task is less straightforward because ( 1 ) no explicit density of @xmath62 exists unless @xmath42 or @xmath63 ; and ( 2 ) @xmath64 only when @xmath65 .      initially reported in arxiv in 2006 , @xcite proposed the _ geometric mean _ estimator where @xmath66 is the gamma function , and the _ harmonic mean _",
    "estimator more recently , @xcite proposed the _ fractional power _",
    "estimator where    all three estimators are unbiased or asymptotically ( as @xmath67 ) unbiased .",
    "figure [ fig_efficiency ] compares their asymptotic variances in terms of the cramr - rao efficiency , which is the ratio of the smallest possible asymptotic variance over the asymptotic variance of the estimator , as @xmath67 .",
    "the _ geometric mean _ estimator , @xmath68 exhibits tail bounds in exponential forms , i.e. , the errors decrease exponentially fast :    @xmath69    the _ harmonic mean _ estimator , @xmath70 , works well for small @xmath23 , and has exponential tail bounds for @xmath71 .    the _ fractional power _",
    "estimator , @xmath72 , has smaller asymptotic variance than both the _ geometric mean _ and _ harmonic mean _ estimators .",
    "however , it does not have exponential tail bounds , due to the restriction @xmath73 in its definition . as shown in @xcite",
    ", it only has finite moments slightly higher than the @xmath74 order , when @xmath23 approaches 2 ( because @xmath75 ) , meaning that large errors may have a good chance to occur .",
    "we will demonstrate this by simulations .      in the definitions of @xmath68 , @xmath70 and @xmath72 , all three estimators require evaluating fractional powers , e.g. , @xmath76 .",
    "this operation is relatively expensive , especially if we need to conduct this tens of billions of times ( e.g. , @xmath77 ) .",
    "for example , @xcite reported that , although the radial basis kernel ( [ eqn_rbs_kernel ] ) with @xmath25 achieved good performance , it was not preferred because evaluating the square root was too expensive .",
    "we propose the _ optimal quantile _ estimator , using the @xmath78th smallest @xmath79 : @xmath80 where @xmath81 is chosen to minimize the asymptotic variance .",
    "this estimator is computationally attractive because * selecting * should be much less expensive than evaluating fractional powers .",
    "if we are interested in @xmath82 instead , then we do not even need to evaluate any fractional powers .",
    "as mentioned , in many cases using either @xmath20 or @xmath82 makes no difference and @xmath20 is often preferred because it avoids taking @xmath19 power",
    ". the radial basis kernel ( [ eqn_rbs_kernel ] ) requires @xmath20 .",
    "thus this study focuses on @xmath20 . on the other hand ,",
    "if we can estimate @xmath82 directly , for example , using ( [ eqn_d_oq ] ) without the @xmath23th power , we might as well just use @xmath82 if permitted . in case",
    "we do not need to evaluate any fractional power , our estimator will be even more computationally efficient .",
    "in addition to the computational advantages , this estimator also has good theoretical properties , in terms of both the variances and tail probabilities :    1 .",
    "figure [ fig_efficiency ] illustrates that , compared with the _ geometric mean _ estimator , its asymptotic variance is about the same when @xmath83 , and is considerably smaller when @xmath2 . compared with the _ fractional power _",
    "estimator , it has smaller asymptotic variance when @xmath84 .",
    "in fact , as will be shown by simulations , when the sample size @xmath31 is not too large , its mean square errors are considerably smaller than the _ fractional power _",
    "estimator when @xmath85 .",
    "the _ optimal quantile _",
    "estimator exhibits tail bounds in exponential forms .",
    "this theoretical contribution is practically important , for selecting the sample size @xmath31 . in learning theory ,",
    "the generalization bounds are often loose . in our case , however , the bounds are tight because the distribution is specified",
    ".    the next section will be devoted to analyzing the _ optimal quantile _ estimator .",
    "recall the goal is to estimate @xmath20 from @xmath86 , where @xmath87 , i.i.d .",
    "since the distribution belongs to the scale family , one can estimate the scale parameter from quantiles . due to symmetry , it is natural to consider the absolute values :    @xmath88    which is best understood by the fact that if @xmath89 , then @xmath90 , or more obviously , if @xmath91 , then @xmath92 . by properties of order statistics @xcite , any @xmath93-quantile will provide an asymptotically unbiased estimator .",
    "lemma [ lem_var_q ] provides the asymptotic variance of @xmath94 .",
    "[ lem_var_q ] denote @xmath95 and @xmath96 the probability density function and the cumulative density function of @xmath97 , respectively .",
    "the asymptotic variance of @xmath94 defined in ( [ eqn_quantile ] ) is @xmath98 where @xmath99 .",
    "* proof : * see appendix [ proof_lem_var_q ] . @xmath100 .",
    "we choose @xmath102 so that the asymptotic variance ( [ eqn_var_q ] ) is minimized , i.e. ,    @xmath103    the convexity of @xmath104 is important .",
    "graphically , @xmath104 is a convex function of @xmath93 , i.e. , a unique minimum exists . an algebraic proof , however , is difficult .",
    "nevertheless , we can obtain analytical solutions when @xmath42 and @xmath105 .",
    "[ lem_convexity ] when @xmath42 or @xmath105 , the function @xmath104 defined in ( [ eqn_g ] ) is a convex function of @xmath93 .",
    "when @xmath42 , the optimal @xmath106 .",
    "when @xmath107 , @xmath108 is the solution to @xmath109 .",
    "* proof : * see appendix [ proof_lem_convexity ] . @xmath100 .",
    "it is also easy to show that when @xmath41 , @xmath110 .",
    "we denote the _ optimal quantile _ estimator by @xmath111 , which is same as @xmath112 . for general @xmath23",
    ", we resort to numerical solutions , as presented in figure [ fig_opt_quantile ] .",
    "although @xmath111 ( i.e. , @xmath112 ) is asymptotically ( as @xmath67 ) unbiased , it is seriously biased for small @xmath31 .",
    "thus , it is practically important to remove the bias . the unbiased version of the _ optimal quantile _ estimator is    @xmath113    where @xmath114 is the expectation of @xmath111 at @xmath115 . for @xmath42 , @xmath63 , or @xmath116 , we can evaluate the expectations ( i.e. , integrals ) analytically or by numerical integrations . for general @xmath23 , as the probability density is not available , the task is difficult and prone to numerical instability . on the other hand ,",
    "since the monte - carlo simulation is a popular alternative for evaluating difficult integrals , a practical solution is to simulate the expectations , as presented in figure [ fig_bias ] .",
    "figure [ fig_bias ] illustrates that @xmath117 , meaning that this correction also reduces variance while removing bias ( because @xmath118 ) .",
    "for example , when @xmath119 and @xmath120 , @xmath121 , which is significant , because @xmath122 implies a @xmath123 difference in terms of variance , and even more considerable in terms of the mean square errors mse = variance + bias@xmath124 .",
    "@xmath114 can be tabulated for small @xmath31 , and absorbed into other coefficients , i.e. , this does not increase the computational cost at run time .",
    "we fix @xmath114 as reported in figure [ fig_bias ] . the simulations in section [ sec_simulations ]",
    "directly used those fixed @xmath114 values .",
    "figure [ fig_compu_ratio ] compares the computational costs of the _ geometric mean _ , the _ fractional power _ , and the _ optimal quantile _ estimators .",
    "the _ harmonic mean _",
    "estimator was not included as it costs very similarly to the _ fractional power _ estimator .",
    "we used the build - in function `` pow''in gcc for evaluating the fractional powers .",
    "we implemented a `` quick select '' algorithm , which is similar to quick sort and requires on average linear time . for simplicity ,",
    "our implementation used recursions and the middle element as pivot . also , to ensure fairness , for all estimators , coefficients which are functions of @xmath23 and/or @xmath31 were pre - computed .",
    "normalized by the computing time of @xmath68 , we observe that relative computational efficiency does not strongly depend on @xmath23 .",
    "we do observe that the ratio of computing time of @xmath68 over that of @xmath125 increases consistently with increasing @xmath31 .",
    "this is because in the definition of @xmath111 ( and hence also @xmath125 ) , it is required to evaluate the fractional power once , which contributes to the total computing time more significantly at smaller @xmath31 .",
    "figure [ fig_compu_ratio ] illustrates that , ( a ) the _ geometric mean _ estimator and the _ fractional power _",
    "estimator are similar in terms of computational efficiency ; ( b ) the _ optimal quantile _ estimator is nearly one order of magnitude more computationally efficient than the _ geometric mean _ and _ fractional power _ estimators .",
    "because we implemented a `` nave '' version of `` quick select '' using recursions and simple pivoting , the actual improvement may be more significant .",
    "also , if applications require only @xmath82 , then no fractional power operations are needed for @xmath125 and the improvement will be even more considerable .",
    "error ( tail ) bounds are essential for determining @xmath31 .",
    "the variance alone is not sufficient for that purpose .",
    "if an estimator of @xmath39 , say @xmath126 , is normally distributed , @xmath127 , the variance suffices for choosing @xmath31 because its error ( tail ) probability @xmath128 is determined by @xmath129 . in general",
    ", a reasonable estimator will be asymptotically normal , for small enough @xmath130 and large enough @xmath31 . for a finite @xmath31 and a fixed @xmath130 , however , the normal approximation may be ( very ) poor .",
    "this is especially true for the _ fractional power _",
    "estimator , @xmath72 .",
    "thus , for a good motivation , lemma [ lem_bounds ] provides the error ( tail ) probability bounds of @xmath94 for any @xmath93 , not just the optimal quantile @xmath78 .",
    "[ lem_bounds ] denote @xmath131 and its probability density function by @xmath132 and cumulative function by @xmath133 .",
    "given @xmath87 , i.i.d .",
    ", @xmath49 to @xmath31 . using @xmath94 in ( [ eqn_quantile ] ) , then    @xmath134    @xmath135    as @xmath136 * proof : *  see appendix [ proof_lem_bounds ] . @xmath100    the limit in ( [ eqn_g_rl_limit ] ) as @xmath137 is precisely twice the asymptotic variance factor of @xmath94 in ( [ eqn_var_q ] ) , consistent with the normality approximation mentioned previously .",
    "this explains why we express the constants as @xmath138 .",
    "( [ eqn_g_rl_limit ] ) also indicates that the tail bounds achieve the `` optimal rate '' for this estimator , in the language of large deviation theory .    by the bonferroni bound ,",
    "it is easy to determine the sample size @xmath31 @xmath139    [ lem_jl_quantile ] using @xmath94 with @xmath140 , any pairwise @xmath0 distance among @xmath4 points can be approximated within a @xmath141 factor with probability @xmath142 .",
    "it suffices to let @xmath143 , where @xmath144 , @xmath145 are defined in lemma [ lem_bounds ] .",
    "the bonferroni bound can be unnecessarily conservative .",
    "it is often reasonable to replace @xmath146 by @xmath147 , meaning that except for a @xmath148 fraction of pairs , any distance can be approximated within a @xmath141 factor with probability @xmath149 .",
    "figure [ fig_bounds ] plots the error bound constants for @xmath150 , for both the recommended _ optimal quantile _",
    "estimator @xmath111 and the baseline _ sample median _",
    "estimator @xmath151 .",
    "although we choose @xmath111 based on the asymptotic variance , it turns out @xmath111 also exhibits ( much ) better tail behaviors ( i.e. , smaller constants ) than @xmath151 , at least in the range of @xmath150 .    consider @xmath152 ( recall we suggest replacing @xmath153 by @xmath154 ) , with @xmath155 , @xmath156 , and @xmath157 .",
    "because @xmath158 around @xmath159 , we obtain @xmath160 , which is still a relatively large number ( although the original dimension @xmath5 might be @xmath161 ) .",
    "if we choose @xmath162 , then approximately @xmath163 .",
    "it is possible @xmath164 might be still conservative , for three reasons : ( a ) the tail bounds , although  sharp , \" are still upper bounds ; ( b ) using @xmath165 is conservative because @xmath166 is usually much smaller than @xmath167 ; ( c ) this type of tail bounds is based on relative error , which may be stringent for small ( @xmath168 ) distances .",
    "in fact , some earlier studies on _ normal random projections _ ( i.e. , @xmath60 ) @xcite empirically demonstrated that @xmath169 appeared sufficient .",
    "we resort to simulations for comparing the finite sample variances of various estimators and assessing the more precise error ( tail ) probabilities .",
    "one advantage of _ stable random projections _ is that we know the ( manually generated ) distributions and the only source of errors is from the random number generations .",
    "thus , we can simply rely on simulations to evaluate the estimators without using real data .",
    "in fact , after projections , the projected data follow exactly the stable distribution , regardless of the original real data distribution .    without loss of generality ,",
    "we simulate samples from @xmath170 and estimate the scale parameter ( i.e. , 1 ) from the samples . repeating the procedure @xmath171 times , we can reliably evaluate the mean square errors ( mse ) and tail probabilities .      as illustrated in figure [ fig_simu_mse ] , in terms of the mse , the _ optimal quantile _",
    "estimator @xmath125 outperforms both the _ geometric mean _ and _ fractional power _",
    "estimators when @xmath85 and @xmath172 .",
    "the _ fractional power _ estimator does not appear to be very suitable for @xmath85 , especially for @xmath23 close to 2 , even when the sample size @xmath31 is not too small ( e.g. , @xmath32 ) . for @xmath83 , however , the _ fractional power _",
    "estimator has good performance in terms of mse , even for small @xmath31 .",
    "figure [ fig_simu_tail ] presents the simulated right tail probabilities , @xmath173 , illustrating that when @xmath2 , the _ fractional power _ estimator can exhibit very bad tail behaviors . for @xmath174 ,",
    "the _ fractional power _ estimator demonstrates good performance at least for the probability range in the simulations .",
    "thus , figure [ fig_simu_tail ] demonstrates that the _ optimal quantile _ estimator consistently outperforms the _ fractional power _ and the _ geometric mean _",
    "estimators when @xmath85 .",
    "there have been many studies of _ normal random projections _ in machine learning , for dimension reduction in the @xmath175 norm , e.g. , @xcite , highlighted by the johnson - lindenstrauss ( jl ) lemma @xcite , which says @xmath176 suffices when using normal ( or normal - like , e.g. , @xcite ) projection methods .",
    "the method of _ stable random projections _ is applicable for computing the @xmath0 distances ( @xmath1 ) , not just for @xmath175 .",
    "* lemma 1 , lemma 2 , theorem 3 ) suggested the _ median _",
    "( i.e. , @xmath177 quantile ) estimator for @xmath178 and argued that the sample complexity bound should be @xmath179 ( @xmath180 in their study ) .",
    "their bound was not provided in an explicit form and required an `` @xmath130 is small enough '' argument . for @xmath181 , (",
    "* lemma 4 ) only provided a conceptual algorithm , which `` is not uniform . '' in this study , we prove the bounds for any @xmath93-quantile and any @xmath12 ( not just @xmath182 ) , in explicit exponential forms , with no unknown constants and no restriction that `` @xmath130 is small enough . ''",
    "the quantile estimator for stable distributions was proposed in statistics quite some time ago , e.g. , @xcite .",
    "@xcite mainly focused on @xmath183 and recommended using @xmath184 quantiles ( mainly for the sake of smaller bias ) .",
    "@xcite focused on @xmath185 and recommended @xmath186 quantiles .",
    "this study considers all @xmath1 and recommends @xmath93 based on the minimum asymptotic variance . because the bias can be easily removed ( at least in the practical sense ) , it appears not necessary to use other quantiles only for the sake of smaller bias .",
    "tail bounds , which are useful for choosing @xmath93 and @xmath31 based on confidence intervals , were not available in @xcite .",
    "finally , one might ask if there might be better estimators . for @xmath42 , @xcite proposed using a linear combination of quantiles ( with carefully chosen coefficients ) to obtain an asymptotically optimal estimator for the cauchy scale parameter . while it is possible to extend their result to general @xmath187 ( requiring some non - trivial work ) , whether or not it will be practically better than the _ optimal quantile _ estimator is unclear because the extreme quantiles severely affect the tail probabilities and finite - sample variances and hence some kind of truncation ( i.e. , discarding some samples at extreme quantiles ) is necessary . also , exponential tail bounds of the linear combination of quantiles may not exist or may not be feasible to derive .",
    "in addition , the _ optimal quantile _ estimator is computationally more efficient .",
    "many machine learning algorithms operate on the training data only through pairwise distances .",
    "computing , storing , updating and retrieving the `` matrix '' of pairwise distances is challenging in applications involving massive , high - dimensional , and possibly streaming , data .",
    "for example , the pairwise distance matrix can not fit in memory when the number of observations exceeds @xmath161 ( or even @xmath188 ) .",
    "the method of _ stable random projections _ provides an efficient mechanism for computing pairwise distances using low memory , by transforming the original high - dimensional data into _ sketches _ , i.e. , a small number of samples from @xmath23-stable distributions , which are much easier to store and retrieve .",
    "this method provides a uniform scheme for computing the @xmath0 pairwise distances for all @xmath12 . choosing an appropriate @xmath23 is often critical to the performance of learning algorithms . in principle",
    ", we can tune algorithms for many @xmath0 distances ; and _ stable random projections _ can provide an efficient tool .    to recover the original distances ,",
    "we face an estimation task . compared with previous estimators based on the _ geometric mean _ , _ the harmonic mean _ , or the _ fractional power _",
    ", the proposed _ optimal quantile _",
    "estimator exhibits two advantages .",
    "firstly , the _ optimal quantile _ estimator is nearly one order of magnitude more efficient than other estimators ( e.g. , reducing the training time from one week to one day ) .",
    "secondly , the _ optimal quantile _ estimator is considerably more accurate when @xmath85 , in terms of both the variances and error ( tail ) probabilities .",
    "note that @xmath189 corresponds to a convex norm ( satisfying the triangle inequality ) , which might be another motivation for using @xmath0 distances with @xmath189 .",
    "one theoretical contribution is the explicit tail bounds for general quantile estimators and consequently the sample complexity bound @xmath176 .",
    "those bounds may guide practitioners in choosing @xmath31 , the number of projections .",
    "the ( practically useful ) bounds are expressed in terms of the probability functions and hence they might be not as convenient for further theoretical analysis . also , we should mention that the bounds do not recover the optimal bound of the _ arithmetic mean _ estimator when @xmath41 , because the _ arithmetic mean _ estimator is statistically optimal at @xmath60 but the _ optimal quantile _ estimator is not .    while we believe that applying _",
    "stable random projections _ in machine learning has become straightforward , there are interesting theoretical issues for future research .",
    "for example , how theoretical properties of learning algorithms may be affected if the approximated ( instead of exact ) @xmath0 distances are used ?",
    "denote @xmath95 and @xmath96 the probability density function and the cumulative density function of @xmath97 , respectively .",
    "similarly we use @xmath190 and @xmath191 for @xmath192 . due to symmetry , the following relations hold @xmath193    let @xmath194 and @xmath195 .",
    "then , following known statistical results , e.g. , ( * ? ? ?",
    "* theorem 9.2 ) , the asymptotic variance of @xmath196 should be @xmath197 by  delta method , \"",
    "i.e. , @xmath198 , @xmath199      first , consider @xmath42 . in this case , @xmath200 it suffices to study @xmath201 .",
    "@xmath202 because @xmath203 for @xmath204 , it is easy to see that @xmath205 , and @xmath206 . thus , @xmath207 , i.e. , @xmath208 is convex and so is @xmath209 .",
    "since @xmath210 , we know @xmath106 .",
    "+ next we consider @xmath105 , using a fact @xcite that as @xmath211 , @xmath212 converges to @xmath213 , where @xmath214 stands for an exponential distribution with mean 1 .",
    "denote @xmath215 and @xmath216 .",
    "the sample quantile estimator becomes @xmath217 in this case , it is straightforward to show that @xmath218 is a convex function of @xmath93 and the minimum is attained by solving @xmath219 , i.e. , @xmath220 .      given @xmath31 i.i.d .",
    "samples , @xmath87 , @xmath49 to @xmath31 .",
    "let @xmath221 , @xmath49 to @xmath31 .",
    "denote by @xmath222 the cumulative density of @xmath223 , and by @xmath224 the empirical cumulative density of @xmath223 , @xmath49 to @xmath31 .",
    "it is the basic fact@xcite about order statistics that @xmath225 follows a binomial , i.e. , @xmath226 . for simplicity ,",
    "we replace @xmath227 by @xmath228 , @xmath229 by @xmath230 , and @xmath20 by @xmath39 , in this proof .",
    "consider the general quantile estimator @xmath94 defined in ( [ eqn_quantile ] ) .",
    "for @xmath235 , ( again , denote @xmath236 ) , @xmath237 where @xmath238 and @xmath239 .",
    "thus @xmath240^k%\\\\\\notag & % = & \\exp\\left ( % -k\\left(-(1-q)\\log\\left(1-f\\left ( \\left((1+\\epsilon)\\right)^{1/\\alpha}w;1\\right)\\right ) - q \\log \\left(f\\left ( \\left((1+\\epsilon)\\right)^{1/\\alpha}w;1\\right)\\right ) + ( 1-q)\\log ( 1-q ) + q\\log(q)\\right)\\right)\\\\\\notag = \\exp\\left(-k\\frac{\\epsilon^2}{g_{r , q}}\\right).\\end{aligned}\\ ] ] where for @xmath241 , @xmath242 where @xmath243 and @xmath244 .",
    "thus , @xmath245^k%\\\\\\notag & = \\exp\\left(-k\\frac{\\epsilon^2}{g_{l , q}}\\right),\\end{aligned}\\ ] ] where                                              chernoff , h. , gastwirth , j.l . , johns , m.v .",
    ": asymptotic distribution of linear combinations of functions of order statistics with applications to estimation .",
    "the annals of mathematical statistics * 38*(1 ) ( 1967 ) 5272"
  ],
  "abstract_text": [
    "<S> the method of _ stable random projections _ is a tool for efficiently computing the @xmath0 distances using low memory , where @xmath1 is a tuning parameter . </S>",
    "<S> the method boils down to a statistical estimation task and various estimators have been proposed , based on the _ geometric mean _ , the _ harmonic mean _ , and the _ fractional power _ etc .    </S>",
    "<S> this study proposes the * _ optimal quantile _ * estimator , whose main operation is * _ selecting _ * , which is considerably less expensive than taking fractional power , the main operation in previous estimators . </S>",
    "<S> our experiments report that the _ optimal quantile _ estimator is nearly one order of magnitude more computationally efficient than previous estimators . for large - scale learning tasks </S>",
    "<S> in which storing and computing pairwise distances is a serious bottleneck , this estimator should be desirable .    </S>",
    "<S> in addition to its computational advantages , the _ optimal quantile _ estimator exhibits nice theoretical properties . </S>",
    "<S> it is more accurate than previous estimators when @xmath2 . </S>",
    "<S> we derive its theoretical error bounds and establish the explicit ( i.e. , no hidden constants ) sample complexity bound . </S>"
  ]
}