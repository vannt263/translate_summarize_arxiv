{
  "article_text": [
    "random forests are an ensemble learning method for classification and regression that constructs a number of randomized decision trees during the training phase and predicts by averaging the results .",
    "since its publication in the seminal paper of @xcite , the procedure has become a major data analysis tool , that performs well in practice in comparison with many standard methods .",
    "what has greatly contributed to the popularity of forests is the fact that they can be applied to a wide range of prediction problems and have few parameters to tune . aside from being simple to use ,",
    "the method is generally recognized for its accuracy and its ability to deal with small sample sizes , high - dimensional feature spaces and complex data structures .",
    "the random forest methodology has been successfully involved in many practical problems , including air quality prediction ( winning code of the emc data science global hackathon in 2012 , see ) , chemoinformatics [ @xcite ] , ecology [ @xcite ] , 3d object recognition [ @xcite ] and bioinformatics [ @xcite ] , just to name a few .",
    "in addition , many variations on the original algorithm have been proposed to improve the calculation time while maintaining good prediction accuracy ; see , for example , @xcite .",
    "breiman s forests have also been extended to quantile estimation [ @xcite ] , survival analysis [ @xcite ] and ranking prediction [ @xcite ] .    on the theoretical side , the story is less conclusive , and regardless of their extensive use in practical settings , little is known about the mathematical properties of random forests . to date , most studies have concentrated on isolated parts or simplified versions of the procedure .",
    "the most celebrated theoretical result is that of @xcite , which offers an upper bound on the generalization error of forests in terms of correlation and strength of the individual trees .",
    "this was followed by a technical note [ @xcite ] that focuses on a stylized version of the original algorithm .",
    "a critical step was subsequently taken by @xcite , who established lower bounds for nonadaptive forests ( i.e. , independent of the training set ) .",
    "they also highlighted an interesting connection between random forests and a particular class of nearest neighbor predictors that was further worked out by @xcite . in recent years",
    ", various theoretical studies [ e.g. , @xcite ] have been performed , analyzing consistency of simplified models , and moving ever closer to practice .",
    "recent attempts toward narrowing the gap between theory and practice are by @xcite , who proves the first consistency result for online random forests , and by @xcite and @xcite who study the asymptotic sampling distribution of forests .",
    "the difficulty in properly analyzing random forests can be explained by the black - box nature of the procedure , which is actually a subtle combination of different components . among the forest essential ingredients , both bagging [ @xcite ] and the classification and regression trees ( cart)-split criterion [ @xcite ]",
    "play a critical role . bagging ( a contraction of bootstrap - aggregating )",
    "is a general aggregation scheme which proceeds by generating subsamples from the original data set , constructing a predictor from each resample and deciding by averaging .",
    "it is one of the most effective computationally intensive procedures to improve on unstable estimates , especially for large , high - dimensional data sets where finding a good model in one step is impossible because of the complexity and scale of the problem [ @xcite ] .",
    "the cart - split selection originated from the most influential cart algorithm of @xcite , and is used in the construction of the individual trees to choose the best cuts perpendicular to the axes . at each node of each tree ,",
    "the best cut is selected by optimizing the cart - split criterion , based on the notion of gini impurity ( classification ) and prediction squared error ( regression ) .    yet , while bagging and the cart - splitting scheme play a key role in the random forest mechanism , both are difficult to analyze , thereby explaining why theoretical studies have , thus far , considered simplified versions of the original procedure .",
    "this is often done by simply ignoring the bagging step and by replacing the cart - split selection with a more elementary cut protocol .",
    "besides , in breiman s forests , each leaf ( i.e. , a terminal node ) of the individual trees contains a fixed pre - specified number of observations ( this parameter , called ` nodesize ` in the r package ` randomforests ` , is usually chosen between @xmath0 and @xmath1 ) .",
    "there is also an extra parameter in the algorithm which allows one to control the total number of leaves ( this parameter is called ` maxnode ` in the r package and has , by default , no effect on the procedure ) .",
    "the combination of these various components makes the algorithm difficult to analyze with rigorous mathematics . as a matter of fact",
    ", most authors focus on simplified , data - independent procedures , thus creating a gap between theory and practice .",
    "motivated by the above discussion , we study in the present paper some asymptotic properties of breiman s ( @xcite )",
    "algorithm in the context of additive regression models .",
    "we prove the @xmath2 consistency of random forests , which gives a first basic theoretical guarantee of efficiency for this algorithm . to our knowledge , this is the first consistency result for breiman s ( @xcite ) original procedure .",
    "our approach rests upon a detailed analysis of the behavior of the cells generated by cart - split selection as the sample size grows .",
    "it turns out that a good control of the regression function variation inside each cell , together with a proper choice of the total number of leaves ( theorem  [ theoremconsistencysemidevelopedbrf ] ) or a proper choice of the subsampling rate ( theorem  [ theoremconsistanceforetbreiman ] ) are sufficient to ensure the forest consistency in a @xmath2 sense .",
    "also , our analysis shows that random forests can adapt to a sparse framework , when the ambient dimension @xmath3 is large ( independent of @xmath4 ) , but only a smaller number of coordinates carry out information .",
    "the paper is organized as follows . in section  [ sec2 ]",
    ", we introduce some notation and describe the random forest method .",
    "the main asymptotic results are presented in section  [ sec3 ] and further discussed in section  [ sec4 ] .",
    "section  [ sec5 ] is devoted to the main proofs , and technical results are gathered in the supplemental article [ @xcite ] .",
    "the general framework is @xmath2 regression estimation , in which an input random vector @xmath5^p$ ] is observed , and the goal is to predict the square integrable random response @xmath6 by estimating the regression function @xmath7 $ ] . to this end ,",
    "we assume given a training sample @xmath8 of @xmath9^p\\times \\mathbb{r}$]-valued independent random variables distributed as the independent prototype pair @xmath10 .",
    "the objective is to use the data set @xmath11 to construct an estimate @xmath12^p \\to \\mathbb r$ ] of the function @xmath13 . in this respect",
    ", we say that a regression function estimate @xmath14 is @xmath2 consistent if @xmath15 ^ 2 \\to0 $ ] as @xmath16 ( where the expectation is over @xmath17 and @xmath18 ) .",
    "a random forest is a predictor consisting of a collection of @xmath19 randomized regression trees . for the @xmath20th tree in the family ,",
    "the predicted value at the query point @xmath21 is denoted by @xmath22 , where @xmath23 are independent random variables , distributed as a generic random variable @xmath24 and independent of @xmath18 . in practice , this variable is used to resample the training set prior to the growing of individual trees and to select the successive candidate directions for splitting .",
    "the trees are combined to form the ( finite ) forest estimate @xmath25 since in practice we can choose @xmath19 as large as possible , we study in this paper the property of the infinite forest estimate obtained as the limit of ( [ finiteforest ] ) when the number of trees @xmath19 grows to infinity as follows : @xmath26,\\end{aligned}\\ ] ] where @xmath27 denotes expectation with respect to the random parameter @xmath24 , conditional on @xmath11 .",
    "this operation is justified by the law of large numbers , which asserts that , almost surely , conditional on @xmath18 , @xmath28 see , for example , @xcite for details . in the sequel , to lighten notation , we will simply write @xmath29 instead of @xmath30 ; @xmath31 .",
    "compute the random forest estimate @xmath32 at the query point @xmath21 according to ( [ finiteforest ] ) .    in breiman s",
    "( @xcite ) original forests , each node of a single tree is associated with a hyper - rectangular cell . at each step of the tree construction ,",
    "the collection of cells forms a partition of @xmath9^p$ ] .",
    "the root of the tree is @xmath9^p$ ] itself , and each tree is grown as explained in algorithm  [ al1 ] .",
    "this algorithm has three parameters :    1 .",
    "@xmath33 , which is the number of pre - selected directions for splitting ; 2 .",
    "@xmath34 , which is the number of sampled data points in each tree ; 3 .",
    "@xmath35 , which is the number of leaves in each tree .    by default , in the original procedure , the parameter @xmath36 is set to @xmath37 , @xmath38 is set to  @xmath4 ( resampling is done with replacement ) and @xmath39 .",
    "however , in our approach , resampling is done without replacement and the parameters @xmath38 , and @xmath40 can be different from their default values",
    ".    in words , the algorithm works by growing @xmath19 different trees as follows . for each tree , @xmath38 data points are drawn at random without replacement from the original data set ; then , at each cell of every tree , a split is chosen by maximizing the cart - criterion ( see below ) ; finally , the construction of every tree is stopped when the total number of cells in the tree reaches the value @xmath40 ( therefore , each cell contains exactly one point in the case @xmath41 ) .",
    "we note that the resampling step in algorithm  [ al1 ] ( line @xmath42 ) is done by choosing @xmath38 out of @xmath4 points ( with @xmath43 ) without replacement .",
    "this is slightly different from the original algorithm , where resampling is done by bootstrapping , that is , by choosing @xmath4 out of @xmath4 data points with replacement .    selecting the points `` without replacement '' instead of `` with replacement '' is harmless  in fact , it is just a means to avoid mathematical difficulties induced by the bootstrap ; see , for example , @xcite .    on the other hand ,",
    "letting the parameters @xmath38 and @xmath40 depend upon @xmath4 offers several degrees of freedom which opens the route for establishing consistency of the method . to be precise , we will study in section  [ sec3 ] the random forest algorithm in two different regimes .",
    "the first regime is when @xmath44 , which means that trees are not fully developed . in this case",
    ", a proper tuning of @xmath40 ensures the forest s consistency ( theorem  [ theoremconsistencysemidevelopedbrf ] ) .",
    "the second regime occurs when @xmath41 , that is , when trees are fully grown . in this case ,",
    "consistency results from an appropriate choice of the subsample rate @xmath45 ( theorem  [ theoremconsistanceforetbreiman ] ) .",
    "so far , we have not made explicit the cart - split criterion used in algorithm  [ al1 ] . to properly define it ,",
    "we let @xmath46 be a generic cell and @xmath47 be the number of data points falling in @xmath46 .",
    "a cut in @xmath46 is a pair @xmath48 , where @xmath20 is a dimension in @xmath49 and @xmath50 is the position of the cut along the @xmath20th coordinate , within the limits of @xmath46 .",
    "we let @xmath51 be the set of all such possible cuts in @xmath46 . then , with the notation @xmath52 , for any @xmath53 , the cart - split criterion [ @xcite ] takes the form @xmath54 \\label{definitionempiricalcartcriterion } \\\\[-8pt ] \\nonumber & & { } - \\frac{1}{n_n(a ) } \\sum_{i=1}^n ( y_i - \\bar{y}_{a_{l } } \\mathbh{1}_{\\mathbf{x}_i^{(j ) } < z } - \\bar{y}_{a_{r } } \\mathbh{1}_{\\mathbf { x}_i^{(j ) } \\geq z})^2 \\mathbh{1}_{\\mathbf{x}_i \\in a},\\end{aligned}\\ ] ] where @xmath55 , @xmath56 , and @xmath57 ( resp . ,",
    "@xmath58 , @xmath59 ) is the average of the @xmath60 s belonging to @xmath46 ( resp . ,",
    "@xmath61 , @xmath62 ) , with the convention @xmath63 .",
    "at each cell @xmath46 , the best cut @xmath64 is finally selected by maximizing @xmath65 over @xmath66 and @xmath51 , that is , @xmath67 to remove ties in the argmax , the best cut is always performed along the best cut direction @xmath68 , at the middle of two consecutive data points .",
    "we consider an additive regression model satisfying the following properties :    _ the response @xmath69 follows @xmath70 where @xmath71 is uniformly distributed over @xmath9^p$ ] , @xmath72 is an independent centered gaussian noise with finite variance @xmath73 and each component @xmath74 is continuous .",
    "_    additive regression models , which extend linear models , were popularized by @xcite and @xcite .",
    "these models , which decompose the regression function as a sum of univariate functions , are flexible and easy to interpret .",
    "they are acknowledged for providing a good trade - off between model complexity and calculation time , and accordingly , have been extensively studied for the last thirty years .",
    "additive models also play an important role in the context of high - dimensional data analysis and sparse modeling , where they are successfully involved in procedures such as the lasso and various aggregation schemes ; for an overview , see , for example , @xcite .",
    "although random forests fall into the family of nonparametric procedures , it turns out that the analysis of their properties is facilitated within the framework of additive models .",
    "our first result assumes that the total number of leaves @xmath40 in each tree tends to infinity more slowly than the number of selected data points @xmath38 .",
    "[ theoremconsistencysemidevelopedbrf ] assume that is satisfied .",
    "then , provided @xmath75 , @xmath76 and @xmath77 , random forests are consistent , that is , @xmath78 ^ 2 = 0.\\end{aligned}\\ ] ]    it is noteworthy that theorem  [ theoremconsistencysemidevelopedbrf ] still holds with @xmath79 . in this case , the subsampling step plays no role in the consistency of the method . indeed , controlling the depth of the trees via the parameter @xmath40 is sufficient to bound the forest error .",
    "we note in passing that an easy adaptation of theorem  [ theoremconsistencysemidevelopedbrf ] shows that the cart algorithm is consistent under the same assumptions .    the term @xmath80 originates from the gaussian noise and allows us to control the noise tail . in the easier situation where the gaussian noise is replaced by a bounded random variable , it is easy to see that the term @xmath80 turns into @xmath81 , a term which accounts for the complexity of the tree partition .",
    "let us now examine the forest behavior in the second regime , where @xmath41 ( i.e. , trees are fully grown ) , and as before , subsampling is done at the rate @xmath45 .",
    "the analysis of this regime turns out to be more complicated , and rests upon assumption ( h2 ) below .",
    "we denote by @xmath82 the indicator that @xmath83 falls into the same cell as @xmath17 in the random tree designed with @xmath18 and the random parameter @xmath24 .",
    "similarly , we let @xmath84 , where @xmath85 is an independent copy of @xmath24 . accordingly , we define @xmath86\\ ] ] and @xmath87.\\ ] ] finally , for any random variables @xmath88 , @xmath89 , @xmath90 , we denote by @xmath91 , @xmath92 the conditional correlation coefficient ( whenever it exists ) .",
    "_ let @xmath93 .",
    "then one of the following two conditions holds _    _ one has _",
    "@xmath94 ^ 2 = 0.\\end{aligned}\\ ] ]    _ there exist a constant @xmath95 and a sequence @xmath96 such that , almost surely _ , @xmath97 } \\leq\\gamma_n\\ ] ] _ and _ @xmath98 shows that ( h2.1 ) may be simply replaced by @xmath99 ^ 2 = 0.\\end{aligned}\\ ] ] therefore , ( h2.1 ) means that the influence of two @xmath69-values on the probability of connection of two couples of random points tends to zero as @xmath100 .    as for assumption ( h2.2 )",
    ", it holds whenever the correlation between the noise and the probability of connection of two couples of random points vanishes quickly enough , as @xmath101 . note that",
    ", in the simple case where the partition is independent of the @xmath60 s , the correlations in ( h2.2 ) are zero , so that ( h2 ) is trivially satisfied .",
    "this is also verified in the noiseless case , that is , when @xmath102 . however , in the most general context , the partitions strongly depend on the whole sample @xmath18 , and unfortunately , we do not know whether or not ( h2 ) is satisfied .    [ theoremconsistanceforetbreiman ]",
    "assume that and are satisfied , and let @xmath41",
    ". then , provided @xmath103 , @xmath104 and @xmath105 , random forests are consistent , that is , @xmath106 ^ 2 = 0.\\ ] ]    to our knowledge , apart from the fact that bootstrapping is replaced by subsampling , theorems  [ theoremconsistencysemidevelopedbrf ] and [ theoremconsistanceforetbreiman ] are the first consistency results for breiman s ( @xcite ) forests .",
    "indeed , most models studied so far are designed independently of @xmath107 and are , consequently , an unrealistic representation of the true procedure .",
    "in fact , understanding breiman s random forest behavior deserves a more involved mathematical treatment .",
    "section  [ sec4 ] below offers a thorough description of the various mathematical forces in action .",
    "our study also sheds some interesting light on the behavior of forests when the ambient dimension @xmath3 is large but the true underlying dimension of the model is small . to see how , assume that the additive model ( h1 ) satisfies a sparsity constraint of the form @xmath108 where @xmath109 represents the true , but unknown , dimension of the model .",
    "thus , among the @xmath3 original features , it is assumed that only the first ( without loss of generality ) @xmath110 variables are informative .",
    "put differently , @xmath69 is assumed to be independent of the last @xmath111 variables . in this dimension reduction context , the ambient dimension @xmath3 can be very large , but we believe that the representation is sparse , that is , that few components of @xmath13 are nonzero . as such , the value @xmath110 characterizes the sparsity of the model : the smaller @xmath110 , the sparser @xmath13 .",
    "proposition  [ theoremrelevantvariables ] below shows that random forests nicely adapt to the sparsity setting by asymptotically performing , with high probability , splits along the @xmath110 informative variables .",
    "in this proposition , we set @xmath112 and , for all @xmath113 , we denote by @xmath114 the first @xmath113 cut directions used to construct the cell containing @xmath17 , with the convention that @xmath115 if the cell has been cut strictly less than @xmath116 times .",
    "[ theoremrelevantvariables ] assume that is satisfied .",
    "let @xmath117 and @xmath118 .",
    "assume that there is no interval @xmath119 $ ] and no @xmath120 such that @xmath74 is constant on @xmath119 $ ] .",
    "then , with probability @xmath121 , for all @xmath4 large enough , we have , for all @xmath122 , @xmath123    this proposition provides an interesting perspective on why random forests are still able to do a good job in a sparse framework . since the algorithm selects splits mostly along informative variables , everything happens as if data were projected onto the vector space generated by the @xmath110 informative variables .",
    "therefore , forests are likely to only depend upon these @xmath110 variables , which supports the fact that they have good performance in sparse framework .",
    "it remains that a substantial research effort is still needed to understand the properties of forests in a high - dimensional setting , when @xmath124 may be substantially larger than the sample size .",
    "unfortunately , our analysis does not carry over to this context .",
    "in particular , if high - dimensionality is modeled by letting @xmath125 , then assumption ( h2.1 ) may be too restrictive since the term @xmath126 will diverge at a fast rate .",
    "one of the main difficulties in assessing the mathematical properties of breiman s ( @xcite ) forests is that the construction process of the individual trees strongly depends on both the @xmath127 s and the @xmath60 s . for partitions that are independent of the @xmath60 s",
    ", consistency can be shown by relatively simple means via stone s ( @xcite ) theorem for local averaging estimates ; see also @xcite , chapter  6 . however , our partitions and trees depend upon the @xmath69-values in the data .",
    "this makes things complicated , but mathematically interesting too .",
    "thus , logically , the proof of theorem  [ theoremconsistanceforetbreiman ] starts with an adaptation of stone s ( @xcite ) theorem tailored for random forests , whereas the proof of theorem  [ theoremconsistencysemidevelopedbrf ] is based on consistency results of data - dependent partitions developed by @xcite .",
    "both theorems rely on proposition  [ variancewithincellforetempirique ] below , which stresses an important feature of the random forest mechanism .",
    "it states that the variation of the regression function @xmath13 within a cell of a random tree is small provided @xmath4 is large enough . to this end , we define , for any cell @xmath46 , the variation of @xmath13 within @xmath46 as @xmath128 furthermore , we denote by @xmath129 the cell of a tree built with random parameter @xmath24 that contains the point @xmath17 .",
    "[ variancewithincellforetempirique ] assume that holds .",
    "then , for all @xmath130 , there exists @xmath131 such that , for all @xmath132 , @xmath133 \\geq1 - \\rho.\\end{aligned}\\ ] ]    it should be noted that in the standard , @xmath69-independent analysis of partitioning regression function estimates , the variance is controlled by letting the diameters of the tree cells tend to zero in probability . instead of such a geometrical assumption ,",
    "proposition  [ variancewithincellforetempirique ] ensures that the variation of @xmath13 inside a cell is small , thereby forcing the approximation error of the forest to asymptotically approach zero .",
    "while proposition  [ variancewithincellforetempirique ] offers a good control of the approximation error of the forest in both regimes , a separate analysis is required for the estimation error . in regime",
    "@xmath0 ( theorem  [ theoremconsistencysemidevelopedbrf ] ) , the parameter @xmath40 allows us to control the structure of the tree .",
    "this is in line with standard tree consistency approaches ; see , for example , @xcite , chapter  20 .",
    "things are different for the second regime ( theorem  [ theoremconsistanceforetbreiman ] ) , in which individual trees are fully grown . in this case , the estimation error is controlled by forcing the subsampling rate @xmath45 to be @xmath134 , which is a more unusual requirement and deserves some remarks .    at first",
    ", we note that the @xmath135 term in theorem  [ theoremconsistanceforetbreiman ] is used to control the gaussian noise @xmath72 .",
    "thus if the noise is assumed to be a bounded random variable , then the @xmath135 term disappears , and the condition reduces to @xmath136 .",
    "the requirement @xmath105 guarantees that every single observation @xmath137 is used in the tree construction with a probability that becomes small with @xmath4 .",
    "it also implies that the query point @xmath21 is not connected to the same data point in a high proportion of trees . if not , the predicted value at @xmath21 would be influenced too much by one single pair @xmath137 , making the forest inconsistent .",
    "in fact , the proof of theorem  [ theoremconsistanceforetbreiman ] reveals that the estimation error of a forest estimate is small as soon as the maximum probability of connection between the query point and all observations is small .",
    "thus the assumption on the subsampling rate is just a convenient way to control these probabilities , by ensuring that partitions are dissimilar enough ( i.e. , by ensuring that @xmath21 is connected with many data points through the forest ) .",
    "this idea of diversity among trees was introduced by @xcite , but is generally difficult to analyze . in our approach , the subsampling is the key component for imposing tree diversity .",
    "theorem  [ theoremconsistanceforetbreiman ] comes at the price of assumption ( h2 ) , for which we do not know if it is valid in all generality .",
    "on the other hand , theorem  [ theoremconsistanceforetbreiman ] , which mimics almost perfectly the algorithm used in practice , is an important step toward understanding breiman s random forests .",
    "contrary to most previous works , theorem  [ theoremconsistanceforetbreiman ] assumes that there is only one observation per leaf of each individual tree .",
    "this implies that the single trees are eventually not consistent , since standard conditions for tree consistency require that the number of observations in the terminal nodes tends to infinity as @xmath4 grows ; see , for example , @xcite .",
    "thus the random forest algorithm aggregates rough individual tree predictors to build a provably consistent general architecture .",
    "it is also interesting to note that our results ( in particular lemma  [ consistencyempiricalcutsbestmultiplecutsspurious ] ) can not be directly extended to establish the pointwise consistency of random forests ; that is , for almost all @xmath138^d$ ] , @xmath139 ^ 2 = 0.\\end{aligned}\\ ] ] fixing @xmath138^d$ ] , the difficulty results from the fact that we do not have a control on the diameter of the cell @xmath140 , whereas , since the cells form a partition of @xmath9^d$ ] , we have a global control on their diameters .",
    "thus , as highlighted by @xcite , random forests can be inconsistent at some fixed point @xmath138^d$ ] , particularly near the edges , while being @xmath2 consistent .",
    "let us finally mention that all results can be extended to the case where @xmath72 is a heteroscedastic and sub - gaussian noise , with for all @xmath138^d$ ] , @xmath141 \\leq\\sigma'^2 $ ] , for some constant @xmath142 .",
    "all proofs can be readily extended to match this context , at the price of easy technical adaptations .",
    "for the sake of clarity , proofs of the intermediary results are gathered in the supplemental article [ @xcite ] .",
    "we start with some notation .      in the sequel , to clarify the notation , we will sometimes write @xmath143 to represent a cut @xmath48 .    recall that , for any cell @xmath46 , @xmath144 is the set of all possible cuts in @xmath46 .",
    "thus , with this notation , @xmath145^p}$ ] is just the set of all possible cuts at the root of the tree , that is , all possible choices @xmath146 with @xmath147 and @xmath148 $ ] .    more generally , for any @xmath138^p$ ] , we call @xmath149 the collection of all possible @xmath150 consecutive cuts used to build the cell containing @xmath21",
    ". such a cell is obtained after a sequence of cuts @xmath151 , where the dependency of @xmath152 upon @xmath21 is understood .",
    "accordingly , for any @xmath153 , we let @xmath154 be the cell containing @xmath21 built with the particular @xmath113-tuple of cuts @xmath155 .",
    "the proximity between two elements @xmath152 and @xmath156 in @xmath157 will be measured via @xmath158 accordingly , the distance @xmath159 between @xmath160 and any @xmath161 is @xmath162    remember that @xmath129 denotes the cell of a tree containing @xmath17 and designed with random parameter @xmath24 .",
    "similarly , @xmath163 is the same cell but where only the first @xmath113 cuts are performed ( @xmath117 is a parameter to be chosen later ) .",
    "we also denote by @xmath164 the @xmath113 cuts used to construct the cell @xmath163 .",
    "recall that , for any cell @xmath46 , the empirical criterion used to split @xmath46 in the random forest algorithm is defined in ( [ definitionempiricalcartcriterion ] ) . for any cut @xmath165",
    ", we denote the following theoretical version of @xmath166 by @xmath167 - \\mathbb{p}\\bigl [ \\mathbf{x}^{(j ) } < z | \\mathbf{x } \\in a \\bigr ] \\mathbb{v}\\bigl[y | \\mathbf{x}^{(j ) } < z , \\mathbf{x}\\in a \\bigr ] \\\\ & & { } - \\mathbb{p}\\bigl [ \\mathbf{x}^{(j ) } \\geq z | \\mathbf{x}\\in a \\bigr ] \\mathbb { v}\\bigl[y | \\mathbf{x}^{(j ) } \\geq z , \\mathbf{x}\\in a \\bigr].\\end{aligned}\\ ] ] observe that @xmath168 does not depend upon the training set and that , by the strong law of large numbers , @xmath169 almost surely as @xmath101 for all cuts @xmath170 .",
    "therefore , it is natural to define the best theoretical split @xmath171 of the cell @xmath46 as @xmath172 in view of this criterion , we define the theoretical random forest as before , but with consecutive cuts performed by optimizing @xmath173 instead of @xmath174 .",
    "we note that this new forest does depend on @xmath24 through @xmath175 , but not on the sample @xmath11 .",
    "in particular , the stopping criterion for dividing cells has to be changed in the theoretical random forest ; instead of stopping when a cell has a single training point , we impose that each tree of the theoretical forest is stopped at a fixed level @xmath176 .",
    "we also let @xmath177 be a cell of the theoretical random tree at level @xmath113 , containing @xmath17 , designed with randomness @xmath24 , and resulting from the @xmath113 theoretical cuts @xmath178 .",
    "since there can exist multiple best cuts at , at least , one node , we call @xmath179 the set of all @xmath113-tuples @xmath180 of best theoretical cuts used to build @xmath181 .",
    "we are now equipped to prove proposition  [ variancewithincellforetempirique ] . for reasons of clarity ,",
    "the proof has been divided in three steps .",
    "first , we study in lemma  [ theoremvariancetendvers0forettheorique ] the theoretical random forest .",
    "then we prove in lemma  [ consistencyempiricalcutsbestmultiplecutsspurious ] ( via lemma  [ cartconvergencecoupureempiriqueplusieurs ] ) that theoretical and empirical cuts are close to each other .",
    "proposition  [ variancewithincellforetempirique ] is finally established as a consequence of lemma  [ theoremvariancetendvers0forettheorique ] and lemma  [ consistencyempiricalcutsbestmultiplecutsspurious ] .",
    "proofs of these lemmas are to be found in the supplemental article [ @xcite ] .",
    "we first need a lemma which states that the variation of @xmath182 within the cell @xmath181 where @xmath183 falls , as measured by @xmath184 , tends to zero .",
    "[ theoremvariancetendvers0forettheorique ] assume that is satisfied .",
    "then , for all @xmath138^p$ ] , @xmath185    the next step is to show that cuts in theoretical and original forests are close to each other . to this end , for any @xmath138^p$ ] and any @xmath113-tuple of cuts @xmath186 , we define @xmath187 where @xmath188 and @xmath189 , and where we use the convention @xmath63 when @xmath190 is empty .",
    "besides , we let @xmath191^p$ ] in the previous equation .",
    "the quantity @xmath192 is nothing but the criterion to maximize in @xmath193 to find the best @xmath113th cut in the cell @xmath194 .",
    "lemma  [ cartconvergencecoupureempiriqueplusieurs ] below ensures that @xmath195 is stochastically equicontinuous , for all @xmath138^p$ ] . to this end , for all @xmath118 , and for all @xmath138^p$ ] , we denote by @xmath196 the set of all @xmath197-tuples @xmath198 such that the cell @xmath190 contains a hypercube of edge length @xmath199 . moreover , we let @xmath200 equipped with the norm @xmath201 .",
    "[ cartconvergencecoupureempiriqueplusieurs ] assume that is satisfied .",
    "fix @xmath138^p$ ] , @xmath176 , and let .",
    "then @xmath195 is stochastically equicontinuous on @xmath202 ; that is , for all @xmath203 , there exists @xmath204 such that @xmath205 \\leq \\rho.\\end{aligned}\\ ] ]    lemma  [ cartconvergencecoupureempiriqueplusieurs ] is then used in lemma  [ consistencyempiricalcutsbestmultiplecutsspurious ] to assess the distance between theoretical and empirical cuts .",
    "[ consistencyempiricalcutsbestmultiplecutsspurious ] assume that is satisfied .",
    "fix @xmath206 and @xmath207 .",
    "then there exists @xmath131 such that , for all @xmath208 , @xmath209 \\geq1 - \\rho.\\ ] ]    we are now ready to prove proposition  [ variancewithincellforetempirique ] .",
    "fix @xmath130 .",
    "since almost sure convergence implies convergence in probability , according to lemma  [ theoremvariancetendvers0forettheorique ] , there exists @xmath210 such that @xmath211 \\geq1 - \\rho.\\end{aligned}\\ ] ] by lemma  [ consistencyempiricalcutsbestmultiplecutsspurious ] , for all @xmath212 , there exists @xmath131 such that , for all @xmath208 , @xmath213 \\geq1 - \\rho.\\end{aligned}\\ ] ] since @xmath13 is uniformly continuous , we can choose @xmath214 sufficiently small such that , for all @xmath138^p$ ] , for all @xmath215 satisfying @xmath216 , we have @xmath217 thus , combining inequalities ( [ finalproof3 ] ) and ( [ finalproof4 ] ) , we obtain @xmath218 \\geq1 - \\rho.\\end{aligned}\\ ] ] using the fact that @xmath219 whenever @xmath220 , we deduce from ( [ finalproof1 ] ) and ( [ finalproof2 ] ) that , for all @xmath208 , @xmath221 \\geq1 - 2\\rho.\\end{aligned}\\ ] ] this completes the proof of proposition  [ variancewithincellforetempirique ] .",
    "we still need some additional notation . the partition obtained with the random variable @xmath24 and the data set @xmath18 is denoted by @xmath222 , which we abbreviate as @xmath223 .",
    "we let @xmath224^d \\times \\mathbb{r}\\bigr\\}\\end{aligned}\\ ] ] be the family of all achievable partitions with random parameter @xmath24 .",
    "accordingly , we let @xmath225 be the maximal number of terminal nodes among all partitions in @xmath226",
    ". given a set @xmath227^d$ ] , @xmath228 denotes the number of distinct partitions of @xmath229 induced by elements of @xmath226 , that is , the number of different partitions @xmath230 of @xmath229 , for @xmath231 .",
    "consequently , the partitioning number @xmath232 is defined by @xmath233^d \\bigr\\rbrace.\\end{aligned}\\ ] ] let @xmath234 be a positive sequence , and define the truncated operator @xmath235 by @xmath236 hence @xmath237 , @xmath238 and @xmath239 are defined unambiguously .",
    "we let @xmath240 be the set of all functions @xmath241^d \\to\\mathbb{r}$ ] piecewise constant on each cell of the partition @xmath223 .",
    "[ notice that @xmath240 depends on the whole data set . ] finally , we denote by @xmath242 the set of indices of the data points that are selected during the subsampling step .",
    "thus the tree estimate @xmath243 satisfies @xmath244    the proof of theorem  [ theoremconsistencysemidevelopedbrf ] is based on ideas developed by @xcite , and worked out in theorem @xmath245 in @xcite .",
    "this theorem , tailored for our context , is recalled below for the sake of completeness .",
    "[ theorem102gyorfi ] let @xmath14 and @xmath240 be as above .",
    "assume that :    @xmath246 ;    @xmath247 ^ 2   ] = 0 $ ] ;    for all @xmath248 , @xmath249 ^ 2 - \\mathbb{e } \\bigl[f(\\mathbf{x } ) -",
    "y_l \\bigr]^2 \\biggr| \\biggr ] = 0.\\end{aligned}\\ ] ]    then @xmath250 ^ 2 = 0.\\end{aligned}\\ ] ]    statement ( ii ) [ resp . , statement ( iii ) ] allows us to control the approximation error ( resp . , the estimation error ) of the truncated estimate .",
    "since the truncated estimate @xmath251 is piecewise constant on each cell of the partition @xmath223 , @xmath251 belongs to the set @xmath240 .",
    "thus the term in ( ii ) is the classical approximation error .",
    "we are now equipped to prove theorem  [ theoremconsistencysemidevelopedbrf ] .",
    "fix @xmath118 , and note that we just have to check statements ( i)(iii ) of theorem  [ theorem102gyorfi ] to prove that the truncated estimate of the random forest is consistent . throughout the proof , we let @xmath252 . clearly , statement ( i ) is true .",
    "to prove ( ii ) , let @xmath253 where @xmath254 is an arbitrary point picked in cell a. since , according to ( h1 ) , @xmath255 , for all @xmath4 large enough such that @xmath256 , we have @xmath257 ^ 2   & \\leq & \\mathbb{e}\\mathop{\\inf_{f \\in\\mathcal{f}_n(\\theta)}}_{\\|f\\|_{\\infty } \\leq\\|m\\|_{\\infty } } \\mathbb{e}_{\\mathbf{x } } \\bigl[f(\\mathbf{x } ) - m(\\mathbf{x } ) \\bigr]^2 \\\\ & \\leq & \\mathbb{e } \\bigl[f_{\\theta , n}(\\mathbf{x } ) - m(\\mathbf{x } ) \\bigr]^2 \\\\ & & { } \\bigl(\\mbox{since $ f_{\\theta , n } \\in\\mathcal{f}_n(\\theta)$ }",
    "\\bigr ) \\\\ &",
    "\\leq & \\mathbb{e } \\bigl[m(\\mathbf{z}_{a_{n}(\\mathbf{x } , \\theta ) } ) - m(\\mathbf{x } ) \\bigr]^2 \\\\ & \\leq & \\mathbb{e } \\bigl [ \\delta\\bigl(m , a_{n}(\\mathbf{x } , \\theta ) \\bigr ) \\bigr]^2 \\\\ & \\leq & \\xi^2 + 4\\|m \\|_{\\infty}^2 \\mathbb{p } \\bigl [ \\delta \\bigl(m , a_{n } ( \\mathbf{x } , \\theta)\\bigr ) > \\xi \\bigr].\\end{aligned}\\ ] ] thus , using proposition  [ variancewithincellforetempirique ] , we see that for all @xmath4 large enough , @xmath258 ^ 2 \\leq2\\xi^2.\\end{aligned}\\ ] ] this establishes ( ii ) .      to prove statement ( iii ) , fix @xmath248 .",
    "then , for all @xmath4 large enough such that @xmath259 , @xmath260 ^ 2 - \\mathbb{e } \\bigl[f(\\mathbf{x } ) - y_l \\bigr]^2 \\biggr{\\vert } > \\xi \\biggr ) \\\\ & & \\qquad \\leq 8 \\exp \\biggl [ \\log\\gamma_n\\bigl(\\pi_n(\\theta ) \\bigr ) + 2 m\\bigl(\\pi _",
    "n(\\theta)\\bigr ) \\log \\biggl ( \\frac{333e\\beta_n^2}{\\xi } \\biggr ) - \\frac { a_n \\xi^2}{2048\\beta_n^4 } \\biggr ] \\\\ & & \\qquad\\quad\\mbox{[according to theorem $ 9.1 $ in \\citet{regression } ] } \\\\ & & \\qquad \\leq8 \\exp \\biggl [ - \\frac{a_n}{\\beta_n^4 } \\biggl ( \\frac{\\xi ^2}{2048 } - \\frac{\\beta_n^4\\log\\gamma_n(\\pi_n)}{a_n } - \\frac{2 \\beta_n^4 m(\\pi_n)}{a_n } \\log \\biggl ( \\frac{333e\\beta_n^2}{\\xi } \\biggr ) \\biggr ) \\biggr].\\end{aligned}\\ ] ] since each tree has exactly @xmath40 terminal nodes , we have @xmath261 , and simple calculations show that @xmath262 hence @xmath263 ^ 2 - \\mathbb{e } \\bigl[f(\\mathbf{x } ) - y_l \\bigr]^2 \\biggr{\\vert } > \\xi \\biggr ) \\\\ & & \\qquad \\leq8 \\exp \\biggl ( - \\frac{a_nc_{\\xi , n}}{\\beta_n^4 } \\biggr),\\end{aligned}\\ ] ] where @xmath264 by our assumption . finally , observe that @xmath265 ^ 2 - \\mathbb{e } \\bigl[f(\\mathbf{x } ) -",
    "y_l \\bigr]^2 \\biggr{\\vert}\\leq2 ( \\beta_n + l)^2,\\end{aligned}\\ ] ] which yields , for all @xmath4 large enough , @xmath266 ^ 2 - \\mathbb{e } \\bigl[f(\\mathbf{x } ) - y_l \\bigr]^2 \\biggr| \\biggr ] \\\\ & & \\!\\!\\qquad \\leq\\xi+ 2(\\beta_n + l)^2 \\mathbb{p } \\biggl [ \\mathop{\\sup _ { f \\in \\mathcal{f}_n(\\theta)}}_{\\|f\\|_{\\infty } \\leq\\beta_n }",
    "\\biggl| \\frac { 1}{a_n } \\sum _ { i=1}^{a_n } \\bigl[f(\\mathbf{x}_i ) - y_{i , l } \\bigr]^2 - \\mathbb{e } \\bigl[f(\\mathbf{x } ) - y_l \\bigr]^2 \\biggr| > \\xi \\!\\!\\biggr ] \\\\ & & \\!\\!\\qquad \\leq\\xi+ 16(\\beta_n + l ) ^2 \\exp \\biggl ( - \\frac{a_nc_{\\xi , n}}{\\beta_n^4 } \\biggr ) \\\\ & & \\!\\!\\qquad \\leq2 \\xi.\\end{aligned}\\ ] ] thus , according to theorem  [ theorem102gyorfi ] , @xmath267 ^ 2 \\to0.\\end{aligned}\\ ] ]      it remains to show the consistency of the nontruncated random forest estimate , and the proof will be complete .",
    "for this purpose , note that , for all @xmath4 large enough , @xmath268 ^ 2 & = & \\mathbb { e } \\bigl [ \\mathbb{e}_{\\theta } \\bigl[m_n(\\mathbf{x } , \\theta)\\bigr ] - m(\\mathbf{x } ) \\bigr]^2 \\\\ & \\leq & \\mathbb{e } \\bigl [ m_n(\\mathbf{x } , \\theta ) - m(\\mathbf{x } ) \\bigr]^2 \\\\ & & { } \\mbox{(by jensen 's inequality ) } \\\\ & \\leq & \\mathbb{e } \\bigl [ m_n(\\mathbf{x } , \\theta ) - t_{\\beta _ n}m_n(\\mathbf{x } , \\theta ) \\bigr]^2 \\\\ & & { } + \\mathbb{e } \\bigl [ t_{\\beta_n}m_n(\\mathbf{x } , \\theta ) - m ( \\mathbf { x } ) \\bigr]^2 \\\\ & \\leq & \\mathbb{e } \\bigl [ \\bigl [ m_n(\\mathbf{x } , \\theta ) - t_{\\beta _ n}m_n(\\mathbf{x } , \\theta ) \\bigr]^2 \\mathbh{1}_{m_n(\\mathbf{x } , \\theta ) \\geq\\beta_n } \\bigr ] + \\xi \\\\ & \\leq & \\mathbb{e } \\bigl [ m_n^2(\\mathbf{x } , \\theta ) \\mathbh { 1}_{m_n(\\mathbf{x } , \\theta ) \\geq\\beta_n } \\bigr ] + \\xi \\\\ & \\leq & \\mathbb{e } \\bigl [ \\mathbb{e } \\bigl [ m_n^2 ( \\mathbf{x } , \\theta ) \\mathbh{1}_{m_n(\\mathbf{x } , \\theta ) \\geq\\beta_n}|\\theta \\bigr ] \\bigr ] + \\xi.\\end{aligned}\\ ] ] since @xmath269 , we have @xmath270 \\\\ & & \\qquad   \\leq\\mathbb{e } \\bigl [ \\bigl(2\\|m\\|_{\\infty}^2 + 2\\max _ { 1 \\leq i \\leq a_n } \\varepsilon_i^2\\bigr ) \\mathbh{1}_{\\mathop{\\mathop{\\max}_{1 \\leq",
    "i \\leq a_n } } \\varepsilon_i \\geq\\sigma\\sqrt{2 } ( \\log a_n)^2 } \\bigr ] \\\\ & & \\qquad \\leq2\\|m\\|_{\\infty}^2 \\mathbb{p } \\bigl [ \\max _ { 1 \\leq i \\leq a_n } \\varepsilon_i \\geq\\sigma\\sqrt{2 } ( \\log a_n)^2 \\bigr ] \\\\ & & \\qquad\\quad { } + 2 \\bigl(\\mathbb{e } \\bigl [ \\max_{1 \\leq i \\leq a_n } \\varepsilon_i^4 \\bigr ] \\mathbb{p } \\bigl [ \\max _ { 1 \\leq i \\leq a_n } \\varepsilon_i \\geq\\sigma\\sqrt{2 } ( \\log a_n)^2 \\bigr ] \\bigr)^{1/2}.\\end{aligned}\\ ] ] it is easy to see that @xmath271 \\leq \\frac{a_n^{1 - \\log a_n } } { 2\\sqrt { \\pi } ( \\log a_n)^2 } .\\end{aligned}\\ ] ] finally , since the @xmath272 s are centered i.i.d .",
    "gaussian random variables , we have , for all @xmath4 large enough , @xmath273 ^ 2 \\\\ & & \\qquad \\leq   \\frac{2\\|m\\|_{\\infty}^2 a_n^{1 - \\log a_n } } { 2\\sqrt{\\pi } ( \\log a_n)^2 } + \\xi + 2 \\biggl(3a_n \\sigma^4 \\frac{a_n^{1 - \\log a_n } } { 2\\sqrt{\\pi } ( \\log a_n)^2 } \\biggr)^{1/2 } \\\\ & & \\qquad   \\leq   3\\xi.\\end{aligned}\\ ] ] this completes the proof of theorem  [ theoremconsistencysemidevelopedbrf ] .",
    "recall that each cell contains exactly one data point .",
    "thus , letting @xmath274,\\end{aligned}\\ ] ] the random forest estimate @xmath14 may be rewritten as @xmath275 we have in particular that @xmath276 .",
    "thus @xmath268 ^ 2 & \\leq &   2 \\mathbb{e } \\biggl [ \\sum _ { i=1}^n w_{ni}(\\mathbf{x } ) \\bigl(y_i - m(\\mathbf{x}_i ) \\bigr ) \\biggr]^2 \\\\ & & { } + 2 \\mathbb{e } \\biggl [ \\sum_{i=1}^n w_{ni}(\\mathbf{x } ) \\bigl(m(\\mathbf { x}_i ) - m(\\mathbf{x } ) \\bigr ) \\biggr]^2 \\\\ & \\stackrel{\\mathrm{def}}{= } &   2 i_n + 2 j_n.\\end{aligned}\\ ] ]      fix @xmath277 .",
    "to upper bound @xmath278 , note that by jensen s inequality , @xmath279 \\\\ & \\leq & \\mathbb{e } \\biggl [ \\sum_{i=1}^n \\mathbh{1}_{\\mathbf{x}_i \\in a_n(\\mathbf{x } , \\theta ) } \\delta^2\\bigl(m , a_n ( \\mathbf{x } , \\theta)\\bigr ) \\biggr ] \\\\ & \\leq & \\mathbb{e } \\bigl [ \\delta^2\\bigl(m , a_n(\\mathbf{x } , \\theta)\\bigr ) \\bigr].\\end{aligned}\\ ] ] so , by definition of @xmath280 , @xmath281 + \\alpha \\\\ & \\leq & \\alpha\\bigl(4 \\|m\\|_{\\infty}^2 + 1\\bigr),\\end{aligned}\\ ] ] for all @xmath4 large enough , according to proposition  [ variancewithincellforetempirique ] .      to bound @xmath282 from above , we note that @xmath283 \\\\ & = &   \\mathbb{e } \\biggl[\\sum_{i=1 } w_{ni}^2(\\mathbf{x } ) \\bigl(y_i - m ( \\mathbf{x}_i)\\bigr)^2 \\biggr ] + i_n',\\end{aligned}\\ ] ] where @xmath284.\\end{aligned}\\ ] ] the term @xmath285 , which involves the double products , is handled separately in lemma  [ lemmainbis ] below . according to this lemma , and by assumption ( h2 ) , for all @xmath4 large enough , @xmath286 consequently ,",
    "recalling that @xmath287 , we have , for all @xmath4 large enough , @xmath288 \\nonumber\\\\[-2pt ] \\label{proofprop2eq1 } & \\leq & \\alpha+ \\mathbb{e } \\biggl [ \\max_{1\\leq\\ell\\leq n } w_{n\\ell } ( \\mathbf{x } ) \\sum_{i=1}^n w_{ni}(\\mathbf{x } ) \\varepsilon_i^2 \\biggr ] \\\\[-2pt ] \\nonumber & \\leq & \\alpha+ \\mathbb{e } \\bigl [ \\max_{1\\leq\\ell\\leq n } w_{n\\ell } ( \\mathbf{x } ) \\max_{1\\leq i \\leq",
    "n } \\varepsilon_i^2 \\bigr].\\end{aligned}\\ ] ] now , observe that in the subsampling step , there are exactly @xmath289 choices to pick a fixed observation @xmath83 .",
    "since @xmath21 and @xmath83 belong to the same cell only if @xmath83 is selected in the subsampling step , we see that @xmath290 \\leq \\frac{{a_n-1 \\choose n-1}}{{a_n\\choose   n } } = \\frac{a_n}{n},\\end{aligned}\\ ] ] where @xmath291 denotes the probability with respect to @xmath24 , conditional on @xmath17 and @xmath11 .",
    "so , @xmath292 \\leq\\frac{a_n}{n}.\\end{aligned}\\ ] ] thus , combining inequalities ( [ proofprop2eq1 ] ) and ( [ proofprop2eq2 ] ) , for all @xmath4 large enough , @xmath293.\\end{aligned}\\ ] ] the term inside the brackets is the maximum of @xmath4 @xmath294-squared distributed random variables .",
    "thus , for some positive constant @xmath295 , @xmath296 \\leq c\\log n;\\ ] ] see , for example , @xcite , chapter  1 . we conclude that for all @xmath4 large enough , @xmath297 since @xmath298 was arbitrary , the proof is complete .",
    "[ lemmainbis ] assume that is satisfied .",
    "then , for all @xmath299 , and all @xmath4 large enough , @xmath300 .    first , assume that ( h2.2 ) is verified .",
    "thus we have for all @xmath301 , @xmath302}{\\mathbb{v}^{1/2 } [ y_i - m(\\mathbf{x}_i ) | \\mathbf{x}_i , \\mathbf{x}_j , y_j   ] \\mathbb{v}^{1/2 } [ \\mathbh{1}_{z_{i , j}=(\\ell_1,\\ell _ 2 ) } | \\mathbf{x } _ i , \\mathbf{x}_j , y_j   ] } \\\\ & & \\qquad = \\frac{\\mathbb{e } [ ( y_i - m(\\mathbf{x}_i ) ) \\mathbh { 1}_{z_{i , j}=(\\ell _ 1,\\ell_2 ) } | \\mathbf{x}_i , \\mathbf{x}_j , y_j   ] } { \\sigma ( \\mathbb{p } [ z_{i , j}=(\\ell_1,\\ell_2 ) | \\mathbf{x}_i , \\mathbf{x}_j , y_j   ] - \\mathbb{p } [ z_{i , j}=(\\ell_1,\\ell_2 ) | \\mathbf{x}_i , \\mathbf{x}_j , y_j ] ^2 ) ^{1/2 } } \\\\ & & \\qquad \\geq\\frac{\\mathbb{e } [ ( y_i - m(\\mathbf{x}_i ) ) \\mathbh { 1}_{z_{i , j}=(\\ell_1,\\ell_2 ) } | \\mathbf{x}_i , \\mathbf{x}_j , y_j   ] } { \\sigma\\mathbb{p } ^{1/2 } [ z_{i , j}=(\\ell_1,\\ell_2 ) | \\mathbf{x}_i , \\mathbf{x}_j , y_j   ] } , \\end{aligned}\\ ] ] where the first equality comes from the fact that , for all @xmath303 , @xmath304,\\end{aligned}\\ ] ] since @xmath305 = 0 $ ] .",
    "thus , noticing that , almost surely , @xmath306 \\\\ & & \\qquad = \\sum_{\\ell_1 , \\ell_2 = 1}^2 \\frac{\\mathbb{e } [ ( y_i - m(\\mathbf{x } _ i ) ) \\mathbh{1}_{z_{i , j}=(\\ell_1,\\ell_2 ) } | \\mathbf{x}_i , \\mathbf { x}_j , y_j   ] } { \\mathbb{p } [ z_{i , j } = ( \\ell_1,\\ell_2 ) | \\mathbf{x}_i , \\mathbf{x}_j , y_j ] } \\mathbh{1}_{z_{i , j}=(\\ell_1,\\ell_2 ) } \\\\ & & \\qquad \\leq4 \\sigma\\max_{\\ell_1 , \\ell_2=0,1}\\frac{|\\operatorname{corr } ( y_i - m(\\mathbf{x}_i ) , \\mathbh{1}_{z_{i , j}=(\\ell_1,\\ell_2)}| \\mathbf{x}_i , \\mathbf{x } _ j , y_j ) |}{\\mathbb{p}^{1/2 } [ z_{i , j}=(\\ell_1,\\ell_2 ) | \\mathbf { x}_i , \\mathbf{x}_j , y_j   ] } \\\\ & & \\qquad \\leq4 \\sigma\\gamma_n,\\end{aligned}\\ ] ] we conclude that the first statement in ( h2.2 ) implies that , almost surely , @xmath307 \\leq4 \\sigma\\gamma_n.\\end{aligned}\\ ] ] similarly , one can prove that the second statement in assumption ( h2.2 ) implies that , almost surely , @xmath308 \\leq4 c \\sigma^2.\\end{aligned}\\ ] ] returning to the term @xmath309 , and recalling that @xmath310 $ ] , we obtain @xmath311 \\\\ & = & \\mathop { \\sum_{i , j}}_{i \\neq j}\\mathbb{e } \\bigl [ \\mathbb { e } \\bigl [ \\mathbh{1}_{\\mathbf{x}\\stackrel{\\theta}{\\leftrightarrow } \\mathbf { x}_i } \\mathbh { 1}_{\\mathbf{x}\\stackrel{\\theta'}{\\leftrightarrow } \\mathbf{x}_j } \\bigl(y_i - m(\\mathbf{x } _ i)\\bigr)\\\\ & & \\hspace*{25pt}\\quad{}\\times \\bigl(y_j - m(\\mathbf{x}_j)\\bigr )   | \\mathbf{x}_i , \\mathbf{x}_j , y_i , \\mathbh { 1}_{\\mathbf{x}\\stackrel{\\theta}{\\leftrightarrow } \\mathbf{x}_i } , \\mathbh{1}_{\\mathbf{x } \\stackrel{\\theta'}{\\leftrightarrow } \\mathbf{x}_j } \\bigr ] \\bigr ] \\\\ & = & \\mathop{\\sum_{i , j}}_{i \\neq j } \\mathbb{e } \\bigl[\\mathbh { 1}_{\\mathbf{x}\\stackrel{\\theta}{\\leftrightarrow } \\mathbf{x}_i } \\mathbh{1}_{\\mathbf{x } \\stackrel{\\theta'}{\\leftrightarrow } \\mathbf{x}_j } \\bigl(y_i - m(\\mathbf { x}_i)\\bigr ) \\\\ & & \\qquad\\hspace*{4pt } { } \\times\\mathbb{e } \\bigl [ y_j - m(\\mathbf{x}_j ) | \\mathbf{x}_i , \\mathbf{x}_j , y_i , \\mathbh{1}_{\\mathbf{x}\\stackrel { \\theta}{\\leftrightarrow } \\mathbf{x}_i } , \\mathbh{1}_{\\mathbf { x}\\stackrel{\\theta ' } { \\leftrightarrow } \\mathbf{x}_j } \\bigr ] \\bigr].\\end{aligned}\\ ] ] therefore , by assumption ( h2.2 ) , @xmath312 \\\\ & \\leq & \\gamma_n \\sum_{i=1}^n \\mathbb{e } \\bigl[\\mathbh{1}_{\\mathbf { x}\\stackrel { \\theta}{\\leftrightarrow } \\mathbf{x}_i } \\bigl|y_i - m ( \\mathbf{x}_i)\\bigr| \\bigr ] \\nonumber \\\\ & \\leq & \\gamma_n \\sum_{i=1}^n \\mathbb{e } \\bigl[\\mathbh{1}_{\\mathbf { x}\\stackrel { \\theta}{\\leftrightarrow } \\mathbf{x}_i } \\mathbb{e } \\bigl [ \\bigl|y_i - m(\\mathbf{x}_i)\\bigr| | \\mathbf{x}_i , \\mathbh{1}_{\\mathbf{x}\\stackrel{\\theta } { \\leftrightarrow } \\mathbf{x}_i } \\bigr ] \\bigr ] \\nonumber \\\\ & \\leq & \\gamma_n \\sum_{i=1}^n \\mathbb{e } \\bigl[\\mathbh{1}_{\\mathbf { x}\\stackrel { \\theta}{\\leftrightarrow } \\mathbf{x}_i } \\mathbb{e}^{1/2 } \\bigl [ \\bigl|y_i - m(\\mathbf{x } _",
    "i)\\bigr|^2 | \\mathbf{x}_i , \\mathbh{1}_{\\mathbf{x}\\stackrel{\\theta } { \\leftrightarrow } \\mathbf{x}_i } \\bigr ] \\bigr ] \\nonumber \\\\ & \\leq & 2 \\sigma c^{1/2 } \\gamma_n.\\end{aligned}\\ ] ]    this proves the result , provided ( h2.2 ) is true .",
    "let us now assume that ( h2.1 ) is verified . the key argument is to note that a data point @xmath83 can be connected with a random point @xmath17 if @xmath137 is selected via the subsampling procedure and if there are no other data points in the hyperrectangle defined by @xmath313 and @xmath17 . data points @xmath83 satisfying the latter geometrical property are called _ layered nearest neighbors _ ( lnn ) ; see , for example , @xcite .",
    "the connection between lnn and random forests was first observed by @xcite , and later worked out by @xcite .",
    "it is known , in particular , that the number of lnn @xmath314 among @xmath38 data points uniformly distributed on @xmath9^d$ ] satisfies , for some constant @xmath315 and for all @xmath4 large enough , @xmath316 & \\leq & a_n \\mathbb{p } \\bigl [ \\mathbf{x}\\mathop{\\leftrightarrow}\\limits^{\\theta}_{\\mathrm{lnn } } \\mathbf{x}_j \\bigr ] + 16 a_n^2 \\mathbb { p } \\bigl [ \\mathbf{x}\\mathop{{\\leftrightarrow}}\\limits^{\\theta}_{\\mathrm{lnn } } \\mathbf{x}_i \\bigr ] \\mathbb{p } \\bigl [ \\mathbf{x}\\mathop { \\leftrightarrow}\\limits_{\\mathrm{lnn}}^{\\theta } \\mathbf{x}_j \\bigr ] \\nonumber \\\\[-8pt ] \\label{lnnnumber } \\\\[-8pt ] \\nonumber & \\leq &   c_1 ( \\log a_n)^{2d-2};\\end{aligned}\\ ] ] see , for example , @xcite .",
    "thus we have @xmath317.\\end{aligned}\\ ] ] consequently , @xmath318 \\biggr],\\end{aligned}\\ ] ] where @xmath319 is the event where @xmath83 is selected by the subsampling and is also a lnn of @xmath17 .",
    "next , with the notation of assumption ( h2 ) , @xmath320 \\\\[-2pt ] & = & \\mathbb{e } \\biggl[\\mathop{\\sum_{i , j}}_{i \\neq j } \\bigl(y_i - m(\\mathbf{x } _",
    "i)\\bigr ) \\bigl(y_j - m(\\mathbf{x}_j)\\bigr ) \\mathbh{1}_{\\mathbf{x}_i \\mathop{\\leftrightarrow}\\limits_{\\mathrm{lnn}}^{\\theta } \\mathbf{x } } \\mathbh { 1}_{\\mathbf{x}_j \\mathop{\\leftrightarrow}\\limits_{\\mathrm{lnn}}^{\\theta ' } \\mathbf{x } }",
    "\\psi_{i , j } \\biggr ] \\\\[-2pt ] & & { } + \\mathbb{e } \\biggl[\\mathop{\\sum_{i , j}}_{i \\neq j } \\bigl(y_i - m(\\mathbf{x}_i)\\bigr ) \\bigl(y_j - m(\\mathbf{x}_j)\\bigr ) \\mathbh{1}_{\\mathbf{x}_i \\mathop{\\leftrightarrow}\\limits_{\\mathrm{lnn}}^{\\theta } \\mathbf{x } } \\mathbh { 1}_{\\mathbf{x}_j \\mathop{\\leftrightarrow}\\limits_{\\mathrm{lnn}}^{\\theta ' } \\mathbf{x } } \\bigl ( \\psi_{i , j}(y_i , y_j ) - \\psi_{i , j}\\bigr ) \\biggr].\\end{aligned}\\ ] ] the first term is easily seen to be zero since @xmath321 \\\\[-2pt ] & & \\qquad = \\mathop{\\sum_{i , j}}_{i \\neq j } \\mathbb{e } \\bigl [ \\mathbh { 1}_{\\mathbf{x}_i \\mathop{\\leftrightarrow}\\limits_{\\mathrm{lnn}}^{\\theta } \\mathbf{x } } \\mathbh{1}_{\\mathbf{x}_j \\mathop{\\leftrightarrow}\\limits_{\\mathrm{lnn}}^{\\theta ' } \\mathbf{x } } \\psi_{i , j } \\\\[-2pt ] & & \\qquad\\qquad\\hspace*{13pt}{}\\times\\mathbb{e } \\bigl [ \\bigl(y_i - m(\\mathbf{x}_i ) \\bigr ) \\bigl(y_j - m(\\mathbf{x}_j)\\bigr ) | \\mathbf{x } , \\mathbf{x}_1,\\ldots , \\mathbf{x}_n , \\theta , \\theta ' \\bigr ] \\bigr ] \\\\ & & \\qquad = 0.\\end{aligned}\\ ] ] therefore , @xmath322 \\\\ & \\leq & \\mathbb{e } \\biggl [ \\max_{1 \\leq\\ell\\leq n}\\bigl|y_i - m ( \\mathbf{x}_i)\\bigr|^2 \\mathop{\\max_{i , j}}_{i \\neq j } \\bigl| \\psi_{i , j } ( y_i , y_j ) - \\psi_{i , j}\\bigr| \\mathop{\\sum_{i , j}}_{i \\neq j } \\mathbh{1}_{\\mathbf{x}_i \\mathop{\\leftrightarrow}\\limits_{\\mathrm{lnn}}^{\\theta } \\mathbf{x } } \\mathbh{1}_{\\mathbf{x}_j \\mathop{\\leftrightarrow}\\limits_{\\mathrm{lnn}}^{\\theta ' } \\mathbf{x } } \\biggr].\\end{aligned}\\ ] ] now , observe that @xmath323 consequently , @xmath324 \\nonumber \\\\[-8pt ] \\label{equationtemporaire } \\\\[-8pt ] \\nonumber & { } & \\times\\mathbb{e}^{1/2 } \\bigl[\\mathop{\\max_{i , j}}_{i \\neq j } \\bigl| \\psi_{i , j } ( y_i , y_j ) - \\psi_{i , j}\\bigr| \\bigr]^2.\\end{aligned}\\ ] ] simple calculations reveal that there exists @xmath315 such that , for all @xmath4 , @xmath325 \\leq c_1 ( \\log n)^2.\\end{aligned}\\ ] ] thus , by inequalities ( [ lnnnumber ] ) and ( [ lnnnumber2 ] ) , the first term in ( [ equationtemporaire ] ) can be upper bounded as follows : @xmath326 \\\\ & & \\qquad = \\mathbb{e}^{1/2 } \\bigl [ l^4_{a_n}(\\mathbf{x } ) \\mathbb{e } \\bigl [ \\max_{1 \\leq \\ell\\leq n } \\bigl|y_i - m ( \\mathbf{x}_i)\\bigr|^4 |\\mathbf{x } , \\mathbf { x}_1,\\ldots , \\mathbf{x}_n \\bigr ] \\bigr ] \\\\ & & \\qquad \\leq c ' ( \\log n ) ( \\log a_n)^{d-1}.\\end{aligned}\\ ] ] finally , @xmath327",
    "^ 2,\\end{aligned}\\ ] ] which tends to zero by assumption .",
    "we greatly thank two referees for valuable comments and insightful suggestions ."
  ],
  "abstract_text": [
    "<S> random forests are a learning algorithm proposed by breiman [ _ mach </S>",
    "<S> . learn . _ </S>",
    "<S> * 45 * ( 2001 ) 532 ] that combines several randomized decision trees and aggregates their predictions by averaging . despite its wide usage and outstanding practical performance , </S>",
    "<S> little is known about the mathematical properties of the procedure . </S>",
    "<S> this disparity between theory and practice originates in the difficulty to simultaneously analyze both the randomization process and the highly data - dependent tree structure . in the present paper , we take a step forward in forest exploration by proving a consistency result for breiman s [ _ mach . learn . </S>",
    "<S> _ * 45 * ( 2001 ) 532 ] original algorithm in the context of additive regression models . </S>",
    "<S> our analysis also sheds an interesting light on how random forests can nicely adapt to sparsity .    </S>",
    "<S> ./style / arxiv - general.cfg    , </S>"
  ]
}