{
  "article_text": [
    "agglomerative hierarchical clustering has been the dominant approach to constructing embedded classification schemes .",
    "it is our aim to direct the reader s attention to practical algorithms and methods  both efficient ( from the computational and storage points of view ) and effective ( from the application point of view ) .",
    "it is often helpful to distinguish between _ method _ , involving a compactness criterion and the target structure of a 2-way tree representing the partial order on subsets of the power set ; as opposed to an _ implementation _ , which relates to the detail of the algorithm used .",
    "as with many other multivariate techniques , the objects to be classified have numerical measurements on a set of variables or attributes .",
    "hence , the analysis is carried out on the rows of an array or matrix .",
    "if we do not have a matrix of numerical values to begin with , then it may be necessary to skilfully construct such a matrix .",
    "the objects , or rows of the matrix , can be viewed as vectors in a multidimensional space ( the dimensionality of this space being the number of variables or columns ) .",
    "a geometric framework of this type is not the only one which can be used to formulate clustering algorithms .",
    "suitable alternative forms of storage of a rectangular array of values are not inconsistent with viewing the problem in geometric terms ( and in matrix terms  for example , expressing the adjacency relations in a graph ) .",
    "motivation for clustering in general , covering hierarchical clustering and applications , includes the following : analysis of data ; interactive user interfaces ; storage and retrieval ; and pattern recognition .",
    "surveys of clustering with coverage also of hierarchical clustering include gordon ( 1981 ) , march ( 1983 ) , jain and dubes ( 1988 ) , gordon ( 1987 ) , mirkin ( 1996 ) , jain , murty and flynn ( 1999 ) , and xu and wunsch ( 2005 ) .",
    "lerman ( 1981 ) and janowitz ( 2010 ) present overarching reviews of clustering including through use of lattices that generalize trees .",
    "the case for the central role of hierarchical clustering in information retrieval was made by van rijsbergen ( 1979 ) and continued in the work of willett ( e.g.  griffiths et al .",
    ", 1984 ) and others .",
    "various mathematical views of hierarchy , all expressing symmetry in one way or another , are explored in murtagh ( 2009 ) .",
    "this article is organized as follows .    in section [ sect5 ]",
    "we look at the issue of normalization of data , prior to inducing a hierarchy on the data .    in section [ sect1 ]",
    "some historical remarks and motivation are provided for hierarchical agglomerative clustering .    in section [ sect3 ]",
    ", we discuss the lance - williams formulation of a wide range of algorithms , and how these algorithms can be expressed in graph theoretic terms and in geometric terms . in section [ sect2 ] , we describe the principles of the reciprocal nearest neighbor and nearest neighbor chain algorithm , to support building a hierarchical clustering in a more efficient manner compared to the lance - williams or general geometric approaches .",
    "in section [ sect6 ] we overview the hierarchical kohonen self - organizing feature map , and also hierarchical model - based clustering .",
    "we conclude this section with some reflections on divisive hierarchical clustering , in general .",
    "section [ sect7 ] surveys developments in grid- and density - based clustering .",
    "the following section , section [ sect8 ] , presents a recent algorithm of this type , which is particularly suitable for the hierarchical clustering of massive data sets .",
    "before clustering comes the phase of data measurement , or measurement of the observables . let us look at some important considerations to be taken into account .",
    "these considerations relate to the metric or other spatial embedding , comprising the first phase of the data analysis _",
    "stricto sensu_.    to group data we need a way to measure the elements and their distances relative to each other in order to decide which elements belong to a group .",
    "this can be a similarity , although on many occasions a dissimilarity measurement , or a `` stronger '' distance , is used .    a distance between any pair of vectors or points @xmath0 satisfies the properties of : symmetry , @xmath1 ; positive definiteness , @xmath2 and @xmath3 iff @xmath4 ; and the triangular inequality , @xmath5 .",
    "if the triangular inequality is not taken into account , we have a dissimilarity .",
    "finally a similarity is given by @xmath6 max@xmath7 .",
    "when working in a vector space a traditional way to measure distances is a minkowski distance , which is a family of metrics defined as follows :    @xmath8    where @xmath9 is the set of positive integers .",
    "the manhattan , euclidean and chebyshev distances ( the latter is also called maximum distance ) are special cases of the minkowski distance when @xmath10 and @xmath11 .    as an example of similarity we have the _ cosine _",
    "similarity , which gives the angle between two vectors .",
    "this is widely used in text retrieval to match vector queries to the dataset . the smaller the angle between a query vector and a document vector ,",
    "the closer a query is to a document .",
    "the normalized cosine similarity is defined as follows :    @xmath12    where @xmath13 is the dot product and @xmath14 the norm .",
    "other relevant distances are the hellinger , variational , mahalanobis and hamming distances .",
    "anderberg ( 1973 ) gives a good review of measurement and metrics , where their interrelationships are also discussed .",
    "also deza and deza ( 2009 ) have produced a comprehensive list of distances in their _ encyclopedia of distances_.    by mapping our input data into a euclidean space , where each object is equiweighted , we can use a euclidean distance for the clustering that follows .",
    "correspondence analysis is very versatile in determining a euclidean , factor space from a wide range of input data types , including frequency counts , mixed qualitative and quantitative data values , ranks or scores , and others .",
    "further reading on this is to be found in benzcri ( 1979 ) , le roux and rouanet ( 2004 ) and murtagh ( 2005 ) .",
    "agglomerative hierarchical clustering algorithms can be characterized as _",
    "greedy _ , in the algorithmic sense . a sequence of irreversible algorithm steps is used to construct the desired data structure .",
    "assume that a pair of clusters , including possibly singletons , is merged or agglomerated at each step of the algorithm .",
    "then the following are equivalent views of the same output structure constructed on @xmath15 objects : a set of @xmath16 partitions , starting with the fine partition consisting of @xmath15 classes and ending with the trivial partition consisting of just one class , the entire object set ; a binary tree ( one or two child nodes at each non - terminal node ) commonly referred to as a dendrogram ; a partially ordered set ( poset ) which is a subset of the power set of the @xmath15 objects ; and an ultrametric topology on the @xmath15 objects .",
    "an ultrametric , or tree metric , defines a stronger topology compared to , for example , a euclidean metric geometry . for three points , @xmath0 , metric and ultrametric respect the properties of symmetry ( @xmath17 , @xmath18 ) and positive definiteness ( @xmath19 and if @xmath20 then @xmath21 ) . a metric",
    "though ( as noted in section [ sect5 ] ) satisfies the triangular inequality , @xmath5 while an ultrametric satisfies the strong triangular or ultrametric ( or non - archimedean ) , inequality , @xmath22 . in section [ sect5 ] , above ,",
    "there was further discussion on metrics .",
    "the single linkage hierarchical clustering approach outputs a set of clusters ( to use graph theoretic terminology , a set of maximal connected subgraphs ) at each level  or for each threshold value which produces a new partition .",
    "the single linkage method with which we begin is one of the oldest methods , its origins being traced to polish researchers in the 1950s ( graham and hell , 1985 ) .",
    "single linkage _ arises since the interconnecting dissimilarity between two clusters or components is defined as the least interconnecting dissimilarity between a member of one and a member of the other .",
    "other hierarchical clustering methods are characterized by other functions of the interconnecting linkage dissimilarities .    as early as the 1970s",
    ", it was held that about @xmath23 of all published work on clustering employed hierarchical algorithms ( blashfield and aldenderfer , 1978 ) .",
    "interpretation of the information contained in a dendrogram is often of one or more of the following kinds : set inclusion relationships , partition of the object - sets , and significant clusters .",
    "much early work on hierarchical clustering was in the field of biological taxonomy , from the 1950s and more so from the 1960s onwards .",
    "the central reference in this area , the first edition of which dates from the early 1960s , is sneath and sokal ( 1973 ) .",
    "one major interpretation of hierarchies has been the evolution relationships between the organisms under study .",
    "it is hoped , in this context , that a dendrogram provides a sufficiently accurate model of underlying evolutionary progression .    a common interpretation made of hierarchical clustering is to derive a partition .",
    "a further type of interpretation is instead to detect maximal ( i.e.  disjoint ) clusters of interest at varying levels of the hierarchy .",
    "such an approach is used by rapoport and fillenbaum ( 1972 ) in a clustering of colors based on semantic attributes .",
    "lerman ( 1981 ) developed an approach for finding significant clusters at varying levels of a hierarchy , which has been widely applied . by developing a wavelet transform _ on _ a dendrogram ( murtagh , 2007 ) , which amounts to a wavelet trasform in the associated ultrametric topological space , the most important  in the sense of best approximating ",
    "clusters can be determined .",
    "such an approach is a topological one ( i.e. , based on sets and their properties ) as contrasted with more widely used optimization or statistical approaches .    in summary",
    ", a dendrogram collects together many of the proximity and classificatory relationships in a body of data .",
    "it is a convenient representation which answers such questions as : `` how many useful groups are in this data ? '' , `` what are the salient interrelationships present ? '' .",
    "but it can be noted that differing answers can feasibly be provided by a dendrogram for most of these questions , depending on the application .",
    "a wide range of agglomerative hierarchical clustering algorithms have been proposed at one time or another .",
    "such hierarchical algorithms may be conveniently broken down into two groups of methods .",
    "the first group is that of linkage methods  the single , complete , weighted and unweighted average linkage methods .",
    "these are methods for which a graph representation can be used .",
    "sneath and sokal ( 1973 ) may be consulted for many other graph representations of the stages in the construction of hierarchical clusterings .",
    "the second group of hierarchical clustering methods are methods which allow the cluster centers to be specified ( as an average or a weighted average of the member vectors of the cluster ) .",
    "these methods include the centroid , median and minimum variance methods .",
    "the latter may be specified either in terms of dissimilarities , alone , or alternatively in terms of cluster center coordinates and dissimilarities .",
    "a very convenient formulation , in dissimilarity terms , which embraces all the hierarchical methods mentioned so far , is the _ lance - williams dissimilarity update formula_. if points ( objects ) @xmath24 and @xmath25 are agglomerated into cluster @xmath26 , then we must simply specify the new dissimilarity between the cluster and all other points ( objects or clusters ) .",
    "the formula is :    @xmath27 where @xmath28 , @xmath29 , @xmath30 , and @xmath31 define the agglomerative criterion .",
    "values of these are listed in the second column of table [ tabhier ] .",
    ".specifications of seven hierarchical clustering methods . [ cols= \" < , < , < , < \" , ]     notes : @xmath32 is the number of objects in cluster @xmath24 .",
    "@xmath33 is a vector in @xmath34-space ( @xmath34 is the set of attributes ) ,  either an intial point or a cluster center .",
    "@xmath35 is the norm in the euclidean metric .",
    "the names upgma , etc .",
    "are due to sneath and sokal ( 1973 ) .",
    "coefficient @xmath29 , with index @xmath25 , is defined identically to coefficient @xmath28 with index @xmath24 .",
    "finally , the lance and williams recurrence formula is ( with @xmath36 expressing absolute value ) : @xmath37    in the case of the single link method , using @xmath38 , @xmath39 , and @xmath40 gives us    @xmath41 which , it may be verified , can be rewritten as    @xmath42    using other update formulas , as given in column 2 of table [ tabhier ] , allows the other agglomerative methods to be implemented in a very similar way to the implementation of the single link method .    in the case of the methods which use cluster centers , we have the center coordinates ( in column 3 of table [ tabhier ] ) and dissimilarities as defined between cluster centers ( column 4 of table [ tabhier ] ) .",
    "the euclidean distance must be used for equivalence between the two approaches . in the case of the _ median method _ , for instance , we have the following ( cf.table [ tabhier ] ) .",
    "let @xmath43 and @xmath44 be two points ( i.e.  @xmath34-dimensional vectors : these are objects or cluster centers ) which have been agglomerated , and let @xmath45 be another point",
    ". from the lance - williams dissimilarity update formula , using squared euclidean distances , we have :    @xmath46    & { { \\| { \\bf a}-{\\bf c } \\|^2 } \\over { 2 } } + { { \\|{\\bf b}-{\\bf c}\\|^2 } \\over { 2 } }           -            { { \\|{\\bf a}-{\\bf b}\\|^2 } \\over { 4 } }    .",
    "\\end{array}\\ ] ]    the new cluster center is @xmath47 , so that its distance to point @xmath48 is    @xmath49    that these two expressions are identical is readily verified . the correspondence between these two perspectives on the one agglomerative criterion is similarly proved for the centroid and minimum variance methods .    for cluster center methods , and with suitable alterations for graph methods ,",
    "the following algorithm is an alternative to the general dissimilarity based algorithm .",
    "the latter may be described as a `` stored dissimilarities approach '' ( anderberg , 1973 ) .",
    "* stored data approach *    step 1 : :    examine all interpoint dissimilarities , and form cluster from two    closest points .",
    "step 2 : :    replace two points clustered by representative point ( center of    gravity ) or by cluster fragment .",
    "step 3 : :    return to step 1 , treating clusters as well as remaining objects ,    until all objects are in one cluster .    in steps 1 and 2 , ``",
    "point '' refers either to objects or clusters , both of which are defined as vectors in the case of cluster center methods .",
    "this algorithm is justified by storage considerations , since we have @xmath50 storage required for @xmath15 initial objects and @xmath50 storage for the @xmath16 ( at most ) clusters . in the case of linkage methods ,",
    "the term `` fragment '' in step 2 refers ( in the terminology of graph theory ) to a connected component in the case of the single link method and to a clique or complete subgraph in the case of the complete link method . without consideration of any special algorithmic `` speed - ups '' , the overall complexity of the above algorithm is @xmath51 due to the repeated calculation of dissimilarities in step 1 , coupled with @xmath50 iterations through steps 1 , 2 and 3 . while the stored data algorithm is instructive , it does not lend itself to efficient implementations . in the section to follow ,",
    "we look at the reciprocal nearest neighbor and mutual nearest neighbor algorithms which can be used in practice for implementing agglomerative hierarchical clustering algorithms .    before concluding this overview of agglomerative hierarchical clustering algorithms",
    ", we will describe briefly the minimum variance method .",
    "the variance or spread of a set of points ( i.e.  the average of the sum of squared distances from the center ) has been a point of departure for specifying clustering algorithms .",
    "many of these algorithms ,  iterative , optimization algorithms as well as the hierarchical , agglomerative algorithms  are described and appraised in wishart ( 1969 ) .",
    "the use of variance in a clustering criterion links the resulting clustering to other data - analytic techniques which involve a decomposition of variance , and make the minimum variance agglomerative strategy particularly suitable for synoptic clustering .",
    "hierarchies are also more balanced with this agglomerative criterion , which is often of practical advantage",
    ".    the minimum variance method produces clusters which satisfy compactness and isolation criteria .",
    "these criteria are incorporated into the dissimilarity .",
    "we seek to agglomerate two clusters , @xmath52 and @xmath53 , into cluster @xmath54 such that the within - class variance of the partition thereby obtained is minimum .",
    "alternatively , the between - class variance of the partition obtained is to be maximized .",
    "let @xmath55 and @xmath56 be the partitions prior to , and subsequent to , the agglomeration ; let @xmath57 , @xmath58 ,  be classes of the partitions :    @xmath59    letting @xmath60 denote _ variance _ , then in agglomerating two classes of @xmath55 , the variance of the resulting partition ( i.e.  @xmath61 ) will necessarily decrease : therefore in seeking to minimize this decrease , we simultaneously achieve a partition with maximum between - class variance .",
    "the criterion to be optimized can then be shown to be : @xmath62                  & { { \\mid c_1 \\mid \\",
    "\\mid c_2 \\mid } \\over                { \\mid c_1 \\mid + \\mid c_2 \\mid } }                \\vert { \\bf c_1 } - { \\bf c_2 } \\vert ^2 \\           , \\end{array}\\ ] ] which is the dissimilarity given in table [ tabhier ] .",
    "this is a dissimilarity which may be determined for any pair of classes of partition @xmath55 ; and the agglomerands are those classes , @xmath52 and @xmath53 , for which it is minimum .",
    "it may be noted that if @xmath52 and @xmath53 are singleton classes , then @xmath63 , i.e.  the variance of a pair of objects is equal to half their euclidean distance .",
    "early , efficient algorithms for hierarchical clustering are due to sibson ( 1973 ) , rohlf ( 1973 ) and defays ( 1977 ) .",
    "their @xmath64 implementations of the single link method and of a ( non - unique ) complete link method , respectively , have been widely cited .    in the early 1980s a range of significant improvements ( de rham , 1980 ; juan , 1982 ) were made to the lance - williams , or related , dissimilarity update schema , which had been in wide use since the mid-1960s .",
    "murtagh ( 1983 , 1985 ) presents a survey of these algorithmic improvements .",
    "we will briefly describe them here .",
    "the new algorithms , which have the potential for _ exactly _ replicating results found in the classical but more computationally expensive way , are based on the construction of _ nearest neighbor chains _ and _ reciprocal _ or mutual nns ( nn - chains and rnns ) .",
    "a nn - chain consists of an arbitrary point ( @xmath65 in fig .",
    "[ figrnns ] ) ; followed by its nn ( @xmath66 in fig .",
    "[ figrnns ] ) ; followed by the nn from among the remaining points ( @xmath54 , @xmath17 , and @xmath67 in fig .",
    "[ figrnns ] ) of this second point ; and so on until we necessarily have some pair of points which can be termed reciprocal or mutual nns .",
    "( such a pair of rnns may be the first two points in the chain ; and we have assumed that no two dissimilarities are equal . )    ( 0,3)(6,2.5 ) ( 7,4 ) ( 5,4 ) ( 4,4 ) ( 2,4 ) ( -1,4 ) ( 7,3)e ( 5,3)d ( 4,3)c ( 2,3)b ( -1,3)a ( -1,4)(1,0)3 ( 2,4)(1,0)2 ( 4,4)(1,0)1 ( 5,4)(-1,0)1    in constructing a nn - chain , irrespective of the starting point , we may agglomerate a pair of rnns as soon as they are found .",
    "what guarantees that we can arrive at the same hierarchy as if we used traditional `` stored dissimilarities '' or `` stored data '' algorithms ?",
    "essentially this is the same condition as that under which no inversions or reversals are produced by the clustering method .",
    "[ figinv ] gives an example of this , where @xmath68 is agglomerated at a lower criterion value ( i.e. dissimilarity ) than was the case at the previous agglomeration between @xmath69 and @xmath70 .",
    "our ambient space has thus contracted because of the agglomeration .",
    "this is due to the algorithm used  in particular the agglomeration criterion  and it is something we would normally wish to avoid .",
    "this is formulated as :    @xmath71 @xmath72    this is one form of bruynooghe s _ reducibility property _ ( bruynooghe , 1977 ; see also murtagh , 1984 ) . using the lance - williams dissimilarity update formula , it can be shown that the minimum variance method does not give rise to inversions ; neither do the linkage methods ; but the median and centroid methods can not be guaranteed _ not _ to have inversions .    to return to fig",
    ".  [ figrnns ] , if we are dealing with a clustering criterion which precludes inversions , then @xmath54 and @xmath17 can justifiably be agglomerated , since no other point ( for example , @xmath66 or @xmath67 ) could have been agglomerated to either of these .",
    "the processing required , following an agglomeration , is to update the nns of points such as @xmath66 in fig .",
    "[ figrnns ] ( and on account of such points , this algorithm was dubbed _ algorithme des clibataires _ , or bachelors algorithm , in de rham , 1980 ) .",
    "the following is a summary of the algorithm :    * nn - chain algorithm *    step 1 : :    select a point arbitrarily .",
    "step 2 : :    grow the nn - chain from this point until a pair of rnns is obtained .",
    "step 3 : :    agglomerate these points ( replacing with a cluster point , or updating    the dissimilarity matrix ) .",
    "step 4 : :    from the point which preceded the rnns ( or from any other arbitrary    point if the first two points chosen in steps 1 and 2 constituted a    pair of rnns ) , return to step 2 until only one point remains .    in murtagh ( 1983 , 1984 , 1985 ) and day and edelsbrunner ( 1984 )",
    ", one finds discussions of @xmath64 time and @xmath50 space implementations of ward s minimum variance ( or error sum of squares ) method and of the centroid and median methods .",
    "the latter two methods are termed the upgmc and wpgmc criteria by sneath and sokal ( 1973 ) .",
    "now , a problem with the cluster criteria used by these latter two methods is that the reducibility property is not satisfied by them .",
    "this means that the hierarchy constructed may not be unique as a result of inversions or reversals ( non - monotonic variation ) in the clustering criterion value determined in the sequence of agglomerations .",
    "murtagh ( 1983 , 1985 ) describes @xmath64 time and @xmath64 space implementations for the single link method , the complete link method and for the weighted and unweighted group average methods ( wpgma and upgma ) .",
    "this approach is quite general vis  vis the dissimilarity used and can also be used for hierarchical clustering methods other than those mentioned .",
    "day and edelsbrunner ( 1984 ) prove the exact @xmath64 time complexity of the centroid and median methods using an argument related to the combinatorial problem of optimally packing hyperspheres into an @xmath34-dimensional volume .",
    "they also address the question of metrics : results are valid in a wide class of distances including those associated with the minkowski metrics .",
    "the construction and maintenance of the nearest neighbor chain as well as the carrying out of agglomerations whenever reciprocal nearest neighbors meet , both offer possibilities for distributed implementation .",
    "implementations on a parallel machine architecture were described by willett ( 1989 ) .",
    "evidently ( from table [ tabhier ] ) both coordinate data and graph ( e.g. , dissimilarity ) data can be input to these agglomerative methods .",
    "gillet et al .",
    "( 1998 ) in the context of clustering chemical structure databases refer to the common use of the ward method , based on the reciprocal nearest neighbors algorithm , on data sets of a few hundred thousand molecules .    applications of hierarchical clustering to bibliographic information retrieval are assessed in griffiths et al .",
    "ward s minimum variance criterion is favored .    from details in white and mccain ( 1997 ) , the institute of scientific information ( isi ) clusters citations ( science , and social science ) by first clustering highly cited documents based on a single linkage criterion , and then four more passes are made through the data to create a subset of a single linkage hierarchical clustering .",
    "in the clustan and r statistical data analysis packages ( in addition to hclust in r , see flashclust due to p.  langfelder and available on cran , `` comprehensive r archive network '' , cran.r-project.org ) there are implementations of the nn - chain algorithm for the minimum variance agglomerative criterion .",
    "a property of the minimum variance agglomerative hierarchical clustering method is that we can use weights on the objects on which we will induce a hierarchy . by default , these weights are identical and equal to 1 . such weighting of observations to be clustered is an important and practical aspect of these software packages .",
    "it is quite impressive how 2d ( 2-dimensional or , for that matter , 3d ) image signals can handle with ease the scalability limitations of clustering and many other data processing operations .",
    "the contiguity imposed on adjacent pixels or grid cells bypasses the need for nearest neighbor finding .",
    "it is very interesting therefore to consider the feasibility of taking problems of clustering massive data sets into the 2d image domain .",
    "the kohonen self - organizing feature map exemplifes this well . in its basic variant ( kohonen , 1984 , 2001 )",
    "is can be formulated in terms of k - means clustering subject to a set of interrelationships between the cluster centers ( murtagh and fernndez - pajares , 1995 ) .",
    "kohonen maps lend themselves well for hierarchical representation .",
    "lampinen and oja ( 1992 ) , dittenbach et al .  (",
    "2002 ) and endo et al .",
    "( 2002 ) elaborate on the kohonen map in this way .",
    "an example application in character recognition is miikkulanien ( 1990 ) .",
    "a short , informative review of hierarchical self - organizing maps is provided by vicente and vellido ( 2004 ) .",
    "these authors also review what they term as probabilistic hierarchical models .",
    "this includes putting into a hierarchical framework the following : gaussian mixture models , and a probabilistic ",
    "bayesian  alternative to the kohonen self - organizing map termed generative topographic mapping ( gtm ) .",
    "gtm can be traced to the kohonen self - organizing map in the following way .",
    "firstly , we consider the hierarchical map as brought about through a growing process , i.e.  the target map is allowed to grow in terms of layers , and of grid points within those layers . secondly , we impose an explicit probability density model on the data .",
    "tino and nabney ( 2002 ) discuss how the local hierarchical models are organized in a hierarchical way .    in wang",
    "et al .  ( 2000 ) an alternating gaussian mixture modeling , and principal component analysis , is described , in this way furnishing a hierarchy of model - based clusters .",
    "aic , the akaike information criterion , is used for selection of the best cluster model overall .",
    "murtagh et al .",
    "( 2005 ) use a top level gaussian mixture modeling with the ( spatially aware ) plic , pseudo - likelihood information criterion , used for cluster selection and identifiability . then at the next level  and potentially also for further divisive , hierarchical levels  the gaussian mixture modeling is continued but now using the marginal distributions within each cluster , and using the analogous bayesian clustering identifiability criterion which is the bayesian information criterion , bic . the resulting output is referred to as a model - based cluster tree .    the model - based cluster tree algorithm of murtagh et al .",
    "( 2005 ) is a divisive hierarchical algorithm .",
    "earlier in this article , we considered agglomerative algorithms .",
    "however it is often feasible to implement a divisive algorithm instead , especially when a graph cut ( for example ) is important for the application concerned .",
    "mirkin ( 1996 , chapter 7 ) describes divisive ward , minimum variance hierarchical clustering , which is closely related to a bisecting k - means also .",
    "a class of methods under the name of spectral clustering uses eigenvalue / eigenvector reduction on the ( graph ) adjacency matrix . as von luxburg ( 2007 ) points out in reviewing this field of spectral clustering , such methods have `` been discovered , re - discovered , and extended many times in different communities '' .",
    "far from seeing this great deal of work on clustering in any sense in a pessimistic way , we see the perennial and pervasive interest in clustering as testifying to the continual renewal and innovation in algorithm developments , faced with application needs .",
    "it is indeed interesting to note how the clusters in a hierarchical clustering may be _ defined _ by the eigenvectors of a dissimilarity matrix , but subject to carrying out the eigenvector reduction in a particular algebraic structure , a semi - ring with additive and multiplicative operations given by `` min '' and `` max '' , respectively ( gondran , 1976 ) .    in the next section , section [ sect7 ] ,",
    "the themes of mapping , and of divisive algorithm , are frequently taken in a somewhat different direction .",
    "as always , the application at issue is highly relevant for the choice of the hierarchical clustering algorithm .",
    "many modern clustering techniques focus on large data sets . in xu and wunsch ( 2008 , p.  215 )",
    "these are classified as follows :    * random sampling * data condensation * density - based approaches * grid - based approaches * divide and conquer * incremental learning    from the point of view of this article , we select density and grid based approaches , i.e. , methods that either look for data densities or split the data space into cells when looking for groups . in this section",
    "we take a look at these two families of methods .",
    "the main idea is to use a grid - like structure to split the information space , separating the dense grid regions from the less dense ones to form groups .",
    "in general , a typical approach within this category will consist of the following steps as presented by grabusts and borisov ( 2002 ) :    1 .   creating a grid structure , i.e.  partitioning the data space into a finite number of non - overlapping cells .",
    "2 .   calculating the cell density for each cell .",
    "sorting of the cells according to their densities .",
    "4 .   identifying cluster centers .",
    "traversal of neighbor cells .",
    "some of the more important algorithms within this category are the following :    * * sting : * statistical information grid - based clustering was proposed by wang et al .",
    "( 1997 ) who divide the spatial area into rectangular cells represented by a hierarchical structure .",
    "the root is at hierarchical level 1 , its children at level 2 , and so on .",
    "this algorithm has a computational complexity of @xmath73 , where @xmath74 is the number of cells in the bottom layer .",
    "this implies that scaling this method to higher dimensional spaces is difficult ( hinneburg and keim , 1999 ) .",
    "for example , if in high dimensional data space each cell has four children , then the number of cells in the second level will be @xmath75 , where @xmath34 is the dimensionality of the database .",
    "* * optigrid : * optimal grid - clustering was introduced by hinneburg and keim ( 1999 ) as an efficient algorithm to cluster high - dimensional databases with noise .",
    "it uses data partitioning based on divisive recursion by multidimensional grids , focusing on separation of clusters by hyperplanes .",
    "a cutting plane is chosen which goes through the point of minimal density , therefore splitting two dense half - spaces .",
    "this process is applied recursively with each subset of data .",
    "this algorithm is hierarchical , with time complexity of @xmath76 ( gan et al .",
    ", 2007 , pp .",
    "210212 ) .",
    "* * gridclus : * proposed by schikute ( 1996 ) is a hierarchical algorithm for clustering very large datasets .",
    "it uses a multidimensional data grid to organize the space surrounding the data values rather than organize the data themselves .",
    "thereafter patterns are organized into blocks , which in turn are clustered by a topological neighbor search algorithm .",
    "five main steps are involved in the gridclus method : ( a ) insertion of points into the grid structure , ( b ) calculation of density indices , ( c ) sorting the blocks with respect to their density indices , ( d ) identification of cluster centers , and ( e ) traversal of neighbor blocks . * * wavecluster : * this clustering technique proposed by sheikholeslami et al .",
    "( 2000 ) defines a uniform two dimensional grid on the data and represents the data points in each cell by the number of points .",
    "thus the data points become a set of grey - scale points , which is treated as an image .",
    "then the problem of looking for clusters is transformed into an image segmentation problem , where wavelets are used to take advantage of their multi - scaling and noise reduction properties .",
    "the basic algorithm is as follows : ( a ) create a data grid and assign each data object to a cell in the grid , ( b ) apply the wavelet transform to the data , ( c ) use the average sub - image to find connected clusters ( i.e.  connected pixels ) , and ( d ) map the resulting clusters back to the points in the original space .",
    "there is a great deal of other work also that is based on using the wavelet and other multiresolution transforms for segmentation .",
    "further grid - based clustering algorithms can be found in the following : chang and jin ( 2002 ) , park and lee ( 2004 ) , gan et al .",
    "( 2007 ) , and xu and wunsch ( 2008 ) .",
    "density - based clustering algorithms are defined as dense regions of points , which are separated by low - density regions .",
    "therefore , clusters can have an arbitrary shape and the points in the clusters may be arbitrarily distributed .",
    "an important advantage of this methodology is that only one scan of the dataset is needed and it can handle noise effectively . furthermore the number of clusters to initialize the algorithm is not required .",
    "some of the more important algorithms in this category include the following :    * * dbscan : * density - based spatial clustering of applications with noise was proposed by ester et al .",
    "( 1996 ) to discover arbitrarily shaped clusters .",
    "since it finds clusters based on density it does not need to know the number of clusters at initialization time .",
    "this algorithm has been widely used and has many variations ( e.g. , see gdbscan by sander et al .",
    "( 1998 ) , pdbscan by xu et al .",
    "( 1999 ) , and dbcluc by zaane and lee ( 2002 ) . *",
    "* bridge : * proposed by dash et al .  ( 2001 ) uses a hybrid approach integrating @xmath77-means to partition the dataset into @xmath77 clusters , and then density - based algorithm dbscan is applied to each partition to find dense clusters . *",
    "* dbclasd : * distribution - based clustering of large spatial databases ( see xu et al . , 1998 ) assumes that data points within a cluster are uniformly distributed .",
    "the cluster produced is defined in terms of the nearest neighbor distance .",
    "* * denclue : * density based clustering aims to cluster large multimedia data .",
    "it can find arbitrarily shaped clusters and at the same time deals with noise in the data .",
    "this algorithm has two steps .",
    "first a pre - cluster map is generated , and the data is divided in hypercubes where only the populated are considered .",
    "the second step takes the highly populated cubes and cubes that are connected to a highly populated cube to produce the clusters . for a detailed presentation of these steps",
    "see hinneburg and keim ( 1998 ) . * * cubn : * this has three steps .",
    "first an erosion operation is carried out to find border points .",
    "second , the nearest neighbor method is used to cluster the border points .",
    "finally , the nearest neighbor method is used to cluster the inner points .",
    "this algorithm is capable of finding non - spherical shapes and wide variations in size .",
    "its computational complexity is @xmath50 with @xmath15 being the size of the dataset . for a detailed presentation of this algorithm",
    "see wang and wang ( 2003 ) .",
    "in the last section , section [ sect7 ] , we have seen a number of clustering methods that split the data space into cells , cubes , or dense regions to locate high density areas that can be further studied to find clusters .    for large data sets clustering via an @xmath34-adic ( @xmath34 integer ,",
    "which if a prime is usually denoted as @xmath78 ) expansion is possible , with the advantage of doing so in linear time for the clustering algorithm based on this expansion .",
    "the usual base 10 system for numbers is none other than the case of @xmath79 and the base 2 or binary system can be referred to as 2-adic where @xmath80 .",
    "let us consider the following distance relating to the case of vectors @xmath81 and @xmath82 with 1 attribute , hence unidimensional :    @xmath83    this distance defines the longest common prefix of strings .",
    "a space of strings , with this distance , is a baire space .",
    "thus we call this the baire distance : here the longer the common prefix , the closer a pair of sequences .",
    "what is of interest to us here is this longest common prefix metric , which is an ultrametric ( murtagh et al . , 2008 ) .",
    "for example , let us consider two such values , @xmath81 and @xmath82 .",
    "we take @xmath81 and @xmath82 to be bounded by 0 and 1 .",
    "each are of some precision , and we take the integer @xmath84 to be the maximum precision .    thus we consider ordered sets @xmath85 and @xmath86 for @xmath87 .",
    "so , @xmath88 is the index of the first decimal place of precision ; @xmath89 is the index of the second decimal place ; . .",
    "; @xmath90 is the index of the @xmath84th decimal place .",
    "the cardinality of the set k is the precision with which a number , @xmath81 , is measured .",
    "consider as examples @xmath91 ; and @xmath92 . in these cases ,",
    "start from the first decimal position . for @xmath88",
    ", we have @xmath94 . for @xmath89 , @xmath95 .",
    "but for @xmath96 , @xmath97 . hence their baire distance is @xmath98 for base @xmath79 .",
    "it is seen that this distance splits a unidimensional string of decimal values into a 10-way hierarchy , in which each leaf can be seen as a grid cell . from equation ( [ eqnbaire ] ) we can read off the distance between points assigned to the same grid cell .",
    "all pairwise distances of points assigned to the same cell are the same .",
    "clustering using this baire distance has been successfully applied to areas such as chemoinformatics ( murtagh et al . ,",
    "2008 ) , astronomy ( contreras and murtagh , 2009 ) and text retrieval ( contreras , 2010 ) .",
    "hierarchical clustering methods , with roots going back to the 1960s and 1970s , are continually replenished with new challenges . as a family of algorithms",
    "they are central to the addressing of many important problems .",
    "their deployment in many application domains testifies to how hierarchical clustering methods will remain crucial for a long time to come .",
    "we have looked at both traditional agglomerative hierarchical clustering , and more recent developments in grid or cell based approaches .",
    "we have discussed various algorithmic aspects , including well - definedness ( e.g.  inversions ) and computational properties .",
    "we have also touched on a number of application domains , again in areas that reach back over some decades ( chemoinformatics ) or many decades ( information retrieval , which motivated much early work in clustering , including hierarchical clustering ) , and more recent application domains ( such as hierarchical model - based clustering approaches ) .",
    "1 .   anderberg mr _ cluster analysis for applications_. academic press , new york , 1973 .",
    "2 .   benzcri et coll .",
    ", jp _ lanalyse des donnes",
    ". i. la taxinomie _ , dunod , paris , 1979 ( 3rd ed . ) .",
    "3 .   blashfield rk and aldenderfer ms the literature on cluster analysis _ multivariate behavioral research _ 1978 , 13 : 271295 .",
    "bruynooghe m mthodes nouvelles en classification automatique des donnes taxinomiques nombreuses _ statistique et analyse des donnes _ 1977 , no .  3 , 2442 . 5 .",
    "chang j - w and jin d - s , a new cell - based clustering method for large , high - dimensional data in data mining applications , in sac 02 : proceedings of the 2002 acm symposium on applied computing .",
    "new york : acm , 2002 , pp .",
    "contreras p search and retrieval in massive data collections phd thesis .",
    "royal holloway , university of london , 2010 .",
    "contreras p and murtagh f fast hierarchical clustering from the baire distance . in classification as a tool for research , eds . h. hocarek - junge and c. weihs , springer , berlin , 235243 , 2010 .",
    "dash m , liu h , and xu x , @xmath99 : merging distance and density based clustering , in dasfaa 01 : proceedings of the 7th international conference on database systems for advanced applications .",
    "washington , dc : ieee computer society , 2001 , pp .",
    "3239 . 9 .",
    "day whe and edelsbrunner h efficient algorithms for agglomerative hierarchical clustering methods _ journal of classification _ 1984 , 1 : 724 .",
    "defays d an efficient algorithm for a complete link method _ computer journal _ 1977 , 20 : 364366",
    "de rham c la classification hirarchique ascendante selon la mthode des voisins rciproques _",
    "les cahiers de lanalyse des donnes _ 1980 , v : 135144 .",
    "deza mm and deza e _ encyclopedia of distances_. springer , berlin , 2009 . 13 .",
    "dittenbach m , rauber a and merkl d uncovering the hierarchical structure in data using the growing hierarchical self - organizing map _ neurocomputing _ , 2002 , 48(14):199216 .",
    "endo m , ueno m and and tanabe t a clustering method using hierarchical self - organizing maps _ journal of vlsi signal processing _ 32:105118 , 2002",
    "ester m , kriegel h - p , sander j , and xu x , a density - based algorithm for discovering clusters in large spatial databases with noise , in 2nd international conference on knowledge discovery and data mining .",
    "aaai press , 1996 , pp .",
    "226231 . 16 .",
    "gan g , ma c and wu j _ data clustering theory , algorithms , and applications _ society for industrial and applied mathematics .",
    "siam , 2007 .",
    "gillet vj , wild dj , willett p and bradshaw j similarity and dissimilarity methods for processing chemical structure databases _ computer journal _ 1998 , 41 : 547558 . 18 .",
    "gondran m valeurs propres et vecteurs propres en classification hirarchique _ rairo informatique thorique _ 1976 , 10(3 ) : 3946 . 19 .",
    "gordon ad _ classification _ , chapman and hall , london , 1981 .",
    "gordon ad a review of hierarchical classification _ journal of the royal statistical society a _ 1987 , 150 : 119137 .",
    "grabusts p and borisov a using grid - clustering methods in data classification , in parelec 02 : proceedings of the international conference on parallel computing in electrical engineering.washington , dc : ieee computer society , 2002 . 22 .",
    "graham rh and hell p on the history of the minimum spanning tree problem _ annals of the history of computing _ 1985 7 : 4357 .",
    "griffiths a , robinson la and willett p hierarchic agglomerative clustering methods for automatic document classification _ journal of documentation _ 1984 , 40 : 175205 .",
    "hinneburg a and keim da , a density - based algorithm for discovering clusters in large spatial databases with noise , in proceeding of the 4th international conference on knowledge discovery and data mining .",
    "new york : aaai press , 1998 , pp .",
    "5868 . 25 .",
    "hinneburg a and keim d optimal grid - clustering : towards breaking the curse of dimensionality in high - dimensional clustering , in vldb 99 : proceedings of the 25th international conference on very large data bases .",
    "san francisco , ca : morgan kaufmann publishers inc . , 1999 , pp .",
    "506517 . 26",
    "jain ak and dubes rc _ algorithms for clustering data _ prentice - hall , englwood cliffs , 1988 .",
    "jain ak , murty , mn and flynn pj data clustering : a review _ acm computing surveys _ 1999 , 31 : 264323 . 28 .",
    "janowitz , mf _ ordinal and relational clustering _ , world scientific , singapore , 2010 .",
    "juan j programme de classification hirarchique par lalgorithme de la recherche en chane des voisins rciproques _ les cahiers de lanalyse des donnes _ 1982 , vii : 219225 .",
    "kohonen t _ self - organization and associative memory _ springer , berlin , 1984 .",
    "kohonen t _ self - organizing maps _ , 3rd edn . , springer , berlin , 2001 .",
    "lampinen j and oja e clustering properties of hierarchical self - organizing maps _ journal of mathematical imaging and vision _ 2 : 261272 , 1992 . 33 .",
    "lerman , ic _ classification et analyse ordinale des donnes _ , dunod , paris , 1981 .",
    "le roux b and rouanet h _",
    "geometric data analysis : from correspondence analysis to structured data analysis _ , kluwer , dordrecht , 2004 .",
    "von luxburg u a tutorial on spectral clustering _ statistics and computing _ 1997 , 17(4 ) : 395416 .",
    "march st techniques for structuring database records _ acm computing surveys _ 1983 , 15 : 4579 . 37 .",
    "miikkulainien r script recognition with hierarchical feature maps _ connection science _ 1990 , 2 : 83101 .",
    "mirkin b _ mathematical classification and clustering _ kluwer , dordrecht , 1996 .",
    "murtagh f a survey of recent advances in hierarchical clustering algorithms _ computer journal _ 1983 , 26 , 354359 .",
    "murtagh f complexities of hierarchic clustering algorithms : state of the art _ computational statistics quarterly _ 1984 , 1 : 101113 .",
    "murtagh f _ multidimensional clustering algorithms _ physica - verlag , wrzburg , 1985 . 42 . murtagh f and hernndez - pajares m the kohonen self - organizing map method : an assessment , _ journal of classification _ 1995 12 , 165 - 190 .",
    "murtagh f , raftery ae and starck jl bayesian inference for multiband image segmentation via model - based clustering trees , _ image and vision computing _ 2005 , 23 : 587596 . 44 .",
    "murtagh f _ correspondence analysis and data coding with java and r _ , chapman and hall , boca raton , 2005 .",
    "murtagh f the haar wavelet transform of a dendrogram _ journal of classification _ 2007 , 24 : 332 .",
    "murtagh f symmetry in data mining and analysis : a unifying view based on hierarchy _ proceedings of steklov institute of mathematics _ 2009 , 265 : 177198 .",
    "murtagh f and downs g and contreras p hierarchical clustering of massive , high dimensional data sets by exploiting ultrametric embedding _ siam journal on scientific computing _ 2008 , 30(2 ) : 707730 .",
    "park nh and lee ws , statistical grid - based clustering over data streams , _",
    "sigmod record _ 2004 , 33(1 ) : 3237 .",
    "rapoport a and fillenbaum s an experimental study of semantic structures , in eds .",
    "romney , r.n .",
    "shepard and s.b .",
    "nerlove , _ multidimensional scaling ; theory and applications in the behavioral sciences .",
    "2 , applications _ , seminar press , new york , 1972 , 93131 . 50 .",
    "rohlf fj algorithm 76 : hierarchical clustering using the minimum spanning tree _",
    "computer journal _ 1973 , 16 : 9395 . 51 .",
    "sander j , ester m , kriegel h .-",
    "p , and xu x , density - based clustering in spatial databases : the algorithm gdbscan and its applications _ data mining knowledge discovery _ 1998 , 2(2 ) : 169194 .",
    "schikuta e grid - clustering : an efficient hierarchical clustering method for very large data sets , in icpr 96 : proceedings of the 13th international conference on pattern recognition .",
    "washington , dc : ieee computer society , 1996 , pp .",
    "101105 . 53 .",
    "sheikholeslami g , chatterjee s and zhang a , wavecluster : a wavelet based clustering approach for spatial data in very large databases , _ the vldb journal _",
    ", 2000 , 8(34 ) : 289304 .",
    "sibson r slink : an optimally efficient algorithm for the single link cluster method _ computer journal _ 1973 , 16 : 3034 .",
    "sneath pha and sokal rr _ numerical taxonomy _ , freeman , san francisco , 1973 .",
    "tino p and nabney i hierarchical gtm : constructing localized non - linear projection manifolds in a principled way , _ ieee transactions on pattern analysis and machine intelligence _",
    ", 2002 , 24(5 ) : 639656 .",
    "van rijsbergen cj _ information retrieval _ butterworths , london , 1979 ( 2nd ed . ) .",
    "wang l and wang z - o , cubn : a clustering algorithm based on density and distance , in proceeding of the 2003 international conference on machine learning and cybernetics .",
    "ieee press , 2003 , pp . 108112 .",
    "wang w , yang j and muntz r sting : a statistical information grid approach to spatial data mining , in vldb 97 : proceedings of the 23rd international conference on very large data bases.san francisco , ca : morgan kaufmann publishers inc . , 1997 , pp",
    "wang y , freedman m.i . and kung s .- y .",
    "probabilistic principal component subspaces : a hierarchical finite mixture model for data visualization , _ ieee transactions on neural networks _ 2000 , 11(3 ) : 625636 .",
    "white hd and mccain kw visualization of literatures , in m.e .",
    "williams , ed . , _ annual review of information science and technology ( arist ) _ 1997 , 32 : 99168",
    "vicente d and vellido a review of hierarchical models for data clustering and visualization . in r. girldez , j.c .",
    "riquelme and j.s .",
    "aguilar - ruiz , eds .",
    ", tendencias de la minera de datos en espaa .",
    "red espaola de minera de datos , 2004 .",
    "willett p efficiency of hierarchic agglomerative clustering using the icl distributed array processor _ journal of documentation _ 1989 , 45 : 145 . 64 .",
    "wishart d mode analysis : a generalization of nearest neighbour which reduces chaining effects , in ed .",
    "cole , _ numerical taxonomy _",
    ", academic press , new york , 282311 , 1969 .",
    "xu r and wunsch d survey of clustering algorithms _ ieee transactions on neural networks _ 2005 , 16 : 645678 .",
    "xu r and wunsch dc _ clustering _",
    "ieee computer society press , 2008 .",
    "xu x , ester m , kriegel h - p and sander j a distribution - based clustering algorithm for mining in large spatial databases , in icde 98 : proceedings of the fourteenth international conference on data engineering .",
    "washington , dc : ieee computer society , 1998 , pp .",
    "324331 . 68 .",
    "xu x , jger j , and kriegel , h - p a fast parallel clustering algorithm for large spatial databases , _ data mining knowledge discovery _ 1999 , 3(3 ) : 263290 . 69",
    "zaane or and lee c - h , clustering spatial data in the presence of obstacles : a density - based approach , in ideas 02 : proceedings of the 2002 international symposium on database engineering and applications.washington , dc : ieee computer society , 2002 , pp ."
  ],
  "abstract_text": [
    "<S> we survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in r and other software environments . </S>",
    "<S> we look at hierarchical self - organizing maps , and mixture models . </S>",
    "<S> we review grid - based clustering , focusing on hierarchical density - based approaches . </S>",
    "<S> finally we describe a recently developed very efficient ( linear time ) hierarchical clustering algorithm , which can also be viewed as a hierarchical grid - based algorithm . </S>"
  ]
}