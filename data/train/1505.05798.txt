{
  "article_text": [
    "reinforcement learning ( rl )  @xcite often requires substantial experience before achieving acceptable performance on individual control problems .",
    "one major contributor to this issue is the _ tabula - rasa _ assumption of typical rl methods , which learn from scratch on each new task . in these settings , learning performance is directly correlated with the quality of the acquired samples .",
    "unfortunately , the amount of experience necessary for high - quality performance increases exponentially with the tasks degrees of freedom , inhibiting the application of rl to high - dimensional control problems .",
    "when data is in limited supply , transfer learning can significantly improve model performance on new tasks by reusing previous learned knowledge during training @xcite .",
    "multi - task learning ( mtl ) explores another notion of knowledge transfer , in which task models are trained simultaneously and share knowledge during the joint learning process  @xcite .    in the _ lifelong learning _ setting  @xcite , which can be framed as an online mtl problem",
    ", agents acquire knowledge incrementally by learning multiple tasks consecutively over their lifetime .",
    "recently , based on the work of  @xcite on supervised lifelong learning , @xcite developed a lifelong learner for policy gradient rl . to ensure efficient learning over consecutive tasks , these works employ a second - order taylor expansion around the parameters that are ( locally ) optimal for each task without transfer .",
    "this assumption simplifies the mtl objective into a weighted quadratic form for online learning , but since it is based on single - task learning , this technique can lead to parameters far from globally optimal . consequently",
    ", the success of these methods for rl highly depends on the policy initializations , which must lead to near - optimal trajectories for meaningful updates . also , since their objective functions average loss over all tasks , these methods exhibit non - vanishing regrets of the form @xmath0 , where @xmath1 is the total number of rounds in a non - adversarial setting .",
    "in addition , these methods may produce control policies with unsafe behavior ( i.e. , capable of causing damage to the agent or environment , catastrophic failure , etc . ) .",
    "this is a critical issue in robotic control , where unsafe control policies can lead to physical damage or user injury .",
    "this problem is caused by using constraint - free optimization over the shared knowledge during the transfer process , which may lead to uninformative or unbounded policies .    in this paper , we address these issues by proposing the first _ safe lifelong learner _ for policy gradient rl operating in an adversarial framework .",
    "our approach rapidly learns high - performance _",
    "safe control policies _ based on the agent s previously learned knowledge and safety constraints on each task , accumulating knowledge over multiple consecutive tasks to optimize overall performance .",
    "we theoretically analyze the regret exhibited by our algorithm , showing _ sublinear _ dependency of the form @xmath2 for @xmath3 rounds , thus outperforming current methods .",
    "we then evaluate our approach empirically on a set of dynamical systems .",
    "an rl agent sequentially chooses actions to minimize its expected cost .",
    "such problems are formalized as markov decision processes ( mdps ) @xmath4 , where @xmath5 is the ( potentially infinite ) state space , @xmath6 is the set of all possible actions , @xmath7 $ ] is a state transition probability describing the system s dynamics , @xmath8 is the cost function measuring the agent s performance , and @xmath9 $ ] is a discount factor . at each time step @xmath10",
    ", the agent is in state @xmath11 and must choose an action @xmath12 , transitioning it to a new state @xmath13 and yielding a cost @xmath14 .",
    "the sequence of state - action pairs forms a trajectory @xmath15 $ ] over a ( possibly infinite ) horizon @xmath16 .",
    "a policy @xmath17 $ ] specifies a probability distribution over state - action pairs , where @xmath18 represents the probability of selecting an action @xmath19 in state @xmath20 .",
    "the goal of rl is to find an optimal policy @xmath21 that minimizes the total expected cost .",
    "* policy search methods * have shown success in solving high - dimensional problems , such as robotic control  @xcite .",
    "these methods represent the policy @xmath22 using a vector @xmath23 of control parameters .",
    "the optimal policy @xmath21 is found by determining the parameters @xmath24 that minimize the expected average cost : @xmath25 where @xmath26 is the total number of trajectories , and @xmath27 and @xmath28 are the probability and cost of trajectory @xmath29 : @xmath30   & \\hspace{7.6em}\\times\\pi_{\\bm{\\alpha}}\\left(\\bm{u}_{m}^{(k)}|\\bm{x}_{m}^{(k)}\\right ) \\end{split } \\\\",
    "\\bm{c}\\left(\\bm{\\tau}^{(k)}\\right ) & = \\frac{1}{m}\\sum_{m=0}^{m-1}\\bm{c}\\left(\\bm{x}_{m+1}^{(k)},\\bm{u}_{m}^{(k)},\\bm{x}_{m}^{(k)}\\right ) \\enspace , \\end{aligned}\\ ] ] with an initial state distribution @xmath31 $ ] .",
    "we handle a constrained version of policy search , in which optimality not only corresponds to minimizing the total expected cost , but also to ensuring that the policy satisfies safety constraints .",
    "these constraints vary between applications , for example corresponding to maximum joint torque or prohibited physical positions .      in this paper",
    ", we employ a special form of _ regret minimization games _ , which we briefly review here .",
    "a regret minimization game is a triple @xmath32 , where @xmath33 is a non - empty decision set , @xmath34 is the set of moves of the adversary which contains bounded convex functions from @xmath35 to @xmath36 , and @xmath1 is the total number of rounds .",
    "the game proceeds in rounds , where at each round @xmath37 , the agent chooses a prediction @xmath38 and the environment ( i.e. , the adversary ) chooses a loss function @xmath39 . at the end of the round",
    ", the loss function @xmath40 is revealed to the agent and the decision @xmath41 is revealed to the environment . in this paper , we handle the full - information case , where the agent may observe the entire loss function @xmath40 as its feedback and can exploit this in making decisions . the goal is to minimize the cumulative regret @xmath42 $ ] .",
    "when analyzing the regret of our methods , we use a variant of this definition to handle the lifelong rl case : @xmath43 \\enspace , \\ ] ] where @xmath44 denotes the loss of task @xmath45 at round @xmath46 .    for our framework , we adopt a variant of regret minimization called `` follow the regularized leader , '' which minimizes regret in two steps . first , the unconstrained solution @xmath47 is determined ( see sect .  [",
    "sec : unconstrained ] ) by solving an unconstrained optimization over the accumulated losses observed so far . given @xmath47 , the constrained solution",
    "is then determined by learning a projection into the constraint set via bregman projections ( see  @xcite ) .",
    "we adopt a lifelong learning framework in which the agent learns multiple rl tasks consecutively , providing it the opportunity to transfer knowledge between tasks to improve learning .",
    "let @xmath48 denote the set of tasks , each element of which is an mdp .",
    "at any time , the learner may face any previously seen task , and so must strive to maximize its performance across all tasks .",
    "the goal is to learn optimal policies @xmath49 for all tasks , where policy @xmath50 for task @xmath45 is parameterized by @xmath51 .",
    "in addition , each task is equipped with safety constraints to ensure acceptable policy behavior : @xmath52 , with @xmath53 and @xmath54 representing the allowed policy combinations .",
    "the precise form of these constraints depends on the application domain , but this formulation supports constraints on ( e.g. ) joint torque , acceleration , position , etc .    at each round @xmath46 , the learner observes a set of @xmath55 trajectories @xmath56 from a task @xmath57 , where each trajectory has length @xmath58 . to support knowledge transfer between tasks ,",
    "we assume that each task s policy parameters @xmath59 at round @xmath46 can be written as a linear combination of a shared latent basis @xmath60 with coefficient vectors @xmath61 ; therefore , @xmath62 .",
    "each column of @xmath63 represents a chunk of transferrable knowledge ; this task construction has been used successfully in previous multi - task learning work @xcite . extending this previous work ,",
    "we ensure that the shared knowledge repository is `` informative '' by incorporating bounding constraints on the frobenius norm @xmath64 of @xmath63 .",
    "consequently , the optimization problem after observing @xmath65 rounds is : @xmath66+\\mu_{1}\\left|\\left|\\bm{s}\\right|\\right|_{\\mathsf{f}}^{2}+\\mu_{2}\\left|\\left|\\bm{l}\\right|\\right|_{\\mathsf{f}}^{2}\\label{eq : originalopti}\\\\   & \\ \\ \\ \\text{s.t.}\\ \\ \\ \\ \\bm{a}_{{{t_{j}}}}\\bm{\\alpha}_{{{t_{j}}}}\\leq\\bm{b}_{{{t_{j}}}}\\ \\ \\forall{{t_{j}}}\\in\\mathcal{i}_{{r}}\\nonumber \\\\   & \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\",
    "\\bm{\\lambda}_{\\text{min}}\\left(\\bm{l}\\bm{l}^{\\mathsf{t}}\\right)\\geq p\\ \\",
    "\\bm{\\lambda}_{\\text{max}}\\left(\\bm{l}\\bm{l}^{\\mathsf{t}}\\right)\\leq q\\enspace,\\nonumber \\end{aligned}\\ ] ] where @xmath67 and @xmath68 are the constraints on @xmath69 , @xmath70 are design weighting parameters s later in sect .",
    "[ sect : mainresults ] to obtain regret bounds , and leave them as variables now for generality . ] , @xmath71 denotes the set of all tasks observed so far through round @xmath65 , and @xmath72 is the collection of all coefficients @xmath73    the loss function @xmath74 in eq",
    ".   corresponds to a policy gradient learner for task @xmath75 , as defined in eq .  .",
    "typical policy gradient methods  @xcite maximize a lower bound of the expected cost @xmath76 , which can be derived by taking the logarithm and applying jensen s inequality : @xmath77=\\log\\!\\left[\\sum_{k=1}^{n_{{{t_{j}}}}}p_{\\bm{\\alpha}_{{{t_{j}}}}}^{\\left({{t_{j}}}\\right)}\\!\\left(\\bm{\\tau}_{{{t_{j}}}}^{(k)}\\right)\\bm{c}^{\\left({{t_{j}}}\\right)}\\!\\left(\\bm{\\tau}_{{{t_{j}}}}^{(k)}\\right)\\right]\\label{eq : loss}\\hspace{-1em}\\\\[-.25em ] \\nonumber   & \\geq \\log\\!\\left[n_{{{t_{j}}}}\\right]\\ !",
    "+ \\mathbb{e}\\!\\!\\left [ \\sum_{m=0}^{m_{{{t_{j}}}}-1}\\!\\!\\!\\log\\!\\left[\\pi_{\\bm{\\alpha}_{{{t_{j}}}}}\\!\\!\\left(\\bm{u}_{m}^{\\left(k,{{t_{j}}}\\right ) } \\mid \\bm{x}_{m}^{\\left(k,{{t_{j}}}\\right)}\\!\\right)\\right]\\ ! \\right]_{k=1}^{n_{{{t_{j}}}}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!+\\ ! \\text{\\footnotesize const}~.\\nonumber\\end{aligned}\\ ] ] therefore , our goal is to minimize the following objective : @xmath78\\!\\!\\right)\\label{eq : relaxedone}\\\\[-0.8em ]   & \\hspace{4em}+\\mu_{1}\\left\\vert \\bm{s}\\right\\vert _ { \\mathsf{f}}^{2}+\\mu_{2}\\left\\vert \\bm{l}\\right\\vert _ { \\mathsf{f}}^{2}\\nonumber \\\\   & \\ \\ \\ \\text{s.t.}\\ \\ \\ \\ \\bm{a}_{{{t_{j}}}}\\bm{\\alpha}_{{{t_{j}}}}\\leq\\bm{b}_{{{t_{j}}}}\\ \\ \\forall{{t_{j}}}\\in\\mathcal{i}_{{r}}\\nonumber \\\\   & \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\bm{\\lambda}_{\\text{min}}\\left(\\bm{l}\\bm{l}^{\\mathsf{t}}\\right)\\geq p\\ \\ \\text{and}\\ \\ \\bm{\\lambda}_{\\text{max}}\\left(\\bm{l}\\bm{l}^{\\mathsf{t}}\\right)\\leq q\\enspace.\\nonumber \\end{aligned}\\ ] ]      the optimization problem above can be mapped to the standard online learning framework by unrolling @xmath63 and @xmath72 into a vector @xmath79^{\\mathsf{t}}\\in\\mathbb{r}^{dk+k{{|\\mathcal{t}|}}}$ ] .",
    "choosing @xmath80 , and @xmath81 , we can write the safe lifelong policy search problem ( eq .  ) as : @xmath82 where @xmath83 is the set of allowable policies under the given safety constraints .",
    "note that the loss for task @xmath75 can be written as a bilinear product in @xmath84 : @xmath85\\ ] ] @xmath86,\\ \\bm{\\theta}_{\\bm{s}_{{{t_{j}}}}}=\\left[\\!\\begin{array}{c } \\bm{\\theta}_{dk+1}\\\\ \\vdots\\\\ \\bm{\\theta}_{(d+1)k+1 } \\end{array}\\!\\right].\\end{aligned}\\ ] ]    we see that the problem in eq .",
    "is equivalent to eq .   by noting that at @xmath65 rounds , @xmath87",
    "we solve eq .   in two steps .",
    "first , we determine the unconstrained solution @xmath88 when @xmath89 ( see sect .  [ sec : unconstrained ] ) . given @xmath88 , we derive the constrained solution @xmath90 by learning a projection @xmath91 to the constraint set @xmath83 , which amounts to minimizing the bregman divergence over @xmath92 ( see sect .  [",
    "sec : constraint ] ) , we linearize the loss around the constrained solution of the previous round to increase stability and ensure convergence . given the linear losses , it suffices to solve the bregman divergence over the regularizer , reducing the computational cost . ] .",
    "the complete approach is given in algorithm  [ alg : safeonlinepg ] and is available as a software implementation on the authors websites .",
    "although eq .",
    "is not jointly convex in both @xmath63 and @xmath72 , it is separably convex ( for log - concave policy distributions ) .",
    "consequently , we follow an alternating optimization approach , first computing @xmath63 while holding @xmath72 fixed , and then updating @xmath72 given the acquired @xmath63 .",
    "we detail this process for two popular pg learners , ereinforce  @xcite and enac  @xcite .",
    "the derivations of the update rules below can be found in appendix  [ app : update ] .",
    "these updates are governed by learning rates @xmath93 and @xmath94 that decay over time ; @xmath93 and @xmath94 can be chosen using line - search methods as discussed by  @xcite . in our experiments ,",
    "we adopt a simple yet effective strategy , where @xmath95 and @xmath96 , with @xmath97 .",
    "* step 1 : updating @xmath63  * holding @xmath72 fixed , the latent repository can be updated according to : @xmath98 with learning rate @xmath99 , and @xmath100 as the inverse of the fisher information matrix  @xcite .    in the special case of gaussian policies , the update for @xmath63 can be derived in a closed form as @xmath101 , where @xmath102 @xmath103 is the covariance of the gaussian policy for a task @xmath75 , and @xmath104 denotes the state features .    * step 2 : updating @xmath72  * given the fixed basis @xmath63 , the coefficient matrix @xmath72 is updated column - wise for all @xmath105 : @xmath106 with learning rate @xmath107 . for gaussian policies ,",
    "the closed - form of the update is @xmath108 , where @xmath109      once we have obtained the unconstrained solution @xmath88 ( which satisfies eq .  , but can lead to policy parameters in unsafe regions ) , we then derive the constrained solution to ensure safe policies .",
    "we learn a projection @xmath91 from @xmath88 to the constraint set : @xmath110 where @xmath111 is the bregman divergence over @xmath112 : @xmath113 solving eq .",
    "is computationally expensive since @xmath92 includes the sum back to the original round . to remedy this problem ,",
    "ensure the stability of our approach , and guarantee that the constrained solutions for all observed tasks lie within a bounded region , we linearize the current - round loss function @xmath114 around the _ constrained _ solution of the previous round @xmath115 : @xmath116 where @xmath117 , & \\hat{\\bm{u}}&=\\left[\\!\\begin{array}{c } \\bm{u}\\\\ 1 \\end{array}\\!\\right ] ~.\\end{aligned}\\ ] ]    given the above linear form , we can rewrite the optimization problem in eq .   as : @xmath118 consequently , determining",
    "_ safe policies _ for lifelong policy search reinforcement learning amounts to solving : @xmath119    to solve the optimization problem above , we start by converting the inequality constraints to equality constraints by introducing slack variables @xmath120 .",
    "we also guarantee that these slack variables are bounded by incorporating @xmath121 : @xmath122 with this formulation , learning @xmath91 amounts to solving second - order cone and semi - definite programs .",
    "this section determines the constrained projection of the shared basis @xmath63 given fixed @xmath72 and @xmath123 .",
    "we show that @xmath63 can be acquired efficiently , since this step can be relaxed to solving a semi - definite program in @xmath124  @xcite . to formulate the semi - definite program , note that @xmath125 from the constraint set , we recognize : @xmath126 since @xmath127 , we can write : @xmath128      having determined @xmath63 , we can acquire @xmath72 and update @xmath123 by solving a second - order cone program  @xcite of the following form : @xmath129    * inputs : * total number of rounds @xmath1 , weighting factor @xmath130 , regularization parameters @xmath131 and @xmath132 , constraints @xmath67 and @xmath68 , number of latent basis vectors @xmath133 .",
    "@xmath134 , @xmath135 with @xmath136 @xmath137 , and update @xmath138 compute * unconstrained solution * @xmath139 ( sect .",
    "[ sec : unconstrained ] ) fix @xmath72 and @xmath123 , and update @xmath63 ( sect .",
    "[ sec : semidefiniteprogram ] ) use updated @xmath63 to derive @xmath72 and @xmath123 ( sect .",
    "[ sec : cone ] ) * output : * safety - constrained @xmath63 and @xmath72",
    "this section quantifies the performance of our approach by providing formal analysis of the regret after @xmath3 rounds .",
    "we show that the safe lifelong reinforcement learner exhibits _ sublinear _ regret in the total number of rounds .",
    "formally , we prove the following theorem :    [ theo : main ] after @xmath3 rounds and choosing @xmath140 , @xmath141 , with @xmath142 being a diagonal matrix among the @xmath143 columns of @xmath63 , @xmath144 , and @xmath145 , the safe lifelong reinforcement learner exhibits sublinear regret of the form : @xmath146    * proof roadmap : * the remainder of this section completes our proof of theorem  [ theo : main ] ; further details are given in appendix  [ appendix : proofs ] .",
    "we assume linear losses for all tasks in the constrained case in accordance with sect .",
    "[ sec : constraint ] .",
    "although linear losses for policy search rl are too restrictive given a single operating point , as discussed previously , we remedy this problem by generalizing to the case of piece - wise linear losses , where the linearization operating point is a resultant of the optimization problem . to bound the regret , we need to bound the dual euclidean norm ( which is the same as the euclidean norm ) of the gradient of the loss function , then prove theorem  [ theo : main ] by bounding : ( 1 ) task @xmath75 s gradient loss ( sect .",
    "[ sec : itgradient ] ) , and ( 2 ) linearized losses with respect to @xmath63 and @xmath72 ( sect .",
    "[ sec : linearbound ] ) .",
    "we start by stating essential lemmas for theorem  [ theo : main ] ; due to space constraints , proofs for all lemmas are available in the supplementary material . here , we bound the gradient of a loss function @xmath147 at round @xmath65 under gaussian policies .",
    "[ ass : pol ] we assume that the policy for a task @xmath75 is gaussian , the action set @xmath148 is bounded by @xmath149 , and the feature set is upper - bounded by @xmath150 .",
    "[ lemma : lemma1 ] assume task @xmath75 s policy at round @xmath65 is given by @xmath151 , for states @xmath152 and actions @xmath153 . for + @xmath154",
    "$ ] ,  the gradient @xmath155 satisfies @xmath156 @xmath157 for all trajectories and all tasks , with @xmath158 and @xmath159 .",
    "as discussed previously , we linearize the loss of task @xmath160 around the constraint solution of the previous round @xmath115 . to acquire the regret bounds in theorem  [ theo : main ] , the next step is to bound the dual norm , @xmath161 of eq .",
    "it can be easily seen @xmath162    since @xmath163 can be bounded by @xmath164 ( see sect .",
    "[ sec : background ] ) , + the next step is to bound @xmath165 , and @xmath166 .",
    "[ lemma : gradientone ] the norm of the gradient of the loss function evaluated at @xmath115 satisfies @xmath167    to finalize the bound of @xmath168 as needed for deriving the regret , we must derive an upper - bound for @xmath166 :    [ lemma : theta ] the l@xmath169 norm of the constraint solution at round @xmath170 , @xmath171 is bounded by @xmath172   & \\hspace{3em}\\max_{{{t_{k}}}\\in\\mathcal{i}_{{r}-1}}\\left\\ { \\left|\\left|\\bm{a}_{{{t_{k}}}}^{\\dagger}\\right|\\right|_{2}^{2}\\left(\\left|\\left|\\bm{b}_{{{t_{k}}}}\\right|\\right|_{2}+\\bm{c}_{\\text{max}}\\right)^{2}\\right\\ } \\bigg ] \\enspace , \\end{aligned}\\ ] ] where @xmath173 is the number of unique tasks observed so far .",
    "given the previous two lemmas , we can prove the bound for @xmath168 :    the l@xmath169 norm of the linearizing term of @xmath114 around @xmath115 , @xmath168 , is bounded by @xmath174 where @xmath164 is the constant upper - bound on @xmath175 , and @xmath176\\\\   & \\hspace{1em}\\times\\left(\\frac{d}{p}\\sqrt{2q}\\sqrt{\\!\\!\\max_{{{t_{k}}}\\in\\mathcal{i}_{{r}-1}}\\!\\!\\left\\{\\ ! \\|\\bm{a}_{{{t_{k}}}}^{\\dagger}\\|_{2}^{2}\\left(\\|\\bm{b}_{{{t_{k}}}}\\|_{2}^{2}+\\bm{c}_{\\text{max}}^{2}\\right)\\!\\right\\ } } \\!+\\!\\sqrt{qd}\\right)\\\\   & \\bm{\\gamma}_{2}({r})\\leq\\sqrt{q\\times d}\\\\   & + \\sqrt{\\left|\\mathcal{i}_{{r}-1}\\right|}\\sqrt{\\!1\\!+\\!\\frac{1}{p^{2}}\\max_{{{t_{k}}}\\in\\mathcal{i}_{{r}-1}}\\!\\!\\left\\ { \\left|\\left|\\bm{a}_{{{t_{k}}}}^{\\dagger}\\right|\\right|_{2}^{2}\\!\\!\\left(\\left|\\left|\\bm{b}_{{{t_{k}}}}\\right|\\right|_{2}+\\bm{c}_{\\text{max}}\\right)^{2}\\right\\ } } ~.\\end{aligned}\\ ] ]      given the lemmas in the previous section , we now can derive the sublinear regret bound given in theorem  [ theo : main ] .",
    "using results developed by  @xcite , it is easy to see that @xmath177 from the convexity of the regularizer , we obtain : @xmath178 we have : @xmath179 therefore , for any @xmath180 @xmath181 assuming that @xmath182 , we can derive : @xmath183    the following lemma finalizes the proof of theorem  [ theo : main ] :    after @xmath3 rounds with @xmath184 , for any @xmath180 we have that @xmath185 .    from eq .",
    ", it follows that @xmath186 with @xmath187 . since @xmath188",
    ", we have that @xmath189 with @xmath190 .    given that @xmath191 , with @xmath192 being a constant , we have : @xmath193 * initializing @xmath63 and @xmath72 : * we initialize",
    "@xmath194 , with @xmath144 and @xmath195 to ensure the invertibility of @xmath63 and that the constraints are met .",
    "this leads to @xmath196 choosing @xmath197 , we acquire sublinear regret , finalizing the statement of theorem  [ theo : main ] : @xmath198\\end{aligned}\\ ] ]",
    "to validate the empirical performance of our method , we applied our safe online pg algorithm to learn multiple consecutive control tasks on three dynamical systems ( figure  [ fig : dynamical ] ) . to generate multiple tasks , we varied the parameterization of each system , yielding a set of control tasks from each domain with varying dynamics .",
    "the optimal control policies for these systems vary widely with only minor changes in the system parameters , providing substantial diversity among the tasks within a single domain .",
    "* simple mass spring damper :  * the simple mass ( sm ) system is characterized by three parameters : the spring constant @xmath133 in n / m , the damping constant @xmath199 in ns / m and the mass @xmath10 in kg . the system s state is given by the position @xmath20 and @xmath200 of the mass , which varies according to a linear force @xmath201 .",
    "the goal is to train a policy for controlling the mass in a specific state @xmath202 .",
    "* cart pole :  * the cart - pole ( cp ) has been used extensively as a benchmark for evaluating rl methods  @xcite .",
    "cp dynamics are characterized by the cart s mass @xmath203 in kg , the pole s mass @xmath204 in kg , the pole s length in meters , and a damping parameter @xmath199 in ns / m .",
    "the state is given by the cart s position @xmath20 and velocity @xmath200 , as well as the pole s angle @xmath84 and angular velocity @xmath205 .",
    "the goal is to train a policy that controls the pole in an upright position .",
    "we generated 10 tasks for each domain by varying the system parameters to ensure a variety of tasks with diverse optimal policies , including those with highly chaotic dynamics that are difficult to control .",
    "we ran each experiment for a total of @xmath3 rounds , varying from @xmath206 for the simple mass to @xmath207 for the quadrotor to train @xmath63 and @xmath72 , as well as for updating the pg - ella and pg models . at each round @xmath46 ,",
    "the learner observed a task @xmath75 through 50 trajectories of 150 steps and updated @xmath63 and @xmath208 .",
    "the dimensionality @xmath133 of the latent space was chosen independently for each domain via cross - validation over 3 tasks , and the learning step size for each task domain was determined by a line search after gathering 10 trajectories of length 150 .",
    "we used enac , a standard pg algorithm , as the base learner .",
    "we compared our approach to both standard pg ( i.e. , enac ) and pg - ella  @xcite , examining both the constrained and unconstrained variants of our algorithm .",
    "we also varied the number of iterations in our alternating optimization from @xmath209 to @xmath210 to evaluate the effect of these inner iterations on the performance , as shown in figures  [ fig : resbenchmark ]  and  [ fig : quad ] .",
    "for the two mtl algorithms ( our approach and pg - ella ) , the policy parameters for each task @xmath75 were initialized using the learned basis ( i.e. , @xmath62 ) . we configured pg - ella as described by @xcite , ensuring a fair comparison .",
    "for the standard pg learner , we provided additional trajectories in order to ensure a fair comparison , as described below .    for the experiments with policy constraints , we generated a set of constraints @xmath211 for each task that restricted the policy parameters to pre - specified `` safe '' regions , as shown in figures  [ fig : trajsm ] and  [ fig : trajcp ] .",
    "we also tested different values for the constraints on @xmath63 , varying @xmath67 and @xmath68 between @xmath212 to @xmath209 ; our approach showed robustness against this broad range , yielding similar average cost performance .      figure  [ fig : resbenchmark ] reports our results on the benchmark simple mass and cart - pole systems .",
    "figures  [ fig : perfsm ] and  [ fig : perfcp ] depicts the performance of the learned policy in a lifelong learning setting over consecutive unconstrained tasks , averaged over all 10 systems over 100 different initial conditions .",
    "these results demonstrate that our approach is capable of outperforming both standard pg ( which was provided with 50 _ additional _ trajectories each iteration to ensure a more fair comparison ) and pg - ella , both in terms of initial performance and learning speed .",
    "these figures also show that the performance of our method increases as it is given more alternating iterations per - round for fitting @xmath63 and @xmath72 .",
    "we evaluated the ability of these methods to respect safety constraints , as shown in figures  [ fig : trajsm ] and  [ fig : trajcp ] .",
    "the thicker black lines in each figure depict the allowable `` safe '' region of the policy space . to enable online learning per - task ,",
    "the same task @xmath75 was observed on each round and the shared basis @xmath63 and coefficients @xmath208 were updated using alternating optimization .",
    "we then plotted the change in the policy parameter vectors per iterations ( i.e. , @xmath62 ) for each method , demonstrating that our approach abides by the safety constraints , while standard pg and pg - ella can violate them ( since they only solve an unconstrained optimization problem ) .",
    "in addition , these figures show that increasing the number of alternating iterations in our method causes it to take a more direct path to the optimal solution .",
    "we also applied our approach to the more challenging domain of quadrotor control .",
    "the dynamics of the quadrotor system ( figure  [ fig : dynamical ] ) are influenced by inertial constants around @xmath213 , @xmath214 , and @xmath215 , thrust factors influencing how the rotor s speed affects the overall variation of the system s state , and the lengths of the rods supporting the rotors .",
    "although the overall state of the system can be described by a 12-dimensional vector , we focus on stability and so consider only six of these state - variables .",
    "the quadrotor system has a high - dimensional action space , where the goal is to control the four rotational velocities @xmath216 of the rotors to stabilize the system . to ensure realistic dynamics",
    ", we used the simulated model described by  @xcite , which has been verified and used in the control of physical quadrotors .",
    "we generated 10 different quadrotor systems by varying the inertia around the x , y and z - axes .",
    "we used a linear quadratic regulator , as described by  @xcite , to initialize the policies in both the learning and testing phases .",
    "we followed a similar experimental procedure to that discussed above to update the models .",
    "figure  [ fig : quad ] shows the performance of the unconstrained solution as compared to standard pg and pg - ella .",
    "again , our approach clearly outperforms standard pg and pg - ella in both the initial performance and learning speed .",
    "we also evaluated constrained tasks in a similar manner , again showing that our approach is capable of respecting constraints . since the policy space is higher dimensional , we can not visualize it as well as the benchmark systems , and so instead report the number of iterations it takes our approach to project the policy into the safe region .",
    "figure  [ fig : numobs ] shows that our approach requires only one observation of the task to acquire safe policies , which is substantially lower then standard pg or pg - ella ( e.g. , which require 545 and 510 observations , respectively , in the quadrotor scenario ) .",
    "we described the first lifelong pg learner that provides sublinear regret @xmath2 with @xmath3 total rounds .",
    "in addition , our approach supports safety constraints on the learned policy , which are essential for robust learning in real applications .",
    "our framework formalizes lifelong learning as online mtl with limited resources , and enables safe transfer by sharing policy parameters through a latent knowledge base that is efficiently updated over time .",
    "24 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    yasin abbasi - yadkori , peter bartlett , varun kanade , yevgeny seldin , & csaba szepesvri .",
    "online learning in markov decision processes with adversarially chosen transition probability distributions . _",
    "advances in neural information processing systems _ 26 , 2013 .",
    "haitham bou ammar , karl tuyls , matthew  e.  taylor , kurt driessen , & gerhard weiss .",
    "reinforcement learning transfer via sparse coding . in _ proceedings of the international conference on autonomous agents and multiagent systems _ ( aamas ) , 2012 .    haitham bou ammar , eric eaton , paul ruvolo , & matthew taylor .",
    "online multi - task learning for policy gradient methods . in _ proceedings of the 31st international conference on machine learning _ ( icml ) , 2014 .",
    "samir bouabdallah .",
    "_ design and control of quadrotors with application to autonomous flying_. hd thesis , cole polytechnique fdrale de lausanne , 2007 .",
    "stephen boyd & lieven vandenberghe .",
    "_ convex optimization_. cambridge university press , new york , ny , 2004 .",
    "lucian busoniu , robert babuska , bart  de schutter , & damien ernst .",
    "_ reinforcement learning and dynamic programming using function approximators_. crc press , boca raton , fl , 2010 .",
    "eliseo ferrante , alessandro lazaric , & marcello restelli .",
    "ransfer of task representation in reinforcement learning using policy - based proto - value functions . in _ proceedings of the 7th international joint conference on autonomous agents and multiagent systems _ ( aamas ) , 2008 .    mohammad gheshlaghi  azar , alessandro lazaric , & emma brunskill .",
    "sequential transfer in multi - armed bandit with finite set of models .",
    "_ advances in neural information processing systems _ 26 , 2013 .",
    "roger  a. horn & roy mathias .",
    "auchy - schwarz inequalities associated with positive semidefinite matrices .",
    "_ linear algebra and its applications _",
    "142:0 6382 , 1990 .",
    "jens kober & jan peters .",
    "olicy search for motor primitives in robotics .",
    "_ machine learning _ , 840 ( 12):0 171203 , 2011 .",
    "abhishek kumar & hal daum iii .",
    "earning task grouping and overlap in multi - task learning . in _ proceedings of the 29th international conference on machine learning _",
    "( icml ) , 2012 .",
    "alessandro lazaric .",
    "ransfer in reinforcement learning : a framework and a survey . in m.  wiering & m.  van otterlo , editors , _ reinforcement learning : state of the art_. springer , 2011 .",
    "jan peters & stefan schaal .",
    "einforcement learning of motor skills with policy gradients .",
    "_ neural networks _ , 2008 .",
    "jan peters & stefan schaal .",
    "atural actor - critic .",
    "_ neurocomputing _ 71 , 2008 .",
    "paul ruvolo & eric eaton . : an efficient lifelong learning algorithm . in _ proceedings of the 30th international conference on machine learning _",
    "( icml ) , 2013 .",
    "richard  s. sutton & andrew  g. barto .",
    "_ introduction to reinforcement learning_. mit press , cambridge , ma , 1998 .",
    "richard  s. sutton , david mcallester , satinder singh , & yishay mansour .",
    "olicy gradient methods for reinforcement learning with function approximation .",
    "_ advances in neural information processing systems _ 12 , 2000 .",
    "matthew  e.  taylor & peter stone .",
    "ransfer learning for reinforcement learning domains : a survey .",
    "_ journal of machine learning research _ , 10:0 16331685 , 2009 .",
    "sebastian thrun & joseph osullivan . iscovering structure in multiple learning tasks : the tc algorithm . in _ proceedings of the 13th international conference on machine learning _ ( icml ) , 1996 .",
    "sebastian thrun & joseph osullivan .",
    "earning more from less data : experiments in lifelong learning .",
    "_ seminar digest _ , 1996 .",
    "holger voos & haitham bou ammar .",
    "onlinear tracking and landing controller for quadrotor aerial robots . in _ proceedings of the ieee multi - conference on systems and control _ , 2010 .",
    "ronald  j. williams .",
    "imple statistical gradient - following algorithms for connectionist reinforcement learning .",
    "_ machine learning _ 80 ( 34):0 229256 , 1992 .",
    "aaron wilson , alan fern , soumya ray , & prasad tadepalli .",
    "ulti - task reinforcement learning : a hierarchical bayesian approach . in _ proceedings of the 24th international conference on machine learning _",
    "( icml ) , 2007 .",
    "jian zhang , zoubin ghahramani , & yiming yang .",
    "flexible latent variable models for multi - task learning . _ machine learning _ , 730 ( 3):0 221242 , 2008 .",
    "in this appendix , we derive the update equations for @xmath63 and @xmath72 in the special case of gaussian policies .",
    "please note that these derivations can be easily extended to other policy forms in higher dimensional action spaces .    for a task @xmath217 ,",
    "the policy @xmath218 is given by : @xmath219 therefore , the safe lifelong reinforcement learning optimization objective can be written as : @xmath220 to arrive at the update equations , we need to derive eq .",
    "with respect to each @xmath63 and @xmath72",
    ".      starting with the derivative of @xmath221 with respect to the shared repository @xmath63 , we can write : @xmath222 \\\\ & = - \\sum_{j=1}^{r}\\left [ \\frac{\\eta_{t_{j}}}{\\sigma_{t_{j}}^{2}n_{t_{j } } } \\sum_{k=1}^{n_{t_{j}}}\\sum_{m=0}^{m_{t_{j}}-1 } \\left(\\bm{u}_{m}^{(k , t_{j})}-\\left(\\bm{l}\\bm{s}_{t_{j}}\\right)^{\\mathsf{t}}\\bm{\\phi}\\left(\\bm{x}_{m}^{(k , t_{j})}\\right)\\right ) \\bm{\\phi}\\left(\\bm{x}_{m}^{(k , t_{j})}\\right)\\bm{s}_{t_{j}}^{\\mathsf{t}}\\right ] + 2 \\mu_{2}\\bm{l } \\enspace .\\end{aligned}\\ ] ] to acquire the minimum , we set the above to zero : @xmath223 + 2 \\mu_{2}\\bm{l }   = 0 \\\\ & \\sum_{j=1}^{r } \\left[\\frac{\\eta_{t_{j}}}{\\sigma_{t_{j}}^{2 } n_{t_{j } } } \\sum_{k=1}^{n_{t_{j } } } \\sum_{m=0}^{m_{t_{j}}-1 } \\bm{s}_{t_{j}}^{\\mathsf{t}}\\bm{l}^{\\mathsf{t } } \\bm{\\phi}\\left(\\bm{x}_{m}^{(k , t_{j})}\\right ) \\bm{\\phi}\\left(\\bm{x}_{m}^{(k , t_{j})}\\right ) \\bm{s}_{t_{j}}^{\\mathsf{t } } \\right ] + 2\\mu_{2 } \\bm{l } = \\sum_{j=1}^{r } \\frac{\\eta_{t_{j}}}{\\sigma_{t_{j}}^{2}n_{t_{j } } } \\sum_{k=1}^{n_{t_{j}}}\\sum_{m=0}^{m_{t_{j}}-1 } \\bm{u}_{m}^{(k , t_{j } ) } \\bm{\\phi}\\left(\\bm{x}_{m}^{(k , t_{j})}\\right ) \\bm{s}_{t_{j}}^{\\mathsf{t } } \\enspace .\\end{aligned}\\ ] ] noting that @xmath224 , we can write : @xmath225 + 2\\mu_{2 } \\bm{l } = \\sum_{j=1}^{r } \\frac{\\eta_{t_{j}}}{\\sigma_{t_{j}}^{2}n_{t_{j } } } \\sum_{k=1}^{n_{t_{j}}}\\sum_{m=0}^{m_{t_{j}}-1 } \\bm{u}_{m}^{(k , t_{j } ) } \\bm{\\phi}\\left(\\bm{x}_{m}^{(k , t_{j})}\\right ) \\bm{s}_{t_{j}}^{\\mathsf{t } } \\enspace .\\ ] ] to solve eq .",
    ", we introduce the standard @xmath226 operator leading to : @xmath227 + 2\\mu_{2 } \\bm{l}\\right )   \\\\ & \\hspace{26em } = \\text{vec}\\left (   \\sum_{j=1}^{r } \\frac{\\eta_{t_{j}}}{\\sigma_{t_{j}}^{2}n_{t_{j } } } \\sum_{k=1}^{n_{t_{j}}}\\sum_{m=0}^{m_{t_{j}}-1 } \\bm{u}_{m}^{(k , t_{j } ) } \\bm{\\phi}\\left(\\bm{x}_{m}^{(k , t_{j})}\\right ) \\bm{s}_{t_{j}}^{\\mathsf{t } } \\right ) \\\\ & \\sum_{j=1}^{r } \\frac{\\eta_{t_{j}}}{\\sigma_{t_{j}}^{2 } n_{t_{j } } } \\sum_{k=1}^{n_{t_{j } } } \\sum_{m=0}^{m_{t_{j}}-1 } \\text{vec}\\left({\\bm{\\phi}\\left(\\bm{x}_{m}^{(k , t_{j})}\\right ) } \\bm{s}_{t_{j}}^{\\mathsf{t}}\\right ) \\text{vec}\\left(\\bm{\\phi}^{\\mathsf{t}}\\left(\\bm{x}_{m}^{(k , t_{j})}\\right ) \\bm{l}\\bm{s}_{t_{j}}\\right ) + 2\\mu_{2 } \\text{vec}(\\bm{l } ) \\\\ & \\hspace{26em}= \\sum_{j=1}^{r } \\frac{\\eta_{t_{j}}}{\\sigma_{t_{j}}^{2}n_{t_{j } } } \\sum_{k=1}^{n_{t_{j}}}\\sum_{m=0}^{m_{t_{j}}-1 } \\text{vec}\\left(\\bm{u}_{m}^{(k , t_{j } ) } \\bm{\\phi}\\left(\\bm{x}_{m}^{(k , t_{j})}\\right ) \\bm{s}_{t_{j}}^{\\mathsf{t } } \\right ) \\enspace .\\end{aligned}\\ ] ] knowing that for a given set of matrices @xmath228 , @xmath229 , and @xmath230 , @xmath231 , we can write @xmath232 by choosing @xmath233 , and @xmath234 , we can update @xmath235 .      to derive the update equations with respect to @xmath72 , similar approach to that of @xmath63 can be followed .",
    "the derivative of @xmath221 with respect to @xmath72 can be computed column - wise for all tasks observed so far : @xmath236 \\\\ & = - \\sum_{t_{k}=t_{j}}\\left [ \\frac{\\eta_{t_{j}}}{\\sigma_{t_{j}}^{2}n_{t_{j } } } \\sum_{k=1}^{n_{t_{j}}}\\sum_{m=0}^{m_{t_{j}}-1 } \\left(\\bm{u}_{m}^{(k , t_{j})}-\\left(\\bm{l}\\bm{s}_{t_{j}}\\right)^{\\mathsf{t}}\\bm{\\phi}\\left(\\bm{x}_{m}^{(k , t_{j})}\\right)\\right ) \\bm{l}^{\\mathsf{t}}\\bm{\\phi}\\left(\\bm{x}_{m}^{(k , t_{j})}\\right)\\right ] + 2 \\mu_{2}\\bm{s}_{t_{j } } \\enspace .\\end{aligned}\\ ] ] using a similar analysis to the previous section , choosing @xmath237 we can update @xmath238 .",
    "in this appendix , we prove the claims and lemmas from the main paper , leading to sublinear regret ( theorem 1 ) .",
    "assume the policy for a task @xmath75 at a round @xmath65 to be given by @xmath239 , for @xmath152 and @xmath153 with @xmath240 and @xmath241 representing the state and action spaces , respectively .",
    "the gradient @xmath242 , for @xmath243 $ ] satisfies @xmath244,\\ ] ] with @xmath245 and @xmath246 for all trajectories and all tasks .        since @xmath239 ,",
    "we can write @xmath250=-\\log\\left[\\sqrt{2\\pi\\sigma_{{{t_{j}}}}^{2}}\\right]-\\frac{1}{2\\sigma_{{{t_{j}}}}^{2}}\\left(\\bm{u}_{m}^{\\left(k,\\ { { t_{j}}}\\right)}-\\bm{\\alpha}_{{{t_{j}}}}^{\\mathsf{t}}\\big|_{\\hat{\\bm{\\theta}}_{{r}}}\\bm{\\phi}\\left(\\bm{x}_{m}^{\\left(k,\\ { { t_{j}}}\\right)}\\right)\\right)^{2}.\\ ] ] therefore : @xmath251\\\\   & \\leq\\frac{m_{{{t_{j}}}}}{\\sigma_{{{t_{j}}}}^{2}}\\bigg[\\max_{k , m}\\left\\ { \\left|\\bm{u}_{m}^{\\left(k,\\ { { t_{j}}}\\right)}\\right|\\times\\left|\\left|\\bm{\\phi}\\left(\\bm{x}_{m}^{\\left(k,\\ { { t_{j}}}\\right)}\\right)\\right|\\right|_{2}\\right\\ } \\\\   & \\hspace{12em}+\\max_{k , m}\\left\\ { \\left|\\bm{\\alpha}_{{{t_{j}}}}^{\\mathsf{t}}\\big|_{\\hat{\\bm{\\theta}}_{{r}}}\\bm{\\phi}\\left(\\bm{x}_{m}^{\\left(k,\\ { { t_{j}}}\\right)}\\right)\\right|\\times\\left|\\left|\\bm{\\phi}\\left(\\bm{x}_{m}^{\\left(k,\\ { { t_{j}}}\\right)}\\right)\\right|\\right|_{2}\\right\\ } \\bigg]\\\\   & \\leq\\frac{m_{{{t_{j}}}}}{\\sigma_{{{t_{j}}}}^{2}}\\bigg[\\max_{k , m}\\left\\ { \\left|\\bm{u}_{m}^{\\left(k,\\ { { t_{j}}}\\right)}\\right|\\right\\ } \\max_{k , m}\\left\\ { \\left|\\left|\\bm{\\phi}\\left(\\bm{x}_{m}^{\\left(k,\\ { { t_{j}}}\\right)}\\right)\\right|\\right|_{2}\\right\\ } \\\\   & \\hspace{8.1em}+\\max_{k , m}\\left\\ { \\left|\\left\\langle \\bm{\\alpha}_{{{t_{j}}}}\\big|_{\\hat{\\bm{\\theta}}_{{r}}},\\bm{\\phi}\\left(\\bm{x}_{m}^{\\left(k,\\ { { t_{j}}}\\right)}\\right)\\right\\rangle \\right|\\right\\ } \\max_{k , m}\\left\\ { \\left|\\left|\\bm{\\phi}\\left(\\bm{x}_{m}^{\\left(k,\\ { { t_{j}}}\\right)}\\right)\\right|\\right|_{2}\\right\\ } \\bigg ] \\enspace .\\end{aligned}\\ ] ] denoting @xmath252 and @xmath253 for all trajectories and all tasks , we can write @xmath254 \\enspace .\\ ] ] using the cauchy - shwarz inequality  @xcite ,",
    "we can upper bound @xmath255 as @xmath256 finalizing the statement of the claim , the overall bound on the norm of the gradient of @xmath74 can be written as @xmath249 \\enspace .\\label{eq : one}\\ ] ]      as mentioned previously , we consider the linearization of the loss function @xmath258 around the constraint solution of the previous round , @xmath115 . since @xmath115 satisfies @xmath259 .",
    "hence , we can write @xmath260 therefore @xmath261 combining the above results with those of eq .   we arrive at @xmath262 \\enspace .\\ ] ]        the derivative of @xmath264 can be written as @xmath265\\\\ \\vdots\\\\ \\nabla_{\\bm{\\alpha}_{{{t_{j}}}}}l_{{{t_{j}}}}^{\\mathsf{t}}(\\bm{\\theta})\\big|_{\\hat{\\bm{\\theta}}_{{r}}}\\left[\\begin{array}{c } \\frac{\\partial\\bm{\\alpha}_{{{t_{j}}}}^{(1)}}{\\partial\\bm{\\theta}_{dk+k{{|\\mathcal{t}|}}}}\\big|_{\\hat{\\bm{\\theta}}_{{r}}}\\\\ \\vdots\\\\ \\frac{\\partial\\bm{\\alpha}_{{{t_{j}}}}^{(d)}}{\\partial\\bm{\\theta}_{dk+k{{|\\mathcal{t}|}}}}\\big|_{\\hat{\\bm{\\theta}}_{{r } } } \\end{array}\\right ] \\end{array}\\right]=\\left[\\begin{array}{c } \\nabla_{\\bm{\\alpha}_{{{t_{j}}}}}l_{{{t_{j}}}}^{\\mathsf{t}}(\\bm{\\theta})\\big|_{\\hat{\\bm{\\theta}}_{{r}}}\\left[\\begin{array}{c } \\bm{\\theta}_{dk+1}\\big|_{\\hat{\\bm{\\theta}}_{{r}}}\\\\ 0\\\\ \\vdots\\\\ 0 \\end{array}\\right]\\\\ \\vdots\\\\ \\nabla_{\\bm{\\alpha}_{{{t_{j}}}}}l_{{{t_{j}}}}^{\\mathsf{t}}(\\bm{\\theta})\\big|_{\\hat{\\bm{\\theta}}_{{r}}}\\left[\\begin{array}{c } 0\\\\ \\vdots\\\\ \\bm{\\theta}_{(d+1)k+1}\\big|_{\\hat{\\bm{\\theta}}_{{r } } } \\end{array}\\right]\\\\ \\vdots\\\\ \\nabla_{\\bm{\\alpha}_{{{t_{j}}}}}l_{{{t_{j}}}}^{\\mathsf{t}}(\\bm{\\theta})\\big|_{\\hat{\\bm{\\theta}}_{{r}}}\\left[\\begin{array}{c } \\bm{\\theta}_{d(k+1)+1}\\big|_{\\hat{\\bm{\\theta}}_{{r}}}\\\\ \\vdots\\\\ \\bm{\\theta}_{dk}\\big|_{\\hat{\\bm{\\theta}}_{{r } } } \\end{array}\\right ] \\end{array}\\right]\\\\   & \\implies \\left\\|\\nabla_{\\bm{\\theta}}l_{{{t_{j}}}}(\\bm{\\theta})\\big|_{\\hat{\\bm{\\theta}}_{{r}}}\\right\\|_{2}^{2}\\leq\\left\\|\\nabla_{\\bm{\\alpha}_{{{t_{j}}}}}l_{{{t_{j}}}}(\\bm{\\alpha}_{{{t_{j}}}})\\big|_{\\hat{\\bm{\\theta}}_{{r}}}\\right\\|_{2}^{2}\\left[d\\left\\|\\bm{s}_{{{t_{j}}}}\\big|_{\\hat{\\bm{\\theta}}_{{r}}}\\right\\|_{2}^{2}+\\left\\|\\bm{l}\\big|_{\\hat{\\bm{\\theta}}_{{r}}}\\right\\|_{\\mathsf{f}}^{2}\\right ] \\enspace .\\end{aligned}\\ ] ]        [ [ bounding - leftbms_t_jbig_hatbmtheta_rright_22-and - bmlbig_hatbmtheta_r_mathsff2 ] ] bounding @xmath269 and @xmath270 : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    considering the constraint @xmath271 for a task @xmath75 , we realize that @xmath272 . therefore , @xmath273 noting that @xmath274 to relate @xmath275 to @xmath276 , we need to bound @xmath277 in terms of @xmath69 . denoting the spectrum of @xmath278 as @xmath279 such that @xmath280 , then @xmath281 such that @xmath282 .",
    "hence , @xmath283 .",
    "noticing that @xmath284 , we recognize @xmath285 . therefore @xmath286 plugging the results of eq .   into eq .",
    ", we arrive at @xmath287 finally , since @xmath115 satisfies the constraints , we note that @xmath288",
    ". consequently , @xmath289    the l@xmath169 norm of the constraint solution at round @xmath170 , @xmath171 is bounded by @xmath290 \\enspace .\\ ] ] with @xmath173 being the cardinality of @xmath291 representing the number of different tasks observed so - far .    noting that @xmath292^{\\mathsf{t}}$ ] , it is easy to see @xmath293\\\\   & \\leq q\\times d\\left[1+\\left|\\mathcal{i}_{{r}-1}\\right|\\frac{1}{p^{2}}\\max_{{{t_{k}}}\\in\\mathcal{i}_{{r}-1}}\\left\\ { \\left|\\left|\\bm{a}_{{{t_{k}}}}^{\\dagger}\\right|\\right|_{2}^{2}\\left(\\left|\\left|\\bm{b}_{{{t_{k}}}}\\right|\\right|_{2}+\\bm{c}_{\\text{max}}\\right)^{2}\\right\\ } \\right].\\\\ \\ ] ]    the l@xmath169 norm of the linearizing term of @xmath147 around @xmath115 , @xmath294 , is bounded by @xmath295 with @xmath296 being the constant upper - bound on @xmath297 , and @xmath298\\\\   & \\hspace{8em}\\times\\left(\\sfrac{d}{p}\\sqrt{2q}\\sqrt{\\max_{{{t_{k}}}\\in\\mathcal{i}_{{r}-1}}\\left\\ { \\|\\bm{a}_{{{t_{k}}}}^{\\dagger}\\|_{2}^{2}\\left(\\|\\bm{b}_{{{t_{k}}}}\\|_{2}^{2}+\\bm{c}_{\\text{max}}^{2}\\right)\\right\\ } } + \\sqrt{qd}\\right ) \\enspace",
    ".\\\\ \\bm{\\gamma}_{2}({r } ) & \\leq\\sqrt{q\\times d}+\\sqrt{\\left|\\mathcal{i}_{{r}-1}\\right|}\\sqrt{\\left[1+\\frac{1}{p^{2}}\\max_{{{t_{k}}}\\in\\mathcal{i}_{{r}-1}}\\left\\ { \\left|\\left|\\bm{a}_{{{t_{k}}}}^{\\dagger}\\right|\\right|_{2}^{2}\\left(\\left|\\left|\\bm{b}_{{{t_{k}}}}\\right|\\right|_{2}+\\bm{c}_{\\text{max}}\\right)^{2}\\right\\ } \\right ] } \\enspace .\\end{aligned}\\ ] ]    we have previously shown that @xmath299 . using the previously derived lemmas we can upper - bound @xmath300 as follows @xmath301\\\\   & \\hspace{8em}\\times\\left(\\sfrac{d}{p}\\sqrt{2q}\\sqrt{\\max_{{{t_{k}}}\\in\\mathcal{i}_{{r}-1}}\\left\\ { \\|\\bm{a}_{{{t_{k}}}}^{\\dagger}\\|_{2}^{2}\\left(\\|\\bm{b}_{{{t_{k}}}}\\|_{2}^{2}+\\bm{c}_{\\text{max}}^{2}\\right)\\right\\ } } + \\sqrt{qd}\\right ) \\enspace .\\end{aligned}\\ ] ] further , @xmath302\\\\ \\implies \\left|\\left|\\hat{\\bm{\\theta}}_{{r}}\\right|\\right|_{2 } & \\leq\\sqrt{q\\times d}+\\sqrt{\\left|\\mathcal{i}_{{r}-1}\\right|}\\sqrt{\\left[1+\\frac{1}{p^{2}}\\max_{{{t_{k}}}\\in\\mathcal{i}_{{r}-1}}\\left\\ { \\left|\\left|\\bm{a}_{{{t_{k}}}}^{\\dagger}\\right|\\right|_{2}^{2}\\left(\\left|\\left|\\bm{b}_{{{t_{k}}}}\\right|\\right|_{2}+\\bm{c}_{\\text{max}}\\right)^{2}\\right\\ } \\right ] } \\enspace .\\end{aligned}\\ ] ] therefore @xmath303 with @xmath296 being the constant upper - bound on @xmath297 , and @xmath298\\\\   & \\hspace{8em}\\times\\left(\\sfrac{d}{p}\\sqrt{2q}\\sqrt{\\max_{{{t_{k}}}\\in\\mathcal{i}_{{r}-1}}\\left\\ { \\|\\bm{a}_{{{t_{k}}}}^{\\dagger}\\|_{2}^{2}\\left(\\|\\bm{b}_{{{t_{k}}}}\\|_{2}^{2}+\\bm{c}_{\\text{max}}^{2}\\right)\\right\\ } } + \\sqrt{qd}\\right).\\\\ \\bm{\\gamma}_{2}({r } ) & \\leq\\sqrt{q\\times d}+\\sqrt{\\left|\\mathcal{i}_{{r}-1}\\right|}\\sqrt{\\left[1+\\frac{1}{p^{2}}\\max_{{{t_{k}}}\\in\\mathcal{i}_{{r}-1}}\\left\\ { \\left|\\left|\\bm{a}_{{{t_{k}}}}^{\\dagger}\\right|\\right|_{2}^{2}\\left(\\left|\\left|\\bm{b}_{{{t_{k}}}}\\right|\\right|_{2}+\\bm{c}_{\\text{max}}\\right)^{2}\\right\\ } \\right ] } \\enspace .\\end{aligned}\\ ] ]    after @xmath3 rounds and choosing @xmath304 , @xmath141 , with @xmath142 being a diagonal matrix among the @xmath143 columns of @xmath63 , @xmath144 , and @xmath145 , for any @xmath180 our algorithm exhibits a sublinear regret of the form @xmath305    given the ingredients of the previous section , next we derive the sublinear regret results which finalize the statement of the theorem .",
    "first , it is easy to see that @xmath177 further , from strong convexity of the regularizer we obtain : @xmath306 it can be seen that @xmath179 finally , for any @xmath180 , we have : @xmath307+\\bm{\\omega}_{0}(\\bm{u})-\\bm{\\omega}_{0}(\\hat{\\bm{\\theta}}_{1 } ) \\enspace .\\ ] ] assuming @xmath308 , we can derive @xmath309        it is then easy to see @xmath311 since @xmath312 with @xmath313 being the total number of tasks available , then we can write @xmath314 with @xmath315",
    ". further , it is easy to see that @xmath191 with @xmath192 being a constant , which leads to @xmath316 * initializing @xmath63 and @xmath72 : * we initialize @xmath141 , with @xmath144 and @xmath145 ensures the invertability of @xmath63 and that the constraints are met .",
    "this leads us to @xmath317 choosing @xmath318 , we acquire sublinear regret , finalizing the statement of the theorem : @xmath319 with @xmath192 being a constant ."
  ],
  "abstract_text": [
    "<S> lifelong reinforcement learning provides a promising framework for developing versatile agents that can accumulate knowledge over a lifetime of experience and rapidly learn new tasks by building upon prior knowledge . </S>",
    "<S> however , current lifelong learning methods exhibit non - vanishing regret as the amount of experience increases , and include limitations that can lead to suboptimal or unsafe control policies . to address these issues , </S>",
    "<S> we develop a lifelong policy gradient learner that operates in an adversarial setting to learn multiple tasks online while enforcing safety constraints on the learned policies . </S>",
    "<S> we demonstrate , for the first time , _ sublinear regret _ for lifelong policy search , and validate our algorithm on several benchmark dynamical systems and an application to quadrotor control . </S>"
  ]
}