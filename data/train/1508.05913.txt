{
  "article_text": [
    "binary classification problems appear from diverse practical applications , such as , financial fraud detection , spam email classification , medical diagnosis with genomics data , drug response modeling , among many others . in these classification problems ,",
    "the goal is to predict class labels based on a given set of variables .",
    "suppose that we observe a training data set consisting of @xmath0 pairs , where @xmath1 , @xmath2 , and @xmath3 .",
    "a classifier fits a discriminant function @xmath4 and constructs a classification rule to classify data point @xmath5 to either class @xmath6 or class @xmath7 according to the sign of @xmath8 .",
    "the decision boundary is given by @xmath9 .",
    "two canonical classifiers are linear discriminant analysis and logistic regression .",
    "modern classification algorithms can produce flexible non - linear decision boundaries with high accuracy .",
    "the two most popular approaches are ensemble learning and support vector machines / kernel machines . ensemble learning such as boosting @xcite and random forest @xcite combine many weak learners like decision trees into a powerful one . the support vector machine ( svm )",
    "@xcite fits an optimal separating hyperplane in the extended kernel feature space which is non - linear in the original covariate spaces . in a recent extensive numerical study by @xcite ,",
    "the kernel svm is shown to be one of the best among 179 commonly used classifiers .",
    "motivated by  data - piling \" in the high - dimension - low - sample - size problems , @xcite invented a new classification algorithm named distance weighted discrimination ( dwd ) that retains the elegant geometric interpretation of the svm and delivers competitive performance . since then much work has been devoted to the development of dwd .",
    "the readers are referred to @xcite for an up - to - date list of work on dwd . on the other hand",
    ", we notice that dwd has not attained the popularity it deserves .",
    "we can think of two reasons for that .",
    "first , the current state - of - the - art algorithm for dwd is based on second - order - cone programming ( socp ) proposed in @xcite .",
    "socp was an essential part of the dwd development .",
    "as acknowledged in @xcite , socp was then much less well - known than quadratic programming , even in optimization .",
    "furthermore , socp is generally more computationally demanding than quadratic programming .",
    "there are two existing implementations of the socp algorithm : @xcite in matlab and @xcite in r. with these two implementations , we find that dwd is usually more time - consuming than the svm .",
    "therefore , socp contributes to both the success and unpopularity of dwd .",
    "second , the kernel extension of dwd and the corresponding kernel learning theory are under - developed compared to the kernel svm . although @xcite proposed a version of non - linear dwd by mimicking the kernel trick used for deriving the kernel svm , theoretical justification of such a kernel dwd is still absent . on the contrary ,",
    "the kernel svm as well as the kernel logistic regression @xcite have mature theoretical understandings built upon the theory of reproducing kernel hilbert space ( rkhs ) @xcite .",
    "most learning theories of dwd succeed to @xcite s geometric view of hdlss data and assume that @xmath10 and @xmath0 is fixed , as opposed to the learning theory for the svm where @xmath11 and @xmath12 is fixed .",
    "we are not against the fixed @xmath0 and @xmath10 theory but it would be desirable to develop the canonical learning theory for the kernel dwd when @xmath12 is fixed and @xmath11 .",
    "in fact , how to establish the bayes risk consistency of the dwd and kernel dwd was proposed as an open research problem in the original dwd paper @xcite .",
    "nearly a decade later , the problem still remains open .    in this paper , we aim to resolve the aforementioned issues .",
    "we show that the kernel dwd in a rkhs has the bayes risk consistency property if a universal kernel is used .",
    "this result should convince those who are less familiar with dwd to treat the kernel dwd as a serious competitor to the kernel svm . to popularize the dwd",
    ", it is also important to allow practitioners to easily try dwd collectively with the svm in real applications . to this end",
    ", we develop a novel fast algorithm to solve the linear and kernel dwd by using the majorization - minimization ( mm ) principle . compared with the socp algorithm ,",
    "our new algorithm has multiple advantages .",
    "first , our algorithm is much faster than the socp algorithm . in some examples ,",
    "our algorithm can be several hundred times faster .",
    "second , dwd equipped with our algorithm can be faster than the svm .",
    "third , our algorithm is easier to understand than the socp algorithm , especially for those who are not familiar with semi - definite and second - order - cone programming .",
    "this could help demystify the dwd and hence may increase its popularity . to give a quick demonstration",
    ", we use a simulation example to compare the kernel dwd and the kernel svm .",
    "we drew 10 centers @xmath13 from @xmath14 . for each data point in the positive class , we randomly picked up a center @xmath15 and then generated the point from @xmath16 .",
    "the negative class was assembled in the same way except that 10 centers @xmath17 were drawn from @xmath18 .",
    "for this model the bayes rule is nonlinear . ]",
    "figure  [ fig : svm_dwd ] displays the training data from the simulation model where 100 observations are from the positive class ( plotted as triangles ) and another 100 observations are from the negative class ( plotted as circles ) . we fitted the svm and dwd using gaussian kernels .",
    "we have implemented our new algorithm for dwd in a publicly available r package ` kerndwd ` .",
    "we computed the kernel svm by using the r package ` kernlab ` @xcite .",
    "we recorded their training errors and test errors . from figure",
    "[ fig : svm_dwd ] , we observe that like the kernel svm , the kernel dwd has a test error close to the bayes error , which is consistent with the bayes risk consistency property of the kernel dwd established in section  [ sec : learningtheory0 ] .",
    "notably , the kernel dwd is about three times as fast as the kernel svm in this example .",
    "the rest of the paper is organized as follows .",
    "to be self - contained , we first review the svm and dwd in section  [ sec : review ] .",
    "we then derive the novel algorithm for dwd in section  [ sec : computation ] .",
    "we introduce the kernel dwd in a reproducing kernel hilbert space and establish the learning theory of kernel dwd in section  [ sec : kerdwd ] .",
    "real data examples are given in section  [ sec : real_data ] to compare dwd and the svm .",
    "technical proofs are provided in the appendix .",
    "the introduction of the svm usually begins with its geometric interpretation as a maximum margin classifier @xcite . consider a case when two classes are separable by a hyperplane @xmath19 such that @xmath20 are all non - negative . without loss of generality , we assume that @xmath21 is a unit vector , i.e. , @xmath22 , and we observe that each @xmath23 is equivalent to the euclidean distance between the data point @xmath24 and the hyperplane .",
    "the reason is that @xmath25 and @xmath26 , where @xmath27 is any data point on the hyperplane and @xmath21 is the unit normal vector .",
    "the svm classifier is defined as the optimal separating hyperplane that maximizes the smallest distance of each data point to the separating hyperplane .",
    "mathematically , the svm can be written as the following optimization problem ( for the separable data case ) : @xmath28 the smallest distance @xmath29 is called the _ margin _ , and the svm is thereby regarded as a _ large - margin classifier_.",
    "the data points closest to the hyperplane , i.e. , @xmath30 , are dubbed the _ support vectors_.    in general , the two classes are not separable , and thus @xmath31 can not be non - negative for all @xmath32 . to handle this issue ,",
    "non - negative slack variables @xmath33 , are introduced to ensure all @xmath34 to be non - negative . with these slack variables ,",
    "the optimization problem is generalized as follows , @xmath35    to compute svms , the optimization problem is usually rephrased as an equivalent quadratic programming ( qp ) problem , @xmath36,\\\\ \\text { subject to } & \\;\\;\\ ; y_i(\\beta_0 + { \\boldsymbol } x_i^t{\\boldsymbol } \\beta ) + \\xi_i \\ge 1,\\ \\xi_i \\ge 0 , \\",
    "\\forall i , \\label{eq : quad_svm } \\end{aligned}\\ ] ] and it can be solved by maximizing its lagrange dual function , @xmath37 , \\\\",
    "\\text { subject to } & \\;\\;\\ ; \\mu_i \\ge 0 \\text { and } \\sum_{i=1}^n \\mu_i y_i = 0 .",
    "\\label{eq : dual } \\end{aligned}\\ ] ] by solving , one can show that the solution of has the form @xmath38 @xmath39 being zero only when @xmath24 lies on the support vectors .",
    "one widely used method to extend the linear svm to non - linear classifiers is the kernel method @xcite , which replaces the dot product @xmath40 in the lagrange dual problem with a kernel function @xmath41 , and hence the solution has the form @xmath42 some popular examples of the kernel function @xmath43 include : @xmath44 ( linear kernel ) , @xmath45 ( polynomial kernel ) , and @xmath46 ( gaussian kernel ) , among others",
    ".        distance weighted discrimination was originally proposed by @xcite to resolve the _ data - piling _ issue .",
    "@xcite observed that many data points become support vectors when the svm is applied on the so - called high - dimension - low - sample - size ( hdlss ) data , and @xcite coined the term data - piling to describe this phenomenon .",
    "we delineate it in figure  [ fig : dp ] through a simulation example .",
    "let @xmath47 be a @xmath48-dimension vector .",
    "we generated @xmath49 points ( indexed from @xmath6 to @xmath49 and represented as triangles ) from @xmath50 as the negative class and another @xmath49 points ( indexed from @xmath51 to @xmath52 and represented as circles ) from @xmath53 as the positive class .",
    "we computed @xmath54 and @xmath55 for svm . in the left panel of figure  [ fig : dp ] , we plotted @xmath56 for each data point , and we portrayed the support vectors by solid triangles and circles .",
    "we observe that @xmath57 out of @xmath52 data points become support vectors .",
    "the right panel of figure  [ fig : dp ] corresponds to dwd ( will be defined shortly ) , where data - piling is attenuated .",
    "a real example revealing the data - piling can be seen in figure 1 of @xcite .",
    "are plotted for svm and dwd .",
    "indices 1 to 50 represent negative class ( triangles ) and indices 51 to 100 are for positive class ( circles ) . in the left panel ,",
    "data points belonging to the support vectors are depicted as solid circles and triangles . ]",
    "@xcite viewed  data - piling \" as a drawback of the svm , because the svm classifier is a function of only support vectors .",
    "another popular classifier logistic regression does classification by using all the data points .",
    "however , the classical logistic regression classifier is derived by following the maximum likelihood principle , not based on a nice margin - maximization motivation penalized logistic regression approaches the margin - maximizing hyperplane for the separable data case .",
    "dwd was first proposed in 2002 . ] .",
    "@xcite wanted to have a new method that is directly formulated by a svm - like margin - maximization picture and also uses all data points for classification .",
    "to this end , @xcite proposed dwd which finds a separating hyperplane minimizing the total inverse margins of all the data points : @xmath58,\\\\ \\text { subject to } & \\;\\;\\ ; d_i = y_i(\\omega_0 + { \\boldsymbol } x_i^t{\\boldsymbol } \\omega ) + \\eta_i \\ge 0 , \\ \\eta_i \\ge 0,\\ \\forall i , \\text { and } { \\boldsymbol } \\omega^t { \\boldsymbol } \\omega=1 . \\end{aligned } \\label{eq : nonsep_dwd}\\ ] ]",
    "there has been much work on variants of the standard dwd .",
    "we can only give an incomplete list here .",
    "@xcite introduced the weighted dwd to tackle unequal cost or sample sizes by imposing different weights on two classes . @xcite",
    "extended the binary dwd to the multiclass case .",
    "@xcite proposed the sparse dwd for high - dimensional classification .",
    "in addition , the work connecting dwd with other classifiers , e.g. , svm , includes but not limited to lum @xcite , dwsvm @xcite , and flame @xcite .",
    "@xcite provided a more comprehensive review of the current dwd literature .",
    "@xcite solved the standard dwd by reformulating as a second - order cone programming ( socp ) program @xcite , which has a linear objective , linear constraints , and second - order - cone constraints .",
    "specifically , for each @xmath59 , let @xmath60 , @xmath61 , and then @xmath62 , @xmath63 , and @xmath64 .",
    "hence the original optimization problem becomes @xmath65,\\\\ \\text { subject to } & \\;\\;\\ ; { \\boldsymbol } \\rho - { \\boldsymbol } \\sigma = \\tilde{{\\boldsymbol } y } { \\boldsymbol } x { \\boldsymbol } \\omega + \\omega_0 \\cdot { \\boldsymbol } y + { \\boldsymbol } \\eta , \\\\ & \\;\\;\\ ; \\eta_i \\ge 0 , \\",
    "( \\rho_i ; \\sigma_i , 1 ) \\in s_3 , \\",
    "\\forall i , \\ ( 1 ; { \\boldsymbol } \\omega)\\in s_{p+1 } , \\end{aligned } \\label{eq : primal_dwd}\\ ] ] where @xmath66 is an @xmath67 diagonal matrix with the @xmath59th diagonal element @xmath68 , @xmath69 is an @xmath70 data matrix with the @xmath59th row @xmath71 , and @xmath72 is the form of the second - order cones . after solving @xmath73 and @xmath74 from , a new observation @xmath75",
    "is classified by @xmath76 .",
    "note that the kernel svm was derived from applying the kernel trick to the dual formulation ( [ eq : soln_svm ] ) .",
    "@xcite followed the same approach to consider a version of kernel dwd for achieving non - linear classification .",
    "the dual function of the problem is @xcite @xmath77,\\\\ \\text { subject to } & \\;\\;\\ ; { \\boldsymbol } y^t { \\boldsymbol } \\alpha = 0 , \\ { \\boldsymbol } 0 \\le { \\boldsymbol } \\alpha \\le c\\cdot { \\boldsymbol } 1 , \\end{aligned } \\label{eq : dual_dwd}\\ ] ] where @xmath78 .",
    "note that only uses @xmath79 , which makes it easy to employ the kernel trick to get a nonlinear extension of the linear dwd . for a given kernel function @xmath43 ,",
    "define the kernel matrix as @xmath80 , @xmath81 .",
    "then a kernel dwd can be defined as @xcite @xmath82,\\\\ \\text { subject to } & \\;\\;\\ ; { \\boldsymbol } y^t { \\boldsymbol } \\alpha = 0 , \\ { \\boldsymbol } 0 \\le { \\boldsymbol } \\alpha \\le c\\cdot { \\boldsymbol } 1 .",
    "\\end{aligned } \\label{eq : dual_kerdwd}\\ ] ] to solve ( [ eq : dual_kerdwd ] ) , @xcite used the cholesky decomposition of the kernel matrix , i.e. , @xmath83 and then replaced the predictors @xmath69 in with @xmath84 .",
    "@xcite also carefully discussed several algorithmic issues that ensure the equivalent optimality in and .    * remark 1 . *",
    "two dwd implementations have been published thus far : a matlab software @xcite and an r package ` dwd ` @xcite .",
    "both implementations are based on a matlab socp solver ` sdpt3 ` , which was developed by @xcite .",
    "we notice that the r package ` dwd ` can only compute the linear dwd .    * remark 2 . * to our best knowledge ,",
    "the theoretical justification for the kernel dwd in @xcite is still unclear .",
    "the reason is likely due to the fact that the nonlinear extension is purely algorithmic .",
    "in fact , the bayes risk consistency of dwd was proposed as an open research problem in @xcite .",
    "the kernel dwd considered in this paper can be rigorously justified to have a universal bayes risk consistency property ; see details in section  [ sec : learningtheory0 ] .",
    "@xcite also attempted to replace the reciprocal in the dwd optimization problem with the @xmath85th power ( @xmath86 ) of the inverse distances , and @xcite also used it as the original definition of dwd .",
    "we name the dwd with this new formulation the generalized dwd : @xmath87,\\\\ \\text { subject to } & \\;\\;\\ ; d_i = y_i(\\omega_0 + { \\boldsymbol } x_i^t{\\boldsymbol } \\omega ) + \\eta_i \\ge 0 , \\ \\eta_i",
    "\\forall i , \\text { and } { \\boldsymbol } \\omega^t { \\boldsymbol } \\omega=1 , \\label{eq : gendwd } \\end{aligned}\\ ] ] which degenerates to the standard dwd when @xmath88 .    the first asymptotic theory for dwd and generalized dwd was given in @xcite who presented a novel geometric representation of the hdlss data .",
    "assuming @xmath89 are the data from the positive class and @xmath90 are from the negative class .",
    "@xcite stated that , when the sample size @xmath0 is fixed and the dimension @xmath12 goes to infinity , under some regularity conditions , there exist two constants @xmath91 and @xmath92 such that for each pair of @xmath59 and @xmath93 , @xmath94 as @xmath10 .",
    "this result was applied the results to study several classifiers including the svm and the generalized dwd . for ease presentation",
    "let us consider the equal subgroup size case , i.e. , @xmath95 .",
    "@xcite assumed that @xmath96 the basic conclusion is that when @xmath97 is greater than a threshold that depends on @xmath98 , the misclassification error converges to zero , and when @xmath97 is less than the same threshold , the misclassification error converges to @xmath99 . for more details , see theorem 1 and theorem 2 in @xcite .",
    "@xcite further relaxed the assumptions thereof .    * remark 3 .",
    "* the generalized dwd has not been implemented yet because the socp transformation only works for the standard dwd ( @xmath88 ) , but its extension to handle the general cases is unclear if not impossible .",
    "that is why the current dwd literature only focuses on dwd with @xmath88 .",
    "in fact , the generalized dwd with @xmath100 was proposed as an open research problem in @xcite .",
    "the new algorithm proposed in this paper can easily solve the generalized dwd problem for any @xmath101 ; see section  [ sec : computation ] .",
    "@xcite originally solved the standard dwd by transforming into a socp problem .",
    "this algorithm , however , can not compute the generalized dwd with @xmath100 . in this section",
    ", we propose an entirely different algorithm based on the majorization - minimization ( mm ) principle .",
    "our new algorithm offers a unified solution to the standard dwd and the generalized dwd .",
    "our algorithm begins with a @xmath102 formulation of the dwd .",
    "lemma  [ lm : dwd_loss ] deploys the result .",
    "note that the loss function also lays the foundation of the kernel dwd learning theory that will be discussed in section  [ sec : kerdwd ] .",
    "the generalized dwd classifier in can be written as @xmath103 , where @xmath104 is computed from @xmath105 , \\label{eq : dwdlossopt}\\ ] ] for some @xmath106 , where @xmath107 [ lm : dwd_loss ]    * remark 4 . * the proof of lemma 1 provides the one - to - one mapping between @xmath106 in and @xmath108 in .",
    "write @xmath109 as the solution to .",
    "define @xmath110 considering using @xmath111 , @xmath112,\\\\ \\text { subject to } & \\ ; d_i = y_i(\\omega_0 + { \\boldsymbol } x_i^t{\\boldsymbol } \\omega ) + \\eta_i \\ge 0 , \\",
    "\\eta_i \\ge 0 , \\",
    "\\forall i , \\text { and } { \\boldsymbol } \\omega^t",
    "{ \\boldsymbol } \\omega=1 , \\label{eq : gendwd2 } \\end{aligned}\\ ] ] we have @xmath113 note that @xmath114 , which means that the generalized dwd classifier defined by is equivalent to the generalized dwd classifier defined by .    by lemma  [ lm : dwd_loss ] , we call @xmath115 the generalized dwd loss",
    ". it can be visualized in figure  [ fig : dwdloss ] .",
    "we observe that the generalized dwd loss decreases as @xmath85 increases and it approaches the svm hinge loss function as @xmath116 .",
    "when @xmath88 , the generalized dwd loss becomes @xmath117 we notice that @xmath118 has appeared in the literature @xcite . in this work",
    "we give a unified treatment of all @xmath85 values , not just @xmath88 .",
    ", and the svm hinge loss . ]",
    "we now show how to develop the new algorithm by using the mm principle @xcite .",
    "some recent successful applications of the mm principle can be seen in @xcite , among others .",
    "the main idea of the mm principle is easy to understand .",
    "suppose @xmath119 and we aim to minimize @xmath120 , defined in .",
    "the mm principle finds a majorization function @xmath121 satisfying @xmath122 for any @xmath123 and @xmath124 , and then we generate a sequence @xmath125 by updating @xmath126 via @xmath127 .",
    "we first expose some properties of the generalized dwd loss functions , which give rise to a quadratic majorization function of @xmath120 .",
    "the generalized dwd loss is differentiable everywhere ; its first - order derivative is given below ,",
    "@xmath128    the generalized dwd loss function",
    "@xmath115 has a lipschitz continuous gradient , @xmath129 which further implies a quadratic majorization function of @xmath115 such that @xmath130 for any @xmath131 and @xmath132 .",
    "[ lm : lps ]    denote the current solution by @xmath133 and the updated solution by @xmath119 .",
    "we settle @xmath134 and @xmath135 without abusing notations .",
    "we have that for any @xmath136 , @xmath137 \\\\ & + \\frac{m}{2n } \\sum_{i=1}^n \\left[y_i(\\beta_0 - \\tilde\\beta_0 ) + y_i { \\boldsymbol } x_i^t ( { \\boldsymbol } \\beta - \\tilde{{\\boldsymbol } \\beta})\\right]^2 + \\lambda { \\boldsymbol } \\beta^t { \\boldsymbol } \\beta \\\\ \\equiv & { \\boldsymbol } d(\\beta_0 , { \\boldsymbol } \\beta ) .",
    "\\label{eq : cled } \\end{split}\\ ] ]    we now find the minimizer of @xmath138 .",
    "the gradients of @xmath138 are given as follows : @xmath139{\\boldsymbol } x_i + 2\\lambda { \\boldsymbol } \\beta \\notag\\\\ = & { \\boldsymbol } x^t { \\boldsymbol } z + \\frac{m}{n}(\\beta_0 - \\tilde{\\beta}_0){\\boldsymbol } x^t { \\boldsymbol } 1 + \\frac{m}{n}\\sum_{i=1}^n { \\boldsymbol } x_i { \\boldsymbol } x_i^t ( { \\boldsymbol } \\beta - \\tilde{{\\boldsymbol } \\beta } ) + 2\\lambda { \\boldsymbol } \\beta \\notag\\\\ = & { \\boldsymbol } x^t { \\boldsymbol } z + \\frac{m}{n}(\\beta_0 - \\tilde{\\beta}_0){\\boldsymbol } x^t { \\boldsymbol } 1 + \\left(\\frac{m}{n } { \\boldsymbol } x^t { \\boldsymbol } x + 2\\lambda{\\boldsymbol } i_p\\right ) ( { \\boldsymbol } \\beta - \\tilde{{\\boldsymbol } \\beta } ) + 2\\lambda \\tilde{{\\boldsymbol } \\beta}\\label{eq : grdnt_alp } , \\\\ \\partial\\frac{{\\boldsymbol } d(\\beta_0 , { \\boldsymbol } \\beta)}{\\partial \\beta_0 } = & \\frac{1}{n } \\sum_{i=1}^n v_q'\\left(y_i(\\tilde{\\beta}_0 + { \\boldsymbol } x_i ^t \\tilde{{\\boldsymbol } \\beta})\\right ) y_i + \\frac{m}{n } \\sum_{i=1}^n \\left[(\\beta_0 - \\tilde{\\beta}_0 ) + { \\boldsymbol } x_i^t({\\boldsymbol } \\beta - \\tilde{{\\boldsymbol } \\beta})\\right ]   \\notag\\\\ = & { \\boldsymbol } 1^t { \\boldsymbol } z + m(\\beta_0 - \\tilde{\\beta}_0 ) + \\frac{m}{n } { \\boldsymbol } 1^t { \\boldsymbol } x ( { \\boldsymbol } \\beta - \\tilde{{\\boldsymbol } \\beta } ) .",
    "\\label{eq : grdnt_b0}\\end{aligned}\\ ] ] where @xmath69 is the @xmath70 data matrix with the @xmath59th row @xmath71 , @xmath140 is an @xmath141 vector with the @xmath59th element @xmath142 , and @xmath143 is the vector of ones . setting @xmath144 $ ] to be zeros , we obtain the minimizer of @xmath138 : @xmath145 so far we have completed all the steps of the mm algorithm .",
    "details are summarized in algorithm  [ alg : linear ] .",
    "initialize @xmath146 compute @xmath147 : @xmath148 compute @xmath149 : @xmath150 compute : @xmath151 set @xmath146 = @xmath152    we have implemented algorithm  [ alg : linear ] in an r package ` kerndwd ` , which is publicly available for download on cran .      in this section ,",
    "we show the superior computation performance of our r implementation , ` kerndwd ` , over the two existing implementations , the r package ` dwd ` @xcite and the matlab software @xcite . to avoid confusion",
    ", we henceforth use ` ours ` , ` huang ` , and ` marron ` to denote ` kerndwd ` , ` dwd ` , and the matlab implementation , respectively .",
    "since ` huang ` is incapable of non - linear kernels and the generalized dwd with @xmath153 , we only attend to the linear dwd with @xmath85 fixed to be one .",
    "all experiments were conducted on an intel core i5 m560 ( 2.67 ghz ) processor .    for a fair comparison ,",
    "we study the four numerical examples used in @xcite , except for different sample sizes and dimensions . in each example",
    ", we generate a data set with sample size @xmath154 and dimension @xmath155 .",
    "the responses are always binary ; one half of the data have responses @xmath156 and the other half have @xmath7 .",
    "data in example 1 are generated from gaussian distribution with means of @xmath157 and an identity covariance for @xmath158 classes respectively .",
    "example 2 has 80% of data drawn as example 1 whereas the other 20% from gaussian distributions with means of @xmath159 for @xmath158 classes . in example 3 , 80% of the data are obtained as example 1 as well , while the means of the remaining 20% have the first coordinate replaced by @xmath160 and one randomly chosen coordinate replaced by @xmath161 for @xmath158 classes .",
    "for example 4 , at the first 25 coordinates , the data from @xmath7 class are standard gaussian and the data from @xmath156 class are @xmath162 times standard gaussian ; for both classes , the last 25 coordinates are just the squares of the first 25 .    in each example , we fitted a linear dwd with five different tuning parameter values @xmath163 . after obtaining @xmath164 , we computed @xmath165 ) and the constant @xmath108 in by using remark 4 .",
    "we then used ` huang ` and ` marron ` to compute their solutions .",
    "note that in theory all three implementations should yield identical @xmath166 from table  [ tab : simu ] we observe that ` ours ` took remarkably less computation time than ` huang ` and ` marron ` .",
    "in example 1 , for instance , ` ours ` spent only 0.012 second on average to fit a dwd model , while ` huang ` used 14.525 seconds , and ` marron ` took 2.204 seconds , which were 1210 and 183 times larger , respectively . in all four examples , the timings of ` ours ` were 700 times above faster than the existing r implementation ` huang ` , and also more than 70 times faster than the matlab implementation ` marron ` .",
    "the kernel svm can be derived by using the kernel trick or using the view of non - parametric function estimation in a reproducing kernel hilbert space ( rkhs ) .",
    "much of the theoretical work on the kernel svm is based on the rkhs formulation of svms .",
    "the derivation of the kernel svm in a rkhs is given in @xcite .",
    "we take a similar approach to derive the kernel dwd , as our goal is to establish the kernel learning theory for dwd .",
    "consider @xmath167 , a reproducing kernel hilbert space generated by the kernel function @xmath43 .",
    "the mercer s theorem ensures @xmath43 to have an eigen - expansion @xmath168 , with @xmath169 and @xmath170 .",
    "then the hilbert space @xmath167 is defined as the collection of functions @xmath171 , for any @xmath172 such that @xmath173 , and the inner product is @xmath174 .",
    "given @xmath167 , let the non - linear dwd be written as @xmath175 where @xmath176 is the solution of @xmath177 , \\label{eq : nonlnr_dwd1}\\ ] ] where @xmath115 is the generalized dwd loss .",
    "the representer theorem concludes that the solution of has a finite expansion based on @xmath178 @xcite , @xmath179 and thus @xmath180 consequently , can be paraphrased with matrix notation , @xmath181 , \\label{eq : nonlnr_dwd}\\ ] ] where @xmath182 is the kernel matrix with the @xmath183th element of @xmath184 and @xmath185 is the @xmath59th column of @xmath182 .",
    "* remark 5 .",
    "* we can compare ( [ eq : nonlnr_dwd ] ) to the kernel svm @xcite @xmath186_+ + \\lambda { \\boldsymbol } \\alpha^t { \\boldsymbol } k { \\boldsymbol } \\alpha \\right ] , \\label{eq : optmztn}\\ ] ] where @xmath187_+$ ] is the hinge loss underlying the svm . as shown in figure 3",
    ", the generalized dwd loss takes the hinge loss as its limit when @xmath188 . in general , the generalized dwd loss and the hinge loss look very similar , which suggests that the kernel dwd and the kernel svm equipped with the same kernel have similar statistical behavior .    the procedure for deriving algorithm  [ alg : linear ] for the linear dwd",
    "can be directly adopted to derive an efficient algorithm for solving the kernel dwd .",
    "we obtain the majorization function @xmath189 , @xmath190+\\lambda { \\boldsymbol } \\alpha^t { \\boldsymbol } k { \\boldsymbol } \\alpha \\\\ & & + \\frac{m}{2n } \\sum_{i=1}^n \\left[y_i(\\beta_0 - \\tilde\\beta_0 ) + y_i { \\boldsymbol } k_i^t ( { \\boldsymbol } \\alpha - \\tilde{{\\boldsymbol } \\alpha})\\right]^2 + \\frac{1}{n } \\sum_{i=1}^n v_q\\left(y_i(\\tilde{\\beta}_0 + { \\boldsymbol } k_i ^t \\tilde{{\\boldsymbol } \\alpha})\\right ) \\end{aligned}\\ ] ] and then find the minimizer of @xmath189 which has a closed - form expression .",
    "we opt to omit the details here for space consideration .",
    "algorithm  [ alg : kernel ] summarizes the entire algorithm for the kernel dwd .",
    "initialize @xmath191 compute @xmath147 : @xmath192 compute @xmath149 : @xmath193 compute : @xmath194 set @xmath191 = @xmath195      @xcite formulated the kernel svm as a non - parametric function estimation problem in a reproducing kernel hilbert space and showed that the population minimizer of the svm loss function is the bayes rule , indicating that the svm directly approximates the optimal bayes classifier .",
    "@xcite further coined a name  fisher consistency \" to describe such a result .",
    "the vapnik - chervonenkis ( vc ) analysis @xcite and the margin analysis @xcite have been used to bound the expected classification error of the svm .",
    "@xcite used the so - called leave - one - out analysis @xcite to study a class of kernel machines .",
    "the exisiting theoretical work on the kernel svm provides us a nice road map to study the kernel dwd . in this section",
    "we first elucidate the fisher consistency @xcite of the generalized kernel dwd , and we then establish the bayes risk consistency of the kernel dwd when a universal kernel is employed .    let @xmath196 denote the conditional probability @xmath197 . under the 0 - 1 loss ,",
    "the theoretical optimal bayes rule is @xmath198 .",
    "assume @xmath196 is a measurable function and @xmath199 throughout .",
    "the population minimizer of the expected generalized dwd loss @xmath200 $ ] is @xmath201 , \\label{eq : fisher_min}\\ ] ] where @xmath202 is the indicator function .",
    "the population minimizer @xmath203 has the same sign as @xmath204 .",
    "[ lm : fisher ]    fisher consistency is a property of the loss function .",
    "the interpretation is that the generalized dwd can approach bayes rule with infinite many samples .",
    "we notice that fisher consistency of @xmath118 has been shown before @xcite . in reality",
    "all classifiers are estimated from a finite sample .",
    "thus , a more refined analysis of the actual dwd classifier is needed , and that is what we achieve in the following .    following the convention in the literature , we absorb the intercept into @xmath205 and",
    "present the kernel dwd as follows : @xmath206 .",
    "\\label{eq : ker_dwd}\\ ] ] the ultimate goal is to show that the misclassification error of the kernel dwd approaches the bayes error rate such that we can say the kernel dwd classifier works as well as the bayes rule ( asymptotically speaking ) .",
    "following @xcite , we derive the following lemma .    for a discrimination function @xmath4",
    ", we define @xmath207 .",
    "$ ] assume that @xmath208 is the bayes rule and @xmath209 is the solution of , then @xmath210 where @xmath211 and @xmath212 are defined as follows and @xmath213 is the generalized dwd loss , @xmath214 - e_{{\\boldsymbol } xy}\\bigg[v_q \\left(y\\tilde{f}({\\boldsymbol } x)\\right)\\bigg],\\\\ \\varepsilon_e=\\varepsilon_e(\\hat{f}_n ) & = e_{{\\boldsymbol } xy}\\bigg[v_q \\left(y\\hat{f}_n({\\boldsymbol } x)\\right)\\bigg ] - \\inf_{f\\in{\\mathcal{h}}_k } e_{{\\boldsymbol } xy}\\bigg[v_q(yf({\\boldsymbol } x))\\bigg ] .",
    "\\end{aligned } \\label{eq : two_err}\\ ] ] [ lm : err_bd ]    in the above lemma @xmath215 is the bayes error rate and @xmath216 is the misclassification error of the kernel dwd applied to new data points .",
    "if @xmath217 , we say the classifier is bayes risk consistent .",
    "based on lemma  [ lm : err_bd ] , it suffices to show that both @xmath211 and @xmath212 approach zero in order to demonstrate the bayes risk consistency of the kernel dwd .",
    "note that @xmath211 is deterministic and is called the approximation error . if the rkhs is rich enough then the approximation error can be made arbitrarily small . in the literature ,",
    "the notation of _ universal kernel _",
    "@xcite has been proposed and studied .",
    "suppose @xmath218 is the compact input space of @xmath69 and @xmath219 is the space of all continuous functions @xmath220 .",
    "the kernel @xmath43 is said to be _",
    "universal _ if the function space @xmath221 generated by @xmath43 is dense in @xmath222 , that is , for any positive @xmath223 and any function @xmath224 , there exists an @xmath225 such that @xmath226 .",
    "suppose @xmath209 is the solution of , @xmath221 is induced by a universal kernel @xmath43 , and the sample space @xmath227 is compact .",
    "then we have    * @xmath228 ; * let @xmath229 .",
    "when @xmath230 and @xmath231 , for any @xmath232 , @xmath233    by ( 1 ) and ( 2 ) and we have @xmath234 in probability .",
    "[ lm : thm ]    the gaussian kernel is universal and @xmath235 .",
    "thus theorem  [ lm : thm ] says that the kernel dwd using the gaussian kernel is bayes risk consistent .",
    "this offers a theoretical explanation to the numerical results in figure  [ fig : svm_dwd ] .",
    "in this section , we investigate the performance of ` kerndwd ` on four benchmark data sets : the bupa liver disorder data , the haberman s survival data , the connectionist bench ( sonar , mines vs. rocks ) data , and the vertebral column data .",
    "all the data sets were obtained from uci machine learning repository @xcite .    for comparison purposes , we considered the svm , the standard dwd ( @xmath88 ) and the generalized dwd models with @xmath236 .",
    "we computed all dwd models using our r package ` kerndwd ` and solved the svm using the r package ` kernlab ` @xcite .",
    "we randomly split each data into a training and a test set with a ratio @xmath237 . for each method using the linear kernel",
    ", we conducted a five - folder cross - validation on the training set to tune @xmath106 . for each method using gaussian kernels ,",
    "the pair of @xmath238 was tuned by the five - folder cross - validation .",
    "we then fitted each model with the selected @xmath106 and evaluated its prediction accuracy on the test set .",
    "table  [ tab : realdata ] displays the average timing and mis - classification rates .",
    "we do not argue that either svm or dwd outperforms the other ; nevertheless , two models are highly comparable .",
    "svm models work better on sonar and vertebral data , and dwd performs better on bupa and haberman data . for three out of the four data sets",
    ", the best method uses a gaussian kernel , indicating that linear classifiers may not be adequate in such cases . in terms of timing , ` kerndwd ` runs faster than ` kernlab ` in all these examples .",
    "it is also interesting to see that dwd with @xmath239 can work slightly better than dwd with @xmath88 on bupa and haberman data , although the difference is not significant .",
    "in this paper we have developed a new algorithm for solving the linear generalized dwd and the kernel generalized dwd .",
    "compared with the current state - of - the - art algorithm for solving the linear dwd , our new algorithm is easier to understand , more general , and much more efficient .",
    "dwd equipped with the new algorithm can be computationally more efficient than the svm .",
    "we have established the statistical learning theory of the kernel generalized dwd , showing that the kernel dwd and the kernel svm are comparable in theory .",
    "our theoretical analysis and algorithm do not suggest dwd with @xmath88 has any special merit compared to the other members in the generalized dwd family .",
    "numerical examples further support our theoretical conclusions .",
    "dwd with @xmath88 is called the standard dwd purely due to the fact that it , not other generalized dwds , can be solved by socp when the dwd idea was first proposed . now with our new algorithm and theory , practitioners have the option to explore different dwd classifiers .    in the present paper",
    "we have considered the standard classification problem under the 0 - 1 loss . in",
    "many applications we may face the so - called non - standard classification problems .",
    "for example , observed data may be collected via biased sampling and/or we need to consider unequal costs for different types of mis - classification . @xcite",
    "introduced a weighted dwd to handle the non - standard classification problem , which follows the treatment of the non - standard svm in @xcite .",
    "@xcite defined the weighted dwd as follows , @xmath240 , \\text { subject to } r_i = y_i(\\beta_0 + { \\boldsymbol } x_i^t{\\boldsymbol } \\beta)+\\xi_i \\ge 0 \\text { and } { \\boldsymbol } \\beta^t { \\boldsymbol } \\beta=1 , \\end{aligned } \\label{eq : wdwd}\\ ] ] which can be further generalized to the weighted kernel dwd : @xmath241 .",
    "\\label{eq : kerwtgendwd}\\ ] ] @xcite gave the expressions for @xmath242 for various non - standard classification problems .",
    "@xcite solved the weighted dwd with @xmath88 based on the second - order - cone programming .",
    "the mm procedure for algorithm  [ alg : linear ] and algorithm  [ alg : kernel ] can easily accommodate the weight factors @xmath242 s to solve the weighted dwd and weighted kernel dwd .",
    "we have implemented the weighted dwd in the r package ` kerndwd ` .",
    "* proof of lemma  [ lm : dwd_loss ] *    write @xmath243 and @xmath244 .",
    "the objective function of can be written as @xmath245 .",
    "we next minimize over @xmath246 for every fixed @xmath59 by computing the first - order and the second - order derivatives of @xmath247 : @xmath248 if @xmath249 , then @xmath250 for all @xmath251 , and @xmath252 is the minimizer .",
    "if @xmath253 , then @xmath254 is the minimizer as @xmath255 and @xmath256 .    by plugging in the minimizer @xmath257 into @xmath245",
    ", we obtain @xmath258 where @xmath259 we now simplify .",
    "suppose @xmath260 and @xmath261 .",
    "we define @xmath262 for each @xmath85 , @xmath263 by setting @xmath264 and @xmath265 , we find that becomes @xmath266 which can be further transformed to with @xmath106 and @xmath267 one - to - one correspondent .            if both @xmath273 and @xmath278 , @xmath281 .",
    "it is trivial that @xmath282 by , , and , we prove .",
    "we now prove .",
    "let @xmath283 from , it is not hard to show that @xmath284 is strictly increasing . therefore @xmath285 is a strictly convex function , and its first - order condition , @xmath286 verifies directly .",
    "* proof of lemma  [ lm : fisher ] * given that @xmath287 , we have that @xmath288 \\equiv e_{{\\boldsymbol } x } \\zeta(f({\\boldsymbol } x))$ ] : @xmath289v_q(-f({\\boldsymbol } x))\\\\ & = \\begin{cases } \\eta({\\boldsymbol } x ) \\dfrac{1}{f({\\boldsymbol } x)^q } \\dfrac{q^q}{(q+1)^{q+1 } } + [ 1-\\eta({\\boldsymbol } x)][1+f({\\boldsymbol } x ) ] , & \\text { if } f({\\boldsymbol } x)>\\dfrac{q}{q+1 } , \\\\ \\eta({\\boldsymbol } x)[1-f({\\boldsymbol } x ) ] + [ 1-\\eta({\\boldsymbol } x)][1+f({\\boldsymbol } x ) ] , & \\text { if } -\\dfrac{q}{q+1 } \\le f({\\boldsymbol } x ) \\le \\dfrac{q}{q+1 } , \\\\ \\eta({\\boldsymbol } x)[1-f({\\boldsymbol } x ) ] + [ 1-\\eta({\\boldsymbol } x)]\\dfrac{1}{[-f({\\boldsymbol } x)]^q}\\dfrac{q^q}{(q+1)^{q+1 } } , & \\text { if } f({\\boldsymbol } x)<-\\dfrac{q}{q+1}. \\end{cases } \\end{aligned}\\ ] ] for each given @xmath290 , we take both @xmath291 and @xmath196 as scalars and hereby write them as @xmath4 and @xmath292 respectively .",
    "we then take @xmath293 as a function of @xmath4 and compute the derivative with respect to @xmath4 : @xmath294 we see that ( 1 ) when @xmath295 , @xmath296 only when @xmath297 , and ( 2 ) when @xmath298 , @xmath296 only when @xmath299 . for these two cases",
    ", we also observe that @xmath300 which follows that @xmath301 is the minimizer of @xmath302 . *",
    "proof of lemma  [ lm : err_bd ] *    as @xmath203 was defined in , we see that for each @xmath290 , @xmath303 v_q\\left(-\\tilde{f}({\\boldsymbol } x)\\right)\\\\ & = \\begin{cases } \\eta({\\boldsymbol } x ) + [ 1-\\eta({\\boldsymbol } x)]^{\\frac{1}{q+1}}\\eta({\\boldsymbol } x)^{\\frac{q}{q+1 } } , & \\text { if } \\eta({\\boldsymbol } x ) \\le 1/2,\\\\ 1 - \\eta({\\boldsymbol } x ) + \\eta({\\boldsymbol } x)^{\\frac{1}{q+1}}[1-\\eta({\\boldsymbol } x)]^{\\frac{q}{q+1 } } , & \\text { if } \\eta({\\boldsymbol } x ) > 1/2 ,   \\end{cases}\\\\ & = \\dfrac{1}{2}\\bigg(1-|2\\eta({\\boldsymbol } x)-1|\\bigg ) + \\dfrac{1}{2}\\bigg(1+|2\\eta({\\boldsymbol } x)-1|\\bigg)^{\\frac{1}{q+1}}\\bigg(1-|2\\eta({\\boldsymbol } x)-1|\\bigg)^{\\frac{q}{q+1}}. \\end{aligned}\\ ] ] for @xmath304 $ ] , we define @xmath305 and compute its first - order derivative as follows , @xmath306 + \\left[\\dfrac{q}{2(q+1 ) } + \\dfrac{q}{2(q+1)}\\left(\\dfrac{1+a}{1-a}\\right)^{\\frac{1}{q+1 } } - \\dfrac{q}{q+1}\\right ] \\ge 0 .",
    "\\end{aligned}\\ ] ] hence for each @xmath307 $ ] , @xmath308 . for each @xmath290 , let @xmath309 and we see that @xmath310 by @xmath311 = e_{\\{{\\boldsymbol } x : f({\\boldsymbol } x)\\ge 0\\}}[1-\\eta({\\boldsymbol } x ) ] + e_{\\{{\\boldsymbol } x : f({\\boldsymbol } x)\\le 0\\}}\\eta({\\boldsymbol } x ) , $ ] we obtain @xmath312 +   e_{\\{{\\boldsymbol } x : \\hat{f}_n({\\boldsymbol } x ) \\le 0,\\ f^\\star({\\boldsymbol } x ) > 0\\}}[2\\eta({\\boldsymbol } x)-1]\\\\ & \\le e_{\\{{\\boldsymbol } x : \\hat{f}_n({\\boldsymbol } x)f^\\star({\\boldsymbol } x)\\le 0\\}}|2\\eta({\\boldsymbol } x)-1|\\\\ & \\le \\dfrac{q+1}{q } e_{\\{{\\boldsymbol } x : \\hat{f}_n({\\boldsymbol } x)f^\\star({\\boldsymbol } x)\\le 0\\ } } \\left[1 - \\zeta\\left(\\tilde{f}({\\boldsymbol } x)\\right)\\right ] .",
    "\\end{aligned } \\label{eq : rf}\\ ] ] since @xmath313 and @xmath314 share the same sign , @xmath315 implies that @xmath316 .",
    "when @xmath316 , 0 is between @xmath317 and @xmath314 , and thus indicates that @xmath318 . from , we conclude that @xmath319\\\\ & \\le \\dfrac{q+1}{q } e_{{\\boldsymbol } x } \\left[\\zeta\\left(\\hat{f}_n({\\boldsymbol } x)\\right ) - \\zeta\\left(\\tilde{f}({\\boldsymbol } x)\\right ) \\right]\\\\ & = \\dfrac{q+1}{q }     e_{{\\boldsymbol } xy } \\left[v_q\\left(y\\hat{f}_n({\\boldsymbol } x)\\right ) - v_q\\left(y\\tilde{f}({\\boldsymbol } x)\\right ) \\right]\\\\ & = \\dfrac{q+1}{q}(\\varepsilon_a + \\varepsilon_e ) .",
    "\\end{aligned}\\ ] ]      * part * ( 1 ) .",
    "we first show that when @xmath167 is induced by a universal kernel , the approximation error @xmath320 . by definition ,",
    "we need to show that for any @xmath232 , there exists @xmath321 such that @xmath322    we first use truncation to consider a truncated version of @xmath323 . for",
    "any given @xmath324 , we define @xmath325 we have that @xmath326 where @xmath327\\\\ & -e_{{\\boldsymbol } x : \\eta({\\boldsymbol } x)>1-\\delta}\\left [ \\eta({\\boldsymbol } x ) v_q \\left(\\tilde{f}({\\boldsymbol } x)\\right )    +   ( 1-\\eta({\\boldsymbol } x ) ) v_q\\left(-\\tilde{f}({\\boldsymbol } x)\\right ) \\right],\\\\ \\kappa_- = & e_{{\\boldsymbol } x : \\eta({\\boldsymbol }",
    "x)<\\delta}\\left [ \\eta({\\boldsymbol } x ) v_q(f_\\delta({\\boldsymbol } x ) ) + ( 1-\\eta({\\boldsymbol } x ) ) v_q(-f_\\delta({\\boldsymbol } x ) ) \\right]\\\\ & -e_{{\\boldsymbol } x : \\eta({\\boldsymbol } x)<\\delta } \\left [ \\eta({\\boldsymbol } x ) v_q \\left(\\tilde{f}({\\boldsymbol } x)\\right )    +   ( 1-\\eta({\\boldsymbol } x ) ) v_q\\left(-\\tilde{f}({\\boldsymbol } x)\\right ) \\right ] .",
    "\\end{aligned}\\ ] ] since @xmath328 when @xmath329 , @xmath330\\\\ & -e_{{\\boldsymbol } x : \\eta({\\boldsymbol } x)>1-\\delta}\\left [ \\eta({\\boldsymbol } x ) v_q\\left(\\tilde{f}({\\boldsymbol } x)\\right )    +   ( 1-\\eta({\\boldsymbol } x ) ) v_q\\left(-\\tilde{f}({\\boldsymbol } x)\\right ) \\right]\\\\ = & \\left[\\delta + ( 1-\\delta)^{\\frac{1}{q+1}}\\delta^\\frac{q}{q+1 } \\right ] - e_{{\\boldsymbol } x : \\eta({\\boldsymbol }",
    "x)>1-\\delta } \\left[1-\\eta({\\boldsymbol } x ) + \\eta({\\boldsymbol } x)^{\\frac{1}{q+1}}(1-\\eta({\\boldsymbol } x))^{\\frac{q}{q+1 } } \\right ] .",
    "\\end{aligned}\\ ] ] we notice that @xmath331 is a continuous function in terms of @xmath332 . since @xmath329 implies that @xmath333 , we conclude that for any given @xmath334 , there exists a sufficiently small @xmath335 such that @xmath336 .",
    "we can also obtain @xmath337 in the same spirit .",
    "therefore , @xmath338    by lusin s theorem , there exists a continuous function @xmath339 such that @xmath340 .",
    "notice that @xmath341 . define @xmath342 then @xmath343 as well .",
    "hence @xmath344 where the first inequality comes from the fact that @xmath345 is lipschitz continuous , i.e. , @xmath346 notice that @xmath347 is also continuous .",
    "the definition of the universal kernel implies the existence of a function @xmath321 such that @xmath348 by combining , , and we obtain .    *",
    "part * ( 2 ) .",
    "in this part we bound the estimation error @xmath349 .",
    "note that rkhs has the following reproducing property @xcite : @xmath350 fix any @xmath232 . by the kkt condition of and the representor theorem , we have @xmath351 we define @xmath352}$ ] as the solution of when the @xmath353th observation is excluded from the training data , i.e. , @xmath354 } = \\operatorname*{argmin}_{f \\in { \\mathcal{h}}_k } \\left [ \\dfrac{1}{n}\\sum_{i=1 , i \\neq k}^n v_q \\left ( y_i ( f({\\boldsymbol } x_i ) \\right ) + \\lambda_n||f||^2_{{\\mathcal{h}}_k } \\right ] .",
    "\\label{eq : ker_dwd_delete_k}\\ ] ] by the definition of @xmath352}$ ] and the convexity of @xmath213 , we have @xmath355}({\\boldsymbol } x_i ) \\right ) - \\lambda_n ||\\hat{f}^{[k]}||^2_{\\mathcal{h}_k}\\\\ \\le &   - \\dfrac{1}{n}\\sum_{i=1 , i\\neq k}^{n}v'_q\\left(y_i   \\hat{f}_n({\\boldsymbol } x_i ) \\right ) y_i\\left(\\hat{f}^{[k ] } ( { \\boldsymbol } x_i ) - \\hat{f}_n({\\boldsymbol } x_i)\\right ) + \\lambda_n ||\\hat{f}_n||^2_{\\mathcal{h}_k } -\\lambda_n ||\\hat{f}^{[k]}||^2_{\\mathcal{h}_k}. \\end{aligned}\\ ] ] by the reproducing property , we further have @xmath356 } ( { \\boldsymbol } x ) - \\hat{f}_n({\\boldsymbol } x)\\right\\rangle_{\\mathcal{h}_k } + \\lambda_n ||\\hat{f}_n||^2_{\\mathcal{h}_k } -\\lambda_n ||\\hat{f}^{[k]}||^2_{\\mathcal{h}_k}\\\\ = & - \\dfrac{1}{n}\\sum_{i=1 , i\\neq k}^{n}v'_q\\left(y_i   \\hat{f}_n({\\boldsymbol } x_i ) \\right ) y_i\\left\\langle   k({\\boldsymbol } x_i , { \\boldsymbol } x ) , \\hat{f}^{[k ] } ( { \\boldsymbol } x ) - \\hat{f}_n({\\boldsymbol } x)\\right\\rangle_{\\mathcal{h}_k } \\\\ & - 2 \\lambda_n \\left\\langle \\hat{f}_n({\\boldsymbol } x ) , \\hat{f}^{[k]}({\\boldsymbol } x)-\\hat{f}_n({\\boldsymbol } x ) \\right\\rangle_{\\mathcal{h}_k } - \\lambda_n||\\hat{f}^{[k]}- \\hat{f}_n||^2_{\\mathcal{h}_k}\\\\ = & \\dfrac{1}{n } v'_q\\left(y_k \\hat{f}_n({\\boldsymbol } x_k ) \\right ) y_k \\left\\langle   k({\\boldsymbol } x_k , { \\boldsymbol } x ) , \\hat{f}^{[k ] } ( { \\boldsymbol } x ) - \\hat{f}_n({\\boldsymbol } x)\\right\\rangle_{\\mathcal{h}_k } - \\lambda_n||\\hat{f}^{[k]}- \\hat{f}_n||^2_{\\mathcal{h}_k } , \\end{aligned}\\ ] ] where the equality in the end holds by .",
    "thus , by cauchy - schwartz inequality , @xmath357}- \\hat{f}_n||^2_{\\mathcal{h}_k } \\le v'_q\\left(y_k \\hat{f}_n({\\boldsymbol } x_k ) \\right ) y_k \\left\\langle   k({\\boldsymbol } x_k , { \\boldsymbol } x ) , \\hat{f}^{[k ] } ( { \\boldsymbol } x ) - \\hat{f}_n({\\boldsymbol } x)\\right\\rangle_{\\mathcal{h}_k}\\\\ \\le & \\left|v'_q\\left(y_k \\hat{f}_n({\\boldsymbol } x_k ) \\right ) \\right| ||k({\\boldsymbol } x_k , { \\boldsymbol } x)||_{\\mathcal{h}_k}||\\hat{f}^{[k]}- \\hat{f}_n||_{\\mathcal{h}_k } \\le \\sqrt{k({\\boldsymbol } x_k , { \\boldsymbol } x_k ) } \\cdot || \\hat{f}^{[k]}- \\hat{f}_n||_{\\mathcal{h}_k } , \\end{aligned}\\ ] ] which implies @xmath358}- \\hat{f}_n||_{\\mathcal{h}_k } \\le \\dfrac{\\sqrt{b}}{n\\lambda_n},\\ ] ] where @xmath359 . by the reproducing property",
    ", we have @xmath360}- \\hat{f}_n||^2_{\\mathcal{h}_k } \\le b \\left(\\dfrac{\\sqrt{b}}{n\\lambda_n}\\right)^2 .",
    "\\end{aligned}\\ ] ] by the lipschitz continuity of the dwd loss , we obtain that for each @xmath361 , @xmath362}({\\boldsymbol } x_k ) \\right ) - v_q\\left(y_k\\hat{f}_n({\\boldsymbol } x_k ) \\right ) \\le & |\\hat{f}^{[k]}({\\boldsymbol } x_k ) - \\hat{f}_n({\\boldsymbol } x_k)| \\le \\dfrac{b}{n\\lambda_n } , \\end{aligned}\\ ] ] and therefore , @xmath363}({\\boldsymbol }",
    "x_k ) \\right ) \\le   \\dfrac{1}{n}\\sum_{k=1}^n v_q\\left(y_k\\hat{f}_n({\\boldsymbol } x_k ) \\right ) +   \\dfrac{b}{n\\lambda_n}. \\label{eq : proof_eq100}\\ ] ] let @xmath364 such that @xmath365 by definition of @xmath209 , we have @xmath366 since each data point in @xmath367 is drawn from the same distribution , we have @xmath368}({\\boldsymbol } x_k ) \\right ) \\right ] = \\dfrac{1}{n}\\sum_{k=1}^n e_{{\\boldsymbol } t_n}v_q\\left(y_k\\hat{f}^{[k]}({\\boldsymbol } x_k ) \\right ) = e_{{\\boldsymbol } t_{n-1}}e_{{\\boldsymbol } xy } v_q\\left(y\\hat{f}_{n-1}({\\boldsymbol } x ) \\right ) .",
    "\\quad \\label{eq : proof_eq103 } \\end{aligned}\\ ] ] by combining ",
    "we have @xmath369 by the choice of @xmath370 , we see that there exits @xmath371 such that when @xmath372 we have @xmath373 , @xmath374 , and hence @xmath375 \\le   \\inf_{f\\in\\mathcal{h}_k}e_{{\\boldsymbol } xy } v_q\\left(yf({\\boldsymbol } x ) \\right)+\\epsilon.\\ ] ] because @xmath223 is arbitrary and @xmath376 \\ge   \\inf_{f\\in\\mathcal{h}_k}e_{{\\boldsymbol } xy } v_q\\left(yf({\\boldsymbol } x ) \\right ) $ ] , we have @xmath377 = \\inf_{f\\in\\mathcal{h}_k}e_{{\\boldsymbol } xy } v_q\\left(yf({\\boldsymbol } x ) \\right)$ ] , which equivalently indicates that @xmath378 since @xmath379 , then by markov inequality , we prove part ( 2 ) .",
    "fernndez - delgado , m. , cernadas , e. , barro , s. , and amorim , d. ( 2014 ) , `` do we need hundreds of classifiers to solve real world classification problems ? '' _ the journal of machine learning research _ , 15 , 31333181 .",
    "huang , h. , liu , y. , du .",
    "y. , perou , c. , hayes , d. , todd , m. , and marron , j.s .",
    "( 2013 ) , `` multiclass distance - weighted discrimination , '' _ journal of computational and graphical statistics _ , 22(4 ) , 953969 .",
    "huang , h. , lu , x. , liu , y. , haaland , p. , and marron , j.s .",
    "( 2012 ) , `` r / dwd : distance - weighted discrimination for classification , visualization and batch adjustment , '' _ bioinformatics _ , 28(8 ) , 11821183 .",
    "qiao , x. , zhang , h. , liu , y. , todd , m. , marron , j.s .",
    "( 2010 ) , `` weighted distance weighted discrimination and its asymptotic properties , '' _ journal of american statistical association _ , 105(489 ) , 401414 .",
    "wahba , g. , gu , c. , wang , y. , and campbell , r. ( 1994 ) , `` soft classification , aka risk estimation , via penalized log likelihood and smoothing spline analysis of variance , '' in _",
    "santa fe institute studies in the sciences of complexity - proceeding vol _",
    ", 20 , addison - wesley publishing co , 331331 .",
    "wahba , g. ( 1999 ) , `` support vector machines , reproducing kernel hilbert spaces and the randomized gacv , '' _ advances in kernel methods - support vector learning _ , 6 , 6987 .",
    "wang , b. and zou , h. ( 2015 ) , `` sparse distance weighted discrimination , '' _ journal of computational and graphical statistics _",
    ", forthcoming ."
  ],
  "abstract_text": [
    "<S> distance weighted discrimination ( dwd ) is a margin - based classifier with an interesting geometric motivation . </S>",
    "<S> dwd was originally proposed as a superior alternative to the support vector machine ( svm ) , however dwd is yet to be popular compared with the svm . </S>",
    "<S> the main reasons are twofold . </S>",
    "<S> first , the state - of - the - art algorithm for solving dwd is based on the second - order - cone programming ( socp ) , while the svm is a quadratic programming problem which is much more efficient to solve . </S>",
    "<S> second , the current statistical theory of dwd mainly focuses on the linear dwd for the high - dimension - low - sample - size setting and data - piling , while the learning theory for the svm mainly focuses on the bayes risk consistency of the kernel svm . </S>",
    "<S> in fact , the bayes risk consistency of dwd is presented as an open problem in the original dwd paper . in this work </S>",
    "<S> , we advance the current understanding of dwd from both computational and theoretical perspectives . we propose a novel efficient algorithm for solving dwd , and our algorithm can be several hundred times faster than the existing state - of - the - art algorithm based on the socp . </S>",
    "<S> in addition , our algorithm can handle the generalized dwd , while the socp algorithm only works well for a special dwd but not the generalized dwd . </S>",
    "<S> furthermore , we consider a natural kernel dwd in a reproducing kernel hilbert space and then establish the bayes risk consistency of the kernel dwd . </S>",
    "<S> we compare dwd and the svm on several benchmark data sets and show that the two have comparable classification accuracy , but dwd equipped with our new algorithm can be much faster to compute than the svm .    * </S>",
    "<S> key words : * bayes risk consistency , classification , dwd , kernel methods , mm principle , socp . </S>"
  ]
}