{
  "article_text": [
    "the mean - field theory was first developed as an _ approximation _ to many physical systems in magnetic or disordered materials @xcite . however , it is interesting that they become _ exact _ in many systems in information processing .",
    "the major reason of its success is that when compared with physical systems , these artificial systems have extensive interactions among their components .",
    "hence when one component is considered , the influence of the rest of the system can be regarded as a background satisfying some averaged properties .",
    "learning in large neural networks is a mean - field process since the examples and weights strongly interact with each other during the learning process .",
    "learning is often achieved by defining an energy function which involves a training set of examples .",
    "the energy function is then minimized by a gradient descent process with respect to the weights until a steady state is reached .",
    "each of the many weights is thus dependent on each of the many examples and vice versa .",
    "this makes it an ideal area for applying mean - field theories .",
    "there have been attempts using mean - field theories to describe the dynamics of learning . in batch learning ,",
    "the same restricted set of examples is provided for _ each _ learning step . using the dynamical mean field theory ,",
    "early work has been done on the steady - state behavior and asymptotic time scales in perceptrons with binary weights , rather than the continuous weights of more common interest @xcite .",
    "much benchmarking of batch learning has been done for linear learning rules such as hebbian learning @xcite or adaline learning @xcite .",
    "the work on adaline learning was further extended to the study of linear perceptrons learning nonlinear rules @xcite .",
    "however , not much work has been done on the learning of nonlinear rules with continuous weights . in this respect , it is interesting to note the recent attempts using the _ dynamical replica theory _ @xcite .",
    "it approximates the temporal correlations during learning by instantaneous effective macroscopic variables .",
    "further approximations facilitate results for nonlinear learning",
    ". however , the rigor of these approximations remain to be confirmed in the general case .",
    "batch learning is different from idealized models of _ on - line _ learning of infinite training sets , which has gained much progress @xcite . in this model ,",
    "an independent example is generated for each learning step . since statistical correlations among the examples",
    "can be ignored , the many - body interactions among the examples , and hence among the weights , are absent .",
    "hence they do not address the many - body aspects of the dynamics , which will be discussed here .",
    "nevertheless , this simplification enables the dynamics to be simply described by instantaneous dynamical variables , resulting in a significant reduction in the complexity of analysis , thereby leading to great advances in our understanding of on - line learning . in multilayer perceptrons , for instance",
    ", the persistence of a permutation symmetric stage which retards the learning process was well studied .",
    "subsequent proposals to speed up learning were made , illustrating the usefulness of the on - line approach @xcite .",
    "here we review models of batch learning @xcite where , however , such simplifications are not available . since the same restricted set of examples is recycled during the learning process , there now exist temporal correlations of the parameters in the learning history .",
    "nevertheless , we manage to consider the learning model as a many - body system .",
    "each example makes a small contribution to the learning process , which can be described by linear response terms in a sea of background examples .",
    "two ingredients are important to our theory :    \\(a ) _ the cavity method _  originally developed as the thouless - anderson - palmer approach to magnetic systems and spin glasses @xcite , the method was adopted to learning in perceptrons @xcite , and subsequently extended to the teacher - student perceptron @xcite , the and machine @xcite , the multiclass perceptron @xcite , the committee tree @xcite , bayesian learning @xcite and pruned perceptrons @xcite .",
    "these studies only considered the equilibrium properties of learning , whereas here we are generalizing the method to study the dynamics @xcite .",
    "it uses a self - consistency argument to compare the evolution of the activation of an example when it is absent or present in the training set .",
    "when absent , the activation of the example is called the _ cavity activation _ , in contrast to its generic counterpart when it is included in the training set .    the cavity method yields macroscopic properties identical to the more conventional replica method @xcite",
    "however , since the replica method was originally devised as a technique to facilitate systemwide averages , it provides much less information on the microscopic conditions of the individual dynamical variables .",
    "\\(b ) _ the diagrammatic approach _",
    "to describe the difference between the cavity activation and its generic counterpart of an example , we apply linear response theory and use green s function to describe how the influence of the added example propagates through the learning history .",
    "the green s function is represented by a series of diagrams , whose averages over examples are performed by a set of pairing rules similar to those introduced for adaline learning @xcite , as well as in the dynamics of layered networks @xcite . here",
    "we take a further step and use the diagrams to describe the changes from cavity to generic activations , as was done in @xcite , rather than the evolution of specific dynamical variables in the case of linear rules @xcite .",
    "hence our dynamical equations are widely applicable to any gradient - descent learning rule which minimizes an _ arbitrary _ cost function in terms of the activation .",
    "it fully takes into account the temporal correlations during learning , and is exact for large networks .",
    "the study of learning dynamics should also provide further insights on the steady - state properties of learning . in this respect",
    "we will review the cavity approach to the steady - state behavior of learning , and the microscopic variables satisfy a set of tap equations .",
    "the approach is particularly transparent when the energy landscape is smooth , i.e. , no local minima interfere with the approach to the steady state . however , the picture is valid only when a stability condition ( equivalent to the almeida - thouless condition in the replica method ) is satisfied",
    ". beyond this regime , local minima begin to appear and the energy landscape is roughened . in this case , a similar set of tap equations remains valid . the physical picture has been presented in @xcite ; a more complete analysis is presented here .",
    "the paper is organized as follows . in section 2",
    "we formulate the dynamics of batch learning . in section 3",
    "we introduce the cavity method and the dynamical equations for the macroscopic variables . in section 4",
    "we present simulation results which support the cavity theory . in sections 5 and 6",
    "we consider the steady - state behaviour of learning and generalize the tap equations respectively to the pictures of smooth and rough energy landscapes , followed by a conclusion in section 7 .",
    "the appendices explain the diagrammatic approach in describing the green s function , the fluctuation response relation , and the equations for macroscopic parameters in the picture of rough energy landscapes .",
    "consider the single layer perceptron with @xmath0 input nodes @xmath1 connecting to a single output node by the weights @xmath2 and often , the bias @xmath3 as well . for convenience",
    "we assume that the inputs @xmath4 are gaussian variables with mean 0 and variance 1 , and the output state is a function @xmath5 of the _ activation _",
    "@xmath6 at the output node , where @xmath7 .",
    "the training set consists of @xmath8 examples which map inputs @xmath9 to the outputs @xmath10 . in the case of random examples ,",
    "@xmath11 are random binary variables , and the perceptron is used as a storage device . in the case of teacher - generated examples , @xmath11 are the outputs generated by a teacher perceptron with weights @xmath12 and often , a bias @xmath13 as well , namely @xmath14 ; @xmath15 .",
    "batch learning is achieved by adjusting the weights @xmath2 iteratively so that a certain cost function in terms of the activations @xmath16 and the output @xmath11 of all examples is minimized .",
    "hence we consider a general cost function @xmath17 .",
    "the precise functional form of @xmath18 depends on the adopted learning algorithm . in previous studies ,",
    "@xmath19 in adaline learning @xcite , and @xmath20 in hebbian learning @xcite .    to ensure that the perceptron fulfills the prior expectation of minimal complexity ,",
    "it is customary to introduce a weight decay term . in the presence of noise , the gradient descent dynamics of the weights",
    "is given by @xmath21 where the prime represents partial differentiation with respect to @xmath6 , @xmath22 is the weight decay strength , and @xmath23 is the noise term at temperature @xmath24 with @xmath25 the dynamics of the bias @xmath3 is similar , except that no bias decay should be present according to consistency arguments @xcite , @xmath26",
    "our theory is the dynamical version of the cavity method @xcite .",
    "it uses a self - consistency argument to consider what happens when a new example is added to a training set .",
    "the central quantity in this method is the _ cavity activation _ , which is the activation of a new example for a perceptron trained without that example .",
    "since the original network has no information about the new example , the cavity activation is random . here",
    "we present the theory for @xmath27 , skipping extensions to biased perceptrons . denoting the new example by the label 0 ,",
    "its cavity activation at time @xmath28 is @xmath29 . for large @xmath30",
    ", @xmath31 is a gaussian variable .",
    "its covariance is given by the correlation function @xmath32 of the weights at times @xmath28 and @xmath33 , that is , @xmath34 , where @xmath35 and @xmath36 are assumed to be independent for @xmath37 .",
    "for teacher - generated examples , the distribution is further specified by the teacher - student correlation @xmath38 , given by @xmath39 .",
    "now suppose the perceptron incorporates the new example at the batch - mode learning step at time @xmath33 .",
    "then the activation of this new example at a subsequent time @xmath40 will no longer be a random variable .",
    "furthermore , the activations of the original @xmath41 examples at time @xmath28 will also be adjusted from @xmath42 to @xmath43 because of the newcomer , which will in turn affect the evolution of the activation of example 0 , giving rise to the so - called onsager reaction effects .",
    "this makes the dynamics complex , but fortunately for large @xmath44 , we can assume that the adjustment from @xmath45 to @xmath46 is small , and linear response theory can be applied .",
    "suppose the weights of the original and new perceptron at time @xmath28 are @xmath47 and @xmath48 respectively .",
    "then a perturbation of ( [ original ] ) yields @xmath49 the first term on the right hand side describes the primary effects of adding example 0 to the training set , and is the driving term for the difference between the two perceptrons .",
    "the second term describes the many - body reactions due to the changes of the original examples caused by the added example , and is referred to as the onsager reaction term .",
    "one should note the difference between the cavity and generic activations of the added example .",
    "the former is denoted by @xmath31 and corresponds to the activation in the perceptron @xmath47 , whereas the latter , denoted by @xmath50 and corresponding to the activation in the perceptron @xmath48 , is the one used in calculating the gradient in the driving term of ( [ dyneqn ] ) . since their notations",
    "are sufficiently distinct , we have omitted the superscript 0 in @xmath50 , which appears in the background examples @xmath46 .",
    "the equation can be solved by the green s function technique , yielding @xmath51 where @xmath52 and @xmath53 is the _ weight green s function _ , which describes how the effects of a perturbation propagates from weight @xmath54 at learning time @xmath33 to weight @xmath55 at a subsequent time @xmath28 . in the present context",
    ", the perturbation comes from the gradient term of example 0 , such that integrating over the history and summing over all nodes give the resultant change from @xmath56 to @xmath57",
    ".    for large @xmath30 the weight green s function can be found by the diagrammatic approach explained in appendix a. the result is self - averaging over the distribution of examples and is diagonal , i.e. @xmath58 , where @xmath59 here the bare green s function @xmath60 is given by @xmath61 @xmath62 is the step function . @xmath63 is the _ example green s function _ given by @xmath64 our key to the macroscopic description of the learning dynamics is to relate the activation of the examples to their cavity counterparts , which is known to be gaussian . multiplying both sides of ( [ dressed ] ) and summing over @xmath65 , we have @xmath66 in turn , the covariance of the cavity activation distribution is provided by the fluctuation - response relation explained in appendix b , @xmath67 furthermore , for teacher - generated examples , its mean is related to the teacher - student correlation given by @xmath68 for a given teacher activation @xmath69 of a trained example , the distribution for a set of student activation @xmath70 of the same example at different times is , in the limit of infinitesimal time steps @xmath71 , given by @xmath72c(t , s)^{-1}[h(s)-r(s)y]\\biggr\\ }      \\nonumber\\\\      & & \\prod_t\\delta\\left[x(t)-h(t)-\\delta t\\sum_sg(t , s)g'(x(s))\\right].\\end{aligned}\\ ] ] this can be written in an integral form which is often derived from path integral approaches , @xmath73      -{1\\over 2}\\int dt\\int ds      \\hat h(t)c(t , s)\\hat h(s)\\biggr\\ }      \\nonumber\\\\      & & \\prod_t\\delta\\left[x(t)-h(t)-\\delta t\\sum_sg(t , s)g'(x(s))\\right].\\end{aligned}\\ ] ] the above distributions and parameters are sufficient to describe the progress of learning . some common performance measures used for such monitoring purpose include :    \\(a ) _ training error _",
    "@xmath74 , which is the probability of error for the training examples , and can be determined from the distribution @xmath75 that the student activation of a trained example takes the value @xmath6 for a given teacher activation @xmath69 of the same example .",
    "\\(b ) _ test error _ @xmath76 , which is the probability of error when the inputs @xmath77 of the training examples are corrupted by an additive gaussian noise of variance @xmath78 .",
    "this is a relevant performance measure when the perceptron is applied to process data which are the corrupted versions of the training data .",
    "when @xmath79 , the test error reduces to the training error .",
    "again , it can be determined from @xmath75 , since the noise merely adds a variance of @xmath80 to the activations .",
    "\\(c ) _ generalization error _",
    "@xmath81 for teacher - generated examples , which is the probability of error for an arbitrary input @xmath4 when the teacher and student outputs are compared .",
    "it can be determined from @xmath38 and @xmath82 since , for an example with teacher activation @xmath69 , the corresponding student activation is a gaussian with mean @xmath83 and variance @xmath82 .",
    "the success of the cavity approach is illustrated by the many results presented previously for the adaline rule @xcite .",
    "this is a common learning rule and bears resemblance with the more common back - propagation rule .",
    "theoretically , its dynamics is particularly convenient for analysis since @xmath84 , rendering the weight green s function time translation invariant , i.e. @xmath85 . in this case , the dynamics can be solved by laplace transform .",
    "the closed form of the laplace solution for adaline learning enables us to examine a number of interesting phenomena in learning dynamics .",
    "for example , an _ overtraining _ with respect to the generalization error @xmath81 occurs when the weight decay is not sufficiently strong , i.e. , @xmath81 attains a minimum at a finite learning time before reaching a higher steady - state value .",
    "overtraining of the test error @xmath76 also sets in at a sufficiently weak weight decay , which is approximately proportional to the noise variance @xmath78 .",
    "we also observe an equivalence between average dynamics and noiseless dynamics , namely that a perceptron constructed using the thermally averaged weights is equivalent to the perceptron obtained at a zero noise temperature .",
    "all these results are well confirmed by simulations .    rather than further repeating previous results",
    ", we turn to present results which provide more direct support to the cavity method . in the simulational experiment in fig .",
    "[ greenfig ] , we compare the evolution of two perceptrons @xmath47 and @xmath48 in adaline learning . at the initial state @xmath86 for all @xmath65 , but otherwise their subsequent learning dynamics are exactly identical . hence the total sum @xmath87 provides an estimate for the averaged green s function @xmath88 , which gives an excellent agreement with the green s function obtained from the cavity method .    using the green s function computed from fig .",
    "[ greenfig ] , we can deduce the cavity activation for each example by measuring their generic counterpart from the simulation and substituting back into eq .",
    "( [ generic ] ) . as shown in the histogram in fig .",
    "[ hdisfig](a ) , the cavity activation distribution agrees well with the gaussian distribution predicted by the cavity method , with the predicted mean 0 and variance @xmath82 .",
    "similarly , we show in fig .",
    "[ hdisfig](b ) the distribution of @xmath89 , i.e. , the cavity activation in the direction of the correct teacher output , the cavity method predicts a gaussian distribution with mean @xmath90 and variance @xmath91 .",
    "again , it agrees well with the histogram obtained from simulation .",
    "[ hbt ]    [ hbt ]",
    "when learning reaches a steady state at @xmath92 , the cavity and generic activations approach a constant . hence eq . ( [ generic ] ) reduces to @xmath93 where @xmath94 is called the local susceptibility in @xcite .",
    "hence @xmath95 is a well - defined function of @xmath96 .",
    "( [ steady ] ) can also be obtained by minimizing the change in the steady - state energy function when example 0 is added , which is @xmath97 , the second term being due to the reaction effects of the background examples .",
    "this was shown in @xcite for the case of a constant weight magnitude , but the same could be shown for the case of a constant weight decay .",
    "a self - consistent expression for @xmath94 can be derived from the steady - state behavior of the green s function . since the system becomes translational invariant in time at the steady state , eqs .",
    "( [ wgreen ] ) and ( [ xgreen ] ) can be solved by laplace transform , yielding @xmath98 with @xmath99 . identifying @xmath100 with @xmath94",
    ", we obtain @xmath101 making use of the functional relation between @xmath102 and @xmath103 , we have @xmath104 where @xmath105 is called the nonlocal susceptibility in @xcite .    at the steady state , the fluctuation response relations in eqs .",
    "( [ correlation ] ) and ( [ tscorrelation ] ) yield the self - consistent equations for the student - student and teacher - student correlations , @xmath106 and @xmath107 respectively , namely @xmath108 substituting eqs .",
    "( [ steady ] ) and ( [ sus ] ) , and introducing the cavity activation distributions , we find @xmath109 since @xmath110 is a gaussian distribution with mean @xmath111 and variance @xmath112 , its derivatives with respect to @xmath113 and @xmath114 are @xmath115 and @xmath116 respectively .",
    "this enables us to use integration by parts and eq .",
    "( [ sus ] ) for @xmath105 to obtain @xmath117 hence we have recovered the macroscopic parameters described by the static version of the cavity method in @xcite by considering the steady - state behavior of the learning dynamics .",
    "we remark that the saddle point equations in the replica method also produce identical results , although the physical interpretation is less transparent @xcite .",
    "we can further derive the microscopic equations by noting that at equilibrium for @xmath92 , eq .",
    "( [ original ] ) yields @xmath118 which leads to the set of equations @xmath119 the tap equations are obtained by expressing these equations in terms of the cavity activations via eq .",
    "( [ steady ] ) , @xmath120 the iterative solution of the equation set was applied to the maximally stable perceptron , which yielded excellent agreement with the cavity method , provided that the stability condition discussed below is satisfied @xcite .",
    "however , the agreement is poorer when applied to the committee tree @xcite and the pruned perceptron @xcite , where the stability condition is not satisfied .    to study the stability condition of the cavity solution",
    ", we consider the change in the steady - state solution when example 0 is added to the training set . consider the magnitude of the displaced weight vector @xmath121 .",
    "using either the static or dynamic version of the cavity method , we can show that @xmath122 in order that the change due to the added example is controllable , the stability condition is thus @xmath123 this is identical to the stability condition of the replica - symmetric ansatz in the replica method , the so - called almeida - thouless condition @xcite .    as a corollary ,",
    "when a band gap exists in the activation distribution , the stability condition is violated .",
    "this is because the function @xmath124 becomes discontinuous in this case , implying the presence of a delta - function component in @xmath125 .",
    "such is the case in the nonlinear perceptron trained with noisy examples using the backpropagation algorithm @xcite . for insufficient examples and weak weight decay ,",
    "the activation distribution exhibits a gap for the more difficult examples , i.e. , when the teacher output @xmath69 and the cavity activation @xmath113 has a large difference . as shown in fig .",
    "[ adisfig](a ) , simulational and theoretical predictions of the activation distributions agree well in the stable regime , but the agreement is poor in the unstable regime shown in fig .",
    "[ adisfig](b ) .",
    "hence the existence of band gaps necessitates the picture of a rough energy landscape , as described in the following section .",
    "to consider what happens beyond the stability regime , one has to take into account the rough energy landscape of the learning space . to keep the explanation simple , we consider the learning of examples generated randomly , the case of teacher - generated examples being similar though more complicated",
    "suppose that the original global minimum for a given training set is @xmath126 . in the picture of a smooth energy landscape , the network state shifts perturbatively after adding example 0 , as schematically shown in fig .",
    "[ roughfig](a ) .",
    "in contrast , in the picture of a rough energy landscape , a nonvanishing change to the system is induced , and the global minimum shifts to the neighborhood of the local minimum @xmath127 , as schematically shown in fig .",
    "[ roughfig](b ) .",
    "hence the resultant activation @xmath128 is no longer a well - defined function of the cavity activation @xmath129 .",
    "instead it is a well - defined function of the cavity activation @xmath130 .",
    "nevertheless , one may expect that correlations exist between the states @xmath126 and @xmath127 .",
    "[ hbt ]    let @xmath131 be the correlation between two local minima labelled by @xmath127 and @xmath94 , i.e. @xmath132 . both of them are centred about the global minimum @xmath126 , so that @xmath133 , where @xmath134 . since both states @xmath126 and @xmath127 are determined in the absence of the added example 0 , the correlation @xmath135 as well .",
    "knowing that both @xmath129 and @xmath130 obey gaussian distributions , the cavity activation distribution can be determined if we know the prior distribution of the local minima .    at this point",
    "we introduce the central assumption in the cavity method for rough energy landscapes : we assume that the number of local minima at energy @xmath136 obeys an exponential distribution @xmath137 similar assumptions have been used in specifying the density of states in disordered systems @xcite .",
    "thus the cavity activation distribution is given by @xmath138\\over          \\int dh_0^\\beta           g(h_0^\\beta|h_0^\\alpha)\\exp[-w\\delta e(x(h_0^\\beta ) ) ] } , \\label{hbeta}\\ ] ] where @xmath139 is a gaussian distribution with mean @xmath140 and variance @xmath141 .",
    "@xmath142 is the change in energy due to the addition of example 0 , and is equal to @xmath143 .",
    "the weights @xmath144 are given by @xmath145 self - consistent equations for the macroscopic parameters are derived in appendix c. the results are identical to the first step replica symmetry - breaking solution in the replica method .",
    "it remains to check whether the microscopic equations have been modified due to the roughening of the energy landscape . in terms of the generic activations ,",
    "the microscopic equations are identical to eq .",
    "( [ micro ] ) for each local minimum . in terms of the cavity activations ,",
    "the tap equations are again identical to eq .",
    "( [ tap ] ) , except that the nonlocal susceptibility @xmath105 is now evaluated in the corresponding local minimum .",
    "the cavity activation distribution is no longer a gaussian distribution , but is modified by the density of states in eq .",
    "( [ hbeta ] ) now .",
    "hence the values of @xmath105 and @xmath94 appearing in the tap equations are no longer identical to the case of restricting learning to a single valley .",
    "in summary , we have introduced a general framework for modeling the dynamics of learning based on the cavity method , which is applicable to general learning cost functions , though its tractable solutions are not generally available .",
    "we have verified its validity by simulations of the cavity activation distributions .",
    "the steady - state behavior is seen to be consistent with the static version of the cavity method in the picture of smooth energy landscapes , which is equivalent to the replica symmetric ansatz in the replica method .",
    "this picture is based on the assumption that the dynamics is stable against perturbations , and is manifested in a stability condition equivalent to the almeida - thouless condition in the replica method . beyond the stability regime ,",
    "rough energy landscapes have to be introduced , but the microscopic tap equations remain valid .",
    "there are two interesting issues concerning the extension of the present work .",
    "first , it is interesting to consider how the dynamics is modified in the picture of rough energy landscapes . in this case",
    ", aging effects may appear , and the dynamics may not be translationally invariant in time @xcite .",
    "second , it is interesting to consider whether the analysis remains tractable for nonlinear learning rules . in general , @xmath63 in ( [ xgreen ] ) has to be expanded as a series .",
    "nevertheless , we have shown that the asymptotic dynamics remains tractable for nonlinear learning rules .",
    "for transient dynamics , we may need to consider appropriate approximations .",
    "another applicable area is the case of batch learning with very large learning steps , whose analysis remains simple due to its fast convergence @xcite .",
    "the method can also be applied to on - line learning of restricted sets of examples .",
    "an alternative general theory for learning dynamics is the dynamical replica theory @xcite .",
    "it yields exact results for hebbian learning , but for less trivial cases , the analysis is approximate and complicated by the need to solve replica saddle point equations at every learning instant .",
    "it is hoped that by adhering to an exact formalism , the cavity method can provide more fundamental insights when extended to multilayer networks .",
    "we thank a. c. c. coolen and d. saad for fruitful discussions .",
    "this work was supported by the research grant council of hong kong ( hkust6130/97p and hkust6157/99p ) .",
    "substituting eq . ( [ dressed ] ) into eq .",
    "( [ dyneqn ] ) , we see that the green s function satisfies @xmath146 introducing the bare green s function @xmath60 in eq .",
    "( [ bare ] ) , @xmath147 this equation is represented diagrammatically in fig .",
    "[ diagfig](a ) .",
    "we use a slanted line to represent an example bit , the top and bottom ends of the line corresponding to the example label and node label respectively .",
    "a filled circle represents @xmath148 .",
    "thin and thick lines represent the bare and dressed green s functions @xmath60 and @xmath149 respectively . the iterative solution to eq .",
    "( [ diagram ] ) can be represented by the series of diagrams in fig .",
    "[ diagfig](b ) .",
    "it is convenient to concurrently introduce the _ example _ green s function @xmath63 as shown in fig .",
    "[ diagfig](c ) .",
    "the average over the distribution of example inputs is done by pairing of example or node labels and are represented by dashed lines connecting the vertices above or below the solid lines .",
    "pairing of example and node labels yield factors of 1 and @xmath126 respectively . noting that crossing diagrams do not contribute @xcite ,",
    "the two green s functions can be expressed in terms of the self - energies @xmath150 and @xmath151 , via the dyson s equations in fig .",
    "[ diagfig](d ) .",
    "the self - energies are defined in fig .",
    "[ diagfig](e ) , and are characterized by having the first node or example paired with the last one only .",
    "the self - energies can in turn be expressed in terms of the green s functions as in fig .",
    "[ diagfig](f ) , thus allowing for self - consistent solutions .    after eliminating the self - energies ,",
    "the results of the diagrammatic analysis are given by eqs .",
    "( [ wgreen ] ) and ( [ xgreen ] ) .",
    "in terms of the bare green s function , the solution to the dynamical equation eq .",
    "( [ original ] ) is @xmath152 multiplying both sides by @xmath153 and summing over @xmath65 , we have @xmath154 the correlation between @xmath153 and @xmath155 can be considered by comparing the learning process with another one which is noiseless between @xmath156 and @xmath157 , but is otherwise identical .",
    "denoting the weight of this alternative process by @xmath158 , we have @xmath159 noting that @xmath160 is uncorrelated with @xmath155 , and @xmath161 has a delta function correlation with @xmath155 as in eq .",
    "( [ noise ] ) , we arrive at eq .",
    "( [ correlation ] ) .",
    "similarly , multiplying both sides by @xmath162 and summing over @xmath65 , we arrive at eq .",
    "( [ tscorrelation ] ) .",
    "from eq . ( [ sus ] ) ,",
    "the nonlocal susceptibility is given by @xmath163 where @xmath164 is a gaussian with mean 0 and variance @xmath165 .",
    "the local susceptibility @xmath94 is given by @xmath166 from the fluctuation response relation in eq .",
    "( [ correlation ] ) , we have @xmath167 substituting eqs . ( [ steady ] ) and ( [ sus ] ) , we find @xmath168 the differentiations of @xmath139 with respect to @xmath130 and @xmath129 introduce factors of @xmath169 and @xmath170 respectively , and that of @xmath164 with respect to @xmath129 introduces @xmath171 .",
    "this allows us to use integration by parts and eq .",
    "( [ sus ] ) for @xmath105 to obtain @xmath172      \\int dh_0^\\alpha g(h_0^\\alpha )          { \\int dh_0^\\beta g(h_0^\\beta|h_0^\\alpha)e^{-w\\delta e }          ( x_0^\\beta - h_0^\\beta)^2\\over          \\int dh_0^\\beta g(h_0^\\beta|h_0^\\alpha)e^{-w\\delta e } }          \\nonumber\\\\          & & + \\alpha{w\\over\\gamma}q_0          \\int dh_0^\\alpha g(h_0^\\alpha)\\biggl\\ {          { \\int dh_0^\\beta g(h_0^\\beta|h_0^\\alpha)e^{-w\\delta e }          ( x_0^\\beta - h_0^\\beta)^2\\over          \\int dh_0^\\beta g(h_0^\\beta|h_0^\\alpha)e^{-w\\delta e } }      -\\left [          { \\int dh_0^\\beta g(h_0^\\beta|h_0^\\alpha)e^{-w\\delta e }          ( x_0^\\beta - h_0^\\beta)\\over          \\int dh_0^\\beta g(h_0^\\beta|h_0^\\alpha)e^{-w\\delta e } }          \\right]^2\\biggr\\}. \\label{q1}\\end{aligned}\\ ] ] next we derive an equation for the interstate overlap @xmath131 .",
    "consider the steady - state solution of a local minimum @xmath144 given by eq .",
    "( [ steady ] ) .",
    "multiplying both sides by the weight vector @xmath173 at another local minimum and summing over @xmath65 , we have @xmath174 proceeding as in the case of @xmath165 , we get @xmath175      \\int dh_0^\\alpha g(h_0^\\alpha)\\biggl [          { \\int dh_0^\\beta g(h_0^\\beta|h_0^\\alpha)e^{-w\\delta e }          ( x_0^\\beta - h_0^\\beta)\\over          \\int dh_0^\\beta g(h_0^\\beta|h_0^\\alpha)e^{-w\\delta e } }      \\biggr]^2      \\nonumber\\\\      & & + \\alpha{w\\over\\gamma}q_0          \\int dh_0^\\alpha g(h_0^\\alpha)\\biggl\\ {          { \\int dh_0^\\beta g(h_0^\\beta|h_0^\\alpha)e^{-w\\delta e }          ( x_0^\\beta - h_0^\\beta)^2\\over          \\int dh_0^\\beta g(h_0^\\beta|h_0^\\alpha)e^{-w\\delta e } }      -\\left [          { \\int dh_0^\\beta g(h_0^\\beta|h_0^\\alpha)e^{-w\\delta e }          ( x_0^\\beta - h_0^\\beta)\\over          \\int dh_0^\\beta g(h_0^\\beta|h_0^\\alpha)e^{-w\\delta e } }          \\right]^2\\biggr\\}. \\label{q0}\\end{aligned}\\ ] ] solving eqs .",
    "( [ q1 ] ) and ( [ q0 ] ) , @xmath176 ^ 2 } , \\label{qq1}\\\\          & & \\int dh_0^\\alpha g(h_0^\\alpha)\\left [          { \\int dh_0^\\beta g(h_0^\\beta|h_0^\\alpha)e^{-w\\delta e }          ( x_0^\\beta - h_0^\\beta)\\over          \\int dh_0^\\beta g(h_0^\\beta|h_0^\\alpha)e^{-w\\delta e } }          \\right]^2      = { q_0\\over\\alpha\\left[1+{w\\over\\gamma}(q_1-q_0)\\right]^2}. \\label{qq0}\\end{aligned}\\ ] ] to determine the distribution of local minima , namely the parameter @xmath177 , we introduce a `` free energy '' @xmath178 for @xmath41 examples and @xmath30 input nodes , given by @xmath179de .",
    "\\label{dos}\\ ] ] this `` free energy '' determines the averaged energy of the local minima and should be an extensive quantity , i.e. it should scale as the system size .",
    "cavity arguments enable us to find an expression @xmath180 .",
    "when the number of examples increases by 1 , the density of states for a given @xmath129 are related by @xmath181 using eq .",
    "( [ dos ] ) we obtain , on averaging over @xmath129 , @xmath182 similarly , we may consider a cavity argument for the addition of one input node , expanding the network size from @xmath30 to @xmath183 . skipping the details , the final result is @xmath184 }          -{1\\over 2w}\\ln\\left[1+{w\\over\\gamma}(q_1-q_0)\\right ]          + { \\lambda\\over 2}q_1.\\ ] ]",
    "since @xmath185 is an extensive quantity , @xmath178 should scale as @xmath30 for a given ratio @xmath186 .",
    "this implies @xmath187 when @xmath188 , the density of states reduces to @xmath189 and the global minimum is reached . hence @xmath190 }      \\nonumber\\\\      & & + { 1\\over 2w}\\ln\\left[1+{w\\over\\gamma}(q_1-q_0)\\right ]      + { \\alpha\\over w}\\int dh_0^\\alpha g(h_0^\\alpha )          \\ln\\int dh_0^\\beta g(h_0^\\beta|h_0^\\alpha)e^{-w\\delta e}. \\label{entropy}\\end{aligned}\\ ] ] eqs .",
    "( [ chi ] ) , ( [ gamma ] ) , ( [ qq1 ] ) , ( [ qq0 ] ) and ( [ entropy ] ) form a set of five equations for @xmath105 , @xmath94 , @xmath165 , @xmath131 and @xmath177",
    ".    99 m. plischke and b. bergersen , equilibrium statistical physics , 2nd edition , world scientific , 1994 .",
    "h. horner , z. phys .",
    "b * 86 * , 291 ( 1992 ) ; z. phys .",
    "b * 87 * , 371 ( 1992 ) . a. c. c. coolen and d. saad , in advances in neural information processing systems * 11 * , m. s. kearns , s. a. solla , d. a. cohn , eds . , mit press , 1999 .",
    "h. c. rae , p. sollich and a. c. c. coolen , in advances in neural information processing systems * 11 * , m. s. kearns , s. a. solla , d. a. cohn , eds .",
    ", mit press , 1999 .",
    "j. hertz , a. krogh , and g. i. thorbergssen , j. phys .",
    "a * 22 * , 2133 ( 1989 ) .",
    "s. bs and m. opper , j. phys .",
    "a * 31 * , 4835 ( 1998 ) .",
    "s. bs , phys .",
    "e * 58 * , 833 ( 1998 ) .",
    "m. biehl and h. schwarze , europhys .",
    "* 20 * , 733 ( 1992 ) .",
    "d. saad and s. solla , phys .",
    "* 74 * , 4337 ( 1995 ) .",
    "d. saad and s. solla , phys .",
    "e * 52 * , 4225 ( 1995 ) .",
    "d. saad and m. rattray , phys .",
    "lett . * 79 * , 2578 ( 1997 ) .",
    "d. saad , ed .",
    ", on - line learning in neural networks , cambridge university press , 1998 .",
    "m. rattray , d. saad and s. amari , phys .",
    "lett . * 81 * , 5461 ( 1998 ) .",
    "k. y. m. wong , s. li and y. w. tong , preprint cond - mat/9909004 ( 1999 ) .",
    "s. li and k. y. m. wong , in advances in neural information processing systems * 12 * , s. a. solla , t. k. leen , k .-",
    "mller , eds . , mit press , 2000 .",
    "m. mzard , g. parisi , and m. virasoro , spin glass theory and beyond ( world scientific , singapore , 1987 ) .",
    "m. mzard , j. phys .",
    "a * 22 * , 2181 ( 1989 ) .",
    "m. bouten , j. schietse , and c. van den broeck , phys .",
    "e * 52 * , 1958 ( 1995 ) .",
    "m. griniasty , phys .",
    "e * 47 * , 4496 ( 1993 ) .",
    "f. gerl and u. krey , j. phys .",
    "a * 28 * , 6501 ( 1995 ) .",
    "k. y. m. wong , europhys .",
    "* 30 * , 245 ( 1995 ) .",
    "k. y. m. wong , in advances in neural information processing systems * 9 * , m. c. mozer , m. i. jordan , t. petsche , eds . , mit press , 1997 .",
    "m. opper and o. winther , phys .",
    "* 76 * , 1964 ( 1996 ) .",
    "k. y. m. wong , theoretical aspects of neural computation , k. y. m. wong , i. king , d. y. yeung , eds . ,",
    "springer , singapore , 1998 . k. y. m. wong , c. campbell , and d. sherrington , j. phys .",
    "a * 28 * , 1602 ( 1995 ) .",
    "k. y. m. wong , europhys .",
    "* 38 * , 631 ( 1997 ) .",
    "m. opper , europhys . lett .",
    "* 8 * , 389 ( 1989 ) .",
    "a. krogh and j. a. hertz , j. phys .",
    "a * 25 * , 1135 ( 1992 ) . c. m. bishop , neural networks for pattern recognition ( clarendon press , oxford , 1995 ) .",
    "s. bs , w. kinzel and m. opper , phys .",
    "e * 47 * , 1384 ( 1993 ) .",
    "k. y. m. wong and d. sherrington , phys .",
    "e * 47 * , 4465 ( 1993 ) .",
    "p. luo and k. y. m. wong , preprint cond - mat/0006206 ( 2000 )",
    ". l. f. cugliandolo and j. kurchan , phys .",
    "lett . * 71 * , 173 ( 1993 ) ."
  ],
  "abstract_text": [
    "<S> using the cavity method and diagrammatic methods , we model the dynamics of batch learning of restricted sets of examples . simulations of the green s function and the cavity activation distributions support the theory well . </S>",
    "<S> the learning dynamics approaches a steady state in agreement with the static version of the cavity method . </S>",
    "<S> the picture of the rough energy landscape is reviewed . </S>"
  ]
}