{
  "article_text": [
    "the classical molecular dynamic ( md ) simulation is a widely used computational method for studying the collective behaviour of interacting particles and appears in many diverse areas of science .",
    "the md simulation involves numerically solving the classical newton s equation of motion for each particle starting from a set of initial conditions . the force on each particle in derived from the potential energy functional which contains the physics of the problem .",
    "since the equation of motion of individual particle is solved , the md simulation allows one to obtain details of the static and dynamical properties at the smallest length scale in the problem .",
    "the computational complexity of the md lies in two factors : the number of particles @xmath0 , and the time scale of the phenomenon under simulation .",
    "a typical example is the melting of a simple cubic solid with dimension 50 lattice constants which requires 125,000 atoms simulation .",
    "also , simulation of few pico - seconds of the real time dynamics would correspond to thousands of time steps which can be prohibitive even for large computers .",
    "considerable research has therefore gone into optimizing algorithms for md simulation depending upon the problem in hand . with development of parallel machines , various scalable algorithms for md simulation",
    "has been given which can be used on a system with few processors to hundreds of processors .",
    "these algorithms use the message - passing model of programming for inter - processor communication .",
    "another advantage of writing simulation code using message passing is due to availability of standard library which allows portability to various computing platforms .    in this report",
    ", we describe the implementation of two parallel md algorithms for simulating 2d vortex dynamics in type - ii superconductors .",
    "the algorithm used here have been previously developed and implemented by others for simulating short range md simulation@xcite ( see also ref.@xcite for a review ) . to a large extent , this report is based on the work by plimpton on short - range md@xcite .",
    "the first algorithm uses the particle decomposition method wherein each processor is assigned a fixed number of particles .",
    "each processor calculates the force on the same particles assigned to it irrespective of its spatial position .",
    "the second algorithm uses force decomposition method in which each processor calculates the sub - block of the force matrix .",
    "the parallelization is implemented using mpi and tested on a system with 4 processors .",
    "the purpose of this report is to present the simplicity of the algorithms in implementing and gain in run time compared to sequential algorithms on a single processor .",
    "the report is presented as follows : in section ii , the computational aspect of the problem is described .",
    "section iii explains the two algorithms in detail , whereas section iv compares the runtime for the two algorithms with vectorized algorithm .",
    "a vortex in the mixed phase of a bulk type - ii superconductor is a line - like object which encompasses a quantum of magnetic flux ( @xmath1 where @xmath2 is planck constant , @xmath3 is the velocity of light , and @xmath4 charge of an electron ) threaded by circulating super - current@xcite .",
    "the interaction between two vortices is electromagnetic and hence of long range nature .",
    "though the vortex origin requires a quantum mechanical explanation , most of the physical properties ( magnetisation , transport ) of the mixed phase can be described by considering the vortices as classical interacting objects . in 2d ,",
    "these vortices can be treated as point massless particles obeying the following overdamped equation of motion @xmath5 where @xmath6 represents the co - ordinate of the @xmath7-th particle , and the first summation runs over all @xmath8 particles .",
    "this term represents the repulsive force between two vortices and is given by the first - order bessel function @xmath9 . at distances",
    "@xmath10 , the function decays rapidly as @xmath11 .",
    "the second term represents the interaction between the vortex and the quenched disorder .",
    "the disorder co - ordinates @xmath12 is distributed randomly in the simulation region , and the potential is attractive and short ranged . the last term is the force due to external condition that can include transport current .",
    "the above equation is a paradigm for driven interacting particles in presence of quenched disorder , and appears in other fields such charge - density wave ( cdw ) , colloids , tribology .",
    "computationally , the maximum processor time in a single md time step goes in calculating the two - body force represented by the first term .",
    "even with overhead computational expenses , this could be as high as 95% ( for instance , with 4800 vortices and 2000 point disorder , the first term accounts for 98% of the processor time ) .",
    "therefore , much of the effort has been directed to optimize algorithms for calculating the two - body force .",
    "since the second term is quenched and short ranged , it can be calculated efficiently by binning , and summing the forces over the closest bins .",
    "the third term is a one - body term and hence be efficiently vectorized and takes small fraction of the total computation time .",
    "rest of the report hence deals with the computational aspect of the first term only .",
    "it is useful to consider the positions of all particles as an @xmath0-dimensional vector @xmath13 where each of the @xmath14 holds the @xmath15-dimensional co - ordinate of the @xmath7-th particle ( henceforth , particle is assumed to imply vortex ) .",
    "thus , for the 2d case @xmath16 .",
    "the force @xmath17 between two particles at @xmath14 and @xmath18 can be thought as an element of a matrix of size @xmath19 .",
    "we denote such a force matrix by * f*. the total force on all @xmath0 particles can then be represented as a column vector of length @xmath0 such that the @xmath7-th element is given by @xmath20 .",
    "symbolically , such a vector * f * can be written as @xmath21 the md simulation thus amounts to calculating @xmath22 at each time step which is then used to update the vector @xmath23 .",
    "the computational expense is reflected in the number of elements needed in the summation for @xmath24 . for long - range forces such as coulomb force and gravitational force , each particle interacts with all @xmath25 particles , thus requiring @xmath8 elements of @xmath26 in the summation for @xmath27 .",
    "therefore , a direct calculation of such forces scales as @xmath28 , and becomes prohibitively expensive even for moderate values of @xmath0 . in the last decade , various approximate methods have been developed which allows considerable run time gain .",
    "they include particle - mesh algorithms which scale as @xmath29 where @xmath30 is the number of mesh points , hierarchical methods which goes as @xmath31@xcite , and fast multipole methods which scale as @xmath0@xcite .",
    "but most of these algorithms are difficult to implement and requires particular attention to data structure .    on the other hand ,",
    "direct methods are specifically suitable for short - range forces in which case each particle interacts with few particles within a distance @xmath32 .",
    "typical examples are md simulation employing lennard - jones potential and van - der waals potential . in these simulations",
    ", a significant computational time is spend in searching for the particles within the cut - off distance @xmath32 .",
    "there are two techniques usually employed for doing this .",
    "the first method constructs list of all atoms within a distance @xmath33 where @xmath34 is chosen to be small compared to @xmath32 .",
    "the list is updated after few timesteps such that the total distance traveled by a particle between successive updates is less than @xmath34 .",
    "the two - body force on a particle is found by summing over all particles within the list .",
    "the second method , known as link - cell method , employs binning of particles at every time step into cells of dimension @xmath32 . since this requires @xmath35 operations , the overhead expenses more than compensates for the time required for searching particles within @xmath32 through the list of all @xmath0 particles .",
    "the fastest md algorithms use both these techniques together .    for the interaction under consideration ,",
    "the inter - particle force decays to a value @xmath36 at a distance @xmath37 ( in reduced units ) .",
    "this length scale is considerably larger ( sometimes nearly half the system size ) than usually employed in short - range md , yet does not cover the whole system .",
    "also , since the particle distribution is uniform , this leads to a significant fraction of total number of particles to be within the distance @xmath32 ( for example , for 1200 particles in a square box of length 36 , there are on an average 200 particles within a distance @xmath32 ) .",
    "the construction of neighbour list is therefore of not much advantage and can occupy large memory .",
    "also in the situation where particles are driven , the position changes randomly which would require frequent updating of the list .",
    "if @xmath32 is as large as half the length of the simulation box , binning of particles is not expected to give significant time gain ( on the other hand , this is used for calculating the force due to disorder which is short ranged ) .",
    "the md simulation presented here therefore does not employ both these techniques .",
    "in the last decade , with the arrival of multi - processor machines , much effort has been spend in constructing parallel md algorithms .",
    "it is said that the md computations are inherently parallel in the sense that force calculation and updates can be done simultaneously on all particles .",
    "the aim of parallelizing md is to distribute the task of force calculation evenly among all processors .",
    "the various parallel algorithms are based on two basic methods . in the first class of methods , a pre - determined set of computation is assigned to each processor that remains fixed throughout the simulation time .",
    "one way of doing is to assign a group of particles to each processor , and at any time the processor computes the force for the particles assigned to it no matter where the particles are located in the simulation region .",
    "this is usually referred as * atom decomposition * ( ad ) or * replicated - data * method .",
    "another possible way is to subscribe sub - blocks of the force matrix computation to each processor that has led to * force - decomposition * ( fd ) algorithms .",
    "both these methods are analogous to lagrangian gridding in computational fluid dynamics ( cfd ) where the grid cells ( computational elements or processors ) move along with the fluid ( particles or atoms in md ) .",
    "these two algorithms are discussed in detail below .",
    "the second class of methods uses * spatial decomposition * ( sd ) wherein the simulation region is decomposed into cells and each cell is assigned to a processor .",
    "each processor then computes the force on the particles within the cell region assigned to it .",
    "this is analogous to eulerian gridding in cfd where the grid remains fixed and the fluid moves through it . in this report , we deal only with the implementation of _ atom decomposition _ and _ force - decomposition _ algorithms . due to small number of processors available for computation , the _ spatial decomposition _ algorithm can not be effectively compared with the others .    for the purpose of further discussions ,",
    "we assume following symbols : the column vectors * r * , * f * and the matrix * f * assigned to a processor with rank @xmath38 is designated by the subscript @xmath38 .",
    "the rank for the processors are indexed from 0 to @xmath39 where @xmath40 is the total number of processors .",
    "thus , @xmath41 and @xmath42 are the position vector and total force vector held in processor @xmath38 , and @xmath43 is the sub - block of the force matrix assigned to it .",
    "the number of particles ( and hence the length of the vector @xmath41 and @xmath42 ) assigned to the @xmath38-th processor is represented by @xmath44 . in all the cases considered here @xmath45 . for simplicity",
    ", @xmath0 is chosen to be an integer multiple of @xmath40 , and is not a constraint on the algorithm .      as mentioned earlier , in this method the computation",
    "is carried out by each of the @xmath40 processors for @xmath46 particles which are assigned to it at the start of the simulation .",
    "this amounts to assigning a sub - block of @xmath44 rows of the force matrix @xmath47 to each processors .",
    "the @xmath38-th processor computes matrix elements of the sub - block @xmath48 .",
    "let us assume that each processor has the updated co - ordinates of all particles initially assigned to it . to calculate the force for the next time step , the particles in @xmath38-th processor requires positions of all other particles held by the @xmath39 processors .",
    "this implies that at each time step , each of the processor needs to communicate the co - ordinates of particles held by it to all other processors .",
    "this kind of collective communication can be implemented by calling _ all - to - all _ operation in mpi .",
    "an efficient algorithm to perform the same operation has been given by fox _",
    "et al._@xcite and is called an * _ expand _ * operation ( see appendix a for the details ) .",
    "there are two versions of atom - decomposition method that is discussed in ref.@xcite .",
    "the * a1 * algorithm is a straightforward implementation of the above mentioned procedure .",
    "it does not use the skew - symmetric property of the force matrix @xmath47 , and hence the two - body force @xmath26 is calculated twice ( one by the processor holding @xmath49 in the sub - vector , and the other holding @xmath50 ) . the * a1 * algorithm is outlined below :    1 .   *",
    "expand * vector @xmath41 and construct @xmath23 in each processor .",
    "2 .   * compute * the sub - block @xmath43 using @xmath51 and @xmath23 , and sum the elements into @xmath52 in each processor .",
    "* update * the sub - vector @xmath51 in each processor using @xmath52 .",
    "the communication cost of an algorithm is gauged by the number of messages ( in mpi , a pair of @xmath53 and @xmath54 ) and the volume of data exchanged .",
    "it would be fruitful to see both these factors for the above algorithm in each step .",
    "the first step expands the @xmath41 in each processor to construct the full vector @xmath23 .",
    "this uses @xmath55 vector length for communication to @xmath39 processors , and therefore scales as @xmath56 ( actually @xmath57 ) .",
    "the second step computes the force on all the @xmath55 particles in @xmath51 .",
    "this operation scales as @xmath58 .",
    "and the last operation updates the vector @xmath59 in each processor which also scales as @xmath55 .",
    "thus overall , the communication cost increases as @xmath0 and computation cost as @xmath60 .",
    "symbolically , the steps in the above algorithm can be represented as @xmath61 \\nonumber \\\\",
    "{ \\bf f}_{k } & = & { \\bf r}_{k } \\otimes { \\bf f } \\cdot { \\bf r } \\end{aligned}\\ ] ] where @xmath62 is an expand operation discussed in appendix a. note that @xmath42 is of length @xmath55 .",
    "the second algorithm , denoted by * a2 * , uses newton s law to avoid double computing of the two - body force .",
    "though this leads to extra communication between processors , this can still outperform the algorithm * a1 * even for moderate values of @xmath0 when number of processors @xmath40 is small .",
    "the * a2 * algorithm references the pair - wise interaction only once by using a modified force matrix @xmath63@xcite , which is defined as follows : @xmath64 , except that @xmath65 when @xmath66 is even for @xmath67 , and also when @xmath66 is odd for @xmath68 .",
    "this makes the matrix @xmath63 appear as a checkerboard ( with the diagonal also set to zero ) as shown in fig.1 .",
    "the difference between * a1 * and * a2 * is in the second step .",
    "the force between @xmath7-th and @xmath69-th particle is calculated , and summed into both @xmath7 and @xmath69 positions of the resulting force vector @xmath70 . for _",
    "e.g. _ , from fig.1 , at the end of the force calculation , the processor 0 and 1 will have following elements in the vector @xmath70    processor 0    @xmath71 + @xmath72 + @xmath73 + @xmath74 + @xmath75 + @xmath76 + @xmath77 + @xmath78 + @xmath79 + @xmath80 + @xmath81 + @xmath82       processor 1    @xmath83 + @xmath84 + @xmath85 + @xmath86 + @xmath87 + @xmath88 + @xmath89 + @xmath90 + @xmath91 + @xmath92 + @xmath93 + @xmath94    this means that the vector obtained as an output from the force calculation in each processor is again of length @xmath0 unlike * a1 * where it is of size @xmath44 . the vector @xmath70 is summed across all processors so that total force is obtained as the sub - vector @xmath52 in each processor .",
    "this operation of summing is called * _ fold _ * and can be performed optimally by fox s algorithm@xcite ( see appendix b )",
    ". the * a2 * algorithm can then be enumerated as follows :    1 .",
    "* expand * vector @xmath51 and construct @xmath23 in each processor .",
    "2 .   * compute * the sub - block of @xmath63 using @xmath51 and @xmath23 , and obtain @xmath70 .",
    "fold * vector @xmath70 across all processors and obtain @xmath42 .",
    "* update * the sub - vector @xmath51 in each processor using @xmath52 .",
    "note that there is an additional communication ( in step 3 ) of order @xmath0 as compared to * a1 * at the expense of halving the force calculation .",
    "a gain in runtime is possible if the communication time is a small percentage of overall time which is not the case if @xmath40 is large . as pointed out by plimpton , in that case * a1 * algorithm is faster than * a2*. but for the case where the force calculation is intensive and @xmath40 is small , * a2 * can outperform * a1 * which is indeed observed in our simulation .",
    "symbolically , the * a2 * algorithm becomes @xmath95 \\nonumber \\\\",
    "{ \\bf f}^ { ' } & = & { \\bf r}_{k } \\otimes { \\bf g } \\cdot { \\bf r } \\nonumber \\\\ { \\bf f}_{k } & = & { \\bf r}_{p } [ { \\bf f}^ { ' } ] \\end{aligned}\\ ] ] where @xmath96 is a fold operation discussed in appendix b.    both * a1 * and * a2 * algorithms has communication operations which scales as @xmath0 .",
    "this for large values of @xmath0 can be prohibitive .",
    "the advantage with these algorithms is its simplicity , which allows parallelizing an existing vectorized code ( and this was indeed the reason why parallelization was attempted in our simulation ) .",
    "the force - decomposition ( fd ) algorithm , which was first implemented by hendrickson and plimpton for short - range forces , was motivated by block - decomposition of matrix for solving linear algebra problems .",
    "an important aspect of this algorithm is that the communication between processors scales as @xmath97 rather than @xmath0 for the atom - decomposition algorithm . for the fd algorithm",
    "the processors are assumed to be arranged in cartesian topology ( see ref.@xcite ) .",
    "also , we use the following notations : the cartesian co - ordinates of the processor is denoted by the subscripts @xmath98 ( @xmath69 is a row index , and @xmath38 the column index ) and runs from @xmath99 to @xmath100 .",
    "thus , @xmath101 and @xmath102 are the position and total force vectors held by the processor with co - ordinates @xmath98 .",
    "we also use notation in which row ( column ) index in the subscript is absent which implies that the same vector is held by all processors in the given column ( row ) index . as an example , @xmath103 is held by all processors in the @xmath69-th row , and @xmath104 by all processors in the @xmath38-th column .",
    "this typically arises when a vector is expanded along a row or column of processors ( see appendix a ) .",
    "also , for simplicity we assume that @xmath40 is an even power of @xmath105 , and @xmath0 is an multiple of @xmath40 .",
    "the block decomposition of the force matrix is performed not on @xmath47 but on a new matrix @xmath106 obtained by using vector @xmath23 and permuted vector @xmath107 .",
    "the permuted vector @xmath107 is formed by lining up @xmath101 held by each processors column wise _",
    "i.e. _ , @xmath108 .",
    "note that the vector @xmath23 is distributed as @xmath109 .",
    "both these vectors are shown in fig.2 .",
    "the elements of the force matrix @xmath110 is the force between particles at @xmath14 and @xmath111 .    to calculate the elements of the sub - block @xmath112 , two sub - vectors of length @xmath97 each from @xmath23 and @xmath107 are required . for a processor with cartesian co",
    "- ordinates @xmath98 , these vectors are in fact obtained by expand operation across @xmath69-th row and @xmath38-th column .",
    "these vectors are represented as @xmath103 and @xmath113 . as an example , from fig.2 , the vectors @xmath103 and @xmath113 are given below for all the 4 processors :    processor 0 : @xmath114 = \\{1,2,3,4,5,6 } and @xmath115 = \\{1,2,3,7,8,9 } + processor 1 : @xmath114 = \\{1,2,3,4,5,6 } and @xmath116 = \\{4,5,6,10,11,12 } + processor 2 : @xmath117 = \\{7,8,9,10,11,12 } and @xmath115 = \\{1,2,3,7,8,9 } + processor 3 : @xmath117 = \\{7,8,9,10,11,12 } and @xmath116 = \\{4,5,6,10,11,12 }    the force is then calculated between the elements of @xmath103 and @xmath113 , and summed in a sub - vector @xmath118 which is of length @xmath97 .",
    "the total force sub - vector @xmath102 is obtained by a fold operation on @xmath118 across the @xmath69-th row of processors .",
    "note that the expand and fold operations are along a row or column requiring only @xmath119 processors , unlike ad algorithm where all @xmath40 processors are involved .",
    "this leads to an overall communication cost which scales as @xmath97 only unlike @xmath0 in ad algorithms .",
    "the algorithm is summarised in the following steps :    1 .   *",
    "expand * vector @xmath101 along @xmath69-th row , and construct @xmath103 in each processor .",
    "2 .   * expand * vector @xmath101 along @xmath38-th column , and construct @xmath113 in each processor .",
    "3 .   * compute * the sub - block of @xmath112 using @xmath103 and @xmath113 , and obtain @xmath118 .",
    "* fold * vector @xmath118 across @xmath69-th row of processors and obtain @xmath102 .",
    "* update * the sub - vector @xmath101 in each processor using @xmath102 .",
    "symbolically , the first four steps of the above algorithm can be represented as @xmath120 \\nonumber \\\\   { \\bf r}^{'}_{,k } & = & { \\bf e}_{p_{j,}}[{\\bf r}_{j , k } ] \\nonumber \\\\   { \\bf f}^{'}_{j , k } & = & { \\bf r}_{j , } \\otimes { \\bf f}^ { ' } \\cdot { \\bf r}_{,k}^ { ' } \\nonumber \\\\   { \\bf f}_{j , k } & = & { \\bf r}_{p_{j , } } [ { \\bf f}^{'}_{j , k}]\\end{aligned}\\ ] ] note that there are 3 communication processes , 2 expand , and 1 fold operation involved in this algorithm .",
    "this algorithm does not make use of newton s law and is designated as * f1 * algorithm .",
    "using newton s law leads to halving of the computation at the expense of increased inter - processor communication .",
    "instead of using @xmath47 for obtaining a permuted matrix , one can use the checkerboard matrix @xmath121 described for * a2 * algorithm .",
    "such a matrix is shown in fig . 3 for @xmath122 and @xmath123 .",
    "using * g * modifies the * f1 * algorithm in step 3 and 4 . in step 3 , the elements of @xmath124 are computed and accumulated in @xmath125 position of @xmath126 and @xmath127 position of @xmath128 . for example , these vectors are given explicitly below for the two processors shown in fig.3 :    processor 0    @xmath129 + @xmath130 + @xmath131 + @xmath132 + @xmath133 + @xmath134 + @xmath135 +       @xmath136 + @xmath137 + @xmath138 + @xmath139 + @xmath140 + @xmath141 + @xmath142 +    processor 1    @xmath143 + @xmath144 + @xmath145 + @xmath146 + @xmath147 + @xmath148 + @xmath149 +       @xmath150 + @xmath151 + @xmath152 + @xmath153 + @xmath154 + @xmath155 + @xmath156 +    the @xmath118 is folded across the row of processors to obtain @xmath157 , and @xmath128 is folded across the column of processors to obtain @xmath158 .",
    "the total force @xmath102 is obtained by subtracting @xmath157 from @xmath158 element by element . in the above example",
    ", folding the vector @xmath129 and @xmath159 along the row gives for particle 1    @xmath160    whereas same operation on @xmath136 and @xmath161 along the column gives    @xmath162    the total force on particle 1 is obtained by subtracting the second sum from the first .",
    "this also brings out an important feature that all elements of the force matrix for an @xmath7-th particle is calculated by those processors which share the row and column with the processor to which @xmath7-th particle is assigned .",
    "the * f2 * algorithm can be enumerated as follows :    1 .",
    "* expand * vector @xmath101 along @xmath69-th row , and construct @xmath103 in each processor .",
    "2 .   * expand * vector @xmath101 along @xmath38-th column , and construct @xmath113 in each processor .",
    "3 .   * compute * the sub - block @xmath124 and accumulate them in @xmath118 and @xmath128 .",
    "* fold * vector @xmath118 across @xmath69-th row of processors and and obtain @xmath157 .",
    "* fold * vector @xmath128 across @xmath38-th column of processors and obtain @xmath158 .",
    "* subtract * @xmath158 from @xmath157 , and obtain @xmath102 7 .",
    "* update * the sub - vector @xmath101 in each processor using @xmath102 .",
    "symbolically , the * f2 * algorithm has the following structure @xmath163 \\nonumber \\\\ { \\bf r}^{'}_{,k } & = & { \\bf e}_{p_{j , } } [ { \\bf r}_{j , k } ] \\nonumber \\\\ { \\bf f}^{'}_{j , k } & = & { \\bf r}_{j , } \\otimes { \\bf g}^ { ' } \\cdot { \\bf r}_{,k}^ { ' } \\nonumber \\\\ { \\bf f}^{'}_{j , } & = & { \\bf r}_{p_{j , } } [ { \\bf f}^{'}_{j , k } ] \\nonumber \\\\ { \\bf f}^{''}_{,k } & = & { \\bf r}_{p_{,k } } [ { \\bf f}^{''}_{j , k } ] \\nonumber \\\\ { \\bf f}_{j , k } & = & { \\bf f}^{'}_{j , } - { \\bf f}^{''}_{,k } \\nonumber \\\\\\end{aligned}\\ ] ] the increased communication and computation is evident in step 5 and 6 at the expense of reduced force computation .",
    "we have implemented all the four algorithms * a1 , a2 , f1 * and * f2 * on a 4 processor ( each of 180mhz ) sgi system . the first order differential equation ( 1 )",
    "is solved using a fourth order predictor - corrector scheme .",
    "this requires evaluating the right hand side of eq.(1 ) twice at each time step .",
    "all the calculation is done in double precision . since the force between two particles is bessel function of order 1 ( @xmath9 ) , evaluating the function at each time step",
    "is expensive .",
    "we follow the usual practice of tabulating the function at regular interval over the full range .",
    "the force at any distance is then obtained by interpolating the values in the table .",
    "the program is written in fortran 90 whereas the inter - processor communication is handled using mpi .",
    "for the ad algorithms , the default communicator world is used which does not specify any topology for the processors . for fd algorithms ,",
    "the cartesian topology is imposed leading to a grid of @xmath164 processors .",
    "the number of particles tested ranged from 120 to 4800 .",
    "the idea here is not to simulate large particles but to show the possibility of parallel computation and its advantage even on a small number of processors .",
    "4(a ) shows the cpu time required for a single md time step with increasing density of particles for different algorithms using all 4 processors .",
    "the time for * a1 * algorithm and * f1 * is nearly equal .",
    "note that both these algorithms computes the inter - particle force twice .",
    "the number of communications for * a1 * is less than that for * f1 * at the expense of increased force computation . on the other hand , for * a2 * and * f2 * algorithms , the number of communications are same , and * a2 * outperforms * f2 * due to less computation required .",
    "note that with increasing number of processors , * f2 * is expected to perform better @xcite as communication cost for this algorithm goes as @xmath165 rather than @xmath35 for * a2*.    though , the total number of processors available is too small to obtain the trends with varying number of processors , it would still be instructive to plot the cpu time as a function of @xmath40 for a system of 4800 particles and is shown in fig.4(b ) .",
    "the dotted line represents the ideal speed up , which is obtained by assuming that the time required for a single processor is equally divided among all available processors .",
    "this report shows that considerable gain in run time can be achieved on implementing parallel md simulation even for a moderate number of processors .",
    "though the algorithms that have been implemented here were used by others for short - range md , present work shows that it can be used even for forces that are inter - mediate range with advantage over vectorized algorithms .",
    "as stressed in this report , the main advantage is its simplicity in implementing and requiring few modifications for an already vectorized simulation code for a sequential machine .    for this work ,",
    "the emphasis is simply on the parallelization of the basic md simulation . in the implementation",
    "reported here , a significant fraction of the computing time goes in searching for particles within the cut - off distance for the force .",
    "it is to be seen as to how the conventional methods of md , which uses nearest neighbour tables and link - cell method would enhance the performance .",
    "as stressed before , the problem at hand is that of driven particles in presence of quenched random disorder . at depinning velocities when a fraction of the particles move",
    ", the neighbourhood of a particle changes rapidly with time .",
    "this would call for frequent updating of the nearest neighbour table . also , maintaining such a table for long range force may affect the overall performance which need to be verified .",
    "other approximate methods for long - range force @xcite need to be explored though these algorithms are difficult to implement on distributed memory machines .",
    "the _ expand _ operation on vectors @xmath166 ( the rank of the processor is denoted by subscript @xmath38 ) held by @xmath40 processors results in a vector @xmath167 in all processors such that @xmath168 .",
    "the length of the vector @xmath169 in all the @xmath40 processors is @xmath170 .",
    "thus , vector @xmath169 is constructed by arranging vector @xmath171 in ascending order of the rank of the processors .",
    "symbolically , this operation can be represented as @xmath172 = { \\bf y}\\ ] ] where @xmath173 and @xmath174 .",
    "thus after an _ expand _ operation , the output buffer in each processor has a vector of length which is @xmath40 times the length of the vector held by each processor .    for a 2d grid of processors ,",
    "if @xmath98 is the cartesian co - ordinates , an expand operation across @xmath69-th row can be symbolically represented as @xmath175 = { \\bf y}_{j,}\\ ] ] and that along the @xmath38-th column as @xmath176 = { \\bf y}_{,k}\\ ] ] the expand operation along the row ( column ) returns a vector of length @xmath44 time the number of processors in a column ( row ) .",
    "et al_.@xcite has prescribed an elegant and simple method to perform this operation ( also see ref.@xcite ) . in the first step , each processor allocates memory for the full vector @xmath169 , and the vector @xmath177 is mapped to its position in @xmath169 .",
    "thus , at the beginning of the _ expand _ , @xmath38-th processor has @xmath178 . in the first communication step ,",
    "each processor partners with the neighbouring processor , and exchange non - zero sub - pieces . at the end of this step , @xmath179 for the @xmath38-th and @xmath180-th processors .",
    "every processor thus obtains a contiguous vector of length @xmath181 . in the next step ,",
    "every processor partners with a processor that is two positions away , and exchanges the new piece ( of length @xmath181 ) .",
    "this leads to @xmath182 in each of @xmath183 processors .",
    "this steps leads to each processor acquiring @xmath184 contiguous vector length .",
    "this procedure is repeated till each processor communicates with a processor that is @xmath185 position away , and at the end of which the entire vector * y * is acquired by all the @xmath40 processors .",
    "a1 shows the procedure for @xmath122 and @xmath186 .    for the _ expand _ operation given above ,",
    "there are @xmath187 messages and @xmath188 length of vector exchanged .",
    "this is an optimal value .",
    "allocation of @xmath189 memory in each of the processing element can become prohibitive when @xmath44 and @xmath40 are large , though for most purpose this is never a real concern .",
    "note that the number of processors involved in _ expand _ operation can be a sub - set of total number of available processors ( see fd algorithms ) .",
    "the _ fold _ operation between @xmath40 processors is the inverse of _",
    "expand_. if each processor holds a vector @xmath190 where @xmath170 , the _ fold _ operation between @xmath40 processors leads to @xmath38-th processor acquiring a vector @xmath191 .",
    "the vector @xmath171 is constructed by summing up vector @xmath169 across all @xmath40 processors and allocating the @xmath38-th segment of it .",
    "thus , if @xmath192 where @xmath193 is the @xmath38-th segment of @xmath169 ( of length @xmath44 ) in @xmath69-th processor , after a _ fold _ operation the @xmath38-th processor receives @xmath194 .",
    "symbolically , we represent this operation as @xmath195 = { \\bf x}_{k}\\ ] ] where @xmath196 and @xmath197 .",
    "thus after a _ fold _ operation , the output buffer has a vector of length @xmath198 where @xmath0 is the vector length used for _",
    "folding_.    the _",
    "expand _ algorithm by fox _",
    "et al_. can be inverted to obtain a simple and optimal method to perform _ fold _ operation ( also see ref.@xcite ) . in the first step ,",
    "each processor pairs up with processor that is @xmath185 position away , and exchanges pieces of vector length @xmath199 .",
    "the pieces that are received are the one that the processor must acquire as a final sum .",
    "thus , processor 0 pairs up with @xmath185 and sends vector segment @xmath200 and receives @xmath201 , whereas the @xmath185-th processor sends vector segment @xmath201 and receives @xmath200 . in the next step",
    ", the processor pairs up with the processor at @xmath202 and exchanges @xmath203 length of the new vector @xmath169 , again noting that each processor in the pair receives that part which it requires in the summation .",
    "this procedure is done recursively , and in each step the vector length exchanged is halved .",
    "a2 shows the entire procedure for @xmath122 and @xmath123 .",
    "this algorithm is again an optimal one requiring @xmath204 steps and requiring exchange of @xmath205 length of vector .",
    "s. plimpton , _ j. comp .",
    "_ , * 117 * , 1 , 1995 .",
    "b. a. hendrickson and s. plimpton , _ j. par . and dist . comp .",
    "_ , * 27 * , 15 , 1995 .",
    "d. m. beazley , p. s. lomdahl , n. grnbech - jensen , r. giles , and p. tamayo , in _ annual reviews of computational physics iii _",
    "d. stauffer , world scientific , singapore ( 1995 ) .",
    "m. tinkham , _ introduction to superconductivity _ ,",
    "krieger publishing company , florida , 1980 . j. e. barnes and p. hut , _ nature _ , * 324 * , 446 , 1986 .",
    "l. greengard and v. rokhlin , _ j. comp .",
    "_ , * 73 * , 325 , 1995 . g.c .",
    "fox , m. a. johnson , g. a. jyzenga , s. w. otto , j. k. salmon , and d. a. walker , _ solving problems on concurrent processors : volume 1 _ , prentice hall , englewood cliffs , nj , 1988 . , ver.1.1 , mpi forum , 1995 .",
    "\\(a ) the arrangement of processors and the vectors * r * and @xmath107 for * f1 * and * f2 * algorithms . note that the vector @xmath107 is obtained by expanding first along column and then across the row , whereas the original vector * r * is generated by first expanding across a row and then along a column .",
    "the block - decomposition of the force matrix is shown in ( b ) .    the block - decomposition of the force matrix for * f2 * algorithm .",
    "note the vector on the top @xmath107 and compare it with that in fig.1 .",
    "the permuted vector leads to change in the checkerboard arrangement of the force matrix .",
    "\\(b ) the cpu time taken for a single md step by ad and fd algorithms with increasing number of processors in a simulation of 4800 particles .",
    "the dotted line is the ideal speed up and is the time for a single processor divided by the number of processors .      the _ expand _ operation across 4 processors ranked 0 to 3 for a vector @xmath177 with 3 elements .",
    "in the first step , each processor partners with nearest processor and exchanges sub - pieces",
    ". thus 0 gives elements \\{1,2,3 } , and receives \\{4,5,6 } elements of 1 .",
    "after this , both 0 and 1 has contiguous array of elements \\{1,2,3,4,5,6}. similarly 2 and 3 has \\{7,8,9,10,11,12}. in the second step , the processor pairs with those situated 2 positions away and repeats the exchange of non - zero pieces thus generating the full vector .",
    "there are only @xmath204 steps .",
    "the _ fold _ operation is an inverse of _ expand_. this is shown here for @xmath122 and @xmath177 with 3 elements ( total vector length @xmath123 ) . in the first step , each processor pairs with the processor that is @xmath185 position away and sends @xmath199 sub - piece in which it is * not * a member .",
    "thus , processor 0 gets the first half of 2 and sends it the second half .",
    "the pieces are added up and stored in the same vector ( the stacking of the pieces sidewise in the figure implies addition ) . in the second step ,",
    "the processor partners with those at position @xmath202 away , and receives the @xmath203 piece in which it is a member .",
    "thus 0 processor receives the \\{1,2,3 } of processor 1 , and sends it the elements \\{4,5,6}. note that the elements \\{1,2,3 } is the sum of elements held by processor 2 and 4 .",
    "the _ fold _ operation also requires only @xmath204 steps ."
  ],
  "abstract_text": [
    "<S> this report discusses the implementation of two parallel algorithms on a distributed memory system for studying vortex dynamics in type - ii superconductors . </S>",
    "<S> these algorithms have been previously implemented for classical molecular dynamics simulation with short - range forces@xcite . </S>",
    "<S> the run time for parallel algorithm is tested on a system containing upto 4 processors and compared with that for vectorized algorithm on a single processor for system size ranging from 120 to 4800 vortices .    </S>",
    "<S> = 1.5 in = -1 truein </S>"
  ]
}