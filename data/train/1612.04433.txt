{
  "article_text": [
    "in the first quarter of 2016 , 85% of smartphone sales were devices running android  @xcite .",
    "due to its popularity , cybercriminals have increasingly targeted this ecosystem  @xcite , as malware running on mobile devices can be particularly lucrative  e.g. , allowing attackers to defeat two factor authentication  @xcite or trigger leakage of sensitive information  @xcite . detecting malware on mobile devices",
    "presents additional challenges compared to desktop / laptop computers : smartphones have limited battery life , making it infeasible to use traditional approaches requiring constant scanning and complex computation  @xcite .",
    "therefore , android malware detection is typically performed by google in a centralized fashion , i.e. , by analyzing apps submitted to the play store using a tool called bouncer  @xcite .",
    "however , many malicious apps manage to avoid detection  @xcite , and anyway android s openness enables manufacturers and users to install apps that come from third - party market places , which might not perform any malware checks at all , or anyway not as accurately  @xcite .    as a result ,",
    "the research community has devoted significant attention to malware detection on android .",
    "previous work has often relied on the permissions requested by apps  @xcite , using models built from malware samples .",
    "this strategy , however , is prone to false positives , since there are often legitimate reasons for benign apps to request permissions classified as dangerous  @xcite .",
    "another approach , used by  @xcite , is to perform classification based on api calls frequently used by malware .",
    "however , relying on the most common calls observed during training prompts the need for constant retraining , due to the evolution of malware and the android api alike .",
    "for instance , `` old '' calls are often deprecated with new api releases , so malware developers may switch to different calls to perform similar actions , which affects s effectiveness due to its use of specific calls .    in this paper , we present a novel malware detection system for android that instead relies on the _ sequence _ of _ abstracted _ api calls performed by an app rather than their use or frequency , aiming to capture the behavioral model of the app .",
    "our system , which we call , abstracts api calls to either the _ package _ name of the call ( e.g. , java.lang ) or its source ( e.g. , java , android , google ) , which we refer to as _",
    "family_. abstraction provides resilience to api changes in the android framework as families and packages are added and removed less frequently than single api calls . at the same time",
    ", this does not abstract away the behavior of an app : for instance , packages include classes and interfaces used to perform similar operations on similar objects , so we can model the types of operations from the package name , independently of the underlying classes and interfaces . for example , we know that the java.io package is used for system i / o and access to the file system , even though there are different classes and interfaces provided by the package for such operations .",
    "after abstracting the calls , analyzes the _ sequence _ of api calls performed by an app , aiming to model the app s behavior .",
    "our intuition is that malware may use calls for different operations , and in a different order , than benign apps .",
    "for example , android.media.mediarecorder can be used by any app that has permission to record audio , but the call sequence may reveal that malware only uses calls from this class _ after _ calls to getrunningtasks ( ) , which allows recording conversations  @xcite , as opposed to benign apps where calls from the class may appear in _ any _ order .",
    "relying on the sequence of abstracted calls allows us to model behavior in a more complex way than previous work , which only looked at the presence or absence of certain api calls or permissions  @xcite , while still keeping the problem tractable  @xcite .",
    "builds a statistical model to represent the transitions between the api calls performed by an app , specifically , we model these transitions as markov chains , and use them to extract features and perform classification ( i.e. , labeling apps as benign or malicious ) .",
    "calls are abstracted to either their package or their family , i.e. , operates in one of two modes , depending on the abstraction granularity .",
    "we present a detailed evaluation of both classification accuracy ( using f - measure , precision , and recall ) and runtime performance of , using a dataset of almost 44k apps ( 8.5k benign and 35.5k malware samples ) .",
    "we include a mix of older and newer apps , from october 2010 to may 2016 , verifying that our model is robust to changes in android malware samples and apis . to the best of our knowledge ,",
    "this is the largest malware dataset used to evaluate an android malware detection system in a research paper .",
    "our experimental analysis shows that can effectively model both benign and malicious android apps , and perform an efficient classification on them .",
    "compared to other systems such as  @xcite , our approach allows us to account for changes in the android api , without the need to frequently retrain the classifier .",
    "we show that is able to effectively detect unknown malware samples not only in the `` present , '' ( with f - measure up to 99% ) but also consistently over the years ( i.e. , when the system is trained on older samples and classification performed over newer ones ) , as it keeps an average detection accuracy , evaluated in terms of f - measure , of 86% after one year and 75% after two years ( as opposed to 46% and 42% achieved by  @xcite ) .",
    "we also highlight that when the system is not efficient anymore ( when the test set is newer than the training set by more than two years ) , it is as a result of having low recall , but maintaining high precision .",
    "we also do the opposite , i.e. , training on newer samples and verifying that the system can still detect old malware .",
    "this is particularly important as it shows that can detect newer threats , while still identifying malware samples that have been in the wild for some time .",
    "* summary of contributions .",
    "* first , we introduce a novel approach , implemented in a tool called , to detect android malware by abstracting api calls to their package and family , and using markov chains to model the behavior of the apps through the sequences of api calls .",
    "second , we can detect unknown samples on the same year of training with an f - measure of 99% , but also years after training the system , meaning that does not need continuous re - training .",
    "our system is scalable as we model every single app independently from the others and can easily append app features in a new training set .",
    "finally , compared to previous work  @xcite , achieves significantly higher accuracy with reasonably fast running times , while also being more robust to evolution in malware development and changes in the android api .    * paper organization . * the rest of the paper",
    "is organized as follows .",
    "the next section presents the system , then , section  [ sec : data ] introduces the datasets used in our evaluation ( section  [ sec : evaluation ] ) , while section  [ sec : discussion ] further discusses our results as well as its limitations .",
    "after reviewing related work in section  [ sec : related ] , the paper concludes in section  [ sec : conclusion ] .",
    "we now introduce , a novel system for android malware detection .",
    "characterizes the transitions between different api calls performed by android apps ",
    "i.e. , the sequence of api calls",
    ". it then models these transitions as markov chains , which are in turn used to extract features for machine learning algorithms to classify apps as benign or malicious .",
    "does not actually use the sequence of _ raw _ api calls , but abstracts each call to either its package or its family .",
    "for instance , the api call getmessage ( ) is parsed as : @xmath0    given these two different types of abstractions , we have two modes of operation for , each using one of the types of abstraction .",
    "we test both , highlighting their advantages and disadvantages  in a nutshell , the abstraction to family is more lightweight , while that to package is more fine - grained .",
    "s operation goes through four phases , as depicted in  [ fig : diagram ] .",
    "first , we extract the call graph from each app by using static analysis ( 1 ) , next we obtain the sequences of api calls for the app using all unique nodes in the call graph and associating , to each node , all its child nodes ( 2 ) .",
    "as mentioned , we abstract a call to either its package or family . finally , by building on the sequences , constructs a markov chain model ( 3 ) , with the transition probabilities used as the feature vector to classify the app as either benign or malware using a machine learning classifier ( 4 ) . in the rest of this section ,",
    "we discuss each of these steps in detail .",
    "the first step in is to extract the app s call graph .",
    "we do so by performing static analysis on the app s apk .",
    "specifically , we use a java optimization and analysis framework , soot  @xcite , to extract call graphs and flowdroid  @xcite to ensure contexts and flows are preserved . to better clarify the different steps involved in our system , we employ a `` running example , '' using a real - world malware sample .",
    "specifically ,  [ lst : java ] lists a class extracted from the decompiled apk of malware disguised as a memory booster app ( with package name com.g.o.speed.memboost ) , which executes commands ( rm , chmod , etc . ) as root . to ease presentation",
    ", we focus on the portion of the code executed in the try / catch block .",
    "the resulting call graph of the try / catch block is shown in  [ fig : calls ] .",
    "note that , for simplicity , we omit calls for object initialization , return types and parameters , as well as implicit calls in a method .",
    "additional calls that are invoked when getshell(true ) is called are not shown , except for the add ( ) method that is directly called by the program code , as shown in  [ lst : java ] .",
    "[ lst : java ]      next , we extract the sequences of api calls from the call graph .",
    "since uses static analysis , the graph obtained from soot represents the sequence of functions that are potentially called by the program .",
    "however , each execution of the app could take a specific _ branch _ of the graph and only execute a subset of the calls .",
    "for instance , when running the code in  [ lst : java ] multiple times , the execute method could be followed by different calls , e.g. , getshell ( ) in the try block only or getshell ( ) and then getmessage ( ) in the catch block . in this phase , operates as follows .",
    "first , it identifies a set of entry nodes in the call graph , i.e. , nodes with no incoming edges ( for example , the execute method in the snippet from fig .",
    "[ lst : java ] is the entry node if there is no incoming edge from any other call in the app ) .",
    "then , it enumerates the paths reachable from each entry node .",
    "the sets of all paths identified during this phase constitutes the sequences of api calls which will be used to build a markov chain behavioral model and to extract features ( see  section  [ sec : macha ] ) . *",
    "abstracting calls to families / packages . * rather than analyzing raw api calls",
    ", we build to work at a higher level , and operate in one of two modes by abstracting each call to either its package or family .",
    "this allows the system to be resilient to api changes and achieve scalability .",
    "in fact , our experiments , presented in section  [ sec : data ] , show that , from a dataset of 44k apps , we extract more than 10 million unique api calls , which would result in a very large number of nodes , with the corresponding graphs ( and feature vectors ) being quite sparse . since as we will see the number of features used by is the square of the number of nodes , having more than 10 million nodes would result in an impractical computational cost . when operating in package mode , we abstract an api call to its package name using the list of android packages , which as of api level 24 ( the current version as of september 2016 ) includes 243 packages , as well as 95 from the google api",
    ". moreover , we abstract developer - defined packages ( e.g. , com.stericson.roottools ) as well as obfuscated ones ( e.g. com.fa.a.b.d ) , respectively , as self - defined and obfuscated .",
    "note that we label an api call s package as obfuscated if we can not tell what its class implements , extends , or inherits , due to identifier mangling  @xcite .",
    "when operating in family mode , we abstract to nine possible families , i.e. , android , google , java , javax , xml , apache , junit , json , dom , which correspond to the android . * ,",
    "* , java . * ,",
    "* , org.xml .",
    "* , org.apache .",
    "* , junit .",
    "* , org.json , and org.w3c.dom . * packages .",
    "again , api calls from developer - defined and obfuscated packages are abstracted to families labeled as self - defined and obfuscated , respectively .",
    "overall , there are 340 ( 243@xmath195@xmath12 ) possible packages and 11 ( 9@xmath12 ) families . in  [ fig : sequence ] , we show the sequence of api calls obtained from the call graph in  [ fig : calls ] .",
    "we also report , in square brackets , the family and the package to which the call is abstracted .",
    "next , builds feature vectors , used for classification , based on the markov chains representing the sequences of extracted api calls for an app . before discussing this in detail , we review the basic concepts of markov chains .",
    "markov chains are memoryless models where the probability of transitioning from a state to another only depends on the current state  @xcite .",
    "markov chains are often represented as a set of nodes , each corresponding to a different state , and a set of edges connecting one node to another labeled with the probability of that transition .",
    "the sum of all probabilities associated to all edges from any node ( including , if present , an edge going back to the node itself ) is exactly 1.the set of possible states of the markov chain is denoted as @xmath2 .",
    "if @xmath3 and @xmath4 are two connected states , @xmath5 denotes the probability of transition from @xmath3 to @xmath4 .",
    "@xmath5 is given by the number of occurrences ( @xmath6 ) of state @xmath4 after state @xmath3 , divided by @xmath7 for all states @xmath8 in the chain , i.e. , @xmath9 .",
    "* building the model .",
    "* uses markov chains to model app behavior , by evaluating every transition between calls .",
    "more specifically , for each app , takes as input the sequence of abstracted api calls of that app ",
    "i.e. , packages or families , depending on the selected mode of operation  and builds a markov chain where each package / family is a state and the transitions represent the probability of moving from one state to another . for each markov chain ,",
    "state @xmath10 is the entry point from which other calls are made in a sequence . as an example",
    ",  [ fig : package ] illustrates the two markov chains built using packages and families , respectively , from the sequences reported in  [ fig : sequence ] .",
    "we argue that considering single transitions is more robust against attempts to evade detection by inserting useless api calls in order to deceive signature - based systems ( see section  [ sec : related ] ) .",
    "in fact , considers all possible calls ",
    "i.e. , all the branches originating from a node  in the markov chain , so adding calls would not significantly change the probabilities of transitions between nodes ( specifically , families or packages , depending on the operational mode ) for each app .    * feature extraction .",
    "* next , we use the probabilities of transitioning from one state ( abstracted call ) to another in the markov chain as the feature vector of each app .",
    "states that are not present in a chain are represented as 0 in the feature vector .",
    "also note that the vector derived from the markov chain depends on the operational mode of .",
    "with families , there are 11 possible states , thus 121 possible transitions in each chain , while , when abstracting to packages , there are 340 states and 115,600 possible transitions .",
    "we also apply principal component analysis ( pca )  @xcite , which performs feature selection by transforming the feature space into a new space made of components that are a linear combination of the original features .",
    "the first components contain as much variance ( i.e. , amount of information ) as possible .",
    "the variance is given as percentage of the total amount of information of the original feature space .",
    "we apply pca to the feature set in order to select the principal components , as pca transforms the feature space into a smaller one where the variance is represented with as few components as possible , thus considerably reducing computation / memory complexity .",
    "furthermore , the use of pca could also improve the accuracy of the classification , by taking misleading features out of the feature space , i.e. , those that make the classifier perform worse .",
    "the last step is to perform classification , i.e. , labeling apps as either benign or malware . to this end",
    ", we test using different classification algorithms : random forests  @xcite , 1-nearest neighbor ( 1-nn )  @xcite , 3-nearest neighbor ( 3-nn )  @xcite , and support vector machines ( svm )  @xcite .",
    "each model is trained using the feature vector obtained from the apps in a training sample .",
    "results are presented and discussed in section [ sec : evaluation ] , and have been validated by using 10-fold cross validation .    also note that , due to the different number of features used in family / package modes , we use two distinct configurations for the random forests algorithm .",
    "specifically , when abstracting to families , we use 51 trees with maximum depth 8 , while , with packages , we use 101 trees of maximum depth 64 . to tune random forests we followed the methodology applied in  @xcite .",
    "in this section , we introduce the datasets used in the evaluation of ( presented later in section  [ sec : evaluation ] ) , which include 43,940 apk files  8,447 benign and 35,493 malware samples .",
    "we include a mix of older and newer apps , ranging from october 2010 to may 2016 , as we aim to verify that is robust to changes in android malware samples as well as apis . to the best of our knowledge",
    ", we are leveraging the largest dataset of malware samples ever used in a research paper on android malware detection .    * benign samples .",
    "* our benign datasets consist of two sets of samples : ( 1 ) one , which we denote as oldbenign , includes 5,879 apps collected by playdrone  @xcite between april and november 2013 , and published on the internet archive on august 7 , 2014 ; and ( 2 ) another , newbenign , obtained by downloading the top 100 apps in each of the 29 categories on the google play store as of march 7 , 2016 , using the googleplay - api tool . due to errors encountered while downloading some apps , we have actually obtained 2,843 out of 2,900 apps . note",
    "that 275 of these belong to more than one category , therefore , the ` newbenign ` dataset ultimately includes 2,568 unique apps .",
    "* android malware samples . *",
    "the set of malware samples includes apps that were used to test drebin  @xcite , dating back to october 2010  august 2012 ( 5,560 ) , which we denote as drebin , as well as more recent ones that have been uploaded on the virusshare site over the years . specifically , we gather from virusshare , respectively , 6,228 , 15,417 , 5,314 , and 2,974 samples from 2013 , 2014 , 2015 , and 2016 .",
    "we consider each of these datasets separately for our analysis .    * api calls and call graphs .",
    "* for each app in our datasets , we extract the list of api calls , using androguard , since , as explained in section  [ sec : compare ] , these constitute the features used by  @xcite , against which we compare our system . due to androguard failing to decompress some of the apks , bad crc-32 redundancy checks , and errors during unpacking",
    ", we are not able to extract the api calls for all the samples , but only for 42,923 ( 8,402 benign , 34,521 malware ) out of the 43,940 apps ( 8,447 benign , 35,493 malware ) in our datasets .",
    "also , to extract the call graph of each apk , we use soot .",
    "note that for some of the larger apks , soot requires a non - negligible amount of memory to extract the call graph , so we allocate 16 gb of ram to the java vm heap space .",
    "we find that for 2,027 ( 410 benign + 1,617 malware ) samples , soot is not able to complete the extraction due to it failing to apply the jb phase as well as reporting an error in opening some zip files ( i.e. , the apk ) . the jb phase is used by soot to transform java bytecode into jimple intermediate representation ( the primary ir of soot ) for optimization purposes .",
    "therefore , we exclude these apps in our evaluation and discuss this limitation further in section  [ sec : limits ] . in table",
    "[ table : dataset ] , we provide a summary of our seven datasets , reporting the total number of samples per dataset , as well as those for which we are able to extract the api calls ( second - to - last column ) and the call graphs ( last column ) .",
    "+     +    * characterization of the datasets . * aiming to shed light on the evolution of api calls in android apps , we also performed some measurements over our datasets . in  [ fig : numcalls ] , we plot the cumulative distribution function ( cdf ) of the number of unique api calls in the apps in different datasets , highlighting that newer apps , both benign and malicious , are using more api calls overall than older apps .",
    "this indicates that as time goes by , android apps become more complex .",
    "when looking at the fraction of api calls belonging to specific families , we discover some interesting aspects of android apps developed in different years .",
    "in particular , we notice that api calls to the android family become less prominent as time passes (  [ fig : famandro ] ) , both in benign and malicious datasets , while google calls become more common in newer apps (  [ fig : famgoogle ] ) . in general",
    ", we conclude that benign and malicious apps show the same evolutionary trends over the years .",
    "malware , however , appears to reach the same characteristics ( in terms of level of complexity and fraction of api calls from certain families ) as legitimate apps with a few years of delay .    *",
    "principal component analysis .",
    "* finally , we apply pca to select the two most important pca components .",
    "we plot and compare the positions of the two components for benign (  [ fig : pcabenign ] ) and malicious samples ( fig .",
    "[ fig : pcamalicious ] ) . as",
    "pca combines the features into components , it maximizes the variance of the distribution of samples in these components , thus , plotting the positions of the samples in the components shows that benign apps tend to be located in different areas of the components space , depending on the dataset , while malware samples occupy similar areas but with different densities .",
    "these differences highlight a different behavior between benign and malicious samples , and these differences should also be found by the machine learning algorithms used for classification .",
    "we now present a detailed experimental evaluation of . using the datasets summarized in table  [ table : dataset ] , we perform four sets of experiments :",
    "( 1 ) we analyze the accuracy of s classification on benign and malicious samples developed around the same time ; ( 2 ) we evaluate its robustness to the evolution of malware as well as of the android framework by using older datasets for training and newer ones for testing ( and vice - versa ) ; ( 3 ) we measure s runtime performance to assess its scalability ; and , finally , ( 4 ) we compare against  @xcite , a malware detection system that relies on the frequency of api calls .",
    "when implementing in family mode , we exclude the json and dom families because they are almost never used across all our datasets , and junit , which is primarily used for testing . in package mode , to avoid mislabeling when self - defined apis have `` android '' in the name , we split the android package into its two classes , i.e. , android.r and android.manifest .",
    "therefore , in family mode , there are 8 possible states , thus 64 features , whereas , in package mode , we have 341 states and 116,281 features ( cf .  section  [ sec : macha ] ) . as discussed in section",
    "[ sec : classification ] , we use four different machine learning algorithms for classification  namely , random forests  @xcite , 1-nn  @xcite , 3-nn  @xcite , and svm  @xcite . since both accuracy and speed are worse with svm than with the other three algorithms , we omit results obtained with svm . to assess the accuracy of the classification , we use the standard f - measure metric , i.e. : where precision @xmath11 tp@xmath12tp@xmath1fp@xmath13 and recall @xmath11 tp@xmath12tp@xmath1fn@xmath13 .",
    "tp denotes the number of samples correctly classified as malicious , while fp an fn indicate , respectively , the number of samples mistakenly identified as malicious and benign .",
    "finally , note that all our experiments perform 10-fold cross validation using at least one malicious and one benign dataset from table  [ table : dataset ] .",
    "in other words , after merging the datasets , the resulting set is shuffled and divided into ten equal - size random subsets .",
    "classification is then performed ten times using nine subsets for training and one for testing , and results are averaged out over the ten experiments .",
    "we start our evaluation by measuring how well detects malware by training and testing using samples that are developed around the same time . to this end , we perform 10-fold cross validations on the combined dataset composed of a benign set and a malicious one .",
    "table  [ table : overallresults ] provides an overview of the detection results achieved by on each combined dataset , in the two modes of operation , both with pca features and without .",
    "the reported f - measure , precision , and recall scores are the ones obtained with random forest , which generally performs better than 1-nn and 3-nn",
    ".    * family mode .",
    "* in  [ fig : fm10fam ] , we report the f - measure when operating in family mode for random forests , 1-nn and 3-nn . the f - measure is always at least 88% with random forests , and , when tested on the 2014 ( malicious ) dataset , it reaches 98% . with some datasets , performs slightly better than with others . for instance , with the 2014 malware dataset , we obtain an f - measure of 92% when using the oldbenign dataset and 98% with newbenign . in general , lower f - measures are due to increased false positives since recall is always above 95% , while precision might be lower , also due to the fact that malware datasets are larger than the benign sets .",
    "we believe that this follows the evolutionary trend discussed in section  [ sec : data ] : while both benign and malicious apps become more complex as time passes , when a new benign app is developed , it is still possible to use old classes or re - use code from previous versions and this might cause them to be more similar to old malware samples .",
    "this would result in false positives by .",
    "in general , performs better when the different characteristics of malicious and benign training and test sets are more predominant , which corresponds to datasets occupying different positions of the feature space . * package mode .",
    "* when runs in package mode , the classification performance improves , ranging from 92% f - measure with 2016 and newbenign to 99% with 2014 and newbenign , using random forests .",
    "[ fig : fm10pac ] reports the f - measure of the 10-fold cross validation experiments using random forests , 1-nn , and 3-nn ( in package mode ) .",
    "the former generally provide better results also in this case .    with some datasets ,",
    "the difference in performance between the two modes of operation is more noticeable : with drebin and oldbenign , and using random forests , we get 95% f - measure in package mode compared to 88% in family mode .",
    "these differences are caused by a lower number of false positives in package mode .",
    "recall remains high , resulting in a more balanced system overall .",
    "in general , abstracting to packages rather than families provides better results as the increased granularity enables identifying more differences between benign and malicious apps . on the other hand",
    ", however , this likely reduces the efficiency of the system , as many of the states deriving from the abstraction are used a only few times .",
    "the differences in time performance between the two modes are analyzed in details in section  [ sub : timeeval ] .        * using pca .",
    "* as discussed in section  [ sec : macha ] , pca transforms large feature spaces into smaller ones , thus it can be useful to significantly reduce computation and , above all , memory complexities of the classification task . when operating in package mode , pca is particularly beneficial , since originally has to operate over 116,281 features .",
    "therefore , we compare results obtained using pca by fixing the number of components to 10 and checking the quantity of variance included in them . in package mode",
    ", we observe that only 67% of the variance is taken into account by the 10 most important pca components , whereas , in family mode , at least 91% of the variance is included by the 10 pca components .",
    "as shown in table  [ table : overallresults ] , the f - measure obtained using random forests and the pca components sets derived from the family and package features is only slightly lower ( up to 4% ) than using the full feature set .",
    "we note that lower f - measures are caused by a uniform decrease in both precision and recall .",
    "as android evolves over the years , so do the characteristics of both benign and malicious apps .",
    "such evolution must be taken into account when evaluating android malware detection systems , since their accuracy might significantly be affected as newer apis are released and/or as malicious developers modify their strategies in order to avoid detection . evaluating this aspect constitutes one of our research questions , and one of the reasons why our datasets span across multiple years ( 20102016 ) .",
    "as discussed in section  [ sec : extraction ] , relies on the sequence of api calls extracted from the call graphs and abstracted at either the package or the family level .",
    "therefore , it is less susceptible to changes in the android api than other classification systems such as  @xcite and drebin  @xcite . since these rely on the use , or the frequency , of certain api calls to classify malware vs benign samples , they need to be retrained following new api releases . on the contrary",
    ", retraining is not needed as often with , since families and packages represent more abstract functionalities that change less over time .",
    "consider , for instance , the android.os.health package : released with api level 24 , it contains a set of classes helping developers track and monitor system resources .",
    "classification systems built before this release  as in the case of  @xcite ( released in 2013 , when android api was up to level 20 )  need to be retrained if this package is more frequently used by malicious apps than benign apps , while only needs to add a new state to its markov chain when operating in package mode , while no additional state is required when operating in family mode . to verify this hypothesis , we test using older samples as training sets and newer ones as test sets .",
    "[ fig : fmfutfam ] reports the f - measure of the classification in this setting , with operating in family mode .",
    "the x - axis reports the difference in years between training and test data .",
    "we obtain 86% f - measure when we classify apps one year older than the samples on which we train .",
    "classification is still relatively accurate , at 75% , even after two years .",
    "then , from  [ fig : fmfutpac ] , we observe that the f - measure does not significantly change when operating in package mode . both modes of operations are affected by one particular condition , already discussed in section [ sec : data ] : in our models , benign datasets seem to `` anticipate '' malicious ones by 12 years in the way they use certain api calls . as a result",
    ", we notice a drop in accuracy when classifying future samples and using drebin ( with samples from 2010 to 2012 ) or 2013 as the malicious training set and oldbenign ( late 2013/early 2014 ) as the benign training set .",
    "more specifically , we observe that correctly detects benign apps , while it starts missing true positives and increasing false negatives  i.e. , achieving lower recall .",
    "we also set to verify whether older malware samples can still be detected by the system  if not , this would obviously become vulnerable to older ( and possibly popular ) attacks . therefore , we also perform the `` opposite '' experiment , i.e. , training with newer datasets , and checking whether it is able to detect malware developed years before .",
    "specifically ,  [ fig : fmpastfam ] and  [ fig : fmpastpac ] report results when training with samples from a given year , and testing it with others that are up to 4 years older : retains similar f - measure scores over the years .",
    "specifically , in family mode , it varies from 93% to 96% , whereas , in package mode , from 95% to 97% with the oldest samples .              the experiment analysis presented above show that detects android malware with high accuracy . as in any detection system , however , the system makes a small number of incorrect classifications , incurring some false positives and false negatives .",
    "next , we discuss a few case studies aiming to better understand these misclassifications .",
    "we focus on the experiments with newer datasets , i.e. , ` 2016 ` and ` newbenign ` .",
    "* false positives .",
    "* we analyze the manifest of the 164 apps mistakenly detected as malware by , finding that most of them use `` dangerous '' permissions  @xcite .",
    "in particular , 67% of the apps write to external storage , 32% read the phone state , and 21% access the device s fine location .",
    "we further analyzed apps ( 5% ) that use the read_sms and send_sms permissions , i.e. , even though they are not sms - related apps , they can read and send smss as part of the services they provide to users .",
    "in particular , a _ `` in case of emergency '' _ app is able to send messages to several contacts from its database ( possibly added by the user ) , which is a typical behavior of android malware in our dataset , ultimately leading to flag it as malicious .    * false negatives .",
    "* we also check the 114 malware samples missed by when operating in family mode , using virustotal . we find that 18% of the false negatives are actually not classified as malware by any of the antivirus engines used by virustotal , suggesting that these are actually legitimate apps mistakenly included in the virusshare dataset .",
    "45% of s false negatives are _ adware _ , typically , repackaged apps in which the advertisement library has been substituted with a third - party one , which creates a monetary profit for the developers . since they are not performing any clearly malicious activity ,",
    "is unable to identify them as malware .",
    "finally , we find that 16% of the false negatives reported by are samples sending text messages or starting calls to premium services .",
    "we also do a similar analysis of false negatives when abstracting to packages ( 74 samples ) , with similar results : there a few more adware samples ( 53% ) , but similar percentages for potentially benign apps ( 15% ) and samples sending smss or placing calls ( 11% ) .    in conclusion",
    ", we find that s sporadic misclassifications are typically due to benign apps behaving similarly to malware , malware that do not perform clearly - malicious activities , or mistakes in the ground truth labeling .    [",
    "table : droidapiresults ]      we also compare the performance of to previous work using api features for android malware classification .",
    "specifically , we compare to  @xcite , because : ( i ) it uses api calls and its parameters to perform classification ; ( ii ) it reports high true positive rate ( up to 97.8% ) on almost 4k malware samples obtained from mcafee and genome  @xcite , and 16k benign samples ; and ( iii ) its source code has been made available to us by the authors .    in , permissions that are requested more frequently by malware samples than by benign apps are used to perform a baseline classification .",
    "since there are legitimate situations where a non - malicious app needs permissions tagged as dangerous , also applies frequency analysis on the list of api calls , specifically , using the 169 most frequent api calls in the malware samples ( occurring at least 6% more in malware than benign samples ) leading to a reported 83% precision .",
    "finally , data flow analysis is applied on the api calls that are frequent in both benign and malicious samples , but do not occur by at least , 6% more in the malware set . using the top 60 parameters , the 169 most frequent calls change , and authors report a precision of 97.8% . after obtaining s source code , as well as a list of packages used for feature refinement ,",
    "we re - implement the system by modifying the code in order to reflect recent changes in androguard ( used by for api call extraction ) , extract the api calls for all apps in the datasets listed in table  [ table : dataset ] , and perform a frequency analysis on the calls .",
    "androguard fails to extract calls for about 2% ( 1,017 ) of apps in our datasets as a result of bad crc-32 redundancy checks and error in unpacking , thus is evaluated over the samples in the second - to - last column of table  [ table : dataset ] .",
    "we also implement classification , which is missing from the code provided by the authors , using k - nn ( with k@xmath113 ) since it achieves the best results according to the paper .",
    "we use 2/3 of the dataset for training and 1/3 for testing as implemented by the authors  @xcite .",
    "a summary of the resulting f - measures obtained using different training and test sets is presented in table  [ table : droidapiresults ] .",
    "we set up a number of experiments to thoroughly compare to .",
    "first , we set up three experiments in which we train using a dataset composed of oldbenign combined with one of the three oldest malware datasets each ( drebin , 2013 , and 2014 ) , and testing on all malware datasets . with this configuration ,",
    "the best result ( with 2014 and oldbenign as training sets ) amounts to 62% f - measure when tested on the same dataset .",
    "the f - measure drops to 33% and 39% , respectively , when tested on samples one year into the future and past .",
    "if we use the same configurations in , in package mode , we obtain up to 97% f - measure ( using 2013 and oldbenign as training sets ) , dropping to 74% and 93% , respectively , one year into the future and into the past . for the datasets where achieves its best result ( i.e. , 2014 and oldbenign )",
    ", achieves an f - measure of 95% , which drops to respectively , 79% and 93% one year into the future and the past .",
    "the f - measure is stable even two years into the future and the past at 78% and 92% , respectively .    as a second set of experiments ,",
    "we train using a dataset composed of newbenign combined with one of the three most recent malware datasets each ( 2014 , 2015 , and 2016 ) .",
    "again , we test on all malware datasets . the best result is obtained with the dataset ( 2014 and newbenign ) used for both testing and training , yielding a f - measure of 92% , which drops to 67% and 75% one year into the future and past respectively .",
    "likewise , we use the same datasets for , with the best results achieved on the same dataset as . in package mode , achieves an f - measure of 99% , which is maintained more than two years into the past , but drops to respectively , 89% and 83% one and two years into the future .",
    "as summarized in table  [ table : droidapiresults ] , achieves significantly higher performance than in all but one experiment , with the f - measure being at least 79% even after two years into the future or the past when datasets from 2014 or later are used for training .",
    "note that there is only one setting in which performs slightly better than : this occurs when the malicious training set is much older than the malicious test set .",
    "specifically , presents low recall in this case : as discussed , s classification performs much better when the training set is not more than two years older than the test set .",
    "we envision to be integrated in offline detection systems , e.g. , run by google play .",
    "recall that consists of different phases , so in the following , we review the computational overhead incurred by each of them , aiming to assess the feasibility of real - world deployment .",
    "we run our experiments on a desktop equipped with an 40-core 2.30ghz cpu and 128 gb of ram , but only use one core and allocate 16 gb of ram for evaluation . s",
    "first step involves extracting the call graph from an apk and the complexity of this task varies significantly across apps . on average",
    ", it takes 9.2s@xmath1414 ( min 0.02s , max 13 m ) to complete for samples in our malware sets .",
    "benign apps usually yield larger call graphs , and the average time to extract them is 25.4s@xmath1463 ( min 0.06s , max 18 m ) per app .",
    "note that we do not include in our evaluation apps for which we could not successfully extract the call graph .",
    "next , we measure the time needed to extract call sequences while abstracting to families or packages , depending on s mode of operation . in family mode , this phase completes in about 1.3s on average ( and at most 11.0s ) with both benign and malicious samples . abstracting to packages",
    "takes slightly longer , due to the use of 341 packages in . on average",
    ", this extraction takes 1.67s@xmath143.1 for malicious apps and 1.73s@xmath143.2 for benign samples .",
    "as it can be seen , the call sequence extraction in package mode does not take significantly more than in family mode .",
    "s third step includes markov chain modeling and feature vector extraction .",
    "this phase is fast regardless of the mode of operation and datasets used . specifically , with malicious samples , it takes on average 0.2s@xmath140.3 and 2.5s@xmath143.2 ( and at most 2.4s and 22.1s ) , respectively , with families and packages , whereas , with benign samples , averages rise to 0.6s@xmath140.3 and 6.7s@xmath143.8 ( at most 1.7s and 18.4s ) .",
    "finally , the last step involves classification , and performance depends on both the machine learning algorithm employed and the mode of operation .",
    "more specifically , running times are affected by the number of features for the app to be classified , and not by the initial dimension of the call graph , or by whether the app is benign or malicious .",
    "regardless , in family mode , random forests , 1-nn , and 3-nn all take less than 0.01s . with packages , it takes , respectively , 0.65s , 1.05s , and 0.007s per app with 1-nn , 3-nn , random forests .",
    "overall , when operating in family mode , malware and benign samples take on average , 10.7s and 27.3s respectively to complete the entire process , from call graph extraction to classification . whereas , in package mode , the average completion times for malware and benign samples are 13.37s and 33.83s respectively . in both modes of operation , time is mostly ( > 80% ) spent on call graph extraction .",
    "we also evaluate the runtime performance of  @xcite .",
    "its first step , i.e. , extracting api calls , takes 0.7s@xmath141.5 ( min 0.01s , max 28.4s ) per app in our malware datasets .",
    "whereas , it takes on average 13.2s@xmath1422.2 ( min 0.01s , max 222s ) per benign app . in the second phase , i.e. , frequency and data flow analysis , it takes , on average , 4.2s per app .",
    "finally , classification using 3-nn is very fast : 0.002s on average .",
    "therefore , in total , takes respectively , 17.4s and 4.9s for a complete execution on one app from our benign and malware datasets , which while faster than , achieves significantly lower accuracy . in conclusion ,",
    "our experiments show that our prototype implementation of is scalable enough to be deployed . assuming that , everyday , a number of apps in the order of 10,000 are submitted to google play , and using the average execution time of benign samples in family ( 27.3s ) and package ( 33.83s ) modes , we estimate that it would take less than an hour and a half to complete execution of all apps submitted daily in both modes , with just 64 cores .",
    "note that we could not find accurate statistics reporting the number of apps submitted everyday , but only the total number of apps on google play . on average ,",
    "this number increases of a couple of thousands per day , and although we do not know how many apps are removed , we believe 10,000 apps submitted every day is likely an upper bound .",
    "we now discuss the implications of our results with respect to the feasibility of modeling app behavior using static analysis and markov chains , discuss possible evasion techniques , and highlight some limitations of our approach .",
    "our work yields important insights around the use of api calls in malicious apps , showing that , by modeling the sequence of api calls made by an app as a markov chain , we can successfully capture the behavioral model of that app .",
    "this allows to obtain high accuracy overall , as well as to retain it over the years , which is crucial due to the continuous evolution of the android ecosystem . as discussed in section  [ sec : data ] , the use of api calls changes over time , and in different ways across malicious and benign samples . from our newer datasets , which include samples up to spring 2016 ( api level 23 )",
    ", we observe that newer apis introduce more packages , classes , and methods , while also deprecating some .",
    "[ fig : numcalls ] ,  [ fig : famandro ] , and  [ fig : famgoogle ] show that benign apps are using more calls than malicious ones developed around the same time .",
    "we also notice an interesting trend in the use of android and google apis : malicious apps follow the same trend as benign apps in the way they adopt certain apis , but with a delay of some years .",
    "this might be a side effect of android malware authors tendency to repackage benign apps , adding their malicious functionalities onto them .    given the frequent changes in the android framework and the continuous evolution of malware , systems like  @xcite  being dependent on the presence or the use of certain api calls  become increasingly less effective with time .",
    "as shown in table  [ table : droidapiresults ] , malware that uses api calls released after those used by samples in the training set can not be identified by these systems . on the contrary , as shown in  [ fig : fmfutfam ] and  [ fig : fmfutpac ] , detects malware samples that are _ 1 year _",
    "newer than the training set obtaining an 86% f - measure ( as opposed to 46% with ) . after 2 years , the value is still at 75% ( 42% with ) , dropping to 51% after 4 years .",
    "we argue that the effectiveness of s classification remains relatively high `` over the years '' owing to markov models capturing app behavior .",
    "these models tend to be more robust to malware evolution because abstracting to families or packages makes the system less susceptible to the introduction of new api calls .",
    "abstraction allows to capture newer classes / methods added to the api , since these are abstracted to already - known families or packages . in case",
    "newer packages are added to the api , and these packages start being used by malware , only requires adding a new state to the markov chains , and probabilities of a transition from a state to this new state in old apps would be 0 . adding only a few nodes",
    "does not likely alter the probabilities of the other 341 nodes , thus , two apps created with the same purpose will not strongly differ in api calls usage if they are developed using almost consecutive api levels .",
    "we also observe that abstracting to packages provides a slightly better tradeoff than families . in family mode ,",
    "the system is lighter and faster , and actually performs better when there are more than two years between training and test set samples however , even though both modes of operation effectively detect malware , abstracting to packages yields better results overall .",
    "nonetheless , this does not imply that less abstraction is always better : in fact , a system that is too granular , besides incurring untenable complexity , would likely create markov models with low - probability transitions , ultimately resulting in less accurate classification",
    ". we also highlight that applying pca is a good strategy to preserve high accuracy and at the same time reducing complexity .",
    "next , we discuss possible evasion techniques and how they can be addressed",
    ". one straightforward evasion approach could be to repackage a benign app with small snippets of malicious code added to a few classes .",
    "however , it is difficult to embed malicious code in such a way that , at the same time , the resulting markov chain looks similar to a benign one . for instance , our running example from section  [ sec : method ] ( malware posing as a memory booster app and executing unwanted commands as root ) is correctly classified by ; although most functionalities in this malware are the same as the original app , injected api calls generate some transitions in the markov chain that are not typical of benign samples . the opposite procedure ",
    "i.e. , embedding portions of benign code into a malicious app  is also likely ineffective against , since , for each app , we derive the feature vector from the transition probability between calls over the entire app .",
    "in other words , a malware developer would have to embed benign code inside the malware in such a way that the overall sequence of calls yields similar transition probabilities as those in a benign app , but this is difficult to achieve because if the sequences of calls have to be different ( otherwise there would be no attack ) , then the models will also be different .",
    "an attacker could also try to create an app from scratch with a similar markov chain to that of a benign app . because this is derived from the sequence of abstracted api calls in the app , it is actually very difficult to create sequences resulting in markov chains similar to benign apps while , at the same time , actually engaging in malicious behavior .",
    "nonetheless , in future work , we plan to systematically analyze the feasibility of this strategy .",
    "moreover , attackers could try using reflection , dynamic code loading , or native code  @xcite .",
    "because uses static analysis , it fails to detect malicious code when it is loaded or determined at runtime .",
    "however , can detect reflection when a method from the reflection package ( java.lang.reflect ) is executed .",
    "therefore , we obtain the correct sequence of calls up to the invocation of the reflection call , which may be sufficient to distinguish between malware and benign apps . similarly , can detect the usage of class loaders and package contexts that can be used to load arbitrary code , but it is not able to model the code loaded ; likewise , native code that is part of the app can not be modeled , as it is not java and is not processed by soot .",
    "these limitations are not specific of , but are a problem of static analysis in general , which can be mitigated by using alongside dynamic analysis techniques .",
    "malware developers might also attempt to evade by naming their self - defined packages in such a way that they look similar to that of the android , java , or google apis , e.g. , creating packages like java.lang.reflect._malware_ and java.lang._malware_ , aiming to confuse into abstracting them to respectively , java.lang.reflect and java.lang .",
    "however , this is easily prevented by whitelisting the list of packages from android , java , or google apis .",
    "another approach could be using dynamic dispatch so that a class x in package a is created to extend class y in package b with static analysis reporting a call to root ( ) defined in y as x.root ( ) , whereas , at runtime y.root ( ) is executed .",
    "this can be addressed , however , with a small increase in s computational cost , by keeping track of self - defined classes that extend or implement classes in the recognized apis , and abstract polymorphic functions of this self - defined class to the corresponding recognized package , while , at the same time , abstracting as self - defined overridden functions in the class .",
    "finally , identifier mangling and other forms of obfuscation could be used aiming to obfuscate code and hide malicious actions . however , since classes in the android framework can not be obfuscated by obfuscation tools , malware developers can only do so for self - defined classes .",
    "labels obfuscated calls as obfuscated so , ultimately , these would be captured in the behavioral model ( and the markov chain ) for the app . in our sample , we observe that benign apps use significantly less obfuscation than malicious apps , indicating that obfuscating a significant number of classes is not a good evasion strategy since this would likely make the sample more easily identifiable as malicious .",
    "requires a sizable amount of memory in order to perform classification , when operating in package mode , working on more than 100,000 features per sample .",
    "the quantity of features , however , can be further reduced using feature selection algorithms such as pca . as explained in section [ sec : evaluation ] when we use 10 components from the pca the system performs almost as well as the one using all the features ; however , using pca comes with a much lower memory complexity in order to run the machine learning algorithms , because the number of dimensions of the features space where the classifier operates is remarkably reduced .",
    "soot  @xcite , which we use to extract call graphs , fails to analyze some apks .",
    "in fact , we were not able to extract call graphs for a fraction ( 4.6% ) of the apps in the original datasets due to scripts either failing to apply the jb phase , which is used to transform java bytecode to the primary intermediate representation ( i.e. , jimple ) of soot or not able to open the apk .",
    "even though this does not really affect the results of our evaluation , one could avoid it by using a different / custom intermediate representation for the analysis or use different tools to extract the call graphs .",
    "in general , static analysis methodologies for malware detection on android could fail to capture the runtime environment context , code that is executed more frequently , or other effects stemming from user input  @xcite .",
    "these limitations can be addressed using dynamic analysis , or by recording function calls on a device .",
    "dynamic analysis observes the live performance of the samples , recording what activity is actually performed at runtime . through dynamic analysis , it is also possible to provide inputs to the app and then analyze the reaction of the app to these inputs , going beyond static analysis limits .",
    "to this end , we plan to integrate dynamic analysis to build the models used by as part of future work .",
    "over the past few years , android security has attracted a wealth of work by the research community . in this section ,",
    "we review ( i ) program analysis techniques focusing on general security properties of android apps , and then ( ii ) systems that specifically target malware on android .      previous work on program analysis applied to android security has used both static and dynamic analysis . with the former ,",
    "the program s code is decompiled in order to extract features without actually running the program , usually employing tools such as dare  @xcite to obtain java bytecode .",
    "the latter involves real - time execution of the program , typically in an emulated or protected environment .",
    "static analysis techniques include work by felt et al .",
    "@xcite , who analyze api calls to identify over - privileged apps , while kirin  @xcite is a system that examines permissions requested by apps to perform a lightweight certification , using a set of security rules that indicate whether or not the security configuration bundled with the app is safe .",
    "riskranker  @xcite aims to identify zero - day android malware by assessing potential security risks caused by untrusted apps .",
    "it sifts through a large number of apps from android markets and examines them to detect certain behaviors , such as encryption and dynamic code loading , which form malicious patterns and can be used to detect stealthy malware .",
    "other methods , such as chex  @xcite , use data flow analysis to automatically vet android apps for vulnerabilities .",
    "static analysis has also been applied to the detection of data leaks and malicious data flows from android apps  @xcite .",
    "droidscope  @xcite and taintdroid  @xcite monitor run - time app behavior in a protected environment to perform dynamic taint analysis .",
    "droidscope performs dynamic taint analysis at the machine code level , while taintdroid monitors how third - party apps access or manipulate users personal data , aiming to detect sensitive data leaving the system .",
    "however , as it is unrealistic to deploy dynamic analysis techniques directly on users devices , due to the overhead they introduce , these are typically used offline  @xcite .",
    "paranoidandroid  @xcite employs a virtual clone of the smartphone , running in parallel in the cloud and replaying activities of the device  however , even if minimal execution traces are actually sent to the cloud , this still takes a non - negligible toll on battery life .",
    "recently , hybrid systems like intellidroid  @xcite have also been proposed that use input generators , producing inputs specific to dynamic analysis tools .",
    "other work combining static and dynamic analysis include  @xcite .",
    "a number of techniques have used _ signatures _ for android malware detection .",
    "networkprofiler  @xcite generates network profiles for android apps and extracts fingerprints based on such traces , while work in  @xcite obtains resource - based metrics ( cpu , memory , storage , network ) to distinguish malware activity from benign one .",
    "chen et al .",
    "@xcite extract statistical features , such as permissions and api calls , and extend their vectors to add dynamic behavior - based features . while their experiments show that their solution outperforms , in terms of accuracy , other antivirus systems , chen et al .",
    "@xcite indicate that the quality of their detection model critically depends on the availability of representative benign and malicious apps for training .",
    "similarly , scanme mobile  @xcite uses the google cloud messaging service ( gcm ) to perform static and dynamic analysis on apks found on the device s sd card .",
    "the sequences of system calls have also been used to detect malware in both desktop and android environments .",
    "hofmeyr et al .",
    "@xcite demonstrate that short sequences of system calls can be used as a signature to discriminate between normal and abnormal behavior of common unix programs .",
    "signature - based methods , however , can be evaded using polymorphism and obfuscation , as well as by call re - ordering attacks  @xcite , even though quantitative measures , such as similarity analysis , can be used to address some of these attacks  @xcite .",
    "inherits the spirit of these approaches , proposing a statistical method to model app behavior that is more robust against evasion attempts . in the android context ,",
    "canfora et al .",
    "@xcite use the sequences of three system calls ( extracted from the execution traces of apps under analysis ) to detect malware .",
    "this approach models specific malware families , aiming to identify additional samples belonging to such families .",
    "in contrast , s goal is to detect previously - unseen malware , and we also show that our system can detect new malware families that even appear years after the system has been trained .",
    "in addition , using strict sequences of system or api calls can be easily evaded by malware authors who could add unnecessary calls to effectively evade detection .",
    "conversely , builds a behavioral model of an android app , which makes it robust to this type of evasion .",
    "dynamic analysis has also been applied to detect android malware by using predefined scripts of common inputs that will be performed when the device is running .",
    "however , this might be inadequate due to the low probability of triggering malicious behavior , and can be side - stepped by knowledgeable adversaries , as suggested by wong and lie  @xcite .",
    "other approaches include random fuzzing  @xcite and concolic testing  @xcite .",
    "dynamic analysis can only detect malicious activities if the code exhibiting malicious behavior is actually running during the analysis .",
    "moreover , according to  @xcite , mobile malware authors often employ emulation or virtualization detection strategies to change malware behavior and eventually evade detection .",
    "aiming to complement static and dynamic analysis tools , machine learning techniques have also been applied to assist android malware detection .",
    "droidmat  @xcite uses api call tracing and manifest files to learn features for malware detection , while gascon et al .",
    "@xcite rely on embedded call graphs .",
    "droidminer  @xcite studies the program logic of sensitive android / java framework api functions and resources , and detects malicious behavior patterns .",
    "mast  @xcite statically analyzes apps using features such as permissions , presence of native code , and intent filters and measures the correlation between multiple qualitative data .",
    "crowdroid  @xcite relies on crowdsourcing to distinguish between malicious and benign apps by monitoring system calls .",
    "appcontext  @xcite models security - sensitive behavior , such as activation events or environmental attributes , and uses svm to classify these behaviors , while revealdroid  @xcite employs supervised learning and obfuscation - resilient methods targeting api usage and intent actions to identify their families .",
    "drebin  @xcite automatically deduces detection patterns and identifies malicious software directly on the device , performing a broad static analysis .",
    "this is achieved by gathering numerous features from the manifest file as well as the app s source code ( api calls , network addresses , permissions ) .",
    "malevolent behavior is reflected in patterns and combinations of extracted features from the static analysis : for instance , the existence of both send_sms permission and the android.hardware.telephony component in an app might indicate an attempt to send premium sms messages , and this combination can eventually constitute a detection pattern . in section  [ sec : evaluation ] , we have already introduced , and compared against ,  @xcite .",
    "this system relies on the top-169 api calls that are used more frequently in the malware than in the benign set , along with data flow analysis on calls that are frequent in both benign and malicious apps , but occur up to 6% more in the latter . as shown in our evaluation , using the most common calls observed during training requires constant retraining , due to the evolution of both malware and the android api . on the contrary",
    ", can effectively model both benign and malicious android apps , and perform an efficient classification on them .",
    "compared to , our approach is more resilient to changes in the android framework than , resulting in a less frequent need to re - train the classifier .",
    "overall , compared to state - of - the - art systems like drebin  @xcite and  @xcite , is more generic and robust as its statistical modeling does not depend on specific app characteristics , but can actually be run on any app created for any android api level .    finally , also related to are markov - chain based models for android malware detection .",
    "chen et al .",
    "@xcite dynamically analyze system- and developer - defined actions from intent messages ( used by app components to communicate with each other at runtime ) , and probabilistically estimate whether an app is performing benign or malicious actions at run time , but obtain low accuracy overall .",
    "canfora et al .",
    "@xcite use a hidden markov model ( hmm ) to identify malware samples belonging to previously observed malware families , whereas , can detect previously unseen malware , not relying on specific malware families .",
    "this paper presented , an android malware detection system based on modeling the sequences of api calls as markov chains .",
    "our system is designed to operate in one of two modes , with different granularities , by abstracting api calls to either families or packages .",
    "we ran an extensive experimental evaluation using , to the best of our knowledge , the largest malware dataset ever analyzed in an android malware detection research paper , and aiming at assessing both the accuracy of the classification ( using f - measure , precision , and recall ) and runtime performances .",
    "we showed that effectively detects unknown malware samples developed earlier or around the same time as the samples on which it is trained ( f - measure up to 99% ) .",
    "it also maintains good detection performance : one year after the model has been trained the f - measure value is 86% , and after two years it is 75% .",
    "we compared to  @xcite , a state - of - the - art system based on api calls frequently used by malware , showing that , not only does outperforms when trained and tested on the same datasets , but that it is also much more resilient over the years to changes in the android api .",
    "overall , our results demonstrate that the type of statistical behavioral models introduced by are more robust than traditional techniques , highlighting how our work can form the basis of more advanced detection systems in the future . as part of future work",
    ", we plan to further investigate the resilience to possible evasion techniques , focusing on repackaged malicious apps as well as injection of api calls to maliciously alter markov models .",
    "we also plan to explore the use of finer - grained abstractions as well as the possibility to seed the behavioral modeling performed by with dynamic instead of static analysis . due to the large size of the data",
    ", we have not made them readily available online but both the datasets and the feature vectors can be obtained upon request .    * acknowledgments .",
    "* we wish to thank the anonymous reviewers for their feedback , our shepherd amir houmansadr for his help in improving our paper , and yousra aafer for kindly sharing the source code with us .",
    "we also wish to thank yanick fratantonio for his comments on an early draft of the paper .",
    "this research was supported by the epsrc under grant ep / n008448/1 , by an epsrc - funded `` future leaders in engineering and physical sciences '' award , a xerox university affairs committee grant , and by a small grant from gchq .",
    "enrico mariconti was supported by the epsrc under grant 1490017 , while lucky onwuzurike was funded by the petroleum technology development fund ( ptdf ) ."
  ],
  "abstract_text": [
    "<S> the rise in popularity of the android platform has resulted in an explosion of malware threats targeting it . as both android malware and </S>",
    "<S> the operating system itself constantly evolve , it is very challenging to design robust malware mitigation techniques that can operate for long periods of time without the need for modifications or costly re - training . in this paper , we present , an android malware detection system that relies on app behavior . </S>",
    "<S> builds a behavioral model , in the form of a markov chain , from the sequence of abstracted api calls performed by an app , and uses it to extract features and perform classification . by abstracting calls to their packages or families , maintains resilience to api changes and keeps the feature set size manageable . </S>",
    "<S> we evaluate its accuracy on a dataset of 8.5k benign and 35.5k malicious apps collected over a period of six years , showing that it not only effectively detects malware ( with up to 99% f - measure ) , but also that the model built by the system keeps its detection capabilities for long periods of time ( on average , 86% and 75% f - measure , respectively , one and two years after training ) . </S>",
    "<S> finally , we compare against , a state - of - the - art system that relies on the frequency of api calls performed by apps , showing that significantly outperforms it . </S>"
  ]
}