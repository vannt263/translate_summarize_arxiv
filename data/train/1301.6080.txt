{
  "article_text": [
    "let @xmath1 be a probability space and let @xmath2 be a random couple . broadly speaking , the goal of statistical learning is to predict @xmath3 given @xmath4 . to achieve this goal",
    ", we observe a dataset @xmath5 that consists of @xmath6 independent copies of @xmath7 and use these observations to construct a function ( learner ) @xmath8 such that @xmath9 is close to @xmath3 in a certain sense .",
    "more precisely , the prediction quality of a ( possibly data dependent ) function @xmath10 is measured by a _ risk function _",
    "@xmath11 associated to a _ loss function _ @xmath12 in the following way @xmath13\\,.\\ ] ] we focus hereafter on loss functions @xmath14 that are _ convex _ in their second argument .",
    "moreover , for the sake of simplicity , throughout this article we restrict ourselves to functions @xmath15 and random variables @xmath7 for which @xmath16 and @xmath17 almost surely , for some fixed @xmath18 . for any real valued measurable @xmath15 on @xmath1 , for which this quantity is finite , we define @xmath19}$ ] .",
    "we are given a finite set @xmath20 of measurable functions from @xmath1 to @xmath21 .",
    "this set is called a _",
    "dictionary_. the elements in @xmath22 may have been constructed using an independent , frozen , dataset at some previous step or may simply be good candidates for the learning task at hand . to focus our contribution on the aggregation problem ,",
    "we restrict our attention to the case where @xmath22 consists of deterministic functions .",
    "the aim of model selection aggregation @xcite is to use the data @xmath23 to construct a function @xmath10 having an _ excess - risk _ @xmath24 as small as possible .",
    "namely , we seek the smallest deterministic _ residual term _",
    "@xmath25 such that the excess risk is bounded above by @xmath26 , either in expectation or with high probability , or , in this instance , in both . in the high probability case ,",
    "such bounds are called _ oracle inequalities_. this problem was studied for instance in @xcite .    from a minimax standpoint",
    ", it has been proved that @xmath27 , @xmath28 is the smallest residual term that one can hope for the regression problem with quadratic loss @xcite .",
    "an estimator @xmath10 achieving such a rate ( up to some multiplying constant ) is called an optimal aggregate .",
    "the aim of this paper is to construct optimal aggregates under general conditions on the loss function @xmath14 .",
    "note that the optimal residuals for model selection aggregation are of the order @xmath29 as opposed to the standard parametric rate @xmath30 .",
    "fast _ rate essentially comes from the strong convexity of the quadratic loss . in what follows",
    "we show that indeed , strong convexity is sufficient to obtain fast rates .",
    "it is known that rates of optional order @xmath29 can not be achieved if the loss function is only assumed to be convex .",
    "indeed , it follows from @xcite , theorem  2 that if the loss is linear then the best achievable residual term is at least of the order @xmath31 . recall that a function @xmath32 is said to be strongly convex on a nonempty convex set @xmath33 if there exists a constant @xmath34 such that @xmath35 for any @xmath36 . in this case",
    ", @xmath34 is called _ modulus of strong convexity_. for technical reasons , we will also need to assume that the loss function is lipschitz .",
    "we now introduce the set of assumptions that are sufficient for our approach .    [",
    "ass : loss ] the loss function @xmath14 is such that for any @xmath37 $ ] , we have @xmath38 moreover , almost surely , the function @xmath39 is _ strongly convex _ with modulus of strong convexity @xmath40 on @xmath41 $ ] .",
    "a central quantity that is used for the construction of aggregates is the empirical risk defined by @xmath42 for any real - valued function @xmath15 defined over @xmath1 .",
    "a natural aggregation procedure consists in taking the function in @xmath22 that minimizes the empirical risk .",
    "this procedure is called empirical risk minimization ( erm ) .",
    "it has been proved that erm is suboptimal for the aggregation problem @xcite .",
    "somehow , this procedure does not take advantages of the convexity of the loss since the class of functions on which the empirical risk is minimized to construct the erm is @xmath22 , a finite set . as it turns out",
    ", the performance of erm relies critically on the convexity of the class of functions on which the empirical risk is minimized @xcite .",
    "therefore , a natural idea is to `` improve the geometry '' of @xmath22 by taking its convex hull @xmath43 and then by minimizing the empirical risk over it .",
    "however , this procedure is also suboptimal @xcite .",
    "the weak point of this procedure lies in the metric complexity of the problem : taking the convex hull of @xmath22 indeed `` improves the geometry '' of @xmath22 but it also increases by too much its complexity .",
    "the complexity of the convex hull of a set can be much larger than the complexity of the set itself and this leads to a failure of this naive convexification trick .",
    "nevertheless , a compromise between geometry and complexity was stricken in @xcite and @xcite where optimal aggregates have been successfully constructed . in @xcite ,",
    "this improvement is achieved by minimizing the empirical risk over a carefully chosen star - shaped subset of the convex hull of @xmath22 . in @xcite ,",
    "a better geometry was achieved by taking the convex hull of an appropriate subset of @xmath22 and then by minimizing the empirical risk over it .    in this paper , we show that a third procedure , called @xmath0-aggregation , and that was introduced in @xcite for fixed design gaussian regression , also leads to optimal rates of aggregation . unlike the above two procedures that rely on finding an appropriate constraint for erm , @xmath0-aggregation is based on a penalization of the empirical risk but the constraint set is kept to be the convex hull of @xmath22 .",
    "let @xmath44 denote the flat simplex of @xmath45 defined by @xmath46 and for any @xmath47 , define the convex combination @xmath48 .",
    "for any fixed @xmath49 , the @xmath0-functional is defined for any @xmath47 by @xmath50 we keep the terminology @xmath0-aggregation from @xcite in purpose .",
    "indeed , @xmath0 stands for _ quadratic _ and while do not employ a quadratic loss , we exploit strong convexity in the same manner as in  @xcite and @xcite .",
    "indeed the first term in @xmath0 acts as a regularization of the linear interpolation of the empirical risk and is therefore a strongly convex regularization .",
    "we consider the following aggregation procedure .",
    "unlike the procedures introduced in  @xcite , the @xmath0-aggregation procedure allows us to put a prior weight given by a prior probability @xmath51 on each element of the dictionary @xmath22 .",
    "this feature turns out to be crucial for applications @xcite .",
    "let @xmath52 be the _ temperature _ parameter and @xmath53 .",
    "consider any vector of weights @xmath54 defined by @xmath55\\,.\\ ] ] it comes out of our analysis that @xmath56 achieves an optimal rate of aggregation if @xmath57 satisfies @xmath58.\\ ] ] where @xmath59    [ th : mainub ] let @xmath22 be a finite dictionary of cardinality @xmath60 and @xmath7 be a random couple of @xmath61 such that @xmath16 and @xmath62 a.s . for some @xmath63 .",
    "assume that assumption  [ ass : loss ] holds and that @xmath57 satisfies .",
    "then , for any @xmath64 , with probability greater than @xmath65 @xmath66+\\frac{2\\beta x}{n}\\,.\\ ] ] moreover , @xmath67\\leq \\min_{j=1,\\ldots , m}\\big[r(f_j)+\\frac{\\beta}{n}\\log\\big(\\frac{1}{\\pi_j}\\big)\\big]\\,.\\ ] ]    if @xmath68 is the uniform distribution , that is @xmath69 for all @xmath70 , then we recover in theorem  [ th : mainub ] the classical optimal rate of aggregation @xmath71 and the estimator @xmath72 is just the one minimizing the @xmath0-functional defined in . in particular",
    "no temperature parameter is needed for its construction . as a result ,",
    "in this case , the parameter @xmath73 need not be known for the construction of the @xmath0-aggregation procedure .",
    "an important part of our analysis is based upon concentration properties of empirical processes .",
    "while our proofs are similar to those employed in @xcite and @xcite , they contain genuinely new arguments .",
    "in particular , this learning setting , unlike the denoising setting considered in @xcite allows us to employ various new tools such as symmetrization and contraction . a classical tool to quantify the concentration of measure phenomenon is given by bernstein s inequality for bounded variables . in terms of laplace",
    "transform , bernstein s inequality ( * ? ? ?",
    "* theorem  1.10 ) states that if @xmath74 are @xmath6 i.i.d .",
    "real - valued random variables such that for all @xmath75 , @xmath76then for any @xmath77 , @xmath78\\leq \\exp\\big(\\frac{nv\\lambda^2}{2(1-c\\lambda)}\\big).\\ ] ] bernstein s inequality usually yields a bound of order @xmath79 for the deviations of a sum around its mean .",
    "as mentioned above , such bounds are not sufficient for our purposes and we thus consider the following concentration result .",
    "[ prop : bernstein - shifted ] let @xmath74 be i.i.d . real - valued random variables and let @xmath80 .",
    "assume that @xmath81 a.s .. then , for any @xmath82 , @xmath83\\leq 1\\ ] ] and @xmath84\\leq 1.\\ ] ]    proof .",
    "it follows from bernstein s inequality   that for any @xmath82 , @xmath85\\leq \\exp\\big(\\frac{n{{\\rm i}\\kern-0.18em{\\rm e}}z_1 ^ 2\\lambda^2}{2(1-c\\lambda)}\\big ) \\exp\\big[-n\\lambda c_0 { { \\rm i}\\kern-0.18em{\\rm e}}z_1 ^ 2\\big]\\leq 1\\end{aligned}\\ ] ] the second inequality is obtained by replacing @xmath86 by @xmath87 .",
    "we will also use the following exponential bound for rademacher processes : let @xmath88 be independent rademacher random variables and @xmath89 be some real numbers then , by hoeffding s inequality , @xmath90    our analysis also relies upon some geometric argument .",
    "indeed , the strong convexity of the loss function in assumption  [ ass : loss ] implies the @xmath91-convexity of the risk in the sense of @xcite .",
    "this translates into a lower bound on the gain obtained when applying jensen s inequality to the risk function @xmath92 .",
    "[ prop : strong-2convexe ] let @xmath7 be a random couple in @xmath61 and @xmath20 be a finite dictionary in @xmath93 such that @xmath94 and @xmath16 a.s .. assume that , almost surely , the function @xmath39 is strongly convex with modulus of strong convexity @xmath40 on @xmath41 $ ] .",
    "then , it holds that , for any @xmath95 , @xmath96    proof . define the random function @xmath97 . by strong convexity and @xcite , theorem 6.1.2",
    ", it holds almost surely that for any @xmath98 in @xmath41 $ ] , @xmath99 for any @xmath100 in the sub - differential of @xmath14 at @xmath101 . plugging @xmath102 , @xmath103 , we get almost surely @xmath104 ^ 2\\,.\\ ] ] now , multiplying both sides by @xmath105 and summing over @xmath106 , we get almost surely ,",
    "@xmath107 ^ 2\\,.\\ ] ] to complete the proof , it remains to take the expectation .",
    "let @xmath64 and assume that assumption  [ ass : loss ] holds throughout this section .",
    "we start with some notation . for any @xmath108 , define @xmath109where we recall that @xmath110 for any @xmath111 .",
    "let @xmath112 .",
    "let @xmath113 is the canonical basis of @xmath45 and for any @xmath114 define @xmath115 we also consider the functions @xmath116let @xmath117 .",
    "consider any oracle @xmath118 such that @xmath119        proof . since @xmath121 is a minimizer of the ( finite ) convex function @xmath122 over the convex set @xmath44",
    ", then there exists a subgradient @xmath123 such that for any @xmath95 it holds , @xmath124 .",
    "it yields @xmath125 it follows from the strong convexity of @xmath126 that @xmath127 where the second inequality follows from the previous display .",
    "it follows from the definition of @xmath72 that @xmath140 it follows from and in that @xmath141 together with proposition  [ prop : taylor ] , it yields @xmath142 we plug the above inequality into to obtain @xmath143thanks to the @xmath91-convexity of the risk ( cf .",
    "proposition  [ prop : strong-2convexe ] ) , we have @xmath144 .",
    "therefore , it follows from that @xmath145 note now that @xmath146 implies that @xmath147 moreover , together , the two conditions of the proposition yield @xmath148 therefore , it follows from the above three displays that @xmath149 + 2z_n\\\\ & \\leq \\min_{j=1,\\ldots , m}\\big[{{\\sf r}}(e_j)+\\frac{\\beta}{n}\\log\\big(\\frac{1}{\\pi_j}\\big)\\big]+ 2z_n.\\end{aligned}\\ ] ]    to complete our proof , it remains to prove that @xmath150\\leq \\exp(-x)$ ] and @xmath151\\le 0 $ ] under suitable conditions on @xmath152 and @xmath57 .",
    "using respectively a chernoff bound and jensen s inequality respectively , it is easy to see that both conditions follow if we prove that @xmath153 .",
    "it follows from the excess loss decomposition : @xmath154 and the cauchy - schwarz inequality implies that it is enough to prove that , @xmath155\\leq 1.\\ ] ] and @xmath156\\leq 1\\,,\\ ] ] for some @xmath157 .",
    "let @xmath132 be as such in the rest of the proof .",
    "we begin by proving  . to that end , define the symmetrized empirical process by @xmath158 where @xmath88 are n i.i.d .",
    "rademacher random variables independent of the @xmath159s .",
    "moreover , take @xmath132 and @xmath152 such that @xmath160 ^ 2}.\\ ] ] it yields @xmath161\\nonumber\\\\ & \\leq { { \\rm i}\\kern-0.18em{\\rm e}}\\exp\\big[s\\max_{\\theta\\in\\theta}\\big((1-\\nu)(p - p_n)(\\ell _ { \\theta}-\\ell_{\\theta^*})-\\mu\\sum_{j=1}^m\\theta_j{\\left\\|f_j - f_{\\theta^*}\\right\\|}^2_2-\\frac1sk(\\theta)\\big)\\big]\\nonumber\\\\ & \\leq { { \\rm i}\\kern-0.18em{\\rm e}}\\exp\\big[s\\max_{\\theta\\in\\theta}\\big(2(1-\\nu)p_{n,\\varepsilon}(\\ell _ { \\theta}-\\ell_{\\theta^*})- \\mu\\sum_{j=1}^m\\theta_j{\\left\\|f_j - f_{\\theta^*}\\right\\|}^2_2-\\frac1sk(\\theta)\\big)\\big ] \\label{align : sym}\\\\ & \\leq { { \\rm i}\\kern-0.18em{\\rm e}}\\exp\\big[s\\max_{\\theta\\in\\theta}\\big(2c_b ( 1-\\nu)p_{n,\\varepsilon}(f_{\\theta}-f_{\\theta^*})- \\mu\\sum_{j=1}^m\\theta_j{\\left\\|f_j - f_{\\theta^*}\\right\\|}^2_2-\\frac1sk(\\theta)\\big)\\big]\\label{align : contrac}\\end{aligned}\\ ] ] where   follows from the symmetrization inequality  ( * ? ? ?",
    "* theorem  2.1 ) and follows from the contraction principle  ( * ? ? ?",
    "* theorem 4.12 ) applied to contractions @xmath162 $ ] and @xmath163 is defined by @xmath164 .",
    "next , using the fact that the maximum of a linear function over a polytope is attained at a vertex , we get @xmath161\\nonumber\\\\ & \\leq \\sum_{k=1}^m\\pi_k{{\\rm i}\\kern-0.18em{\\rm e}}{{\\rm i}\\kern-0.18em{\\rm e}}_\\varepsilon\\exp\\big[s\\big(2c_b ( 1-\\nu)p_{n,\\varepsilon}(f_k - f_{\\theta^*})- \\mu{\\left\\|f_k - f_{\\theta^*}\\right\\|}^2_2\\big)\\big]\\nonumber\\\\&\\leq \\sum_{k=1}^m\\pi_k{{\\rm i}\\kern-0.18em{\\rm e}}\\exp\\big[\\frac{[2c_b ( 1-\\nu)s)]^2}{2n}\\big(p_n-\\frac{2\\mu n}{[2c_b(1-\\nu)]^2s}p\\big)(f_k - f_{\\theta^*})^2\\big ] \\label{align : rademacher}\\\\   & \\leq\\sum_{k=1}^m\\pi_k{{\\rm i}\\kern-0.18em{\\rm e}}\\exp\\big[\\frac{(2c_b(1-\\nu)s)^2}{2n}\\big((p_n - p)(f_k - f_{\\theta^*})^2-\\frac{1}{4b^2}p(f_k - f_{\\theta^*})^4\\big)\\big]\\label{align : cond0}\\end{aligned}\\ ] ] where follows from and   follows from  . together with the above display , proposition  [ prop : bernstein - shifted ] yields   as long as @xmath165 we now prove  .",
    "we have @xmath166\\\\ & \\leq \\sum_{j=1}^m\\theta^*_j\\sum_{k=1}^m \\pi_k{{\\rm i}\\kern-0.18em{\\rm e}}\\exp\\big[s\\big(\\nu(p - p_n)(\\ell_{e_k}-\\ell_{e_j})-\\mu { \\left\\|f_j - f_k\\right\\|}_2 ^ 2\\big)\\big]\\\\ & \\leq \\sum_{j=1}^m\\theta^*_j\\sum_{k=1}^m \\pi_k{{\\rm i}\\kern-0.18em{\\rm e}}\\exp\\big[s\\nu\\big((p - p_n)(\\ell_{e_k}-\\ell_{e_j})-\\frac{\\mu}{\\nu c_b^2}p(\\ell_{e_j}-\\ell_{e_k})^2\\big)\\big]\\leq1\\end{aligned}\\ ] ] where the last inequality follows from proposition  [ prop : bernstein - shifted ] when @xmath167 it is now straightforward to see that the conditions of proposition  [ prop : risk - bound ] , the ones of , and are fulfilled when @xmath168 and @xmath169.\\ ] ]                                      .",
    "grundlehren text editions .",
    "springer - verlag , berlin , 2001 .",
    "abridged version of it convex analysis and minimization algorithms .",
    "i [ springer , berlin , 1993 ; mr1261420 ( 95m:90001 ) ] and it ii [ ibid . ; mr1295240 ( 95m:90002 ) ] .        , vol .",
    "2033 of _ lecture notes in mathematics_. springer , heidelberg , 2011 .",
    "lectures from the 38th probability summer school held in saint - flour , 2008 , cole dt de probabilits de saint - flour .",
    "[ saint - flour probability summer school ] .",
    "1738 of _ lecture notes in mathematics_. springer - verlag , berlin , 2000 .",
    "lectures from the 28th summer school on probability theory held in saint - flour , august 17september 3 , 1998 , edited by pierre bernard ."
  ],
  "abstract_text": [
    "<S> we consider a general supervised learning problem with strongly convex and lipschitz loss and study the problem of model selection aggregation . in particular , given a finite dictionary functions ( learners ) together with the prior , we generalize the results obtained by dai , rigollet and zhang ( 2012 ) for gaussian regression with squared loss and fixed design to this learning setup . specifically , we prove that the @xmath0-aggregation procedure outputs an estimator that satisfies optimal oracle inequalities both in expectation and with high probability . </S>",
    "<S> our proof techniques somewhat depart from traditional proofs by making most of the standard arguments on the laplace transform of the empirical process to be controlled . </S>"
  ]
}