{
  "article_text": [
    "in the last years , there has been an increasing interest in the study of networked physical systems of excitable elements with time - varying connections between them .",
    "such systems are often studied using the statistical mechanics formalism and techniques , and include many different physical , biological and social systems , such that ionic diffusion in magnetic materials @xcite , inter - relations among metabolic reactions within a cell or in food webs @xcite , logistic ( transport and communication ) networked systems @xcite , and the friendship , social , professional and business relationships @xcite . in many of these studies , the emergent behavior observed is a consequence of a non - trivial interplay between the excitability of units , the presence of intrinsic noise , and the nonlinear dynamics affecting the links among excitable units .",
    "a paradigmatical example of networked system relevant in this context is the so called autoassociative neural network ( ann ) . in the last years",
    ", there has been an attempt to revitalize the study of anns by including biologically realistic mechanisms among their features @xcite . in particular , the consideration of activity - dependent mechanisms in their links or synapses , such as short - term depression ( std ) and facilitation ( stf ) @xcite has revealed itself as a quite proficient topic .",
    "examples of the rich and complex phenomenology found are the role of networks with depressing synapses as variable frequency oscillators @xcite , the emergence of dynamical phases @xcite , the associated instability of attractors consequence of the fatigue of the links @xcite , the emergence , in some cases , of a chaotic itineracy among the attractors with optimal computational consequences @xcite , the efficient detection of weak signals via stochastic multiresonances @xcite , the emergence of a critical dynamics in bistable systems driven by multiplicative colored noise @xcite , or the study of storage capacity and retrieval properties of ann with dynamic synapses @xcite , to name a few .",
    "most of these studies usually tackle the problem by considering hopfield @xcite networks , which frequently allow to obtain information both analytically and numerically for the particular problem of interest . quite often , to obtain such information one has to assume restricted conditions .",
    "for instance , to investigate the role of std and stf on network dynamics , many studies have focused on networks with a finite number of stored patterns @xmath0 ( so that the network load , defined as @xmath1 with @xmath2 being the number of neurons in the network , tends to zero in the thermodynamic limit @xmath3 ) @xcite .",
    "other studies consider , for instance , systems where the temperature parameter @xmath4 remains low ( or even zero , which corresponds to deterministic dynamics ) in order to evaluate maximal retrieval abilities of neural networks @xcite .",
    "however , results on the influence of std and stf on neural networks in more general situations , such as with strong noise and nonzero network load , are still lacking .",
    "they could be particularly interesting in the case of stf , since it has been found to have positive effects in the critical storage capacity of anns @xcite , in opposition to std @xcite .",
    "in this work , we study the effect of dynamic synapses , and in particular std and stf , in the properties of ann in general conditions of temperature and network load . to do this ,",
    "we have numerically computed the phase diagrams of the system , and we have used a _ nave _",
    "mean - field approach to interpret the results .",
    "we find that stf substantially improves the retrieval properties of the network , by increasing the area of the @xmath5 region of the phase diagram in which the system is able to efficiently recover stored activity patterns .",
    "this _ memory phase _ continues increasing its area even for conditions in which stf no longer improves the storing abilities of deterministic networks , suggesting that stf has a stronger impact in noisy neural networks than in deterministic ones .",
    "networks with depressing synapses , on the other hand , have a smaller memory phase compared with standard hopfield networks ( that is , networks with no short - term synaptic plasticity ) .",
    "finally , we particularize to the case of @xmath6 , and we demonstrate that the critical temperature @xmath7 increases with the level of synaptic facilitation . although the absolute strength of synaptic couplings is upper - bounded as in classical ann models , the particular value of @xmath7 may be arbitrarily high in our model , with the particular value depending on synaptic parameters .",
    "this suggests a key role of stf in situations in which previously stored memory has to be maintained in spite of noise , such as in working memory tasks @xcite .",
    "we consider a fully connected network of @xmath2 binary neurons whose states @xmath8 representing , respectively , silent or firing neurons , follow a probabilistic parallel , synchronous dynamics  @xcite @xmath9=\\frac{1}{2}\\left \\{1+\\tanh[2\\beta ( h_i({\\bf s},t)-\\theta_i)]\\right \\ } , \\label{prob}\\ ] ] with @xmath10 , and where @xmath11 is the local field or the total input synaptic current to neuron @xmath12 , namely @xmath13 we denote here @xmath14 as the inverse of the temperature ( or noise ) parameter , in such a way that @xmath15 implies deterministic dynamics .",
    "the quantity @xmath16 represents the neuron firing threshold of the neuron @xmath12 , and the coefficients @xmath17 are fixed synaptic conductances , determined by a slow learning process of @xmath0 patterns of activity . to model such learning process",
    ", we choose a hebbian prescription given by the standard covariance rule  @xcite @xmath18 where @xmath19 represents the @xmath0 stored random patterns with mean activity @xmath20 in addition , the variables @xmath21 appearing in @xmath22 describe the std and stf mechanisms , respectively .",
    "we assume that these variables evolve according to a well - known discrete dynamics  @xcite @xmath23 @xmath24 where the dynamics of @xmath25 has been normalized with respect to @xcite for simplicity . here",
    ", @xmath26 is a parameter related with the fraction of neurotransmitters released after the presynaptic neurons fires , and @xmath27 are , respectively , the time constants for depressing and facilitating mechanisms .",
    "the static - synapses situation ( that is , when synapses do not display std nor stf ) may be obtained if one sets @xmath28 or , alternatively , in the limit in which synaptic time constants @xmath29 become too small ( for more details on this limit , see @xcite ) .",
    "the neuron firing thresholds are given by @xmath30 this last expression allows to recover the original hopfield model when one considers static synapses ( that is , the case @xmath28 ) .",
    "in order to numerically compute the phase diagrams of the system , one has to determine , for a given set of values @xmath31 , the steady - state values of the order parameters characterizing the behavior of the network .",
    "following @xcite , these order parameters are the overlap functions @xmath32 , the _ spin - glass _ order parameter @xmath33 $ ] , and the pattern interference parameter @xmath34 , where the index @xmath35 denotes non - condensed patterns ( i.e. satisfying @xmath36 ) .",
    "typically , situations in which @xmath37 for a given pattern @xmath38 ( that is , the pattern @xmath38 has a _",
    "macroscopic _ overlap ) , @xmath36 for @xmath39 , and @xmath40 , correspond to the recall phase . on the other hand , situations in which the state of the system has an overlap @xmath36 for all stored patterns and @xmath41 corresponds to the spin - glass phase @xcite .",
    "we are interested here in the critical line between memory and spin - glass phases , which delimits the area of the memory phase @xcite .",
    "a simple way to obtain these phase diagrams is to compute , for a given value of @xmath4 , the maximal value of @xmath42 for which the stationary value of the macroscopic overlap is @xmath43 .",
    "this criterion is usually taken for the numerical evaluation of storage capacities in neural networks @xcite , and is illustrated in top panel of fig .",
    "equations corresponding to the mean - field solutions ( see appendix ) may be solved by employing any standard minimization algorithm to drive the absolute value of the difference between both sides of each equation to zero .    following these methods",
    ", we first compute the phase diagram of a stochastic neural network whose synapses present std . in the following , together with numerical results ,",
    "we show the predictions of a simplified mean - field approach previously introduced in @xcite , to support our numerical results and provide some theoretical insight of the phenomenology ( see appendix for details ) .",
    "[ fig1 ] shows a diagram @xmath5 where we can see the critical line separating the memory phase and the spin - glass phase , for different values of the std time constant and a fixed value of @xmath26 .",
    "the mechanism of stf is not present here ( that is , @xmath44 ) .",
    "as we can see , std reduces the area of the memory phase with respect to the standard hopfield model ( that is , when std is not present ) . such reduction becomes more prominent as the depression time scale @xmath45 increases , and indicates that std , while being quite convenient for filtering and processing information in spike trains @xcite or for dynamic memories",
    "@xcite , is not adequate for associative memory tasks .",
    "previous works had already suggested this effect @xcite , although only for the particular case of deterministic ( @xmath46 ) networks . here ,",
    "on the other hand , the adverse effect of std on associative memory is shown for the more general and realistic case of stochastic ( @xmath47 ) neural networks , a result which corroborates those of recent studies @xcite .",
    "we find that the reduction of the memory area is due to the temporary weakening of synaptic connections due to std , which decreases the stability of the fixed points of the dynamics @xcite .",
    "the effects of stf in the retrieval properties of stochastic neural networks may be investigated as well .",
    "top panel of fig .",
    "[ fig2 ] shows such effect , for a network with a fixed degree of std ( @xmath48 ) and increasing levels of stf . as we can see , the net effect of stf is the enlargement of the memory phase as @xmath49 increases , thus inducing an beneficial effect on associative memory tasks as opposed to std .",
    "such enlargement is even able to overcome the initial reduction of the memory phase caused by std ( red curve in top panel of the figure ) and to expand the limits of the memory phase far beyond the critical line of the standard hopfield model .",
    "this may be better appreciated in bottom panel of fig .",
    "[ fig2 ] , where the area of the memory phase ( namely , @xmath50 ) is computed as a function of @xmath49 for different values of @xmath26 . for clarity purposes , we have normalized the area units so that the standard hopfield model ( that is , no short - term synaptic plasticity ) has @xmath51 . for any of the curves shown , one may see that , as @xmath49 increases , the area also increases from @xmath52 ( the area is less than one due to the presence of std ) , surpassing the hopfield memory area at @xmath53 , and finally saturating at @xmath54 for @xmath55 .",
    "this saturation limit on @xmath50 depends strongly on the synaptic parameters , and in particular this limit increases when the parameter @xmath26 takes lower values , as the figure also shows . for very large values of @xmath49",
    ", we have found that @xmath50 starts to slowly decrease , although such decay is only present when @xmath56 ( data not shown ) . if std is not present , the effect of stf is a monotonic increment of @xmath50 with @xmath49 .",
    "the increment of memory area may be explained as follows : when the network is in ( or close to ) a previously stored pattern ( or equivalently , in a fixed point of the dynamics ) , neurons which were active for such pattern start firing .",
    "such firing induces , via stf dynamics , a temporal reinforcement of the synaptic connections of such neurons ( as one may infer from eq .",
    "( [ a3 ] ) ) , and this in turn improves the stability of such fixed point state . as a consequence , patterns which were unstable for networks with classical hebbian synapses are now stable in the presence of stf , and therefore the area of the memory phase is enlarged .",
    "it is particularly interesting to note that facilitating synapses present typically low values of @xmath26 compared with other synapses @xcite , suggesting that they could be contributing to enhance the retrieval properties of the network .",
    "it is also worth noting that @xmath50 continues rising with @xmath49 even for conditions in which stf no longer improves the storing abilities of deterministic networks , suggesting a nontrivial role of stf on the retrieval properties of the system .",
    "this may be seen for instance in top panel of fig .",
    "[ fig2 ] , where the maximum storage capacity at @xmath46 is larger for @xmath57 than for @xmath58 , although , as the bottom panel shows , the later has a larger value of @xmath50 .",
    "we may conclude then that networks with facilitating synapses display larger memory phases than standard hopfield networks , and are therefore able to efficiently retrieve previously stored information in more general conditions and in particular , in the presence of strong noise .",
    "a more careful inspection of top panel in fig . [ fig2 ] at low @xmath42 values indicates that stf yields an enlargement of the memory phase towards high - temperature regions . in order to further explore this effect ,",
    "we particularize to the case of @xmath6 .",
    "this limit corresponds to the case in which one considers a finite number of stored patterns @xmath0 ( and therefore @xmath59 in the thermodynamic limit @xmath3 ) .",
    "a relevant issue to consider here is the influence of dynamic synapses on the _ critical temperature _",
    "@xmath7 for which the system passes from a recall ( memory ) phase to a non - recall phase .",
    "when @xmath6 , the non - recall phase corresponds to the _ paramagnetic _ phase , in which @xmath60 and @xmath61 are zero in the thermodynamic limit ( @xmath3 ) @xcite .",
    "the critical temperature @xmath7 may be evaluated from a numerical point of view by computing the value of @xmath4 for which the steady state of the macroscopic overlap decays to zero @xcite .",
    "additionally , and to complement these numerical results , one may easily simplify our mean - field approach ( see appendix ) by doing @xmath6 and looking for the condition of nonzero solutions in eqs .",
    "( [ m]-[r ] ) , which gives a critical temperature of the form @xmath62 the evaluation of @xmath7 is depicted in fig .",
    "[ fig3]a , which also shows the influence of the synaptic time constants @xmath45 and @xmath49 on the value of @xmath7 . for the case of static synapses",
    ", the critical temperature takes the usual value @xmath63 @xcite .",
    "when std is present , one observes that lower values of @xmath7 are obtained ( around @xmath64 in the figure ) .",
    "such result indicates that anns with depressing synapses require a low level of intrinsic stochasticity to retrieve stored information via associative memory processes , even for vanishing network load . on the other hand",
    ", the inclusion of stf leads to larger values of @xmath7 ( around @xmath65 in the figure ) , indicating that an optimal performance in retrieval tasks may be achieved by ann with facilitating synapses even in the presence of high levels of intrinsic noise .",
    "this is also shown in panels b and c of fig .",
    "[ fig3 ] , where one can see that @xmath7 is a monotonically increasing function of @xmath49 , for different values of @xmath45 ( panel b ) or @xmath26 ( panel c ) .",
    "again , we find that the drastic increment of the critical temperature is due to the temporal reinforcement of concrete synaptic connections via stf dynamics .",
    "it is worth noting that the critical temperature of networks with stf comfortably surpass the critical temperature of the standard hopfield network ( namely , @xmath63 ) , saturating at a certain value @xmath66 for large @xmath49 . indeed , from eq .",
    "( [ tcrit ] ) we can obtain @xmath67 by considering @xmath68 ( which corresponds to absence of std if one considers the limit of smooth dynamics of @xmath69 , see @xcite for details ) and @xmath70 , obtaining @xmath71 .",
    "such prediction is shown in fig .",
    "[ fig3]d and confirmed with numerical simulations .",
    "this may be quite relevant since , as stated above , facilitating synapses in actual neural systems usually present very low @xmath26 values @xcite , which would correspond to high values of @xmath67 .",
    "these findings indicate that dynamics synapses , and in particular stf , may play a key role in the retrieval of information in networks with low load and high noise conditions .",
    "in this work we have shown the effect of short - term synaptic dynamics , and in particular std and stf , in the recall properties of stochastic anns .",
    "our findings indicates that std has a negative effect on the retrieval abilities of the network , by decreasing the area of the memory phase . on the other hand ,",
    "the presence of stf is able to enlarge this area far beyond the limits of the standard hopfield model , highlighting a tremendous beneficial impact of stf on memory tasks .",
    "the effects of stf in the particular case of deterministic dynamics ( @xmath46 ) have been addressed previously @xcite and therefore they have not been discussed here , although our results are in agreement by them . on the other hand , the limit of vanishing network load ( @xmath6 ) has been studied in detail , revealing that the presence of stf yields arbitrarily high critical temperature values , with the maximum value given by @xmath72 for no std and large enough @xmath49 .",
    "such results indicate a potential role of stf in memory tasks in noisy situations .",
    "this is especially interesting when modeling working memory tasks , where neural populations have to maintain information actively in the form of activity patterns in high noisy conditions . in these situations ,",
    "networks are known to handle simultaneously only a few activity patterns @xcite , a situation which could be modelled as ann with low network load and high noise , as we have done here .",
    "another surprising feature is that this increment of @xmath7 with @xmath49 is simply due to the fast dynamics of stf , and does not involve having higher values of the absolute coupling strength .",
    "indeed , the coupling strength of synapses is always upper - bounded in our model ( that is , synaptic strengths can not be higher than those of the hopfield model ) .",
    "the fact that such high performance in noisy conditions may be achieved by simply introducing a fast dynamics in the network links may have implications not only for neural network models , but for a wide class of cooperative networked systems with time - varying connections @xcite .",
    "finally , it is worth noting that the good retrieval abilities of neural networks with facilitating synapses highlights a new way to develop efficient neural network paradigms . indeed , good performance under high - noise conditions is a highly desirable feature from a technological point of view , and such improvement could be used to build more efficient hopfield - based categorization systems , for instance .",
    "in order to obtain an approximate mean - field theory , with the only purpose of orienting and supporting our numerical results , we follow a similar reasoning as the one presented in @xcite . briefly , from the definition of @xmath11 and eqs .",
    "( [ hebb ] ) and ( [ threshold ] ) , we obtain @xmath73=\\sum_{\\mu } \\epsilon_i^{\\mu } \\overline{m}^{\\mu}({\\bf s},t)-2 \\alpha x_i(t ) u_i(t ) s_i(t ) + \\alpha \\label{ht}\\ ] ] where @xmath74 , @xmath75 $ ] and , as stated above , @xmath1 . here",
    ", we consider only situations in which the system reaches a fixed point of the dynamics .",
    "a necessary condition for this is to keep relatively low values of @xmath45 , since when this time constant is large enough , the fixed point solutions of the dynamics ( [ x]-[u ] ) become unstable , leading to the appearance of _ switching _ behavior between activity patterns @xcite . on the other hand ,",
    "when @xmath45 takes relatively low values , the fixed points of the dynamics ( [ x]-[u ] ) are stable .",
    "in this situation , a plausible approximation is to completely neglect any temporal correlations in the dynamics ( [ x]-[u ] ) .",
    "one may then consider both @xmath76 and @xmath77 as binary variables switching between two possible values , namely @xmath78 when @xmath79 , and @xmath80 when @xmath81 .",
    "if we choose such values to be the maximum and minimum values that @xmath76 and @xmath77 may reach , this will give us an estimation of their mean values as a function of the mean value of @xmath82 .",
    "indeed , these maximum and minimum values may be easily computed by substituting @xmath83 , and @xmath84 , in eqs .",
    "( [ x]-[u ] ) , yielding @xmath85 where @xmath86 and @xmath87 , as in @xcite .",
    "it is worth noting that , with this criterion , the values @xmath88 ( which are associated with @xmath83 ) correspond to the minimum values reached by @xmath89 , and therefore @xmath90 correspond to the maximum values . considering the above assumptions",
    ", one has @xmath91 we can therefore provide an estimation for the quantity @xmath92 appearing in eq .",
    "( [ ht ] ) .",
    "such estimation is given by @xmath93 note that this expression , once time - averaged , is equivalent to the one obtained for low temperatures @xcite ( although that was obtained under different conditions ) , and therefore the resulting mean - field will be valid for all possible values of @xmath4 .            where @xmath97 indicates an average over @xmath98 ( the normal - distributed noise ) , variables @xmath99",
    "are the order parameters of the model ( see main text ) , and the parameters @xmath100 , @xmath101 are given , respectively , by @xmath102 and @xmath103 .",
    "as we have already stated , eqs .",
    "( [ m]-[r ] ) constitute a strongly simplified mean - field description of the model , used to support our numerical findings .",
    "therefore , it should not be considered as a general mean - field solution of anns with dynamic synapses , due to the existence of switching behavior @xcite which can not be explained within this framework .",
    "extensions of this theory to account for switching dynamics constitutes an interesting research line that remains open .",
    "we acknowledge financial support from spanish micinn project fis2009 - 08451 , micinn cei - greib translational project greib.pt@xmath1042011@xmath10419 , junta de andaluca project p06fqm01505 , and nserc canada discovery accelerator supplement program ."
  ],
  "abstract_text": [
    "<S> short - term synaptic depression and facilitation have been found to greatly influence the performance of autoassociative neural networks . </S>",
    "<S> however , only partial results , focused for instance on the computation of the maximum storage capacity at zero temperature , have been obtained to date . in this work </S>",
    "<S> , we extended the study of the effect of these synaptic mechanisms on autoassociative neural networks to more realistic and general conditions , including the presence of noise in the system . </S>",
    "<S> in particular , we characterized the behavior of the system by means of its phase diagrams , and we concluded that synaptic facilitation significantly enlarges the region of good retrieval performance of the network . </S>",
    "<S> we also found that networks with facilitating synapses may have critical temperatures substantially higher than those of standard autoassociative networks , thus allowing neural networks to perform better under high - noise conditions . </S>"
  ]
}