{
  "article_text": [
    "after more than 40 years of development internet has created a revolution in communication for humans because it allows people to access and exchange information efficiently .",
    "although internet is highly accessible , approximately 60 - 70% of people worldwide do not have the internet reported by international telecommunications union  @xcite in june 2013 .",
    "this stems from a fact that many areas such as africa , asia , and pacific , can not offer internet connections due to geographical and infrastructure issues .",
    "therefore , the idea of providing internet connections via wireless networks has become more and more popular .    in wireless internet",
    ", mobile users can connect to the internet service provider ( isp ) through base stations or access points .",
    "however , deployment of base stations for every location on the earth seems to be impossible , e.g. , oceans and mountains .",
    "therefore , the idea of providing internet from the sky was introduced .",
    "the early version is based on the satellites , which suffers from high cost and long transmission delay  @xcite . as a result , the cheaper and faster alternative , i.e. , google loon project  @xcite , was proposed . in loon project , access points will be placed on balloons flying at an altitude of about 20 km which is safe from bad weather and flights .",
    "the balloons will travel around the earth and form a network of access points for internet users in remote places .",
    "when receiving data from the user , the balloon will find the shortest route to transfer data to the nearest base station on the ground , which will be forwarded to an isp .",
    "data transmission and reception by the access points on the balloon consume energy , which requires continuous supply to sustain the operations . only viable energy sources for the balloon",
    "are through energy harvesting such as solar energy or radio frequency ( rf ) .",
    "the harvested energy will be stored in the energy storage of the access points and it will be used for data transmission and reception .",
    "however , batteries equipped on balloons are often limited by size and energy harvested from solar or rf is random .",
    "moreover , the balloons may have to serve data transfer requests from different types of users , e.g. , from other balloons , on the ground , satellites , or aircrafts , with different quality of service ( qos ) requirements .",
    "therefore , energy management for the balloons is an important issue .    in this paper",
    ", we aim to find an optimal admission control policy for the access points deployed on the balloons .",
    "the goal is to ensure high energy efficiency while maximizing profit of service providers .",
    "we formulate a markov decision process ( mdp ) for the energy allocation optimization problem . to obtain the optimal policy ,",
    "we apply a learning algorithm based on the policy gradient method and simulation - based method .",
    "the proposed learning algorithm not only avoids the curse of dimensionality problem caused by the explosion of state and action spaces , but also eliminates the need for complete knowledge about the model , which may not be possible to have from an unpredictable environment .",
    "numerical results show the convergence as well as the efficiency of the proposed learning algorithm .",
    "we consider energy management problem for a balloon with an access point as shown in fig .",
    "[ the system model 2 ] .",
    "specifically , the access point is to receive and transmit data from users .",
    "the access point has a battery to store energy harvested from solar and rf . when a request is sent to the access point",
    ", it will check the amount of energy remaining in the battery and apply an admission control policy by deciding whether the request will be accepted or rejected .",
    "if the request is accepted , a certain amount of energy from the battery will be used to receive data and transmit data to the next hop .",
    "different requests have different qos requirements .",
    "therefore , we divide requests into three classes , i.e. , requests from other balloons ( i.e. , class-1 ) , requests from users on the ground ( class-2 ) , and requests from satellites or aircrafts ( class-3 ) .",
    "the arrival processes of requests from class-1 , class-2 , and class-3 are assumed to follow the poisson distribution with mean rates @xmath0 , @xmath1 , and @xmath2 , respectively .",
    "when a request is accepted , the balloon will receive an immediate reward ( i.e. , revenue ) .",
    "the immediate revenues for accepting requests from class-1 , class-2 , and class-3 are @xmath3 , @xmath4 , and @xmath5 , respectively .    in the loon network ,",
    "balloons are assumed to be equipped with solar panels  @xcite for harvesting energy from sunlight .",
    "additionally , we assume that the balloons can be equipped with a rectenna  @xcite to harvest energy from rf waves .",
    "we assume that the energy arrival from both solar panel and rf rectenna follows the poisson distribution with mean rates @xmath6 , and the successful energy harvesting probability is @xmath7 .",
    "the maximum capacity of the battery is @xmath8 . at each time epoch ,",
    "when the access point receives a request , it will consult with the admission control policy .",
    "the admission control policy will determine a decision to accept or reject the request based on the request class and the current energy level in the battery .",
    "in this paper , we are interested in maximizing the profit for the service provider in the terms of average reward for the balloon .",
    "in this section , we will formulate the optimization problem as markov decision process ( mdp ) and study a learning algorithm to obtain the optimal policy for balloons .",
    "an mdp is defined by a tuple of @xmath9 where @xmath10 is a state space , @xmath11 is an action space , @xmath12 is a transition probability function , and @xmath13 is a reward function .",
    "the state space @xmath10 of our admission control for the access point on a balloon is defined as follows : @xmath14 where @xmath15 is the energy state space whose elements represent energy levels in the battery .",
    "@xmath16 is a set of events in which @xmath17 , @xmath18 , and @xmath19 are the events when a request is from another access point , a user , and a satellite , respectively .",
    "@xmath20 is an event for energy arrival .",
    "when the system is at state @xmath21 , if an event @xmath22 happens , an accept / reject decision must be made .",
    "thus , we can define the action space as follows : @xmath23 where @xmath24    to derive the transition probability function @xmath25 , we consider a discrete time system by using uniformization technique  @xcite with a uniformization parameter @xmath26 obtained as follows : @xmath27 based on the uniformization parameter @xmath26 , we can determine the probabilities of the events as follows . in the event @xmath21",
    ", the probabilities of a request arriving from other balloons , a user , and a satellite / an aircraft are @xmath28 , @xmath29 , @xmath30 , respectively .",
    "the probability of energy arrival is @xmath31 .",
    "then , we can derive the transition probability matrix for the system .",
    "however , to do so , we need to know the environment parameters , e.g. , successful energy harvesting probability , requests arrival rates .",
    "these parameters are generally not known in advance and building the model with complete information may not be possible . therefore , we apply a learning algorithm based on simulation - based method  @xcite .",
    "the main idea of the simulation - based method is based on a `` simulator '' that can simulate the environment by generating environment parameters ( e.g. , a successful energy harvesting probability and arrival rates ) .",
    "then we use the parameters from simulations to derive the admission control policy for the access point . based on the simulation - based method",
    ", the transition probability function can be defined as follows : @xmath32 where @xmath33 is environment parameter ( e.g. , the successful energy harvesting probability ) , @xmath34 is the probability that the system is at state @xmath35 , and @xmath36 is the probability that action @xmath37 is taken .",
    "when there is a request @xmath22 arriving at the access point , if the request is accepted , the balloon will receive an immediate reward @xmath38 corresponding to the type of the request .",
    "otherwise , the access point gains nothing , i.e. , @xmath39 note that , for the case @xmath40 , the access point will always receive energy if the battery is not full . however , there is no reward for such an action .",
    "we consider a parameterized randomized policy  @xcite . with the parameterized randomized policy ,",
    "when there is a request arriving at the access point , the request will be accepted with probability defined as follows : @xmath41 where @xmath42 is the parameter vector of the learning algorithm , @xmath21 is the current energy level of the battery and @xmath43 is the parameter for requests from event @xmath22 .",
    "additionally , the parameterized randomized policy @xmath44 must not be negative and meet the following condition , @xmath45    with the randomized parameterized policy , the transition probability function will be parameterized as follows : @xmath46 similarly , we can parameterize the immediate reward function as follows : @xmath47    we aim to maximized the average reward under randomized parameterized policy denoted by @xmath48 and it can be defined as follows : @xmath49    , \\ ] ] where @xmath50 is the system state at step @xmath51 and @xmath52 $ ] is the expected reward of the system .",
    "we then make some assumptions as follows :    [ recurrent_state ] the markov chain corresponding to every @xmath53 is aperiodic .",
    "furthermore , there exists a state @xmath54 which is recurrent for every of such markov chain .",
    "[ derivatives ] for every state @xmath55 , the functions @xmath56 and @xmath57 are bounded , twice differentiable , and have bounded first and second derivatives .",
    "assumption  [ recurrent_state ] implies that the system has a markov property and assumption  [ derivatives ] guarantees that the transition probability function and the average reward function depend smoothly on the parameter vector @xmath42 after they are parameterized by @xmath42 .",
    "assumption  [ derivatives ] is necessary when we use the policy gradient method to adjust vector @xmath42 . under assumption  [ derivatives ] , the average reward @xmath58 is well defined for every @xmath42 and does not depend on an initial state .",
    "furthermore , we have the following balance equations : @xmath59 where @xmath60 is steady state probability at state @xmath35 under the parameter vector , and thus the average reward function can be also defined as follows : @xmath61      we now can apply the policy gradient method  @xcite to update for the parameter vector @xmath42 as follows : @xmath62 where @xmath63 is a step size parameter . in the policy gradient method , we start with an initial parameter vector @xmath64 , and then the parameter vector @xmath42 will be updated iteratively based on ( [ eq : policy_gradient_method ] ) . under assumption  [ derivatives ] and an appropriate step size , it was proved in  @xcite that , @xmath65 . that is , the average reward @xmath66 converges almost surely .",
    "we now propose proposition  [ prop : policy_gradient ] to calculate the gradient for the average reward @xmath48 .",
    "[ prop : policy_gradient ] let assumption  [ recurrent_state ] and assumption  [ derivatives ] hold , then + @xmath67    @xmath68 is the differential reward at state @xmath35 and it can be defined as follows : @xmath69 , \\ ] ] where @xmath70 is the first future time that the state @xmath71 is visited .",
    "because of limited space , the proof of proposition  [ prop : policy_gradient ] can be found in  @xcite .",
    "we update the parameter vector @xmath42 iteratively based on ( [ eq : policy_gradient_method ] ) with the value of the gradient of average reward calculated from proposition  [ prop : policy_gradient ] .",
    "however , it is not easy to calculate the terms in ( [ eq : gradient_average_reward ] ) .",
    "additionally , when the state space and action space are large , it is intractable to calculate exactly the value of the gradient of the average reward function . therefore , in this paper , we consider the approach that can estimate the gradient of the average reward function and then the parameter vector @xmath42 can be adjusted in an online manner .    from  ( [ for : randomized_policy ] ) , we have @xmath72 , so we derive @xmath73 . from  ( [ eq : parameterized_reward ] )",
    ", we have : @xmath74 this is from the fact that @xmath73 .",
    "moreover , we have @xmath75    therefore , along with proposition  [ prop : policy_gradient ] , we derive the following results : @xmath76 where @xmath77 . \\end{aligned}\\ ] ]    here again @xmath78 is the first future time that the current state @xmath54 is visited .",
    "@xmath79 can be interpreted as the differential reward if action @xmath37 is taken based on policy @xmath80 at state @xmath35 .",
    "we need to note that @xmath81 is the cost at state @xmath35 and it is different from the different cost at state @xmath35 under action @xmath37 , i.e. , @xmath79 .",
    "then , we present algorithm  [ algorithm0 ] to update the parameter vector @xmath42 at the visits to the recurrent state @xmath54 .    at the time @xmath82 of the @xmath83th visit to state @xmath54",
    ", we update the parameter vector @xmath42 and the estimated average reward @xmath84 as follows : + @xmath85 @xmath86 where @xmath87 @xmath88    in algorithm  [ algorithm0 ] , @xmath89 is a positive scalar and @xmath90 is a step size parameter .",
    "we derive the following convergence result for algorithm  [ algorithm0 ] .",
    "[ prop2 ] let assumption  [ recurrent_state ] and assumption  [ derivatives ] hold , and let ( @xmath91 ) be the sequence of parameter vectors generated by algorithm  [ algorithm0 ] with a suitable step size parameter @xmath92 satisfied assumption  [ ass : step size ] , then @xmath93 converges and @xmath94 with probability 1",
    ".    the proof of the proposition  [ prop2 ] can be found in  @xcite .",
    "[ ass : step size ] the step size @xmath90 is deterministic , nonnegative and satisfies the following condition , @xmath95    in algorithm  [ algorithm0 ] , to update the value of the parameter vector @xmath42 at the next visit time to the state @xmath54 , we need to store all values of @xmath96 and @xmath97 between two successive visits . however , this method could result in slow processing .",
    "therefore , we modify algorithm  [ algorithm0 ] to improve the efficiency .",
    "first , we rewrite @xmath98 as follows : @xmath99 where @xmath100    the detail of the algorithm can be expressed as in algorithm  [ algorithm1 ] , where @xmath89 is a positive scalar and @xmath101 is the step size of the algorithm .    at a typical time @xmath51 ,",
    "the state is @xmath102 , and the values of @xmath103 , and @xmath104 are available from the previous iteration .",
    "we update @xmath42 and @xmath84 according to : + @xmath105 @xmath106 @xmath107",
    "0.55     0.55     0.55       in this section , we perform simulations using matlab to evaluate the performance of the proposed learning algorithm . in the experiment , we consider the scenario as depicted in fig .",
    "[ the system model 2 ] . the maximum queue size is set at @xmath108 units .",
    "there are three classes of users , namely , class-1 , class-2 , and class-3 , corresponding to requests from balloons , users on the ground , and satellites / aircrafts , respectively .",
    "the arrival rates of requests from users of class-1 , class-2 , and class-3 , are @xmath109 , @xmath110 , and @xmath108 requests per hour , respectively .",
    "when a request is accepted , the access point will use one unit of energy from the battery to serve for the request ( i.e. , to receive data and transmit the data to the destination ) . upon accepting requests",
    ", the access point receives the rewards of @xmath111 , @xmath112 , and @xmath113 monetary units for class-1 , class-2 , and class-3 , respectively .",
    "the energy arrival rate is @xmath114 per hour and the successful energy harvesting probability is @xmath115 .",
    "if the balloon harvest energy successfully and the battery is not full , the battery will increase one unit .",
    "for the learning algorithm , the initial parameter vector is set at @xmath116 , and the chosen initial estimated average reward is @xmath117 .",
    "we first consider the convergence of the proposed learning algorithm ( i.e. , algorithm  [ algorithm1 ] ) .",
    "figures  [ fig : sub_a ] and  [ fig : sub_b ] show the convergence in the terms of the average reward and the parameter vector . in both figures ,",
    "the proposed learning algorithm converges within around @xmath118-@xmath119 iterations . in fig .",
    "[ fig : sub_a ] , we also compare the average rewards obtained by the learning algorithm and the greedy policy that always accepts requests . at the convergence points",
    ", the average reward obtained by the learning algorithm reaches approximately @xmath120 which @xmath121% higher than that obtained by the greedy policy .    in fig .",
    "[ fig : sub_b ] , the parameter vector @xmath42 converges to ( -1.5577 , 4.3448 , 1.7029 ) for class-1 , class-2 , and class-3 , respectively .",
    "then , from the parameter vector obtained from the learning algorithm , we can determine the policy for the access point as shown in fig .",
    "[ fig : sub_c ] . in fig .",
    "[ fig : sub_c ] , the requests from other balloons will be always accepted , while the requests from users on the ground and satellites will only be accepted only when the energy level in the battery is high enough . in particular",
    ", the access point will accept the requests from a user on the ground and satellites when the energy level is higher than @xmath111 units and @xmath112 units , respectively .",
    "we then evaluate the impacts of the battery capacity to the performance of the system .",
    "specifically , in fig .",
    "[ performance_of_system_vary_energy_queue ] , we vary the maximum battery capacity and observe its impacts to the average number of accepted requests and the average reward of the access point . when the maximum battery capacity increases , the average reward and average number of accepted requests obtained by the learning algorithm ( la ) and the greedy policy ( gp ) will increase and saturate when the maximum battery capacity is greater than @xmath122",
    ". however , it is interesting that when the maximum battery capacity increases from @xmath111 to @xmath122 , the average number requests accepted by the learning algorithm is lower than that of the greedy policy .",
    "however , the average reward obtained by the learning algorithm is higher than that of the greedy policy",
    ". the reason can be found from the policy of the learning algorithm and the policy of the greedy policy as shown in fig .",
    "[ average_no_accepted_requests ] .",
    "while the greedy policy always accepts requests if the battery is not empty , the learning algorithm selectively accepts requests from class-2 and class-3 when the energy level is high enough .",
    "it is also worth to note that , when the maximum battery capacity is greater than @xmath122 , the average numbers of requests obtained by the learning algorithm and the greedy policy are equal . however , the average reward obtained by the learning algorithm is always greater than that of the greedy policy .",
    "the reason is because when the battery capacity is small , the amount of energy harvested will be limited and thus learning algorithm will accept requests which yield high reward and reject requests which yield low reward .",
    "when the battery capacity increases , the amount of energy harvested will increase , and thus , there is more chance for the requests with low reward to be accepted ( as shown in fig .",
    "[ average_no_accepted_requests ] ) .",
    "however , when the battery capacity is greater than @xmath122 , the performance of the system will be saturated .",
    "the reason is , the number of accepted requests depends not only the battery capacity , but also on the energy arrival rate . in other words ,",
    "the system performance is constrained by energy arrival , if the battery capacity is large enough .",
    "we fix the battery capacity at @xmath108 and vary the energy arrival rate",
    ". when the energy arrival rate increases from @xmath123 to @xmath124 , the probability of accepting requests by the greedy policy and the learning algorithm will increase . as a result , the average reward as well as the average energy harvested by both policies will also increase .",
    "moreover , as shown in fig .",
    "[ energy_arriv_impact ] , when the energy arrival rate is small , the average reward and the average energy in the battery with the learning algorithm are much greater than those of the greedy policy . however , when the energy arrival rate increases , the performance gap between the learning algorithm and greedy algorithm becomes smaller .",
    "eventually , when the energy arrival rate is large , the results obtained by the greedy policy will approach those of the learning algorithm .",
    "in this paper , we have studied and developed an optimization model for the optimal energy control problem for a network in the sky .",
    "the aim is to maximize the network performance as well as the profit for the network provider .",
    "we have first formulated the problem as a markov decision process and then applied an online learning algorithm based on the gradient method to obtain the optimal policy for the access point deployed on a balloon .",
    "the numerical results have been presented to show the impacts of parameters to the system performance as well as to show the convergence and the efficiency of the proposed learning algorithm .",
    "v.  raghunathan , a.  kansal , j.  hsu , j.  friedman , and m.  srivastava , `` design considerations for solar energy harvesting wireless embedded systems , '' in _ fourth international symposium on information processing in sensor networks _ , pp .",
    "457 - 462 , april 2005 .",
    "a.  georgiadis , g.  andia , and a.  collado , `` rectenna design and optimization using reciprocity theory and harmonic balance analysis for electromagnetic ( em ) energy harvesting , '' in _ ieee antennas and wireless propagation letters _ ,",
    "vol . 9 , pp .",
    "444 - 446 , 2010 ."
  ],
  "abstract_text": [
    "<S> google s project loon  @xcite was launched in 2013 with the aim of providing internet access to rural and remote areas . in the loon network , </S>",
    "<S> balloons travel around the earth and bring access points to the users who can not connect directly to the global wired internet . </S>",
    "<S> the signals from the users will be transmitted through the balloon network to the base stations connected to the internet service provider ( isp ) on earth . </S>",
    "<S> the process of transmitting and receiving data consume a certain amount of energy from the balloon , while the energy on balloons can not be supplied by stable power source or by replacing batteries frequently . instead , the balloons can harvest energy from natural energy sources , e.g. , solar energy , or from radio frequency energy by equipping with appropriate circuits . </S>",
    "<S> however , such kinds of energy sources are often dynamic and thus how to use this energy efficiently is the main goal of this paper . in this paper , we study the optimal energy allocation problem for the balloons such that network performance is optimized and the revenue for service providers is maximized . </S>",
    "<S> we first formulate the stochastic optimization problem as a markov decision process and then apply a learning algorithm based on simulation - based method to obtain optimal policies for the balloons . </S>",
    "<S> numerical results obtained by extensive simulations clearly show the efficiency and convergence of the proposed learning algorithm .    </S>",
    "<S> = 1    _ keywords- _ internet in the sky , google loon project , markov decision process . </S>"
  ]
}