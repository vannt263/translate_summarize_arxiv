{
  "article_text": [
    "recent successes of deep neural networks have spanned many domains , from computer vision @xcite to speech recognition @xcite and many other tasks . in particular ,",
    "sequence - to - sequence recurrent neural networks ( rnns ) with long short - term memory ( lstm ) cells @xcite have proven especially successful at natural language processing ( nlp ) tasks , including machine translation @xcite .    the basic sequence - to - sequence architecture for machine translation is composed of an rnn encoder which reads the source sentence one token at a time and transforms it into a fixed - sized state vector .",
    "this is followed by an rnn decoder , which generates the target sentence , one token at a time , from the state vector .",
    "while a pure sequence - to - sequence recurrent neural network can already obtain good translation results @xcite , it suffers from the fact that the whole sentence to be translated needs to be encoded into a single fixed - size vector .",
    "this clearly manifests itself in the degradation of translation quality on longer sentences ( see figure  [ fig : len ] ) and hurts even more when there is less training data @xcite .    in @xcite , a successful mechanism to overcome this problem",
    "was presented : a neural model of attention . in a sequence - to - sequence model with attention ,",
    "one retains the outputs of all steps of the encoder and concatenates them to a _ memory _",
    "tensor . at each step of the decoder , a probability distribution over this memory",
    "is computed and used to estimate a weighted average encoder representation to be used as input to the next decoder step .",
    "the decoder can hence focus on different parts of the encoder representation while producing tokens .",
    "figure  [ fig : attn ] illustrates a single step of this process .",
    "( 0 , 0.4 ) grid ( 3.0 , 0.6 ) ; ( -0.4 , 0.4 ) grid ( -0.2 , 0.6 ) ;    ( state ) at ( -0.3 , 0.8 ) state ; ( mem ) at ( 1.5 , 0.8 ) memory ;    ( 0.5 , 0.61 )  ( 0.5 , 1.09 ) ; ( -0.3 , 0.61 )  ( 0.5 , 1.09 ) ;    ( 0 , 1.09 ) rectangle ( 3.0 , 1.3 ) ; ( 0.4 , 1.09 ) rectangle ( 0.6 , 1.3 ) ; ( 0 , 0.99 ) grid ( 3.0 , 1.2 ) ;    ( mask ) at ( 1.5 , 1.5 ) mask over memory ;    ( 0.5 , 1.31 )  ( -0.3 , 1.79 ) ;    ( 0 , 1.79 ) grid ( 3.0 , 2.0 ) ; ( -0.4 , 1.79 ) grid ( -0.2 , 2.0 ) ;    ( nstate ) at ( -0.3 , 2.2 ) new state ; ( nmem ) at ( 1.5 , 2.2 ) new memory = memory ;    ( -0.3 , 0.61 ) edge [ bend left=30 ] ( -0.3 , 1.79 ) ;    the attention mechanism has proven useful well beyond the machine translation task .",
    "image models can benefit from attention too ; for instance , image captioning models can focus on the relevant parts of the image when describing it  @xcite ; generative models for images yield especially good results with attention , as was demonstrated by the draw model @xcite , where the network focuses on a part of the image to produce at a given time .",
    "another interesting use - case for the attention mechanism is the neural turing machine @xcite , which can learn basic algorithms and generalize beyond the length of the training instances .",
    "while the attention mechanism is very successful , one important limitation is built into its definition . since",
    "the attention mask is computed using a softmax , it by definition tries to focus on a _ single _ element of the memory it is attending to . in the extreme case ,",
    "also known as _ hard attention _",
    "@xcite , one of the memory elements is selected and the selection is trained using the reinforce algorithm ( since this is not differentiable )  @xcite . it is easy to demonstrate that this restriction can make some tasks almost unlearnable for an attention model . for example , consider the task of adding two decimal numbers , presented one after another like this :    [ cols=\"^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     one can see from table  [ tab : res ] that an active memory model can indeed match an attention model on the machine translation task , even with slightly fewer parameters .",
    "it is interesting to note that the active memory model does not need the length normalization that is necessary for the attention model ( esp . when rare words are spelled ) .",
    "we conjecture that active memory inherently generalizes better from shorter examples and makes decoding easier , a welcome news , since tuning decoders is a large problem in sequence - to - sequence models .",
    "in addition to the summary results from table  [ tab : res ] , we analyzed the performance of the models on sentences of different lengths .",
    "this was the key problem solved by the attention mechanism , so it is worth asking if active memory solves it as well . in figure",
    "[ fig : len ] we plot the bleu scores on the test set for sentences in each length bucket , bucketing by @xmath0 , i.e. , for lengths @xmath1 , ( 10 , 20]$ ] and so on .",
    "we plot the curves for the extended neural gpu model , the long baseline gru model with attention , and  for comparison  we add the numbers for a non - attention model from figure 2 of @xcite .",
    "( note that these numbers are for a model that uses different tokenization , so they are not fully comparable , but still provide a context . )    as can be seen , our active memory model is less sensitive to sentence length than the attention baseline .",
    "it indeed solves the problem that the attention mechanism was designed to solve .    [",
    "[ parsing . ] ] parsing . + + + + + + + +    in addition to the main large - scale translation task , we tested the extended neural gpu on english constituency parsing , the same task as in @xcite .",
    "we only used the standard wsj dataset for training .",
    "it is small by neural network standards , as it contains only 40k sentences .",
    "we trained the extended neural gpu with the same settings as above , only with @xmath2 ( instead of @xmath3 ) and dropout of @xmath4 in each step . during decoding",
    ", we selected well - bracketed outputs with the right number of pos - tags from all lengths considered .",
    "evaluated with the standard evalb tool on the standard wsj 23 test set , we got @xmath5 f1 score .",
    "this is lower than @xmath6 reported in @xcite , but we did nt use any of their optimizations ( no early stopping , no pos - tag substitution , no special tuning ) . since a pure sequence - to - sequence model has f1 score well below @xmath7 , this shows that the extended neural gpu is versatile and can learn and generalize well even on small data - sets .",
    "( 0,15 ) grid ( 60,31 ) ; in 0 , 10 , ... , 60 ( x ) at ( , 14.8 ) ; in 15 , 18 , ... , 30 ( x ) at ( -2 , ) ; ( bottom ) at ( 30 , 14 ) sentence length ; ( left ) at ( -5 , 18 ) bleu score ; ( 62 , 16.85 ) rectangle ( 63 , 17.15 ) ; ( l ) at ( 63 , 17 ) extended neural gpu ; ( 62 , 15.95 ) rectangle ( 63 , 16.25 ) ; ( l ) at ( 63 , 16.1 ) gru+attention ; ( 62 , 15.05 ) rectangle ( 63 , 15.35 ) ; ( l ) at ( 63 , 15.2 ) no attention ; plot coordinates ( 5 , 23.6 ) ( 15 , 28.4 ) ( 25 , 30.8 ) ( 35 , 30.3 ) ( 45 , 29.4 ) ( 55 , 30.3 ) ; plot coordinates ( 5 , 20.6 ) ( 15 , 24.7 ) ( 25 , 27.7 ) ( 35 , 27.2 ) ( 45 , 26.4 ) ( 55 , 23.2 ) ; plot coordinates ( 5 , 19.7 ) ( 15 , 22.0 ) ( 25 , 23.0 ) ( 35 , 21.1 ) ( 45 , 19.2 ) ( 55 , 14.8 ) ;",
    "to better understand the main shortcoming of previous active memory models , let us look at the average log - perplexities of different attention models in table  [ tab : res ] . a pure neural gpu model yields @xmath8 , a markovian one yields @xmath9 , and only a model with full dependence , trained with teacher forcing , achieves @xmath10 .",
    "the recurrent dependence in generating the output distribution turns out to be the key to achieving good performance .",
    "we find it illuminating that the issue of dependencies in the output distribution can be disentangled from the particularities of the model or model class . in earlier works ,",
    "such dependence ( and training with teacher forcing ) was always used in lstm and gru models , but very rarely in other kinds models .",
    "we show that it can be beneficial to consider this issue separately from the model architecture .",
    "it allows us to create the extended neural gpu and this way of thinking might also prove fruitful for other classes of models .",
    "when the issue of recurrent output dependencies is addressed , as we do in the extended neural gpu , an active memory model can indeed match or exceed attention models on a large - scale real - world task .",
    "does this mean we can always replace attention by active memory ?    the answer could be * yes * for the case of soft attention .",
    "its cost is approximately the same as active memory , it performs much worse on some tasks like learning algorithms , and  with the introduction of the extended neural gpu  we do not know of a task where it performs clearly better .    still , an attention mask is a very natural concept , and it is probable that some tasks can benefit from a selector that focuses on single items by definition .",
    "this is especially obvious for hard attention : it can be used over large memories with potentially much less computational cost than an active memory , so it might be indispensable for devising long - term memory mechanisms .",
    "luckily , active memory and attention are not exclusive , and we look forward to investigating models that combine these mechanisms .",
    "kyunghyun cho , bart van merrienboer , caglar gulcehre , fethi bougares , holger schwenk , and yoshua bengio .",
    "learning phrase representations using rnn encoder - decoder for statistical machine translation . , abs/1406.1078 , 2014 .",
    "kelvin xu , jimmy  lei ba , ryan kiros , kyunghyun cho , aaron courville , ruslan salakhutdinov , richard  s. zemel , and yoshua bengio .",
    "show , attend and tell : neural image caption generation with visual attention . in _",
    "icml _ , 2015 .",
    "xingjian shi , zhourong chen , hao wang , dit - yan yeung , wai kin wong , and wang chun woo .",
    "convolutional lstm network : a machine learning approach for precipitation nowcasting . in _ advances in neural information processing systems _ , 2015 .",
    "george toderici , sean  m. omalley , sung  jin hwang , damien vincent , david minnen , shumeet baluja , michele covell , and rahul sukthankar .",
    "variable rate image compression with recurrent neural networks . in _ international conference on learning representations _ , 2016 .",
    "martn abadi , ashish agarwal , paul barham , eugene brevdo , zhifeng chen , craig citro , greg corrado , andy davis , jeffrey dean , matthieu devin , sanjay ghemawat , ian goodfellow , andrew harp , geoffrey irving , michael isard , yangqing jia , rafal jozefowicz , lukasz kaiser , manjunath kudlur , josh levenberg , dan man , rajat monga , sherry moore , derek murray , chris olah , mike schuster , jonathon shlens , benoit steiner , ilya sutskever , kunal talwar , paul tucker , vincent vanhoucke , vijay vasudevan , fernanda vigas , oriol vinyals , pete warden , martin wattenberg , martin wicke , yuan yu , and xiaoqiang zheng .",
    "tensorflow : large - scale machine learning on heterogeneous distributed systems , 2015 ."
  ],
  "abstract_text": [
    "<S> several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years . </S>",
    "<S> attention has improved image classification , image captioning , speech recognition , generative models , and learning algorithmic tasks , but it had probably the largest impact on neural machine translation .    </S>",
    "<S> recently , similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel , in a uniform way . such mechanism , which we call _ active memory _ , improved over attention in algorithmic tasks , image processing , and in generative modelling .    </S>",
    "<S> so far , however , active memory has not improved over attention for most natural language processing tasks , in particular for machine translation . </S>",
    "<S> we analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences . </S>",
    "<S> we investigate this model and explain why previous active memory models did not succeed . </S>",
    "<S> finally , we discuss when active memory brings most benefits and where attention can be a better choice . </S>"
  ]
}