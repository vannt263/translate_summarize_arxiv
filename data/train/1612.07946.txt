{
  "article_text": [
    "in statistical decision theory , a bayes estimator for a prior @xmath3 is a point estimator whose expected risk ( with respect to @xmath3 ) is as small as possible @xcite .",
    "bayes estimators have utility beyond strictly bayesian statistics ; they also provide lower bounds on _ minimax _ risk ( a frequentist concept ) , and the greatest lower bound ( that is , the maximum bayes risk over all priors ) coincides with the minimax risk .",
    "this duality enables efficient numerical algorithms to find minimax estimators by optimizing over bayesian priors , rather than over the computationally intractable space of estimators @xcite .",
    "different loss functions produce different risk functions , and the bayes estimator is sensitive to this choice . for a certain class of risks known as bregman divergences",
    "the bayes estimator is always the mean of the posterior distribution @xcite . for the multinomial model that we consider , both mean - squared - error and kullback - leibler divergence ( relative entropy ) are bregman divergences .",
    "but not every useful risk function has this property .",
    "we consider two loss functions defined through the bhattacharyya coefficient @xmath4 , a fisher - adjusted measure of distinguishability between probability distributions that finds use in machine learning ( as a metric on probability distributions @xcite ) and in quantum information theory ( as _ fidelity _ @xcite , the most commonly used measure of quantum state distinguishability in both theoretical and experimental studies ) .    in this paper , we find the bayes estimators for @xmath0 and @xmath1 by mapping their respective optimizations to simple linear programs .",
    "we also state , but do not solve , the non - commutative generalization of the problem , which has applications in quantum information theory .",
    "finally , we demonstrate the utility of the bayes estimator by finding the minimax estimator for a binomial parameter .",
    "suppose we have a coin , a @xmath5-sided die , or any process that generates i.i.d .",
    "samples from a finite set of size @xmath5 .",
    "we wish to estimate the probabilities @xmath6 .",
    "they form a parameter vector @xmath7 which belongs to the @xmath5-simplex @xmath8 estimation requires some data . from the data , we wish to produce an estimate of @xmath7 , call it @xmath9 . although the data are _ most _ commonly obtained by repeated sampling directly from @xmath7 , other experiments are possible .",
    "we might have `` noisy '' data ( drawn from an ancillary distribution @xmath10 , where the stochastic matrix @xmath11 is known @xcite ) , or side information .",
    "happily , the bayes analysis turns out to be independent of how the data were obtained .",
    "instead , it depends only on the posterior probability density @xmath12which is to say that , given the posterior , the expected risk is independent of the likelihood @xmath13 .",
    "the quality of an estimate @xmath9 is formalized by a loss function @xmath14 that quantifies how bad it is to report an estimate @xmath9 when the truth is @xmath7 .",
    "the best estimate should be @xmath7 itself , so @xmath15 for all @xmath9 .",
    "a `` good '' estimator is one that minimizes expected loss , or _ risk _ :",
    "@xmath16.\\ ] ] the risk is a function of the underlying @xmath7 , and it is generally impossible to minimize risk for _ all _ @xmath7 simultaneously .",
    "the bayesian solution is to consider ( and minimize ) the average risk with respect to a given prior @xmath17 .",
    "this is called the _ bayes risk _ of the estimator @xmath9 with respect to @xmath3 : @xmath18.\\ ] ] an estimator @xmath19 that minimizes the bayes risk is called a _ bayes estimator _ for @xmath17 .",
    "the bayes estimator can also be defined ( under normal circumstances ; @xcite ) explicitly for each dataset as the minimizer of _ posterior _ risk .",
    "this identity is extremely useful , because it means that a bayes estimator can be constructed explicitly for any given value of ` data ` without considering others .",
    "different loss functions yield different bayes estimators .",
    "for example , for quadratic loss functions of the form @xmath20 where @xmath21 is a positive definite matrix , the bayes estimator is the mean of the posterior distribution @xcite @xmath22.\\ ] ] another important loss function is the _ relative entropy _ or kullback - leibler divergence : @xmath23 here , again , the bayes estimator is the posterior mean @xcite .",
    "this is not an accident ; it holds true whenever the loss is a _ bregman divergence _ @xcite .",
    "quadratic loss and kullback - leibler loss are both bregman divergences .",
    "unfortunately , not every useful and important loss function is a bregman divergence .",
    "the loss functions we consider here are not bregman divergences , and their bayes estimators are not the posterior mean .",
    "they are based on the bhattacharyya coefficient @xcite , @xmath24 which quantifies the statistical indistinguishability or _",
    "fidelity _ of two distributions . in different contexts , both @xmath25 and @xmath26 are useful loss functions , and so we find bayes estimators for both of them . in the limit of small deviations (",
    "@xmath9 is close to @xmath7 ) , they coincide up to a factor of 2 .",
    "both are fisher - adjusted , meaning that their 2nd order series expansions are proportional to the fisher metric .",
    "however , @xmath27 and @xmath28 have different global behavior , and therefore different bayes estimators .    since the minimizer of @xmath29 is the maximizer of @xmath30 , we define two bayes estimators as follows : @xmath31 , \\label{p_1}\\\\      \\hat { \\vec p}_{2 } & = \\underset{\\hat { \\vec p}}{\\rm argmax } \\;\\mathbb e_{\\vec p}[b^2(\\vec p , \\hat{\\vec p})],\\label{p_2}\\end{aligned}\\ ] ] subject to @xmath32 . note that , since the bayes estimator depends only on the posterior , with no explicit dependence on the prior or the data , we hereafter drop the conditional notation on the data .",
    "that is , @xmath33 means expectation with respect to an arbitrary distribution of @xmath7 .",
    "let us now derive expressions for the bayes estimators defined in equations [ p_1 ] and [ p_2 ] . a very useful way of writing @xmath34 is to define @xmath35 as the _ element - wise _ square root of @xmath36 , then write : @xmath37 the posterior risk involves an average of @xmath34 over @xmath7 . both this average and the dot product in @xmath34 are _ linear _ in @xmath35 , which means that they commute , so :",
    "@xmath38 = \\mathbb e_{\\vec p}[\\sqrt{\\vec p}]\\cdot \\sqrt{{\\hat{\\vec p}}}.\\ ] ] this also implies the normalization @xmath39 thus , the problem in becomes @xmath40\\cdot \\sqrt{{\\hat{\\vec p } } } \\\\ & \\text{subject to } & & \\sqrt{\\hat{\\vec p}}\\cdot \\sqrt{\\hat{\\vec p } } = 1 , \\\\ & & & \\sqrt{\\hat{\\vec p } } \\geq 0 .",
    "\\end{aligned}\\ ] ] this is a textbook @xcite convex optimization problem , and the solution is @xmath41 }                                    { \\| \\mathbb e_{\\vec p}[\\sqrt{\\vec p } ] \\|},\\ ] ] implying @xmath42 ^ 2 }                             { ( \\mathbb e_{\\vec p}[\\sqrt{\\vec p}])\\cdot(\\mathbb e_{\\vec p}[\\sqrt{\\vec p } ] ) } , \\label{eq : p1sol}\\ ] ] where the square in the numerator is _ element - wise_.    minimizing @xmath1 , as in , requires a bit more work , but we can again reduce it to a linear program .",
    "we start by defining @xmath43 the projection operator onto @xmath35 , and similarly for @xmath44 .",
    "the normalization condition on @xmath45 is @xmath46 = 1.\\ ] ] in terms of these objects , the squared bhattacharyya coefficient is @xmath47.\\ ] ] this expression is now linear in @xmath45 , which allows us to apply the trick of commuting the expectation through : @xmath48 = { \\mathrm{tr}}[\\mathbb e_{\\vec p}[p]\\hat p].\\ ] ] the matrix elements of the expectation on the rhs can be written out explicitly as @xmath49\\right)_{ij } = \\mathbb e_{\\vec p}[\\sqrt{p_i p_j}].\\ ] ] putting everything together , becomes @xmath50\\hat p ] \\\\ & \\text{subject to } & & { \\mathrm{tr}}[\\hat p ] = 1 , \\\\ & & & \\hat p \\geq 0 , \\end{aligned}\\ ] ] where @xmath51 means positive semi - definiteness as a matrix .",
    "this is another textbook @xcite linear semidefinite program .",
    "if we define @xmath52 as the normalized eigenvector of @xmath53 $ ] with maximal eigenvalue , then the solution to is @xmath54 explicitly , as a vector in @xmath55 , we have @xmath56 where @xmath57 is the eigenvector of @xmath58 $ ] with maximal eigenvalue , and the square is element - wise as in eq .",
    "[ eq : p1sol ] .",
    "quantum mechanics can be described as a non - commutative generalization of probability theory @xcite .",
    "quantum random variables ( that is , the `` state '' of a quantum system ) are represented not by @xmath5-element vectors of probabilities @xmath7 , but by @xmath59 self - adjoint complex matrices or _",
    "density matrices_. in this framework , observable events are also represented by @xmath59 self - adjoint matrices called _ effects_. states can be sampled or observed in more than one way . the quantum analogue to `` sampling '' is `` measuring '' the system , and the observer must choose _ how _ to measure it .",
    "this means specifying a mutually exclusive and exhaustive set of possible _ events _ , each represented by a positive semidefinite @xmath60 matrix .",
    "the set of event matrices @xmath61 is called a _ positive operator valued measure _ ( povm ) , and satisfies the condition @xmath62 .",
    "if an quantum system is state @xmath63 is observed , then the probability of observing the event represented by effect @xmath64 is given by @xmath65 .",
    "as a result , the law of total probability ( `` exactly one of the possible events occurs '' ) manifests itself as the constraint @xmath66 , and the non - negativity of probabilities corresponds to @xmath67 .",
    "thus , the set of quantum states ( analogous to the simplex @xmath68 of valid probability vectors ) is @xmath69 the subset of @xmath70 containing only diagonal matrices ( `` classical '' states ) is in 1:1 correspondence with @xmath5-element probability vectors , since @xmath71 ensures positivity of the diagonal entries , and @xmath72 ensures their normalization .",
    "thus , if we consider _ only _ states that are simultaneously diagonalizable , we can identify @xmath73 and @xmath74 , and the bhattacharyya coefficient has a simple expression in terms of @xmath63 and @xmath75 : @xmath76 its square  the @xmath77 risk that we considered previously  is called _ classical fidelity _ in the quantum mechanics literature .",
    "hence , our solution to can be applied directly to estimation of quantum states , whenever the support of the averaging measure ( e.g. a posterior distribution ) consists entirely of _ commuting _ quantum states : @xmath78 \\\\ & \\text{subject to } & & { \\mathrm{tr}}(\\hat\\rho ) = 1 , \\\\ & & & \\hat\\rho \\geq 0 . \\end{aligned}\\ ] ]    however , quantum states are not usually restricted to be diagonal , which means that in the estimation of quantum states ( _ quantum tomography _",
    "@xcite ) , the posterior will have support on non - commuting density matrices .",
    "this requires new definitions of loss and risk . without getting into the details ( see ref .",
    "@xcite ) , the general definition of _ quantum fidelity _ between two states is : @xmath79 ^ 2,\\ ] ] and the corresponding loss function is _ infidelity _ or @xmath80 . for co - diagonal states",
    "@xmath75 and @xmath63 , fidelity coincides with @xmath77 .",
    "thus , if the posterior _ does _ happen to be supported only on mutually commuting states , then our solution to gives the bayes estimator .",
    "in general , however , the problem of finding a bayes estimator for quantum infidelity is : @xmath81 \\\\ & \\text{subject to } & & { \\mathrm{tr}}(\\hat\\rho ) = 1 , \\\\ & & & \\hat\\rho \\geq 0 . \\end{aligned}\\ ] ] this problem appears to be quite difficult ( some bounds are given in @xcite ) , and we do not attempt to solve it in full generality here .",
    "as an interesting and useful application of our main result , we now compute ( and examine ) the bayes estimator @xmath82 for @xmath26 in the simple case of a binomial distribution  that is , a directly observed coin toss or bernoulli process . here , @xmath83 , with @xmath84 .",
    "the canonical ( conjugate ) prior is a beta distribution : @xmath85 where @xmath86 is the beta function .",
    "if the coin is flipped @xmath87 times , yielding @xmath88 heads and @xmath89 tails , then the posterior is @xmath90 the matrix @xmath91 $ ] is given explicitly in terms of beta functions by @xmath92 to compute the bayes estimator @xmath82 , we diagonalize this matrix , extract the eigenvector corresponding to the larger eigenvalue , and square its entries .",
    "this can be written down in closed form , but the result is lengthy , messy , and opaque . instead",
    ", we illustrate its properties for @xmath93 flips and for conjugate priors with @xmath94 ( laplace s prior ) and @xmath95 ( jeffrey s prior ) , in fig . [",
    "fig : estimators_n10 ] . for comparison ,",
    "we also show ( 1 ) the posterior mean estimator , and ( 2 ) the maximum likelihood estimator .     the maximum likelihood estimator ( mle ) , optimal bayes estimator and the posterior mean for @xmath93 coin flips . on the left , the latter two estimators begin with a canonical beta prior with @xmath95 ( jeffrey s prior ) . on the right , the same but with @xmath94 ( laplace s uniform prior ) .",
    "the fidelity performance of these estimators is shown in figure [ fig : fid_n10].,title=\"fig : \" ]   the maximum likelihood estimator ( mle ) , optimal bayes estimator and the posterior mean for @xmath93 coin flips . on the left ,",
    "the latter two estimators begin with a canonical beta prior with @xmath95 ( jeffrey s prior ) . on the right ,",
    "the same but with @xmath94 ( laplace s uniform prior ) .",
    "the fidelity performance of these estimators is shown in figure [ fig : fid_n10].,title=\"fig : \" ]    the bayes estimator is generally similar to the posterior mean estimator .",
    "both `` hedge '' away from @xmath96 and @xmath97 , in contrast to the mle , but the bayes estimator is more aggressive ( less hedged ) than the mean .",
    "since the mean estimator is bayes for bregman divergences , this implies that the @xmath1 loss function is more forgiving of extreme estimates ( that is , ones close to @xmath96 or @xmath97 ) than any bregman divergence .",
    "however , although the difference between the two estimators ( bayes and posterior mean ) appears significant , the difference in their _ performance _ is not . while the bayes estimator does have lower bayes risk , the difference is small even for @xmath93 .",
    "figure [ figreldiff_n10 ] shows the _ relative suboptimality _ of the mean estimator , defined as : @xmath98 where @xmath99 $ ] and likewise for @xmath100 .",
    "the relative suboptimality of the mean estimator  even at @xmath93is always less than @xmath101 , decreases rapidly as a function of @xmath87 and is largely independent of @xmath102 .",
    "given that the mean estimator is far easier to calculate ( the bayes estimator requires computing the matrix @xmath91 $ ] and its eigenvectors ) , we suggest that in practice for most purposes , the mean estimator is a completely satisfactory heuristic . for small @xmath87 , there is a noticeable difference in the estimate itself ( see fig .",
    "[ fig : fid_n10 ] ) , but _ not _ in performance ( see fig . [ figreldiff_n10 ] ) .     the relative difference , @xmath103 in eq .",
    ", between the average loss of the bayes estimator @xmath100 and that of the mean estimator @xmath104 . ]",
    "frequentist analyses of estimators average the risk _ only _ over the data ( that is with respect to @xmath13 ) , rather than over the joint distribution @xmath105 .",
    "this more restricted average defines _ pointwise _ risk , @xmath106,\\ ] ] which retains a dependence on @xmath7 that has to be gotten rid of somehow in order to define an `` optimal '' estimator . instead of averaging over a prior ( to get the bayes risk in eq .",
    ") , the most common frequentist way to remove this dependence is to consider the `` worst case '' , and maximize the risk over @xmath7 : @xmath107    an estimator that minimizes the maximum risk is called a _",
    "minimax _ estimator : @xmath108 the minimax criterion seeks an estimator whose performance is `` pretty good '' for any value of the parameters , without reference to prior probability .",
    "in practice , minimax estimators achieve ( approximately ) equal risk for all @xmath7 .",
    "the bayes estimators for the binomial parameter that we derived in the previous section are not minimax ; figure [ fig : fid_n10 ] shows that their pointwise risk varies with @xmath109 .",
    "however , they re pretty close  the variations are small ( e.g. , compared with those of the mle ) .     the pointwise risk ( @xmath1 ) of the estimators shown in figure [ fig : estimators_n10 ] , as a function of @xmath109.,title=\"fig : \" ]   the pointwise risk ( @xmath1 ) of the estimators shown in figure [ fig : estimators_n10 ] , as a function of @xmath109.,title=\"fig : \" ]    however , while the minimax estimator is unique , each prior has a different bayes estimator .",
    "we can rewrite eq .",
    ", making this explicit , letting @xmath3 denote the prior density : @xmath110\\ ] ] somewhat remarkably , there _ is _",
    "( at least under weak regularity conditions @xcite ) a prior whose bayes estimator is minimax .",
    "least favorable prior _ is the prior whose bayes estimator has the highest bayes risk ( out of all priors ) , and its bayes estimator is minimax .",
    "explicitly , the estimator is @xmath111 this remarkable relation between bayesian and frequentist optimality is called _ bayes - minimax duality _ :",
    "@xmath112    one useful consequence of bayes - minimax duality is that it provides a method for finding minimax estimators by searching for the least favorable prior .",
    "while the space of priors is large , it is enormously smaller than the space of _",
    "estimators_. however , this approach is only practical if we have a closed - form expression for bayes estimators , since this allows fast and efficient computation of the bayes risk for each prior considered .",
    "our result enables this sort of analysis for @xmath0 and @xmath1 loss functions .",
    "we used our result to construct minimax estimators for the binomial parameter .",
    "unlike the bayes estimator , the minimax estimators do not have explicit closed forms ; the least favorable priors are not elegant , and require numerical optimization .",
    "we performed two numerical optimizations .",
    "first , we did a _ restricted _ optimization over conjugate priors ( beta distributions , of the form given in eq .",
    "[ eq : beta ] ) , to find the minimax value of @xmath102 .",
    "then , we performed an unrestricted optimization over _ all _ priors to find the true minimax estimator    in the first ( restricted ) optimization over conjugate priors , we found that the optimal @xmath102 varied only weakly with @xmath87 , and was given by @xmath113 for all @xmath87 .",
    "this is extremely consistent with other answers to the general question `` what value of @xmath102 works best ? '' ; a variety of other analyses have yielded answers close to the @xmath95 that defines jeffrey s estimator . just for reference",
    ", we considered what prior would yield the lowest maximum risk if we were to use the posterior mean estimator rather than the bayes estimator derived in this paper . for this ad - hoc procedure",
    ", the optimal @xmath102 is approximately @xmath114 .",
    "this estimator is not minimax in any sense , but it does support our general observation that the bayes estimator can be approximated by the posterior mean without much damage ( @xmath115 and @xmath116 are not very different , and the achieved maximum risks are also not very different ) .    to find the true minimax estimator , we used the algorithm of kempthorne @xcite to find the least favorable prior .",
    "roughly speaking , this algorithm iteratively adds support points to a discrete prior , each time maximizing the bayes risk over the small finite search space of support points and weights .",
    "it proceeds until the average risk and the maximum risk are within a pre - defined tolerance .",
    "we provide pseudo - code for our implementation of the algorithm in algorithm [ kempthorne ] .",
    "the results for the binomial parameter for @xmath93 are plotted in figure [ fig : minimax_n10 ] .",
    "number of measurements @xmath117 .",
    "support points of initial guess @xmath36 .",
    "probability weights of the support points @xmath118 .",
    "tolerance @xmath119 ( default @xmath101 ) .",
    "mixing parameter @xmath120 ( default 0.01 ) .",
    "least favorable prior @xmath121 .",
    "lower bound on the minimax risk @xmath122 .",
    "upper bound on the minimax risk @xmath123 .",
    "@xmath124 # make sure we enter the loop @xmath125 prior with same number of support points which maximizes the bayes risk @xmath126 the maximum value of the bayes risk for the prior found above @xmath127 global maximum of risk using the bayes estimator of @xmath121 @xmath128 add a new support where the maximum risk is attained @xmath129 for each @xmath130 , @xmath131 @xmath132     in this figure , we compare three estimators : ( 1 ) the minimax estimator , which is the bayes estimator for the least favorable prior ; ( 2 ) the bayes estimator for the least favorable _ conjugate _ prior , given by @xmath133 ; and ( 3 ) the posterior mean estimator for the least favorable [ with respect to the posterior mean estimator ] conjugate prior , given by @xmath114 .",
    "on the left , we show the estimators themselves . on the right",
    ", we show their pointwise fidelity ( 1-risk ) as a function of @xmath109 .",
    "@xmath93 in all cases.,title=\"fig : \" ]   in this figure , we compare three estimators : ( 1 ) the minimax estimator , which is the bayes estimator for the least favorable prior ; ( 2 ) the bayes estimator for the least favorable _ conjugate _ prior , given by @xmath133 ; and ( 3 ) the posterior mean estimator for the least favorable [ with respect to the posterior mean estimator ] conjugate prior , given by @xmath114 .",
    "on the left , we show the estimators themselves . on the right",
    ", we show their pointwise fidelity ( 1-risk ) as a function of @xmath109 .",
    "@xmath93 in all cases.,title=\"fig : \" ]",
    "optimality of estimators is an always - relevant topic in statistics . even when optimal estimators are intractable or impractical , they provide a useful benchmark , and make it possible to show rigorously that some more tractable estimator is `` good enough '' .",
    "our main technical contributions in this paper are simple , constructive formulas to compute bayes estimators for `` fidelity''-type loss functions based on the bhattacharyya coefficient",
    ". this result may be directly useful for bayesian machine learning , quantum state estimation , and other inference problems where it ( 1 ) provides a more accurate estimator , and ( 2 ) establishes a provable bound on performance .",
    "we find the examples provided in the second half of our paper interesting because they suggest certain qualitative properties of multinomial estimation .",
    "two of the most widely used estimators for this problem are the mle and the posterior mean ( which is bayes - optimal for bregman divergences ) .",
    "our results indicate that the bayes estimator for bhattacharya loss interpolates between the mle and the posterior mean ; it hedges away from @xmath96 like the mean , but not as much .",
    "most interestingly , our analysis shows that all the estimators we considered have nearly identical average risk  and therefore suggests that worrying about finding exact bayes estimators may be unnecessary .",
    "superficially , this may appear to undercut our result . who cares about deriving the bayes estimator if it is nt significantly better ?",
    "but this is exactly the point : our result _ proves _ that the posterior mean is `` good enough '' , at least in this particular case .",
    "furthermore , it shows that although the bayes and mean _ estimators _ are visibly different , their performance is not .",
    "this suggests that  at least for bhattacharya loss functions  a rather wide range of estimators achieve near - optimal performance .",
    "this conclusion is reinforced by the behavior of the minimax ( frequentist - optimal ) estimators that we construct using our result , where we found that small changes in the precise definition of `` minimax '' produced fairly large changes in the `` optimal '' estimator . and ,",
    "while we did not fully solve the quantum problem ( by finding bayes estimators for quantum fidelity ) , we hope that our partial solution ( for commuting states ) provides a stepping stone to a full solution in the future .",
    "cf acknowledges funding from the iarpa mqco program , the arc via equs project number ce11001013 , and by the us army research office grant numbers w911nf-14 - 1 - 0098 and w911nf-14 - 1 - 0103 .",
    "sandia national laboratories is a multi - mission laboratory managed and operated by sandia corporation , a wholly owned subsidiary of lockheed martin corporation , for the u.s .",
    "department of energy s national nuclear security administration under contract de - ac04 - 94al85000 .",
    "a. banerjee , xin guo and hui wang , _ on the optimality of conditional expectation as a bregman predictor _ , http://dx.doi.org/10.1109/tit.2005.850145[ieee transactions on information theory * 51 * , 2664 ( 2005 ) ] .",
    "a. djouadi , o .",
    "snorrason and f.  d. garber , _ the quality of training sample estimates of the bhattacharyya coefficient _ , http://dx.doi.org/10.1109/34.41388[ieee transactions on pattern analysis and machine intelligence * 12 * , 92 ( 1990 ) ] .",
    "l. m. artiles , r. d. gill and m. i. guta , _ an invitation to quantum tomography _ , http://dx.doi.org/10.1111/j.1467-9868.2005.00491.x[journal of the royal statistical society : series b ( statistical methodology ) , * 67 * , 109 ( 2005 ) ] .",
    "b. s. clarke and a. r. barron , _",
    "jeffreys prior is asymptotically least favorable under entropy risk_,http://dx.doi.org/10.1016/0378 - 3758(94)90153 - 8[journal of statistical planning and inference * 41 * , 37 ( 1994 ) ] ."
  ],
  "abstract_text": [
    "<S> we derive the bayes estimator for the parameters of a multinomial distribution under two loss functions ( @xmath0 and @xmath1 ) that are based on the bhattacharyya coefficient @xmath2 . </S>",
    "<S> we formulate a non - commutative generalization relevant to quantum probability theory as an open problem . as an example application </S>",
    "<S> , we use our solution to find minimax estimators for a binomial parameter under bhattacharyya loss ( @xmath1 ) . </S>"
  ]
}