{
  "article_text": [
    "this paper is concerned with the analysis of _ the mean squared derivative cost function _ : given @xmath0 , the mean squared derivative cost function is defined by @xmath1 where the infimum is taken over all curves @xmath2,\\r^d)$ ] that satisfies the boundary conditions @xmath3 finally , the pre - factor @xmath4 is a scaling factor .",
    "it is a constant and does not affect the analysis of the minimizing problem .",
    "however , for consistence with many applications , we prefer to keep this pre - factor .",
    "this cost function plays a central role in various practical applications and theoretical research . for the motivation of this paper",
    ", we now review some important literature here .      _",
    "applications in engineering and applied sciences_. in the literature , the minimizing problem for @xmath5 are respectively called the minimum principles of acceleration , jerk , snap , crackle , and pop ; see for instance @xcite .",
    "the minimum jerk principle was initially used to model velocity profiles generated by elbow movements @xcite and latter extended to trajectory prediction for reaching movements between visual targets in the horizontal plan and to curved and obstacle - avoidance movements @xcite . since",
    "then the minimum jerk principle and other mean squared derivative cost functions have been applied to many other fields and the mathematical models based on this cost functions fit well with experimental results .",
    "examples of such applications include motor control @xcite , biometrics and online - signatures @xcite and robotics @xcite , just to name a few .",
    "we refer to the mentioned papers and references therein for more information .",
    "_ usages in theoretical research_. the mean squared derivative cost functions also appear in theoretical research such as in optimal transport theory and partial differential equations .",
    "the cost function @xmath6 between two points @xmath7 in the euclidean space @xmath8 can be used to define a monge - kantorovich optimal transport cost function between two probability measures @xmath9 and @xmath10 on @xmath8 as follows @xmath11 where @xmath12 denotes the set of all probability measures on @xmath13 having @xmath14 and @xmath15 as the first and second marginals .",
    "the monge - kantorovich cost function is the central object in optimal transport theory with many applications in other fields of mathematics and economics see for instance @xcite for nice expositions on optimal transport theory and its applications . in particular , for @xmath16 , then @xmath17 and @xmath18 becomes the well - known wasserstein distance .",
    "when @xmath19 , then @xmath20 and @xmath18 is the minimal acceleration cost function .",
    "both the wasserstein metric and the minimal acceleration cost function have been studied extensively in the fields of partial differential equation .",
    "the former form an important class , namely the wasserstein gradient flows , of dissipative evolution equations , see for instance @xcite .",
    "the latter has been used to construct variational formulations for many evolution equations such as the kramers ( kinetic fokker - planck ) equation @xcite , the system of isentropic euler equations @xcite and the compressible euler equations @xcite .",
    "another aspect that the cost function @xmath6 plays an important role is that it is related to _ the fundamental solution _ of partial differential equations .",
    "for instance , the function @xmath21 is the fundamental solution of the diffusion / heat equation .",
    "while the function @xmath22 where @xmath23 is a normalising constant , is the fundamental solution to the following ultra - parabolic equation , see for instance @xcite , @xmath24 ultra - parabolic equations have been investigated extensively in many papers from various aspects such as trends to equilibrium @xcite , gaussian - estimates for the fundamental solution @xcite , connections to particle systems and coarse - graining @xcite .    when @xmath25 , we expect that @xmath6 is also connected to the fundamental solution of some evolution equation and that @xmath6 and the corresponding monge - kantorovich cost function @xmath18 can be used to establish variational formulation for this evolution equation .",
    "we will investigate this issue in a separate article .",
    "although the cost function has a wide range of applications as reviewed in the previous section , it has not been analysed thoroughly in the literature . only special cases corresponding to small @xmath26 and special boundary conditions",
    "have been investigated   @xcite . in the literature , there exists no explicit formula as well as computational methods for the general case .",
    "mathematically , @xmath6 is a constrained minimizing problem and a thorough analysis of this function is interesting on its own .",
    "the aim of this paper is to provide such an analysis .",
    "we show some qualitative properties of the cost function @xmath6 , obtain explicitly analytical formula , and offer some computational methods and numerical verifications .",
    "we now describe our main results .",
    "the detailed statements will be given in the subsequent sections .",
    "our first result concerns the qualitative behaviour of the cost function as function of @xmath4 and @xmath26 .",
    "[ theo : qual theo ] the cost function @xmath6 is scalable with respect to @xmath4 and monotone increasing with respect to @xmath26 .",
    "the full description and proof of this theorem is given in theorem [ theo : qual theo 2 ] in section [ sec : qualitative ] .",
    "moreover , we also provide an interpretation of the cost function based on the theory of large - deviation .",
    "our second theorem is an explicitly analytical formula for the cost function .",
    "[ theo : explicit formula ] the cost function @xmath6 has an explicit formula as @xmath27^{-1}\\mathbf{b}_n(h),\\ ] ] where the vector @xmath28 and the two matrices @xmath29 and @xmath30 are given explicitly in , and respectively .",
    "the proof of this theorem is given in section [ sec : formula ] .",
    "the last theorem provides explicit formulas for the @xmath31 decomposition of @xmath32 and for @xmath33 .",
    "[ theo : lu of a ] the matrix @xmath34 has a @xmath31-decomposition as in - .",
    "the inverse of the matrices @xmath35 and @xmath36 are given explicitly in - .",
    "we prove this theorem in section [ sec : lu ] ( cf .",
    "theorem [ theo : lu of a 2 ] ) .",
    "as will be shown there , the matrix @xmath34 is a wronskian matrix , which plays an important role in linear algebra and differential equations ; hence , this theorem is of independent interest .",
    "the rest of this paper is structured as follows . in section [ sec : qualitative ] , we study some qualitative properties of the cost function @xmath6 . in section [ sec : formula ]",
    "we provide an explicitly formula for @xmath6 .",
    "the @xmath31-decomposition of the matrix @xmath37 is presented in section [ sec : lu ] . in section",
    "[ sec : simulations ] we show some numerical demonstrations .",
    "in this section , we provide the full description and proof of theorem [ theo : qual theo ] on qualitative properties of the cost function , which characterise its behaviour as functions of @xmath26 and @xmath4 .      [",
    "theo : qual theo 2 ]    1 .",
    "( scaling property of the cost functions ) .",
    "it holds that @xmath38 where the infimum is taken over the curves @xmath39,\\r^{d})$ ] such that @xmath40 2 .",
    "( monotonicity of the cost function ) .",
    "it holds that @xmath41    note the difference between the right - hand sides of and . in the former , the dependence on @xmath4 appears in the interval of the integral , while in the latter , the dependence is moved to the boundary conditions .",
    "the pre - factor is also rescaled accordingly .",
    "we first prove the first part .",
    "the assertion is simply followed from the change of variables : @xmath42 .",
    "define @xmath43 .",
    "then for any @xmath44 we have @xmath45 substituting this into the integral , we obtain @xmath46 and the boundary conditions become @xmath47 the assertion then follows from these computations .",
    "next we prove the second statement .",
    "let @xmath48,\\r^{d})$ ] be the optimal curve in the definition of @xmath49 .",
    "we define @xmath50 . since @xmath51,\\r^{d})$ ] and @xmath52 it follows that @xmath53 is an admissible curve in the definition of @xmath54 .",
    "it implies that @xmath55 this finishes the proof of the theorem .",
    "the next lemma shows that among all the cost functions , only @xmath56 is a distance in the euclidean space @xmath8 .",
    "[ lem : c=0 ] @xmath57 is always non - negative and it equals to @xmath58 if and only if @xmath59 for @xmath60 . in particular , for any @xmath61 we have @xmath62 and @xmath63 iff @xmath64 .",
    "this lemma is a direct consequence of the definition of @xmath6 .",
    "obviously @xmath65 and it is equal to @xmath58 if and only if the optimal curve satisfies @xmath66 i.e. , it is a polynomial of order @xmath67 .",
    "this together with the boundary conditions imply .",
    "lemma [ lem : c=0 ] shows that @xmath68 is not a distance when @xmath69 .",
    "it is not symmetric neither satisfies the condition @xmath70 iff @xmath71 .",
    "as mentioned in the introduction , the mean squared cost function has been proposed as a mathematical model to maximizing the smoothness of a movement between two points @xcite .",
    "experimental observations has confirmed the predictions of the model  @xcite .",
    "however , a priori it is not clear why @xmath6 is the right object . in other words ,    _ why should one minimize the integral of the square of the derivative ? _",
    "we provide here an interpretation based on the theory of large - deviation .",
    "we consider a system of @xmath26 coupled oscillators , each of them moving vertically and being connected to the nearest neighbours directly , the last oscillator being forced by a random noise .",
    "note that a more general system was considered in @xcite .",
    "mathematically , the system is given by the following system of stochastic equations @xmath72 where @xmath73 represents the amplitude of the noise .",
    "this system can be formally written as where the subscript indicates the dependence on @xmath73 @xmath74 by the freidlin - wentzell theorem ( see e.g.  ( * ? ? ? * theorem 5.6.3 ) ) , the process @xmath75 satisfies a large - deviation principle in @xmath76,\\r^d)$ ] with a rate functional @xmath77,\\r^d)\\to \\r\\cup\\{+\\infty\\}$ ] given by @xmath78 the large deviation principle formally can be formulated as follows @xmath79})\\approx \\exp\\big(-\\frac{1}{\\varepsilon } i(\\xi)\\big ) , \\quad \\text{as}\\quad \\varepsilon\\to 0.\\ ] ] in other words , the large - deviation principle provides an asymptotic formula to compute the probability of observing any profile @xmath80 .",
    "the probability depends on the value of the rate function at the profile : the smaller the rate functional is , the higher the probability is .",
    "the most probable profile corresponds to the curves that minimizes the rate functional . hence to obtain the most probable profile , one should minimize the rate functional .",
    "we observe that the mean squared derivative cost functions is defined exactly by minimizing the rate functional @xmath81 over all the curves satisfying the boundary constraints .",
    "hence the minimizer in the minimizing problem is the most probable curves observed when the noise tends to zero according to the freidlin - wentzell large - deviation theory .",
    "this explains why it makes sense to work with the cost @xmath6 .",
    "in this section , we prove theorem [ theo : explicit formula ] on the explicit formula for the cost function .",
    "we first recall a notation , namely the wronskian matrix , that will be used at several places later on .",
    "the wronskian matrix @xmath82 associated to n functions @xmath83 of the class @xmath84 is defined by the determinant of the wronskian matrix defined here . ]",
    "@xmath85 so that the @xmath86-entry of this matrix is @xmath87 , which is the @xmath88order derivative of @xmath89    the optimal curve @xmath80 in the definition of the cost function @xmath6 satisfies the euler - lagrange equation @xmath90 therefore , it is a polynomial of order @xmath91 @xmath92 the coefficients @xmath93 will be determined from the boundary conditions .",
    "the @xmath94-order derivative of @xmath80 can be easily computed as @xmath95 writing these equations for @xmath96 in matrix form , we obtain @xmath97 where the matrices @xmath98 and @xmath32 depend on @xmath99 and are given by @xmath100,\\end{aligned}\\ ] ] and    @xmath101.\\label{a}\\end{aligned}\\ ] ]    note that @xmath102 and @xmath103 can be written in compact forms using the notation of the wronskians @xmath104    in particular , when @xmath105 , @xmath106 is the diagonal matrix , and @xmath107 .",
    "this follows that @xmath108 similarly , when @xmath109 , we obtain @xmath110 therefore , we have the following equation to define @xmath111 .",
    "@xmath112 where the vector @xmath113 on the right - hand side is given by @xmath114 the @xmath115-component of @xmath28 can be computed explicitly using the definition of @xmath116 as follows @xmath117 & = & \\xi^{(i)}(h)-\\sum_{j = i}^{n-1}i!\\begin{pmatrix}j\\\\ i \\end{pmatrix}h^{j - i}\\frac{1}{j!}\\xi^{(j)}(0)\\nonumber\\\\   & = & \\xi^{(i)}(h)-\\sum_{j = i}^{n-1}i!\\frac{j!}{(j - i)!i!}h^{j - i}\\frac{1}{j!}\\xi^{(j)}(0)\\nonumber\\\\   & = & \\xi^{(i)}(h)-\\sum_{j = i}^{n-1}\\frac{1}{(j - i)!}h^{j - i}\\xi^{(j)}(0)\\nonumber\\\\    & = & y_i-\\sum_{j = i}^{n-1}\\frac{1}{(j - i)!}h^{j - i}x_i.\\label{b}\\end{aligned}\\ ] ] according to the following lemma whose proof is postponed after this proof , the matrix @xmath34 is invertible .    [",
    "lem : deta ] the matrix @xmath30 is invertible .",
    "therefore , we can compute the coefficients @xmath118 from the matrix @xmath34 and the vector @xmath113 . @xmath119    on the other hand , by integrating by parts successively , we obtain @xmath120 next we will compute the last expression using the relation .",
    "we denote by @xmath121 the matrix obtained from @xmath103 by taking @xmath26-order derivative of each entry of @xmath103 .",
    "then the @xmath122-element , @xmath123 , of @xmath124 is given by @xmath125 in other words , @xmath126\\ ] ] it follows from that @xmath127\\left(\\begin{array}{c } a_{n}\\\\ a_{n+1}\\\\ \\vdots\\\\ a_{k}\\\\ \\vdots\\\\ a_{2n-1 } \\end{array}\\right).\\end{aligned}\\ ] ] therefore , we have @xmath128 where @xmath129.\\ ] ]    it follows that @xmath130^{-1}\\boldsymbol{b}_{n}(h),\\ ] ] where the matrix @xmath131 is given by @xmath132=\\begin{cases } \\frac{(n+i_{2})!}{(i_{2}-i_{1})!}\\xi^{(n - i_{1}-1)}(h)h^{-i_{1}+i_{2 } } , & i_{2}>i_{1}\\\\ \\frac{(n+i_{2})!}{(i_{2}-i_{1})!}[\\xi^{(n - i_{1}-1)}(h)-\\xi^{(n - i_{1}-1)}(0 ) ] , & i_{2}=i_{1}\\\\ 0 , & i_{2}<i_{1 } \\end{cases},\\ ] ] for all @xmath133 .",
    "therefore , substituting this back to , we obtain @xmath134e_{n}(h)[a_{n}(h)]^{-1}\\boldsymbol{b}_{n}(h).\\label{eq1 - 1}\\ ] ] we now use the following lemma whose proof is postponed after the proof of the main theorem    [ lem : bn ] it holds that @xmath135e_{n}(h ) & = & [ \\boldsymbol{b}_{n}(h)]^{t}b_{n}(h)\\label{eq2},\\end{aligned}\\ ] ] where the matrix @xmath136 is defined as follows @xmath137=\\begin{cases } ( -1)^{n - i_{1}-1}\\frac{(n+i_{2})!}{(i_{1}+i_{2}-n+1)!}h^{i_{2}+i_{1}-n+1 } , & i_{2}+i_{1}\\ge n-1\\\\ 0 & i_{2}+i_{1}<n-1 , \\end{cases}\\label{b}\\ ] ] for all @xmath138 .    substituting into , we obtain that @xmath139^{t}b_{n}(h)[a_{n}(h)]^{-1}\\boldsymbol{b}_{n}(h),\\ ] ] with @xmath140 and @xmath29 defined in , and respectively .",
    "this finishes the proof of the main result given that lemma [ lem : deta ] and [ lem : bn ] are true .",
    "the proofs of these lemmas are given below .",
    "we still need to prove lemma [ lem : deta ] and lemma [ lem : bn ] .    recalling that @xmath30 can be written in terms of the wronskian @xmath141 therefore @xmath142 according to ( * ? ? ?",
    "* lemma 1 ) , we have @xmath143 where @xmath144 is the vandermonde determinant @xmath145 hence , we obtain @xmath146 which is non - zero .",
    "this completes the proof of the lemma .",
    "the equality is interesting on its own . below",
    "we will prove it using purely combinatorial techniques .",
    "the @xmath147 element on the left hand side of is @xmath148=(n+k)!(-1)^{k}(\\xi^{(n - k-1)}(h)-\\xi^{(n - k-1)}(0))+\\sum_{i=0}^{k-1}(-1)^{i}\\frac{(n+k)!}{(k - i)!}\\xi^{(n - i-1)}(h)h^{-i+k}.\\end{aligned}\\ ] ] the @xmath149 element of @xmath136 is @xmath150=\\begin{cases } ( -1)^{n - i_{1}-1}\\frac{(n+i_{2})!}{(i_{1}+i_{2}-n+1)!}h^{i_{2}+i_{1}-n+1 } , & i_{2}+i_{1}\\ge n-1\\\\ 0 & i_{2}+i_{1}<n-1 .",
    "\\end{cases}\\ ] ]    so that the @xmath147 element on the right hand side of is @xmath151 & = & \\sum_{i=0,i+k\\ge n-1}^{n-1}\\boldsymbol{b}_{n}(i)(-1)^{n - i-1}\\frac{(n+k)!}{(k+i - n+1)!}h^{k+i - n+1}\\\\   & = & \\sum_{i = n-1-k}^{n-1}\\boldsymbol{b}_{n}(i)(-1)^{n - i-1}\\frac{(n+k)!}{(k+i - n+1)!}h^{k+i - n+1}\\\\   & = & \\sum_{i = n-1-k}^{n-1}[\\xi^{(i)}(h)-\\sum_{j = i}^{n-1}\\frac{1}{(j - i)!}h^{j - i}\\xi^{(j)}(0)](-1)^{n - i-1}\\frac{(n+k)!}{(k+i - n+1)!}h^{k+i - n+1}\\end{aligned}\\ ] ]    to establish , we need to show that @xmath152(-1)^{n - i-1}\\frac{(n+k)!}{(k+i - n+1)!}h^{k+i - n+1}\\nonumber \\\\&=(n+k)!(-1)^{k}(\\xi^{(n - k-1)}(h)-\\xi^{(n - k-1)}(0))+\\sum_{i=0}^{k-1}(-1)^{i}\\frac{(n+k)!}{(k - i)!}\\xi^{(n - i-1)}(h)h^{-i+k}.\\label{eq : equality}\\end{aligned}\\ ] ] we will show this by transforming the left - hand side . first we write it as follows @xmath152(-1)^{n - i-1}\\frac{(n+k)!}{(k+i - n+1)!}h^{k+i - n+1 } \\\\&=\\sum_{i = n-1-k}^{n-1}\\xi^{(i)}(h)(-1)^{n - i-1}\\frac{1}{(k+i - n+1)!}h^{k+i - n+1 } \\\\&\\qquad -\\sum_{i = n-1-k}^{n-1}\\sum_{j = i}^{n-1}\\frac{1}{(j - i)!}h^{j - i}\\xi^{(j)}(0)(-1)^{n - i-1}\\frac{1}{(k+i - n+1)!}h^{k+i - n+1 }   \\\\&=(i)-(ii).\\end{aligned}\\ ] ] now we transform further @xmath153 and @xmath154 .",
    "we have @xmath155 the term @xmath154 can be transformed as follows .",
    "@xmath156    we now show that the second term of vanishes by showing that @xmath157 for each @xmath158 .",
    "in fact , we have @xmath159 as a consequence , @xmath160 . therefore",
    ", the right - hand side of is equal to @xmath161 which is exactly its right - hand side .",
    "this finishes the proof of lemma [ lem : bn ] .",
    "recall that the cost function @xmath6 is given by @xmath162 we can show that the matrix @xmath163 is positive definite .",
    "indeed , let @xmath61 be arbitrary . then by definition of @xmath6 and from",
    ", we have @xmath164 then the positive definiteness of @xmath163 follows from lemma [ lem : c=0 ] . note further that we have the following property , for any matrix @xmath165 then @xmath166 therefore we can write @xmath167 where @xmath168 $ ] is a symmetric positive matrix",
    ".    we can compute the cost function @xmath6 more directly as follows .",
    "@xmath169 define the matrix @xmath170 with entries @xmath171 clearly @xmath172 is symmetric .",
    "then @xmath6 can be written as @xmath173 where @xmath174 and @xmath28 and @xmath34 are defined in and respectively .",
    "the advantage of the formula derived in the previous section is that it involves @xmath33 only one time and the matrix @xmath175 is triangular .    using the scaling property in theorem [ theo : qual theo 2 ] , the cost function",
    "can be written as follows @xmath176^tb_n(1)[a_n(1)]^{-1}\\tilde{\\mathbf{b}}_n(h),\\ ] ] where @xmath177 by analogous computations to , the @xmath115-component of this vector is @xmath178=y_ih^i-\\sum_{j = i}^{n-1}\\frac{1}{(j - i)!}h^{j}x_i = h^i\\mathbf{b}_n(h)[i].\\ ] ]",
    "the analytical formulas for the cost functions obtained in the previous section involve the inverse of the matrix @xmath179 , which is a matrix of order @xmath26 . in this section ,",
    "we provide explicit formula for the @xmath31 decomposition of @xmath32 and for @xmath33 . for simplicity of notation",
    ", we leave out the dependence on @xmath4 of @xmath32 ( and hence @xmath180 ) . recalling that @xmath181 is the wronskian matrix associated to the polynomials @xmath182 , therefore the analysis of this section is of independent interest since the wronskian matrix plays an important role in linear algebra and differential equations .    the main result of this section is the following theorem , which is summarised as theorem [ theo : lu of a ] in the introduction .",
    "[ theo : lu of a 2 ]    1 .",
    "@xmath183 where @xmath36 and @xmath35 are defined as follows @xmath184= \\begin{cases } \\frac{(j-1)!}{(j - i)!}h^{j+n - i } & \\text { if } j\\ge i,\\\\ 0 & \\text{otherwise } , \\end{cases}\\ ] ] and @xmath185=\\begin{cases } h^{j - k}\\begin{pmatrix}k-1\\\\ j-1 \\end{pmatrix}\\frac{n!}{(n - k+j ) ! } & \\text { if } j\\le k,\\\\ 0 & \\text{otherwise}. \\end{cases}\\ ] ] 2 .",
    "the inverse of @xmath32 is given by the product of the following two matrices : @xmath186=\\begin{cases } \\frac{(-1)^{i+j}}{((i-1)!(j - i)!)h^{-j+i+n } } & \\text { if } j\\ge i,\\\\ 0 & \\text{otherwise } \\end{cases}\\label{heq3}\\ ] ] and @xmath187&=\\begin{cases } ( -1)^{j - i}h^{i - j}\\frac{(j-1)!}{(j - i)!(i-1)!}\\frac{(n+j - i-1)!}{(n-1 ) ! } & \\text { if } j\\ge i\\\\ 0 & \\text{otherwise } \\end{cases}\\label{eq21 } \\\\&=\\begin{cases } ( -1)^{j - i}h^{i - j}\\frac{(j-1)!}{(i-1)!}\\begin{pmatrix } n+j - i-1\\\\ j - i \\end{pmatrix}\\nonumber & \\text { if } j\\ge i\\\\ 0 & \\text{otherwise}. \\end{cases}\\end{aligned}\\ ] ]    we first prove the first statement about the @xmath31 decomposition of the matrix @xmath32 we will show that @xmath188 can be computed as follows : @xmath186=\\begin{cases } \\frac{1}{((i-1)!(j - i)!)h^{-j+i+n } } & \\text { if } j\\ge i,\\\\ 0 & \\text{otherwise } \\end{cases}\\ ] ] in fact , we have @xmath189u^{-1}(j , k ) & = & \\sum_{j\\ge i , k\\ge j}u[i , j]u^{-1}[j , k]\\\\   & = & \\sum_{j = i}^{k}u[i , j]u^{-1}[j , k]\\\\   & = & \\sum_{j = i}^{k}\\frac{(j-1)!}{(j - i)!}h^{j+n - i}\\frac{(-1)^{j+k}}{((j-1)!(k - j)!)h^{n - k+j}}\\\\   & = & h^{k - i}(-1)^{k}\\sum_{j = i}^{k}\\frac{1}{(j - i)!}\\frac{(-1)^{j}}{(k - j)!}\\end{aligned}\\ ] ] if @xmath190 , then @xmath191u^{-1}[j , k]=(-1)^{k}\\frac{1}{(k - i)!}\\frac{(-1)^{k}}{(k - k)!}=1 $ ] .",
    "+   + if @xmath192 , then we have @xmath193 therefore , @xmath194u^{-1}[j , k]=\\begin{cases } 1 & \\text { if } k = i\\\\ 0 & \\text { if } k\\ne i. \\end{cases}\\ ] ] in other words , @xmath188 can be defined as in eq .",
    "( [ heq3 ] ) .",
    "now we establish the formula for @xmath35 . by definition @xmath195 , so that @xmath196 $ ] can be written as follows @xmath197=\\sum_{i\\le j}a_{n}[k , i]u^{-1}[i , j ] & = & h^{j - k}\\sum_{i=1}^{j}\\frac{(n+i-1)!}{(n+i - k)!}\\frac{(-1)^{i+j}}{((i-1)!(j - i)!)}.\\end{aligned}\\ ] ]    we now simplify the expression above .",
    "consider the function @xmath198 .",
    "on the one hand , we have @xmath199 therefore , @xmath200 it follows that @xmath201 and by multiplying both sides of this equality with @xmath202 , we get @xmath203 on the other hand , according to leibniz formula , we have @xmath204^{(k-1)}\\\\   & = \\sum_{i=0}^{k-1}\\begin{pmatrix}k-1\\\\ i \\end{pmatrix}[x^{n}]^{(k-1-i)}[(1-x)^{j-1}]^{(i)}.\\end{aligned}\\ ] ] if @xmath205 , then @xmath206 , which implies that @xmath207u^{-1}[i , j]=0 $ ] .",
    "+   + if @xmath208 , then we have @xmath209^{(k-1-i)}[(1-x)^{j-1}]^{(i)}|_{x=1}\\\\   & = \\sum_{i=0}^{j-1}\\begin{pmatrix}k-1\\\\ i",
    "\\end{pmatrix}[x^{n}]^{(k-1-i)}[(1-x)^{j-1}]^{(i)}|_{x=1}\\\\   & + \\sum_{i = j}^{k-1}\\begin{pmatrix}k-1\\\\ i \\end{pmatrix}[x^{n}]^{(k-1-i)}[(1-x)^{j-1}]^{(i)}|_{x=1}\\\\   & = \\begin{pmatrix}k-1\\\\ j-1 \\end{pmatrix}[x^{n}]^{(k-1-(j-1))}(-1)^{j-1}[(1-x)^{j-1}]^{(j-1)}|_{x=1}\\\\   & \\qquad+0\\\\   & = \\begin{pmatrix}k-1\\\\ j-1 \\end{pmatrix}\\frac{n!}{(n-(k - j)!)}x^{n-(k - j)}(j-1)!(-1)^{j-1}|_{x=1}\\\\   & = \\begin{pmatrix}k-1\\\\ j-1 \\end{pmatrix}\\frac{n!}{(n-(k - j))!}(j-1)!(-1)^{j-1}.\\end{aligned}\\ ] ] therefore , @xmath210 it then follows that @xmath211 which is the desired formula .",
    "this completes the proof of the first statement of the theorem .",
    "now we will prove the second statement of the theorem .",
    "we will show that @xmath212 has the form as defined in ( [ eq21 ] ) .",
    "the element @xmath122 of the product of @xmath35 and @xmath212 is given by @xmath213l^{-1}[j , i]$ ]    if @xmath214 , then @xmath215l^{-1}[j , i]=\\sum_{j=1}^{k}l[k , j]l^{-1}[j , i]=0\\ ] ] if @xmath216 , then we have @xmath217l^{-1}[j , i ] & = \\sum_{j=1,j\\ge i , j\\le k}^{n}l[k , j]l^{-1}[j , i]\\\\   & = \\sum_{j = i}^{k}h^{j - k}\\begin{pmatrix}k-1\\\\ j-1 \\end{pmatrix}\\frac{n!}{(n - k+j)!}(-1)^{j - i}h^{i - j}\\frac{(j-1)!}{(j - i)!(i-1)!}\\frac{(n+j - i-1)!}{(n-1)!}\\\\   & = h^{i - k}\\sum_{j = i}^{k}\\frac{(k-1)!}{(k - j)!}\\frac{n!}{(n - k+j)!}(-1)^{j - i}\\frac{1}{(j - i)!(i-1)!}\\frac{(n+j - i-1)!}{(n-1)!}\\\\   & = h^{i - k}\\frac{(k-1)!n!}{(k - i)!(i-1)!(n-1)!}\\sum_{j=0}^{k - i}\\frac{(k - i)!}{(k - i - j)!j!}\\frac{(n+j-1)!}{(n - k+i+j)!}(-1)^{j},\\end{aligned}\\ ] ] where to obtain the last equality we have changed variable @xmath218 .",
    "next , we will show that the summation in the above expression is equal to @xmath58 .",
    "let @xmath219 and consider @xmath220 .",
    "on the on hand , we have @xmath221 which follows that @xmath222^{(\\alpha-1)}(-1)^{\\alpha - j}\\\\   & = \\sum_{j=0}^{\\alpha}\\begin{pmatrix}\\alpha\\\\ j \\end{pmatrix}\\frac{(n+j-1)!}{(n+j-1-(\\alpha-1))!}x^{n+j-\\alpha}(-1)^{\\alpha - j}\\\\   & = \\sum_{j=0}^{\\alpha}\\begin{pmatrix}\\alpha\\\\ j \\end{pmatrix}\\frac{(n+j-1)!}{(n+j-\\alpha)!}x^{n+j-\\alpha}(-1)^{\\alpha - j}\\\\   & = ( -1)^{\\alpha}\\sum_{j=0}^{k - i}\\frac{(k - i)!}{(k - i - j)!j!}\\frac{(n+j-1)!}{(n+j - k+i)!}x^{n+j-\\alpha}(-1)^{j}.\\end{aligned}\\ ] ] in particular , we obtain @xmath223 on the other hand , we have @xmath224^{(\\alpha-1)}\\\\   & = \\sum_{j=0}^{\\alpha-1}[x^{n-1}]^{(\\alpha-1-j)}[(x-1)^{\\alpha}]^{(j)},\\end{aligned}\\ ] ] so @xmath225 .    therefore , @xmath226 and hence we obtain for @xmath216 , @xmath227l^{-1}[j , i]=0.\\ ] ] finally when @xmath190 , we have @xmath217l^{-1}[j , i ] & = \\sum_{j=1}^{i}l[k , j]l^{-1}[j , k]+\\sum_{j = i+1}^{n}l[k , j]l^{-1}[j , k]\\nonumber\\\\   & = \\sum_{j=1}^{k}l[k , j]l^{-1}[j , k]\\nonumber\\\\   & = l[k , k]l^{-1}[k , k]\\nonumber\\\\   & = 1,\\label{eq : l^-1.3}\\end{aligned}\\ ] ] where in the last step we have used that @xmath228=l^{-1}[k , k]=1 $ ] . from , , and",
    ", we conclude that is indeed the formula for the inverse of @xmath35 .",
    "this finishes the proof of the theorem .",
    "in this section , we provide some numerical demonstrations for the analytical analysis in the previous sections . below we show @xmath229 and @xmath6 for @xmath230 . for simplicity of notation",
    ", we leave out the dependence on @xmath4 of the matrices .",
    "+   + * for @xmath16*. @xmath231 * for @xmath19 * @xmath232 * for @xmath233 * @xmath234 and @xmath235 * for @xmath236 * @xmath237 and @xmath238 we observe that these cost functions satisfy a property that @xmath239\\big|^2,\\end{aligned}\\ ] ] for some @xmath240 , which is in accordance with lemma [ theo : qual theo 2 ] ."
  ],
  "abstract_text": [
    "<S> in this paper , we investigate the mean squared derivative cost functions that arise in various applications such as in motor control , biometrics and optimal transport theory . </S>",
    "<S> we provide qualitative properties , explicitly analytical formula and computational algorithms for the cost functions . </S>",
    "<S> we also perform numerical simulations to illustrate analytical results . </S>"
  ]
}