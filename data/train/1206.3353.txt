{
  "article_text": [
    "several measures of information have been proposed in the literature @xcite , appart from shannon entropy @xcite . by maximizing these information measures @xcite ,",
    "their corresponding probability distributions can be calculated .",
    "some of these generalized information and entropy measures and their potential physical applications have been discussed @xcite . + also , by considering non equilibrium systems with a long - term stationary state that possess a spatio- temporally fluctuating intensive quantity , more general statistics were formulated , called superstatistics @xcite .",
    "the temperature was selected as a fluctuating quantity among various available intensive quantities .",
    "an extensive discussion exists in the literature analyzing the possible viability of these kind of models to explain several physical phenomena @xcite . for general distributions @xmath7",
    ", one can get an effective boltzmann factor .",
    "@xmath8    where @xmath9 is the energy of a microstate associated with each of the considered cells .",
    "the ordinary boltzmann factor is recovered for @xmath10 .",
    "one can , however , consider other distributions for the temperature that will lead to their corresponding boltzmann factors .",
    "the gamma @xmath11 , log - normal and the @xmath1-distributions were studied in this context as well as their corresponding boltzmann factors .",
    "the analysis of these @xmath12 showed that all these statistics present the same behaviour for small variance of the fluctuations .",
    "in @xcite a new formalism was developed to deduce entropies associated to each one of the above mentioned boltzmann factors @xmath12 arising from their corresponding @xmath7 distributions . following this procedure , the boltzmann - gibbs entropy and the so called non - extensive statistical mechanics , tsallis entropy @xmath13 ( corresponding to the gamma distribution @xmath11 and depending on a constant parameter @xmath14 were obtained . for the log - normal , @xmath1-distribution and other distributions",
    "it is not possible to get closed analytic expression for their associated entropies and the calculations were performed numerically utilizing the corresponging @xmath12 in each case .",
    "all these @xmath7 distributions and the boltzmann factors @xmath12 obtained from them , depend then on a constant parameter @xmath5 , actually the @xmath1-distribution also depends on a second constant parameter .",
    "consequently the associated entropies depend on @xmath5 .    in this work",
    "we are proposing new generalized distributions , boltzmann factors and information entropies depending on @xmath2 .",
    "it is not our purpose in this first setting of these new structures to discuss their possible physical consequences or applications .",
    "this is left for future work .",
    "as shown , these new proposals depending on @xmath2 resemble the already well known @xmath7 , @xmath12 and information entropies depending on @xmath5 which possible applications and limitations , in relation with certain physical systems , have been discussed in the literature @xcite .    in , we will propose @xmath7 distributions that do not depend on an arbitrary constant parameter ( the @xmath1-distribution will also be considered , it will depend now on one constant parameter instead of two , as usual ) , but instead of @xmath2 that can be identified with the probability associated with the microscopic configuration of the system .",
    "we will calculate the associated boltzmann factors .",
    "it will be shown that for small variance of the fluctuations a universal behaviour is exhibited by these different statistics .    in",
    ", we use the new gamma @xmath11 distribution , depending on @xmath2 , and its associated boltzmann factor to calculate the entropy ; some of these calculations were already presented in @xcite .",
    "we will maximize this information entropy to get the corresponding @xmath15 probability distribution . in our model",
    ", the entropy resembles the one proposed in non - extensive statistical mechanics resulting also by assuming a @xmath11 distribution . in our case , however , it does not depend on a free constant parameter @xmath5 , but instead on the probability @xmath2",
    ". we will show that this new entropy can be expanded in a series , which first term corresponds to shannon entropy @xcite .    in",
    ", we will consider other well known information entropies @xcite that can be generalized and the new proposed entropies will not depend on the constant parameter @xmath5 , but on the probability @xmath2 in each case .",
    "as examples we will consider the @xmath2 modified kaniadakis and sharma - mittal entopies , that , as expected , will have also the shannon entropy as a first term when expanded in series .    to complete this work we analyze two further aspects , the validity of the saddle - point approximation @xcite , and following @xcite we also discuss a generalized version of the khinchin axioms .",
    "as shown in @xcite , three of these axioms are kept and the fourth of them is replaced by a more general version proposed in @xcite , obtaining the tsallis entropy .",
    "we will study these two aspects only for the boltzmann factor @xmath6 and its associated entropy , arising from the proposed @xmath2-gamma distribution .",
    "for the other entropies we will not discuss these two features .",
    "however , by example , for these entropies a similar procedure to that followed in @xcite could be worked out in relation with an extension of the khinchin axioms .",
    "is dedicated to discuss the saddle - point approximation @xcite",
    ". we will consider it in relation to the boltzmann factor @xmath16 arising from the @xmath2-gamma distribution . in",
    ", we discuss how one can replace the fourth khinchin axiom @xcite to get a set of axioms from which the entropy proposed here follows .",
    "as mentioned , this entropy is obtained from the boltzmann factor @xmath17 and this one from the @xmath2-gamma distribution .",
    "is devoted to discussion and outlook .",
    "we begin by assuming a gamma ( or @xmath18 ) distributed inverse temperature @xmath0 depending on @xmath2 , the probability associated with the microscopic configuration of the system .",
    "we may write these @xmath2 gamma distributions as    @xmath19    where @xmath20 is the average inverse temperature .",
    "integration over @xmath0 yields the generalized boltzman factor    @xmath21    as shown in @xcite , this kind of expression can be expanded for small @xmath22 , to get    @xmath23.\\label{4}\\ ] ]    the log - normal distribution can also be written in terms of @xmath2 as    @xmath24^{1/2 } }   \\exp \\ { - \\frac { \\left [ \\ln   \\frac{\\beta ( p_l+1)^{1/2}}{\\beta_0 } \\right]^2}{2 \\ln ( p_l+1 ) } \\}. \\label{5}\\ ] ]    the generalized boltzmann factor ( [ 1 ] ) can be obtained in leading order , for small variance of the inverse temperature fluctuations ,    @xmath25 . \\label{6}\\ ] ]    in general , the @xmath1-distribution has two free constant parameters . in @xcite ,",
    "the authors considered , particularly , the case in which one of these constant parameters is chosen as @xmath26 .",
    "for this same value of this constant parameter we define a @xmath1-distribution in function of the inverse of the temperature and @xmath2 as    @xmath27    once more the associated boltzmann factor can not be evaluated in a closed form , but for small variance of the fluctuations we obtain the series expansion    @xmath28 . \\label{8}\\ ] ]    beck and cohen @xcite have demonstrated that all superstatics depending on a constant parameter @xmath5 are the same for sufficiently small variance of the fluctuations . for our proposed distributions ( [ 2 ] , [ 5 ] , [ 7 ] ) depending now on @xmath2 , the corresponding boltzmann factors ( [ 4],[6 ] , [ 8 ] ) also satisfy these conditions .",
    "the examples studied in @xcite have been nicely addressed in @xcite in order to deduce the entropies from their corresponding boltzmann factors .",
    "another possible way to reconstruct the entropy has been proposed in @xcite , this provides other expressions and consequently predicts different physical consequences . in @xcite it has been shown that there exist a duality between these two procedures .",
    "we will refer to the first proposal , as shown there @xcite , the boltzmann - gibbs entropy and the non - extensive statistical mechanics entropy can be obtained in a closed analytic form .",
    "however , the entropies corresponding to the boltzmann factors associated to the log - normal and to the @xmath1-distributions can not be obtained analitically and were calculated numerically .",
    "following @xcite and a previous work by one of us @xcite we present the procedure to obtain the entropy corresponding to our @xmath7 distribution ( [ 2 ] ) and to its associated generalized boltzmann factor ( [ 3 ] ) .",
    "we begin by defining the entropy @xmath29 in terms of a generic @xmath30 ; for @xmath31 one has the shannon entropy . as in @xcite",
    "it is possible to express @xmath32 and as well a generic @xmath33 in terms of integrals on a function @xmath34 that is obtained from the boltzmann factor @xmath12 of interest . by these means",
    "@xmath32 and @xmath33 can be written as    @xmath35    and @xmath36 where @xmath34 is to be identified with the inverse function of @xmath37 .",
    "one selects first the @xmath7 of interest , then @xmath12 is calculated and the integral @xmath38 is performed .",
    "inverting the axes of the variables , @xmath34 for several superstatistics can be found @xcite , and from it @xmath39 . in our case ,",
    "the starting points are the distribution ( [ 2 ] ) and the boltzmann factor ( [ 3 ] ) .",
    "@xmath34 results in @xmath40 @xmath41 .",
    "a straightforward calculation gives for @xmath33 @xmath42 where @xmath43 has been determined by means of the condition @xmath44 ; @xmath32 results in @xmath45 expressions ( [ 12 ] , [ 13 ] ) fulfill the conditions @xmath46 and @xmath47 . by these means the entropy results in @xmath48 where @xmath49 is the conventional constant and @xmath50 .",
    "the expansion of ( [ 14 ] ) gives    @xmath51    the first term corresponds to shannon entropy .    in the functional @xmath52 @xmath53 and @xmath0 are lagrange parameters .",
    "considering the appropiate @xmath32 and @xmath33 and imposing the condition @xmath54 it is possible to calculate @xmath2 , as it is known for the shannon entropy @xcite and for the non extensive statistical mechanichs @xcite . in our case by means of ( eqs . [ 12],[14 ] ) one gets @xmath55 as we have shown in the expansion of the entropy the dominant term is the first one corresponding to the shannon entropy , for it one gets the usual expression for @xmath2 namely @xmath56 .",
    "we can not analytically express @xmath2 as function of @xmath4 for the model we are considering .",
    "we can however , plot a figure of @xmath4 as function of @xmath2 and invert the axes to get @xmath57 . by means of this procedure we are able to plot for different values of @xmath4 .",
    "we notice that for relative large values of @xmath4 the usual values for @xmath2 coincide with the ones given by ( [ 17 ] ) .",
    "as expected they coincide also for @xmath2 approaching one .",
    "comparison of the two probabilities .",
    "line corresponds to the standard one @xmath58 , and  line to @xmath59    as we have shown by choosing @xmath60 ( [ 2 ] ) , @xmath61 ( [ 3 ] ) is obtained by integrating over @xmath0 and from it by inverting the axes of the variable the inverse function @xmath34 ( [ 11 ] ) and @xmath62 are found .",
    "this procedure has allowed us to calculate @xmath33 and @xmath32 and consequently the entropy ( [ 14],[15 ] ) . if we assume in @xmath63 ( [ 2 ] ) a @xmath64 , that is equipartition , the distribution takes the form    @xmath65    and we get the boltzmann factor    @xmath66    from this the entropy results in    @xmath67 , \\label{20}\\ ] ]    as boltzmann s entropy is @xmath68 , the expansion of expression ( [ 20 ] ) in terms of @xmath69 gives    @xmath70    figures [ f1 ] and [ f2 ] show the boltzman entropy @xmath71 and the entropy @xmath72 .",
    "we notice that in the range of `` small '' @xmath73 ( large equipartition probabilities ) these entropies differ . instead for large @xmath73 the two entopies basically coincide .",
    "entropies as function of @xmath73 .",
    "and   lines correspond to @xmath74 and @xmath72 respectively ( @xmath75 equipartition ) ]     entropies as function of @xmath73 .",
    "and    lines correspond to @xmath74 and @xmath72 respectively ( @xmath75 equipartition ) ]    we have then proposed a new entropy ( [ 14],[20 ] ) that does not depend on a constant arbitrary parameter , but on the probability @xmath2 ( [ 17 ] ) associated with the microscopic configuration of the system .",
    "its expansion provides as a first term the shannon entropy ( [ 15 ] ) and correspondingly boltzmann s entropy ( [ 21 ] ) .",
    "this entropy corresponds to the gamma distribution ( [ 2],[18 ] ) .",
    "other two distributions ( [ 5],[7 ] ) were also assumed as functions of @xmath2 and their approximated corresponding boltzmann factors found ( [ 6],[8 ] ) . as mentioned , their associated entropies can not be expressed in an analytic closed form .",
    "more general measures of information than the shannon entropy have been proposed in the literature @xcite . maximizing these entropies subject to suitable constraints allow us to obtain associated probability distributions . in @xcite",
    "several of these entropies have been reviewed and their potential physical applications discussed .",
    "similar entropies , to most of those studied in @xcite , but now in terms of @xmath2 , can be proposed . as examples",
    "let us consider modified kaniadakis and sharma - mittal entropies .",
    "the kaniadakis entropy is defined by the expression    @xmath76    this is an entropy , which reduces to the original shannon entropy for @xmath77 @xcite . inspired in this kaniadakis entropy",
    "we propose to consider the following generalized entropy    @xmath78    the two terms in this expression can be expanded in a similar manner as ( [ 14],[15 ] ) to get    @xmath79    the first term corresponds to shannon entropy .",
    "it is interesting to notice that the expansion ( [ 24 ] ) of the entropy ( [ 23 ] ) differs from the expansion ( [ 15 ] ) of the entropy ( [ 14 ] ) corresponding to the gamma distribution ( [ 2 ] ) and that we have analyzed in more detail in the previous sections ; in ( [ 24 ] ) only the `` odd '' terms in the expansion arise .",
    "we consider now the sharma - mittal entropies , these are two constant parameters families of entropic forms .",
    "they can be written as    @xmath80    we now assume that @xmath81 and @xmath82 are not constant parameters but are functions only of @xmath2 , we get the entropy ( [ 14 ] ) for @xmath83 and @xmath84 and the entropy ( [ 23 ] ) is obtained for @xmath85 and @xmath86 .",
    "these sharma - mittal entropies can be generalized in several manners as functions of the probability @xmath2 by means of other different assumptions .",
    "two of them correspond to the entropies ( [ 14],[23 ] ) in this work .",
    "other entropies considered , by example in @xcite , can also be generalized as functions of @xmath2 instead of a constant parameter @xmath5 . as examples",
    "we have analyzed the @xmath2 dependent generalized kaniadakis and two sharma - mittal entropies that reduce to the entropies ( [ 14],[23 ] ) .",
    "we have already obtained the low energy asymptotics ( [ 4 ] ) for the boltzmann factor ( [ 3 ] ) arising from the @xmath2-gamma distribution ( [ 2 ] ) .",
    "we have also shown that up to second order the bolztmann factor ( [ 6 ] ) corresponding to the log - normal distribution ( [ 5 ] ) and the boltzmann factor ( [ 8 ] ) arising from the @xmath1-distribution ( [ 7 ] ) concide with the boltzmann factor ( [ 4 ] ) .",
    "these approximations represent the leading order correction to ordinary statistical mechanics in the nonhomogeneous systems with temperature fluctuations for small values of the energy @xmath9 .",
    "the zeroth - order approximation to these boltzmann factors corresponds , as expected , to the boltzmann statistics @xmath87 with inverse temperature @xmath20 .    to find the high - energy asymptotics of @xmath12 we follow @xcite where the fact is used that ( [ 1 ] ) has the form of a laplace integral for @xmath88 . in this limit , the integral",
    "can be approximated by its largest integrand .",
    "this is the essence of the saddle - point approximation , namely the laplace method .",
    "the conditions of applicability of this approximation method are basically the conditions that one assumes regarding the shape of @xmath7 and its differenttiability . by putting @xmath16 in the form    @xmath89    one",
    "attempts to find the unique value of @xmath0 which maximizes the exponential function    @xmath90    for any large enough energy value @xmath9 .",
    "the value of @xmath0 maximizing @xmath91 for large fixed @xmath9 is denoted @xmath92 .",
    "having assumed @xmath7 to be unimodal ensures us the uniqueness of @xmath92 .",
    "being @xmath7 unimodal , @xmath93 must be a concave function of @xmath0 . in this manner the maximum of @xmath91 can only be obtained at the single point @xmath92 .",
    "it is such that    @xmath94^\\prime = \\frac{f_{p_l}^\\prime ( \\beta)}{f_{p_l}(\\beta ) } .",
    "\\label{28}\\ ] ]    in this way we get @xmath92 and in the limit @xmath95    @xmath96    this saddle - point or laplace approximation can be improved by using a gaussian approximation of the integrand in ( [ 26 ] ) . the refined high energy asymptotics results in    @xmath97^{\\prime \\prime } } } } .",
    "\\label{30}\\ ] ]    the approximation of @xmath12 ( [ 29 ] , [ 30 ] ) show that the mixture of boltzmann statistics defining @xmath12 reduces at high energy @xmath9 to a particular boltzmann statistics , like in the equilibrium situation , but now this boltzmann statistics is a function of @xmath92 which depends on @xmath9 , the energy considered and is determined by @xmath98 ( [ 28 ] ) . the long term stationary behaviour of the non equilibrium system considered for high values of @xmath9 is dominated by the equilibrium behaviour of a subset of cells having an inverse temperature close to @xmath92 .",
    "we now consider the asymptotic behaviour of @xmath6 ( [ 1 ] ) for @xmath95 and @xmath63 given by ( [ 2 ] ) .",
    "we first solve ( [ 28 ] ) to find @xmath92 for this case .",
    "one gets    @xmath99    as expected @xcite as @xmath95 , @xmath100 .",
    "we want now to calculate @xmath6 ( [ 29 ] ) for this @xmath92 .",
    "this can be expressed as    @xmath101 } , \\label{32}\\ ] ]    for @xmath95 and a certain value of @xmath2    @xmath102    the more refined approximation ( [ 30 ] ) can be obtained by dividing ( [ 33 ] ) by @xmath103^{\\prime\\prime}}$ ] which in the high energy limit is proportional to @xmath9 . in this way",
    "@xmath104    in this more accurate calculation , we get , asymptotically a decaying power law for the effective boltzmann factor . as mentioned in @xcite power law superstatistics seem to be physically relevant for several physical systems .",
    "the well established four khinchin axioms are nicely well discussed and presented in @xcite .",
    "as known , the celebrated shannon @xcite entropy @xmath105 satisfies all these axioms .",
    "it has been however , argued in the literature that the fourth of these axioms is not an obvious property @xcite .",
    "we will concentrate our discussion on it .",
    "this fourth axiom deals with the composition of two systems i and ii ( not necessarily independent ) .",
    "we denote the probabilities of the first system as @xmath106 , those of the second system as @xmath107 .",
    "the joint system is described by the joint probabilities @xmath108 , where @xmath109 is the conditional probability of event @xmath110 in system ii under the condition that event @xmath111 has already ocurred in system i. the conditional information of system ii formed with the conditional probabilities @xmath112 is denoted by @xmath113 , under the condition that system i is in the state @xmath111 . the fourth axiom states that the conditional informations are related by    @xmath114",
    "this axiom postulates that the information measure should be independent of the way the information is collected .",
    "we can collect the information in ii , assuming a given event @xmath111 in system i , and then sum the result over all possible events @xmath111 in system i , weighting with the probabilities @xmath106 .",
    "if the two systems are independent the probability of the two systems factorizes @xmath115 . only in this case (",
    "[ 35 ] ) reduces to    @xmath116    the rule of additivity of information for independent systems . from a physical point of view",
    "this axiom ( [ 35 ] ) is not an obvious property .",
    "should the information be considered as independent from the way we collect it ?",
    "in complex systems , the order in which the information is collected can be very relevant .",
    "this has lead to the replacement of the fourth khinchin axiom by something more general .",
    "in particular in @xcite it was shown that tsallis entropy follows uniquely by replacing only the fourth axiom ( [ 35 ] ) by the more general version    @xmath117    the meaning of this new axiom is that if we collect information from two subsystems , the total information should be the sum of the information collected from system i and the conditional information from system ii , plus a correction term .",
    "apriori this correction term can be anything .",
    "one restricts the possible asumptions to    @xmath118    where @xmath119 is some function .",
    "one of the simplest forms is of the kind given by ( [ 37 ] ) .",
    "we may as well , formulate another axioms which then lead to other possible information measures .",
    "this is the case , by example , in @xcite where a set of axioms has been assumed that lead to the sharma - mittal entropy .",
    "the entropies ( [ 14 ] ) associated with the @xmath2-gamma distribution ( [ 2 ] ) are composable .",
    "suppose the two systems i and ii are not independent .",
    "in this case one can still write the joint probability @xmath120 as a product of @xmath121 and the conditional probability @xmath122 , the probability of event @xmath110 under the condition that event @xmath111 has already ocurred is @xmath123 .",
    "then the conditional entropy associated with system ii , under the condition that system i is in state @xmath111 , is @xmath124    @xmath125    one can then verify the condition    @xmath126    this relation is similar to the original axiom four ( [ 35 ] ) , one has however the probability with an exponent that is the probability itself .",
    "we weight now the events in system i with @xmath127 instead of @xmath121 .",
    "hence , the @xmath2 dependent information considered ( [ 14 ] ) is not independent of the way it is collected for the various subsystems .",
    "the distributions ( [ 2],[5],[7 ] ) , boltzmann factors ( [ 3],[4],[6],[8 ] ) and entropies ( [ 14],[20],[23],[25 ] ) proposed in this work do not depend on a constant parameter @xmath5 , but all are functions of @xmath2 ( for the @xmath1-distribution the remaining constant parameter has been chosen @xmath26 ) . by maximizing the functional ( [ 16 ] ) , for the entropy ( [ 14 ] ) a closed relation between @xmath2 and @xmath4 has been obtained ( [ 17 ] ) .",
    "it has been shown that for small variance of the fluctuations , the boltzmann factors ( [ 4],[6],[8 ] ) concide for the following distributions : gamma ( [ 2 ] ) , log - normal ( [ 5 ] ) and @xmath1 -distributions ( [ 7 ] ) . moreover ,",
    "the generalized information entropies proposed in this work ( [ 14],[23],[25 ] ) can be expanded to get as a first term the shannon entropy ( [ 15],[24 ] ) .",
    "we have analyzed the saddle point approximation for the @xmath2-gamma distribution and have got , in the high energy limit , an asymtotically decaying power law for the effective boltzmann factor ( [ 34 ] ) , this seems to be the physically expected appropiate behaviour @xcite .",
    "we have also shown how the fourth khinchin axiom should be modified so that the associated entropy results in ( [ 14 ] ) . in figure 1",
    "we have compared the probability distribution arising from this entropy with the standard one . in figures 2 and 3",
    "we have compared ( for @xmath128 , equipartition ) the entropy ( [ 20 ] ) with the boltzmann entropy . in this first proposal of these new entropies",
    "we do not analyze physical systems in which these entropies could possibly be of interest . as shown",
    ", the new entropies ( [ 14],[23],[25 ] ) resemble the well known non - extensive statistical mechanichs entropy , the kaniadakis and two sharma - mittal entropies correspondingly ( other well known @xmath5 entropies could also be generalized in terms of @xmath2 ) .",
    "for all these , depending on a constant parameter @xmath5 , there exist an extensive literature on their possible physical reach , their relation with the experiments is under discussion @xcite .",
    "also , other theoretical developments have been proposed @xcite .",
    "we will study , in further work , some of the @xmath2 dependent entropies proposed here in connection with these aspects .",
    "10 rnyi a 1970 _ probability theory _ , ( north holland , amsterdam ) + kaniadakis g 2002 e. * 66 * 056125 + tsallis c 1988 _ j. stat . phys . _ * 52 * 479 + abe s 1997 a. * 224 * 326 + sharman bd and mittal dp 1975 _ j. math .",
    "sci . _ * 10 * 28            wilk g and wlodarczyk z 2000 2770 + sakaguchi h 2001 _ j. phys .",
    "soc . japan _ * 70 * 3247 + jung s and swinney hl 2002 _ superstatistics in taylor couette flow , university of austin _ _ prepint _ + tsallis c and souza amc 2003 e. * 67 * 026106"
  ],
  "abstract_text": [
    "<S> systems with a long - term stationary state that possess as a spatio - temporally fluctuation quantity @xmath0 can be described by a superposition of several statistics , a `` superstatistics '' . </S>",
    "<S> we consider first , the gamma , log - normal and @xmath1-distributions of @xmath0 . </S>",
    "<S> it is assumed that they depend only on @xmath2 , the probability associated with the microscopic configuration of the system . </S>",
    "<S> for each of the three @xmath3distributions we calculate the boltzmann factors and show that they coincide for small variance of the fluctuations . </S>",
    "<S> for the gamma distribution it is possible to calculate the entropy in a closed form , depending on @xmath2 , and to obtain then an equation relating @xmath2 with @xmath4 . </S>",
    "<S> we also propose , as other examples , new entropies close related with the kaniadakis and two possible sharma - mittal entropies . </S>",
    "<S> the entropies presented in this work do not depend on a constant parameter @xmath5 but on @xmath2 .    </S>",
    "<S> for the @xmath2-gamma distribution and its corresponding @xmath6 boltzmann factor and the associated entropy , we show the validity of the saddle - point approximation . </S>",
    "<S> we also briefly discuss the generalization of one of the four khinchin axioms to get this proposed entropy . </S>"
  ]
}