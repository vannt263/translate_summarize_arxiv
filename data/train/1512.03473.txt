{
  "article_text": [
    "information @xcite @xcite is a classical measure in information , estimation and decision theory with various applications .",
    "it can be used in order to predict the performance of efficient unbiased estimation algorithms in a compact way @xcite or for the construction of strong statistical tests @xcite .",
    "further , it describes the covariance of maximum - likelihood estimates around the true parameter in the asymptotic regime where the amount of observations @xmath0 is large @xcite and finds application in bayesian inference through the bernstein - von mises theorem @xcite . while access to the fisher information is useful in different situations , calculation of the information measure itself can turn out to be quite complex or even impossible . in particular",
    "this is the case when the evaluation of the log - likelihood function is intractable , when the integration of the score function is difficult or when the underlying statistical model is unknown .",
    "for example for a nonlinear squaring device with gaussian input , the probability density function is in general of the noncentral chi - squared type and contains a modified bessel function making the evaluation of fisher information nontrivial .",
    "if one imagines a multivariate gaussian distribution with @xmath0 dimensions which is element - wise hard - limited , the output likelihood function requires the orthant probabilities of the multivariate gaussian model for which up to the present day no general closed - form exists @xcite . the required integration over the score function of the hard - limited gaussian variable additionally requires to calculate a sum with @xmath1 components , resulting in prohibitively high computational complexity when @xmath0 is large .",
    "considering measurement systems in practical scenarios , the exact analytical representation of the system model can rarely be deduced due to a high number of nonlinear and random effects like amplification , quantization , filtering , internal noise sources and rounding . the only possibility to access",
    "the fisher information measure then is to approximate the parametric output distribution by means of empirical methods like histograms @xcite . for multivariate problems this is demanding and requires a high amount of memory .",
    "early works dealing with the analysis of nonlinear systems are @xcite @xcite @xcite and concentrate on the problem of representing the output directly in terms of the values of the input by using polynomials or series expansions with orthogonal functions .",
    "@xcite considers the approximation of moments by polynomial representations of nonlinear functions .",
    "another group of researchers , mainly concerned with communication issues , focuses on other important output characteristics like the power spectrum @xcite @xcite @xcite or correlation functions @xcite @xcite @xcite of nonlinear devices while @xcite discusses signal - to - noise ratio and @xcite distortion - to - signal power ratio at the output of nonlinearities .",
    "classical attempts to derive the output distributions of nonlinear systems are found in @xcite @xcite , while a recent result with applications in the field of bayesian inference is @xcite .",
    "a discussion on the properties of fisher information is found in @xcite , while @xcite considers similar problems for a generalized version of fisher information .",
    "the problem of constraint distributions providing minimum fisher information is discussed in @xcite @xcite @xcite , whereas @xcite @xcite focus on the gaussian assumption for additive systems under an estimation theoretic perspective .",
    "anticipating that for future signal processing and communication systems analog complexity will be shifted into the digital domain in order to exploit the consequences of moore s law and to obtain an energy- and hardware - efficient design , we have identified that a framework is required which allows to analyze the possible signal processing performance with challenging nonlinear stochastic models . trying to circumvent exact calculation of the required fisher information ,",
    "we have recently shown @xcite @xcite that compact generic lower bounds for the information measure can be obtained . these approximations benefit from the fact that they exclusively rely on the first two @xcite or four moments @xcite of the output .",
    "statistical moments are usually better tractable than the probability density or mass function itself and can be deduced by simple measurements @xcite . through lower bounds ,",
    "the derived expressions are guaranteed to be pessimistic .",
    "this makes them suitable as figure of merit for the problem of system design and optimization @xcite .",
    "while our last approach @xcite on bounding the fisher information measure turns out to provide tight results for various cases ( bernoulli , exponential , gaussian or poisson distribution ) , we have also identified an example where the information bound is loose ( laplace distribution ) .",
    "looking at the properties of the example distributions , it becomes obvious that for the tight cases the sufficient statistics are the first two moments @xmath2 or @xmath3 .",
    "in contrast , the zero - mean laplace distribution has the sufficient statistic @xmath4 .",
    "such a statistic is not well captured by the first two moments , used in the construction of our bounding approach @xcite .",
    "might this be the reason why the approximation @xcite fails to generate a tight bound under the laplacian distribution ?      addressing this question , we pick the log - normal distribution with known scale parameter @xmath5 and the weibull distribution with known shape parameter @xmath6 as additional examples . like the laplace distribution ,",
    "the log - normal has sufficient statistics which can not be represented by a finite number of raw moments , i.e. , @xmath7 and @xmath8 .",
    "the weibull distribution has the property that it s sufficient statistic is @xmath9 .",
    "therefore , the fisher information bound @xcite should be tight for the cases @xmath10 and @xmath11 and loose in any other configuration in order to confirm an existing connection between the construction of the information bound and the sufficient statistics .    a second interesting observation is the fact that all example distributions discussed in @xcite belong to the class of the exponential family .",
    "therefore , in order to obtain a better understanding on fisher information and possible lower bounds , we adapt to this level of abstraction and study the properties of the fisher information measure evaluated for distributions of the exponential family .",
    "restricting to such a class of distributions , allows to provide an identity connecting the fisher information measure and a weighted sum of the derivatives of the expected sufficient statistics .",
    "the weight of each derivative is the derivative of the associated natural parameter .",
    "the derived identity provides a guideline for the construction of strong fisher information lower bounds for any kind of stochastic system . to this end",
    ", the original system is replaced by a counterpart in the exponential family which is equivalent with respect to a set of auxiliary statistics .",
    "we show that the fisher information of this replacement is always dominated by the information measure of the original system such that an optimization of the replacement model will lead to a strong lower bound .",
    "the presented information bound has the advantage that instead of full characterization of the model likelihood , exclusively the expected values , the derivatives of the expected values and the covariance of the used auxiliary statistics are required in order to evaluate the estimation theoretic quality of the system .",
    "we utilize this generic result in order to formulate a specific information bound involving the derivatives of the first @xmath12 raw moments . for the initial example of a log - normal and a weibull distribution , we test the quality of this approximation of the fisher information . by constructing a second bound which takes into consideration raw moments , the expected absolute value and the expected log - value",
    ", we show how to use the information bound in order to learn informative statistics of a parametric system with unknown output model and how to determine the minimum guaranteed interference capability of the model by calibrated measurements . in order to emphasize the practical impact of our discussion",
    ", we demonstrate this aspect with the rapp model , which is popular for modeling the saturation effect of solid - state power amplifiers .",
    "finally , based on the learned statistics and their optimized weights , we demonstrate that consistent estimators can be obtained from compressed observations of the system output which achieve a performance equivalent to the inverse of our pessimistic approximation for the fisher information measure . by reformulation of the proposed estimation algorithm",
    ", we reveal that the exponential replacement forms a conservative framework which allows to derive hansen s famous estimation approach known as generalized method of moments @xcite by a maximum - likelihood argument .",
    "special cases of the main result and further examples are found in our conference publication @xcite . in @xcite",
    "we use the presented method in order to discuss the problem of parametric covariance estimation with @xmath13-bit hard - limiting .",
    "consider a parametrized family of probability measures , characterized by a probability density or mass function @xmath14 , with random variable @xmath15 and a deterministic parameter @xmath16 .",
    "@xmath17 is the support of the random variable @xmath2 and @xmath18 the parameter space of @xmath19 . throughout our discussion",
    "we assume that all integrands are absolutely integrable on @xmath20 .",
    "all density or mass functions @xmath14 exhibit regularity and are differentiable with respect to the parameter @xmath19 .",
    "if the first @xmath21 and the second central moment @xmath22 as well as the third @xmath23 and the fourth central moment in normalized form @xmath24 are at hand , it can be shown that the fisher information @xmath25 is in general bounded from below by the expression @xcite @xmath26 the weighting factor @xmath27 which provides the maximum value on the right - hand side of is @xmath28 setting the weighting factor @xmath27 to zero , gives a simple special case of the information bound @xmath29 the inequality was the starting point of our discussion on lower bounds for the fisher information in @xcite . in our subsequent work",
    "@xcite it was shown that produces tight results for bernoulli , exponential , gaussian and poisson distributions and is loose for the laplacian distribution .",
    "in order to provide further cases where the bound attains loose results , we start with the example of a log - normal distribution .",
    "the log - normal distribution with known scale parameter @xmath30 is characterized by the probability density @xmath31 with @xmath32 .",
    "the log - likelihood function is given by @xmath33 the score function is @xmath34 with it s derivative @xmath35 therefore , the fisher information measure with respect to the parameter @xmath19 is given by @xmath36}\\notag\\\\ & = -{\\operatorname{e}_{z;\\theta } \\left[\\frac { \\partial^2 \\ln p(z;\\theta)}{\\partial \\theta^2}\\right]}=\\frac{1}{\\sigma^2}.\\end{aligned}\\ ] ] for the evaluation of the moments we use the fact that the @xmath37-th raw moment of the log - normal distribution follows from @xmath38}&=\\tilde{\\mu}_l(\\theta)\\notag\\\\ & = { \\operatorname{e}^{l\\theta+\\frac{1}{2 } l^2 \\sigma^2}}.\\end{aligned}\\ ] ] accordingly , the first moment and its derivative are given by @xmath39}\\notag\\\\ & = { \\operatorname{e}^{\\theta+\\frac{1}{2}\\sigma^2}},\\\\ \\frac{\\partial \\mu_1(\\theta)}{\\partial \\theta}&={\\operatorname{e}^{\\theta+\\frac{1}{2}\\sigma^2}}.\\end{aligned}\\ ] ] the second central moment and its derivative are @xmath40 } \\notag\\\\ & = \\tilde{\\mu}_2(\\theta)-\\mu_1 ^ 2(\\theta),\\notag\\\\ & = { \\operatorname{e}^{2\\theta+\\sigma^2}}({\\operatorname{e}^{\\sigma^2}}-1)\\\\ \\frac{\\partial \\mu_2(\\theta)}{\\partial \\theta}&= 2{\\operatorname{e}^{2\\theta+\\sigma^2}}({\\operatorname{e}^{\\sigma^2}}-1).\\end{aligned}\\ ] ] the third normalized central moment is obtained by @xmath41}\\notag\\\\ & = \\frac { \\tilde{\\mu}_3 -3\\mu_1(\\theta)\\tilde{\\mu}_2(\\theta)+2\\mu_1 ^ 3(\\theta ) } { \\mu_2^{\\frac{3}{2}}(\\theta)}\\notag\\\\ & = {   \\sqrt{({\\operatorname{e}^{\\sigma^2}}-1 ) } ( { \\operatorname{e}^{\\sigma^2}}+2 )   } \\end{aligned}\\ ] ] and the fourth normalized central moment is found to be @xmath42}\\notag\\\\ & = \\frac{\\tilde{\\mu}_4 - 4 \\mu_1(\\theta)\\tilde{\\mu}_3(\\theta)+6\\mu_1 ^ 2(\\theta)\\tilde{\\mu}_2(\\theta)-3\\mu_1 ^ 4(\\theta ) } { \\mu_2 ^ 2(\\theta)}\\notag\\\\ & = 3{\\operatorname{e}^{2\\sigma^2}}+2{\\operatorname{e}^{3\\sigma^2}}+{\\operatorname{e}^{4\\sigma^2}}-3.\\end{aligned}\\ ] ] for the assessment of the quality we define the ratio between the approximation and the exact information measure @xmath43 as the gap is independent of @xmath19 , in fig .",
    "[ lognormal_loss ] , we depict @xmath44 for different values of the known scale parameter @xmath5 .",
    "it can be observed that for @xmath45 the difference between @xmath46 and the exact fisher information @xmath47 becomes large .",
    "table[x index=0 , y index=1]lognormal_loss.txt ;      as a second example , we study the quality of the fisher information bound @xmath46 for the case of a weibull distribution .",
    "the weibull distribution with known shape parameter @xmath6 is given by the probability density function @xmath48 with @xmath49 .",
    "the log - likelihood of the distribution is @xmath50 and the score function is given by @xmath51 the derivative of the score function has the form @xmath52 consequently , the fisher information measure is given by @xmath53}-1\\bigg ) \\notag\\\\ & = \\frac{k}{\\theta^2}\\big((k+1 ) \\gamma(2)-1\\big)=\\bigg(\\frac{k}{\\theta}\\bigg)^2,\\end{aligned}\\ ] ] where we used the property that the @xmath37-th raw moment of the weibull distribution is @xmath54}\\notag\\\\ & = \\theta^l\\gamma_l\\end{aligned}\\ ] ] with the shorthand notational convention @xmath55 for the gamma function @xmath56 for the information bound @xmath46 we require the first moment @xmath39}\\notag\\\\ & = \\theta\\gamma_1,\\end{aligned}\\ ] ] it s derivative @xmath57 the second central moment @xmath40 } \\notag\\\\ & = \\tilde{\\mu}_2(\\theta)-\\mu_1 ^ 2(\\theta)\\notag\\\\ & = \\theta^2 \\big ( \\gamma_2 - \\gamma_1 ^ 2 \\big),\\end{aligned}\\ ] ] it s derivative @xmath58 the third normalized central moment @xmath41}\\notag\\\\ & = \\frac { \\tilde{\\mu}_3 -3\\mu_1(\\theta)\\tilde{\\mu}_2(\\theta)+2\\mu_1 ^ 3(\\theta ) } { \\mu_2^{\\frac{3}{2}}(\\theta)}\\notag\\\\ & = \\frac {   \\gamma_3 -3 \\gamma_1\\gamma_2 + 2\\gamma_1 ^ 3 } {   \\big ( \\gamma_2 - \\gamma_1 ^ 2 \\big)^{\\frac{3}{2 } } } \\end{aligned}\\ ] ] and the fourth normalized central moment @xmath42}\\notag\\\\ & = \\frac{\\tilde{\\mu}_4 - 4 \\mu_1(\\theta)\\tilde{\\mu}_3(\\theta)+6\\mu_1 ^ 2(\\theta)\\tilde{\\mu}_2(\\theta)-3\\mu_1 ^ 4(\\theta ) } { \\mu_2 ^ 2(\\theta)}\\notag\\\\ & = \\frac{\\gamma_4",
    "-     4\\gamma_1\\gamma_3 + 6 \\gamma_1 ^ 2\\gamma_2 - 3\\gamma_1 ^ 4    } { \\big ( \\gamma_2 - \\gamma_1 ^ 2 \\big)^2}.\\end{aligned}\\ ] ] in fig .",
    "[ weibull_loss ] we plot the information loss @xmath44 for different shape values @xmath59 .",
    "note , that also in this example the loss @xmath44 is independent of the parameter @xmath19 .",
    "table[x index=0 , y index=1]weibull_loss.txt ;    it can be observed that the information bound @xmath46 is tight for the cases where the shape of the distribution is @xmath10 and @xmath11 while the quality degrades significantly for the cases @xmath60 .",
    "the weibull example is of special interest for the analysis of the quality of the approximation @xmath46 as it contains cases where the bound is tight and cases where it is not . in @xcite",
    "it has been verified that @xmath46 is tight for the bernoulli , exponential , gaussian and poisson distributions .",
    "these distributions have in common that their sufficient statistics are @xmath2 and @xmath3 .",
    "interestingly , for the weibull distribution the sufficient statistic is given by @xmath9 and the information bound @xmath46 is only tight for @xmath10 and @xmath11 .",
    "additionally , the approximation of the fisher information @xmath46 is loose for the log - normal distribution and the laplacian distribution @xcite , both cases where the sufficient statistics are distinct from @xmath2 or @xmath3 .",
    "this provides an indication for the existence of a connection between the sufficient statistics and the quality of fisher information bounds .",
    "note , that in @xcite the information bound @xmath46 was constructed through the cauchy - schwarz inequality @xmath61 by using the functions @xmath62 and @xmath63 the latter contains the statistics @xmath2 and @xmath3 in normalized central form .",
    "this corresponds to the sufficient statistics of the distributions where ( [ bound : fisher : two ] ) provides tight results .",
    "consider as an example the univariate gaussian distribution @xmath98 with @xmath64 and parameter @xmath65 .",
    "the natural parameters are @xmath99 and the two corresponding sufficient statistics are @xmath100 therefore , the expectations of the sufficient statistics are @xmath101}&={\\mu}(\\theta),\\\\   { \\operatorname{e}_{z;\\theta } \\left[t_2(z)\\right]}&={\\sigma}^2(\\theta)+{\\mu}^2(\\theta).\\end{aligned}\\ ] ] with the derivatives of the natural parameters @xmath102 and the derivatives of the expected sufficient statistics @xmath103}}{\\partial \\theta}&=\\frac{\\partial \\mu(\\theta)}{\\partial \\theta},\\notag\\\\ \\frac{\\partial{\\operatorname{e}_{z;\\theta } \\left[t_2(z)\\right]}}{\\partial \\theta}&=\\frac{\\partial{\\sigma}^2(\\theta)}{\\partial \\theta}+2{\\mu}(\\theta)\\frac{\\partial \\mu(\\theta)}{\\partial \\theta}\\end{aligned}\\ ] ] using the identity for the exponential family we obtain @xmath104}}{\\partial \\theta}+\\frac{\\partial w_2(\\theta)}{\\partial \\theta}\\frac{\\partial{\\operatorname{e}_{z;\\theta } \\left[t_2(z)\\right]}}{\\partial \\theta}\\notag\\\\ & = \\frac{1}{{\\sigma}^2(\\theta ) } \\big(\\frac{\\partial { \\mu}(\\theta)}{\\partial \\theta}\\big)^2 + \\frac{1}{2{\\sigma}^4(\\theta ) } \\big(\\frac{\\partial   { \\sigma}^2(\\theta ) } { \\partial \\theta}\\big)^2.\\end{aligned}\\ ] ] it can be verified @xcite that is the actual fisher information measure for the parametric gaussian model .",
    "we demonstrate such a measurement - driven learning approach in a calibrated setup with the example of a solid - state power amplifier .",
    "the system parameter @xmath19 of interest is assumed to be the direct - current offset ( the mean ) at the input of the nonlinear device .",
    "for the mapping from the input @xmath146 to the output @xmath147 of the amplifier , we apply the rapp model @xcite @xmath148 where @xmath149 is a smoothness factor .",
    "[ rapp : io ] depicts the input - to - output relation of this nonlinear system model with different values of @xmath149 .",
    "table[x index=0 , y index=5]rapp_io.txt ;    table[x index=0 , y index=4]rapp_io.txt ;    table[x index=0 , y index=3]rapp_io.txt ;    table[x index=0 , y index=2]rapp_io.txt ;    table[x index=0 , y index=1]rapp_io.txt ;    we apply a gaussian input @xmath150 with @xmath151 to the nonlinear system , we set @xmath152 and for each value of @xmath19 we approximate the expectations and , with @xmath153 realizations of the system output , by their sample mean .",
    "table[x index=0 , y index=1]learnrapp_2.txt ;    fig .",
    "[ rapp_loss ] shows the obtained performance loss @xmath154 which is introduced by the nonlinear rapp model with smoothness factor @xmath155 .",
    "note , that @xmath156 is the fisher information with respect to @xmath19 at the input @xmath146 of the nonlinear rapp model .",
    "it is observed that for an input mean @xmath157 , the saturation of the non - linear rapp model introduces a significant information loss .",
    "[ rapp_weights ] shows the normalized absolute weights @xmath158 associated with each statistic , which have been attained by the optimization .",
    "table[x index=0 , y index=2]learnrapp_2.txt ; ;    table[x index=0 , y index=3]learnrapp_2.txt ; ;    table[x index=0 , y index=4]learnrapp_2.txt ; ;    table[x index=0 , y index=5]learnrapp_2.txt ; ;    table[x index=0 , y index=6]learnrapp_2.txt ; ;    table[x index=0 , y index=7]learnrapp_2.txt ; ;    it can be seen that the second moment and the expected absolute value play a dominant role in the approximation of the fisher information .",
    "in order to strengthen the insights on a possible connection of the sufficient statistics and strong fisher information bounds , we require an approach that allows to investigate and cover a brought class of distributions .",
    "all example distributions discussed so far belong to the univariate exponential family @xmath64 with a single parameter @xmath65 .",
    "therefore , in the following we study the properties of the fisher information measure for distributions from this particular class . in order to provide the results in a generalized form we focus in the following on the multivariate case @xmath66 and a multi - dimensional parameter where @xmath67 .",
    "the multivariate exponential family with a parameter @xmath67 is the set of probability density or mass functions , which can be factorized @xmath68 @xmath69 is the @xmath37-th natural parameter , @xmath70 is the associated sufficient statistic , @xmath71 is the log - normalizer and @xmath72 is the so - called carrier measure .",
    "the log - likelihood function of the exponential family is given by @xmath73 while the score function attains the structure @xmath74 note , that we strictly follow the notational convention @xmath75_{ij } = \\frac{\\partial { x}_i({\\boldsymbol{y}})}{\\partial y_j}.\\end{aligned}\\ ] ] an essential property of the score function is that it s expected value vanishes , i.e. , @xmath76}={\\boldsymbol{0}}^{\\operatorname{t}}.\\end{aligned}\\ ] ] therefore , with @xmath77 } = \\frac{\\partial\\lambda({\\boldsymbol{\\theta}})}{\\partial { \\boldsymbol{\\theta}}}.\\end{aligned}\\ ] ]      the fisher information measure for the case of a multi - dimensional parameter @xmath67 has matrix form @xmath78 and is defined by @xmath79 note that the fisher information measure exists for all probability laws of the form .    for any parametric probability density or mass function @xmath80 belonging to the exponential family",
    ", the fisher information matrix is given by @xmath81 } } { \\partial { \\boldsymbol{\\theta } } }   \\bigg)^{\\operatorname{t } }    \\frac{\\partial w_l({\\boldsymbol{\\theta}})}{\\partial { \\boldsymbol{\\theta}}}.\\end{aligned}\\ ] ]    with the definition of the fisher information we obtain @xmath82 where we have used in the second step to substitute one of the involved score functions .",
    "using , we obtain @xmath83 } } { \\partial { \\boldsymbol{\\theta } } }   \\bigg)^{\\operatorname{t } }    \\frac{\\partial w_l({\\boldsymbol{\\theta}})}{\\partial { \\boldsymbol{\\theta}}}.\\end{aligned}\\ ] ]    defining the vector of sufficient statistics @xmath84 it s expected value @xmath85},\\end{aligned}\\ ] ] and the vector of natural parameters @xmath86 we can reformulate the identity in a compact form @xmath87    @xmath88    @xmath89    while the score function attains the structure @xmath90    for any probability density or mass function @xmath14 belonging to an exponential family the fisher information is given by @xmath91}}{\\partial\\theta}.\\end{aligned}\\ ] ]    by the definition of the fisher information and the score function for exponential families we have @xmath92}\\notag\\\\ & = \\int_{\\mathcal{z } } \\bigg(\\frac{\\partial \\ln{p(z;\\theta)}}{\\partial\\theta}\\bigg ) \\bigg(\\frac{\\partial \\ln{p(z;\\theta)}}{\\partial\\theta}\\bigg ) p(z;\\theta )   { \\rm d}z\\notag\\\\ & = \\int_{\\mathcal{z } } \\bigg(\\frac{\\partial \\ln{p(z;\\theta)}}{\\partial\\theta}\\bigg ) \\bigg ( \\sum_{l=1}^{l } \\frac{\\partial w_l(\\theta)}{\\partial \\theta } t_l(z)- \\frac{\\partial\\lambda(\\theta)}{\\partial \\theta } \\bigg ) p(z;\\theta )   { \\rm d}z.\\end{aligned}\\ ] ] by a straightforward manipulation , we obtain @xmath93 using the fact that with the regularity of @xmath14 we can interchange differentiation and integration @xmath94 we finally obtain @xmath95}}{\\partial\\theta}.\\end{aligned}\\ ] ]    the identity shows that the fisher information measure for an exponential family can be derived from a weighted sum of the derivatives of the @xmath12 expected sufficient statistics @xmath96}}{\\partial\\theta}$ ] .",
    "the weighting factors are the derivatives of the associated @xmath12 natural parameters @xmath97 with respect to the system parameter @xmath19 .",
    "if the parametric model @xmath80 belongs to the exponential family , the @xmath12 natural parameters @xmath105 and the associated expected values of the sufficient statistics @xmath106}$ ] are known , the identity shows that the fisher information measure can be computed by evaluating a simple sum . in the inconvenient situation where it is unclear",
    "if the model @xmath80 belongs to the exponential family and the sufficient statistics @xmath107 or the natural parameters @xmath108 are unknown , the identity ca nt be applied . the conceptual idea behind our approach for such a scenario is to replace the original system by an equivalent distribution @xmath109 which is member of the exponential family . to this end , we select an arbitrary set of @xmath12 auxiliary statistics @xmath110 , determine the expected values @xmath111}$ ] with the original system @xmath80 and choose the exponential family distribution @xmath109 with sufficient statistics @xmath112 and equivalent expected values @xmath113}={\\operatorname{e}_{{\\boldsymbol{z}};{\\boldsymbol{\\theta } } } \\left [   \\phi_l({\\boldsymbol{z } } ) \\right]}$ ] .      through the covariance inequality @xcite",
    "we show that the fisher information of the equivalent exponential family replacement @xmath109 is always dominated by the information measure of the original system @xmath80 .    for any probability density or mass function @xmath80 , any set of @xmath12 auxiliary statistics @xmath110 and with arbitrary weightings @xmath114",
    ", the fisher information matrix dominates @xmath115}}{\\partial { \\boldsymbol{\\theta } } } \\bigg)^{\\operatorname{t } }   { \\boldsymbol{b}}^{\\operatorname{t}}_l({\\boldsymbol{\\theta } } )   \\bigg )   \\notag\\\\ & \\cdot\\bigg({\\operatorname{e}_{{\\boldsymbol{z}};{\\boldsymbol{\\theta } } } \\left [ \\bigg ( \\sum_{l=1}^{l } { \\boldsymbol{b}}_l({\\boldsymbol{\\theta } } ) \\phi_l({\\boldsymbol{z}})\\bigg ) \\bigg ( \\sum_{l=1}^{l } { \\boldsymbol{b}}_l({\\boldsymbol{\\theta } } ) \\phi_l({\\boldsymbol{z } } ) \\bigg)^{\\operatorname{t } } \\right]}\\notag\\\\ & -\\bigg(\\sum_{l=1}^{l } { \\boldsymbol{b}}_l({\\boldsymbol{\\theta } } ) { \\operatorname{e}_{{\\boldsymbol{z}};{\\boldsymbol{\\theta } } } \\left [   \\phi_l({\\boldsymbol{z } } ) \\right ] } \\bigg ) \\bigg ( \\sum_{l=1}^{l } { \\boldsymbol{b}}_l({\\boldsymbol{\\theta } } ) { \\operatorname{e}_{{\\boldsymbol{z}};{\\boldsymbol{\\theta } } } \\left [ \\phi_l({\\boldsymbol{z}})\\right]}\\bigg)^{\\operatorname{t}}\\bigg)^{-1}\\notag\\\\ & \\cdot \\bigg(\\sum_{l=1}^{l } { \\boldsymbol{b}}_l({\\boldsymbol{\\theta } } ) \\frac{\\partial   { \\operatorname{e}_{{\\boldsymbol{z}};{\\boldsymbol{\\theta } } } \\left [   \\phi_l({\\boldsymbol{z } } ) \\right]}}{\\partial { \\boldsymbol{\\theta } } } \\bigg).\\end{aligned}\\ ] ]    see appendix [ append : fish : bound : vector ] .",
    "defining the matrix @xmath116 the vector of auxiliary statistics @xmath117 and it s expected value @xmath118},\\end{aligned}\\ ] ] it is possible to write @xmath119}}{\\partial { \\boldsymbol{\\theta } } } \\bigg)^{\\operatorname{t } }   { \\boldsymbol{b}}^{\\operatorname{t}}_l({\\boldsymbol{\\theta } } ) & = \\bigg(\\frac{\\partial { \\boldsymbol{\\mu}}_{{\\boldsymbol{\\phi}}}({\\boldsymbol{\\theta } } ) } { \\partial { \\boldsymbol{\\theta } } } \\bigg)^{\\operatorname{t } } { \\boldsymbol{b}}({\\boldsymbol{\\theta}}),\\\\ \\sum_{l=1}^{l } { \\boldsymbol{b}}_l({\\boldsymbol{\\theta } } ) \\phi_l({\\boldsymbol{z } } ) & = { \\boldsymbol{b}}^{\\operatorname{t}}({\\boldsymbol{\\theta}}){\\boldsymbol{\\phi}}({\\boldsymbol{z}}),\\\\ \\sum_{l=1}^{l } { \\boldsymbol{b}}_l({\\boldsymbol{\\theta } } ) { \\operatorname{e}_{{\\boldsymbol{z}};{\\boldsymbol{\\theta } } } \\left [   \\phi_l({\\boldsymbol{z } } ) \\right ] } & = { \\boldsymbol{b}}^{\\operatorname{t}}({\\boldsymbol{\\theta}}){\\boldsymbol{\\mu}}_{{\\boldsymbol{\\phi}}}({\\boldsymbol{\\theta}}).\\end{aligned}\\ ] ] further , defining @xmath120 } - { \\boldsymbol{\\mu}}_{{\\boldsymbol{\\phi}}}({\\boldsymbol{\\theta } } ) { \\boldsymbol{\\mu}}^{\\operatorname{t}}_{{\\boldsymbol{\\phi}}}({\\boldsymbol{\\theta}})\\end{aligned}\\ ] ] the bound can be reformulated @xmath121      note , that it is left to choose the auxiliary statistics @xmath122 and optimize the associated weighting factors @xmath123 such that the right hand side of the information bound is maximized in the matrix sense .",
    "if the underlying statistical model @xmath80 is from the exponential family type and the sufficient statistics @xmath124 are used for the approximation , it is possible to obtain a tight information bound .",
    "if the probability density or mass function @xmath80 belongs to the exponential family and the @xmath12 auxiliary functions @xmath122 are the sufficient statistics @xmath107 of the statistical model @xmath80 , the optimization of the weighting matrix @xmath125 in will lead to a tight fisher information bound .",
    "see appendix [ append : fish : bound : vector : tight ] .",
    "in the following we discuss how to perform the optimization of the right hand side of when the sufficient statistics @xmath107 are unknown and we have to resort to the auxiliary statistics @xmath122 . substituting @xmath126 in under the constraint that @xmath127 we obtain a modified bound @xmath128 the right hand side is maximized in the matrix sense under the constraint with @xmath129    for any probability density or mass function @xmath80 and any set of @xmath12 auxiliary statistics @xmath122 , with the definitions and , the fisher information matrix @xmath47 dominates @xmath130    follows from using and in .",
    "note , that due to the tightness of the bound for exponential family models , besides we have an additional identity for such kind of system models @xmath131 in the following we will demonstrate applications of the presented main result",
    ". for simplicity , we focus on univariate problems @xmath64 with a single parameter @xmath65 .",
    "in order to test a generalization of the approach with the derivatives of @xmath12 raw moments , we use the obtained result under the convention @xmath132 and apply the resulting information bound to the log - normal and the weilbull model .",
    "the required expectations and of the auxiliary statistics @xmath133 are directly available by the fact that for the log - normal distribution the @xmath37-th raw moment is given by @xmath38}&={\\operatorname{e}^{l\\theta+\\frac{1}{2 } l^2 \\sigma^2}}\\end{aligned}\\ ] ] and therefore it s derivative is defined by @xmath134}}{\\partial \\theta}&=l{\\operatorname{e}^{l\\theta+\\frac{1}{2 } l^2 \\sigma^2}}.\\end{aligned}\\ ] ] accordingly , for the weibull distribution we have @xmath38}=\\theta^l\\gamma_l,\\end{aligned}\\ ] ] and @xmath134}}{\\partial \\theta}=l\\theta^{l-1}\\gamma_l.\\end{aligned}\\ ] ]    table[x index=0 , y index=1]weibull_loss_lmo_k1.txt ; ;    table[x index=0 , y index=1]weibull_loss_lmo_k2.txt ; ;    table[x index=0 , y index=1]weibull_loss_lmo_k3.txt ; ;    table[x index=0 , y index=1]weibull_loss_lmo_k4.txt ; ;    table[x index=0 , y index=1]lognormal_loss_lmo_l1.txt ; ;    table[x index=0 , y index=1]lognormal_loss_lmo_l2.txt ; ;    table[x index=0 , y index=1]lognormal_loss_lmo_l3.txt ; ;    table[x index=0 , y index=1]lognormal_loss_lmo_l4.txt ; ;    in fig .",
    "[ lmo_weibull_loss ] and fig .",
    "[ lmo_lognormal_loss ] we depict the approximation loss @xmath135 for different values @xmath12 . for the weibull distribution , for which the result is depicted in fig .",
    "[ lmo_weibull_loss ] , we observe , that the bound with @xmath136 @xcite and @xmath137 @xcite can be significantly improved by incorporating the derivatives of the third and the fourth moment .",
    "in contrast for the log - normal distribution ( see fig . [ lmo_lognormal_loss ] ) taking into account more than the first two raw moments results only in a slight performance improvement . in order to visualize the result of the optimization of the bound , in fig .",
    "[ 4mo_weibull_weights ] the normalized absolute weights @xmath138 are plotted for the weibull example , where with and @xmath139_l}{\\sqrt{\\big(\\frac{\\partial { \\boldsymbol{\\mu}}_{{\\boldsymbol{\\phi}}}(\\theta)}{\\partial \\theta}\\big)^{\\operatorname{t } }   { \\boldsymbol{r}}_{{\\boldsymbol{\\phi}}}^{-1}(\\theta)\\frac{\\partial { \\boldsymbol{\\mu}}_{{\\boldsymbol{\\phi}}}(\\theta)}{\\partial \\theta}}}.\\end{aligned}\\ ] ] the individual normalized weights @xmath140 indicate the importance of the corresponding auxiliary statistic @xmath141 in the approximation of the fisher information .",
    "it can be seen , that for the weibull distribution the sufficient statistic @xmath9 attains the full weight in the cases @xmath142 .",
    "table[x index=0 , y index=1]weibull_4mo_weight.txt ; ;    table[x index=0 , y index=2]weibull_4mo_weight.txt ; ;    table[x index=0 , y index=3]weibull_4mo_weight.txt ; ;    table[x index=0 , y index=4]weibull_4mo_weight.txt ; ;    in contrast for the log - normal distribution it is observed in fig .",
    "[ 4mo_lognormal_weights ] that none of the moments obtains full weight .",
    "however , the first moment plays a dominant role , in particular when @xmath143 .",
    "note , that for the log - normal distribution with known scale parameter @xmath5 , @xmath7 is a sufficient statistic .",
    "incorporating this statistics into the approximation by using @xmath144 would change the situation and provide a tight approximation for @xmath47 .",
    "table[x index=0 , y index=1]lognormal_4mo_weights.txt ; ;    table[x index=0 , y index=2]lognormal_4mo_weights.txt ; ;    table[x index=0 , y index=3]lognormal_4mo_weights.txt ; ;    table[x index=0 , y index=4]lognormal_4mo_weights.txt ; ;",
    "the previous section indicates an interesting application of the presented result . being able to describe the expectations and for an arbitrary statistical model @xmath14 with an arbitrary set of auxiliary functions @xmath145 , the optimization result can be used in order to identify , among the auxiliary statistics @xmath145 , candidates for the sufficient statistics of the system model .",
    "further , together with and , the information bound allows to specify the minimum inference capability that can be guaranteed to be achievable for the model of interest .",
    "this has high practical relevance as in real - world applications technical systems are subject to various random and non - linear effects . under such circumstances an accurate analytical description of the probability density or mass function @xmath14 is usually hard to obtain",
    "nevertheless , to be able to identify transformations of the data exhibiting high information content is attractive in such a situation .",
    "such functions can be used for applications like efficient data compression and for the formulation of high - resolution estimation algorithms .",
    "further , a conservative approximation of the fisher information measure like allows to benchmark the performance of estimation algorithms on the system under investigation or to identify system layouts of high estimation theoretic quality .",
    "if the system parameter @xmath19 can be controlled in a calibrated setup , the entities and can be determined for any system @xmath14 by measurements at the system output @xmath2 .",
    "finally , we address the question how to perform estimation of the system parameter @xmath19 under an unknown statistical system model @xmath14 after having learned the properties @xmath159 and @xmath160 with an arbitrary set of @xmath12 auxiliary statistics @xmath145 by calibrated measurements .      observing @xmath0 samples at the system output of @xmath14 ,",
    "the data vector @xmath161 is available .",
    "first we apply compression by using the auxiliary statistics @xmath145 to form the sample mean @xmath162 and subsequently discarding the original data @xmath163 .",
    "note that this reduces the size of the data by a factor of @xmath164 .",
    "if the analytic characterization of @xmath14 is available , given the data @xmath163 one usually resorts to the asymptotically optimum estimator based on the maximization of the log - likelihood @xmath165 this is not possible if no description of the model @xmath14 is available .",
    "we propose to replace the original system @xmath14 by a distribution of the exponential family with log - likelihood function @xmath166 note , that the replacement has as sufficient statistics the auxiliary functions @xmath145 . assuming arbitrary weighting factors @xmath167",
    ", the score function takes the form @xmath168 a conservative maximum - likelihood estimate ( cmle ) is then found by setting the score of the data to zero , i.e. @xmath169 for a memoryless system , the receive score can be written @xmath170 such that the cmle @xmath171 is found by solving @xmath172 with respect to @xmath19 .",
    "note , that for the calculation of the cmle , access to the original data @xmath163 is not required .",
    "if @xmath14 is the data - generating model , the cmle @xmath171 , asymptotically in @xmath0 , produces consistent estimates which are gaussian distributed @xmath173    see appendix [ append : amle : perf : unbias ] .    using the best weighting @xmath174 for the auxiliary statistics ,",
    "the cmle is found by solving @xmath175 the estimator then achieves a performance equivalent to the inverse of the approximation for the fisher information measure .",
    "if @xmath14 is the data - generating model , the cmle @xmath171 calculated with optimized weights @xmath176 , asymptotically in @xmath0 , produces consistent estimates which are gaussian distributed @xmath177    follows from the fact that with @xmath178      by squaring the cmle can be reformulated @xmath179 which is identified as special case of hansen s estimator @xcite @xmath180 with the moment condition @xmath181 and an optimized weighting matrix @xmath182 the generalized method of moments is an extension of the classical method of moments @xcite , derived by considering orthogonality conditions @xcite @xmath183}={\\boldsymbol{0}}\\end{aligned}\\ ] ] under the true parameter @xmath184 .",
    "it is interesting to observe , that we obtain the method as a straightforward maximum - likelihood estimator after approximating the original system model @xmath185 through a set of auxiliary statistics @xmath145 by the closest ( in the sense of the fisher information measure ) equivalent distribution @xmath186 within the exponential family .",
    "therefore the equivalent exponential replacement provides a potential unifying link , like subtly requested by @xcite , between pearson s method of moments @xcite and fisher s competing concept of likelihood @xcite .",
    "we have established a generic approach for the construction of strong lower bounds for the fisher information measure . for an arbitrary statistical model @xmath80 and a set of auxiliary statistics @xmath122 , we approximate the original likelihood function by using the likelihood function @xmath109 of a distribution which belongs to the exponential family and exhibits the chosen set of auxiliary statistics @xmath122 as it s sufficient statistics @xmath107 .",
    "such a replacement model exhibits lower fisher information than the original data - generating probability law . through optimization of the derivative of the natural parameters of the replacement model we find the closest ( in the fisher information sense ) replacement in the exponential family and",
    "therefore obtain an accurate pessimistic approximation for the fisher information measure .",
    "the presented method has the advantage that the statistical output model @xmath14 does not have to be known .",
    "the required expected values of the statistics can be learned from calibrated measurements at the system output .",
    "we have demonstrated that based on the learned auxiliary statistics and their optimized weights , consistent estimators can be formulated which achieve a performance that is equivalent to the inverse of the presented pessimistic approximation of the fisher information measure .",
    "this forms a framework for the problem of parameter estimation with nonlinear systems . for any system model",
    "the results can be used to conservatively predict the theoretically possible estimation performance by analytical derivation or calibrated measurement of the mean and covariance of the auxiliary statistics .",
    "in addition our discussion provides a straightforward derivation of a data - efficient estimation method linked to the method of moments which allows to achieve the pessimistically approximated asymptotic system performance in practice .",
    "with @xmath12 functions @xmath187 , @xmath12 weighting vectors @xmath188 and a normalizer @xmath189 , we have @xmath190 } ) } { \\partial { \\boldsymbol{\\theta } } } \\bigg)^{\\operatorname{t } } { \\boldsymbol{b}}^{\\operatorname{t}}_l({\\boldsymbol{\\theta}})\\end{aligned}\\ ] ] and @xmath191 } ) } { \\partial { \\boldsymbol{\\theta}}}.\\end{aligned}\\ ] ] supposing that the best choice for @xmath189 is @xmath192},\\end{aligned}\\ ] ] it holds that @xmath193}\\notag\\\\ & = { \\operatorname{e}_{{\\boldsymbol{z}};{\\boldsymbol{\\theta } } } \\left [ \\bigg ( \\sum_{l=1}^{l } { \\boldsymbol{b}}_l({\\boldsymbol{\\theta } } ) \\phi_l({\\boldsymbol{z}})\\bigg ) \\bigg ( \\sum_{l=1}^{l } { \\boldsymbol{b}}_l({\\boldsymbol{\\theta } } ) \\phi_l({\\boldsymbol{z } } ) \\bigg)^{\\operatorname{t } } \\right]}\\notag\\\\&-\\bigg(\\sum_{l=1}^{l } { \\boldsymbol{b}}_l({\\boldsymbol{\\theta } } ) { \\operatorname{e}_{{\\boldsymbol{z}};{\\boldsymbol{\\theta } } } \\left [   \\phi_l({\\boldsymbol{z } } ) \\right ] } \\bigg ) \\bigg ( \\sum_{l=1}^{l } { \\boldsymbol{b}}_l({\\boldsymbol{\\theta } } ) { \\operatorname{e}_{{\\boldsymbol{z}};{\\boldsymbol{\\theta } } } \\left [ \\phi_l({\\boldsymbol{z}})\\right]}\\bigg)^{\\operatorname{t}}.\\end{aligned}\\ ] ] through the covariance inequality @xcite , we obtain @xmath194}}{\\partial { \\boldsymbol{\\theta } } } \\bigg)^{\\operatorname{t } }   { \\boldsymbol{b}}^{\\operatorname{t}}_l({\\boldsymbol{\\theta } } )   \\bigg )   \\notag\\\\ & \\cdot\\bigg({\\operatorname{e}_{{\\boldsymbol{z}};{\\boldsymbol{\\theta } } } \\left [ \\bigg ( \\sum_{l=1}^{l } { \\boldsymbol{b}}_l({\\boldsymbol{\\theta } } ) \\phi_l({\\boldsymbol{z}})\\bigg ) \\bigg ( \\sum_{l=1}^{l } { \\boldsymbol{b}}_l({\\boldsymbol{\\theta } } ) \\phi_l({\\boldsymbol{z } } ) \\bigg)^{\\operatorname{t } } \\right]}\\notag\\\\ & -\\bigg(\\sum_{l=1}^{l } { \\boldsymbol{b}}_l({\\boldsymbol{\\theta } } ) { \\operatorname{e}_{{\\boldsymbol{z}};{\\boldsymbol{\\theta } } } \\left [   \\phi_l({\\boldsymbol{z } } ) \\right ] } \\bigg ) \\bigg ( \\sum_{l=1}^{l } { \\boldsymbol{b}}_l({\\boldsymbol{\\theta } } ) { \\operatorname{e}_{{\\boldsymbol{z}};{\\boldsymbol{\\theta } } } \\left [ \\phi_l({\\boldsymbol{z}})\\right]}\\bigg)^{\\operatorname{t}}\\bigg)^{-1}\\notag\\\\ & \\cdot \\bigg(\\sum_{l=1}^{l } { \\boldsymbol{b}}_l({\\boldsymbol{\\theta } } ) \\frac{\\partial   { \\operatorname{e}_{{\\boldsymbol{z}};{\\boldsymbol{\\theta } } } \\left [   \\phi_l({\\boldsymbol{z } } ) \\right]}}{\\partial { \\boldsymbol{\\theta } } } \\bigg).\\end{aligned}\\ ] ]",
    "note , that with @xmath195 and optimized weightings @xmath125 , due to the definition , we have @xmath196}\\notag\\\\ & -{\\boldsymbol{b}}^{\\star\\operatorname{t}}({\\boldsymbol{\\theta}}){\\boldsymbol{\\mu}}_{{\\boldsymbol{t}}}({\\boldsymbol{\\theta } } ) { \\boldsymbol{\\mu}}^{\\operatorname{t}}_{{\\boldsymbol{t}}}({\\boldsymbol{\\theta } } )   { \\boldsymbol{b}}^{\\star}({\\boldsymbol{\\theta}}).\\end{aligned}\\ ] ] now let us assume that one possible optimizer is @xmath197",
    ". then @xmath198}\\notag\\\\   & -\\big(\\frac{\\partial { \\boldsymbol{w}}({\\boldsymbol{\\theta } } ) } { \\partial   { \\boldsymbol{\\theta } } } \\big)^{\\operatorname{t } } { \\boldsymbol{\\mu}}_{{\\boldsymbol{t}}}({\\boldsymbol{\\theta } } ) { \\boldsymbol{\\mu}}^{\\operatorname{t}}_{{\\boldsymbol{t}}}({\\boldsymbol{\\theta } } ) \\frac{\\partial { \\boldsymbol{w}}({\\boldsymbol{\\theta } } ) } { \\partial   { \\boldsymbol{\\theta}}}.\\end{aligned}\\ ] ] with @xmath199 and @xmath200 } & = { \\boldsymbol{\\mu}}_{{\\boldsymbol{t}}}^{\\operatorname{t}}({\\boldsymbol{\\theta } } ) \\frac{\\partial { \\boldsymbol{w}}({\\boldsymbol{\\theta } } ) } { \\partial   { \\boldsymbol{\\theta } } }   - \\frac{\\partial\\lambda({\\boldsymbol{\\theta}})}{\\partial { \\boldsymbol{\\theta}}}\\notag\\\\ & = { \\boldsymbol{0}}^{\\operatorname{t}}\\end{aligned}\\ ] ] we obtain @xmath201}\\notag\\\\ & -\\bigg ( \\frac{\\partial\\lambda({\\boldsymbol{\\theta}})}{\\partial { \\boldsymbol{\\theta } } } \\bigg)^{\\operatorname{t } } \\bigg (   \\frac{\\partial\\lambda({\\boldsymbol{\\theta}})}{\\partial { \\boldsymbol{\\theta } } } \\bigg)\\notag\\\\ & = { \\operatorname{e}_{{\\boldsymbol{z}};{\\boldsymbol{\\theta } } } \\left [ \\bigg ( \\frac{\\partial \\ln p({\\boldsymbol{z}};{\\boldsymbol{\\theta}})}{\\partial { \\boldsymbol{\\theta } } } \\bigg)^{\\operatorname{t } }   \\bigg(\\frac{\\partial \\ln p({\\boldsymbol{z}};{\\boldsymbol{\\theta}})}{\\partial { \\boldsymbol{\\theta } } }   \\bigg ) \\right ] } \\notag\\\\ & = \\bigg ( \\frac{\\partial { \\boldsymbol{\\mu}}_{{\\boldsymbol{t}}}({\\boldsymbol{\\theta}})}{\\partial { \\boldsymbol{\\theta } } } \\bigg)^{\\operatorname{t } } \\frac{\\partial { \\boldsymbol{w}}({\\boldsymbol{\\theta}})}{\\partial { \\boldsymbol{\\theta}}}.\\end{aligned}\\ ] ] using in we obtain @xmath202 which holds with equality due to the identity .",
    "in order to analyze the performance of the cmle , we proceed according to @xcite and use a taylor expansion of the used score function around the true parameter @xmath184 @xmath203 due to the property @xmath204 such that @xmath205 the denominator of @xmath206 converges towards the constant value @xmath207}&= \\sum_{l=1}^{l } b_l(\\theta_t ) \\frac { \\partial { \\operatorname{e}_{z;\\theta_t } \\left[\\phi_l(z)\\right ] } } { \\partial \\theta } \\notag\\\\ & = { \\boldsymbol{b}}^{\\operatorname{t}}(\\theta_t)\\frac{\\partial { \\boldsymbol{\\mu}}_{{\\boldsymbol{\\phi}}}(\\theta_t)}{\\partial \\theta}\\end{aligned}\\ ] ] where we used the derivative of the replacement score @xmath208}\\big)\\notag\\\\   & - b_l(\\theta ) \\frac { \\partial { \\operatorname{e}_{z;\\theta } \\left[\\phi_l(z)\\right ] } } { \\partial \\theta } \\end{aligned}\\ ] ] and the property @xmath209 } & =   -   \\sum_{l=1}^{l } b_l(\\theta ) \\frac { \\partial { \\operatorname{e}_{z;\\theta } \\left[\\phi_l(z)\\right ] } } { \\partial \\theta } .\\end{aligned}\\ ] ] due to the central limit theorem and the property @xmath210 } = 0,\\end{aligned}\\ ] ] the nominator of @xmath211 converges to a gaussian random variable with zero mean @xmath212}\\notag\\\\ & = \\sqrt{n } { \\operatorname{e}_{z;\\theta } \\left [ \\left.\\frac{\\partial \\ln \\tilde{p}(z;{\\theta})}{\\partial \\theta } \\right|_{\\theta={\\theta}_t}\\right]}=0\\end{aligned}\\ ] ] and variance @xmath213}\\notag\\\\ & = { \\operatorname{e}_{z;\\theta } \\left [ \\bigg(\\left.\\frac{\\partial \\ln \\tilde{p}(z;{\\theta})}{\\partial \\theta }   \\bigg)^2 \\right|_{\\theta={\\theta}_t}\\right]}\\notag\\\\ & = { \\operatorname{e}_{z;\\theta } \\left[\\bigg(\\sum_{l=1}^{l } b_l(\\theta ) \\big(\\phi_l(z ) - { \\operatorname{e}_{z;\\theta } \\left[\\phi_l(z)\\right]}\\big)\\bigg)^{2}\\right]}\\notag\\\\ & = { \\operatorname{e}_{z;\\theta } \\left[\\bigg(\\sum_{l=1}^{l } b_l(\\theta ) \\phi_l(z ) \\bigg)^2 \\right ] } - \\bigg(\\sum_{l=1}^{l } b_l(\\theta ) { \\operatorname{e}_{z;\\theta } \\left[\\phi_l(z)\\right ] } \\bigg)^2\\notag\\\\ & = { \\boldsymbol{b}}^{\\operatorname{t}}(\\theta ) { \\boldsymbol{r}}_{{\\boldsymbol{\\phi}}}(\\theta ) { \\boldsymbol{b}}(\\theta).\\end{aligned}\\ ] ] with slutsky s theorem @xcite , it follows that asymptotically @xmath214                              k. barb , l. gonzales fuentes , l. barford and l. lauwers ,  a guaranteed blind and automatic probability density estimation of raw measurements , \" _ ieee trans .",
    "9 , pp . 21202128 , sept . 2014 .                                                          m. stein , j. a. nossek and k. barb ,  measurement - driven quality assessment of nonlinear systems by exponential replacement , \" submitted for presentation at _ ieee int . instrumentation and measurement technology conf .",
    "( i2mtc ) _ , taipei , taiwan , 2016 .",
    "m. stein , k. barb and j. a. nossek ,  doa parameter estimation with 1-bit quantization : bounds , methods and the exponential replacement , \" submitted for presentation at _ int .",
    "itg workshop on smart antennas ( wsa ) _ , munich , germany , 2016 ."
  ],
  "abstract_text": [
    "<S> the problem how to derive a generic lower bound for the fisher information measure is considered . </S>",
    "<S> we review a recent approach by two examples and identify a connection between the construction of strong fisher information bounds and the sufficient statistics of the underlying system model . in order to present the problem of such information bounds within a broad scope </S>",
    "<S> , we discuss the properties of the fisher information measure for distributions belonging to the exponential family . under this restriction , we establish an identity connecting fisher information , the natural parameters and the sufficient statistics of the system model . replacing an arbitrary system model by an equivalent distribution within the exponential family </S>",
    "<S> , we then derive a general lower bound for the fisher information measure . with the optimum estimation theoretic model matching rule </S>",
    "<S> we show how to obtain a strong version of the information bound . </S>",
    "<S> we then demonstrate different applications of the proposed conservative likelihood framework and the derived fisher information bound . in particular , we discuss how to determine the minimum guaranteed inference capability of a memoryless system with unknown statistical output model and show how to achieve this pessimistic performance assessment with a root - n consistent estimator operating on a nonlinear compressed version of the observed data . </S>",
    "<S> finally , we identify that the derived conservative maximum - likelihood algorithm can be formulated as a special version of hansen s generalized method of moments .    </S>",
    "<S> fisher information , cramr - rao lower bound , nonlinear stochastic system , estimation theory , exponential family , compression , nonlinear learning , rapp model , generalized method of moments , measurement uncertainty . </S>"
  ]
}