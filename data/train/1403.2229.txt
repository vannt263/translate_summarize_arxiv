{
  "article_text": [
    "a critical problem faced by participants in investment markets is the so - called optimal liquidation problem , viz . how best to trade a given block of shares at minimal cost",
    "here , cost can be interpreted as in perold s implementation shortfall ( @xcite ) , i.e. adverse deviations of actual transaction prices from an arrival price baseline when the investment decision is made .",
    "alternatively , cost can be measured as a deviation from the market volume - weighted trading price ( vwap ) over the trading period , effectively comparing the specific trader s performance to that of the average market trader . in each case , the primary problem faced by the trader / execution algorithm is the compromise between price impact and opportunity cost when executing an order .",
    "price impact here refers to adverse price moves due to a large trade size absorbing liquidity supply at available levels in the order book ( temporary price impact ) .",
    "as market participants begin to detect the total volume being traded , they may also adjust their bids / offers downward / upward to anticipate order matching ( permanent price impact ) @xcite . to avoid price impact , traders may split a large order into smaller child orders over a longer period . however , there may be exogenous market forces which result in execution at adverse prices ( opportunity cost ) .",
    "this behaviour of institutional investors was empirically demonstrated in @xcite , where they observed that typical trades of large investment management firms are almost always broken up into smaller trades and executed over the course of a day or several days .",
    "several authors have studied the problem of optimal liquidation , with a strong bias towards stochastic dynamic programming solutions .",
    "see @xcite , @xcite , @xcite , @xcite as examples . in this paper",
    ", we consider the application of a machine learning technique to the problem of optimal liquidation .",
    "specifically we consider a case where the popular almgren - chriss closed - form solution for a trading trajectory ( see @xcite ) can be enhanced by exploiting microstructure attributes over the trading horizon using a reinforcement learning technique .",
    "reinforcement learning in this context is essentially a calibrated policy mapping states to optimal actions .",
    "each state is a vector of observable attributes which describe the current configuration of the system .",
    "it proposes a simple , model - free mechanism for agents to learn how to act optimally in a controlled markovian domain , where the quality of action chosen is successively improved for a given state @xcite . for the optimal liquidation problem",
    ", the algorithm examines the salient features of the current order book and current state of execution in order to decide which action ( e.g. child order price or volume ) to select to service the ultimate goal of minimising cost .",
    "the first documented large - scale empirical application of reinforcement learning algorithms to the problem of optimised trade execution in modern financial markets was conducted by @xcite .",
    "they set up their problem as a minimisation of implementation shortfall for a buying / selling program over a fixed time horizon with discrete time periods . for actions ,",
    "the agent could choose a price to repost a limit order for the remaining shares in each discrete period .",
    "state attributes included elapsed time , remaining inventory , current spread , immediate cost and signed volume . in their results",
    ", they found that their reinforcement learning algorithm improved the execution efficiency by 50% or more over traditional submit - and - leave or market order policies .    instead of a pure reinforcement learning solution to the problem , as in @xcite",
    ", we propose a hybrid approach which _ enhances _ a given analytical solution with attributes from the market microstructure . using the almgren - chriss ( ac ) model as a base , for a finite liquidation horizon with discrete trading periods , the algorithm determines the proportion of the ac - suggested trajectory to trade based on prevailing volume / spread attributes .",
    "one would expect , for example , that allowing the trajectory to be more aggressive when volumes are relatively high and spreads are tight may reduce the ultimate cost of the trade . in our implementation , a static volume trajectory is preserved for the duration of the trade , however the proportion traded is dynamic with respect to market dynamics . as in @xcite ,",
    "a market order is executed at the end of the trade horizon for the remaining volume , to ensure complete liquidation .",
    "an important consideration in our analysis is the specification of the problem as a finite - horizon markov decision process ( mdp ) and the consequences for optimal policy convergence of the reinforcement learning algorithm . in @xcite , they use an approximation in their framework to address this issue by incorporating elapsed time as a state attribute , however they do not explicitly discuss convergence .",
    "we will use the findings of @xcite in our model specification and demonstrate near - optimal policy convergence of the finite - horizon mdp problem .",
    "the model described above is compared with the base almgren - chriss model to determine whether it increases / decreases the cost of execution for different types of trades consistently and significantly .",
    "this study will help determine whether reinforcement learning is a viable technique which can be used to extend existing closed - form solutions to exploit the nuances of the microstructure where the algorithms are applied .",
    "this paper proceeds as follows : section 2 introduces the standard almgren - chriss model .",
    "section 3 describes the specific hybrid reinforcement learning technique proposed , along with a discussion regarding convergence to optimum action values .",
    "section 4 discusses the data used and results , comparing the 2 models for multiple trade types .",
    "section 5 concludes and proposes some extensions for further research .",
    "bertsimas and lo are pioneers in the area of optimal liquidation , treating the problem as a stochastic dynamic programming problem @xcite .",
    "they employed a dynamic optimisation procedure which finds an explicit closed - form best execution strategy , minimising trading costs over a fixed period of time for large transactions .",
    "almgren and chriss extended the work of @xcite to allow for risk aversion in their framework @xcite .",
    "they argue that incorporating the uncertainty of execution of an optimal solution is consistent with a trader s utility function .",
    "in particular , they employ a price process which permits linear permanent and temporary price impact functions to construct an efficient frontier of optimal execution .",
    "they define a trading strategy as being _ efficient _ if there is no strategy which has lower execution cost variance for the same or lower level of expected execution cost .",
    "the exposition of their solution is as follows : they assume that the security price evolves according to a discrete arithmetic random walk : @xmath0 where : @xmath1    here , permanent price impact refers to changes in the equilibrium price as a direct function of our trading , which persists for at least the remainder of the liquidation horizon .",
    "temporary price impact refers to adverse deviations as a result of absorbing available liquidity supply , but where the impact dissipates by the next trading period due to the resilience of the order book .",
    "almgren and chriss introduce a temporary price impact function @xmath2 to their model , where @xmath2 causes a temporary adverse move in the share price as a function of our trading rate @xmath3 @xcite .",
    "given this addition , the actual security transaction price at time @xmath4 is given by : @xmath5    assuming a _ sell _ program , we can then define the total trading revenue as : @xmath6 where @xmath7 for @xmath8 .",
    "the total cost of trading is thus given by @xmath9 , i.e. the difference between the target revenue value and the total actual revenue from the execution .",
    "this definition refers to perold s implementation shortfall measure ( see @xcite ) , and serves as the primary transaction cost metric which is minimised in order to maximise trading revenue .",
    "since implementation shortfall is a random variable , almgren and chriss compute the following : @xmath10 and @xmath11 the distribution of implementation shortfall is gaussian if the @xmath12 are gaussian .",
    "given the overall goal of minimising execution costs and the variance of execution costs , they specify their objective function as : @xmath13 where : @xmath14    the intuition of this objective function can be thought of as follows : consider a stock which exhibits high price volatility and thus a high risk of price movement away from the reference price . a risk averse trader would prefer to trade a large portion of the volume immediately , causing a ( known ) price impact , rather than risk trading in small increments at successively adverse prices . alternatively ,",
    "if the price is expected to be stable over the liquidation horizon , the trader would rather split the trade into smaller sizes to avoid price impact .",
    "this trade - off between speed of execution and risk of price movement is what governs the shape of the resulting trade trajectory in the ac framework .",
    "a detailed derivation of the general solution can be found in @xcite . here ,",
    "we state the general solution : @xmath15 the associated trade list is : @xmath16 where : @xmath17    this implies that for a program of selling an initially long position , the solution decreases monotonically from its initial value to zero at a rate determined by the parameter @xmath18 .",
    "if trading intervals are short , @xmath19 is essentially the ratio of the product of volatility and risk - intolerance to the temporary transaction cost parameter .",
    "we note here that a larger value of @xmath18 implies a _ more rapid _ trading program , again conceptually confirming the propositions of @xcite that an intolerance for execution risk leads to a larger concentration of quantity traded early in the trading program .",
    "another consequence of this analysis is that different sized baskets of the same securities will be liquidated in the same manner , barring scale differences and provided the risk aversion parameter @xmath20 is held constant .",
    "this may be counter - intuitive , since one would expect larger baskets to be effectively less liquid , and thus follow a _ less rapid _ trading program to minimise price impact costs .",
    "it should be noted that the ac solution yields a suggested volume trajectory over the liquidation horizon , however there is no discussion in @xcite as to the prescribed _ order type _ to execute the trade list . we have assumed that the trade list can be executed as a series of _ market orders_. given that this implies we are always crossing the spread",
    ", one needs to consider that traversing an order book with thin volumes and widely - spaced prices could have a significant transaction cost impact .",
    "we thus consider a reinforcement learning technique which learns _ when _ and _ how much _ to cross the spread , based on the current order book dynamics .",
    "the general solution outlined above assumes linear price impact functions , however the model was extended by almgren in @xcite to account for non - linear price impact .",
    "this extended model can be considered as an alternative base model in future research .",
    "the majority of reinforcement learning research is based on a formalism of markov decision processes ( mdps ) @xcite . in this context ,",
    "reinforcement learning is a technique used to numerically solve for a calibrated policy mapping states to optimal or near - optimal actions .",
    "it is a framework within which a learning agent repeatedly observes the state of its environment , and then performs a chosen action to service some ultimate goal .",
    "performance of the action has an immediate numeric reward or penalty and changes the state of the environment @xcite .",
    "the problem of solving for an optimal policy mapping states to actions is well - known in stochastic control theory , with a significant contribution by bellman @xcite .",
    "bellman showed that the computational burden of an mdp can be significantly reduced using what is now known as dynamic programming .",
    "it was however recognised that two significant drawbacks exist for classical dynamic programming : firstly , it assumes that a complete , known model of the environment exists , which is often not realistically obtainable .",
    "secondly , problems rapidly become computationally intractable as the number of state variables increases , and hence , the size of the state space for which the value function must be computed increases .",
    "this problem is referred to as the _ curse of dimensionality _ @xcite .",
    "reinforcement learning offers two advantages over classical dynamic programming : firstly , agents learn online and continually adapt while performing the given task . secondly , the methods can employ function approximation algorithms to represent their knowledge .",
    "this allows them to generalize across the state space so that the learning time scales much better @xcite .",
    "reinforcement learning algorithms do not require knowledge about the exact model governing an mdp and thus can be applied to mdp s where exact methods become infeasible .",
    "although a number of implementations of reinforcement learning exist , we will focus on _ q - learning_. this is a model - free technique first introduced by @xcite , which can be used to find the optimal , or near - optimal , action - selection policy for a given mdp .    during _ q - learning _",
    ", an agent s learning takes place during sequential episodes .",
    "consider a discrete finite world where at each step @xmath21 , an agent is able to register the current state @xmath22 and can choose from a finite set of actions @xmath23 .",
    "the agent then receives a probabilistic reward @xmath24 , whose mean value @xmath25 depends only on the current state and action . according to @xcite , the state of the world changes probabilistically to @xmath26 according to : @xmath27 the agent is then tasked to learn the optimal policy mapping states to actions , i.e. one which maximises total discounted expected reward . under some policy mapping @xmath28 and discount rate @xmath29 ( @xmath30 ) ,",
    "the value of state @xmath31 is given by : @xmath32 according to @xcite and @xcite , the theory of dynamic programming says there is at least one optimal stationary policy @xmath33 such that @xmath34 we also define @xmath35 as the expected discounted reward from choosing action @xmath36 in state @xmath31 , and then following policy @xmath28 thereafter , i.e. @xmath37    the task of the _ q - learning _",
    "agent is to determine @xmath38 , @xmath33 and @xmath39 where @xmath40 is unknown , using a combination of exploration and exploitation techniques over the given domain .",
    "it can be shown that @xmath41 and that an optimal policy can be formed such that @xmath42 .",
    "it thus follows that if the agent can find the optimal q - values , the optimal action can be inferred for a given state @xmath31 .",
    "it is shown in @xcite that an agent can learn q - values via experiential learning , which takes place during sequential episodes . in the @xmath43 episode , the agent :    *",
    "observes its current state @xmath44 , * selects and performs an action @xmath45 , * observes the subsequent state @xmath26 as a result of performing action @xmath45 , * receives an immediate reward @xmath24 and * uses a learning factor @xmath46 , which decreases gradually over time .",
    "@xmath47 is updated as follows : @xmath48    provided each state - action pair is visited infinitely often , @xcite show that @xmath47 converges to @xmath49 for any exploration policy .",
    "singh et al .",
    "provide guidance as to specific exploration policies for asymptotic convergence to optimal actions and asymptotic exploitation under the _ q - learning _",
    "algorithm , which we incorporate in our analysis @xcite .",
    "the above exposition presents an algorithm which guarantees optimal policy convergence of a stationary infinite - horizon mdp .",
    "the stationarity assumption , and hence validity of the above result , needs to be questioned when considering a finite - horizon mdp , since states , actions and policies are time - dependent @xcite .",
    "in particular , we are considering a discrete period , finite trading horizon , which guarantees execution of a given volume of shares . at each decision step in the trading horizon , it is possible to have different state spaces , actions , transition probabilities and reward values .",
    "hence the above model needs revision .",
    "garcia and ndiaye consider this problem and provide a model specification which suits this purpose @xcite .",
    "they propose a slight modification to the bellman optimality equations shown above : @xmath50 for all @xmath51 , @xmath52 , @xmath53 , @xmath54 and @xmath55 .",
    "this optimality equation has a single solution @xmath56 that can be obtained using dynamic programming techniques .",
    "the equivalent discounted expected reward specification thus becomes : @xmath57 they propose a novel transformation of an @xmath58-step non - stationary mdp into an infinite - horizon process ( @xcite ) .",
    "this is achieved by adding an artifical final reward - free absorbing state @xmath59 , such that all actions @xmath60 lead to @xmath59 with probability 1 .",
    "hence the revised _ q - learning _ update equation becomes : @xmath61 where @xmath62 if @xmath63 , @xmath64 , otherwise choose randomly in @xmath65 . if @xmath66 , select @xmath67 .",
    "the learning rule for @xmath68 is thus equivalent to setting @xmath69 @xmath70 @xmath71 .",
    "garcia and ndiaye further show that the above specification ( in the case where @xmath72 ) will converge to the optimal policy with probability one , provided that each state - action pair is visited infinitely often , @xmath73 and @xmath74 @xcite .",
    "given the above description , we are able to discuss our specific choices for state attributes , actions and rewards in the context of the optimal liquidation problem .",
    "we need to consider a specification which adequately accounts for our state of execution and the current state of the limit order book , representing the opportunity set for our ultimate goal of executing a volume of shares over a fixed trading horizon .",
    "we acknowledge that the complexity of the financial system can not be distilled into a finite set of states and is not likely to evolve according to a markov process",
    ". however , we conjecture that the essential features of the system can be sufficiently captured with some simplifying assumptions such that meaningful insights can still be inferred . for simplicity ,",
    "we have chosen a look - up table representation of @xmath47 .",
    "function approximation variants may be explored in future research for more complex system configurations .",
    "as described above , each state @xmath75 represents a vector of observable attributes which describe the configuration of the system at time @xmath21 . as in @xcite",
    ", we use _ elapsed time @xmath76 _ and _ remaining inventory @xmath77 _ as private attributes which capture our state of execution over a finite liquidation horizon @xmath78 .",
    "since our goal is to modify a given volume trajectory based on favourable market conditions , we include _ spread _ and _ volume _ as candidate market attributes .",
    "the intuition here is that the agent will learn to increase ( decrease ) trading activity when _ spreads _ are narrow ( wide ) and _ volumes _ are high ( low ) .",
    "this would ensure that a more significant proportion of the total volume - to - trade would be secured at a favourable price and , similarly , less at an unfavourable price , ultimately reducing the post - trade implementation shortfall . given the look - up table implementation ,",
    "we have simplified each of the state attributes as follows :    * @xmath78 = trading horizon , @xmath79 = total volume - to - trade , * @xmath80 = hour of day when trading will begin , * @xmath81 = number of remaining inventory states , * @xmath82 = number of spread states , * @xmath83 = number of volume states , * @xmath84 = % ile spread of the @xmath43 tuple , * @xmath85 = % ile bid / ask volume of the @xmath43 tuple , * * elapsed time * : @xmath86 , * * remaining inventory * : @xmath87 , * * spread state * : @xmath88 * * volume state * : @xmath89    thus , for the @xmath43 episode , the state attributes can be summarised as the following tuple : @xmath90 for @xmath84 and @xmath85 , we first construct a historical distribution of spreads and volumes based on the training set .",
    "it has been empirically observed that major equity markets exhibit _ u_-shaped trading intensity throughout the day , i.e. more activity in mornings and closer to closing auction",
    ". a further discussion of these insights can be found in @xcite and @xcite .",
    "in fact , @xcite empirically demonstrates that south african stocks exhibit similar characteristics .",
    "we thus consider simlulations where training volume / spread tuples are _",
    "h_-hour dependent , such that the optimal policy is further refined with respect to trading time ( _ h _ ) .      based on the almgren - chriss ( ac ) model specified above",
    ", we calculate the ac volume trajectory ( @xmath91 ) for a given volume - to - trade ( @xmath79 ) , fixed time horizon ( @xmath78 ) and discrete trading periods ( @xmath92 ) .",
    "@xmath93 represents the proportion of @xmath79 to trade in period @xmath76 , such that @xmath94 .",
    "for the purposes of this study , we assume that each child order is executed as a _ market order _ based on the prevailing limit order book structure .",
    "we would like our learning agent to modify the ac volume trajectory based on prevailing volume and spread characteristics in the market .",
    "as such , the possible actions for our agent include :    * @xmath95 = proportion of @xmath93 to trade , * @xmath96 = lower bound of volume proportion to trade , * @xmath97 = upper bound of volume proportion to trade , * * action * : @xmath98 , where @xmath99 + and @xmath100    the aim here is to train the learning agent to trade a higher ( lower ) proportion of the overall volume when conditions are favourable ( unfavourable ) , whilst still broadly preserving the volume trajectory suggested by the ac model . to ensure that the total volume - to - trade is executed over the given time horizon , we execute any residual volume at the end of the trading period with a _ market order_.      each of the actions described above results in a volume to execute with a _ market order _ , based on the prevailing structure of the limit order book .",
    "the size of the child order volume will determine how deep we will need to traverse the order book .",
    "for example , suppose we have a _ buy _ order with a _ volume - to - trade _ of 20000 , split into child orders of 10000 in period @xmath76 and 10000 in period @xmath101 .",
    "if the structure of the limit order book at time @xmath76 is as follows :    * _ level-1 ask price _ = 100.00 ; _",
    "level-1 ask volume _",
    "= 3000 * _ level-2 ask price _ = 100.50 ; _ level-2 ask volume _ = 4000 * _ level-3 ask price _ = 102.30 ; _ level-3 ask volume _ = 5000 * _ level-4 ask price _ = 103.00 ; _ level-4 ask volume _ = 6000 * _ level-5 ask price _ = 105.50 ; _ level-5 ask volume _",
    "= 2000    the volume - weighted execution price will be : @xmath102 trading more ( less ) given this limit order book structure will result in a higher ( lower ) volume - weighted execution price .",
    "if the following trading period @xmath101 has the following structure :    * _ level-1 ask price _ = 99.80 ; _ level-1 ask volume _",
    "level-2 ask price _",
    "= 99.90 ; _",
    "level-2 ask volume _",
    "level-3 ask price _",
    "= 101.30 ; _",
    "level-3 ask volume _",
    "level-4 ask price _",
    "= 107.00 ; _",
    "level-4 ask volume _",
    "= 3000 * _ level-5 ask price _ = 108.50 ; _ level-5 ask volume _",
    "= 1000    the volume - weighted execution price for the second child order will be : @xmath103 if the reference price of the stock at @xmath104 is 99.5 , then the _ implementation shortfall _ from this trade is:@xmath105 @xmath106 since the conditions of the limit order book were more favourable for _ buy _ orders in period @xmath101 , if we had modified the child orders to , say 8000 in period @xmath76 and 12000 in period @xmath101 , the resulting _ implementation shortfall _ would be:@xmath107 @xmath108 in this example , increasing the child order volume when _ ask prices _ are lower and _ level-1 volumes _ are higher decreases the overal cost of the trade .",
    "it is for this reason that _ implementation shortfall _ is a natural candidate for the rewards matrix in our reinforcement learning system .",
    "each action implies a child order volume , which has an associated volume - weighted execution price .",
    "the agent will learn the consequences of each action over the trading horizon , with the ultimate goal of minimising the total trade s _",
    "implementation shortfall_.      given the above specification , we followed the following steps to generate our results :    * specify a stock ( @xmath109 ) , volume - to - trade ( @xmath79 ) , time horizon ( @xmath78 ) , and trading datetime ( from which the trading hour @xmath80 is inferred ) , * partition the dataset into independent _ training sets _ and _ testing sets _ to generate results ( the _ training set _",
    "always pre - dates the _ testing set _ ) , * calibrate the parameters for the almgren - chriss ( ac ) volume trajectory ( @xmath110 ) using the historical _ training set _ ; set @xmath111 , since we assume order book is resilient to trading activity ( see below ) , * generate the ac volume trajectory ( @xmath112 ) , * train the _ q - matrix _ based on the state - action tuples generated by the _ training set _ , * execute the ac volume trajectory at the specified trading datetime ( @xmath80 ) on each day in the _ testing set _ , recording the _ implementation shortfall _ , * use the trained _ q - matrix _ to modify the ac trajectory as we execute @xmath79 at the specified trading datetime , recording the _ implementation shortfall _ and * determine whether the reinforcement learning ( rl ) model improved / worsened realised _",
    "implementation shortfall_.    in order to train the _ q - matrix _ to learn the optimal policy mapping , we need to traverse the training data set ( @xmath113 ) times , where @xmath114 is the total number of possible actions .",
    "the following pseudo - code illustrates the algorithm used to train the _ q - matrix _ :    .... optimal_strategy < v , t , i , a >     for ( episode 1 to n ) {        record reference price at t=0            for t = t to 1 {           for i = 1 to i              calculate episode 's state attributes < s , v >              for a = 1 to a {                  set x = < t , i , s , v >                  determine the action volume a                  calculate is from trade , r(x , a )                  simulate transition x to y                  look up max_p q(y , p )                  update q(x , a ) = q(x , a ) + alpha*u } } } select the lowest - is action max_p q(y , p ) for optimal policy ....    an important assumption in this model specification is that our trading activity does not affect the market attributes .",
    "although temporary price impact is incorporated into execution prices via depth participation of the _ market order _ in the prevailing limit order book , we assume the limit order book is resilient with respect to our trading activity .",
    "market resiliency can be thought of as the number of quote updates before the market s spread reverts to its competitive level .",
    "degryse et al",
    ". showed that a pure limit order book market ( euronext paris ) is fairly resilient with respect to most order sizes , taking on average 50 quote updates for the spread to normalise following the most aggressive orders @xcite .",
    "since we are using 5-minute trading intervals and small trade sizes , we will assume that any permanent price impact effects dissipate by the next trading period .",
    "a preliminary analysis of south african stocks revealed that there were on average over 1000 quote updates during the 5-minute trading intervals and the pre - trade order book equilibrium is restored within 2 minutes for large trades .",
    "the validity of this assumption however will be tested in future research , as well as other model specifications explored which incorporate permanent effects in the system configuration .",
    "for this study , we collected 12 months of market depth tick data ( jan-2012 to dec-2012 ) from the thomson reuters tick history ( trth ) database , representing a universe of 166 stocks that make up the south african local benchmark index ( alsi ) as at 31-dec-2012 .",
    "this includes 5 levels of order book depth ( bid / ask prices and volumes ) at each tick .",
    "the raw data was imported into a mongodb database and aggregated into 5-minute intervals showing average level prices and volumes , which was used as the basis for the analysis .      to test the robustness of the proposed model in the south african ( sa ) equity market we tested a variety of stock types , trade sizes and model parameters . due to space constraints",
    ", we will only show a representative set of results here that illustrate the insights gained from the analysis .",
    "the following summarises the stocks , parameters and assumptions used for the results that follow :    * * stocks * * * sbk ( large cap , financials ) * * agl ( large cap , resources ) * * sab ( large cap , industrials ) * * model parameters * * * @xmath96 : 0 , @xmath97 : 2 , @xmath115 : 0.25 * * @xmath20 : 0.01 , @xmath116 : 5-min , @xmath117 : 1 , @xmath29 : 1 * * @xmath79 : 100 000 , 1000 000 * * @xmath78 : 4 ( 20-min ) , 8 ( 40-min ) , 12 ( 60-min ) * * @xmath80 : 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 * * @xmath118 : 5 , 10 * * buy / sell : buy * * assumptions * * * max volume participation rate in order book : 20% * * market is resilient to our trading activity    note , we set @xmath72 since @xcite states that this is a necessary condition to ensure convergence to the optimal policy with probability one for a finite - horizon mdp .",
    "we also choose an arbitrary value for @xmath20 , although sensitivities to these parameters will be explored in future work .",
    "ac parameters are calibrated and _ q - matrix _ trained over a 6-month _ training set _ from 1-jan-2012 to 30-jun-2012 .",
    "the resultant ac and rl trading trajectories are then _ executed _ on each day at the specified trading time @xmath80 in the _ testing set _ from 1-jul-2012 to 20-dec-2012 .",
    "the _ implementation shortfall _ for both models is calculated and the difference recorded .",
    "this allows us to construct a distribution of _ implementation shortfall _ for each of the ac and rl models , and for all trading hours @xmath119 .",
    "table 1 shows the average % improvement in median _ implementation shortfall _ for the complete set of stocks and parameter values .",
    "these results suggest that the model is more effective for shorter trading horizons ( @xmath120 ) , with an average improvement of up to 10.3% over the base ac model .",
    "this result may be biased due to the assumption of order book resilience .",
    "indeed , the efficacy of the trained q - matrix may be less reliable for stocks which exhibit slow order book resilience , since permanent price effects would affect the state space transitions . in future work",
    ", we plan to relax this order book resilience assumption and incoporate permanent effects into state transitions .",
    "figure 1 illustrates the improvement in median post - trade _ implementation shortfall _ when executing the volume trajectories generated by each of the models , for each of the candidate stocks at the given trading times . in general ,",
    "the rl model is able to improve ( lower ) ex - post _ implementation shortfall _ , however the improvement seems more significant for early morning / late afternoon trading hours .",
    "this could be due to the increased trading activity at these times , resulting in more state - action visits in the _ training set _ to refine the associated q - matrix values .",
    "we also notice more dispersed performance between 10:00 and 11:00 .",
    "this time period coincides with the uk market open , where global events may drive local trading activity and skew results , particularly since certain sa stocks are dual - listed on the london stock exchange ( lse ) .",
    "the improvement in _ implementation shortfall _ ranges from 15 bps ( 85.3% ) for trading 1000 000 of sbk between 16:00 and 17:00 , to -7 bps ( -83.4% ) for trading 100 000 sab between 16:00 and 17:00 .",
    "overall , the rl model is able to improve _ implementation shortfall _ by 4.8% .",
    ".average % improvement in median _ implementation shortfall _ for various parameter values , using ac and rl models .",
    "training @xmath80-dependent .",
    "[ cols=\"<,<,<,^,^,^,^,^,^,^,^ , > \" , ]     figure 2 shows the % of _ correct actions _",
    "implied by the q - matrix , as it evolves through the training process after each tuple visit . here ,",
    "correct action _ is defined as a reduction ( addition ) in the volume - to - trade based on the max q - value action , in the case where _ spreads _ are above ( below ) the 50%ile and _ volumes _ are below ( above ) the 50%ile level .",
    "this coincides with the intuitive behaviour we would like the rl agent to learn .",
    "these results suggest that finer state granularity ( @xmath121 ) improves the overall accuracy of the learning agent , as demonstrated by the higher % _ correct actions _ achieved .",
    "all model configurations seem to converge to some _ stationary _ accuracy level after approximately 1000 tuple visits , suggesting that a shorter training period may yield similar results .",
    "we do however note that improving the % of correct actions by increasing the granularity of the state space does not necessarily translate into better model performance .",
    "this can be seen by table 1 , where the results where @xmath122 do not show any significant improvement over those with @xmath123 .",
    "this suggests that the market dynamics may not be fully represented by _",
    "volume _ and _ spread _ state attributes , and alternative state attributes should be explored in future work to improve ex - post model efficacy .",
    "table 2 shows the average standard deviation of the resultant _ implementation shortfall _ when using each of the ac and rl models .",
    "since we have not explicitly accounted for _ variance of execution _ in the rl reward function , we see that the resultant trading trajectories generate a higher standard deviation compared to the base ac model .",
    "thus , although the rl model provides a performance improvement over the ac model , this is achieved with a higher degree of execution risk , which may not be acceptable for the trader .",
    "we do note that the rl model exhibits comparable risk for @xmath120 , thus validating the use of the rl model to reliably improve is over short trade horizons . a future refinement on the rl model",
    "should incorporate _ variance of execution _ , such that it is consistent with the ac objective function . in this way",
    ", a true comparison of the techniques can be done , and one can conclude as to whether the rl model indeed outperforms the ac model at a statistically significant level .",
    "in this paper , we introduced reinforcement learning as a candidate machine learning technique to _ enhance _ a given optimal liquidation volume trajectory .",
    "nevmyvaka , feng and kearns showed that reinforcement learning delivers promising results where the learning agent is trained to choose the optimal limit order price at which to place the remaining inventory , at discrete periods over a fixed liquidation horizon @xcite . here , we show that reinforcement learning can also be used successfully to modify a given volume trajectory based on market attributes , executed via a sequence of _ market orders _ based on the prevailing limit order book .",
    "specifically , we showed that a simple look - up table _ q - learning _ technique can be used to train a learning agent to modify a static almgren - chriss volume trajectory based on prevailing spread and volume dynamics , assuming order book resiliency . using a sample of stocks and trade sizes in the south african equity market",
    ", we were able to reliably improve post - trade _ implementation shortfall _ by up to 10.3% on average for short trade horizons , demonstrating promising potential applications of this technique .",
    "further investigations include incorporating _ variance of execution _ in the rl reward function , relaxing the order book resiliency assumption and alternative state attributes to govern market dynamics .",
    "the authors thank dr nicholas westray for his contribution in the initiation of this work , as well as the insightful comments from the anonymous reviewers .",
    "this work is based on the research supported in part by the national research foundation of south africa ( grant number cprr 70643 )"
  ],
  "abstract_text": [
    "<S> reinforcement learning is explored as a candidate machine learning technique to enhance existing analytical solutions for optimal trade execution with elements from the market microstructure . given a volume - to - trade , fixed time horizon and discrete trading periods , the aim is to adapt a given volume trajectory such that it is dynamic with respect to favourable / unfavourable conditions during realtime execution , thereby improving overall cost of trading . </S>",
    "<S> we consider the standard almgren - chriss model with linear price impact as a candidate base model . </S>",
    "<S> this model is popular amongst sell - side institutions as a basis for arrival price benchmark execution algorithms . by training a learning agent to modify a volume trajectory based on the market s </S>",
    "<S> prevailing spread and volume dynamics , we are able to improve post - trade implementation shortfall by up to 10.3% on average compared to the base model , based on a sample of stocks and trade sizes in the south african equity market . </S>"
  ]
}