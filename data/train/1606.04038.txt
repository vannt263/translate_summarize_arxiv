{
  "article_text": [
    "multi - task learning ( mtl ) @xcite aims to learn multiple tasks jointly , so that knowledge obtained from one task can be reused by others .",
    "we first briefly review some studies in this area .    * * matrix - based multi - task learning**matrix - based mtl is usually built on linear models , i.e. , each task is parameterised by a @xmath0-dimensional weight vector @xmath1 , and the model prediction is @xmath2 , where @xmath3 is a @xmath0-dimensional feature vector representing an instance .",
    "the objective function for matrix - based mtl can be written as @xmath4 . here",
    "@xmath5 is a loss function of the true label @xmath6 and predicted label @xmath7 .",
    "@xmath8 is the number of tasks , and for the @xmath9-th task there are @xmath10 training instances .",
    "assuming the dimensionality of every task s feature is the same , the models ",
    "@xmath11s  are of the same size",
    ". then the collection of @xmath11s forms a @xmath12 matrix @xmath13 of which the @xmath9-th column is the linear model for the @xmath9-t task . to achieve mtl",
    "we exploit a regulariser @xmath14 that couples the learning problems , typically by encouraging @xmath13 to be a low - rank matrix .",
    "some choices include the @xmath15 norm @xcite , and trace norm @xcite .",
    "an alternative approach @xcite is to explicitly formulate @xmath13 as a low - rank matrix , i.e. , @xmath16 where @xmath17 is a @xmath18 matrix and @xmath19 is a @xmath20 matrix with @xmath21 as a hyper - parameter ( matrix rank ) .    * * tensor - based multi - task learning**in the classic mtl setting , each task is indexed by a single factor .",
    "but in many real - world problems , tasks are indexed by multiple factors .",
    "for example , to build a restaurant recommendation system , we want a regression model that predicts the scores for different aspects ( food quality , environment ) by different customers",
    ". then the task is indexed by aspects @xmath22 customers .",
    "the collection of linear models for all tasks is then a 3-way tensor @xmath23 of size @xmath24 , where @xmath25 and @xmath26 is the number of aspects and customers respectively .",
    "consequently @xmath27 has to be a tensor regulariser @xcite .",
    "for example , sum of the trace norms on all matriciations @xcite , and scaled latent trace norm @xcite .",
    "an alternative solution is to concatenate the one - hot encodings of the two task factors and feed it as input into a two - branch neural network model @xcite . * * multi - task learning for neural networks**with the success of deep learning , many studies have investigated deep multi - task learning .",
    "@xcite use a convolutional neural network to find facial landmarks as well as recognise face attributes ( e.g. , emotions ) .",
    "@xcite propose a neural network for query classification and information retrieval ( ranking for web search ) .",
    "a key commonality of these studies is that they use a predefined sharing strategy .",
    "a typical design is to use the same parameters for the bottom layers of the deep neural network and task - specific parameters for the top layers .",
    "this kind of architecture can be traced back to 2000s @xcite .",
    "however , modern neural network architectures contain a large number of layers , which makes the decision of ` _ _ at which layer to split the neural network for different tasks ? _ _ ' extremely hard .",
    "instead of predefining a parameter sharing strategy , we propose the following framework : for @xmath8 tasks , each is modelled by a neural network of the same architecture .",
    "we collect the parameters in a layer - wise fashion , and put a tensor norm on every collection .",
    "we illustrate the idea by a simple example : assume that we have @xmath28 tasks , and each is modelled by a @xmath29-layer convolution neural network ( cnn ) .",
    "the cnn architecture is : ( 1 ) convolutional layer ( ` conv1 ' ) of size @xmath30 , ( 2 ) ` conv2 ' of size @xmath31 , ( 3 ) fully - connected layer ( ` fc1 ' ) of size @xmath32 , ( 4 ) fully - connected layer ` fc2'@xmath33 of size @xmath34 for the first task and fully - connected layer ( ` fc2'@xmath35 ) of size @xmath36 for the second task .",
    "since the two tasks have different numbers of outputs , the potentially shareable layers are ` conv1 ' , ` conv2 ' , and ` fc1 ' , excluding the final layer of different dimensionality .    for single task learning , the parameters are ` conv1'@xmath33 , ` conv2'@xmath33 , ` fc1'@xmath33 , and ` fc2'@xmath33 for the first task ; ` conv1'@xmath35 , ` conv2'@xmath35 , ` fc1'@xmath35 , and ` fc2'@xmath35 for the second task .",
    "we can see that there is not any parameter sharing between these two tasks .",
    "in one possible predefined deep mtl architecture , the parameters could be ` conv1 ' , ` conv2 ' , ` fc1'@xmath33 , and ` fc2'@xmath33 for the first task ; ` conv1 ' , ` conv2 ' , ` fc1'@xmath35 , and ` fc2'@xmath35 for the second task , i.e. , the first and second layer are fully shared in this case . for our proposed method , the parameter setting is the same as single task learning mode , but we put three tensor norms on the stacked \\{`conv1'@xmath33 , ` conv1'@xmath35 } ( a tensor of size @xmath37 ) , the stacked \\{`conv2'@xmath33 , ` conv2'@xmath35 } ( a tensor of size @xmath38 ) , and the stacked \\{`fc1'@xmath33 , ` fc1'@xmath35 } ( a tensor of size @xmath39 ) respectively .    * * tensor norm**we choose to use the trace norm , the sum of a matrix s singular values @xmath40 .",
    "it has a nice property that it is the tightest convex relation of matrix rank @xcite .",
    "when directly restricting the rank of a matrix is challenging , trace norm serves as a good proxy .",
    "the extension of trace norm from matrix to tensor is not unique , just like tensor rank has multiple definitions .",
    "how to define tensor rank depends on how we assume the tensor is factorised , e.g. , tucker @xcite and tensor - train @xcite decompositions .",
    "we propose three tensor trace norm designs here , which correspond to three variants of the proposed method .    for an @xmath41-way tensor @xmath23 of size @xmath42 .",
    "we define @xmath43 @xmath44 ) , [ d_i ,",
    "\\prod_{j\\neg i } d_j])$ ] is the mode-@xmath9 tensor flattening .",
    "this is the simplest definition .",
    "given that in our framework , the last axis of tensor indexes the tasks , i.e. , @xmath45 , it is the most straightforward way to adapt the technique of matrix - based mtl  reshape the @xmath46 tensor to @xmath47 matrix .",
    "to advance , we define two kinds of tensor trace norm that are closely connected with tucker - rank ( obtained by tucker decomposition ) and tt - rank ( obtained by tensor train decomposition ) . @xmath48}|| _ * \\label{eq : ttmtl}\\end{aligned}\\ ] ]    here @xmath49}$ ] is yet another way to unfold the tensor , which is obtained by @xmath49 } = \\operatorname{reshape}(\\mathcal{w } , [ d_1 d_2 \\dots d_i , d_{i+1 } d_{i+2 } \\dots d_n])$ ] .",
    "it is interesting to note that unlike laf , tucker and tt also encourage within - task parameter sharing , e.g , sharing across filters in a neural network context .    * * optimisation**using gradient - based methods for optimisation involving trace norm is _ not _ a common choice , as there are better solutions based on semi - definite programming or proximal gradients since the trace norm is essentially non - differentiable .",
    "however , deep neural networks are usually trained by gradient descent , and we prefer to keep the standard training process",
    ". therefore we use ( sub-)gradient descent .",
    "the sub - gradient for trace norm can be derived as @xmath50 . a more numerical stable method instead of computing the inverse matrix square root is @xmath51 where @xmath52 and @xmath53 are obtained from svd : @xmath54 @xcite .",
    "our method is implemented in tensorflow @xcite , and released on github .",
    "we experiment on the omniglot dataset @xcite .",
    "omniglot contains handwritten letters in 50 different alphabets ( e.g. , cyrillic , korean , tengwar ) , each with its own number of unique characters ( @xmath55 ) . in total , there are 1623 unique characters , each with 20 instances . each task is a multi - class character recognition problem for the corresponding alphabet .",
    "the images are monochrome of size @xmath56 .",
    "we design a cnn with @xmath57 convolutional and @xmath58 fc layers .",
    "the first conv layer has @xmath59 filters of size @xmath60 ; the second conv layer has @xmath61 filters of size @xmath62 , and the third convolutional layer has @xmath63 filters of size @xmath62 .",
    "each convolutional layer is followed by a @xmath64 max - pooling .",
    "the first fc layer has @xmath65 neurons , and the second fc layer has size corresponding to the number of unique classes in the alphabet .",
    "the activation function is @xmath66 .",
    "we compare the three variants of the proposed framework  laf ( eq .  [ eq : mtxmtl ] ) , tucker ( eq .",
    "[ eq : tuckermtl ] ) , and tt ( eq .  [ eq : ttmtl ] ) with single task learning ( stl ) . for every layer , there are one ( laf ) or more ( tucker and tt ) @xmath67 that control the trade - off between the classification loss ( cross - entropy ) and the trace norm terms , for which we set all @xmath68 .",
    "the experiments are repeated @xmath69 times , and every time @xmath70 training data and @xmath71 testing data are randomly selected .",
    "we plot the change of cross - entropy loss in training set and the values of norm terms with the neural networks parameters updating .",
    "as we can see in fig  [ fig : loss_plot_ce_and_norm ] , stl has the lowest training loss , but worst testing performance , suggesting over - fitting .",
    "our methods alleviate the problem with multi - task regularisation .",
    "we roughly estimate the strength of parameter sharing by calculating @xmath72 , we can see the pattern that with bottom layers share more compared to the top ones .",
    "this reflects the common design intuition that the bottom layers are more data / task independent .",
    "finally , it appears that the choice on laf , tucker , or tt may not be very sensitive as we observe that when optimising one , the loss of the other norms still reduces .",
    "this technique provides a data - driven solution to the branching architecture design problem in deep multi - task learning .",
    "it is a flexible norm regulariser - based alternative to explicit factorisation - based approaches to the same problem @xcite .",
    "martn abadi , ashish agarwal , paul barham , eugene brevdo , zhifeng chen , craig citro , greg  s. corrado , andy davis , jeffrey dean , matthieu devin , sanjay ghemawat , ian goodfellow , andrew harp , geoffrey irving , michael isard , yangqing jia , rafal jozefowicz , lukasz kaiser , manjunath kudlur , josh levenberg , dan man , rajat monga , sherry moore , derek murray , chris olah , mike schuster , jonathon shlens , benoit steiner , ilya sutskever , kunal talwar , paul tucker , vincent vanhoucke , vijay vasudevan , fernanda vigas , oriol vinyals , pete warden , martin wattenberg , martin wicke , yuan yu , and xiaoqiang zheng . :",
    "large - scale machine learning on heterogeneous systems , 2015 .",
    "url http://tensorflow.org/. software available from tensorflow.org .",
    "xiaodong liu , jianfeng gao , xiaodong he , li  deng , kevin duh , and ye - yi wang .",
    "representation learning using multi - task deep neural networks for semantic classification and information retrieval .",
    "_ naacl _ , 2015 .",
    "characterization of the subdifferential of some matrix norms .",
    "_ linear algebra and its applications _ , 170:0 33  45 , 1992 .",
    "issn 0024 - 3795 .",
    "doi : http://dx.doi.org/10.1016/0024-3795(92)90407-2 .",
    "url http://www.sciencedirect.com/science/article/pii/0024379592904072 .",
    "kishan wimalawarne , masashi sugiyama , and ryota tomioka .",
    "multitask learning meets tensor factorization : task imputation via convex optimization . in _ neural information processing systems ( nips ) _ , 2014"
  ],
  "abstract_text": [
    "<S> we propose a framework for training multiple neural networks simultaneously . </S>",
    "<S> the parameters from all models are regularised by the tensor trace norm , so that each neural network is encouraged to reuse others parameters if possible  this is the main motivation behind multi - task learning . </S>",
    "<S> in contrast to many deep multi - task learning models , we do not predefine a parameter sharing strategy by specifying which layers have tied parameters . </S>",
    "<S> instead , our framework considers sharing for all shareable layers , and the sharing strategy is learned in a data - driven way . </S>"
  ]
}