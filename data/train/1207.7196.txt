{
  "article_text": [
    "observations of psychophysical and neurophysiological order have brought into attention the so - called familiarity discrimination or detection task , where tested subjects need only to recognise once - seen objects without being asked to recollect detailed feature or context descriptions @xcite . from the computational perspective",
    ", the essential aim is to devise a neural network model that is biologically plausible up to a certain degree of realism and that is able to explain in part the seemingly limitless memorising ability of the brain to solve this task @xcite .    as in previous familiarity memory neural network modelling efforts @xcite , the formulation of the task that we consider involves a set of @xmath0 patterns @xmath1 that have been presented to the network for learning and that ought to be recognised as familiar in future presentations , while any other pattern not belonging to @xmath2 should be classified as novel . each of the patterns is a binary vector @xmath3 , @xmath4 representing the ( silent - firing ) activity of the @xmath5-th neuron at a given time frame @xmath6 ; the task itself is as well binary , in the sense that we seek to decide if a certain presented pattern @xmath7 is either familiar or novel .",
    "the structure of the network is given at any time by the @xmath8 connectivity matrix @xmath9 , where the entry @xmath10 denotes the strength of the bond from presynaptic neuron @xmath5 to postsynaptic neuron @xmath11 .",
    "to learn the desired mapping , each neuron should be able to determine at the synapse level ( ` locally ' ) the network connectivity structure so that in subsequent pattern presentations one can extract from the collective activity of the @xmath12 neurons the desired novel - familiar response . the model is then characterised by a local synaptic learning rule and by a discrimination function . on the one hand ,",
    "given a pattern @xmath13 that should be memorised , the former determines each synaptic weight solely by inspection of the variables @xmath10 , @xmath14 and @xmath15 ; the latter , given a query pattern @xmath7 and the structure of the network @xmath9 , elicits the binary familiarity response .",
    "we focus on modelling long - term memory , in opposition to palimpsestic working memory @xcite , where ` overwriting ' takes place and the familiarity signal of past memories decays over time . for long - term familiarity detection , a model that is capable of storing an extensive number of patterns per synapse has been proposed @xcite and recently shown to correspond to the optimal linear , local familiarity learning prescription @xcite .",
    "however , the network is only capable of storing a rather small amount of information per synapse , and the proposed synaptic update scheme requires maintenance of real - valued synapses over a long period of time .    in our work , we consider as an alternative the binary non - linear willshaw ( or steinbuch ) prescription @xcite in the context of familiarity discrimination .",
    "this learning rule has certain properties that have made it desirable when applied to the associative memory problem , where it has been extensively analysed ( see , e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "? * ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) ; namely , the high storage capacity attained when the model is correctly parametrised , its simplicity , and the fact that the generated synaptic matrix @xmath9 is binary .",
    "this last feature is particularly interesting since in cortical regions supporting memory - related tasks the synaptic transitions may operate in a discrete ( few steps ) or even in a binary switch - like fashion .",
    "there is accumulating experimental evidence supporting discrete transitions at least in the initial phase of long - term potentiation , although it remains unclear whether or not long - term synaptic efficacies may still have a gradual distribution @xcite .",
    "furthermore , an inhibitory variant of the willshaw rule has just been proposed by @xcite , motivated by the possibility of structural plasticity by synaptic pruning and growth as a support for long - term memory encoding in the adult mammalian brain @xcite , alongside well - established synaptic weight change mechanisms such as long - term potentiation and depression . in the associative case ,",
    "the inhibitory willshaw rule has led to the discovery of new efficient working regimes where few active synapses can carry a high shannon information content .    in this article",
    "we show in a first step that for medium - sized networks the classical pattern and shannon capacities of the willshaw model are comparable to those of the real - valued network of @xcite , provided that the patterns exhibit low activity levels at any time ( the so - called sparse coding regime ) , a fact that has already been pointed out in the dynamical synapse analysis of @xcite . we also show that in the limit of large networks @xmath16 , the network capacity vanishes unless the coding rates are extremely low .    in line with the recent observations of @xcite , we then investigate alternative parametrisations of the willshaw model .",
    "we find that the high pattern loadings associated with the familiarity discrimination task lead to dense potentiation of the memory matrix , a regime where the inhibitory interpretation of the original willshaw model is especially efficient .",
    "it is shown that if the low cost of silent synapses ( which might even be pruned in the long - term ) is neglected , the inhibitory network is capable of achieving large synaptic capacities that increase with the number of neurons , under realistic moderately low coding rates . finally , we take into consideration the effects of varying the coding level per pattern ; at least when the level follows a binomial distribution , introducing a feedforward inhibitory correction in the discriminator compensates for the additional signal variability and the system remains qualitatively intact , albeit operating with lower overall efficiency in the finite - size case .",
    "the simplest possible local , non - linear , binary synaptic rule is the well - known willshaw prescription @xcite . here",
    ", the weight update equation is an extreme case of hebbian learning , where a single coincidental firing activity at any given time @xmath6 ( i.e. , @xmath17 and @xmath18 ) is sufficient to arise long - term potentiation at the synaptic contact @xmath19 . as there is just one potentiation level ,",
    "each synapse @xmath10 is a binary variable , either at the 0-state ( silent synapse ) or at the 1-state ( present synapse ) .",
    "after @xmath0 pattern presentations , @xmath10 is given by @xmath20    originally proposed in the context of an associative network with one - step ( non - iterative ) synchronous retrieval , the 0 - 1 hebb rule has been employed as well to embed patterns in attractor networks with symmetric couplings @xmath21 . in this case , if an appropriate retrieval strategy is used so as to form large basins of attraction surrounding the desired fixed points , iteration generally leads to a more robust recall process , in terms of allowed cue distortion ( given by a metric such as the hamming distance @xmath22 ) as well as in terms of resistance to stochastic synaptic failure , where the @xmath10 may randomly switch states with a certain probability @xcite .    for familiarity discrimination , there is no need per se to extract the whole pattern @xmath13 from the network ; rather , what one seeks is a prescription to determine a binary ( novel - familiar ) answer starting from a cue @xmath7 , given the information stored in the synaptic connectivity matrix @xmath9 .",
    "the discriminator proposed by @xcite and studied in formal memory models of familiarity @xcite , is based on the quadratic form @xmath23 usually referred to as the energy function ) with no self - couplings ( @xmath24 ) there is a strong analogy with the hamiltonian of the zero - temperature ising model @xcite . ] of the network at a given state @xmath25 , presented in its mean corrected form @xcite , where @xmath26 is the coding rate , i.e. , the expected fraction of firing units per pattern . as it has already been pointed out in the previous works , equation [ eq : energy ]",
    "has a network implementation and it is closely related to other measures of familiarity ( see , e.g. , the appendix of * ? ? ?",
    "* ) .    in the proposed discrimination scheme ,",
    "the desired binary decision is computed by ` clamping ' into the network state a certain input pattern @xmath7 and then , without ( or before ) the retrieval dynamics takes place , by thresholding the resulting energy , i.e. @xmath27 } \\in \\{0 , 1\\},\\ ] ] where @xmath28}$ ] is the binary random variable which is 1 if the argument holds and 0 otherwise .",
    "an appropriate choice of @xmath29 and @xmath30 should ensure that , given a weight matrix @xmath9 encoded according to a certain synaptic learning rule , as many as possible patterns belonging to @xmath2 are assigned one of the two decision outcomes ( say , one ) , and all the others to the opposite class ( say , zero ) .",
    "it has been recently shown by @xcite that for such discriminator , the asymptotically optimal ( @xmath31 and a size - dependent load @xmath0 ) local linear synaptic weight setting when we allow the @xmath10 to assume real values is given by the covariance learning rule @xcite : @xmath32    in this article we address the question of how well does the clipped hebbian rule fare with a discriminator of the form .",
    "specifically , for simplicity we redefine @xmath33 letting @xmath34 , performing the double summation over all @xmath35 , and dropping the mean correction , @xmath36 recalling that each weight @xmath10 is now a 0 - 1 binary variable .    following the analysis of the associative willshaw network carried out by @xcite , we proceed by calculating three essential quantities : the maximal number of patterns @xmath37 that the system can discriminate allowing a certain ( known ) error level , the network capacity @xmath38 ( in bits per synaptic contact ) , and the synaptic capacity @xmath39 ( in bits per active synapse ) .",
    "we will then see that the willshaw model becomes especially interesting regarding the latter quantity , as a modification to the clipped rule leads to the activation of a subset of few synapses within the full contact space of order @xmath40 .",
    "the calculation of the maximal pattern load @xmath37 when the average activity is low ( @xmath41 ) can be performed analytically using a series of approximations which have been shown to be near - exact even for finite networks where @xmath12 is not large @xcite .",
    "we consider the two usual simplified binary pattern generation scenarios : first , we deal with the case where every pattern @xmath13 presented to the network for learning has a fixed , known a priori activity level @xmath42 as in the analysis of @xcite ; later ( in section [ sec : binomial ] ) , we consider patterns where @xmath43 is a binomially - distributed random variable with characteristic probability equal to the coding rate @xmath44 , @xmath45 being again a fixed known a priori parameter . in this case , although the activity of each pattern is allowed to vary , by construction the average level is @xmath46 and all neurons are activated equally and independently @xcite .    with these statistics at hand",
    "we can determine the average weight matrix load , @xmath47 the approximation assumes that the coding rates are low , i.e. , @xmath48 .    clearly , as observed when employing the willshaw rule to solve the associative task",
    ", @xmath49 is a critical quantity : to recover information about the patterns in @xmath2 one must control both the cardinality @xmath0 and the sparseness parameter @xmath50 so as to avoid @xmath51 .",
    "it is useful to calculate @xmath0 given @xmath49 , @xmath52    regarding familiarity detection in general , two types of error may occur : omission errors ( denoted as ` 10 ' errors ) whenever @xmath53 but the system fails to classify the pattern as familiar ; conversely , commission errors ( denoted as ` 01 ' errors ) when @xmath54 but the discriminator indicates familiarity .",
    "for patterns with fixed ( for all @xmath6 ) activity @xmath45 and @xmath9 set according to the willshaw rule , there is a simple threshold setting which avoids omission errors at all , i.e. , a @xmath30 such that for all @xmath6 we have with probability one @xmath55 . for a familiar cue @xmath53 corresponding to a certain learned @xmath13 we have @xmath56 where the equality from to is valid since @xmath57 . in a sense",
    ", @xmath58 is the familiarity discrimination threshold which corresponds to the classical willshaw threshold @xmath59 for the noise - free associative task @xcite .",
    "when @xmath58 is the discrimination threshold and @xmath7 is a novel pattern , generated according to the same statistics as the @xmath13 but not presented for learning , if the non - zero @xmath10 coincide with active @xmath35 units enough such that @xmath60 reaches @xmath61 , a commission error will occur .",
    "we can calculate this error probability resorting to @xmath49 ; assuming that the ` ones ' in @xmath9 were randomly and independently set @xcite . ] , @xmath62 where the @xmath63 correction comes from the symmetry in @xmath9 . to reach our final expression , we approximate @xmath64 by the leading term @xmath65 ,",
    "although equation [ eq : p01-k ] would yield a better approximation to the true value of @xmath66 as the learning rule sets the diagonal entries of @xmath9 to one with high probability .",
    "while parametrising a memory device , to ensure the system performs the desired task correctly it is common to require that the probability of error remains below a certain bound . in the associative memory literature",
    "there are many criteria to enforce a quality level in the process ; usually , the task parameters are found so that the error probability grows according to some controlled function of network size and the expected pattern activity level @xcite . in the familiarity detection task , however , as there is no obvious reason to couple the probabilities to the parameters @xmath45 and @xmath12 , it seems reasonable to maintain @xmath66 and @xmath67 below a fixed level @xcite .    to keep the error probability @xmath66 lower than a desired level @xmath68",
    ", we establish the ` breakdown ' value @xmath69 for the pattern load , as a function of the coding rate @xmath50 . using the binomial approximation given by equation [ eq : p01 ] , we have @xmath70 yielding , with respect to @xmath0 , @xmath71 which is the pattern capacity we sought . note that in the large network limit @xmath72 , for any coding rate such that @xmath73",
    ", @xmath69 is independent of the fixed error bound @xmath68 , as we have @xmath74    notice how the maximal pattern load is a function of @xmath45 and @xmath12 .",
    "this result is in contrast with the real - valued network employing the covariance rule , where the familiarity discrimination capacity is essentially independent of the pattern activity level @xcite . just as in the analyses of the willshaw rule for the associative case @xcite ,",
    "however , we find a dependence of @xmath69 on @xmath45 .",
    "with the binary synapses induced by willshaw learning , it is clear that @xmath69 is maximised in the sparse coding regime @xmath41 ; the actual optimal activity level parameter @xmath75 is just a function of @xmath68 and can easily be found numerically . to gain additional insight on the typical size of @xmath75 ,",
    "let us obtain an approximation for the pattern capacity , @xmath76 which is maximal when @xmath77 recalculating @xmath69 with @xmath78 , we find that @xmath79 just to illustrate the result above , if one sets the desired error rate at @xmath80 , the obtained breakdown quantity of patterns per synapse becomes about @xmath81 .",
    "although ` greedily ' maximising @xmath69 leads to an extensive quantity of patterns per synapse , this approach also imposes a heavy coding restriction in the form of quite small values for @xmath45 and an optimising expression that does not vary with @xmath12 , a parametrisation that is referred to by @xcite as the ultra - sparse coding regime . in the next sections we proceed to richer performance measures where the required underlying resources and the shannon information of the task are also taken into account .",
    "the commission error probability @xmath66 can as well be used to calculate the traditional network capacity measure @xmath38 in bits per synaptic contact .",
    "here there is a fundamental difference between the associative and familiarity tasks , as observed by @xcite : a familiarity discrimination network can only ` transmit ' at most one bit per learned pattern ( the perfect output of @xmath82 ) , instead of order @xmath45 bits per pattern as in the associative case @xcite .",
    "the optimal local , linear , additive covariance rule ( that induces real - valued synaptic weights ) can then only obtain @xmath83 bits per synapse in the @xmath84 errorful regime @xcite , which is rather low when compared to the @xmath85 bits per synapse that the same rule can achieve in the high fidelity pattern association task @xcite .",
    "the analogy at hand is to interpret the familiarity network as a discrete binary channel which transmits novel and familiar patterns with a certain error probability , and then calculate the information - theoretic channel capacity , which is the maximal mutual information @xcite normalised by the number of required synaptic contacts , @xmath86 here @xmath87 is a binary random variable indicating whether the @xmath88-th presented pattern is familiar ( @xmath89 ) or novel ( @xmath90 ) , and @xmath91 is the network output for the @xmath88-th pattern . as in previous work @xcite , we assume that @xmath92 patterns are presented and an equal prior probability of a pattern being familiar or novel @xmath93 . besides allowing for a direct fair comparison with the previously obtained results , a prior model with equiprobable pattern classes maximises the channel capacity when the conditional error probabilities are equal @xmath94 . in our case , assuming the network is parametrised for high fidelity , this choice is approximately optimal , as we have @xmath95 and @xmath96 .",
    "since we are ` transmitting ' @xmath0 learned and @xmath0 novel patterns independently generated according to the statistics of section [ sec : pattern - capacity ] , the process can be decomposed into @xmath97 transmissions of a single ( say , the @xmath88-th ) pattern , @xmath98 , \\label{eq : network - capacity-2}\\end{aligned}\\ ] ] where @xmath66 is the commission error probability , defined in as a function of the task parameters @xmath12 , @xmath45 , @xmath0 .",
    "the derivation of the single - pattern mutual information is given in appendix [ apx : mutual - info ] ; a similar calculation has been carried out in the single - neuron information maximisation framework of @xcite , in a comparison of the willshaw rule with more elaborate stochastic synaptic learning .",
    "unfortunately , unlike the network capacity achieved in the associative case , in our task @xmath38 is largest for finite small @xmath12 ( see figure [ fig : c - k - m - large ] ) , but vanishes when @xmath72 , for any activity level function @xmath45 that increases with @xmath12 .     in bits per",
    "synaptic contact vs. network size @xmath12 ( in logarithmic scale ) for a variety of activity level orders , with pattern load @xmath37 given by at conditional error rate @xmath80 . for @xmath45 of order",
    "@xmath99 the capacity is stable , yet slowly decreasing towards zero as predicted by the asymptotic analysis .",
    "less sparse patterns ( e.g. , when @xmath100 ) lead to low capacity even for small @xmath12 . when the activity level increases to @xmath101 the network capacity becomes near - zero for any network size . for @xmath80 , integer - constrained numerical optimisation of @xmath38 with respect to @xmath45 while @xmath0 is accordingly set at @xmath69 reveals that the maximum @xmath102 is achieved when @xmath103 , a result which is in agreement with the previous findings of @xcite.[fig : c - k - m - large ] ]    to show this , let us take an arbitrary , finite probability @xmath68 close to zero , to keep the discrimination error from growing large ; in this case , the bracketed quantity in becomes approximately one .",
    "then , the capacity becomes @xmath104 in the limit @xmath105 , we can take @xmath69 from equation [ eq : meps - limit ] ; the capacity @xmath38 no longer depends on the error bound @xmath68 and is given by @xmath106    we have reached a result which describes a qualitative behaviour that is rather different from the one found in the typical long - term associative memory task , where capacity is clearly a function of network size , and an increasing one when the activity level @xmath45 is of correct order @xcite . for a given fixed probability error @xmath68 ,",
    "the capacity @xmath38 of the willshaw network for discrimination is not directly a function of network size @xmath12 . in our case , for any order of @xmath45 as an increasing function of @xmath12 , in the limit of @xmath72 , the capacity of the system collapses , even if the limit is reached slowly .",
    "one can avoid near - zero capacity for large networks only in the ultra - sparse regime , where @xmath45 is kept small and constant ( e.g. , @xmath103 ) and the capacity remains non - zero ( and independent of @xmath12 ) .",
    "let us consider now the synaptic capacity measure @xmath39 ( in bits per active synapse ) recently suggested by @xcite . here ,",
    "only functional synapses ( i.e. , non - zero synaptic connections @xmath10 which play a role in the network task ) are considered to count ; silent synapses are either assumed to be wired but metabolically cheap to maintain or even that the network is endowed with structural plasticity and is able to prune irrelevant synapses and rewire new connections as needed ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "in the simple pattern statistics we consider , we obtain @xmath39 by renormalising the network capacity @xmath38 ( as given by equation [ eq : network - capacity-2 ] ) by a factor @xmath107 denoting the fraction of functional synapses : @xmath108    in the classical willshaw model , the functional elements correspond to the 1-synapses , the expected fraction of which is @xmath49 ( our @xmath107 , then ) as defined in equation [ eq : p1 ] . however , at the maximal pattern load @xmath69 , even when the discrimination error bound @xmath68 is kept low , most synapses are in the potentiated state .",
    "we can see this by rewriting @xmath49 as a function of @xmath68 ; when @xmath0 is given by @xmath69 , combining equations and , we obtain @xmath109 which approaches unity as we let @xmath110 and is already larger than @xmath63 , even for small @xmath68 close to zero and low activity @xmath45 . once again , in the limit @xmath72 , when @xmath45 is allowed to vary as a function of @xmath12 , we have @xmath111 , which implies a capacity collapse @xmath112 .",
    "the differences between @xmath39 and @xmath38 for finite @xmath12 are also rather small , as illustrated by figure [ fig : p1 ] .     between network and synaptic capacities for the willshaw model , when the error probability bound is @xmath80 , shown for different activity functions @xmath113 .",
    "since the maximal network capacity for each pair @xmath114 is achieved at a higher connectivity level @xmath49 as the coding rate increases , the relative advantage of considering only functional synapses becomes negligible.[fig : p1 ] ]    however , parametrisations leading to the so - called dense potentiation regime @xmath115 ( as @xmath31 ) can be quite advantageous in terms of synaptic capacity when the connectivity matrix @xmath9 is set according to the inhibitory willshaw learning rule . in the associative task ,",
    "this rule is able to achieve a synaptic capacity already an order of magnitude larger than that of the original excitatory model for reasonable pattern activity @xmath45 and plausible network size , and arbitrarily higher values in large networks with appropriate activity levels @xcite .",
    "furthermore , it is one of the limit cases of the optimal non - linear bayesian local synaptic update @xcite .",
    "the inhibitory rule is a subtle variation of equation , as the synaptic states set by the original rule are simply switched : each 0-synapse ( encoding non - coincidental activity ) becomes functional as an inhibitory synapse @xmath116 ; conversely , each @xmath117-synapse becomes silent @xmath118 .",
    "we denote the synaptic connectivity matrix of the inhibitory variant by @xmath119 ; after @xmath0 pattern presentations , the state of synapse @xmath120 is @xmath121 where @xmath10 is the 0 - 1 weight that would be induced by the excitatory rule .",
    "the energy for a familiar cue @xmath53 is now @xmath122 , following the reasoning which led to the derivation of @xmath58 .",
    "novel patterns should activate the inhibitory synapses so that for a given @xmath54 , @xmath123 ; thus , the discrimination function remains unchanged .",
    "the ( classical ) network capacity of the inhibitory network is    notice that the excitatory and inhibitory networks are functionally equivalent and that the ( classical ) network capacities of both implementations are equal , i.e. , @xmath124 .",
    "it is the synaptic capacity @xmath125 of the inhibitory network the fundamental quantity to observe , as it is inversely proportional to the fraction @xmath126 of inhibitory synapses @xmath127 where we have used approximation for @xmath49 .",
    "alternatively , @xmath126 can be obtained as a function of the error probability bound @xmath68 from , @xmath128 expanding the network capacity @xmath38 as in and inserting in the factor @xmath126 we have just derived , we arrive at the synaptic capacity of the inhibitory network as a function of @xmath45 and @xmath68 : @xmath129 which is approximately @xmath130 the approximation improving as @xmath45 increases .",
    "asymptotically , letting @xmath73 , the capacity further simplifies to @xmath131 notice that for large @xmath45 , the @xmath132 factor that was hampering the capacity in the excitatory model has disappeared , both in the finite case and in the large network limit .",
    "( in bits per synapse ) for the inhibitory willshaw rule in the same conditions of figure [ fig : c - k - m - large ] , calculated through normalisation of @xmath38 ( cf . equation [ eq : network - capacity-2 ] ) by @xmath133 . in the moderately - sparse coding regime ( supra - logarithmic @xmath113 ) , which would otherwise lead to quickly vanishing @xmath38 and @xmath39 in the excitatory willshaw model , the inhibitory network is capable of storing more than one bit per functional synapse already at surprisingly small @xmath12 . as discussed in the main text , the synaptic capacity increases with @xmath12 , as long as @xmath45 is as well an increasing function of @xmath12.[fig : cs ] ]    what is remarkable is that as @xmath72 , the synaptic capacity @xmath125 diverges for any @xmath45 that increases with @xmath12 , assuming that the binomial approximative theory we employ remains valid . for finite networks and activity levels of order @xmath134 with @xmath135 ,",
    "@xmath125 already surpasses unity for small- and medium - sized systems ( see figure [ fig : cs ] ) . even for ` classical ' sparseness where @xmath45 is of logarithmic size",
    ", the capacity increases with network size ( recall that @xmath38 was always vanishing for any non - constant @xmath45 ) and is always well above zero .",
    "to picture the difference in capacities , for a network of size @xmath136 , an error rate of @xmath80 and a logarithmic activity level @xmath137 , we obtain the network capacity @xmath138 , while the synaptic capacity is @xmath139 .",
    "if the coding level rises to a more realistic setting such as @xmath140 , the difference becomes drastic , as we have @xmath141 and @xmath142 .",
    "there is a major qualitative change when the excitatory rule is replaced by the inhibitory one .",
    "since @xmath143 as @xmath73 , in the limit of large networks the system is characterised by few synapses carrying a great amount of information . for moderate sparseness where @xmath45 is of the form @xmath134 , @xmath135 , and any setting of @xmath144 , the synaptic capacity is ( asymptotically ) @xmath145 which grows with @xmath12 as fast as the corresponding asymptotic bound for the associative case ( see table 1 , * ? ? ?",
    "* ) , although here the high fidelity requirement enforced through the constant @xmath146 affects more strongly the obtained capacity .",
    "note that the maximal pattern load is still large ; substituting @xmath45 for @xmath134 in equation [ eq : meps ] we find @xmath147 which becomes , in the limit of large networks @xmath72 , @xmath148 when @xmath45 is of order @xmath149 , asymptotically we obtain the pattern capacity @xmath150 , which is still supralinear in @xmath12 , while the number of required functional synapses",
    "@xmath126 tends to zero .    in summary , considering that only functional synapses are relevant for the capacity measure , the willshaw - type inhibitory learning rule leads to efficient familiarity discrimination in the limit of synaptic precision ( two - state synapses ) .",
    "interestingly , as in the pattern association task @xcite , the network achieves high storage capacities for coding rates of the form @xmath151 , @xmath152 , which for most cortical regions are ( arguably ) more realistic than the logarithmic levels required by the excitatory rule .",
    "if one accepts the logarithmic coding requirement , then the inhibitory model offers a pattern load that grows as @xmath153 ( see equation [ eq : meps - limit ] ) , still achieving capacities around one bit per synapse while maintaining high fidelity in the discriminator output and low anatomical connectivity .      to reach the former results we have assumed that the activity level per pattern was fixed at exactly @xmath45 firing neurons , at",
    "any given time , i.e. , @xmath154 was kept constant across all @xmath6 .",
    "thus , all patterns were permutations of each other chosen from the @xmath155 possible configurations as in the analysis of @xcite",
    ". however , from the biological modelling perspective it might be more reasonable to take the assembly size as a random variable . in this section",
    "we let @xmath156 and @xmath157 assume a binomial distribution with characteristic probability @xmath158 , so that the mean activity level is still @xmath159 , but the activity levels are allowed to vary .    in this case , the treatment is harder since we have to replace the constant parameter @xmath45 in the capacity analyses by a random variable .",
    "we denote by a star superscript ` @xmath160 ' whenever appropriate to differentiate quantities where @xmath157 and @xmath156 are random variables .",
    "first , since the patterns have varying activity levels , to recover the ` no - omission - errors ' property @xmath95 , we adjust the discrimination threshold for the excitatory network accordingly on a cue - by - cue basis , @xmath161 denoting the binomially - distributed pattern activity level by random variable @xmath162 .",
    "the variable threshold could be implemented , alternatively , introducing an external feedforward inhibition field in the energy read - out , corresponding to a translation in the energy function , @xmath163 implying @xmath164 for familiar @xmath53 , as in the inhibitory willshaw network implementation .    when the weights are set according to the inhibitory rule , there is no need for the explicit external field , as the energy reads immediately @xmath165 and the threshold can be simply set fixed @xmath166 as before . for the excitatory network , however , the variable threshold control is fundamental to stabilise the energy , as can be seen for instance through inspection of the variances of non - translated vs. translated energies ( not shown here ) .    in the following , @xmath167 is the probability mass function of the binomial distribution .",
    "we first approximate the conditional error probability by @xmath168 which is the expression found by @xcite for the associative task under the same statistical assumptions , now adjusted to the quadratic familiarity discriminator ; the full analysis of the distribution is due to @xcite .",
    "notice that equation [ eq : p01-star - full ] is just an approximation , as the analyses of the associative case assume independence among the columns of @xmath9 . to compute the exact conditional error probability of the quadratic discriminator , however , would require analysing a @xmath169 sub - matrix of @xmath9 , which is a difficult combinatorial problem we do not solve .",
    "approximating the exponent and employing the binomial approximation , as in , we obtain @xmath170 @xmath49 being the expected matrix load as given by .",
    "notice that in general , as expected and as in the case of the covariance rule @xcite , the error probability is never smaller than when the activity level is kept constant .     between the obtained synaptic capacities ( calculated through normalisation by @xmath126 of the network capacity of equation [ eq : network - capacity-2 ] ) in the binomial- and fixed - activity pattern generation scenarios .",
    "connecting ( interpolating ) lines are visual aids ; solid markers represent the ratio of capacities computed for actual measured @xmath171 ( binomially - distributed @xmath162 ) vs. theoretical maximal @xmath69 ( fixed @xmath172 ) as given by .",
    "the pattern load @xmath171 was found numerically by bisecting search over equation [ eq : p01-star ] with the target @xmath173 set at @xmath80 .",
    "the relative difference between @xmath174 and @xmath38 fades as @xmath12 grows and when the expected activity level order @xmath113 increases.[fig : cbin ] ]    it is hard to obtain the pattern load @xmath171 as a function of @xmath173 without writing the summation in in closed - form , which is difficult to accomplish due to the quadratic exponent .",
    "however , we can find numerically the @xmath171 such that the commission error probability @xmath173 is approximately equal to some arbitrary bound close to zero ( say , @xmath80 ) , from which we compute the corresponding synaptic capacity @xmath175 .",
    "then , to assess the impact of letting @xmath45 vary , we can see how the ratio @xmath176 evolves as @xmath12 grows , for different mean activity levels .",
    "as plotted in figure [ fig : cbin ] , @xmath177 approaches unity as the network size parameter @xmath12 increases , and quickly so when the patterns are moderately sparse ( @xmath178 ) . for small , finite @xmath12",
    "there is a rather large factor affecting @xmath171 that originates in the disorder introduced by the variability in the activity levels .",
    "this factor can be ( approximately ) as large as @xmath179 for @xmath45 of logarithmic size but attenuates as @xmath12 grows .",
    "our numerical analysis strongly suggests then that the system remains qualitatively intact and the former conclusions drawn for fixed @xmath45 should hold , even for finite networks , although the discriminator is subject to a correcting factor which decreases the capacity of the model .",
    "if one restricts the model to operate with two - state synapses , a well - known and simple local update scheme can offer a surprising familiarity discrimination capacity , provided that the firing rates are kept low .",
    "we have analysed both the original willshaw rule @xcite and a variation for inhibitory synapses recently proposed by @xcite .    at high pattern loads ,",
    "the traditional excitatory implementation imposes high connectivity and a heavy coding restriction ; we have seen that for large enough networks the network capacity eventually approaches zero unless the activity levels are kept constant ( independent of network size ) and very low at all times . for neural populations of moderate size and low activity levels ( e.g. , of logarithmic order )",
    ", one can obtain in the high - fidelity regime information and pattern capacities that are comparable to those found for the optimal linear rule . in this case",
    ", we find a rather low overall stored information content per synapse in comparison to the typical values achieved in the associative memory task , a fact that has already been discussed by @xcite .",
    "taking into consideration that in the long - term the brain might prune silent synapses ( that play a non - functional role and are mere spatial candidates for future potentiation ) in stable memories and then place synapses in new locations as needed , @xcite suggested the so - called synaptic capacity measure where only functional resources are taken into account .",
    "the critical observation we reach in our work is that the familiarity detection task parametrisation leads naturally to the dense potentiation regime , even for logarithmic sparse coding , which explains the large capacities achieved by the inhibitory willshaw rule . in this case , we recover the increasing capacity function ( with respect to network size ) that is typical of the associative task .",
    "of course , another question altogether is to locate such structures in the actual central nervous system , and to ascertain if the less conservative inhibitory rule ( where connections corresponding to previous coincidental activity are depressed and then pruned ) is plausible and if it is actually observed in real synapses .",
    "it is worth noting that we have switched to an inhibitory circuit so that the energy ` readout ' mechanism could remain intact , except for a change in the threshold .",
    "however , one could consider a sign - reversed connectivity matrix , i.e. , an excitatory network implementation with exactly the same couplings as the inhibitory one . in this case , the less well - known inhibitory synaptic plasticity processes would be avoided , but the task would change , as a stronger excitatory signal would be elicited in the presence of novel patterns .",
    "such a model could be appropriate to describe a novelty detection mechanism in regions where stronger excitatory activity is observed as a response to non - familiar stimuli .",
    "our analysis should hold , as only the number ( and not the type ) of required functional synapses matters for the synaptic capacity measure we have considered .    following the previous studies of familiarity detection , our analysis has focused on simple high - level modelling assumptions that could be refined if the biological implications require so . for instance",
    ", one could consider incorporating well - known features of more realistic or detailed models , such as stochastic synaptic transmission , arbitrary query noise , or spiking neurons .",
    "we would like to thank the two anonymous reviewers for detailed and thorough comments , as well as francisco burnay , ngelo cardoso , francisco s. melo and diogo rendeiro for carefully reading a preliminary version of the manuscript .",
    "this research was supported by the portuguese _ fundao para a cincia e tecnologia _ ( inesc - id multiannual funding ) through the piddac program funds and a doctoral grant awarded to the first author ( contract sfrh / bd/66398/2009 ) .",
    "for a given pattern transmission described by the true class ( novel - familiar ) of the pattern @xmath180 and the network output @xmath181 , we can define the mutual information @xmath182 in terms of the discriminator entropy @xmath183 and the conditional entropy @xmath184 of the discrimination outcome given the correct classification , @xmath185    let us denote by @xmath186 the shannon entropy in bits of a binary random variable @xmath187 with @xmath188 and @xmath189 .",
    "then , we can write the entropies in with respect to the prior probability @xmath190 and the error probabilities @xmath67 and @xmath66 @xcite , leading to @xmath191 and @xmath192 recalling that @xmath95 under the threshold setting .    inserting the expanded entropies into expression [ eq : single - mutual - info ] , and substituting @xmath193 ( the probability of a pattern being familiar )",
    ", we obtain @xmath194 which is the expression presented in the main text ( equation [ eq : network - capacity-2 ] ) .",
    "palm g , sommer ft ( 1996 ) associative data storage and retrieval in neural networks . in : domany e ,",
    "van hemmen jl , schulten k ( eds ) models of neural networks iii : association , generalization , and representation ( physics of neural networks ) , springer - verlag , new york , pp 79118"
  ],
  "abstract_text": [
    "<S> in this work we investigate from a computational perspective the efficiency of the willshaw synaptic update rule in the context of familiarity discrimination , a binary - answer , memory - related task that has been linked through psychophysical experiments with modified neural activity patterns in the prefrontal and perirhinal cortex regions . </S>",
    "<S> our motivation for recovering this well - known learning prescription is two - fold : first , the switch - like nature of the induced synaptic bonds , as there is evidence that biological synaptic transitions might occur in a discrete stepwise fashion . </S>",
    "<S> second , the possibility that in the mammalian brain , unused , silent synapses might be pruned in the long - term . besides the usual pattern and network capacities , we calculate the synaptic capacity of the model , a recently proposed measure where only the functional subset of synapses is taken into account . we find that in terms of network capacity , willshaw learning is strongly affected by the pattern coding rates , which have to be kept fixed and very low at any time to achieve a non - zero capacity in the large network limit . </S>",
    "<S> the information carried per functional synapse , however , diverges and is comparable to that of the pattern association case , even for more realistic moderately low activity levels that are a function of network size .    </S>",
    "<S> * keywords : * familiarity memory , willshaw rule , synaptic capacity , sparse coding    = 1 </S>"
  ]
}