{
  "article_text": [
    "it is widely recognized that modern statistical problems are increasingly high - dimensional , i.e. require estimation of more parameters than the number of observations / samples .",
    "examples abound from signal processing @xcite , to genomics @xcite , collaborative filtering @xcite and so on .",
    "a number of successful estimation techniques have been developed over the last ten years to tackle these problems .",
    "a widely applicable approach consists in optimizing a suitably regularized likelihood function .",
    "such estimators are , by necessity , non - linear and non - explicit ( they are solution of certain optimization problems ) .",
    "the use of non - linear parameter estimators comes at a price . in general",
    ", it is impossible to characterize the distribution of the estimator .",
    "this situation is very different from the one of classical statistics in which either exact characterizations are available , or asymptotically exact ones can be derived from large sample theory @xcite .",
    "this has an important and very concrete consequence . in classical statistics , generic and",
    "well accepted procedures are available for characterizing the uncertainty associated to a certain parameter estimate in terms of confidence intervals or @xmath0-values @xcite .",
    "however , no analogous procedures exist in high - dimensional statistics .    in this paper we develop a computationally efficient procedure for constructing confidence intervals and @xmath0-values for a broad class of high - dimensional regression problems .",
    "the salient features of our procedure are :    1 .",
    "our approach guarantees nearly optimal confidence interval sizes and testing power .",
    "it is the first one to achieve this goal under essentially no assumptions beyond the standard conditions for high - dimensional consistency .",
    "3 .   it allows for a streamlined analysis with respect to earlier work in the same area .    for the sake of clarity",
    ", we will focus our presentation on the case of linear regression , under gaussian noise .",
    "section [ sec : nongaussian ] provides a detailed study of the case of non - gaussian noise .",
    "a preliminary report on our results was presented in nips 2013 @xcite , which also discusses generalizations of the same approach to generalized linear models , and regularized maximum likelihood estimation .    in a linear regression model ,",
    "we are given @xmath1 i.i.d .",
    "pairs @xmath2 , with vectors @xmath3 and response variables @xmath4 given by @xmath5 here @xmath6 and @xmath7 is the standard scalar product in @xmath8 . in matrix form , letting @xmath9 and denoting by @xmath10 the design matrix with rows @xmath11 , we have @xmath12 the goal is to estimate the unknown ( but fixed ) vector of parameters @xmath6 .    in the classic setting , @xmath13 and the estimation method of choice is ordinary least squares yielding @xmath14 . in particular",
    "@xmath15 is gaussian with mean @xmath16 and covariance @xmath17 .",
    "this directly allows to construct confidence intervals , @xmath18 $ ] is a @xmath19 confidence interval @xcite . ] .",
    "in the high - dimensional setting where @xmath20 , the matrix @xmath21 is rank deficient and one has to resort to biased estimators .",
    "a particularly successful approach is the lasso  @xcite which promotes sparse reconstructions through an @xmath22 penalty : @xmath23 in case the right hand side has more than one minimizer , one of them can be selected arbitrarily for our purposes .",
    "we will often omit the arguments @xmath24 , @xmath10 , as they are clear from the context .",
    "we denote by @xmath25 the support of @xmath6 , defined as @xmath26:\\ , \\theta_{0,i } \\neq 0\\}\\,,\\ ] ] where we use the notation @xmath27 = \\{1,\\dotsc , p\\}$ ] .",
    "we further let @xmath28 .",
    "a copious theoretical literature @xcite shows that , under suitable assumptions on @xmath10 , the lasso is nearly as accurate as if the support @xmath29 was known _ a priori_. namely , for @xmath30 , we have @xmath31",
    ".    measurement vector @xmath32 , design matrix @xmath10 , parameters @xmath33 , @xmath34 .",
    "unbiased estimator @xmath35 .",
    "let @xmath36 be the lasso estimator as per eq .  .",
    "set @xmath37 .",
    "let @xmath38 be a solution of the convex program : @xmath39 where @xmath40 is the vector with one at the @xmath41-th position and zero everywhere else .",
    "set @xmath42 .",
    "if any of the above problems is not feasible , then set @xmath43 .",
    "define the estimator @xmath35 as follows : @xmath44    as mentioned above , these remarkable properties come at a price . deriving an exact characterization for the distribution of @xmath45 is not tractable in general , and",
    "hence there is no simple procedure to construct confidence intervals and @xmath0-values .",
    "a closely related property is that @xmath45 is biased , an unavoidable property in high dimension , since a point estimate @xmath46 must be produced from data in lower dimension @xmath47 , @xmath48 .",
    "we refer to section [ sec : biasdiscussion ] for further discussion of this point .    in order to overcome this challenge",
    ", we construct a de - biased estimator from the lasso solution .",
    "the de - biased estimator is given by the simple formula @xmath49 , as in eq .",
    "( [ eq : hthetau ] ) .",
    "the basic intuition is that @xmath50 is a subgradient of the @xmath22 norm at the lasso solution @xmath45 . by adding a term proportional to this subgradient",
    ", our procedure compensates the bias introduced by the @xmath22 penalty in the lasso .",
    "we will prove in section [ sec : debiased ] that @xmath35 is approximately gaussian , with mean @xmath16 and covariance @xmath51 , where @xmath52 is the empirical covariance of the feature vectors .",
    "this result allows to construct confidence intervals and @xmath0-values in complete analogy with classical statistics procedures .",
    "for instance , letting @xmath53 , @xmath54 $ ] is a @xmath19 confidence interval .",
    "the size of this interval is of order @xmath55 , which is the optimal ( minimum ) one , i.e. the same that would have been obtained by knowing _ a priori _ the support of @xmath16 . in practice",
    "the noise standard deviation is not known , but @xmath56 can be replaced by any consistent estimator @xmath57 ( see section [ sec : inference ] for more details on this ) .",
    "a key role is played by the matrix @xmath58 whose function is to ` decorrelate ' the columns of @xmath10 .",
    "we propose here to construct @xmath59 by solving a convex program that aims at optimizing two objectives .",
    "one one hand , we try to control @xmath60 ( here and below @xmath61 denotes the entrywise @xmath62 norm ) which as shown in theorem [ thm : main_thm] controls the non - gaussianity and bias of @xmath35 . on the other , we minimize @xmath63_{i , i}$ ] , for each @xmath64 $ ] , which controls the variance of @xmath65 .",
    "the idea of constructing a de - biased estimator of the form @xmath49 was used by the present authors in @xcite , that suggested the choice @xmath66 , with @xmath67 the population covariance matrix and @xmath68 a positive constant .",
    "a simple estimator for @xmath69 was proposed for sparse covariances , but asymptotic validity and optimality were proven only for uncorrelated gaussian designs ( i.e. gaussian @xmath10 with @xmath70 ) .",
    "van de geer , blhmann , ritov and dezeure @xcite used the same construction with @xmath59 an estimate of @xmath71 which is appropriate for sparse inverse covariances .",
    "these authors prove semi - parametric optimality in a non - asymptotic setting , provided the sample size is at least @xmath72 .    from a technical point of view , our proof starts from a simple decomposition of the de - biased estimator @xmath35 into a gaussian part and an error term , already used in @xcite .",
    "however departing radically from earlier work we realize that @xmath59 need not be a good estimator of @xmath71 in order for the de - biasing procedure to work .",
    "we instead set @xmath59 as to minimize the error term and the variance of the gaussian term .",
    "as a consequence of this choice , our approach applies to general covariance structures @xmath69 .",
    "by contrast , earlier approaches applied only to sparse @xmath69 , as in @xcite , or sparse @xmath71 as in @xcite .",
    "the only assumptions we make on @xmath69 are the standard compatibility conditions required for high - dimensional consistency @xcite .",
    "a detailed comparison of our results with the ones of @xcite can be found in section [ sec : gbr - comparison ] .",
    "our presentation is organized as follows .",
    "section [ sec : debiasedgen ] : :    considers a general debiased estimator of the form    @xmath73 .",
    "we introduce a figure of merit of the pair    @xmath74 , termed the generalized coherence parameter    @xmath75 .",
    "we show that , if the generalized    coherence is small , then the debiasing procedure is effective ( for a    given deterministic design ) , see theorem [ thm : deterministic ] .",
    "+    we then turn to random designs , and show that the generalized    coherence parameter can be made as small as    @xmath76 , though a convex optimization    procedure for computing @xmath59 .",
    "this results in a bound on    the bias of @xmath35 , cf .",
    "theorem [ thm : main_thm ] : the    largest entry of the bias is of order @xmath77 .",
    "this must be compared with the standard deviation of    @xmath78 , which is of order    @xmath55 .",
    "the conclusion is that , for    @xmath79 , the bias of    @xmath35 is negligible .",
    "section [ sec : inference ] : :    applies these distributional results to deriving confidence intervals    and hypothesis testing procedures for low - dimensional marginals of    @xmath80 .",
    "the basic intuition is that    @xmath35 is approximately gaussian with mean    @xmath16 , and known covariance structure .",
    "hence standard    optimal tests can be applied .",
    "+    we prove a general lower bound on the power of our testing procedure ,    in theorem [ thm : error - power ] .",
    "in the special case of gaussian random    designs with i.i.d .",
    "rows , we can compare this with the upper bound    proved in @xcite , cf .",
    "theorem [ thm : generalupperbound ] . as a    consequence ,",
    "the asymptotic efficiency of our approach is    constant - optimal .",
    "namely , it is lower bounded by a constant    @xmath81 which is bounded away from    @xmath82 , cf .",
    "theorem [ thm : optimality ] .",
    "( for instance    @xmath83 , and @xmath84 is    always upper bounded by the condition number of @xmath69 . ) section [ sec : nongaussian ] : :    uses the a central limit theorem for triangular arrays to generalize    the above results to non - gaussian noise .",
    "section [ sec : simulation ] : :    illustrates the above results through numerical simulations both on    synthetic and on real data .",
    "note that our proofs require stricter sparsity @xmath85 ( or larger sample size @xmath1 ) than required for consistent estimation .",
    "we assume @xmath86 instead of @xmath87 @xcite .",
    "the same assumption is made in @xcite , on top of additional assumptions on the sparsity of @xmath71 .",
    "it is currently an open question whether successful hypothesis testing can be performed under the weaker assumption @xmath88 .",
    "we refer to @xcite for preliminary work in that direction .",
    "the barrier at @xmath89 is possibly related to an analogous assumption that arises in gaussian graphical models selection @xcite .      the theoretical literature on high - dimensional statistical models is vast and rapidly growing . estimating sparse linear regression models is the most studied problem in this area , and a source of many fruitful ideas . limiting ourselves to linear regression , earlier work investigated prediction error @xcite , model selection properties @xcite , @xmath90 consistency @xcite . . of necessity",
    ", we do not provide a complete set of references , and instead refer the reader to @xcite for an in - depth introduction to this area .",
    "the problem of quantifying statistical significance in high - dimensional parameter estimation is , by comparison , far less understood .",
    "zhang and zhang @xcite , and bhlmann @xcite proposed hypothesis testing procedures under restricted eigenvalue or compatibility conditions @xcite .",
    "these papers provide deterministic guarantees but in order to achieve a certain target significance level @xmath91 and power @xmath92 they require @xmath93 .",
    "the best lower bound @xcite shows that any such test requires instead @xmath94 .",
    "( the lower bound of @xcite is reproduced as theorem [ thm : generalupperbound ] here , for the reader s convenience . )    in other words , the guarantees of @xcite can be suboptimal by a factor as large as @xmath95 .",
    "equivalently , in order for the coefficient @xmath96 to be detectable with appreciable probability , it needs to be larger than the overall @xmath90 error . here",
    "we will propose a test that for random designs achieves significance level @xmath91 and power @xmath92 for @xmath97 .",
    "lockhart et al .",
    "@xcite develop a test for the hypothesis that a newly added coefficient along the lasso regularization path is irrelevant .",
    "this however does not allow to test arbitrary coefficients at a given value of @xmath33 , which is instead the problem addressed in this paper .",
    "these authors further assume that the current lasso support contains the actual support @xmath98 and that the latter has bounded size .",
    "belloni , chernozhukov and collaborators  @xcite consider inference in a regression model with high - dimensional data . in this model",
    "the response variable relates to a scalar main regressor and a @xmath0-dimensional control vector .",
    "the main regressor is of primary interest and the control vector is treated as nuisance component . assuming that the control vector is @xmath85-sparse , the authors propose a method to construct confidence regions for the parameter of interest under the sample size requirement @xmath99 .",
    "the proposed method is shown to attain the semi - parametric efficiency bounds for this class of models .",
    "the key modeling assumption in this paper is that the scalar regressor of interest is random , and depends linearly on the @xmath0-dimensional control vector , with a sparse coefficient vector ( with sparsity again of order @xmath100 .",
    "this assumption is closely related to the sparse inverse covariance assumption of @xcite ( with the difference that only one regressor is tested ) .    finally , resampling methods for hypothesis testing were studied in @xcite .",
    "these methods are perturbation - based procedures to approximate the distribution of a general class of penalized parameter estimates for the case @xmath101 .",
    "the idea is to consider the minimizer of a stochastically perturbed version of the regularized objective function , call it @xmath102 , and characterize the limiting distribution of the regularized estimator @xmath103 in terms of the distribution of @xmath102 . in order to estimate the latter ,",
    "a large number of random samples of the perturbed objective function are generated , and for each sample the minimizer is computed .",
    "finally the theoretical distribution of @xmath102 is approximated by the empirical distribution of these minimizers .",
    "after the present paper was submitted for publication , we became aware that bhlmann and dezeure @xcite had independently worked on similar ideas .",
    "in this section we introduce some basic definitions used throughout the paper , starting with simple notations .    for a matrix @xmath104 and set of indices @xmath105 , we let @xmath106 denote the submatrix formed by the rows in @xmath107 and columns in @xmath108 .",
    "also , @xmath109 ( resp .",
    "@xmath110 ) denotes the submatrix containing just the rows ( reps .",
    "columns ) in @xmath107 . likewise , for a vector @xmath111",
    ", @xmath112 is the restriction of @xmath111 to indices in @xmath107 .",
    "we use the shorthand @xmath113 .",
    "in particular , @xmath114 . the maximum and the minimum singular values of @xmath104",
    "are respectively denoted by @xmath115 and @xmath116 .",
    "we write @xmath117 for the standard @xmath118 norm of a vector @xmath111 , i.e. , @xmath119 . and @xmath120 for the number of nonzero entries of @xmath111 . for a matrix @xmath104",
    ", @xmath121 is the @xmath118 operator norm , and @xmath122 is the elementwise @xmath118 norm . for a vector @xmath111 , @xmath123 represents the positions of nonzero entries of @xmath111 . throughout",
    ", @xmath124 denotes the cdf of the standard normal distribution . finally , _ with high probability _",
    "( w.h.p ) means with probability converging to one as @xmath125 .",
    "we let @xmath126 be the sample covariance matrix . for @xmath127",
    ", @xmath128 is always singular .",
    "however , we may require @xmath128 to be nonsingular for a restricted set of directions .    given a symmetric matrix @xmath129 and a set @xmath130 $ ] , the corresponding _ compatibility constant _ is defined as @xmath131 we say that @xmath129 satisfies the _ compatibility condition _ for the set @xmath130 $ ] , with constant @xmath132 if @xmath133 .",
    "we say that it holds for the design matrix @xmath10 , if it holds for @xmath134 .    in the following , we shall drop the argument @xmath128 if clear from the context .",
    "note that a slightly more general definition is used normally ( * ? ? ?",
    "* section 6.13 ) , whereby the condition @xmath135 , is replaced by @xmath136 .",
    "the resulting constant @xmath137 depends on @xmath138 . for the sake of simplicity ,",
    "we restrict ourselves to the case @xmath139 .",
    "the _ sub - gaussian norm _ of a random variable @xmath140 , denoted by @xmath141 , is defined as @xmath142 for a random vector @xmath143 , its sub - gaussian norm is defined as @xmath144 , where @xmath145 denotes the unit sphere in @xmath146 .    the _ sub - exponential norm _ of a random variable @xmath140 , denoted by @xmath147 , is defined as @xmath148 for a random vector @xmath143 , its sub - exponential norm is defined as @xmath149 , where @xmath145 denotes the unit sphere in @xmath146 .",
    "in this section we present our characterization of the de - biased estimator @xmath35 ( subsection [ sec : debiased ] ) .",
    "this characterization also clarifies in what sense the lasso estimator is biased .",
    "we discuss this point in subsection [ sec : biasdiscussion ] .",
    "as emphasized above , our approach is based on a de - biased estimator defined in eq .  , and on its distributional properties .",
    "in order to clarify the latter , it is convenient to begin with a slightly broader setting and consider a general debiasing procedure that makes use of a an arbitrary @xmath150 .",
    "namely , we define @xmath151 for notational simplicity , we shall omit the arguments @xmath152 unless they are required for clarity .",
    "the quality of this debiasing procedure depends of course on the choice of @xmath59 , as well as on the design @xmath10 .",
    "we characterize the pair @xmath153 by the following figure of merit .",
    "[ def : coherence ] given the pair @xmath154 and @xmath155 , let @xmath134 denote the associated sample covariance .",
    "then , the _ generalized coherence parameter of @xmath156 _ , denoted by @xmath75 , is @xmath157 the _ minimum ( generalized ) coherence _ of @xmath10 is @xmath158 .",
    "we denote by @xmath159 any minimizer of @xmath75 .",
    "note that the minimum coherence can be computed efficiently since @xmath160 is a convex function ( even more , the optimization problem is a linear program ) .",
    "the motivation for our terminology can be grasped by considering the following special case .",
    "assume that the columns of @xmath10 are normalized to have @xmath90 norm equal to @xmath161 ( i.e. @xmath162 for all @xmath64 $ ] ) , and @xmath163 .",
    "then @xmath164 , and the maximum @xmath165 .",
    "in other words @xmath166 is the maximum normalized scalar product between distinct columns of @xmath10 : @xmath167    the quantity ( [ eq : stdcoherence ] ) is known as the _ coherence parameter _ of the matrix @xmath168 and was first defined in the context of approximation theory by mallat and zhang @xcite , and by donoho and huo @xcite .",
    "assuming , for the sake of simplicity , that the columns of @xmath10 are normalized so that @xmath162 , a small value of the coherence parameter @xmath169 means that the columns of @xmath10 are roughly orthogonal .",
    "we emphasize however that @xmath75 can be much smaller than its classical coherence parameter @xmath169 .",
    "for instance , @xmath170 if and only if @xmath168 is an orthogonal matrix . on the other hand ,",
    "@xmath171 if and only if @xmath10 has rank .",
    "it is the simplest example that illustrates the difference between coherence and generalized coherence , and it is not hard to find related examples with @xmath48 . ] @xmath0 .",
    "the following theorem is a slight generalization of a result of @xcite .",
    "let us emphasize that it applies to deterministic design matrices @xmath10 .",
    "[ thm : deterministic ] let @xmath172 be any ( deterministic ) design matrix , and @xmath173 be a general debiased estimator as per eq .",
    "( [ eq : generaldebiased ] ) . then , setting @xmath174 , we have @xmath175 further , assume that @xmath10 satisfies the compatibility condition for the set @xmath176 , @xmath177 , with constant @xmath132 , and has generalized coherence parameter @xmath178 , and let @xmath179 } ( \\bx^{\\st}\\bx / n)_{ii}$ ] .",
    "then , letting @xmath180 , we have @xmath181 further , if @xmath182 minimizes the convex cost function @xmath183 , then @xmath184 can be replaced by @xmath185 in eq .",
    "( [ eq : firstbounddelta ] ) .",
    "the above theorem decomposes the estimation error @xmath186 into a zero mean gaussian term @xmath187 and a bias term @xmath188 whose maximum entry is bounded as per eq .",
    "( [ eq : firstbounddelta ] ) .",
    "this estimate on @xmath189 depends on the design matrix through two constants : the compatibility constant @xmath132 and the generalized coherence parameter @xmath190 .",
    "the former is a well studied property of the design matrix @xcite , and assuming @xmath132 of order one is nearly necessary for the lasso to achieve optimal estimation rate in high dimension . on the contrary , the definition of @xmath190 is a new contribution of the present paper .",
    "the next theorem establishes that , for a natural probabilistic model of the design matrix @xmath10 , both @xmath132 and @xmath190 can be bounded with probability converging rapidly to one as @xmath191 .",
    "further , the bound on @xmath192 hold for the special choice of @xmath59 that is constructed by algorithm 1 .    [ thm : event_thm ]",
    "let @xmath193 be such that @xmath194 , and @xmath195 , and @xmath196}\\sigma_{ii}\\le 1 $ ] .",
    "assume @xmath197 to have independent subgaussian rows , with zero mean and subgaussian norm @xmath198 , for some constant @xmath199 .    1 .   for @xmath200 , let @xmath201 be the event that the compatibility condition holds for @xmath202 , for all sets @xmath130 $ ] , @xmath177 with constant @xmath203 , and that @xmath196}\\ , \\hsigma_{i , i } \\le k$ ] .",
    "explicitly @xmath204}\\ , \\hsigma_{i , i } \\le k , \\;\\ ; \\hsigma = ( \\bx^{\\st}\\bx / n)\\big\\}\\ , . \\ ] ] then there exists @xmath205 such that the following happens . if @xmath206 , @xmath207 , @xmath208 , and @xmath209 , then @xmath210 2 .",
    "for @xmath211 , @xmath212 be the event that the problem ( [ eq : optimization ] ) is feasible for @xmath213 , or equivalently @xmath214 then , for @xmath215 @xmath216    the proof of this theorem is given in section  [ proof : thm_eventa ] ( for part @xmath217 ) and section [ proof : thm_eventb ] ( part @xmath218 ) .    the proof that event @xmath219 holds with high probability relies crucially on a theorem by rudelson and zhou ( * ? ? ? * theorem 6 ) .",
    "simplifying somewhat , the latter states that , if the restricted eigenvalue condition of @xcite holds for the population covariance @xmath69 , then it holds with high probability for the sample covariance @xmath128 .",
    "( recall that the restricted eigenvalue condition is implied by a lower bound on the minimum singular value can be further weakened . ] , and that it implies the compatibility condition @xcite . )    finally , by putting together theorem [ thm : deterministic ] and theorem [ thm : event_thm ] , we obtain the following conclusion .",
    "[ thm : main_thm ] consider the linear model   and let @xmath35 be defined as per eq .   in algorithm 1 , with @xmath220 .",
    "then , setting @xmath174 , we have @xmath221 further , under the assumptions of theorem [ thm : event_thm ] , and for @xmath222 , @xmath223 , and @xmath180 , we have @xmath224 where @xmath225 and @xmath226 are given by eqs .",
    "( [ eq : boundeva ] ) and ( [ eq : boundevg ] ) .",
    "finally , the tail bound ( [ eq : tailboundquant ] ) holds for any choice of @xmath59 that is only function of the design matrix @xmath10 , and satisfies the feasibility condition in eq .",
    "( [ eq : optimization ] ) , i.e. @xmath227 .",
    "assuming @xmath228 of order one , the last theorem establishes that , for random designs , the maximum size of the ` bias term ' @xmath229 over @xmath230 $ ] is : @xmath231 on the other hand , the ` noise term ' @xmath232 is roughly of order @xmath233_{ii}}$ ] .",
    "bounds on the variances @xmath234_{ii}$ ] will be given in section [ sec : hypothesistesting ] showing that , if @xmath59 is computed through algorithm 1 , @xmath234_{ii}$ ] is of order one for a broad family of random designs . as",
    "a consequence @xmath235 is much smaller than @xmath236 whenever @xmath79 .",
    "we summarize these remarks below .",
    "[ rem : sparsity_cond ] theorem  [ thm : main_thm ] only requires that the support size satisfies @xmath237 .",
    "if we further assume @xmath79 , then we have @xmath238 with high probability .",
    "hence , @xmath35 is an asymptotically unbiased estimator for @xmath16 .",
    "a more formal comparison of the bias of @xmath35 , and of the one of the lasso estimator @xmath45 can be found in section [ sec : biasdiscussion ] below .",
    "section [ sec : gbr - comparison ] compares our approach with the related one in @xcite .    as it can be seen from the statement of theorem [ thm : deterministic ] and theorem [ thm : event_thm ] , the claim of theorem  [ thm : main_thm ] does not rely on the specific choice of the objective function in optimization problem   and only uses the constraint on @xmath239 . in particular",
    "it holds for any matrix @xmath59 that is feasible . on the other hand ,",
    "the specific objective function problem   minimizes the variance of the noise term @xmath240 .",
    "theorems [ thm : deterministic ] and [ thm : event_thm ] provide a quantitative framework to discuss in what sense the lasso estimator @xmath45 is asymptotically biased , while the de - biased estimator @xmath35 is asymptotically unbiased .    given an estimator @xmath45 of the parameter vector @xmath16 , we define its _ bias _ to be the vector @xmath241 note that , if the design is random , @xmath242 is a measurable function of @xmath10 .",
    "if the design is deterministic , @xmath242 is a deterministic quantity as well , and the conditioning is redundant .",
    "it follows from eq .",
    "( [ eq : generalrepresentation ] ) that @xmath243 theorem [ thm : main_thm ] with high probability , @xmath244 .",
    "the next corollary establishes that this translates into a bound on @xmath245 for all @xmath10 in a set that has probability rapidly converging to one as @xmath1 , @xmath0 get large .",
    "[ coro : uisnotbiased ] under the assumptions of theorem [ thm : main_thm ] , let @xmath246 , @xmath247 be defined as per eqs .",
    "( [ eq : boundeva ] ) , ( [ eq : boundevg ] )",
    ". then we have @xmath248    the proof of this corollary can be found in appendix [ app : uisnotbiased ] .",
    "this result can be contrasted with a converse result for the lasso estimator .",
    "namely , as stated below , there are choices of the vector @xmath16 , and of the design covariance @xmath69 , such that @xmath242 is the sum of two terms .",
    "one is of order order @xmath249 and the second is of order @xmath250 . if @xmath85 is significantly smaller than @xmath251 ( which is the main regime studied in the rest of the paper ) , the first term dominates and @xmath252 is much larger than @xmath250 .",
    "if on the other hand @xmath85 is significantly larger than @xmath253 then @xmath252 is of the same order as @xmath250 .",
    "this justify referring to @xmath35 as to an _",
    "unbiased estimator_.    notice that , since we want to establish a negative result about the lasso , it is sufficient to exhibit a specific covariance structure @xmath69 satisfying the assumptions of the previous corollary . remarkably it is sufficient to consider standard designs , i.e. @xmath254 .    [ coro : lassoisbiased ] under the assumptions of theorem [ thm : main_thm ] , further consider the case @xmath70",
    ". then , there exists a numerical constant @xmath255 , a set of design matrices @xmath256 , and coefficient vectors @xmath257 , @xmath258 , such that @xmath259 in particular @xmath260 ( which follows from @xmath261 ) then we have @xmath262 on the other hand , if @xmath263 , then @xmath264    a formal proof of this statement is deferred to appendix [ app : lassoisbiased ] , but the underlying mathematical mechanism is quite simple and instructive . recall that the kkt conditions for the lasso estimator ( [ eq : lassoestimator ] ) read @xmath265 with @xmath266 a vector in the subgradient of the @xmath22 norm at @xmath45 . adding @xmath267 to both sides , and taking expectation over the noise , we get @xmath268 where @xmath269 a debiased estimator of the general form eq .",
    "( [ eq : generaldebiased ] ) , for @xmath270 . this suggest that @xmath242 can be decomposed in two contributions as described above , and as shown formally in appendix [ app : lassoisbiased ] ,      in this section we briefly compare the above debiasing procedure and in particular theorems [ thm : deterministic ] , [ thm : event_thm ] and [ thm : main_thm ] to the results of @xcite . in the case of linear statistical models considered here ,",
    "the authors of @xcite construct a debiased estimator of the form ( [ eq : generaldebiased ] ) .",
    "however , instead of solving the optimization problem ( [ eq : optimization ] ) , they follow @xcite and use the regression coefficients of the @xmath41-th column of @xmath10 on the other columns to construct the @xmath41-th row of @xmath59 .",
    "these regression coefficients are computed once again using the lasso ( node - wise lasso ) .",
    "it useful to spell out the most important differences between our contribution and the ones of @xcite :    1 .",
    "the case of fixed non - random designs is covered by ( * ? ? ?",
    "* theorem 2.1 ) , which should be compared to our theorem [ thm : deterministic ] .",
    "while in our case the bias is controlled by the generalized coherence parameter , a similar role is played in @xcite by the regularization parameters of the nodewise lasso .",
    "the case of random designs is covered by ( * ? ? ?",
    "* theorem 2.2 , theorem 2.4 ) , which should be compared with our theorem [ thm : main_thm ] . in this case , the assumptions underlying our result are significantly less restrictive .",
    "more precisely : 1 .",
    "* theorem 2.2 , theorem 2.4 ) assume @xmath10 to have i.i.d .",
    "rows , while we only assume the rows to be independent .",
    "* theorem 2.2 , theorem 2.4 ) assume the rows inverse covariance matrix @xmath71 be sparse .",
    "more precisely , letting @xmath271 be the number of non - zero entries of the @xmath272-th row of @xmath71 , @xcite assumes @xmath273}s_j = o(n/\\log p)$ ] , that is much smaller than @xmath0 .",
    "we do not make any sparsity assumption for @xmath71 , and @xmath271 can be as large as @xmath0 .",
    "+ ( in fact ( * ? ? ?",
    "* theorem 2.4 ) also consider the assumption of @xmath10 with bounded entries , but even stricter sparsity assumptions are made in that case . )    in addition our theorem [ thm : main_thm ] provides the specific dependence on the maximum and minimum singular value of @xmath128 .",
    "let us also note that solving the convex problem ( [ eq : optimization ] ) is not more burdensome than solving the nodewise lasso as in @xcite , this can be confirmed by checking that the dual of the problem ( [ eq : optimization ] ) is an @xmath22-regularized quadratic optimization problem .",
    "it has therefore the same complexity as the nodewise lasso ( but it is different from the nodewise lasso ) .",
    "a direct application of theorem  [ thm : main_thm ] is to derive confidence intervals and statistical hypothesis tests for high - dimensional models . throughout",
    ", we make the sparsity assumption @xmath79 and omit explicit constants that can be readily derived from theorem [ thm : main_thm ] .",
    "as discussed above , the bias term @xmath274 is negligible with respect to the random term @xmath275 in the decomposition ( [ eq : generalrepresentationbis ] ) , provided the latter has variance of order one .",
    "our first lemma establishes that this is indeed the case .",
    "[ lem : missing_bound ] let @xmath42 be the matrix with rows @xmath276 obtained by solving convex program   in algorithm 1 .",
    "then for all @xmath64 $ ] , @xmath277_{i , i } \\ge \\frac{(1-\\coh)^2}{\\hsigma_{i , i}}\\,.\\ ] ]    lemma  [ lem : missing_bound ] is proved in appendix  [ app : missing_bound ] .",
    "using this fact , we can then characterize the asymptotic distribution of the residuals @xmath278 .",
    "theorem [ thm : main_thm ] naturally suggests to consider the scaled residual @xmath279_{i , i}^{1/2})$ ] . in the next lemma",
    "we consider a slightly more general scaling , replacing @xmath56 by a consistent estimator @xmath57 .",
    "[ lemma : lastdistribution ] consider a sequence of design matrices @xmath154 , with dimensions @xmath280 , @xmath281 satisfying the following assumptions , for constants @xmath282 independent of @xmath1 . for each @xmath1",
    ", @xmath193 is such that @xmath194 , and @xmath195 , and @xmath196}\\sigma_{ii}\\le 1 $ ] .",
    "assume @xmath197 to have independent subgaussian rows , with zero mean and subgaussian norm @xmath283 ,    consider the linear model   and let @xmath35 be defined as per eq .   in algorithm 1 , with @xmath284 and @xmath285 , with @xmath286 large enough constants .",
    "finally , let @xmath287 an estimator of the noise level satisfying , for any @xmath288 , @xmath289 if @xmath290 ( @xmath291 ) , then , for all @xmath292 , we have @xmath293_{i , i}^{1/2 } }   \\le x   \\right\\ } -\\phi(x)\\right| = 0\\ , .",
    "\\end{aligned}\\ ] ]    the proof of this lemma can be found in section [ proof : lastdistribution ] .",
    "we also note that the dependence of @xmath286 on @xmath294 can be easily reconstructed from theorem [ thm : event_thm ] .",
    "the last lemma requires a consistent estimator of @xmath56 , in the sense of eq .",
    "( [ eq : consistencysigma ] ) .",
    "several proposal have been made to estimate the noise level in high - dimensional linear regression .",
    "a short list of references includes @xcite .",
    "consistency results have been proved or can be proved for several of these estimators .    in order to demonstrate that the consistency criterion ( [ eq : consistencysigma ] ) can be achieved , we use the scaled lasso  @xcite given by @xmath295 this is a joint convex optimization problem which provides an estimate of the noise level in addition to an estimate of @xmath16 .    the following lemma uses the analysis of @xcite to show that @xmath57 thus defined satisfies the consistency criterion ( [ eq : consistencysigma ] ) .    [ lemma : consistencysigma ] under the assumptions of lemma [ lemma : lastdistribution ] ,",
    "let @xmath296 be the scaled lasso estimator of the noise level , see eq .",
    "( [ eq : slasso ] ) , with @xmath297",
    ". then @xmath57 thus satisfies eq .",
    "( [ eq : consistencysigma ] ) .",
    "the proof of this lemma is fairly straightforward and can be found in appendix [ sec : consistencysigma ] .      in view of lemma [ lemma :",
    "lastdistribution ] , it is quite straightforward to construct asymptotically valid confidence intervals .",
    "namely , for @xmath64 $ ] and significance level @xmath298 , we let @xmath299\\ , , \\\\",
    "\\delta(\\alpha , n ) & \\equiv \\phi^{-1}(1-\\alpha/2 ) \\ ,   \\frac{\\hsigma}{\\sqrt{n } }   [ m    \\hsigma m^\\st]^{1/2}_{i , i}\\ , .",
    "\\end{split}\\end{aligned}\\ ] ]    [ coro : interval ] consider a sequence of design matrices @xmath154 , with dimensions @xmath280 , @xmath281 satisfying the assumptions of lemma [ lemma : lastdistribution ] .",
    "consider the linear model   and let @xmath35 be defined as per eq .   in algorithm 1 , with @xmath284 and @xmath285 , with @xmath286 large enough constants .",
    "finally , let @xmath287 a consistent estimator of the noise level in the sense of eq .",
    "( [ eq : consistencysigma ] ) . then the confidence interval @xmath300 is asymptotically valid , namely @xmath301    the proof is an immediate consequence of lemma [ lemma : lastdistribution ] since @xmath302_{i , i}^{1/2 } }   \\le",
    "\\phi^{-1}(1-\\alpha/2 ) \\right\\ } \\\\ & - \\lim_{n\\to\\infty}\\prob \\left\\{\\frac{\\sqrt{n}(\\htheta^u_i - \\theta_{0,i})}{\\hsigma [ m \\hsigma m^\\st]_{i , i}^{1/2 } }   \\le -\\phi^{-1}(1-\\alpha/2 ) \\right\\}\\\\   = & 1-\\alpha \\ , .   \\end{aligned}\\ ] ]      an important advantage of sparse linear regression models is that they provide parsimonious explanations of the data in terms of a small number of covariates .",
    "the easiest way to select the ` active ' covariates is to choose the indexes @xmath41 for which @xmath303 .",
    "this approach however does not provide a measure of statistical significance for the finding that the coefficient is non - zero .",
    "more precisely , we are interested in testing an individual null hypothesis @xmath304 versus the alternative @xmath305 , and assigning @xmath0-values for these tests .",
    "we construct a @xmath0-value @xmath306 for the test @xmath307 as follows : @xmath308_{i , i}^{1/2 } } \\bigg)\\bigg)\\ , .",
    "\\label{eq : p - value}\\end{aligned}\\ ] ] the decision rule is then based on the @xmath0-value @xmath306 : @xmath309 where @xmath91 is the fixed target type i error probability .",
    "we measure the quality of the test @xmath310 in terms of its significance level @xmath311 and statistical power @xmath312 . here",
    "@xmath311 is the probability of type i error ( i.e. of a false positive at @xmath41 ) and @xmath313 is the probability of type ii error ( i.e. of a false negative at @xmath41 ) .",
    "note that it is important to consider the tradeoff between statistical significance and power .",
    "indeed any significance level @xmath91 can be achieved by randomly rejecting @xmath307 with probability @xmath91 .",
    "this test achieves power @xmath314 .",
    "further note that , without further assumption , no nontrivial power can be achieved .",
    "in fact , choosing @xmath315 arbitrarily close to zero , @xmath307 becomes indistinguishable from its alternative .",
    "we will therefore assume that , whenever @xmath315 , we have @xmath316 as well .",
    "we take a minimax perspective and require the test to behave uniformly well over @xmath85-sparse vectors . formally , given a family of tests @xmath317 , indexed by @xmath318 $ ] , @xmath154 , we define , for @xmath319 a lower bound on the non - zero entries : @xmath320 here , we made dependence on @xmath1 explicit . also , @xmath321 denotes the induced probability for random design @xmath10 and noise realization @xmath322 , given the fixed parameter vector @xmath323 .",
    "our next theorem establishes bounds on @xmath324 and @xmath325 for our decision rule ( [ eq : decision - rule ] ) .",
    "[ thm : error - power ] consider a sequence of design matrices @xmath154 , with dimensions @xmath280 , @xmath281 satisfying the assumptions of lemma [ lemma : lastdistribution ] .    consider the linear model   and let @xmath35 be defined as per eq .   in algorithm 1 , with @xmath284 and @xmath285 , with @xmath286 large enough constants",
    "finally , let @xmath287 a consistent estimator of the noise level in the sense of eq .",
    "( [ eq : consistencysigma ] ) , and @xmath326 be the test defined in eq .",
    "( [ eq : decision - rule ] ) .",
    "then the following holds true for any fixed sequence of integers @xmath327 : @xmath328^{1/2}}\\bigg)\\ , , \\label{eq : power}\\end{aligned}\\ ] ] where , for @xmath329 $ ] and @xmath330 , the function @xmath331 is defined as follows : @xmath332    theorem  [ thm : error - power ] is proved in appendix  [ proof : error - power ] .",
    "it is easy to see that , for any @xmath333 , @xmath334 is continuous and monotone increasing .",
    "moreover , @xmath335 which is the trivial power obtained by randomly rejecting @xmath307 with probability @xmath91 .",
    "as @xmath336 deviates from zero , we obtain nontrivial power . notice that in order to achieve a specific power @xmath337 , our scheme requires @xmath338 , for some constant @xmath339 that depends on @xmath340 .",
    "this is because @xmath341 .",
    "the authors of  @xcite prove an upper bound for the minimax power of tests with a given significance level @xmath91 , under random designs .",
    "for the readers convenience , we recall here this result .",
    "( the following is a restatement of ( * ? ? ?",
    "* theorem 2.3 ) , together with a standard estimate on the tail of chi - squared random variables . )",
    "[ thm : generalupperbound ] assume @xmath154 to be a random design matrix with i.i.d .",
    "gaussian rows with zero mean and covariance @xmath69 . for @xmath64 $",
    "] , let @xmath342 be a hypothesis testing procedure for testing @xmath343 , and denote by @xmath344 and @xmath345 its fraction of type i and type ii errors , cf . eqs .",
    "( [ eq : alphadef ] ) and ( [ eq : betadef ] ) . finally , for @xmath130\\setminus \\{i\\}$ ] , define @xmath346 .    for any @xmath347 and @xmath348 ,",
    "if @xmath349 , then @xmath350 for any @xmath351 $ ] .    the intuition behind this bound is straightforward : the power of any test for @xmath343 is upper bounded by the power of an oracle test that is given access to the support of @xmath352 , with the eventual exclusion of @xmath41 .",
    "namely , the oracle has access to @xmath353 and outputs a test for @xmath307 . computing the minimax power of such oracle reduces to a classical hypothesis testing problem .",
    "let us emphasize that the last theorem applies to _ gaussian _ random designs . since this theorem establishes a negative result ( an upper bound on power ) it makes sense to consider this somewhat more specialized setting",
    ".    using this upper bound , we can restate theorem [ thm : error - power ] as follows .",
    "[ thm : optimality ] consider a gaussian random design model that satisfies the conditions of theorem  [ thm : error - power ] , and let @xmath326 be the testing procedure defined in eq .",
    "( [ eq : decision - rule ] ) , with @xmath35 as in algorithm 1 . further , let @xmath354;s } \\big\\{\\sigma_{i|s}\\ , \\sigma_{ii}^{-1 } : \\,\\ , s \\subseteq[p]\\backslash\\{i\\},\\ , |s|",
    "< s_0 \\big\\}\\,.\\end{aligned}\\ ] ] under the sparsity assumption @xmath79 , the following holds true . if @xmath355 is any sequence of tests with @xmath356 , then @xmath357 in other words , the asymptotic efficiency of the test @xmath326 is at least @xmath81 .",
    "hence , our test @xmath326 has nearly optimal power in the following sense .",
    "it has power at least as large as the power of any oter test @xmath358 , provided the latter is applied to a sample size increased by a factor @xmath84 .",
    "further , under the assumptions of theorem  [ thm : main_thm ] , the factor @xmath84 is a bounded constant . indeed @xmath359 since @xmath360 , and @xmath361 due to @xmath362 .",
    "note that @xmath1 , @xmath363 and @xmath56 appears in our upper bound ( [ eq : upperboundpower ] ) in the combination @xmath364 , which is the natural measure of the signal - to - noise ratio ( where , for simplicity , we neglected @xmath365 with respect to @xmath1 ) .",
    "hence , the above result can be restated as follows .",
    "the test @xmath326 has power at least as large as the power of any oter test @xmath358 , provided the latter is applied at a noise level augmented by a factor @xmath366 .      in many situations ,",
    "it is necessary to perform statistical inference on more than one of the parameters simultaneously .",
    "for instance , we might be interested in performing inference about @xmath367 for some set @xmath368 $ ] .",
    "the simplest generalization of our method is to the case in which @xmath369 stays finite as @xmath191 . in this case",
    "we have the following generalization of lemma [ lemma : lastdistribution ] .",
    "( the proof is the same as for lemma [ lemma : lastdistribution ] , and hence we omit it . )",
    "[ lemma : lowdim ] under the assumptions of lemma [ lemma : lastdistribution ] , define @xmath370\\ , . \\ ] ] let @xmath371 be a sequence of sets @xmath372 $ ] , with @xmath373 fixed as @xmath374 , and further assume @xmath290 , with @xmath291 .",
    "then , for all @xmath375 , we have @xmath376 where @xmath377 indicates that @xmath378,  @xmath379 , and @xmath380 .",
    "this lemma allows to construct confidence regions for low - dimensional projections of @xmath16 , much in the same way as we used lemma [ lemma : lastdistribution ] to compute confidence intervals for one - dimensional projections in section [ sec : confidenceinterval ] .",
    "explicitly , let @xmath381 be any borel set such that @xmath382 , where @xmath383 is the @xmath384-dimensional gaussian density .",
    "then , for @xmath368 $ ] , we define @xmath385 as follows @xmath386 then lemma [ lemma : lowdim ] implies ( under the assumptions stated there ) that @xmath387 is a valid confidence region @xmath388    a more challenging regime is the one of large - scale inference , that corresponds to @xmath389 with @xmath1 . even in the seemingly simple case in which a correct @xmath0-value is given for each individual coordinate ,",
    "the problem of aggregating them has attracted considerable amount of work , see e.g. @xcite for an overview .    here",
    "we limit ourself to designing a testing procedure for the family of hypotheses @xmath390}$ ] that controls the familywise error rate ( fwer ) .",
    "namely we want to define @xmath391 , for each @xmath64 $ ] , @xmath154 such that @xmath392:\\;\\ ; \\theta_{0,i } = 0 , t_{i,\\bx}(y)= 1\\big\\}\\ , , \\ ] ] in order to achieve familywise error control , we adopt a standard trick based on bonferroni inequality .",
    "given @xmath0-values defined as per eq .",
    "( [ eq : p - value ] ) , we let @xmath393 then we have the following error control guarantee .",
    "[ thm : fwer ] consider a sequence of design matrices @xmath154 , with dimensions @xmath280 , @xmath281 satisfying the assumptions of lemma [ lemma : lastdistribution ] .",
    "consider the linear model   and let @xmath35 be defined as per eq .   in algorithm 1 , with @xmath284 and @xmath285 , with @xmath286 large enough constants .",
    "finally , let @xmath287 be a consistent estimator of the noise level in the sense of eq .",
    "( [ eq : consistencysigma ] ) , and @xmath326 be the test defined in eq .",
    "( [ eq : decision - rule - fwer ] ) .",
    "then : @xmath394    the proof of this theorem is similar to the one of lemma [ lemma : lastdistribution ] and theorem [ thm : error - power ] , and is deferred to appendix [ app : fwer ] .",
    "as can be seen from the proof of theorem  [ thm : main_thm ] , @xmath395 , and since the noise is gaussian , i.e. , @xmath396 , we have @xmath397 .",
    "we claim that the distribution of the coordinates of @xmath275 is asymptotically gaussian , even if @xmath398 is non - gaussian , provided the definition of @xmath59 is modified slightly . as a consequence , the definition of confidence intervals and @xmath0-values in corollary [ coro : interval ] and   remain valid in this broader setting .    in case of non - gaussian noise , we write @xmath399^{1/2}_{i , i } }   & = \\frac{1}{\\sqrt{n } } \\frac{m_i^\\st \\bx^\\st w}{\\sigma [ m_i^\\st \\hsigma m_i]^{1/2 } } + o(1)\\\\ & = \\frac{1}{\\sqrt{n } } \\sum_{j=1}^n \\frac{m_i^\\st x_j w_j}{\\sigma [ m_i^\\st \\hsigma m_i]^{1/2 } } + o(1)\\,.\\end{aligned}\\ ] ] conditional on @xmath10 , the summands @xmath400^{1/2})$ ] are independent and zero mean .",
    "further , @xmath401 . therefore , if lindenberg condition holds , namely for every @xmath402 , almost surely @xmath403 then @xmath404 , from which we can build the valid @xmath0-values as in  .    in order to ensure that the lindeberg condition holds , we modify the optimization problem   as follows : @xmath405 next theorem shows the validity of the proposed @xmath0-values in the non - gaussian noise setting .",
    "[ thm : nongauss ] suppose that the noise variables @xmath406 are independent with @xmath407 , @xmath408 , and @xmath409 for some @xmath410 .",
    "let @xmath42 be the matrix with rows @xmath276 obtained by solving optimization problem  .",
    "then under the assumptions of theorem  [ thm : main_thm ] , and for sparsity level @xmath79 , an asymptotic two - sided confidence interval for @xmath96 with significance @xmath91 is given by @xmath411 $ ] where @xmath412_{i , i}}\\ , .",
    "\\end{aligned}\\ ] ] further , an asymptotically valid @xmath0-value @xmath306 for testing null hypothesis @xmath307 is constructed as : @xmath413_{i , i}^{1/2}}\\bigg ) \\bigg)\\,.\\ ] ]    theorem  [ thm : nongauss ] is proved in section  [ proof : nongauss ] .",
    "we consider linear model  , where the rows of design matrix @xmath10 are fixed i.i.d .",
    "realizations from @xmath414 , where @xmath415 is a circulant symmetric matrix with entries @xmath416 given as follows for @xmath417 : @xmath418 regarding the regression coefficient , we consider a uniformly random support @xmath419 $ ] , with @xmath420 and let @xmath421 for @xmath422 and @xmath423 otherwise .",
    "the measurement errors are @xmath424 , for @xmath425 $ ] .",
    "we consider several configurations of @xmath426 and for each configuration report our results based on @xmath427 independent realizations of the model with fixed design and fixed regression coefficients .",
    "in other words , we repeat experiments over @xmath427 independent realization of the measurement errors .",
    "we use the regularization parameter @xmath428 , where @xmath57 is given by the scaled lasso as per equation   with @xmath429 . furthermore , parameter @xmath430 ( cf .",
    "eq .  ) is set to @xmath431 this choice of @xmath430 is guided by theorem  [ thm : event_thm ] @xmath218 . throughout",
    ", we set the significance level @xmath432 .",
    "* confidence intervals . * for each configuration , we consider @xmath427 independent realizations of measurement noise and for each parameter @xmath96 , we compute the average length of the corresponding confidence interval , denoted by @xmath433 where @xmath300 is given by equation   and the average is taken over the realizations .",
    "we then define @xmath434 } \\avglength(j_i(\\alpha))\\,.\\end{aligned}\\ ] ] we also consider the average length of intervals for the active and inactive parameters , as follows : @xmath435 similarly , we consider average coverage for individual parameters . we define the following three metrics : @xmath436 } \\hprob[\\theta_{0,i }",
    "\\in j_i(\\alpha)]\\,,\\\\ \\cov_s & \\equiv s_0^{-1 } \\sum_{i\\in s } \\hprob[\\theta_{0,i } \\in j_i(\\alpha)]\\,,\\\\ \\cov_{s^c } & \\equiv ( p - s_0)^{-1 } \\sum_{i\\in s^c } \\hprob[0 \\in j_i(\\alpha)]\\,,\\end{aligned}\\ ] ] where @xmath437 denotes the empirical probability computed based on the @xmath427 realizations for each configuration . the results are reported in table  [ tbl : confidence ] . in fig .",
    "[ fig : ci ] , we plot the constructed @xmath19-confidence intervals for one realization of configuration @xmath438 . for sake of clarity ,",
    "we plot the confidence intervals for only 100 of the 1000 parameters .",
    "( -105,155)coordinates of @xmath352 ( -105,138)coordinates of @xmath35    [ cols=\"<,^,^,^,^,^,^,^,^\",options=\"header \" , ]     ( -187,-15)quantiles of standard normal distribution ( -220,50 )      as a real data example , we consider a high - throughput genomic data set concerning riboflavin ( vitamin @xmath439 ) production rate .",
    "this data set is made publicly available by  @xcite and contains @xmath440 samples and @xmath441 covariates corresponding to @xmath441 genes .",
    "for each sample , there is a real - valued response variable indicating the logarithm of the riboflavin production rate along with the logarithm of the expression level of the @xmath441 genes as the covariates .",
    "following  @xcite , we model the riboflavin production rate as a linear model with @xmath441 covariates and @xmath442 samples , as in eq .  .",
    "we use the @xmath443 package @xmath444  @xcite to fit the lasso estimator .",
    "similar to the previous section , we use the regularization parameter @xmath428 , where @xmath57 is given by the scaled lasso as per equation   with @xmath429 .",
    "this leads to the choice @xmath445 .",
    "the resulting model contains 30 genes ( plus an intercept term ) corresponding to the nonzero parameters of the lasso estimator .",
    "we use eq .   to construct @xmath0-values for different genes .",
    "adjusting fwer to @xmath446 significance level , we find two significant genes , namely genes yxld - at and yxle - at . by contrast",
    ", the multisample - splitting method proposed in  @xcite finds only the gene yxld - at at the fwer - adjusted @xmath446 significance level .",
    "also the ridge - type projection estimator , proposed in  @xcite , returns no significance gene .",
    "( see  @xcite for further discussion on these methods . )",
    "this indicates that these methods are more conservative and produce typically larger @xmath0-values .    in fig .",
    "[ fig : p - rib ] we plot the empirical cdf of the computed @xmath0-values for riboflavin example .",
    "clearly the plot confirms that the @xmath0-values are distributed according to uniform distribution .",
    "( -187,-15)quantiles of standard normal distribution ( -220,50 )",
    "substituting @xmath447 in the definition ( [ eq : generaldebiased ] ) , we get @xmath448 with @xmath449 defined as per the theorem statement . further @xmath275 is gaussian with the stated covariance because it is a linear function of the gaussian vector @xmath450 .",
    "we are left with the task of proving the bound ( [ eq : firstbounddelta ] ) on @xmath274 .",
    "note that by definition ( [ def : coherence ] ) , we have @xmath451 by ( * ? ? ?",
    "* theorem 6.1 , lemma 6.2 ) , we have , for any @xmath452 @xmath453 ( more precisely , we consider the trivial generalization of ( * ? ? ?",
    "* lemma 6.2 ) to the case @xmath454 , instead of @xmath455 for all @xmath64 $ ] . )",
    "substituting eq .",
    "( [ eq : linftyl1 ] ) in the last bound , we get @xmath456 finally , the claim follows by selecting @xmath457 so that @xmath458 .",
    "note that the event @xmath459 requires two conditions .",
    "hence , its complement @xmath460}\\ , \\hsigma_{i , i } \\le k , \\;\\ ; \\hsigma = ( \\bx^{\\st}\\bx / n)\\big\\}\\ , . \\ ] ] we will bound separately the probability of @xmath461 and the probability of @xmath462 .",
    "the claim of theorem  [ thm : event_thm].@xmath217 follows by union bound .",
    "it is also useful to recall the notion of restricted eigenvalue , introduced by bickel , ritov and tsybakov @xcite .    given a symmetric matrix @xmath464 an integer @xmath291 , and @xmath465 , the restricted eigenvalue of @xmath466 is defined as @xmath467 , |s|\\le s_0}\\min_{\\theta\\in\\reals^p}\\big\\{\\frac{\\<\\theta , q\\,\\theta\\>}{\\|\\theta_s\\|_2 ^ 2 } : \\;\\ ; \\theta\\in\\reals^p ,   \\;\\ ; \\|\\theta_{s^c}\\|_1\\le l\\|\\theta_s\\|_1\\big\\}\\ , .\\end{aligned}\\ ] ]    rudelson and zhou @xcite prove that , if the population covariance satisfies the restricted eigenvalue condition , then the sample covariance satisfies it as well , with high probability .",
    "more precisely ( * ? ? ?",
    "* theorem 6 ) , the following happens for some @xmath205 , @xmath468 , and every @xmath469 we have @xmath470 note that @xmath471 and , by cauchy - schwartz @xmath472 .",
    "with the definitions in the statement ( cf .",
    "( [ eq : boundeva ] ) ) , we therefore have @xmath473 equivalently , @xmath474 .      by definition @xmath476 note that @xmath477 are independent centered random variables .",
    "further ( recalling that , for any random variables @xmath478 , @xmath479 , and @xmath480 ) they are subexponential with subexponential norm @xmath481 by bernstein - type inequality for centered subexponential random variables @xcite , we get @xmath482\\ , . \\ ] ] hence , for all @xmath483 such that @xmath484 $ ] , @xmath485}\\hsigma_{ii}\\ge 1+\\eps\\big)&\\le 2 p\\ ,   \\exp\\big(-\\frac{n\\eps^2}{24e^2\\kappa^4 } \\big ) \\le 2e^{-c_1n}\\ , , \\ ] ] which implies @xmath486 for all @xmath487 .",
    "obviously , we have @xmath488 and hence the statement follows immediately from the following estimate .    [ lem : main_lem ] consider a random design matrix @xmath489 , with i.i.d .",
    "rows having mean zero and population covariance @xmath69 .",
    "assume that    * we have @xmath490 , and @xmath491 .",
    "* the rows of @xmath492 are sub - gaussian with @xmath493 .",
    "let @xmath494 be the empirical covariance .",
    "then , for any constant @xmath495 , the following holds true .",
    "@xmath496 with @xmath497 .    the proof is based on bernstein - type inequality for sub - exponential random variables  @xcite .",
    "let @xmath498 , for @xmath499 $ ] , and write @xmath500 fix @xmath501 $ ] , and for @xmath499 $ ] , let @xmath502 , where @xmath503 .",
    "notice that @xmath504 , and the @xmath505 are independent for @xmath499 $ ] . also , @xmath506 . by  (",
    "* remark 5.18 ) , we have @xmath507 moreover , for any two random variables @xmath140 and @xmath24 , we have @xmath508 hence , by assumption @xmath509 , we obtain @xmath510 let @xmath511 . applying bernstein - type inequality for centered sub - exponential random variables  @xcite , we get @xmath512\\,.\\ ] ] choosing @xmath513 , and assuming @xmath514 ^ 2 \\log p$ ] , we arrive at @xmath515 the result follows by union bounding over all possible pairs @xmath516 $ ] .      let @xmath517 be a shorthand for the bound on @xmath189 appearing in eq .",
    "( [ eq : tailboundquant ] ) .",
    "then we have @xmath518 where , in the firsr equation @xmath519 denotes the complement of event @xmath520 and the second inequality follows from theorem [ thm : event_thm ] .",
    "notice , in particular , that the bound ( [ eq : boundeva ] ) can be applied for @xmath521 since , under the present assumptions @xmath522 .",
    "finally @xmath523 here the last inequality follows from theorem [ thm : deterministic ] applied per given @xmath524 and hence using the bound ( [ eq : firstbounddelta ] ) with @xmath525 , @xmath521 , @xmath526 .",
    "we will prove that , under the stated assumptions @xmath527_{i , i}^{1/2 } }   \\le x   \\right\\ } \\le \\phi(x )   \\ , .\\label{eq : lastdistrub}\\end{aligned}\\ ] ] a matching lower bound follows by a completely analogous argument .",
    "notice that by eq .",
    "( [ eq : generalrepresentationbis ] ) , we have @xmath528_{ii}^{1/2 } } = \\frac{e_i^\\st m\\bx^\\st w}{\\sigma [ m\\hsigma m^\\st]_{ii}^{1/2 } } + \\frac{\\delta_i}{\\sigma [ m\\hsigma m^\\st]_{ii}^{1/2}}\\ , .",
    "\\end{aligned}\\ ] ] let @xmath529_{ii}^{1/2})$ ] and @xmath530 .",
    "we claim that @xmath531 . to see this , note that @xmath532 , and @xmath533 and @xmath398 are independent .",
    "hence , @xmath534 which proves our claim . in order to prove eq .",
    "( [ eq : lastdistrub ] ) , fix @xmath288 and write @xmath535_{i , i}^{1/2 } }   \\le x   \\right ) & = \\prob\\left(\\frac{\\sigma}{\\hsigma } \\tz + \\frac{\\delta_i}{\\hsigma[m \\hsigma m^\\st]_{i , i}^{1/2}}\\ge x\\right)\\\\ & \\le   \\prob\\big(\\frac{\\sigma}{\\hsigma } \\tz \\le x+\\eps\\big)+\\prob\\left(\\frac{|\\delta_i|}{\\hsigma[m \\hsigma    m^\\st]_{i , i}^{1/2}}\\ge \\eps\\right)\\\\ & \\le   \\prob\\big(\\tz \\le x+2\\eps+\\eps|x|\\big)+\\prob\\left(\\frac{|\\delta_i|}{\\hsigma[m \\hsigma    m^\\st]_{i , i}^{1/2}}\\ge \\eps\\right ) + \\prob\\big(\\big|\\frac{\\hsigma}{\\sigma}-1\\big|\\ge \\eps\\big)\\ , . \\ ] ] by taking the limit and using the assumption ( [ eq : consistencysigma ] ) , we obtain @xmath536_{i , i}^{1/2 } }   \\le x   \\right ) \\le \\\\ & \\phi(x+2\\eps+\\eps|x| ) + \\lim\\sup_{n\\to\\infty}\\sup_{\\|\\theta_0\\|_0 \\le s_0 } \\prob\\left(\\frac{|\\delta_i|}{\\hsigma[m \\hsigma    m^\\st]_{i , i}^{1/2}}\\ge \\eps\\right)\\ , .\\nonumber \\ ] ] since @xmath288 is arbitrary , it is therefore sufficient to show that the limit on the right hand side vanishes for any @xmath288 .",
    "note that @xmath537_{i , i}\\ge 1/(4\\hsigma_{ii})$ ] for all @xmath1 large enough , by lemma [ lem : missing_bound ] , and since @xmath538 as @xmath191 .",
    "we have therefore @xmath539_{i , i}^{1/2}}\\ge \\eps\\right)&\\le \\prob\\big(\\frac{2}{\\hsigma}\\hsigma_{ii}^{1/2}\\ , |\\delta_i|\\ge \\eps\\big)\\\\ & \\le \\prob\\big(\\frac{8}{\\sigma}\\ , |\\delta_i|\\ge \\eps\\big)+\\prob\\big(\\frac{\\hsigma}{\\sigma}\\ge 2\\big)+\\prob(\\hsigma_{ii}\\ge \\sqrt{2})\\ , . \\ ] ] note that @xmath540 by assumption ( [ eq : consistencysigma ] ) , and @xmath541 by theorem [ thm : event_thm].@xmath218 .",
    "hence @xmath542_{i , i}^{1/2}}\\ge \\eps\\right)&\\le\\lim\\sup_{n\\to\\infty}\\sup _ { \\|\\theta_0\\|_0 \\le s_0 } \\prob\\big(\\|\\delta\\|_{\\infty}\\ge \\frac{\\eps\\sigma}{8}\\big)\\\\ & \\le \\lim\\sup_{n\\to\\infty}\\big(4\\ , e^{-c_1n}+4\\ , p^{-(\\tilde{c}_0\\wedge c_2)}\\big ) = 0\\ , , \\ ] ] where the last inequality follows from eq .  (",
    "[ eq : tailboundquant ] ) since @xmath79 and hence @xmath543 for all @xmath1 large enough .",
    "this completes the proof of eq .",
    "( [ eq : lastdistrub ] ) .",
    "the matching lower bound follows by the same argument .",
    "we begin with proving eq .  .",
    "defining @xmath544_{i , i}^{1/2})$ ] , we have @xmath545,\\ , \\|\\theta_0\\|_0 \\le s_0,\\ , \\theta_{0,i } = 0 \\big\\}\\\\ & = \\lim_{n\\to \\infty } \\sup_{\\theta_0 } \\big\\{\\prob\\big(\\phi^{-1}(1-\\frac{\\alpha}{2 } ) \\le \\frac{\\sqrt{n } |\\htheta^u_i|}{\\hsigma [ m\\hsigma m^\\st]_{i , i}^{1/2 } } \\big ) : \\,i\\in [ p],\\ , \\|\\theta_0\\|_0 \\le s_0,\\ , \\theta_{0,i } = 0 \\big\\}\\\\ & = \\lim_{n\\to \\infty } \\sup_{\\theta_0 } \\big\\ { \\prob\\big(\\phi^{-1}(1-\\frac{\\alpha}{2 } ) \\le |z_i| \\big ) : \\,i\\in [ p],\\ , \\|\\theta_0\\|_0 \\le s_0 \\big\\ } \\le \\alpha\\,,\\end{aligned}\\ ] ] where the last inequality follows from lemma [ lemma : lastdistribution ] .",
    "we next prove eq .  .",
    "recall that @xmath546 is a feasible solution of  , for @xmath547 with probability at least @xmath548 , as per lemma  [ lem : main_lem ] ) . on this event ,",
    "letting @xmath38 be the solution of the optimization problem  , we have @xmath549 where @xmath550 are i.i.d . with @xmath551 and sub - gaussian norm @xmath552 .",
    "letting @xmath553 , we have that @xmath554 is zero mean and sub - exponential with @xmath555 . hence , by applying bernstein inequality ( as , for instance , in the proof of lemma [ lem : main_lem ] ) , we have , for @xmath556 , @xmath557 therefore , by borel - cantelli ( since we can make @xmath558 by a suitable choice of @xmath559 ) , we have , almost surely @xmath560\\le 0\\ , .",
    "\\label{eq : sigmaib}\\end{aligned}\\ ] ]    this bound leads to a lower bound for the power .",
    "first of all , a straightforward manipulation yields as follows , letting @xmath561 : @xmath562_{i , i}^{1/2 } } \\big ) : \\|\\theta_0\\|_0 \\le s_0,\\,|\\theta_{0,i}| \\ge \\lb \\big\\}\\\\ & =   \\lim\\inf_{n\\to \\infty } \\frac{1}{1-\\beta_{i , n}^{*}(\\lb ) } \\inf_{\\theta_0 } \\big\\ { \\prob\\big(z_*\\le \\big|z_i + \\frac{\\sqrt{n } \\theta_{0,i } } { \\hsigma   [ m \\hsigma m^\\st]_{i , i}^{1/2}}\\big| \\big ) : \\|\\theta_0\\|_0 \\le s_0,\\,|\\theta_{0,i}| \\ge \\lb \\big\\}\\\\ & \\stackrel{(a)}{\\ge } \\lim\\inf_{n\\to \\infty } \\frac{1}{1-\\beta_{i , n}^{*}(\\lb ) } \\inf_{\\theta_0 } \\big\\ { \\prob\\big(z_*\\le \\big|z_i +   \\frac{\\sqrt{n } \\lb}{\\sigma   [ \\sigma_{i , i}^{-1}]^{1/2}}\\big|\\big ) : \\|\\theta_0\\|_0 \\le s_0 \\big\\}\\\\ & =   \\lim\\inf_{n\\to \\infty } \\frac{1}{1-\\beta_{i , n}^{*}(\\lb ) } \\big\\{1- \\phi\\big(z_*- \\frac{\\sqrt{n } \\lb}{\\sigma [ \\sigma_{i , i}^{-1}]^{1/2}}\\big )   + \\phi\\big(-z_*- \\frac{\\sqrt{n } \\lb}{\\sigma [ \\sigma_{i , i}^{-1}]^{1/2}}\\big ) \\big\\}\\\\ & = \\lim\\inf_{n\\to \\infty } \\frac{1}{1-\\beta_{i , n}^{*}(\\lb ) } g\\big(\\alpha , \\frac{\\sqrt{n } \\lb}{\\sigma   [ \\sigma_{i , i}^{-1}]^{1/2 } } \\big ) = 1\\,.\\end{aligned}\\ ] ] here @xmath217 follows from eq .   and",
    "the fact @xmath563 .      under the assumptions of theorem  [ thm : main_thm ] and assuming @xmath79 , we have @xmath564 with @xmath238 . using lemma  [ lem : missing_bound ] ,",
    "we have @xmath565_{i , i}^{1/2 } } = z_i + o(1)\\ , , \\quad \\text{with } \\ , z_i \\equiv \\frac{1}{\\sqrt{n}}\\frac{m_i^\\st \\bx^\\st w}{\\sigma [ m_i^\\st \\hsigma m_i]^{1/2 } } \\,.\\ ] ]",
    "the following lemma characterizes the limiting distribution of @xmath566 which implies the validity of the proposed @xmath0-value @xmath306 and confidence intervals .",
    "[ lem : nongauss ] suppose that the noise variables @xmath406 are independent with @xmath407 , and @xmath408 , and @xmath567 for some @xmath410 .",
    "let @xmath42 be the matrix with rows @xmath276 obtained by solving optimization problem  . for @xmath568 $ ] , define @xmath569^{1/2 } } \\,.\\ ] ] under the assumptions of theorem  [ thm : main_thm ] , for any sequence @xmath570 $ ] , and any @xmath571 , we have @xmath572    lemma  [ lem : nongauss ] is proved in appendix  [ app : nongauss ] .",
    "is supported by a caroline and fabian pease stanford graduate fellowship .",
    "this work was partially supported by the nsf career award ccf-0743978 , the nsf grant dms-0806211 , and the grants afosr / darpa fa9550 - 12 - 1 - 0411 and fa9550 - 13 - 1 - 0036 .",
    "let @xmath573 be the optimal value of the optimization problem  .",
    "we claim that @xmath574 to prove this claim notice that the constraint implies ( by considering its @xmath41-th component ) : @xmath575 therefore if @xmath576 is feasible and @xmath577 , then @xmath578 minimizing over all feasible @xmath576 gives @xmath579 the minimum over @xmath580 is achieved at @xmath581 . plugging in for @xmath580",
    ", we get @xmath582 optimizing this bound over @xmath68 , we obtain the claim ( [ eq : claimmm ] ) , with the optimal choice being @xmath583 .",
    "write @xmath584^{1/2}}\\,.\\ ] ] conditional on @xmath10 , the summands @xmath585 are zero mean and independent .",
    "furthermore , @xmath586 . we next prove the lindenberg condition as per eq .  .",
    "let @xmath587 .",
    "by lemma [ lem : missing_bound ] , we have , almost surely , @xmath588 .",
    "if all the optimization problems in   are feasible , then @xmath589 . hence , @xmath590 where @xmath591 and the last limit follows by taking @xmath410 as per the assumptions .",
    "using lindenberg central limit theorem , we obtain @xmath592 converges weakly to standard normal distribution , and hence , @xmath10-almost surely @xmath572 what remains is to show that with high probability all the @xmath0 optimization problems in   are feasible .",
    "in particular , we show that @xmath593 is a feasible solution to the @xmath41-th optimization problem , for @xmath568 $ ] . by lemma  [ lem : main_lem ] , @xmath594 , with high probability .",
    "moreover , @xmath595 } \\|\\sigma^{-1}_{i,\\cdot } x_j\\|_{\\psi_2 } & = \\sup_{j\\in [ p ] } \\|\\sigma^{-1/2}_{i,\\cdot } \\sigma^{-1/2 } x_j\\|_{\\psi_2}\\\\ & = \\|\\sigma^{-1/2}_{i,\\cdot}\\|_2 \\sup_{j\\in [ p ] } \\|\\sigma^{-1/2 } x_j\\|_{\\psi_2}\\\\   & = [ \\sigma^{-1}_{i , i}]^{1/2 }   \\sup_{j\\in [ p ] } \\|\\sigma^{-1/2 } x_j\\|_{\\psi_2 } = o(1)\\,.\\end{aligned}\\ ] ] using tail bound for sub - gaussian variables @xmath596 and union bounding over @xmath597 $ ] , we get @xmath598 for some constant @xmath599 .",
    "note that @xmath79 implies @xmath600 .",
    "hence , eventually almost surely , @xmath593 is a feasible solution to optimization problem  , for all @xmath568 $ ] .",
    "by theorem [ thm : deterministic ] , for any @xmath601 , we have @xmath602 ( this is obtained by setting @xmath603 , @xmath521 , @xmath526 in eq .",
    "( [ eq : firstbounddelta ] ) .",
    "hence @xmath604 which coincides with eq .  (",
    "[ eq : boundcoroperx ] ) .",
    "the probability estimate ( [ eq : coroprobabilitybound ] ) simply follows from theorem [ thm : event_thm ] using union bound .      by theorem [ thm : event_thm].@xmath217",
    ", we have ( setting @xmath605 ) : @xmath606 further , by lemma [ lem : main_lem ] , with @xmath607 , we have @xmath608 finally , by an obvious consequence of the proof of theorem [ thm : event_thm].@xmath217 @xmath609}\\hsigma_{ii}\\ge \\frac{1}{2}\\big\\}\\big)\\ge 1 - 2\\ , e^{-n / c_*}\\ , . \\ ] ] hence , defining @xmath610}\\hsigma_{ii}\\ge \\frac{1}{2}\\big\\ } \\ ] ] we have the desired probability bound ( [ eq : coropbound2 ] ) .",
    "let @xmath611 where @xmath612 is the lasso solution with @xmath285 . by theorem [ thm :",
    "deterministic ] , we have , for any @xmath613 @xmath614 and further @xmath615 whence , proceeding as in the proof in the last section , we get , for some universal numerical constant @xmath616 , @xmath617 next by eq .",
    "( [ eq : twobiasrelation ] ) we have @xmath618 hence , in order to prove eq .",
    "( [ eq : corobiasstd ] ) , it is sufficient to prove that @xmath619 .",
    "note that @xmath620 whenever @xmath621 and , and @xmath622 , and therefore ( letting @xmath623 ) @xmath624 with @xmath625 the standard normal distribution function , and in the last inequality we used the fact that @xmath196}\\hsigma_{ii}\\le 3/2 $ ] on @xmath626 .",
    "we then choose @xmath16 so that @xmath627 , for @xmath64 $ ] in the support of @xmath16 .",
    "we therefore obtain @xmath628 this finishes the proof of eq .",
    "( [ eq : corobiasstd ] ) .",
    "equations ( [ eq : corobiasstd_final ] ) and ( [ eq : corobiasstd_final2 ] ) are obtained by substituting @xmath249 and using eq .",
    "( [ eq : corobiasstd ] ) .",
    "let @xmath629 be the event defined as per theorem [ thm : event_thm].@xmath217 .",
    "in particular , we take @xmath208 , and @xmath209 ( for , instance @xmath630 will work for all @xmath1 large enough since @xmath631 , with @xmath291 , by assumption ) .",
    "further note that we can assume without loss of generality @xmath206 , since @xmath79 .",
    "fixing @xmath288 , we have therefore @xmath632 where @xmath633 is a constant defined as per theorem [ thm : event_thm].@xmath217 .",
    "we are therefore left with the task of bounding the first term in the last expression above , uniformly over @xmath634 , @xmath258 . for @xmath635",
    ", we can apply ( * ? ? ?",
    "* theorem 1.2 ) whereby ( using the notations of @xcite , with their @xmath636 replaced by @xmath637 ) @xmath638 , @xmath639 , @xmath640 , @xmath641 . by a straightforward manipulation of eq .",
    "( 13 ) in @xcite , we have , for @xmath642 , and @xmath643 ( letting @xmath644 the oracle estimator of @xmath56 introduced there ) @xmath645 where the last inequality follows for all @xmath1 large enough since @xmath646 .",
    "hence @xmath647 where we note that the right hand side is independent of @xmath16 .",
    "the first term vanishes as @xmath280 by a standard tail bound on the supremum of @xmath0 gaussian random variables .",
    "the second term also vanishes because it is controlled by the tail of a chi - squared random variable @xcite .",
    "by definition , letting @xmath648 , and fixing @xmath649 @xmath650\\setminus\\supp(\\theta_0 ) \\mbox { s.t . }",
    "\\frac{\\sqrt{n}\\ ,        |\\htheta^u_i-\\theta_{0,i}|}{\\hsigma [ m \\hsigma        m^\\st]_{i , i}^{1/2 } }   \\ge \\phi^{-1}\\big(1-\\frac{\\alpha}{2p}\\big)\\right\\}\\\\ & \\le \\sup_{\\theta_0\\in\\cf_{p , s_0 } } \\prob\\left\\{\\exists i\\in      [ p]\\setminus\\supp(\\theta_0 ) \\mbox { s.t . } \\frac{\\sqrt{n}\\ ,        |\\htheta^u_i-\\theta_{0,i}|}{\\sigma [ m \\hsigma        m^\\st]_{i , i}^{1/2 } }   \\ge ( 1-\\eps ) \\phi^{-1}\\big(1-\\frac{\\alpha}{2p}\\big)\\right\\}\\\\ & \\nonumber\\phantom{\\le\\le}+ \\sup_{\\theta_0\\in\\cf_{p , s_0 } } \\prob\\big(\\big|\\frac{\\hsigma}{\\sigma}-1\\big|\\ge\\frac{\\eps}{2}\\big)\\ , . \\ ] ] since the second term vanishes as @xmath280 by assumption eq .",
    "( [ eq : consistencysigma ] ) , it is sufficient to consider the first term . using bonferroni inequality , letting @xmath651 , we have @xmath652_{i , i}^{1/2 } }   \\ge z_\\alpha(\\eps ) \\right\\}\\\\ & =   \\underset{n\\to\\infty}{\\lim\\sup}\\,\\,\\sum_{i=1}^p \\sup_{\\theta_0\\in\\cf_{p , s_0},\\theta_{0,i}=0 } \\prob\\left\\ { \\left|\\tz_i+\\frac{\\delta_i}{\\sigma[m\\hsigma        m^{\\st}]_{ii}^{1/2}}\\right|\\ge z_\\alpha(\\eps)\\right\\ } \\ ] ] where , by theorem [ thm : main_thm ] , @xmath653 and @xmath229 is given by eq .",
    "( [ eq : generalrepresentationbis ] ) .",
    "we then have @xmath654}\\prob(\\hsigma_{ii}\\ge 2)\\nonumber\\\\   & \\,\\,\\,+\\underset{n\\to\\infty}{\\lim\\sup}\\,\\ ,   \\sup_{\\theta_0\\in\\cf_{p , s_0},\\theta_{0,i}=0}\\ , p\\ ,   \\prob\\left\\{\\|\\delta\\|_{\\infty}\\ge    \\frac{\\eps \\sigma}{4}\\right\\}\\ , , \\label{eq : bonferronilast } \\ ] ] where in the first inequality , we used @xmath655_{i , i}\\ge 1/(4\\hsigma_{ii})$ ] for all @xmath1 large enough , by lemma [ lem : missing_bound ] , and since @xmath538 as @xmath191 .",
    "now the second term in the right hand side of eq .",
    "( [ eq : bonferronilast ] ) vanishes by theorem [ thm : event_thm].@xmath217 , and the last term is zero by theorem [ thm : main_thm ] since @xmath656 . therefore @xmath657 and the claim follows by letting @xmath658 .",
    "j.  fan , s.  guo , and n.  hao , _ variance estimation using refitted cross - validation in ultrahigh dimensional regression _",
    ", journal of the royal statistical society : series b ( statistical methodology ) * 74 * ( 2012 ) , no .  1 , 14679868 .                                      j.  peng , j.  zhu , a.  bergamaschi , w.  han , d .- y .",
    "noh , j.  r. pollack , and p.  wang , _ regularized multivariate regression for identifying master predictors with application to integrative genomics study of breast cancer _ , the annals of applied statistics * 4 * ( 2010 ) , no .  1 , 5377 .                      r.  vershynin , _ introduction to the non - asymptotic analysis of random matrices _ , compressed sensing : theory and applications ( y.c . eldar and g.  kutyniok , eds . ) , cambridge university press , 2012 , pp .",
    "210268 .",
    "zhang and s.  s. zhang , _ confidence intervals for low - dimensional parameters in high - dimensional linear models _ , journal of the royal statistical society : series b ( statistical methodology ) * 76 * ( 2014 ) , 217242 ."
  ],
  "abstract_text": [
    "<S> fitting high - dimensional statistical models often requires the use of non - linear parameter estimation procedures . as a consequence , </S>",
    "<S> it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates . </S>",
    "<S> this in turn implies that it is extremely challenging to quantify the _ uncertainty _ </S>",
    "<S> associated with a certain parameter estimate . </S>",
    "<S> concretely , no commonly accepted procedure exists for computing classical measures of uncertainty and statistical significance as confidence intervals or @xmath0-values for these models .    </S>",
    "<S> we consider here high - dimensional linear regression problem , and propose an efficient algorithm for constructing confidence intervals and @xmath0-values . </S>",
    "<S> the resulting confidence intervals have nearly optimal size . </S>",
    "<S> when testing for the null hypothesis that a certain parameter is vanishing , our method has nearly optimal power .    </S>",
    "<S> our approach is based on constructing a ` de - biased ' version of regularized m - estimators . </S>",
    "<S> the new construction improves over recent work in the field in that it does not assume a special structure on the design matrix . </S>",
    "<S> we test our method on synthetic data and a high - throughput genomic data set about riboflavin production rate , made publicly available by  @xcite . </S>"
  ]
}