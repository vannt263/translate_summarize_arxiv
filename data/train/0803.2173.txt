{
  "article_text": [
    "consider the familiar linear regression model , @xmath3 where @xmath4 is an @xmath5-dimensional vector of responses , @xmath6 is the @xmath7 dimensional model matrix and @xmath8 is an @xmath5-dimensional vector of independent noise variables which are normally distributed , @xmath9 with variance @xmath10 .",
    "let some arbitrary subset of the regression coefficients , @xmath11 , be zero meaning that the corresponding regressors do not contribute to the response in the underlying model .",
    "the ordinary least squares ( ols ) estimate is obtained by minimizing the squared error loss .",
    "although an unbiased estimator in this setting , the ols estimator may have large variance and will incorrectly estimate the coefficients that are zero in the underlying model . as a consequence , the estimator will usually result in overly complex models which may be difficult to interpret .",
    "conventionally , an analyst will use subset selection to arrive at a reduced model , which is easier to interpret and attains better prediction accuracy .",
    "the subset selection problem has been studied extensively @xcite . in cases with large numbers of variables ,",
    "such methods suffer a fundamental limitation , the need for a greedy algorithm to search the discrete model space .",
    "a more direct approach to improve the prediction accuracy of ols is based on `` shrinkage '' estimators @xcite which trade off increased estimator bias in return for a compensating decrease in variance leading to a smaller mean squared error ( mse ) .",
    "a number of methods have been proposed that realize variable selection and estimation via some penalized least squares criteria , e.g. the nonnegative garotte @xcite , the lasso @xcite , the elastic net @xcite , the adaptive lasso @xcite and more recently the dantzig selector @xcite , etc . while traditional ridge regression @xcite proposes an @xmath12-penalty on the coefficients , the lasso , and its variants make use of the @xmath13-penalty .",
    "@xcite demonstrates that the @xmath13-polytope , unlike @xmath12 , can touch the contours of the least squares objective function on one or more of the axes leading to estimates of zero for the associated regression coefficients .",
    "@xcite synthesized several current ideas in the machine learning literature and offered two important hybrid algorithms for model selection and fitting in the kernel regression context .",
    "in particular , by combining kernel transformations of independent variables with classical elements of bayesian hierarchical modeling @xcite he created the relevance vector machine ( rvm ) .",
    "this approach is able to efficiently reduce the huge feature space created by the kernel transformations to a very parsimonious and predictive set improving significantly upon the less statistical support vector regression method of @xcite .",
    "this continuous approach typically converges in a finite number of steps and provides very fast model selection for large number of variables without the concerns of monte - carlo noise or incomplete optimization typical of subset selection .",
    "a number of extensions to the rvm have been offered ; see @xcite .    recognizing the rvm as a bayesian random effects model , we offer an alternative formulation which offers a more complete hierarchical structure .",
    "one major new development exploits this hierarchical structure to rapidly fit the model given a fixed value for the key hyper - parameter . unlike @xcite",
    ", we do not integrate the regression coefficients out of the joint posterior distribution of the parameters .",
    "instead , we use a conditional maximization procedure @xcite to obtain the posterior mode in an elegant and efficient way .",
    "this approach to model fitting was criticized by ( * ? ? ?",
    "* sec 8.3 ) in the context of standard linear mixed effects models due to the fact that it can lead to estimators of variance components that are identically zero and necessarily far from the posterior mean .",
    "contrary to harville s conclusion , we show that the zeroing effect is theoretically justified and can be easily exploited for variable selection .",
    "we refer to this procedure as the adaptive ridge selector ( aris ) . like the lasso , this results in a sparse shrinkage estimator which will zero irrelevant coefficients . the marginal likelihood @xmath14 is maximized over the hyper - parameter @xmath1 in order to select the best estimator .",
    "this final empirical bayes step adjusts the amount of shrinkage imposed on the model . like rvm",
    ", this algorithm typically converges in a finite number of steps and provides rapid and effective model selection and fitting for models with very large numbers of variables .",
    "section [ hierarchical model ] introduces a hierarchical random effects model similar to that of @xcite and motivates our interest .",
    "section [ optimize ] explains how the hierarchical structure is exploited to efficiently fit the model and describes the steps in the proposed aris algorithm .",
    "it also analyzes the problem in the form of a regularized least squares problem and contrasts the estimator with one based on the marginal distribution of the regression coefficients .",
    "next , section [ marginal ] focuses on deriving the marginal likelihood which is used to determine the optimal model . because the marginal likelihood is analytically intractable",
    ", one must compute it through either analytical or simulation based approximation .",
    "we provide a laplace approximation which is evaluated at the posterior mode along with a simulation based technique which results in somewhat more accurate solutions .",
    "section [ examples ] presents comparisons of the proposed method along with alternatives on simulated data examples .",
    "conclusions are discussed in section [ conclusions ] .",
    "beginning with a standard hierarchical linear model ( * ? ? ?",
    "* section 6.2.2 ) we propose a basic modification . in this case , the joint posterior of the parameters is proportional to , @xmath15 here a normal likelihood is assumed , @xmath16 , along with a conjugate normal prior on the regression coefficients , @xmath11 , and a typical jeffreys prior on the error variance @xmath17 , @xmath18 as with the relevance vector machine of @xcite , the vector @xmath19 where @xmath20 is a diagonal matrix with elements @xmath21 , @xmath22 the reciprocals of which are independent and identically distributed from a gamma distribution , @xmath23 where @xmath24 is the inverse scale parameter , and @xmath1 is the shape parameter . by definition @xmath25 , @xmath26 and @xmath27 .",
    "notice that when @xmath28 , this becomes an exponential distribution which we will consider as a special case .",
    "@xcite sets @xmath29 and @xmath30 which leads to a scale invariant improper prior .",
    "he then derives a marginal likelihood @xmath31 through direct integration which is then maximized with respect to @xmath10 and @xmath32 .",
    "hypothetically as the algorithm proceeds some @xmath21s will tend toward @xmath33 which correspond to the irrelevant variables in the model .",
    "@xcite does not check the validity of the joint posterior density having assumed an improper prior on @xmath32 .",
    "in contrast to @xcite , we choose not to integrate the regression coefficients out of the joint posterior distribution , but instead proceed to find the modal value given the data and @xmath1 . here",
    "we fix @xmath24 to be a very small number ( e.g. machine epsilon ) and adjust @xmath1 to control shrinkage .",
    "sparsity is obtained via the combination of these two particular priors , @xmath34 and @xmath35 .",
    "integration over @xmath32 in the joint prior distribution @xmath36 will reveal that the marginal prior density of the regression coefficients is a product of univariate @xmath37 densities , with a scale of @xmath38 and degrees of freedom of @xmath39 .",
    "it is important to note that the product of univariate @xmath37-densities is not equivalent to a multivariate @xmath37 and does not have elliptical contours but instead produces ridges along the axes .",
    "these ridges can be made more drastic by choosing the scale parameter to be small ; see figure [ aris_fig1 ] . then the posterior will be maximized where ever these ridges first touch the contours of the likelihood .",
    "the parameter @xmath1 plays a very similar role to the regularization parameter of the ridge regression , lasso , etc . , with larger values encouraging further shrinkage .",
    "hence the proposed hierarchical structure implies independent @xmath37 priors being placed on each regression coefficient .",
    "direct use of such @xmath37 priors would obscure the conjugate nature of the model . from an optimization perspective",
    ", a direct use of such priors leads to a non - convex objective function which would not be desirable .",
    "as we will discuss , within the hierarchical structure each iteration solves a simpler convex problem leading to an efficient solution .",
    "once @xmath32s are integrated out of the joint posterior , the problem can be seen analogously as a regularized least squares problem as @xmath40 which solves @xmath41    having specified a complete hierarchical model , the joint posterior distribution of the parameters is obtained by the product of the likelihood and the specified priors up to a normalizing constant as @xmath42 where @xmath43 .    given the priors in ( [ aris_eq2 ] ) , ( [ aris_eq3 ] ) , ( [ aris_eq4 ] ) , the product of these prior densities and the normal likelihood , ( [ aris_eq10 ] ) is the kernel of a posterior density function for @xmath11 , @xmath10 , and @xmath44 .",
    "[ thm1 ]    proof for this theorem can be found in appendix [ apppost ] .",
    "the conditional distributions of the parameters can now easily be derived from this joint distribution .    *",
    "the regression coefficients are distributed as multivariate normal conditional on the error variance @xmath10 and the prior covariance of the regression coefficients @xmath20 .",
    "@xmath45 where @xmath46 and @xmath47 . *",
    "the error variance is distributed as inverse gamma conditional upon all other parameters .",
    "@xmath48 thus , @xmath49 where @xmath50 and @xmath51 . * the prior precisions of the regression coefficients , conditional on all other parameters , follow a gamma distribution .",
    "@xmath52 @xmath53 thus , @xmath54 where @xmath55 and @xmath56 .    deriving the full set of conditional distributions",
    "has several uses .",
    "as is frequently done , we may utilize these to simulate from the posterior distribution using gibbs sampling .",
    "such an approach would allow us to compute traditional bayes estimators for the regression coefficients . in section [ optimize ]",
    "we show how to use the conditional distributions to maximize the joint posterior in a surprisingly simple and effective way .",
    "maximization will also facilitate computation of the laplace approximation to the marginal likelihood @xcite .",
    "@xcite proposed an optimization algorithm to find the joint posterior modes ; see also @xcite .",
    "once the fully conditional densities of the model parameters are obtained , it is possible to maximize the joint posterior distribution by iteratively maximizing these conditional densities .",
    "since the conditional posterior distributions obtained in equations ( [ aris_eq11 ] ) , ( [ aris_eq13 ] ) , and ( [ aris_eq15 ] ) are well - known distributions with readily available modes , the lindley - smith optimization algorithm becomes rather appealing to implement .",
    "the modes for the distributions in equations ( [ aris_eq11 ] ) , ( [ aris_eq13 ] ) , and ( [ aris_eq15 ] ) respectively are @xmath57 where @xmath58 , @xmath59 were defined in ( [ aris_eq13 ] ) .",
    "the maximization proceeds through sequential re - estimation of @xmath60 , @xmath61 , @xmath62 , where @xmath63 , @xmath64 is the number of iterations , and @xmath65 is the ols estimator .      given @xmath20 , the modal value of @xmath11 can be obtained as a solution to a penalized least squares problem as with ridge regression . since we have an iterative procedure , let @xmath66 be the @xmath67th diagonal of @xmath68 at the @xmath69th iteration and be given .",
    "then the @xmath69th iterate for @xmath11 is the solution to a similar penalized least squares problem : @xmath70 if we substitute @xmath21 with the estimate from ( [ aris_eq16 ] ) and let @xmath40 , we obtain @xmath71 where @xmath72 .",
    "this procedure is essentially re - weighting the predictor variables by the positive square root of the ratio between the current estimate of the coefficients and the residual variance due to them .",
    "after this re - weighting procedure , the problem takes the form of a standard ridge regression , @xmath73 where @xmath74 , @xmath75 and @xmath76 .",
    "the solution to the problem above at iteration @xmath69 is given by @xmath77 hence the mode is computed through a sequence of re - weighted ridge estimators . the final estimate @xmath78 then can be recovered as @xmath79 ( this multiplication is understood component wise ) .",
    "note that when @xmath80 , this procedure results in the ols estimator .",
    "we construct a two - dimensional example to illustrate the method .",
    "consider the model @xmath81 , where @xmath82 and @xmath83 .",
    "we generate 30 observations and run aris for @xmath28 .",
    "figure [ fig2 ] clearly demonstrates how the shrinkage proceeds throughout our algorithm .",
    "the constrained region eventually becomes singular along the dimension which has no contribution to the response in the underlying model .",
    "aris iteratively updates the constrained region and converges to a solution .    ]",
    "an expectation - maximization approach may be used to obtain the marginal posterior mode of @xmath11 .",
    "consider the identity @xmath84 taking the logarithm and then taking the expectations of both sides with respect to @xmath85 yields @xmath86 where @xmath87 is the current guess of @xmath11 ( * ? ?",
    "the em algorithm involves working with the first term of ( [ aris_eq40 ] ) .",
    "the em procedure in our case would consist of the following two steps : ( i ) expectation of @xmath88 with respect to @xmath85 , ( ii ) maximization of the expected value with respect to @xmath11 .",
    "an iterative procedure results by replacing the initial guess @xmath87 with the solution of the maximization procedure @xmath89 and repeating ( i ) and ( ii ) until convergence .    with a slight change in the hierarchical model used , the above expectation will become quite trivial .",
    "unlike ( [ aris_eq2 ] ) , let us not condition the prior density of @xmath11 on @xmath10 .",
    "under such a setup , @xmath90 . notice that the conditional posteriors in ( [ aris_eq13 ] ) and ( [ aris_eq15 ] ) now become @xmath91 @xmath92 given the new prior , let us re - write ( [ aris_eq10 ] ) in the log form excluding the terms that do not depend on @xmath11 , @xmath10 and @xmath44 : @xmath93 next we compute @xmath94 as @xmath40 : @xmath95 where @xmath96 .",
    "having completed the expectation step , the maximization of ( [ aris_eq43 ] ) with respect to @xmath11 yields an estimator as the solution to the following sequence of convex minimization problems : @xmath97 hence , the marginal posterior mode of @xmath11 is extremely similar in form to the joint posterior mode .",
    "note that we can still adopt the weighting perspective mentioned earlier .",
    "recall from section [ hierarchical model ] that the integration over @xmath32 in the prior distribution of @xmath98 results in a univariate @xmath37-density with degrees of freedom @xmath39 .",
    "therefore , a value of @xmath99 will actually lead to a flat prior over @xmath98 resulting in the ols estimator ( note that when @xmath99 , the kernel of the @xmath37 density has power @xmath33 resulting in a flat density ) . also , the solution to the marginal when @xmath29 will be identical to the maximization of the joint posterior when @xmath28 .",
    "@xcite mentions that the estimator of @xcite based on joint maximization may be far from the bayes estimator and suggests that the maximization of the marginal mode of the variance components would be a superior approach .",
    "above we have shown that in our case the mode of the marginal density has the same form as the joint mode justifying the conditional maximization approach .",
    "in fact , tipping adopts the approach suggested by harville , maximizing the joint posterior mode of @xmath44 and @xmath10 after integrating over @xmath11 but still achieves the zeroing effect .",
    "we needed a slight change in the model to ease our work for the expectation step above , that is , we made the prior distribution of the regression coefficients independent of the error variance .",
    "one may think that while we were trying to show the equivalence of these two solutions ( the joint and the marginal solutions ) , we actually created two different models and show that their solutions are identical in form , yet , they do not follow the same model . in the traditional bayesian analysis of the linear regression models ,",
    "the regression coefficients are conditioned over the noise variance .",
    "this provides an estimator for the regression coefficients that is independent of the noise variance .",
    "we followed this convention when we were forming our hierarchical model .",
    "however , in our case , there is no such thing as independence between the solutions of the regression coefficients and the noise variance .",
    "although in an explicit statement such as ( [ aris_eq16c ] ) it may seem that the solution for the regression coefficients does not depend on the error variance , there exists an implicit dependence through the solution of @xmath32s .",
    "we could have very well constructed our hierarchical model using a prior on regression coefficients independent of the noise variance .",
    "this would lead to a solution that is only slightly different .",
    "the re - estimation equation in ( [ aris_eq16c ] ) , ( [ aris_eq16b ] ) and ( [ aris_eq16 ] ) would become @xmath100 now , the solution for @xmath21 is independent of @xmath10 , yet the solution of @xmath11 explicitly depends on @xmath10 .",
    "that said , the implicit dependence has become an explicit one .",
    "thus the iterative solution for the regression coefficients as @xmath40 can be written as @xmath101 in which , apart from the tuning quantity ( @xmath102 ) , the only difference with ( [ aris_eq44 ] ) is the plug - in estimator used for the noise variance .",
    "having shown that these procedures are fundamentally identical in form to each other , let us discuss another important point , the choice of initial values to start the algorithm .",
    "let us consider the solution following ( [ aris_eq44 ] ) with @xmath29 : @xmath103\\right)^{-1}\\mathbf{x}'\\mathbf{y } \\label{aris_eq44b}\\ ] ] we can not just plug any @xmath104 as an initial estimator . consider @xmath105 . in this case all the regression coefficients will be zeroed . or let only a subset of @xmath104 be zero . then in the solution those components will remain zero . although we are solving a series of simple convex problems , the dependency of the solution to the initial value proves that here we are dealing with a multi - modal objective function as would be expected . thus using the ols estimator as an initial value will take us to a local stationary point which is most likely under the support of the data in hand .    to gain further intuition ,",
    "let us consider an orthogonal case and let the predictors be scaled so that they have unit @xmath106-norm , i.e. @xmath107 .",
    "in such a case the ols estimator for @xmath11 would have a variance - covariance matrix @xmath108 where @xmath109 is a plug - in estimator , e.g. the maximum likelihood estimator or the bias corrected estimator . testing the null hypothesis @xmath110 , a @xmath37-statistic can be computed for a component @xmath111 as @xmath112 .",
    "notice in ( [ aris_eq44b ] ) the quantities at the diagonal of the second piece under the matrix inverse operation , @xmath113 , resemble the inverse of a squared @xmath37-statistic .",
    "in fact , recall from section [ rrls ] that we formed a sequence of ridge regression problems out of this procedure by re - weighting our predictors by @xmath114 which is the absolute value of a @xmath37-statistic .",
    "thus , following a conventional testing procedure , those predictors which correspond to coefficients with larger @xmath37-statistics will be given more importance .",
    "this is yet another point that intuitively explains our procedure .",
    "critical to the aris procedure is the choice of the hyper - parameter @xmath1 .",
    "we propose an empirical bayes estimation of @xmath1 through the maximization of the marginal likelihood @xmath14 .",
    "hence , we must integrate the joint posterior over all parameters , @xmath115 where @xmath116 . in the case of the hierarchical model developed in section [ hierarchical model ] ,",
    "the direct calculation is intractable .",
    "below we propose both analytical and simulation - based approximations .",
    "a standard analytical approximation of the marginal likelihood can be computed using the laplacian method @xcite .",
    "the approximation is obtained as @xmath117+\\frac{p}{2}\\log(2\\pi)-\\frac{1}{2}\\log\\left|\\mathbf{h}_{\\widetilde{\\boldsymbol\\theta}}\\right| , \\label{eq13d}\\ ] ] where @xmath118 is the mode of the joint posterior and @xmath119 is the hessian matrix given in [ app ] evaluated at the posterior mode .",
    "recall that the aris is designed to drive the values of @xmath98 and @xmath21 to zero for those independent variables @xmath120 which provide no explanatory value . as @xmath40 , the prior precisions and the regression coefficients related to irrelevant independent variables will tend toward @xmath121 and @xmath33 respectively .",
    "in fact we can see in ( [ aris_eq21 ] ) that along these @xmath98 the curvature approaches @xmath121 as we converge to the solution thus driving their variance to 0 . at the joint posterior mode",
    ", the corresponding dimensions of @xmath6 do not contribute and become irrelevant . under the support of the data , we claim these variables to be insignificant and suggest their removal from the model .",
    "the integration follows removal of these irrelevant variables from the model .",
    "the resulting laplace approximation to the log - marginal likelihood is @xmath122 where @xmath123 represents the reduced model after the removal of the irrelevant variables at the mode .",
    "laplace approximation may not perform well in certain cases as will be seen in section [ examples ] .",
    "@xmath11 and @xmath10 can be analytically integrated out of the joint posterior given in eq .",
    "[ aris_eq10 ] .",
    "the resulting likelihood conditioned on the prior variances is , @xmath124 where @xmath125 see also ( * ? ? ?",
    "* equations  3.11,3.12 ) .",
    "the marginal likelihood conditioned over @xmath1 can now be obtained through integration as @xmath126 $ ] where the expectation is taken over the prior distribution of @xmath44 .    in order to ensure efficient sampling , we define a hypercube around the mode of the joint posterior in order to obtain a sampling region over @xmath127 .",
    "the sampling region is the set @xmath128 where @xmath129 is the modal value of @xmath32 , @xmath130 is the square root of inverse curvature at the mode and @xmath131 is to be chosen to adjust the width of the box .",
    "this section reports the results of a simulation study comparing the aris estimates with a number of computationally efficient penalized least squares methods . in the study",
    "we consider a model of the form @xmath132 . for each data",
    "set , we center @xmath4 and scale the columns of @xmath6 so that they have unit @xmath106-norm .",
    "the lasso and elastic net were fit using the ` lars ` and ` elasticnet ` libraries in ` r ` .",
    "_ model 0 _ : this model is adopted from @xcite and is a special case where the lasso estimate fails to improve asymptotically .",
    "the true regression coefficients are @xmath133 .",
    "the predictors @xmath134 ( @xmath135 ) are iid @xmath136 where @xmath137 is defined in @xcite ( corollary 1 , pg .",
    "1420 ) with @xmath138 and @xmath139 . under this scenario",
    ", @xmath137 does not allow consistent lasso selection . in this context",
    "@xcite proposes the adaptive lasso ( adalasso ) for consistent model selection . in this setting",
    ", we simulate 1000 data sets from the above model for different combinations of sample size and error variance .",
    "table [ tab1 ] reports the proportion of the cases where the solution paths included the true model for aris , lasso and adaptive lasso .",
    "we also report the results of aris in the special case when @xmath28 .",
    "the results indicate that the aris algorithm performs nearly as well as the adaptive lasso and far better than the ordinary lasso in terms of consistent model selection under this particular setting . for @xmath28 ,",
    "the aris produces a consistent estimate and does not require a search over the solution path . for medium and large values of @xmath5 we can see that it significantly outperforms the lasso .",
    "results for the lasso and adalasso agree with those of @xcite .",
    "we next compare prediction accuracy and model selection consistency using the following three models which are drawn from @xcite .",
    "_ model 1 _ : in this example , we let @xmath140 with iid normal predictors @xmath134 ( @xmath135 ) . the pairwise correlation between the predictors @xmath120 and @xmath141 are adjusted to be @xmath142 .",
    "_ model 2 _ : we use the same setup as model 1 with @xmath143 for all @xmath67 .",
    "_ model 3 _ : we use the same setup as model 1 with @xmath144 .",
    "we test models 1,2 , and 3 for two different sample sizes , @xmath145 and two noise levels @xmath146 .",
    "this experiment is conducted 100 times under each setting . in table",
    "[ tab2 ] , we report the median prediction error ( mse ) on a test set of 10,000 observation for each of the 100 cases .",
    "the values in the parentheses give the bootstrap standard error of the median mse values obtained .",
    "c , i and cm respectively stand for the number of correct predictors chosen , number of incorrect predictors chosen and the proportion of cases ( out of 100 ) where the correct model was found by the method .",
    "the bootstrap standard error was calculated by generating 500 bootstrap samples from 100 mse values , finding the median mse for each case , and then calculating the standard error of the median mse .",
    "lasso , adalasso , elastic net , nonnegative garrote , ridge and ordinary least squares estimates are computed along with the aris estimate . for the ridge estimator",
    ", the ridge parameter is determined by a gcv ( generalized cross - validation ) type statistic , while for all the others we use 10-fold cross - validation for the choice of the tuning parameters .",
    "we also consider the lasso where the tuning parameter is chosen by the method of @xcite .",
    "aris hyper - parameter @xmath1 is determined both by the laplace approximation and the numerical integration to the marginal likelihood .",
    "we also report the results for the particular case of @xmath28 . in each example the numerical integration step of aris - eb is carried out for values of @xmath147 and only the best result is reported .",
    "this is a rather arbitrary choice and will depend upon the number of samples drawn .",
    "model 3 is the only example where the same value of @xmath131 is consistently chosen ( @xmath148 ) .",
    ".results for model 0 . [",
    "cols=\"<,^,^,^\",options=\"header \" , ]     [ tab2 ]    model 3 demonstrates the most striking feature of the aris algorithm , the ability to identify the correct model under sparse setups . when utilized along with the empirical bayes step , it is able to identify the correct model in a very large proportion of cases with very low prediction error .",
    "this is especially surprising for the cases where @xmath149 ( @xmath146 ) . in the case where @xmath150 and @xmath28 the algorithm still outperform all other methods in terms of correct model choice and mse .    among all the variants of lasso ( lasso , adalasso , elastic net , lasso(cml ) ) , lasso(cml )",
    "is optimal in terms of prediction accuracy . in the case of @xmath150 ,",
    "its prediction error is almost identical to that of aris(@xmath28 ) but correct model identification is strongly weaker . observe that a cross - validation approach may not accurately choose the tuning parameter for the lasso - variants .",
    "for example , as we moved from @xmath149 to @xmath150 , the proportion of cases where the correct model was chosen decreased for all the lasso - variants except lasso(cml ) where the tuning parameter is chosen via an empirical bayes step similar to our approach .",
    "the nonnegative garrote estimator performs quite poorly in this situation along with the ridge and ols estimators .",
    "results indicate that aris provides superior performance for model 3 .    in the case of model 1 , for @xmath150 and @xmath151",
    ", aris performs best in terms of prediction accuracy and strongly outperforms other algorithms in terms of model selection accuracy .",
    "aris(@xmath28 ) outperformed other versions which required a search over the solution path .",
    "both the laplace approximation and the numerical integration fail to detect this value of @xmath1 . for the case of @xmath149",
    ", aris performs within a standard error of all the other estimators in terms of prediction accuracy , yet does better in terms of model selection accuracy .",
    "the ridge estimator does almost as well as the lasso - variants in terms of prediction accuracy .",
    "similar results follow for the case @xmath150 , @xmath152",
    ". however , elastic net seems to have slightly lower prediction error .",
    "the case @xmath149 , @xmath152 shows fairly weak results across all estimators .",
    "model 2 demonstrates the biggest weakness of the aris and several other estimators .",
    "when there are many small effects present in the underlying model , these estimators do not perform well since they favor parsimony . for all cases",
    "the clear winner is the ridge estimator .",
    "we have introduced a bayesian model fitting and variable selection method , aris , which makes use of a hierarchical model and enforces parsimony .",
    "the method combines an efficient optimization procedure which is tailored to the fully conditional posterior densities with various techniques to derive and maximize the marginal likelihood . this development , although radically different in detail , is similar in spirit to modern implementation of the lasso which has been described as a bayesian procedure which combines a normal likelihood with a laplace prior on the regression coefficients ; see @xcite .",
    "considering the simulation results of section [ examples ] , we note two key features of the aris : ( i ) its superior prediction and model selection accuracy when the underlying model is sparse , and ( ii ) the significant improvement in performance accompanying an increase in the sample size indicating asymptotic consistency .",
    "computationally , for a specific @xmath1 value , aris requires one matrix inversion at each iteration .",
    "this point is obvious from the description of the method as a series of ridge regressions .",
    "thus the computational cost for each iteration of aris is at most @xmath153 . in practice , because variables are eliminated throughout the procedure , the cost often decreases dramatically with each iteration .",
    "our experiments indicate fast convergence of this procedure across sample sizes .",
    "lasso methods offer a computational advantage due to the _ lars _ algorithm @xcite which can compute the entire solution path of the lasso with the cost of a single ols estimator .",
    "however , our experimental results indicate that these methods are often inferior in terms of model selection and prediction accuracy .",
    "large scale experiments have shown that the procedure remains computationally feasible in situations where the number of predictor variables is very large .",
    "hence the proposed method offers the most advantage in problems where one is attempting to select a small or moderate number of variables from a large initial group , a common situation in many modern statistical and data mining applications .",
    "an open issue is the empirical bayes step via numerical integration . due to the large scale simulations throughout our experiments",
    "we have only drawn @xmath154 samples for the integration of @xmath44 . obviously in practice a much larger set of samples could be drawn at little additional cost particularly for sparse models .",
    "the process will become more stable as we draw larger samples .",
    "in such a case , the choice of @xmath131 may just be fixed at a larger value , i.e. @xmath148 .",
    "the authors would like to express their appreciation to robert mee and william m. briggs for their suggestions which have significantly added to the clarity of the manuscript .",
    "@xmath11 and @xmath10 can tractably be integrated out of ( [ aris_eq10 ] ) . as a result of this integration ,",
    "the only remaining terms that are dependent upon @xmath32 are @xmath155 it will suffice to show that ( [ app - eq1 ] ) is finitely integrable with respect to @xmath32 .",
    "@xmath156 and @xmath157 eliminating the terms again that are not dependent upon @xmath21 , we reduce ( [ app - eq1 ] ) to @xmath158 integrating ( [ app - eq2 ] ) is equivalent to @xmath159 .",
    "this expectation is taken with respect to ( [ aris_eq4 ] ) and is finite for @xmath26 .",
    "@xmath162 \\label{aris_eq25 } \\\\",
    "-\\frac{\\partial^{2}}{\\partial\\sigma^{2}\\partial v_{k}^{-1}}\\log p\\left(\\mathbf{y},\\boldsymbol\\theta|\\eta\\right ) & = & -\\frac{\\beta_{k}^{2}}{2\\sigma^{4}}. \\label{aris_eq26}\\end{aligned}\\ ] ]                  drucker , h. , burges , c. j.  c. , kaufman , l. , smola , a. , and vapnik , v. `` support vector regression machines . '' in mozer , m.  c. , jordan , m.  i. , and petsche , t. ( eds . ) , _ advances in neural information processing systems _ , volume  9 , 155 . the mit press ( 1997 ) .",
    "dsouza , a. , vijayakumar , s. , and schaal , s. `` the bayesian backfitting relevance vector machine . '' in _",
    "icml 04 : proceedings of the twenty - first international conference on machine learning _ , 31 ( 2004 ) ."
  ],
  "abstract_text": [
    "<S> we introduce a new shrinkage variable selection operator for linear models which we term the _ adaptive ridge selector _ ( aris ) . </S>",
    "<S> this approach is inspired by the _ relevance vector machine _ ( rvm ) , which uses a bayesian hierarchical linear setup to do variable selection and model estimation . extending the rvm algorithm </S>",
    "<S> , we include a proper prior distribution for the precisions of the regression coefficients , @xmath0 , where @xmath1 is a scalar hyperparameter . </S>",
    "<S> a novel fitting approach which utilizes the full set of posterior conditional distributions is applied to maximize the joint posterior distribution @xmath2 given the value of the hyper - parameter @xmath1 . </S>",
    "<S> an empirical bayes method is proposed for choosing @xmath1 . </S>",
    "<S> this approach is contrasted with other regularized least squares estimators including the lasso , its variants , nonnegative garrote and ordinary ridge regression . </S>",
    "<S> performance differences are explored for various simulated data examples . </S>",
    "<S> results indicate superior prediction and model selection accuracy under sparse setups and drastic improvement in accuracy of model choice with increasing sample size .    </S>",
    "<S> * artin armagan * + .05 in department of statistics , operations , and management science + the university of tennessee + knoxville , tn , 37996 + _ e - mail : _ aarmagan@utk.edu .1 in * and * + .1 in * russell l. zaretzki * + .05 in department of statistics , operations , and management science + the university of tennessee + knoxville , tn , 37996 + _ e - mail : _ rzaretzk@utk.edu + .1 in    keywords : lasso ; elastic net ; shrinkage estimation ; rvm ; ridge regression ; lars algorithm ; penalized least squares . </S>"
  ]
}