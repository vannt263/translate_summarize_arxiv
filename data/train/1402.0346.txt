{
  "article_text": [
    "understanding , modeling and predicting the volatility of financial time series has been extensively researched for more than 30 years and the interest in the subject is far from decreasing .",
    "volatility prediction has a very wide range of applications in finance , for example , in portfolio optimization , risk management , asset allocation , asset pricing , etc .",
    "the two most popular approaches to model volatility are based on the autoregressive conditional heteroscedasticity ( arch ) type and stochastic volatility ( sv ) type models .",
    "the seminal paper of @xcite proposed the primary arch model while @xcite generalized the purely autoregressive arch into an arma - type model , called the generalized autoregressive conditional heteroscedasticity ( garch ) model .",
    "since then , there has been a very large amount of research on the topic , stretching to various model extensions and generalizations .",
    "meanwhile , the researchers have been addressing two important topics : looking for the best specification for the errors and selecting the most efficient approach for inference and prediction .",
    "besides selecting the best model for the data , distributional assumptions for the returns are equally important .",
    "it is well known , that every prediction , in order to be useful , has to come with a certain precision measurement . in this way",
    "the agent can know the risk she is facing , i.e.  uncertainty .",
    "distributional assumptions permit to quantify this uncertainty about the future .",
    "traditionally , the errors have been assumed to be gaussian , however , it has been widely acknowledged that financial returns display fat tails and are not conditionally gaussian .",
    "therefore , it is common to assume a student - t distribution , see @xcite , @xcite and @xcite , among others .",
    "however , the assumption of gaussian or student - t distributions is rather restrictive .",
    "an alternative approach is to use a mixture of distributions , which can approximate arbitrarily any distribution given a sufficient number of mixture components .",
    "a mixture of two normals was used by @xcite , @xcite and @xcite , among others .",
    "these authors have shown that the models with the mixture distribution for the errors outperformed the gaussian one and do not require additional restrictions on the degrees of freedom parameter as the student - t one .    as for the inference and prediction ,",
    "the bayesian approach is especially well - suited for garch models and provides some advantages compared to classical estimation techniques , as outlined by @xcite .",
    "firstly , the positivity constraints on the parameters to ensure positive variance , may encumber some optimization procedures . in the bayesian setting",
    ", constraints on the model parameters can be incorporated via priors . secondly , in most of the cases we are more interested not in the model parameters directly , but in some non - linear functions of them . in the maximum likelihood ( ml )",
    "setting , it is quite troublesome to perform inference on such quantities , while in the bayesian setting it is usually straightforward to obtain the posterior distribution of any non - linear function of the model parameters .",
    "furthermore , in the classical approach , models are usually compared by any other means than the likelihood . in the bayesian setting , marginal likelihoods and bayes factors allow for consistent comparison of non - nested models while incorporating occam s razor for parsimony .",
    "also , bayesian estimation provides reliable results even for finite samples . finally , @xcite add that the ml approach presents some limitations when the errors are heavy tailed , also the convergence rate is slow and the estimators may not be asymptotically gaussian .",
    "this survey reviews the existing bayesian inference methods for univariate and multivariate garch models while having in mind their error specifications .",
    "the main emphasis of the paper is on the recent development of an alternative inference approach for these models using bayesian non - parametrics .",
    "the classical parametric modeling , relying on a finite number of parameters , although so widely used , has some certain drawbacks .",
    "since the number of parameters for any model is fixed , one can encounter underfitting or overfitting , which arises from the misfit between the data available and the parameters needed to estimate . then , in order to avoid assuming wrong parametric distributions , which may lead to inconsistent estimators , it is better to consider a semi- or non - parametric approach . bayesian non - parametrics may lead to less constrained models than classical parametric bayesian statistics and provide an adequate description of the data , especially when the conditional return distribution is far away from gaussian .",
    "up to our knowledge , there has been a very few papers using bayesian non - parametrics for garch models .",
    "these are @xcite for univariate garch , @xcite and @xcite for mgarch .",
    "all of them have considered infinite mixtures of gaussian distributions with a dirichlet process ( dp ) prior over the mixing distribution , which results into dp mixture ( dpm ) models .",
    "this approach so far proves to be the most popular bayesian non - parametric modeling procedure .",
    "the results over the papers have been consistent : the bayesian non - parametric approach leads to more flexible models and is better in explaining heavy - tailed return distributions , which parametric models can not fully capture .",
    "the outline of this survey is as follows .",
    "section [ section : univ_garch ] shortly introduces univariate garch models and different inference and prediction methods .",
    "section [ section : mult_garch ] overviews the existing models for multivariate garch and different inference and prediction approaches .",
    "section [ section : non_param ] introduces the bayesian non - parametric modeling approach and reviews the limited literature of this area in time - varying volatility models .",
    "section [ section : illustration ] presents a real data application .",
    "finally , section [ section : conclusions ] concludes .",
    "as mentioned before , the two most popular approaches to model volatility are garch type and sv type models . in this survey",
    "we focus on garch models , therefore , sv models will not be included thereafter .",
    "also , we are not going to enter into the technical details of the bayesian algorithms and refer to @xcite for a more detailed description of bayesian techniques .",
    "the general structure of an asset return series modeled by a garch - type models can be written as : @xmath0 where @xmath1 $ ] is the conditional mean given @xmath2 , the information up to time @xmath3 , @xmath4 is the mean corrected returns of the asset at time @xmath5 , @xmath6 $ ] is the conditional variance given @xmath2 and @xmath7 is the standard white noise shock .",
    "there are several ways to model the conditional mean , @xmath8 .",
    "the usual assumptions are to consider that the mean is either zero , equal to a constant ( @xmath9 ) , or follows an arma(@xmath10,@xmath11 ) process .",
    "however , sometimes the mean is also modeled as a function of the variance , say @xmath12 , which leads to the garch - in - mean models . on the other hand , the conditional variance , @xmath13 ,",
    "is usually modeled using the garch - family models . in the basic garch model the conditional variance of the returns depends on a sum of three parts : a constant variance as the long - run average , a linear combination of the past conditional variances and a linear combination of the past mean squared returns . for instance , in the garch(1,1 ) model , the conditional variance at time @xmath14 is given by @xmath15 , for @xmath16 .",
    "there are some restrictions which have to be imposed such as @xmath17 , @xmath18 for positive variance , and @xmath19 for the covariance stationarity .",
    "@xcite proposed the exponential garch ( egarch ) model that acknowledges the existence of asymmetry in the volatility response to the changes in the returns , sometimes also called the `` leverage effect '' , introduced by @xcite .",
    "negative shocks to the returns have a stronger effect on volatility than positive .",
    "other arch extensions that try to incorporate the leverage effect are the gjr model by @xcite and the tgarch of @xcite , among many others . as @xcite puts it",
    ", `` there is now an alphabet soup '' of arch family models , such as aarch , aparch , figarch , starch etc , which try to incorporate such return features as fat tails , volatility clustering and volatility asymmetry .",
    "papers by @xcite , @xcite , @xcite , @xcite provide extensive reviews of the existing arch - type models .",
    "@xcite review arch type models , discuss their extensions , estimation and testing , also numerous applications .",
    "also , one can find an explicit review with examples and applications concerning garch - family models in @xcite and chapter 1 in @xcite .",
    "the main estimation approach for garch - family models is the classical maximum likelihood method .",
    "however , recently there has been a rapid development of bayesian estimation techniques , which offer some advantages compared to the frequentist approach as already discussed in the introduction .",
    "in addition , in the empirical finance setting , the frequentist approach presents an uncertainty problem .",
    "for instance , optimal allocation is greatly affected by the parameter uncertainty , which has been recognized in a number of papers , see @xcite and @xcite , among others .",
    "these authors conclude that in the frequentist setting the estimated parameter values are considered to be the true ones , therefore , the optimal portfolio weights tend to inherit this estimation error",
    ". however , instead of solving the optimization problem on the basis of the choice of unique parameter values , the investor can choose the bayesian approach , because it accounts for parameter uncertainty , as seen in @xcite and @xcite , for example .",
    "a number of papers in this field have explored different bayesian procedures for inference and prediction and different approaches to modeling the fat - tailed errors and/or asymmetric volatility .",
    "the recent development of modern bayesian computational methods , based on monte carlo approximations and mcmc methods have facilitated the usage of bayesian techniques , see e.g.  @xcite .",
    "the standard gibbs sampling procedure does not make the list because it can not be used due to the recursive nature of the conditional variance : the conditional posterior distributions of the model parameters are not of a simple form .",
    "one of the alternatives is the _ griddy - gibbs _ sampler as in @xcite .",
    "they discuss that previously used importance sampling and metropolis algorithms have certain drawbacks , such as that they require a careful choice of a good approximation of the posterior density .",
    "the authors propose a griddy - gibbs sampler which explores analytical properties of the posterior density as much as possible . in this paper",
    "the garch model has student - t errors , which allows for fat tails .",
    "the authors choose to use flat ( uniform ) priors on parameters @xmath20 with whatever region is needed to ensure the positivity of variance , however , the flat prior for the degrees of freedom can not be used , because then the posterior density is not integrable .",
    "instead , they choose a half - right side of cauchy .",
    "the posteriors of the parameters were found to be skewed , which is a disadvantage for the commonly used gaussian approximation . on the other hand , @xcite modeled the errors of a garch model with a mixture of two gaussian distributions .",
    "the advantage of this approach compared to that of student - t errors , is that if the number of the degrees of freedom is very small ( less than 5 ) , some moments may not exist .",
    "the authors have chosen flat priors for all the parameters , and discovered that there is little sensitivity to the change in the prior distributions ( from uniform to beta ) , unlike in @xcite , where the sensitivity for the prior choice for the degrees of freedom is high .",
    "more articles using a griddy - gibbs sampling approach are by @xcite , who have modeled asymmetric volatility with gaussian innovations and have used uniform priors for all the parameters , and by @xcite , who explored an asymmetric garch model with student - t errors .",
    "another mcmc algorithm used in estimating garch model parameters , is the _ metropolis - hastings _ ( mh ) method , which samples from a candidate density and then accepts or rejects the draws depending on a certain acceptance probability .",
    "@xcite modeled the errors as gaussian distributed with zero mean and unit variance while the priors are chosen as gaussian and a mh algorithm is used to draw samples from the joint posterior distribution .",
    "the author has carried out a comparative analysis between ml and bayesian approaches , finding , as in other papers , that some posterior distributions of the parameters were skewed , thus warning against the abusive use of the gaussian approximation .",
    "also , @xcite has performed a sensitivity analysis of the prior means and scale parameters and concluded that the initial priors in this case are vague enough .",
    "this approach has been also used by @xcite , @xcite and @xcite , among others .",
    "a special case of the mh method is the random walk metropolis - hastings ( rwmh ) where the proposal draws are generated by randomly perturbing the current value using a spherically symmetric distribution .",
    "a usual choice is to generate candidate values from a gaussian distribution where the mean is the previous value of the parameter and the variance can be calibrated to achieve the desired acceptance probability .",
    "this procedure is repeated at each mcmc iteration .",
    "@xcite have also carried out a comparison of estimation approaches , griddy - gibbs , rwmh and ml . apparently , rwmh has difficulties in exploring the tails of the posterior distributions and ml estimates may be rather different for those parameters where posterior distributions are skewed .    in order to select one of the algorithms",
    ", one might consider some criteria , such as fast convergence for example .",
    "@xcite numerically compares some of these approaches in the context of garch .",
    "the griddy - gibbs method is capable in handling the shape of the posterior by using smaller mcmc outputs comparing with other methods , also , it is flexible regarding parametric specification of a model .",
    "however , it can require a lot of computational time .",
    "this author also investigates mh , adaptive rejection metropolis sampling ( arms ) , proposed by @xcite , and acceptance - rejection mh algorithms ( armh ) , proposed by @xcite . for more in detail about each method in garch models see @xcite and @xcite , among others . using simulated data , @xcite calculated geometric averages of inefficiency factors for each method .",
    "inefficiency factor is just an inverse of @xcite efficiency factor . according to this ,",
    "the armh algorithm performed the best .",
    "also , computational time was taken into consideration , where armh clearly outperformed mh and arms , while griddy - gibbs stayed just a bit behind .",
    "the author observes that even though the armh method showed the best results , the posterior densities for each parameter did not quite explore the tails of the distributions , as desired . in this case",
    "griddy - gibbs performs better ; also , it requires less draws than armh .",
    "@xcite investigate one more convergence criteria , proposed by @xcite , which is based on cumulative sum ( cumsum ) statistics .",
    "it basically shows that if mcmc is converging , the graph of a certain cumsum statistic against time should approach zero .",
    "their employed griddy - gibbs algorithm converged in all four parameters quite fast .",
    "then , the authors explored the advantages and disadvantages of alternative approaches : the importance sampling and the mh algorithm . considering importance sampling , one of the main disadvantages , as mentioned before ,",
    "is to find a good approximation of the posterior density ( importance function ) .",
    "also , comparing with griddy - gibbs algorithm , the importance sampling requires much more draws to get smooth graphs of the marginal densities . for the mh algorithm , same as in importance sampling ,",
    "a good approximation needs to be found .",
    "also , compared to griddy - gibbs , the mh algorithm did not fully explore the tails of the distribution , unless for a very big number of draws .    another important aspect of the bayesian approach , as commented before , are the advantages in model selection compared to the classical methods .",
    "@xcite reviews some bayesian model selection methods using mcmc for garch - type models , which allow for the estimation of either marginal model likelihoods , bayes factors or posterior model probabilities .",
    "these are compared to the classical model selection criteria showing that the bayesian approach clearly considers model complexity in a more unbiased way .",
    "also , @xcite includes a revision of bayesian selection methods for asymmetric garch models , such as the gjr - garch and threshold garch .",
    "they show how using the bayesian approach it is possible to compare complex and non - nested models to choose for example between garch and stochastic volatility models , between symmetric or asymmetric garch models or to determine the number of regimes in threshold processes , among others .",
    "an alternative approach to the previous parametric specifications is the use of bayesian non - parametric methods , that allow to model the errors as an infinite mixture of normals , as seen in the paper by @xcite .",
    "the bayesian non - parametric approach for time - varying volatility models will be discussed in detail in section [ section : non_param ] .",
    "to sum up , considering the amount of articles published quite recently regarding the topic of estimating univariate garch models using mcmc methods indicates still growing interest in the area .",
    "although numerous garch - family models have been investigated using different mcmc algorithms , there are still a lot of areas that need further research and development .",
    "returns and volatilities depend on each other , so multivariate analysis is a more natural and useful approach . the starting point of multivariate volatility models is a univariate garch , thus the most simple mgarch models can be viewed as direct generalizations of their univariate counterparts . consider a multivariate return series @xmath21 of size @xmath22 .",
    "then @xmath23 where @xmath24 $ ] , @xmath25 are mean - corrected returns , @xmath7 is a random vector , such that @xmath26=0}$ ] and @xmath27=i_k}$ ] and @xmath28 is a positive definite matrix of dimensions @xmath29 , such that @xmath30 is the conditional covariance matrix of @xmath31 , i.e. , @xmath32=h_{t}^{1/2 } \\tx{cov}[\\epsilon_t ] ( h_{t}^{1/2})'=h_t$ ] .",
    "there is a wide range of mgarch models , where most of them differ in specifying @xmath30 . in the rest of this section we will review the most popular and widely used and the different bayesian approaches to make inference and prediction . for general reviews on mgarch models ,",
    "see @xcite , @xcite and @xcite ( chapter 10 ) , among others .",
    "regarding inference , one can also consider the same arguments provided in the univariate garch case above .",
    "maximum likelihood estimation for mgarch models can be obtained by using numerical optimization algorithms , such as fisher scoring and newton - raphson . @xcite",
    "have estimated several bivariate arch and garch models and found that some classical estimates of the parameters were quite different from their bayesian counterparts .",
    "this was due to the non - normality of the parameters .",
    "thus , the authors suggest careful interpretation of the classical estimation approach . also , @xcite found it difficult to evaluate the classical estimates under the stationarity conditions , and consequently the resulting parameters , evaluated ignoring the stationarity constraints , produced non - stationary estimates .",
    "these difficulties can be overcome using the bayesian approach .",
    "the vec model was proposed by @xcite , where every conditional variance and covariance ( elements of the @xmath30 matrix ) is a function of all lagged conditional variances and covariances , as well as lagged squared mean - corrected returns and cross - products of returns .",
    "using this unrestricted vec formulation , the number of parameters increases dramatically .",
    "for example , if @xmath33 , the number of parameters to estimate will be 78 , and if @xmath34 , the number of parameters increases to 210 , see @xcite for the explicit formula for the number of parameters in vec models . to overcome this difficulty , @xcite simplified the vec model by proposing a diagonal vec model , or dvec , as follows : @xmath35 where @xmath36 indicates the hadamard product , @xmath37 , @xmath38 and @xmath39 are symmetric @xmath29 matrices . as noted in @xcite , @xmath30 is positive definite provided that @xmath37 , @xmath38 , @xmath39 and the initial matrix @xmath40 are positive definite .",
    "however , these are quite strong restrictions on the parameters .",
    "also , dvec model does not allow for dynamic dependence between volatility series . in order to avoid such strong restrictions on the parameter matrices , @xcite propose the bekk model , which is just a special case of a vec and , consequently , less general .",
    "it has the attractive property that the conditional covariance matrices are positive definite by construction .",
    "the model looks as follows : @xmath41 where @xmath42 is a lower triangular matrix and @xmath43 and @xmath44 are @xmath29 matrices . in the bekk model",
    "it is easy to impose the definite positiveness of the @xmath30 matrix .",
    "however , the parameter matrices @xmath43 and @xmath44 do not have direct interpretations since they do not represent directly the size of the impact of the lagged values of volatilities and squared returns .",
    "@xcite present a paper that compares the performance of various bivariate arch and garch models , such as vec , bekk , etc , estimated using bayesian techniques .",
    "as the authors observe , they are the first to perform model comparison using bayes factors and posterior odds in the mgarch setting .",
    "the algorithm used for parameter estimation and inference is metropolis - hastings , and to check for convergence they rely on the cumsum statistics , introduced by @xcite , and used by @xcite in the univariate garch setting .",
    "using the real data the authors found that the t - bekk models performed the best , leaving t - vec not so far behind ; t - vec model , sometimes also called t - vech , is a more general form of a dvec , seen above , where the mean - corrected returns follow a student - t distribution .",
    "the name comes from a function called @xmath45 , which reshapes the lower triangular portion of a symmetric variance - covariance matrix into a column vector . to sum up",
    ", the authors choose t - bekk model as clearly better than the t - vec , because it is relatively simple and has less parameters to estimate .    on the other hand",
    ", @xcite developed a prior distribution for a vech specification that directly satisfy both necessary and sufficient conditions for positive definiteness and covariance stationarity , while remaining diffuse and non - informative over the allowable parameter space .",
    "these authors employed mcmc methods , including metropolis - hastings , to help enforce the conditions in this prior .",
    "more recently , @xcite use the bekk - garch model to show the usefulness of a new posterior sampler called the adaptive hamiltonian monte carlo ( ahmc ) .",
    "hamiltonian monte carlo ( hmc ) is a procedure to sample from complex distributions .",
    "the ahmc is an alternative inferential method based on hmc that is both fast and locally adaptive .",
    "the ahmc appears to work very well when the dimension of the parameter space is very high .",
    "model selection based on marginal likelihood is used to show that full bekk models are preferred to restricted diagonal specifications .",
    "additionally , @xcite suggests an approach called constrained hamiltonian monte carlo ( chmc ) in order to deal with high dimensional bekk models with targeting , which allow for a parameter dimension reduction without compromising the model fit , unlike the diagonal bekk .",
    "model comparison of the full bekk and the bekk with targeting is performed indicating that the latter dominates the former in terms of marginal likelihood .",
    "factor - garch was first proposed by @xcite to reduce the dimension of the multivariate model of interest using an accurate approximation of the multivariate volatility .",
    "the definition of the factor - garch model , proposed by @xcite , says that bekk model in is a factor - garch , if @xmath43 and @xmath44 have rank one and the same left and right eigenvalues : @xmath46 , @xmath47 , where @xmath48 and @xmath49 are scalars and @xmath50 and @xmath51 are eigenvectors .",
    "several variants of the factor model have been proposed .",
    "one of them is the full - factor multivariate garch by @xcite : @xmath52 where @xmath53 is a @xmath54 vector of constants , which is time invariant , @xmath55 is a @xmath29 parameter matrix , @xmath56 is a @xmath54 vector of factors and @xmath57 is a @xmath29 diagonal variance matrix such that @xmath58 , where @xmath59 is the conditional variance of the @xmath60th factor at time @xmath5 such that @xmath61 , @xmath62 , @xmath63 .",
    "then , the factors in the @xmath56 vector are garch(1,1 ) processes and the vector @xmath25 is a linear combination of such factors . it can be easily shown that @xmath30 is always positive definite by construction . however , the structure of @xmath30 depends on the order of the time series in @xmath31 .",
    "@xcite have considered this problem to find the best ordering under the proposed model .",
    "furthermore , @xcite investigate a full - factor mgarch model using the ml and bayesian approaches .",
    "the authors compute maximum likelihood estimates using fisher scoring algorithm . as for the bayesian analysis ,",
    "the authors have adopted a metropolis - hastings algorithm , and found that the algorithm is very time consuming , especially in high - dimensional data .",
    "to speed - up the convergence , @xcite have proposed reparametrization of positive parameters and also a blocking sampling scheme , where the parameter vector is divided into three blocks : mean , variance and the matrix of constants @xmath55 . as mentioned before , the ordering of the univariate time series in full - factor models is important , thus to select `` the best '' model one has to consider @xmath64 possibilities for a multivariate dataset of dimension @xmath65 . instead of choosing one model and making inference ( as if the selected model was the true one ) , the authors employ a bayesian approach by calculating the posterior probabilities for all competing models and model averaging to provide `` combined '' predictions .",
    "the main contribution of this paper is that the authors were able to carry out an extensive bayesian analysis of a full - factor mgarch model considering not only parameter uncertainty , but model uncertainty as well .    as already discussed above",
    ", a very common stylized feature of financial time series is the asymmetric volatility . @xcite",
    "have proposed a new class of tree structured mgarch models that explore the asymmetric volatility effect .",
    "same as the paper by @xcite , the authors consider not only parameter - related uncertainty , but also uncertainty corresponding to model selection .",
    "thus in this case bayesian approach becomes particularly useful because an alternative method based on maximizing the pseudo - likelihood is only able to work after selecting a single model .",
    "the authors develop an mcmc stochastic search algorithm that generates candidate tree structures and their posterior probabilities .",
    "the proposed algorithm converged fast .",
    "such modeling and inference approach leads to more reliable and more informative results concerning model - selection and individual parameter inference .",
    "there are more models that are nested in bekk , such as the orthogonal garch for example , see @xcite and @xcite , among others .",
    "all of them fall into the class of direct generalizations of univariate garch or linear combinations of univariate garch models .",
    "another class of models are the nonlinear combinations of univariate garch models , such as conditional correlation ( ccc ) , dynamic condition correlation ( dcc ) , general dynamic covariance ( gdc ) and copula - garch models .",
    "a very recent alternative approach that also considers bayesian estimation can be found in @xcite who proposes a new dynamic component models of returns and realized covariance ( rcov ) matrices based on time - varying wishart distributions . in particular , bayesian estimation and model comparison is conducted with a existing range of multivariate garch models and rcov models .",
    "the ccc model , proposed by @xcite and the simplest in its class , is based on the decomposition of the conditional covariance matrix into conditional standard deviations and correlations .",
    "then , the conditional covariance matrix @xmath30 looks as follows : @xmath66 where @xmath67 is diagonal matrix with the @xmath68 conditional standard deviations and @xmath69 is a time - invariant conditional correlation matrix such that @xmath70 and @xmath71 . the ccc approach can be applied to a wide range of univariate garch family models , such as exponential garch or gjr - garch , for example .",
    "@xcite have estimated some real data using a variety of bivariate arch and garch models in order to select the best model specification and to compare the bayesian parameter estimates to those of the ml .",
    "these authors have considered three arch and three garch models , all of them with constant conditional correlations ( ccc ) .",
    "they have used a metropolis - hastings algorithm , which allows to simulate from the joint posterior distribution of the parameters . for model comparison and selection , @xcite",
    "have obtained predictive distributions and assessed comparative validity of the analyzed models , according to which the ccc model with diagonal covariance matrix performed the best considering one - step - ahead predictions .      a natural extension of the simple ccc model are the dynamic conditional correlation ( dcc ) models , firstly proposed by @xcite and @xcite .",
    "the dcc approach is more realistic , because the dependence between returns is likely to be time - varying .",
    "the models proposed by @xcite and @xcite consider that the conditional covariance matrix @xmath30 looks as @xmath72 , where @xmath73 is now a time varying correlation matrix at time @xmath14 .",
    "the models differ in the specification of @xmath73 . in the paper by @xcite ,",
    "the conditional correlation matrix is @xmath74 , where @xmath75 and @xmath76 are non - negative scalar parameters , such that @xmath77 , @xmath69 is a positive definite matrix such that @xmath78 and @xmath79 is a @xmath80 sample correlation matrix of the past @xmath81 standardized mean - corrected returns @xmath82 . on the other hand , in the paper by @xcite ,",
    "the specification of @xmath83 is @xmath84 , where @xmath85 , @xmath86 is a mean - corrected standardized returns , @xmath48 and @xmath49 are non - negative scalar parameters , such that @xmath19 and @xmath87 is unconditional covariance matrix of @xmath88 . as noted in @xcite , the model by @xcite",
    "does not formulate the conditional correlation as a weighted sum of past correlations , unlike in the dcc model by @xcite , seen above .",
    "the drawback of both these models is that @xmath75 , @xmath76 , @xmath48 and @xmath49 are scalar parameters , so all conditional correlations have the same dynamics .",
    "however , as @xcite notes it , the models are parsimonious .",
    "moreover , as financial returns display not only asymmetric volatility , but also excess kurtosis , previous research , as in univariate case , has mostly considered using a multivariate student - t distribution for the errors . however , as already discussed above , this approach has several limitations .",
    "@xcite propose a mgarch - dcc model , where the standardized innovations follow a mixture of gaussian distributions .",
    "this allows to capture long tails without being limited by the degrees of freedom constraint , which is necessary to impose in the student - t distribution so that higher moments could exist .",
    "the authors estimate the proposed model using the classical ml and bayesian approaches . in order to estimate model parameters , dynamics of single assets and dynamic correlations , and the parameters of the gaussian mixture ,",
    "@xcite have relied on rwmh algorithm .",
    "bic criteria was used for selecting the number of mixture components , which performed well in simulated data . using real data ,",
    "the authors provide an application to calculating the value at risk ( var ) and solving a portfolio selection problem .",
    "mle and bayesian approaches have performed similarly in point estimation , however , the bayesian approach , besides giving just point estimates , allows the derivation of predictive distributions for the portfolio var .",
    "an extension of the dcc model of @xcite is the asymmetric dcc also proposed by @xcite , which incorporates an asymmetric correlation effect .",
    "it means that correlations between asset returns decrease more in the bear market than they increase when the market performs well .",
    "@xcite generalizes the adcc model into the agdcc model , where the parameters of the correlation equation are vectors , and not scalars .",
    "this allows for asset - specific correlation dynamics . in the agdcc model ,",
    "the @xmath89 matrix in the dcc model is replaced with : @xmath90 where @xmath91 are mean corrected standardized returns , @xmath92 selects just negative returns , `` diag '' stands for either taking just the diagonal elements from the matrix , or making a diagonal matrix from a vector , @xmath93 is a sample correlation matrix of @xmath94 , @xmath95 and @xmath96 are @xmath97 vectors , @xmath98 , @xmath99 and @xmath100 . to ensure positivity and stationarity of @xmath89 , it is necessary to impose @xmath101 and @xmath102 , @xmath103 .",
    "the agdcc by @xcite is just a special case where @xmath104 , @xmath105 and @xmath106 .",
    "up to our knowledge , the only paper that considers the agdcc model in the bayesian setting is @xcite that propose to model the distribution of the standardized returns as an infinite scale mixture of gaussian distributions by relying on bayesian non - parametrics .",
    "this approach is presented in more detail in section [ section : non_param ] .",
    "the use of copulas is an alternative approach to study return time series and their volatilities .",
    "the main convenience of using copulas is that individual marginal densities of the returns can be defined separately from their dependence structure .",
    "then , each marginal time series can be modeled using univariate specification and the dependence between the returns can be modeled by selecting an appropriate copula function .",
    "a @xmath65-dimensional copula @xmath107 , is a multivariate distribution function in the unit hypercube @xmath108^k}$ ] , with uniform @xmath108}$ ] marginal distributions . under certain conditions",
    ", the sklar theorem affirms that ( see , @xcite ) , every joint distribution @xmath109 , whose marginals are given by @xmath110 , can be written as @xmath111 , where @xmath112 is a copula function of @xmath113 , which is unique if the marginal distributions are continuous .    the most popular approach to volatility modeling through copulas",
    "is called the copula - garch model , where univariate garch models are specified for each marginal series and the dependence structure between them is described using a copula function .",
    "a very useful feature of copulas , as noted by @xcite , is that the marginal distributions of each random variable do not need to be similar to each other .",
    "this is very important in modeling time series , because each of them might be following different distributions .",
    "the choice of copulas can vary from a simple gaussian copula to more flexible ones , such as clayton , gumbel , mixed gaussian , etc . in the existing literature different parametric and non - parametric specifications",
    "can be used for the marginals and copula function @xmath114 .",
    "also , the copula function can be assumed to be constant or time varying , as seen in @xcite , among others .",
    "the estimation for copula - garch models can be performed in a variety of ways .",
    "maximum likelihood is the obvious choice for fully parametric models .",
    "estimation is generally based on a multi - stage method , where firstly the parameters of the marginal univariate distributions are estimated and then used to condition in estimating the parameters of the copula .",
    "another approach is non- or semi - parametric estimation of the univariate marginal distributions followed by a parametric estimation of the copula parameters .",
    "as @xcite has showed , the two - stage maximum likelihood approach lead to consistent , but not efficient estimators .",
    "an alternative is to employ a bayesian approach , as done by @xcite .",
    "the authors have developed a one - step bayesian procedure where all parameters are estimated at the same time using the entire likelihood function and , provided the methodology , for obtaining optimal portfolio , calculating var and cvar .",
    "@xcite have used a gibbs sampler to sample from a joint posterior , where each parameter is updated using a rwmh . in order to reduce computational cost ,",
    "the model and copula parameters are updated not one - by - one , but rather by blocks , that consist of highly correlated vectors of model parameters .",
    "@xcite have also used bayesian inference for copula - garch models .",
    "these authors have proposed a methodology for modelling dynamic dependence structure by allowing copula functions or copula parameters to change across time .",
    "the idea is to use a threshold approach so these changes , that are assumed to be unknown , do not evolve in time but occur in distinct points .",
    "these authors have also employed a rwmh for parameter estimation together with a laplace approximation .",
    "the adoption of an mcmc algorithm allows the choice of different copula functions and/or different parameter values between two time thresholds .",
    "bayesian model averaging is considered for predicting dependence measures such as the kendall s correlation .",
    "they conclude that the new model performs well and offers a good insight into the time - varying dependencies between the financial returns .",
    "@xcite developed bayesian inference of a multivariate garch model where the dependence is introduced by a d - vine copula on the innovations .",
    "a d - vine copula is a special case of vine copulas which are very flexible to construct multivariate copulas because it allows to model dependency between pairs of margins individually .",
    "inference is carried out using a two - step mcmc method closely related with the usual two - step maximum likehood procedure for estimating copula - garch models .",
    "the authors then focus on estimating the var of a portfolio that shows asymmetric dependencies between some pairs of assets and symmetric dependency between others .",
    "all the previously introduced methods rely on parametric assumptions for the distribution of the errors .",
    "however , imposing a certain distribution can be rather restrictive and lead to underestimated uncertainty about future volatilities , as seen in @xcite",
    ". therefore , bayesian non - parametric methods become especially useful , since they do not impose any specific distribution on the standardized returns .",
    "bayesian non - parametrics is an alternative approach to the classical parametric bayesian statistics , where one usually gives some prior for the parameters of interest , whose distribution is unknown , and then observes the data and calculates the posterior .",
    "the priors come from the family of parametric distributions .",
    "bayesian non - parametrics uses a prior over distributions with the support being the space of all distributions .",
    "then , it can be viewed as a distribution over distributions .",
    "one of the most popular bayesian non - parametric modeling approach is based on dirichlet processes ( dp ) and mixtures of dirichlet processes ( dpm ) .",
    "dp was firstly introduced by @xcite .",
    "suppose that we have a sequence of exchangeably distributed random variables @xmath115 from an unknown distribution @xmath116 , where the support for @xmath117 is @xmath37 . in order to perform bayesian inference , we need to define the prior for @xmath116 .",
    "this can be done by considering partitions of @xmath37 , such as @xmath118 , and defining priors over all possible partitions .",
    "we say that @xmath116 has a dirichlet process prior , denoted as @xmath119 , if the set of associated probabilities given @xmath116 for any partition follows a dirichlet distribution , @xmath120 , where @xmath121 is a precision parameter that represents our prior certainty of how concentrated the distribution is around @xmath122 , which is a known base distribution on @xmath37 .",
    "the dirichlet process is a conjugate prior .",
    "thus , given @xmath123 independent and identically distributed samples from @xmath116 , the posterior distribution of @xmath116 is also a dirichlet process such that @xmath124 where @xmath125 is the empirical distribution function .",
    "there are two main ways for generating a sample from the marginal distribution of @xmath126 , where @xmath127 and @xmath128 : the polya urn and stick breaking procedures . on the one hand ,",
    "the polya urn scheme can be illustrated in terms of a urn with @xmath48 black balls ; when a non - black ball is drawn , it is placed back in the urn together with another ball of the same color . if the drawn ball is black , a new color is generated from @xmath129 and a ball of this new color is added to the urn together with the black ball we drew .",
    "this process gives a discrete marginal distribution for @xmath126 since there is always a probability that a previously seen value is repeated .",
    "on the other hand , the stick - breaking procedure is based on the representation of the random distribution @xmath116 as a countably infinite mixture : @xmath130 where @xmath131 is a dirac measure , @xmath132 and the weights are such that @xmath133 where @xmath134 .",
    "this implies that the weights @xmath135 as @xmath136 .",
    "the discreteness of the dirichlet process is clearly a disadvantage in practice .",
    "a solution was proposed by @xcite by using dpm models where a dp prior is imposed over over the distribution of the model parameters , @xmath137 , as follows : @xmath138 observe that @xmath139 is a random distribution drawn from the dp and because it is discrete , multiple @xmath140 s can take the same value simultaneously , making it a mixture model .",
    "in fact , using the stick - breaking representation , the hierarchical model above can be seen as an infinite mixture of distributions : @xmath141 where the weights are obtained as before : @xmath142 , @xmath143 , for @xmath144 , and where @xmath145 and @xmath146 .",
    "regarding inference algorithms , there are two main types of approaches . on the one hand , the marginal methods , such as those proposed by @xcite , @xcite and @xcite , which rely on the polya urn representation .",
    "all these algorithms are based on integrating out the infinite dimensional part of the model .",
    "recently , another class of algorithms , called conditional methods , have been proposed . these approaches , based on the stick - breaking scheme ,",
    "leave the infinite part in the model and sample a finite number of variables .",
    "these include the procedure by @xcite , who introduces slice sampling schemes to deal with the infiniteness in dpm , and the retrospective mcmc method of @xcite , that is later combined by @xcite with slice sampling method by @xcite to obtain a new composite algorithm , which is better , faster and easier to implement .",
    "generally , the stick - breaking compared to the polya urn procedures produce better mixing and simpler algorithms .",
    "as mentioned above , so far there has been little research in modeling volatilities with mgarch using the dpm models . up to our knowledge , these only include : @xcite for univariate garch , and @xcite and @xcite , for mgarch .",
    "@xcite have applied semi - parametric bayesian techniques to estimate univariate garch - type models .",
    "these authors have used the class of scale mixtures of gaussian distributions , that allow for the variances to change over components , with a dirichlet process prior on the mixing distribution to model innovations of the garch process . the resulting class of models is called dpm - garch type models . in order to perform bayesian inference on the new model , the authors employ a stick - breaking sampling scheme and make use of the ideas proposed in @xcite , @xcite and @xcite .",
    "the new scale mixture model was compared to a simpler mixture of two gaussians , student - t and the usual gaussian models .",
    "the estimation results in all three cases were quite similar , however , the scale mixture model is able to capture skewness as well as kurtosis and , based on the approximated log marginal likelihood ( lml ) and @xmath147 , provided the best performance in simulated and real data .",
    "finally , @xcite have applied the resulting model to perform one - step - ahead predictions for volatilities and var . in general",
    ", the non - parametric approach leads to wider bayesian credible intervals and can better describe long tails .",
    "@xcite propose a bayesian non - parametric modeling approach for the innovations in mgarch models .",
    "they use a mgarch specification , proposed by @xcite , which is a different representation of a well known dvec model , introduced above .",
    "the innovations are modeled as an infinite mixture of multivariate normals with a dp prior .",
    "the authors have employed polya urn and stick - breaking schemes and , using two data sets , compared the three model specifications : parametric mgarch with student - t innovations ( mgarch - t ) , garch - dpm-@xmath148 that allows for different covariances ( scale mixture ) and mgarch - dpm , allowing for different means and covariances of each component ( location - scale mixture ) . in general , both semi - parametric models produced wider density intervals . however , in mgarch - t model a single degree of freedom parameter determines the tail thickness in all directions of the density , meanwhile the non - parametric models are able to capture various deviations from normality by using a certain number of components .",
    "these results are consistent with the ones in @xcite . as for predictions , both semi - parametric models performed equally good and outperformed the parametric mgarch - t specification .",
    "finally , the paper by @xcite can be seen as a direct generalization of the paper by @xcite to the multivariate framework . here , same as in @xcite ,",
    "the authors have proposed using an infinite scale mixture of normals for the standardized returns . for the mgarch model",
    "a gjr - adcc was chosen , allowing for asymmetric volatilities and asymmetric time - varying correlations .",
    "moreover , the authors have carried - out a simulation study that illustrated the adaptability of the dpm model .",
    "finally , the authors provided one real data application to portfolio decision problem concluding that dpm models are less restrictive and more adaptive to whatever distribution the data comes from , therefore , can better capture the uncertainty about financial decisions .",
    "to sum up , the findings in the above papers are consistent : the bayesian semi - parametric approach leads to more flexible models and is better in explaining heavy - tailed return distributions , which parametric models can not fully capture .",
    "the parameters are less precise , i.e. wider bayesian credible intervals are observed because the semi - parametric models are less restricted .",
    "this provides a more adequate measure of uncertainty .",
    "if in the gaussian setting the credible intervals are very narrow and the real data is not gaussian , this makes the agent overconfident about her decisions , and she takes more risk than she would like to assume .",
    "@xcite observes that the combination of bayesian methods and mcmc computational algorithms provide new modeling possibilities and calls for more research regarding non - parametric bayesian time series modeling .",
    "this illustration study using real data has basically two goals : firstly , to show the advantages of the bayesian approach , such as the ability to obtain posterior densities of quantities of interest and the facility to incorporate various constraints on the parameters . secondly , to illustrate the flexibility of the bayesian non - parametric approach for garch modeling .",
    "the data used for estimation are the log - returns ( in percentages ) , obtained from close prices adjusted for dividends and splits , of two market indices : ftse100 and s&p500 from november 10th , 2004 till december 10th , 2012 , resulting into a sample size of 2000 observations .",
    "ftse100 is a share index of the 100 companies listed on the london stock exchange with the highest market capitalization .",
    "s&p500 is a stock market index based on the common stock prices of 500 top publicly traded american companies .",
    "the data was obtained from yahoo finance .",
    "figure [ f : returns ] and table [ t : descriptive_table ] present the basic plots and descriptive statistics of the two log - return series .",
    "[ f : returns ]    ccc    ' '' ''    & ftse100 & s&p500 + mean & 0.0112 & 0.0099 + median & 0.0344 & 0.0779 + variance & 1.7164 & 1.9617 + skewness & -0.0974 & -0.3001 + kurtosis & 10.5464 & 12.5674 + correlation & 0.6060 +    as seen from the plot and descriptive statistics , the data is slightly skewed and with high kurtosis , therefore , assuming a gaussian distribution for the standardized returns would be inappropriate",
    ". therefore , we estimate this bivariate time series using an adcc model by @xcite , presented in section 3.4 . , which incorporates an asymmetric correlation effect .",
    "the univariate series are assumed to follow gjr - garch@xmath149 models in order to incorporate the leverage effect in volatilities . as for the errors , we use an infinite scale mixture of gaussian distributions .",
    "therefore , we call the final model gjr - adcc - dpm . inference and prediction is carried out using bayesian non - parametric techniques , as seen in @xcite .",
    "the selection of the mgarch specification is arbitrary and other models might work equally well .",
    "for the sake of comparison , we estimate a restricted gjr - adcc - gaussian model using maximum likelihood and bayesian approaches .",
    "the estimation results are presented in table [ table : estimation ] .",
    "[ table : estimation ]    c c c c c c c & & & + & estimate & st .",
    "& mean & @xmath150 ci&mean & @xmath150 ci +   + @xmath151 & 0.0166 & 0.0020 & 0.0192 & ( 0.0130 , 0.0258 ) & 0.0181 & ( 0.0104 , 0.0264 ) + @xmath152 & 0.0190 & 0.0016 & 0.0249 & ( 0.0174 , 0.0316 ) & 0.0219 & ( 0.0153 , 0.0293 ) + @xmath153 & @xmath154 & @xmath155 & 0.0058 & ( 0.0002 , 0.0177 ) & 0.0046 & ( 0.0003 , 0.0112 ) + @xmath156 & @xmath157 & @xmath158 & 0.0053 & ( 0.0002 , 0.0173 ) & 0.0059 & ( 0.0002 , 0.0151 ) + @xmath159 & 0.9087 & 0.0045 & 0.9010 & ( 0.8841 , 0.9152 ) & 0.8956 & ( 0.8762 , 0.9139 ) + @xmath160 & 0.9079 & 0.0050 & 0.8888 & ( 0.8705 , 0.9088 ) & 0.8851 & ( 0.8675 , 0.9041 ) + @xmath161 & 0.1535 & 0.0085 & 0.1587 & ( 0.1351 , 0.1871 ) & 0.1586 & ( 0.1057 , 0.2089 ) + @xmath162 & 0.1483 & 0.0092 & 0.1737 & ( 0.1398 , 0.2020 ) & 0.1758 & ( 0.1134 , 0.2142 ) + @xmath163 & 0.0075 & 0.0020 & 0.0071 & ( 0.0014 , 0.0145 ) & 0.0095 & ( 0.0040 , 0.0156 ) + @xmath164 & 0.9898 & 0.0029 & 0.9818 & ( 0.9665 , 0.9898 ) & 0.9806 & ( 0.9693 , 0.9901 ) + @xmath96 & @xmath165 & @xmath166 & 0.0076 & ( 0.0002 , 0.0153 ) & 0.0039 & ( 0.0001 , 0.0114 ) +    the estimated parameters are very similar for all three approaches , except for @xmath167 , the asymmetric correlation coefficient @xmath96 .",
    "since @xmath167 and @xmath96 are so close to zero , the ml has some trouble in estimating those parameters .",
    "overall , the @xmath96 is small , indicating little evidence of asymmetrical behavior in correlations .",
    "figure [ fig : tails ] shows the estimated marginal predictive densities of the one - step - ahead returns in log scale using the bayesian approach .",
    "we can observe the differences in tails arising from different specification of the errors .",
    "the dpm model allows for a more flexible distribution , therefore , for more extreme returns , i.e.  fatter tails .",
    "the estimated densities were obtained using the procedure described in @xcite .     for bayesian gaussian and dpm models , scaledwidth=100.0% ]",
    "table [ table : volatilities ] presents the estimated mean , median and @xmath150 credible intervals of one - step - ahead volatility matrices in bayesian context .",
    "the matrix element ( 1,1 ) represents the volatility for the ftse100 series , ( 2,2 ) for the s&p500 , and the elements in the diagonal ( 1,2 ) and ( 2,1 ) represent the covariance of both financial returns .",
    "figure [ fig : vols_returns ] draws the posterior distributions for volatilities and correlation .",
    "the estimated mean volatilities for both , dpm and gaussian approaches , are very similar , however , the main differences arise from the shape of the posterior distribution .",
    "@xmath150 credible intervals for dpm model correlation are wider providing a more realistic measure of uncertainty about future correlations between two assets .",
    "this is a very important implication in financial setting , because if an investor chooses to be gaussian , she would be overconfident about her decision and unable to adequately measure the risk she is facing .",
    "see @xcite for a more detailed comparison of dpm and alternative parametric approaches in portfolio decision problems .",
    "[ table : volatilities ]    c c c c c c c & & & + & constant & ml gaussian & mean & @xmath150 ci&mean & @xmath150 ci + & & & median & ci length&median & ci length +   + @xmath168 & 1.7164 & 0.4007 & 0.4098 & ( 0.3681 , 0.4538 ) & 0.3996 & ( 0.3550 , 0.4512 ) + & & & 0.4099 & 0.0857 & 0.3983 & 0.0962 + @xmath169 & 1.1120 & 0.2911 & 0.2800 & ( 0.2571 , 0.3077 ) & 0.2751 &",
    "( 0.2421 , 0.3123 ) + & & & 0.2790 & 0.0506 & 0.2742 & 0.0702 + @xmath170 & 1.9617 & 0.4939 & 0.4635 & ( 0.4159 , 0.5193 ) & 0.4431 & ( 0.3912 , 0.5059 ) + & & & 0.4606 & 0.1034 & 0.4408 & 0.1146 +        to sum up , this illustration has shown the main differences between the standard estimation procedures and the new non - parametric approach .",
    "even though the point estimates for the parameters and the one - step - ahead volatilities are very similar , the main differences arise from the thickness of tails of predictive distributions of one - step - ahead returns and the shape of the posterior distribution for the one - step - ahead volatilities .",
    "in this paper we reviewed univariate and multivariate garch models and inference methods , putting emphasis on the bayesian approach .",
    "we have surveyed the existing literature that concerns various bayesian inference methods for mgarch models , outlining the advantages of the bayesian approach versus the classical procedures .",
    "we have also discussed in more detail the recent bayesian non - parametric method for garch models , which avoid imposing arbitrary parametric distributional assumptions .",
    "this new approach is more flexible and can describe better the uncertainty about future volatilities and returns , as has been illustrated using real data .",
    "we are grateful to an anonymous referee for helpful comments .",
    "the first and second authors are grateful for the financial support from mec grant eco2011 - 25706 .",
    "the third author acknowledges financial support from mec grant eco2012 - 38442 .",
    "81 natexlab#1#1[1]`#1 ` [ 2]#2 [ 1]#1 [ 1]http://dx.doi.org/#1 [ ] [ 1]pmid:#1 [ ] [ 2]#2 , & ( ) . . , .",
    "( ) . . , _",
    "_ , . . , & ( )",
    "( ) . . , _",
    "_ , . , & ( ) . . in ( ed",
    ". ) , _ _ chapter  .",
    "( pp . ) . : .",
    "( ) . . , _",
    "_ , . . , & ( )",
    ", , & ( ) . . , _",
    "_ , . . , & ( )",
    ", , & ( ) . . , _",
    "_ , . , , & ( ) . . ,",
    "_ , . , & ( ) .",
    ", _ _ , . , & ( ) . . ,",
    "_ , . , & ( ) . . ,",
    ", ( pp . ) .",
    "( ) . . , _",
    "( ) . . , _",
    "( ) . . , _",
    "_ , . , , & ( ) .",
    ", , & ( ) . . in , & ( eds . ) , _ _ ( pp . ) . : .",
    ", , & ( ) . . , _",
    "( ) . . , . , & ( )",
    ", , & ( ) . . , _",
    ", , & ( ) . . , _",
    "_ , . . , & ( )",
    "_ , . , & ( ) .",
    "( ) . . , _",
    "( ) . . , _",
    "( ) . . , _",
    "( ) . . , _",
    "_ , . , & ( ) .",
    ", , & ( ) . . , _",
    "_ , . , & ( ) .",
    "( ) . . , _",
    "_ , . , & ( ) .",
    "( ) . . , _",
    ", , & ( ) . . , _",
    ", , & ( ) . . , _",
    ", , & ( ) . . , _",
    ", , & ( ) . . ,",
    "_ , . , & ( ) .",
    "_ , . , & ( ) .",
    "_ , . , & ( ) .",
    ", & ( ) . . , _",
    "_ , . , & ( ) .",
    ". , ( pp . ) . , & ( ) .",
    ". in , , , & ( eds . ) , _ _ chapter  .",
    "( pp . ) . : .",
    "( ed . ) . , &",
    "_ _ , . , & ( ) .",
    "( ) . . , _",
    "( ) . . , _",
    ", , & ( ) . . , _",
    "( ) . . , _",
    "( ) . . , _",
    "_ , . , & ( ) .",
    ", _ _ , . , & ( ) .",
    "( ) . . , _",
    "( ) . . , _",
    "( ) . . , _ _ , . , & ( ) . . , _",
    ", ( pp . ) . , & ( ) . . , _",
    "( ) . . , _",
    "( ) . . in , , , & ( eds . ) , _ _ chapter  .",
    "( pp . ) . : . , & ( ) . .",
    "( ed . ) . .",
    ", & ( ) . . in , , , & ( eds . ) , _ _ ( pp . ) . : .",
    "( ) . . , _",
    "( ) . . in , & ( eds . ) , _ _ . : .",
    "( ) . . in , , , & ( eds . ) , _ _ ( pp . ) . : . ( ) . . , _",
    "( ed . ) . : , & ( ) . . , _ _ , . .",
    "( ) . . , _",
    ", , & ( ) . . , . , , & ( ) . . , _",
    ", , & ( ) . . , _",
    ", , & ( ) . . , _",
    "( ) . . , _ _ , . .",
    "( ) . . , _",
    "_ , . . , & ( )",
    "( ) . . , _"
  ],
  "abstract_text": [
    "<S> this survey reviews the existing literature on the most relevant bayesian inference methods for univariate and multivariate garch models . </S>",
    "<S> the advantages and drawbacks of each procedure are outlined as well as the advantages of the bayesian approach versus classical procedures . </S>",
    "<S> the paper makes emphasis on recent bayesian non - parametric approaches for garch models that avoid imposing arbitrary parametric distributional assumptions . </S>",
    "<S> these novel approaches implicitly assume infinite mixture of gaussian distributions on the standardized returns which have been shown to be more flexible and describe better the uncertainty about future volatilities . </S>",
    "<S> finally , the survey presents an illustration using real data to show the flexibility and usefulness of the non - parametric approach .    </S>",
    "<S> * keywords : * bayesian inference ; dirichlet process mixture ; financial returns ; garch models ; multivariate garch models ; volatility . </S>"
  ]
}