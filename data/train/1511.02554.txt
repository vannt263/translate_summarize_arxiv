{
  "article_text": [
    "machine learning is a practical approach to deal with real world challenges such as to model neck pain and motor training induced plasticity @xcite , @xcite .",
    "such methods have been widely used to solve difficult optimization problems @xcite , @xcite .",
    "opposition based learning is one example which has achieved successful results in medical image processing and optimization problems @xcite , @xcite , @xcite .",
    "the genome - wide association ( gwa ) studies have discovered many convincingly replicated associations for complex human diseases using high - throughput single - nucleotide polymorphism ( snp ) genotypes @xcite , @xcite . the genotype imputation has been used for fine - map associations and facilitates the combination of results across studies @xcite .",
    "the issue of missing genotype data and its imputation implies creating individualistic genotype data @xcite .",
    "impact of even small amounts of missing data on a multi - snp analysis is of great importance for the complex diseases research @xcite .",
    "there are several programs such as beagle @xcite , mach @xcite , and impute2 @xcite , which provide imputation capability of untyped variants .    the sparse partial least squares ( spls ) and least absolute shrinkage and selection operator ( lasso ) methods are well - known for simultaneous dimension reduction and variable selection @xcite , @xcite .",
    "the lasso is a shrinkage and selection method for linear regression , which attempts to minimize an error function .",
    "this function is typically the sum of squared errors with a bound on the sum of the absolute values of the coefficients @xcite",
    ". the partial least squares ( pls ) regression is used as an alternative approach to the ordinary least squares ( ols ) regression method @xcite .",
    "the spls method is the sparse version of pls method , which simultaneously works to achieve good predictive performance and variable selection by producing sparse linear combinations of the original predictors @xcite .    in general ,",
    "matrix factorization is a technique to decompose a matrix for multivariate data into two matrices with @xmath0 latent features @xcite .",
    "many matrix factorization techniques have been proposed to increase its performance , such as non - negative @xcite , sparse @xcite , non - linear @xcite , and kernel - based approaches @xcite .",
    "a kernel non - negative matrix factorization method is proposed for feature extraction and classification of micro - array data in @xcite .",
    "performance evaluation of this method for eight different gene samples has showed better performance over linear as well as other well - known kernel - based matrix factorization approaches .",
    "a sparse matrix factorization method has been proposed for tumor classification using gene expression data , @xcite . in this approach",
    ", the gens are selected using a sparse matrix factorization method and then the features are extracted to be fed into a support vector machine ( svm ) for tumor samples classification .",
    "it is reported that the performance results have been improved versus the non - sparse matrix factorization techniques . the artificial neural network ( ann )",
    "is another successful machine learning approach for prediction and classification applications @xcite . as an example",
    ", a feed - forward anns model and a bayesian approach are utilized to impute missing genotype data of snps in @xcite .",
    "sequence modelling is one of the most areas in machine learning .",
    "this is due to the fact that a large class of phenomenal and data around us is made of sequences of data with particular patterns .",
    "some examples are retail data , speed recognition , natural language modelling , music generation and genotype data for medical applications . with the great practical advances in deep learning ,",
    "this state - of - the - art machine learning technique is the key for many problems in science and engineering .",
    "recurrent neural network ( rnn ) , due to its recurrent connections is considered as a subcategory of deep learning methods .",
    "this powerful model is capable of learning temporal patters and in sequential data .",
    "the power of rnn arises from its hidden state , which works as the `` memory '' of system to remember past important features for the future decision makings .",
    "the hidden state is consisted of high - dimensional non - linear dynamics which enables modelling any phenomena , if trained well @xcite , @xcite .    in this paper",
    ", we are proposing a new model for missing genotype imputation and phenotype prediction using matrix factorization and rnns . in this model ,",
    "a simple but efficient matrix factorization method is used for missing genotype imputation .",
    "then , the imputed genotypes are used along the sequence of available phenotype data to train our rnn with the recently developed relu learning approach . in order to evaluate performance of the relu approach in learning long - term dependencies in phenotype data , it is compared with the lstm - rnn and srnn approaches .    in the next section , the data structure of the dataset used for the experiments is described . in section iii , the methodologies based on the matrix factorization technique and drnn is discussed in detail .",
    "the experimental results as well are comparative analysis are provided in section iv .",
    "finally , the paper is concluded in section v and some guidelines are further developments are provided .",
    "for the experiments , we are using a set of data provided for our research by afzalipour research hospital .",
    "the genotype data contains genotypes of 1980 snps for 604 observations and the phenotype data provides measurements of two phenotype , called trait 1 and trait 2 . out of 1980 snps",
    "provided in the genotype data , 5@xmath1 contain missing genotypes .",
    "the percentage of observations with missing genotypes for each snp varies from 1 to 25 . for each trait ,",
    "30 randomly selected observations have missing values .",
    "in order to deal with the missing genotype and phenotype problem , we are utilizing the matrix factorization ( mf ) and rnns techniques to fit prediction models as in figure [ fig : system_model ] .",
    "to do so , after data pre - processing , the genotype dataset with missing values is imported into the mf system to predict the missing genotype values . by having the estimated genotype dataset and corresponding phenotypes ,",
    "the rnn is utilized in a supervised manner to train a network model for prediction of phenotypes , based on the known genotype - phenotype pairs .",
    "each stage is described in details in the following subsections .",
    "0.28        0.45          in general , the snp genotypes ( aa , bb , ab , or null ) are denoted with integer numbers for computational purposes , however , some programs may be able to work with this aa / ab / bb format directly .",
    "the data pre - processing step is an opportunity to clean data , remove noise , and translate the genotype data and indicate / distinguish missing values from the available data .",
    "the proposed mf structure for genotype data imputation is presented in figure [ fig : mf_model ] . in this model",
    "we consider @xmath2 number of samples and @xmath3 number of snps .",
    "therefore , the genotype data is structured as a @xmath4 matrix , called @xmath5 .",
    "the objective of mf technique is to estimate two matrices , @xmath6 and @xmath7 with @xmath0 latent features , such that their product @xmath8 estimates @xmath5 as :    @xmath9    where each element of the genotype matrix @xmath10 is computed by using the dot product such as :    @xmath11    in order to find the best values for the matrices @xmath12 and @xmath13 , we need to minimize the objective function which describes the difference between the @xmath14 and @xmath10 genotype matrices @xcite . to do so ,",
    "the gradient descent algorithm is utilized as the optimizer in figure [ fig : mf_model ] to update the feature matrices @xmath12 and @xmath13 iteratively .",
    "the above procedure is illustrated in pseudocode as in algorithm [ psudocode ] .",
    "as it is demonstrated , the parameters are set in the initialization step and random values in range @xmath15 $ ] are allocated to the feature matrices @xmath12 and @xmath13 , @xcite .",
    "based on the availability of each genotype such as @xmath16 for all @xmath17 , the estimated genotype matrix @xmath18 is computed .",
    "the objective function is then formulated with respect to @xmath12 and @xmath13 as :    @xmath19    where forbenius norms of @xmath12 and @xmath13 are used for regularization under control of parameter @xmath20 to prevent over - fitting of model by penalizing it with extreme parameter values @xcite .",
    "normally @xmath20 is set to some values in the range of 0.02 , such that @xmath12 and @xmath13 can approximate @xmath14 without having to contain large numbers .",
    "the feature matrices of @xmath12 and @xmath13 are updated as :    @xmath21    and @xmath22 respectively , where @xmath23 represents the learning rate and is practically set to @xmath24 .",
    "@xmath25 @xmath26)$ ] @xmath27)$ ] @xmath28 @xmath29      the utilized rnn in the proposed model in figure  [ fig : system_model ] is consisted of input , hidden , and output layers , where each layer is consisted of corresponding units",
    ". the input layer is consisted of @xmath30 input units , where its inputs are defined as a sequence of vectors through time @xmath31 such as @xmath32 where @xmath33 . in a fully connected rnn ,",
    "the inputs units are connected to hidden units in the hidden layer , where the connections are defined with a weight matrix @xmath34 .",
    "the hidden layer is consisted of @xmath35 hidden units @xmath36 , which are connected to each other through time with recurrent connections . as it is demonstrated in figure  [ fig : srnn_unfolded ] ,",
    "the hidden units are initiated before feeding the inputs .",
    "the hidden layer structure defines the state space or `` memory '' of the system , defined as    @xmath37    where @xmath38 is the hidden layer activation function and @xmath39 is the bias vector of the hidden units .",
    "the hidden units are connected to the output layer with weighted connections @xmath40 .",
    "the output layer has @xmath12 units such as @xmath41 which are estimated as @xmath42 where @xmath43 is the output layer activations functions and @xmath44 is the bias vector .    learning long term dependencies in rnns is a difficult task @xcite .",
    "this is due to two major problems which are vanishing gradients and exploding gradients .",
    "the long - short - term memory ( lstm ) method is one of the popular methods to overcome the vanishing gradient problem .",
    "a recent proposed method suggests that proper initialization of the rnn weights with rectified linear units has good performance in modeling long - range dependencies @xcite . in this approach ,",
    "the model is trained by utilizing back - propagation through time ( bptt ) technique to compute the derivatives of error with respect to the weights .",
    "the reported performance analysis show that this method has comparable results in comparision to the lstm method , with much less complexity .    in this model , each new hidden state vector",
    "is inherited from the previous hidden vector by copying its values , adding the effects of inputs , and finally , replacing negative state values by zero . in other words",
    ", this means that the recurrent weight matrix is initialized to an identity matrix and the biases are set to zero .",
    "this procedure is in fact replacing the `` tanh '' activation function ( figure [ fig : tanh ] ) with a rectified linear unit ( relu)(figure [ fig : relu ] ) .",
    "the relu in fact is modelling the behaviour of lstm . in lstm",
    ", the gates are set in a way that there is no decay to model long - term dependencies . in relu ,",
    "when the error derivatives for the hidden units are back - propagated through time they remain constant provided no extra error - derivatives are added @xcite .",
    "the proposed model is implemented for parallel processing using theano in python @xcite , @xcite , @xcite . in this section",
    ", we are presenting the performance result from comparison between simple rnn , lstm , and relu training methods for phenotype sequence prediction .",
    "the relu method is then compared with the well - known sparse partial least square ( spls ) method .",
    ".parameter setting for the experiments . [ cols=\"^,^,^\",options=\"header \" , ]     [ tab : mf_res ]     & train & @xmath182.75 & @xmath180.53 & + 1 & validation & @xmath186.56 & @xmath180.22 & + & test & @xmath184.66 & @xmath180.25 & + & train & @xmath180.62 & @xmath175.01 & + 2 & validation & @xmath185.25 & @xmath180.54 & + & test & @xmath179.58 & @xmath178.29 & +    [ tab : ann_res ]     1 & train & @xmath175.10 & @xmath169.52 & + & test & @xmath154.56 & @xmath151.53 & + 2 & train & @xmath180.53 & @xmath172.85 & + & test & @xmath169.82 & @xmath156.42 & +    [ t : spls ]",
    "in this paper , a novel model is proposed which utilizes matrix factorization and deep recurrent neural networks ( drnn ) for genotype imputation and phenotype sequences prediction .",
    "sine we are interested in keeping track of sequences with long - term dependencies in genomics , the state - of - the - art recited linear unit learning method is used .",
    "the performance results show the with the relu methods has a better performance in training comparing to the lstm - rnn and simple rnn methods .",
    "the relu learning methods also has less computational complexity comparing to the lstm method . for future research , it is interesting to analyze other recent advances in deep learning for genotype - phenotype application ; particularly that these algorithms are moving toward more simple designs which is suitable for big data application .",
    "j. marchini and b. howie , ",
    "genotype imputation for genome - wide association studies , \" _ nature reviews genetics , _ vol .",
    "499 - 511 , 2010 .",
    "y. v. sun and s. lr .",
    "kardia1 ,  imputing missing genotypic data of single - nucleotide polymorphisms using neural networks , \" _",
    "european journal of human genetics _ ,",
    "487 - 495 , 2008 .    sr . browning and bl .",
    "browning ,  rapid and accurate haplotype phasing and missing - data inference for whole - genome association studies by use of localized haplotype clustering ,  _ american journal of human genetics , _ vol .",
    "81 , pp . 10841097 , 2007 . y. li , cj .",
    "willer , j. ding , p. scheet , and gr .",
    "abecasis , `` mach : using sequence and genotype data to estimate haplotypes and unobserved genotypes , '' _ genetic epidemiology _ , vol .",
    "816834 2010 .",
    "howie , p. donnelly , and j. marchini ,  a flexible and accurate genotype imputation method for the next generation of genome - wide association studies ,  _ plos genetics 5 : e1000529 _ , 2009 .",
    "h. chun and s. kele ,  sparse partial least squares regression for simultaneous dimension reduction and variable selection , \" j r stat soc series b stat methodol , vol .",
    "1 , pp . 3 - 25 , 2010 .",
    "r. tibshirani ,  regression shrinkage and selection via the lasso , \" j. royal .",
    "soc b. , vol .",
    "267 - 288 , 1996 .",
    "j. baarb , et al .",
    "`` a novel protocol to investigate motor training - induced plasticity and sensorimotor integration in the cerebellum and motor cortex , '' _ journal of neurophysiology , _ 111.4 , pp .",
    "715 - 721 , 2014 .",
    "j. daligadu , et al .",
    "`` alterations in cortical and cerebellar motor processing in subclinical neck pain patients following spinal manipulation , '' _ journal of manipulative and physiological therapeutics , _ 36.8 , pp .",
    "527 - 537 , 2013 .",
    "y. li and a. ngom ,  a new kernel non - negative matrix factorization and its application in microarray data analysis ,  in _ proc .",
    "ieee symposium on computational intelligence in bioinformatics and computational biology , _ pp .",
    "371 - 378 , 2012 .",
    "p. binbin , l. jianhuang , and c. wen - sheng ,  nonlinear nonnegative matrix factorization based on mercer kernel construction ,  _ pattern recognition , _ vol .",
    "44 , iss . 1011 , pp .",
    "2800 - 2810 , 2011 .",
    "r. gribonval and k. schnass ,  dictionary identification  sparse matrix - factorization via @xmath45-minimization , \" _ ieee transactions on information theory , _ vol .",
    "56 , no . 7 , pp . 3523 - 3539 , 2010 .",
    "e. jones , e. oliphant , p. peterson , and et .",
    "`` scipy : open source scientific tools for python , '' 2001 , http://www.scipy.org/ [ online ; accessed 2015 - 04 - 07 ] j. bergstra , o. breuleux , f. bastien , and et .",
    "al . ,  theano : a cpu and gpu math expression compiler ,  in _ proceedings of the python for scientific computing conference ( scipy ) , _ austin , tx , 2010 .",
    "matlab and artificial neural networks toolbox , release 2012b , the mathworks , inc . , natick , massachusetts , united states .",
    "marquardt , d. ,  an algorithm for least - squares estimation of nonlinear parameters , `` _ siam journal on applied mathematics _ , vol .",
    "2 , pp . 431441 , june 1963 .",
    "a. ranganathan , ' ' the levenberg - marquardt algorithm , ",
    "june 2004 .",
    "f. bastien , p. lamblin , r. pascanu , j. bergstra , i. goodfellow , a. bergeron , n. bouchard , d. warde - farley and y. bengio . ?",
    "theano : new features and speed improvements ?",
    ". nips 2012 deep learning workshop .",
    "s. mahdavi - jafari , h. salehinejad , and s. talebi .",
    "`` a pistachio nuts classification technique : an ann based signal processing scheme . '' in computational intelligence for modelling control and automation , 2008 international conference on , pp .",
    "447 - 451 .",
    "ieee , 2008 .",
    "n. boulanger - lewandowski , y. bengio , and p. vincent , `` modeling temporal dependencies in high - dimensional sequences : application to polyphonic music generation and transcription , '' appearing in proc .",
    "icml , 2012 ."
  ],
  "abstract_text": [
    "<S> in analyzing of modern biological data , we are often dealing with ill - posed problems and missing data , mostly due to high dimensionality and multicollinearity of the dataset . in this paper , we have proposed a system based on matrix factorization ( mf ) and deep recurrent neural networks ( drnns ) for genotype imputation and phenotype sequences prediction . in order to model the long - term dependencies of phenotype data , the new recurrent linear units </S>",
    "<S> ( relu ) learning strategy is utilized for the first time . </S>",
    "<S> the proposed model is implemented for parallel processing on central processing units ( cpus ) and graphic processing units ( gpus ) . </S>",
    "<S> performance of the proposed model is compared with other training algorithms for learning long - term dependencies as well as the sparse partial least square ( spls ) method on a set of genotype and phenotype data with 604 samples , 1980 single - nucleotide polymorphisms ( snps ) , and two traits . </S>",
    "<S> the results demonstrate performance of the relu training algorithm in learning long - term dependencies in rnns .    </S>",
    "<S> genotype imputation ; phenotype prediction ; recurrent neural networks ; sequence learning ; </S>"
  ]
}