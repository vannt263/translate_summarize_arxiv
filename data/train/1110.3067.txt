{
  "article_text": [
    "in this appendix , we show that for the simple model represented by the likelihood function presented in equation , the fisher information given by reduces to the form claimed in . to show this",
    ", we first note that the likelihood for a vector @xmath84 of observations at times @xmath85 is given by a product of the likelihoods for each individual measurement , @xmath86 thus , the log - likelihood function is simply a sum over the individual log - likelihoods . since the derivative operator commutes with summation , we obtain that @xmath87 this in turn implies that the fisher information for a vector of measurements is given by the sum for each measurement of that measurement s fisher information .",
    "to calculate the single - measurement fisher information , we find the second derivative of the log - likelihood for a single measurement is given by @xmath88 thus , we find that the single - measurement fisher information is given by @xmath89 we conclude that @xmath90 , as claimed .    for the model with finite @xmath16 and limited visibility , given by the likelihood function",
    ", we can follow the same logic .",
    "we find the second derivative of with respect to @xmath2 gives us @xmath91 the expected value of this derivative then gives us the fisher information for a single measurement in the finite-@xmath16 model , @xmath92 taking the sum of this information then produces the cramer - rao bound given in .",
    "in this appendix , we derive expressions for posterior distributions under the assumption of a normally - distributed prior , and then apply these expressions to show the asymptotic scaling of the bayes risk .",
    "we also derive update rules that allow for expedient implementation of the greedy algorithm described in the main text .    under the assumption of a normally - distributed prior",
    ", all prior information about the parameter @xmath2 can be characterized by the mean @xmath53 and variance @xmath54 of the prior distribution .",
    "thus , we shall write our priors as @xmath93 to reflect the assumption of normality . then",
    ", the probability of obtaining a datum @xmath94 at time @xmath6 given such prior information is then given by @xmath95 applying bayes rule then produces the posterior distribution @xmath96 the mean and variance of this distribution are given by : @xmath97   & = \\frac{2 \\left((2 d-1 ) e^{-\\frac{1}{2 } \\sigma ^2 t^2 } \\left(\\sigma ^2 t \\sin ( \\mu   t)-\\mu   \\cos ( \\mu   t)\\right)+\\mu \\right)}{2-(2 d-1 ) \\left(1+e^{2 i \\mu   t}\\right ) e^{-\\frac{1}{2 } t \\left(\\sigma ^2 t+2 i \\mu \\right ) } } \\\\   \\mathbb{v}[\\omega | d , t ; \\mu , \\sigma^2 ] & = \\mu ^2+\\sigma ^2-\\frac{2 \\left((2 d-1 ) e^{-\\frac{1}{2 } \\sigma ^2 t^2 } \\left(\\sigma ^2 t \\sin ( \\mu   t)-\\mu   \\cos ( \\mu   t)\\right)+\\mu \\right)}{2-(2 d-1 ) \\left(1+e^{2 i \\mu   t}\\right ) e^{-\\frac{1}{2 } t \\left(\\sigma ^2 t+2 i \\mu \\right)}}\\\\   & \\quad -\\frac{2 ( 2 d-1 ) \\sigma ^2 t",
    "e^{i \\mu   t } \\left(\\sigma ^2 t \\cos ( \\mu   t)+2 \\mu   \\sin ( \\mu   t)\\right)}{(2 d-1 ) \\left(1+e^{2 i \\mu   t}\\right)-2 e^{\\frac{1}{2 } t \\left(\\sigma ^2",
    "t+2 i \\mu \\right)}}\\end{aligned}\\ ] ] to chose optimal times , we wish to pick @xmath6 so as to minimize the expected value over of the variance , where this expectation is taken over possible data .",
    "based on the previous expressions , we find that @xmath98 = \\sigma^2\\left(1 + \\frac { t^2 \\sigma^2 \\sin(\\mu t)^2}{- e^{t^2 \\sigma^2 } + \\cos(\\mu t)^2}\\right),\\ ] ] in agreement with equation .",
    "the risk envelope @xmath99 , and the risk @xmath100 for the examples where @xmath101 and @xmath102 ( left ) and @xmath103 ( right ) .",
    "note that as @xmath54 shrinks , there intersections between @xmath104 and @xmath105 ( marked by dots ) become more tightly packed.,title=\"fig : \" ]   the risk envelope @xmath99 , and the risk @xmath100 for the examples where @xmath101 and @xmath102 ( left ) and @xmath103 ( right ) .",
    "note that as @xmath54 shrinks , there intersections between @xmath104 and @xmath105 ( marked by dots ) become more tightly packed.,title=\"fig : \" ]    this expected variance , which describes our risk incurred by measuring at a given @xmath6 , is bounded below by an envelope @xmath106 .",
    "a pair of examples of the envelope @xmath107 and achievable risk @xmath108 is illustrated in figure [ fig : risk - envelope ] .",
    "note that the envelope is minimized by @xmath109 .",
    "moreover , the expected variance saturates the lower bound at intervals in @xmath6 of @xmath110 , but the width of the envelope s minimum grows as @xmath111 , so that as more measurements are performed , the bound becomes a good approximation for the minimum achievable risk .",
    "thus , in the asymptotic limit of large numbers of experiments , we have that the risk at scales with each step as the minimum of the envelope , @xmath112 we conclude that in the asymptotic limit , the risk decays as @xmath113 , where @xmath32 is the number of measurements performed .",
    "in this appendix , we state without derivation the update rules for @xmath53 and @xmath54 after obtaining a measurement result @xmath94 from an experiment performed at time @xmath6 , under the assumption of an normal prior . for the simple model described by equation , @xmath114 & = \\mu -\\frac{\\pi   ( 2 d-1 ) \\sigma ^2 ( -1)^{k } \\left(2 k-1\\right ) \\exp \\left(-\\frac{\\pi ^2 \\sigma ^2 \\left(1 - 2 k\\right)^2}{8 \\mu ^2}\\right)}{2 \\mu } \\\\   \\mathbb{v}\\left[\\omega |d\\right ] & = \\sigma ^2-\\frac{\\pi ^2 ( 1 - 2 d)^2 \\sigma ^4 \\left(1 - 2 k\\right)^2 \\exp \\left(-\\frac{\\pi ^2 \\sigma ^2 \\left(1 - 2 k\\right)^2}{4 \\mu ^2}\\right)}{4 \\mu ^2},\\end{aligned}\\ ] ] where @xmath115 $ ] is used to pick the intersection of @xmath99 and @xmath116 to the minimum of @xmath104 , as described in appendix b.    for the finite-@xmath16 model , the updated mean and variance are given by @xmath114 & = \\mu + \\frac{\\pi   ( 2 d-1 ) ( -1)^k ( 2 k-1 ) \\sigma",
    "^2 \\exp \\left(-\\frac{(\\pi -2 \\pi   k ) \\left(-2 \\pi   k \\sigma ^2 t_2 + 4 \\mu + \\pi   \\sigma ^2 t_2\\right)}{8 \\mu ^2 t_2}\\right)}{2 \\mu } \\\\   \\mathbb{v}\\left[\\omega |d\\right ] & = \\sigma ^2-\\frac{\\pi ^2 ( 2 d-1)^2 ( 2 k-1)^2 \\sigma ^4 \\exp \\left(-\\frac{(\\pi -2 \\pi   k ) \\left(-2 \\pi   k \\sigma ^2 t_2 + 4 \\mu + \\pi   \\sigma ^2 t_2\\right)}{4 \\mu ^2 t_2}\\right)}{4 \\mu ^2},\\end{aligned}\\ ] ] where in this case , @xmath117.\\ ] ]      barna , e. d.  laue , m. r.  mayger , j.  skilling , and s. j. p. worrall . exponential sampling , an alternative method for sampling in two - dimensional nmr experiments . , 73(1):6977 , june 1987 .",
    "http://dx.doi.org/10.1016/0022-2364(87)90225-3[doi:10.1016/0022-2364(87)90225-3 ] .",
    "chylla and john l. markley .",
    "theory and application of the maximum likelihood principle to nmr parameter estimation of multidimensional nmr data .",
    ", 5 , april 1995 . http://dx.doi.org/10.1007/bf00211752[doi:10.1007/bf00211752 ] .",
    "christopher ferrie , christopher  e granade , and d.  g cory .",
    "adaptive hamiltonian estimation using bayesian experimental design . 1443:165173 , 2011 .",
    "http://dx.doi.org/10.1063/1.3703632[doi:10.1063/1.3703632 ] .",
    "http://arxiv.org/abs/1107.4333[arxiv:1107.4333 ] .",
    "mark  w. maciejewski , harry  z. qui , iulian rujan , mehdi mobli , and jeffrey  c. hoch .",
    "nonuniform sampling and spectral aliasing .",
    ", 199(1):8893 , july 2009 . http://dx.doi.org/10.1016/j.jmr.2009.04.006[doi:10.1016/j.jmr.2009.04.006 ] ."
  ],
  "abstract_text": [
    "<S> projective measurements of a single two - level quantum mechanical system ( a qubit ) evolving under a time - independent hamiltonian produce a probability distribution that is periodic in the evolution time . </S>",
    "<S> the period of this distribution is an important parameter in the hamiltonian . here </S>",
    "<S> , we explore how to design experiments so as to minimize error in the estimation of this parameter . </S>",
    "<S> while it has been shown that useful results may be obtained by minimizing the risk incurred by each experiment , such an approach is computationally intractable in general . here , we motivate and derive heuristic strategies for experiment design that enjoy the same exponential scaling as fully optimized strategies . </S>",
    "<S> we then discuss generalizations to the case of finite relaxation times , @xmath0 .    </S>",
    "<S> _ introduction . </S>",
    "<S> _ measurement adaptive tomography has recently been suggested as an efficient means of performing partial quantum process tomography @xcite . </S>",
    "<S> little is known about optimal protocols when realistic experimental restrictions are imposed  as opposed to the case where one is allowed arbitrary quantum resources . </S>",
    "<S> indeed , even in the simplest examples , not even bounds have been given on the proposed protocols . </S>",
    "<S> here , we give analytic bounds on both non - adaptive and adaptive estimation protocols for a hamiltonian parameter estimation problem . </S>",
    "<S> moreover , we derive estimation protocols which asymptotically achieve these bounds . </S>",
    "<S> adaptive protocols are typically difficult to implement because a complex optimization problem must be solved after each measurement . </S>",
    "<S> we instead derive a heuristic that is easy to implement _ and _ achieves the exponentially improved asymptotic risk scaling of the optimal solution .    within the nuclear magnetic resonance ( nmr ) community </S>",
    "<S> , similar concerns have motivated the examination of the use of maximum entropy @xcite and maximum likelihood @xcite methods for obtaining spectra . recently , </S>",
    "<S> computational power has become available such as to make these methods feasible for use in analyzing non - uniform data obtained from high - dimensional nmr experiments @xcite . </S>",
    "<S> these studies have produced qualitatively similar strategies for how to best design experiments when each sample is expensive to collect .    </S>",
    "<S> the paper is organized as follows . </S>",
    "<S> first , we define the model hamiltonian which we want to estimate the parameters of , along with our metric of success . </S>",
    "<S> then we give both frequentist and bayesian lower bounds on the _ risk _ derived from this metric . </S>",
    "<S> finally , we derive strategies which achieve the asymptotic scaling of these bounds .    </S>",
    "<S> _ problem statement . _ </S>",
    "<S> the model we consider is a qubit evolving under the hamiltonian @xmath1 here @xmath2 is the unknown parameter whose value we want to ascertain . </S>",
    "<S> we make the problem dimensionless by assuming @xmath3 . </S>",
    "<S> an experiment consists of preparing a single known input state @xmath4 , evolving under the hamiltonian @xmath5 for a controllable time @xmath6 and performing a measurement in the @xmath7 basis . </S>",
    "<S> we emphasize here that we are assuming strong projective measurements on individual copies of a quantum preparation , rather than weak measurements on physical ensembles such as those studied in nmr experiments .    </S>",
    "<S> the outcomes of the measurement we label @xmath8 , where @xmath9 and @xmath10 refer to @xmath11 and @xmath12 , respectively . </S>",
    "<S> an experiment design consists of a specification of the time @xmath6 that we evolve a qubit under @xmath5 before we measure . the likelihood function for a given experiment @xmath6 </S>",
    "<S> is then given by the born rule @xmath13 and @xmath14 . </S>",
    "<S> using our model hamiltonian , we can express the likelihood more simply as : @xmath15 note that this model does not include noise . </S>",
    "<S> below , we somewhat generalize this model by including limited visibility and a @xmath16 dephasing process .    if we desire an estimate @xmath17 of the true value @xmath2 , a commonly used figure of merit is the _ squared error loss _ : @xmath18 the _ risk _ of an estimator , which is a function that takes data sets @xmath19 to estimates @xmath20 , is its expected performance with respect to the loss function : @xmath21 for squared error loss , the risk is also called the _ </S>",
    "<S> mean squared error _ ( mse ) .    </S>",
    "<S> _ mean squared error lower bound . _ </S>",
    "<S> the difficulty here is that the random outcomes of the measurements are _ not _ identically distributed . </S>",
    "<S> in fact , since they depend on the measurement time , each one could be different . </S>",
    "<S> although , asymptotic results exist for non - identically distributed random variables , these results are derived for insufficient statistics , such as the sample mean . </S>",
    "<S> moreover , we desire to provide computationally tractable heuristics that permit useful estimates with a finite number of samples .    </S>",
    "<S> although it is quite difficult to obtain exact expressions for the risk for arbitrary measurement times , in some cases we have obtained an asymptotically tight lower bound . </S>",
    "<S> for _ unbiased _ estimators , we can appeal to the cramer - rao bound @xcite @xmath22 where @xmath23 is called the fisher information . in our particular case , the fisher information reduces to quite a simple form in @xmath24 which is conveniently independent of @xmath2 ( a derivation is given in appendix a of [ arxiv - version ] ) . </S>",
    "<S> thus , the mean squared error is lower bounded by @xmath25 later we show that this bound becomes exponentially suppressed when we include noise in our model . in general , this quantity is dependent on the true parameter @xmath2 .    </S>",
    "<S> the bayesian solution considers the average of the risk , called the _ bayes risk _ , with respect to some prior @xmath26 : @xmath27 as in references @xcite , we choose a uniform prior for @xmath3 . </S>",
    "<S> then , the final figure of merit is the _ average _ mean squared error : @xmath28 the goal is to find a strategy which minimizes this quantity . </S>",
    "<S> although there exist bayesian generalizations of the cramer - rao bound @xcite , ours is independent of @xmath2 and thus remains unchanged by integrating equation over the parameter space : @xmath29 note also that , in general , bayesian cramer - rao bounds require fewer assumptions to derive than the standard ( frequentist ) bound . </S>",
    "<S> although they are the same for this model , they differ for a more general model considered later . in broad strokes , </S>",
    "<S> the difference in practice between bayesian and frequentist methods is averaging versus optimization . </S>",
    "<S> below we demonstrate a heuristic strategy which draws from both methods to achieve the goal of determining the measurement times which give the lowest possible achievable bound on the bayes risk .    </S>",
    "<S> _ looseness of the cramer - rao bound . </S>",
    "<S> _ as useful as the bayesian cramer - rao lower bound is , it is simple to see that it is not always achievable . </S>",
    "<S> we can obtain a lower bound by considering the best protocol we could possibly hope for in any two - outcome experiment . in such a protocol , one bit of experimental data provides exactly one bit of certainty about the parameter @xmath2 . if we learn the bits of @xmath2 in sequence , at each step @xmath30 , our risk is upper bounded by the worst - case where all the remaining bits of @xmath2 are either all 0 or all 1 . in either case , the error incurred by estimating a point between the two extremes is given by @xmath31 , leading to the best possible mse after @xmath32 measurements being @xmath33 , even though we can make a smaller cramer - rao bound by choosing times that grow faster than this exponential function . note that this risk is achievable via the standard phase estimation protocol @xcite , but that this protocol requires quantum resources which are _ not _ part of our model .    </S>",
    "<S> _ examples . </S>",
    "<S> _ let us consider a couple of examples for which the lower bound can be further simplified . </S>",
    "<S> first , consider the case when all the measurement times are the same . </S>",
    "<S> this is by far the simplest case , since the outcomes become identically distributed . </S>",
    "<S> recall @xmath3 </S>",
    "<S> . then , the measurement time should be less then the first nyquist time , @xmath34 , or the data will be consistent with more than one @xmath2 . </S>",
    "<S> that is , for @xmath35 ( but less than @xmath36 , say ) , the likelihood function will have two equally likely maxima . </S>",
    "<S> we minimize the risk , then , by choosing @xmath37 . </S>",
    "<S> then , the maximum likelihood estimator ( mle ) , for example , will be asymptotically efficient @xcite achieving the cramer - rao lower bound @xmath38    now consider a uniform grid of times . since @xmath3 , we should choose the nyquist sampling rate : @xmath39 . </S>",
    "<S> then , for any estimator @xmath40 using data collected at these measurement times , the cramer - rao bound gives @xmath41 again , the maximum likelihood estimator will be asymptotically efficient </S>",
    "<S> . however , since the likelihood function will have many local maxima , the maximum likelihood estimator is non - trivial to find as gradient methods are not guaranteed to work . </S>",
    "<S> bayesian estimators were derived in @xcite , where simulations yielded @xmath42 risk scaling which is asymptotically efficient .    </S>",
    "<S> note that since we are considering a uniform spacing of times , we can apply a fourier estimation technique without worrying about spectral aliasing introduced by non - uniformity @xcite . </S>",
    "<S> that is , we apply the discrete fourier transform and estimate the peak of the power spectrum . </S>",
    "<S> since the resolution in the frequency domain is @xmath43 , we expect the bayes risk to be @xmath44 the sampling theorem requires that we sample from a _ deterministic _ function , not a probability distribution . in practice , this condition is often approximately satisfied by sampling some stable statistic such as the mean value of the distribution at each time . </S>",
    "<S> this can be achieved by measuring at the same time until a sufficiently accurate estimate of the mean at that time is obtained , then repeating this for many other times . </S>",
    "<S> but as we have shown , this method can be quadratically improved by performing every _ single _ measurement at a different time .    </S>",
    "<S> _ exponentially achievable lower bound . </S>",
    "<S> _ it has been shown that bayesian adaptive solutions lead to risk decreasing exponentially with the number of measurements @xcite . </S>",
    "<S> however , these results are given by fits to numerical data . here , we give an analytic lower bound on the risk of these protocols .    the local ( in time ) bayesian adaptive protocol can be described as follows : ( 1 ) begin with a uniform prior @xmath45 and determine the first measurement time @xmath46 which minimizes the average ( over the two possible outcomes ) variance of the posterior distribution ; ( 2 ) perform a measurement at @xmath47 , record the outcome @xmath48 , and update the distribution @xmath49 via bayes rule ; ( 3 ) repeat step ( 1 ) replacing the current prior with the current posterior . </S>",
    "<S> note that the expected variance in the posterior is the bayes risk . </S>",
    "<S> thus , the protocol attempts to minimize the risk assuming the next measurement is the last . </S>",
    "<S> strategies that are local in this sense are called a _ </S>",
    "<S> greedy _ strategies , as opposed to strategies which attempt to minimize the risk over all future experiments .    for some choices of measurement times , including those given by the protocol above </S>",
    "<S> , the posterior will be approximately normally distributed . </S>",
    "<S> this is guaranteed in the asymptotic limit , but the posterior distribution near its peak is also remarkably well approximated by a gaussian after as few as 15 reasonably chosen measurements ( we found a uniform grid @xmath50 to be sufficient for `` warming up '' to the gaussian approximation ) . </S>",
    "<S> thus , we approximate the current distribution ( at given some sufficiently long measurement record @xmath51 ) as @xmath52 with some arbitrary mean @xmath53 and variance @xmath54 implied by @xmath51 . the expected posterior variance ( which is equal to the bayes risk ) of the probability distribution of the next measurement is @xmath55 ( derived in appendix b of [ arxiv - version ] ) which oscillates with frequency @xmath56 within an envelope @xmath57 . </S>",
    "<S> asymptotically , the minimum risk will approach the minimum of the envelope for all @xmath53 , but will be a lower bound on the risk otherwise . </S>",
    "<S> this minimum occurs at @xmath58 with a risk of @xmath59 , which is also the variance of the updated probability distribution since both outcomes are equally probable at @xmath60 . </S>",
    "<S> thus , at each measurement step we reduce the risk by @xmath61 . </S>",
    "<S> thus , the risk scales exponentially as @xmath62 and is achieved at measurement times which scale as @xmath63    these times are guaranteed to be optimal only in the asymptotic limit . for finite numbers of samples , we suggest two simple heuristics . </S>",
    "<S> first , we suggest the use of exponentially increasing times , where the base of the exponent is optimized offline , followed by the use of the maximum likelihood estimator for these times . </S>",
    "<S> second , we suggest a simpler adaptive scheme based on the assumption that the distribution remains gaussian after each measurement . making use of this normality assumption , we only need update equations for the mean and variance of the distribution over @xmath2 . in deriving the update equations </S>",
    "<S> , we also take into account the oscillations of the expected bayes risk by finding the nearest achievable minima to the one given by the lower bound . </S>",
    "<S> we provide the update equations in appendix c of [ arxiv - version ] .     the bayes risk  </S>",
    "<S> the average ( over a uniform prior ) mean ( over data ) squared error  of the strategies discussed in the paper . </S>",
    "<S> data points are at evenly spaced measurement numbers @xmath64 and the lines are linear interpolants to guide the eye . </S>",
    "<S> each data point is the average of @xmath65 simulations . in each figure , the noise parameter @xmath66 since its inclusion only gives a constant offset . from top to bottom , the relaxation characteristic time is @xmath67 . </S>",
    "<S> the thin solid lines indicate the lower bound given by equation .,title=\"fig : \" ] +   the bayes risk  the average ( over a uniform prior ) mean ( over data ) squared error  of the strategies discussed in the paper . </S>",
    "<S> data points are at evenly spaced measurement numbers @xmath64 and the lines are linear interpolants to guide the eye . </S>",
    "<S> each data point is the average of @xmath65 simulations . in each figure , the noise parameter @xmath66 since its inclusion only gives a constant offset . from top to bottom </S>",
    "<S> , the relaxation characteristic time is @xmath67 . </S>",
    "<S> the thin solid lines indicate the lower bound given by equation .,title=\"fig : \" ] +   the bayes risk  the average ( over a uniform prior ) mean ( over data ) squared error  of the strategies discussed in the paper . </S>",
    "<S> data points are at evenly spaced measurement numbers @xmath64 and the lines are linear interpolants to guide the eye . </S>",
    "<S> each data point is the average of @xmath65 simulations . in each figure , the noise parameter @xmath66 since its inclusion only gives a constant offset . from top to bottom </S>",
    "<S> , the relaxation characteristic time is @xmath67 . </S>",
    "<S> the thin solid lines indicate the lower bound given by equation .,title=\"fig : \" ]    _ generalization to finite @xmath16 . _ in practice , we will have to consider not only experimental restrictions but also noise and relaxation processes . </S>",
    "<S> processes which do not affect the quantum state can be effectively modeled by random bit - flip errors occurring with probability @xmath68 . </S>",
    "<S> processes which do affect the quantum state ( decoherence ) are modeled by an exponential decay of phase coherence only manifests as a contribution to @xmath16 . ] with characteristic time @xmath16 . </S>",
    "<S> since the state being measured lies in the @xmath69-plane of the bloch sphere , this loss of phase coherence manifests as an exponential decaying envelope being applied to the original likelihood ( [ eq : likelihood - no - t2 ] ) . </S>",
    "<S> the model is thus fully specified by the likelihood function @xmath70    the cramer - rao bound is now given by @xmath71 note that unlike the cramer - rao bound for the noiseless case , the above bound is not independent of @xmath2 and thus we must appeal to the bayesian cramer - rao bound so that the measurement times can be chosen independently of the true parameter . </S>",
    "<S> however , the bayesian bound turns out to be very loose . </S>",
    "<S> a sharper bound is given by first upper bounding each term in the denominator to give @xmath72 the noise term ( or visibility ) @xmath73 simply gives a constant reduction in the achievable accuracy . </S>",
    "<S> the relaxation process provides a more interesting dynamic as we see that the gains from longer times are exponentially suppressed . in other words , </S>",
    "<S> strategies are restricted to explore @xmath74 . </S>",
    "<S> we can thus do no better than @xmath75    the adaptive strategy discussed above can be generalized to include noise and relaxation but the expressions are more lengthy ( see appendix b of [ arxiv - version ] ) . to illustrate the performance of our adaptive strategy </S>",
    "<S> , we simulate the adaptive strategy along with offline strategies using identical times ( @xmath76 ) , linearly spaced times ( @xmath39 ) and exponentially sparse times ( @xmath77 ) . for each strategy , we perform simulations for experiments consisting of different numbers of samples @xmath32 , up to @xmath78 , and repeat each such simulation @xmath65 to obtain an estimate of the bayes risk for that strategy and experiment size . in fig . [ </S>",
    "<S> fig : results ] , we present the results of these simulations for the noiseless case , and for the cases @xmath79 and @xmath80 .    note that in all cases , the adaptive strategy achieves exponential scaling until the times selected reach @xmath81 . at that point </S>",
    "<S> , the risk will then scale linearly if the remaining measurement times are @xmath81 . </S>",
    "<S> however , if the protocol continues to select larger measurement times , the information gained from those measurements will tend to zero and the risk will remain constant .    _ summary and conclusions . </S>",
    "<S> _ by using the cramer - rao bound along with analytic expressions for the variance of each posterior distribution , we have motivated a heuristic method for choosing experiment designs that asymptotically admits exponentially small error scaling in the number of measurements . for finite measurements </S>",
    "<S> , we have relied on numerical simulation to demonstrate that this scaling is well - achieved even for @xmath82 . </S>",
    "<S> numerical simulations for finite @xmath16 , moreover , have suggested that we can enjoy exponential scaling of the risk until the measurement times saturate the @xmath16 bound , at which point the risk scaling switches to the asymptotic scaling of @xmath83 . in both cases , </S>",
    "<S> the heuristics used to design experiments are quite computationally tractable , thus motivating the utility of our heuristics to actual experimental practice .    </S>",
    "<S> _ acknowledgements . </S>",
    "<S> _ we thank miriam diamond for assistance in testing and developing the simulation software . </S>",
    "<S> cf thanks josh combes for helpful discussions . </S>",
    "<S> this work was financially supported by nserc and cerc . </S>"
  ]
}