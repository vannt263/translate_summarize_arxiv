{
  "article_text": [
    "multi - core and multiprocessor - based computers are increasingly used to support a wider range of parallel and distributed computing environments , such as multi - clusters , grid and more recently the cloud computing infrastructures . in these environments ,",
    "the productivity and performance gains largely depend on the effective exploitation of application parallelism across the available computing resources . due to the increasing scale and the dynamic nature of these modern computing platforms ,",
    "there is a need for more efficient scheduling strategies in order to effectively allocate the available computing resources to the parallel applications @xcite .",
    "since most multi - clusters , grid and cloud computing platforms have been built on top of the existing local resource management systems , a hierarchy of schedulers has been naturally established @xcite .",
    "a typical scheduling prototype in grid platforms is shown in fig .",
    "[ fig : grid ] . at the highest level",
    "there is a grid - level scheduler responsible for the management of the overall resources but it lacks the detailed knowledge about the local scheduling environment , where the parallel jobs will be eventually executed . to understand the scheduling complexity in such a hierarchical structure",
    ", one should note that the process involves several phases of scheduling at different levels , including determining available computing resources , determining task requirements , invoking a scheduler to determines how many processors are allocated to each task , and monitoring task execution .        to date",
    ", much work has been reported in the two - level scheduling paradigm @xcite , where a global scheduler focuses on efficiently allocating computing resources to a partition and a local scheduler focuses on effectively using the allocated resources to schedule the ready tasks in each partition .",
    "although these studies address certain important aspects of hierarchical scheduling , such as fairness and efficiency of resource allocation , little attention has been paid to the scalability of the scheduling algorithms with increasing hierarchical complexity in large - scale systems , such as today s grid or cloud computing platforms . in this paper , we present a more general hierarchical and adaptive scheduling model , where the structure of the system and the number of hierarchical levels are not restricted . each job is submitted into the system from a leaf node in the hierarchy , and each intermediate node in the hierarchy contains either a group of jobs or an aggregation of job groups . the objective is to design hierarchical scheduling algorithms that allocate the available processors from the root of the hierarchy down to the jobs in the leaf nodes through all intermediate levels to minimize the overall completion time , or the makespan .",
    "the tree - structured view of a system is a useful and general notion since its succinctly describes many different system architectures and helps us focus on the important similarities among different systems .",
    "for instance , a virtual tree - structured approach is recently used to model and evaluate the performance of grid computing infrastructures @xcite .",
    "while the machines in real systems often exhibit different forms of heterogeneity , we restrict our attention to the ones that consist of homogeneous processors or cores . due to advancement of virtualization technologies",
    ", we believe that the differences in these individual processors or cores can be effectively resolved , and are therefore hidden from the application interface , which makes our model a reasonable representation of many typical hierarchical systems .",
    "to formalize the scheduling problem in our study , we adopt the online and non - clairvoyant scheduling model , which requires the algorithm to operate in an online manner , that is , to make irrevocable scheduling decisions without any knowledge about the jobs future characteristics , such as their release times , processor requirements and remaining work .",
    "compared to the previous work , we present a general hierarchical scheduling paradigm for multiprocessor - based systems , which provides a flexible approach to hierarchically allocating computing resources for a set of parallel applications .",
    "in addition , to verify the effectiveness of different hierarchical scheduling algorithms , we develop a malleable parallel job model using a generic set of internal parallelism variations , which proves to be an effective tool to evaluate the performance of adaptive scheduling algorithms .",
    "the main contributions of our paper are summarized as follows :    * integrating a feedback - driven adaptive scheduler , a desire aggregation scheme and a resource allocation policy , we propose a hierarchical scheduling algorithm ac - ds , which scalably achieves @xmath0-competitiveness with respect to the makespan regardless of the number of hierarchical levels .",
    "in addition , we provide a generalized analysis framework for any hierarchical scheduling algorithm with certain performance guarantees . *",
    "based on a new malleable parallel job model , we conduct experiments to evaluate and compare three hierarchical scheduling algorithms .",
    "the results demonstrate that ac - ds achieves better and more stable performance than the other strategies , and in general feedback - driven algorithms outperform the simple strategy based on equal resource partitioning for a wide range of malleable workloads .",
    "the rest of this paper is organized as follows . in section 2 ,",
    "we formally define the hierarchical scheduling model .",
    "section 3 describes the hierarchical algorithm ac - ds and analyzes its performance with respect to makespan . in section 4 ,",
    "a malleable job model is first presented followed by the experimental evaluations that compare different scheduling strategies . in section 5",
    ", we review some related work .",
    "finally , section 6 concludes the paper and discusses some future directions .",
    "this section formally describes the hierarchical scheduling model . a set of @xmath1 parallel jobs , denoted by @xmath2 , arrive over time in an online manner .",
    "the jobs are assumed to be malleable , that is , they can be executed with a variable number of processors during runtime @xcite .",
    "( see section 5 for a detailed classification of parallel jobs . ) moreover , the parallelism or the number of ready threads of a job can also change during their executions . at any time",
    "@xmath3 , suppose the parallelism of a job @xmath4 is @xmath5 and the job is allocated @xmath6 processors , the execution rate @xmath7 for the job , that is , the amount of work done per unit of time , is given by @xmath8 . for each job",
    "@xmath9 , let @xmath10 denote its release time .",
    "we also define @xmath11 and @xmath12 to be its total work and total span , which are two important parameters representing the time to execute the job with one processor and infinite number of processors , respectively .",
    "the hierarchical system is organized as a tree structure with a single root and an arbitrary number of levels , denoted by @xmath13 .",
    "each job @xmath14 is released into the system at time @xmath10 from one of the leaf nodes at the bottom of the hierarchy .",
    "the problem is to design an online scheduling algorithm that allocates a total number of @xmath15 processors from the root of the tree down to the available jobs through all intermediate levels without any knowledge of the future job arrivals .",
    "the objective is to minimize the overall completion time of the jobs , or the makespan .",
    "note that when @xmath16 , i.e. , there are two levels , the problem is reduced to the classical makespan scheduling problem for malleable parallel jobs @xcite . hence , our hierarchical model represents a more general setting for the multiprocessor scheduling problem .",
    "while the tree structure reflects the characteristics of the system hierarchy , it is assumed to be fixed and hence can not be altered by the online algorithm during the executions of the jobs .",
    "moreover , we require an online algorithm to be non - clairvoyant , that is , it must make all scheduling decisions without knowing the characteristics of a job , such as its remaining work and span , as well as the future parallelism variations .",
    "finally , the scheduling decisions at each intermediate node can only be made with direct feedbacks from its immediate children , without knowing the decisions at the other nodes .",
    "therefore , the processors need to be allocated in a distributed manner .",
    "these additional challenges make the hierarchical scheduling problem significantly more complex than many classical scheduling problems , where only centralized decisions need to be made on a single root node with relatively flat system structures .",
    "we evaluate the performance of our online scheduling algorithms using both theoretical analysis and simulation study .",
    "theoretically , the performance is bounded using competitive analysis @xcite , which compares an online algorithm with the optimal offline scheduler which knows complete information of the jobs in advance .",
    "suppose the makespan of an online algorithm for a job set @xmath17 is @xmath18 and the makespan of the optimal offline algorithm for the same job set is @xmath19 .",
    "then the online algorithm is said to be @xmath20-competitive if @xmath21 holds for any job set @xmath17 . for the purpose of simulation , we develop a novel malleable parallel job model by augmenting an existing moldable job model @xcite with a set of generic parallelism variations .",
    "we use the new model to drive the simulations and to evaluate the performance and scalability of our scheduling algorithms .",
    "in this section , we present a hierarchical scheduling algorithm , which consists of an adaptive feedback - driven scheduler at the bottom level , a desire aggregation scheme at the intermediate levels , and a dynamic resource allocation policy for allocating the processor resources .",
    "we show that the algorithm achieves scalable performance with respect to the makespan , and we generalize its analysis framework to other scheduling algorithms that satisfy certain properties .",
    "we first present a bottom - level scheduler that interacts directly with the jobs and provides feedbacks to the higher level . for this purpose",
    ", we apply a feedback - driven scheduler , called a - control ( or ac for short ) @xcite , which predicts the processor requirements or desires for each job periodically after a pre - defined interval of time , commonly known as scheduling quantum .",
    "the processor desires are then provided to the higher - level scheduler for the readjustment of the jobs processor allocations in the next quantum .",
    "specifically , ac calculates the processor desire for a job in the next quantum based on the information collected in the current quantum , namely , the job s average parallelism .",
    "suppose in any scheduling quantum @xmath22 , which starts at time @xmath23 and lasts for @xmath24 amount of time , job @xmath4 completes @xmath25 amount of work and reduces its span by @xmath26 .",
    "due to the time - varying characteristic of the job s parallelism , the two parameters can be obtained by @xmath27 , and @xmath28 @xcite , where @xmath7 and @xmath5 denote the execution rate and the parallelism of job @xmath4 at time @xmath3 , respectively .",
    "then the average parallelism of the job during this quantum is given by @xmath29 , which is a well - known approach for calculating the average parallelism of a job .",
    "ac then directly utilizes this average parallelism as the job s processor desire for the next quantum .",
    "that is , the processor desire of the job for quantum @xmath30 is set to be @xmath31    the rationale behind this simple desire - calculation strategy is as follows : although the average parallelism of a job could change over time , its current parallelism is likely to be still representative of the job s resource requirement for the near future under reasonable assumptions about the quantum length and the job s parallelism variation . the initial processor desire for the job in the first scheduling quantum when it is just submitted into the system is simply set to be 1 . for the ease of analysis , we say that job @xmath4 is satisfied in quantum @xmath22 if the number of processors @xmath32 actually allocated to the job is at least its processor desire , i.e. , @xmath33 .",
    "otherwise , the job is said to be deprived if @xmath34 .",
    "we now present a desire aggregation scheme for any intermediate node that takes the processor desires from the lower levels and provides a feedback to the higher level .",
    "the scheme , called desire - sum ( or ds for short ) , collects the desires from the immediate children of the node , sums them up as its own desire and reports the sum to the parent node .    formally , suppose an intermediate node @xmath35 at level @xmath36 has @xmath37 immediate children at level @xmath38 in the system hierarchy .",
    "at the end of each quantum @xmath22 , the @xmath37 children report to node @xmath35 their processor desires for quantum @xmath30 , which are denoted as @xmath39 .",
    "then , node @xmath35 calculates the aggregate desire @xmath40 for quantum @xmath30 as follows : @xmath41    for the hierarchical scheduling problem , all levels may not share the same quantum length .",
    "for instance , compared to the lower levels , a higher level may need a substantially longer quantum in order to reduce the overhead in the reallocation of resources . in this paper , we make the reasonable assumption that the quantum length at a particular level can only be an integral multiple of that at the immediate lower level .",
    "suppose the quantum at level @xmath36 has not expired when the nodes at level @xmath38 report their desires , then these desires will be discarded , and only the most recent ones from the lower levels are considered when the quantum at level @xmath36 does expire .",
    "this is a reasonable strategy because as the number of levels increases , it is not likely that the execution status of the jobs in the distant past is still relevant to predict the future processor requirements .      finally , to allocate processors to the nodes at each level including the jobs at the bottom level , we apply a fair and efficient policy , called dynamic equi - partitioning ( or deq for short ) @xcite .",
    "deq allocates the processors received at any node to its immediate children based on their processor desires . generally speaking , it attempts to give a fair share of processors to each child , but for efficiency it does not allocate more processors to a child than what the child desires .",
    "for ease of analysis , we allow fractional processor allocation as in @xcite .",
    "this can be considered as time - sharing a processor among several concurrently running jobs .",
    "suppose the quantum for level @xmath36 expires at the end of quantum @xmath22 , and the node @xmath35 receives @xmath42 processors from the higher level at the beginning of quantum @xmath30 .",
    "let @xmath43 denote the set of @xmath37 children of node @xmath35 .",
    "the processors are then allocated to the children nodes in @xmath44 as described in algorithm 1 . for the processor",
    "desires and processor allocations . ]",
    "@xmath45 @xmath46 @xmath47 deq@xmath48    as can be seen from the pseudocode , the algorithm allocates the processors by first satisfying the children with small processor desires in a recursive manner , and then it gives an equal share to the remaining children with large desires . in line 3",
    ", the algorithm considers those children whose processor desires are not more than the current equal share @xmath49 , and they will be satisfied ( lines 8 - 10 ) . then , the policy is recursively invoked by excluding the jobs already satisfied and the processors already allocated . as the new equal share may be increased , the process above will be repeated until all jobs are satisfied ( lines 1 - 2 ) or no more job can be satisfied . in the latter case ,",
    "each of the remaining children will get the latest equal share ( lines 4 - 7 ) .    in a straightforward implementation of the algorithm",
    ", each iteration scans all remaining jobs and compares their processor desires with the current equal share . in the worst case , only one job will be satisfied and hence the algorithm will be invoked @xmath37 times .",
    "the time complexity of the algorithm is therefore @xmath50 .",
    "note that the deq policy is applied to all levels , including the jobs at the bottom level . at any particular level",
    ", it is only executed when the quantum for this level expires .",
    "the processors allocated to a node ( or job ) will stay with the node ( or job ) till the beginning of the next quantum when deq is invoked again . however , the lower levels may have smaller quantum lengths .",
    "hence , the processors could be reallocated among the nodes ( or jobs ) at the lower levels more frequently than at higher levels .      combining the adaptive feedback - driven scheduler",
    "ac , the desire aggregation scheme ds , and the processor allocation policy des , we obtain a hierarchical scheduling algorithm , which we call ac - ds . in this section ,",
    "we provide the performance analysis of the ac - ds algorithm when there is negligible cost for processor reallocation and all levels share the same quantum length .",
    "specifically , we show that the competitive ratio achieved by ac - ds in this case is scalable , that is , it does not increase with the number of levels in the hierarchy .",
    "experimental studies are performed for the more general cases with different quantum lengths and reallocation costs in the next section .",
    "before analyzing the performance of ac - ds , we first define two relevant concepts for the jobs . for any job @xmath14",
    ", we define @xmath51 to be its total satisfied time , that is , the overall execution time of the job whenever the job is satisfied , and define @xmath52 to be the job s total processor allocation , that is , the aggregate processor allocation the job receives throughout its execution . for convenience , we assume that the quantum length @xmath24 is normalized to @xmath53 .",
    "therefore , we can formally expressed the total satisfied time and the total processor allocation as @xmath54 $ ] and @xmath55 , where @xmath56 denotes the set of all scheduling quanta , @xmath57 denotes the set of satisfied jobs in quantum @xmath22 , and @xmath58 $ ] returns 1 if the proposition @xmath59 is true and 0 otherwise .",
    "we now introduce an important parameter , which is called the transition factor and is denoted by @xmath60 .",
    "this parameter indicates the maximum ratio on the average parallelism of any job over two adjacent quanta @xcite .",
    "specifically , let @xmath61 and @xmath62 denote the average parallelism of job @xmath4 in quantum @xmath22 and @xmath30 , respectively .",
    "then the average parallelism of the job should satisfy @xmath63 for any quantum @xmath22 .",
    "the following lemma , which was formally proven in @xcite , gives the bounds for the total satisfied time @xmath51 and total processor allocation @xmath52 of any job @xmath4 scheduled under ac in terms of the job s transition factor , work , and span .",
    "[ property ] for any job @xmath9 scheduled by the ac scheduler , its total satisfied time @xmath51 and total processor allocation @xmath52 are given by @xmath64 where @xmath11 and @xmath12 denote the work and the span of job @xmath4 , respectively , and @xmath20 denotes the transition factor of the job .",
    "note that the bounds shown in lemma [ property ] hold for any job scheduled by ac , regardless of the desire aggregation scheme and the processor allocation policy used at the higher levels .",
    "we will now rely on these two bounds to show the makespan performance of ac - ds .",
    "[ acds_theorem ] for the hierarchical scheduling problem with the same quantum length in all levels , the makespan @xmath18 for a job set @xmath17 scheduled by the ac - ds algorithm satisfies @xmath65 where @xmath19 denotes the makespan of the job set scheduled by the optimal offline algorithm .",
    "the performance is obtained by bounding the total satisfied time and the total deprived time of the last completed job in the job set , separately .",
    "let @xmath66 denote the last completed job in job set @xmath17 scheduled by ac - ds .",
    "then , the makespan is the same as the completion time of @xmath67 , which includes its release time @xmath68 , total satisfied time @xmath69 , and total deprived time @xmath70 .",
    "the total satisfied time of @xmath67 , according to inequality ( 3 ) , is given by @xmath71 .",
    "when @xmath67 is deprived , since all levels share the same quantum length , according to the desire aggregation scheme ds and the processor allocation policy deq , all the ancestors of @xmath67 , including the root node , in the hierarchy are also deprived .",
    "this is because if any ancestor of @xmath67 is satisfied , it could have satisfied all of its descendants , including @xmath67 , and this property holds regardless of the number of levels .",
    "hence , all @xmath15 processors must be allocated to the jobs in this case due to the deprivation .",
    "based on inequality ( 4 ) , the total deprived time of job @xmath67 is therefore bounded by @xmath72 .",
    "the makespan of the jobs , which is the completion time of @xmath67 , is then given by @xmath73 .",
    "since the optimal offline algorithm takes at least the span @xmath74 time to complete job @xmath67 and hence the whole job set after the release of @xmath67 , so we have @xmath75 . also , we have @xmath76 , since this is the time needed to complete all work of the jobs even when all processors are efficiently utilized without any waste @xcite .",
    "based on these two lower bounds , the theorem is directly implied .    under the reasonable assumption that the jobs have smooth parallelism variations , that is , their transition factor @xmath20 can be considered as a constant , theorem [ acds_theorem ] shows that the hierarchical scheduling algorithm ac - ds achieves @xmath0-competitiveness in terms of the makespan of the jobs .",
    "moreover , theorems [ acds_theorem ] also suggests that the competitive ratio of ac - ds does not increase with the number of levels in the hierarchy .",
    "hence , the algorithm is scalable and can be used to schedule malleable parallel jobs in any hierarchical system with the same performance guarantee .",
    "the reason of such scalability comes from the nice properties of the algorithm s three components , namely , the performance guarantee of the ac scheduler in terms of each individual job , the effectiveness of the ds scheme for aggregating the processor desires at the intermediate nodes , and the efficiency of the deq policy for allocating the processors throughout the hierarchy .",
    "[ acds_corollary ] the scheduling algorithm ac - ds scalably achieves @xmath0-competitiveness with respect to the makespan regardless of the number of hierarchical levels .      from the analysis of the ac - ds algorithm",
    ", we can observe that its competitive ratio is mainly determined by the properties of the ac scheduler at the job level , while the ds algorithm and the deq algorithm are designed to maintain the performance in the presence of scheduling hierarchies .",
    "based on this observation , we generalize its analysis in this section to other scheduling algorithms that can offer similar guarantees in terms of the running time and the processor allocations .",
    "let x - ds denote any hierarchical scheduling algorithm that uses scheduler x to calculate processor desires for each job and uses ds and deq for aggregating desires and allocating processors , respectively .",
    "as with the analysis of ac - ds , define @xmath51 to be the total satisfied time and define @xmath77 to be the total processor allocation for any job @xmath78 scheduled by x - ds .",
    "moreover , the two parameters can be bounded in terms of the job s work and span as follows : @xmath79    by following the analysis given in the previous section , we can easily bound the performance of the x - ds algorithm , which is stated in the corollary below .",
    "[ general ] the scheduling algorithm x - ds achieves @xmath80-competitiveness with respect to the makespan regardless of the number of hierarchical levels , provided that inequalities ( 6 ) and ( 7 ) are satisfied for each job .",
    "for instance , we can apply corollary [ general ] to the algorithm ag - ds , which uses another feedback - driven scheduler , called a - greedy ( or ag for short ) @xcite , at the bottom level . like ac",
    ", ag is also a quantum - based scheduler , but employs a multiplicative increase and decrease strategy that either doubles or halves the processor desire for a job in the next quantum depending on the job s processor utilization in the current quantum .",
    "it was shown in @xcite that the total satisfied time and the total processor allocation of any sufficiently large job under ag can be bounded by a constant factor in terms of the job s span and work respectively , i.e. , @xmath81 .",
    "hence , corollary [ general ] implies that ag - ds also achieves constant competitiveness with respect to makespan .    despite achieving @xmath0-competitiveness ,",
    "the processor desires predicted by the ag scheduler are , however , less stable compared to that of ac , even for jobs with constant parallelism profile @xcite",
    ". this will inevitably affect the performance of the algorithm by introducing extra scheduling overhead .",
    "in the next section , we will evaluate the practical performances of these algorithms in the hierarchical scheduling environment under the more general setting with variable quantum lengths and reallocation costs over different levels .",
    "in this section , we empirically evaluate the performance of our hierarchical scheduling algorithm presented in the previous section .",
    "we first present a malleable parallel job model derived from a traditional moldable job model augmented with a generic set of internal parallelism variations .",
    "we then build a hierarchical scheduling framework based on the new model and conduct a set of simulations to evaluate the scalability , utilization and makespan of our algorithm .",
    "we also compare ac - ds with a simple policy based on equal resource sharing and another feedback - driven adaptive scheduler at the bottom level .",
    "finally , we study the impact of different quantum patterns and reallocation costs on the performances of these schedulers .",
    "many parallel job models exist but very few of them generates malleable parallel jobs , which take the internal parallelism variations of the jobs into account . in this paper ,",
    "we derive a novel malleable parallel job model for the empirical evaluation of adaptive scheduling algorithms .",
    "our model is based on the traditional moldable job models , which generate parallel jobs whose processor allocations can not be changed over time once decided .",
    "hence , these models only provide external information about the jobs , such as their work requirements , arrival patterns , average parallelism , etc .",
    "the key task of constructing malleable job model is , therefore , to represent the internal parallelism variations of parallel programs over time . to achieve that , we divide a parallel job generated by a moldable job model into a series of phases and each phase is captured by an internal structure randomly selected from one of the seven generic forms of distinct parallelism variation curves we have identified .",
    "these curves include step , log , poly(ii ) , ramp , poly(i ) , exp and impulse functions as shown in fig .",
    "[ fig : profile ] .",
    "the step profile describes the stable parallelism requirement in a given period of time ; the impulse profile , on the other hand , represents the drastic variation of parallelism in instant time ; the ramp profile describes linear increasing and decreasing parallelism ; the exp , log and the two kinds of poly profiles describe sub - linear and super - linear changing parallelism , respectively .",
    "moreover , these kinds of parallelism variation curves can reflect a wide range of real parallel program running patterns .",
    "for instance , the impulse profile can emulate a drastic one - off increase in parallelism typically encountered in , e.g. , a short parallel for loop , while the step profile can represent a more stable data - parallel section of the job .",
    "the ramp profile as well as the other profiles can model increases in the job s parallelism with different rates for spawning parallel threads . fig .",
    "[ fig : pattern ] demonstrates several ideal running parallelism patterns through real parallel program segments . in the figure , function f0 ( ) represents a thread with a large amount of computation invoked repeatedly by four different functions constructing the given parallelism profiles . as shown in the fig .",
    "[ fig : pattern ] , function f1 ( ) consists of a fully parallelized for loop without interdependency profiling the step curve ; functions f2 ( ) , f3 ( ) and f4 ( ) recursively spawns themselves and other threads with different calling patterns , hence creating various rates of increasing parallelism .",
    "these kinds of internal parallelism profiles provide a flexible way to construct malleable jobs whose parallelism changes with time .",
    "however , it is also a challenge to maintain consistency with the original moldable job model .",
    "when implementing our model , we follow a basic rule to maintain such consistency and ensure that all kinds of internal variation curves are coherent with each other .",
    "specifically , we always maintain the same work , average parallelism and length for each phase regardless of the variation curve , as shown in fig .",
    "[ fig : profile ] .",
    "moreover , we combine a pair of increasing and decreasing profiles together to create a basic parallelism variation block . to ease generation ,",
    "we first construct the step profile which is the simplest one to ensure that the work and the average parallelism adhere to those initially generated from the moldable job model .",
    "then , other parallelism variation blocks are derived from the step profile by varying the degree of the internal parallelism curves but with the same phase length , work requirement and average parallelism .",
    "therefore , any combination of the variation curves for a job is ultimately consistent with the original moldable one .      based on the new malleable job model",
    ", we build a multi - level scheduling framework , which simulates the execution of parallel jobs on 256 processors . in this framework",
    ", we implement a request - allotment protocol to support the feedback mechanism and the processor reallocation among different levels .",
    "the number of levels @xmath13 in the system hierarchy is increased from 2 to 5 , and for a given @xmath13 the tree structure that represents the system is randomly generated with the number of children of each intermediate node uniformly selected from @xmath82 $ ] .",
    "each level has its independent scheduling quantum to aggregate processor requirements from its children and to readjust resource allocations . the quantum length at the bottom level , denoted by @xmath24 ,",
    "is set to be @xmath83 .",
    "the malleable workloads are generated by following downey s moldable job model @xcite , and the profile type of each internal phase is randomly selected with the phase length set to be @xmath84 . in downey",
    "s model , the job arrivals are modeled by a poisson process , and the arrival rate is related to the system load . the load of the system is in turn proportional to the number of jobs , which in our experiments is increased from 20 to 500 with an increment of 20 each time .",
    "each released job is randomly assigned to one of the leaf nodes in the system hierarchy .",
    "the parameters used in downey s model are listed in table 1 .",
    ".parameters used in downey s model [ cols=\"^,^\",options=\"header \" , ]     [ table : tb1 ]    besides implementing our hierarchical algorithm ac - ds , we also implement two other natural scheduling algorithms and compare their performances .",
    "the first one , called equi - equi , is based on the simple equi - partitioning ( equi ) scheduler @xcite that divides the received processors at each node evenly among its immediate children that still contain unfinished jobs .",
    "the other scheduler is the feedback - driven adaptive scheduler ag - ds introduced in section  [ sec : framework ] . to compare the performances of these scheduling algorithms , we use processor utilization and makespan as the metrics . for any algorithm ,",
    "its cost at a particular load is taken by carrying out the experiments 10 times and taking the average .",
    "\\(1 ) _ scalability of feedback - driven scheduling policies _",
    "the scalability of a hierarchical scheduling algorithm measures its capability to respond to the increasing number of hierarchical levels , which to a large extent reflects the complexity of the system . from the simulation results , which is shown in fig .",
    "[ fig : scalability ] , we can see that ac - ds achieves slightly better scalability than ag - ds .",
    "the makespan of ac - ds nearly converge to a single line when the number of levels increases from 2 to 5 . on the other hand , the performance of ag - ds experiences some instability with increasing number of levels . in particular , ag - ds exhibits slightly degrading performance when the number of levels reaches 4 , as shown in fig .",
    "[ fig : scalability : ag ] .",
    "the reason is that the task scheduler ac used by ac - ds provides a more stable and efficient feedback scheme than the scheduler ag used by ag - ds when calculating the processor requests .",
    "this influences the aggregate resource feedbacks and hence the overall performance of the algorithms .",
    "\\(2 ) _ utilization comparison of different scheduling policies",
    "[ fig : utilization ] shows the utilizations of ac - ds , ag - ds , and equi - equi when the number of levels increases from 2 to 5 . from the simulation results",
    ", we can see that the utilization of ac - ds is much better than that of the other two scheduling algorithms .",
    "the main reason is that ac - ds takes advantage of its stable scheduler in providing parallelism feedbacks and therefore has the ability to efficiently reallocate processors among the nodes and jobs .",
    "the simulation results also demonstrate that the utilizations of the two feedback - driven scheduling algorithms ac - ds and ag - ds are significantly better than that of equi - equi for a wide range of workloads .",
    "specifically , both ac - ds and ag - ds achieve a higher and more stable utilization that reaches nearly 90% . on the other hand ,",
    "the utilization of equi - equi is heavily influenced by the system load .",
    "the reason is that equi - equi is blind to the resource requirements at both job and node levels when allocating processors and thus it inevitably wastes a lot of processor cycles .",
    "only when the system is heavily loaded , the utilization gap of the three algorithms becomes smaller because in this case there are not enough processor resources to be reallocated , although some nodes or jobs may still have high processor requirements .",
    "the simulation results demonstrate that the feedback - driven adaptive schedulers are more suitable to the situation where the system has light to medium loads .",
    "when the system load is heavy , however , the benefit of adaptive scheduling may be offset by the cost of reallocation overhead .",
    "\\(3 ) _ makespan comparison of different scheduling policies _    due to the advantage of proactive parallelism feedbacks ,",
    "both ac - ds and ag - ds achieve better performance than equi - equi with respect to the makespan , as shown in fig .",
    "[ fig : comparison ] .",
    "note that in this set of figures , we present the makespan ratios by normalizing the makespans of the two feedback - driven schedulers with that of equi - equi for easier comparison . as we can see from the figure , ac - ds has better and more stable performance than the other two algorithms , especially with increasing number of hierarchical levels .",
    "for example , when the number of levels is 3 , the makespan of ac - ds improves over ag - ds by only 4% on average while the improvement becomes 16% on average when the number of levels is 4 .",
    "moreover , compared with the two feedback - driven schedulers , only when the system has a small number of jobs , equi - equi shows its advantages with slightly better makespan .",
    "the reason is that under light load almost all jobs can be easily satisfied by equi - equi , which provides sufficiently good performance without the need of adaptive processor allocation . with increasing system load",
    ", however , the competition for resources becomes more intensive among the nodes and the jobs .",
    "therefore , the performances of the feedback - driven algorithms become better than that of equi - equi , since they can dynamically adjust the processor allocations based on the jobs execution history . when the system load continues to become much heavier , as shown in fig .",
    "[ fig : comparison ] , the performances of ac - ds and ag - ds tend to converge to that of equi - equi , because in this case any job can only receive very few processors most of time , and thus frequent processor reallocations have no obvious benefits .",
    "\\(4 ) _ impact of scheduling quantum and reallocation cost",
    "_    for the feedback - driven algorithms , scheduling quantum and processor reallocation cost are important system parameters , which may significantly affect the overall performance . in this section ,",
    "we focus on evaluating the impact of these parameters on the performance of feedback - driven scheduling algorithms and compare them with that of equi - equi , whose reallocation cost is much lighter at runtime . in our simulation , we fix the number of levels to be 5 and the quantum length at a particular level is set to be @xmath85 , where @xmath86 denotes the level index that ranges from 2 to 5 , @xmath24 is the quantum length at level 2 , and @xmath87 is a quantum factor that denotes different quantum patterns .",
    "for example , when @xmath87 is set to be 1 , the quantum lengths of all levels are the same , namely @xmath24 .",
    "when @xmath87 is set to be 2 , the quantum lengths from the lowest level to the highest level are @xmath88 respectively .",
    "furthermore , to evaluate the impact of reallocation cost , we set the delay of reallocating a processor from one job to another to be proportional to the smallest quantum length @xmath24 , i.e. , @xmath89 , where @xmath90 is a cost factor set to be @xmath91 respectively .",
    "hence , the overall cost of reallocating @xmath59 processors from a job is given by @xmath92 .",
    "since the reallocation cost should be successively more expensive when climbing up the hierarchical tree , the corresponding delay at a high - level node is calculated by accumulating all its children s delay when its quantum expires .",
    "note that we only focus on ac - ds in this section , as ag - ds has similar results .",
    "we can see from fig .",
    "[ fig : diffquantum ] that different quantum patterns indeed have impacts on the performances of the scheduling algorithms .",
    "as shown in fig .",
    "[ fig : diffquantum : l4ac ] , compared with equi - equi , the makespan of ac - ds tends to become larger when @xmath87 increases .",
    "for example , the makespan ratio of equi - equi over ac - ds is @xmath93 on average when @xmath87 is set to be 1 while the ratio becomes only 1.02 on average when @xmath87 is set to be 6 .",
    "this means that the feedback - driven scheduling algorithm has more benefits when @xmath87 is small since they can adjust processor allocations more effectively in this case .",
    "however , smaller @xmath87 also leads to larger reallocation cost and hence affect performance .",
    "as shown in fig .",
    "[ fig : diffquantum : l4acwithcost ] , when the reallocation cost @xmath90 is set to be @xmath94 , the performance of ac - ds with @xmath95 clearly degrades compared to equi - equi , and the degradation is obviously more significant than the other values of @xmath87 . to clearly show the impact of reallocation cost on ac - ds , fig .",
    "[ fig : utilizationac ] gives the simulation results when the number of jobs is fixed to be 300 and the reallocation cost @xmath90 is varied from 0 to 1/2 . from these results , we can see that increases in reallocation cost has a larger impact for the relative performance of ac - ds when the quantum factor @xmath87 is small .",
    "for example , when @xmath90 is set to be 1/10 , the best makespan ratio of ac - ds is achieved at @xmath96 , while ac - ds achieves the best makespan ratio with @xmath97 when @xmath90 is changed to 1/2 . in summary",
    ", these simulation results suggest that if the reallocation overhead in the system is small enough , having a uniform quantum length across different levels will give the feedback - driven scheduling algorithms more benefits .",
    "otherwise , gradually increasing the length of the scheduling quantum when climbing up the system hierarchy seems to be a better option in order to achieve the optimal performance from the feedback - driven schedulers .",
    "in this section , we review some related work on parallel workload modeling and non - clairvoyant adaptive scheduling .    _ parallel workload modeling . _ according to the well - known classification by feitelson and rudolph @xcite , parallel jobs can be divided into three categories from the scheduling perspective , namely , rigid jobs , moldable jobs and malleable jobs .",
    "rigid jobs are often scheduled by static schedulers as they can not run on less or more processors than specified .",
    "moldable jobs can run on a variable number of processors , but it can not be modified once the job is started . hence , the initial decision by the scheduler will determine the overall system performance . for malleable jobs ,",
    "their processor allocation can be dynamically changed at runtime , and hence they provide the most flexibility for the schedulers to optimize performance .",
    "many existing parallel job models @xcite exist , but they only consider rigid and moldable jobs , and to the best of our knowledge no previous work has explicitly modeled malleable jobs .",
    "this paper provides a malleable parallel job model by specifying a generic set of interval parallelism variation curves .    _ adaptive scheduling . _",
    "to take advantages of malleable parallel jobs , adaptive scheduling has been extensively studied both theoretically and empirically in the literature . from theoretical perspective ,",
    "agrawal et al .",
    "@xcite studied adaptive scheduling using parallelism feedback .",
    "they proposed two adaptive schedulers , namely a - greedy and a - steal , based on a multiplicative - increase multiplicative - decrease strategy , and proved that both scheduling algorithms are efficient in terms of the running time and the processor waste for an individual job . in @xcite , he et al .",
    "combined a - greedy and a - steal with os allocator deq @xcite and proved that the resulting two - level adaptive schedulers agdeq and asdeq are o(1)-competitive in terms of the makespan . under the same two - level adaptive scheduling framework ,",
    "sun et al .",
    "@xcite proposed an improved adaptive scheduler acdeq , where the parallelism feedback is calculated by using an adaptive controller called a - control based on principles from the classical control theory .",
    "from algorithmic perspective , they proved that the two - level adaptive scheduler acdeq achieved a competitive ratio of o(1 ) with respect to the makespan .",
    "many empirical studies on adaptive scheduling also exist in the literature .",
    "agrawal et al .",
    "@xcite presented experimental results on feedback - driven adaptive schedulers .",
    "they showed that the feedback - driven schedulers indeed have superior performance than the schedulers without parallelism feedback .",
    "@xcite evaluated the performance agdeq under a wide range of workloads , and showed that it actually performs much better in practice than predicted by the theoretical bounds .",
    "using simulations , sun et al .",
    "@xcite also confirmed that the acdeq scheduler with more stable parallelism feedback outperforms the other feedback - driven algorithms in terms of both individual job performance and makespan for a set of jobs .",
    "in addition , weissman et al .",
    "@xcite , corbaln et al .",
    "@xcite and sudarson et al .",
    "@xcite have implemented various adaptive scheduling strategies on different platforms based on measurements of certain job characteristics such as speedup , efficiency , execution time , etc .",
    "all of them reported success in improving the system performances with adaptive scheduling .",
    "_ non - clairvoyant scheduling .",
    "_ we now review some related work for the non - clairvoyant scheduling scenario .",
    "non - clairvoyant scheduling was first introduced by motwani et al .",
    "@xcite in an attempt to design algorithms that are provably efficient for practical purposes . in multiprocessor environments ,",
    "a well - known non - clairvoyant scheduling algorithm is equi ( equi - partitioning ) @xcite , which equally shares the available processors among all active jobs . for the makespan minimization problem ,",
    "it was shown in @xcite that equi achieves a competitive ratio of @xmath98 when jobs are organized in two levels , where @xmath1 is the total number of jobs in the system , and that no better ratio is possible .",
    "two closely related work to ours in a similar setting are by robert et al .",
    "@xcite and sun et al .",
    "@xcite . in @xcite ,",
    "the authors considered a three - level hierarchy by organizing the jobs in different job sets and present an online scheduling algorithm equi@xmath99equi , which first splits evenly the available processors among the job sets , and then splits evenly the allocated processors among the jobs of each set .",
    "they considered the objective of set response time , i.e. , the sum of makespan of all sets , and prove that equi@xmath99equi achieves a competitive ratio of @xmath100 .",
    "the same performance metric was considered in @xcite , but the authors combined equi with the feedback - driven adaptive policies agdeq @xcite and acdeq @xcite to allocate the processor resources in a both fair and efficient manner under the non - clairvoyant setting . the proposed algorithm were shown to achieve @xmath0-competitiveness . finally , sun et al .",
    "@xcite generalized the result to an arbitrary number of hierarchical levels for the metric of set response time .",
    "in this paper , we have focused on the problem of hierarchical scheduling for malleable parallel jobs on multilayered computing systems .",
    "we proposed a feedback - driven adaptive scheduling algorithms , called ac - ds , and showed that it achieves competitive and scalable performance in terms of makespan .",
    "a novel malleable job model is developed to verify the effectiveness of this algorithm .",
    "the results demonstrate that our algorithm has good scalability with increasing number of hierarchical levels and it outperforms two other natural schedulers for a wide range of workloads .",
    "this work is partially supported by china national hi - tech research and development program ( 863 project ) under the grants no .",
    "2011aa01a201 , 2009aa01a131 and natural science foundation of china under the grant no.61073011,61133004 , 61173039 .",
    "agrawal k , he y , leiserson ce ( 2006 ) an empirical evaluation of work stealing with parallelism feedback . in : proceedings of the international conference on distributed computing systems , lisbon , portugal , pp 19 - 29 .",
    "sun h , cao y , hsu w - j ( 2009 ) competitive two - level adaptive scheduling using resource augmentation . in : proceedings of the workshop on job scheduling strategies for parallel processing , rome , italy , pp 1 - 24 .",
    "agrawal k , he y , hsu w - j et al ( 2006 ) adaptive scheduling with parallelism feedback . in : proceedings of the acm sigplan symposium on principles and practice of parallel programming , new york city , ny , usa , pp 100 - 109 .",
    "jann j , pattnaik p , franke h , wang f , skovira j and riordan j ( 1997 ) modeling of workload in mpps . in : proceedings of workshop on job scheduling strategies for parallel processing , geneva , switzerland .",
    "sudarsan r , ribbens cj ( 2007 ) reshape : a framework for dynamic resizing and scheduling of homogeneous applications in a parallel environment , international conference on parallel processing ( icpp ) , pp 44 - 44        sun h , cao y , hsu w - j ( 2011 ) fair and efficient online adaptive scheduling for multiple sets of parallel applications . in : proceedings of the international conference on parallel and distributed systems , tainan , taiwan , pp 38 - 45 ."
  ],
  "abstract_text": [
    "<S> the proliferation of multi - core and multiprocessor - based computer systems has led to explosive development of parallel applications and hence the need for efficient schedulers . in this paper , we study hierarchical scheduling for malleable parallel jobs on multiprocessor - based systems , which appears in many distributed and multilayered computing environments . </S>",
    "<S> we propose a hierarchical scheduling algorithm , named ac - ds , that consists of a feedback - driven adaptive scheduler , a desire aggregation scheme and an efficient resource allocation policy . from theoretical perspective </S>",
    "<S> , we show that ac - ds has scalable performance regardless of the number of hierarchical levels . </S>",
    "<S> in particular , we prove that ac - ds achieves @xmath0-competitiveness with respect to the overall completion time of the jobs , or the makespan . </S>",
    "<S> a detailed malleable job model is developed to experimentally evaluate the effectiveness of the proposed scheduling algorithm . </S>",
    "<S> the results verify the scalability of ac - ds and demonstrate that ac - ds outperforms other strategies for a wide range of parallel workloads .    </S>",
    "<S> hierarchical scheduling ; feedback - driven adaptive scheduling ; malleable parallel jobs ; multiprocessors </S>"
  ]
}