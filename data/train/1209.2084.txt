{
  "article_text": [
    "over the last decades the @xmath0-calculus has been connecting mathematics and physics in applications that span from quantum theory and statistical mechanics , to number theory and combinatorics ( see @xcite and references therein ) .",
    "its history dates back to the beginnings of the last century when , based on pioneering works of euler and heine , the english reverend frank hilton jackson developed the @xmath0-calculus in a systematic way @xcite .",
    "his work gave rise to generalizations of series , functions and special numbers within the context of the @xmath0-calculus @xcite .",
    "more important , he reintroduced the concepts of the @xmath0-derivative @xcite ( also known as jackson s derivative ) and introduced the @xmath0-integral @xcite .",
    "the @xmath0-derivative of a function @xmath1 of one variable is defined as @xmath2 where @xmath0 is a real number different from @xmath3 and @xmath4 is different from @xmath5 . in the limit of @xmath6 ( or @xmath7 ) , the @xmath0-derivative reduces to the classical derivative .",
    "let @xmath8 , for example . in this case , the classical derivative of @xmath9 is @xmath10 and the @xmath0-derivative is @xmath11x^{n-1}$ ] , where @xmath11 $ ] is the @xmath0-analogue of @xmath12 given by @xmath13= \\frac{q^n -1}{q-1}.\\ ] ] as @xmath6 , @xmath11 $ ] tends to @xmath12 .",
    "this definition is used to calculate the @xmath0-binomial and establish a @xmath0-analogue of taylor s formula that encompasses many results such as the euler s identities for @xmath0-exponential functions , gauss s @xmath0-binomial formula , heine s formula for a @xmath0-hypergeometric function , among others mathematical results @xcite .",
    "considering arbitrary functions @xmath1 and @xmath14 , the @xmath0-derivative operator satisfy the following properties @xcite :    1 .",
    "the @xmath0-derivative is a linear operator for any constants @xmath15 and @xmath16 @xmath17 2 .",
    "the @xmath0-derivative of the product of @xmath1 and @xmath14 is given by @xmath18 that , by symmetry , is equivalent to @xmath19 3 .",
    "the @xmath0-derivative of the quotient of @xmath1 and @xmath14 is calculated as @xmath20 or equivalently @xmath21    the chain rule for @xmath0-derivatives does not exist , except for a function of the form @xmath22 , where @xmath23 , with @xmath24 being constants .",
    "more details on the properties of @xmath0-derivatives can be found in @xcite .",
    "let a general nonlinear unconstrained optimization problem be defined as @xmath25 where @xmath26 is the vector of the independent variables and @xmath27 is the objective function .",
    "the steepest descent method ( also known as the gradient descent method ) uses information on the gradient of the objective function in seeking the optimum .",
    "the search direction is given by the negative of the gradient of @xmath28 .",
    "this search strategy is an obvious choice since along this direction the objective function decreases most rapidly .    requiring only information about first - derivatives ,",
    "the steepest descent method is attractive because of its limited computational cost and storage requirements @xcite .",
    "however , for multimodal functions , unless one knows in advance where to start from , the search procedure frequently gets stuck in one of the local minima .",
    "consequently , the steepest descent method is not recommended for real - world optimization problems that are usually multimodal .",
    "nevertheless , because of its inherent simplicity , it represents a good starting point for the development of more advanced optimization methods .    here",
    "we propose a generalization of the steepest descent method in which the gradient of the objective function is replaced by its @xmath0-analogue .",
    "accordingly , the search direction is taken as the negative of the @xmath0-gradient of @xmath28 . for @xmath29 ,",
    "the here called @xmath0-gradient method reduces to the classical steepest descent method . in order to evaluate the performance of the @xmath0-gradient method we consider three unimodal and three multimodal test functions commonly used as benchmarks",
    "we compare our results with those obtained with the genetic algorithms ( gas ) g3-pcx developed by deb et al .",
    "@xcite , and the spc - vsbx and spc - pnx developed by ballester and carter @xcite , which previous studies have shown to be effective in minimizing multidimensional unimodal and multimodal functions .",
    "the rest of the paper is organized as follows . in section [ sec : qgrad ] the @xmath0-gradient vector is defined . in section [ sec : qgradmethod ] the strategies to obtain the parameter @xmath0 and the step length are described .",
    "section [ sec : description ] shows the computational experiments and section [ sec : results ] discusses the results . finally , in section [",
    "sec : conclusions ] some conclusions and future work are presented .",
    "given a differentiable function of @xmath12 variables @xmath30 , the gradient of @xmath28 is the vector of the @xmath12 first - order partial derivatives of @xmath28 .",
    "similarly , the @xmath0-gradient is the vector of the @xmath12 first - order partial @xmath0-derivatives of @xmath28 .",
    "thus , let the parameter @xmath0 be a vector @xmath31 , where @xmath32 , the first - order partial @xmath0-derivative with respect to the variable @xmath33 is given by @xmath34 with @xmath35 and @xmath36 this framework can be extended to define the @xmath0-gradient of a function of @xmath12 variables as @xmath37\\ ] ] with the classical gradient being recovered in the limit of @xmath38 , for all @xmath39 .    the fig .",
    "[ fig : geometrico ] illustrates the geometric interpretation of the classical gradient and the @xmath0-gradient of a function of one variable @xmath40 . in this case , the gradient is simply the slope of the tangent line at @xmath4 .",
    "similarly , the @xmath0-gradient is the slope of the secant line passing through the points @xmath41 and @xmath42 . if the slope of the secant line is positive ( negative ) , the @xmath0-gradient points to the right ( left ) direction .",
    "-derivative of @xmath1 at @xmath43 and different values of the parameter @xmath0.,width=336 ]    since the slope of the tangent line ( dotted line ) at @xmath44 is positive , the steepest descent method at this point will move necessarily to the left and , thus , will be trapped by the local minimum at @xmath45 .",
    "the slope of the secant line passing through the points @xmath41 and @xmath42 can be positive or negative depending on the value of the parameter @xmath0 .",
    "for instance , if @xmath46 the @xmath0-derivative is negative at @xmath44 ( see the secant line passing through @xmath41 and @xmath47 in fig . [",
    "fig : geometrico ] ) , which potentially allows a minimization strategy based on the value of the @xmath0-gradient to take a leap to the right , towards the global minimum of @xmath9 .",
    "note that there is a value @xmath48 for which @xmath44 is a stationary point of the @xmath0-gradient ( @xmath49 ) but that does not correspond to any minimum or maximum of @xmath9 .",
    "the stationary points of the @xmath0-gradient are avoided in the method by generating a new the parameter @xmath0 .",
    "finally , for @xmath50 or @xmath51 the slope of the secant line is positive and the @xmath0-gradient method will move to the left as the steepest descent method .",
    "this simple example above shows that the use of the @xmath0-gradient offers a new mechanism to escape from local minima .",
    "moreover , the transition from global to local search might be controlled by the parameter @xmath0 , provided a suitable strategy for generating @xmath0-values is incorporated into the minimization algorithm .",
    "a general optimization strategy is to consider an iterative procedure that , starting from @xmath52 , generates a sequence @xmath53 given by @xcite @xmath54 where @xmath55 is the search direction and @xmath56 is the step length or the distance moved along @xmath55 in the iteration @xmath57 .",
    "optimization methods can be characterized according to the direction and step length used in eq .",
    "( [ eq : iterprocess ] ) .",
    "the steepest descent method sets @xmath58 and the step length @xmath56 is usually determined by a line - search technique that minimizes the objective function along the direction @xmath55 . in the @xmath0-gradient method , as here proposed , the search direction is the negative of the @xmath0-gradient of the objective function @xmath59 .",
    "thus the optimization procedure defined by eq .",
    "( [ eq : iterprocess ] ) becomes @xmath60 key to the performance of the @xmath0-gradient method , the strategies used to specify the parameter @xmath0 and the step length @xmath61 are described below .      considering a function of @xmath12 variables @xmath30 , a set of @xmath12 different parameters @xmath63 ( @xmath39 ) are needed to compute the @xmath0-gradient vector of @xmath28 .",
    "the overall strategy adopted here is to draw the values of @xmath64 ( or some variable related to them ) from some suitable probability density function ( pdf ) , and with a standard deviation that decreases as the iterative search proceeds . in this sense",
    ", the role of the standard deviation here is reminiscent of the one played by the temperature in a simulated annealing ( sa ) algorithm , that is , to make the algorithm go from a very random ( at the beginning ) to a very deterministic search ( at the end ) .    in the current implementation we opted to first draw the values of @xmath65 from a gaussian pdf given by @xmath66,\\ ] ] with @xmath67 and @xmath68 ; then , we computed the values of @xmath69 .",
    "starting from @xmath70 , the standard deviation of the pdf is decreased by the following `` cooling '' schedule , @xmath71 , where @xmath72 is the reduction factor .",
    "as @xmath73 approaches zero , the values of @xmath69 tend to unity , the algorithm reduces to the steepest descent method , and the search process becomes essentially local . as in a sa algorithm , the performance of the minimization algorithm depends crucially on the choice of parameters @xmath70 and @xmath74 .",
    "a too rapid decrease of @xmath73 , for example , may cause the algorithm to be trapped in a local minimum .",
    "the calculation of the step length @xmath61 is a tradeoff . on the one hand",
    ", @xmath61 should give a considerable reduction of the objective function .",
    "on the other hand , its calculation should not take too many evaluations of @xmath28 @xcite .",
    "steepest descent algorithms generally use line - search techniques to determine the step length @xmath75 along the steepest descent direction @xmath76 at the iteration @xmath57 . a first version of our algorithm @xcite applied the golden section for step length determination .",
    "however , traditional line - search algorithms , like the golden section , ensure that the condition @xmath77 is always satisfied , what obviously is a poor strategy when dealing with multimodal minimization problems .",
    "in addition , depending on the value of @xmath0 , the negative of the @xmath0-gradient may not point to the local descent direction .",
    "one way to circumvent these problems is to use a diminishing step length @xmath75 , i.e. , the initial step length @xmath78 is reduced by @xmath79 , where , for the sake of simplicity , @xmath74 is the same reduction factor used to compute @xmath73 .",
    "as the step length decreases ( and the values of @xmath69 , in parallel , tend to unity ) , a smooth transition to an increasingly local search process occurs . + the main idea behind the @xmath0-gradient method is to use the negative of the @xmath0-gradient of @xmath28 , instead of the negative of the classical gradient of @xmath28 , as the search direction .",
    "strategies for generating the parameter @xmath80 and the step length @xmath75 , at each iteration , complement this very simple algorithm .",
    "note that there are three free parameters to be specified , namely , @xmath70 , @xmath78 and @xmath74 .",
    "the initial standard deviation @xmath70 determines how stochastic is the search . for multimodal functions ,",
    "it must be high enough to allow the method to properly sample the search space .",
    "the reduction factor @xmath74 controls the speed of the transition from stochastic to deterministic search . a @xmath74 close to",
    "@xmath3 reduces the risk of being trapped in a local minimum .",
    "the last free parameter , the initial step length @xmath78 , depends heavily on the topology of the search space and , thus , requires some empirical exploration . in the end , as with the choice of the cooling schedule in a sa algorithm @xcite , an appropriate specification of the three free parameters is strictly dependent on the objective function .",
    "although a bad choice may lead to some deterioration in its performance , the @xmath0-gradient method has shown to be sufficiently robust to still be capable of reaching the global minimum .",
    "the performance of the @xmath0-gradient method was evaluated over six @xmath81-variable test functions ( @xmath82 ) commonly employed in the literature .",
    "we use the same experimental setup and stopping criteria as described in @xcite and @xcite in order to allow a direct comparison with their results .",
    "the stopping criteria are : maximum of @xmath83 function evaluations or @xmath84 .    as in @xcite and @xcite",
    ", we set the three free parameters for the @xmath0-gradient method after preliminary exploratory runs .",
    "the results presented here are for those which yielded the best performance .",
    "the benchmark consists of the following analytical functions :    1 .",
    "ellipsoidal function ( @xmath85 ) @xmath86 although the ellipsoidal function is convex and unimodal , it is an example of a poorly scaled function . 2 .",
    "schwefel s function ( @xmath87 ) @xmath88 the schwefel s function is an extension of the ellipsoidal function and it is also a unimodal and poorly scaled function .",
    "4 .    5 .",
    "generalized rosenbrock s function ( @xmath89 ) @xmath90.\\ ] ] although the rosenbrock s function is a well - known unimodal function for @xmath91 , numerical experiments have shown that for @xmath92 the function has two minima , the global one at @xmath93 and a local minimum that changes with the dimensionality @xmath12 @xcite .",
    "the rosenbrock s function is considered a test case for premature convergence once the global minimum lays inside a long , narrow , and parabolic shaped flat valley .",
    "6 .    7 .",
    "ackley s function ( @xmath94 ) @xmath95 the ackley s function is highly multimodal and the basin of the local minima increase in size as one moves away from the global minimum @xcite .",
    "rastrigin s function ( @xmath96 ) @xmath97 the rastrigin s function has a parabolic landscape away from the global minimum , but as we move towards the global minimum , the size of the basins increase .",
    "the function is highly multimodal and its characteristics are known to be difficult for many optimization algorithms to achieve the global minimum @xcite .",
    ".   11 . rotated rastrigin s function ( @xmath98 ) . @xmath99",
    "the rotated rastrigin s is a highly multimodal function without local minima arranged along the axis @xcite .",
    "12 .     for all these functions the global minimum is @xmath100 at @xmath101 , except the generalized rosenbrock s function where @xmath102 .",
    "the initial point set @xmath52 for each function is generated by a uniform distribution within @xmath103 $ ] , as used in @xcite and @xcite .",
    "extensive comparisons between the gas g3-pcx ( results obtained from @xcite for ellipsoidal , schwefel , rosenbrock and rastrigin functions ; and from @xcite for ackley and rotated rastrigin ) , spc - vsbx and spc - pnx ( results obtained from @xcite ) and the @xmath0-gradient method are presented in tables [ tab : unimodal ] and [ tab : multimodal ] . as in @xcite and @xcite ,",
    "the `` best '' , `` median '' and `` worst '' columns refer to the number of function evaluations required to reach the accuracy @xmath104 .",
    "when this condition is not achieved , the best value found so far for the test function after @xmath83 evaluations is given in column `` @xmath105 '' .",
    "the column `` success '' refers to how many runs reached the target accuracy , for unimodal functions , or ended up within the global minimum basin , for multimodal ones .",
    "the best performances are highlighted in bold in each table .",
    "the corresponding values of the best parameters @xmath70 , @xmath78 and @xmath74 used in each test function are given in table [ tab : parameters ] .",
    "llll functions & @xmath106 & @xmath107 & @xmath108 + ellipsoidal & @xmath109 & @xmath110 & @xmath111 + schwefel & @xmath112 & @xmath3 & @xmath113 + rosenbrock & @xmath112 & @xmath112 & @xmath114 + ackley & @xmath81 & @xmath115 & @xmath116 + rastrigin & @xmath117 & @xmath118 & @xmath114 + rotated rastrigin & @xmath119 & @xmath120 & @xmath121 +    lllllll function & method & best & median & worst & @xmath105 & success + & * g3-pcx * & @xmath122 & @xmath123 & @xmath124 & @xmath125 & @xmath126 + & spc - vsbx & @xmath127 & @xmath128 & @xmath129 & @xmath104 & @xmath130 + & spc - pnx & @xmath131 & @xmath132 & @xmath133 & @xmath104 & @xmath130 + & * @xmath0-gradient * & @xmath134 & @xmath135 & @xmath136 & @xmath125 & @xmath137 + & * g3-pcx * & @xmath138 & @xmath139 & @xmath140 & @xmath125 & @xmath126 + & spc - vsbx & @xmath141 & @xmath142 & @xmath143 & @xmath104 & @xmath130 + & spc - pnx & @xmath144 & @xmath145 & @xmath146 & @xmath104 & @xmath130 + & @xmath0-gradient & @xmath147 & @xmath148 & @xmath149 & @xmath104 & @xmath150 + & * g3-pcx * & @xmath151 & @xmath152 & @xmath153 & @xmath125 & @xmath154 + & spc - vsbx & @xmath83 & - & - & @xmath155 & @xmath156 + & spc - pnx & @xmath83 & - & - & @xmath157 & @xmath158 + & @xmath0-gradient & @xmath83 & - & - & @xmath157 & @xmath150 +    lllllll function & method & best & median & worst & @xmath105 & success + & g3-pcx & @xmath83 & - & - & @xmath159 & @xmath5 + & spc - vsbx & @xmath160 & @xmath161 & @xmath162 & @xmath157 & 10/10 + & spc - pnx & @xmath163 & @xmath164 & @xmath165 & @xmath157 & 10/10 + & * @xmath0-gradient * & @xmath166 & @xmath167 & @xmath168 & @xmath169 & @xmath137 + & g3-pcx & @xmath83 & - & - & @xmath170 & @xmath5 + & spc - vsbx & @xmath171 & @xmath172 & @xmath173 & @xmath104 & 6/10 + & spc - pnx & @xmath83 & - & - & @xmath174 & 0 + & * @xmath0-gradient * & @xmath175 & @xmath176 & @xmath177 & @xmath125 & @xmath178 + & g3-pcx & @xmath83 & - & - & @xmath179 & @xmath5 + rotated & spc - vsbx & @xmath83 & - & - & @xmath180 & @xmath5 + rastrigin & spc - pnx & @xmath83 & - & - & @xmath181 & @xmath5 + & * @xmath0-gradient * & @xmath182 & @xmath183 & @xmath184 & @xmath125 & @xmath185 +    in table [ tab : unimodal ] , for the ellipsoidal function , the @xmath0-gradient method achieved the required accuracy @xmath104 for all 50 runs , with an overall performance similar to the one displayed by the g3-pcx , the best algorithm among the gas .",
    "as for the schwefel s function , the @xmath0-gradient method again attained the required accuracy for all runs but was outperformed by the g3-pcx in terms of the number of function evaluations .",
    "finally , for the rosenbrock s function , the @xmath0-gradient was beaten by the g3-pcx ( the only to achieve the required accuracy ) but performed better then the two other gas .",
    "the overall evaluation of the @xmath0-gradient method performance in these numerical experiments with unimodal ( or quasi - unimodal ) test functions indicates that it reaches the required accuracy ( or the minimum global basin ) in @xmath186 of the runs , but it is not faster than the g3-pcx .",
    "this picture improves a lot when it comes to tackle the multimodal ackley s and rastringin s functions .    in table",
    "[ tab : multimodal ] , due to limited computing precision the required accuracy for the ackley s function was set to @xmath157 for the gas and @xmath187 in our simulations with double precision is equal to @xmath188e@xmath189 and not zero . ] .",
    "the @xmath0-gradient method was here clearly better than the gas , reaching the required accuracy in more runs or in less functions evaluations .",
    "for the rastrigin s function , the g3-pcx and the spc - pnx were unable to attain the global minimum basin .",
    "the other two algorithms reached the required accuracy @xmath104 , but the @xmath0-gradient method was the only to do it in @xmath190 of the runs ( 48 over 50 ) .",
    "finally , in the case of the rotated rastrigin s function , the @xmath0-gradient was the only algorithm to reach the minimum , attaining the required accuracy in @xmath81 out of @xmath191 independent runs .",
    "summarizing the results with multimodal functions , we may say that the @xmath0-gradient method outperformed the gas in all the three test cases considered , reaching the minimum with less function evaluations or in more independent runs .",
    "the main idea behind the @xmath0-gradient method is the use of the negative of the @xmath0-gradient of the objective function  a generalization of the classical gradient based on the jackson s derivative  as the search direction .",
    "the use of jackson s derivative provides an effective mechanism for escaping from local minima .",
    "the method has strategies for generating the parameter @xmath0 and the step length that makes the search process gradually shifts from global in the beginning to almost local search in the end .    for testing this new approach , we considered six commonly used 20-variable test functions . these functions display features of real - world optimization problems ( multimodality , for example ) and are notoriously difficult for optimization algorithms to handle .",
    "we compared the @xmath0-gradient method with gas developed by deb et al .",
    "@xcite , and ballester and carter @xcite with promising results .",
    "overall , the @xmath0-gradient method clearly beat the competition in the hardest test cases , those dealing with the multimodal functions .",
    "it comes without suprise the ( relatively ) poor results of the @xmath0-gradient method with the rosenbrock s function , a unimodal test function specially difficult to be solved by the steepest descent method .",
    "this result highlights the need for the development of a @xmath0-generalization of the well - known conjugate - gradient method , a research line currently being explored .",
    "+                      ballester , p.j . ,",
    "carter , j.n . : an effective real - parameter genetic algorithm with parent centric normal crossover for multimodal optimisation . in : proceedings of the genetic and evolutionary computation conference ,",
    ". 901913 .",
    "seattle , wa , usa ( 2004 )        soterroni , a. c. , galski , r. l. , ramos , f. m. : the @xmath0-gradient vector for unconstrained continuous optimization problems . in : operations",
    "research proceddings 2010 , pp . 365370 .",
    "springer - verlag berlin heidelberg ( 2011 )    locatelli , m. : simulated annealing algorithms for continuous global optimization . in : pardalos ,",
    "j.m . and romeijn , h. e. ( eds . ) handbook of global optimization ii , pp .",
    "kluwer academic publishers , dordrecht ( 2002 )"
  ],
  "abstract_text": [
    "<S> the @xmath0-gradient is an extension of the classical gradient vector based on the concept of jackson s derivative . here </S>",
    "<S> we introduce a preliminary version of the @xmath0-gradient method for unconstrained global optimization . </S>",
    "<S> the main idea behind our approach is the use of the negative of the @xmath0-gradient of the objective function as the search direction . in this sense , the method here proposed is a generalization of the well - known steepest descent method . </S>",
    "<S> the use of jackson s derivative has shown to be an effective mechanism for escaping from local minima . </S>",
    "<S> the @xmath0-gradient method is complemented with strategies to generate the parameter @xmath0 and to compute the step length in a way that the search process gradually shifts from global in the beginning to almost local search in the end . for testing this new approach </S>",
    "<S> , we considered six commonly used test functions and compared our results with three genetic algorithms ( gas ) considered effective in optimizing multidimensional unimodal and multimodal functions . for the multimodal test functions , the @xmath0-gradient method outperformed the gas , reaching the minimum with a better accuracy and with less function evaluations . </S>"
  ]
}