{
  "article_text": [
    "let @xmath0 be independent identically distributed ( i.i.d . )",
    "random variables ( r.v.s ) with a common distribution function @xmath1 with a real unknown parameter @xmath2 .",
    "an @xmath3-estimator of @xmath2 is defined as a statistic @xmath4 which is a solution w.r.t .",
    "@xmath5 of the estimating equation @xmath6 where @xmath7 is a suitably chosen function .",
    "for example , if @xmath2 is a location parameter in the normal family of distribution functions , the choice @xmath8 gives the mle ( maximum likelihood estimator ) .",
    "for the same problem , if @xmath9 the solution of reduces to the median of @xmath10 .",
    "in general , if @xmath11 is the probability density function ( or probability function ) of @xmath12 ( w.r.t . a @xmath13-finite measure",
    "@xmath14 ) then the choice @xmath15 yields the mle .",
    "suppose now that @xmath0 are not necessarily independent or identically distributed r.vs , with a joint distribution depending on a real parameter @xmath2 .",
    "then an @xmath3-estimator of @xmath2 is defined as a solution of the estimating equation @xmath16 where @xmath17 with @xmath18 .",
    "so , the @xmath7-functions may now depend on the past observations as well .",
    "for instance , if @xmath19 s are observations from a discrete time markov process , then one can assume that @xmath20 .",
    "in general , if no restrictions are placed on the dependence structure of the process @xmath19 , one may need to consider @xmath7-functions depending on the vector of all past and present observations of the process ( that is , @xmath21 ) .",
    "if the conditional probability density function ( or probability function ) of the observation @xmath22 given @xmath23 is @xmath24 , then one can obtain the mle on choosing @xmath25 besides mles , the class of @xmath3-estimators includes estimators with special properties such as robustness . under certain regularity and ergodicity conditions",
    ", it can be proved that there exists a consistent sequence of solutions of which has the property of local asymptotic linearity .",
    "( a comprehensive bibliography can be found in , e.g. , hampel at al ( 1986 ) and rieder ( 1994 ) . )",
    "if @xmath7-functions are nonlinear , it is rather difficult to work with the corresponding estimating equations , especially if for every sample size @xmath26 ( when new data are acquired ) , an estimator has to be computed afresh . in this paper",
    "we consider estimation procedures which are recursive in the sense that each successive estimator is obtained from the previous one by a simple adjustment .",
    "note that for a linear estimator , e.g. , for the sample mean , @xmath27 we have @xmath28 , that is @xmath29 , indicating that the estimator @xmath30 at each step @xmath26 can be obtained recursively using the estimator at the previous step @xmath31 and the new information @xmath32 .",
    "such an exact recursive relation may not hold for nonlinear estimators ( see , e.g. , the case of the median ) .    in general , the following heuristic argument can be used to establish a possible form of an approximate recursive relation ( see also jure@xmath33kov@xmath34 and sen ( 1996 ) , khasminskii and nevelson ( 1972 ) , lazrieva and toronjadze ( 1987 ) ) . since @xmath30 is defined as a root of the estimating equation , denoting the left hand side of by @xmath35 we have @xmath36 and @xmath37 .",
    "assuming that the difference @xmath38 is `` small '' we can write @xmath39 @xmath40 @xmath41 therefore , @xmath42 where @xmath43 .",
    "now , depending on the nature of the underlying model , @xmath44 can be replaced by a simpler expression .",
    "for instance , in i.i.d .",
    "models with @xmath15 ( the mle case ) , by the strong law of large numbers , @xmath45   = -i(\\theta)\\ ] ] for large @xmath26 s , where @xmath46 is the one - step fisher information .",
    "so , in this case , one can use the recursion    @xmath47    to construct an estimator which is `` asymptotically equivalent '' to the mle .",
    "motivated by the above argument , we consider a class of estimators @xmath48 where @xmath49 is a suitably chosen vector process , @xmath50 is a ( possibly random ) normalizing matrix process and @xmath51 is some initial value .",
    "if the conditional probability density function ( or the probability function ) of the observation @xmath52 given @xmath53 is @xmath54 , then one can obtain a ml ( maximum likelihood ) type recursive estimator on choosing @xmath55 ( the dot denotes the row - vector of partial derivatives w.r.t . @xmath56 and @xmath57 is the transposition ) .    note",
    "that while the main goal is to study recursive procedures with non - linear @xmath49 functions , it is worth mentioning that any linear estimator can be written in the form with linear , w.r.t .",
    "@xmath2 , @xmath49 functions . indeed , if @xmath58 where @xmath59 and @xmath60 are matrix and vector processes of suitable dimensions , then ( see section 4.2 for details ) @xmath61 which is obviously of the form with @xmath62    note also that in the iid case , can be regarded as a stochastic iterative scheme , i.e. , a classical stochastic approximation procedure , to detect the root of an unknown function when the latter can only be observed with random errors ( see remark 3.1 in sharia ( 2006a ) ) .",
    "a theoretical implication of this is that by studying the procedures , or in general , we study asymptotic behaviour of the estimator of the unknown parameter . as far as applications are concerned , there are several advantages in using .",
    "firstly , these procedures are easy to use since each successive estimator is obtained from the previous one by a simple adjustment and without storing all the data unnecessarily .",
    "this is especially convenient when the data come sequentially .",
    "another potential benefit of using is that it allows one to monitor and detect certain changes in probabilistic characteristics of the underlying process such as change of the value of the unknown parameter .",
    "so , there may be a benefit in using these procedures in linear cases as well .    in i.i.d .",
    "models , estimating procedures similar to have been studied by a number of authors using methods of stochastic approximation theory ( see , e.g. , khasminskii and nevelson ( 1972 ) , fabian ( 1978 ) , ljung and soderstrom ( 1987 ) , ljung et al ( 1992 ) , and references therein ) . some work has been done for non i.i.d .",
    "models as well .",
    "in particular , englund et al ( 1989 ) give an asymptotic representation results for certain type of @xmath32 processes . in sharia ( 1998 ) , theoretical results on convergence , rate of convergence and the asymptotic representation are given under certain regularity and ergodicity assumptions on the model , in the one - dimensional case with @xmath63 ( see also campbell ( 1982 ) , sharia ( 1992 ) , and lazrieva et al ( 1997 ) ) .",
    "we study multidimensional estimation procedures of type for the general statistical model . in sharia ( 2006a ) , imposing `` global '' restrictions on the processes @xmath7 and @xmath64 , we study `` global '' convergence of the recursive estimators , that is the convergence for an arbitrary starting value @xmath65 . in sharia ( 2006b )",
    ", we present results on the rate of the convergence . in this paper",
    "we are concerned with asymptotic behaviour of the estimators defined by .",
    "since the model considered is very general , the main objective is to prove that @xmath66 is locally asymptotically linear , that is , for each @xmath2 there exist a matrix process @xmath67 such that @xmath68 where @xmath69 in probability @xmath70 ( see section 2 for a more general definition ) .",
    "since @xmath71 is typically a martingale - difference , asymptotic distribution of an asymptotically linear estimator can be studied using a suitable form of the central limit theorem for martingales ( see e.g. , feigin ( 1985 ) , hutton and nelson ( 1986 ) , jacod and shiryayev ( 1987 ) .",
    "detailed discussion of the literature on this subject can be found in barndorff - nielsen and sorensen ( 1994 ) , heyde ( 1997 ) and prakasa - rao ( 1999 ) ) .",
    "for example , results in shiryayev ( 1984 ) ( see , e.g. , ch.vii , @xmath728 , theorem 4 ) show that under certain conditions , local asymptotic linearity implies asymptotic normality . in the standard case of i.i.d .",
    "observations , assuming that @xmath73 has zero mean and a finite second moment and @xmath74 for some non - random invertible @xmath75 , it follows that @xmath76 where @xmath77 in particular , in the case of likelihood recursion with @xmath78 if @xmath75 is the one - step fisher information , that is , @xmath79 it follows that @xmath66 is asymptotically normal with parameters @xmath80 , i.e. @xmath81 meaning that @xmath66 is asymptotically efficient .",
    "in general , in the case of one dimensional parameter @xmath2 , an estimator is said to be _ asymptotically efficient _ if it is asymptotically linear with @xmath82 where @xmath83 is the conditional fisher information .",
    "this kind of efficiency is called asymptotic first order efficiency .",
    "the motivation behind this general definition is the same as in the classical scheme of i.i.d . observations . for",
    "a detailed discussion of this notion see , e.g. , hall and heyde ( 1980 ) , section 6.2 .",
    "under relatively mild conditions , asymptotically efficient estimators are asymptotically equivalent to the mle @xmath84 , i.e. @xmath85 in probability ( see , e.g. , hall and heyde ( 1980 ) , section 6.2 , theorem 6.2 . ) . for the generalisation of these concepts see heyde ( 1997 ) .",
    "it is worth mentioning that the global convergence results for were obtained in sharia ( 2006a ) under conditions that allow @xmath50 to belong to quite a wide class of processes which does not directly depend on the choice of @xmath86 s ( see remark 3.1 below ) . in order to study the rate of convergence",
    ", one has to restrict the class of allowed @xmath50 s ( see sharia ( 2006b ) ) .",
    "it turns out that when dealing with local asymptotic linearity , one has to restrict this class even further - to an explicit choice of @xmath50 , depending on the choice of @xmath86 ( see remark 3.2(iv)(vii ) below ) . in other words ,",
    "the results of the paper tell one how to construct a locally asymptotically linear procedure with given @xmath86 s .",
    "the fact that one is restricted to this choice of @xmath87 is probably not very surprising in retrospective , but this issue does not seem to have been discussed in the existing literature .",
    "an estimator defined by is a recursive analogue of the corresponding @xmath3-estimator defined as a solution of the estimating equation .",
    "it should also be noted that the recursive procedure is not a numerical solution of .",
    "nevertheless , under quite mild conditions , the recursive estimator and the corresponding @xmath3-estimator are expected to have the same ( or equivalent ) asymptotic linearity expansions .",
    "it therefore follows that they are asymptotically equivalent , in the sense that , depending on the regularity and ergodicity properties of the underlying model , they both have the same asymptotic distribution .",
    "the paper is organized as follows .",
    "section 2 introduces the main objects and definitions .",
    "the main results are obtained in section 3 with various comments and explanations of the conditions used there . in section 4",
    "we give examples to illustrate the results of the paper .",
    "let @xmath88 be observations taking values in a measurable space @xmath89 equipped with a @xmath13-finite measure @xmath90 suppose that the distribution of the process @xmath91 depends on an unknown parameter @xmath92 where @xmath93 is an open subset of the @xmath94-dimensional euclidean space @xmath95 .",
    "suppose also that for each @xmath96 , there exists a regular conditional probability density of @xmath91 given values of past observations of @xmath97 , which will be denoted by @xmath98 where @xmath99 is the probability density of the random variable @xmath100 without loss of generality we assume that all random variables are defined on a probability space @xmath101 and denote by @xmath102 the family of the corresponding distributions on @xmath103    let @xmath104 be the @xmath13-field generated by the random variables @xmath105 by @xmath106 we denote the @xmath94-dimensional euclidean space with the borel @xmath13-algebra @xmath107 .",
    "transposition of matrices and vectors is denoted by @xmath57 . by @xmath108",
    "we denote the standard scalar product of @xmath109 that is , @xmath110 and the corresponding norm is denoted by @xmath111 .",
    "suppose that @xmath112 is a real valued function defined on @xmath113 .",
    "we denote by @xmath114 the row - vector of partial derivatives of @xmath115 with respect to the components of @xmath2 , that is , @xmath116 the @xmath117 identity matrix is denoted by @xmath118 .    if for each @xmath96 , the derivative @xmath119 w.r.t .",
    "@xmath2 exists , then we can define @xmath120 and the process @xmath121 ( with the convention @xmath122 ) .",
    "let us denote @xmath123 the _ one step conditional fisher information matrix _ for @xmath96 is defined as @xmath124 note that the process @xmath125 is _ `` predictable '' _ , that is , the random variable @xmath126 is @xmath127 measurable for each @xmath128 note also that by definition , @xmath125 is a version of the conditional expectation w.r.t . @xmath129",
    "that is , @xmath130 everywhere in the present work conditional expectations are meant to be calculated as integrals w.r.t . the conditional probability densities .",
    "the _ conditional fisher information _ at time @xmath131 is @xmath132    we say that @xmath133 is a sequence of estimating functions and write @xmath134 , if for each @xmath135 @xmath136 is a borel function .",
    "let @xmath137 and denote @xmath138 we write @xmath139 if   @xmath71 is a martingale - difference process for each @xmath92   i.e. ,   if @xmath140 for each @xmath141 ( we assume that the conditional expectations above are well - defined and @xmath142 is the trivial @xmath13-algebra ) .",
    "note that if differentiation of the equation @xmath143 is allowed under the integral sign , then @xmath144 .",
    "suppose that @xmath137 and @xmath145 is a predictable @xmath117 matrix process ( i.e. a matrix with predictable components @xmath146 ) with @xmath147 we say that an estimator @xmath148 is _ locally asymptotically linear _",
    "if for each @xmath149 @xmath150 and @xmath151 in probability @xmath152 where @xmath153 is a sequence of @xmath117 matrices such that @xmath154 in probability @xmath155 and @xmath156 weakly w.r.t .",
    "@xmath70 for some random matrix @xmath157 that is , @xmath148 is locally asymptotically linear if @xmath158 in probability @xmath70 , where @xmath159 is a linear statistic .",
    "+ 0.5 cm    * convention * _ everywhere in the present work @xmath160 is an arbitrary but fixed value of the parameter .",
    "convergence and all relations between random variables are meant with probability one w.r.t . the measure @xmath70 unless specified otherwise . a sequence of random variables @xmath161 has some property eventually if for every @xmath162 in a set @xmath163 of @xmath70 probability 1",
    ", @xmath164 has this property for all @xmath131 greater than some @xmath165 . _",
    "suppose that @xmath134 and @xmath166 , for each @xmath167 , is a predictable @xmath117 matrix process with @xmath168 , @xmath169 .",
    "consider the estimator @xmath170 defined by @xmath171 where @xmath172 is an arbitrary initial point .",
    "let @xmath160 be an arbitrary but fixed value of the parameter and for any @xmath173 define @xmath174 denote @xmath175 .",
    "then can be rewritten as @xmath176 where @xmath177 is a @xmath178-martingale difference .",
    "let @xmath179 and for @xmath169 denote @xmath180 where @xmath181 is defined by .",
    "then , @xmath182 it therefore follows that @xmath183 satisfies the recursive relation given by @xmath184 where @xmath185 and @xmath186 . by comparing equations and ,",
    "one can obtain the following result on the asymptotic relationship between @xmath148 and @xmath187    suppose that @xmath134 and there exists a sequence of invertible random matrices @xmath153 such that @xmath188 in probability @xmath70 and    ( e ) : :    @xmath189    weakly w.r.t .",
    "@xmath155 where @xmath190    is a random matrix with @xmath191    @xmath70-a.s . ;",
    "( 1 ) : :    @xmath192 in probability @xmath70 ; ( 2 ) : :    @xmath193    in probability @xmath70 , where    @xmath194    then @xmath195 in probability @xmath70 ( i.e. , @xmath196 is locally asymptotically linear ) .",
    "* to simplify notation we drop the fixed argument or the index @xmath2 in some of the expressions below .",
    "denote @xmath197 subtraction from yields the recursive relation @xmath198 denote @xmath199 and @xmath200.$ ] then the expression @xmath201 can easily be obtained by inspecting the difference between @xmath131th and @xmath202th term of this sequence ( exactly in the same way as in ) , to check that holds .",
    "now , ( 1 ) implies that @xmath203 in probability @xmath70 . also , by ( 2 ) , @xmath204 in probability @xmath70 .",
    "so , using ( e ) , it follows that @xmath205 in probability @xmath70 .",
    "@xmath206    next result gives sufficient conditions for ( 1 ) and ( 2 ) .",
    "* proposition 3.1 *    _ * ( a ) * suppose that @xmath153 in lemma 3.1 are diagonal matrices with non - decreasing ( w.r.t .",
    "@xmath131 ) elements and _    ( l1 ) : :    @xmath207 \\to 0\\ ] ]    in probability @xmath70 ;    then ( 1 ) holds .    *",
    "( b ) * suppose that @xmath153 in lemma 3.1 are diagonal non - random matrices , @xmath139 and    ( l2 ) : :    @xmath208 in probability @xmath70 ,    where @xmath209 is the @xmath210-th    diagonal element of the matrix @xmath153 and    @xmath211 is the @xmath210-th    component of @xmath212 which is defined in ( 2 )",
    ".    then ( 2 ) holds .    * ( c )",
    "* suppose that @xmath153 in lemma 3.1 are diagonal with non - decreasing elements @xmath213 @xmath139 and    ( ll2 ) : :    @xmath214    @xmath70-a.s . , where    @xmath211 is the @xmath210-th    component of @xmath212 which is defined in ( 2 )",
    ".    then ( 2 ) holds .    * proof .",
    "* see appendix a.    * remark 3.1 *    before analyzing the above results , let us understand how the procedure works .",
    "consider the maximum likelihood recursive procedure in the one - dimensional case @xmath215 where @xmath216 and @xmath217 is the conditional fisher information .",
    "+ 0.1 cm denote @xmath175 and rewrite the above recursion as @xmath218 then , @xmath219 where @xmath220 under usual regularity conditions ( see sharia ( 2006a ) remark 3.2 for details ) , @xmath221 and @xmath222 implying that @xmath223 for small values of @xmath224 .",
    "now , assuming that holds for all @xmath225 suppose that at time  @xmath226   @xmath227 that is , @xmath228 then , by , @xmath229 so , the next step @xmath148 will be in the direction of @xmath2 . if at time  @xmath226   @xmath230 by the same reason , @xmath231 so , on average , at each step the procedure moves towards @xmath2 .",
    "however , the magnitude of the jumps @xmath232 should decrease , for otherwise , @xmath148 may oscillate around @xmath2 without approaching it . on the other hand",
    ", care should be taken to ensure that the jumps do not decrease too rapidly to avoid failure of @xmath148 to reach @xmath233    + 0.1 cm these issues are addressed in sharia ( 2006a ) and the conditions are introduced to ensure global convergence of , that is , convergence for any arbitrary starting value .",
    "these conditions are flexible enough to allow for a quite wide choice of the normalising sequence @xmath64 for any particular @xmath7 .",
    "* remark 3.2 *    * ( i ) * as was mentioned above , strong consistency of the recursive estimator @xmath148 , that is the convergence @xmath234 ( @xmath70-a.s . )",
    "is established in sharia ( 2006a ) .",
    "here we are interested in the asymptotic behaviour of the recursive estimator given that it is consistent .",
    "note that although consistency is not formally required in lemma 3.1 , it is easy to see that if @xmath148 is not consistent , conditions ( 1 ) and ( 2 ) will be satisfied for very special cases only . note also that given that @xmath235 , conditions ( 1 ) and ( 2 ) are local in the sense that they are determined by local ( w.r.t .",
    "the parameter ) behaviour of the functions involved .    *",
    "( ii ) * condition ( e ) is an ergodicity type assumption on the statistical model .",
    "if @xmath236 ( the conditional fisher information ) and @xmath153 and @xmath190 are non - random , then the model is called ergodic . further discussion of this concept and related work appears in basawa and scott ( 1983 ) , hall and heyde ( 1980 ) @xmath72 6.2 , and barndorff - nielsen and sorensen ( 1994 ) .    *",
    "( iii ) * let us examine condition ( 2 ) in lemma 3.1 .",
    "given that @xmath235 , if the functions @xmath237 and @xmath166 are continuous w.r.t .",
    "@xmath2 ( with certain uniformity w.r.t .",
    "@xmath131 ) , we expect @xmath238 parts ( b ) and ( c ) in proposition 3.1 give sufficient conditions for ( 2 ) .",
    "if there exists a non - random sequence @xmath239 then obviously ( l2 ) is less restrictive then ( ll2 ) .",
    "but unfortunately , ( l2 ) can only be used for non - random @xmath153 . in the case of random @xmath153 , when ( ll2 ) may be used , just the convergence @xmath240 may not be enough since in many models the components of @xmath153 have the rate @xmath241 .",
    "in such cases one may also use the result on the rate of convergence of @xmath170 presented in sharia ( 2006b ) ( see examples 4.1 and 4.3 in the next section ) .    *",
    "( iv ) * condition ( 1 ) gives an important clue for an optimal choice of the normalizing sequence @xmath145 . to see this ,",
    "let us assume that @xmath139 so that @xmath242 and have a look at ( 1 ) and ( l1 ) in the case of one dimensional parameter @xmath243 now we can write @xmath244 in most applications , the rate of @xmath245 is @xmath241 and the best one can hope for is that @xmath246 is stochastically bounded .",
    "therefore we must at least have the convergence @xmath247 .",
    "given that @xmath248 we expect @xmath249 for large @xmath131 s .",
    "also , since @xmath250 , if @xmath251 is smooth in @xmath252 , we can write that @xmath253 so , denoting @xmath254 we expect @xmath255 where @xmath256 using the similar arguments , for the multidimensional case , we expect to hold for large @xmath131 s , where @xmath257 is the total differential of @xmath258 in @xmath259 therefore , @xmath260 is an obvious candidate for the normalizing sequence . if @xmath237 is differentiable in @xmath2 and differentiation of @xmath261 is allowed under the integral sign , then @xmath262 this implies that , for a given sequence of estimating functions @xmath263 another possible choice of the normalizing sequence is @xmath264 or any sequence with the increments @xmath265 also , if the differentiation w.r.t .",
    "@xmath2 of @xmath266 is allowed under the integral sign , then by the product rule , @xmath267 so , @xmath268 where , as before , @xmath269 therefore , denoting @xmath270 another possible choice of the normalizing sequence is @xmath271 or any sequence with the increments @xmath272 since typically , for each @xmath273 the process @xmath274 is a @xmath70  martingale , can be rewritten as @xmath275 where @xmath276 is the score martingale .    * ( v ) * part ( iv ) above highlights a very important point .",
    "suppose we wish to construct a recursive estimator with a given sequence @xmath7 of estimating functions .",
    "in order to achieve consistency , we are quite flexible in choice of the normalizing sequence @xmath64 ; the recursive procedure will converge even when @xmath64 sequence is not related to @xmath7 ( see sharia ( 2006a ) ) .",
    "( of course , the rate of the normalizing sequence still has to be `` right '' but is mostly determined by the model . )",
    "if we want to obtain a recursive estimator which is also asymptotically linear , then the normalizing sequence @xmath64 has to be ( or , , or a sequence asymptotically equivalent to ) .    * ( vi ) * let us consider a likelihood case , that is @xmath277 since @xmath278 the process in this case is the conditional fisher information @xmath279 so , the corresponding recursive procedure is @xmath280 also , given that the model possesses certain ergodicity properties , asymptotic linearity of implies asymptotic efficiency .",
    "in particular , in the case of i.i.d .",
    "observations , it follows that the above recursive procedure is asymptotically normal with parameters @xmath281 ( see corollary 4.1 in section 4 ) .    *",
    "( vii ) * normalizing sequences suggested in ( iv ) have been derived from the asymptotic considerations . in practice",
    "however , behaviour of @xmath64 sequence for the first several steps might also be important .",
    "this can happen when the number of observations is small or even moderately large . according to ( iv ) , to achieve asymptotic linearity",
    ", one has to choose a normalizing sequence @xmath64 with the property that @xmath282 for large @xmath131 s .",
    "so , we can consider any sequence of the form @xmath283 , where @xmath87 is one of the sequences introduced above ( by , , or ) , @xmath284 is a sequence of non - negative r.v.s such that @xmath285 eventually and @xmath286 is a suitably chosen constant . in practice ,",
    "@xmath284 and @xmath286 can be treated as tuning constants to control behaviour of the procedure for the first several steps ( see sharia ( 2006a ) , remark 4.4 ) . under certain assumptions , at each step , the recursive procedure , ( on average ) moves towards the direction of the unknown parameter ( see remark 3.1 or sharia ( 2006a ) , remark 3.2 for details ) .",
    "nevertheless , if the values of the normalizing sequence are too small for the first several steps , then the procedure will oscillate excessively around the true value of the parameter .",
    "on the other hand , too large values of the normalizing sequence will result in slower convergence of the procedure .",
    "a good balance can be achieved by using the tuning constants .",
    "the detailed discussion of these and related topics will appear elsewhere , but as a rough guide , the graph of @xmath148 against @xmath131 should ideally have a shape of those in figure 1 in sharia ( 2006a ) ( that is , a reasonable oscillation at the beginning of the procedure before settling down at a particular level ) .",
    "* 4.1 . * * the i.i.d",
    ". scheme . * consider the classical scheme of i.i.d .",
    "observations @xmath287 with a common probability density / mass function @xmath288 suppose that @xmath289 is an estimating function with @xmath290 let us define the recursive estimator @xmath170 by @xmath291 where @xmath51 is any initial value . according to remark 3.2 ( iv ) and the condition ( v ) below , an optimal choice of @xmath292 would be either @xmath293 or @xmath294 or any non - random invertible matrix function that satisfies conditions listed below .",
    "suppose that @xmath295 and consider the following conditions .",
    "( i ) : :    for any @xmath296    @xmath297 ( ii ) : :    @xmath298for each @xmath299    @xmath300 for some constant @xmath301 ( iii ) : :    @xmath298 @xmath292 is continuous in    @xmath233 ( iv ) : :    @xmath302 ( v ) : :    @xmath303    where @xmath304 as    @xmath305 for some @xmath306 .",
    "suppose that for any @xmath56 conditions * ( i ) * - * ( v ) * are satisfied .",
    "then the estimator @xmath307 is strongly consistent and   @xmath308-a.s . ) for any @xmath309 and any initial value @xmath65 .",
    "furthermore , @xmath170 is asymptotically normal with parameters @xmath310 , that is , @xmath311 in particular , in the case of the maximum likelihood type recursive procedure with @xmath312 and @xmath313 , the estimator @xmath307 is asymptotically efficient ( i.e. , asymptotically normal with parameters @xmath314 ) .",
    "* proof * see appendix a.    similar results ( for i.i.d .",
    "schemes ) were obtained by khasminskii and nevelson ( 1972 ) ( when @xmath315 and @xmath316 , ch.8 , @xmath724 ) and fabian ( 1978 ) .",
    "* linear procedures . *",
    "consider the recursive procedure @xmath317 where the @xmath87 and @xmath318 are predictable matrix processes , @xmath319 is an adapted process ( i.e. , @xmath319 is @xmath320-measurable for @xmath169 ) and all three are independent of @xmath233 the following result gives a sets of sufficient conditions for the asymptotic linearity of the estimator defined by in the case when the linear @xmath321 is a martingale - difference , i.e. , @xmath322 for @xmath323    suppose that @xmath324 and @xmath325 in probability @xmath70 , where @xmath326 then the recursive estimator defined by is asymptotically linear with @xmath327 where @xmath328 in probability @xmath329    * proof * let us check the conditions of lemma 3.1 for @xmath330 condition ( e ) trivially holds . then , since @xmath321 and @xmath331 we have @xmath332 therefore , ( 1 ) is equivalent to .",
    "then , it is easy to see that for @xmath333 defined in ( 2 ) we have @xmath334 implying that ( 2 ) holds which completes the proof .",
    "condition trivially holds if @xmath335 that is @xmath336 in this case , the solution of is @xmath337 this can be easily seen by inspecting the difference @xmath232 for the sequence ( exactly in the same way as in ) , to check that holds .",
    "also , since can obviously be rewritten as @xmath338 it follows that in this case , @xmath339 is indeed an obvious necessary and sufficient condition for @xmath148 to be asymptotically linear ( for arbitrary starting value @xmath65 ) .",
    "+ 0.5 cm    + 0.3 cm    * 4.3 . *",
    "* exponential family of markov processes * consider a conditional exponential family of markov processes in the sense of feigin ( 1981 ) ( see also barndorf - nielson ( 1988 ) ) .",
    "this is a time homogeneous markov chain with the one - step transition density @xmath340 where @xmath341 is a @xmath94-dimensional vector and @xmath342 is one dimensional .",
    "then in our notation @xmath343 and @xmath344 it follows from standard exponential family theory ( see , e.g. , feigin ( 1981 ) ) that @xmath345 is a martingale - difference and the conditional fisher information is @xmath346 a maximum likelihood type recursive procedure can be defined as @xmath347    now suppose that @xmath2 is one dimensional and the process belongs to the conditionally additive exponential family , that is , @xmath348 with @xmath349 where @xmath350 and @xmath351 ( see feigin ( 1981 ) ) .",
    "then , @xmath352 assuming that @xmath353 the likelihood recursive procedure is @xmath354     consistency and rate of convergence of the estimator derived by is studied in sharia ( 2006b ) . to ensure that has the same asymptotic properties as the maximum likelihood estimator",
    ", one has to impose certain restrictions on the @xmath75 and @xmath355 . in corollary a1 in appendix",
    "a , the conditions of section 3 written in terms of this model are presented .",
    "these conditions will be satisfied if there is a certain balance between requirements of smoothness on @xmath356 , the rate at which @xmath357 , and ergodicity of the model .",
    "for instance , suppose that the model is ergodic , that is , there exists a non - random sequence @xmath358 such that @xmath359 weakly .",
    "then @xmath360 will hold if the process @xmath361 converges to zero ( criterion based on the lenglart - rebolledo inequality , see ( l2 ) and formula ( a5 ) in appendix a ) .",
    "so , assuming that the estimator is consistent ( that is @xmath362 ) , by the toeplits lemma , the above will be guaranteed by the continuity of @xmath363 . on the other hand , if the model is non - ergodic , then one may need to impose smoothness of higher order on @xmath356 function ( see condition ( iii ) below ) and restrictions on the growth of the sequence @xmath355 ( see condition ( i ) below ) .",
    "the following result gives one possible set of sufficient conditions for the recursive estimator to be consistent and to have the same asymptotic properties as the maximum likelihood estimator .",
    "* proposition 4.3 *    _ suppose that @xmath364 and _    ( i ) : :    @xmath365 ( ii ) : :    there exists a constant @xmath366 such that    @xmath367    for each @xmath368 .",
    "( iii ) : :    the function @xmath369 is locally lipschitz ,    that is , for any @xmath2 there exists a constant    @xmath370 and @xmath371 such    that    @xmath372    for small @xmath373 s .    then @xmath148 defined by is strongly consistent ( i.e. , @xmath374-a.s . ) for any initial value @xmath375 .",
    "furthermore , @xmath376-a.s .",
    "for any @xmath3770,1/2[$ ] , and @xmath148 is asymptotically linear with @xmath378 where @xmath328 in probability @xmath329    * 4.4 . * * ar(m ) process * consider an ar(m ) process @xmath379 where @xmath380 @xmath381 and @xmath382 is a sequence of i.i.d .",
    "random variables .    in sharia ( 2006a )",
    "we discuss convergence of the recursive estimators of the form @xmath383 where @xmath384 and @xmath385 ( @xmath386 ) are respectively suitably chosen vector and matrix processes . if the probability density function of @xmath387 w.r.t .",
    "lebesgue s measure is @xmath388 then the conditional probability density function of @xmath91 given values of past observations of @xmath389 is obviously @xmath390 and so , @xmath391 it follows from the results of section 3 ( see remark 3.2 ( vi ) ) that an optimal choice of the normalizing sequence is the conditional fisher information @xmath392 , ( or any sequence with the increments equal to @xmath393 ) .",
    "it is easy to see that in this case , @xmath394 where @xmath395 since in this case the conditional fisher information can also be found recursively , a likelihood recursive procedure is @xmath396 for @xmath169 and an arbitrary starting point @xmath65 . the strong consistency of the estimators and , in particular , that of is studied in sharia ( 2006a ) .",
    "the class of estimators includes recursive versions of robust modifications of the least squares method .",
    "these are recursive estimators defined by @xmath397 where @xmath398 is a bounded scalar function and @xmath399 is a vector function of the form @xmath400 for some non - negative function @xmath112 of @xmath401    since is of the form with @xmath402 assuming that @xmath403 is differentiable ( almost everywhere w.r.t .",
    "lebesgue s measure ) we obtain @xmath404 so , according to lemma 3.1 ( see remark 3.2 ( iv ) formula ) , an optimal normalizing sequence @xmath87 for is @xmath405 where @xmath406 or a sequence with the increments equal to @xmath407    consider for instance a recursive m - estimator of the parameter of an ar(1 ) process defined as @xmath408 where @xmath409 and @xmath410 are scale estimates and @xmath411 is the huber function , @xmath412 and @xmath413 is a tuning constant .",
    "this is a recursive version of a robust generalized m - estimator of the parameter of an ar(1 ) process proposed by see denby and martin ( 1979 ) .",
    "another example is @xmath414 where @xmath415 is hampel s two - part redescending function @xmath416 with tuning constants @xmath417    for the procedure , @xmath418 and so @xmath419 similarly , for , @xmath420 @xmath421    below we present a brief simulation study .",
    "the time series were generated from the additive effect outliers ( ao ) model : @xmath422 where innovations @xmath423 are i.i.d .",
    "gaussian @xmath424 the variables @xmath425 are also i.i.d .",
    "with distribution @xmath426 where @xmath427 is the distribution that assigns probability @xmath428 to the origin . therefore , with probability @xmath429 the @xmath430 process @xmath431 is observed , and with probability @xmath432 the observation is the @xmath430 process @xmath431 plus the error with gaussian distribution @xmath433 . in this simulation , @xmath434 , @xmath435 and @xmath436 .",
    "the figures below show the performances of the estimator @xmath148 defined by , the estimator @xmath437 defined by and the least squares estimator @xmath438 ( which is equivalent to the recursive procedure defined by with @xmath439 ) .",
    "the estimators are computed for the series of length @xmath440 , with the additional @xmath441 observations at the beginning on which initial estimates are based ; as an estimates for @xmath409 and @xmath410 we take the median of the absolute values of the data and residuals respectively , divided by 0.6745 .",
    "the p.d.f .",
    "@xmath388 in and is replaced by the p.d.f . of @xmath442 and",
    "the values of the tuning constants are @xmath443 @xmath444 and @xmath445 .",
    "figure 1 shows single realizations and the mean squared errors over 300 replications of the estimators @xmath446 @xmath148 and @xmath437 for @xmath447 .",
    "further simulation study is required to study performances of these procedures .",
    "as this brief simulation suggests , both @xmath148 and @xmath437 outperform @xmath448",
    "this is a final part of a series of three papers ( see sharia ( 2006a ) and sharia ( 2006b ) ) .",
    "we have introduced estimation procedures which are recursive in the sense that each successive estimator is obtained from the previous one by a simple adjustment . to guarantee the convergence one has to impose global restrictions on the functions in ( w.r.t . the parameter @xmath2 ) such as a monotonicity type assumption and a restriction on the growth at infinity ( see sharia ( 2006a ) ) .",
    "this is the price one has to pay for the nice recursive structure .",
    "once the convergence is ensured , the rate of convergence ( see sharia ( 2006b ) ) and asymptotic linearity can be deduced from local ( in @xmath2 ) conditions . also , results presented give an explicit way of constructing a normalising sequence to ensure local asymptotic linearity .",
    "the rest relies on the ergodicity of the model .",
    "asymptotic properties such as asymptotic distribution and efficiency of recursive ( as well as non - recursive ) estimators depend on limit theorems possessed by the model .",
    "for example , in the i.i.d . case ( see corollary 4.1 ) , the central limit theorem and the law of large numbers imply that the corresponding recursive procedures are asymptotically normal and , in addition , the likelihood procedure is asymptotically efficient . in general , one can obtain asymptotic distribution and efficiency from asymptotic linearity ( lemma 3.1 ) and an appropriate central limit theorem .    the model considered in the paper",
    "is very general as we do not impose any preliminary restrictions on probabilistic nature of the observation process and cover a wide class of nonlinear recursive procedures for estimation of a multidimensional parameter .",
    "the results are new even for the case of a scalar parameter and provide a new insight even for the case of i.i.d . observations .",
    "while the advantage of this approach is its universality , verification of the conditions may be a nontrivial matter in some models .",
    "examples considered give a flavour of what is usually involved in this process and show where our restrictions come from .",
    "it is worth mentioning , that even in the cases where one has difficulties with verifying our conditions , the results of the paper can be used to determine the form of a recursive procedure ( in fact , an algorithm , see remark 3.2 ( iv)(vi ) ) , which is expected to have the same asymptotic properties as the corresponding non - recursive one defined as a solution of the equation .        to prove * ( a ) * , denote @xmath449\\ ] ] and @xmath450= a_t^{-1}\\sum_{s=1}^t\\;\\;a_s^{-1}\\chi_s.\\ ] ]",
    "applying the formula ( summation by parts ) @xmath451 with @xmath452 and @xmath453 we obtain @xmath454 then , @xmath455 where the last equality follows since @xmath456 is diagonal .",
    "therefore , @xmath457 finally , since @xmath245 s are diagonal with non - decreasing elements , applying the toeplits lemma to the components of the right hand side of latter formula we obtain that @xmath458 to prove * ( b ) * and * ( c ) * denote   @xmath459 since @xmath460 it follows from that @xmath461 is a martingale .",
    "denote by @xmath462 the @xmath210-th component of @xmath463 then the square characteristic @xmath464 of the martingale @xmath462 is @xmath465 and , by ( ll2 ) ,   @xmath466 it therefore follows that @xmath467 @xmath70 -a.s .",
    "( see e.g. , shiryayev ( 1984 ) , ch.vii ,  5 , theorem 4 ) .",
    "this proves ( c ) .",
    "now , use of the lenglart - rebolledo inequality ( see , e.g. , liptser and shiryayev ( 1989 ) , ch.1 , @xmath729 ) yields @xmath468 for each @xmath469 and @xmath470 then , by ( l2 ) ,   @xmath471 in probability @xmath70 .",
    "this implies that @xmath467 in probability @xmath472 and so , since @xmath245 is diagonal , ( 2 ) follows .",
    "@xmath473    * proof of corollary 4.1 * using corollary 4.1 in sharia ( 2006a ) it follows that ( i ) and ( ii ) imply @xmath474 .",
    "we have @xmath475 and @xmath476 it is easy to see that ( ii ) implies ( b2 ) from corollary 4.1 in sharia ( 2006b ) , and ( v ) implies that ( b1 ) of the same corollary holds with @xmath477 so , for any @xmath478 @xmath479 let us check that conditions of lemma 3.1 are also satisfied with @xmath480 condition ( ee ) trivially holds .",
    "according to proposition 3.1 , condition ( 1 ) follows from ( l1 ) . to check ( l1 ) , it is sufficient to show that @xmath481\\sqrt s \\to 0 , \\leqno{(a2)}\\ ] ] where @xmath482 by ( v ) ,   @xmath483   and @xmath484\\sqrt s   = \\sqrt s \\gm(\\theta)\\gm^{-1}(\\theta+\\dl_{s-1})\\alpha^\\theta(\\dl_{s-1 } ) = \\sqrt s\\|\\dl_{s-1}\\|^{1+\\ve}\\dl_s,\\ ] ] where , by ( iii ) and ( v ) , @xmath485 . then , @xmath486 which , by ( a1 ) ( since @xmath487 ) converges to zero",
    ". therefore , ( a2 ) is now a consequence of the toeplits lemma .",
    "for the process @xmath333 from ( l2 ) ( since @xmath488 ) , we have @xmath489 @xmath490 from ( iii ) and ( v ) we obtain that @xmath491   and @xmath492 as @xmath493 .",
    "so , using ( iv ) , it is easy to see that@xmath494 since @xmath495 ( l2 ) follows from the toeplitz lemma .    therefore , the conditions of lemma 3.1 hold for @xmath496 this implies that @xmath497 in probability @xmath155 where @xmath498 the asymptotic normality now obviously follows from the central limit theorem for i.i.d . random variables . @xmath473        ( i ) : :    @xmath500    where    @xmath501 ( ii ) : :    one of the following two conditions are satisfied ;    @xmath502    or    @xmath503    where    @xmath504 and    @xmath505 is a predictable process with    @xmath506      * proof .",
    "* let us check the conditions of lemma 3.1 for @xmath507 @xmath508 and @xmath509 since @xmath345 is a martingale - difference , we have @xmath510 and so @xmath511 and @xmath512 where @xmath513 then , since @xmath514 we have @xmath515 now , since @xmath516 it is easy to see that the first condition in ( ii ) implies ( 1 ) in lemma 3.1 and the second condition in ( ii ) implies ( l1 ) in proposition 3.1 .",
    "therefore , ( 1 ) holds .      *",
    "proof of proposition 4.3 * since , by ( iii ) , @xmath369 is obviously a continuous function , condition ( m2 ) of proposition 4.1 in sharia ( 2006b ) holds .",
    "also , ( m1 ) in the same proposition obviously follows from ( i ) .",
    "so , it follows that all the conditions of proposition 4.1 and corollary 4.2 in sharia ( 2006b ) are satisfied implying that @xmath519-a.s . ) . also , by ( i ) , @xmath520 implying that @xmath521 so , @xmath522 to establish asymptotic linearity , let us verify the conditions of corollary a1 is satisfied .",
    "since @xmath523-a.s . ) and @xmath524 , by ( iii ) we obtain that @xmath525 eventually .",
    "so , @xmath526 eventually .",
    "now , @xmath527 so , since @xmath528 we obtain that @xmath529 therefore , by the toeplits lemma , the second condition of ( ii ) holds .",
    "now , since @xmath333 is a martingale - difference , to verify ( i ) , it is sufficient to show that ( see e.g. , shiryayev ( 1984 ) , ch.vii ,  5 , theorem 4 ) @xmath530 since @xmath531 the above series can be rewritten as @xmath532 where , by ( iii ) , @xmath533 now , using ( a6 ) and continuity of @xmath369 we deduce that @xmath534 also , @xmath535 ( see sharia ( 2006b ) , appendix a , proposition a2 ) , implying that the above series converge which completes the proof .",
    "@xmath206           englund , j .- e . , holst , u. , and ruppert , d.(1989 ) .",
    "recursive estimators for stationary , strong mixing processes  a representation theorem and asymptotic distributions , _ stochastic processes appl .",
    "_ , * 31 * , 203222 .",
    "lazrieva , n. and toronjadze , t. ( 1987 ) .",
    "ito - ventzel s formula for semimartingales , asymptotic properties of mle and recursive estimation , _ lect .",
    "notes in control and inform .",
    "sciences , 96 , stochast .",
    "systems , h.j , engelbert , w. schmidt ( eds . ) , _ ( pp . 346355 ) . springer ."
  ],
  "abstract_text": [
    "<S> we consider estimation procedures which are recursive in the sense that each successive estimator is obtained from the previous one by a simple adjustment . </S>",
    "<S> the model considered in the paper is very general as we do not impose any preliminary restrictions on the probabilistic nature of the observation process and cover a wide class of nonlinear recursive procedures . in this paper </S>",
    "<S> we study asymptotic behaviour of the recursive estimators . </S>",
    "<S> the results of the paper can be used to determine the form of a recursive procedure which is expected to have the same asymptotic properties as the corresponding non - recursive one defined as a solution of the corresponding estimating equation .    </S>",
    "<S> _ department of mathematics + royal holloway , university of london + egham , surrey tw20 0ex + e - mail : t.sharia@rhul.ac.uk_    keywords : recursive estimation , estimating equations , stochastic approximation . </S>"
  ]
}