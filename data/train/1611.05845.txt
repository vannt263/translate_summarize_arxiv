{
  "article_text": [
    "bayesian optimisation ( bo ) @xcite is a powerful tool to optimise functions with some prior assumptions about the function .",
    "the common process in which bo is utilised is the gaussian process ( gp ) @xcite . in gp , points in the function to be optimised are assumed to be correlated with the points surrounding them .",
    "several algorithms have been proposed using gp . among them",
    "are gp probability improvement ( gp - pi ) @xcite , gp expected improvement ( gp - ei ) @xcite , and gp upper confidence bound ( gp - ucb ) @xcite . in these algorithms ,",
    "several samples of the functions are taken and then the posterior distribution of the function for the unseen positions is updated .",
    "the algorithms then compute a number of acquisition functions for every unseen position and take a sample from the position that gives the highest acquisition function .",
    "it has been shown theoretically that gp - ucb will eventually converge to the global maximum of a function , provided that the next sampled point is the point that has the highest upper confidence bound @xcite",
    ".    one major drawback of gp is that it needs an auxiliary search to look for the highest acquisition function value .",
    "simultaneous optimistic optimisation ( soo ) @xcite is an optimisation algorithm that does not need an auxiliary search .",
    "several methods also take advantage of soo and bo by combining them , such as in bamsoo @xcite and imgpo @xcite .",
    "however , these algorithms only work in a moderate number of dimensions and they do not perform well in higher dimensions .    in physical applications , there are several problems that require optimisation of functionals , instead of functions .",
    "simple examples are `` what is the trajectory of a ball to reach a point in the shortest amount of time ? '' ( brachistochrone ) , and `` what is the shape of a moving object to minimise the drag in a fluid ? '' .",
    "these problems can be simulated to calculate the functionals , but it is difficult to calculate the gradient of the functionals . as a result ,",
    "the optimisation algorithm required is one that does not compute gradients and yet can still work for functionals . in this paper , an extension of soo",
    "is presented to work for optimising functionals and it will be tested later to solve several physical problems .",
    "consider a function , @xmath0 with @xmath1 , a bounded set in @xmath2 .",
    "in most cases , @xmath3 is a hyper - rectangle with @xmath4 dimensions .",
    "the function has at least a global optimiser , @xmath5 of @xmath6 , and for all @xmath7 , it is assumed that @xmath8 . the function @xmath9 is a semi - metric function between two points in @xmath3 .    with the properties and assumptions above ,",
    "r. munos @xcite advised optimistic optimisation algorithms to find the global optimum value of @xmath6 .",
    "one of the algorithms , simultaneous optimistic optimisation ( soo ) , does not even need to have the knowledge of the semi - metric function , @xmath10 .",
    "soo is a tree - based search algorithm which partitions the search space , @xmath3 into several cells .",
    "initially , there is only one cell , which contains all the search space .",
    "the central point of the cell is evaluated .",
    "this is the root of the tree .",
    "then the cell is divided into @xmath11 smaller cells and the centre points of the cells are evaluated .",
    "the smaller cells become the children of the divided cell .",
    "if a cell has a centre value larger than any cells with larger or the same size , then the cell is divided .",
    "the tree s depth is limited by a function of the number of evaluations , @xmath12 .",
    "the pseudo - code of the algorithm is shown in [ alg : soo ] , where the @xmath13-th cell of depth @xmath14 is denoted as @xmath15 and the central value of the cell is @xmath16 .",
    "@xmath17 ; @xmath18 ; and @xmath19 any nodes in @xmath20 without children @xmath21 @xmath22 expand @xmath15 to @xmath11 children , evaluate the values of the children , and add them to @xmath20 @xmath23 ; @xmath24 any stopping conditions @xmath25    the regret of the algorithm after @xmath26 function evaluations is defined as @xmath27 , where @xmath28 is the maximum value found so far . for some functions ,",
    "the regret is even exponential , i.e. @xmath29 for some constant , @xmath30 .",
    "soo is also combined with gaussian processes to give better empirical performance and theoretical guarantee , as seen in bamsoo @xcite and imgpo @xcite .",
    "soo is a powerful algorithm to optimise function with moderate dimensions ( e.g. 10 dimensions ) .",
    "however , to optimise functionals that have infinite dimensions , some modifications are required .",
    "suppose that we have a 1d function , @xmath31 where @xmath32 , which is an input of a functional , @xmath33 .",
    "the problem considered here is to find a function , @xmath34 , to maximise the black - box functional , @xmath35 , i.e. @xmath36.$ ]    it is assumed that the end points of @xmath6 are fixed , i.e. at @xmath37 and @xmath38 , where @xmath39 .",
    "the functional is also assumed to be frchet differentiable and @xmath34 is twice differentiable . in order to solve it numerically ,",
    "the function @xmath40 is discretised at regular spacing points and then the number of discrete points in @xmath40 is gradually increased .",
    "the values of @xmath40 for @xmath7 is obtained by linearly interpolating the values from the discretised points .",
    "let one denote @xmath41 as the discretised function of @xmath40 at @xmath42 points , excluding the end points , with spacing @xmath43 , and then interpolate linearly .",
    "the number of discretised points will always be counted without the end points , unless indicated .",
    "the set of all possible discretised functions is denoted as @xmath44 , where @xmath45 .",
    "the discretised optimiser function , @xmath34 , is denoted by @xmath46 .",
    "the global optimiser in @xmath44 is defined as @xmath47 .",
    "$ ] the global optimiser in @xmath44 is not necessarily the same as @xmath48 , but @xmath49 as @xmath50 .",
    "[ assumption : locality - global - maximiser ] for a frchet differentiable functional , @xmath51 $ ] , there exists @xmath52 and @xmath53 where @xmath54 - j[f ] \\leq l |\\!| f _ * - f |\\!|^2_1 $ ] for all @xmath55 .",
    "[ lemma : distance - between - two - optima ] the @xmath56-distance between @xmath57 and @xmath34 is bounded by @xmath58 for some @xmath59 .",
    "the variable @xmath59 depends on the intrinsic parameters of @xmath35 and @xmath34 . from the lemma above",
    ", the search space can be decreased by @xmath60 as @xmath10 increased , provided @xmath59 is chosen to be large enough . based on lemma [ lemma : distance - between - two - optima ] , it is possible to perform optimisations using soo as in algorithm [ alg : soo ] with the use of several rules in dividing cells as below , letting @xmath61 .    1 .",
    "the initial cell is one dimensional with a width of @xmath62 and @xmath63 .",
    "the position of the central point of the cell represents the value of a point in @xmath64 .",
    "if all dimensions in the cell have lengths less than or equal to @xmath65 , then new @xmath66 dimensions are added to the cell .",
    "the new dimensions represent the position of the new points in the discretised @xmath67 which is placed in the middle positions of discretised points in @xmath41 .",
    "the new dimensions have widths of @xmath65 each .",
    "the value of @xmath10 of the cell is increased by 1 .",
    "a cell is divided along the longest dimension into @xmath68 smaller cells as in @xcite .",
    "if there are more than one dimension that have the same length , then the ` oldest ' dimension is chosen .",
    "[ rule : dimensions - dependency ] if a cell is divided along an ` old ' dimension , the change in position in the dimension will also change the position in the ` newer ' dimensions around it .    as an example let @xmath69 .",
    "the first cell has @xmath70 and only specifies @xmath71 points on @xmath6 , which is @xmath72 , the middle point between @xmath73 and @xmath74 . on the first division , the cell is divided into 3 smaller cells along the first dimension .",
    "after the division , the children s dimension width is now @xmath75 and it is equal to @xmath65 with @xmath76 and @xmath70 . therefore , @xmath77 new dimensions are added to the children cell which correspond to the middle points between @xmath73 and @xmath72 , and between @xmath72 and @xmath74 .    with @xmath78 and @xmath79 dimensions ,",
    "let one denote the discretised points as @xmath80 with @xmath81 .",
    "the second dimension is the oldest dimension because it was the same dimension as the dimension with @xmath70 .",
    "the first and third dimensions are the newer dimensions .",
    "as all the dimensions now has width of @xmath75 , the older dimension is divided first , i.e. @xmath82 . because an older dimension is divided , some of its children have different positions in the second dimension and thus change the positions in the first and third dimensions . if one of the children is shifted in the second dimension by @xmath83 with respect to its parent s position , then the first and third dimensions also shift by @xmath84 , because linear interpolation is used .    for the next division ,",
    "as the longest dimensions are the first and the third dimension , it can choose any dimension to be divided . as the divided dimension is the newest dimension",
    ", it does not shift the positions in other dimensions .",
    "the division process of cells is repeated until the algorithm stops .",
    "to test the performance of the algorithm , some numerical experiments on physical cases have been performed .",
    "the algorithm is benchmarked against soo @xcite , imgpo @xcite .",
    "those algorithms are also gradient - free optimisations , i.e. they do not need information about gradient of the functionals to optimise . as the other algorithms are designed for fixed dimensions , it is tested with 7 dimensions .",
    "the algorithms are tested for solving brachistochrone problems . given two points in 2d space , the algorithms need to determine the path between the points so that a bead starting from the first point can travel , in influence of gravity pointing in @xmath85-direction , to the other point as fast as possible .",
    "the gravity is assumed to be 1 . in the first test case ,",
    "the positions of the end points are @xmath86 and @xmath87 and the beads have initial velocity @xmath88 . the minimum time required in this case is @xmath89 . in the second case ,",
    "the points positions are @xmath86 and @xmath90 with the initial velocity and minimum time are @xmath91 and @xmath92 , respectively .",
    "the optimum shape of the brachistochrone problems is cycloid where it can be expressed as parametric functions , @xmath93 and @xmath94 with some @xmath95 .",
    "output of the functional is the time required to travel such shape and returns infinity if it is impossible to reach the other end .",
    "minimisation can be done by maximising the negative of the output value of the functional .",
    "another test case is the catenary problem , which is the minimum area when the line between two end points is rotated along the @xmath96-axis .",
    "the end points are located at @xmath97 and @xmath98 .",
    "the optimum shape of this problem can be expressed as a hyperbolic function , i.e. @xmath99 .    in soo and imgpo",
    ", a cell is divided into three smaller children along the longest dimension .",
    "the parameters that were used in imgpo are the parameters used in @xcite .",
    "the search space for the algorithms is confined to @xmath100 $ ] for each dimension . for the new algorithm , the initial bound",
    "is set to be @xmath101 $ ] and reduced by factor of 4 every time new dimensions are added .",
    "the performance of the algorithms in all test cases is shown in figure [ fig : results - log - regret](a)-(c ) .",
    "the solutions obtained by the new algorithm are also plotted with the optimum solution in figure [ fig : results - log - regret](d)-(f ) . as demonstrated in the figures , the new algorithm performs better than the other algorithms in all test cases .",
    "the solutions obtained by the new algorithm are also quite close to the optimum solutions . in the first case ,",
    "the solution found by the new algorithm consists of 15 points which is already more than the dimensions tested by other algorithms .",
    "the number of discretised points in the solutions can be increased by running the algorithm for longer .",
    "top : log regret versus number of functional evaluations using multi - level soo , soo , and imgpo for ( a ) the first brachistochrone problem , ( b ) the second brachistochrone problem , and ( c ) the catenary problem .",
    "bottom : comparison of the solutions obtained by the algorithm with the actual solutions for each case .",
    "the obtained solutions have 15 discrete points , excluding the end points.,title=\"fig : \" ]   top : log regret versus number of functional evaluations using multi - level soo , soo , and imgpo for ( a ) the first brachistochrone problem , ( b ) the second brachistochrone problem , and ( c ) the catenary problem .",
    "bottom : comparison of the solutions obtained by the algorithm with the actual solutions for each case .",
    "the obtained solutions have 15 discrete points , excluding the end points.,title=\"fig : \" ]",
    "a novel extension of simultaneous optimistic optimisation ( soo ) for infinite dimensional optimisation has been presented for the first time in this paper .",
    "the objective of the algorithm is to find a curve or a 1d function to obtain the maximum value of the given functional .",
    "the algorithm has been tested against the original soo and imgpo with fixed dimensions to solve analytically known physical systems , such as the brachistochrone and the catenary problems . in all test cases ,",
    "the new multi - level soo gives faster convergence to the actual solution , compared to the other two algorithms .",
    "it also gives 15 discrete points in 1000 times functional evaluations .",
    "the biggest advantage of using multi - level soo to optimise physical systems is that it does not need the gradient of the system , just a functional to calculate the value of tested functions .",
    "it is suitable to optimise shapes in physical systems that require simulations to compute the functional values .",
    "the authors would like to acknowledge the supports from the plasma physics hec consortium epsrc grant number ep / l000237/1 , as well as the central laser facility and the computer science department at the rutherford appleton laboratory for the use of scarf - lexicon computer cluster .",
    "we would also like to thank archer uk national supercomputing service for the use of the computing service .",
    "we also wish to thank the ucla / ist osiris consortium for the use of osiris .",
    "m.f.k . would like to gratefully thank indonesian endowment fund for education ( lpdp ) for its support .",
    "the authors gratefully acknowledge support from oxcheds and p.a.n . for his william penney fellowship with awe plc .",
    "m.f.k . would also like to thank dr .",
    "raoul trines for the useful discussion about functionals and ayesha chairannisa for proof - reading the manuscript ."
  ],
  "abstract_text": [
    "<S> this paper presents a novel numerical optimisation method for infinite dimensional optimisation . </S>",
    "<S> the functional optimisation makes minimal assumptions about the functional and without any specific knowledge on the derivative of the functional . </S>",
    "<S> the algorithm has been tested on several physical systems ( brachistochrone and catenary problems ) and it is shown that the solutions obtained are close to the actual solutions in one thousand functional evaluations . </S>",
    "<S> it is also shown that for the tested cases , the new algorithm provides better convergence to the optimum value compared to the tested existing algorithms . </S>"
  ]
}