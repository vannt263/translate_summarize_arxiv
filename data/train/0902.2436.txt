{
  "article_text": [
    "characterizing the capacity of general relay networks has been of great interest for many years . in this paper",
    ", we confine our interest to the capacity of single source multicast relay networks , which is still an open problem .",
    "for instance , the capacity of single relay channels is still unknown except for some special cases @xcite .",
    "however , if we confine the class of networks further , there are several cases in which the capacity is characterized .    recently , in @xcite , the multicast capacity of wireline networks was characterized .",
    "the capacity is given by the max - flow min - cut bound , and the key ingredient to achieve the bound is a new coding technique called network coding .",
    "starting from this seminal work , many efforts have been made to incorporate wireless effects in the network model , such as broadcast , interference , and noise . in @xcite ,",
    "the broadcast nature was incorporated into the network model by requiring each relay node to send the same signal on all outgoing channels , and the unicast capacity was determined .",
    "however , the model assumed that the network is deterministic ( noiseless ) and has no interference in reception at each node . in @xcite ,",
    "the work was extended to multicast capacity . in @xcite , the interference nature",
    "was also incorporated , and an achievable multicast rate was computed .",
    "this achievable rate has a cut - set - like representation and meets the information theoretic cut - set bound @xcite in some special cases . to incorporate the noise , erasure networks with broadcast or interference",
    "only were considered in @xcite .",
    "however , the network models in @xcite , @xcite assumed that the side information on the location of all erasures in the network is provided to destination nodes .",
    "noisy networks without side information at destination nodes were considered in @xcite and @xcite for finite - field additive noise and erasure cases , respectively .    along the same lines of the previous work on wireless networks mentioned above",
    ", we consider the multicast problem in a special class of networks called relay networks with interference .",
    "more specifically , we assume that all outgoing channels at each node are orthogonal , e.g. , using frequency or time division multiplexing , but signals incoming from multiple neighbor nodes to a node can interfere with each other .",
    "since wireless networks are often interference limited , our setup focuses on the more important aspect of them .",
    "this model covers those networks considered in @xcite .",
    "our interest in the relay networks with interference was inspired by @xcite , in which the capacity of single relay channels with interference was established . in this paper , we focus on two special subclasses of general networks with interference ; gaussian relay networks with interference and linear finite - field symmetric networks with interference .    for the gaussian relay networks with interference , we propose a scheme based on nested lattice codes @xcite which are formed from a lattice chain and compute an achievable multicast rate .",
    "the basic idea of using lattice codes is to exploit the structural gain of _ computation coding _",
    "@xcite , which corresponds to a kind of combined channel and network coding .",
    "previously , lattices were used in gaussian networks in @xcite , and an achievability was shown . however",
    ", our network model differs from the one in @xcite in that we assume general unequal power constraints for all incoming signals at each node , while an equal power constraint was mainly considered in @xcite .",
    "in addition , our lattice scheme is different from that in @xcite in that we use lattices to produce nested lattice codes , while lattices were used as a source code in @xcite .",
    "we also show that our achievable rate is within a constant number of bits from the information theoretic cut - set bound of the network .",
    "this constant depends only on the network topology and not on other parameters , e.g. , transmit powers and noise variances .",
    "this is similar to the recent result in @xcite , which showed an approximate capacity characterization for general gaussian relay networks using a random coding scheme .",
    "however , our achievability uses a structured code instead of a random one .",
    "thus , our scheme has a practical interest because structured codes may reduce the complexity of encoding and decoding .",
    "finally , we introduce a model of linear finite - field symmetric networks with interference , which generalizes those in @xcite . in the finite - field case",
    ", we use a linear coding scheme , which corresponds to the finite - field counterpart of the lattice coding scheme .",
    "the techniques for deriving an achievable rate for the finite - field network are basically the same as those for the gaussian case . however , in this case , the achievable rate always meets the information theoretic cut - set bound , and , thus , the capacity is fully established .",
    "this paper is organized as follows .",
    "section [ sec : systemmodel ] defines notations and parameters used in this paper and introduces the network model and the problem of interest . in section [ sec : gaussian ] , we analyze gaussian relay networks with interference and give the upper and lower bounds for the multicast capacity . in section [ sec : finite ] , we define a model of linear finite - field symmetric networks with interference and present the multicast capacity . section [ sec : conclusion ] concludes the paper .",
    "we begin with a description of the class of networks that will be dealt with in this paper .",
    "the memoryless relay networks with interference are characterized such that all outgoing channels from a node to its neighbors are orthogonal to each other .",
    "we still assume that incoming signals at a node can interfere with each other through a memoryless multiple - access channel ( mac ) .",
    "an example of this class of networks is shown in fig .",
    "[ fig : general ] . some special cases and subclasses of these networks",
    "have been studied in many previous works @xcite .",
    "[ t ]    we will begin by giving a detailed description of the network and some definitions of the parameters . the network is represented by a directed graph @xmath0 , where @xmath1 is a vertex set and @xmath2 is an edge set .",
    "each vertex and edge correspond to a communication node and a channel in the network , respectively . in this paper , we focus on a multicast network : vertex 1 represents the source node and is denoted by @xmath3 , and the set of destination nodes is denoted by @xmath4 , where @xmath5 .",
    "it will be assumed that the source node has no incoming edge , and the destination nodes have no outgoing edge .",
    "all the other nodes , which are neither the source nor the destination , are called the relay nodes . since all broadcast channels in the network",
    "are orthogonal , we associate a discrete or continuous random variable @xmath6 at time @xmath7 with edge @xmath8 as a channel input ( output of a node ) . as a channel output ( input of a node )",
    ", we associate a discrete or continuous random variable @xmath9 at time @xmath7 with node @xmath10 . from now on ,",
    "we sometimes drop the superscript ` @xmath11 ' when doing so causes no confusion .    at node @xmath12 , the set of incoming and outgoing nodes",
    "are denoted by @xmath13 set @xmath14 is called a cut if it contains node @xmath3 and its complement @xmath15 contains at least one destination node @xmath16 , i.e. , @xmath17 .",
    "let @xmath18 denote the set of all cuts .",
    "the boundaries of @xmath19 and @xmath15 are defined as @xmath20 for node @xmath21 , the set of incoming nodes across @xmath19 is defined as @xmath22 for any sets @xmath23 and @xmath24 , we define @xmath25 and @xmath26    using the aforementioned notations , we can formally define the class of networks of interest .",
    "the memoryless relay network with interference is characterized by the channel distribution function @xmath27 over all input and output alphabets .",
    "the multicast over the relay network consists of encoding functions @xmath28 , @xmath8 , @xmath29 , and decoding functions @xmath30 , @xmath31 .",
    "the source node @xmath3 has a random message @xmath32 that is uniform over @xmath33 and transmits @xmath34 at time @xmath7 on the outgoing channels @xmath35 , @xmath36 .",
    "the relay node @xmath37 transmits @xmath38 at time @xmath7 on the outgoing channels @xmath39 , @xmath40 , where @xmath41 . at destination",
    "node @xmath31 , after time @xmath42 , an estimate of the source message is computed as @xmath43 then , the probability of error is @xmath44 we say that the multicast rate @xmath45 is _ achievable _ if , for any @xmath46 and for all sufficiently large @xmath42 , encoders and decoders with @xmath47 exist such that @xmath48 .",
    "the _ multicast capacity _ is the supremum of the achievable multicast rates .",
    "as stated in section [ sec : introduction ] , we are interested in characterizing the multicast capacity of the memoryless relay networks with interference . however , as shown in @xcite , even for a relatively simple parallel relay channel , finding the capacity is not easy . thus , we further restrict our interest to the gaussian networks in section [ sec : gaussian ] and the linear finite - field symmetric networks in section [ sec : finite ] .",
    "in this section , we consider gaussian relay networks with interference . at node @xmath37 at time @xmath7 , the received signal is given by @xmath49 where @xmath50 is an independent identically distributed ( i.i.d . )",
    "gaussian random variable with zero mean and unit variance . for each block of channel input @xmath51",
    ", we have the average power constraint given by @xmath52 in @xcite , nazer et al . studied the achievable rate of the gaussian relay networks with interference for the equal power constraint case , where @xmath53 for all @xmath54 . in our work",
    ", we generalize it such that @xmath55 s can be different .",
    "the main result of this section is as follows .    for a gaussian relay network with interference , an upper bound for the multicast capacity",
    "is given by @xmath56 where @xmath57 .",
    "for the same network , we can achieve all rates up to @xmath58^+ \\text , \\label{eq : gaussianach}\\ ] ] where @xmath59^+ \\triangleq \\max \\{x,0\\}$ ] .",
    "furthermore , the gap between the upper bound and the achievable rate is bounded by @xmath60 [ th : gaussian ]    note that , in the equal power case , i.e. , @xmath61 , the achievable multicast rate ( [ eq : gaussianach ] ) has terms in the form of @xmath62 for some integer @xmath63 .",
    "similar forms of achievable rate were observed in @xcite for some equal power gaussian networks .",
    "the following subsections are devoted to proving theorem [ th : gaussian ] .",
    "the cut - set bound @xcite of the network is given by @xmath64 though the cut - set bound is a general and convenient upper bound for the capacity , it is sometimes challenging to compute the exact cut - set bound in a closed form .",
    "this is due to the optimization by the joint probability density function ( pdf ) @xmath65 . in some cases , such as the finite - field networks in @xcite , it is easy to compute the cut - set bound because a product distribution maximizes it . for the gaussian case , however , the optimizing distribution for the cut - set bound is generally not a product distribution .",
    "thus , we consider another upper bound which is easier to compute than the cut - set bound .",
    "this bound is referred to as the _ relaxed cut - set bound _ and given by @xmath66 due to the max - min inequality , the relaxed cut - set bound is looser than the original cut - set bound ( [ eq : cutset ] ) . for the relay network with interference , we can further simplify ( [ eq : rcutset ] ) as @xmath67 where the second and the third equalities follow by the structure of the network , i.e. ,    * @xmath68 , * @xmath69 , * @xmath70 .    for cut",
    "@xmath19 , the mutual information @xmath71 is maximized when there is a perfect coherence between all inputs to a gaussian mac across the cut .",
    "thus , we have @xmath72 then by ( [ eq : rcutset ] ) and ( [ eq : rcutset2 ] ) , the upper bound ( [ eq : gaussianub ] ) follows .",
    "[ t ]    before proving the achievable part of theorem [ th : gaussian ] , let us establish some preliminaries for the lattices and nested lattice codes , which are key ingredients of our achievability proof",
    ". for a more comprehensive review on lattices and nested lattice codes , see  @xcite .",
    "an @xmath73-dimensional lattice @xmath74 is defined as a discrete subgroup of euclidean space @xmath75 with ordinary vector addition .",
    "this implies that for any lattice points @xmath76 , we have @xmath77 , @xmath78 , and @xmath79 . the nearest neighbor lattice quantizer associated with @xmath74",
    "is defined as @xmath80 and the @xmath81 operation is @xmath82 the ( fundamental ) voronoi region of @xmath74 , denoted by @xmath83 , is defined as the set of points in @xmath75 closer to the origin than to any other lattice points , i.e. , @xmath84 where ties are broken arbitrarily . in fig .",
    "[ fig : lattice ] , an example of a two - dimensional lattice , and its voronoi region are depicted .",
    "we now define some important parameters that characterize the lattice .",
    "the covering radius of the lattice @xmath85 is defined as the radius of a sphere circumscribing around @xmath83 , i.e. , @xmath86 where @xmath87 is an @xmath73-dimensional unit sphere centered at the origin , and , thus , @xmath88 is a sphere of radius @xmath89 .",
    "in addition , the effective radius of @xmath74 , denoted by @xmath90 , is the radius of a sphere with the same volume as @xmath83 , i.e. , @xmath91 where @xmath92 denotes the volume of a region .",
    "the second moment per dimension of @xmath74 is defined as the second moment per dimension associated with @xmath83 , which is given by @xmath93 in the rest of this paper , we also use @xmath94 and @xmath95 , which have the same meaning as @xmath96 and @xmath97 , respectively . finally , we define the normalized second moment of @xmath74 as @xmath98 for any @xmath74 , @xmath99 is greater than @xmath100 , which is the normalized second moment of a sphere whose dimension tends to infinity .",
    "we consider a sequence of lattices @xmath101 .",
    "the sequence of lattices is said to be _",
    "rogers - good _ if @xmath102 which implies that @xmath101 is asymptotically efficient for sphere covering @xcite .",
    "this also implies the goodness of @xmath101 for mean - square error quantization , i.e. , @xmath103 we now define the goodness of lattices related to the channel coding for the additive white gaussian noise ( awgn ) channel . a sequence of lattices is said to be _ poltyrev - good _ if , for @xmath104 , @xmath105 where @xmath106 is the poltyrev exponent @xcite and @xmath107 is the volume - to - noise ratio ( vnr ) defined as @xmath108 note that ( [ eq : poltyrev ] ) upper bounds the error probability of the nearest lattice point decoding ( or equivalently , euclidean lattice decoding ) when we use lattice points as codewords for the awgn channel . since @xmath109 for @xmath110 , a necessary condition for reliable decoding is @xmath110 .",
    "now we consider two lattices @xmath74 and @xmath111 .",
    "assume that @xmath74 is coarse compared to @xmath111 in the sense that @xmath112 .",
    "we say that the coarse lattice @xmath74 is a sublattice of the fine lattice @xmath111 if @xmath113 and call the quotient group ( equivalently , the set of cosets of @xmath74 relative to @xmath111 ) @xmath114 a _ lattice partition_. for the lattice partition , the _ set of coset leaders _ is defined as @xmath115 and the _ partitioning ratio _ is @xmath116    formally , a lattice code is defined as an intersection of a lattice ( possibly translated ) and a bounding ( shaping ) region , which is sometimes a sphere . a _ nested lattice code _ is a special class of lattice codes , whose bounding region is the voronoi region of a sublattice . that is , the nested lattice code is defined in terms of lattice partition @xmath114 , in which @xmath111 is used as codewords and @xmath74 is used for shaping .",
    "the coding rate of the nested lattice code is given by @xmath117 nested lattice codes have been studied in many previous articles @xcite , and proved to have many useful properties , such as achieving the capacity of the awgn channel . in the next subsection",
    ", we deal with the nested lattice codes for the achievability proof of theorem [ th : gaussian ] .      as an achievable scheme",
    ", we use a lattice coding scheme . in  @xcite , lattices were also used to prove an achievable rate of gaussian relay networks with interference ( called gaussian mac networks ) .",
    "however , they used the lattice as a source code with a distortion and then related the achievable distortion to the information flow through the network",
    ". our approach is different from  @xcite in that we use lattices to produce coding and shaping lattices , and form nested lattice codes . as a result",
    ", our approach can handle unequal power constraints where incoming links have different power at a mac .",
    "our scheme is a generalization of the nested lattice codes used for the gaussian two - way relay channel in  @xcite .",
    "let us consider a standard model of a gaussian mac with @xmath118 input nodes : @xmath119 where @xmath120 denotes the awgn process with zero mean and unit variance .",
    "each channel input @xmath121 is subject to the average power constraint @xmath122 , i.e. , @xmath123 . without loss of generality",
    ", we assume that @xmath124 .",
    "the standard mac in ( [ eq : stdmac ] ) is a representative of macs in the gaussian relay network with interference .",
    "now , we introduce encoding and decoding schemes for the standard mac .",
    "let us first consider the following theorem which is a key for our code construction .    for any @xmath125 and @xmath126 , a sequence of @xmath73-dimensional lattice chains @xmath127 exists that satisfies the following properties .",
    "\\a ) @xmath128 , @xmath129 , are simultaneously rogers - good and poltyrev - good while @xmath130 is poltyrev - good .",
    "\\b ) for any @xmath131 , @xmath132 , @xmath129 , for sufficiently large @xmath73 .",
    "\\c ) the coding rate of the nested lattice code associated with the lattice partition @xmath133 can approach any value as @xmath73 tends to infinity , i.e. , @xmath134 where @xmath135 and @xmath136 as @xmath137 .",
    "furthermore , for @xmath138 , the coding rate of the nested lattice code associated with @xmath139 is given by @xmath140 where @xmath141 .",
    "[ th : latticechain ]    see appendix [ sec : latticechain ] .    a conceptual representation of the lattice chain and the corresponding sets of coset leaders",
    "are given in fig .",
    "[ fig : latchain ] for a two - dimensional case .",
    "[ t ]    we consider a lattice chain as described in theorem [ th : latticechain ] .",
    "we assign the @xmath142-th input node to the mac with the set of coset leaders @xmath143 . for each input",
    "node , the message set @xmath144 is arbitrarily mapped onto @xmath143 .",
    "we also define random dither vectors @xmath145 , @xmath146 , where @xmath147 denotes the voronoi region of @xmath148 ( we dropped the superscript ` @xmath149 ' for simplicity ) .",
    "these dither vectors are independent of each other and also independent of the message of each node and the noise .",
    "we assume that each @xmath150 is known to both the @xmath142-th input node and the receiver .",
    "to transmit a message that is uniform over @xmath151 , node @xmath142 chooses @xmath152 associated with the message and sends @xmath153    let us introduce a useful lemma , which is known as the _ crypto - lemma _ and frequently used in the rest of this paper .",
    "the lemma is given in @xcite , and we repeat it here for completeness .",
    "[ crypto - lemma @xcite ] let @xmath154 be a finite or compact group with group operation @xmath155 . for independent random variables @xmath156 and @xmath157 over @xmath154 , let @xmath158 .",
    "if @xmath156 is uniform over @xmath154 , then @xmath159 is independent of @xmath157 and uniform over @xmath154 .",
    "[ lem : cryptolemma ]    by lemma [ lem : cryptolemma ] , @xmath160 is uniformly distributed over @xmath147 and independent of @xmath161 .",
    "thus , regardless of @xmath161 , the average transmit power of node @xmath142 is equal to @xmath162 , which approaches @xmath122 as @xmath73 tends to infinity .",
    "thus , the power constraint is met .    upon receiving @xmath163 , where @xmath164 is a vector of i.i.d .",
    "gaussian noise with zero mean and unit variance , the receiver computes @xmath165 \\md_1 \\nonumber\\\\ & = \\left ( { \\bf t } + \\tilde{\\bf z }",
    "\\right ) \\md_1 \\text , % \\label{eq : mlan}\\end{aligned}\\ ] ] where @xmath166 \\md_1 \\nonumber\\\\ & = \\left [ { \\bf w}_1 + \\sum_{j=2}^k \\left ( { \\bf w}_j - q_j ( { \\bf w}_j + { \\bf u}_j ) \\right ) \\right ] \\md_1 \\text , \\label{eq : detmac}\\\\ \\tilde{\\bf z } & = - ( 1- \\alpha ) \\sum_{j=1}^k { \\bf x}_j + \\alpha { \\bf z } \\nonumber \\text,\\end{aligned}\\ ] ] @xmath167 is a scaling factor , and @xmath168 denotes the nearest neighbor lattice quantizer associated with @xmath169 .",
    "we choose @xmath170 as the minimum mean - square error ( mmse ) coefficient to minimize the variance of the effective noise @xmath171 .",
    "thus , @xmath172 and the resulting noise variance satisfies @xmath173 note that , though the relation in ( [ eq : noisevar ] ) is given by an inequality , it becomes tight as @xmath137 by theorem [ th : latticechain ] . by the chain relation of the lattices in theorem [",
    "th : latticechain ] , it is easy to show that @xmath174 .",
    "regarding @xmath175 , we have the following lemma .",
    "@xmath175 is uniform over @xmath176 and independent of @xmath171 .",
    "[ lem : indepeffnoise ]    define @xmath177 , and , thus , @xmath178 .",
    "note that @xmath179 is correlated with @xmath160 , @xmath180 , and @xmath181 .",
    "since @xmath182 is uniform over @xmath176 and independent of @xmath179 , @xmath175 is independent of @xmath179 and uniformly distributed over @xmath176 ( crypto - lemma ) .",
    "hence , if @xmath175 and @xmath171 are correlated , it is only through @xmath182 .",
    "however , @xmath182 and @xmath171 are independent of each other , and , consequently , @xmath175 is also independent of @xmath171 .",
    "the receiver tries to retrieve @xmath175 from @xmath183 instead of recovering @xmath161 , @xmath129 , separately . for the decoding method",
    ", we consider _ euclidean lattice decoding _",
    "@xcite-@xcite , which finds the closest point to @xmath183 in @xmath111 . from the symmetry of the lattice structure and the independence between @xmath175 and @xmath171 ( lemma [ lem : indepeffnoise ] ) , the probability of decoding error is given by @xmath184 where @xmath185 denotes the nearest neighbor lattice quantizer associated with @xmath111 and @xmath186 denotes the voronoi region of @xmath111 .",
    "then , we have the following theorem .",
    "let @xmath187^+ \\text.\\ ] ] for any @xmath188 and a lattice chain as described in theorem [ th : latticechain ] with @xmath189 approaching @xmath190 , i.e. , @xmath191 , the error probability under euclidean lattice decoding ( [ eq : errorprob ] ) is bounded by @xmath192 [ th : latticeach ]    see appendix [ sec : latticeach ] .",
    "according to theorem [ th : latticeach ] , the error probability vanishes as @xmath137 if @xmath188 since @xmath193 for @xmath194 .",
    "this implies that the nested lattice code can achieve any rate below @xmath195 .",
    "thus , by c ) of theorem [ th : latticechain ] and theorem [ th : latticeach ] , the coding rate @xmath196 , @xmath129 , can approach @xmath197 arbitrarily closely while keeping @xmath198 arbitrarily small for sufficiently large @xmath73 , where @xmath199^+ \\text .",
    "\\label{eq : coderate}\\ ] ]    in theorem",
    "[ th : latticeach ] , we showed the error exponent of lattice decoding and the achievability of @xmath189 directly followed .",
    "however , if we are only interested in finding the achievability of @xmath189 , not in the error exponent , we can use the argument on the bounding behavior of lattice decoding in @xcite , which gives the same result in a much simpler way .    since @xmath200",
    ", we have @xmath201 . now , consider the case that , for some @xmath202 , the rates @xmath197 , @xmath203 , are zero while @xmath197 , @xmath204 , are nonzero . in this situation , nodes @xmath205 ,  , @xmath118 can not transmit any useful information to the receiver , and , thus , we can turn them off so as not to hinder the transmissions of nodes @xmath206 , ",
    ", @xmath207 .",
    "then , the variance of @xmath171 decreases and we have extended rates given by @xmath208^+ , \\ ; 1 \\leq i \\leq \\hat{i } \\text.\\ ] ] however , for the ease of exposition , we do not consider the transmitter turning - off technique and assume that nodes @xmath205 , ",
    ", @xmath118 just transmit @xmath209 when their coding rates are zero .",
    "we consider @xmath210 blocks of transmissions from the source to destinations .",
    "each block consists of @xmath73 channel uses . in block",
    "@xmath211 , an independent and uniform message @xmath212 \\in \\{1 , \\ldots , 2^{nr}\\}$ ] is sent from the source node @xmath3 .",
    "it takes at most @xmath213 blocks for all the @xmath210 messages to be received by destination nodes . after receiving @xmath214 blocks , destination nodes",
    "decode the source message @xmath215 , \\ldots , w[b ] \\right)$ ] .",
    "thus , the overall rate is @xmath216 , which can be arbitrarily close to @xmath45 by choosing @xmath210 sufficiently large .",
    "[ t ]    for ease of analysis , we consider the @xmath210 blocks of transmissions over the time - expanded network @xcite , @xmath217 , obtained by unfolding the original network @xmath218 over @xmath219 time stages . in @xmath217 ,",
    "node @xmath12 at block @xmath220 appears as @xmath221 $ ] , and @xmath221 $ ] and @xmath222 $ ] are treated as different nodes if @xmath223 .",
    "there are a virtual source and destination nodes , denoted by @xmath224 and @xmath225 , respectively .",
    "we assume that @xmath224 and @xmath226 $ ] s are connected through virtual error - free infinite - capacity links , and , similarly , @xmath225 and @xmath227 $ ] s are . for instance , the network in fig . [ fig : general ] is expanded to the network in fig .",
    "[ fig : expandgraph ] . dealing with the time - expanded network",
    "does not impose any constraints on the network .",
    "any scheme for the original network can be interpreted to a scheme for the time - expanded network and vice - versa . in our case ,",
    "the transmissions of @xmath210 messages @xmath212 $ ] , @xmath228 , from @xmath3 to @xmath31 over @xmath218 correspond to the transmission of a single message @xmath32 from @xmath224 to @xmath229 over @xmath217 , where @xmath230 denotes the set of virtual destination nodes .",
    "a main characteristic of the time - expanded network is that it is always a layered network @xcite which has equal length paths from the source to each destination .",
    "we define the set of nodes at length @xmath220 from the virtual source node as @xmath231 = \\left\\ { v[k ] : v \\in v \\right\\}\\ ] ] and call it the @xmath220-th layer .",
    "we use the subscript ` @xmath232 ' to differentiate parameters of @xmath218 and @xmath217 .",
    "the set of nodes and edges of @xmath217 are defined as @xmath233 \\right ) \\text,\\\\ e_{\\te } & = \\left\\ { \\left ( u[k],v[k+1 ] \\right ) : ( u , v ) \\in e , k=1,\\ldots , l \\right\\}\\\\ & \\;\\;\\ ; \\cup \\left\\ { ( s[k-1],s[k ] ) : k=1,\\ldots , l \\right\\}\\\\ & \\;\\;\\ ; \\cup \\left\\ { ( d[k],d[k+1 ] ) : k=1,\\ldots , l \\right\\ } \\text,\\end{aligned}\\ ] ] where we define @xmath234 = s_{\\te}$ ] and @xmath235 = d_{\\te}$ ] .",
    "note that , since @xmath217 is layered , edges only appear between adjacent layers . from @xmath236 and @xmath237 , the other parameters ,",
    "e.g. , @xmath238 , @xmath239 , @xmath240 , @xmath241 , @xmath242 , and @xmath243 , are similarly defined as @xmath244 , @xmath245 , @xmath19 , @xmath246 , @xmath18 , and @xmath247 , respectively .",
    "we apply the nested lattice codes in section [ sec : latticecode ] over the all gaussian macs in the network .",
    "thus , node @xmath221 $ ] is assigned with sets of coset leaders @xmath248,w[k^+]}$ ] , @xmath249 \\in \\theta_{\\te}(v[k])$ ] , where @xmath250 .",
    "we do not change the lattice scheme over blocks , and , thus , @xmath248,w[k^+ ] } = \\mathcal{c}_{v , w}$ ]    at node @xmath226 $ ] , the indices @xmath251 are uniformly randomly mapped onto vectors in @xmath252 , @xmath253 .",
    "we define the random mapping as @xmath254,w[k^+ ] } ( \\cdot)$ ] .",
    "then , node @xmath226 $ ] receives @xmath255 , \\ldots , w[b ] \\right)$ ] from @xmath256 $ ] through the error - free link , where @xmath257 , and transmits @xmath258,w[k^+ ] } = f_{s[k],w[k^+]}(w[k])\\ ] ] on channel @xmath259,w[k^+])$ ] using a random dither vector @xmath260,w[k^+]}$ ] . at node @xmath221 $ ] that is not @xmath226 $ ] or @xmath227 $ ] , the received signal is given by @xmath261 } = \\left ( { \\bf t}_{v[k ] } + \\tilde{\\bf z}_{v[k ] } \\right ) \\md_v \\text , \\label{eq : multienc1}\\ ] ] where @xmath262 } = \\left [ \\sum_{u[k^- ] \\in \\atop \\delta_{\\te } ( v[k ] ) } \\left ( { \\bf w}_{u[k^-],v[k ] } - q_{u , v } \\left({\\bf w}_{u[k^-],v[k ] } + { \\bf u}_{u[k^-],v[k ] } \\right ) \\right ) \\right ] \\md_v \\text , \\label{eq : multienc2}\\ ] ] and @xmath263}$ ] is an effective noise vector . in ( [ eq : multienc1 ] ) , @xmath264 denotes the lattice associated with the incoming channel to node @xmath37 with the largest power .",
    "then , @xmath265}$ ] is decoded using euclidean lattice decoding , which yields an estimate @xmath266}$ ] .",
    "next , @xmath266}$ ] is uniformly and randomly mapped onto vectors in @xmath267 , @xmath268 .",
    "this mapping is denoted by @xmath269,w[k^+ ] } ( \\cdot)$ ] , and node @xmath221 $ ] transmits @xmath270,w[k^+ ] } = f_{v[k],w[k^+ ] } \\left ( \\hat{\\bf t}_{v[k ] } \\right)\\ ] ] on channel @xmath271,w[k^+])$ ] using a random dither vector @xmath272,w[k^+]}$ ] .",
    "node @xmath227 $ ] , @xmath31 , receives @xmath273}$ ] and computes @xmath274}$ ] .",
    "it also receives @xmath275 } , \\ldots , \\hat{\\bf t}_{d[k^- ] } \\right)$ ] from @xmath276 $ ] through the virtual error - free infinite - capacity link and passes @xmath275 } , \\ldots , \\hat{\\bf t}_{d[k ] } \\right)$ ] to node @xmath277 $ ] .",
    "we assume that all the random mappings @xmath278,v[k^+]}$ ] , @xmath279,v[k^+ ] ) \\in e_{\\te}$ ] are done independently .",
    "while decoding , a virtual destination node @xmath229 assumes that there is no error in decoding @xmath265}$ ] s in the network and that the network is deterministic .",
    "therefore , with knowledge of all deterministic relations ) is deterministic . ]",
    "( [ eq : multienc2 ] ) in the network , node @xmath225 decodes @xmath32 by simulating all @xmath280 messages and finding one that yields the received signal @xmath281 } , \\ldots , \\hat{\\bf t}_{d[l+1 ] } \\right)$ ] .    in the above decoding rule",
    ", we will declare an error if at least one of the following events occurs .",
    "* @xmath282 : there is an error in decoding @xmath283}$ ] at at least one node in the network .",
    "* @xmath284 : a message @xmath285 exists that yields the same received signal @xmath286 , which is obtained under @xmath32 , at at least one virtual destination node @xmath287 .    thus , the error probability is given by @xmath288    let us consider the first term in ( [ eq : multierr1 ] ) . using the union bound , we have @xmath289 \\in v[k ] \\atop \\setminus \\{s[k]\\ } } p_{e , v[k ] } \\text,\\ ] ] where @xmath290 } \\triangleq \\pr \\left\\ { \\hat{\\bf t}_{v[k ] } \\neq { \\bf t}_{v[k ] } \\right\\ } \\text.\\ ] ] note that the summation is from @xmath291 since nodes in the first layer do not have any received signal except for node @xmath292 $ ] . by theorem [ th : latticeach ] , at node @xmath10 for any @xmath293 , @xmath294}$ ] is less than @xmath295 for sufficiently large @xmath73 if @xmath296^+ \\label{eq : multierr1.5}\\end{aligned}\\ ] ] for all @xmath297 .",
    "therefore , in this case @xmath298    now , we consider the second term in ( [ eq : multierr1 ] ) . under the condition @xmath299",
    ", we have @xmath266 } = { \\bf t}_{v[k]}$ ] , and , thus , the network is deterministic .",
    "let us use the notation @xmath300,v[k ] } ( w)$ ] and @xmath265 } ( w)$ ] to explicitly denote the signals under message @xmath32 .",
    "we say that node @xmath221 $ ] can distinguish @xmath32 and @xmath301 if @xmath265}(w ) \\neq { \\bf t}_{v[k ] } ( w')$ ] .",
    "thus , from the argument of a deterministic network in @xcite , the error probability is bounded by @xmath302 we briefly denote the probabilities in the summation in ( [ eq : multierr2 ] ) as @xmath303    here , we redefine the cut in the time - expanded network @xmath217 for convenience sake . from the encoding scheme , since the source message propagates through nodes @xmath226 $ ] , @xmath304 , they can clearly distinguish @xmath32 and @xmath301 . similarly ,",
    "if a virtual destination node @xmath225 can not distinguish @xmath32 and @xmath301 , nodes @xmath227 $ ] , @xmath305 can not either .",
    "thus , when we analyze the error probability ( [ eq : multierr2 ] ) , we can always assume that @xmath226 \\in s_{\\te}$ ] and @xmath227 \\in s_{\\te}^c$ ] , @xmath304 , without loss of generality .",
    "from the fact that @xmath217 is layered , we have @xmath306 \\right\\ } \\nonumber\\\\ & \\;\\ ; \\cdot \\prod_{k=2}^{l+1 } \\pr \\left\\ { \\bar{\\mathcal{d } } = s_{\\te}^c [ k ] | \\mathcal{d}=s_{\\te } [ k^- ] , \\bar{\\mathcal{d } } = s_{\\te}^c [ k^- ] \\right\\ } \\nonumber\\\\ & \\leq \\prod_{k=2}^{l+1 } \\pr \\left\\ { \\bar{\\mathcal{d } } = s_{\\te}^c [ k ] | \\mathcal{d}=s_{\\te } [ k^- ] , \\bar{\\mathcal{d } } = s_{\\te}^c [ k^- ] \\right\\ } \\text , \\label{eq : multierr3}\\end{aligned}\\ ] ] where @xmath307 $ ] and @xmath308 $ ] denote the sets of nodes in @xmath240 and @xmath309 in the @xmath220-th layer , i.e. , @xmath310 & \\triangleq s_{\\te } \\cap v_{\\te}[k ] \\text,\\\\ s_{\\te}^c [ k ] & \\triangleq s_{\\te}^c \\cap v_{\\te}[k ] \\text.\\end{aligned}\\ ] ] also , from the fact that the random mapping for each channel is independent , we have @xmath311 | \\mathcal{d}=s_{\\te } [ k^- ] , \\bar{\\mathcal{d } } = s_{\\te}^c [ k^- ] \\right\\ } \\nonumber\\\\ = \\prod_{v[k ] \\in \\atop s_{\\te}^c [ k ] } \\pr \\left\\ { \\bar{\\mathcal{d } } = \\{v[k ] \\ } | \\mathcal{d}=s_{\\te } [ k^- ] , \\bar{\\mathcal{d } } = s_{\\te}^c [ k^- ] \\right\\ } \\text .",
    "\\label{eq : multierr4}\\end{gathered}\\ ] ] then , we have the following lemma .    consider the time - expanded network @xmath217 with independent uniform random mapping at each node .",
    "for any cut \\in s_{\\te}$ ] and @xmath227 \\in s_{\\te}^c$ ] , @xmath305 .",
    "] @xmath240 in @xmath217 , we have @xmath312 \\ } | \\mathcal{d}=s_{\\te } [ k^- ] , \\bar{\\mathcal{d } } = s_{\\te}^c [ k^- ] \\right\\ } \\leq 2^{-n \\left ( \\underset{u[k^- ] \\in \\atop \\delta_{\\te , s } ( v[k])}{\\max } r_{u , v}\\right ) } \\ ] ] for node @xmath221 \\in \\bar{s}_{\\te}^c [ k]$ ] , where @xmath313 \\triangleq \\bar{s}_{\\te}^c \\cap v_{\\te}[k]$ ] . for node",
    "@xmath221 \\in s_{\\te}^c [ k ] \\setminus \\bar{s}_{\\te}^c [ k]$ ] , we have @xmath312 \\ } | \\mathcal{d}=s_{\\te } [ k^- ] , \\bar{\\mathcal{d } } = s_{\\te}^c [ k^- ] \\right\\ } = 1\\text.\\ ] ] [ lem : multierr1 ]    see appendix [ sec : multierr1 ] .",
    "thus , by ( [ eq : multierr2])-([eq : multierr4 ] ) and lemma [ lem : multierr1 ] , it follows that @xmath314 \\in \\atop \\bar{s}_{\\te}^c [ k]}{\\sum } \\left ( \\underset{u[k^- ] \\in \\atop \\delta_{\\te , s } ( v[k])}{\\max } r_{u , v } \\right ) } \\text .",
    "\\label{eq : multierr5}\\ ] ] we now consider the following lemma .    in the time - expanded @xmath217 with @xmath219 layers , the term in the exponent of ( [ eq : multierr5 ] ) @xmath315 \\in \\atop \\bar{s}_{\\te}^c [ k ] } \\left ( \\underset{u[k^- ] \\in \\atop \\delta_{\\te , s } ( v[k])}{\\max } r_{u , v } \\right)\\ ] ] is upper bounded by @xmath316 and lower bounded by @xmath317 [ lem : multierr2 ]    see appendix [ sec : multierr2 ] .",
    "therefore , by ( [ eq : multierr1.5 ] ) , ( [ eq : multierr5 ] ) , and lemma [ lem : multierr2 ] , @xmath318 is less than @xmath319 for sufficiently large @xmath73 if @xmath320^+ \\text .",
    "\\label{eq : multierr6}\\ ] ] thus , the total probability of error ( [ eq : multierr1 ] ) is less than @xmath321 , and the achievability follows from ( [ eq : multierr6 ] ) .      to compute the gap between the upper bound ( [ eq : gaussianub ] ) and the achievable rate ( [ eq : gaussianach ] ) ,",
    "we can rely on the following lemmas .",
    "assume that @xmath322 .",
    "for any nonempty set @xmath323 and @xmath324 , we have @xmath325^+ \\leq \\log k \\text.\\end{aligned}\\ ] ] [ lem : gap1 ]    @xmath326    [ lem : gap2 ]    the proof of lemma [ lem : gap1 ] is given in appendix [ sec : gap ] , and the proof of lemma [ lem : gap2 ] is omitted since it is straightforward . using lemmas [ lem : gap1 ] and [ lem : gap2 ] ,",
    "the gap in ( [ eq : gaussiangap ] ) directly follows .",
    "[ t ]    let us consider a particular class of discrete memoryless relay networks with interference .",
    "the linear finite - field symmetric networks with interference are characterized by a special structure of macs in them , which is shown in fig .",
    "[ fig : ffsymmac ] . in more detail ,",
    "the linear finite - field symmetric network with interference is described as follows :    * every input alphabet to a mac at node @xmath37 is the finite field , @xmath327 . * the received symbol at node @xmath37 , @xmath328 ,",
    "is determined to be the output of a _ symmetric discrete memoryless channel _ ( dmc ) @xmath329 with input @xmath330 where @xmath331 denotes the channel coefficient . for the definition of the symmetric dmc , see ( * ? ? ?",
    "* the input field size @xmath332 and channel transition function @xmath333 associated with node @xmath37 need not be identical .",
    "a major characteristic of the symmetric dmc is that linear codes can achieve the capacity ( * ? ? ?",
    "6.2 ) . using this ,",
    "nazer and gastpar @xcite showed that the _ computation capacity _ for any linear function of sources can be achieved in the linear finite - field symmetric mac in fig .",
    "[ fig : ffsymmac ] . also , in @xcite , it was shown that linear codes achieve the multicast capacity of linear finite - field additive noise and erasure networks with interference , which are special cases of the class of networks stated above . extending this line ,",
    "we characterize the multicast capacity of the linear finite - field symmetric network with interference .",
    "the multicast capacity of a linear finite - field symmetric network with interference is given by @xmath334 where @xmath335 is the capacity of the channel @xmath336 .",
    "[ th : finite ]    the proof of theorem [ th : finite ] is very similar to the proof of theorem [ th : gaussian ] .",
    "the difference is that we use linear codes instead of the nested lattice codes .",
    "we show the outline of the proof in the next subsections .",
    "the capacity proof for linear finite - field additive noise networks in @xcite can also be extended to the linear finite - field symmetric networks in theorem [ th : finite ] . however ,",
    "the proof in @xcite relies on algebraic network coding , and , thus , it has a restriction on the field size , i.e. , @xmath337 . in our proof",
    ", we do not use the algebraic network coding , and the field size is not restricted .      as in the gaussian case in section [ sec : gaussianub ] , the upper bound follows from the relaxed cut - set bound ( [ eq : rcutset ] ) . in particular , for the linear finite - field symmetric network with interference , we have the markov chain relation @xmath338 , where @xmath339 . using the data processing inequality",
    ", we have @xmath340 thus the upper bound is given by @xmath341      let us denote the vectors of channel input and output of the symmetric dmc @xmath342 as @xmath343^t$ ] and @xmath344^t$ ] , respectively . without loss of generality , we assume that the encoder input is given by a uniform random vector @xmath345 for some @xmath346",
    ". then we have the following lemma related to linear coding for the dmc .",
    "[ lemma 3 of @xcite ] for the symmetric dmc @xmath347 , a sequence of matrices @xmath348 and associated decoding function @xmath349 exist such that when @xmath350 , @xmath351 for any @xmath46 and @xmath73 large enough if @xmath352 .",
    "[ lem : finiteach1 ]    we now consider linear encoding for nodes in the network .",
    "we let @xmath353 and thus , @xmath354 where @xmath355 by lemma [ lem : finiteach1 ] , a linear code with sufficiently large dimension exists such that node @xmath37 can recover @xmath356 with an arbitrarily small error probability if @xmath357 .",
    "now , we can do the same as in section [ sec : gaussianach ] with ( [ eq : finitedetrelation ] ) replacing ( [ eq : multienc2 ] ) , and the achievability part follows .",
    "in this paper , we considered the multicast problem for relay networks with interference and examined roles of some structured codes for the networks .",
    "initially , we showed that nested lattice codes can achieve the multicast capacity of gaussian relay networks with interference within a constant gap determined by the network topology .",
    "we also showed that linear codes achieve the multicast capacity of linear finite - field symmetric networks with interference .",
    "finally , we should note that this work is an intermediate step toward more general networks . as an extension to multiple source networks",
    ", we showed that the same lattice coding scheme considered in this work can achieve the capacity of the gaussian two - way relay channel within @xmath358 bit @xcite .",
    "as another direction of extension , we can consider applying structured codes to networks with non - orthogonal broadcast channels .",
    "there is a recent work on the interference channel @xcite which is related to this issue .",
    "consider a lattice ( more precisely , a sequence of lattices ) @xmath359 with @xmath360 , which is simultaneously rogers - good and poltyrev - good ( simultaneously good shortly ) . in @xcite , it was shown that such a lattice always exists .",
    "then , by the argument in @xcite , we can find a fine lattice @xmath361 such that @xmath362 and @xmath361 is also simultaneously good .",
    "we let the partitioning ratio be @xmath363 for some @xmath364 .",
    "since the partitioning ratio can approach an arbitrary value as @xmath73 tends to infinity , for any @xmath131 , @xmath365 exists such that we can choose @xmath366 when @xmath367 .",
    "we now have @xmath368 where the second equality follows from ( [ eq : ap1 - 1 ] ) . since @xmath361 is rogers - good , @xmath369 exists such that @xmath370 , for @xmath371 .",
    "thus , for @xmath372 , we have @xmath373 by repeating the same procedure , we obtain a lattice chain @xmath374 , where @xmath375 , @xmath129 , are simultaneously good and @xmath376 for sufficiently large @xmath73 .    moreover , by theorem 5 of @xcite , if @xmath377 is simultaneously good , a poltyrev - good lattice @xmath378 exists such that @xmath379 and the coding rate @xmath380 can be arbitrary as @xmath137 , i.e. , @xmath381 given @xmath380 , the coding rates @xmath196 , @xmath138 , are given by @xmath382 where the third equality follows by the fact that @xmath375 and @xmath377 are both rogers - good , and the fourth follows by the fact that @xmath383 .",
    "@xmath384      let @xmath385 and @xmath386 denote the covering and effective radii of @xmath148 , respectively .",
    "then the second moment per dimension of @xmath387 is given by @xmath388 next , we define independent gaussian random variables @xmath389 and @xmath390 then , we have the following lemmas .",
    "now , we bound the error probability by @xmath398 where ( [ eq : pferror ] ) follows from lemma [ lem : exponent2 ] .",
    "note that @xmath392 is a vector of i.i.d .",
    "zero - mean gaussian random variables , and the vnr of @xmath111 relative to @xmath392 is given by @xmath399 where ( [ eq : vnr1 ] ) follows from lemma [ lem : exponent1 ] and the fact that @xmath148 , @xmath400 , are rogers - good , ( [ eq : vnr2 ] ) from the definition of @xmath401 , and ( [ eq : vnr3 ] ) from the fact that @xmath402 is rogers - good and @xmath403 .",
    "when we consider the poltyrev exponent , we are only interested in the case that @xmath404 .",
    "thus , from the definition of @xmath195 and ( [ eq : vnr3 ] ) , we can write @xmath405 for @xmath188 . finally , from ( [ eq : pferror ] ) and by the fact that @xmath111 is poltyrev - good , we have @xmath406 @xmath384      for notational simplicity , we prove this lemma in the standard mac in section [ sec : latticecode ] .",
    "we assume that the uniform random mapping is done at each input node of the standard mac , as was done in the network .",
    "let @xmath407 and @xmath408 be nonempty partitions of @xmath409 , i.e. , @xmath410 , and @xmath411 .",
    "we assume that @xmath407 implies the set of nodes that can distinguish @xmath32 and @xmath301 , and @xmath408 implies the set of nodes that can not . for node @xmath412 , @xmath413 and @xmath414",
    "are uniform over @xmath143 and independent of each other due to the uniform random mapping . however , for node @xmath415 , we always have @xmath416 .",
    "thus , if @xmath417 , @xmath418 always holds , i.e. , @xmath419 if @xmath420 , given @xmath421 and @xmath422 , the event @xmath423 is equivalent to @xmath424 , where @xmath425 \\md_1 \\text,\\ ] ] and @xmath426 is given accordingly .",
    "now , let @xmath427 , then @xmath428 \\md_l \\text,\\end{aligned}\\ ] ] which follows from the fact that @xmath429 , and , thus , @xmath430 .",
    "note that , due to the crypto - lemma and the uniform random mapping , @xmath431 and @xmath432 are uniform over @xmath433 and independent of each other .",
    "therefore , @xmath434 thus , by changing notations properly to those of the network , we complete the proof .",
    "@xmath384      in the time - expanded network , there are two types of cuts , steady cuts and wiggling cuts @xcite .",
    "the steady cut separates the nodes in different layers identically .",
    "that is , for a steady cut @xmath240 , @xmath221 \\in s_{\\te}$ ] for some @xmath220 if and only if @xmath435 , \\ldots , v[l+1 ] \\in s_{\\te}$ ] .",
    "let us denote the set of all steady cuts as @xmath436 . then , since @xmath437 , @xmath438 \\in \\atop \\bar{s}_{\\te}^c [ k ] } \\left ( \\underset{u[k^- ] \\in \\atop \\delta_{\\te , s } ( v[k])}{\\max } r_{u , v } \\right ) & \\leq \\underset{s_{\\te } \\in \\tilde{\\gamma}_{\\te}}{\\min } \\sum_{k=2}^{l+1 } \\sum_{v[k ] \\in \\atop \\bar{s}_{\\te}^c [ k ] } \\left ( \\underset{u[k^- ] \\in \\atop \\delta_{\\te , s } ( v[k])}{\\max } r_{u , v } \\right)\\\\ & = l \\cdot \\underset{s \\in \\gamma}{\\min } \\sum_{v \\in \\bar{s}^c } \\left ( \\underset{u \\in \\delta_s ( v)}{\\max } r_{u , v } \\right ) \\text.\\end{aligned}\\ ] ]        the proof of lemma [ lem : submod ] is tedious but straightforward .",
    "similar lemmas were presented and proved in ( * ? ? ?",
    "* lemma 6.4 ) , ( * ? ? ?",
    "* lemma 2 ) , and the proof of lemma [ lem : submod ] also follows similarly .",
    "now , since @xmath450 , it follows that @xmath451 also , since @xmath452 $ ] s correspond to cuts in @xmath453 , we can rewrite @xmath315 \\in",
    "\\atop \\bar{s}_{\\te}^c [ k ] } \\left ( \\underset{u[k^- ] \\in \\atop \\delta_{\\te , s } ( v[k])}{\\max } r_{u , v } \\right ) = \\underset{s_{\\te } \\in \\gamma_{\\te}}{\\min } \\sum_{k=2}^{l+1 } \\xi \\left ( s_{\\te}[k^- ] , s_{\\te}[k ] \\right ) \\text.\\ ] ] since there are @xmath454 different cuts , at least the first @xmath455 of the sequence @xmath456 , \\ldots , s_{\\te}[l+1]$ ] form loops , and , thus , by lemma [ lem : submod ] and ( [ eq : submod ] ) , we have @xmath457 , s_{\\te}[k ] \\right ) \\geq \\left ( l - |\\gamma| + 2 \\right ) \\cdot \\underset{s \\in \\gamma}{\\min } \\sum_{v \\in \\bar{s}^c } \\left ( \\underset{u \\in \\atop \\delta_s ( v)}{\\max } r_{u , v } \\right)\\text.\\ ] ] @xmath384        in this case , @xmath460 , and the gap is @xmath461^+ \\\\ & \\leq \\frac{1}{2 } \\log \\left ( 1 + \\left ( \\sum_{j=1}^k \\sqrt{p_j } \\right)^2 \\right ) - \\frac{1}{2 } \\log \\left ( \\left ( \\frac{1}{\\sum_{j=1}^k p_j } + 1 \\right ) p_1 \\right ) \\\\ & \\leq \\frac{1}{2 } \\log \\left ( 1 + k^2 p_1 \\right ) - \\frac{1}{2 } \\log \\left(\\frac{1}{k } + p_1 \\right ) \\\\ & \\leq \\log k \\text.\\end{aligned}\\ ] ]    since @xmath462 , @xmath463 .",
    "now , the gap is given by @xmath464^+ \\\\ & \\leq \\frac{1}{2 } \\log \\left ( 1 + ( k-1)^2 p_l \\right ) - \\left [ \\frac{1}{2 } \\log p_l \\right]^+ \\\\ & \\leq \\frac{1}{2 } \\log ( 1 + ( k-1)^2 ) \\\\ & \\leq \\log k \\text.\\end{aligned}\\ ] ] @xmath384"
  ],
  "abstract_text": [
    "<S> in this paper , a class of relay networks is considered . </S>",
    "<S> we assume that , at a node , outgoing channels to its neighbors are orthogonal , while incoming signals from neighbors can interfere with each other . </S>",
    "<S> we are interested in the multicast capacity of these networks . as a subclass , </S>",
    "<S> we first focus on gaussian relay networks with interference and find an achievable rate using a lattice coding scheme . </S>",
    "<S> it is shown that there is a constant gap between our achievable rate and the information theoretic cut - set bound . </S>",
    "<S> this is similar to the recent result by avestimehr , diggavi , and tse , who showed such an approximate characterization of the capacity of general gaussian relay networks . </S>",
    "<S> however , our achievability uses a structured code instead of a random one . using the same idea used in the gaussian case </S>",
    "<S> , we also consider linear finite - field symmetric networks with interference and characterize the capacity using a linear coding scheme .    </S>",
    "<S> wireless networks , multicast capacity , lattice codes , structured codes , multiple - access networks , relay networks </S>"
  ]
}