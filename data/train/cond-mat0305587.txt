{
  "article_text": [
    "by now it is common knowledge that layered feedforward models are the workhorses in practical applications of neural networks and , hence , progress in the theoretical understanding of their capabilities and limitations should thus be welcome .",
    "recently , it has been shown @xcite how information theory can be used to construct neural network models leading to optimal performance .",
    "optimal for the task of retrieving an embedded pattern when the network starts far from it with a vanishingly small initial mutual information . for two - state networks",
    "this approach recovers the well - known hopfield model @xcite and for three - state networks a hamiltonian is found reminiscent of the blume - emery - griffiths ( beg ) model @xcite ( see @xcite for further references in a spin - glass context ) with a novel hebbian - like learning rule .",
    "both the extremely diluted asymmetric version @xcite and the fully connected version @xcite of this model have already been studied .",
    "these studies reveal that the retrieval performance of these so - called beg networks , compared with the one of other three - state networks of the same architecture ( see @xcite and references therein ) , is better in the sense that there is a selective increase in the information content of the network and that a considerably larger retrieval region exists in the phase diagrams leading also to a sizable increase in the critical capacity .",
    "in particular , new information carrying states , the so - called quadrupolar or pattern - fluctuation retrieval states appear .",
    "they play an explicit role in enhancing the retrieval performance of the network and they might also be important in practical applications . in pattern recognition ,",
    "e.g. , looking at a black and white picture on a grey background , these states would describe the situations where the exact location of the picture with respect to the background is known but , the details of the picture itself are not focused .",
    "furthermore , these pattern - fluctuation retrieval states might be helpful in modelling these focusing problems discussed in the framework of cognitive neuroscience @xcite .",
    "consequently , a study of this beg - model with a layered architecture is relevant .",
    "moreover , the study of this system is interesting in itself as an exactly solvable non - trivial dynamical system .",
    "compared with the extremely diluted asymmetric architecture it contains correlations among the neurons because of the presence of common ancestors , although there are no feedback loops @xcite . nevertheless , these correlations can be handled exactly giving rise to layer - to - layer evolution equations in closed form , as also seen in ising - type models @xcite . since in the diluted beg - architecture",
    "long transients appear due to the presence of saddle - point solutions which slow down considerably the dynamics of the network @xcite , it is worthwhile to find out whether such a dynamical behavior survives in the presence of correlations . finally , one also likes to find out what further differences there exist between these beg - architectures and their analogues in other three - state models .",
    "the outline of the paper is the following . in section 2",
    "we introduce the three - state layered beg network model and the relevant macroscopic variables . in section 3",
    "we solve the dynamics of this model by deriving the recursion relations for these variables .",
    "we discuss the results in section 4 , for both the stationary phase diagrams and the dynamic flow diagrams .",
    "we end with some concluding remarks in section 5 .",
    "consider a network that consists of @xmath2 layers , where each layer index may be taken as a time step @xmath3 .",
    "on each layer there are @xmath4 neurons that can take values @xmath5 , @xmath6 from the set @xmath7 , where @xmath8 denote the active states .",
    "a macroscopic number of @xmath9 ternary patterns is taken from a set of independent identically distributed random variables @xmath10 , @xmath11 , where @xmath8 are the active patterns , with the following probability distribution on layer @xmath3 , @xmath12 this distribution is assumed to be the same for every layer and the mean over it , @xmath13 , denotes the activity of the patterns .",
    "together with this , a set @xmath14 of normalized fluctuations of the binary patterns @xmath15 about their average is introduced , @xmath16 both , patterns and fluctuations , are embedded in the network by means of a generalized learning rule that consists of two hebbian - like parts , @xmath17 the first part is the usual rule in a three - state layered network that codifies the patterns , while the second part codifies the fluctuations of the binary active patterns @xmath18 about their average .    given the configuration on the first layer , @xmath19 , the state of a unit @xmath20 on layer @xmath21",
    "is determined by the configuration @xmath22 of the units in the previous layer according to the stochastic law @xmath23 }      { \\sum_{s\\in{s}}\\exp[-\\beta\\epsilon_i(s|\\bsigma_n^{t } ) ] } \\,\\,\\ , .",
    "\\label{4}\\ ] ] in this expression , the single - site energy function for unit @xmath24 on layer @xmath21 , @xmath25 is given by @xmath26 where @xmath27 are the local fields acting on that unit .",
    "in distinction to the usual three - state model @xcite , where the coefficient of the quadratic part in @xmath25 is an externally adjustable threshold parameter , we have here a random self - adjusting function @xmath28 that depends on both the states of the network and the patterns .",
    "next , we consider the relevant quantities that describe the performance of the network . for both , the macroscopic order parameters and the mutual information , we need the conditional probability distribution @xmath29",
    "that a neuron @xmath24 is in the state @xmath5 on layer @xmath3 given that the site @xmath24 of the stored pattern to be retrieved is @xmath30 .",
    "as a consequence of the independence of the states of the units on a given layer , it is sufficient to consider the distribution for a single typical neuron , so we can omit the index @xmath24 .",
    "we also omit the layer index @xmath3 and take from previous work @xcite @xmath31 where @xmath32 here , @xmath33 is the thermodynamic limit , @xmath34 , of the retrieval overlap @xmath35 between the state of the network and pattern @xmath36 , where the brackets denote the average over the probability distribution eq.(7 ) and the bar denotes the configurational average over the patterns .",
    "the other parameters are the thermodynamic limits @xmath37 and @xmath38 , of the neural ( dynamical ) activity , respectively the activity overlap @xmath39 finally , @xmath40 , is the thermodynamic limit of the fluctuation overlap between the binary state variables @xmath41 and @xmath42 defined as , @xmath43 as is clear from its definition , the fluctuation overlap is connected with the activity overlap .",
    "we remark that an underlying assumption that leads to the beg model and that should be preserved in the implementation for any network architecture is that the dynamic activity @xmath44 .",
    "the necessity of such an activity control system has been emphasized before ( cf . @xcite and references therein ) .",
    "next , the mutual information between patterns and neurons , regarding the patterns as the inputs and the neuron states as the output of the network channel on each layer , is an architecture independent property given by @xcite @xmath45 where @xmath46 + is the entropy and @xmath47 is the equivocation term with @xmath48 here , @xmath49 and @xmath50 is the parameter in the conditional probability @xmath51 .",
    "the mutual information can then be used to obtain the information @xmath52 , where @xmath53 is the storage ratio of the network .",
    "to solve the dynamics and obtain the recurrence relations for the macroscopic variables we need the expressions for the local fields which can be written as @xmath54 in terms of the actual overlaps @xmath55 at this point we remark that the fluctuation overlap @xmath56 can be viewed as the retrieval overlap between the binary states @xmath57 and the patterns @xmath58 and it is , in general , independent of the retrieval overlap @xmath59 .",
    "one would expect the fluctuation overlap to become relevant for larger synaptic noise when the states of the network no longer distinguish between the active patterns .",
    "indeed , it can be finite in a state of dynamic activity without necessarily a finite retrieval overlap @xmath59 , as has been found before for both the extremely diluted and the fully connected network . as will be seen in the next section ,",
    "the fluctuation overlap is responsible for an enhancement of the information in most of the retrieval regime and for a finite information carried in the absence of retrieval .    in eq .",
    "( [ 17 ] ) the brackets denote thermal averages with the probability distribution eq .",
    "now , @xmath60 and @xmath61 are given , respectively , by @xmath62 which , in the zero temperature limit , @xmath63 , become @xmath64 where @xmath65 is the usual step function .",
    "we assume that a single pattern , @xmath66 , and fluctuation , @xmath67 , are condensed at each layer , that is , @xmath68 and @xmath69 are of order @xmath70 , and that @xmath71 and @xmath72 , @xmath73 , are of order @xmath74 .",
    "we call the former @xmath75 and @xmath76 , respectively . in accordance with this",
    ", we also assume that in eq .",
    "( 8) @xmath77 and @xmath78 are both of @xmath79 , for @xmath80 , and we denote the information content of interest @xmath81 . following @xcite each local field may then be separated into a signal term and a noise @xmath82 where @xmath83 and @xmath84 are gaussian random variables with zero mean and unit variance .",
    "the layer - dependent variances of the local fields are given by @xmath85 together with eqs .",
    "( [ 17])-([19 ] ) , we have thus recurrence relations for the overlaps and for the variances of the local fields .",
    "the recurrence relations for the overlaps and the connecting equations for the other dynamical variables become @xmath86 where , as usual , @xmath87 .",
    "these expressions yield the fluctuation overlap @xmath88 and the dynamic activity @xmath89 .",
    "a further variable , @xmath90 , is introduced in the derivation of the recurrence relations for the variances of the two noises @xcite and it is given by @xmath91 \\ , .",
    "\\label{26}\\end{aligned}\\ ] ] introducing the susceptibilities with respect to the @xmath92 and the @xmath93 fields @xmath94 where @xmath95 , @xmath96 and @xmath97 \\,\\ , ,   \\label{29}\\end{aligned}\\ ] ] the recurrence relations for the gaussian noises become @xmath98    in contrast to these equations , the variances of the two local fields in the extremely diluted network do not depend on the susceptibilities but are simply given by the @xmath99-terms @xcite .",
    "since one expects somewhat different behavior for the two architectures , it may be interesting to see how the properties of the layered network model changeover to those of the extremely diluted network .",
    "this can be achieved most easily by means of a single amplitude @xmath100 , varying between @xmath101 and @xmath102 , in front of both second terms in the variance of the noises , as we will discuss at the end of the next section .    with the above equations we also get the time evolution of the information @xmath24 by means of eqs .",
    "( [ 13 ] ) to ( [ 15 ] ) .",
    "the recurrence relations for the macroscopic order parameters and the connecting equations can now be used to study the evolution of the network and to determine the properties of the stable stationary states .",
    "the stationary states are reached when @xmath103 , @xmath104 and @xmath105 .",
    "then @xmath106 and also @xmath107 , @xmath108 and @xmath109 reach stationary values .",
    "in this section we study both the stationary solutions and the flow diagrams for the layered beg - model . concerning the stable stationary states of the network we find three kinds of phases .",
    "we have one or more retrieval phases @xmath110 , one or more pattern - fluctuation retrieval phases @xmath111 , and a spin - glass phase @xmath112 .",
    "all three are sustained activity solutions in the sense that @xmath113 .    the existence of the @xmath114 phases can be understood as follows .",
    "a non - zero @xmath115 will appear when the active binary neuron states @xmath116 , that do not distinguish between @xmath8 neurons , coincide with the active patterns .",
    "at the same time the actual @xmath8 active neuron states may fail to recognize the active patterns , meaning @xmath117 .",
    "this is expected to occur at high @xmath118 where a stable @xmath114 phase should appear .",
    "there is a finite @xmath119 for either form of the active neuron states .",
    "the presence of a stable @xmath114 phase only at high @xmath118 has been checked already for both the extremely diluted @xcite and the fully connected network @xcite . in particular",
    ", it appears in the phase diagram for @xmath120 , which is independent of the architecture .",
    "since for the extremely diluted network it is found that the @xmath114 phase is not stable at zero - temperature , but is instead a saddle - point @xcite even for non - zero @xmath121 , we consider first in fig .",
    "1 the capacity - activity phase diagram at @xmath122 for the layered network .    there is a stable retrieval phase below the heavy solid line and a second retrieval phase with a smaller overlap appears in the lower shaded triangular regions .",
    "the pattern - fluctuation retrieval states are only saddle - point solutions below the light solid line .",
    "there is also everywhere a stable spin - glass solution and all the lines denote discontinuous transitions . for comparison ,",
    "the heavy dashed line shows the retrieval phase boundary for the optimal three - state ising layered network , optimal in the sense that the adjustable threshold parameter @xmath123 was chosen to optimize the storage capacity @xmath121 .",
    "clearly , for intermediate activity @xmath124 the beg network has a larger critical storage capacity than the ising network @xcite .",
    "another performance measure is the information content @xmath24 of the network and in figs .",
    "2 we show the information - capacity diagrams for various activities , at @xmath122 , for the beg and ising networks . again , the beg performance is better for intermediate activity and the information content is purely due , at this temperature , to the only stable retrieval phase . at larger activity ,",
    "@xmath125 say , the beg and ising networks compete for better performance at intermediate or larger @xmath121 values , as seen in fig .",
    "2c .    in order to highlight the role of the temperature and the activity in the appearance of a stable @xmath114 phase , we consider the temperature dependence for given activities and show in fig .",
    "3a the temperature - capacity diagram for @xmath126 and in fig .",
    "3b for @xmath125 . in the first one",
    "there is only a stable retrieval phase ( a second one in the shaded area ) below the heavy phase boundary , in addition to the spin - glass phase .",
    "there are no @xmath114 states , even not saddle - point solutions , in this region . in the case of @xmath125 , instead",
    ", there is a single stable retrieval phase below the solid ( dotted ) heavy lines which denote discontinuous ( continuous ) transitions , respectively , with a tricritical point at @xmath127 and @xmath128 .",
    "there is also a stable ( unstable saddle - point ) @xmath114 phase above ( below ) the heavy lines , as indicated in the figure , and the @xmath114 phase solutions end at discontinuous transitions shown as solid light lines .",
    "there is also everywhere a stable spin - glass solution which only disappears as @xmath129 .",
    "it is clear from fig . 3b",
    "that a stable @xmath114 phase will only appear for sufficiently large synaptic noise and large activity , a feature also found for the extremely diluted @xcite and the fully connected network @xcite .",
    "we consider now the role of the activity and show the capacity - activity phase diagrams in fig .",
    "4a , for @xmath130 , and in fig .",
    "4b , for @xmath131 . in the case of the former",
    "there is a stable retrieval phase below the heavy solid line and a second stable retrieval phase in the shaded region . again",
    ", there is a considerably enhanced storage capacity for retrieval , @xmath132 , for an intermediate activity of @xmath133 .",
    "the pattern - fluctuation retrieval states are only saddle - point solutions and this is the case below the light solid line . when @xmath131 there is a stable retrieval phase below the heavy dotted ( continuous transition ) and heavy solid ( discontinuous transition ) lines which merge at a tricritical point at @xmath134 and @xmath135 .",
    "the pattern - fluctuation retrieval solution is a stable phase below the light solid line and only a saddle point below the heavy and light dotted lines .",
    "there is now a finite storage capacity , @xmath136 , at @xmath137 for the retrieval of active patterns as a pattern - fluctuation retrieval phase .",
    "thus , as @xmath118 increases , the useful performance of the network goes over from the retrieval to the pattern - fluctuation retrieval phase .",
    "since the information content is a common performance measure for both phases , we show its temperature dependence in an information - capacity phase diagram in fig . 5 , for @xmath125 and various @xmath118 , where the solid ( dotted )",
    "lines represent information due to a stable retrieval ( pattern - fluctuation retrieval ) phase .    in all the above situations",
    "the @xmath138 solutions appear as a stable phase , and we consider now the flow diagrams in the @xmath139 order - parameter space .",
    "we show the results for @xmath131 , activity @xmath125 for @xmath140 in the presence of an @xmath141 phase in fig .",
    "6a , and for @xmath142 in the presence of a @xmath114 phase in fig",
    "these correspond to states on either side of the dotted phase boundary in fig .",
    "3b . in the first one",
    "there is a stable retrieval solution and there are two @xmath114 saddle points ( open circles ) and in the second one there is a stable and an unstable @xmath114 solution .",
    "in both cases there is a stable @xmath138 solution which can be accessed only below the lower @xmath114 saddle point .",
    "the chains of dots actually indicate time steps and , as can be seen from these figures , the flows to the stable solutions are considerably delayed by the saddle points in the form of slow transients of the dynamics .",
    "a remarkable feature of the flow diagrams is the presence of quite large basins of attraction either to the stable @xmath141 state or to the stable @xmath114 state , even for the fairly high @xmath118 ( and small @xmath121 ) for this case .",
    "also , not surprisingly , one finds a much smaller basin of attraction to the @xmath138 states .",
    "similar features have also been found in the dynamics of the extremely diluted network except for the @xmath138 states , which are absent in that case @xcite .",
    "we turn now to a brief study of the way in which the phase diagrams for the layered beg network turn into those for the extremely diluted network by means of a variable amplitude @xmath100 in the second terms in eq .",
    "( [ 31 ] ) , i.e. , @xmath143 when @xmath144 we have the extremely diluted architecture , @xmath145 corresponds to the layered one . to be specific ,",
    "consider the phase diagram for the layered network at @xmath131 shown in fig .",
    "4b where the phase boundary for existence of a stable @xmath114 phase ends at @xmath120 when the activity @xmath146 .",
    "as @xmath100 decreases from unity , this part of the phase boundary moves up and the critical @xmath121 gradually increases at @xmath146 . for @xmath147 it becomes already @xmath148 . at the same time",
    "the maximum activity @xmath149 for the lower retrieval phase boundary to a stable @xmath141 phase starts to increase towards larger values .",
    "the phase boundary itself continues to end at @xmath120 . for @xmath150 , say , the maximum @xmath149 for retrieval is still less than unity and the @xmath138 phase becomes now restricted to a region on both sides of the left phase boundary in the @xmath121 vs. @xmath149 phase diagram .",
    "ultimately , when the extremely diluted limit is reached , the stable @xmath141 phase goes up to @xmath146 , still with @xmath120 , with a fairly large @xmath114 phase and the @xmath138 phase becomes a self - sustained activity phase @xmath151 , with @xmath117 , @xmath152 and @xmath153 . in part of the phase diagram the @xmath151 and @xmath114 phases coexist , in accordance with our earlier results on the extremely diluted network @xcite",
    "we have derived the recursion relations that describe the time evolution of the macroscopic variables for an exactly solvable three - state network on a feed - forward layered architecture , optimizing the mutual information .",
    "this so - called layered blume - emery - griffiths ( beg ) network shows distinct stationary phase diagrams from either its extremely diluted or fully connected versions studied in the literature .",
    "being a truly dynamical system , there is no phase boundary of local stability in the layered network between either the retrieval , @xmath141 , or the pattern - fluctuation retrieval phase , @xmath114 , and the spin - glass phase , @xmath138 , in contrast to the behavior in the fully connected network .",
    "but this does not mean that within the retrieval regime the network can not be trapped in @xmath138 states , as it is clear from the flow diagrams .",
    "this makes the layered network different from the extremely diluted network in which the @xmath141 or @xmath114 states are the only stable states over most of the regions where these phases exist .",
    "we have found that , in common with both the extremely diluted and the fully connected network , a stable pattern - fluctuation retrieval phase appears only at high @xmath118 and for intermediate - to - large , but not full , activity @xmath149 . at low @xmath118 , in particular at @xmath122 ,",
    "this phase is not stable but is instead a saddle - point solution .",
    "nevertheless , the beg layered network has , selectively , a quite better performance than the three - state ising layered network , not only as far as the retrieval capacity is concerned , as in the case of both the fully connected and the extremely diluted network , but it also yields a considerably larger information content .",
    "this additional information is due to the enhancement by the pattern - fluctuation states of the stable retrieval phase at low and intermediate @xmath118 .",
    "the pattern - fluctuation retrieval phase , instead , is responsible for a much smaller but non - negligible information content .",
    "to summarize , the beg network on a feed - forward layered structure is not only an interesting dynamical system in itself but it also performs better than other , for instance , ising layered networks within relevant regimes of temperature and activity .",
    "we thank d. r. c. dominguez and t. verbeiren for critical discussions .",
    "we are indebted to both the fund for scientific research - flanders , belgium , and the fundao de amparo  pesquisa do estado do rio grande do sul ( fapergs ) , brazil , for financial support .",
    "one of us ( db ) thanks the warm hospitality of the instituto de fsica of the ufrgs , porto alegre , where this work was initiated .",
    "both re and wkt thank the kind hospitality and the support of the institute for theoretical physics of the k.u.leuven , where part of the work was done .",
    "the work of one of us ( wkt ) is also partially supported by the conselho nacional de desenvolvimento cientfico e tecnolgico ( cnpq ) , brazil .",
    "d. r. carreta dominguez and e. korutcheva , phys .",
    "e * 62 * , 2620 ( 2000 ) .",
    "d. boll and t. verbeiren , phys .",
    "a * 297 * , 156 ( 2002 ) .",
    "j.j.hopfield , proc .",
    "usa * 79 * , 2554 ( 1982 ) .",
    "m. blume , v. j. emery and r. b. griffiths , phys .",
    "rev . a * 4 * , 1071 ( 1971 ) ; m. blume , phys",
    ". rev . * 141 * , 517 ( 1966 ) ; h. w. capel , physica * 32 * , 966 ( 1966 ) .",
    "j. m. de arajo , f. a. da costa and f. d. nobre , eur .",
    "j. b * 14 * , 661 ( 2000 ) .",
    "d. r. c. dominguez , e. korutcheva , w. k. theumann , and r. erichsen jr .",
    ", in lecture notes in computer science ( springer , berlin ) * 2415 * , 129 ( 2002 ) .",
    "d. boll , d. r. dominguez , r. erichsen , jr .",
    ", e. korutcheva and w. k. theumann , cond - mat/0208281 .",
    "d. boll and t. verbeiren , j. phys .",
    "a * 36 * , 295 ( 2003 ) .",
    "d. boll , j. busquets blanco and g. m. shim , physica a * 318 * , 613 ( 2003 ) .",
    "d. boll , j. busquets blanco , g. m. shim and t. verbeiren , cond - mat/0304553 j. s. yedidia , j. phys .",
    "a * 22 * , 2265 ( 1989 ) .",
    "d. boll , g. m. shim , b. vinck , and v. a. zagrebnov , j. stat . phys . * 74 * , 565 ( 1994 ) .",
    "schacter , k.a .",
    "norman and w. koutstaal , annu .",
    "rev . psychol . * 49 * , 289 ( 1998 ) .",
    "e. barkai , i. kanter and h. sompolinsky , phys .",
    "a * 41 * , 590 ( 1996 ) .",
    "e. domany , w. kinzel and r. meir , j. phys .",
    "a * 22 * , 2081(1989 ) .",
    "d. boll , g. m. shim and b. vinck , j. stat . phys . * 74 * , 583 ( 1994 ) .",
    "d. boll and d. dominguez carreta , physica a * 286 * , 401(2000 ) .",
    "d. r. c. dominguez and d. boll , phys .",
    "* 80 * , 2961 ( 1998 ) .",
    "m. okada , neural netw .",
    "* 9 * , 1429 ( 1996 ) . c. e. shannon , bell syst .",
    "j. * 27 * , 379 ( 1948 ) .",
    "r. e. blahut , _ principles and practice of information theory _",
    "( addison - wesley , reading , ma 1990 ) , chapter 5 ."
  ],
  "abstract_text": [
    "<S> the time evolution of an exactly solvable layered feedforward neural network with three - state neurons and optimizing the mutual information is studied for arbitrary synaptic noise ( temperature ) . </S>",
    "<S> detailed stationary temperature - capacity and capacity - activity phase diagrams are obtained . </S>",
    "<S> the model exhibits pattern retrieval , pattern - fluctuation retrieval and spin - glass phases . </S>",
    "<S> it is found that there is an improved performance in the form of both a larger critical capacity and information content compared with three - state ising - type layered network models . </S>",
    "<S> flow diagrams reveal that saddle - point solutions associated with fluctuation overlaps slow down considerably the flow of the network states towards the stable fixed - points .    </S>",
    "<S> @xmath0instituut voor theoretische fysica , katholieke universiteit leuven , + celestijnenlaan 200 d , b-3001 leuven , belgium + @xmath1instituto de fsica , universidade federal do rio grande do sul , + caixa postal 15051 . </S>",
    "<S> 91501 - 970 porto alegre , rs , brazil </S>"
  ]
}