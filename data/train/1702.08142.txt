{
  "article_text": [
    "_ matrix balancing _ is the problem of rescaling a given square nonnegative matrix @xmath2 to a _ doubly stochastic matrix _",
    "@xmath3 , where every row and column sums to one , by multiplying two diagonal matrices @xmath4 and @xmath5 .",
    "this is a fundamental process for analyzing and comparing matrices in a wide range of applications including input - output analysis in economics , called a ras approach  @xcite , seat assignments in elections  @xcite , hi - c data analysis  @xcite , the sudoku puzzle  @xcite , and the optimal transportation problem  @xcite .",
    "an excellent review of theory and applications is given by  @xcite .",
    "the standard matrix balancing algorithm is the _ sinkhorn - knopp algorithm _",
    "@xcite , known to be a special case of bregman s balancing method  @xcite , which iterates rescaling of each row and column until convergence .",
    "the algorithm is widely used in the above applications due to its simple implementation and theoretically guaranteed convergence .",
    "however , the algorithm converges linearly  @xcite , which is prohibitively slow for recently emerging large and sparse matrices .",
    "although  @xcite and  @xcite tried to achieve quadratic convergence by newton s method , none of them succeeded because a nave application of newton s method involves solving a system of linear equations at each iteration , which requires @xmath6 computational cost for an @xmath7 matrix and significantly deteriorates the advantage of quadratic convergence .",
    "another open problem is _ tensor balancing _ , which is a generalization of balancing from matrices to higher - order multidimentional arrays , or _",
    "tensors_. the task is to rescale an @xmath0th order nonnegative tensor to a _ multistochastic tensor _ ,",
    "in which every fiber sums to one , by multiplying @xmath1th order @xmath0 tensors .",
    "there are some results about mathematical properties of multistochastic tensors  @xcite .",
    "however , there is no result on tensor balancing algorithms with guaranteed convergence that transforms a given tensor to a multistochastic tensor until now .",
    "here we show that newton s method with quadratic convergence can be applied to tensor balancing with avoiding solving a linear system on the full tensor .",
    "our strategy is to realize matrix and tensor balancing as _ projection onto a dually flat riemmanian submanifold _ ( figure  [ fig : overview ] ) , which is a statistical manifold and known to be the essential structure for probability distributions in information geometry  @xcite . using a partially ordered outcome space",
    ", we generalize the _ log - linear model _",
    "@xcite , which has been used to model the higher - order combinations of binary variables  @xcite , which allows as to model tensors as probability distributions in the statistical manifold .",
    "the remarkable property of our model is that the gradient of the manifold can be analytically computed using the _ mbius inversion formula _",
    "@xcite , the heart of combinatorial mathematics  @xcite , which enables us to directly obtain the jacobian matrix in newton s method .",
    "moreover , we show that @xmath8 entries for the size @xmath9 of a tensor are invariant with respect to one of the two coordinate systems of the statistical manifold",
    ". thus the number of equations in newton s method reduces from @xmath9 to @xmath10 .",
    "the reminder of this paper is organized as follows : we begin with a low - level description of our matrix balancing algorithm in section  [ sec : algorithm ] and demonstrate its efficiency in numerical experiments in section  [ sec : exp ] .",
    "to guarantee the correctness of the algorithm and extend it to tensor balancing , we provide theoretical analysis in section  [ sec : theory ] . in section  [ subsec : formulation ]",
    ", we introduce a generalized log - linear model associated with a partial order structured outcome space , followed by introducing the dually flat riemannian structure in section  [ subsec : duallyflat ] . in section  [ subsec : proj ] , we show how to use newton s method to compute projection of a probability distribution onto a submanifold .",
    "finally , we formulate the matrix and tensor balancing problem in section  [ sec : balance ] and summarize our contributions in section  [ sec : conclusion ] .",
    "given a nonnegative square matrix @xmath11 , the task of _ matrix balancing _ is to find a pair of vectors @xmath12 that satisfies @xmath13 where @xmath14 and @xmath15 .",
    "the balanced matrix @xmath16 is called _ doubly stochastic _ , in which each entry @xmath17 and all the rows and columns sum to one .",
    "the most popular algorithm is the sinkhorn - knopp algorithm , which repeats updating the following : @xmath18,\\\\   a_{ij } & = a_{ij } \\bigg/ \\sum_{i = 1}^n a_{ij } \\text { for each column } j \\in [ n].\\end{aligned}\\ ] ] we denote by @xmath19 = \\{1 , 2 , \\dots , n\\}$ ] hereafter .    in our algorithm , instead of directly updating each entry @xmath20 of @xmath21 , we update two parameters @xmath22 and @xmath23 defined as for each @xmath24 $ ] @xmath25 where we normalized entries as @xmath26 so that @xmath27 . we assume for simplicity that each entry is strictly larger than zero .",
    "the assumption will be removed in section  [ sec : balance ] .     and @xmath23 .",
    "]    the key to our approach is that we update @xmath28 with @xmath29 or @xmath30 by newton s method at each iteration @xmath31 while fixing @xmath32 with @xmath33 so that @xmath34 satisfies the following condition ( figure  [ fig : matrix ] ) : @xmath35 note that the rows and columns sum to not @xmath36 but @xmath37 due to the normalization .",
    "the update formula is described as @xmath38    \\theta_{12}^{(t + 1)}\\\\[5pt ]    \\vdots\\\\[5pt ]    \\theta_{1n}^{(t + 1)}\\\\[5pt ]    \\theta_{21}^{(t + 1)}\\\\[5pt ]    \\vdots\\\\[5pt ]    \\theta_{n1}^{(t + 1 ) }   \\end{bmatrix }   =   \\begin{bmatrix }    \\theta_{11}^{(t)}\\\\[5pt ]    \\theta_{12}^{(t)}\\\\[5pt ]    \\vdots\\\\[5pt ]    \\theta_{1n}^{(t)}\\\\[5pt ]    \\theta_{21}^{(t)}\\\\[5pt ]    \\vdots\\\\[5pt ]    \\theta_{n1}^{(t ) }   \\end{bmatrix }   - j^{-1 }   \\begin{bmatrix }    \\eta_{11}^{(t ) } - \\frac{n - 1 + 1}{n}\\\\[5pt ]    \\eta_{12}^{(t ) } - \\frac{n - 2 + 1}{n}\\\\[5pt ]    \\vdots\\\\[5pt ]    \\eta_{1n}^{(t ) } - \\frac{n - n + 1}{n}\\\\[5pt ]    \\eta_{21}^{(t ) } - \\frac{n - 2 + 1}{n}\\\\[5pt ]    \\vdots\\\\[5pt ]    \\eta_{n1}^{(t ) } - \\frac{n - n + 1}{n }   \\end{bmatrix},\\end{aligned}\\ ] ] where @xmath39 is the jacobian matrix given as @xmath40 which is derived from our theoretical result in theorem  [ theorem : metric ] .",
    "since @xmath39 is a @xmath41 matrix , the time complexity of each update is @xmath42 , which is needed to compute the inverse of @xmath39 .",
    "in contrast , if we apply newton s method to the full matrix @xmath21 as discussed by  @xcite , the cost becomes @xmath6 , resulting in inefficient computation .    after updating to @xmath43 ,",
    "we can compute @xmath44 and @xmath45 by equation  . since this update does not ensure the condition @xmath46",
    ", we again update @xmath47 as @xmath48 and recompute @xmath44 and @xmath45 for each @xmath24 $ ] .    by iterating the above update process in equation   until convergence ,",
    "the resulting matrix @xmath49 becomes doubly stochastic .",
    "we evaluate the efficiency of our algorithm compared to the two prominent balancing methods , the standard sinkhorn - knopp algorithm  @xcite and the state - of - the - art algorithm bnewt  @xcite , which uses newton s method - like iterations with conjugate gradients .",
    "all experiments were conducted on amazon linux ami release 2016.09 with a single core of 2.3 ghz intel xeon cpu e5 - 2686 v4 and 256 gb of memory .",
    "all methods were implemented in ` c++ ` with the ` eigen ` library and compiled with gcc 4.8.3 . we have carefully implemented bnewt by directly translating the matlab code provided in  @xcite into ` c++ ` with the ` eigen ` library for fair comparison , and used the default parameters .",
    "we measured the residual of a matrix @xmath50 by the squared norm @xmath51 , where @xmath52 in our algorithm , and ran each of three algorithms until the residual is below the tolerance threshold @xmath53 .",
    "an implementation of algorithms is available at https://github.com/mahito-sugiyama/newton-balancing .    .",
    "]    . ]",
    "* hessenberg matrix .",
    "* the first set of experiments were performed using a hessenberg matrix , which has been used as a standard benchmark for matrix balancing  @xcite .",
    "each entry of an @xmath7 hessenberg matrix @xmath54 is given as @xmath55 if @xmath56 and @xmath57 otherwise .",
    "we varied the size @xmath58 from @xmath59 to @xmath60 , and measured running time ( in seconds ) and the number of iterations of each method .    .",
    "]    results are plotted in figure  [ fig : hessenberg ] .",
    "our balancing algorithm with the newton s method ( plotted in blue in the figures ) is clearly the fastest : it is three to five orders of magnitude faster than the standard sinkhorn - knopp algorithm ( plotted in red ) .",
    "although the bnewt algorithm ( plotted in green ) is competitive if @xmath58 is small , it suddenly fails to converge whenever @xmath61 , which is consistent with results in the original paper  @xcite where there is no result for the setting @xmath61 on the same matrix .",
    "moreover , our method converges around @xmath59 to @xmath62 steps , which is about three and seven orders of magnitude smaller than bnewt and sinkhorn - knopp , respectively , at @xmath63 .    to see the behavior of the rate of convergence in detail , we plot the convergence graph in figure  [ fig : convergence ] for @xmath64 , where we observe the slow convergence rate of the sinkhorn - knopp algorithm and unstable convergence of the bnewt algorithm , which contrast to our quick convergence .    * trefethen matrix . *",
    "next we collected a set of trefethen matrices from a matrix collection website , which are nonnegative diagonal matrices .",
    "results are plotted in figure  [ fig : trefethen ] , where we observe the same trend as before : our algorithm is the fastest and about four orders of magnitude faster than the sinkhorn - knopp algorithm .",
    "note that larger matrices with @xmath65 do not have total support , which is the necessary condition for matrix balancing  @xcite , while the bnewt algorithm fails to converge if @xmath66 .",
    "in the following , we provide theoretical support to our algorithm by formulating the problem as projection within a statistical manifold , in which a matrix corresponds to an element , a probability distribution , in the manifold .",
    "we show that a balanced matrix forms a submanifold and matrix balancing is projection of a given distribution onto the submanifold , where the jacobian matrix in equation   is derived from the gradient of the manifold .",
    "first we introduce our log - linear probabilistic model , where the outcome space is a partially ordered set , or a _ poset_. we prepare basic notations and the key mathematical tool for posets , the mbius inversion formula , followed by formulating the log - linear model .",
    "a poset @xmath67 , the set of elements @xmath5 and a partial order @xmath68 on @xmath5 , is a fundamental structured space in computer science .",
    "a _ partial order _",
    "`` @xmath68 '' is a relation between elements in @xmath5 that satisfies the following three properties : for all @xmath69 ,    1 .",
    "@xmath70 ( reflexivity ) , 2 .",
    "@xmath71 , @xmath72 ( antisymmetry ) , 3 .",
    "@xmath71 , @xmath73 ( transitivity ) .    in",
    "what follows , @xmath5 is always finite and includes the least element ( bottom ) @xmath74 ; that is , @xmath75 for all @xmath76 .",
    "we denote @xmath77 by @xmath78 .",
    "@xcite introduced the _ mbius inversion formula _ on posets by generalizing the inclusion - exclusion principle .",
    "let @xmath79 be the _ zeta function _ defined as @xmath80 the _ mbius function _",
    "@xmath81 is defined as @xmath82 , the inverse of @xmath83 .",
    "alternatively , the mbius function can be inductively defined for all @xmath84 with @xmath71 as @xmath85 from the definition , it follows that @xmath86 with the kronecker delta @xmath87 such that @xmath88 if @xmath89 and @xmath90 otherwise . then for any functions @xmath91 , @xmath92 , and @xmath93 with the domain @xmath5 such that @xmath94 @xmath91 is uniquely recovered with the mbius function : @xmath95 this is called the _ mbius inversion formula _ and is at the heart of enumerative combinatorics  @xcite .",
    "we consider a probability vector @xmath96 on @xmath67 that gives a discrete probability distribution with the outcome space @xmath5 .",
    "a probability vector is treated as a mapping @xmath97 such that @xmath98 , where every entry @xmath99 is assumed to be strictly larger than zero .",
    "using the zeta and the mbius functions , let us introduce two mappings @xmath100 and @xmath101 as @xmath102 from the mbius inversion formula , we have @xmath103 they are generalization of the _ log - linear model _",
    "@xcite that gives the probability @xmath104 of an @xmath58-dimensional binary vector @xmath105 as @xmath106 where @xmath107 is a parameter vector and @xmath108 represents the expectation of variable combinations such that @xmath109 = \\pr(x^i = 1),\\quad   \\eta^{ij } = { \\mathbf{e}}[x^ix^j ] = \\pr(x^i = x^j = 1),\\ i < j , \\dots\\\\   \\eta^{1\\dots n } & = { \\mathbf{e}}[x^1 \\dots x^n ] = \\pr(x^1 = \\dots = x^n = 1).\\end{aligned}\\ ] ] they coincide with equations   and   when we let @xmath110 with @xmath111 , each @xmath76 as the set of indices of `` @xmath36 '' of @xmath112 , and the order @xmath68 as the inclusion relationship , that is , @xmath71 if and only if @xmath113 .",
    "@xcite have pointed out that @xmath114 can be computed from @xmath96 using the inclusion - exclusion principle in the log - linear model .",
    "we exploit this combinatorial property of the log - linear model using the mbius inversion formula on posets and extend the log - linear model from the power set @xmath115 to any kind of posets @xmath67 .",
    "a relevant log - linear model was proposed by  @xcite , but its geometric structure and the relationship with mbious inversion formula has been not analyzed yet .",
    "we theoretically analyze our log - linear model introduced in equation   and show that they form dual coordinate systems on a dually flat manifold , which has been mainly studied in the area of information geometry  @xcite moreover , we show that the riemannian metric and connection of our model can be analytically computed in closed forms .    in the following ,",
    "we denote by @xmath116 the function @xmath22 or @xmath23 and by @xmath117 the gradient operator with respect to @xmath118 , i.e. , @xmath119 for @xmath120 , and denote by @xmath121 the set of probability distributions specified by probability vectors , which forms a statistical manifold .",
    "we use uppercase letters @xmath122 for points ( distributions ) in @xmath121 and their lowercase letters @xmath123 for the corresponding probability vectors treated as mappings .",
    "we write @xmath124 and @xmath125 if they are connected with @xmath96 by equation  , and abbreviate subscripts if there is no ambiguity .",
    "we show that @xmath121 has the _ dually flat riemannian structure _ induced by two functions @xmath22 and @xmath23 in equation  .",
    "first we define @xmath126 as @xmath127 which corresponds to the normalizer of @xmath96 .",
    "it is a convex function since we have @xmath128 from @xmath129 .",
    "we apply the _ legendre transformation _ to @xmath126 given as @xmath130 then @xmath131 coincides with the negative entropy .",
    "[ theorem : negentropy ] @xmath132    from equation  , we have @xmath133 thus it holds that @xmath134 hence it is maximized with @xmath135 .    since they are connected with each other by the legendre transformation , they form a _ dual coordinate system _ @xmath136 and @xmath137 of @xmath121  ( * ? ? ?",
    "* section  1.5 ) , which coincide with @xmath22 and @xmath23 as follows .",
    "@xmath138    they can be directly derived from our definitions ( equations  ) as @xmath139 where @xmath140 from equation  .",
    "moreover , we can confirm the _ orthogonality _ of @xmath22 and @xmath23 as @xmath141   & = \\sum_{s \\in",
    "s}\\left[\\,p(s ) \\frac{\\partial}{\\partial \\theta(x ) } \\sum_{u \\in s } \\zeta(u , s)\\theta(u ) \\frac{\\partial}{\\partial \\eta(y ) } \\log\\left(\\sum_{u \\in s } \\mu(s , u)\\eta(u)\\right)\\,\\right]\\\\   & = \\sum_{s \\in s}\\left[\\,p(s ) \\left(\\zeta(x , s ) - \\eta(x)\\right ) \\frac{\\mu(s , y)}{p(s)}\\,\\right]\\\\   & = \\sum_{s \\in s } \\zeta(x , s)\\mu(s , y ) = \\delta_{xy}.\\end{aligned}\\ ] ] the last equation holds from equation  , hence the mbius inversion directly leads to the orthogonality .",
    "the _ bregman divergence _ is known to be the canonical divergence  ( * ? ? ?",
    "* section  6.6 ) to measure the difference between two distributions @xmath142 and @xmath143 on a dually flat manifold , which is defined as @xmath144 = \\psi(\\theta_p ) + { \\varphi}(\\eta_q ) - \\theta_p \\eta_q.\\end{aligned}\\ ] ] in our case , since we have @xmath145 from theorem  [ theorem : negentropy ] and equation  , it is given as @xmath144   = \\sum_{x \\in s } q(x ) \\log\\frac{q(x)}{p(x)},\\end{aligned}\\ ] ] which coincides with the _ kullback  leibler divergence _",
    "( kl divergence ) from @xmath143 to @xmath142 : @xmath146 = { d_{\\mathrm{kl}}}\\left[q , p\\right]$ ] .",
    "next we analyze the riemannian structure on @xmath121 and show that the mbius inversion formula enables us to compute the riemannian metric of @xmath121 .    [",
    "theorem : metric ] the manifold @xmath147 is a riemannian manifold with the riemannian metric @xmath148 such that for all @xmath149 @xmath150    since the riemannian metric is defined as @xmath151 when @xmath152 we have @xmath153 when @xmath154 , it follows that @xmath155    since @xmath148 coincides with the fisher information matrix , @xmath156 = g_{xy}(\\theta),\\quad   { \\mathbf{e}}\\left[\\,\\frac{\\partial}{\\partial \\eta(x ) } \\log p(s ) \\frac{\\partial}{\\partial \\eta(y ) } \\log p(s)\\,\\right ] = g_{xy}(\\eta).\\end{aligned}\\ ] ] then the riemannian ( levi  chivita ) connection @xmath157 with respect to @xmath116 , which is defined as @xmath158 for all @xmath159 , can be analytically obtained .",
    "the riemannian connection @xmath157 on the manifold @xmath147 is given in the following for all @xmath159 , @xmath160    we have for all @xmath69 , @xmath161 where @xmath162 and @xmath163",
    "it follows that @xmath164 on the other hand , @xmath165 therefore , from the definition of @xmath157 , it follows that @xmath166      projection of a distribution onto a submanifold is essential ; a number of machine learning algorithms are known to be formulated as projection of a distribution empirically estimated from data onto a submanifold that is specified by the target model  @xcite . here",
    "we define projection of distributions on posets and show that newton s method can be applied to perform projection as the jacobian matrix can be analytically computed .",
    "let @xmath167 be a submanifold of @xmath121 such that @xmath168 specified by a function @xmath169 with @xmath170 .",
    "projection of @xmath171 onto @xmath167 , called _",
    "@xmath172-projection _ , which is defined as the distribution @xmath173 such that @xmath174 is the minimizer of the kl divergence from @xmath142 to @xmath167 : @xmath175.\\end{aligned}\\ ] ] the dually flat structure with the coordinate systems @xmath22 and @xmath23 guarantees that the projected distribution @xmath176 always exists and is unique  ( * ? ? ?",
    "* section  1.6 ) .",
    "moreover , the _ pythagorean theorem _ holds in the dually flat manifold , that is , for any @xmath177 we have @xmath178 = { d_{\\mathrm{kl}}}[p , p_{\\beta } ] + { d_{\\mathrm{kl}}}[p_{\\beta } , q].\\end{aligned}\\ ] ] we can switch @xmath23 and @xmath22 in the submanifold @xmath167 by changing @xmath179 $ ] to @xmath180 $ ] , where the projected distribution @xmath176 of @xmath142 is given as @xmath181 this projection is called _ @xmath182-projection_.    given a boltzmann machine represented as an undirected graph @xmath183 with a vertex set @xmath184 and an edge set @xmath185 .",
    "the set of probability distributions that can be modeled by a boltzmann machine @xmath186 coincides with the submanifold @xmath187 with @xmath110 .",
    "let @xmath188 be an empirical distribution estimated from a given dataset .",
    "the learned model is the @xmath172-projection of the empirical distribution @xmath188 onto @xmath189 , where the resulting distribution @xmath190 is given as @xmath191      here we show how to compute projection of a given probability distribution .",
    "we show that newton s method can be used to efficiently compute the projected distribution @xmath176 by iteratively updating @xmath192 as @xmath193 until converging to @xmath176 .",
    "let us start with the @xmath172-projection with initializing @xmath192 . in each iteration @xmath194",
    ", we update @xmath195 for all @xmath196 while fixing @xmath197 for all @xmath198 , which is possible from the orthogonality of @xmath22 and @xmath23 . using newton s method ,",
    "@xmath199 should satisfy @xmath200 for every @xmath201 , where @xmath202 is an entry of the @xmath203 jacobian matrix @xmath39 and given as @xmath204 from theorem  [ theorem : metric ] .",
    "therefore we have the update formula for all @xmath201 as @xmath205 in @xmath182-projection , update @xmath206 for @xmath201 while fixing @xmath207 for all @xmath198 . to ensure @xmath208 , we add @xmath209 to @xmath210 and @xmath211 .",
    "we update @xmath212 at each step @xmath194 as @xmath213 in this case , we also need to update @xmath214 as it is not guaranteed to be fixed .",
    "let us define @xmath215 since we have @xmath216 it follows that @xmath217 the time complexity of each iteration is @xmath218 , which is required to compute the inverse of the jacobian matrix .",
    "now we are ready to solve the problem of matrix and tensor balancing as projection on a dually flat manifold .",
    "recall that the task of matrix balancing is to find @xmath12 that satisfy @xmath219 and @xmath220 with @xmath14 and @xmath15 for a given nonnegative square matrix @xmath11 .",
    "let us define @xmath5 as @xmath221 \\text { and } a_{ij } \\not= 0},\\end{aligned}\\ ] ] where we remove zero entries from the outcome space @xmath5 as our formulation can not treat zero probability , and give each probability as @xmath222 .",
    "the partial order @xmath68 of @xmath5 is naturally introduced as @xmath223 resulting in @xmath224 . in addition",
    ", we define @xmath225 for each @xmath226 $ ] and @xmath227 such that @xmath228 where the minimum is with respect to the order @xmath68 .",
    "if @xmath225 does not exist , we just remove the entire @xmath229th row if @xmath230 or @xmath229th column if @xmath231 from @xmath21 .",
    "then we switch rows and columns of @xmath21 so that the condition @xmath232 is satisfied for each @xmath227 , which is possible for any matrices . since we have @xmath233    \\sum_{i = 1}^{n } p((i , k ) ) & \\text{if } m = 2   \\end{array }   \\right.\\end{aligned}\\ ] ] if the condition   is satisfied , the probability distribution is balanced if for all @xmath226 $ ] and @xmath227 @xmath234 therefore we obtain the following result .      given a matrix @xmath235 with its normalized probability distribution @xmath171 such that @xmath222 .",
    "define the poset @xmath67 by equations   and   and let @xmath167 be the submanifold of @xmath121 such that @xmath236 where the function @xmath169 is given as @xmath237 , m \\in \\{1 , 2\\}\\},\\\\   \\beta({\\boldsymbol{\\iota}}_{k , m } ) & = \\frac{n - k + 1}{n}.\\end{aligned}\\ ] ] matrix balancing is the @xmath182-projection of @xmath142 onto the submanifold @xmath167 , that is , the balanced matrix @xmath238 is the distribution @xmath176 such that @xmath239 which is unique and always exists in @xmath121 , thanks to its dually flat structure .",
    "moreover , two balancing vectors @xmath240 and @xmath241 are @xmath242 for every @xmath243$].@xmath244      next we generalize our approach from matrices to tensors . for an @xmath0th order tensor @xmath245 and a vector @xmath246 ,",
    "the @xmath172-mode product of @xmath21 and @xmath247 is defined as @xmath248 we define _ tensor balancing _ as follows : given a tensor @xmath249 with @xmath250 , find @xmath1 order tensors @xmath251 such that @xmath252 for all @xmath253 $ ] , i.e. , @xmath254 , where each entry @xmath255 of the balanced tensor @xmath50 is given as @xmath256 } } r^m_{i_1 \\dots i_{m - 1 } i_{m + 1 } \\dots i_n}.\\end{aligned}\\ ] ] a tensor @xmath50 that satisfies equation   is called _ multistochastic _",
    "note that this is exactly the same as the matrix balancing problem if @xmath257 .",
    "it is straightforward to extend matrix balancing to tensor balancing as @xmath182-projection onto a submanifold . given a tensor @xmath249 with its normalized probability distribution @xmath142 such that @xmath258 for all @xmath259 .",
    "the objective is to obtain @xmath176 such that @xmath260 for all @xmath253 $ ] and @xmath261 $ ] . in the same way as matrix balancing ,",
    "we define @xmath5 as @xmath262^n | a_{i_1 i_2\\dots i_n } \\not= 0}\\end{aligned}\\ ] ] with removing zero entries and the partial order @xmath68 as n@xmath263 , i_m \\le j_m.\\end{aligned}\\ ] ] in addition , we introduce @xmath225 as @xmath264 and require the condition in equation  .      given a tensor @xmath265 with its normalized probability distribution @xmath171 given in equation  .",
    "the submanifold @xmath167 of multistochastic tensors is given as @xmath236 where the domain of the function @xmath169 is given as @xmath266 , m \\in [ n]}\\end{aligned}\\ ] ] and each value is described using the zeta function as @xmath267}\\zeta({\\boldsymbol{\\iota}}_{k , m } , { \\boldsymbol{\\iota}}_{l , m } ) \\frac{1}{n^{n - 1}}.\\end{aligned}\\ ] ] tensor balancing is the @xmath182-projection of @xmath142 onto the submanifold @xmath167 , that is , the multistochastic tensor is the distribution @xmath176 such that @xmath239 which is unique and always exists in @xmath121 , thanks to its dually flat structure .",
    "moreover , each balancing tensor @xmath268 is @xmath269 for every @xmath253$].@xmath244    our result means that the @xmath182-projection algorithm based on newton s method proposed in section  [ subsec : proj ] converges to the unique balanced tensor whenever @xmath270 holds .",
    "in this paper , we have solved the open problem of tensor balancing and presented an efficient balancing algorithm using newton s method .",
    "our algorithm quadratically converges , while the popular sinkhorn - knopp algorithm linearly converges .",
    "we have examined the efficiency of our algorithm in numerical experiments on matrix balancing and showed that the proposed algorithm is several orders of magnitude faster than the existing approaches .",
    "we have analyzed theories behind the algorithm , and proved that balancing is @xmath182-projection in a special type of a statistical manifold , in particular , a dually flat riemannian manifold studied in information geometry .",
    "our key finding is that the gradient of the manifold , equivalent to riemannian metric or the fisher information matrix , can be analytically obtained using the mbius inversion formula .",
    "our information geometric formulation can model a number of machine learning applications such as statistical analysis on a dag structure .",
    "thus we can perform efficient learning as projection using information of the gradient of manifolds by reformulating such models , which we will study in future work .",
    "e.  ganmor , r.  segev , and e.  schneidman .",
    "sparse low - order interaction network underlies a highly correlated and learnable neural population code .",
    "_ proceedings of the national academy of sciences _ , 1080 ( 23):0 96799684 , 2011 .",
    "s.  s.  p. rao , m.  h. huntley , n.  c. durand , e.  k. stamenova , i.  d. bochkov , j.  t. robinson , a.  l. sanborn , i.  machol , a.  d. omer , e.  s. lander , and e.  l. aiden . a 3d map of the human genome at kilobase resolution reveals principles of chromatin looping .",
    "_ cell _ , 1590 ( 7):0 16651680 , 2014 .",
    "j.  solomon , f.  de  goes , g.  peyr , m.  cuturi , a.  butscher , a.  nguyen , t.  du , and l.  guibas .",
    "convolutional wasserstein distances : efficient optimal transportation on geometric domains .",
    "_ acm transactions on graphics _ , 340 ( 4):0 66:166:11 , 2015"
  ],
  "abstract_text": [
    "<S> we solve _ tensor balancing _ , rescaling an @xmath0th order nonnegative tensor by multiplying @xmath1th order @xmath0 tensors so that every fiber sums to one . </S>",
    "<S> this generalizes a fundamental process of _ matrix balancing _ used to compare matrices in a wide range of applications from biology to economics . </S>",
    "<S> we present an efficient balancing algorithm with quadratic convergence using newton s method and show in numerical experiments that the proposed algorithm is several orders of magnitude faster than existing ones . to theoretically prove the correctness of the algorithm , we model tensors as probability distributions in a statistical manifold and realize tensor balancing as projection onto a submanifold . </S>",
    "<S> the key to our algorithm is that the gradient of the manifold , used as a jacobian matrix in newton s method , can be analytically obtained using the _ mbius inversion formula _ , the essential of combinatorial mathematics . </S>",
    "<S> our model is not limited to tensor balancing but has a wide applicability as it includes various statistical and machine learning models such as weighted dags and boltzmann machines .    </S>",
    "<S> = 1 </S>"
  ]
}