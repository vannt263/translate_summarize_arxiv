{
  "article_text": [
    "the modern cloud computing platform has leveraged virtualization to achieve economical multiplexing benefit while achieving isolation and flexibility simultaneously . separating the software from the underlying hardware , virtual machines ( vms )",
    "are used to host various cloud services @xcite .",
    "vms can share a common physical host as well as be migrated from one host to another .",
    "live migration , @xmath0 , moving vms from one physical machine to another without disrupting services , is the fundamental technique that enables flexible resource management in the virtualized data centers . by adjusting the locations of vms dynamically , we can optimize various objective functions to provide better services , such as improving performance , minimizing failure impact and reducing energy consumption @xcite .",
    "while there are continuous efforts on the optimal vm placements to reduce network traffic@xcite , vm migration has received relatively less attention .",
    "we argue that careful planning of vm migration is needed to improve the system performance . specifically , the migration process consumes not only cpu and memory resources at the source and the migrated target s physical machines @xcite , but also the network bandwidth on the path from the source to the destination@xcite .",
    "the amount of available network resource has a big impact on the total migration time , @xmath1 , it takes longer time to transfer the same size of vm image with less bandwidth . as a consequence",
    ", the prolonged migration time should influence the application performance .",
    "moreover , when multiple vm migrations occur at the same time , we need an intelligent scheduler to determine which migration tasks to occur first or which ones can be done simultaneously , in order to minimize the total migration time .",
    "more specifically , there can be complex interactions between different migration tasks . while some independent migrations can be performed in parallel",
    ", other migrations may share the same bottleneck link in their paths .",
    "in this case , performing them simultaneously leads to longer total migration time . in a big data center",
    ", hundreds of migration requests can take place in a few minutes @xcite , where the effect of the migration order becomes more significant .",
    "therefore , we aim to design a migration plan to minimize the total migration time by determining the orders of multiple migration tasks , the paths taken by each task , and the transmission rate of each task .    there have been a number of works on vm migration in the literature .",
    "work @xcite focused on minimizing migration cost by determining an optimal sequence of migration .",
    "however , their algorithms were designed under the model of one - by - one migration , and thus can not perform migration in parallel simultaneously , leading to a bad performance in terms of the total migration time .",
    "bari @xmath2 @xcite also proposed a migration plan of optimizing the total migration time by determining the migration order .",
    "however , they assumed that the migration traffic of one vm only can be routed along one path in their plan .",
    "compared with single - path routing , multipath routing is more flexible and can provide more residual bandwidth .",
    "thus , we allow multiple vms to be migrated simultaneously via multiple routing paths in our migration plan .    in this paper",
    ", we investigate the problem of how to reduce the total migration time in software defined network ( sdn ) scenarios@xcite .",
    "we focus on sdn because with a centralized controller , it is easier to obtain the global view of the network , such as the topology , bandwidth utilization on each path , and other performance statistics . on the other hand , sdn provides a flexible way to install forwarding rules so that we can provide multipath forwarding between the migration source and destination . in sdn ,",
    "the forwarding rules can be installed dynamically and we can split the traffic on any path arbitrarily . we allow multiple vms to be migrated simultaneously via multiple routing paths .",
    "the objective of this paper is to develop a scheme that is able to optimize the total migration time by determining their migration orders and transmission rates .",
    "our contribution is threefold , and is summarized as follows :    * we formulate the problem of vm migration from the network s perspective , which aims to reduce the total migration time by maximizing effective transmission rate in the network , which is much easier to solve than directly minimizing the total migration time . specifically , we formulate it as a mixed integer programming ( mip ) problem , which is np - hard . * we propose an approximation scheme via linear approximation plus fully polynomial time approximation , termed as fpta algorithm , to solve the formulated problem in a scalable way .",
    "moreover , we obtain its theoretical performance bound . * by extensive simulations , we demonstrate that our proposed fpta algorithm achieves good performance in terms of reducing total migration time , which reduces the total migration time by up to 40% and shorten the service downtime by up to 20% compared with the state - of - the - art algorithms .",
    "the rest of the paper is organized as follows . in section [ sec :",
    "system ] , we give a high - level overview of our system , and formulate the problem of maximizing effective transmission rate in the network . in section [ sec : approximation ] , we propose an approximation scheme composed of a linear approximation and a fully polynomial time approximation to solve the problem .",
    "further , we provide its performance bound . in section [ sec : evaluation ] , we evaluate the performance of our solution through extensive simulations . after presenting related works in section [ sec : relatedworks ] ,",
    "we draw our conclusion in section [ sec : conclusion ] .",
    "we first provide a high - level system overview in this section . as shown in fig .",
    "[ fig : fig1 ] , in such a network , all networking resources are under the control of the sdn controller , while all computing resources are under the control of some cloud management system , such as openstack .",
    "our vm migration plan runs at the coordinator and it is carried out via the openstack and sdn controller .",
    "more specifically , devices in the network , switches or routers , implement forwarding according to their obtained forwarding tables and do some traffic measurement .",
    "the sdn controller uses a standardized protocol , openflow , to communicate with these network devices , and gather link - state information measured by them .",
    "meanwhile , the sdn controller is responsible for computing the forwarding tables for all devices . on the other hand ,",
    "the cloud controller , openstack , is responsible for managing all computing and storage resources .",
    "it keeps all the necessary information about virtual machines and physical hosts , such as the memory size of the virtual machine , the residual cpu resource of the physical host .",
    "meanwhile , all computing nodes periodically report their up - to - date information to it .",
    "besides , openstack also provides general resource management functions such as placing virtual machines , allocating storage , etc .",
    "[ t ]        the processes of vm migration are described as follows .",
    "firstly , migration requests of applications are sent to the coordinator .",
    "based on the data collected from the openstack and sdn controller , the vm migration plan outputs a sequence of the vms to be migrated with their corresponding bandwidths . after reconfiguring the network and providing a bandwidth guarantee by sdn controller ,",
    "the vm migration plan is carried out at the corresponding time by openstack . by this way",
    ", it realizes the control and management of vm migrations .    to compute the migration sequence of vms , we need network topology and traffic matrix of the data center . besides , memory sizes and page dirty rates of vms , and residual physical resources such as cpu and memory are also needed .",
    "most of them can be obtained directly from the sdn controller or openstack , but measurements of page dirty rate and traffic matrix need special functions of the platform .",
    "we next present the approach to measure them in details :    * page dirty rate measurement : * we utilize a mechanism called shadow page tables provided by xen @xcite to track dirtying statistics on all pages @xcite .",
    "all page - table entries ( ptes ) are initially read - only mappings in the shadow tables .",
    "modifying a page of memory would result a page fault and then it is trapped by xen . if write access is permitted , appropriate bit in the vms dirty bitmap is set to 1 .",
    "then by counting the dirty pages in an appropriate period , we obtain the page dirty rate .",
    "* traffic measurement : * we assume sdn elements , switches and routers , can spilt traffic on multiple next hops correctly , and perform traffic measurements at the same time @xcite . to aid traffic measurements , an extra column in the forwarding table",
    "is used to record the node in the network that can reach the destination ip address as in work @xcite .",
    "take fig .",
    "[ fig : ft](a ) , which is the topology of the inter - datacenter wan of google , as an example , where all nodes are sdn forwarding elements . for instance",
    ", we assume node 8 ( ip address 64.177.64.8 ) is the node that can reach the subset 195.112/16 , and the shortest path from node 7 to node 8 goes through node 6 ( ip address 64.177.64.6 ) . then , the forwarding table of node 7 is shown in fig .",
    "[ fig : ft](b ) , where the first entry is corresponding to the longest matched prefix 195.112/16 . when a packet with the longest matched prefix 195.112/16 is processed by node 7 , @xmath3 showed in the figure increases by the packet length .",
    "thus , it tracks the number of bytes routed from node 7 to node 8 with the longest matched prefix 195.112/16 . using these data",
    ", the sdn controller easily obtains the traffic between arbitrary two nodes as well as residual capacity of each link .      in the system , we assume there is no alternate network dedicated to vm migrations , because of the cost of its deployment , especially in large - scale infrastructures .",
    "thus , only residual bandwidth can be used to migrate vms .",
    "then , our goal is to determining the vms migration orders and transmission rates that satisfy various constraints , such as capacity constraints for memory and links , to optimize the total migration time .",
    "now we give an example in fig .",
    "[ fig : mre ] . in this network , there are 2 switches ( @xmath4 and @xmath5 ) and 4 physical machines ( @xmath6 to @xmath7 ) hosting 4 vms ( @xmath8 to @xmath9 )",
    ". assume the capacity of each link is 100mbps and memory size of each vm is 500 mb .",
    "we want to migrate @xmath8 from @xmath6 to @xmath10 , @xmath11 from @xmath10 to @xmath12 , and @xmath9 from @xmath12 to @xmath7 .",
    "the optimal plan of migration orders and transmission rates is that first migrate @xmath8 and @xmath9 simultaneously , respectively with paths @xmath13 and \\{(@xmath12,@xmath5,@xmath7 ) } and the corresponding maximum bandwidths of 100mbps .",
    "then migrate @xmath11 with paths @xmath14 ,  @xmath15 } and the corresponding maximum bandwidth of 200mbps .",
    "it totally takes 7.5s to finish all the migrations .",
    "then , take random migration orders for example , @xmath0 , first migrate @xmath8 and @xmath11 simultaneously , respectively with paths @xmath13 and @xmath16 and the corresponding maximum bandwidths of 100mbps . then migrate @xmath17 with path @xmath18 and the corresponding maximum bandwidth of 100mbps .",
    "it totally takes 10s to finish all the migrations .    in this example",
    ", @xmath8 and @xmath9 can be migrated in parallel , while @xmath11 can be migrated with multipath .",
    "however , @xmath8 and @xmath11 , @xmath9 and @xmath11 share same links in their paths , respectively . by determining a proper order",
    ", these migrations can be implemented making full use of the network resources .",
    "thus , the total migration time is reduced by 25% in the example , illustrating the effect of the migration plan .",
    "[ t ]          in this section , we present the mathematical model of live migration , which is presented in  @xcite .",
    "we use @xmath19 to represent the memory size of the virtual machine .",
    "let @xmath20 denote the page dirty rate during the migration and @xmath21 denote the bandwidth allocated for the migration .",
    "then , the process of the live migration is shown in fig .  [ fig : lm ] .",
    "as we can observe , live migration copies memory in several rounds .",
    "assume it proceeds in @xmath22 rounds , and the data volume transmitted at each round is denoted by @xmath23 @xmath24 . at the first round , all memory pages are copied to the target host , and we have @xmath25 . then in each round , pages that have been modified in the previous round are copied to the target host .",
    "the transmitted data can be calculated as @xmath26 .",
    "thus , the elapsed time at each round can be calculated as @xmath27    @xmath28    let @xmath29 denote the ratio of @xmath20 to @xmath21 , that is @xmath30 combining the above analysis , the total migration time can be represented as : @xmath31    let @xmath32 denote the threshold value of the remaining dirty memory that should be transferred at the last iteration .",
    "we can calculate the total rounds of the iteration by the inequality @xmath33 . using the previous equations we obtain : @xmath34    in this model , the downtime caused in the migration",
    "can be represented as @xmath35 , where @xmath36 is the time spent on transferring the remaining dirty pages , and @xmath37 is the time spent on resuming the vm at the target host . for simplicity , we assume the size of remaining dirty pages is equal to @xmath32 .",
    "@xmath38 & set of network nodes and the number of network nodes .",
    "+ @xmath39 & set of network links and the number of network links .",
    "+ @xmath40 & the number of vms to be migrated .",
    "+ @xmath41 & the number of hosts in the network . + @xmath42 &",
    "capacity of the link @xmath43 .",
    "+ @xmath44 & source host of the migration @xmath45 .",
    "+ @xmath46 & target host of the migration @xmath45 .",
    "+ @xmath47 & memory size of the migration @xmath45 .",
    "+ @xmath48 & bandwidth allocated for the migration @xmath45 .",
    "+ @xmath49 & page dirty rate of the virtual machine corresponding to the migration @xmath45 .",
    "+ @xmath50 & set of all paths in the network .",
    "+ @xmath51 & set of paths between @xmath44 and @xmath46 .",
    "+ @xmath52 & set of paths using link @xmath43 .",
    "+    [ tab : symbal ]    the network is represented by a graph @xmath53 , where @xmath54 denotes the set of network nodes and @xmath55 denotes the set of links .",
    "let @xmath42 denote the residual capacity of the link @xmath43 .",
    "let a migration tuple @xmath56 denote that a virtual machine should be migrated from the node @xmath44 to the node @xmath46 with the memory size @xmath47 and the page dirty rate @xmath49 .",
    "there are totally @xmath40 migration tuples in the system . for the migration @xmath45",
    ", @xmath48 represents the bandwidth allocated for it .",
    "let @xmath51 denote the set of paths between @xmath44 and @xmath46 .",
    "the flow in path @xmath57 is represented by the variable @xmath58 .",
    "besides , as different migrations are started at different times , we define binary variable @xmath59 to indicate whether migration @xmath45 has been started at the current time .",
    "[ t ]        we first discuss the optimization objective . to obtain an expression of the total migration time is difficult in our model , because we allow multiple vms to be migrated simultaneously .",
    "thus , the total migration time can not simply be represented as the sum of the migration time of each vm like work @xcite , whose migration plans were designed under the model of one - by - one migration . moreover , even though we obtain the expression of the total migration time , the optimization problem is still difficult and can not be solved efficiently .",
    "for example , work @xcite gives an expression of the total migration time by adopting a discrete time model .",
    "however , they did not solve the problem directly , instead , they proposed a heuristic algorithm independent with the formulation without any theoretical bound .",
    "thus , we try to obtain the objective function reflecting the total migration time from other perspectives .    on the other hand , since the downtime of live migration is required to be unnoticeable by users , the number of the remaining dirty pages in the stop - and - copy round , @xmath60 , need to be small enough . according to the model provided in the last subsection",
    ", we have @xmath61 .",
    "thus , @xmath62 must be small enough .",
    "for example , if migrating a vm , whose memory size is 10 gb , with the transmission rate of 1gbps , to reduce the downtime to 100ms , we must ensure @xmath63 .",
    "thus , by ignoring @xmath62 in the equation  ( [ equ : tm ] ) , we have : @xmath64 we call the denominator as @xmath65 . from an overall viewpoint ,",
    "the sum of memory sizes of vms is reduced with the speed of @xmath66 , which is the total net transmission rate in the network . in turn , the integration of the net transmission rate respect to time is the sum of memory sizes . by maximizing the total net transmission rate , we can reduce the total migration time efficiently .",
    "thus , it is reasonable for us to convert the problem of reducing the migration time to maximizing the net transmission rate , which is expressed as @xmath67 .",
    "we now analyze constraints of the problem .",
    "a vm is allowed to be migrated with multipath in our model .",
    "thus , we have a relationship between @xmath48 and @xmath58 : @xmath68 besides , the total flow along each link must not exceed its capacity .",
    "thus , we have : @xmath69 for a migration that has not been started , there is no bandwidth allocated for it .",
    "thus , we have constraints expressed as follow : @xmath70 where @xmath71 is a constant large enough so that the maximum feasible bandwidth allocated for each migration can not exceed it .",
    "then , the problem of maximizing the net transmission rate can be formulated as follows : @xmath72 which is a mixed integer programming ( mip ) problem .    when some new migration requests come or old migrations are finished , the input of the problem changes .",
    "thus , we recalculate the programming under the new updated input . we notice that migrations that have been started can not be stopped .",
    "otherwise , these migrations must be back to square one because of the effect of the page dirty rate .",
    "thus , when computing this problem next time , we add the following two constraints to it : @xmath73 where @xmath74 and @xmath75 are equal to the value of @xmath59 and @xmath48 in the last computing , respectively .",
    "it means a migration can not be stopped and its bandwidth does not decrease .    by solving the programming",
    ", we obtain the vms that should be migrated with their corresponding transmission rates , maximizing the total net transmission rate under the current condition . by dynamically determining the vms to be migrated in tune with changing traffic conditions and migration requests , we keep the total net transmission rate maximized , which is able to significantly reduce the total migration time .",
    "solving the formulated mip problem , we obtain a well - designed sequence of the vms to be migrated with their corresponding bandwidths .",
    "however , the mip problem is np - hard , and the time to find its solution is intolerable on large scale networks . for example , we implement the mip problem using yalmip  a language for formulating generic optimization problems  @xcite , and",
    "utilize the glpk to solve the formulation  @xcite .",
    "then , finding the solution of a network with 12 nodes and 95 vms to be migrated on a quad - core 3.2ghz machine takes at least an hour .",
    "therefore , we need an approximation algorithm with much lower time complexity .",
    "let us reconsider the formulated mip problem  ( [ equ : prim ] ) . in this problem ,",
    "only @xmath76 are integer variables .",
    "besides , the coefficient of @xmath59 in the objective function is @xmath49 . in practical data center , @xmath49 is usually much less than @xmath48 , @xmath0 , the migration bandwidth of the vm .",
    "thus , we ignore the part of @xmath77 in the objective function , and remove variables @xmath78 .",
    "then , we obtain a linear programming ( lp ) problem as follows :    @xmath79    we select the optimal solution @xmath80 for  ( [ equ : lp ] ) with most variables that are equal to zero as our approximate solution",
    ". then we let @xmath81 denote the number of variables that are not zero in our approximate solution @xmath80 , and the corresponding binary decision variables @xmath59 are then set to be 1 , while the other binary decision variables are set to be 0",
    ". then the final approximate solution is denoted by @xmath82 .    as for the primary problem with the additional constraints shown in  ( [ equ : aprim ] ) , by a series of linear transformations , the problem is converted to a lp problem with the same form as  ( [ equ : lp ] ) except for a constant in the objective function , which can be ignored .",
    "thus we obtain a linear approximation for the primary mip problem .",
    "the exact solution of the lp problem  ( [ equ : lp ] ) still can not be found in polynomial time , which means unacceptable computation time for large scale networks .",
    "thus , we further propose an algorithm to obtain the solution in polynomial time at the cost of accuracy .    actually ,",
    "ignoring the background of our problem and removing the intermediate variable @xmath48 , we can express the lp problem  ( [ equ : lp ] ) as : @xmath83 this is a maximum multicommodity flow problem , that is , finding a feasible solution for a multicommodity flow network that maximizes the total throughput .",
    "* input : * network @xmath84 , link capacities @xmath42 for @xmath85 , migration requests @xmath86 + * output : * bandwidth @xmath48 , binary decision variable @xmath59 for each migration @xmath45 , and the amount of flow @xmath58 in path @xmath87",
    ". + initialize @xmath88 +    @xmath89 and @xmath58    fleischer @xmath2@xcite proposed a fully polynomial - time approximation scheme ( fptas ) algorithm independent of the number of commodities @xmath40 for the maximum multicommodity flow problem .",
    "it can obtain a feasible solution whose objective function value is within @xmath90 factor of the optimal , and the computational complexity is at most a polynomial function of the network size and @xmath91 .",
    "specifically , the fptas algorithm is a primal - dual algorithm .",
    "we denote @xmath92 as the dual variables of this problem . for all @xmath43 ,",
    "we call @xmath92 as the length of link @xmath93 . then",
    ", we define dist@xmath94 as the length of path @xmath57 .",
    "this algorithm starts with initializing @xmath92 to be @xmath95 for all @xmath43 and @xmath58 to be 0 for all @xmath87 .",
    "@xmath95 is a function of the desired accuracy level @xmath96 , which is set to be @xmath97 in the algorithm .",
    "the algorithm proceeds in phases , each of which is composed of @xmath40 iterations . in the @xmath98 phase ,",
    "as long as there is some @xmath99 for some @xmath45 with dist@xmath100min@xmath101 , we augment flow along @xmath57 with the capacity of the minimum capacity edge in the path .",
    "the minimum capacity is denoted by @xmath102 . then , for each edge @xmath93 on @xmath57 , we update @xmath92 by @xmath103 . at the end of the @xmath98 phase",
    ", we ensure every @xmath104 pair is at least @xmath105 or @xmath106 apart . when the lengths of all paths belonging to @xmath51 for all @xmath45 are between @xmath106 and @xmath90 , we stop",
    ". thus , the number of phases is at most @xmath107 .",
    "then , according to theorem in @xcite , the flow obtained by scaling the final flow obtained in previous phases by log@xmath108 is feasible .",
    "we modified the fptas algorithm by adding some post - processes to obtain the feasible @xmath109 and @xmath58 to the primal mip problem , and the modified algorithm is given in more detail in algorithm 1 .",
    "the computational complexity of the post - processes is only a linear function of the number of the vms to be migrated .",
    "in addition , the computational complexity of the fptas algorithm is at most a polynomial function of the network size and @xmath91 @xcite .",
    "thus , the computational complexity of our approximation algorithm is also polynomial .",
    "and we obtain a fully polynomial time approximation ( termed as fpta ) to the primal mip problem .",
    "to demonstrate the effectiveness of our proposed algorithm , we now analyze the bound of it .",
    "we first analyze the bound of the linear approximation compared with the primary mip problem ( [ equ : prim ] ) , then analyze the bound of the fpta algorithm compared with the linear approximation ( [ equ : lp ] ) . with these two bounds ,",
    "we finally obtain the bound of the fpta algorithm showing in algorithm 1 compared with the primary mip problem ( [ equ : prim ] ) .",
    "we discuss the bound of the linear approximation compared with the primary mip problem in normal data center network scenarios .",
    "common topologies of data center networks , such as fat tree , usually provide full bisection bandwidth , which enables all hosts communicating with each other with full bandwidth at the same time .",
    "thus , we can ignore the routing details , and only guarantee the traffic at each host not exceeds its maximum bandwidth .",
    "then , the lp problem ( [ equ : lp ] ) becomes : @xmath110 where @xmath111 is the maximum amount of traffic that can be received at host @xmath112 , while @xmath113 is the maximum amount of traffic that can be sent at host @xmath112 .",
    "besides , there are @xmath41 hosts in the data center . then , we let @xmath114 be the minimum of @xmath111 and @xmath113 .",
    "that is , min@xmath115 and min@xmath116 .",
    "similarly , we let @xmath117 be the maximum of @xmath49 . that is , max@xmath118 . we now provide some supplement knowledge about linear programming . for a linear programming with standard form , which can be represented as : @xmath119 where @xmath120 , @xmath121",
    ", @xmath122 has full rank @xmath123 , we have the following definitions and lemmas .    * definition 1 ( basic solution ) * given the set of @xmath123 simultaneous linear equations in @xmath22 unknowns of @xmath124 in ( [ equ : lpsf ] ) , let @xmath125 be any nonsingular @xmath126 submatrix made up of columns of @xmath127 .",
    "then , if all @xmath128 components of @xmath129 not associated with columns of @xmath125 are set equal to zero , the solution to the resulting set of equations is said to be a basic solution to @xmath124 with respect to the basis @xmath125 .",
    "the components of @xmath129 associated with columns of @xmath125 are called basic variables , that is , @xmath130  @xcite .    *",
    "definition 2 ( basic feasible solution ) * a vector @xmath129 satisfying ( [ equ : lpsf ] ) is said to be feasible for these constraints .",
    "a feasible solution to the constraints ( [ equ : lpsf ] ) that is also basic is said to be a basic feasible solution  @xcite .    * lemma 1 ( fundamental theorem of lp ) * given a linear program in standard form ( [ equ : lpsf ] ) where @xmath127 is an @xmath131 matrix of rank @xmath123 . if there is a feasible solution , there is a basic feasible solution .",
    "if there is an optimal feasible solution , there is an optimal basic feasible solution  @xcite .",
    "these definitions and the lemma with its proof can be found in the textbook of linear programming  @xcite . with these preparations",
    ", we have the following lemma :    * lemma 2 * there exists an optimal solution for ( [ equ : lpr ] ) , such that there are at least @xmath81 equalities that hold in inequality constraints of ( [ equ : lpr ] ) .    *",
    "proof * : problem ( [ equ : lpr ] ) can be represented in standard form as : @xmath132 \\left[\\begin{array}{cc}l \\\\ s \\end{array}\\right ] = c \\\\ & \\!\\!\\!\\!\\!\\!l , s\\geq 0 \\end{cases } \\end{array}\\ ] ] where @xmath133 , @xmath134 , @xmath135 is the identity matrix of the order @xmath40 , @xmath136 is composed of 0 and 1 , and each column of @xmath127 has and only has two elements of 1 . besides , @xmath137 \\in r^{2h\\times k+2h}$ ] has full rank @xmath138 .",
    "@xmath139    [ tmtvsdp1 ]    by lemma 1 , if the lp problem ( [ equ : lpmfsf ] ) has an optimal feasible solution , we can find an optimal basic feasible solution @xmath140 for ( [ equ : lpmfsf ] ) . by the definition of basic solution",
    ", the number of nonzero variables in @xmath140 is less than @xmath138 .",
    "meanwhile , by the definition of @xmath81 , the number of nonzero variables in @xmath141 , which is represented by @xmath142 , is greater than @xmath81 .",
    "thus the number of nonzero variables in @xmath143 is less than @xmath144 .",
    "then there are at least @xmath81 variables that are equal to zero in @xmath143 .",
    "meanwhile , @xmath145 means the equality holds in the inequality constraint corresponding @xmath146 row in @xmath127 .",
    "therefore , we have at least @xmath81 equalities that hold in inequality constraints of ( [ equ : lpr]).@xmath147    * theorem 1 * assume @xmath148 .",
    "let @xmath149 be the optimal value of the primal mip problem ( [ equ : prim ] ) , and @xmath54 be the optimal value of the lp problem ( [ equ : lpr ] ) .",
    "then we have @xmath150 , where @xmath151 . *",
    "proof * : we first prove @xmath152 . by lemma 2 , we know that there exists an optimal solution of ( [ equ : lpr ] ) such that there are at least @xmath81 equalities that hold in inequality constraints of ( [ equ : lpr ] ) .",
    "we select the corresponding rows @xmath153 of @xmath127 and corresponding elements @xmath154 of @xmath102 .",
    "then we have @xmath155 . because each column of @xmath127 has and only has two elements of 1 , elements of @xmath156 are at most 2 .",
    "thus , we have @xmath157 .    by definition of @xmath149 and @xmath54 , we have @xmath158 and @xmath159 .",
    "then we have @xmath160 .",
    "besides , by the last paragraph , we have @xmath161 .",
    "thus , we have @xmath162 , @xmath0 , @xmath150 .",
    "@xmath147    by the definitions of @xmath81 and @xmath117 , we have that the net transmission rate corresponding to the selected solution of ( [ equ : lpr ] ) is at least @xmath163 .",
    "thus , we obtain the bound of the linear approximation compared with the primary mip problem .",
    "we next analyze the bound of the fpta algorithm .",
    "according to theorem in @xcite , we have the following lemma : * lemma 3 * if @xmath57 is selected in each iteration to be the shortest @xmath164 path among all commodities , then for a final flow value @xmath165 obtained from the fptas algorithm , we have @xmath166 , where @xmath96 is the desired accuracy level .    because the value of @xmath58 is unchanged in our post - processes of algorithm 1 , @xmath167 is also the final flow value of our proposed fpta algorithm . note that it is not the bound of the fpta algorithm compared with the lp problem ( [ equ : lp ] ) , because our objective function is the net transmission rate , while @xmath167 is only the transmission rate of the solution of the fpta algorithm .",
    "besides , @xmath54 is not the maximum net transmission rate as well .",
    "the bound of the fpta algorithm is given in the following theorem :    * theorem 2 * let @xmath168 be the net transmission rate corresponding to the solution of algorithm 1 . in the data center networks providing full bisection bandwidth , we have @xmath169 , where @xmath149 is the optimal value of the primal mip problem ( [ equ : prim ] ) .    * proof * : by the definitions of @xmath81 and @xmath117 , we have that the net transmission rate corresponding to the solution of the fpta algorithm is at least @xmath170 , @xmath0 , @xmath171 .",
    "thus we have @xmath172 .",
    "meanwhile , by @xmath173 , we have @xmath174 .    by theorem 1",
    ", we have @xmath175 .",
    "thus we obtain the bound of the fpta algorithm compared with the primal mip problem.@xmath147",
    "with the increasing trend of owning multiple datacenter sites by a single company , migrating vms across datacenters becomes a common scenario .",
    "thus , to evaluate the performance of our proposed migration plan inside one datacenter and across datacenters , we select the following two topologies to implement our experiments : ( 1 ) the topology of a private enterprise data center located in midwestern united states ( prv1 in @xcite ) .",
    "( 2 ) b4 , google s inter - datacenter wan with 12 data centers interconnected with 19 links  @xcite ( showing in fig .",
    "[ fig : ft](a ) ) . in b4",
    ", each node represents a data center .",
    "besides , the network provides massive bandwidth .",
    "however , to evaluate the performance of our proposed algorithm under relatively hard conditions , we assume the capacity of each link is only 1gbps . on the other hand ,",
    "the capacities of links in rpv1 are set ranging from 1 gb to 10 gb according to  @xcite .",
    "the page dirty rate is set to 100mbps . besides , @xmath32 and @xmath37 are set to 100 mb and 20ms , respectively .",
    "the memory sizes of vms are also set ranging from 1 gb to 10 gb unless stated otherwise .    in our experiments ,",
    "we evaluate the performance of our proposed fpta algorithm compared with the optimal solution of the mip problem ( referred to as optimal algorithm ) and two state - of - the - art algorithms . in the two state - of - the - art algorithms ,",
    "one is the algorithm based on one - by - one migration scheme ( referred to as one - by - one algorithm ) , which is proposed in @xcite .",
    "the other is the algorithm that migrates vms by groups ( referred to as grouping algorithm ) , just as the algorithm proposed in @xcite . in this algorithm , vms that can be migrated in parallel are divided into the same group , while vms that share the same resources , such as the same link in their paths , are divided into different groups .",
    "then vms are migrated by groups according to their costs @xcite .",
    "we further set the function of the cost as the weighted value of the total migration time and the number of vms in each group .        in our first group of experiments , we compare the total migration time of our proposed fpta algorithm with that of other algorithms introduced above , with the variation of different parameters , @xmath0 , the number of vms to be migrated , the amount of background traffic , the average memory size of vms , in prv1 and b4 , respectively .",
    "the results are shown in fig .",
    "[ tmtvsdp ] and fig .",
    "[ tmtvsdp2 ] .    as we can observe from the fig .",
    "[ tmtvsdp ] , the performance of the one - by - one algorithm is much worse than that of the other three algorithms : when there are 100 vms to be migrated , the total migration time it takes is about 10 times more than that of the other three algorithms , illustrating its inefficiency in reducing the total migration time .",
    "since the performance gap between one - by - one algorithm and the other algorithms is huge , we do not show its performance in fig .",
    "[ tmtvsdp2 ] .",
    "besides , from fig .  [ tmtvsdp2](d ) we can observe that the computation time of our proposed fpta algorithm is at most a polynomial function of the number of the migrations , much less than that of using glpk to solve the lp problem ( [ equ : lp ] ) .",
    "as for the performance of the other three algorithms , their total migration time vs different parameters in data center networks and inter - datacenter wan has a similar trend : the total migration time of fpta algorithm is very close to that of the optimal algorithm , and much less than that of the grouping algorithm . take fig .",
    "[ tmtvsdp](a ) and fig .",
    "[ tmtvsdp2](a ) for example . in prv1",
    "( showing in fig .",
    "[ tmtvsdp](a ) ) , total migration time of the fpta algorithm and the optimal algorithm almost can not be distinguished , while in b4 ( showing in fig .",
    "[ tmtvsdp2](a ) ) the gap is less than 15% relative to the optimal algorithm .",
    "meanwhile , fpta algorithm performs much better than the grouping algorithm : its migration time is reduced by 40% and 50% in comparison with the grouping algorithm in prv1 and b4 , respectively .",
    "thus , the solution of our proposed fpta algorithm approaches to the optimal solution and outperforms the state - of - the - art solutions .      to illustrate the effectiveness of maximizing the net transmission rate , we implement the second group of experiments in the scenario where there are 40 vms to be migrated in b4 .",
    "net transmission rates of the fpta algorithm and the grouping algorithm are evaluated , as functions of time .",
    "the result is shown in fig .",
    "[ fig : ntrvst ] .",
    "[ t ]        according to previous theoretic analysis , we know that the sum of memory sizes of vms to be migrated is approximately equal to the integration of the net transmission rate with respect to time . in the experiments , the sum of memory sizes of the 40 vms to be migrated are 203 gb .",
    "meanwhile , in fig .",
    "[ fig : ntrvst ] , the shadow areas of the fpta and grouping algorithm , which can represent the integrations of the net transmission rates with respect to time , are 203.0 gb and 212.0 gb , respectively .",
    "the relative errors are less than 5% .",
    "it proves the correctness of our theoretic analysis . besides , from the figure we observe that the net transmission rate with the fpta algorithm remains a relatively high level in the process of migrations , about 2 times higher than that of the grouping algorithm on average",
    "thus , the integration of the net transmission rate can reaches @xmath176 with less time .",
    "specifically , in this group of experiments , the total migration time of fpta algorithm is reduced by up to 50% compared with grouping algorithm .",
    "thus , our fpta algorithm significantly reduces the total migration time by maximizing the net transmission rate .",
    "the scenarios of this group of experiments are to optimize the average delay of services in b4 .",
    "assume there are some vms located randomly in the data centers in b4 at the beginning , and they are providing services to the same user , who is located closely to the node 8 ( data center 8) . thus we need to migrate these vms to data centers as close to the node 8 as possible .",
    "however , memory that each data center provides is not unlimited , which is set to be 50 gb in our experiments .",
    "besides , there are 11 , 19 , 27 , 41 vms in the network , respectively .",
    "we find the final migration sets by minimizing the average delay .",
    "then we use the fpta and grouping algorithm to implement these migrations .",
    "the results are shown in fig .",
    "[ fig : rs ] .    fig .",
    "[ fig : rs](a ) and ( b ) show the total migration time and downtime , respectively . as we observe",
    ", fpta algorithm reduces the total migration time and downtime by 43.7% and 22.6% on average compared with those of the grouping algorithm , respectively .",
    "thus , our proposed fpta algorithm outperforms the grouping algorithm uniformly , which provides better services for the user .",
    "works related to our paper can be divided by two topics : live migration and migration planning .",
    "since clark proposed live migration @xcite , there have been plenty of works that have been done in this field .",
    "ramakrishnan @xmath2 @xcite advocated a cooperative , context - aware approach to data center migration across wans to deal with outages in a non - disruptive manner .",
    "wood @xmath2 @xcite presented a mechanism that provides seamless and secure cloud connectivity as well as supports live wan migration of vms . on the other hand ,",
    "vm migration in sdns has made some progress .",
    "mann @xmath2 @xcite presented crossroads ",
    "a network fabric that provides layer agnostic and seamless live and offline vm mobility across multiple data centers .",
    "boughzala @xmath2 @xcite proposed a network infrastructure based on openflow that solves the problem of inter - domain vm migration .",
    "meanwhile , keller @xmath2 @xcite proposed lime , a general and efficient solution for joint migration of vms and the network .",
    "these works indicate that sdn has big advantages in implementing vm migration .",
    "in contrast , we focus on developing a vm migration plan to reduce the total migration time in software defined network ( sdn ) scenarios .    meanwhile , there have been some works about vm migration planning .",
    "however , most of them were designed under the model of one - by - one migration@xcite or their main focuses were not to optimize the total migration time @xcite .",
    "ghorbani @xmath2 @xcite proposed a heuristic algorithm of determining the ordering of vm migrations and corresponding openflow instructions .",
    "however , they concentrated on bandwidth guarantees , freedom of loops , and their algorithm is based on the model of one - by - one migration .",
    "haj @xmath2 @xcite also focused on finding a sequence of migration steps .",
    "their main goal was to satisfy security , dependency , and performance requirements .",
    "in this work , we focus on reducing the total migration time by determining the migration orders and transmission rates of vms . since solving",
    "this problem directly is difficult , we convert the problem to another problem , @xmath0 , maximizing the net transmission rate in the network .",
    "we formulate this problem as a mixed integer programming problem , which is np - hard .",
    "then we propose a fully polynomial time approximation ( fpta ) algorithm to solve the problem .",
    "results show that the proposed algorithm approaches to the optimal solution with less than 10% variation and much less computation time .",
    "meanwhile , it reduces the total migration time and the service downtime by up to 40% and 20% compared with the state - of - the - art algorithms , respectively .                c.  mastroianni , m.  meo , g.  papuzzo , `` self - economy in cloud data centers : statistical assignment and migration of virtual machines '' , _ euro - par 2011 parallel processing_. springer berlin heidelberg , 2011 , pp .",
    "407 - 418 .",
    "a.  sridharan , r.  guerin , c.  diot , `` achieving near - optimal traffic engineering solutions for current ospf / is - is networks '' , _ ieee / acm transactions on networking ( ton ) _ , vol .",
    "13 , no .  2 , pp .  234 - 247 , 2005 .",
    "k.  k.  ramakrishnan , p.  shenoy , j.  van der merwe , `` live data center migration across wans : a robust cooperative context aware approach'',in _ proceedings of the 2007 sigcomm workshop on internet network management _  , 2007 , pp .  262 - 267 ."
  ],
  "abstract_text": [
    "<S> live migration is a key technique for virtual machine ( vm ) management in data center networks , which enables flexibility in resource optimization , fault tolerance , and load balancing . despite its usefulness </S>",
    "<S> , the live migration still introduces performance degradations during the migration process . </S>",
    "<S> thus , there has been continuous efforts in reducing the migration time in order to minimize the impact . from the network s perspective , the migration time is determined by the amount of data to be migrated and the available bandwidth used for such transfer . in this paper , we examine the problem of how to schedule the migrations and how to allocate network resources for migration when multiple vms need to be migrated at the same time . </S>",
    "<S> we consider the problem in the software - defined network ( sdn ) context since it provides flexible control on routing .    </S>",
    "<S> more specifically , we propose a method that computes the optimal migration sequence and network bandwidth used for each migration . </S>",
    "<S> we formulate this problem as a mixed integer programming , which is np - hard . to make it computationally feasible for large scale data centers , we propose an approximation scheme via linear approximation plus fully polynomial time approximation , and obtain its theoretical performance bound . through extensive simulations , </S>",
    "<S> we demonstrate that our fully polynomial time approximation ( fpta ) algorithm has a good performance compared with the optimal solution and two state - of - the - art algorithms . </S>",
    "<S> that is , our proposed fpta algorithm approaches to the optimal solution with less than 10% variation and much less computation time . </S>",
    "<S> meanwhile , it reduces the total migration time and the service downtime by up to 40% and 20% compared with the state - of - the - art algorithms , respectively . </S>"
  ]
}