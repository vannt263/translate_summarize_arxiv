{
  "article_text": [
    "consider an arbitrary riemannian manifold @xmath0 .",
    "geodesics on @xmath0 are locally shortest curves that are parametrized by the arc length . because they satisfy an initial value problem , they are uniquely determined by specifying a starting point @xmath1 and a starting velocity @xmath2 from the tangent space at @xmath3 .",
    "geodesics give rise to the _ riemannian exponential function _ that maps a tangent vector @xmath4 to the endpoint @xmath5 of a geodesic path @xmath6 \\rightarrow \\mathcal{m}$ ] starting at @xmath7 with velocity @xmath8 .",
    "it thus depends on the base point @xmath3 and is denoted by @xmath9 the riemannian exponential is a local diffeomorphism , @xcite .",
    "this means that it is locally invertible and that its inverse , called the _",
    "riemannian logarithm _ is also differentiable .",
    "moreover , the exponential is radially isometric , i.e. , the riemannian distance between the starting point @xmath3 and the endpoint @xmath10 on @xmath0 is the same as the length of the velocity vector @xmath11 of the geodesic @xmath12 when measured on the tangent space @xmath13 , ( * ? ? ?",
    "5.10 & cor . 6.11 )",
    "in this way , the exponential mapping gives a local parametrization from the ( flat , euclidean ) tangent space to the ( possibly curved ) manifold .",
    "this is also referred to as to representing the manifold in _ normal coordinates _",
    "@xcite .",
    "the riemannian exponential and logarithm are important both from the theoretical perspective as well as in practical applications .",
    "the latter fact holds true in particular , when @xmath0 is a _ matrix manifold _",
    "examples range from data analysis and signal processing @xcite over computer vision @xcite to adaptive model reduction and subspace interpolation @xcite and , more generally speaking , optimization techniques on manifolds @xcite .",
    "this list is far from being exhaustive .",
    "[ [ original - contribution ] ] original contribution + + + + + + + + + + + + + + + + + + + + +    in the work at hand , we present a matrix - algebraic derivation of an algorithm for computing the riemannian logarithm on the _ stiefel manifold_. the matrix - algebraic perspective allows us to prove local linear convergence . the approach is based on an iterative inversion of the closed formula for the associated riemannian exponential that has been derived in @xcite .",
    "our main tools are dynkin s explicit baker - campbell - hausdorff formula @xcite and goldberg s exponential series @xcite , both of which represent a solution @xmath14 to the matrix equation @xmath15 where @xmath16 and @xmath17 are the standard matrix exponential and matrix logarithm ( * ? ? ?",
    "* ,  11 ) .",
    "as an aside , we improve thompson s norm bound from @xcite on @xmath18 for the goldberg series by a factor of @xmath19 , where @xmath20 is any submultiplicative matrix norm .",
    "the stiefel log algorithm can be implemented in @xmath21 lines of ( commented ) matlab @xcite code , which we include in appendix [ app : code ] .",
    "[ [ comparison - with - previous - work ] ] comparison with previous work + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to the best of our knowledge , up to now , the only algorithm for evaluating the stiefel logarithm appeared in q. rentmeesters thesis ( * ? ? ?",
    "* alg . 4 , p.  91 ) .",
    "this algorithm is based on a riemannian optimization problem .",
    "it turns out that this approach and the ansatz that is pursued here , though very different in their course of action , lead to essentially the same numerical scheme .",
    "rentmeesters observes numerically a linear rate of convergence ( * ? ? ?",
    "* , p.100 ) .",
    "proving linear convergence for ( * ? ? ?",
    ". 4 , p.  91 ) would require estimates on the hessian , see @xcite , ( * ? ? ?",
    "in contrast , the derivation presented here uses only elementary matrix algebra and the convergence proof given here formally avoids the requirements of computing / estimating step sizes , gradients and hessians that are inherent to analyzing the convergence of optimization approaches .",
    "in fact , the convergence proof applies to ( * ? ? ? * alg . 4 , p.  91 ) and yields the linear convergence of this optimization approach when using a fixed _ unit step size _ ,",
    "but only on a sufficiently small domain .",
    "the thesis @xcite was published under a two - years access embargo and the fundamentals of the work at hand were developed independently before @xcite was accessible .",
    "[ [ transition - to - the - complex - case ] ] transition to the complex case + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the basic geometric concepts of the stiefel manifold , the algorithm for the riemannian log mapping developed here and its convergence proof carry over to complex matrices , where orthogonal matrices have to be replaced with unitary matrices and skew - symmetric matrices with skew - hermitian matrices and so forth , see also @xcite .",
    "the thus adjusted log mapping algorithm was also confirmed numerically to work in the complex case .",
    "[ [ organization ] ] organization + + + + + + + + + + + +    background information on the stiefel manifold are reviewed in section [ sec : stiefel_essentials ] .",
    "the new derivation for the stiefel log algorithm is in section [ sec : alg ] , convergence analysis is performed in section [ sec : convproof ] , experimental results are in section [ sec : experiments ] , and the conclusions follow in section [ sec : conclusions ] .    [ [ sec : notation ] ] notational specifics + + + + + + + + + + + + + + + + + + + +    the @xmath22-_identity matrix _ is denoted by @xmath23 . if the dimension is clear , we will simply write @xmath24 . the @xmath22-_orthogonal group _ ,",
    "i.e. , the set of all square orthogonal matrices is denoted by @xmath25 the standard matrix exponential and matrix logarithm are denoted by @xmath26 we use the symbols @xmath27 for the riemannian counterparts on the stiefel manifold .",
    "when we employ the qr - decomposition of a rectangular matrix @xmath28 , we implicitly assume that @xmath29 and refer to the ` economy size ' qr - decomposition @xmath30 , with @xmath31 , @xmath32 .",
    "this section reviews the essential aspects of the numerical treatment of stiefel manifolds , where we rely heavily on the excellent references @xcite .",
    "the _ stiefel manifold _ is the compact homogeneous matrix manifold of all column - orthogonal rectangular matrices @xmath33 the _ tangent space _",
    "@xmath34 at a point @xmath35 can be thought of as the space of velocity vectors of differentiable curves on @xmath36 passing through @xmath37 : @xmath38 for any matrix representative @xmath39 , the tangent space of @xmath40 at @xmath37 is represented by @xmath41 every tangent vector @xmath42 may be written as @xmath43 the dimension of both @xmath34 and @xmath36 is @xmath44 .",
    "each tangent space carries an inner product @xmath45 with corresponding norm @xmath46 .",
    "this is called the _ canonical metric _ on @xmath34 .",
    "it is derived from the quotient space representation @xmath47 that identifies two square orthogonal matrices in @xmath48 as the same point on @xmath36 , if their first @xmath49 columns coincide @xcite . endowing each tangent space with this",
    "metric ( that varies differentiably in @xmath37 ) turns @xmath36 into a _",
    "riemannian manifold_. we now turn to the riemannian exponential but for @xmath50 .",
    "an efficient algorithm for evaluating the stiefel exponential was derived in @xcite .",
    "the algorithm starts with decomposing an input tangent vector @xmath51 into its horizontal and vertical components with respect to the base point @xmath37 , @xmath52 because @xmath11 is tangent , @xmath53 is skew .",
    "then the matrix exponential is invoked to compute @xmath54 the final output is is used to emphasize that these matrices stem from the stiefel exponential as opposed to the closely related matrices @xmath55 that will appear in the procedure for the stiefel logarithm . ]",
    "@xmath56 ( a matlab function for the stiefel exponential is in the supplement in appendix [ supp : stexp ] . )",
    "the matrix exponential in is related with the solution of the initial value problem that defines a geodesic on @xmath36 , see @xcite for details .",
    "it turns out that the main obstacle in computing the inverse of the stiefel exponential and thus the stiefel logarithm is inverting , i.e. finding @xmath57 given @xmath58 , compare to ( * ? ? ?",
    "* eq . ( 5.21 ) ) .",
    "let @xmath59 and assume that @xmath60 is contained in a neighborhood @xmath61 of @xmath37 such that @xmath62 is a diffeomorphism from a neighborhood of @xmath63 onto @xmath61 .",
    "the central objective is to find @xmath42 such that @xmath64    because of alg .",
    "[ alg : stexp ] , we know that @xmath60 allows for a representation @xmath65 . hence , we have to determine the unknown matrices @xmath66 , @xmath67 , which feature the following properties : @xmath68 and @xmath69 .",
    "( note that by , @xmath70 and @xmath71 are the left upper and lower @xmath72 blocks of a @xmath73 orthogonal matrix . )",
    "we directly obtain @xmath74 we compute candidates for @xmath75 via a qr - decomposition @xmath76 the set of all orthogonal matrices with @xmath77 as an upper diagonal and lower off - diagonal block is parametrized via @xmath78 where @xmath79 is a specific orthogonal completion , computed , say , via the gram - schmidt process .",
    "thus , the objective is reduced to solving the following nonlinear matrix equation @xmath80 writing @xmath81 , this means finding a rotation @xmath82 such that @xmath83 .",
    "the first result is that solving indeed leads to the riemannian logarithm on the stiefel manifold .",
    "[ thm : bigthm ] let @xmath59 and assume that @xmath60 is contained in a neighborhood @xmath61 of @xmath37 such that @xmath62 is a diffeomorphism from a neighborhood of @xmath63 onto @xmath61 .",
    "let @xmath70 , @xmath75 , @xmath84 , @xmath85 as introduced in the above setting .",
    "suppose that @xmath86 solves , i.e. , @xmath87 define @xmath88",
    ". then @xmath89 , i.e. , @xmath90 .    by construction",
    ", it holds @xmath91 and hence @xmath92 now , we apply the stiefel exponential alg .",
    "[ alg : stexp ] to @xmath93 .",
    "this gives @xmath94 and @xmath95 with @xmath96 , we obtain    @xmath97    keeping in mind that @xmath98 , this leads to an output of @xmath99 thus , @xmath11 is a valid tangent vector in @xmath34 such that @xmath100 . from abstract differential geometry , we know that @xmath101 is the unique tangent with @xmath102 .",
    "we arrive at the claim @xmath103    having established theorem [ thm : bigthm ] , we now focus on solving .",
    "let    @xmath104    up to terms of first order , it holds @xmath105 . hence , the choice @xmath106 gives an approximate solution to .",
    "we define @xmath107 and iterate .",
    "this is the essential idea of alg .",
    "[ alg : stlog ] for the riemannian logarithm . in section [ sec : convproof ] we make use of the baker - campbell - hausdorff formula ( * ? ? ?",
    "* , p. 22 ) that corrects for the misfit in the approximative matrix relation @xmath108 for two non - commuting matrices @xmath109 in order to show that the above procedure leads to @xmath110 for all @xmath111 and a constant @xmath112 and is thus convergent .",
    "since the riemannian exponential is a local diffeomorphism , we have to postulate a suitable bound on the distance between the input matrices @xmath37 and @xmath60 .",
    "suppose that @xmath113 .",
    "recalling the definitions @xmath114 and @xmath115 , this gives the following bounds for the horizontal and the vertical component of @xmath116 with respect to the subspace spanned by @xmath37 : @xmath117    however , it turns out that for the convergence proof , estimates on the norms of @xmath118 , @xmath119 and @xmath120 are also required . by the cs - decomposition of orthonormal matrices (",
    "* thm 2.6.3 , p. 78 ) , the diagonal blocks @xmath70 and @xmath119 share the same singular values and so do the off - diagonal blocks @xmath121 . hence , @xmath122 .",
    "let @xmath123 be the svd of @xmath70 and @xmath124 be the svd of @xmath119 .",
    "an estimate for the singular values of @xmath70 can be obtained as follows : @xmath125 where we have used that @xmath126 .",
    "now , we replace the @xmath119 that has been obtained via , say , gram - schmidt by @xmath127 ( and , correspondingly , @xmath118 by @xmath128 ) .",
    "essentially , this is the orthogonal procrustes method , ( * ? ? ?",
    "* , p.601 ) , applied to @xmath129.this operation preserves the orthogonality of @xmath130 , but the new @xmath119 is symmetric with eigenvalue decomposition @xmath131 .",
    "this gives @xmath132 in summary , if @xmath113 and if we start the iterations indicated by with the procrustes orthogonal completion @xmath85 rather than the standard gram - schmidt process , we obtain alg . [ alg : stlog ] with the starting conditions",
    "@xmath133    [ [ computational - costs ] ] computational costs + + + + + + + + + + + + + + + + + + +    w.l.o.g .",
    "suppose that @xmath29 .",
    "in fact the most important case in practical applications is @xmath134 . because of the matrix product in step 1 and the qr - decomposition in step 2 of alg .",
    "[ alg : stlog ] , the preparatory steps 13 require @xmath135 flops .",
    "the dominating costs in the iterative loop , steps 510 , are the evaluation of the matrix logarithm for a @xmath136-by-@xmath136 orthogonal matrix and the matrix exponential for a @xmath49-by-@xmath49 skew - symmetric matrix in every iteration , both of which can be achieved efficiently via the schur decomposition .",
    "the costs are @xmath137 , see ( * ? ? ?",
    "a matlab function for alg .",
    "[ alg : stlog ] is in appendix [ app : code ] .",
    "in this section , we establish the convergence of alg .",
    "[ alg : stlog ] under suitable conditions .",
    "we state the main result as theorem [ thm : conv_thm ] ; the proof is subdivided into the auxiliary results lemma [ lem : bchlem ] , and lemma [ lem : preserve_norms ] as well as lemma [ lem : goldbergseries ] that appears in appendix [ app : improvedgoldberg ] . an essential requirement is that the point @xmath138 that is to be mapped to the tangent space @xmath139 is sufficiently close to the base point @xmath39 in the sense that @xmath140 . throughout",
    ", we will make extensive use of dynkin s explicit bch formula ( * ? ? ?",
    "* , p. 22 ) .",
    "[ thm : conv_thm ] let @xmath141 .",
    "assume that @xmath142 .",
    "let @xmath143 be the sequence of orthogonal matrices generated by alg .",
    "[ alg : stlog ] . if @xmath144 , then alg . [ alg : stlog ] converges to a limit matrix @xmath145 such that @xmath146 given a numerical convergence threshold @xmath147 , see alg .",
    "[ alg : stlog ] , line 7 , the algorithm requires at most @xmath148 iteration steps to meet the convergence criterion under the above conditions .     _",
    "[ alg : stlog ] generates a sequence of orthonormal matrices @xmath149 the proof of theorem [ thm : conv_thm ] will show that @xmath150 , see .",
    "therefore , the sequence of orthogonal products @xmath151 converges to a limit @xmath152 for @xmath153 .",
    "the limit @xmath152 solves .",
    "however , it is not required to actually form @xmath152 . _    in pursuit of the proof of theorem [ thm : conv_thm ] , we first show that if the norm of the matrix logarithm of the orthogonal matrix @xmath154 produced by alg .",
    "[ alg : stlog ] at iteration @xmath155 is sufficiently small , then the norm of the lower @xmath49-by-@xmath49 diagonal block of the matrix logarithm of the next iterate @xmath156 is strictly decreasing by a constant factor .    [",
    "lem : bchlem ] let @xmath141 .",
    "let @xmath157 be the sequence of orthogonal matrices generated by alg .",
    "[ alg : stlog ] .",
    "suppose that at stage @xmath155 , it holds @xmath158 then @xmath159 features a lower @xmath22-diagonal block of norm @xmath160    given @xmath161 , alg .",
    "[ alg : stlog ] computes the next iterate @xmath156 via @xmath162 where @xmath163 . for brevity ,",
    "we introduce the notation @xmath164 for the matrix logarithm .",
    "recall that @xmath165:= vw - wv$ ] denotes the commutator or lie - bracket of the matrices @xmath109 . from dynkin",
    "s formula for the baker - campbell - hausdorff series , see ( * ? ? ?",
    "* , p. 22 ) , we obtain    @xmath166 \\\\",
    "\\nonumber        & \\quad+\\frac{1}{12 }        \\left (          \\bigl[l_{v_k } , [ l_{v_k},l_{w_k}]\\bigr ]         + \\bigl[l_{w_k } , [ l_{w_k } , l_{v_k}]\\bigr ]        \\right)\\\\      \\nonumber        & \\quad-\\frac{1}{24 } \\biggl[l_{w_k } , \\bigl[l_{v_k } , [ l_{v_k},l_{w_k}]\\bigr ] \\ \\biggr ]         + \\sum_{l=5}^{\\infty}{z_l(l_{v_k } , l_{w_k } ) } ,      \\end{aligned}\\ ] ]    where @xmath167 are the terms of fifth order and higher in the series . in the case at hand , it holds    @xmath168       & = \\begin{pmatrix } 0   & b_k^tc_k \\\\ c_kb_k & 0\\end{pmatrix}.      \\nonumber      \\end{aligned}\\ ] ]    ( note that the basic idea in designing alg .",
    "[ alg : stlog ] was exactly to choose @xmath169 such that the lower diagonal block in the bch - series cancels in the first order terms . )",
    "the third and fourth order terms are    @xmath170    therefore , the series expansion for the lower diagonal block in @xmath171 starts with the terms of third order :    @xmath172    we tackle the higher order terms via lemma [ lem : goldbergseries ] from the appendix .",
    "the lemma applies because @xmath173 . in this setting",
    ", it gives @xmath174 since each of the `` letters '' @xmath175 appears at least once in every `` word '' that contributes to @xmath176 , see appendix [ app : improvedgoldberg ] and @xcite .",
    "writing @xmath177 and substituting in leads to @xmath178 the proof is complete , if we can show that @xmath179 .",
    "note that @xmath180 . as a consequence @xmath181 an obvious bound on the size of @xmath182",
    "is obtained via observing that @xmath183 , if @xmath184 .",
    "the corresponding @xmath185 is @xmath186 .",
    "a sharper bound can be obtained via solving the associated quartic equation .",
    "this shows that the inequality even holds for all @xmath187 .    in order to make use of lemma [ lem : bchlem ] , we establish conditions such that @xmath188 holds throughout the iterations of alg .",
    "[ alg : stlog ] .",
    "this is the goal of the the next lemma .",
    "it relies on the auxiliary results proposition [ prop : log_exp_bound ] , proposition [ prop : logm_bound ] and lemma [ lem : norm_startv0 ] from appendix [ app : logm_bound ] .",
    "proposition [ prop : log_exp_bound ] shows that @xmath189 for @xmath190 skew - symmetric ; proposition [ prop : logm_bound ] establishes a bound in the opposite direction : if @xmath191 is orthogonal such that @xmath192 , then @xmath193 .",
    "finally , lemma [ lem : norm_startv0 ] shows that @xmath194 for the first iterate @xmath195 of alg .",
    "[ alg : stlog ] , provided that @xmath142 .",
    "[ lem : preserve_norms ] let @xmath141 with @xmath142 .",
    "let @xmath157 be the sequence of orthogonal matrices generated by alg .",
    "[ alg : stlog ] , where @xmath196 .",
    "let @xmath197 and @xmath198 .",
    "if @xmath199 is small enough such that @xmath200 , then @xmath201    let @xmath202 . by lemma [ lem : norm_startv0 ] from appendix [ app : logm_bound",
    "] , it holds @xmath203 in particular , @xmath204 by alg .",
    "[ alg : stlog ] , @xmath205 , where @xmath206 is orthogonal . by proposition [ prop : log_exp_bound ] from appendix [ app : logm_bound ] @xmath207 writing @xmath208 , this leads to the estimate    @xmath209    where we have used and the fact that @xmath210 , see , . by lemma [ lem : norm_startv0 ] , @xmath211 .",
    "thus , the claim holds for @xmath212 .",
    "lemma [ lem : bchlem ] applies to @xmath213 and leads to @xmath214 for the lower diagonal block @xmath215 of the next iterate @xmath216 . therefore , by using proposition [ prop : log_exp_bound ] once more , we see that @xmath217 by induction , we obtain @xmath218 with    @xmath219    where @xmath220 .",
    "we can estimate @xmath221 as follows : by the induction hypothesis , we assume that we have checked that @xmath222 for @xmath223 .",
    "hence , lemma [ lem : bchlem ] ensures that @xmath224 for the lower diagonal block of @xmath225 , @xmath223 . as above , this gives @xmath226 .",
    "we thus may write @xmath227 with @xmath228 .",
    "this gives @xmath229 it holds @xmath230 using this estimate in gives @xmath231 and we finally arrive at @xmath232 recalling , we have @xmath233 with @xmath234 at every iteration @xmath155 . by lemma [ lem : norm_startv0 ] , @xmath235 and we see that the postulate on the size of @xmath236 is such that @xmath237 .",
    "thus lemma [ lem : bchlem ] indeed applies at iteration @xmath155 , which closes the induction .",
    "* remark : * the inequality @xmath238 holds precisely for @xmath239 .",
    "a further calculations shows that if @xmath240 , then @xmath241 , i.e. , the conditions of lemma [ lem : preserve_norms ] hold , for all @xmath240 .    with the tools established above at hand , we are now in a position to prove theorem [ thm : conv_thm ] .",
    "let @xmath242 be the sequence of orthogonal matrices generated by alg .",
    "[ alg : stlog ] . by lemma [ lem : bchlem ] and lemma [",
    "lem : preserve_norms ] , it holds @xmath243 for all @xmath244 , where @xmath245 . from this equation and the continuity of the matrix exponential , we obtain @xmath246 the convergence result is now an immediate consequence of alg .",
    "[ alg : stlog ] , step 10 .",
    "the upper bound on the iteration count required for numerical convergence is also obvious from .",
    "in this section , we discuss a special case that can be treated analytically . following , we present numerical results on the performance of alg .",
    "[ alg : stlog ] .      here",
    ", we consider the special situation , where the two points @xmath141 are such that their columns span the same subspace.:=\\mbox{colspan}(u)$ ] and @xmath247:=\\mbox{colspan}(\\tilde{u})$ ] are the same points on the grassmann manifold @xmath248 = [ \\tilde{u } ] \\in gr(n , p)$ ] . ]",
    "hence , there exists an orthogonal matrix @xmath249 such that @xmath250 . in this case , alg .",
    "[ alg : stlog ] produces the initial matrices @xmath251 and @xmath252 .",
    "note that the corresponding @xmath253 commutes with @xmath195 .",
    "thus , we have the reduced bch formula @xmath254 , i.e. , alg .",
    "[ alg : stlog ] converges after a single iteration and gives @xmath255 ( of course , it is also straight forward to show this directly without invoking alg .",
    "[ alg : stlog ] . )",
    "let @xmath256 be the spectrum of @xmath249 and suppose that @xmath70 is such that none of its eigenvalues is on the negative real axis , i.e. , @xmath257 .",
    "then , the maximal riemannian distance between two points @xmath37 and @xmath258 is bounded by    @xmath259    as a consequence @xmath260 the latter fact holds , because the eigenvalues of @xmath70 come in complex conjugate pairs . hence ,",
    "if @xmath49 is odd , there is at least one real eigenvalue @xmath261 and because @xmath257 , there is at least one zero argument @xmath262 . related is ( * ? ? ?",
    "* eq . ( 2.15 ) ) .",
    "first , we try to mimic the experiments featured in @xcite .",
    "5.5 ( lower left ) of the aforementioned reference shows the average iteration count when applying the optimization - based stiefel logarithm to matrices within a riemannian annulus of inner radius @xmath263 and outer radius @xmath264 around @xmath265 for dimensions of @xmath266 .",
    "convergence is detected , if @xmath267 , where @xmath268 is the same as in alg .",
    "[ alg : stlog ] .",
    "( ( * ? ? ?",
    "4 , p.  91 ) uses @xmath269 ) . since @xcite does not list the precise input data , we create comparable data randomly .",
    "to this end , we fix an arbitrary point @xmath270 and create artificially but randomly another point @xmath271 such that the riemannian distance from @xmath37 to @xmath60 is exactly @xmath264 . for full comparability , we replace the @xmath19-norm in alg .",
    "[ alg : stlog ] , line 7 with the frobenius norm .",
    "we average over @xmath272 random experiments and arrive at an average iteration count of @xmath273 . a matlab script that performs the required computations is available in appendix [ supp : ex52 ] .",
    "when the distance of @xmath37 and @xmath60 is lowered to @xmath263 , the average iteration count drops to a value of @xmath274 . as a second experiment",
    ", we now return to the @xmath19-norm and lower the convergence threshold to @xmath275 in the convergence criterion of alg .",
    "[ alg : stlog ] .",
    "we create randomly points @xmath276 that are also a riemannian distance of @xmath264 away from each other , where we consider various different dimensions @xmath277 , see table [ tab : convhist ] .",
    "we apply alg .",
    "[ alg : stlog ] to compute @xmath278 .",
    ".convergence of alg .",
    "[ alg : stlog ] for random data to an accuracy of @xmath279 . [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]      for random data @xmath141 for various @xmath280 and @xmath49 .",
    "convergence accuracy is set to @xmath279 . left : convergence graphs for dist@xmath281 ; right : for dist@xmath282.,scaledwidth=100.0% ]    fig .",
    "[ fig : convplots ] shows the associated convergence histories .",
    "the associated computation times are listed in table [ tab : convhist ] . as can be seen from the figure and the table , alg .",
    "[ alg : stlog ] converges slowest ( in terms of the iteration count ) in the case of @xmath283 .",
    "note that in this case , the constant @xmath284 that played a major role in the convergence analysis of alg .",
    "[ alg : stlog ] is largest .",
    "moreover , we observe that the algorithm converges in all test cases even though in only one of the experiments the theoretical convergence guarantee @xmath285 is satisfied , so that the theoretical bound derived here can probably be improved .",
    "table [ tab : convhist ] suggests that the impact of the size of @xmath284 on the iteration count is more direct than that of the actual riemannian distance .",
    "we repeat the exercise with random data @xmath276 that are a distance of @xmath286 apart , which is the lower bound for the injectivity radius on the stiefel manifold given in ( * ? ? ?",
    "* eq . ( 5.14 ) ) . in the case of @xmath283",
    ", we hit a random matrix pair @xmath287 , where the associated value@xmath284 is so large that the conditions of theorem [ thm : conv_thm ] _ and _ lemma [ lem : bchlem ] , lemma [ lem : preserve_norms ] do not hold .",
    "in fact , we have @xmath288 for the starting point of alg .",
    "[ alg : stlog ] in this case , which is close to @xmath289 .",
    "yet , the algorithm converges , but very slowly so , see table [ tab : convhist ] , second row and fig .",
    "[ fig : convplots ] , right side . in all of the other cases , alg .",
    "[ alg : stlog ] converges in well under ten iterations , even for the larger test cases . a matlab script that performs the required computations is available in appendix [ supp : ex52 ] .      in this section",
    ", we examine the convergence of alg .",
    "[ alg : stlog ] depending on the riemannian distance @xmath290 and the distance @xmath291 in the euclidean operator-@xmath19-norm . to this end",
    ", we create a random point @xmath39 with matlab by computing the thin qr - decomposition of an @xmath292 matrix with entries sampled uniformly from @xmath293 .",
    "likewise , we create a random tangent vector @xmath51 by chosing randomly a skew - symmetric matrix @xmath294 and a matrix @xmath295 , where the entries of @xmath296 and @xmath297 are again uniformly sampled from @xmath293 , and setting @xmath298 .",
    "we normalize @xmath299 according to the canonical metric @xmath300 , see section [ sec : stiefel_essentials ] . in this way , we obtain for every @xmath301 a point @xmath302 that is a riemannian distance of @xmath303 away from @xmath37 .",
    "we discretize the interval @xmath304 by @xmath305 equidistant points @xmath306 and compute    * the number of iterations until convergence when computing @xmath307 with alg .",
    "[ alg : stlog ] for @xmath308 . * the distance in spectral norm @xmath309 , @xmath308 . * the norm of the matrix logarithm of the first iterate @xmath213 from alg .",
    "[ alg : stlog ] , step 3 .",
    "the results are displayed in figures [ fig : plot_dist_vs_conv_st10000_400 ]  [ fig : plot_dist_vs_conv_st4_2 ] for dimensions of @xmath310 , @xmath311 and @xmath312 , respectively . in all cases ,",
    "the convergence threshold was set to @xmath313 .",
    "the algorithm converged in all cases , where @xmath314 and produced a tangent vector @xmath315 of accuracy @xmath316 . a matlab script that performs the required computations is available in appendix [ supp : ex53 ] .     for @xmath317 , where @xmath11 is a random tangent vector of canonical norm @xmath318 and @xmath319 , @xmath320 .",
    "convergence accuracy is set to @xmath279 .",
    "left : number of iterations until convergence vs. @xmath290 ; middle : @xmath291 vs. @xmath290 ; right : @xmath213 vs. @xmath290.,scaledwidth=100.0% ]    in the case of @xmath312 , the algorithm starts to fail for @xmath321 , where @xmath213 jumps to a value of @xmath289 .",
    "this indicates that @xmath195 features ( up to numerical errors ) an eigenvalue @xmath322 so that the standard principal matrix logarithm is no longer well - defined . in all the experiments that were conducted ,",
    "this behavior was observed only for small values of @xmath323 , while there was never produced a random data set where alg .",
    "[ alg : stlog ] failed for @xmath324 and @xmath325 .    , but for @xmath326 , @xmath327.,scaledwidth=100.0% ]    , but for @xmath328 , @xmath329.,scaledwidth=100.0% ]    the figures suggest that for small column - numbers @xmath49 , the ratio between the riemannian distance @xmath290 and the spectral distance @xmath284 is smaller than in higher dimensions .",
    "moreover , for smaller @xmath49 , it seems to be more likely to hit a random tangent direction along which alg .",
    "[ alg : stlog ] fails early than for higher @xmath49 .",
    "this may partly be explained by the star - shaped nature of the domain of injectivity of the riemannian exponential , ( * ? ? ?",
    "* lemma 5.7 ) , and the richer variety of directions in higher dimensions .    from these observations , it is tempting to conjecture that alg .",
    "[ alg : stlog ] will converge , whenever @xmath314 .",
    "however , these results are based on a limited notion of randomness and a more thorough examination of the numerical behavior of alg .",
    "[ alg : stlog ] is required to obtained conclusive results , which is beyond the scope of this work .",
    "note that the domain of convergence of alg .",
    "[ alg : stlog ] is related to the injectivity radius of @xmath36 but it does not have to be the same . in appendix [ supp : critical_convergence ] from the supplement , we state an explicit example in @xmath312 , where alg .",
    "[ alg : stlog ] produces a first iterate @xmath195 with @xmath322 for an input pair @xmath330 with @xmath331 , while the injectivity radius is estimated to be @xmath332 in @xcite . an analytical investigation in @xmath312",
    "might be possible and may shed more light on the precise value of the stiefel manifold s injectivity radius .",
    "we have presented a matrix - algebraic derivation of an algorithm for evaluating the riemannian logarithm @xmath333 on the stiefel manifold .",
    "in contrast to ( * ? ? ?",
    "* alg . 4 , p.  91 ) ,",
    "the construction here is not based on an optimization procedure but on an iterative solution to a non - linear matrix equation .",
    "yet , it turns out that both approaches lead to essentially the same numerical scheme .",
    "more precisely , our alg .",
    "[ alg : stlog ] coincides with ( * ? ? ?",
    "* alg . 4 , p.  91 ) , when a unit step size is employed in the optimization scheme associated with the latter method .",
    "apart from its comparatively simplicity , a key benefit is that our matrix - algebraic approach allows for a convergence analysis that does not require estimates on gradients nor hessians and we are able to prove that the convergence rate of alg .",
    "[ alg : stlog ] is at least linear .",
    "this , in turn , proves the local linear convergence of ( * ? ? ?",
    "* alg . 4 , p.  91 ) when using a unit step size .",
    "the algorithm shows a very promising performance in numerical experiments , even when the dimensions @xmath334 become large .",
    "so far , we have carried out a theoretical _ local _ convergence analysis .",
    "open questions to be tackled in the future include estimates on how large the convergence domain of alg .",
    "[ alg : stlog ] is in terms of the riemannian distance of the input points @xmath290 .",
    "this is related with the question of determining the injectivity radius of the stiefel manifold .",
    "estimates on the injectivity radius are featured in @xcite .",
    "as an alternative to dynkin s bch formula of nested commutators , goldberg has shown in @xcite that the solution to the exponential equation @xmath335 can be written as a formal series @xmath336 each term @xmath337 in is the sum over all _ words _ of length @xmath155 in the alphabet @xmath338 .",
    "for example , @xmath339 and @xmath340 are such words of length @xmath341 and @xmath342 and thus contributing to @xmath343 and @xmath344 , respectively .",
    "the coefficients are rational numbers @xmath345 , called goldberg coefficients .",
    "thompson @xcite has shown that the series converges provided that @xmath346 for any submultiplicative norm @xmath20 .",
    "more precisely , his result is that @xmath347 for @xmath348 , see also ( * ? ? ?",
    "2 ) . in the next lemma , we improve this bound by cutting the factor @xmath19 .    [",
    "lem : goldbergseries ] let @xmath346 .",
    "the goldberg series is majorized by @xmath349    one ingredient of thompson s proof is the following basic estimate on binomial terms : @xmath350 here , @xmath351 denotes the largest integer smaller or equal to @xmath352 .",
    "thompson s argument is that @xmath353 and that @xmath354 is the largest out of the @xmath355 terms in the binomial sum .",
    "( it appears twice , if @xmath356 is odd . ) in the following , we prefer to write this term with using the ceil - operator as @xmath357 , because in this way , the same index @xmath356 appears in the upper and lower entry of the binomial coefficient . for larger @xmath355 ,",
    "the inequality can in fact be improved by a factor of @xmath19 : @xmath358 for @xmath359 , we have @xmath360 ; for @xmath361 , the inequality evaluates to @xmath362 . to prove the claim , we proceed by induction .",
    "[ [ case-1-m - even ] ] case 1 : `` @xmath355 even '' + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this case , @xmath363 and    @xmath364    where we have used the symmetry in the pascal triangle ( @xmath356 is odd ) and the induction hypothesis to arrive at .    [",
    "[ case-2-m - odd ] ] case 2 : `` @xmath355 odd '' + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in this case , @xmath365 and    @xmath366    note that @xmath367 is the second - to - largest term in the binomial expansion of @xmath368 .",
    "moreover , since @xmath356 is even , the relation to the largest term is @xmath369 substituting in and applying the induction hypothesis gives @xmath370 using rather than in thompson s original proof leads to the improved bound of @xmath371 for @xmath372 .",
    "we tackle the terms involving words of lengths @xmath373 manually .",
    "the reference @xcite lists explicit expressions of the summands in the goldberg bch series up to @xmath374 .",
    "the first three of them read    @xmath375    the expressions for @xmath343 and @xmath344 are too cumbersome to be restated here .",
    "however , for our purposes , a very rough counting argument is sufficient : the expression for @xmath343 features @xmath376 length-@xmath341 words with non - zero goldberg coefficient and the largest goldberg coefficient is @xmath377 .",
    "hence , @xmath378 ( a more careful consideration reveals @xmath379 . )    the expression for @xmath344 features @xmath380 length-@xmath342 words with non - zero goldberg coefficient and the largest goldberg coefficient is @xmath381 .",
    "hence , @xmath382",
    "[ prop : log_exp_bound ] let @xmath383 be skew - symmetric with @xmath384 .",
    "then @xmath385    since @xmath190 is skew - symmetric , it features an evd @xmath386 with @xmath387 , where @xmath388 and @xmath389 .",
    "therefore , @xmath390 with @xmath391 and @xmath392 ( the latter estimate may also be deduced from fig .",
    "[ fig : plot_logm_norm_estimate ] . )",
    "[ prop : logm_bound ] let @xmath393 be such that @xmath394 .",
    "then @xmath395    let @xmath396 .",
    "the matrices @xmath191 and @xmath397 share the same ( orthonormal ) basis of eigenvectors @xmath398 and the spectrum of @xmath191 is precisely the spectrum of @xmath397 shifted by @xmath399 . by assumption ,",
    "hence , the eigenvalues @xmath401 are complex numbers of modulus one of the form @xmath402 , with @xmath403 .",
    "thus , @xmath404 lies on the unit circle but within a ball of radius @xmath405 around @xmath406 , see fig .",
    "[ fig : plot_logm_norm_estimate ] .",
    "the maximal angle @xmath185 for such a @xmath404 is bounded by the slope of the line that starts in @xmath407 and crosses the points of intersection of the two circles @xmath408 and @xmath409 .",
    "the intersection points are @xmath410 .",
    "therefore @xmath411 as a consequence , @xmath412     in the complex plane.,scaledwidth=90.0% ]    [ lem : norm_startv0 ] let @xmath141 with @xmath413 .",
    "let @xmath414 and @xmath415 be as constructed in the first steps of alg .",
    "[ alg : stlog ] .",
    "then @xmath416    because @xmath195 is orthogonal ,    @xmath417    where @xmath418 denotes the numerical radius of @xmath195 , see ( * ? ? ?",
    "* eq . 1.21 , p. 21 ) .",
    "likewise , @xmath419 so that the singular values of @xmath70 and @xmath119 range between @xmath420 and @xmath318",
    ". moreover , by the procrustes preprocessing outlined at the end of section [ sec : alg ] , @xmath421 see . combining these facts , we obtain @xmath422 , where    @xmath423    applying proposition [ prop : logm_bound ] to @xmath424 proves the claim .",
    "we present an example that shows that alg . [ alg : stlog ]",
    "may fail at computing @xmath425 even for @xmath59 that are only a riemannian distance of @xmath331 apart .",
    "consider @xmath426 and set @xmath427 note that @xmath428 and that @xmath429 with @xmath430 is the qr - decomposition of the tangent vector @xmath11 .",
    "hence , the stiefel exponential applied to this data set yields @xmath431 because of the simple structure of @xmath432 , the matrix exponential can be computed explicitly @xmath433 recall from section [ sec : stiefel_essentials ] that @xmath434 , which in this setting evaluates to @xmath435 , since @xmath11 is of unit norm also with respect to the canonical metric . for @xmath436",
    ", we obtain @xmath437 if we now apply alg .",
    "[ alg : stlog ] to the matrix pair @xmath287 , then we obtain in step 3 of the algorithm a corresponding @xmath438 which features @xmath439 as an eigenvalue and thus leads to a failure in the principal matrix logarithm .",
    "the problem here is the ambiguity in the orthogonal completion .",
    "if we replace the first row of the above @xmath195 with its negative , then we have still a valid orthogonal completion , and the method works .",
    "this example suggests that in a practical implementation of alg .",
    "[ alg : stlog ] , one should try and explore strategies to compute a suitable starting iterate @xmath195 with small @xmath213 .",
    "an important matrix manifold that is related with the stiefel manifold and that arises frequently in applications is the _",
    "grassmann manifold_. it is defined as the set of all @xmath49-dimensional subspaces @xmath440 , i.e. , @xmath441 in this supplementary section , i give sketches for derivations for the riemannian exponential and logarithm on the grassmannian .",
    "closed - form expressions for both mappings are known from the literature and i try to explain why the stiefel case is more difficult .",
    "for background theory , the reader is referred to @xcite , @xcite .",
    "the grassmann manifold can be realized as a quotient manifold of the stiefel manifold under actions of the orthogonal group via @xmath442| \\quad u\\in st(n , p)\\}.\\ ] ] the quotient view point allows for using points @xmath39 as representatives for points @xmath248 \\in gr(n , p)$ ] , i.e. , subspaces , see @xcite for details . for any matrix representative @xmath443 of @xmath444\\in gr(n , p)$ ] , the tangent space at @xmath445",
    "is represented by @xmath446 this representation also stems from considering @xmath447 as a quotient manifold with @xmath36 as the total space .",
    "in fact , the tangent space of the stiefel manifold can be decomposed into the so - called vertical space and the horizontal space with respect to the quotient mapping , @xmath448 , see ( * ? ? ?",
    "* problem 3.8 ) , @xcite , @xcite .",
    "the explicit representation of vectors in @xmath449}gr(n , p)$ ] that we have introduced above corresponds to the identification of the actual abstract tangent space @xmath449}gr(n , p)$ ] with the horizontal space @xmath450 .    from the quotient perspective ,",
    "grassmann tangent vectors are special stiefel tangent vectors @xmath451 , namely those associated with the special skew - symmetric matrix @xmath452 , cf . .",
    "hence , we may use the stiefel exponential to compute the grassmann exponential :    * given @xmath453}gr(n , p)$ ] , compute the qr - decomposition @xmath454 * compute the matrix exponential @xmath455 * return @xmath456}(\\delta ) = [ u_0 m + q_en_e]$ ] .",
    "it is precisely the extra upper - left zero - block in the matrix exponential in , that makes the grassmann case easier to tackle than the stiefel case : by using the svd @xmath457 and the series expansion of @xmath458 , it is straight - forward to show that @xmath459 which gives @xmath460 ( in the above formulae , it is understood that @xmath461 and @xmath462 are to be applied pointwise to the diagonal elements of the diagonal matrix @xmath463 . ) eventually , we arrive at @xmath464 = [ u_0d\\cos(\\sigma)d^t + q \\phi\\sin(\\sigma)d^t].\\ ] ] instead of starting with the qr - decomposition @xmath465 , we now see that we could have directly worked with the svd @xmath466 , which yields @xmath467.$ ]    this is exactly the expression that edelman et al .",
    "have found in ( * ? ? ?",
    "2.3 ) for the riemannian exponential on @xmath447 and the derivation above can be considered as a thin svd-version of ( * ? ? ?",
    "2.3 , proof 2 , p. 320 ) .",
    "the inverse of this mapping , i.e. , the riemannian logarithm on @xmath447 can be deduced as follows : consider @xmath468,[\\tilde{u}]\\in gr(n , p)$ ] . under the assumption that @xmath247 $ ] is sufficiently close to @xmath248 $ ] , it holds that @xmath469 and the task is to find @xmath470 and @xmath471 such that @xmath472 .",
    "the first matrix factor @xmath70 is uniquely determined by @xmath473 .",
    "we obtain candidates for @xmath84 by computing the qr - decomposition @xmath474 . yet , in order to reverse , it is require to work with consistent coordinates .",
    "taking , into account , this is established by setting @xmath475 and computing the svd @xmath476 , because by defining @xmath477 , we can decompose @xmath478 this shows that the choice @xmath479 yields a tangent vector @xmath480 such that @xmath481\\\\ \\nonumber      & = & [ u_0 d\\cos(\\sigma ) d^t + q\\phi \\sin(\\sigma ) d^t ]   = [ \\tilde{u } ] .   \\end{aligned}\\ ] ] note that @xmath482 . hence , we now see that we could have directly started with the svd of @xmath483 to arrive at @xmath484 this is the well - known closed - form of the grassmann logarithm .",
    "unfortunatley , i was not able to track down the original derivation .",
    "the earliest appearance in the literature that i found was ( * ? ? ?",
    "however , this reference only mentions the above formuala but does not cite a source . in summary ,",
    "the grassmann case is easier to deal with because of the extra off - diagonal block structure in the associated matrix exponential , which leads to a cs - decomposition in by a _ similarity transformation _ ; compare this to ( * ? ? ?",
    "* thm . 2.6.3 , p.78 ) .",
    ".... % function [ delta , k , conv_hist , norm_logv0 ] = ...                                   stiefel_log_supp(u0 , u1 , tau ) % ------------------------------------------------------------- % @author : ralf zimmermann , imada , sdu odense % % input arguments        %   u0 , u1 : points on st(n , p ) %      tau : convergence threshold % output arguments %    delta : log^{st}_u0(u1 ) ,   %            i.e. tangent vector such that exp^st_u0(delta ) = u1 %        k : iteration count upon convergence % supplementary output %   conv_hist : convergence history % norm_logv0 : norm of matrix log of first iterate v0 % ------------------------------------------------------------- % get dimensions [ n , p ] = size(u0 ) ; % store convergence history conv_hist = [ 0 ] ;    % step 1 m = u0'*u1 ; % step 2 [ q , n ] = qr(u1 - u0*m,0 ) ;    % thin qr of normal component of u1 % step 3 [ v , ~ ] = qr([m;n ] ) ;                    % orthogonal completion    % \" procrustes preprocessing \" [ d , s , r ]       = svd(v(p+1:2*p , p+1:2*p ) ) ; v(:,p+1:2*p ) = v(:,p+1:2*p)*(r*d ' ) ; v             = [ [ m;n ] , v(:,p+1:2*p ) ] ;   %           |m   x0|                                         % now , v = |n   y0|   % just for the record norm_logv0 = norm(logm(v),2 ) ;                                                                              % step 4 : for - loop for k = 1:10000      % step 5      [ lv , exitflag ] = logm(v ) ;                                    % standard matrix logarithm                                    %",
    "|ak   -bk'|                                    % now , lv =    |bk    ck |      c = lv(p+1:2*p , p+1:2*p ) ;      % lower ( pxp)-diagonal block      % steps 6 - 8 : convergence check      normc = norm(c , 2 ) ;      conv_hist(k ) = normc ;      if normc < tau ;          disp(['stiefel log converged after ' , num2str(k ) , ...                ' iterations . ' ] ) ;          break ;      end      % step 9      phi = expm(-c ) ;               % standard matrix exponential      % step 10      v(:,p+1:2*p ) = v(:,p+1:2*p)*phi ;    % update last p columns end % prepare output                          |a   -b'| % upon convergence , we have   logm(v ) =    |b    0 | = lv %      a = lv(1:p,1:p ) ;      b = lv(p+1:2*p , 1:p ) % delta = u0*a+q*b delta = u0*lv(1:p,1:p ) + q*lv(p+1:2*p , 1:p ) ; return ; end ....    * note : * the performance of this method may be enhanced by computing @xmath458 , @xmath485 via a schur decomposition .",
    "[ [ first - experiment - discribed - in - section-5.2 ] ] first experiment discribed in section 5.2 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    .... % ------------------------------------------------------------- % script_stiefel_log_supp52.m % % @author : ralf zimmermann , imada , sdu odense % ------------------------------------------------------------- clear ; % set dimensions n = 10 ; p = 2 ; % fix stream of random numbers for reproducability s = randstream('mt19937ar','seed',1 ) ; % set number of random experiments runs = 100 ; dist = 0.4*pi ; average_iters = 0 ; for j=1:runs      % create random stiefel data      [ u0 , u1 , delta ] = create_random_stiefel_data(s , n , p , dist ) ;",
    "% ' project ' delta onto st(n , p ) via the stiefel exponential      u1 = stiefel_exp_supp(u0 , delta ) ;      % compute the stiefel logarithm      [ delta_rec , k ] = stiefel_log_supp(u0 , u1 , 1.0e-13 ) ;                        % uncomment the following lines to check                            % if stiefel logarithm recovers delta      % norm(delta_rec - delta )      average_iters = average_iters + k ; end average_iters = average_iters / runs ; disp(['the average iteration count of the stiefel log is ' , ...        num2str(average_iters ) ] ) ;             % eof : script_stiefel_log_supp52.m % -------------------------------------------------------------    ....    [ [ second - experiment - discribed - in - section-5.2 ] ] second experiment discribed in section 5.2 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    .... % ------------------------------------------------------------- % script_stiefel_log_supp52b.m % % @author : ralf zimmermann , imada , sdu odense % ------------------------------------------------------------- clear ; close all ;    dist = 0.44*pi ; % ------------------------------------------------------------- % set dimensions n = 10 ; p = 2 ; % fix stream of random numbers for reproducability s = randstream('mt19937ar','seed',1 ) ;    % create random stiefel matrix : [ u0 , u1 , delta ] = create_random_stiefel_data(s , n , p , dist ) ; norm_u0_u1 = norm(u0 - u1,2 ) % compute the stiefel logarithm tic ; [ delta_rec , k , conv_hist1 , norm_logv01 ] = ...                             stiefel_log_supp(u0 , u1 , 1.0e-13 ) ; toc ; norm_recon11 = norm(delta_rec - delta ) % -------------------------------------------------------------    % ------------------------------------------------------------- % reset dimensions n = 1000 ; p = 200 ; % create random stiefel matrix : [ u0 , u1 , delta ] = create_random_stiefel_data(s , n , p , dist ) ; norm_u0_u1 = norm(u0 - u1,2 ) % compute the stiefel logarithm tic ; [ delta_rec , k , conv_hist2 , norm_logv02 ] = ...                             stiefel_log_supp(u0 , u1 , 1.0e-13 ) ; toc ; norm_recon12 = norm(delta_rec - delta ) % -------------------------------------------------------------    % ------------------------------------------------------------- % reset dimensions n = 1000 ; p = 900 ; % create random stiefel matrix : [ u0 , u1 , delta ] = create_random_stiefel_data(s , n , p , dist ) ; norm_u0_u1 = norm(u0 - u1,2 ) % compute the stiefel logarithm tic ; [ delta_rec , k , conv_hist3 , norm_logv03 ] = ...                             stiefel_log_supp(u0 , u1 , 1.0e-13 ) ; toc ; norm_recon13 = norm(delta_rec - delta ) % -------------------------------------------------------------    % ------------------------------------------------------------- % reset dimensions",
    "n = 100000 ; p = 500 ; % create random stiefel matrix : [ u0 , u1 , delta ] = create_random_stiefel_data(s , n , p , dist ) ; norm_u0_u1 = norm(u0 - u1,2 ) % compute the stiefel logarithm tic ; [ delta_rec , k , conv_hist4 , norm_logv04 ] = ...                             stiefel_log_supp(u0 , u1 , 1.0e-13 ) ; toc ; norm_recon14 = norm(delta_rec - delta ) % -------------------------------------------------------------     % plot convergence history figure ; subplot(1,2,1 ) ; semilogy(1:length(conv_hist1 ) , conv_hist1 , ' k - s ' , ...",
    "1:length(conv_hist2 ) , conv_hist2 , ' k : * ' , ...           1:length(conv_hist3 ) , conv_hist3 , ' k-.o ' , ...           1:length(conv_hist4 ) , conv_hist4 , ' k -- x ' ) ; legend('st(10,2 ) ' , ' st(1000,200 ) ' , ' st(1000,900 ) ' , ...         ' st(100000,500 ) ' ) % eof : script_stiefel_log_supp52b.m % -------------------------------------------------------------    ....",
    ".... % ------------------------------------------------------------- % script_stiefel_log_supp53.m % @author : ralf zimmermann , imada , sdu odense % ------------------------------------------------------------- clear ; close all ;    % set dimensions n = 100 ; p = 10 ; % fix stream of random numbers for reproducability s = randstream('mt19937ar','seed',1 ) ;    % create random stiefel data [ u0 , u1 , delta ] = create_random_stiefel_data(s , n , p , 1.0 ) ;    % discretize the interval [ 0.1 , 0.9pi ] with resolution res res = 100 ; start = 0.01 ; t = linspace(start , 0.9*pi , res ) ' ;   % * * * * * * * * * * * * * * * * * * * * * * * * * % initialize observations % * * * * * * * * * * * * * * * * * * * * * * * * * % spectral distance u , uk norm_u_uk   = zeros(res,1 ) ; % iterations until convergence iters_convk = zeros(res,1 ) ; % norm log(v0 ) norm_logv0k = zeros(res,1 ) ; % accuracy of the reconstruction norm_delta_delta_rec_k = zeros(res,1 ) ;    for k = 1:res      % ' project ' tdelta onto st(n , p ) via the stiefel exponential      uk = stiefel_exp_supp(u0 , t(k)*delta ) ;      % compute spectral norm      norm_u_uk(k ) = norm(u0-uk,2 ) ;        % execute the stiefel logarithm      disp(['compute log for t= ' , num2str(t(k ) ) ] ) ;      [ delta_rec , iters_conv , conv_hist , norm_logv0 ] = ...          stiefel_log_supp(u0 , uk , 1.0e-13 ) ;           % store data      iters_convk(k ) = iters_conv ;      norm_logv0k(k ) = norm_logv0 ;      norm_delta_delta_rec_k(k ) = norm(t(k)*delta - delta_rec , 2 ) ; end    % visualize results figure ; subplot(1,3,1 ) ; plot(t , iters_convk , ' k- ' ) ; legend('iters until convergence ' ) ; hold on   subplot(1,3,2 ) ; plot(t , norm_u_uk , ' k- ' ) ; legend('norm(u_0-u_k ) ' ) ; hold on   subplot(1,3,3 ) ; plot(t , norm_logv0k , ' k- ' ) ; legend('norm(log_m(v_0 ) ) ' ) ;    figure ;",
    "plot(t , norm_delta_delta_rec_k ) ; legend('reconstruction error ' ) ; % eof : script_stiefel_log_supp53.m % -------------------------------------------------------------    ....",
    "[ [ stiefel - exponential ] ] stiefel exponential + + + + + + + + + + + + + + + + + + +    .... % ------------------------------------------------------------- % file : stiefel_exp_supp.m % @author : ralf zimmermann , imada , sdu odense % ------------------------------------------------------------- function [ u1 ] = stiefel_exp_supp(u0 , delta ) % ------------------------------------------------------------- % input arguments        %    u0     : base point on st(n , p ) %    delta : tangent vector in t_u0 st(n , p ) % output arguments %    u1     : exp^{st}_u0(delta ) ,   % ------------------------------------------------------------- % get dimensions [ n , p ] = size(u0 ) ; a = u0'*delta ;                           % horizontal component k = delta - u0*a ;                              % normal component [ qe , re ] = qr(k , 0 ) ;                    % qr of normal component % matrix exponential mne = expm([[a , -re'];[re , zeros(p ) ] ] ) ; u1 = [ u0 , qe]*mne(:,1:p ) ; return ; end % eof : stiefel_exp_supp.m % -------------------------------------------------------------    ....      .... % ------------------------------------------------------------- % file : create_random_stiefel_data.m % @author : ralf zimmermann , imada , sdu odense % ------------------------------------------------------------- function [ u0 , u1 , delta ] = ...      create_random_stiefel_data(s , n , p , dist ) % ------------------------------------------------------------- % create a random data set   % u0 , u1 on st(n , p ) , %   delta on t_u st(n , p ) with canonical norm ' dist ' , % which is also the riemannian distance dist(u0,u1 ) % % input arguments %      s = random stream ( for reproducability ) % ( n , p ) = dimension of the stiefel matrices % dist   = riemannian distance between the points u0,u1 %          that are to be created % ------------------------------------------------------------- % create random stiefel matrix : x = rand(s , n , p ) ; [ u0,~ ] = qr(x , 0 ) ; % create random tangent vector in t_u0 st(n , p ) a = rand(s , p , p ) ; a = a - a ' ;             % random p - by - p skew symmetric matrix t = rand(s , n , p ) ; delta = u0*a + t - u0*(u0'*t ) ;   % normalize delta w.r.t . the canonical metric norm_delta = sqrt(trace(delta'*delta ) - 0.5*trace(a'*a ) ) ; delta = ( dist / norm_delta)*delta ; % ' project ' delta onto st(n , p ) via the stiefel exponential u1 = stiefel_exp_supp(u0 , delta ) ; return ; end % eof : create_random_stiefel_data.m % -------------------------------------------------------------    ....      , _ riemannian geometry of grassmann manifolds with a view on algorithmic computation _ , acta applicandae mathematica , 80 ( 2004 ) , pp .  199220 , , _ optimization algorithms on matrix manifolds _ , princeton university press , princeton , new jersey , 2008 , , _ online identification and tracking of subspaces from highly incomplete information _ , in proceedings of allerton , september 2010 .",
    ", _ affine invariance revisited _ , 2012 ieee conference on computer vision and pattern recognition , 2 ( 2006 ) , pp .  20872094 , , _ a survey of projection - based model reduction methods for parametric dynamical systems _ , siam review , 57 ( 2015 ) , pp .  483531 , , _ the geometry of algorithms with orthogonality constraints _ ,",
    "siam journal on matrix analysis and application , 20 ( 1999 ) , pp .  303353 , , _ efficient algorithms for inferences on grassmann manifolds _ , in statistical signal processing , 2003 ieee workshop on , 2003 , pp .",
    "315318 , , _ the formal power series for @xmath486 _ , duke math . j. , 23 ( 1956 ) , pp .  1321 , , _ matrix computations _ , the john hopkins university press , baltimore  london , 3  ed . , 1996 .                  ,",
    "_ algorithms for data fitting on some common homogeneous spaces _ , phd thesis , universit catholique de louvain , louvain , belgium , july 2013/2015 , _ lie groups : an introduction through linear groups _",
    ", oxford graduate texts in mathematics , oxford university press , 2006 , , _ convergence proof for goldberg s exponential series _ , linear algebra and its applications , 121 ( 1989 ) , pp .  37 ."
  ],
  "abstract_text": [
    "<S> we derive a numerical algorithm for evaluating the riemannian logarithm on the stiefel manifold with respect to the canonical metric . </S>",
    "<S> in contrast to the existing optimization - based approach , we work from a purely matrix - algebraic perspective . </S>",
    "<S> moreover , we prove that the algorithm converges locally and exhibits a linear rate of convergence .    </S>",
    "<S> stiefel manifold , riemannian logarithm , riemannian exponential , dynkin series , goldberg series , baker - campbell - hausdorff series    15a16 , 15b10 , 15b57 , 33b30 , 33f05 , 53 - 04 , 65f60 </S>"
  ]
}