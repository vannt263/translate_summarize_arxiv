{
  "article_text": [
    "in the last decade , statistical inference for stochastic delay differential equations ( sddes ) has been studied from various viewpoints . early work on maximum likelihood estimation was done by kchler and mensch  @xcite .",
    "gushchin and kchler  @xcite and kchler and kutoyants  @xcite determined the non - standard asymptotic properties of the maximum likelihood estimator for sddes , and kchler and vasiljev  @xcite constructed sequential procedures with a given accuracy in the @xmath0 sense .",
    "nonparametric estimators for affine sddes were investigated by rei  @xcite and rei  @xcite .",
    "all these studies were concerned with continuous observation of the solution process .",
    "as opposed to the situation for ordinary stochastic differential equations , observations at discrete time points have been little studied for sddes .",
    "rei  @xcite studied nonparametric estimation .",
    "kchler and srensen  @xcite proposed a simple estimator for the parameters @xmath1 in the particular type of sdde given by ( [ affinn ] ) below .",
    "this estimator is biased , however , and can only be expected to work well for high - frequency observations . in this paper",
    "we report a first attempt at investigating parametric inference for affine stochastic delay equations of the general type ( [ geneqn ] ) observed at discrete time points .",
    "we propose a pseudo - likelihood function and study it in the framework of prediction - based estimating functions . applying the methods proposed here in practice",
    "often requires the ability to simulate solutions of sddes .",
    "this problem has been studied by , among other , kchler and platen @xcite and buckwar  @xcite .",
    "a practical application of one of the simplest sddes , discussed in example  [ affinex2 ] below , was provided by kchler and platen @xcite .",
    "we consider the model given by the stochastic differential equation @xmath2 where @xmath3 is a measure on @xmath4 $ ] ( @xmath5 ) such that ( [ geneqn ] ) has a unique stationary solution ( for a suitable given initial condition ) .",
    "conditions under which ( [ geneqn ] ) has a unique stationary solution were given by gushchin and kchler  @xcite . by theorem 3.1 in this paper ,",
    "the stationary solution is a gaussian process .",
    "we assume that the measure @xmath3 depends on a parameter @xmath6 .",
    "the parameter about which inference is to be drawn is @xmath7 or @xmath8 , ( @xmath9 ) . as usual , we denote the parameter space by @xmath10 .",
    "the process @xmath11 is a wiener process .",
    "the initial condition is that the distribution of @xmath12 \\}$ ] is the stationary distribution , which always has expectation 0 .",
    "the data are observations at discrete time points : @xmath13 .",
    "an interesting particular case of ( [ geneqn ] ) is @xmath14 here the measure @xmath3 is concentrated in the discrete points @xmath15 , ( @xmath16 ) .",
    "the vector @xmath17 may be among the parameters to be estimated .",
    "the particular case where @xmath18 and @xmath19 is considered in detail in example  [ affinex2 ] .    in section  [ sec2 ]",
    "we discuss how to calculate the likelihood function for discrete time observations , and propose a pseudo - likelihood function that closely approximates the likelihood function and is considerably easier to calculate .",
    "we consider two examples in detail . in section [ sec3 ]",
    "we present prediction - based estimating functions for affine stochastic delay equations , find the optimal estimating function in this class , and show that the pseudo - likelihood estimator is a particular case of a prediction - based estimator .",
    "the prediction - based estimating functions provide a good framework for discussing the asymptotics of the pseudo - likelihood estimator and in particular the efficiency loss compared with the optimal prediction - based estimating function .",
    "we do this in section  [ sec4 ] , specifying conditions ensuring consistency and asymptotic normality .",
    "finally , in section  [ simstudy ] we present a simulation study of properties of the pseudo - likelihood estimator .",
    "because the stationary solution to ( [ geneqn ] ) is a zero - mean gaussian process , gushchin and kchler  @xcite , the data are in fact a gaussian time series with expectation 0 .",
    "therefore , in principle the likelihood function can be calculated if we can determine , analytically or numerically , the autocovariances @xmath20 the autocovariance function , @xmath21 , satisfies the differential equation @xmath22 with @xmath23 , provided that we define @xmath24 for @xmath25 ( see gushchin and kchler  @xcite ) .",
    "the condition @xmath26 also can be written in the form @xmath27 equation ( [ yw ] ) is a continuous - time analogue of the yule  walker equation known from time - series analysis , and hereinafter we refer to ( [ yw ] ) as the delay yule  walker equation of ( [ genk ] ) . in general , this equation must be solved numerically , but we consider two particular examples where it can be solved explicitly .    to calculate the _ likelihood function _ , define for every @xmath28 the @xmath29-dimensional vector @xmath30 , and the @xmath31-matrix @xmath32 . here and",
    "later @xmath33 denotes transposition of vectors and matrices .",
    "the matrix @xmath34 is the covariance matrix of the vector of the first @xmath29 observations @xmath35 .    the conditional distribution of the observation @xmath36 given the previous observations @xmath37 is the gaussian distribution with expectation @xmath38 and variance @xmath39 , where @xmath40 is the @xmath41-dimensional vector given by @xmath42 , @xmath43 , and @xmath44 , @xmath45 .",
    "the vector @xmath46 and the conditional variance @xmath39 can be found using the durbin ",
    "levinson algorithm ( see , e.g. , page 169 in brockwell and davis @xcite ) .",
    "specifically , @xmath47 and @xmath48 , whereas @xmath49 and @xmath50    the likelihood function based on the data @xmath51 is @xmath52 \\\\[-8pt ] & & { } \\times\\prod_{i=1}^{n-1 } \\biggl [ \\frac{1}{\\sqrt{2 \\uppi v_i(\\theta ) } } \\exp\\biggl ( - \\frac{1}{2 v_i(\\theta ) } \\bigl ( x\\bigl((i+1 ) \\delta\\bigr ) - \\phi_i(\\theta)^t x_{i:1 } \\bigr)^2 \\biggr ) \\biggr ] .",
    "\\nonumber\\end{aligned}\\ ] ] calculation of this function quickly becomes very time - consuming as the sample size @xmath53 increases .",
    "in particular , @xmath40 and @xmath39 must be calculated for every observation time point .",
    "however , the autocovariances @xmath54 decrease exponentially with @xmath41 ( see diekmann et  al .",
    "@xcite , page 34 ) . using the durbin  levinson algorithm ,",
    "it is readily apparent that this implies that the quantities @xmath55 decrease exponentially with @xmath56 . thus the conditional distribution of @xmath36 given @xmath37 depends only very little on observations in the distant past .    therefore , we propose using instead a _ pseudo - likelihood function _ obtained by replacing in the likelihood function the conditional density of @xmath36 given @xmath37 with the conditional density of @xmath36 given @xmath57 , where @xmath58 typically is relatively small .",
    "this pseudo - likelihood function was proposed by h. srensen  @xcite in connection with stochastic volatility models , but the idea is widely applicable .",
    "the pseudo - likelihood is given by @xmath59.\\ ] ] we have not included the density of @xmath60 .",
    "note that the computational gain is large because we calculate ( [ pslik ] ) using the same values of @xmath61 and @xmath62 for all observation time points .",
    "thus , these quantities must be calculated only once for every value of @xmath63 .",
    "we call the number @xmath58 the _ depth _ of the pseudo - likelihood function .",
    "we consider the influence of @xmath58 on the quality of the estimators in the simulation study reported in section  [ simstudy ] . as would be expected , the quality increases with increasing depth . for the model considered in section  [ simstudy ]",
    ", the present study indicates that the bias and the variance of the estimators do not depend much on the depth when @xmath58 is larger than 35 times @xmath64 .",
    "[ affinex2 ] consider the equation @xmath65\\,\\mathrm{d}t + \\sigma \\,\\mathrm{d}w(t),\\ ] ] where @xmath66 , @xmath67 .",
    "this is a particular case of the model ( [ affinn ] ) .",
    "the real parameters @xmath68 and @xmath69 are chosen such that a stationary solution of ( [ affin ] ) exists .",
    "this is the case exactly when @xmath70 and @xmath71 if @xmath72 , and when @xmath73 if @xmath74 . here",
    "the function @xmath75 is the root of @xmath76 if @xmath77 , and @xmath78 .",
    "the stationary solution is unique if it exists .",
    "details of this have been provided by kchler and mensch @xcite , who explicitly found the covariance function of the stationary solution by solving the yule  walker delay differential equation ( [ yw ] ) , @xmath79 they found that @xmath80 } & \\quad $ \\mbox{when } |b| < - a$,\\vspace*{+2pt}\\cr \\displaystyle\\sigma^2 ( b r -1)/(4 b ) & \\quad $ \\mbox{when } b = a$,\\vspace*{+2pt}\\cr \\displaystyle\\frac { \\sigma^2 ( b \\sin(\\lambda(a , b)r ) - \\lambda(a , b ) ) } { 2 \\lambda(a , b)[a + b \\cos(\\lambda(a , b)r ) ] } & \\quad$\\mbox{when } b < - |a|$,}\\ ] ] where @xmath81 , and that for @xmath82 $ ] the covariance function is @xmath83 because @xmath21 is known in @xmath84 $ ] , the yule ",
    "walker equation becomes an ordinary differential equation for @xmath21 in @xmath85 $ ] , which can be easily solved .",
    "similarly , for @xmath86 , the autocovariance function @xmath21 is given by @xmath87 , n \\in\\n.\\ ] ] thus @xmath21 can be determined iteratively in each of the intervals @xmath88 $ ] , @xmath89 .",
    "note that the covariance function depends on @xmath90 and @xmath64 in a simple and smooth way , so that these parameters also can be estimated by maximizing the pseudo - likelihood function ( [ pslik ] ) .    for @xmath91 ,",
    "the model ( [ affin ] ) is the ornstein  uhlenbeck process , for which ( [ affink0 ] ) and ( [ affinkt0 ] ) simplifies to the well - known result @xmath92 @xmath93 in the stationary case @xmath94 . for @xmath74",
    ", we obtain the model @xmath95 this process is stationary if and only if @xmath96 , and in this case , by ( [ affink0 ] ) and ( [ affinkt0 ] ) , the autocovariance function is given by @xmath97 when @xmath82 $ ] . by ( [ k2r ] ) , we find that @xmath98\\end{aligned}\\ ] ] for @xmath99 $ ] .",
    "[ affinex4 ] consider the equation @xmath100 where @xmath66 , @xmath67 .",
    "the set of values of the parameters @xmath68 and @xmath69 for which a unique stationary solution of ( [ exp ] ) exists was studied by rei  @xcite .",
    "this set is rather complicated and irregular ; for instance , it is not convex .",
    "however , it contains the region @xmath101 . for @xmath102 ,",
    "corresponding to a uniform delay measure , a stationary solution exists exactly when @xmath103 .",
    "when @xmath104 , the situation is much simpler . in that case",
    ", a stationary solution exists for all @xmath105 and @xmath106.=-1    when @xmath74 ( and @xmath64 is finite ) , @xmath107 for @xmath105 , an explicit expression for @xmath21 involving trigonometric functions exists as well ( see rei  @xcite , page 41 ) , but it is somewhat complicated , and thus we omit it here .",
    "in this section , we discuss the pseudo - likelihood estimator in the framework of prediction - based estimating functions .",
    "this class of estimating functions was introduced by srensen  @xcite as a generalization of the martingale estimating functions that is also applicable to non - markovian processes such as solutions to stochastic delay differential equations .",
    "applications of the methodology to observations of integrated diffusion processes and sums of diffusions have been described by ditlevsen and srensen  @xcite and forman and srensen  @xcite .",
    "an up - to - date review of the theory of prediction - based estimating functions has been provided by srensen  @xcite .",
    "we show that the pseudo - likelihood estimator is a prediction - based estimator , and find the optimal prediction - based estimating function , which turns out to be different from the pseudo - score function .",
    "optimality is in the sense of godambe and heyde @xcite ( see heyde  @xcite ) .",
    "we impose the following condition that is satisfied for the models considered in examples  [ affinex2 ] and  [ affinex4 ] .",
    "[ cona0 ] the function @xmath21 is continuously differentiable with respect to @xmath108 .    under this assumption , we find the following expression for the pseudo - score function : @xmath109 & = & \\sum_{i = k}^{n-1 } \\frac{\\partial_\\theta\\phi_k(\\theta)^t x_{i : i+1-k}}{v_k(\\theta ) } \\bigl ( x\\bigl((i+1 ) \\delta\\bigr ) -",
    "\\phi_k(\\theta)^t x_{i : i+1-k } \\bigr ) \\\\[-2pt ] & & { } + \\frac{\\partial_\\theta v_k(\\theta)}{2 v_k(\\theta)^2 } \\sum_{i = k}^{n-1 } \\bigl [ \\bigl ( x\\bigl((i+1 ) \\delta\\bigr ) - \\phi_k(\\theta)^t x_{i : i+1-k } \\bigr)^2 - v_k(\\theta)\\bigr ] .",
    "\\nonumber\\vadjust{\\goodbreak}\\end{aligned}\\ ] ] the derivatives @xmath110 and @xmath111 exist when @xmath21 is differentiable and can be found by the following algorithm , which is obtained by differentiating the durbin ",
    "levinson algorithm : @xmath112 v_{i-1}(\\theta)^{-2},\\\\ % % \\pmatrix{\\partial_{\\theta_j } \\phi_{i,1}(\\theta)\\cr \\vdots\\cr \\partial_{\\theta_j } \\phi_{i , i-1}(\\theta ) } & = & \\pmatrix{\\partial_{\\theta_j } \\phi_{i-1,1}(\\theta)\\cr \\vdots\\cr \\partial_{\\theta_j } \\phi_{i-1,i-1}(\\theta)}- \\partial_{\\theta_j } \\phi_{i , i}(\\theta)\\pmatrix{\\phi_{i-1,i-1}(\\theta ) \\cr \\vdots\\cr \\phi_{i-1,1}(\\theta)}\\\\ & & { } - \\phi_{i , i}(\\theta)\\pmatrix{\\partial_{\\theta_j } \\phi_{i-1,i-1}(\\theta ) \\cr \\vdots\\cr \\partial_{\\theta_j } \\phi_{i-1,1}(\\theta)}\\end{aligned}\\ ] ] for @xmath113 , and @xmath114    the minimum mean squared error linear predictors of @xmath36 and @xmath115 given @xmath116 are @xmath117 and @xmath62 , respectively .",
    "this is because for the gaussian processes considered in this paper , the two conditional expectations are linear in @xmath116 .",
    "thus the pseudo - score function is a prediction - based estimating function as defined in srensen  @xcite , where estimating functions of a slightly more general type than in the original paper ( srensen  @xcite ) are treated .",
    "the generalization allows the predicted function to depend both on the parameter and on the previous observations .",
    "exploring the relation of the pseudo - score function to the optimal estimating function based on these predictors is of interest .",
    "we start by defining a class of prediction - based estimating functions .",
    "define the @xmath118-matrices @xmath119 and the @xmath120-dimensional vectors @xmath121 then the class of prediction - based estimating functions to which ( [ psscore ] ) belongs is given by @xmath122 where @xmath123 is a @xmath124 matrix of weights that can depend on the parameter , but not on the data .",
    "the pseudo - score function ( [ psscore ] ) is obtained if the weight matrix @xmath123 is chosen as @xmath125 within the class of estimators obtained by solving the estimating equation @xmath126 for some choice of @xmath123 , the estimator with the smallest asymptotic variance is obtained by choosing the optimal weight matrix @xmath127 .",
    "the optimal estimating function is the one closest to the true score function in an @xmath128-sense ( for details , see heyde  @xcite ) .",
    "we now can find the optimal weight matrix @xmath127 .",
    "the covariance matrix of the @xmath120-dimensional random vector @xmath129 is @xmath130 where @xmath131\\vspace*{-2pt}\\ ] ] and @xmath132 with @xmath133 denoting here and later the @xmath134-matrix of 0s , and with @xmath135 denoting the covariance matrix of @xmath136 defined in section  [ sec2 ] .",
    "we have used @xmath137 , which is a general property of prediction - based estimating functions ( see srensen  @xcite ) . in this particular case , this is easily seen by finding the conditional expectation of @xmath138 given @xmath116 , which is  0 .    to find the optimal estimating function , we also need the @xmath124 _ sensitivity - matrix _ @xmath139 , given by @xmath140 for the derivation of @xmath141 and @xmath139 , we use that the model is gaussian and , in particular , we use that @xmath142 is the conditional expectation of @xmath143 and not just the minimum mean squared linear predictor as in the general theory of prediction - based estimating functions .",
    "the optimal weight matrix is given by @xmath144 see srensen  @xcite .",
    "the class of estimating functions considered above is not the full class of prediction - based estimating functions to which ( [ psscore ] ) belongs , as defined by srensen  @xcite and srensen  @xcite .",
    "the full class is obtained by replacing in ( [ g ] ) @xmath123 with a @xmath145 matrix and @xmath138 with the @xmath146-dimensional vectors @xmath147 obtained when @xmath148 is replaced by the @xmath149 matrix , @xmath150 in the definition of @xmath138 . in this way ,",
    "@xmath138 is extended by @xmath151 extra coordinates .",
    "because the moments of an odd order of a centered multivariate gaussian distribution equal 0 , we see that the extra @xmath151 coordinates of @xmath147 have expectation 0 under the true probability measure irrespective of the value of the parameter @xmath108 ; therefore , they can not be expected to be a useful addition to @xmath138 .",
    "however , the extra coordinates might be correlated with the coordinates of @xmath138 , and thus might be used to reduce the variance of the estimating function . to see that this is not the case , the optimal estimating function based on @xmath147 can be calculated .",
    "the covariance matrix of the random vector @xmath152 can be shown to be a block - diagonal matrix with two @xmath153-blocks , the first of which equals @xmath154 .",
    "this follows from the fact that moments of an odd order of a centered multivariate gaussian distribution equal 0 .",
    "moreover , the sensitivity matrix corresponding to @xmath147 is @xmath155 therefore , the optimal weight matrix is @xmath156 and thus the optimal prediction - based estimating function obtained from @xmath147 equals the optimal estimating function obtained from @xmath138 .",
    "it is therefore sufficient to consider the aforementioned smaller class of prediction - based estimating functions , which we do in the rest of the paper .",
    "the pseudo - score function , @xmath157 , is not equal to the optimal prediction - based estimating function .",
    "in fact , @xmath158 the magnitude of the difference between the two estimating functions depends on how small the entries of @xmath159 are relative to the entries of @xmath141 . because correlations decrease exponentially with the distance in time ( see rei  @xcite , page 26 ) , the terms in the sum defining @xmath159 can be small compared with the entries of @xmath141 ; however , under what conditions this occurs and exactly how small the terms are depend on @xmath108 , @xmath160 , and @xmath58 .    in the next section we show that the limit @xmath161\\ ] ] exists .",
    "therefore , we can define the following weight matrix , which does not depend on  @xmath53 : @xmath162 where @xmath163 the estimating function @xmath164 is asymptotically optimal and theoretically is easier to handle than @xmath165 . in practice ,",
    "the optimal weight matrices @xmath166 or @xmath127 usually must be calculated by simulation .",
    "the amount of computation can be reduced by using the approximation to @xmath167 obtained by replacing @xmath127 or @xmath166 with the matrix obtained from ( [ astar ] ) and ( [ mb ] ) when @xmath168 is replaced by a suitably truncated version of the series in ( [ m2 ] ) .",
    "this does not make much difference , because the terms in the sum ( [ m2 ] ) decrease exponentially fast .",
    "in this section , we present the asymptotic properties of estimators obtained by solving the estimating equation @xmath169 , where @xmath170 is given by ( [ g ] ) .",
    "important particular cases of this are the maximum pseudo - likelihood estimator obtained by maximizing ( [ pslik ] ) and the optimal prediction - based estimator obtained by solving @xmath171 with @xmath172 given by ( [ gstar ] ) .",
    "the depth , @xmath58 , of @xmath170 is assumed fixed .",
    "the asymptotic properties are proven for a solution to the general equation ( [ geneqn ] ) under the following assumption :    [ conaa ]    the functions @xmath21 and @xmath123 are twice continuously differentiable with respect to  @xmath108 .",
    "the @xmath124 matrix @xmath173 has rank @xmath174 ( in particular , @xmath175 ) .",
    "@xmath176 if and only if @xmath177 .    here @xmath178 and @xmath179    if @xmath180 equals @xmath181 ( corresponding to the pseudo - score function ) or @xmath180 equals @xmath182 ( corresponding to the optimal prediction - based estimating function ) , then condition [ conaa](a ) is satisfied if @xmath21 is three times continuously differentiable , which is the case for the models considered in examples [ affinex2 ] and  [ affinex4 ] .",
    "condition  [ conaa](a ) ensures that the functions @xmath61 and @xmath62 are continuously differentiable .",
    "condition  [ conaa](c ) is an identifiability condition that ensures eventual uniqueness of the estimator .",
    "[ tha1 ] assume that the true parameter value @xmath183 belongs to the interior of the parameter space @xmath184 .",
    "suppose that condition [ conaa ] is satisfied , and that the matrix @xmath185 has full rank @xmath174",
    ". then a consistent estimator @xmath186 that solves the estimating equation @xmath187 exists and is unique in any compact subset of @xmath184 containing @xmath183 with a probability tending to 1 as @xmath188 . moreover , @xmath189 as @xmath188 , where @xmath190 with @xmath191 given by ( [ mb ] ) , and @xmath192 here @xmath139 is the sensitivity matrix given by ( [ s ] ) .",
    "note that it follows from ( [ astar0 ] ) and ( [ atilde ] ) that @xmath193 and @xmath194 have rank @xmath174 if @xmath195 has rank @xmath174 , because @xmath196 and @xmath197 are non - singular covariance matrices . that @xmath195 has rank @xmath174 follows from condition  [ conaa](b ) by ( [ s0 ] ) below .",
    "proof of theorem  [ tha1 ] the theorem follows from general asymptotic statistical results for stochastic processes ( see , e.g. , jacod and srensen @xcite ) . we need to establish that a law of large numbers and a central limit theorem hold and to check regularity conditions .    under our general assumption that @xmath198 is stationary , rei @xcite ( page 25 ) showed that @xmath198 is exponentially @xmath199-mixing .",
    "therefore , a law of large numbers holds for sums of the form @xmath200 .",
    "the process @xmath201 is exponentially @xmath6-mixing , and because the process @xmath198 is gaussian , @xmath138 has moments of all orders .",
    "therefore , it follows from theorem 1 in section  1.5 of doukhan  @xcite that ( [ m2 ] ) converges , and that @xmath202 as @xmath188 .",
    "next , we need to check regularity conditions that ensure the asymptotic results .",
    "the estimating function satisfies that @xmath203 .",
    "furthermore , it is obvious from the continuous differentiability of the functions @xmath61 and @xmath62 that the derivatives @xmath204 are locally dominated integrable . finally , the matrix @xmath205 is invertible because @xmath185 has full rank and @xmath206 with @xmath207 given by ( [ kbar ] ) .",
    "the first matrix has full rank by condition  [ conaa](b ) , and @xmath207 is invertible because @xmath208 is the covariance matrix of @xmath209 , which is not degenerate . now",
    "the existence and consistency of @xmath186 , as well as the eventual uniqueness of a consistent estimator on any compact subset of @xmath184 containing @xmath183 , follow ( see jacod and srensen  @xcite ) .",
    "the locally dominated integrability of @xmath210 ( which follows from condition [ conaa](a ) ) implies that @xmath211 converges uniformly to @xmath212 for @xmath108 in a compact set .",
    "the fact that the limit is a continuous functions of @xmath108 and satisfies @xmath213 for @xmath214 implies that any non - consistent solution to the estimating equation will eventually leave any compact subset of @xmath184 containing @xmath183 .",
    "the asymptotic normality follows by standard arguments ( see , e.g. , jacod and srensen  @xcite ) .",
    "a simpler estimator with the same asymptotic distribution as in the estimator from ( [ g ] ) is obtained from the estimating function @xmath215 where @xmath216 is some consistent estimator of @xmath108 , obtained , for instance , by simply using @xmath174 suitably chosen coordinates of @xmath138 . for this estimating function",
    ", the identifiability condition condition  [ conaa](c ) can be replaced by the following condition :    [ conaaa ]    the function @xmath217 is one - to - one .",
    "@xmath218 for all @xmath219 , where @xmath220 is the null space of the matrix @xmath221 .",
    "this readily follows from the fact that the limit of @xmath222 is @xmath223 . in the case of the pseudo - likelihood function",
    ", we have the simple expression @xmath224 condition  [ conaaa](a ) is a basic assumption without which there is no hope of estimating @xmath108 using the pseudo - score function ( [ psscore ] )",
    ". the condition must be checked for individual models .",
    "obviously , it is not always satisfied , as demonstrated by the following examples . consider the model in example  [ affinex2 ] with the restriction that @xmath225 .",
    "for this model , the autocovariance function depends on @xmath64 and @xmath69 only through @xmath226 for @xmath82 $ ] .",
    "in example  [ affinex4 ] with the restriction that @xmath74 , the autocovariance function depends on @xmath64 and @xmath69 only through @xmath227 for @xmath82 $ ] .",
    "theorem  [ tha1 ] implies that the asymptotic distribution of the optimal prediction - based estimator , @xmath228 , is @xmath229 and the asymptotic distribution of the pseudo - likelihood estimator , @xmath230 , is @xmath231 where @xmath232 and @xmath233 the result for @xmath228 follows because @xmath234 and the result for @xmath230 follows because @xmath235 and @xmath236    according to the general theory of estimating functions ( see , e.g. , heyde  @xcite ) , the matrix @xmath237 is positive definite ; that is , the asymptotic covariance matrix of @xmath230 is larger than that of @xmath228 ( in the usual ordering of positive semi - definite matrices ) .",
    "thus the asymptotic variance of @xmath238 is larger than that of @xmath239 for any differentiable function @xmath240 .",
    "if @xmath241 is invertible , then @xmath242^{-1 } = w(\\theta_0 ) - [ b(\\theta_0)^{-1 } + w(\\theta_0)^{-1}]^{-1},\\ ] ] and if @xmath243 is invertible , then @xmath244^{-1}m^{(1)}(\\theta_0)^{-1},\\ ] ] where we have used twice that @xmath245 for a matrix @xmath180 .",
    "thus the difference between the two inverse asymptotic covariance matrices can be expressed as @xmath246^{-1}\\nonumber\\\\ & & \\quad=[b(\\theta_0)^{-1 } + w(\\theta_0)^{-1}]^{-1 } \\nonumber\\\\ & & \\qquad{}- s(\\theta_0)m^{(1)}(\\theta_0)^{-1 } \\bigl[m^{(1)}(\\theta_0)^{-1 } + m^{(2)}(\\theta_0)^{-1}\\bigr]^{-1}m^{(1)}(\\theta_0)^{-1}s(\\theta_0)^t \\\\ & & \\quad=\\bigl [ \\bigl(\\tilde{a}(\\theta_0 ) m^{(1)}(\\theta_0 ) \\tilde{a}(\\theta_0)^t \\bigr)^{-1 } + \\bigl(\\tilde{a}(\\theta_0)m^{(2)}(\\theta_0 ) \\tilde{a}(\\theta_0)^t \\bigr)^{-1 } \\bigr ] ^{-1 } \\nonumber\\\\ & & \\qquad{}- \\tilde{a}(\\theta_0 ) \\bigl[m^{(1)}(\\theta_0)^{-1 } + m^{(2)}(\\theta_0)^{-1 } \\bigr]^{-1 } \\tilde{a}(\\theta_0)^t.\\nonumber\\end{aligned}\\ ] ] it is considerably easier to calculate the pseudo - likelihood function ( [ pslik ] ) than the optimal estimating function ( [ gstar ] ) , because the latter involves derivatives with respect to @xmath108 of the covariance function and higher - order moments of @xmath198 . in particular , in cases where the covariance function is not explicitly known and must be determined by simulation , it is much easier to calculate ( [ pslik ] ) than  ( [ gstar ] ) .",
    "thus the maximum pseudo - likelihood estimator is preferred in practice .",
    "the formula  ( [ loss ] ) then can be used to assess whether the loss of efficiency relative to the optimal estimator is acceptable .    [ lossex ] as an example , we calculated the efficiency loss for the model ( [ affin ] ) in example  [ affinex2 ] in a number of cases . when @xmath58 is sufficiently large , the pseudo - likelihood function is almost efficient , and thus the information loss ( [ loss ] ) is necessarily small . therefore , it is most interesting to calculate the efficiency loss when @xmath58 is small .",
    "we calculate the relative information loss , that is , the information loss ( [ loss ] ) relative to the information for the optimal estimator given by ( [ asyopt ] ) .",
    "the main problem is to calculate the matrix ( [ m2 ] ) .",
    "however , for @xmath247 , a simple expression for each term in the sum ( [ m2 ] ) can be obtained using the formula of isserlis  @xcite , and so a suitably truncated version of the sum can be easily calculated . for @xmath248 , the matrix ( [ m2 ] ) can be determined by simulation using ( [ mb ] ) .",
    "specifically , we determine the covariance matrix @xmath249 , with @xmath53 suitably large , by simulation .",
    "this is computationally more demanding .",
    "we first considered the efficiency loss for the parameter @xmath69 in the case @xmath247 .",
    "the parameters @xmath250 and @xmath64 were fixed at a value of 1 , and @xmath68 was chosen equal to @xmath251 . for @xmath252 ( the value for which the mixing rate is maximal ) , the relative information loss was found to be very small , less than 0.1 percent for @xmath253 , @xmath254 , and @xmath255 .",
    "next , we calculated the information loss for a number of values of @xmath69 with @xmath255 .",
    "for @xmath256 , and @xmath257 , the mixing rate is relatively high , and the information loss is less than 0.1 percent . for @xmath258 , and @xmath259 , the information loss is between 0.1 and 1 percent , whereas for @xmath260 , and @xmath261 , it is 1.4 percent , 3.0 percent , and 9.8 percent , respectively .    finally , we calculated the relative the information loss for @xmath262 and @xmath263 .",
    "in this case , information loss was calculated for both @xmath68 and @xmath69 .",
    "the parameters @xmath68 , @xmath250 , and @xmath64 had the same value as before , and @xmath255 . for @xmath264 , the relative information loss for both @xmath68 and @xmath69 is less than 0.1 percent for both values of @xmath58 . for @xmath265 the information loss is less than 0.1 percent for @xmath69 and 0.2 percent for @xmath68 .    in most cases ,",
    "the relative information loss is so tiny that in practice it is preferable to use the maximum pseudo - likelihood estimator .",
    "the information loss increases as the mixing rate decreases . only for @xmath247 and @xmath266",
    "is the information loss large enough to justify the use of the more complicated optimal estimator .",
    "in this section we report the results of a simulation study in which we investigated some properties of the pseudo - likelihood estimator introduced in section [ sec2 ] . we restrict ourselves to the model considered in example [ affinex2 ] and to estimating @xmath267 .",
    "the delay time @xmath64 is chosen equal to 1 , and @xmath250 is fixed at 1 .",
    "this study was not intended to serve as a complete simulation study ; rather , the intention was to illustrate some properties of the estimator and give a first impression of how the joint distribution of the two - dimensional estimator @xmath268 depends on the time between observations @xmath160 , the depth @xmath58 of the pseudo - likelihood function , and the true parameter value .",
    "we performed simulations for three values of @xmath108 : @xmath269 near the upper boundary of the domain of stationarity , @xmath270 which is the parameter value with the highest possible mixing rate for the stationary solution @xmath198 when @xmath271 , and @xmath272 near the lower boundary of the domain of stationarity . for each parameter value",
    ", we considered four sampling frequencies with the same number of observation time points ( 200 ) ; specifically , the observation time points were @xmath273 , @xmath274 , with @xmath275 .",
    "the simulations of the sdde were conducted with a step size of 0.001 . in all cases , 1000 data sets were simulated , and thus 1000 estimates were generated . for each data",
    "set , a new trajectory of the driving wiener process was generated .",
    "the full simulation study is reported in kchler and srensen  @xcite .",
    "table  [ table4b ] reports the mean values and standard deviations of the simulated estimates of @xmath68 and @xmath69 for @xmath276 . for @xmath277 , the estimators are more biased and have a larger standard deviation for small values of @xmath160 and @xmath58 , whereas for @xmath278",
    ", the bias is small and the standard deviations are comparable in all cases . for @xmath277 , the estimators of @xmath68 and @xmath69 are highly correlated , whereas this is the case only for small values of @xmath160 and @xmath58 for the other parameter values .",
    "@d1.2d4.0d2.2 d2.2d2.2d2.2d2.2d2.2d2.2@ & & + [ -5pt ] & & + & & & & & & & & + 0.05 & 4000 & -1.73 & -1.07 & -1.04 & -1.02 & -1.02 & -1.01 & -1.01 + & & 2.32 & 0.21 & 0.14 & 0.11 & 0.11 & 0.10 & 0.09 + 0.1 & 2000 & -1.27 & -1.03 & -1.03 & -1.01 & -1.01 & -1.01 & -1.01 + & & 0.83 & 0.13 & 0.10 & 0.09 & 0.09 & 0.09 & 0.09 + [ 3pt ] 0.5 & 400 & -1.04 & -1.01 & -1.01 & -1.01 & -1.01 & -1.01 & -1.01 + & & 0.14 & 0.10 & 0.09 & 0.09 & 0.10 & 0.09 & 0.10 + [ 3pt ] 1.0 & 200 & -1.02 & -1.01 & -1.01 & -1.02 & -1.02 & -1.01 & -1.02 + & & 0.12 & 0.11 & 0.11 & 0.11 & 0.11 & 0.11 & 0.12 + 2.0 & 100 & -1.10 & -1.04 & -1.03 & -1.03 & -1.04 & -1.04 & -1.04 + & & 0.43 & 0.21 & 0.19 & 0.18 & 0.21 & 0.19 & 0.17 + 0.05 & 4000 & 0.42 & -0.09 & -0.11 & -0.15 & -0.13 & -0.14 & -0.14 + & & 2.66 & 0.44 & 0.31 & 0.23 & 0.19 & 0.14 & 0.09 + 0.1 & 2000 & 0.08 & -0.13 & -0.14 & -0.14 & -0.14 & -0.13 & -0.13 + & & 1.14 & 0.27 & 0.18 & 0.13 & 0.11 & 0.09 & 0.09 + 0.5 & 400 & -0.12 & -0.14 & -0.14 & -0.13 & -0.14 & -0.14 & -0.14 + & & 0.28 & 0.10 & 0.11 & 0.11 & 0.11 & 0.11 & 0.10 + 1.0 & 200 & -0.14 & -0.14 & -0.14 & -0.13 & -0.14 & -0.13 & -0.13 + & & 0.16 & 0.13 & 0.13 & 0.13 & 0.13 & 0.13 & 0.14 + 2.0 & 100 & -0.26 & -0.15 & -0.14 & -0.13 & -0.15 & -0.14 & -0.14 + & & 0.65 & 0.29 & 0.27 & 0.25 & 0.30 & 0.27 & 0.24 +    the most remarkable observations from our simulation study can be summarized as follows :    * for a fixed number of observation time points , the bias and standard deviation of the estimators worsen as the time between observations @xmath160 decreases , at least when @xmath279 . for @xmath280 , the quality does not change much with @xmath160 , and whether the bias and variance increase or decrease with @xmath160 depends on the parameter value . * the smaller the @xmath160 value , the more the choice of the depth @xmath58 of the pseudo - likelihood functions influences the quality of the estimators when @xmath281 . for @xmath280 , the importance of @xmath58 increases again for some parameter values .",
    "* it is surprising that a similar pattern is seen when the length of the observation interval @xmath282 is fixed so that the sample size decreases as @xmath160 increases .",
    "however , here there is a clearer tendency for the estimators to deteriorate when @xmath280 , so that there is an optimal value of  @xmath160 , which seems to be around @xmath64 .",
    "* the absolute value of the correlation between @xmath283 and @xmath284 decreases with increasing depth @xmath58 to a limit , which is strongly dependent on the true parameter value . near the upper boundary of the stability region ,",
    "the estimators are highly correlated .",
    "a high absolute value of the correlation indicates that it is difficult to distinguish between the effects of the lagged term and the nonlagged term in the drift ; thus , it is not surprising that the absolute correlation is large when the depth is small . * for small values of the depth @xmath58",
    ", the joint distribution of the estimators of @xmath68 and @xmath69 can deviate from a two - dimensional normal distribution by having crescent - shaped contours .",
    "we thank adrian m. kmmler for assisting with the efficiency loss calculations , katja krol for writing the pc program used in the simulation study , and daniel skodlerack for providing an earlier version of that program .",
    "we also thank the referees , the associate editor , and the editor for their constructive criticism that led to considerable improvement of the manuscript .",
    "the research of michael srensen was supported by the danish center for accounting and finance , funded by the danish social science research council ; by the center for research in econometric analysis of time series , funded by the danish national research foundation ; and by a grant from the university of copenhagen excellence programme ."
  ],
  "abstract_text": [
    "<S> statistical inference for discrete time observations of an affine stochastic delay differential equation is considered . </S>",
    "<S> the main focus is on maximum pseudo - likelihood estimators , which are easy to calculate in practice . </S>",
    "<S> a more general class of prediction - based estimating functions is investigated as well . </S>",
    "<S> in particular , the optimal prediction - based estimating function and the asymptotic properties of the estimators are derived . </S>",
    "<S> the maximum pseudo - likelihood estimator is a particular case , and an expression is found for the efficiency loss when using the maximum pseudo - likelihood estimator , rather than the computationally more involved optimal prediction - based estimator . </S>",
    "<S> the distribution of the pseudo - likelihood estimator is investigated in a simulation study . </S>",
    "<S> two examples of affine stochastic delay equation are considered in detail . </S>"
  ]
}