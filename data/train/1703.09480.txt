{
  "article_text": [
    "this technical report describes code to generate simulated time series classification ( tsc ) problems and presents an experimental evaluation of ten tsc algorithms on a range of simulation settings .",
    "detailed algorithmic background can be found in  @xcite .",
    "this research is part of our goal to try to determine why some tsc algorithms work better than others on different types of problems with the ultimate aim of having better _ a priori _ estimates of what algorithm will be the most accurate for a new classification problem .",
    "the simulation code is part of an extensive tsc codebase , and is in the package ` statistics.simulators ` .",
    "examples on how to use the simulators are in class ` examples.simulationexperiments ` .",
    "there are two base structures of the simulations code : the ` model ` class and the ` datasimulator ` class . `",
    "datasimulator ` contains a ` model ` for each problem class and is used to generate data .",
    "there are three basic use cases to generate a simulated data set .    1",
    ".   set up models externally then call ` generatedata ` .",
    "+ .... arraylist < model > m = .... datasimulator ds = new datasimulator(m ) ; instances data = ds.generatedata ( ) ; .... 2 .",
    "use a subclass of ` datasimulator ` + .... datasimulator ds = new simulateshapeletdataset ( ) ; instances data = ds.generatedata ( ) ; .... 3 .   use a bespoke static method in a subclass of ` datasimulator",
    "` + ....   data = simulateshapeletdata.generateshapeletdata(serieslength , casesperclass ) ; ....    each subclass of ` datasimulator ` has its own set of parameters , which can be passed as a 2-d array .",
    "otherwise defaults will be used .",
    "the ` datasimulator ` has parameters with the following defaults :    ....      int nosclasses=2 ;      int serieslength=100 ;      int nosperclass=50 ;      int [ ] casesperclass = new int[]{nosperclass , nosperclass } ; ....    these can all be set by modifier methods .",
    "default settings are in method ` examples.simulationexperiments.setstandardglobalparameters `",
    ".    currently implemented model subclasses are ` shapeletmodel , dictionarymodel , armamodel , intervalmodel , elasticmodel ` and ` whitenoisemodel ` .",
    "each simulator is based on placing one or more of the shapes shown in figure  [ shapes ] on a ( usually longer ) white noise series . the location , size , frequency and/or type of shape define the simulator .",
    "there have been many approaches for solving tsc problems proposed in the literature . due to the rich and diverse nature of these solutions",
    ", we believe the best way to understand them is to group algorithms by the nature of the discriminatory features that they use . a more extensive description can be found in  @xcite .",
    "it is possible , and indeed often sensible , to treat tsc problems as standard classification problems , i.e. to not explicitly use the fact the attributes are ordered . as a general principle",
    ", we advocate always starting with a traditional classifier .",
    "this acts as a sanity check and provides evidence that the more complex tsc approaches are in fact of value for a specific problem .",
    "we evaluate using two vector based classifiers : rotation forest ( rotf )  @xcite and a heterogeneous ensemble of standard classifiers ( hesca )  @xcite .",
    "whole series techniques compare two series by a distance measure that uses all data points , then classify with a nearest neighbour ( nn ) approach .",
    "the simplest such approach is to compare series using euclidean distance .",
    "however , this baseline is easily beaten in practice , and most research effort has been directed at finding techniques that can compensate for small misalignments between series using * elastic * distance measures .",
    "the almost universal benchmark for whole series measures is dynamic time warping ( dtw ) .",
    "numerous alternatives have been proposed .",
    "these involve alternative warping criteria  @xcite , using versions of edit distance  @xcite and transforming to use first order differences  @xcite .",
    "we use two elastic distance classifiers .",
    "1-nn dtw with window size set through cross validation ( dtw )  @xcite and the elastic ensemble ( ee )  @xcite , an ensemble of thirteen different 1-nn elastic classifiers .      rather than use the whole series , the interval class of algorithm select one or more phase - dependent intervals of the series . at its simplest ,",
    "this involves a feature selection of a contiguous subset of attributes .",
    "however , the three most effective techniques generate multiple intervals , each of which is processed and forms the basis of a member of an ensemble classifier  @xcite .",
    "we use the time series forest ( tsf )  @xcite to represent interval based classifiers , because in  @xcite it was shown to be not significantly different in accuracy to the alternatives and it is considerably simpler and faster .",
    "shapelet  @xcite approaches are a family of algorithms that focus on finding patterns that define a class and can appear anywhere in the series .",
    "a class is distinguished by the presence or absence of one or more shapelets somewhere in the whole series .",
    "two common ways of finding shapelets are through enumerating the candidate shapelets in the training set  @xcite or searching the space of all possible shapelets with a form of gradient descent  @xcite .",
    "we use the balanced shapelet transform in conjunction with hesca ( st - hesca )  @xcite because in  @xcite it was shown to be significantly more accurate than the alternatives .",
    "some problems are distinguished by the frequency of repetition of subseries , rather than by their presence or absence .",
    "dictionary - based methods form frequency counts of recurring patterns , then build classifiers based on the resulting histograms  @xcite .",
    "we use bag of sfa symbols ( boss )  @xcite to represent dictionary classifiers , because in  @xcite it was shown to be significantly more accurate than the alternatives .",
    "the frequency domain will often contain discriminatory information that is hard to detect in the time domain .",
    "methods include constructing an autoregressive model  @xcite or combinations of autocorrelation , partial autocorrelation and autoregressive features  @xcite .",
    "the random interval spectral ensemble  @xcite is a way of combing spectral features in a decision tree ensemble and is used to represent spectral classifiers .",
    "these distinctions are not absolute , and some classifiers could be assigned to multiple groups .",
    "for example , boss uses a fourier transform , so could be classed as spectral , and rise uses random intervals , so could come in the interval category .",
    "we differentiate between this inevitable fuzziness in algorithm classification and algorithms that explicitly use features that combine two or more of the above approaches to representation into a single classifier .",
    "two examples of this combination approach are presented in  @xcite , where a classifier is built on concatenated different feature spaces , and  @xcite which uses forward selection of features for a linear classifier . however , the most accurate method for combining approaches is the collective of transform based ensembles ( cote )  @xcite which involves transformation into a feature space that represents each group and ensembling classifiers together .",
    "the original version of cote , flat - cote  @xcite , involves ensembling 35 classifiers in the elastic , shapelet and spectral domains into a single ensemble .",
    "more recently , hierarchical cote ( hive - cote )  @xcite adopts a more structured modular approach by encapsulating classifiers on each representation in a single ensemble then hierarchically ensembling .",
    "an experiment consists of a number of repetitions of the process of randomly generating a data set , splitting it into train and test sets , building the classifier on the train ( including all parameter optimisation ) then evaluating once on the train set .",
    "we tabulate the results for the mean accuracy , the standard error in accuracy and the mean rank for the ten algorithms listed in table  [ classifiers ] .. to test the accuracy of multiple classifiers over multiple randomly generated independent datasets , we start with a friedman s test to detect for a significant difference between any classifiers , then identify where significant differences occur by constructing a critical difference diagram as described by @xcite . however , following recommendations in ( @xcite ) and ( @xcite ) , we have abandoned the nemenyi post - hoc test used in ( @xcite ) to form groups of classifiers within which there is no significant difference ( cliques ) .",
    "instead , we compare all classifiers with pairwise wilcoxon signed rank tests , and form cliques using the holm correction , which adjusts family - wise error less conservatively than a bonferonni adjustment .",
    "it is worthwhile pointing out that cliques formed this way do not necessarily reflect the rank order .",
    "for example , if we have three classifiers @xmath0 with average ranks @xmath1 , it is possible for a to be significantly worse than @xmath2 but not significantly worse than @xmath3 in pairwise tests .",
    "this relationship can not easily be displayed on a critical difference diagram .",
    "happily , we did not encounter this phenomena with any of the results we present in this paper .",
    "elastic techniques are based on an elastic distance measure between two series .",
    "the measures are called elastic because they attempt to compensate for misalignments between otherwise similar series .",
    "the elastic simulator models this scenario by using a single shape ( from those shown in figure  [ shapes ] ) to define each class .",
    "variation within the class is achieved by altering the length of the shape for each series .",
    "elastic classifiers should be able to compensate for this stretching , but it may confound other techniques .",
    "for each instance of the class the length of the shape is randomly selected to be between 20% and 100% of the total series length .",
    "example series with low and standard noise are shown in figure  [ elasticex ] .    [",
    "cols=\"^,^ \" , ]      broadly , the results are in line with our expectations .",
    "+ * vector based , elastic , interval , shapelet and dictionary*. the nature of this data would make us believe that all of these approaches would be equally poor . +",
    "* spectral*. rise would , we hope , be significantly more accurate than all other approaches . +",
    "* combined*. if we are correct , the cote classifiers will be combining one good classifier ( rise ) , with several poor ones .",
    "we would hope that it could detect this through cross validation , but suspect both flat - cote an hive - cote may be significantly worse than rise .",
    "this sequence of simulation experiments was primarily conducted to test the hypothesis that hive - cote produces significantly more accurate classifiers .",
    "overall , we think these results support our core hypothesis that hive - cote is the best currently available technique for tsc .",
    "it is significantly more accurate than all competitors on the ucr / uea repository problems ( see  @xcite , significantly more accurate than all classifiers when the data is randomly selected from one of five simulators and significantly better than , or not significantly worse than , the best other approach on three out of five of the individual simulators .",
    "the two it is significantly worse than the best approach are interval ( hesca wins , hive second ) and spectral ( rise wins , hive second ) .",
    "hive - cote represents the state - of - the - art in tsc , and our efforts are now directed towards making it faster .",
    "our secondary goal is to begin the process of better understanding why one classifier is better on certain data types than others , and these experiments have highlighted several issues we think worthy of further investigation .    1 .",
    "why is hesca significantly better than rotf on the simulators , and can we find a general case in classification where the hesca approach is better than standard classifiers ? 2 .",
    "why is ee better than dtw on the interval data , and can we better understand how the components of ee interact ? 3 .",
    "does tsf actually offer any benefit over ee , or is it simply duplicating its performance ?",
    "4 .   if tsf and interval approaches are distinct , what type of data are they best for and is there scope for a better interval approach ? 5 .   what data characteristics are optimal for dictionary based approaches and why does boss do so much better than the alternatives ?"
  ],
  "abstract_text": [
    "<S> there are now a broad range of time series classification ( tsc ) algorithms designed to exploit different representations of the data . </S>",
    "<S> these have been evaluated on a range of problems hosted at the ucr - uea tsc archive ( www.timeseriesclassification.com ) , and there have been extensive comparative studies </S>",
    "<S> . however , our understanding of why one algorithm outperforms another is still anecdotal at best . </S>",
    "<S> this series of experiments is meant to help provide insights into what sort of discriminatory features in the data lead one set of algorithms that exploit a particular representation to be better than other algorithms . </S>",
    "<S> we categorise five different feature spaces exploited by tsc algorithms then design data simulators to generate randomised data from each representation . </S>",
    "<S> we describe what results we expected from each class of algorithm and data representation , then observe whether these prior beliefs are supported by the experimental evidence . </S>",
    "<S> we provide an open source implementation of all the simulators to allow for the controlled testing of hypotheses relating to classifier performance on different data representations . </S>",
    "<S> we identify many surprising results that confounded our expectations , and use these results to highlight how an over simplified view of classifier structure can often lead to erroneous prior beliefs . </S>",
    "<S> we believe ensembling can often overcome prior bias , and our results support the belief by showing that the ensemble approach adopted by the hierarchical collective of transform based ensembles ( hive - cote ) is significantly better than the alternatives when the data representation is unknown , and is significantly better than , or not significantly significantly better than , or not significantly worse than , the best other approach on three out of five of the individual simulators . </S>"
  ]
}