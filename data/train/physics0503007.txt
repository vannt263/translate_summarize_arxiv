{
  "article_text": [
    "short - term financial data usually exhibit similar properties called ` stylized facts ' like , e.g. , leptokurtosis , dependence of simultaneous extremes , radial asymmetry , volatility clustering , etc . , especially if the log - price changes ( called the ` log - returns ' ) of stocks , stock indices , and foreign exchange rates are considered . particularly , high - frequency data usually are non - stationary , have jumps , and are strongly dependent .",
    "e.g. , bouchaud , cont , and potters , 1998 , breymann , dias , and embrechts , 2003 , eberlein and keller , 1995 , embrechts , frey , and mcneil , 2004 ( section 4.1.1 ) , engle , 1982 , fama , 1965 , junker and may , 2002 , mandelbrot , 1963 , and mikosch , 2003 ( chapter 1 ) .",
    "figure 1 contains qq - plots of @xmath0 residuals of daily log - returns of the nasdaq and the s&p 500 indices from 1993 - 01 - 01 to 2000 - 06 - 30 .",
    "it is clearly indicated that the normal distribution hypothesis is not appropriate for the loss parts of the distributions whereas the gaussian law seems to be acceptable for the profit parts .",
    "hence the probability of extreme losses is higher than suggested by the normal distribution assumption .",
    "+    * fig .",
    "1 : * qq - plots of nasdaq ( left hand ) and s&p 500 ( right hand ) @xmath0 residuals from 1993 - 01 - 01 to 2000 - 06 - 30 ( @xmath1 ) .",
    "+ the next picture shows the joint distribution of the garch residuals considered above .",
    "+    * fig . 2 : * nasdaq vs. s&p 500 @xmath0 residuals from 1993 - 01 - 01 to 2000 - 06 - 30 ( @xmath1 ) .",
    "+ except for one element all extremes occur simultaneously .",
    "the effect of simultaneous extremes can be observed more precisely in the following picture .",
    "it shows the total numbers of s&p 500 stocks whose absolute values of daily log - returns exceeded @xmath2 for each trading day during 1980 - 01 - 02 to 2003 - 11 - 26 . on the 19th october 1987 ( i.e. the ` black monday ' ) there occurred 239 extremes .",
    "this is suppressed for the sake of transparency .",
    "+ * fig . 3 : * number of extremes in the s&p 500 during 1980 - 01 - 02 to 2003 - 11 - 26 .",
    "+    the latter figure shows the concomitance of extremes . if extremes would occur independently then the number of extremal events ( no matter if losses or profits ) should be small and all but constant over time . obviously , this is not the case .",
    "in contrast one can see the october crash of 1987 and several extremes which occur permanently since the beginning of the bear market in 2000 .",
    "hence there is an increasing tendency of simultaneous losses which is probably due to globalization effects and relaxed market regulation .",
    "the phenomenon of simultaneous extremes is often denoted by ` asymptotic dependence ' or ` tail dependence ' .    the traditional class of elliptically symmetric distributions ( cambanis , huang , and simons , 1981 , fang , kotz , and ng , 1990 , and kelker , 1970 ) is often proposed for the modeling of financial data ( cf . ,",
    "e.g. , bingham and kiesel , 2002 ) .",
    "but elliptical distributions suffer from the property of radial symmetry .",
    "the pictures above show that financial data are not always symmetrically distributed . for this reason the authors will bear on the assumption of generalized",
    "elliptically distributed ( frahm , 2004 ) log - returns .",
    "this allows for the modeling of tail dependence and radial asymmetry .",
    "the quintessence of modern portfolio theory is that the portfolio diversification effect depends essentially on the covariances .",
    "but the parameters for portfolio optimization , i.e. the mean vector and the covariance matrix , have to be estimated . especially for portfolio risk minimization a reliable estimate of the covariance matrix is necessary ( chopra and ziemba , 1993 ) . for covariance matrix estimation generally one should use as much available data as possible .",
    "but since daily log - returns and all the more high - frequency data are not normally distributed , standard estimators like the sample covariance matrix may be highly inefficient leading to erroneous implications ( see , e.g. , oja , 2003 and visuri , 2001 ) .",
    "this is because the sample covariance matrix is very sensitive to outliers .",
    "the smaller the distribution s tail index ( hult and lindskog , 2002 ) , i.e. the heavier the tails of the log - return distributions the higher the estimator s variance .",
    "so the quality of the parameter estimates depends essentially on the true multivariate distribution of log - returns .    in the following it",
    "is shown how the linear dependence structure of generalized elliptical random vectors can be estimated robustly .",
    "more precisely , it is shown that tyler s ( 1987 ) robust m - estimator for the dispersion matrix @xmath3 of elliptically distributed random vectors remains completely robust for generalized elliptically distributed random vectors .",
    "this estimator is not disturbed neither by asymmetries nor by outliers and all the available data points can be used for estimation purposes .",
    "further , the impact of high - dimensional ( financial ) data on statistical inference will be discussed .",
    "this is done by referring to a branch of statistical physics called ` random matrix theory ' ( hiai and petz , 2000 and mehta , 1990 ) .",
    "random matrix theory ( rmt ) is concerned with the distribution of eigenvalues of high - dimensional randomly generated matrices .",
    "if each component of a sample is independent and identically distributed then the distribution of the eigenvalues of the sample covariance matrix converges to a specified law which does not depend on the specific distribution of the sample components .",
    "the circumstances under which this result of rmt can be properly adopted to generalized elliptically distributed data will be examined .",
    "it is well known that an elliptically distributed random vector @xmath4 can be represented stochastically by @xmath5 , where @xmath6 , @xmath7 with @xmath8 , @xmath9 is a @xmath10-dimensional random vector uniformly distributed on the unit hypersphere @xmath11 , and @xmath12 is a nonnegative random variable stochastically independent of @xmath9 .",
    "the positive semi - definite matrix @xmath13 characterizes the linear dependence structure of @xmath4 and is referred to as the ` dispersion matrix ' .",
    "the @xmath14-dimensional random vector @xmath4 is said to be ` generalized elliptically distributed ' if and only if    @xmath15    where @xmath9 is a @xmath10-dimensional random vector uniformly distributed on @xmath11 , @xmath12 is a random variable , @xmath16 , and @xmath17 .",
    "note that the definition of generalized elliptical distributions preserves all the ordinary components of elliptically symmetric distributions ( i.e. @xmath18 , @xmath3 , and @xmath12 ) .",
    "but in contrast the generating variate @xmath12 may be negative and even more it may depend on @xmath9 .",
    "it is worth to point out that the class of generalized elliptical distributions contains the class of skew - elliptical distributions ( branco and dey , 2001 , and frahm , 2004 , section 3.2 ) .",
    "the next figure shows once again the joint distribution of the garch residuals of the nasdaq and s&p 500 log - returns from 1993 - 01 - 01 to 2000 - 06 - 30 from figure 2 .",
    "the right hand of figure 4 contains simulated garch residuals on the basis of a generalized @xmath19-distribution .",
    "more precisely , the generating variate @xmath12 corresponds to @xmath20 but the number of degrees of freedom @xmath21 depends on @xmath22 , i.e. @xmath23 @xmath24 . here",
    "@xmath25 is a function that measures the distance between @xmath26 and the reference vector @xmath27 , @xmath28 .",
    "hence , random vectors which are close to the reference vector ( i.e. close to the ` perfect loss scenario ' ) are supposed to be @xmath19-distributed with @xmath29 degrees of freedom whereas random vectors which are opposite are assumed to be nearly gaussian ( @xmath30 ) distributed .",
    "this is consistent with the phenomenon observed in figure 1 .",
    "the pseudo - correlation coefficient is set to @xmath31 .",
    "+    * fig .",
    "4 : * observed @xmath0 residuals of nasdaq and s&p 500 ( left hand ) and simulated generalized @xmath19-distributed random noise ( @xmath1 ) ( right hand ) .",
    "it is well - known that the sample covariance matrix corresponds both to the moment estimator and to the ml - estimator for the dispersion matrix @xmath3 of normally distributed data . but",
    "given any other elliptical distribution family the dispersion matrix usually does not correspond to the covariance matrix .",
    "generally , robust covariance matrix estimation means to estimate the dispersion matrix , that is the covariance matrix up to a scaling constant .",
    "there are many applications like , e.g. , principal components analysis , canonical correlation analysis , linear discriminant analysis , and multivariate regression where only the dispersion matrix is demanded ( oja , 2003 ) .",
    "particularly , by tobin s two - fund separation theorem ( tobin , 1958 ) the optimal portfolio of risky assets does not depend on the scale of the covariance matrix .",
    "thus in the following we will loosely speak of ` covariance matrix estimation ' rather than of estimating the dispersion matrix for the sake of simplicity .",
    "as mentioned before the true linear dependence structure of elliptically distributed data can not be estimated efficiently by the sample covariance matrix , generally .",
    "especially , if the data stem from a regularly varying random vector the smaller the tail index , i.e. the heavier the tails the larger the estimator s variance .",
    "but in the following it is shown that there exists a completely robust alternative to the sample covariance matrix .",
    "let @xmath4 be a @xmath14-dimensional generalized elliptically distributed random vector where @xmath18 is supposed to be known , @xmath17 with @xmath32 , and @xmath33 .",
    "further , let the unit random vector generated by @xmath34 be defined as    @xmath35    due to the stochastic representation of @xmath4 the following relations hold ,    @xmath36    where @xmath37 .",
    "the random vector @xmath38 does not depend on the absolute value of @xmath12 .",
    "so it is completely robust against extreme outcomes of the generating variate .",
    "but the sign of @xmath12 still remains and this may depend on @xmath9 , anymore .",
    "suppose for the moment that @xmath39 is known for each realization of @xmath12 .",
    "then the dispersion matrix of @xmath4 can be estimated robustly via maximum - likelihood estimation using the density function of @xmath40 which is only a function of @xmath34 .",
    "this is given by the next theorem .",
    "the spectral density function of the unit random vector generated by @xmath17 corresponds to    @xmath41    where @xmath42 .",
    "see , e.g. , frahm , 2004 , pp .",
    "59 - 60 .",
    "since @xmath43 is a symmetric density function the sign of @xmath12 does not matter at all .",
    "hence the ml - estimation approach works even if the data are skew - elliptically distributed , for instance .",
    "the desired ` spectral estimator ' is given by the fixed - point equation ( frahm , 2004 , section 4.2.2 )    @xmath44    where @xmath45 for @xmath46 .",
    "since the solution of the fixed - point equation is only unique up to a scaling constant in the following it is implicitly required that the upper left element of @xmath47 corresponds to @xmath48 .",
    "the spectral estimator @xmath47 corresponds to tyler s robust m - estimator ( tyler , 1983 and tyler , 1987 ) for elliptical distributions , i.e.    @xmath49    hence tyler s m - estimator remains completely robust within the class of generalized elliptical distributions .",
    "the following figure shows the sample covariance matrix ( left hand ) of a sample with @xmath50 observations and @xmath51 dimensions drawn from a multivariate @xmath19-distribution with @xmath29 degrees of freedom .",
    "note that the tail index of the multivariate @xmath19-distribution corresponds to @xmath21 .",
    "each cell of the plots represents a matrix element where the blue colored cells symbolize small numbers and the red colored cells indicate large numbers .",
    "the true dispersion matrix is given in the middle whereas the spectral estimate is given by the right hand .     +    * fig .",
    "5 : * sample covariance matrix ( left hand ) , true covariance matrix ( middle ) , and spectral estimate ( right hand ) of multivariate @xmath19-distributed realizations ( @xmath52 ) .",
    "rmt is concerned with the distribution of the eigenvalues of high - dimensional randomly generated matrices .",
    "a random matrix is simply a matrix of random variables .",
    "we will consider only symmetric random matrices .",
    "thus the corresponding eigenvalues are always real .",
    "the empirical distribution function of eigenvalues is defined as follows .",
    "let @xmath53 be a @xmath54 symmetric random matrix with eigenvalues @xmath55 .",
    "then the function @xmath56 is called the ` empirical distribution function of the eigenvalues ' of @xmath57 .",
    "note that each eigenvalue of a random matrix in fact is random but per se not a random variable since there is no single - valued mapping @xmath58 @xmath59 but rather @xmath60 where @xmath61 denotes the set of all eigenvalues of @xmath53 .",
    "this can be simply fixed by assuming that the eigenvalues @xmath62 are sorted either in an increasing or decreasing order .",
    "[ mp_law ] let @xmath63 @xmath64 be sequences of independent random vectors uniformly distributed on the unit hypersphere @xmath65 and consider the random matrix @xmath66where its empirical distribution function of the eigenvalues is denoted by @xmath67 .",
    "suppose that @xmath68,@xmath69 , @xmath70 .",
    "then @xmath71 at all points where @xmath72 is continuous .",
    "more precisely , @xmath73 where the dirac part is given by @xmath74and the lebesgue part @xmath75 is determined by the density function@xmath76where@xmath77    marenko and pastur , 1967 .    in the following @xmath78",
    "will be called ` marenko - pastur operator ' .",
    "the next corollary states that the marenko - pastur law @xmath72 holds not only for the empirical distribution function of eigenvalues of the marenko - pastur operator but also for that obtained by the sample covariance matrix if the data are standard normally distributed and independent .",
    "let @xmath79 @xmath80 be sequences of independent and standard normally distributed random vectors with uncorrelated components . then the empirical distribution function of the eigenvalues of @xmath81 converges in probability to the marenko - pastur law stated in theorem [ mp_law ]",
    ".    due to the strong law of large numbers @xmath82 @xmath83 and thus @xmath84    ' '' ''    moreover , the marenko - pastur law holds even if @xmath4 is an arbitrary random vector with standardized i.i.d .",
    "components provided the second moment is finite ( yin , 1986 ) .",
    "more precisely , consider the random vector @xmath4 with @xmath85 and @xmath86 where the components of @xmath4 are supposed to be stochastically independent .",
    "then the marenko - pastur law can be applied on the empirical distribution function of the eigenvalues of @xmath87 where @xmath53 denotes the sample covariance matrix and @xmath88    hence , the marenko - pastur law can be applied virtually ever on the empirical distribution function of @xmath89 where the estimated eigenvalues are given by the sample covariance matrix provided the sample elements , i.e. the realized random vectors consist of stochastically independent components . but within the class of elliptical distributions this holds only for uncorrelated normally distributed data . hence linear independence and stochastical independence",
    "are not equivalent for generalized elliptically distributed data .",
    "this is because even if there is no linear dependence between the components of an elliptically distributed random vector another sort of nonlinear dependence caused by the generating variate @xmath12 remains , generally .",
    "for instance , consider the unit random vector @xmath90 .",
    "then @xmath91i.e .",
    "@xmath92 depends strongly on @xmath93 though indeed the elements of @xmath22 are uncorrelated .",
    "tail dependent random variables can not be stochastically independent .",
    "especially , if the random components of an elliptically distributed random vector are heavy tailed , i.e. if the generating variate is regularly varying then they possess the property of tail dependence ( schmidt , 2002 ) . in that case",
    "the eigenspectrum generated by the sample covariance matrix may lead to erroneous implications .",
    "for instance , consider a sample ( with sample size @xmath50 ) of @xmath94-dimensional random vectors where each vector element is standardized @xmath19-distributed with @xmath95 degrees of freedom and stochastically independent of each other . here",
    "the eigenspectrum obtained by the sample covariance matrix indeed is consistent with the marenko - pastur law ( upper left part of figure 6 ) .",
    "but if the data stem from a multivariate @xmath19-distribution possessing the same parameters and each vector component is uncorrelated then the eigenspectrum obtained by the sample covariance matrix does not correspond to the marenko - pastur law ( upper right part of figure 6 ) . actually , there are @xmath96 eigenvalues exceeding the marenko - pastur upper bound @xmath97 and the largest eigenvalue corresponds to @xmath98 .",
    "but fortunately the eigenspectra obtained by the spectral estimator are consistent with the marenko - pastur law as indicated by the lower part of figure 6 .",
    "+    +    * fig .",
    "6 : * eigenspectra of univariate ( left part ) and multivariate ( right part ) uncorrelated @xmath19-distributed data ( @xmath99 ) obtained by the sample covariance matrix ( upper part ) and by the spectral estimator ( lower part )",
    ". + tyler ( 1987 ) shows that the spectral estimator converges strongly to the true dispersion matrix @xmath100 .",
    "that means @xmath101for @xmath102 and @xmath103-almost all realizations .",
    "consequently , if @xmath104 ( up to a scaling constant ) then@xmath105as @xmath106 and @xmath14 constant .",
    "hence the spectral estimator and the marenko - pastur operator are asymptotically equivalent provided @xmath107 .",
    "the authors believe that the strong convergence holds even for @xmath68 , @xmath108 , @xmath109 for @xmath103-almost all realizations where the spectral estimate exists .",
    "the proof of this conjecture is due to a forthcoming work .",
    "note that for @xmath110 the spectral estimate does not exist at all .",
    "further , tyler ( 1987 ) shows that the spectral estimate exists ( a.s . )",
    "if @xmath111 , i.e. @xmath112 .",
    "indeed , this is a sufficient condition for the existency of the spectral estimator .",
    "but in practice the spectral estimator seems to exist in most cases when @xmath113 is already slightly larger than @xmath14 .",
    "we conclude that testing high - dimensional data for the null hypothesis @xmath107 by means of the sample covariance matrix may lead to wrong conclusions provided the data are generalized elliptically distributed . in contrast",
    ", the spectral estimator seems to be a robust alternative for applying the results of rmt in the context of generalized elliptical distributions .",
    "in this section it is supposed that @xmath114 , i.e. from the viewpoint of rmt we study low - dimensional problems .",
    "let @xmath115 be an elliptically distributed random vector of short - term ( e.g. daily ) log - returns .",
    "if the fourth order cross moments of the log - returns are finite then the elements of the sample covariance matrix are multivariate normally distributed , asymptotically .",
    "the asymptotic covariance of each element is given by ( see , e.g. , praag and wesselman , 1989 ) @xmath116 where @xmath117 $ ] denotes the true covariance matrix of @xmath118 and @xmath119 is called the ` kurtosis parameter ' .",
    "note that the kurtosis parameter does not depend on @xmath120 .",
    "it is well - known that in the case of normality @xmath121 .",
    "a distribution with positive ( or even infinite ) @xmath122 is called ` leptokurtic ' .",
    "particularly , regularly varying distributions are leptokurtic .",
    "it is well - known that the portfolio which minimizes the portfolio return variance ( the so called ` global minimum variance portfolio ' ) is given by the vector of portfolio weights @xmath123    now , suppose for the sake of simplicity that @xmath118 is spherically distributed , i.e. that @xmath124 and @xmath3 is proportional to the identity matrix . since the weights of the global minimum variance portfolio do not depend on the scale of @xmath3 we may assume @xmath125 w.l.o.g .",
    "then the asymptotic covariances of the sample covariance matrix elements are simply given by @xmath126    for instance suppose that the random vector @xmath118 is multivariate @xmath19-distributed with @xmath127 degrees of freedom .",
    "then the kurtosis parameter corresponds to @xmath128 ( see , e.g. , frahm , 2004 , p. 91 ) . hence , the smaller @xmath21 the larger the asymptotic variances and covariances and these quantities tend to infinity for @xmath129 .",
    "further , if @xmath130 the sample covariance matrix even is no longer multivariate normally distributed , asymptotically .",
    "in contrast , the asymptotic covariance of each element of the spectral estimator ( frahm , 2004 , p. 76 ) is given by @xmath131    note that the same holds even if @xmath118 is not @xmath19-distributed but only generalized elliptically distributed since @xmath47 does not depend on the generating variate of @xmath118 .",
    "particularly , the spectral estimator is not disturbed by the tail index of @xmath118 .",
    "now one may ask when the sample covariance matrix is dominated ( in a component - wise manner ) by the spectral estimator provided the data are multivariate @xmath19-distributed . regarding the main diagonal entries of the covariance matrix estimate this",
    "is given by @xmath132 i.e. if @xmath133 the variance of the spectral estimator s main diagonal elements is smaller than the variance of the corresponding main diagonal elements of the sample covariance matrix , asymptotically .",
    "concerning its off diagonal entries we obtain @xmath134",
    "i.e. @xmath135 . it is worth to note that several empirical studies indicate that the tail indices of daily log - returns generally lie between @xmath136 and @xmath137 ( see , e.g. , embrechts , frey , and mcneil , 2004 , p. 81 and",
    "junker and may , 2002 ) .    in the following the daily log - returns from 1980 - 01 - 02 to 2003 - 10 - 06 of 285 s&p 500 stocks",
    "are analyzed for studying the robustness of the spectral estimator vs. the sample covariance matrix .",
    "the considered stocks belong to the ` survivors ' of the s&p 500 composite at the last quarter of 2003 .",
    "the sample size corresponds to @xmath138 .",
    "the total sample period is partitioned into @xmath139 sub - periods each containing @xmath140 daily log - returns .",
    "further , each sub - period is divided into ` even ' and ` odd ' days , i.e. there is a sub - sample containing the 1st , 3rd ,  , 599th log - returns and another sub - sample with the 2nd , 4th ,  , 600th log - returns .",
    "hence each sub - sample contains @xmath141 daily log - returns of @xmath142 stocks .",
    "both the sample covariance matrix and the spectral estimator are used for estimating the relative eigenspectrum of the true covariance matrix , i.e. @xmath143 for each even and odd sub - sample , separately .",
    "if the covariance matrix estimator is robust against outliers then the estimated eigenspectra of each sub - sample should be similar since even if the true eigenspectrum changes dynamically over time this must affect both the even and the odd days , equally .",
    "the eigenspectrum obtained in the even sub - sample can be compared with the eigenspectrum given by the odd sub - sample simply by the differences of the ordered ( relative ) eigenvalues .",
    "+    * fig . 7 : * eigenvalue differences for each ordered eigenvalue given by the sample covariance matrix ( left hand ) and by the spectral estimate ( right hand ) .",
    "+ on the left hand of figure 7 we see the eigenvalue differences for each @xmath139 sub - periods caused by the sample covariance matrix .",
    "similarly , the right hand of figure 7 shows the eigenvalue differences given by the spectral estimate .",
    "figure 7 indicates that the spectral estimator leads to more robust estimates of the eigenspectra of financial data .",
    "but note that - concerning the overall eigenspectrum - the sample covariance matrix performs well up to the 4th sub - period .",
    "this is the period which contains the famous october crash of @xmath144 .",
    "in contrast , the spectral estimator is not affected by extreme values .      +    * fig . 8 : * eigenvalue differences for the largest @xmath145 eigenvalues given by the sample covariance matrix ( left hand ) and by the spectral estimate ( right hand ) . + figure 8 focuses on the differences of the @xmath145 largest eigenvalues .",
    "it shows that the sample covariance matrix particularly fails for estimating the largest eigenvalue .",
    "once again this phenomenon is caused by the black monday which belongs to the even sub - sample of the 4th sub - period .",
    "note that the largest eigenvalue of the even sub - sample exceeds the largest eigenvalue of the odd sub - sample by almost @xmath146 percentage points .",
    "we conclude that although the sample covariance matrix works quite good for the most time it is not appropriate for measuring the linear dependence structure of financial data .",
    "this is due to a few but extreme fluctuations on financial markets .      now , consider a @xmath14-dimensional vector @xmath147 of long - term ( e.g. yearly ) i.i.d .",
    "log - returns .",
    "due to the central limit theorem each vector component of @xmath118 is approximately normal distributed provided the covariance matrix of the short - term ( e.g. daily ) log - returns exists and is finite . since the sum of i.i.d .",
    "elliptical random vectors is always elliptically distributed , too ( see , e.g. , hult and lindskog , 2002 ) one may take for granted that the vector components of @xmath118 are jointly normally distributed , approximately .",
    "but this is not true if the number of dimensions @xmath14 is large relative to the sample size @xmath113 .",
    "for instance , consider a @xmath14-dimensional random vector @xmath4 which is multivariate @xmath19-distributed with @xmath148 degrees of freedom , location vector @xmath124 , and dispersion matrix @xmath149 . due to the multivariate central limit theorem",
    "one could believe that @xmath150 where @xmath151 are independent copies of @xmath4 .",
    "but indeed @xmath152 holds only if @xmath153 is large rather than @xmath113 being large ( cf .",
    "frahm , 2004 , section 6.2 ) .",
    "thus the quantity @xmath154 can be interpreted as ` effective sample size ' .    in the following it",
    "is assumed that @xmath118 is elliptically distributed with location vector @xmath18 and dispersion matrix @xmath3 .",
    "let @xmath155 be a spectral decomposition of @xmath3 .",
    "then @xmath156 where @xmath157 spherically distributed with @xmath125 .",
    "we assume that the elements of @xmath158 , i.e. the eigenvalues of @xmath3 are given in a descending order and that the first @xmath10 eigenvalues are large whereas the residual ones are small .",
    "the elements of @xmath157 are called ` principal components ' of @xmath118 .",
    "since @xmath159 is orthonormal the distribution of @xmath160 remains up to a rotation in @xmath161 .",
    "the direction of each principal component is given by the corresponding column of @xmath159 .",
    "hence the first @xmath10 eigenvalues correspond to the variances ( up to a scaling constant ) of the ` driving risk factors ' contained in the first part of @xmath157 , i.e. @xmath162 . for the purpose of dimension reduction @xmath10 shall not be too large . because the @xmath163 residual risk factors contained in @xmath164",
    "are supposed to have ( relatively ) small variances they can be interpreted as the components of the idiosyncratic risks of each firm , i.e. @xmath165 where @xmath166 .",
    "thus we obtain the following principal components model for long - term log - returns , @xmath167 where the driving risk factors @xmath168 are uncorrelated .",
    "further , each noise term @xmath169 @xmath170 is uncorrelated to @xmath168 , too . but note that @xmath171 are correlated , generally .",
    "the ` betas ' are given by @xmath172 for @xmath173 and @xmath174 .",
    "the purpose of principal components analysis is to reduce the complexity caused by the number of dimensions .",
    "this can be done successfully only if there is indeed a number of principal components accountable for the most part of the distribution .",
    "additionally , the covariance matrix estimator which is used for extracting the principal components should be robust against outliers .",
    "for example , let the daily log - returns be multivariate @xmath19-distributed with @xmath21 degrees of freedom and suppose that @xmath51 and @xmath50 .",
    "note that due to the central limit theorem the normality assumption concerning the long - term log - returns makes sense whenever @xmath175 .",
    "the black lines in figure 9 show the true proportion of the total variation for a set of @xmath94 eigenvalues .",
    "we see that the largest @xmath176 of the eigenvalues accounts for @xmath177 of the overall variance .",
    "this is known in economics as ` 80/20 rule ' or ` pareto s principle ' .",
    "the estimated eigenvalue proportions obtained by the sample covariance matrix are represented by the red lines whereas the corresponding estimates based on the spectral estimator are given by the green lines .",
    "each line is an average over @xmath178 concentration curves drawn from samples of the corresponding multivariate @xmath19-distribution .",
    "if the data have a small tail index as given by the lower right of figure 9 then the sample covariance matrix tends to underestimate the number of driving risk factors , essentially .",
    "this is similar to the phenomenon observed in figure 6 where the number of large eigenvalues is overestimated .",
    "in contrast , the concentration curves obtained by the spectral estimator are robust against heavy tails .",
    "this holds even if the long - term log - returns are not asymptotically normal distributed .",
    "+    +    * fig . 9 :",
    "* true proportion of the total variation ( black line ) and proportions obtained by the sample covariance matrix ( red lines ) and by the spectral estimator ( green lines ) .",
    "the samples are drawn from a multivariate @xmath19-distribution with @xmath179 ( i.e. the multivariate normal distribution , upper left ) , @xmath180 ( upper right ) , @xmath181 ( lower left ) , and @xmath182 ( lower right ) .",
    "+ in the simulated example of figure 9 it is assumed that the small eigenvalues are equal .",
    "this is equivalent to the assumption that the residual risk factors are spherically distributed , i.e. that they contain no more information about the linear dependence structure of @xmath118 .",
    "but even if the true eigenvalues are equal the corresponding estimates will not share this property because of estimation errors .",
    "yet it is important to know whether the residual risk factors have structural information or the differences between the eigenvalue estimates are only caused by random noise .",
    "this is not an easy task , especially if the data are not normally distributed and the number of dimensions is large which is the issue of the next section .      in the previous section",
    "it was mentioned that the central limit theorem fails in the context of high - dimensional data , i.e. if @xmath183 is small .",
    "hence , now we leave the field of classical multivariate analysis and get to the domain of rmt .",
    "let @xmath184 be a spectral decomposition where @xmath158 shall be a diagonal matrix containing a ` bulk ' of small and equal eigenvalues and some large ( but not necessarily equal ) eigenvalues .",
    "for the sake of simplicity suppose@xmath185 \\qquad c >",
    "b>0,\\ ] ] where @xmath163 is large .",
    "hence @xmath3 has two different characteristic manifolds .",
    "the ` major ' one is determined by the first @xmath10 column vectors of @xmath159 ( the ` signal part ' of @xmath3 ) whereas the ` minor ' one is given by the @xmath163 residual column vectors of @xmath159 ( the ` noise part ' of @xmath3 ) .",
    "we are interested in separating signal from noise that is to say estimating @xmath10 , properly .    for instance , assume that @xmath50 , @xmath51 , and that a sample consists of normally distributed random vectors with covariance matrix @xmath3 , where @xmath186 , @xmath187 , and @xmath188 . by using the sample covariance matrix and normalizing the eigenvalues one obtains exemplarily the histogram of eigenvalues given on the left hand of figure 10 .",
    "as might be expected the marenko - pastur law is not valid due to the two different regimes of eigenvalues . in contrast ,",
    "when focusing on the smallest @xmath189 eigenvalues , i.e. on the noise part of @xmath53 the marenko - pastur law becomes valid as we see on the right hand of figure 10 .",
    "+    * fig .",
    "10 : * histogram of all @xmath51 eigenvalues ( left hand ) and of the noise part ( right hand ) consisting of the @xmath190 smallest eigenvalues .",
    "the marenko - pastur law is represented by the green lines .",
    "+ thus separating signal from noise means sorting out the largest eigenvalues successively until the residual eigenspectrum is consistent with the marenko - pastur law .",
    "this is given , e.g. , when there are no more eigenvalues exceeding the marenko - pastur upper bound @xmath191 . in our case - study this is given for @xmath192 eigenvalues ( see the figure below ) , i.e. @xmath193 .",
    "11 : * histogram of the remaining @xmath192 eigenvalues after signal - noise separation .",
    "+    as it was shown in section [ rmt ] this approach is promising only if the data are not regularly varying .",
    "hence for financial data not the sample covariance matrix but the spectral estimator is proposed for a proper signal - noise separation .",
    "due to the stylized facts of empirical finance the gaussian distribution hypothesis is not appropriate for the modeling of financial data .",
    "for that reason the authors rely on the broad class of generalized elliptical distributions .",
    "this class allows for tail dependence and radial asymmetry .",
    "although the sample covariance matrix works quite good with financial data for the most time it is not appropriate for measuring their linear dependence structure .",
    "this is due to a few but extreme fluctuations on financial markets .",
    "it is shown that there exists a completely robust ml - estimator ( the ` spectral estimator ' ) for the dispersion matrix of generalized elliptical distributions .",
    "this estimator corresponds to tyler s m - estimator for elliptical distributions .",
    "further , it is shown that the marenko - pastur law fails if the sample covariance matrix is considered as random matrix in the context of elliptically or even generalized elliptically distributed data .",
    "this is due to the fact that stochastical independence implies linear independence but conversely uncorrelated random variables are not necessarily independent .",
    "in contrast , the marenko - pastur law remains valid if the data are uncorrelated and the spectral estimator is considered as random matrix .",
    "the robustness property of the spectral estimator can be demonstrated for several financial applications like , e.g. , portfolio risk minimization , principal components analysis , and signal - noise separation .",
    "if the data are heavy tailed the principal components analysis tends to underestimate the number of driving risk factors if the sample covariance matrix is used for extracting the eigenspectrum .",
    "this means that the contribution of the largest eigenvalues to the total variation of the data is overestimated , systematically .",
    "consequently , in the context of signal - noise separation the largest eigenvalues are overestimated by the sample covariance matrix .",
    "this can be fixed simply by using the spectral estimator , instead .",
    "bouchaud , j.p . ,",
    "cont , r. , and potters , m. ( 1998 ) . ` scaling in stock market data : stable laws and beyond . ' in : dubrulle , b. , graner , f. , and sornette , d. ( eds . ) , _ scale invariance and beyond _ , proceedings of the cnrs workshop on scale invariance , les houches , march 1997 , springer .",
    "embrechts , p. , frey , r. , and mcneil , a.j .",
    "( 2004 ) . ` quantitative methods for financial risk management . ' in progress , but various chapters are retrievable from ` http://www.math.ethz.ch/mcneil/book.html ` .",
    "frahm , g. ( 2004 ) . `",
    "generalized elliptical distributions : theory and applications . ' ph.d .",
    "thesis , university of cologne , faculty of management , economics , and social sciences , department of statistics , germany .",
    "retrievable from ` http://kups.ub.uni-koeln.de/volltexte/2004/1319/ ` .",
    "junker , m. and may , a. ( 2002 ) . `",
    "measurement of aggregate risk with copulas . '",
    "working paper , caesar , bonn , germany . retrieved 2004 - 10 - 14 from ` http://www.caesar.de/uploads/media/cae_pp_0021_junker_2002-05-09.pdf ` .",
    "mikosch , t. ( 2003 ) . ` modeling dependence and tails of financial time series . ' in : finkenstaedt , b. and rootzn , h. ( eds . ) , _ extreme values in finance , telecommunications , and the environment _ , chapman & hall .",
    "oja , h. ( 2003 ) . `",
    "multivariate m - estimates of location and shape . ' in : hglund , r. , jntti , m. , and rosenqvist , g. ( eds . ) , _ statistics , econometrics and society .",
    "essays in honor of leif nordberg _ , statistics finland ."
  ],
  "abstract_text": [
    "<S> the traditional class of elliptical distributions is extended to allow for asymmetries . </S>",
    "<S> a completely robust dispersion matrix estimator ( the ` spectral estimator ' ) for the new class of ` generalized elliptical distributions ' is presented . </S>",
    "<S> it is shown that the spectral estimator corresponds to an m - estimator proposed by tyler ( 1983 ) in the context of elliptical distributions . </S>",
    "<S> both the generalization of elliptical distributions and the development of a robust dispersion matrix estimator are motivated by the stylized facts of empirical finance . </S>",
    "<S> random matrix theory is used for analyzing the linear dependence structure of high - dimensional data . </S>",
    "<S> it is shown that the marenko - pastur law fails if the sample covariance matrix is considered as a random matrix in the context of elliptically distributed and heavy tailed data . but </S>",
    "<S> substituting the sample covariance matrix by the spectral estimator resolves the problem and the marenko - pastur law remains valid . </S>"
  ]
}