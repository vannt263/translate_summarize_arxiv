{
  "article_text": [
    "let @xmath4 be a probability space and let @xmath5 be a random vector defined on it with each @xmath6 taking values in @xmath7 .",
    "suppose that the joint distribution of @xmath1 has a density @xmath2 with respect to a reference measure @xmath8 on @xmath3 .",
    "for most of this paper ( except for the purposes of discussion in this section ) , the reference measure is simply lebesgue measure @xmath9 on @xmath3 .",
    "the random variable @xmath10 may be thought of as the ( random ) information content of @xmath1 .",
    "such an interpretation is well - justified in the discrete case , when @xmath11 is the counting measure on some countable subset of @xmath3 on which the distribution of @xmath1 is supported . in this case , @xmath12 is essentially the number of bits needed to represent @xmath1 by a coding scheme that minimizes average code length  @xcite . in the continuous case ( with reference measure @xmath9 ) ,",
    "one may still call @xmath12 the information content even though the coding interpretation no longer holds . in statistics",
    ", one may think of the information content as the log likelihood function .",
    "the average value of the information content of @xmath1 is known more commonly as the entropy .",
    "indeed , the entropy of @xmath1 is defined by @xmath13 observe that we adopt here the usual abuse of notation : we write @xmath14 even though the entropy is a functional depending only on the distribution of @xmath1 and not on the value of @xmath1 . in general , @xmath14 may or may not exist ( in the lebesgue sense ) ; if it does , it takes values in the extended real line @xmath15 $ ] .    because of the relevance of the information content in various areas such as information theory , probability and statistics , it is intrinsically interesting to understand its behavior . in particular ,",
    "a natural question arises : is it true that the information content concentrates around the entropy in high dimension ?",
    "in general , there is no reason for such a concentration property to hold .",
    "a main purpose in this note is , however , to show that when the probability measure on @xmath3 of interest is absolutely continuous and log - concave , @xmath16 does possess a powerful concentration property . specifically , we prove the following theorem .",
    "[ thm : exp ] suppose @xmath17 is distributed according to a log - concave density @xmath2 on @xmath3 .",
    "then , for all @xmath18 , @xmath19 where @xmath20 is a universal constant",
    ". in fact , one may take @xmath21 .",
    "note that under the assumption of log - concavity and absolute continuity , @xmath14  always exists and is finite ( see , e.g. ,  @xcite ) .",
    "let us emphasize that the distribution of the difference @xmath22 is stable under all affine transformations of the space , that is , @xmath23 for all invertible affine maps @xmath24 .",
    "in particular , the variance of the information content @xmath25 represents an affine invariant . by theorem  [ thm :",
    "exp ] , when @xmath2 is log - concave , this variance is bounded by @xmath26 with some universal constant @xmath27 .",
    "in fact , the deviation inequality in theorem  [ thm : exp ] amounts to a stronger bound @xmath28 with respect to the orlicz norm , generated by the young function @xmath29 .",
    "this is consistent with the observation that in many standard examples @xmath12 behaves like the sum of @xmath30 independent random variables .",
    "for example , when @xmath1 is standard normal , we have @xmath31 more generally , if @xmath32 has independent components , then @xmath33 these examples show that @xmath34-normalization in theorem [ thm : exp ] is chosen correctly and can not be improved for the class of log - concave distributions .",
    "when the dimension @xmath30 is large , the exponential decay in theorem [ thm : exp ] may be improved to the gaussian decay on the interval @xmath35 .",
    "[ thm : gaus ] given a random vector @xmath1 in @xmath3 with log - concave density  @xmath2 , @xmath36 where @xmath20 is a universal constant .",
    "in fact , one may take @xmath21 .    substituting @xmath37 ,",
    "rewrite the above inequality as @xmath38 for @xmath39 .",
    "equivalently , in terms of the entropy power @xmath40 , we get for the value , say , @xmath41 , @xmath42 thus , with high probability , @xmath43 is very close to @xmath44 , and the distribution of @xmath1 itself is effectively the uniform distribution on the class of typical observables , or the `` typical set '' [ defined to be the collection of all points @xmath45 such that @xmath46 lies between @xmath47 and @xmath48 , for some small fixed @xmath49 .",
    "a similar concentration inequality was obtained by klartag and milman  @xcite , who compared the value @xmath50 to the maximum @xmath51 of the density @xmath2 and proved that @xmath52 with some absolute constants @xmath53 .",
    "note this result readily follows from theorem  [ thm : gaus ] , but not conversely .",
    "theorems  [ thm : exp ] and  [ thm : gaus ] , by entailing an effective uniformity of the distribution of @xmath1 on some compact set , provide a strong , quantitative formulation of the _ asymptotic equipartition property _ for log - concave measures . to describe this interpretation ,",
    "suppose @xmath54 is a stochastic process on the probability space @xmath4 , with each @xmath6 taking values in @xmath7 , and define the corresponding projections @xmath55 .",
    "if @xmath56 is stationary , the limit @xmath57 exists as long as the increments @xmath58 are finite , and is called the entropy rate of @xmath56 . for stationary processes @xmath56",
    ", the question of whether the information content per coordinate @xmath59 converges to the limit @xmath60 ( in @xmath61 or in probability or almost surely ) has been extensively studied . in the discrete case , the affirmative answer to",
    "this question goes back to shannon  @xcite , mcmillan  @xcite and breiman  @xcite , and the eponymous theorem has been called `` the basic theorem of information theory . ''",
    "the continuous case was partially developed by moy  @xcite , perez  @xcite and kieffer  @xcite .",
    "the definitive version [ almost sure convergence for stochastic processes defined on a standard borel space , and allowing more general reference measures @xmath8 than lebesgue and counting measure ] is due independently to barron  @xcite and orey  @xcite ; the former in particular gives a clear exposition and recounting of the history .",
    "specifically , these works imply that if @xmath56 is stationary and ergodic , then , as @xmath62 , @xmath63 an elementary proof of this fact , called by mcmillan the `` asymptotic equipartition property '' was later given by algoet and cover  @xcite .",
    "for nonstationary processes with arbitrary dependence , the entropy rate @xmath60 typically does not exist ; so there is no question of a statement like ( [ eq : aep ] ) holding . nonetheless , together with borel ",
    "cantelli s lemma theorem  [ thm : exp ] immediately yields the following extension of the shannon ",
    "breiman phenomenon .",
    "[ cor : aep ] suppose that @xmath56 has a log - concave distribution on @xmath64 with absolutely continuous finite - dimensional projections . if the limit @xmath60 exists , the property ( [ eq : aep ] ) holds .",
    "note that log - concavity of a probability measure is defined on arbitrary locally convex spaces via a brunn  minkowski type inequality and is equivalent to the log - concavity of densities of finite - dimensional projections ( in case they are absolutely continuous with respect to lebesgue measure ; see  @xcite for a general theory ) .",
    "corollary  [ cor : aep ] trivially extends to processes @xmath54 where each @xmath6 takes values in @xmath65 instead of @xmath7 , as long as the finite - dimensional projections @xmath66 have log - concave distributions .",
    "this , for instance , means that corollary  [ cor : aep ] can be applied to nonstationary markov chains in @xmath65 that preserve log - concavity of the joint distribution and also have a unique invariant probability measure ( the latter condition ensures existence of the entropy rate , which can also be easily computed as the mean under the invariant measure of the entropy of the conditional density of @xmath67 given @xmath68 ) .",
    "furthermore , if the process mixes well enough so that @xmath69 converges rapidly to @xmath60 , then theorem  [ thm : gaus ] may be used to give a convergence rate in probability .",
    "it should also be mentioned that , for gaussian distributions , tight concentration inequalities may be derived by simple explicit calculation .",
    "this was done by cover and pombra  @xcite as an ingredient in studying the feedback capacity of time - varying additive gaussian noise channels .",
    "the paper is organized in the following way . as a first step ,",
    "we consider a one - dimensional version of theorem  [ thm : exp ] ( section  [ sec:1d ] ) . in section [ sec : revlyap ] , we recall some previous work on reverse lyapunov inequalities , and present a new variant . it is applied to establish a concentration property of the logarithm function under what we call log - concave measures of order @xmath70 ( sections  [ sec : p ] and  [ sec : log ] ) .",
    "section  [ sec : ls ] uses the localization lemma of lovsz and simonovits to reduce the general case to a specific one - dimensional statement .",
    "section  [ sec : pf ] completes the proof .",
    "we begin by proving the one - dimensional case of theorem  [ thm : exp ] .",
    "[ prop:1d ] if a random variable @xmath1 has a log - concave density @xmath2 , then , @xmath71    let @xmath1 be a random variable with log - concave density @xmath46 .",
    "the distribution of @xmath1 is supported on some interval @xmath72 , finite or not , where @xmath2 is positive and @xmath73 is concave .",
    "introduce the function @xmath74 where @xmath75 is the inverse to the distribution function @xmath76 , @xmath77 .",
    "the function @xmath78 is positive and concave on @xmath79 and uniquely determines @xmath80 up to a shift parameter ( @xcite , proposition a.1 ) .",
    "given a function @xmath81 , write a general identity @xmath82 in particular , for any @xmath83 , @xmath84 here the right - hand side does not change when multiplying @xmath78 by a positive scalar , so one may assume that @xmath85 .",
    "but then , by concavity of @xmath78 , we have @xmath86 from this , @xmath87 so @xmath88    hence , the right - hand side of ( [ eq : subst ] ) does not exceed @xmath89    finally , by jensen s inequality with respect to @xmath90 , the left - hand side of ( [ eq : subst ] ) majorizes @xmath91 so that we have @xmath92 choosing the value @xmath93 , and observing that @xmath94 , we may conclude",
    ".    note also that a direct application of chebyshev s inequality yields @xmath95 for all @xmath18 .",
    "while the exponent here is slightly better than that in theorem  [ thm : exp ] , we make no effort here ( or anywhere in this paper ) to come up with optimal constants .",
    "given a random variable @xmath96 , the lyapunov inequality states that @xmath97 where @xmath98 is the moment function of @xmath99 .",
    "equivalently , it expresses a well - known and obvious property that the function @xmath100 is convex on the positive half - axis @xmath101 .    what is less obvious , for certain classes of probability distributions on @xmath102 , the inequality ( [ eq : lyap ] )",
    "may be reversed after a suitable normalization of the moment function . in particular , when @xmath99 has a distribution with increasing hazard rate ( in particular , if @xmath99 has a log - concave density ) , then as was shown by barlow , marshall and proschan ( @xcite , page 384 ) , we have @xmath103 for the normalized moment function @xmath104 note that @xmath105 for all @xmath101 for the standard exponential distribution , which thus plays an extremal role in this class .",
    "this result has many interesting applications .",
    "for example , applying it to the parameters @xmath106 , @xmath107 , @xmath108 , we have @xmath109 provided that @xmath110 is integer .",
    "if the distribution of @xmath99 is log - concave , the case @xmath111 can also be included in this inequality , which is due to a khinchine - type inequality by karlin , proschan and barlow  @xcite , namely , @xmath112    however , in some problems , it is desirable to remove the requirement that @xmath113 is integer in ( [ eq : bmp - lyap ] ) .",
    "this is implied by results of borell  @xcite for the class of log - concave densities . to be more precise",
    ", he proved the following ( theorem  2 in  @xcite ) .",
    "[ prop31 ] let @xmath99 be a nonnegative concave function , defined on an open convex body @xmath114 .",
    "then the function @xmath115 is log - concave in @xmath116 .    to relate this to ( [ eq : bmp - lyap ] ) ,",
    "let us start with a continuous convex function @xmath117 , defined on some closed segment @xmath118 , such that @xmath119 is a probability density . for large @xmath30",
    ", consider convex bodies @xmath120 their volumes satisfy , as @xmath121 , @xmath122 and for every @xmath116 , @xmath123 by proposition  [ prop31 ] , applied to @xmath124 , the functions @xmath125 are log - concave , so the limit will also be a log - concave function , if it exists .",
    "( note that we have added a log - linear factor @xmath126 . )",
    "but @xmath127 therefore , in view of ( [ eq : revlyap - mid1 ] ) and ( [ eq : revlyap - mid2 ] ) , the resulting limit @xmath128 represents a log - concave function , as well .    on this step ,",
    "the assumption that @xmath129 was defined on a closed segment can be relaxed , and we arrive at the following corollary ( which seems not to be mentioned in  @xcite or anywhere else ) .",
    "[ cor : rev - lyap ] if a random variable @xmath96 has a log - concave distribution , then the function @xmath130 is log - concave .",
    "equivalently , we have a reverse lyapunov s inequality @xmath131    in connection with the concentration problem and the kannan  lov ' asz  simonovits conjecture within the class of spherically symmetric distributions on  @xmath3 , reverse lyapunov s inequalities were considered in  @xcite .",
    "the following alternative variant of corollary  [ cor : rev - lyap ] is proposed there .",
    "[ prop : rev - lyap ] given a random variable @xmath96 with a log - concave distribution , the function @xmath132 is log - concave in @xmath133 , and therefore satisfies ( [ eq : rev ] ) .",
    "this is proved in  @xcite by an application of the pr ' ekopa  leindler inequality , and is perhaps more convenient for applications involving asymptotics .",
    "there is much more that can be ( and has been ) said about reverse lyapunov inequalities ; a gentle introduction may be found in  @xcite .",
    "a random variable @xmath134 will be said to have a log - concave distribution of order @xmath135 , if it has a density of the form @xmath136 where the function @xmath137 is log - concave on @xmath102 .",
    "when @xmath111 , we obtain the class of all ( nondegenerate ) log - concave probability distributions on @xmath102 .",
    "the meaning of the parameter @xmath70 is that it is responsible for a strengthened concentration .",
    "for example , the inequality ( [ eq : bmp - lyap2 ] ) , which holds by corollary  [ cor : rev - lyap ] for all real @xmath135 , may equivalently be rewritten in terms of @xmath138 as @xmath139 alternatively , if we start with proposition  [ prop : rev - lyap ] and apply ( [ eq : rev ] ) with @xmath106 , @xmath107 , @xmath108 ( @xmath140 ) , we get @xmath141 with constants @xmath142 .",
    "equivalently , @xmath143 in the class of log - concave @xmath138 of order @xmath70 .",
    "asymptotically @xmath144 , as @xmath145 , so the bound ( [ eq : var2 ] ) is very close to ( [ eq : var1 ] ) for large values of @xmath70 .",
    "let @xmath138 have a gamma distribution with shape parameter @xmath70 ( where @xmath101 is real ) , that is , with density @xmath146 it is log - concave if and only if @xmath135 , in which case @xmath70 will be the order of log - concavity for this distribution .",
    "note that @xmath147 , and ( [ eq : var1 ] ) becomes equality .",
    "hence , the factor @xmath148 in ( [ eq : var1 ] ) is optimal .",
    "[ prop : var - p ] if @xmath149 has a log - concave distribution of order @xmath135 , then @xmath150 equality is attained at the gamma distribution with shape parameter @xmath70 .",
    "write the density of @xmath138 as @xmath151 with log - concave @xmath137 .",
    "one may assume that @xmath137 is a density , as well .",
    "indeed , otherwise consider random variables @xmath152 @xmath153 .",
    "then @xmath154 and @xmath155 has density @xmath156 where @xmath157 .",
    "since @xmath2 decays at infinity exponentially fast , the same is true for @xmath137 .",
    "hence , @xmath137 is integrable , and one can choose @xmath113 such that @xmath158 .",
    "so the reduction to the case where @xmath137 is a density is achieved .",
    "thus , let @xmath137 be a log - concave probability density , such that @xmath151 is the density of @xmath138 .",
    "consider a random variable @xmath96 with density @xmath137 .",
    "then , by corollary  [ cor : rev - lyap ] , the function @xmath159 is concave .",
    "differentiating twice with respect to @xmath160 , we get @xmath161 but at the point @xmath162 , we have @xmath163 and so @xmath164 proposition  [ prop : var - p ] is proved .",
    "it is to be noted that the right - hand side in proposition  [ prop : var - p ] is the trigamma function , which has the alternate representation @xmath165 and behaves like @xmath148 for large values of @xmath70 .",
    "hence , @xmath166 with some absolute constant @xmath27 ( in fact , one may take @xmath167 ) .",
    "this can also be seen by using proposition  [ prop : rev - lyap ] .",
    "indeed , the same argument as above yields @xmath168 which holds for any @xmath169 .",
    "here the right - hand side has an incorrect behavior when @xmath70 is close to 1 .",
    "in fact , for all log - concave @xmath138 , we have @xmath170 with some absolute constant @xmath27 . for the proof",
    ", one can apply , for example , borell s concentration lemma ( @xcite , lemma 3.1 ) .",
    "together with ( [ eq : lc - conc ] ) , ( [ eq : lcp - conc2 ] ) also yields the bound ( [ eq : lcp - conc ] ) .    in the proof of theorem  [ thm : exp ] , we use the values @xmath171 , the dimension of the space .",
    "since the one - dimensional case can be treated separately ( rather easily ) , the assumption @xmath110 can be made in applications .",
    "[ rmk : aff ] the notion of a log - concave measure of order @xmath70 may be extended in a natural way to the class of one - dimensional log - concave probability measures @xmath172 on @xmath3 .",
    "more precisely , we say that @xmath172 has order @xmath70 , if @xmath172 is supported on some interval @xmath173 , bounded or not , and has a density there of the form @xmath174 where @xmath175 is a positive affine function on @xmath176 , @xmath137 is log - concave on @xmath176 , and where @xmath9 stands for the lebesgue measure on this interval . in this case",
    ", the inequality ( [ eq : lcp - conc ] ) and other similar results should be properly read in terms of @xmath175 .",
    "for example , we have @xmath177 with respect to @xmath172 .",
    "it is natural to try to sharpen proposition  [ prop : var - p ] and the resulting asymptotic bound ( [ eq : lcp - conc ] ) in terms of deviations of @xmath178 from its mean or quantiles .",
    "let @xmath149 be a random variable with log - concave distribution of order @xmath179 , that is , with density of the form @xmath180 where @xmath116 and @xmath137 is a log - concave function .",
    "let @xmath181 be an independent copy of @xmath138 .",
    "then for all @xmath182 $ ] , @xmath183    the quantity @xmath184 does not change if we multiply @xmath138 and @xmath181 by a positive scalar . hence , as in the proof of proposition  [ prop : var - p ] , we may assume that @xmath137 is a probability density of some random variable , say , @xmath99 . applying jensen s inequality ,",
    "we thus conclude that @xmath185 provided that @xmath186 ( which means that @xmath2 is a density ) .",
    "but by the reverse lyapunov s inequality of corollary [ cor : rev - lyap ] , applied with @xmath187 , @xmath107 , @xmath188 , we obtain that @xmath189 note that when @xmath190 , this inequality returns us to inequality ( [ eq : var1 ] ) .",
    "thus , from ( [ eq : log - mid ] ) , @xmath191    the right - hand side here seems perhaps not quite convenient to deal with , especially when @xmath192 are not integer .",
    "alternatively , it might be better to use proposition [ prop : rev - lyap ] , which gives @xmath193 indeed , write @xmath194 the first factor on the right may be bounded just by 1 . for the second one , using @xmath195 ( @xmath196 ) , one has @xmath197 therefore , we have a preliminary gaussian estimate : @xmath198    similarly , one may also obtain a one - sided estimate , since like in inequality ( [ eq : log - mid ] ) we also have @xmath199 provided that @xmath186 . these estimates are collected below after replacing @xmath70 by @xmath200 for convenience .",
    "[ lem : logconc ] if @xmath149 has a log - concave distribution of order @xmath201 , then @xmath202    in particular , we obtain for log - concave densities of order @xmath70 on the positive half - line a @xmath70-dependent version of proposition  [ prop:1d ] ( which was stated for log - concave densities on the line ) .",
    "[ cor : log ] if @xmath149 has a log - concave distribution of order @xmath135 , then @xmath203    first , assume @xmath110 and choose @xmath204 in ( [ eq : log1 ] ) with @xmath205 ( so that @xmath206 ) .",
    "then , using @xmath207 , we have @xmath208 taking , for example , @xmath209 , the right - hand side will not exceed @xmath210 .",
    "hence , @xmath211    for the remaining range @xmath212 , one has @xmath213 , and we have by proposition  [ prop:1d ] [ or more precisely , inequality ( [ eq : gen - alpha ] ) ] that @xmath214 thus , the desired statement is proved with a uniform bound of 3 .",
    "observe that proposition  [ prop:1d ] corresponds to @xmath111 , and that while it clearly applies as stated to log - concave densities of order @xmath70 ( since these are subclasses of the log - concave densities ) , corollary  [ cor : log ] with the additional @xmath215 term in the exponent provides the correct generalization for large @xmath70 .",
    "to reduce theorems  [ thm : exp ] and  [ thm : gaus ] to a specific statement about dimension one ( in fact  about log - concave distributions of order @xmath216 ) , we apply a localization argument of lovsz and simonovits  @xcite .",
    "more precisely , we need one variant of the localization lemma , proposed in  @xcite , corollary 2.4 , which we state with minor modification as a lemma .    [",
    "lem : ls ] let @xmath137 and @xmath217 be integrable continuous functions on a bounded open convex set @xmath218 in @xmath3 , such that @xmath219 then for some interval @xmath220 and a positive affine function @xmath175 on @xmath176 , @xmath221 where the integrals are with respect to lebesgue measure on @xmath176 .",
    "equivalently , given that @xmath222 , if for all couples @xmath223 with @xmath224 , we have that @xmath225 then @xmath226 this formulation enables the desired - dimensional reduction .",
    "[ lem : ls - red ] suppose @xmath1 is a random vector taking values in an open convex set @xmath218 in @xmath3 , where it has a positive continuous density @xmath2 , such that @xmath227 is finite .",
    "let @xmath228 denote a probability measure on a line segment @xmath229 with density @xmath230 where @xmath175 is a positive affine function , defined on @xmath176 , and @xmath231 is a normalizing constant .",
    "given @xmath232 and @xmath233 , if for any such one - dimensional measure @xmath228 , we have @xmath234 where @xmath235 stands for the expectation with respect to @xmath236 , then @xmath237    without loss of generality , take @xmath218 to be bounded , and assume that @xmath238 , or in other words , @xmath239 in this case , ( [ eq : then - n ] ) becomes @xmath240 with @xmath241 hence , to derive ( [ eq : then - ag ] ) under ( [ eq : ent0 ] ) , it suffices to take an arbitrary interval @xmath220 and a positive affine function @xmath175 on @xmath176 , such that @xmath242 and to show that latexmath:[\\[\\label{eq : alm2 } \\int_\\delta\\bigl(e^{({\\alpha}/{\\sqrt{n } } )     using the definition of @xmath228 , inequalities ( [ eq : alm1 ] ) and ( [ eq : alm2 ] ) take the form @xmath244 which can be written together as ( [ eq : if-1 ] ) .",
    "keeping the same notation as in the previous section , first note that @xmath245 so @xmath246 by convexity of the functional @xmath247 , we have that @xmath248\\\\[-8pt ] & & { } + \\tfrac{1}{2 } \\log{\\mathbf{e } } _ { \\ell } e^{({\\alpha(n-1)}/{\\sqrt{n } } ) |{\\log\\ell } - { \\mathbf{e}}_\\ell\\log\\ell|}.\\nonumber\\end{aligned}\\ ] ]      to estimate the second expectation in ( [ eq : collect ] ) , it is useful to note that @xmath228 has order @xmath216 ( cf .",
    "remark  [ rmk : aff ] ) .",
    "if @xmath252 , this expectation is just 1 . if @xmath253 , by the inequality ( [ eq : log1 ] ) of lemma  [ lem : logconc ] , we have @xmath254 provided that @xmath255 .",
    "this bound automatically holds for @xmath252 , as well .    collecting the bounds ( [ eq : piece1 ] ) and ( [ eq : piece2 ] ) in ( [ eq : collect ] )",
    ", we get that , for all @xmath250 , @xmath256 hence , using @xmath257 ( to simplify the constant ) , @xmath258 now , replace @xmath259 with @xmath260 .",
    "we then get that @xmath261 recalling lemma  [ lem : ls - red ] ( whose assumptions hold for all log - concave densities ) , we arrive at the following theorem .          by applying chebyshev s inequality ,",
    "we arrive at theorem  [ thm : exp ] with @xmath21 . from theorem  [ thm : mgf ] , by chebyshev s inequality , we also have @xmath268 provided that @xmath269 .",
    "taking the optimal value @xmath270 gives theorem  [ thm : gaus ] ."
  ],
  "abstract_text": [
    "<S> a concentration property of the functional @xmath0 is demonstrated , when a random vector @xmath1 has a log - concave density @xmath2 on @xmath3 . </S>",
    "<S> this concentration property implies in particular an extension of the shannon  mcmillan  </S>",
    "<S> breiman strong ergodic theorem to the class of discrete - time stochastic processes with log - concave marginals .    and    .    </S>"
  ]
}