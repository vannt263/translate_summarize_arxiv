{
  "article_text": [
    "consider a symmetric positive definite ( spd ) matrix @xmath2 , which is also assumed to be large and sparse .",
    "we will denote as @xmath3 the eigenvalues of @xmath2 and @xmath4 the corresponding ( normalized ) eigenvectors .",
    "the computation of the @xmath5 leftmost eigenpairs of @xmath2 is a common task in many scientific applications .",
    "typical examples are offered by the vibrational analysis of mechanical structures @xcite , the lightwave technology  @xcite , electronic structure calculations @xcite , and the spectral superposition approach for the solution of large sets of 1st order linear differential equations  @xcite .",
    "computation of a few eigenpairs is also crucial in the approximation of the generalized inverse of the graph laplacian  @xcite .    in this paper",
    "we propose to use an efficiently preconditioned newton method for the nonlinear system of equations : @xmath6 is the rayleigh quotient .",
    "the idea of employing the newton method for this nonlinear system is obviously not new : among the others we mention davidson ( @xcite ) who approximated the jacobian of ( [ nonlinear ] ) with @xmath7 and combined this system solution with a rayleigh - ritz procedure .    the newton method in the unit sphere @xcite or newton - grassman method , constructs a sequence of vectors @xmath8 by solving the linear systems @xmath9 ensuring that the correction @xmath10 be orthogonal to @xmath11 .",
    "then the next approximation is set as @xmath12 where @xmath13 .",
    "linear system ( [ gras ] ) is shown to be better conditioned than the one with @xmath14 .",
    "the same linear system represents the _ correction equation _ in the well - known jacobi - davidson method  @xcite , which in its turn can be viewed as an accelerated inexact newton method @xcite .",
    "when @xmath2 is spd and the leftmost eigenpairs are being sought , it has been proved in @xcite that the preconditioned conjugate gradient ( pcg ) method can be employed in the solution of the correction equation .",
    "there are still a number of drawbacks that advises against using pure newton method : first , the choice of an initial vector . in the jacobi - davidson algorithm",
    "the rayleigh - ritz procedure implements a sort of restart that in part solves this problem .",
    "second , even the projected jacobian @xmath15 in the correction equation is ill - conditioned , or at least more ill - conditioned than the original matrix @xmath2 , being its smallest eigenvalue smaller than @xmath16 when seeking the @xmath17-th eigenpair ( see lemma [ lemmanotay ] ) . in @xcite the problem of finding a `` well - conditioned '' jacobian matrix for the newton method is considered by the authors , who describe some low - rank variants of the jacobian of ( [ nonlinear ] ) and perform a large set of numerical experiments showing that the best choice is problem dependent .",
    "many authors have also tried to find a good preconditioner for matrix @xmath14 since it is the key for efficient iterative solution of the correction equation .",
    "this remains an open problem ( see for example @xcite ) .",
    "starting from the findings in @xcite and @xcite , the main contribution of this paper is the development of a sequence of preconditioners @xmath18 for the pcg solution of the newton correction equation ( [ gras ] ) , based on the bfgs update of a given initial preconditioner for the coefficient matrix @xmath2 .",
    "we will theoretically prove that the sequence of the preconditioned jacobians will remain close to the identity matrix if the first preconditioned jacobian is so .",
    "a similar approach has been used in @xcite where a rank - two modification of a given preconditioner is used to accelerated minres in the framework of the inexact rayleigh quotient iteration .    the bfgs formula as used in this paper is one more example of the strict connection between two overlapping worlds : numerical linear algebra and optimization .",
    "many papers have discussed this relationship . among the others we refer to  @xcite and the references therein .",
    "also the problem of finding efficient preconditioners for the linearized systems has become a crucial issue for the efficient implementation of the interior point method in constrained optimization , see e.g. @xcite . often , the coefficient matrices of the linear systems to be solved at each newton iteration are very close in structure and this motivates a number of works which study the possibility of updating a given preconditioner to obtain with small computational effort a new preconditioner @xcite .",
    "to overcome the problem of the starting point , we also propose to start the newton process after a small number of iterations of a conjugate gradient procedure for the minimization of the rayleigh quotient ( dacg , @xcite ) to yield a good initial vector .",
    "the combined dacg - newton algorithm is used in the approximation of @xmath19 $ ] eigenpairs of a number of matrices arising from various realistic applications of size up to @xmath20 and number of nonzeros up to @xmath21 .",
    "numerical results show that , in the solution of the correction equation , the pcg method preconditioned by bfgs displays much faster convergence than the same method when the preconditioner is kept fixed during the newton process , in every test case . moreover , the proposed approach is shown to be competitive with the jacobi - davidson method .    the remaining of the paper proceeds as follows : in section 2 we introduce the preconditioner ; section 3 is devoted to the proof of the main theorem which states the closeness of the preconditioned matrix to the identity matrix . in section 4",
    "we describe implementation details while section 5 reports some numerical results of the proposed method for the eigensolution of the test matrices .",
    "section 6 reports comparisons against the jacobi - davidson method and section 7 draws the conclusions .",
    "following the idea described in @xcite , we propose a sequence of preconditioners for the newton systems using the bfgs rank - two update . to precondition the initial newton system @xmath22 , where @xmath23 we chose to use a projected incomplete cholesky preconditioner with partial fill - in  @xcite :    @xmath24 with @xmath25 being",
    "@xmath26 an incomplete triangular cholesky factor of @xmath2 , with parameters @xmath27 , maximum fill - in of a row in @xmath28 , and @xmath29 the threshold for dropping small elements in the factorization .",
    "then a sequence of projected preconditioners for the subsequent linear systems @xmath30 may be defined by using the bfgs formula as : @xmath31 and @xmath32 is the solution of the @xmath33-th newton system whereas @xmath34 .",
    "we propose here a simplification of the preconditioner update formula based on the well - known cubic convergence of the newton method which implies that @xmath35 , in a suitable neighborhood of the solution ( i.e. for a suitable @xmath36 s.t .",
    "@xmath37 ) . as a consequence",
    "also the residual norm satisfies @xmath38 .",
    "we can then approximate @xmath39 with @xmath40 and write the preconditioner at level @xmath41 as ( with @xmath42 ) : @xmath43 theorem [ spd ] of next section will prove that the preconditioner defined in ( [ lastprec ] ) is spd if @xmath44 is so .",
    "the idea of the bfgs preconditioner is taken from the general analysis in @xcite where a sequence of preconditioners is devised in order to precondition the sequence of newton systems for a general nonlinear problem .",
    "one of the `` standard assumptions '' made in these papers was the nonsingularity of the jacobian in the solution of the nonlinear system .",
    "here the situation is different , the jacobian in the correction equation @xmath45 is singular whatever @xmath46 , in particular it is singular when @xmath46 is equal to the exact eigenvector .",
    "the theoretical analysis of the goodness of the preconditioner will be therefore completely different , though obtaining similar results , than that proposed in  @xcite .",
    "we start by recalling some known results about convergence of the newton method for eigenproblems . at every step of our newton method",
    "we approximately solve @xmath47 where @xmath48 , in the space orthogonal to @xmath11 .",
    "then we set @xmath49 in view of @xmath50 and @xmath51 , where we have defined @xmath52 .",
    "the above mentioned newton iteration is shown to converge cubically if the correction equation is solved exactly . since this is not the case when it is iteratively solved , we simply assume convergence , namely that for a suitable @xmath53 such that @xmath54 there is a constant @xmath55 such that @xmath56    * notation*. + in the sequel we will indicate as @xmath57 the exact eigenvector corresponding to the smallest exact eigenvalue @xmath58 . the error vector at step @xmath33 is denoted by @xmath59 , while the error in the eigenvalue approximation is @xmath60 .",
    "it is easily proved that there is a constant @xmath61 independent of @xmath33 such that @xmath62 with @xmath63 we mean the largest modulus eigenvalue of @xmath2 while @xmath64 refers to a generic eigenvalue of matrix @xmath2 .",
    "as the matrix norm of a symmetric matrix @xmath2 we will use the euclidean norm @xmath65 .    [ rem1 ] at first sight the jacobian matrix in the correction equation is singular , but this does not matter since the pcg algorithm is run within the subspace of vectors orthogonal to @xmath11 ( in fact also @xmath66 .",
    "thus , notion of positive definiteness , eigenvalue distribution , condition number , norms , etc , apply as usual but with respect to matrices restricted to this subspace .",
    "the following lemma will bound the extremal eigenvalues of @xmath67 in the subspace orthogonal to @xmath11 .",
    "[ lemmanotay ] there is a positive number @xmath36 such that if @xmath68 then @xmath69 is spd in the subspace orthogonal to @xmath11 .",
    "moreover the following bounds hold : @xmath70 for every unit norm vector @xmath71 orthogonal to @xmath11 .    from lemma 3.1 in @xcite and the definition of @xmath72 , we have that @xmath73 . now using ( [ varep ] ) , @xmath74 showing that @xmath67 is spd .",
    "the upper bound for the eigenvalues of @xmath67 is straightforward .",
    "the previous lemma allows us to prove that the preconditioner defined in ( [ lastprec ] ) is spd , as stated in the following theorem .",
    "[ spd ] if the correction equation is solved exactly , then any matrix @xmath75 defined by ( [ lastprec ] ) is spd and hence @xmath76 is spd in the subspace orthogonal to @xmath11 .    the proof is carried out by induction .",
    "@xmath77 is spd being an incomplete cholesky factorization of the spd matrix @xmath2 , then from @xmath78 , we can write @xmath79 we define @xmath80 and note that , by previous lemma , @xmath81 since @xmath82 .",
    "let now @xmath44 be spd by induction hypothesis , then for every @xmath83 , @xmath84 which proves that @xmath85 is spd .",
    "if we now take @xmath86 , we have @xmath87 which completes the proof .",
    "let us define the difference between the preconditioned jacobian and the identity matrix as @xmath88 since by definition we have @xmath89 then @xmath11 is the eigenvector of @xmath67 corresponding to the zero eigenvalue .",
    "hence , since also @xmath90 the error @xmath91 can also be defined as @xmath92 the following technical lemma will bound the norm of @xmath93 in terms of that of @xmath94 .",
    "being @xmath44 spd we can define its norm in the space orthogonal to @xmath11 as @xmath95    [ lemma1 ] there is a positive number @xmath36 such that if @xmath68 then @xmath96    let @xmath97 , @xmath98 , @xmath99 .",
    "then since @xmath67 ( and thus @xmath100 ) is spd in the subspace orthogonal to @xmath11 the linear system @xmath101 has a unique solution @xmath71 such that @xmath102",
    ". therefore @xmath103",
    "the next lemma will relate the norms of the difference @xmath10 and of the norm of the error vector @xmath104 :    [ lemma3 ] there exists a positive number @xmath36 s.t .",
    "if @xmath68 then @xmath105    from ( [ beta ] ) we have @xmath106 hence , taking norms and using ( [ ekp1 ] ) , @xmath107 the last 2nd order inequality , when solved for @xmath108 gives @xmath109 choosing @xmath110 implies @xmath111 , which , combined with ( [ fs ] ) provides the desired result .",
    "now we need to prove that the distance between two consecutive jacobians is bounded by a constant times the error vector :    [ lemma4 ] there exists a positive number @xmath36 s.t . if @xmath68 then @xmath112 for a suitable constant @xmath113 .",
    "@xmath114    where we set @xmath115 . the first term in ( [ eq22 ] ) can be bounded , using ( [ varep ] ) , as @xmath116 to bound @xmath117 recall that @xmath118 therefore @xmath119 with @xmath120 , by lemma [ lemma3 ] , then @xmath121 whence , using ( [ theta ] ) and again lemma [ lemma3 ] , @xmath122 combining ( [ delta ] ) , ( [ theta ] ) and ( [ g ] ) we have : @xmath123 setting @xmath124 completes the proof .    before stating theorem [ theo ] we need to prove as a last preliminary result that also the difference between the square root of two consecutive jacobians is bounded in terms of the norm of the error vector :    [ lemma5 ] let @xmath125 .",
    "then there is a positive number @xmath36 s.t . if @xmath68 then @xmath126 for a suitable constant @xmath127 .    by squaring the equation @xmath128",
    "we obtain @xmath129 let now @xmath130 be a normalized eigenvector of the symmetric matrix @xmath131 such that @xmath132 .",
    "premultiplying by @xmath133 and postmultiplying by @xmath130 equation ( [ 2nds ] ) yields @xmath134 this quadratic equation has two solutions : @xmath135 the smallest solution is not an eigenvalue of @xmath136 since from the definition of @xmath136 , an eigenvalue @xmath137 of @xmath136 would satisfy @xmath138 then , considering the largest solution of ( [ 2ndord ] ) @xmath139 let @xmath140 be the eigenpair corresponding to the largest modulus eigenvalue of @xmath131 .",
    "then @xmath141    we are finally ready to prove the main results of this section .",
    "the following theorem will state the so called _ bounded deterioration",
    "_ @xcite of the preconditioner at step @xmath41 with respect to that of step @xmath33 , namely that the distance of the preconditioned matrix from the identity matrix at step @xmath41 is less or equal than that at step @xmath33 plus a constant that may be small as desired , depending on the closeness of @xmath142 to the exact solution .",
    "[ theo ] let @xmath143 be such that @xmath144 , there is a positive number @xmath36 s.t . if @xmath54 then @xmath145 for a suitable constant @xmath146 .",
    "the distance of the preconditioned jacobian from the identity matrix can be written as follows , where we have defined @xmath147 : @xmath148 now set @xmath149 and @xmath150 ; @xmath151 is an orthogonal projector since @xmath152 . then @xmath153 to bound @xmath154 we will use lemma [ lemma1 ] and lemma [ lemma5 ] : @xmath155 now taking norms in ( [ ekp1 ] ) yields @xmath156 which can be rewritten as @xmath157 from ( [ recur ] ) , we derive a bound for @xmath158 .",
    "if @xmath159 then @xmath160    again from ( [ recur ] ) and using the bound ( [ bounde ] ) we finally have @xmath161 setting @xmath162 completes the proof .",
    "it is more usual to evaluate the goodness of a preconditioner by bounding the extremal eigenvalues of the preconditioned matrix ( if spd ) instead of using norms .",
    "however , it is worth observing that the initial preconditioner can be selected so as to give @xmath163 . in such case ,",
    "in the most common situation we would have @xmath164 so that minimizing @xmath158 is the same as maximizing the smallest eigenvalue of the preconditioned matrix .",
    "when seeking an eigenvalue different from @xmath58 , say @xmath165 , the jacobian matrix changes as @xmath166 where @xmath167     $ ] is the matrix whose first @xmath17 columns are the previously computed eigenvectors .",
    "analogously , also the preconditioner must be chosen orthogonal to @xmath168 as @xmath169 the theoretical analysis developed in the previous section applies with small technical variants also in this case since it is readily proved that @xmath170 .",
    "the most significant changes regard the definition of @xmath171 and the statement of lemma [ lemmanotay ] ( and the proof of lemma [ lemma4 ] that uses its results ) , namely the bound for the smallest eigenvalue of @xmath67 which in the general case becomes : @xmath172 for every unit norm vector @xmath71 such that @xmath173 .",
    "as mentioned in section 1 , another important issue in the efficiency of the newton approach for eigenvalue computation is represented by the appropriate choice of the initial guess .",
    "we propose here to perform some preliminary iterations of another eigenvalue solver , in order to start the newton iteration ` sufficiently ' close to the exact eigenvector .",
    "we chose as the ` preliminary ' eigenvalue solver dacg @xcite , which is based on the preconditioned conjugate gradient ( nonlinear ) minimization of the rayleigh quotient .",
    "this method has proven very robust , and not particularly sensitive to the initial vector , in the computation of a few eigenpairs of large spd matrices .      in this section",
    "we give the main lines of the implementation of the product of our preconditioner times a vector , which is needed when using a preconditioned krylov method . at a certain nonlinear iteration level , @xmath33",
    ", we need to compute @xmath174 where @xmath175 is the residual of the linear system at iteration @xmath176 .",
    "let us suppose we compute an initial preconditioner @xmath177 .",
    "then , at the initial nonlinear iteration @xmath178 , we simply have @xmath179 . at step",
    "@xmath41 the preconditioner @xmath85 is defined recursively by ( [ lastprec ] ) while @xmath180 using ( [ precq ] ) can be written as @xmath181 to compute vector @xmath182 first we observe that @xmath175 is orthogonal to @xmath168 so there is no need to apply matrix @xmath183 on the right of ( [ bfgs ] ) .",
    "application of preconditioner @xmath85 to the vector @xmath175 can be performed at the price of @xmath184 dot products and @xmath184 daxpys as described in algorithm [ bfgspk ] .",
    "the scalar products @xmath185 which appear at the denominator of @xmath85 , can be computed once and for all before starting the solution of the @xmath186-th linear system .",
    "last , the obtained vector @xmath182 must be orthogonalized against the columns of @xmath168 by a classical gram - schimdt procedure .",
    "= 1.0em=0.0em=0.5em    input : vector @xmath175 , scalar products @xmath187 .    @xmath188    for @xmath189  to   @xmath190    .",
    "= 3.0em=0.1em=-0.5em    @xmath191    @xmath192    end for    @xmath193    for @xmath194  to   @xmath195    .",
    "= 3.0em=0.1em=-0.5em    @xmath196    @xmath197    end for    @xmath198    @xmath199    [ bfgspk ]      as a krylov subspace solver for the correction equation we chose the preconditioned conjugate gradient ( pcg ) method since the jacobian @xmath67 has been shown to be spd in the subspace orthogonal to @xmath11 . regarding the implementation of pcg",
    ", we mainly refer to the work @xcite , where the author shows that it is possible to solve the linear system in the subspace orthogonal to @xmath11 and hence the projection step needed in the application of @xmath67 can be skipped .",
    "moreover , we adopted the exit strategy for the linear system solution described in the above paper , which allows for stopping the pcg iteration , in addition to the classical exit test based on a tolerance on the relative residual and on the maximum number of iterations , whenever the current solution @xmath200 satisfies @xmath201 or when the decrease of @xmath202 is slower than the decrease of @xmath203 , because in this case further iterating does not improve the accuracy of the eigenvector .",
    "note that this dynamic exit strategy implicitly defines an inexact newton method since the correction equation is not solved `` exactly '' i.e. up to machine precision .",
    "we have implemented the pcg method as described in algorithm 5.1 of @xcite with the obvious difference in the application of the preconditioner which is described here in algorithm [ bfgspk ] .",
    "the bfgs preconditioner defined in algorithm [ bfgspk ] suffers from two main drawbacks , namely increasing costs of memory for storing @xmath10 and @xmath204 , and the increasing cost of preconditioner application with the iteration index @xmath33 .",
    "note that these drawbacks are common to many iterative schemes , such as for example sparse ( limited memory ) broyden implementations  @xcite , gmres  @xcite and arnoldi method for eigenvalue problems  @xcite .",
    "there are different ways to overcome these difficulties , all based on variations of a restart procedure , that is , the iteration scheme is reset after a fixed number of iterations . if the number of nonlinear iterations is high ( e.g. more than ten iterations ) , the application of bfgs preconditioner may be too heavy to be counterbalanced by a reduction in the iteration number . to this aim",
    "we define @xmath205 the maximum number of rank two corrections we allow . when the nonlinear iteration counter @xmath33 is larger than @xmath205 , the vectors @xmath206 are substituted with the last computed @xmath207 .",
    "vectors @xmath208 are stored in a matrix @xmath209 with @xmath210 rows and @xmath211 columns .",
    "the implementation of our dacg - newton method for computing the leftmost eigenpairs of large spd matrices is described in algorithm [ dacg - newton ] .",
    "= 1.0em=0.0em=0.5em    input :    1 .",
    "@xmath2 ; 2 .",
    "@xmath212 ; 3 .",
    "@xmath213 , itmax ; 4 .",
    "@xmath214 ; 5 .",
    "@xmath215 , itmax@xmath216 ; 6 .   ,",
    "lfil and @xmath29 ; 7 .   :",
    "@xmath205 .    @xmath217 $ ] .",
    "compute an incomplete cholesky factorization of @xmath2 : @xmath77 with parameters @xmath218 and @xmath29 .",
    "for @xmath219  to   @xmath212    .",
    "= 3.0em=0.1em=-0.5em    choose @xmath220 such that @xmath221 .",
    "compute @xmath142 , an approximation to @xmath222 by the dacg procedure with initial vector @xmath220 , preconditioner @xmath77 and tolerance @xmath214 .",
    "@xmath223 , @xmath224 .    while   @xmath225  and  @xmath226  do    .",
    "= 3.0em=0.1em=-0.5em    @xmath227.$ ]    solve @xmath228 for @xmath229 by the pcg method with preconditioner @xmath76 and tolerance @xmath215 .",
    "@xmath230 ,  @xmath231 .",
    "@xmath232 mod @xmath205 ; @xmath233    @xmath234    end while    assume @xmath235 and @xmath236 .",
    "set @xmath237 $ ]    end for    the above described implementation is well suited to parallelization provided that an efficient matrix - vector product routine is available .",
    "the bottleneck is represented by the high number of scalar products which may worsen the parallel efficiency when a very large number of processor is employed .",
    "preliminary numerical results are encouraging as documented in @xcite .",
    "in this section we provide numerical results which compare the performance of the dacg - newton algorithm for various @xmath205 values .",
    "we tested the proposed algorithm in the computation of the 20 smallest eigenpairs of a number of small to very large matrices arising from various realistic applications .",
    "the list of the selected problems together with their size @xmath210 , and nonzero number @xmath238 is reported in table [ list ] , where ( m)fe stands for ( mixed ) finite elements .",
    "[ list ]    .main characteristics of the matrices used in the tests .",
    "[ cols=\"<,<,>,>\",options=\"header \" , ]     [ em1 ]     ( top figure ) , @xmath239 ( middle figure ) and @xmath240 ( bottom figure ) . ]",
    "( top figure ) , @xmath239 ( middle figure ) and @xmath240 ( bottom figure ) . ]     ( top figure ) , @xmath239 ( middle figure ) and @xmath240 ( bottom figure ) . ]    regarding problem emilia-923 we provide in figure [ em1 ] the plot of the relative residual norm vs cumulative linear iteration as per equation ( [ residual ] ) for both dacg - newton method ( newton phase only ) and jd , corresponding to levels @xmath241 and @xmath242 .",
    "we can appreciate the very similar convergence profiles of these two methods .",
    "the dacg - newton algorithm is faster for lower @xmath17-values while the opposite holds for high values of @xmath17 where the rayleigh - ritz projection seems to win against the bfgs acceleration .",
    "we have developed and theoretically analyzed a sequence of preconditioners aiming at accelerating the pcg method in the solution of the correction equation .",
    "this equation is to be solved at each newton iteration to approximate a few eigenpairs of an spd matrix .",
    "both theoretical analysis and experimental results onto a heterogeneous set of test matrices reveal that the bfgs sequence of preconditioners greatly improves the pcg efficiency as compared to using an initially evaluated fixed preconditioner .",
    "the dacg - newton method with the aforementioned preconditioner proves a robust and efficient algorithm for the partial eigensolution of spd matrices and makes `` pure '' newton method competitive with jacobi - davidson without making use of any rayleigh - ritz projection . on the average",
    "the latter method proves a little bit more performing than the one proposed in this work , and this is mainly due to the excessive dacg preprocessing time to devise a good initial vector for the subsequent newton phase .",
    "however , we wonder whether the preconditioning technique studied in this work may be seen as an alternative to jacobi - davidson or , rather , a possible improvement of it . at present",
    "we have neither theoretical nor experimental evidence in favor of this second option .",
    "we therefore let as future work the attempt to insert our preconditioner in the framework of the jacobi - davidson method .",
    "we will also compare the proposed algorithm with the recent implementations of inexact arnoldi s ( lanczos ) method @xcite where the inner linear system is solved with a variable accuracy depending on the closeness to the wanted eigenvector .    the sequence of linear systems ( [ gras ] ) , the correction equation , has in common with the normal equations to be solved at each interior point iteration the fact that the matrices involved get ill - conditioned as the iteration proceeds .",
    "the bfgs sequence of preconditioners developed in this paper is expected to perform well also for preconditioning the normal equations since :    1 .",
    "the bounded deterioration property , proved in theorem [ theo ] , is expected to mitigate the ill - conditioning of the linear systems toward the interior point solution .",
    "the approach described in the previous sections allows to perform only a ( either complete or inexact ) factorization of the initial jacobian , thus saving on the cost of subsequent factorizations which is know to represent the main computational burden @xcite of the whole interior point method for large and sparse constrained optimization problems .                                                                              , _ inexact newton preconditioning techniques for large symmetric eigenvalue problems _ , electron . trans .",
    ", 7 ( 1998 ) , pp .",
    "202214 ( electronic ) .",
    "large scale eigenvalue problems ( argonne , il , 1997 ) ."
  ],
  "abstract_text": [
    "<S> in this paper we propose an efficiently preconditioned newton method for the computation of the leftmost eigenpairs of large and sparse symmetric positive definite matrices . </S>",
    "<S> a sequence of preconditioners based on the bfgs update formula is proposed , for the preconditioned conjugate gradient solution of the linearized newton system to solve @xmath0 , @xmath1 being the rayleigh quotient . </S>",
    "<S> we give theoretical evidence that the sequence of preconditioned jacobians remains close to the identity matrix if the initial preconditioned jacobian is so . </S>",
    "<S> numerical results onto matrices arising from various realistic problems with size up to one million unknowns account for the efficiency of the proposed algorithm which reveals competitive with the jacobi - davidson method on all the test problems .    * key words*. eigenvalues , spd matrix , newton method , bfgs update , incomplete cholesky preconditioner    ams subject classifications . 65f05 , 65f15 , 65h17 </S>"
  ]
}