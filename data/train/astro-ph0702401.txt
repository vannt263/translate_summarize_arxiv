{
  "article_text": [
    "astronomers commonly seek to estimate the _ space density _ of objects , and a sky survey such as the sloan digital sky survey ( sdss ) @xcite can yield a representative sample useful for this purpose , due to the assumed isotropy of the universe .",
    "figure [ rawdata ] depicts redshift and absolute magnitude measurements for a sample of quasars given in @xcite .",
    "these are a subset of the sdss quasar sample ( data release 3 ) , chosen to be statistically valid for purposes such as exploring the evolution with redshift of the luminosity function , i.e. the space density of quasars as a function of absolute magnitude .",
    "this paper describes a new method for estimating these luminosity functions , and presents results from the analysis of this quasar sample .",
    "for the purposes of the statistical inference problem , imagine the dots in figure [ rawdata ] as observations of bivariate data @xmath7 from some distribution with probability density @xmath2 , i.e. the probability that a randomly chosen quasar falls in a region @xmath8 is @xmath9 .",
    "( equivalently , in a sample of size @xmath10 , one expects that @xmath11 will fall in the region @xmath8 . )",
    "hence , the luminosity function at redshift @xmath12 is , up to a multiplicative constant , the cross - section of the bivariate density at @xmath12 , denoted @xmath13 .",
    "the main challenge is estimation of this bivariate density given truncated data .",
    "only objects with apparent magnitude within some range are observable . when this bound on apparent magnitude is transformed into a bound on absolute magnitude , @xmath14 , @xmath15 is assumed when making this transformation . ]",
    ", the truncation bound takes an irregular shape , varying with redshift .",
    "@xmath16-corrections further complicate this boundary , leading to the dashed region in figure [ rawdata ] .",
    "also , the sample is not assumed to be complete within this region , and the probability of observing an object will vary with position on the sky , along with other factors . incorporating this _ selection function _ into the analysis is a secondary challenge .",
    "nonparametric estimators are advantageous in cases where either there does not exist a commonly agreed upon parametric physical model , or there is a desire to validate a parametric model .",
    "see @xcite for an overview of the potential of nonparametric methods in astronomy and cosmology .",
    "a fully nonparametric approach is not possible here , since some assumptions must be placed on the form of the density in order to infer its shape over the unobservable region . under such conditions , one approach would be to fit a sequence of increasingly complex parametric models in an attempt to obtain a good fit to the data .",
    "a less subjective alternative is a _ semiparametric _ approach which merges a nonparametric method with sufficient structure from a parametric form to obtain useful results .",
    "this work describes a semiparametric approach to estimating the bivariate density , and hence the luminosity functions , under irregular truncation .",
    "this is a long - standing challenge in astronomical data analysis , with a variety of proposed methods .",
    "interesting qualitative and simulations - based comparisons between different approaches can be found in @xcite and @xcite .",
    "a parametric model fit using maximum likelihood is a common choice , since it addresses the truncation bias in a natural manner ; see , for instance , @xcite , @xcite and the parametric models fit and referenced in  6 of @xcite .",
    "these models have the drawback of imposing a tight constraint on the luminosity function in a case where there is not a consensus parametric form .",
    "some proposed methods are nonparametric , but assume that redshift and absolute magnitude are independent , and hence assume that there is no evolution of the luminosity function with redshift .",
    "these include the nonparametric maximum likelihood method described in @xcite and @xcite and adapted for double truncation in @xcite , along with the methods in @xcite , @xcite , the @xmath17 estimator of @xcite and @xcite .",
    "the semiparametric method of @xcite also assumes independence . @xcite",
    "apply a nonparametric technique which assumes independence after having transformed the bivariate data using a parametric form .",
    "any method which assumes independence can be applied over small redshift ranges ( usually called bins ) . @xcite and @xcite describe other binning approaches .",
    "binning forces the difficult choices of bin centers and widths , and independence is still assumed over the width of the bin .",
    "this work was motivated by the goal of developing a statistically rigorous method which ( 1 ) does not assume a strict parametric form for the bivariate density ; ( 2 ) does not assume independence between redshift and absolute magnitude ; ( 3 ) does not require dividing the data into arbitrary bins ; and ( 4 ) naturally incorporates a varying selection function .",
    "this was accomplished by decomposing the bivariate density @xmath2 into @xmath3 where @xmath18 will take an assumed parametric form ; it is intended to model the dependence between the two random variables .",
    "for example , there may be a physical , parametric model for the evolution of the luminosity function which could be incorporated into @xmath18 .",
    "alternatively , one could use @xmath19 as a first - order approximation to the dependence .",
    "the functions @xmath4 and @xmath5 are estimated nonparametrically , with _ bandwidth _ parameters to control the amount of smoothness in the estimate . using the quasar sample of figure [ rawdata ] ,",
    "the estimates obtained here are quite consistent , if not a bit smoother , than those found in @xcite .",
    "this analysis confirms the finding of the flattening of the slope of the luminosity function at higher redshift .",
    "the paper is organized as follows .  [ data ] briefly describes the quasar sample used here .",
    " [ ourmodel ] gives an overview of the idea of local maximum likelihood , a nonparametric extension of maximum likelihood , and describes in detail the semiparametric approach taken here .",
    " [ bandselect ] describes how the integrated mean squared error can be approximated using cross - validation ; the bandwidths can then be chosen to minimize this quantity . ",
    "[ results ] presents some results from the analysis of the @xcite quasar sample , along with the results from some simulations .",
    "more detailed derivations , along with theory for approximating the distribution of the estimator , can be found in @xcite .",
    "the approach was implemented as a fortran subroutine with r wrapper ] ] .",
    "the full @xcite sample , shown in figure [ rawdata ] , consists of 15,343 quasars . from these ,",
    "any quasar is removed if it has @xmath20 , @xmath21 , @xmath22 , or @xmath23 .",
    "in addition , for quasars of redshift less than 3.0 , only those with apparent magnitude between 15.0 and 19.1 , inclusive , ( after the application of @xmath16-corrections ) are kept ; for quasars of redshift greater than or equal to 3.0 , only those with apparent magnitude between 15.0 and 20.2 are retained .",
    "these boundaries combine to create the irregular shape shown by the dashed line in figure [ rawdata ] .",
    "this truncation removes two groups of quasars from the @xcite sample .",
    "first , there are 62 quasars removed with @xmath22 .",
    "this was done to mitigate the effect of the irregularly - shaped , very narrow region in the lower left corner of figure [ rawdata ] .",
    "second , there are 224 additional quasars with @xmath24 and apparent magnitude larger than 19.1 ; these fall in an extremely poorly sampled region , which can also be noted from figure [ rawdata ] .",
    "hence there are 15,057 quasars remaining after this truncation .",
    "the sample is not assumed to be complete within this region .",
    "associated with each sampled quasar is a value for the _ selection function _ , which can be interpreted as the probability that a quasar at this location , and of these characteristics would be captured by the sample .",
    "details regarding how the selection function was approximated via simulations , along with many other details regarding the sample , can be found in @xcite .",
    "the approach taken here is built upon a nonparametric extension of maximum likelihood called _ local likelihood modeling_. this section begins by describing local likelihood density estimation in the general case .",
    "this is then adapted to the problem at hand , initially for the case assuming the random variables are independent .",
    "the case where dependence is allowed is then described as a simple extension .      to contrast the standard _",
    "global _ approach to estimation with the local approach employed here , consider the following .",
    "assume the data @xmath25 are realizations ( observations ) of independent , identically distributed random variables from a distribution with density @xmath26 . with classic maximum likelihood estimation ,",
    "one chooses a single estimate from among a class of candidates for @xmath26 ; let @xmath27 denote this class .",
    "specifically , the maximum likelihood estimator ( @xmath28 ) for @xmath26 is defined as the @xmath29 which maximizes @xmath30     \\label{usuallike}\\ ] ] or , equivalently , the @xmath29 which maximizes @xmath31 ( the notation @xmath32 simultaneously indicates a random variable with unknown density @xmath26 , and the observed realization of that random variable . ) although written here like a density estimation problem , one could imagine the class @xmath27 being indexed by a parameter @xmath33 ; hence this also captures the usual maximum likelihood estimator for parametric problems .",
    "for example , one could define @xmath27 to consist of all gaussian densities as mean @xmath34 and variance @xmath35 vary . in cases",
    "where each @xmath29 is a density ( e.g. , the aforementioned gaussian case ) , the expression in brackets of equation ( [ usuallike ] ) is always zero , and thus unnecessary .",
    "however , it is often advantageous to let @xmath27 be a wider class of smooth , nonnegative functions ; then the bracketed term forces @xmath28 to be a probability density . with local modeling , instead of seeking the single member of the class @xmath27 to be the estimate of @xmath36 , the goal is to approximate @xmath37 for @xmath38 near @xmath39 , yielding the _ local estimate _ @xmath40 .",
    "typically , @xmath41 can be approximated locally by a polynomial ; in fact , a linear form for @xmath42 usually suffices .",
    "see figure [ explainloclike ] . on the left plot",
    ", the dashed line gives the logarithm of the gaussian density with mean zero and variance one .",
    "local linear estimates @xmath42 are shown for each of @xmath43 .",
    "it is unimportant that @xmath44 is not a good estimate of @xmath37 for @xmath38 far from @xmath39 , since many such local estimates will be found and then smoothed together .",
    "these local estimates were calculated with a simulated data set consisting of 10,000 values . the method for finding these local estimates",
    "is outlined next .    in independent work ,",
    "@xcite and @xcite localized the likelihood criterion of equation ( [ shortlike ] ) near @xmath45 by writing @xmath46 where @xmath47 is a kernel function parametrized by @xmath48 . a standard choice would be @xmath49 where @xmath16 is a probability density , but more specific forms will be considered ( and required ) below .",
    "the choice of @xmath50 typically has much more influence on the estimator than does the choice of the kernel function .",
    "the local estimate @xmath40 is found by maximizing @xmath51 over @xmath52 belonging to some simple class , usually degree @xmath53 polynomials expanded around @xmath39 : @xmath54 thus , the model is locally parametric with parameters @xmath55 .",
    "one imagines repeating this procedure at a grid of @xmath39-values , call this grid @xmath56 , and hence obtaining a family of local estimates @xmath57 . as a result , @xmath58 is the family @xmath4 of local estimates which maximizes @xmath59 the final local likelihood estimator @xmath60 is constructed by smoothing together the local estimates : @xmath61 thus making dual use of @xmath50 .",
    "returning to figure [ explainloclike ] , the plot on the right shows @xmath60 , the result of smoothing together 101 local linear estimates ( @xmath56 consists of 101 values between -3 and 3 ) . in this case , @xmath62 .",
    "it is clear that the estimate comes very close to the true density .    in what follows , simply assume that @xmath63 is chosen so that @xmath64 for all @xmath38 and hence @xmath65 this is a departure from the original approach of @xcite and @xcite , who instead used @xmath66 .",
    "the criterion @xmath67 appears awkward upon first sight , but it possesses the following property : considering @xmath68 again as random variables with unknown density @xmath26 , then @xmath69 is maximized by choosing the family @xmath4 which sets @xmath70 for all @xmath39 and all @xmath38 .",
    "if that choice were made , the estimate would be @xmath71 .",
    "thus , since @xmath72 , the local estimate @xmath42 will approximate the degree @xmath53 taylor expansion of @xmath73 for @xmath38 around @xmath39 .",
    "the expected value of the standard likelihood criterion is also maximized by setting the density equal to the truth , but this localized version has the advantage of allowing the choice of @xmath50 to adjust the amount of smoothness in the estimator . in  [ bandselect ] , an objective method for bandwidth selection is described .",
    "there is an apparent conflict between the choice of @xmath50 and the choice of the number of local models ( the cardinality of @xmath56 ) since small @xmath56 will lead to smooth estimates . in the applications here",
    ", @xmath56 is chosen large , so that the amount of smoothing is completely dictated by @xmath50 .",
    "now return to the bivariate density estimation problem using truncated astronomical data .",
    "the available data are denoted @xmath74 and @xmath75 , the vectors of redshifts and absolute magnitudes , respectively .",
    "let @xmath76 denote the region outside of which the data are truncated and let @xmath77 denote the cross - section of @xmath76 at @xmath12 ; @xmath78 is defined similarly .",
    "let @xmath2 denote the unknown joint density of random variables @xmath12 and @xmath79 .",
    "the approach taken here originates in the following naive method . for the moment assume @xmath12 and @xmath79 are independent so that @xmath80 where @xmath81 is the density for redshift and @xmath82 is the density for absolute magnitude .",
    "clearly , the available data allow estimation of the redshift density for observable quasars , denote this density @xmath83 .",
    "this is related to @xmath81 by @xmath84 where @xmath85 is a normalizing constant which forces @xmath83 to integrate to one .",
    "assuming for the moment that @xmath82 were known , it is possible to turn an estimator for @xmath83 into an estimator for @xmath81 by solving equation ( [ xmarg ] ) for @xmath81 : @xmath86 starting with an initial guess at @xmath82 , we could iterate between assuming @xmath82 is known , and estimating @xmath81 , and vice versa .",
    "this procedure is portrayed in figure [ explainmethod ] . using the quasar data",
    "set described in  [ data ] , the upper left plot depicts @xmath87 along the vertical axis , with absolute magnitudes ranging from -29.9 to -25.85 .",
    "an ( arbitrary ) assumption is made regarding the density for absolute magnitude @xmath88 , shown as the solid curve in the upper right plot .",
    "for example , one can find that @xmath89 and thus conclude that the observed sample catches 24% of the quasars at @xmath90 .",
    "( the fact that some quasars are missed within @xmath76 is considered later when the selection function is incorporated into the analysis . )",
    "the lower left plot shows how the proportion of quasars observed varies with redshift , i.e. it is a graph of @xmath91 versus @xmath92 .",
    "the dashed line in the lower right plot is @xmath93 , the estimated redshift density for observable quasars .",
    "the solid curve is @xmath94 , as defined above , found by dividing @xmath93 by the proportion of quasars observed at redshift @xmath12 , and then normalizing to force the estimate to be a density .",
    "figure [ explainmethod ] also illustrates problems with this approach .",
    "first , the sharp corner of @xmath76 at @xmath95 leads to a sharp feature in the estimate @xmath94 .",
    "in other words , smooth @xmath83 does not produce a smooth @xmath94 .",
    "second , consider the behavior of @xmath96 for @xmath12 where @xmath97 is small , for instance @xmath98 : even a small error in the estimate of @xmath97 will lead to a large error in @xmath96 .",
    "the fundamental challenge is that a well - chosen estimator ( i.e. , well - chosen smoothing parameters ) for @xmath83 does not necessarily lead to @xmath94 being a good estimator for @xmath81 .",
    "in addition , it is possible to construct examples where this iterative approach will converge to different estimates starting from different initial values for @xmath82 .",
    "despite the aforementioned problems with the use of @xmath94 , that approach can be improved using the local likelihood methods of  [ locliketut ] . in what follows ,",
    "@xmath83 is estimated using local polynomial models which include an additive _ offset _ term .",
    "this offset is chosen so that when subtracted off , what remains is a good estimator for @xmath81 .",
    "the procedure is fundamentally the same as that for constructing @xmath94 : starting with an initial guess as to the value of the density for absolute magnitude ( @xmath82 ) , the relationship between @xmath81 , @xmath82 , and @xmath83 ( shown in equation ( [ xmarg ] ) ) is exploited to construct an estimator for @xmath81 .",
    "( here it is assumed that @xmath2 is normalized so that @xmath99 , but this choice is arbitrary since the estimate can be extended outside of @xmath76 and then renormalized as appropriate . )    to start , rewrite equation ( [ xmarg ] ) as @xmath100 where @xmath85 is the constant required to force @xmath99 .",
    "consider the goal of estimating @xmath101 for @xmath38 near @xmath39 .",
    "ideally , it would be possible to fit a local model @xmath102 to obtain both the local estimate @xmath40 and the needed normalizing constant @xmath85 , but truncation does not allow for direct estimation of @xmath81 .",
    "instead , write a local version of equation ( [ withoff ] ) as @xmath103 and then substitute in the expression for @xmath104 from equation ( [ ideal ] ) into equation ( [ locverswithoff ] ) to get @xmath105 of course , it is possible to estimate @xmath83 with the available data and equation ( [ offsetexp ] ) makes it clear that a good way of doing this would be to fit a local polynomial model with @xmath106 included as an offset .",
    "( recall that , for the moment , @xmath82 is assumed known . ) in other words , instead of maximizing the local likelihood criterion @xmath107 over @xmath108 that are polynomials expanded around @xmath39 ( as in equation ( [ polyexp ] ) ) , maximize over functions of the form @xmath109 write @xmath110 as the local likelihood at @xmath39 when the offset is included .",
    "label the parameters which maximize @xmath110 as @xmath111 . comparing equations ( [ ideal ] ) and ( [ offsetexp ] ) , note that @xmath112 is an estimate of @xmath113 and hence @xmath114 is the local ( near @xmath39 ) estimate of @xmath115 . as before , this is repeated for a grid of values @xmath116 and the result is the family @xmath58 which maximizes @xmath117 and the estimate of @xmath118 is found by smoothing together these local estimates : @xmath119 here , it is stressed that estimates of @xmath118 are smoothed together , instead of estimates of @xmath83 .",
    "this is important because now @xmath50 can be chosen to obtain the optimal amount of smoothing for the best estimate of @xmath118 .",
    "this avoids the problems which were evident at @xmath95 in figure [ explainmethod ] .",
    "a method for choosing @xmath50 is described in  [ bandselect ] . also , the constant @xmath85 is present in all of these estimates , but it will turn out in the next step that this is exactly what we need : there is no need to renormalize and get separate estimates of @xmath81 and @xmath85 .    in this next step ,",
    "@xmath82 will be estimated holding @xmath118 fixed at its current estimate . to ease notation , define @xmath120 with an estimate of @xmath118 in hand ,",
    "now let @xmath121 denote the density for the observable @xmath79 so that since @xmath122 it follows that @xmath123 now consider local models of the form @xmath124 where @xmath125 and now @xmath126 is the logarithm of the offset ; note that an estimator for this was found above in equation ( [ estoff ] ) : @xmath127d{z}.\\ ] ] this leaves @xmath128 as an estimator for @xmath82 .",
    "this is then used to reestimate the offset term used in equation ( [ offsetexp ] ) , and the process repeats .",
    "this is conceptually the same procedure as was used to create @xmath94 above , since the estimate of @xmath129 is found by alternating estimating @xmath81 and @xmath82 .",
    "this section will tie together the ideas of the previous .",
    "the iterative procedure described above is computationally tractable , and has intuitive appeal .",
    "remarkably , it is also possible to pose the estimation problem in another manner which is not as computationally useful , but will lead to analytical results .",
    "define @xmath130     \\left [     \\sum_{u \\in { \\cal g } }      k^*{\\hspace{-.04in}}\\left({z},u,\\lambda\\right )     \\exp\\!\\left (     { \\bf a}_u\\!\\left({z}\\right)\\right )     \\right ]     d{m}\\:d{z}\\right\\}\\label{critind } \\ ] ] as the global criterion .",
    "it is a function of both families of local models , @xmath4 and @xmath5 .",
    "the key is to notice that if @xmath5 is held fixed at its current estimate @xmath131 , maximizing @xmath132 over local models @xmath4 is identical to maximizing @xmath133 with fixed estimate of the offset term . to see this , recall equation ( [ foffset ] ) and note that an estimator for @xmath134 is @xmath135 and from equations ( [ callf ] ) and ( [ loclike ] ) , @xmath136     \\right ] d{z}\\\\     & & \\hspace{-1.2 in } = { k'}+ \\sum_{j=1}^n     \\left\\ {     \\sum_{u \\in { \\cal g } }     k^*{\\hspace{-.04in}}\\left({z}_j , u,\\lambda\\right ) { \\bf a}_u\\!\\left({z}_j\\right ) \\right.\\\\     & & \\hspace{-1.0in}\\left.- \\!\\int_{\\cal a }      \\left[\\sum_{u \\in { \\cal g}}k^*{\\hspace{-.04in}}\\left({z},u,\\lambda\\right )     \\exp\\!\\left({\\bf a}_u\\!\\left({z}\\right)\\right)\\right ]     \\left [     \\sum_{v \\in { \\cal g } } k^*\\!\\left({m},v,\\lambda\\right )      \\exp(\\widehat { \\bf b}_v\\!\\left({m}\\right))\\right ] d{z}\\:d{m}\\right\\ } \\\\     & & \\hspace{-1.2 in } = { k''}+ { \\cal l}^*\\!\\left({\\bf f } , \\widehat { \\bf g } , { \\bf { z}},{\\bf { m}}\\right)\\end{aligned}\\ ] ] where @xmath137 and @xmath138 are constants which do not depend on @xmath4 , and @xmath139 and @xmath140 are the lower and upper bounds on redshift , respectively .",
    "an analogous statement could be made for finding @xmath5 when @xmath58 is held fixed .",
    "thus , the iterative search method described in  [ itermethod ] is equivalent to maximizing this global criterion .      until now",
    ", the derivation of the approach has assumed that random variables @xmath12 and @xmath79 are independent .",
    "dependence will be incorporated by including a parametric portion @xmath18 so that the assumption becomes that @xmath141 a restriction placed on @xmath6 is that it must be linear in the real - valued parameters @xmath33 . in the absence of a physically - motivated model ,",
    "a useful first - order approximation is @xmath19 .",
    "the global criterion of equation ( [ critind ] ) is naturally updated to @xmath142     \\!\\left [     \\sum_{u \\in { \\cal g } }      k^*{\\hspace{-.04in}}\\left({z},u,\\lambda\\right )     \\exp\\!\\left (     { \\bf a}_u\\!\\left({z}\\right)\\right )     \\right ]     \\!d{m}\\:d{z}\\right\\}.     \\label{globcrit}\\end{aligned}\\ ] ] note that with this form , when @xmath4 and @xmath5 are held constant , maximizing @xmath143 over @xmath33 is equivalent to finding the maximum likelihood estimate of @xmath33 .",
    "note also that the sum over the @xmath10 data pairs has also been updated to allow specification of a weight @xmath144 . in this case , the natural choice for the weight is the inverse of the selection function for that data pair .",
    "the intuition is that a pair with selection function of 0.5 is `` like '' two observations at that location .    finally , with a criterion of this form",
    ", this estimator can be fit into a general class of statistical procedures called _ m - estimators_. see the appendix (  [ mest ] ) for an overview of m - estimators .",
    "the described procedure returns an estimate normalized to be a probability density over the observable region @xmath76 .",
    "of course , it could be renormalized to meet the goals of the analysis , but care should be taken if the renormalization involves multiplying by a constant which is itself estimated from the data . in certain cases , namely when there is a small sample , this could result in significantly understated standard errors .",
    "luminosity curves are usually stated in units of @xmath145 , and are obtained by multiplying the bivariate density ( normalized to be a probability density over @xmath76 ) by a redshift - dependent constant ; thus no adjustment of the standard errors is needed in this case .",
    "the choice of the bandwidth @xmath50 ( the smoothing parameter ) is critical . choosing @xmath50 too large results in an oversmoothed , highly biased estimator ; choosing @xmath50 too small leads to a rough , highly variable estimator .",
    "this is the _ bias / variance tradeoff_. fortunately , it is possible to select @xmath50 to balance these two in a meaningful , objective manner .",
    "although this discussion applies in general to the problem of density estimation , here it will be described in terms of estimating the bivariate density @xmath146 over @xmath76 .",
    "let @xmath147 denote a general estimator for @xmath146 which is a function of a smoothing parameter @xmath50 .",
    "then , @xmath148     \\:d{z}\\:d{m}\\nonumber \\\\      & = &     \\int_{\\cal a }      \\left [     \\mbox{variance}\\!\\left(\\widehat { \\phi}_{\\lambda}\\!\\left({z},{m}\\right)\\right )     +     \\mbox{bias}^2\\!\\left(\\widehat { \\phi}_{\\lambda}\\!\\left({z},{m}\\right)\\right )     \\right ]     \\:d{z}\\:d{m}\\label{balance}\\end{aligned}\\ ] ] is the _ integrated mean squared error _ for @xmath147 .",
    "imse is a natural measure of the error in the estimator , and it is apparent from equation ( [ balance ] ) how it balances the bias and variance of the estimator .",
    "although imse can not be calculated , there is an unbiased estimator .",
    "it holds that @xmath149 where @xmath85 is a constant which does not depend on @xmath50 , so it can be ignored .",
    "let @xmath150 denote the estimate of the density at @xmath151 found using the data set with this @xmath152 data pair removed .",
    "following @xcite , @xmath153 so that the _ least - squares cross - validation score _ ( lscv ) , @xmath154 is an unbiased estimator for imse@xmath155 , and hence minimizing it over @xmath50 approximates minimizing the imse .",
    "see @xcite and @xcite for theoretical results showing the large - sample optimality of choosing smoothing parameters to minimize this criterion .",
    "figure [ explainlscv ] gives an example of bandwidth selection by minimizing lscv .",
    "here , 100 simulated values are taken from the gaussian distribution with mean zero and variance one .",
    "the left plot shows how lscv varies with the choice of bandwidth , and leads to a choice of @xmath156 .",
    "the right plot compares the density estimate using three bandwidths @xmath157 with the true density . with the bandwidth too small , there are nonsmooth features , and the bias is low but the variance is high . with",
    "the bandwidth too large , the estimate is smoothing out the prominent peak in the center . here , the variance of the estimate is low , but the bias is high .",
    "the optimal choice gives an estimate close to the truth , and is found using a bandwidth which balances estimates of the bias and variance .",
    "the weighting due to the selection function needs to be taken into account in the previous discussion . recall that the weight @xmath158 is conceptualized as the number of equivalent observations represented by this data pair .",
    "thus `` leaving out '' observation @xmath159 is achieved by reducing its weight from @xmath158 to @xmath160 in the criterion ( equation [ globcrit ] ) .",
    "but one must imagine repeating this @xmath158 times ( for each equivalent observation which observation @xmath159 represents ) .",
    "let @xmath161 denote the _ effective sample size_. the new relationship is @xmath162 where @xmath150 now indicates the estimator evaluated at @xmath151 when the weight on observation @xmath159 is reduced from @xmath158 to @xmath163 .",
    "direct calculation of the leave - one - out estimates would be computationally intractable .",
    "@xcite describes an approximation based on the second - order taylor expansion of the criterion function .",
    "this approximation proves to be highly accurate and computationally simple .",
    "the method described in  [ itermethod ] involves fitting local polynomial models at each of a grid of values @xmath116 , for both the @xmath12 and @xmath79 directions .",
    "these derivations were all performed assuming fixed bandwidth @xmath50 used for each of these models .",
    "this was merely for notational convenience ; there is no reason that different bandwidths could not be chosen for each of these local models .",
    "in fact , given that the variables are on different scales , it would be unreasonable to assume the same bandwidth would be a good choice for each . in the results given in the next section , a stated bandwidth is assumed to be on the scale of the variables after they have been transformed to lie in the unit interval , and bandwidths are given as @xmath164 pairs .",
    "allowing the bandwidth to further vary over the different local models gives the overall model fit much flexibility , and lscv can be minimized as before .",
    "a full search over this high - dimensional space is not feasible in practice , however .",
    "this section describes the results of the application of this method to some real and simulated data sets . in all cases ,",
    "linear models are fit when doing the local likelihood modeling ( @xmath165 ) , and @xmath56 is a grid of 100 evenly spaced values in both the @xmath92 and @xmath166 dimensions .",
    "the parametric portion is set as @xmath19 .",
    "bandwidths @xmath164 are stated as proportions of the range for that variable , e.g. @xmath167 means that the bandwidth for the local models for redshift cover 5% of the range @xmath168 .",
    "this method was applied to the sample of quasars described in  [ data ] . as stated above",
    ", the method is capable of incorporating the selection function via differential weighting in the criterion ( equation ( [ globcrit ] ) ) , but the selection function does present some challenges in this case . for quasars with",
    "@xmath169 the selection function drops as low as 0.04 due to difficulty in distinguishing quasars from stars of spectral type a and f. this gives a weight of 25 to these quasars , which would be fine if it were exact , but these weights are calculated based on simulations and @xcite states that the selection function in this region `` is quite sensitive to such uncertain details of the simulation . '' they limit the weight on any observation to 3.0 to account for this .",
    "this limit was also imposed in the analysis here .",
    "figure [ lscvplot ] shows how lscv varies with @xmath170 and @xmath171 .",
    "the criterion is minimized when @xmath172 and @xmath173 .",
    "the grid of values at which lscv is calculated is spaced by 0.01 because , as will be seen below , fluctuations of the bandwidths on this scale lead to very little change in the estimates .",
    "the minimum value is -0.0078262 , but no significance can be attached to this value , since lscv is not an unbiased estimate of imse , but instead of imse plus an unknown additive constant .",
    "figure [ bivest ] shows , using the solid contours , the estimate of the quasar density ( two - dimensional luminosity function ) as a function of @xmath92 and @xmath166 , when @xmath167 and @xmath174 .",
    "this estimate is normalized to integrate to one over the entire dashed ( observable ) region .",
    "recall from  [ loclikewithoff ] that this is the form which the algorithm provides .",
    "fortunately , this is the ideal form for the estimate .",
    "the ( effective ) count of quasars in the surveyed region is @xmath175 and the survey covers @xmath176 .",
    "thus , the quasar count in a region @xmath177 of @xmath178 space can be estimated using @xmath179.\\ ] ] the estimate of @xmath33 is @xmath180 , with a standard error of 0.03 .",
    "although it is not possible to assign physical significance to this value for @xmath33 , it is clear that the possibility that @xmath181 is ruled out , and hence there is very strong evidence for evolution of the luminosity function with redshift .",
    "this estimate has an apparent irregularity in the shape of the density estimate for @xmath182 .",
    "( note the `` bumps '' in the solid contours for all values of @xmath166 at @xmath182 . )",
    "quasars of this redshift are given larger weight due to interference from stars of spectral type g and k. although it is not possible to be certain , it appears that the weighting may not be sufficiently accurate for the quasars .",
    "the weights may be underestimated leading to a corresponding dip in the density estimate .",
    "the bandwidth ( @xmath170 ) is sufficiently small to pick up this artifact .",
    "in fact , lscv forces the bandwidth to be small enough so it can model this feature .",
    "it is hoped that in future work the uncertainty in the weights can be incorporated into lscv . for comparison ,",
    "another estimate was constructed using @xmath183 for local models centered on redshift values larger than 2.0 , while still using @xmath167 for @xmath184 .",
    "this estimate is shown as the dotted contours .",
    "the increased smoothing removes the artifact .",
    "figure [ margs ] shows the estimated count of quasars with @xmath185 as a function of redshift . as in figure",
    "[ bivest ] , the solid curve is the estimate with the lscv - optimal bandwidths , and the dashed estimate is found using the increased smoothing . figure [ allconds ] shows quasar counts as a function of absolute magnitude at a collection of redshift values .",
    "comparisons are made with the estimates given in @xcite which were found using the bin - based method of @xcite .",
    "the error bars in both figures [ margs ] and [ allconds ] are one standard error , but represent statistical errors only .",
    "the error bars do not account for incorrect specification for the parametric form @xmath6 .",
    "but , if there is bias from the incorrect specification of @xmath6 , the binned estimates must share these biases .",
    "this would be surprising since , while having higher variance , estimates constructed from binning do not make assumptions regarding the evolution of the luminosity function , and hence a well - constructed estimate should be approximately unbiased",
    ".    figure [ allconds ] also provides insight into the sensitivity of the estimate to the bandwidth choice .",
    "it would be of great concern if small changes in bandwidth led to significant changes in the estimate . to explore this ,",
    "eight additional estimates were constructed using every possible combination of @xmath186 and @xmath187 .",
    "the results are shown as gray curves in each plot of figure [ allconds ] , but are only visible at @xmath188 and @xmath189 .",
    "the fluctuations are small relative to the size of the error bars . clearly , the estimates are insensitive to these perturbations .",
    "simulations were performed to further explore the behavior of the estimator .",
    "for these , the estimate shown in the dotted contours in figure [ bivest ] is taken to be the true bivariate density ; the truncation region is unchanged .",
    "the idea is to ask the following : if the truth were , in fact , the estimate found here , would this method be able to reach a good estimate of the density under identical conditions ( same sample size and truncation region ) ? hence ,",
    "the new data sets were simulated consisting of 16,589 @xmath178 pairs within the observable region .",
    "the first of these data sets was utilized to find the optimal smoothing parameters ; these were found to be @xmath190 and @xmath191 .",
    "each of the other 19 data sets was analyzed using these values , so that these simulations also provide insight into the adequacy of this approach to bandwidth selection .",
    "figure [ simconds ] shows the results from the simulations by comparing estimates of the cross - sections of the estimates @xmath192 at four different redshifts .",
    "each dashed curve is an estimate from one of the 20 data sets .",
    "the solid curve is the truth .",
    "these results show strong agreement between the estimates and the truth over the regions where data are observed .",
    "there is some bias in the tails , but this is in regions far from any observable data .",
    "in addition , these simulations provide strong evidence that the estimates of the standard errors are accurate : the variability in the estimates is comparable to the size of the error bars .",
    "the semiparametric method described here is a strong alternative to previous approaches to estimating luminosity functions .",
    "the primary advantage is that it allows one to estimate the evolution of the luminosity function with redshift without assuming a strict parametric form for the bivariate density .",
    "instead , one only needs to specify the parametric form for a term which models the dependence between redshift and absolute magnitude .",
    "future work will focus on specifying a physically - motivated form for this parametric portion , but the results from the analysis of a sample of quasars reproduce well those from @xcite while only assuming a simple , first - order approximation to the dependence .",
    "other portions of the bivariate density are modeled nonparametrically , and are functions of smoothing parameters .",
    "using least - squares cross - validation , these smoothing parameters can be chosen in an objective manner , by minimizing a quantity which is a good approximation to the integrated mean squared error .",
    "results from simulations show that , with a data set of this size , the method is indeed capable of recapturing the true luminosity curves under the truncation observed in these cosmological data sets .",
    "the author gratefully acknowledges the comments of the referee , which greatly improved this paper , and the contributions of peter freeman , chris genovese , and larry wasserman of the department of statistics at carnegie mellon university .",
    "the author s work is supported by nsf grants # 0434343 and # 0240019 .",
    "funding for the sdss and sdss - ii has been provided by the alfred p. sloan foundation , the participating institutions , the national science foundation , the u.s .",
    "department of energy , the national aeronautics and space administration , the japanese monbukagakusho , the max planck society , and the higher education funding council for england .",
    "the sdss web site is http://www.sdss.org/. the sdss is managed by the astrophysical research consortium for the participating institutions .",
    "the participating institutions are the american museum of natural history , astrophysical institute potsdam , university of basel , cambridge university , case western reserve university , university of chicago , drexel university , fermilab , the institute for advanced study , the japan participation group , johns hopkins university , the joint institute for nuclear astrophysics , the kavli institute for particle astrophysics and cosmology , the korean scientist group , the chinese academy of sciences ( lamost ) , los alamos national laboratory , the max - planck - institute for astronomy ( mpia ) , the max - planck - institute for astrophysics ( mpa ) , new mexico state university , ohio state university , university of pittsburgh , university of portsmouth , princeton university , the united states naval observatory , and the university of washington .",
    "15    natexlab#1#1    boyle , b. , shanks , t. , croom , s. , smith , r. , miller , l. , loaring , n. , and heymans , c. 2000 , mnras , 317 , 1014    choloniewski , j. 1986 , mnras , 223 , 1    efron , b. & petrosian , v. 1999 , j. am .",
    "assoc . , 94 ,",
    "824    efstathiou , g. , ellis , r. , peterson , b. 1988 , mnras , 232 , 431    felten , j. 1976 , apj , 207 , 700    hall , p. 1983 , ann .",
    "stat . , 11 , 1156    hjort , n. & jones , m. 1996 , ann .",
    "stat . , 24 , 1619    jackson , j. 1974 , mnras , 166 , 281    loader , c. 1996 , ann . stat . , 24 , 1602    lynden - bell , d. 1971 , mnras , 155 , 95    maloney , a. , & petrosian , v. 1999 , apj , 518 , 32    nicoll , j. & segal , i. 1983 , a&a , 118 , 180    page , m. and carrera , f. 2000 , mnras , 433    qin , y .- p . , & xie , g .- z . 1999 ,",
    "a&a , 341 , 693    richards , g. , et al .",
    "2006 , apj , 131 , 2766    rudemo , m. 1982 , scan .",
    "j. of stat .",
    ", 9 , 65    sandage , a. , tammann , g. , & yahil , a. 1979 , apj , 232 , 352    schafer , c. ( 2006 ) submitted .",
    "available as cmu dept . of stat .",
    "tech report # 842 http://www.stat.cmu.edu/tr/tr842/tr842.html    schmidt , m. 1968 , apj , 151 , 393    stone , c. 1983 , ann .",
    ", 12 , 1285    takeuchi , t. , yoshikawa , k. , & ishi , t. 2000 , apjs , 129 , 1    willmer , c. 1997 aj , 114 , 898    wang , m .- c .",
    "1989 , j. of amer .",
    "84 , 742    wasserman , l. , miller , c. , nicol , r. , genovese , c. jang , w. , connolly , a. , moore , a. , & schneider , j. 2001 , astro - ph/0112050    woodroofe , m. 1985 , ann .",
    "stat . , 13 , 163    york , d. 2000 , aj , 120 , 1579",
    "the procedure described in  [ ourmodel ] can be fit into a general class of statistical estimators called _ m - estimators_. in the simplest case , a m - estimator for a parameter is constructed by maximizing a criterion of the form @xmath193\\ ] ] where @xmath194 are the observed data , assumed to be realizations of independent , identically distributed random variables and @xmath195 is the parameter to be estimated .",
    "the function @xmath196 is some criterion .",
    "for example , in the case of finding the maximum likelihood estimate of @xmath195 , the function @xmath197 , where @xmath198 is the density corresponding to parameter @xmath195 .",
    "most least squares problems can be stated as m - estimators .",
    "standard theory for m - estimators can be applied to obtain an approximation to the distribution of @xmath199 , which can then be used to find standard errors and form confidence intervals .    in the case at hand",
    ", @xmath32 is the pair @xmath200 , @xmath201 , and @xmath202     \\left [     \\sum_{u \\in { \\cal g } }      k^*{\\hspace{-.04in}}\\left({z},u,\\lambda\\right )     \\exp\\!\\left (     { \\bf a}_u\\!\\left({z}\\right)\\right )     \\right ]     d{m}\\:d{z}. \\ ] ] see schafer ( 2006 ) to see the derivations of the approximate distribution for the estimator in this case .",
    "the m - estimator could be generalized to the following : @xmath203\\ ] ] where @xmath204 is the weight given to the @xmath152 observation .",
    "this allows for easy incorporation of the selection function into the analysis .",
    "the statistical theory for this _ weighted m - estimator _ is a simple extension of that for the standard m - estimator ."
  ],
  "abstract_text": [
    "<S> the observational limitations of astronomical surveys lead to significant statistical inference challenges . </S>",
    "<S> one such challenge is the estimation of luminosity functions given redshift @xmath0 and absolute magnitude @xmath1 measurements from an irregularly truncated sample of objects . </S>",
    "<S> this is a bivariate density estimation problem ; we develop here a statistically rigorous method which ( 1 ) does not assume a strict parametric form for the bivariate density ; ( 2 ) does not assume independence between redshift and absolute magnitude ( and hence allows evolution of the luminosity function with redshift ) ; ( 3 ) does not require dividing the data into arbitrary bins ; and ( 4 ) naturally incorporates a varying selection function . </S>",
    "<S> we accomplish this by decomposing the bivariate density @xmath2 via @xmath3 where @xmath4 and @xmath5 are estimated nonparametrically , and @xmath6 takes an assumed parametric form . </S>",
    "<S> there is a simple way of estimating the integrated mean squared error of the estimator ; smoothing parameters are selected to minimize this quantity . </S>",
    "<S> results are presented from the analysis of a sample of quasars . </S>"
  ]
}