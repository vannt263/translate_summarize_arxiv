{
  "article_text": [
    "evolutionary algorithms ( eas ) belong to iterative methods . as iterative methods",
    ", a fundamental question is their convergence rates : how fast does an ea converge to the optimum per generation ? according to  @xcite , existing results on the convergence rate of genetic algorithms can be classified into two categories .",
    "the first category is related to the eigenvalues of the transition matrix associated with an ea .",
    "a lower bound of convergence rate is derived in @xcite for simple genetic algorithms by analyzing eigenvalues of the transition matrix . then the work is extended in @xcite and it is found that the convergence rate is determined by the second largest eigenvalue of the transition matrix .",
    "the other category is based on doeblin s condition .",
    "the upper bound on the convergence rate is derived using deoblin s condition in @xcite . as to continuous optimization , the local convergence rate of eas on the sphere function , quadratic convex functions and convex objective functions",
    "are discussed in @xcite .",
    "the research of the convergence rate covers various types of eas such as isotropic algorithms @xcite , gene expression programming @xcite , multiobjective optimization eas  @xcite .",
    "the relationship between the convergence rate and population size is investigated in @xcite .",
    "the convergence rate in previous studies  @xcite is based on markov chain theory .",
    "suppose that an ea is modeled by a finite markov chain with a transition matrix @xmath0 , in which a state is a population  @xcite .",
    "let @xmath1 be the probability distribution of the @xmath2th generation population on a population space , @xmath3 an invariant probability distribution of @xmath0 .",
    "then @xmath1 is called _ convergent _ to @xmath3 if @xmath4 where @xmath5 is a norm ; and the _ convergence rate _ refers to the order of how fast @xmath1 converges to @xmath3  @xcite .",
    "the goal is to obtain a bound @xmath6 such that @xmath7 . but to obtain a closed form of @xmath6 often is difficult in both theory and practice .",
    "the current paper aims to seek a convergence rate satisfying two requirements : it is easy to calculate the convergence rate in practice while it is possible to make a rigorous analysis in theory .",
    "inspired from conventional iterative methods  @xcite , a new measure of the convergence rate , called the average convergence rate , is presented .",
    "the paper is organized as follows : section  [ secdefinition ] defines the average convergence rate .",
    "section  [ secanalysis ] establishes lower bounds on the average convergence rate .",
    "section  [ secconnections ] discusses the connections between the average convergence rate and other performance measures .",
    "section  [ secdiscussion ] introduces an alternative definition of the average convergence rate if the optimal fitness value is unknown .",
    "section  [ secconclusions ] concludes the paper .",
    "consider the problem of minimizing ( or maximizing ) a function @xmath8 .",
    "an ea for solving the problem is regarded as an iterative procedure ( algorithm  [ alg1 ] ) : initially construct a population of solutions @xmath9 ; then generate a sequence of populations @xmath10 , @xmath11 , @xmath12 and so on .",
    "this procedure is repeated until a stopping criterion is satisfied .",
    "an archive is used for recording the best found solution .    initialize a population of solutions @xmath13 and set @xmath14 ; an archive records the best solution in @xmath9 ; generate a new population of solutions @xmath15 ; update the archive if a better solution is generated ; @xmath16 ;    the fitness of population @xmath17 is defined by the best fitness value among its individuals , denoted by @xmath18 . since @xmath18 is a random variable",
    ", we consider its mean value @xmath19 $ ] .",
    "let @xmath20 denote the optimal fitness .",
    "the _ fitness difference _ between @xmath21 and @xmath22 is @xmath23 .",
    "the _ convergence rate _ for one generation is @xmath24 since @xmath25 , calculating the above ratio is unstable in practice .",
    "thus a new average convergence rate for eas is proposed in the current paper .",
    "given an initial population @xmath9 , the _ average ( geometric ) convergence rate of an ea for @xmath2 generations _",
    "is @xmath26 if @xmath27 , let @xmath28 . for the sake of simplicity , @xmath29 is short for @xmath30 .",
    "the rate represents a normalized geometric mean of the reduction ratio of the fitness difference per generation",
    ". the larger the convergence rate , the faster the convergence .",
    "the rate takes the maximal value of 1 at @xmath31 .",
    "inspired from conventional iterative methods ( * ? ? ?",
    "* definition 3.1 ) , the _ average ( logarithmic ) convergence rate _ is defined as follows : @xmath32 formula ( [ equlograte ] ) is not adopted since its value is @xmath33 at @xmath31 .",
    "but in most cases , average geometric and logarithmic convergence rates are almost the same .",
    "let @xmath34 .",
    "usually @xmath35 and @xmath36 , then @xmath37 .",
    "in practice , the average convergence rate is calculated as follows : given @xmath38 with @xmath20 known in advance ,    1 .",
    "run an ea for @xmath39 times ( @xmath40 ) .",
    "2 .   then calculate the mean fitness value @xmath22 as follows , @xmath41}_t ) + \\cdots + f(\\phi^{[t]}_t ) \\right),\\ ] ] where @xmath42}_t)$ ] denotes the fitness @xmath18 at the @xmath43th run .",
    "the law of large numbers guarantees ( [ equft ] ) approximating to the mean fitness value @xmath44 when @xmath39 tends towards @xmath33 .",
    "3 .   finally , calculate @xmath29 according to formula ( [ equaveragerate ] ) .",
    "the calculation is applicable for most eas on both continuous and discrete optimization .",
    "we take an example to illustrate the average convergence rate .",
    "consider the problem of minimizing ackley s function : @xmath45^{\\frac{1}{2}}\\ }   -\\exp[\\sum^n_{i=1 } \\cos(2 \\pi x_i+2 \\pi e ) /n ]   + 20 + e,\\end{aligned}\\ ] ] where @xmath46 , i=1 , \\cdots , n$ ] .",
    "the optimum is @xmath47 and @xmath48 .",
    "we compare the multi - grid ea ( mea )  @xcite with the fast evolutionary programming ( fep )  @xcite under the same experiment setting ( where @xmath49 is 30 and population size is 100 ) .",
    "run the two eas for 1500 generations and 100 times .",
    "calculate @xmath22 according to ( [ equft ] ) and then @xmath29 according to ( [ equaveragerate ] ) .",
    "[ figackley ] illustrates the convergence rates of mea and fep .",
    "table f10-geom-rate-mea.dat ; table f10-geom-rate-fep.dat ;    the average convergence rate is different from the progress rate such as @xmath50 or logarithmic rate @xmath51 used in @xcite .",
    "the progress rate measures the fitness change ; but the convergence rate measures the rate of the fitness change .",
    "we demonstrate this difference by an example .",
    "let @xmath52 . in terms of @xmath50 ,",
    "the progress rate on @xmath53 is 100 times that on @xmath38 . in terms of @xmath54 , the progress rate on @xmath53",
    "is @xmath55 times that on @xmath38 .",
    "however , the average convergence rate is the same on both @xmath38 and @xmath53 .",
    "looking at fig .  [ figackley ] again , two questions may be raised : what is the lower bound or upper bound on @xmath29 ? does @xmath29 converge or not ? for discrete optimization , a theoretical answer is provided to these questions in this section . for continuous optimization",
    ", its analysis is left for future research .    in the rest of the paper ,",
    "we analyze eas for discrete optimization and assume that their genetic operators do not change with time .",
    "such an ea can be modeled by a homogeneous markov chain  @xcite with transition probabilities @xmath56 where populations @xmath57 denote states of @xmath17 and @xmath58 denotes the set of populations ( called the _ population space _ ) .",
    "let @xmath0 denote the transition matrix with entries @xmath59 .",
    "a population is called _ optimal _ if it includes an optimal solution ; otherwise called _ non - optimal_. let @xmath60 denote the set of optimal populations , and @xmath61 .",
    "because of the stopping criterion , the optimal set is always absorbing , @xmath62    transition matrix @xmath63 can be split into four parts : @xmath64 where @xmath65 is a submatrix representing probability transitions among optimal states ; @xmath66 a submatrix for probability transitions from optimal states to non - optimal ones , of which all entries take the value of zero ; @xmath67 a submatrix denoting probability transitions from non - optimal states to optimal ones ; and @xmath68 a submatrix for probability transitions among non - optimal states .",
    "since @xmath17 is a random variable , we investigate the probability distribution of @xmath17 instead of @xmath17 itself .",
    "let @xmath69 denote the probability of @xmath17 at a non - optimal state @xmath70 , @xmath71 let vector @xmath72 represent all non - optimal states and vector @xmath73 denote the probability distribution of @xmath17 in the non - optimal set , where @xmath74 here notation @xmath75 is a column vector and @xmath76 the row column with the transpose operation . for the initial probability distribution , @xmath77 where @xmath78 .",
    "only when the initial population is chosen from the optimal set , @xmath79 .",
    "consider probability transitions among non - optimal states only , which can be represented by matrix iteration @xmath80    an ea is called _ convergent _ if @xmath81 for any @xmath82 or @xmath83 .",
    "it is equivalent to saying that the probability of finding an optimal solution is @xmath84 as @xmath2 tends towards @xmath33 .    the mean fitness value @xmath22 is given as follows : @xmath85=\\sum_{x \\in s } f(x ) \\pr(\\phi_t = x ) . \\end{aligned}\\ ] ] then it follows @xmath86",
    "let vector @xmath87 denote the fitness values of all non - optimal populations @xmath88 .",
    "then ( [ equfitnessvalue ] ) can be rewritten in a vector form @xmath89 where @xmath90 denotes the vector product and @xmath91 .    for a vector @xmath92 ,",
    "denote @xmath93 since @xmath94 iff @xmath95 ; @xmath96 and @xmath97 , thus @xmath98 is a vector norm . for a matrix @xmath99 ,",
    "let @xmath100 be the induced matrix norm , given by @xmath101    using the above markov chain model , we are able to estimate lower bounds on the average convergence rate .",
    "[ theaveragerate ] let @xmath102 be the transition submatrix associated with a convergent ea . for any @xmath103 ,    1 .",
    "the average convergence rate for @xmath2 iterations is lower - bounded by @xmath104 2 .",
    "the limit of the average convergence rate for @xmath2 generations is lower - bounded by @xmath105 where @xmath106 is the spectral radius ( i.e. , the supremum among the absolute values of all eigenvalues of @xmath102 ) .",
    "3 .   under random initialization (",
    "that is , @xmath107 for any @xmath108 or @xmath109 ) , it holds @xmath110 4 .   under particular initialization (",
    "that is , set , denote @xmath111 . ] @xmath112 where @xmath92 is an eigenvector corresponding to the eigenvalue @xmath106 with @xmath113 but @xmath114 .",
    "the existence of such a @xmath92 is given in the proof ) , it holds for all @xmath115 , @xmath116    \\1 ) from ( [ equmatrixiteration ] ) : @xmath117 we have @xmath118    hence @xmath119 which proves the first conclusion .",
    "\\2 ) according to gelfand s spectral radius formula @xcite , we get @xmath120 the second conclusion follows by combining ( [ equsecondresult ] ) with ( [ equfirstconclusion ] ) .",
    "\\3 ) since @xmath121 , according to the extension of perron - frobenius theorems to non - negative matrices  @xcite , @xmath106 is an eigenvalue of @xmath102 .",
    "there exists an eigenvector @xmath92 corresponding to @xmath106 such that @xmath113 but @xmath122 .",
    "in particular , @xmath123    let @xmath124 denote the maximum value of the entries in vector @xmath92 .",
    "due to random initialization , @xmath125 .",
    "let @xmath126 denote the minimum value of the entries in vector @xmath82 .",
    "set @xmath127 from ( [ equeigenvector ] ) , we get @xmath128 thus vector @xmath129 is an eigenvector of @xmath106",
    ".    let @xmath130 then from ( [ equu ] ) , we know @xmath131 . since @xmath132 @xmath131 and @xmath133",
    ", we deduce that @xmath134 it follows that @xmath135 @xmath136    since both @xmath137 and @xmath138 are independent of @xmath2 , we let @xmath139 and get @xmath140 then we get @xmath141 @xmath142 the third conclusion follows by combining ( [ equthirdinequality ] ) with ( [ equsecondconclusion ] ) .",
    "\\4 ) set @xmath143 where @xmath92 is given in step 3 .",
    "then @xmath82 is an eigenvector corresponding to the eigenvalue @xmath106 such that @xmath144 . from ( [ equmatrixiteration ] ) : @xmath145 , we get @xmath146 thus we have for any @xmath147 @xmath148 then @xmath149 which gives the fourth conclusion .",
    "the above theorem provides lower bounds on the average convergence rate .",
    "furthermore , it reveals that @xmath29 converges to @xmath150 under random initialization and @xmath149 for any @xmath147 under particular initialization .",
    "similar to conventional iterative methods @xcite , we call @xmath150 the _ asymptotic average convergence rate _ of an ea , denoted by @xmath151 . according to ( [ equthirdconclusion ] ) , its value can be approximately calculated as follows : under random initialization , @xmath29 approximates to @xmath150 if @xmath2 is sufficiently large . note that this definition is different from another asymptotic convergence rate , given by @xmath152 in  @xcite . in most cases ,",
    "the two rates are almost the same since usually @xmath153 and then @xmath154 .",
    "since @xmath150 is independent of @xmath2 and initialization , hence using asymptotic average convergence rate is convenient for comparing two eas , for example , to analyze mixed strategy eas  @xcite .",
    "the average convergence rate is different from other performance measures of eas : the expected hitting time is the total number of generations for obtaining an optimal solution  @xcite ; and fixed budget analysis focuses on the performance of eas within fixed budget computation  @xcite .",
    "however , there are some interesting connections between them",
    ".    there exists a link between the asymptotic average convergence rate and the hitting time .",
    "let @xmath155 be the expected number of generations for a convergent ea to hit @xmath60 when starting from state @xmath70 ( called the _ expected hitting time _ ) .",
    "denote @xmath156 where @xmath72 represent all non - optimal states .",
    "[ thelink ] let @xmath102 be the transition submatrix associated with a convergent ea .",
    "then @xmath157 is not more than @xmath158    according to the fundamental matrix theorem  ( * ? ? ?",
    "* theorem 11.5 ) , @xmath159 , where @xmath160 is the unit matrix .",
    "then @xmath161 where the last equality takes use of a fact : @xmath162 is an eigenvalue and spectral radius of @xmath163 .",
    "the above theorem shows that @xmath157 is a lower bound on the expected hitting time .",
    "following theorem  [ theaveragerate ] , a straightforward connection can be established between the spectral radius @xmath106 and the progress rate @xmath164 .",
    "[ corlink]let @xmath102 be the transition submatrix associated with a convergent ea .    1 .   under random initialization ( that is @xmath109 )",
    ", it holds @xmath165 2 .   under particular initialization ( that is , set @xmath112 where @xmath92 is an eigenvector corresponding to the eigenvalue @xmath106 with @xmath113 but @xmath114 )",
    ", it holds for all @xmath115 , @xmath166    the exponential decay , @xmath167 , provides a theoretical prediction for the trend of @xmath168 .",
    "the corollary confirms that the spectral radius @xmath106 plays an important role on the convergence rate  @xcite .",
    "we explain the theoretical results by a simple example .",
    "consider a ( 1 + 1 ) ea for maximizing the onemax function @xmath169 where @xmath170 .",
    "_ onebit mutation _ : choose a bit of @xmath17 ( one individual ) uniformly at random and flip it .",
    "let @xmath171 denote the child .",
    "_ elitist selection : _ if @xmath172 , then let @xmath173 ; otherwise @xmath174 .    denote subset @xmath175 where @xmath176 .",
    "transition probabilities satisfy that @xmath177 and @xmath178 . writing them in matrix @xmath0 ( where submatrix @xmath102 in the bold font ) : @xmath179     0 &    \\mathbf{\\frac{2}{n } } &    \\mathbf{1- \\frac{2}{n } } & \\cdots & \\mathbf{0 } &   \\mathbf{0 } & \\mathbf { 0}\\\\   \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots   \\\\ 0 &   \\mathbf{0 } &   \\mathbf{0 } & \\cdots &   \\mathbf{\\frac{n-1}{n } } &   \\mathbf{1-\\frac{n-1}{n } } &   \\mathbf{0}\\\\   0   &   \\mathbf{0 } &   \\mathbf{0 } & \\cdots & \\mathbf{0 } &    \\mathbf{1 } &   \\mathbf{0}\\\\   \\end{pmatrix}.\\ ] ]    the spectral radius @xmath180 and the asymptotic average convergence rate @xmath181 .",
    "notice that @xmath182 is less than the expected hitting time @xmath183 .    in the onemax function , set @xmath184 , and then @xmath185 and @xmath186 . choose @xmath9 uniformly at random , run the ( 1 + 1 ) ea for 50 generations and 2000 times , and then calculate @xmath22 according to ( [ equft ] ) and @xmath29 according to formula ( [ equaveragerate ] ) . since @xmath9 is chosen uniformly at random , @xmath187 .",
    "[ figonemax ] demonstrates that @xmath29 approximates @xmath188 .",
    "[ figonemax2 ] shows that the theoretical exponential decay , @xmath167 , and the computational progress rate , @xmath168 , coincide perfectly .",
    "table geom-rate.dat ;    table difference.dat ; ;",
    "so far the calculation of the average convergence rate needs the information about @xmath20 .",
    "however this requirement is very strong .",
    "here we introduce an alternative average convergence rate without knowing @xmath20 , which is given as below , @xmath189 where @xmath190 is an appropriate time interval .",
    "its value relies on an ea and a problem .    for the ( 1 + 1 ) ea on the onemax function with @xmath184 , we set @xmath191 .",
    "choose @xmath9 uniformly at random , run the ( 1 + 1 ) ea for 60 generations and 2000 times , and then calculate @xmath22 according to ( [ equft ] ) and @xmath192 according to formula ( [ equnewaveragerate ] ) .",
    "due to @xmath191 , @xmath192 has no value for @xmath193 and @xmath194 according to formula ( [ equnewaveragerate ] ) .",
    "[ figonemax3 ] demonstrates that @xmath192 approximates @xmath195 .",
    "but the calculation of @xmath192 is not as stable as that of @xmath29 in practice .",
    "table rate.dat ;    the above average convergence rate converges to @xmath150 but under stronger conditions than that in theorem  [ theaveragerate ] .",
    "[ thenewaveragerate ] let @xmath102 be the transition submatrix associated with a convergent ea .    1 .   under particular initialization ( that is , set @xmath112 where @xmath92 is an eigenvector corresponding to the eigenvalue @xmath106 with @xmath113 but @xmath114 ) , it holds for all @xmath115 , @xmath196 2 .   under random initialization ( that is @xmath109 ) , choose an appropriate @xmath190 such that @xmath197 for a maximization problem ( or @xmath198 for a minimization problem ) since @xmath199 and @xmath200 for a maximization problem ( or @xmath201 for a minimization problem ) . ]",
    ". if @xmath202 is positive could be relaxed to non - negative @xmath102 if taking a similar argument to the extension of perron - frobenius theorems to non - negative matrices  @xcite .",
    "] , then it holds @xmath203    from ( [ equmatrixiteration ] ) : @xmath145 and ( [ equdifference ] ) , we get @xmath204    \\1 ) since @xmath82 is an eigenvector corresponding to the eigenvalue @xmath106 such that @xmath144 . from ( [ equnewq ] ) and ( [ equmatrixiteration ] ) : @xmath205 , we get @xmath206 then @xmath207 which gives the first conclusion .",
    "\\2 ) without loss of the generality , consider @xmath208 . since @xmath209 let @xmath210_i}{[\\mathbf{q}^t_{t-\\delta t }",
    "] _ i } , & & \\overline{\\lambda}_t=\\max_i \\frac{[\\mathbf{q}^t_{t-\\delta t } \\mathbf{q}^{\\delta t } ] _ i}{[\\mathbf{q}^t_{t-\\delta t } ] _ i},\\end{aligned}\\ ] ] where @xmath211_i$ ] represents the @xmath212th entry in vector @xmath92 .    according to collatz formula  @xcite ( * ? ? ?",
    "* theorem 2 ) , @xmath213 hence for any @xmath214_i > 0",
    "$ ] , it holds @xmath215_i [ \\mathbf{g}]_i}{[\\mathbf{q}^t_{t-\\delta t } ] _ i [ \\mathbf{g}]_i}= \\rho(\\mathbf{q}^{\\delta t } ) , & & \\lim_{t \\to + \\infty } \\max_i \\frac{[\\mathbf{q}^t_{t-\\delta t } \\mathbf{q}^{\\delta t } ] _ i [ \\mathbf{g}]_i}{[\\mathbf{q}^t_{t-\\delta t } ] _ i [ \\mathbf{g}]_i}= \\rho(\\mathbf{q}^{\\delta t } ) .\\end{aligned}\\ ] ] using @xmath216 , we get @xmath217_i [   \\mathbf{g } ] _",
    "i}{\\sum_i[\\mathbf{q}^t_{t-\\delta t } ] _ i [ \\mathbf{g}]_i}= \\rho(\\mathbf{q}^{\\delta t } ) .\\end{aligned}\\ ] ] equivalently @xmath218 then @xmath219 finally it comes to the second conclusion .",
    "the theorem shows that the average convergence rate @xmath192 plays the same role as @xmath29 .",
    "but the calculation of @xmath192 is not as stable as that of @xmath29 in practice .",
    "this paper proposes a new convergence rate of eas , called the average ( geometric ) convergence rate .",
    "the rate represents a normalized geometric mean of the reduction ratio of the fitness difference per generation .",
    "the calculation of the average convergence rate is simple and easy to implement on most eas in practice .",
    "since the rate is normalized , it is convenient to compare different eas on optimization problems .    for discrete optimization ,",
    "lower bounds on the average convergence rate of eas have been established .",
    "it is proven that under random initialization , the average convergence rate @xmath29 for @xmath2 generations converges to a limit , called the asymptotic average convergence rate ; and under particular initialization , @xmath29 equals to the asymptotic average convergence rate for any @xmath147 .",
    "the analysis of eas for continuous optimization is different from that for discrete optimization . in continuous optimization",
    ", an ea is modeled by a markov chain on a general state space , rather than a finite markov chain .",
    "so a different theoretical analysis is needed , rather than matrix analysis used in the current paper .",
    "this topic is left for future research .",
    "f.  schmitt and f.  rothlauf , `` on the importance of the second largest eigenvalue on the convergence rate of genetic algorithms , '' in _ proceedings of 2001 genetic and evolutionary computation conference _ , h.  beyer , e.  cantu - paz , d.  goldberg , parmee , l.  spector , and d.  whitley , eds.1em plus 0.5em minus 0.4emmorgan kaufmann publishers , 2001 , pp .",
    "559564 .",
    " , `` convergence rates of evolutionary algorithms for quadratic convex functions with rank - deficient hessian , '' in _ adaptive and natural computing algorithms_.1em plus 0.5em minus 0.4emspringer , 2013 , pp . 151160 .",
    "o.  teytaud , s.  gelly , and j.  mary , `` on the ultimate convergence rates for isotropic algorithms and the best choices among various forms of isotropy , '' in _ parallel problem solving from nature ( ppsn ix)_.1em plus 0.5em minus 0.4emspringer , 2006 , pp .",
    "3241 .",
    "n.  beume , m.  laumanns , and g.  rudolph , `` convergence rates of sms - emoa on continuous bi - objective problem classes , '' in _ proceedings of the 11th workshop on foundations of genetic algorithms_.1em plus 0.5em minus 0.4emacm , 2011 , pp .",
    "243252 .",
    "m.  jebalia and a.  auger , `` log - linear convergence of the scale - invariant ( @xmath220 , @xmath221)-es and optimal @xmath222 for intermediate recombination for large population sizes , '' in _ parallel problem solving from nature ( ppsn xi)_.1em plus 0.5em minus 0.4emspringer , 2010 , pp .",
    "f.  teytaud and o.  teytaud , `` convergence rates of evolutionary algorithms and parallel evolutionary algorithms , '' in _ theory and principled methods for the design of metaheuristics_.1em plus 0.5em minus 0.4em springer , 2014 , pp .",
    "2539 .                j.",
    "he , f.  he , and h.  dong , `` pure strategy or mixed strategy ? '' in _ evolutionary computation in combinatorial optimization _",
    "hao and m.  middendorf , eds.1em plus 0.5em minus 0.4emspringer , 2012 , pp ."
  ],
  "abstract_text": [
    "<S> in evolutionary optimization , it is important to understand how fast evolutionary algorithms converge to the optimum per generation , or their convergence rates . </S>",
    "<S> this paper proposes a new measure of the convergence rate , called the average convergence rate . </S>",
    "<S> it is a normalized geometric mean of the reduction ratio of the fitness difference per generation . </S>",
    "<S> the calculation of the average convergence rate is very simple and it is applicable for most evolutionary algorithms on both continuous and discrete optimization . a theoretical study of the average convergence rate is conducted for discrete optimization . lower bounds on the average convergence rate are derived . </S>",
    "<S> the limit of the average convergence rate is analyzed and then the asymptotic average convergence rate is proposed .    </S>",
    "<S> evolutionary algorithms , evolutionary optimization , convergence rate , markov chain , matrix analysis </S>"
  ]
}