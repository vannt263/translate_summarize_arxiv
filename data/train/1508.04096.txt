{
  "article_text": [
    "solving systems of linear equations given by symmetric diagonally matrices ( sdd ) is of interest to researchers in a variety of fields .",
    "such constructs , for example , are used to determine solutions to partial differential equations  @xcite and computations of maximum flows in graphs  @xcite .",
    "other application domains include machine learning  @xcite , and computer vision  @xcite .",
    "much interest has been devoted to determining fast algorithms for solving sdd systems .",
    "recently , spielman and teng  @xcite , utilized the multi - level framework of  @xcite , pre - conditioners  @xcite , and spectral graph sparsifiers  @xcite , to propose a nearly linear - time algorithm for solving sdd systems . further exploiting these ingredients , koutis _ et .",
    "@xcite developed an even faster algorithm for acquiring @xmath2-close solutions to sdd linear systems .",
    "further improvements have been discovered by kelner _",
    "@xcite , where their algorithm relied on only spanning - trees and eliminated the need for graph sparsifiers and the multi - level framework .    motivated by applications ,",
    "much progress has been made in developing parallel versions of these algorithms .",
    "koutis and miller  @xcite proposed an algorithm requiring nearly - linear work ( i.e. , total number of operations executed by a computation ) and @xmath3 depth ( i.e. , longest chain of sequential dependencies in the computation ) for planar graphs .",
    "this was then extended to general graphs in  @xcite leading to depth close to @xmath4 . since then",
    ", peng and spielman  @xcite have proposed an efficient parallel solver requiring nearly - linear work and poly - logarithmic depth without the need for low - stretch spanning trees .",
    "their algorithm , which we provide a distribute construction for , requires sparse approximate inverse chains  @xcite which facilitates the solution of the sdd system .",
    "less progress , on the other hand , has been made on the distributed version of these solvers .",
    "contrary to the parallel setting , memory is not shared and is rather distributed in the sense that each unit abides by its own memory restrictions .",
    "furthermore , communication in a distributed setting fundamentally relies on message passing through communication links .",
    "current methods , e.g. , jacobi iteration  @xcite , can be used for such distributed solutions but require substantial complexity . in  @xcite ,",
    "the authors propose a gossiping framework for acquiring a solution to sddm systems in a distributed fashion .",
    "recent work  @xcite considers a local and asynchronous solution for solving systems of linear equations , where they acquire a bound on the number of needed multiplication proportional to the degree and condition number of the graph for one component of the solution vector .",
    "* contributions : * in this paper , we propose a fast distributed solver for linear equations given by symmetric diagonally dominant m - matrices .",
    "our approach distributes the parallel solver in  @xcite by considering a specific approximated inverse chain which can be computed efficiently in a distributed fashion .",
    "we develop two versions of the solver .",
    "the first , requires full communication in the network , while the second is restricted to r - hop neighborhood of nodes for some @xmath0 .",
    "similar to the work in  @xcite , our algorithms operate in two phases . in the first ,",
    "a `` crude '' solution to the system of equations is retuned , while in the second a distributed r - hop restricted pre - conditioner is proposed to drive the `` crude '' solution to an @xmath2-approximate one for any @xmath5 . due to the distributed nature of the setting considered , the direct application of the sparsfier and pre - conditioner of peng and spielman  @xcite is difficult due to the need of global information .",
    "consequently , we propose a new sparse inverse chain which can be computed in a decentralized fashion for determining the solution to the sddm system .",
    "interestingly , due to the involvement of powers of matrices with eigenvalues less than one , our inverse chain is substantially shorter compared to that in  @xcite .",
    "this leads us to a distributed sddm solver with lower computational complexity compared to state - of - the - art methods .",
    "specifically , our algorithm s complexity is given by @xmath6 with @xmath7 being the number of nodes in graph @xmath8 , @xmath9 and @xmath10 denoting the largest and smaller weights of the edges in @xmath8 , respectively , @xmath11 representing the upper bound on the size of the r - hop neighborhood @xmath12 , and @xmath13 $ ] being the precision parameter .",
    "furthermore , our approach improves current linear methods by a factor of @xmath14 and by a factor of the degree compared to   @xcite for each component of the solution vector .",
    "having developed such distributed solvers , we next contribute by proposing an accurate distributed newton method for network flow optimization . exploiting the sparsity pattern of the dual hessian , we propose a newton method for network optimization that is both faster and more accurate than state - of - the - art techniques .",
    "our method utilizes the proposed sddm distributed solvers to approximate the newton direction up to any arbitrary @xmath1 .",
    "the resulting algorithm is an efficient and accurate distributed second - order method which performs almost identically to exact newton .",
    "we analyze the properties of the proposed algorithm and show that , similar to conventional newton methods , superlinear convergence within a neighborhood of the optimal value is attained .",
    "we finally demonstrate the effectiveness of the approach in a set of experiments on randomly generated and barbell networks .",
    "we now review the parallel solver for symmetric diagonally dominant ( sdd ) linear systems  @xcite .      as detailed in  @xcite , sddm solvers consider the following system of linear equations : @xmath15 where @xmath16 is a symmetric diagonally dominant m - matrix ( sddm ) .",
    "namely , @xmath16 is symmetric positive definite with non - positive off diagonal elements , such that for all @xmath17 : @xmath18_{ii } \\ge -\\sum_{j=1 , j\\ne i}^{n}\\left[\\bm{m}_{0}\\right]_{ij}.\\ ] ] the system of equations in  [ lin_sys ] can be interpreted as representing an undirected weighted graph , @xmath8 , with @xmath16 being its laplacian .",
    "namely , @xmath19 , with @xmath20 representing the set of nodes , @xmath21 denoting the edges , and @xmath22 representing the weighted graph adjacency .",
    "nodes @xmath23 and @xmath24 are connected with an edge @xmath25 iff @xmath26 , where : @xmath27_{ij}.\\ ] ] following  @xcite , we seek @xmath2-approximate solutions to @xmath28 , being the exact solution of @xmath29 , defined as :    let @xmath30 be the solution of @xmath29 .",
    "a vector @xmath31 is called an @xmath32 approximate solution , if : @xmath33    the r - hop neighbourhood of node @xmath34 is defined as @xmath35 .",
    "we also make use of the diameter of a graph , @xmath8 , defined as @xmath36 .",
    "we say that a matrix @xmath37 has a sparsity pattern corresponding to the r - hop neighborhood if @xmath38 for all @xmath39 and for all @xmath40 such that @xmath41 .",
    "we will denote the spectral radius of a matrix @xmath42 by @xmath43 , where @xmath44 represents an eigenvalue of the matrix @xmath42 .",
    "furthermore , we will make use of the condition number , @xmath45 of a matrix @xmath42 defined as @xmath46 . in",
    "@xcite it is shown that the condition number of the graph laplacian is at most @xmath47 where @xmath9 and @xmath10 represent the largest and the smallest edge weights in @xmath8 . finally , the condition number of a sub - matrix of the laplacian is at most @xmath48 , see  @xcite .",
    "our first contribution is a distributed version of the parallel solver for sddm systems of equations previously proposed in  @xcite . before detailing our solver ,",
    "however , we next introduce basic mathematical machinery needed for developing the parallel solver of  @xcite .",
    "the parallel solver commences by considering the standard splitting of the symmetric matrix @xmath16 :    the standard splitting of a symmetric matrix @xmath16 is : @xmath49    here , @xmath50 is a diagonal matrix consisting of the diagonal elements in @xmath16 such that : @xmath51_{ii } = \\left[\\bm{m}_{0}\\right]_{ii } \\ \\ \\forall i=1,2,\\dots , n.\\ ] ] furthermore , @xmath52 is a non - negative symmetric matrix such that : @xmath53_{ij}= \\begin{cases } -\\left[\\bm{m}_{0}\\right]_{ij } & : \\text{if",
    "$ i \\ne j$ , } \\\\ 0 & : \\text{otherwise . } \\end{cases}\\end{aligned}\\ ] ]    to quantify the quality of the acquired solutions , we define two additional mathematical constructs .",
    "first , the loewner ordering is defined as :    let @xmath54 be the space of @xmath55-symmetric matrices .",
    "the loewner ordering @xmath56 is a partial order on @xmath54 such that @xmath57 if and only if @xmath58 is positive semidefinite .",
    "having defined the loewner order , we next define the notion of approximation for matrices `` @xmath59 '' :    [ def : ordering ] let @xmath60 and @xmath61 be positive semidefinite symmetric matrices . then @xmath62 if and only iff @xmath63 with @xmath64 meaning @xmath65 is positive semidefinite .",
    "based on the above definitions , the following lemma represents the basic characteristics of the @xmath59 operator :    @xcite[approx_lemma_facts ] let @xmath66 and , @xmath67 be symmetric positive semi definite matrices",
    ". then    1 .",
    "\\(1 ) if @xmath62 , then @xmath68 , 2 .",
    "\\(2 ) if @xmath62 and @xmath69 , then @xmath70 , 3 .",
    "\\(3 ) if @xmath71 and @xmath72 , then @xmath73 4 .",
    "\\(4 ) if @xmath60 , and @xmath61 are non singular and @xmath62 , then @xmath74 , 5 .",
    "\\(5 ) if @xmath62 and @xmath75 is a matrix , then @xmath76 .",
    "since the parallel solver returns an approximation , @xmath77 , to @xmath78 ( see section  [ sec : parallelsolver ] ) , the following lemma shows that `` good '' approximations to @xmath79 guarantee `` good '' approximate solutions to @xmath29 .",
    "[ lemma_approx_matrix_inverse ] let @xmath80 , and @xmath81 , then @xmath82 is @xmath83 approximate solution to @xmath29",
    ".    the proof can be found in the appendix .",
    "[ sec : parrallelsolver ] the parallel sddm solver proposed in  @xcite is a parallelized technique for solving the problem of section  [ sec : probsetting ] .",
    "it makes use of inverse approximated chains ( see definition  [ def : invchain ] ) to determine @xmath82 and can be split in two steps . in the first , algorithm  [ algo : inv ] , a `` crude '' approximation , @xmath84 , of @xmath85",
    "is returned .",
    "@xmath84 is driven to the @xmath2-close solution , @xmath82 , using richardson preconditioning in algorithm  [ algo : inv2 ] .",
    "before we proceed , we start with the following two lemmas which enable the definition of inverse chain approximation .    @xcite[sddm_splitting_lemma ] if @xmath86 is an sddm matrix , with @xmath87 being positive diagonal , and @xmath42 denoting a non - negative symmetric matrix , then @xmath88 is also sddm .",
    "@xcite[approx_inverse_formulae_lemma ] let @xmath89 be an sddm matrix , where @xmath87 is positive diagonal and @xmath42 a symmetric matrix .",
    "then @xmath90.\\end{aligned}\\ ] ]    given the results in lemmas [ sddm_splitting_lemma ] and [ approx_inverse_formulae_lemma ] , we now can consider inverse approximated chains of @xmath91 :    [ def : invchain ] let @xmath92 be a collection of sddm matrices such that @xmath93 , with @xmath94 a positive diagonal matrix , and @xmath95 denoting a non - negative symmetric matrix .",
    "then @xmath96 is an inverse approximated chain if there exists positive real numbers @xmath97 such that :    1 .",
    "\\(1 ) @xmath98 , 2 .",
    "\\(2 ) @xmath99 , and 3 .",
    "\\(3 ) @xmath100 .",
    "it is shown in  @xcite that an approximate inverse chain allows for `` crude '' solutions to the system of linear equations in @xmath101 in time proportional to the number of non - zeros entries in the matrices in the inverse chain .",
    "such a procedure is summarized in the following algorithm :    [ algo : crudeparallel ]    * input * : inverse approximated chain , @xmath102 , and @xmath103 being * output * : the `` crude '' approximation , @xmath104 , of @xmath28 @xmath105 @xmath106 @xmath107 $ ] * return * @xmath108    on a high level , algorithm  [ algo : crudeparallel ] operates in two phases . in the first ( i.e. , lines  3 - 5 ) a forward loop ( up - to the length of the inverse chain @xmath109 ) computes intermediate vectors @xmath110 as : @xmath111 for @xmath112 .",
    "these can then be used to compute the `` crude '' solution @xmath84 using a `` backward '' loop ( i.e. , lines  7 - 9 ) .",
    "consequently , the crude solution is computed iteratively backwards as :    @xmath113,\\ ] ]    with @xmath114 and @xmath110 as defined in equation  [ eq : bupdate ] .",
    "the quality of the `` crude '' solution returned by the algorithm is quantified in the following lemma :    @xcite[rude_alg_guarantee_lemma ] let @xmath115 be the inverse approximated chain and denote @xmath116 be the operator defined by @xmath117 , namely , @xmath118",
    ". then @xmath119    having returned a `` crude '' solution to @xmath29 , the authors in  @xcite obtain arbitrary close solutions using the _ preconditioned richardson iterative scheme_. the first step in the exact solver is the usage of algorithm  [ algo : crudeparallel ] to obtain the `` crude '' solution @xmath120 .",
    "this is then updated through the loop in lines  4 - 8 to obtain an @xmath2-close solution to @xmath28 , see algorithm  [ algo : inv2 ]",
    ".    * input * : inverse approximated chain @xmath102 , @xmath103 , and @xmath2 .",
    "* output * : @xmath2 close approximation , @xmath82 , of @xmath121 * initialize * : @xmath122 ; + @xmath123 ( i.e. , algorithm  [ algo : inv ] ) @xmath124 @xmath125 @xmath126 @xmath127 * return * @xmath82    following the analysis in  @xcite , lemma  [ exact_alg_guarantee_lemma ] provides the iteration count needed by algorithm  [ algo : inv2 ] to arrive at @xmath82 :    @xcite[exact_alg_guarantee_lemma ] let @xmath128 be an inverse approximated chain such that @xmath129",
    ". then @xmath130 requires @xmath131 iterations to arrive at an @xmath2 close solution of @xmath28 with : @xmath132 .",
    "having introduced the parallel solver , next we detail our first contribution by proposing a distributed solver for sddm linear systems . in particular , we develop two versions .",
    "the first , requires full communication in the network , while the second restricts communication to the r - hop neighborhood increasing its applicability .",
    "not only our solver improves the computational complexity of distributed methods for system of equations represented by an sddm matrix , but can also be applied to a variety of fields including distributed newton methods , computer vision , among others .    to compute the solution of sddm systems in a distributed fashion ,",
    "we follow a similar strategy to that of  @xcite with major differences .",
    "our distributed solver requires two steps to arrive at an @xmath2-close approximation to @xmath28 .",
    "similar to  @xcite , the first step adopts an inverse approximated chain to determine a `` crude '' solution to @xmath28 .",
    "the inverse chain proposed in  @xcite can not be computed in a distributed fashion rendering its immediate application to our setting difficult .",
    "hence , we par - ways with  @xcite by proposing an inverse chain which can be computed in a distributed fashion .",
    "this chain , defined in section  [ sec : chain ] , enables the distributed computation of both a crude and exact solution to @xmath29 .",
    "interestingly , due to the involvement of matrices with eigenvalues less than 1 , the length , @xmath109 , of our inverse chain is substantially shorter compared to that of  @xcite , allowing for fast and efficient distributed solvers . given the crude solution , the second step computes an @xmath2-close approximation to @xmath28 .",
    "this is achieved by proposing a distributed version of the richardson pre - conditioning scheme . definitely , this step is also similar in spirit to that in  @xcite , but generalizes the aforementioned authors work into a distributed setting and allows for @xmath2-close approximation to @xmath28 for _ any arbitrary _ @xmath5 .",
    "main results on the full communication version of the solver are summarized in the following theorem :    [ main_theorem ] there exists a distributed algorithm , @xmath133_{k1},\\ldots [ \\bm{m}_0]_{kn}\\ } , [ \\bm{b}_0]_k , \\epsilon\\right),\\ ] ] that computes @xmath2-close approximations to the solution of @xmath29 in @xmath134 time steps , with @xmath7 the number of nodes in @xmath8 , @xmath135 the condition number of @xmath91 , and @xmath136_{k\\cdot}$ ] the @xmath137 row of @xmath16 , as well as @xmath138 $ ] representing the precision parameter .",
    "the above distributed algorithms require no knowledge of the graph s topology , but do require the information from all other nodes ( i.e. , full communication ) for computing solutions to @xmath29 . in a variety of real - world applications ( e.g. , smart - grids , transportation )",
    "load , capacity , money and resource restrictions pose problems for such a requirement .",
    "consequently , we extend the previous solvers to an r - hop version in which communication is restricted to the r - hop neighborhood between nodes for some @xmath139 . again",
    "we follow a two - step strategy , where in the first we compute the crude solution and in the second an @xmath2-close approximation to @xmath84 is determined using an `` r - hop restricted '' richardson pre - conditioner .",
    "these results are captured in the following theorem :    [ main_theoremtwo ] there is a decentralized algorithm , @xmath140_{k1},\\ldots [ \\bm{m}_0]_{kn}\\ } , [ \\bm{b}_0]_k , r , \\epsilon),\\ ] ] that uses only @xmath141-hop communication between the nodes and computes @xmath2-close solutions to @xmath29 in @xmath142 time steps , with @xmath7 being the number of nodes in @xmath8 , @xmath143 denoting the maximal degree , @xmath135 the condition number of @xmath91 , and @xmath144 representing the upper bound on the size of the r - hop neighborhood @xmath145 , and @xmath146 $ ] being the precision parameter .",
    "the remainder of the section details the above distributed solvers and provides rigorous theoretical guarantees on the convergence and convergence rates of each of the algorithms .",
    "we start by describing solvers requiring full network communication and then detail the r - hop restricted versions .",
    "as mentioned previously , our strategy for a distributed implementation of the parallel solver in  @xcite requires two steps . in the first a `` crude '' solution",
    "is returned , while in the second an @xmath2-close approximation ( for any arbitrary @xmath1 ) to @xmath84 is computed .",
    "the distributed crude solver , represented in algorithm  [ algo : disrudeapprox ] , resembles similarities to the parallel one of  @xcite with major differences . on a high level ,",
    "algorithm  [ algo : disrudeapprox ] operates in two distributed phases .",
    "in the first , a forward loop computes intermediate @xmath147 vectors which are then used to update the crude solution of @xmath29 .",
    "the crucial difference to  @xcite , however , is the distributed nature of these computations . precisely , the algorithm is responsible for determining the crude solution for each node @xmath148 . due to such distributed nature , the inverse approximated chain used in  @xcite is inapplicable to our setting . therefore , the second crucial difference to the parallel sddm solver is the introduction of a new chain which can be computed in a distributed fashion .",
    "starting from @xmath149 , our `` crude '' distributed solver makes use of the following collection as the inverse approximated chain : @xmath150 where @xmath151 , @xmath152 , for @xmath153 with @xmath109 being the length of the inverse chain .",
    "note that since the magnitude of the eigenvalues of @xmath154 is strictly less than 1 , @xmath155 tends to zero as @xmath156 increases which reduces the length of the chain needed .",
    "this length is explicitly computed in section  [ sec : length ] for attaining @xmath2-close approximations to @xmath28 .",
    "it is relatively easy to verify that @xmath96 is an inverse approximated chain , since :    1 .",
    "\\(1 ) @xmath157 with @xmath158 for @xmath159 , 2 .",
    "\\(2 ) @xmath160 with @xmath158 for @xmath159 , and 3 .",
    "\\(3 ) @xmath161 .",
    "algorithm  [ algo : disrudeapprox ] returns the @xmath137 component of the approximate solution vector , @xmath162_{k}$ ] . as inputs",
    "it requires the inverse chain of equation  [ eq : inversechain ] , the @xmath137 component of @xmath163 , and the length of the inverse chain .",
    "namely , each node , @xmath164 , receives the @xmath137 row of @xmath91 , the @xmath137 value of @xmath163 ( i.e. , @xmath165_k$ ] ) , and the length of the inverse approximated chain @xmath109 and then operates in two parts . in the first (",
    "i.e. , lines  1 - 8 ) a forward loop computes the @xmath137 component of @xmath147 exploiting the distributed inverse chain , while in the second a backward loop ( lines  9 - 17 ) is responsible for computing the @xmath137 component of the``crude '' solution @xmath162_{k}$ ] which is then returned .",
    "essentially , in both the forward and backward loops each of the @xmath147 and @xmath166 vectors are computed in a distributed fashion based on the relevant components of the matrices , explaining the usage of @xmath167 loops in algorithm  [ algo : disrudeapprox ] .    *",
    "part one : computing @xmath168_{k}$ ] * @xmath169_k = [ \\bm{b}_0]_k + \\sum_{j : \\bm{v}_j\\in \\mathbb{n}_{1}\\left(\\bm{v}_k\\right)}[\\bm{a}_0\\bm{d}^{-1}_0]_{kj}[\\bm{b}_{0}]_j$ ] @xmath170_{kj } =   \\sum_{r=1}^{n}\\frac{[\\bm{d}_0]_{rr}}{[\\bm{d}_0]_{jj}}\\left[(\\bm{a}_0\\bm{d}^{-1}_0)^{2^{i-2}}\\right]_{kr } \\left[(\\bm{a}_0\\bm{d}^{-1}_0)^{2^{i-2}}\\right]_{jr}$ ] @xmath171_k = [ \\bm{b}_{i-1}]_k + \\sum_{j : \\bm{v}_j \\in \\mathbb{n}_{2^{i-1}}\\left(\\bm{v}_k\\right)}\\left[(\\bm{a}_0\\bm{d}^{-1}_0)^{2^{i-1}}\\right]_{kj}[\\bm{b}_{i-1}]_{j}$ ] + * part two : computing @xmath162_{k}$ ] * @xmath172_k = \\sfrac{[\\bm{b}_d]_k}{[\\bm{d}_0]_{kk}}$ ] @xmath173_{kj } =               \\sum_{r=1}^{n}\\frac{[\\bm{d}_0]_{jj}}{[\\bm{d}_0]_{rr}}\\left[(\\bm{d}^{-1}_0\\bm{a}_0)^{2^{i-1}}\\right]_{kr } \\left[(\\bm{d}^{-1}_0\\bm{a}_0)^{2^{i-1}}\\right]_{jr}$ ] @xmath174_k =   \\frac{[\\bm{b}_i]_k}{2[\\bm{d}_0]_{kk } } + \\frac{[\\bm{x}_{i+1}]_{k+1}}{2 } + \\frac{1}{2}\\sum_{j : \\bm{v}_j \\in \\mathbb{n}_{2^{i}}(\\bm{v}_k)}\\left[(\\bm{d}^{-1}_0\\bm{a}_0)^{2^i}\\right]_{kj}[\\bm{x}_{i+1}]_j$ ] @xmath175_k = \\frac{[\\bm{b}_0]_k}{2[\\bm{d}_{0}]_{kk } } + \\frac{[\\bm{x}_{1}]_k}{2 } + \\frac{1}{2}\\sum_{j:\\bm{v}_j\\in \\mathbb{n}_{1}(\\bm{v}_k)}[\\bm{d}^{-1}_0\\bm{a}_0]_{kj}[\\bm{x}_1]_j$ ] * return : * @xmath175_k$ ]    * theoretical guarantees of algorithm  [ algo : disrudeapprox ] : * due to the modifications made to the original parallel solver , new theoretical analysis quantifying convergence and accuracy of the returned `` crude '' solution is needed .",
    "we show that @xmath176 computes the @xmath137 component of the `` crude '' approximation of @xmath28 and provide time complexity analysis .",
    "these results are summarized in the following lemma :    [ rude_dec_alg_guarantee_lemma ] let @xmath177 be the standard splitting of @xmath16 .",
    "let @xmath178 be the operator defined by @xmath179_{k1},\\ldots , [ \\bm{m}_0]_{kn}\\ } , [ \\bm{b}_0]_k , d)$ ] ( i.e. , @xmath180 ) .",
    "then @xmath181 moreover , algorithm  [ algo : disrudeapprox ] requires @xmath182 time steps .    in words",
    ", lemma  [ rude_dec_alg_guarantee_lemma ] states that algorithm  [ algo : disrudeapprox ] requires @xmath182 to arrive at an @xmath183 approximation to the real inverse @xmath78 , where this approximation is quantified using definition  [ def : ordering ] : @xmath184    @xmath185 can then be used to compute the crude solution as @xmath186 .",
    "note that the accuracy of approximating @xmath16 is limited to @xmath183 motivating the need for an `` exact '' distributed solver reducing the error to any @xmath187 .",
    "having introduced @xmath176 , we are now ready to present a distributed version of algorithm  [ algo : inv2 ] which enables the computation of @xmath2 close solutions for @xmath29 .",
    "contrary to the work of  @xcite , our algorithm is capable of acquiring solutions up to any arbitrary @xmath5 .",
    "similar to @xmath176 , each node @xmath164 receives the @xmath137 row of @xmath91 , @xmath165_k$ ] , @xmath109 and a precision parameter @xmath2 as inputs .",
    "node @xmath188 then computes the @xmath137 component of the @xmath2 close approximation of @xmath28 by using @xmath176 as a sub - routine and updates the solution iteratively as shown in lines  2 - 6 in algorithm  [ alg_exactbla ] .",
    "* initialize * : @xmath189_k = 0 $ ] ; @xmath190_k = \\text{distrrsolve}\\left(\\{[\\bm{m}_0]_{k1},\\ldots , [ \\bm{m}_0]_{kn}\\ } , [ \\bm{b}_0]_k , d\\right)$ ] ( i.e. , algorithm  [ algo : disrudeapprox ] ) @xmath191_k = [ \\bm{d}_0]_{kk}[\\bm{y}_{t-1}]_k - \\sum_{j : \\bm{v}_j\\in \\mathbb{n}_{1}(\\bm{v}_k)}[\\bm{a}_{0}]_{kj}[\\bm{y}_{t-1}]_j$ ] @xmath192_k = \\text{distrrsolve}(\\{[\\bm{m}_0]_{k1},\\ldots , [ \\bm{m}_0]_{kn}\\ } , \\left[\\bm{u}^{(1)}_{t}\\right]_k , d,)$ ] @xmath193_k = [ \\bm{y}_{t-1}]_k - \\left[\\bm{u}^{(2)}_{t}\\right]_k + [ \\chi]_k$ ] @xmath194_k = [ \\bm{y}_q]_k$ ] * return * @xmath194_k$ ]    [ alg_exactbla ]    * analysis of algorithm  [ alg_exactbla ] : * here , we again provide the theoretical analysis needed for quantifying the convergence and computational time of the exact algorithm for returning @xmath2-close approximation to @xmath28 .",
    "the following lemma shows that @xmath195 computes the @xmath137 component of the @xmath2-close approximation of @xmath196 :    [ dist_exact_algorithm_guarantee_lemma ] let @xmath177 be the standard splitting .",
    "further , let @xmath197 in the nverse approximated chain @xmath198 .",
    "then @xmath199_{k1},\\ldots , [ \\bm{m}_0]_{kn}\\ } , [ b_0]_k , d , \\epsilon)$ ] requires @xmath200 iterations to return the @xmath137 component of the @xmath2 close approximation for @xmath28 .",
    "the above lemma proofs that the algorithm requires @xmath201 iterations for attaining for returning the @xmath137 of the @xmath2-close approximation to @xmath28 .",
    "consequently , the overall complexity can be summarized as :    [ time_complexity_of_distresolve ] let @xmath202 be the standard splitting .",
    "further , let @xmath197 in the inverse approximated chain @xmath198 .",
    "then , @xmath199_{k1},\\ldots , [ \\bm{m}_0]_{kn}\\ } , [ \\bm{b}_0]_k , d , \\epsilon)$ ] requires @xmath203 time steps .",
    "both introduced algorithms depend on the length of the inverse approximated chain , @xmath109 . here",
    ", we provide an analysis to determine the value of @xmath109 which guarantees @xmath197 in @xmath198 :    [ eps_d_lemma ] let @xmath177 be the standard splitting and @xmath135 denote the condition number of @xmath91 . consider the inverse approximated chain @xmath204 with @xmath205{2}}{\\sqrt[3]{2 } - 1}\\right)\\kappa\\right)\\rceil$",
    "] , then @xmath206 with @xmath197 .",
    "the proof will be given as a collection of claims :    let @xmath135 be the condition number of @xmath177 , and @xmath207 denote the eigenvalues of @xmath154 .",
    "then , @xmath208 , for all @xmath209    see appendix .",
    "notice that if @xmath210 represented an eigenvalue of @xmath154 , then @xmath211 is an eigenvalue of @xmath212 for all @xmath213 .",
    "therefore , we have @xmath214    let @xmath215 be an sddm matrix and consider the splitting @xmath216 , with @xmath87 being non negative diagonal and @xmath42 being symmetric non negative .",
    "further , assume that the eigenvalues of @xmath217 lie between @xmath218 and @xmath219",
    ". then , @xmath220    see appendix",
    ".    combining the above results , gives @xmath221\\bm{d}_d\\preceq \\bm{d}_d - \\bm{a}_d\\preceq\\left[1 + \\left(1 - \\frac{1}{\\kappa}\\right)^{2^d}\\right]\\bm{d}_d.\\ ] ] hence , to guarantee that @xmath161 , the following system must be satisfied : @xmath222 introducing @xmath223 for @xmath224 , we arrive at : @xmath225 hence , @xmath226 .",
    "now , notice that if @xmath227 then , @xmath228 .",
    "hence , @xmath229 .",
    "this gives @xmath230{2}}{\\sqrt[3]{2 } - 1}\\right)\\rceil$ ] , implying @xmath231 .",
    "using the above results the time complexity of @xmath195 with @xmath205{2}}{\\sqrt[3]{2 } - 1}\\right)\\kappa\\right)\\rceil$ ] is @xmath232 times steps , which concludes the proof of theorem [ main_theorem ] .",
    "the above version of the distributed solver requires no knowledge of the graph s topology , but does require the information from all other nodes .",
    "next , we will outline an r - hop version of the algorithm in which communication is restricted to the r - hop neighborhood between nodes . due to such communication constraints ,",
    "the r - hop solver is general enough to be applied in a variety of fields including but not limited to , network flow problems ( see section  [ sec : newton ] ) . along with theorem  [ main_theoremtwo ]",
    ", the following corollary summarizes the results of the r - hop distributed solver :    [ corollarytwo ] let @xmath91 be the weighted laplacian of @xmath19 .",
    "there exists a decentralized algorithm that uses only @xmath141-hop communication between nodes and computes @xmath2 close solutions of @xmath29 in @xmath233 time steps , with @xmath7 being the number of nodes in @xmath8 , @xmath234 denoting the largest and the smallest weights of edges in @xmath8 , respectively , @xmath144 representing the upper bound on the size of the r - hop neighborhood @xmath145 , and @xmath146 $ ] being the precision parameter .",
    "similar to development of the full communication solver , the r - hop version also requires two steps to attain the @xmath2-close approximation to @xmath28 , i.e. , the `` crude r - hop '' and the `` exact r - hop '' solutions .    *",
    "part one : * @xmath235_{k1},\\ldots,[\\bm{c}_0]_{kn}\\ } = \\text{f}_0\\left([\\bm{m}_0]_{k1},\\ldots , [ \\bm{m}_0]_{kn } , r\\right)$ ] @xmath236_{k1},\\ldots,[\\bm{c}_1]_{kn}\\ } = \\text{f}_1\\left([\\bm{m}_0]_{k1},\\ldots , [ \\bm{m}_0]_{kn } , r\\right)$ ] + * part two : * @xmath237 @xmath238_k = [ \\bm{a}_0\\bm{d}^{-1}_0\\bm{b}_{i-1}]_k$ ] @xmath239_k = \\text{f}_2([\\bm{u}^{(i-1)}_{1}]_k)$ ] @xmath171_k = [ \\bm{b}_{i-1}]_k + [ \\bm{u}^{(i-1)}_{l_{i-1}}]_k$ ] +   + * part three :",
    "* @xmath172_k = \\sfrac{[\\bm{b}_d]_k}{[\\bm{d}_0]_{kk}}$ ] @xmath240 @xmath241_k = [ \\bm{d}^{-1}_0\\bm{a}_0\\bm{x}_{i+1}]_k$ ] @xmath242_k   = \\text{f}_3([\\bm{\\eta}^{(i+1)}_{1}]_k)$ ]    @xmath243_k = \\frac{1}{2}\\left[\\frac{[\\bm{b}_i]_k}{[\\bm{d}_0]_{kk } }   + [ \\bm{x}_{i+1}]_k + [ \\bm{\\eta}^{i+1}_{l_i}]_k \\right]$ ] + @xmath175_k = \\frac{1}{2}\\left[\\frac{[\\bm{b}_0]_k}{[\\bm{d}_0]_{kk } } + [ \\bm{x}_1]_k + [ \\bm{d}^{-1}_0\\bm{a}_0\\bm{x}_1]_k \\right]$ ] * return * @xmath175_k$ ]      the `` crude r - hop '' solver uses the same inverse approximated chain as that of the full communication version ( see equation  [ eq : inversechain ] ) to acquire a `` crude '' approximation for the @xmath137 component @xmath84 while only requiring r - hop communication between the nodes .",
    "algorithm  [ algo : distrhop ] represents the `` crude '' r - hop solver requiring the inverse chain , @xmath137 component of @xmath163 , length of the inverse chain @xmath109 , and the communication bound @xmath141 as inputs .",
    "namely , each node @xmath148 receives the @xmath137 row of @xmath16 , @xmath137 component , @xmath244_{k}$ ] , of @xmath163 , the length of the inverse chain , @xmath109 , and the local communication bound is assumed to be in the order of powers of 2 , i.e. , @xmath245 . ] @xmath141 as inputs to output the @xmath137 component of the `` crude '' approximation to @xmath28 .",
    "algorithm  [ algo : distrhop ] operates in three major parts . due to the need of the r - powers of @xmath246 and @xmath247 ,",
    "the first step is to compute such matrices in a distributed manner .",
    "@xmath248_{kj } = \\sum\\limits_{r:\\bm{v}_r\\in \\mathbb{n}_1(v_j)}\\frac{[\\bm{d}_0]_{rr}}{[\\bm{d}_0]_{jj}}[(\\bm{a}_0\\bm{d}^{-1}_0)^l]_{kr}[\\bm{a}_0\\bm{d}^{-1}_0]_{jr}$ ] * return * @xmath249_{k1},\\ldots,[(\\bm{a}_0\\bm{d}^{-1}_0)^{r}]_{kn } \\}$ ]    given the inverse chain and the communication bound , r , @xmath250 and @xmath251 serve this cause as detailed in algorithms  [ alg_4 ] and  [ alg_5 ] , respectively . essentially , these algorithms execute multiplications needed for determining @xmath252 and @xmath253 in a distributed fashion looping over the relevant hops of the network . for a node , @xmath148 , the @xmath137 component of these powers are returned to algorithm  [ algo : distrhop ] as @xmath254_{ki}=\\left[\\left(\\bm{a}_{0}\\bm{d}_{0}^{-1}\\right)^{r}\\right]_{ki}$ ] and @xmath255_{ki}=\\left[\\left(\\bm{d}_{0}\\bm{a}_{0}^{-1}\\right)^{r}\\right]_{ki}$ ] for @xmath256 ; see part one in algorithm  [ algo : distrhop ] .",
    "similar to the full communication version , the second two parts of the `` crude r - hop '' solver run two loops . in the first ,",
    "the @xmath137 component of @xmath110 is computed by looping forward through the inverse chain , while in the second the @xmath137 component of the crude solution is determined by looping backwards .",
    "@xmath257_{kj } = \\sum\\limits_{r:\\bm{v}_r\\in \\mathbb{n}_1(\\bm{v}_j)}\\frac{[\\bm{d}_0]_{jj}}{[\\bm{d}_0]_{rr}}[(\\bm{d}^{-1}_0\\bm{a}_0)^l]_{kr}[\\bm{d}^{-1}_0\\bm{a}_0]_{jr}$ ] * return * @xmath258_{k1},\\ldots,[(\\bm{d}^{-1}_0\\bm{a}_0)^{r}]_{kn } \\}$ ]    the second part of the solver is better depicted in the flow diagrams of figures  [ fig : flowdiagramone ] and  [ fig : flowdiagramtwo ] . within the first loop running through the length of the inverse chain",
    ", the condition @xmath259 is checked . in case",
    "this condition is true , @xmath260 , @xmath261 , and the previous iteration vector @xmath262 are used to update @xmath110 as shown in figure  [ fig : flowdiagramone ] .",
    "@xmath263_{k}=\\left[\\bm{b}_{i-1}\\right]_{k}+\\left[\\bm{u}_{2^{i-1}}^{(i-1)}\\right]_{k}\\ ] ]    this is performed using another loop constructing a series of @xmath264_{k}$ ] vectors for @xmath265 , used to update the @xmath137 component of @xmath110 at the @xmath266 iteration : at the next @xmath266 iteration , the condition @xmath259 is checked again . in case",
    "this condition is met , the previous computations are executed again . otherwise , the commands depicted in figure  [ fig : flowdiagramtwo ] run . here",
    ", a temporary variable denoting the fraction to the communication bound @xmath141 , @xmath267 , is used to determine the upper iterate bound in @xmath40 . again throughout this loop , a series of @xmath268 vectors used to update @xmath168_{k}$ ] are constructed .",
    "having terminated the forward loop , the @xmath137 component of the `` crude '' solution is computed by looping backward through the inverse approximated chain ( see part three in algorithm  [ algo : distrhop ] ) . for @xmath269 running backwards to @xmath270 , an r - hop condition , @xmath271 , is checked . in case @xmath272 , the following update is performed : @xmath273_{k}=\\left[\\bm{d}^{-1}_{0}\\bm{a}_{0}\\bm{\\eta}_{j-1}^{(i+1)}\\right]_{k},\\ ] ] for @xmath274 , @xmath275 , and @xmath276_{k}=\\left[\\bm{d}^{-1}_{0}\\bm{a}_{0}\\bm{x}_{i+1}\\right]_{k}.\\ ] ]    the role of @xmath277 are backward intermediate solutions needed for updating the `` crude '' solution to @xmath29 : @xmath278_{k}=\\frac{1}{2}\\left[\\frac{[\\bm{b}_{i}]_{k}}{[\\bm{d}_{0}]_{kk}}+[\\bm{x}_{1}]_{k}+[\\bm{\\eta}^{(i+1)}_{2^{i}}]\\right]_{k},\\ ] ] for a node @xmath148 . in case",
    "@xmath279 a similar set of computations are executed for updating the crude solution using : @xmath278_{k}=\\frac{1}{2}\\left[\\frac{[\\bm{b}_{i}]_{k}}{[\\bm{d}_{0}]_{kk}}+[\\bm{x}_{1}]_{k}+[\\bm{d}_{0}^{-1}\\bm{a}\\bm{x}_{1}]\\right]_{k}.\\ ] ]    * analysis of algorithm  [ algo : distrhop ] * similar to the previous section , we next provide the theoretical analysis needed for quantifying the performance of the crude r - hop solver .",
    "the following lemma shows that @xmath280 computes the @xmath137 component of the `` crude '' approximation of @xmath28 and provides the algorithm s time complexity :    [ r_hop_rude_lemma ] let @xmath177 be the standard splitting and let @xmath178 be the operator defined by @xmath280 , namely , @xmath180 , then @xmath281 .",
    "furthermore , @xmath280 requires @xmath282 where @xmath144 to arrive at @xmath84 .",
    "the proof of the above lemma can be obtained by proving a collection of claims :    [ claim_1 ] matrices @xmath212 and @xmath283 have sparsity patterns corresponding to the @xmath284-hop neighborhood for any @xmath213 .",
    "the above claim is proved by induction on @xmath141 .",
    "we start with the base case : for @xmath285 , @xmath286_{ij } = \\begin{cases }   & \\frac{[\\bm{a}_0]_{ij}}{[\\bm{d}_0]_{ii } } \\ \\ \\   \\text{if } j : \\bm{v}_j\\in \\mathbb{n}_{1}(\\bm{v}_i ) \\\\   & 0 \\ \\ \\ \\text{otherwise . } \\end{cases}\\end{aligned}\\ ] ] therefore , @xmath287 has a sparsity pattern corresponding to the @xmath270-hop neighborhood .",
    "assume that for all @xmath288 , @xmath289 has a sparsity pattern corresponding to the @xmath290 neighborhood .",
    "consider , @xmath283 @xmath291_{ij } = \\sum_{k=1}^{n}[(\\bm{a}_0\\bm{d}^{-1}_0)^{r-1}]_{ik}[\\bm{a}_0\\bm{d}^{-1}_0]_{kj}\\ ] ] since @xmath287 is non negative , then @xmath292_{ij } \\ne 0 $ ] iff there exists @xmath156 such that @xmath293 and @xmath294 , namely , @xmath295 .",
    "the proof can be done in a similar fashion for @xmath154 .",
    "the next claim provides complexity guarantees for @xmath250 and @xmath251 described in algorithms  [ alg_4 ] and  [ alg_5 ] , respectively .",
    "[ claim_2 ] algorithms  [ alg_4 ] and  [ alg_5 ] use only the r - hop information to compute the @xmath137 row of @xmath296 and @xmath297 , respectively , in @xmath298 time steps , where @xmath144 .",
    "the proof will be given for @xmath250 described in algorithm  [ alg_4 ] as that for @xmath251 can be performed similarly .",
    "due to claim  [ claim_1 ] , we have @xmath299_{kj } & = \\sum\\limits_{r=1}^{n}\\left[\\left(\\bm{a}_0\\bm{d}^{-1}_0\\right)^{l}\\right]_{kr}\\left[\\bm{a}_0\\bm{d}^{-1}_0\\right]_{rj } = \\sum\\limits_{r : \\bm{v}_r\\in \\mathbb{n}_1(\\bm{v}_j)}\\left[\\left(\\bm{a}_0\\bm{d}^{-1}_0\\right)^{l}\\right]_{kr}\\left[\\bm{a}_0\\bm{d}^{-1}_0\\right]_{rj}\\end{aligned}\\ ] ] therefore at iteration @xmath300 , @xmath188 computes the @xmath137 row of @xmath301 using :    1 .",
    "\\(1 ) the @xmath137 row of @xmath302 , and 2 .",
    "\\(2 ) the @xmath303 column of @xmath287 .",
    "node @xmath304 , however , can only send the @xmath303 row of @xmath305 making @xmath305 non - symmetric . noting that @xmath306_{rj}}{[\\bm{d}_0]_{rr } }",
    "= \\sfrac{[\\bm{a}_0\\bm{d}^{-1}_0]_{jr}}{[\\bm{d}_0]_{jj}}$ ] , since @xmath307 is symmetric , leads to @xmath308_{kj } =   \\sum\\limits_{r : \\bm{v}_r\\in \\mathbb{n}_1(\\bm{v}_j)}\\frac{[\\bm{d}_0]_{rr}}{[\\bm{d}_0]_{jj}}[(\\bm{a}_0\\bm{d}^{-1}_0)^{l}]_{kr}[\\bm{a}_0\\bm{d}^{-1}_0]_{jr}$ ] . to prove the time complexity guarantee , at each iteration @xmath188 computes at most @xmath309 values , where @xmath310 is the upper bound on the size of the r - hop neighborhood @xmath145 .",
    "each such computation requires at most @xmath311 operations .",
    "thus , the overall time complexity is given by @xmath312 .",
    "we are now ready to provide the proof of lemma [ r_hop_rude_lemma ] .    from * parts two * and * three * of algorithm  [ algo : distrhop ] , it is clear that node @xmath188 computes @xmath169_k,[\\bm{b}_2]_k,\\ldots , [ \\bm{b}_d]_k$ ] and @xmath172_k , [ \\bm{x}_{d-1}]_k,\\ldots , [ \\bm{x}_0]_k$ ] , respectively . these are determined using the inverse approximated chain as follows @xmath313 = \\frac{1}{2}[\\bm{d}^{-1}_0\\bm{b}_i + \\bm{x}_{i+1 } + ( \\bm{d}^{-1}_0\\bm{a}_0)^{2^i}\\bm{x}_{i+1}]\\end{aligned}\\ ] ]    considering the computation of @xmath169_k,\\ldots , [ \\bm{b}_d]_k$ ] for @xmath314 , we have @xmath315_k & = [ \\bm{b}_{i-1}]_k + [ ( \\bm{a}_0\\bm{d}^{-1}_0)^{2^{i-1}}\\bm{b}_{i-1}]_k = [ \\bm{b}_{i-1}]_k + [ \\underbrace{\\bm{a}_0\\bm{d}^{-1}_0 \\ldots \\bm{a}_0\\bm{d}^{-1}_0}_{2^{i-1}}\\bm{b}_{i-1}]_k \\\\ & = [ \\bm{b}_{i-1}]_k + [ \\underbrace{\\bm{a}_0\\bm{d}^{-1}_0 \\ldots",
    "\\bm{a}_0\\bm{d}^{-1}_0}_{2^{i-1}-1}\\bm{u}^{(i-1)}_1]_k \\dots = [ \\bm{b}_{i-1}]_k + \\left[\\bm{u}^{(i-1)}_{2^{i-1}}\\right]_k\\end{aligned}\\ ] ] since @xmath287 has a sparsity pattern corresponding to 1-hop neighborhood ( see claim  [ claim_1 ] ) , node @xmath188 computes @xmath316_k$ ] , based on @xmath317 , acquired from its 1-hop neighbors . it is easy to see that @xmath318 the computation of @xmath171_k$ ] requires @xmath319 time steps .",
    "thus , the computation of @xmath169_k , \\ldots , [ \\bm{b}_{\\rho}]_k$ ] requires @xmath320 .",
    "now , consider the computation of @xmath171_k$ ] but for @xmath321 @xmath315_k & = [ \\bm{b}_{i-1}]_k + [ ( \\bm{a}_0\\bm{d}^{-1}_0)^{2^{i-1}}\\bm{b}_{i-1}]_k   =",
    "[ \\bm{b}_{i-1}]_k + [ \\underbrace{\\bm{c}_0\\ldots \\bm{c}_0}_{l_{i-1}}\\bm{b}_{i-1}]_k \\\\ & = [ \\bm{b}_{i-1}]_k + [ \\underbrace{\\bm{c}_0 \\ldots \\bm{c}_0}_{l_{i-1}-1}\\bm{u}^{(i-1)}_1]_k   = [ \\bm{b}_{i-1}]_k + \\left[\\bm{u}^{(i-1)}_{l_{i-1}}\\right]_k\\end{aligned}\\ ] ] with @xmath322 , @xmath323 , and @xmath324 for @xmath325 .",
    "since @xmath326 has a sparsity pattern corresponding to r - hop neighborhood ( see claim  [ claim_1 ] ) , node @xmath188 computes @xmath327_k$ ] based on the components of @xmath317 attained from its r - hop neighbors . for each @xmath269 such that @xmath328 the computing @xmath171_k$ ] requires @xmath329 time steps , where @xmath144 being the upper bound on the number of nodes in the @xmath330 hop neighborhood @xmath331 .",
    "therefore , the overall computation of @xmath332_k , [ \\bm{b}_{\\rho + 2}]_k , \\ldots , [ \\bm{b}_d]_k$ ] is achieved in @xmath333 time steps . finally , the time complexity for the computation of all of the values @xmath169_k,[\\bm{b}_2]_k,\\ldots , [ \\bm{b}_d]_k$ ] is @xmath334 .",
    "similar analysis can be applied to determine the computational complexity of @xmath172_k,[\\bm{x}_{d-1}]_k,\\ldots , [ \\bm{x}_1]_k$ ] , i.e. , * part three * of algorithm  [ algo : distrhop ] .",
    "we arrive at @xmath281 . finally , using claim [ claim_2 ] , the time complexity of @xmath280 ( algorithm  [ algo : distrhop ] ) is @xmath335 .",
    "having developed an r - hop version which computes a `` crude '' approximation to the solution of @xmath29 , now we provide an exact r - hop solver presented in algorithm  [ alg_exactrhop ] .",
    "similar to @xmath280 , each node @xmath188 receives the @xmath137 row @xmath91 , @xmath165_k$ ] , @xmath109 , @xmath141 , and a precision parameter @xmath2 as inputs , and outputs the @xmath137 component of the @xmath2 close approximation of vector @xmath28 .",
    "[ algo : edistr ]    * initialize * : @xmath189_k = 0 $ ] , and @xmath336_k = \\text{rdistrsolve}(\\{[m_0]_{k1},\\ldots , [ m_0]_{kn}\\ } , [ b_0]_k , d , r)$ ] @xmath337_k = [ \\bm{d}_0]_{kk}[\\bm{y}_{t-1}]_k - \\sum_{j : \\bm{v}_j\\in \\mathbb{n}_{1}(\\bm{v}_k)}[\\bm{a}_{0}]_{kj}[\\bm{y}_{t-1}]_j$ ] @xmath338_k = \\text{rdistrsolve}(\\{[\\bm{m}_0]_{k1},\\ldots , [ \\bm{m}_0]_{kn}\\ } , [ \\bm{u}^{(1)}_{t}]_k , d , r)$ ] @xmath193_k = [ \\bm{y}_{t-1}]_k - [ \\bm{u}^{(2)}_{t}]_k + [ \\bm{\\chi}]_k$ ] * return * @xmath194_k = [ \\bm{y}_q]_k$ ]    * analysis of algorithm  [ alg_exactrhop ] : * the following lemma shows that @xmath339 computes the @xmath137 component of the @xmath2 close approximation to @xmath28 and provides the time complexity analysis .",
    "[ dist_exact_algorithm_guarantee_lemma_bla ] let @xmath177 be the standard splitting .",
    "further , let @xmath340 .",
    "then algorithm  [ alg_exactrhop ] requires @xmath200 iterations to return the @xmath137 component of the @xmath2 close approximation to @xmath28 .",
    "the following lemma provides the time complexity analysis of @xmath339 :    [ time_complexity_of_distresolve_bla ] let @xmath177 be the standard splitting and let @xmath341 , then @xmath339 requires @xmath342 time steps .",
    "moreover , for each node @xmath188 , @xmath339 only uses information from the r - hop neighbors .",
    "* length of the inverse chain : * again these introduced algorithms depend on the length of the inverse approximated chain , @xmath109 . the analysis in section  [ sec : length ] can be applied again to determine the @xmath205{2}}{\\sqrt[3]{2 } - 1}\\right)\\kappa\\right)\\rceil$ ] as the length of the inverse chain .      as mentioned before",
    ", the proposed solver is a distributed version of the parallel sddm solver of  @xcite .",
    "our approach is capable of acquiring @xmath2-close solutions for arbitrary @xmath187 in @xmath343 , with @xmath7 the number of nodes in graph @xmath8 , @xmath9 and @xmath10 denoting the largest and smaller weights of the edges in @xmath8 , respectively , @xmath11 representing the upper bound on the size of the r - hop neighborhood @xmath12 , and @xmath13 $ ] as the precision parameter . after developing the full communication version , we proposed a generalization to the r - hop case where communication is restricted .",
    "our method is faster than state - of - the - art methods for iteratively solving linear systems .",
    "typical linear methods , such as jacobi iteration  @xcite , are guaranteed to converge if the matrix is _ strictly _ diagonally dominant .",
    "we proposed a distributed algorithm that generalizes this setting , where it is guaranteed to converge in the sdd / sddm scenario .",
    "furthermore , the time complexity of linear techniques is @xmath344 , hence , a case of strictly diagonally dominant matrix @xmath16 can be easily constructed to lead to a complexity of @xmath345 .",
    "consequently , our approach not only generalizes the assumptions made by linear methods , but is also faster by a factor of @xmath14 .",
    "furthermore , such algorithms require average consensus to decentralize vector norm computations .",
    "contrary to these methods which lead to additional approximation errors to the real solution , our approach resolves these issues by eliminating the need for such a consensus framework .",
    "in centralized solvers , nonlinear methods ( e.g. , conjugate gradient descent  @xcite , etc . ) typically offer computational advantages over linear methods ( e.g. , jacobi iteration ) for iteratively solving linear systems .",
    "these techniques , however , can not be easily decentralized .",
    "for instance , the stopping criteria for nonlinear methods require the computation of weighted norms of residuals ( e.g. , @xmath346 with @xmath347 being the search direction at iteration @xmath156 ) .",
    "to the best of our knowledge , the distributed computation of weighted norms is difficult .",
    "namely using the approach in  @xcite , this requires the calculation of the top singular value of @xmath16 which amounts to a power iteration on @xmath348 leading to the loss of sparsity .",
    "furthermore , conjugate gradient methods require global computations of inner products .",
    "another existing method which we compare our results to is the recent work of the authors  @xcite where a local and asynchronous solution for solving systems of linear equations is considered . in their work , the authors derive a complexity bound , for one component of the solution vector , of @xmath349 , with @xmath2 being the precision parameter , @xmath109 a constant bound on the maximal degree of @xmath8 , and @xmath350 is defined as @xmath351 which can be directly mapped to @xmath352 .",
    "the relevant scenario to our work is when @xmath42 is psd and @xmath350 is symmetric . here ,",
    "the bound on the number of multiplications is given by @xmath353 , with @xmath354 being the condition number of @xmath42 .",
    "in the general case , when the degree depends on the number of nodes ( i.e. , @xmath355 ) , the minimum in the above bound will be the result of the second term ( @xmath356 ) leading to @xmath357 .",
    "consequently , in such a general setting , our approach outperforms  @xcite by a factor of @xmath358 .",
    "* special cases : * to better understand the complexity of the proposed sddm solvers , next we detail the complexity for three specific graph structures . before deriving these special cases ,",
    "however , we first note the following simple yet useful connection between weighted and unweighted laplacians of a graph @xmath8 . denoting by @xmath359 the incidence matrix of @xmath8 and @xmath22 a diagonal matrix with edge weights as diagonal elements , we can write : @xmath360 hence , we can easily establish : @xmath361    this implies that the condition number of the weighted laplacian satisfies : @xmath362 with @xmath363 and @xmath364 are the minimal and maximal edge weights of @xmath8 . using the above , we now consider four different graph topologies :      similar to the hitting time of a markov chain on a path graph which is given by @xmath365 , the time complexity of the r - hop sddm solver is given by :    given a path graph @xmath366 with @xmath7 nodes , the time complexity of the r - hop sddm solver is given by : @xmath367 for any @xmath5 and for @xmath368 .      recognizing that a grid graph @xmath369 can be represented as a product of two path graphs , @xmath370 , our solver s computational time can be summarized by :    given a grid graph , @xmath369 , the time complexity of the distributed sddm - solver",
    "can be bounded by : @xmath371 for any @xmath187 .      using the results developed in  @xcite the total time complexity of the distributed r - hop solver",
    "is bounded by :    given a scale - free network , @xmath372 , the time complexity of the r - hop sdd solver for r=1 is given by : @xmath373      for @xmath109 regular ramanujan expanders in which @xmath109 does not depend on @xmath7 , we have @xmath374 and @xmath375 .",
    "hence , the time complexity of the sdd - solver is given by constant time .",
    "the developed r - hop distributed sddm solver is a fundamental contribution with wide ranging applicability .",
    "next , we develop one such application from .",
    "we apply our solver for proposing an efficient and accurate distributed newton method for network flow optimization .",
    "namely , the distributed sddm solver is used for computing the newton direction in a distributed fashion up - to any arbitrary @xmath1 .",
    "this results in a novel distributed newton method outperforming state - of - the - art techniques in both computational complexity and accuracy .",
    "conventional methods for distributed network optimization are based on sub - gradient descent in either the primal or dual domains , see . for a large class of problems , these techniques yield iterations that can be implemented in a distributed fashion using only local information . their applicability , however , is limited by increasingly slow convergence rates .",
    "second order newton methods  @xcite are known to overcome this limitation leading to improved convergence rates .",
    "unfortunately , computing exact newton directions based only on local information is challenging . specifically , to determine the newton direction ,",
    "the inverse of the dual hessian is needed . determining this inverse",
    ", however , requires global information .",
    "consequently , authors in  @xcite proposed approximate algorithms for determining these newton iterates in a distributed fashion . accelerated dual descent ( add )  @xcite , for instance , exploits the fact that the dual hessian is the weighted laplacian of the network and performs a truncated neumann expansion of the inverse to determine a local approximate to the exact direction .",
    "add allows for a tradeoff between accurate hessian approximations and communication costs through the n - hop design , where increased n allows for more accurate inverse approximations arriving at increased cost , and lower values of n reduce accuracy but improve computational times .",
    "though successful , the effectiveness of these approaches highly depend on the accuracy of the truncated hessian inverse which is used to approximate the newton direction . as shown later , the approximated iterate can resemble high variation to the real newton direction , decreasing the applicability of these techniques .",
    "* contributions : * exploiting the sparsity pattern of the dual hessian , here we tackle the above problem and propose a newton method for network optimization that is both faster and more accurate .",
    "using the above developed solvers for sddm linear equations , we approximate the newton direction up - to any arbitrary precision @xmath376 .",
    "this leads to a distributed second - order method which performs almost identically the exact newton method .",
    "contrary to current distributed newton methods , our algorithm is the first which is capable of attaining an @xmath2-close approximation to the newton direction up to any arbitrary @xmath1 .",
    "we analyze the properties of the proposed algorithm and show that , similar to conventional newton methods , superlinear convergence within a neighborhood of the optimal value is attained .",
    "we finally demonstrate the effectiveness of the approach in a set of experiments on randomly generated and barbell networks .",
    "namely , we show that our method is capable of significantly outperforming state - of - the - art methods in both the convergence speeds and in the accuracy of approximating the newton direction .",
    "we consider a network represented by a directed graph @xmath377 with node set @xmath378 and edge set @xmath379 .",
    "the flow vector is denoted by @xmath380_{e\\in\\mathcal{e}}$ ] , with @xmath381 representing the flow on edge @xmath382 .",
    "the flow conservation conditions at nodes can be compactly represented as @xmath383 where @xmath42 is the @xmath384 node - edge incidence matrix of @xmath8 defined as      and the vector @xmath386 denotes the external source , i.e. , @xmath387 ( or @xmath388 ) indicates @xmath389 units of external flow enters ( or leaves ) node @xmath269 .",
    "a cost function @xmath390 is associated with each edge @xmath382 .",
    "namely , @xmath391 denotes the cost on edge @xmath382 as a function of the edge flow @xmath381 .",
    "we assume that the cost functions @xmath392 are strictly convex and twice differentiable .",
    "consequently , the minimum cost network optimization problem can be written as @xmath393    our goal is to investigate newton type methods for solving the problem in  [ eq : optimall ] in a distributed fashion . before diving into these details ,",
    "however , we next present basic ingredients needed for the remainder of the paper .",
    "the dual function @xmath396 is then derived as @xmath397 hence , it can be clearly seen that the evaluation of the dual function @xmath396 decomposes into e one - dimensional optimization problems .",
    "we assume that each of these optimization problems have an optimal solution , which is unique by the strict convexity of the functions @xmath392 . denoting the solutions by @xmath398 and using the first order optimality conditions",
    ", it can be seen that for each edge , e , @xmath399 is given by @xmath400^{-1}\\left(\\lambda^{(i)}-\\lambda^{(j)}\\right),\\ ] ] where @xmath401 and @xmath402 denote the source and destining nodes of edge @xmath403 , respectively ( see  @xcite for details ) .",
    "therefore , for an edge @xmath382 , the evaluation of @xmath399 can be performed based on local information about the edge s cost function and the dual variables of the incident nodes , @xmath269 and @xmath40 .",
    "the dual problem is defined as @xmath404 .",
    "since the dual function is convex , the optimization problem can be solved using gradient descent according to @xmath405 with @xmath156 being the iteration index , and @xmath406 denoting the gradient of the dual function evaluated at @xmath407 .",
    "importantly , the computation of the gradient can be performed as @xmath408 , with @xmath409 being a vector composed of @xmath410 as determined by equation  [ eq : mapback ] .",
    "further , due to the sparsity pattern of the incidence matrix @xmath42 , the @xmath266 element , @xmath411 , of the gradient @xmath412 can be computed as @xmath413    clearly , the algorithm in equation  [ eq : gd ] can be implemented in a distributed fashion , where each node , @xmath269 , maintains information about its dual , @xmath414 , and primal , @xmath410 , iterates of the outgoing edges @xmath403 .",
    "gradient components can then be evaluated as per  [ eq : gddist ] using only local information .",
    "dual variables can then be updated using  [ eq : gd ] .",
    "given the updated dual variables , the primal variables can be computed using  [ eq : mapback ] .",
    "although the distributed implementation avoids the cost and fragility of collecting all information at centralized location , practical applicability of gradient descent is hindered by slow convergence rates .",
    "this motivates the consideration of newton methods discussed next .",
    "newton s method is a descent algorithm along a scaled version of the gradient .",
    "its iterates are typically given by @xmath415 with @xmath416 being the newton direction at iteration @xmath156 , and @xmath417 denoting the step size .",
    "the newton direction satisfies @xmath418 with @xmath419 being the hessian of the dual function at the current iteration @xmath156 .                  1 .",
    "the dual hessian , @xmath427 , is a weighted laplacian of @xmath8 : @xmath428^{-1}\\bm{a}^{\\mathsf{t}}.\\ ] ] 2 .",
    "the dual hessian @xmath427 is lispshitz continuous with respect to the laplacian norm ( i.e. , @xmath429 ) where @xmath430 is the unweighted laplacian satisfying @xmath431 with @xmath42 being the incidence matrix of @xmath8 .",
    "namely , @xmath432 : @xmath433 with @xmath434 where @xmath435 and @xmath436 denote the largest and second smallest eigenvalues of the laplacian @xmath430 .",
    "as detailed in  @xcite , the exact computation of the inverse of the hessian needed for determining the newton direction can not be attained exactly in a distributed fashion .",
    "authors in  @xcite proposed approximation techniques for computing this direction . the effectiveness of these algorithms , however , highly depends on the accuracy of such an approximation . in this work , we propose a distributed approximator for the newton direction capable of acquiring @xmath2-close solutions for any arbitrary @xmath2 .",
    "our results show that this new algorithm is capable of significantly surpassing others in literature where its performance accurately traces that of the standard centralized newton approach .      using the results of the distributed r - hop solver",
    ", we propose a novel technique requiring only r - hop communication for the distributed approximation of the newton direction .",
    "given the results of lemma  [ lemma : crap ] , we can determine the approximate newton direction by solving a system of linear equations represented by an sdd matrix with @xmath440 .",
    "formally , we consider the following iteration scheme : @xmath441 with @xmath156 representing the iteration number , @xmath417 the step - size , and @xmath442 denoting the approximate newton direction .",
    "we determine @xmath442 by solving @xmath443 using algorithm  [ algo : edistr ] .",
    "it is easy to see that our approximation of the newton direction , @xmath442 , satisfies @xmath444 approximates @xmath445 according to the routine of algorithm  [ algo : edistr ] .",
    "the accuracy of this approximation is quantified in the following lemma          given such an accurate approximation , next we analyze the iteration scheme of our proposed method showing that similar to standard newton methods , we achieve superlinear convergence within a neighborhood of the optimal value .",
    "we start by analyzing the change in the laplacian norm of the gradient between two successive iterations"
  ],
  "abstract_text": [
    "<S> in this paper , we propose distributed solvers for systems of linear equations given by symmetric diagonally dominant m - matrices based on the parallel solver of spielman and peng . </S>",
    "<S> we propose two versions of the solvers , where in the first , full communication in the network is required , while in the second communication is restricted to the r - hop neighborhood between nodes for some @xmath0 . </S>",
    "<S> we rigorously analyze the convergence and convergence rates of our solvers , showing that our methods are capable of outperforming state - of - the - art techniques .    </S>",
    "<S> having developed such solvers , we then contribute by proposing an accurate distributed newton method for network flow optimization . exploiting the sparsity pattern of the dual hessian </S>",
    "<S> , we propose a newton method for network flow optimization that is both faster and more accurate than state - of - the - art techniques . </S>",
    "<S> our method utilizes the distributed sddm solvers for determining the newton direction up to any arbitrary precision @xmath1 . </S>",
    "<S> we analyze the properties of our algorithm and show superlinear convergence within a neighborhood of the optimal . finally , </S>",
    "<S> in a set of experiments conducted on randomly generated and barbell networks , we demonstrate that our approach is capable of significantly outperforming state - of - the - art techniques . </S>"
  ]
}