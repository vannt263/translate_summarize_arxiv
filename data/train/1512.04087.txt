{
  "article_text": [
    "temporal - difference ( td ) learning is a core learning technique in modern reinforcement learning @xcite .",
    "one of the main challenges in reinforcement learning is to make predictions , in an initially unknown environment , about the ( discounted ) sum of future rewards , the return , based on currently observed feature values and a certain behaviour policy . with td learning it is possible to learn good estimates of the expected return quickly by bootstrapping from other expected - return estimates .",
    "td(@xmath0 ) @xcite is a popular td algorithm that combines basic td learning with eligibility traces to further speed learning .",
    "the popularity of td(@xmath0 ) can be explained by its simple implementation , its low - computational complexity and its conceptually straightforward interpretation , given by its forward view . the forward view of td(@xmath0 ) states that the estimate at each time step is moved towards an update target known as the @xmath0-return , with @xmath0 determining the fundamental trade - off between bias and variance of the update target .",
    "this trade - off has a large influence on the speed of learning and its optimal setting varies from domain to domain .",
    "the ability to improve this trade - off by adjusting the value of @xmath0 is what underlies the performance advantage of eligibility traces .",
    "although the forward view provides a clear intuition , td(@xmath0 ) closely approximates the forward view only for appropriately small step - sizes . until recently , this",
    "was considered an unfortunate , but unavoidable part of the theory behind td(@xmath0 ) .",
    "this changed with the introduction of true online td(@xmath0 ) @xcite , which computes exactly the same weight vectors as the forward view at any step - size .",
    "this gives true online td(@xmath0 ) full control over the bias - variance trade - off .",
    "in particular , true online td(1 ) can achieve fully unbiased updates .",
    "moreover , true online td(@xmath0 ) only requires small modifications to the td(@xmath0 ) update equations , and the extra computational cost is negligible in most cases .",
    "we hypothesize that true online td(@xmath0 ) , and its control version true online sarsa(@xmath0 ) , not only have better theoretical properties than their regular counterparts , but also dominate them empirically .",
    "we test this hypothesis by performing an extensive empirical comparison between true online td(@xmath0 ) , regular td(@xmath0 ) ( which is based on accumulating traces ) , and the common variation based on replacing traces .",
    "in addition , we perform comparisons between true online sarsa(@xmath0 ) and sarsa(@xmath0 ) ( with accumulating and replacing traces ) .",
    "the domains we use include random markov reward processes , a real - world myoelectric prosthetic arm , and a domain from the arcade learning environment @xcite .",
    "the representations we consider range from tabular values to linear function approximation with binary and non - binary features .    besides the empirical study",
    ", we provide an in - depth discussion on the theory behind true online td(@xmath0 ) .",
    "this theory is based on a new online forward view .",
    "the traditional forward view , based on the @xmath0-return , is inherently an offline forward view meaning that updates only occur at the end of an episode , because the @xmath0-return requires data up to the end of an episode .",
    "we extend this forward view to the online case , where updates occur at every time step , by using a bounded version of the @xmath0-return that grows over time .",
    "whereas td(@xmath0 ) approximates the traditional forward view only at the end of an episode , we show that td(@xmath0 ) approximates this new online forward view at all time steps .",
    "true online td(@xmath0 ) is equivalent to this new online forward view at all time steps .",
    "we prove this by deriving the true online td(@xmath0 ) update equations directly from the online forward view update equations .",
    "this derivation forms a blueprint for the derivation of other true online methods . by making variations to the online forward view and following the same derivation as for true online td(@xmath0 ) ,",
    "we derive several other true online methods .",
    "this article is organized as follows .",
    "we start by presenting the required background in section 2 .",
    "then , we present the new online forward view in section 3 , followed by the presentation of true online td(@xmath0 ) in section 4 .",
    "section 5 presents the empirical study .",
    "furthermore , in section 6 , we present several other true online methods . in section 7 , we discuss in detail related papers . finally , section 8 concludes .",
    "here , we present the main learning framework . as a convention , we indicate scalar - valued random variables by capital letters ( e.g. , @xmath1 , @xmath2 ) , vectors by bold lowercase letters ( e.g. , @xmath3 , @xmath4 ) , functions by non - bold lowercase letters ( e.g. , @xmath5 ) , and sets by calligraphic font ( e.g. , @xmath6 , @xmath7 ) .. ]      reinforcement learning ( rl ) problems are often formalized as _ markov decision processes _ ( mdps ) , which can be described as 5-tuples of the form @xmath8 , where @xmath6 indicates the set of all states ; @xmath7 indicates the set of all actions ; @xmath9 indicates the probability of a transition to state @xmath10 , when action @xmath11 is taken in state @xmath12 ; @xmath13 indicates the expected reward for a transition from state @xmath14 to state @xmath15 under action @xmath16 ; the discount factor @xmath17 specifies how future rewards are weighted with respect to the immediate reward .",
    "actions are taken at discrete time steps @xmath18 according to a _ policy _",
    "@xmath19 $ ] , which defines for each action the selection probability conditioned on the state .",
    "the _ return _ at time @xmath20 is defined as the discounted sum of rewards , observed after @xmath20 : @xmath21 where @xmath22 is the reward received after taking action @xmath23 in state @xmath1 .",
    "some mdps contain special states called _",
    "terminal states_. after reaching a terminal state , no further reward is obtained and no further state transitions occur .",
    "hence , a terminal state can be interpreted as a state where each action returns to itself with a reward of 0 . an interaction sequence from the initial state to a terminal state is called an _ episode_.    each policy @xmath24 has a corresponding state - value function @xmath25 , which maps any state @xmath12 to the expected value of the return from that state , when following policy @xmath24 : @xmath26 in addition , the action - value function @xmath27 gives the expected return for policy @xmath24 , given that action @xmath11 is taken in state @xmath12 : @xmath28 because no further rewards can be obtained from a terminal state , the state - value and action - values for a terminal state are always 0 .",
    "there are two tasks that are typically associated with an mdp .",
    "first , there is the task of determining ( an estimate of ) the value function @xmath29 for some given policy @xmath24 .",
    "the second , more challenging task is that of determining ( an estimate of ) the optimal policy @xmath30 , which is defined as the policy whose corresponding value function has the highest value in each state : @xmath31 in rl , these two tasks are considered under the condition that the reward function @xmath32 and the transition - probability function @xmath33 are unknown",
    ". hence , the tasks have to be solved using samples obtained from interacting with the environment .",
    "let s consider the task of learning an estimate @xmath34 of the value function @xmath25 from samples , where @xmath29 is being estimated using linear function approximation .",
    "that is , @xmath34 is the inner product between a feature vector @xmath35 of @xmath14 , and a weight vector @xmath36 : @xmath37 if @xmath14 is a terminal state , then by definition @xmath38 , and hence @xmath39 .",
    "we can formulate the problem of estimating @xmath29 as an error - minimization problem , where the error is a weighted average of the squared difference between the value of a state and its estimate : @xmath40 with @xmath41 the stationary distribution induced by @xmath24 .",
    "the above error function can be minimized by using stochastic gradient descent while sampling from the stationary distribution , resulting in the following update rule : @xmath42 using @xmath43 as a shorthand for @xmath44 .",
    "the parameter @xmath45 is called the _ step - size_. using the chain rule , we can rewrite this update as : @xmath46 because @xmath29 is in general unknown , an estimate @xmath47 of @xmath48 is used , which we call the _ update target _ , resulting in the following general update rule : @xmath49    there are many different update targets possible .",
    "for an unbiased estimator the full return can be used , that is , @xmath50 . however , the full return has the disadvantage that its variance is typically very high .",
    "hence , learning with the full return can be slow .",
    "temporal - difference ( td ) learning addresses this issue by using update targets based on other value estimates . while the update target is no longer unbiased in this case , the variance is typically much smaller , and learning much faster .",
    "td learning uses the bellman equations as its mathematical foundation for constructing update targets .",
    "these equations relate the value of a state to the values of its successor states : @xmath51 writing this equation in terms of an expectation yields : @xmath52 sampling from this expectation , while using linear function approximation to approximate @xmath29 , results in the update target : @xmath53 this update target is called a one - step update target , because it is based on information from only one time step ahead . applying the bellman equation multiple times results in update targets based on information further ahead .",
    "such update targets are called multi - step update targets .",
    "the td(@xmath0 ) algorithm implements the following update equations : @xmath54 for @xmath55 , and with @xmath56 . the scalar @xmath57 is called the _ td error _ , and the vector @xmath58 is called the _ eligibility - trace _ vector . the update of @xmath59 shown above is referred to as the _ accumulating - trace _ update . as a shorthand",
    ", we will refer to this version of td(@xmath0 ) as ` accumulate td(@xmath0 ) ' , to distinguish it from a slightly different version that is discussed below . while these updates appear to deviate from the general , gradient - descent - based update rule given in ( [ eq : linear update ] ) , there is a close connection to this update rule .",
    "this connection is formalized through the forward view of td(@xmath0 ) , which we discuss in detail in the next section .",
    "[ al : td(lambda ) ] shows the pseudocode for accumulate td(@xmath0 ) .",
    "@xmath60 loop ( over episodes ) : obtain initial @xmath4 @xmath61 while terminal state has not been reached , do : obtain next feature vector @xmath62 and reward @xmath63 @xmath64 @xmath65 @xmath66 @xmath67    accumulate td(@xmath0 ) can be very sensitive with respect to the @xmath45 and @xmath0 parameters .",
    "especially , a large value of @xmath0 combined with a large value of @xmath45 can easily cause divergence , even on simple tasks with bounded rewards . for this reason , a variant of td(@xmath0 )",
    "is sometimes used that is more robust with respect to these parameters .",
    "this variant , which assumes binary features , uses a different trace - update equation : @xmath68= \\begin{cases } \\gamma\\lambda   { { \\boldsymbol}e}_{t-1}[i]\\ , ,   & \\mbox{if } { { \\boldsymbol \\phi}}_{t}[i ] = 0;\\\\ 1\\ , , & \\mbox{if }   { { \\boldsymbol \\phi}}_{t}[i]= 1\\ , , \\end{cases }   \\qquad\\mbox { for all features } i\\thinspace.\\ ] ] where @xmath69 $ ] indicates the @xmath70-th component of vector @xmath71 .",
    "this update is referred to as the _ replacing - trace _ update . as a shorthand",
    ", we will refer to the version of td(@xmath0 ) using the replacing - trace update as ` replace td(@xmath0 ) ' .",
    "the traditional forward view relates the td(@xmath0 ) update equations to the general update rule shown in equation ( [ eq : linear update ] ) .",
    "specifically , for small step - sizes the weight vector at the end of an episode computed by accumulate td(@xmath0 ) is approximately the same as the weight vector resulting from a sequence of equation ( [ eq : linear update ] ) updates ( one for each visited state ) using a particular multi - step update target , called the _",
    "@xmath0-return _ @xcite .",
    "the @xmath0-return for state @xmath1 is defined as : @xmath72 where @xmath73 is the time step the terminal state is reached , and @xmath74 is the @xmath75-step return , defined as : @xmath76    we call a method that updates the value of each visited state at the end of the episode an _ offline _ method ; we call a method that updates the value of each visited state immediately after the visit ( i.e. , at the time step after the visit ) an _ online _ method .",
    "td(@xmath0 ) is an online method .",
    "the update sequence of the traditional forward view , however , corresponds with an offline method , because the @xmath0-return requires data up to the end of an episode .",
    "this leaves open the question of how to interpret the weights of td(@xmath0 ) _ during _ an episode . in this section ,",
    "we provide an answer to this long - standing open question .",
    "we introduce a bounded version of the @xmath0-return that only uses information up to a certain horizon and we use this to construct an online forward view .",
    "this online forward view approximates the weight vectors of accumulate td(@xmath0 ) at _ all _ time steps , instead of only at the end of an episode .",
    "the concept of an online forward view contains a paradox . on the one hand ,",
    "multi - step update targets require data from time steps far beyond the time a state is visited ; on the other hand , the online aspect requires that the value of a visited state is updated immediately .",
    "the solution to this paradox is to assign a sequence of update targets to each visited state .",
    "the first update target in this sequence contains data from only the next time step , the second contains data from the next two time steps , the third from the next three time steps , and so on .",
    "now , given an initial weight vector and a sequence of visited states , a new weight vector can be constructed by updating each visited state with an update target that contains data up to the current time step .",
    "below , we formalize this idea .",
    "we define the _ interim @xmath0-return _ for state @xmath77 with horizon @xmath78 as follows : @xmath79 note that this update target does not use data beyond the horizon @xmath80 .",
    "@xmath81 implicitly defines a sequence of update targets for @xmath77 : @xmath82 . as time increases ,",
    "update targets based on data further away become available for state @xmath77 . at a particular time",
    "step @xmath20 , a new weight vector is computed by performing an equation ( [ eq : linear update ] ) update for each visited state using the interim @xmath0-return with horizon @xmath20 , starting from the initial weight vector @xmath83 .",
    "hence , at time step @xmath20 , a sequence of @xmath20 updates occurs . to describe this sequence mathematically",
    ", we use weight vectors with two indices : @xmath84 .",
    "the superscript indicates the time step at which the updates are performed ( this value corresponds with the horizon of the interim @xmath0-returns that are used in the updates ) .",
    "the subscript is the iteration index of the sequence ( it corresponds with the number of updates that have been performed at a particular time step ) . as an example , the update sequences for the first three time steps are : @xmath85 with @xmath86 for all @xmath20 . more generally , the update sequence at time step @xmath20 is : @xmath87 we define @xmath88 ( without superscript ) as the final weight vector of the update sequence at time @xmath20 , that is , @xmath89 .",
    "we call the algorithm implementing equation ( [ eq : real - time update ] ) the _ online @xmath0-return algorithm_. by contrast , we call the algorithm that implements the traditional forward view the _ offline @xmath0-return algorithm_.    the update sequence performed by the online @xmath0-return algorithm at time step t ( the time step that a terminal state is reached ) is very similar to the update sequence performed by the offline @xmath0-return algorithm . in particular , note that @xmath90 and @xmath91 are the same , under the assumption that the weights used for the value estimates are the same .",
    "because these weights are in practise not exactly the same , there will typically be a small difference .",
    "there is never a difference because there is no bootstrapping . ]",
    "figure [ fig : offline vs online ] illustrates the difference between the online and offline @xmath0-return algorithm , as well as accumulate td(@xmath0 ) , by showing the rms error on a random walk task .",
    "the task consists of 10 states laid out in a row plus a terminal state on the left .",
    "each state transitions with 70% probability to its left neighbour and with 30% probability to its right neighbour ( or to itself in case of the right - most state ) .",
    "all rewards are 1 and @xmath92 .",
    "furthermore , @xmath93 and @xmath94 .",
    "the right - most state is the initial state .",
    "whereas the offline @xmath0-return algorithm only makes updates at the end of an episode , the online @xmath0-return algorithm , as well as accumulate td@xmath95 ) , make updates at every time step .     and",
    "the error shown is the rms error over all states , normalized by the initial rms error.,height=188 ]    the comparison on the random walk task shows that accumulate td(@xmath0 ) behaves similar to the online @xmath0-return algorithm . in fact , the smaller the step - size , the smaller the difference between accumulate td(@xmath0 ) and the online @xmath0-return algorithm .",
    "this is formalized by theorem 1 .",
    "the proof of the theorem can be found in appendix [ sec : proof ] .",
    "the theorem uses the term @xmath96 , which is defined as : @xmath97 with @xmath98 the interim @xmath0-return for state @xmath99 with horizon @xmath20 that uses @xmath100 for all value evaluations . note that @xmath96 is independent of the step - size .",
    "let @xmath100 be the initial weight vector , @xmath101 be the weight vector at time @xmath20 computed by accumulate td(@xmath0 ) , and @xmath102 be the weight vector at time @xmath20 computed by the online @xmath0-return algorithm .",
    "furthermore , assume that @xmath103 .",
    "then , for all time steps @xmath20 : @xmath104 [ theorem : main ]    theorem [ theorem : main ] generalizes the traditional result to arbitrary time steps .",
    "the traditional result states that the difference between the weight vector at the end of an episode computed by the offline @xmath0-return algorithm and the weight vector at the end of an episode computed by accumulate td(@xmath0 ) goes to 0 , if the step - size goes to 0 @xcite .      while accumulate td(@xmath0 ) behaves like the online @xmath0-return algorithm for small step - sizes , small step - sizes often result in slow learning . hence , higher step - sizes are desirable .",
    "for higher step - sizes , however , the behaviour of accumulate td(@xmath0 ) can be very different from that of the online @xmath0-return algorithm . and",
    "as we show in the empirical section of this article ( section [ sec : empirical study ] ) , when there is a difference , it is almost exclusively in favour of the online @xmath0-return algorithm . in this section ,",
    "we analyze why the online @xmath0-return algorithm can outperform accumulate td(@xmath0 ) , using the one - state example shown in the left of figure [ fig : one - state example ] .",
    ".,title=\"fig:\",width=151 ] .,title=\"fig:\",width=226 ]    the right of figure [ fig : one - state example ] shows the rms error over the first 10 episodes of the one - state example for different step - sizes and @xmath105 . while for small step - sizes accumulate td(@xmath0 ) behaves indeed like the online @xmath0-return algorithm  as predicted by theorem [ theorem : main] , for larger step - sizes the difference becomes huge . to understand the reason for this",
    ", we derive an analytical expression for the value at the end of an episode .",
    "first , we consider accumulate td(@xmath0 ) . because there is only one state involved",
    ", we indicate the value of this state simply by v. the update at the end of an episode is @xmath106 . in our example",
    ", @xmath107 for all time steps @xmath20 , except for @xmath108 , where @xmath109 . because @xmath57 is 0 for all time steps except the last , @xmath110 .",
    "furthermore , @xmath111 for all time steps @xmath20 , resulting in @xmath112 . substituting all this in the expression for @xmath113 yields : @xmath114 so for accumulate td(@xmath0 )",
    ", the total value difference is simply a summation of the value difference corresponding to a single update .",
    "now , consider the online @xmath0-return algorithm .",
    "the value at the end of an episode , @xmath113 , is equal to @xmath115 , resulting from the update sequence : @xmath116 by incremental substitution , we can directly express @xmath113 in terms of the initial value , @xmath117 , and the update targets : @xmath118 because @xmath119 for all @xmath120 in our example , the weights of all update targets can be added together and the expression can be rewritten as a single pseudo - update , yielding : @xmath121    the term @xmath122 in ( [ eq : vt on ] ) acts like a pseudo step - size . for larger @xmath45 or @xmath73 this pseudo step - size increases in value , but as long as @xmath123 the value will never exceed 1 . by contrast , for accumulate td(@xmath0 ) the pseudo step - size is @xmath124 , which can grow much larger than 1 even for @xmath125 , causing divergence of values .",
    "this is the reason that accumulate td(@xmath0 ) can be very sensitive to the step - size and it explains why the optimal step - size for accumulate td(@xmath0 ) is much smaller than the optimal step - size for the online @xmath0-return algorithm in figure [ fig : one - state example ] ( @xmath126 versus @xmath127 , respectively ) .",
    "moreover , because the variance on the pseudo step - size is higher for accumulate td(@xmath0 ) the performance at the optimal step - size for accumulate td(@xmath0 ) is worse than the performance at the optimal step - size for the online @xmath0-return algorithm .",
    "the sensitivity of accumulate td(@xmath0 ) to divergence , demonstrated in the previous subsection , has been known for long .",
    "in fact , replace td(@xmath0 ) was designed to deal with this .",
    "but while replace td(@xmath0 ) is much more robust with respect to divergence , it also has its limitations .",
    "one obvious limitation is that it only applies to binary features , so it is not generally applicable .",
    "but even in domains where replace td(@xmath0 ) can be applied , it can perform poorly .",
    "the reason is that replacing previous trace values , rather than adding to it , reduces the multi - step characteristics of td(@xmath0 ) .    to illustrate this ,",
    "consider the two - state example shown in the left of figure [ fig : two - state example ] .",
    "it is easy to see that the value of the left - most state is 2 and of the other state is 0 .",
    "the state representation consists of only a single , binary feature that is 1 in both states and 0 in the terminal state . because there is only a single feature , the state values can not be represented exactly .",
    "the weight that minimizes the mean squared error assigns a value of 1 to both states , resulting in an rms error of 1 . now consider the graph shown in the right of figure [ fig : two - state example ] , which shows the asymptotic rms error for different values of @xmath0 .",
    "the error for accumulate td(@xmath0 ) converges to the least mean squares ( lms ) error for @xmath93 , as predicted by the theory @xcite .",
    "the online @xmath0-return algorithm has the same convergence behaviour ( due to theorem 1 ) .",
    "by contrast , replace td(@xmath0 ) converges to the same value as td(0 ) for any value of @xmath0 .",
    "the reason for this behaviour is that because the single feature is active at all time steps , the multi - step behaviour of td(@xmath0 ) is fully removed , no matter the value of @xmath0 .",
    "hence , replace td(@xmath0 ) behaves exactly the same as td(0 ) for any value of @xmath0 at all time steps . as a result",
    ", it also behaves like td(0 ) asymptotically .",
    "the two - state example very clearly demonstrates that there is a price payed by replace td(@xmath0 ) to achieve robustness with respect to divergence : a reduction in multi - step behaviour .",
    "by contrast , the online @xmath0-return algorithm , which is also robust to divergence , does not have this disadvantage .",
    "of course , the two - state example , as well as the one - state example from the previous section , are extreme examples , merely meant to illustrate what can go wrong .",
    "but in practise , a domain will often have some characteristics of the one - state example and some of the two - state example , which negatively impacts the performance of both accumulate and replace td(@xmath0 ) .",
    "( at @xmath128 ) .",
    "we consider values to be converged if the error changed less than 1% over the last 100 time steps .",
    ", title=\"fig:\",width=264 ]   ( at @xmath128 ) .",
    "we consider values to be converged if the error changed less than 1% over the last 100 time steps .",
    ", title=\"fig:\",width=226 ]",
    "the online @xmath0-return algorithm is impractical on many domains : the memory it uses , as well as the computation required per time step increases linearly with time .",
    "fortunately , it is possible to rewrite the update equations of the online @xmath0-return algorithm to a different set of update equations that can be implemented with a computational complexity that is independent of time .",
    "in fact , this alternative set of update equations differs from the update equations of accumulate td(@xmath0 ) only by two extra terms , each of which can be computed efficiently .",
    "the algorithm implementing these equations is called true online td(@xmath0 ) and is discussed below .      for the online @xmath0-return algorithm , at each time step a sequence of updates is performed .",
    "the length of this sequence , and hence the computation per time step , increases over time .",
    "however , it is possible to compute the weight vector resulting from the sequence at time step @xmath129 directly from the weight vector resulting from the sequence at time step @xmath20 .",
    "this results in the following update equations ( see appendix [ sec : derivation ] for the derivation ) : @xmath130 for @xmath55 , and with @xmath56 . compared to accumulate td(@xmath0 ) ,",
    "both the trace update and the weight update have an additional term .",
    "we call a trace updated in this way a _ dutch trace _ ; we call the term @xmath131 the _ td - error time - step correction _ , or simply the @xmath132-correction . algorithm",
    "[ al : true td(lambda ) ] shows pseudocode that implements these equations .. for reasons explained in that section this requires a modified trace update .",
    "that pseudocode is the same as the pseudocode from @xcite . ]",
    "@xmath60 loop ( over episodes ) : obtain initial @xmath4 @xmath133 while terminal state has not been reached , do : obtain next feature vector @xmath62 and reward @xmath63 @xmath134 @xmath135 @xmath136 @xmath137 @xmath138 @xmath139 @xmath67    in terms of computation time , true online td(@xmath0 ) has a ( slightly ) higher cost due to the two extra terms that have to be accounted for .",
    "while the computation - time complexity of true online td(@xmath0 ) is the same as that of accumulate / replace td(@xmath0)@xmath140 per time step with @xmath75 being the number of features , the actual computation time can be close to twice as much in some cases .",
    "in other cases ( for example if sparse feature vectors are used ) , the computation time of true online td(@xmath0 ) is only a fraction more than that of accumulate / replace td(@xmath0 ) . in terms of memory ,",
    "true online td(@xmath0 ) has the same cost as accumulate / replace td(@xmath0 ) .      in section [ sec : online forward view ] , a number of examples were shown where the online @xmath0-return algorithm outperforms accumulate / replace td(@xmath0 ) .",
    "because true online td(@xmath0 ) is simply an efficient implementation of the online @xmath0-return algorithm , true online td(@xmath0 ) will outperform accumulate / replace td(@xmath0 ) on these examples as well .",
    "but not in all cases will there be a performance difference .",
    "for example , it follows from theorem [ theorem : main ] that when appropriately small step - sizes are used , the difference between the online @xmath0-return algorithm / true online td(@xmath0 ) and accumulate td(@xmath0 ) is negligible . in this section , we identify two other factors that affect whether or not there will be a performance difference . while the focus of this section is on performance _ difference _ rather than performance _ advantage _ , our experiments will show that true online td(@xmath0 ) performs always at least as well as accumulate td(@xmath0 ) and replace td(@xmath0 ) .",
    "in other words , our experiments suggest that whenever there is a performance difference , it is in favour of true online td(@xmath0 ) .",
    "the first factor is the @xmath0 parameter and follows straightforwardly from the true online td(@xmath0 ) update equations .    for @xmath141 ,",
    "accumulate td(@xmath0 ) , replace td(@xmath0 ) and the online @xmath0-return algorithm / true online td(@xmath0 ) behave the same .    for @xmath141 ,",
    "the accumulating - trace update , the replacing - trace update and the dutch - trace update all reduce to @xmath142 .",
    "in addition , because @xmath142 , the @xmath132-correction of true online td(@xmath0 ) is 0 .    because the behaviour of td(@xmath0 ) for small @xmath0 is close to the behaviour of td(0 )",
    ", it follows that significant performance differences will only be observed when @xmath0 is large .",
    "the second factor is related to how often a feature has a non - zero value .",
    "we start again with a proposition that highlights a condition under which the different td(@xmath0 ) versions behave the same .",
    "the proposition makes use of an accumulating trace at time step @xmath143 , @xmath144 , whose non - recursive form is : @xmath145 furthermore , the proposition uses @xmath69 $ ] to denote the @xmath70-th element of vector @xmath71 .",
    "if for all features @xmath70 and at all time steps @xmath20 @xmath146 \\cdot { { \\boldsymbol \\phi}}_t[i ]   = 0\\ , , \\label{prop : eq}\\ ] ] then accumulate td(@xmath0 ) , replace td(@xmath0 ) and the online @xmath0-return algorithm / true online td(@xmath0 ) behave the same ( for any @xmath0 ) .",
    "[ prop : e ]    condition ( [ prop : eq ] ) implies that if @xmath147 \\neq 0 $ ] , then @xmath148   = 0 $ ] . from this",
    "it follows that for binary features the accumulating - trace update can be written as a replacing - trace update at every time step : @xmath149 & : = & \\gamma\\lambda { { \\boldsymbol}e}_{t-1}^{acc}[i ] +   { { \\boldsymbol \\phi}}_t[i ] \\,,\\\\ & = &   \\begin{cases } \\gamma\\lambda   { { \\boldsymbol}e}^{acc}_{t-1}[i]\\ , ,   & \\mbox{if } { { \\boldsymbol \\phi}}_{t}[i ] = 0;\\\\ 1\\ , , & \\mbox{if }   { { \\boldsymbol \\phi}}_{t}[i]= 1\\,.\\end{cases}\\end{aligned}\\ ] ] hence , accumulate td(@xmath0 ) and replace td(@xmath0 ) perform exactly the same updates .",
    "furthermore , condition ( [ prop : eq ] ) implies that @xmath150 .",
    "hence , the accumulating - trace update can also be written as a dutch trace update at every time step : @xmath151 in addition , note that the @xmath132-correction is proportional to @xmath152 , which can be written as @xmath153 .",
    "the value @xmath153 is proportional to @xmath154 for accumulate td(@xmath0 ) . because @xmath150 , accumulate td(@xmath0 ) can add a @xmath132-correction at every time step without any consequence",
    "this shows that accumulate td(@xmath0 ) makes the same updates as true online td(@xmath0 ) .",
    "an example of a domain where the condition of proposition [ prop : e ] holds is a domain with tabular features ( each state is represented with a unique standard - basis vector ) , where a state is never revisited within the same episode .",
    "the condition of proposition [ prop : e ] holds approximately when the value @xmath155 \\cdot { { \\boldsymbol \\phi}}_t[i ] \\right|$ ] is close to 0 for all features at all time steps . in this case , the different td(@xmath0 ) versions will perform very similarly .",
    "it follows from equation ( [ eq : non - recursive ] ) that this is the case when there is a long time delay between the time steps that a feature has a non - zero value . specifically , if there is always at least @xmath75 time steps between two subsequent times that a feature @xmath70 has a non - zero value with @xmath156 being very small , then @xmath155 \\cdot { { \\boldsymbol \\phi}}_t[i ] \\right|$ ] will always be close to 0 .",
    "therefore , in order to see a large performance difference , the same features should have a non - zero value often and within a small time frame ( relative to @xmath157 ) . summarizing the analysis so far : in order to see a performance difference @xmath45 and @xmath0 should be sufficiently large , and the same features should have a non - zero value often and within a small time frame . based on this summary",
    ", we can address a related question : on what type of domains will there be a performance difference between true online td(@xmath0 ) with optimized parameters and accumulate / replace td(@xmath0 ) with optimized parameters .",
    "the first two conditions suggest that the domain should result in a relatively large optimal @xmath45 and optimal @xmath0 .",
    "this is typically the case for domains with a relatively low variance on the return .",
    "the last condition can be satisfied in multiple ways .",
    "it is for example satisfied by domains that have non - sparse feature vectors ( that is , domains for which at any particular time step most features have a non - zero value ) .",
    "td(@xmath0 ) and true online td(@xmath0 ) are policy evaluation methods .",
    "however , they can be turned into control methods in a straightforward way . from a learning perspective",
    ", the main difference is that the prediction of the expected return should be conditioned on the state and action , rather than only on the state .",
    "this means that an estimate of the action - value function @xmath27 is being learned , rather than of the state - value function @xmath25 .",
    "another difference is that instead of having a fixed policy that generates the behaviour , the policy depends on the action - value estimates . because these estimates typically improve over time , so does the policy .",
    "the ( on - policy ) control counterpart of td(@xmath0 ) is the popular sarsa(@xmath0 ) algorithm .",
    "the control counterpart of true online td(@xmath0 ) is ` true online sarsa(@xmath0 ) ' .",
    "[ al : true online sarsa(lambda ) ] shows pseudocode for true online sarsa(@xmath0 ) .",
    "@xmath60 loop ( over episodes ) : obtain initial state @xmath158 select action @xmath159 based on state @xmath158 ( for example @xmath160-greedy ) @xmath161 features corresponding to @xmath162 @xmath163 while terminal state has not been reached , do : take action @xmath159 , observe next state @xmath164 and reward @xmath63 select action @xmath165 based on state @xmath164 @xmath166 features corresponding to @xmath167 ( if @xmath164 is terminal state , @xmath168 ) @xmath169 @xmath170 @xmath171 @xmath172 @xmath173 @xmath174 @xmath175    to ensure accurate estimates for all state - action values are obtained , typically some exploration strategy has to be used .",
    "a simple , but often sufficient strategy is to use an @xmath160-greedy behaviour policy .",
    "that is , given current state @xmath1 , with probability @xmath160 a random action is selected , and with probability @xmath176 the greedy action is selected : @xmath177 with @xmath178 an action - feature vector , and @xmath179 a ( linear ) estimate of @xmath180 at time step @xmath20 .",
    "a common way to derive an action - feature vector @xmath178 from a state - feature vector @xmath181 involves an action - feature vector of size @xmath182 , where @xmath75 is the number of state features and @xmath183 is the number of actions .",
    "each action corresponds with a block of @xmath75 features in this action - feature vector .",
    "the features in @xmath178 that correspond to action @xmath16 take on the values of the state features ; the features corresponding to other actions have a value of 0 .",
    "this section contains our main empirical study , comparing td(@xmath0 ) , as well as sarsa(@xmath0 ) , with their true online counterparts . for each method and each domain ,",
    "a scan over the step - size @xmath45 and the trace - decay parameter @xmath0 is performed such that the optimal performance can be compared . in section [ sec : discussion ] , we discuss the results .      for our first series of experiments we used",
    "randomly constructed markov reward processes ( mrps ) .",
    "an mrp can be interpreted as an mdp with only a single action per state .",
    "consequently , there is only one policy possible .",
    "we represent a random mrp as a 3-tuple @xmath184 , consisting of @xmath120 , the number of states ; @xmath185 , the branching factor ( that is , the number of next states with a non - zero transition probability ) ; and @xmath186 , the standard deviation of the reward .",
    "an mrp is constructed as follows .",
    "the @xmath185 potential next states for a particular state are drawn from the total set of states at random , and without replacement .",
    "the transition probabilities to those states are randomized as well ( by partitioning the unit interval at @xmath187 random cut points ) .",
    "the expected value of the reward for a transition is drawn from a normal distribution with zero mean and unit variance .",
    "the actual reward is drawn from a normal distribution with a mean equal to this expected reward and standard deviation @xmath186 .",
    "there are no terminal states .",
    "we compared the performance of td(@xmath0 ) on three different mrps : one with a small number of states , @xmath188 , one with a larger number of states , @xmath189 , and one with a larger number of states but a low branching factor and no stochasticity for the reward , @xmath190 .",
    "the discount factor @xmath17 is @xmath191 for all three mrps .",
    "each mrp is evaluated using three different representations .",
    "the first representation consists of _ tabular _ features , that is , each state is represented with a unique standard - basis vector of @xmath120 dimensions .",
    "the second representation is based on _ binary _ features .",
    "this binary representation is constructed by first assigning indices , from 1 to @xmath120 , to all states .",
    "then , the binary encoding of the state index is used as a feature vector to represent that state .",
    "the length of a feature vector is determined by the total number of states : for @xmath192 , the length is 4 ; for @xmath193 , the length is 7 . as an example , for @xmath192 the binary feature vectors of states 1 , 2 and 3 are @xmath194,@xmath195 and @xmath196 , respectively .",
    "finally , the third representation uses non - binary features . for this representation each state is mapped to a 5-dimensional feature vector , with the value of each feature drawn from a normal distribution with zero mean and unit variance .",
    "after all the feature values for a state are drawn , they are normalized such that the feature vector has unit length .",
    "once generated , the feature vectors are kept fixed for each state .",
    "note that replace td(@xmath0 ) can not be used with this representation , because replacing traces are only defined for binary features ( tabular features are a special case of this ) .    in each experiment",
    ", we performed a scan over @xmath45 and @xmath0 .",
    "specifically , between 0 and 0.1 , @xmath45 is varied according to @xmath197 with @xmath70 varying from -3 to -1 with steps of 0.2 , and from 0.1 to 2.0 ( linearly ) with steps of 0.1 .",
    "in addition , @xmath0 is varied from 0 to 0.9 with steps of 0.1 and from 0.9 to 1.0 with steps of 0.01 .",
    "the initial weight vector is the zero vector in all domains . as performance metric",
    "we used the mean - squared error ( mse ) with respect to the lms solution during early learning ( for k = 10 , we averaged over the first 100 time steps ; for k = 100 , we averaged over the first 1000 time steps ) .",
    "we normalized this error by dividing it by the mse under the initial weight estimate .    , and three different representations .",
    "the error shown is at optimal @xmath45 value . ]",
    "figure [ fig : mdp_all ] shows the results for different @xmath0 at the best value of @xmath45 . in appendix [ sec : detailed mrps ] , the results for all @xmath45 values are shown .",
    "the optimal performance of true online td(@xmath0 ) is at least as good as the optimal performance of accumulate td(@xmath0 ) and replace td(@xmath0 ) , on all domains and for all representations .",
    "a more in - depth discussion of these results is provided in section [ sec : discussion ] .      in this experiment",
    ", we compared the performance of true online td(@xmath0 ) and td(@xmath0 ) on a real - world data - set consisting of sensorimotor signals measured during the human control of an electromechanical robot arm .",
    "the source of the data is a series of manipulation tasks performed by a participant with an amputation , as presented by @xcite . in this study ,",
    "an amputee participant used signals recorded from the muscles of their residual limb to control a robot arm with multiple degrees - of - freedom ( figure [ fig : myocontrol ] ) .",
    "interactions of this kind are known as _ myoelectric control _",
    "( see , for example , * ? ? ?",
    "* ) .        for consistency and comparison of results",
    ", we used the same source data and prediction learning architecture as published in @xcite . in total , two signals are predicted : grip force and motor angle signals from the robot s hand .",
    "specifically , the target for the prediction is a discounted sum of each signal over time , similar to return predictions ( see general value functions and nexting ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) . where possible",
    ", we used the same implementation and code base as @xcite .",
    "data for this experiment consisted of 58,000 time steps of recorded sensorimotor information , sampled at 40 hz ( i.e. , approximately 25 minutes of experimental data ) .",
    "the state space consisted of a tile - coded representation of the robot gripper s position , velocity , recorded gripping force , and two muscle contraction signals from the human user .",
    "a standard implementation of tile - coding was used , with ten bins per signal , eight overlapping tilings , and a single active bias unit .",
    "this results in a state space with 800,001 features , 9 of which were active at any given time .",
    "hashing was used to reduce this space down to a vector of 200,000 features that are then presented to the learning system .",
    "all signals were normalized between 0 and 1 before being provided to the function approximation routine .",
    "the discount factor for predictions of both force and angle was @xmath198 , as in the results presented by @xcite .",
    "parameter sweeps over @xmath0 and @xmath45 are conducted for all three methods .",
    "the performance metric is the mean absolute return error over all 58,000 time steps of learning , normalized by dividing by the error for @xmath141 .",
    "figure [ fig : myoelectricresults ] shows the performance for the angle as well as the force predictions at the best @xmath45 value for different values of @xmath0 . in appendix",
    "[ sec : detailed myo ] , the results for all @xmath45 values are shown .",
    "the relative performance of replace td(@xmath0 ) and accumulate td(@xmath0 ) depends on the predictive question being asked . for predicting the robot s grip force signal  a signal with small magnitude and rapid changes  replace td(@xmath0 ) is better than accumulate td(@xmath0 ) at all @xmath0 values larger than 0 .",
    "however , for predicting the robot s hand actuator position , a smoothly changing signal that varies between a range of @xmath199300500 , accumulate td(@xmath0 ) dominates replace td(@xmath0 ) . on both prediction tasks , true online td(@xmath0 ) dominates accumulate td(@xmath0 ) and replace td(@xmath0 ) .     at the optimal @xmath45 value , for the prediction of the servo motor angle ( left ) , as well as the grip force ( right).,title=\"fig:\",width=432 ] +      in this final experiment",
    ", we compared the performance of true online sarsa(@xmath0 ) with that of accumulate sarsa(@xmath0 ) and replace sarsa(@xmath0 ) , on a domain from the arcade learning environment ( ale ) @xcite , called asterix .",
    "the ale is a general testbed that provides an interface to hundreds of atari 2600 games .    in the asterix domain",
    ", the agent controls a yellow avatar , which has to collect ` potion ' objects , while avoiding ` harp ' objects ( see figure [ fig : asterix ] for a screenshot ) .",
    "both potions and harps move across the screen horizontally . every time the agent collects a potion it receives a reward of 50 points , and every time it touches a harp it looses a life ( it has three lives in total ) .",
    "the agent can use the actions _ up , right , down , _ and _ left _ , combinations of two directions , and a _ no - op _",
    "action , resulting in 9 actions in total .",
    "the game ends after the agent has lost three lives , or after 5 minutes , whichever comes first .",
    "we use linear function approximation using features derived from the screen pixels .",
    "specifically , we use what @xcite call the _ basic _ feature set , which `` encodes the presence of colours on the atari 2600 screen . ''",
    "it is obtained by first subtracting the game screen background ( see * ? ? ? * ) and then dividing the remaining screen in to @xmath200 tiles of size @xmath201 pixels .",
    "finally , for each tile , one binary feature is generated for each of the @xmath202 available colours , encoding whether a colour is active or not in that tile .",
    "this generates 28,672 features ( plus a bias term ) .    because episode lengths can vary hugely ( from about 10 seconds all the way up to 5 minutes ) ,",
    "constructing a fair performance metric is non - trivial .",
    "for example , comparing the average return on the first @xmath203 episodes of two methods is only fair if they have seen roughly the same amount of samples in those episodes , which is not guaranteed for this domain . on the other hand ,",
    "looking at the total reward collected for the first @xmath204 samples is also not a good metric , because there is no negative reward associated to dying . to resolve this",
    ", we look at the return per episode , averaged over the first @xmath204 samples . more specifically , our metric consists of the average score per episode while learning for 20 hours ( 4,320,000 frames ) .",
    "in addition , we averaged the resulting number over 400 independent runs .    , at optimal @xmath45 , on the asterix domain .",
    ", width=340 ]    as with the evaluation experiments , we performed a scan over the step - size @xmath45 and the trace - decay parameter @xmath0 .",
    "specifically , we looked at all combinations of @xmath205 , @xmath206 and @xmath207 ( these values were determined during a preliminary parameter sweep ) .",
    "we used a discount factor @xmath208 and @xmath160-greedy exploration with @xmath209 .",
    "the weight vector was initialized to the zero vector . also , as @xcite , we take an action at each 5 frames .",
    "this decreases the algorithms running time and avoids `` super - human '' reflexes .",
    "the results are shown in figure [ fig : results_asterix ] . on this domain ,",
    "the optimal performance of all three versions of sarsa(@xmath0 ) is similar .",
    "note that the way we evaluate a domain is computationally very expensive : we perform scans over @xmath0 and @xmath45 , and use a large number of independent runs to get a low standard error . in the case of asterix",
    ", this results in a total of @xmath210 runs per method .",
    "this rigorous evaluation prohibits us unfortunately to run experiments on the full suite of ale domains .",
    "figure [ fig : results summary ] summarizes the performance of the different td(@xmath0 ) versions on all evaluation domains .",
    "specifically , it shows the error for each method at its best settings of @xmath45 and @xmath0 .",
    "the error is normalized by dividing it by the error at @xmath141 ( remember that all versions of td(@xmath0 ) behave the same for @xmath141 ) .",
    "because @xmath141 lies in the parameter range that is being optimized over , the normalized error can never be higher than 1 .",
    "if for a method / domain the normalized error is equal to 1 , this means that setting @xmath0 higher than 0 either has no effect , or that the error gets worse . in either case ,",
    "eligibility traces are not effective for that method / domain .",
    "overall , true online td(@xmath0 ) is clearly better than accumulate td(@xmath0 ) and replace td(@xmath0 ) in terms of optimal performance .",
    "specifically , for each considered domain / representation , the error for true online td(@xmath0 ) is either smaller or equal to the error of accumulate / replace td(@xmath0 ) .",
    "this is especially impressive , given the wide variety of domains , and the fact that the computational overhead for true online td(@xmath0 ) is small ( see section [ sec : algorithm ] for details ) .",
    "the observed performance differences correspond well with the analysis from section [ sec : relative performance ] .",
    "in particular , note that mrp ( 10 , 3 , 0.1 ) has less states than the other two mrps , and hence the chance that the same feature has a non - zero value within a small time frame is larger .",
    "the analysis correctly predicts that this results in larger performance differences .",
    "furthermore , mrp @xmath211 is less stochastic than mrp @xmath212 , and hence it has a smaller variance on the return . also here , the experiments correspond with the analysis , which predicts that this results in a larger performance difference .    on the asterix domain ,",
    "the performance of the three sarsa(@xmath0 ) versions is similar .",
    "this is in accordance with the evaluation results , which showed that the size of the performance difference is domain dependent . in the worst case ,",
    "the performance of the true online method is similar to that of the regular method .",
    "-settings for all domains / representations , normalized with the td(0 ) error.,width=642 ]    the optimal performance is not the only factor that determines how good a method is ; what also matters is how easy it is to find this performance . the detailed plots in appendices [ sec : detailed mrps ] and [ sec : detailed myo ] reveal that the parameter sensitivity of accumulate td(@xmath0 ) is much higher than that of true online td(@xmath0 ) or replace td(@xmath0 ) .",
    "this is clearly visible for mrp ( 10 , 3 , 0.1 ) ( figure [ fig : randommrp1 ] ) , as well as the experiments with the myoelectric prosthetic arm ( figure [ fig : myoelectricresults_all ] ) .",
    "there is one more thing to take away from the experiments . in mrp",
    "( 10 , 3 , 0.1 ) with non - binary features , replace td(@xmath0 ) is not applicable and accumulate td(@xmath0 ) is ineffective .",
    "however , true online td(@xmath0 ) was still able to obtain a considerable performance advantage with respect to td(0 ) .",
    "this demonstrates that true online td(@xmath0 ) expands the set of domains / representations where eligibility traces are effective .",
    "this could potentially have far - reaching consequences . specifically , using non - binary features becomes a lot more interesting .",
    "replacing traces are not applicable to such representations , while accumulating traces can easily result in divergence of values .",
    "for true online td(@xmath0 ) , however , non - binary features are not necessarily more challenging than binary features . exploring new , non - binary representations could potentially further improve the performance for true online td(@xmath0 ) on domains such as the myoelectic prosthetic arm or the asterix domain .",
    "in appendix [ sec : derivation ] , it is shown that the true online td(@xmath0 ) equations can be derived directly from the online forward view equations . by using different online forward views , new true online methods can be derived .",
    "sometimes , small changes in the forward view , like using a time - dependent step - size , can result in surprising changes in the true online equations . in this section ,",
    "we look at a number of such variations .      when using a time - dependent step - size in the base equation of the forward view ( equation [ eq : real - time update ] ) and deriving the update equations following the procedure from appendix [ sec : derivation ] , it turns out that a slightly different trace definition appears .",
    "we indicate this new trace using a ` + ' superscript : @xmath213 . for fixed step - size ,",
    "this new trace definition is equal to : @xmath214 of course , using @xmath215 instead of @xmath58 also changes the weight vector update slightly .",
    "below , the full set of update equations is shown : @xmath216 in addition , @xmath217 . we can simplify the weight update equation slightly , by using @xmath218 which changes the update equations to :",
    "@xmath219 algorithm [ al : true td(lambda ) ] shows the corresponding pseudocode . of course , this pseudocode can also be used for constant step - size .",
    "@xmath220 , @xmath221 for @xmath55 @xmath222 loop ( over episodes ) : obtain initial @xmath4 @xmath223 while terminal state is not reached , do : obtain next feature vector @xmath62 and reward @xmath63 @xmath224 @xmath225 @xmath226 @xmath227 @xmath228 @xmath229 @xmath67 @xmath230      so far , we just considered _ on - policy _ methods , that is , methods that evaluate a policy that is the same as the policy that generates the samples .",
    "however , the true online principle can also be applied to _ off - policy _ methods , for which the evaluation policy is different from the behaviour policy . as a simple example , consider watkins s q(@xmath0 ) @xcite .",
    "this is an off - policy method that evaluates the greedy policy given an arbitrary behaviour policy .",
    "it does this by combining accumulating traces with a td error that uses the maximum state - action value of the successor state : @xmath231 in addition , traces are reset to 0 whenever a non - greedy action is taken .",
    "@xmath60 loop ( over episodes ) : obtain initial state @xmath158 select action @xmath159 based on state @xmath158 ( for example @xmath160-greedy ) @xmath161 features corresponding to @xmath162 @xmath163 while terminal state has not been reached , do : take action @xmath159 , observe next state @xmath164 and reward @xmath63 select action @xmath165 based on state @xmath164 @xmath232 ( if @xmath165 ties for the max , then @xmath233 ) @xmath166 features corresponding to @xmath234 ( if @xmath164 is terminal state , @xmath168 ) @xmath235 @xmath170 @xmath171 @xmath236 @xmath237 if @xmath238 : @xmath61 @xmath174 @xmath175    from an online forward - view perspective , the strategy of watkins s q(@xmath0 ) method can be interpreted as a growing update target that stops growing once a non - greedy action is taken .",
    "specifically , let @xmath239 be the first time step _ after _ time step @xmath20 that a non - greedy action is taken , then the interim update target for time step @xmath20 can be defined as : @xmath240 with @xmath241    algorithm",
    "[ al : true online watkins q ] shows the pseudocode for the true online method that corresponds with this update target definition .",
    "a problem with watkins s q(@xmath0 ) is that if the behaviour policy is very different from the greedy policy traces are reset very often , reducing the overall effect of the traces . in section [ sec : related work ] , we discuss more advanced off - policy methods .",
    "tabular features are a special case of linear function approximation .",
    "hence , the update equations for true online td(@xmath0 ) that are presented so far also apply to the tabular case . however , we discuss it here separately , because the simplicity of this special case can provide extra insight .    rewriting the true online update equations ( equations [ eq : to_delta_update ]  [ eq : to_weight_update ] ) for the special case of tabular features results in : @xmath242",
    "what is interesting about the tabular case is that the dutch - trace update reduces to a particularly simple form .",
    "in fact , for the tabular case , a dutch - trace update is equal to the weighted average between an accumulating - trace update and a replacing - trace update , with the weight of the former @xmath243 and the latter @xmath45 .",
    "[ al : tabular true online td(lambda ) ] shows the corresponding pseudocode .",
    "initialize @xmath244 for all @xmath14 loop ( over episodes ) : initialize @xmath158 @xmath245 for all @xmath14 @xmath246 while @xmath158 is not terminal , do : obtain next state @xmath164 and reward @xmath63 @xmath247 @xmath248 @xmath249 @xmath250 for all @xmath14 : @xmath251 @xmath252 @xmath253 @xmath254",
    "since the first publication on true online td(@xmath0 ) @xcite , several related papers have been published , extending the underlying concepts and improving the presentation . in sections",
    "[ sec : dutch traces ] , [ sec : bv derivation ] and [ sec : non - linear ] , we review those papers . in section [ sec : other variations ] , we discuss other variations of td(@xmath0 ) .",
    "as mentioned before , the traditional forward view , which is based on the @xmath0-return , is inherently an offline forward view , because the @xmath0-return is constructed from data up to the end of an episode . as a consequence , the work regarding equivalence between a forward view and a backward view traditionally focused on the final weight vector @xmath255 .",
    "this changed in 2014 , when two papers introduced an _",
    "forward view with a corresponding backward view that has an exact equivalence at each moment in time @xcite . while both papers introduced an online forward view , the two forward views presented are very different from each other .",
    "one difference is that the forward view introduced by van seijen & sutton is an on - policy forward view , whereas the forward view by sutton et al .",
    "is an off - policy forward view .",
    "however , there is an even more fundamental difference related to how the forward views are constructed . in particular ,",
    "the forward view by van seijen & sutton is constructed in such a way that at each moment in time the weight vector can be interpreted as the result of a sequence of updates of the form : @xmath256 by contrast , the forward view by sutton et al .",
    "gives the following interpretation : @xmath257 with @xmath258 some multi - step td error .",
    "of course , the different forward views also result in different backward views . whereas the backward view of sutton et al .",
    "uses a generalized version of an accumulating trace , the backward view of van seijen & sutton introduced a completely new type of trace .",
    "the advantage of a forward view based on ( [ eq : on update ] ) instead of ( [ eq : off update ] ) is that it results in much more stable updates . in particular ,",
    "the sensitivity to divergence of accumulate td(@xmath0 ) is a general side - effect of ( [ eq : off update ] ) , whereas ( [ eq : on update ] ) is much more robust with respect to divergence . as a result , true online td(@xmath0 )",
    "not only has the property that it has an exact equivalence with an online forward view at all times , it consistently dominates td(@xmath0 ) empirically .",
    "the strong performance of true online td(@xmath0 ) motivated @xcite to construct an off - policy version of the forward view of true online td(@xmath0 ) .",
    "the corresponding backward view resulted in the algorithm true online gtd(@xmath0 ) , which empirically outperforms gtd(@xmath0 ) .",
    "they also introduced the term ` dutch traces ' for the new eligiblity trace .",
    "van hasselt & sutton ( @xcite ) showed that dutch traces are not only useful for td learning . in an offline",
    "setting with no bootstrapping using dutch traces can result in certain computational advantages . to understand why ,",
    "consider the monte carlo algorithm ( mc ) , which updates state values at the end of an episode using ( [ eq : on update ] ) , with the full return as update target .",
    "mc requires storing all the feature vectors and rewards during an episode , so the memory complexity is linear in the length of the episode .",
    "moreover , the required computation time is distributed very unevenly : during an episode almost no computation is required , but at the end of an episode there is a huge peak in computation time due to all the updates that need to be performed . with dutch traces an alternative implementation can be made that results in the same final weight vector but that does not require storing all the feature vectors and where the required computation time is spread out evenly over all the time steps .",
    "van hasselt & sutton refer to this appealing property as span - independence : the memory and computation time required per time step is constant and independent of the span of the prediction .",
    "the task of finding an efficient backward view that corresponds exactly with a particular online forward view is not easy .",
    "moreover , there is no guarantee that there exists an efficient implementation of a particular online forward view .",
    "often , minor changes in the forward view determine whether or not an efficient backward view can be constructed .",
    "this created the desire to somehow automate the process of constructing an efficient backward view .",
    "van seijen & sutton ( @xcite ) did not provide a direct derivation of the backward view update equations ; they simply proved that the forward view and the backward view equations result in the same weight vectors .",
    "@xcite were the first to attempt to come up with a general strategy for deriving a backward view ( although for forward views based on equation [ eq : off update ] ) .",
    "van hasselt et al .",
    "( @xcite ) took the approach of providing a theorem that proves equivalence between a general forward view and a corresponding general backward view .",
    "they showed that the forward / backward view of true online td(@xmath0 ) is a special case of this general forward / backward view .",
    "they showed the same for the off - policy method that they introduced ",
    "true online gtd(@xmath0 ) .",
    "recently , @xcite extended this theorem further by proving equivalence between an even more general forward view and backward view .",
    "furthermore , @xcite derived backward views for a series of increasingly complex forward views .",
    "the derivation of the true online td(@xmath0 ) equations in appendix [ sec : derivation ] is similar to those derivations .",
    "the linear update equations of the online forward view presented in section [ sec : online forward view equations ] can be easily extended to the case of non - linear function approximation .",
    "unfortunately , it appears to be impossible to construct an efficient backward view with exact equivalence in the case of non - linear function approximation .",
    "the reason is that the derivation in appendix [ sec : derivation ] makes use of the fact that the gradient with respect to the value function is independent of the weight vector ; this does not hold for non - linear function approximation .",
    "fortunately , @xcite shows that many of the benefits of true online learning can also be achieved in the case of non - linear function approximation by using an alternative forward view ( but still based on equation [ eq : on update ] ) . while this alternative forward view is not fully online ( there is a delay in the updates ) , it can be implemented efficiently .",
    "several variations on td(@xmath0 ) other than those treated in this article have been suggested in the literature .",
    "@xcite introduced a variation of td(@xmath0 ) for which upper and lower bounds on performance can be derived and proven .",
    "@xcite introduced td@xmath259 , a parameter - free alternative to td(@xmath0 ) based on a multi - step update target called the @xmath17-return .",
    "td@xmath259 is an offline algorithm with a computational cost proportional to the episode - length .",
    "furthermore , @xcite proposed a method based on a multi - step update target , which they call the @xmath260-return .",
    "the @xmath260-return can account for the correlation of different length returns , something that both the @xmath0-return and the @xmath17-return can not .",
    "however , it is expensive to compute and it is open question whether efficient approximations exist .",
    "we tested the hypothesis that true online td(@xmath0 ) ( and true online sarsa(@xmath0 ) ) dominates td(@xmath0 ) ( and sarsa(@xmath0 ) ) with accumulating as well as with replacing traces by performing experiments over a wide range of domains .",
    "our extensive results support this hypothesis . in terms of learning speed ,",
    "true online td(@xmath0 ) was often better , but never worse than td(@xmath0 ) with either accumulating or replacing traces , across all domains / representations that we tried .",
    "our analysis showed that especially on domains with non - sparse features and a relatively low variance on the return a large difference in learning speed can be expected .",
    "more generally , true online td(@xmath0 ) has the advantage over td(@xmath0 ) with replacing traces that it can be used with non - binary features , and it has the advantage over td(@xmath0 ) with accumulating traces that it is less sensitive with respect to its parameters .",
    "in terms of computation time , td(@xmath0 ) has a slight advantage . in the worst case , true online td(@xmath0 )",
    "is twice as expensive . in the typical case of sparse features",
    ", it is only fractionally more expensive than td(@xmath0 ) . memory requirements are the same . finally , we outlined an approach for deriving new true online methods , based on rewriting the equations of an online forward view .",
    "this may lead to new , interesting methods in the future .",
    "we prove the theorem by showing that @xmath261 can be approximated by @xmath262 as @xmath263 , with @xmath264 . for readability",
    ", we will not use the ` td ' and ` @xmath0 ' superscripts ; instead , we always use weights with double indices for the online @xmath0-return algorithm and weights with single indices for accumulate td(@xmath0 ) .    the update equations for accumulate td(@xmath0 )",
    "are : @xmath265 by incremental substitution , we can write @xmath88 directly in terms of @xmath100 : @xmath266 using the summation rule @xmath267 we can rewrite this as : @xmath268    as part of the derivation shown in appendix [ sec : derivation ] , we prove the following ( see equation [ eq : l_difference ] ) : @xmath269 with @xmath270 by applying this sequentially for @xmath271 , we can derive : @xmath272 furthermore , @xmath273 can be written as : @xmath274 substituting this in ( [ eq : eq33 ] ) yields : @xmath275 using that @xmath276 , it follows that @xmath277 as @xmath263 , we can approximate this as : @xmath278 with @xmath279 the interim @xmath0-return that uses @xmath100 for all value evaluations . substituting this in ( [ eq : eq111 ] )",
    "yields : @xmath280    for the online @xmath0-return algorithm , we can derive the following by sequential substitution of equation ( [ eq : real - time update ] ) : @xmath281 as @xmath263 , we can approximate this as : @xmath282    combining ( [ eq : eq12 ] ) and ( [ eq:1234 ] ) , it follows that as @xmath263 : @xmath283 with @xmath284 from the condition @xmath103 it follows that @xmath264 .",
    "in this subsection , we derive the update equations of true online td(@xmath0 ) directly from the online forward view , defined by equations ( [ eq : interim lambda return1 ] ) and ( [ eq : real - time update ] ) ( and @xmath86 ) . the derivation is based on expressing @xmath285 in terms of @xmath286 .",
    "we start by writing @xmath286 directly in terms of the initial weight vector and the interim @xmath0-returns .",
    "first , we rewrite ( [ eq : real - time update ] ) as : @xmath287 with @xmath288 the identity matrix . now , consider @xmath289 for @xmath290 and @xmath291 : @xmath292 for general @xmath293 , we can write : @xmath294 where @xmath295 is defined as : @xmath296 and @xmath297 .",
    "we are now able to express @xmath286 as : @xmath298 because for the derivation of true online td(@xmath0 ) , we only need ( [ eq : w_kh ] ) and the definition of @xmath299 , we can drop the double indices for the weight vectors and use @xmath89 .",
    "we now derive a compact expression for the difference @xmath300 : @xmath301 note that the difference @xmath302 is naturally expressed using a term that looks like a td error but with a modified time step .",
    "we call this the modified td error , @xmath303 : @xmath304 the modified td error relates to the regular td error , @xmath57 , as follows : @xmath305 using @xmath303 , the difference @xmath302 can be compactly written as : @xmath306    to get the update rule , @xmath307 has to be expressed in terms of @xmath308 .",
    "this is done below , using ( [ eq : w_kh ] ) , ( [ eq : modified_delta_relation ] ) and ( [ eq : l_difference ] ) : @xmath309    we now have the update rule for @xmath88 , in addition to an explicit definition of @xmath58 . next ,",
    "using this explicit definition , we derive an update rule to compute @xmath58 from @xmath310 : @xmath311 equations ( [ eq : to_theta_update ] ) and ( [ eq : to_e_update ] ) , together with the definition of @xmath57 , form the true online td(@xmath0 ) update equations .",
    ", @xmath312 and @xmath313 .",
    "mse is the mean squared error averaged over the first 100 time steps , as well as 50 runs , and normalized using the initial error .",
    "the top graphs summarize the results from the graphs below it ; they show the mse error , for each @xmath0 , at the best step - size . ]    , @xmath314 and @xmath313 .",
    "mse is the mean squared error averaged over the first 1000 time steps , as well as 50 runs , and normalized using the initial error . ]    ,",
    "@xmath312 and @xmath315 .",
    "mse is the mean squared error averaged over the first 1000 time steps , as well as 50 runs , and normalized using the initial error . ]",
    "+            hebert , j.  s. , olson , j.  l. , morhart , m.  j. , dawson , m.  r. , marasco , p.  d. , kuiken , t.  a. , and chan , k.  m. ( 2014 ) .",
    "novel targeted sensory reinnervation technique to restore functional hand sensation after transhumeral amputation .",
    ", 22(4):763773 .",
    "konidaris , g. , niekum , s. , and thomas , p.  s. ( 2011 ) .",
    "td@xmath316 : re - evaluating complex backups in temporal difference learning . in _ proceedings of advances in neural information processing systems ( nips ) _ , pages 24022410 .",
    "mahmood , a.  r. and sutton , r.  s. ( 2015 ) .",
    "off - policy learning based on weighted importance sampling with linear computational complexity . in _ proceedings of the 31th conference on uncertainty in artificial intelligence ( uai)_.    mnih , v. , kavukcuoglu , k. , silver , d. , rusu , a.  a. , veness , j. , bellemare , m.  g. , graves , a. , riedmiller , m. , fidjeland , a.  k. , ostrovski , g. , petersen , s. , beattie , c. , sadik , a. , antonoglou , i. , kumaran , h. k.  d. , wierstra , d. , legg , s. , and hassabis , d. ( 2015 ) .",
    "human - level control through deep reinforcement learning .",
    ", 518:529533 .",
    "pilarski , p.  m. , dawson , m.  r. , degris , t. , carey , j.  p. , chan , k.  m. , hebert , j.  s. , and sutton , r.  s. ( 2013 ) .",
    "adaptive artificial limbs : a real - time approach to prediction and anticipation . , 20(1):5364 .",
    "sutton , r.  s. , maei , h.  r. , precup , d. , bhatnagar , s. , silver , d. , szepesvri , c. , and wiewiora , e. ( 2009a ) .",
    "fast gradient - descent methods for temporal - difference learning with linear function approximation . in _ proceedings of the 26th international conference on machine learning ( icml ) _ , pages 9931000 .",
    "sutton , r.  s. , maei , h.  r. , and szepesvri , c. ( 2009b ) . a convergent @xmath140 algorithm for off - policy temporal - difference learning with linear function approximation . in _ proceedings of advances in neural information processing systems 21 ( nips )",
    "_ , pages 16091616 .",
    "sutton , r.  s. , modayil , j. , delp , m. , degris , t. , pilarski , p.  m. , white , a. , and precup , d. ( 2011 ) .",
    "horde : a scalable real - time architecture for learning knowledge from unsupervised sensorimotor interaction . in _ proceedings of the 10th international conference on autonomous agents and multiagent systems ( aamas )",
    "_ , pages 761768 .",
    "thomas , p.  s. , niekum , s. , theocharous , g. , and konidaris , g. ( 2015 ) .",
    "policy evaluation using the @xmath260-return . in _ proceedings of advances in neural information processing systems 28 ( nips ) _ , pages 334342 .",
    "van hasselt , h. , mahmood , a.  r. and sutton , r.  s. ( 2014 ) .",
    "off - policy td(@xmath0 ) with a true online equivalence . in _ proceedings of the 30th conference on uncertainty in artificial intelligence ( uai)_."
  ],
  "abstract_text": [
    "<S> the temporal - difference methods td(@xmath0 ) and sarsa(@xmath0 ) form a core part of modern reinforcement learning . </S>",
    "<S> their appeal comes from their good performance , low computational cost , and their simple interpretation , given by their forward view . </S>",
    "<S> recently , new versions of these methods were introduced , called true online td(@xmath0 ) and true online sarsa(@xmath0 ) , respectively @xcite . </S>",
    "<S> algorithmically , these true online methods only make two small changes to the update rules of the regular methods , and the extra computational cost is negligible in most cases . </S>",
    "<S> however , they follow the ideas underlying the forward view much more closely . in particular , they maintain an exact equivalence with the forward view at all times , whereas the traditional versions only approximate it for small step - sizes . </S>",
    "<S> we hypothesize that these true online methods not only have better theoretical properties , but also dominate the regular methods empirically . in this article </S>",
    "<S> , we put this hypothesis to the test by performing an extensive empirical comparison . </S>",
    "<S> specifically , we compare the performance of true online td(@xmath0)/sarsa(@xmath0 ) with regular td(@xmath0)/sarsa(@xmath0 ) on random mrps , a real - world myoelectric prosthetic arm , and a domain from the arcade learning environment . </S>",
    "<S> we use linear function approximation with tabular , binary , and non - binary features . </S>",
    "<S> our results suggest that the true online methods indeed dominate the regular methods . across all domains / representations the learning speed of the true online methods are often better , but never worse than that of the regular methods . </S>",
    "<S> an additional advantage is that no choice between traces has to be made for the true online methods . besides the empirical results </S>",
    "<S> , we provide an in - depth analysis of the theory behind true online temporal - difference learning . in addition , we show that new true online temporal - difference methods can be derived by making changes to the online forward view and then rewriting the update equations .    temporal - difference learning , eligibility traces , forward - view equivalence </S>"
  ]
}