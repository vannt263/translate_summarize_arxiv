{
  "article_text": [
    "there is a growing interest in evolving complex networks , in particular , networks with scale  free ( sf ) topology  @xcite .",
    "sf occurs in many different contexts , including the www and the internet , e - mail and scientific  citation networks , ecological , protein and gene interaction networks , etc . in these examples , the degree @xmath0 of a vertex , i.e. , the number of arcs linking it to other vertices , is power - law distributed , @xmath1 ( see figure  [ fig1 ] ) .",
    "this implies that the network includes a relatively large number of nodes with small connectivity , defining what we call the network _ boundary _ , and a few nodes , the _ hubs _ , with a large connectivity , comparable to the network size @xmath2 . as a consequence ,",
    "sf networks exhibit the interesting _ small - world _",
    "property , that is , the average path length between two nodes is very small compared to the network size .",
    "evolving networks with such complex topology are also common in biology .",
    "neuronal networks , for instance , seem to exhibit the small - world property .",
    "this was recently demonstrated in a set of _ in - vitro _ experiments of growing cultured neurons  @xcite .",
    "although an impressive amount of work has been done in the last few years concerning sf networks , it has only recently been reported on the specific consequences of such an architecture on the performance of auto  associative neural networks  @xcite .",
    "the authors in  @xcite show that a sf neural network is able to store and retrieve @xmath3 patterns with a lower computer  memory cost than the fully  connected hopfield neural network .",
    "they also find a similar performance with a ( biologically unrealistic ) nearest  neighbor hypercubic ising lattice .",
    "the authors in  @xcite study the zero temperature behavior of different topologies , namely , the barabsi ",
    "albert ( ba ) sf , small  world and random diluted networks , and a better performance for the random diluted case than for the other topologies is reported . however , for the relative large mean connectivity these authors use ( @xmath4 ) , the ba network has not the sf property  @xcite , so that this result lacks interest .",
    "we here report on the influence of topology on the associative",
    " memory task of the network as a function of temperature .",
    "in particular we focus on two main issues , namely , on the robustness of the network performance against thermal noise for varying topology , and on the effect of varying the sf connectivity distribution @xmath5 on the network performance .",
    "consider the ba _ evolving _ network  @xcite with @xmath2 nodes and @xmath6 links . here",
    ", @xmath7 is the initial number of nodes generating the network , @xmath8 is the number of links that are added during the _ evolution _ at each time step , and @xmath2 is the final number of nodes in the network .",
    "this will latter be generalized to consider other sf networks . in order to have a neural system with the chosen topology , we place a binary neuron , @xmath9 or @xmath10 at each node @xmath11 and then store  @xmath3 binary random patterns , @xmath12 @xmath13 with mean activity level @xmath14 this",
    "is done in practice by associating a synaptic intensity @xmath15 at each link according to the hebbian learning rule , @xmath16    a meaningful direct comparison of this sf neural network ( sfnn ) and the standard hopfield neural network can not be made because the second case involves @xmath17 synaptic connections .",
    "a hypercubic ising lattice has the same number of synapses than the sfnn ; however , real neural systems are known to exhibit more complex neuron connectivity than the ising network .",
    "consequently , we compare the performance of the sfnn with that of a _ highly diluted _ hopfield network ( hdhn ) .",
    "the hdhn is obtained from the standard hopfield network by randomly suppressing synapses until only @xmath18 of them remain , i.e. , the number of synapses scales as @xmath2 and not as @xmath17 . to maintain the sf behavior in the ba network , the value of @xmath19 must be very small compared to the network size , that is , @xmath20",
    "the connectivity distribution of the hdhn is illustrated in figure  [ fig1 ] ( left ) .",
    "the main differences between this distribution and the corresponding one for a sf network is that the latter has no typical connectivity value . more specifically , the sf network distribution is a power ",
    "law while the hdhn distribution has a maximum and an gaussian decay and , consequently , may be characterized by a ( typical ) mean connectivity .",
    "a relevant magnitude to monitor in order to compare the performance of different topologies is the overlap function , defined for pattern @xmath21 as @xmath22 the performance of the two networks is compared in figure  [ fig2 ] for @xmath23 and @xmath24 this clearly shows that , excluding very low temperature , the retrieval of information as characterized by @xmath25 is better for the sfnn than for the hdhn . in both cases ,",
    "the retrieval of information deteriorates as @xmath3 is increased .",
    "however , we also observe that , at finite temperature , the performance of the sfnn increases significantly if one considers only the retrieval of    information concerning neurons with a connectivity degree higher than certain value , @xmath26 i.e. , the hubs  @xcite .",
    "this can be understood on simple grounds by computing the local overlap @xmath27 with one pattern for neuron @xmath28 and averaging over all neurons with the same connectivity @xmath29 the resulting mean overlap for a given connectivity @xmath30 is plotted in figure  [ fig2 ] ( right ) for different temperatures .",
    "this shows that , even at high temperature , the local overlap for hubs is close to one whereas it is very small for the boundary ( fluctuations in the lower curves are due to the small number of hubs present in the network for the relative small network size we are using ) .",
    "this finding reflects the `` negative '' role of the boundary and the `` positive '' role of the hubs on the sfnn performance during each retrieval experiment when thermal fluctuations are considered .",
    "this observation is in agreement with the @xmath31 behavior reported in  @xcite .",
    "another important issue is how the exponent of the distribution influences the performance of sf network for associative memory tasks . in order to analyze this ,",
    "we studied networks characterized by different power  law exponents . with this end",
    ", we used a molloy  red ( mr ) sf network  @xcite with @xmath32 where @xmath33 is a tunable parameter .",
    "as illustrated in figure  [ fig1 ] ( right ) , the number of neurons in the network with a high connectivity increases as @xmath33 is decreased .",
    "even more interesting is when one compares the behavior of the mr network with that of the hdhn in the limit @xmath31 ( cf .",
    "figure  [ fig3 ] ) . as thermal fluctuations",
    "are then suppressed , the network performance is only perturbed by the interference among the stored patterns . in order to consider the limit of interest , we started with one of the stored patterns , and computed the state of each neuron according to the deterministic rule @xmath34 here , @xmath35 is the heaviside step function , and @xmath36 is the local field associated with neuron @xmath28 with the sum over all neurons @xmath37 connected to it . at the end of the network evolution , we recorded the overlap , @xmath38    with the starting pattern as a function of the total number of stored patterns @xmath3 . in order to visualize the difference in performance between the two types of networks",
    ", we defined @xmath39 .",
    "figure  [ fig3 ] shows how this difference in performance varies with the number of stored patterns .",
    "the graph illustrates that the sfnn has a better and better performance as compared with the hdhn as the number of stored patterns is increased .",
    "this effect is enlarged as @xmath33 is increased .",
    "this can be understood by considering the different decays of @xmath5 for large @xmath0 in both sfnn and hdhn , and the fact that for increasing values of @xmath40 the relative position of @xmath41 moves to the left for both topologies , but due to the power  law decay the effect of the hubs remains for the sfnn .",
    "summing up , the topology of a neural network has a key role in the processes of memorization and retrieval of patterns .",
    "in particular , neural networks with scale  free topology may exhibit a better performance than hopfield  like networks with the same number of synapses distributed randomly over the network .",
    "our study can be useful to understand the role of regions with different connectivity degrees in real neural systems during memorization and retrieval of information .",
    "in particular , it may improve our understanding of how fluctuations or perturbations in the typical number of synapses of some brain areas can affect the processing of information and memorization in these regions .",
    "our study also suggests the convenience of developing new methods to store the more relevant information into the hubs , increasing in this way the effective network - performance and efficiency",
    ". it would be desirable to check our findings against experimental observations on real neural systems focusing on the topology which is built up by natural selection .",
    "we tanks dr . pastor - satorras for fruitful suggestions .",
    "this work was supported by the spanish mcyt and feder `` ramn y cajal '' contract and project no bfm2001 - 2841 ."
  ],
  "abstract_text": [
    "<S> we studied the computational properties of an attractor neural network ( ann ) with different network topologies . </S>",
    "<S> though fully connected neural networks exhibit , in general , a good performance , they are biologically unrealistic , as it is unlikely that natural evolution leads to such a large connectivity . </S>",
    "<S> we demonstrate that , at finite temperature , the capacity to store and retrieve binary patterns is higher for ann with scale  free ( sf ) topology than for highly random  diluted hopfield networks with the same number of synapses . we also show that , at zero temperature , the relative performance of the sf network increases with increasing values of the distribution power - law exponent . some consequences and possible applications of our findings are discussed .    </S>",
    "<S> ,    scale - free topology , autoassociative networks , storage capacity 87.10.+e , 05.10.-a , 05.50.+q </S>"
  ]
}