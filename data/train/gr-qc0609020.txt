{
  "article_text": [
    "this proceeding constitutes a short introduction to _",
    "spectral methods_. the aim is not to present an exhaustive mathematical presentation of those methods .",
    "numerous books can be consulted for this purpose ( see the bibliography for a sample of them ) .",
    "the following material should instead be considered as a toolkit for implementing simple spectral methods solvers .",
    "thus , a particular emphasize will be put on practical examples . for the sake of simplicity",
    ", we will restrict ourselves to one dimensional solvers .",
    "spectral methods are just one of the many ways to represent a function on a computer .",
    "the basic idea of all numerical techniques is to approximate any function by polynomials , those being the only functions than a computer can exactly calculate .",
    "so a function @xmath0 will be approximate by @xmath1 where the @xmath2 are polynomials and called the _",
    "trial functions_. depending on the choice of trial functions , one can generate various classes of numerical techniques .",
    "for example , the _ finite difference _",
    "schemes are obtained by choosing local polynomials of low degree . in all the following",
    ", we will be interested in spectral methods where the @xmath2 are global polynomials , typically legendre of chebyshev ones .",
    "if spectral methods are basically more evolved than finite difference schemes , they have long prove their ability to tackle a wide variety of problems .",
    "in particular , they allow to reach very good accuracy with only moderate computational resources .    in the first part of this article , we present the basic properties of spectral methods . after a brief overview of the foundations of the spectral expansion , we show the power of the method on simple but non - trivial examples . in particular , we show that , for @xmath3 functions , the error decays exponentially , as one increases the degree of the approximation .",
    "we then present some basic features of the two type of polynomials that are used in all the rest of this article , legendre and chebyshev polynomials .",
    "the second section is devoted to the actual implementation of partial differential equation solvers .",
    "we restrict ourselves to one dimensional equations on a bounded domain ( i.e. @xmath4 $ ] ) .",
    "three different types of solvers are presented :  the _ tau - method _ , the _ collocation method _ and the _ galerkin method_. they are explicitly implemented on a simple example .    the last part of this work is concerned with the implementation of _ multi - domain _ solvers . after having explained why this is often a valuable tool for the physicist , we present , once again , three different methods , all of them being tested an implemented on a simple , one dimensional equation , with a decomposition of space using two domains .",
    "let us consider an interval @xmath5 $ ] . in order to talk about basis , one needs to define a scalar product on @xmath6 .",
    "if @xmath7 is a positive function on @xmath6 , one can define the scalar product of two functions @xmath8 and @xmath9 , with respect to the _ measure _ @xmath7 as being [ scal_prod ] (f , g)_w = _",
    "f(x ) g(x ) w(x ) d x. using this scalar product , one can find a set of orthogonal polynomials @xmath10 , each of them of degree @xmath11 .",
    "the set composed of those polynomials , up to a given degree @xmath12 is a basis of @xmath13 .",
    "one can then hope to represent any function @xmath0 on @xmath6 by its projection on the polynomials @xmath10 .",
    "doing so , we define the projection of @xmath0 simply by [ proj ] p_nu = _ n=0^n _ n p_n(x ) , where the coefficients of the projections are given by @xmath14 .",
    "the difference between @xmath0 and its projection is called the _ truncation error _ and one can show that it goes to zero when the order of the approximation increases : u - p_nu 0 n .",
    "this convergence is illustrated on fig .",
    "[ f : proj ] , where a test function @xmath0 is plotted , with its projection for @xmath15 and @xmath16 .",
    "chebyshev polynomials are used .",
    "let us note that , even if @xmath0 is not a polynomial function , no discrepancy can be seen by eye , for @xmath12 as small as 8 .",
    "@xmath17 and its projection of chebyshev polynomials of degree smaller than @xmath18 ( left panel ) and @xmath19 ( right panel ) . ,",
    "title=\"fig:\",height=207 ]    @xmath17 and its projection of chebyshev polynomials of degree smaller than @xmath18 ( left panel ) and @xmath19 ( right panel ) .",
    ", title=\"fig:\",height=207 ]    this seems very appealing but for the fact that one needs to calculate the @xmath20 by computing integrals like @xmath21 . if one needs to evaluate @xmath0 at a lot of points in order to do so ( like when using standard newton method )",
    ", one would lose all the advantage of using spectral expansion .",
    "the solution to this problem is given by the gauss quadratures .",
    "the theorem can be stated as follows .",
    "_ there exist @xmath22 positive reals @xmath23 and @xmath22 reals @xmath24 in @xmath6 such that : _ [ gauss ] f_2n+ , _",
    "f(x ) w(x ) dx = _",
    "n=0^n f(x_n ) w_n .",
    "the @xmath23 are called the _ weights _ and the @xmath24 the collocation points .",
    "the exact degree of applicability depends on the quadrature .",
    "the three usual choices are :    * gauss : @xmath25 * gauss - radau : @xmath26 and @xmath27 . * gauss - lobatto : @xmath28 and @xmath27 and @xmath29 .",
    "gauss quadrature is the best possible choice in terms of degree ( it is not possible to find a quadrature with @xmath30 ) .",
    "however , the other quadratures have the property that the boundaries of the interval coincides with collocation points , which can be useful , to enforce boundary conditions for example . in all the following",
    ", we will use the gauss - lobatto quadrature .      if one applies the gauss quadratures to approximate the coefficient of the expansion , one obtains [ tilde_u ] _",
    "n = _ j=0^n",
    "u(x_j ) p_n(x_j ) w_j _ n = _ j=0^n p_n^2 (x_j ) w_j .",
    "let us precise that this is not exact in the sense that @xmath31 .",
    "however , the computation of @xmath32 only requires to evaluate @xmath0 at the @xmath22 collocation points .",
    "the _ interpolant _ of @xmath0 is then defined as the following polynomial [ interpol ] i_n u =",
    "n p_n(x ) .",
    "the difference between @xmath33 and @xmath34 is called the _",
    "aliasing error_. the interpolant of @xmath0 is the spectral approximate of @xmath0 and one can show that it is the only polynomials of degree @xmath12 that coincides with @xmath0 at each collocation point : [i_nu](x_i ) = u(x_i ) in .",
    "figure [ f : interpol ] shows the same function as fig.[f : proj ] but the interpolant is also plotted .",
    "one can see that , indeed @xmath35 coincides with @xmath0 at the collocation points that are indicated by the circles .",
    "once again , even with as few points as @xmath16 , no difference can be seen between the various functions .     same as fig .",
    "[ f : proj ] with also the interpolant of @xmath0 .",
    "the collocation points are denoted by the circles .",
    ", title=\"fig:\",height=207 ]   same as fig .",
    "[ f : proj ] with also the interpolant of @xmath0 .",
    "the collocation points are denoted by the circles .",
    ", title=\"fig:\",height=207 ]    figure [ f : conv_interpol ] shows the maximum difference between @xmath36 and @xmath0 on @xmath6 , as a function of the degree of the approximation @xmath12 .",
    "we can observe the very general feature of spectral methods that the error decreases exponentially , until one reaches the machine accuracy ( here @xmath37 , the computation being done in double precision ) .",
    "this very fast convergence explains why spectral methods are so efficient , especially compared to finite difference ones , where the error follows only a power - law in terms of @xmath12 .",
    "we will later be more quantitative about the convergence properties of the spectral approximation .",
    "maximum difference between @xmath35 and @xmath0 as a function of the degree of the approximation @xmath12 .",
    ", height=302 ]    let us note that a function @xmath0 can be described either by its value at each collocation point @xmath38 or by the coefficients of the interpolant @xmath39 .",
    "if the values at collocation points are known one is working in the _ configuration space _ and in the _ coefficient space _ if @xmath0 is given in terms of its coefficients .",
    "there is a bijection between the two descriptions and one simply goes from one space to another by using :    * @xmath40 ( configuration @xmath41 coefficient ) * @xmath42 ( coefficient @xmath41 configuration )    depending on the operation one has to perform , one choice of space is usually more suited than the other . for instance",
    ", let us assume that one wants to compute the derivative of @xmath0 .",
    "this is easily done if @xmath0 is known in the coefficient space .",
    "indeed , one can simply approximate @xmath43 by the derivative of the interpolant : u(x ) [i_n u] = _ n=0^n _ n p_n(x ) .",
    "such an approximation only requires the knowledge of the coefficients of @xmath0 and how the basis polynomials are derived .",
    "let us note that the obtained polynomial , even if it is a good approximation of @xmath43 , is not the interpolant of @xmath43 .",
    "in other terms , the interpolation and the derivation are two operations that do not commute :  @xmath44 .",
    "this is clearly illustrated on fig .",
    "[ f : der ] where the derivative of @xmath17 is plotted along with the functions @xmath45 and @xmath46 .",
    "in particular , one can note that the functions used to represent @xmath43 , i.e. @xmath45 does not coincide with @xmath43 at the collocation points .",
    "however , even with @xmath16 only , the three functions can not be distinguished by eye .",
    "first derivative of @xmath17 , interpolant of the derivative and derivative of the interpolant , for @xmath15 and @xmath16 .",
    ", title=\"fig:\",height=207 ]   first derivative of @xmath17 , interpolant of the derivative and derivative of the interpolant , for @xmath15 and @xmath16 .",
    ", title=\"fig:\",height=207 ]    the maximum difference between @xmath43 and @xmath45 , as a function of @xmath12 , is shown on fig . [",
    "f : conv_der ] .",
    "once , again , the convergence is exponential .",
    "maximum difference between @xmath45 and @xmath43 as a function of the degree of the approximation @xmath12 .",
    ", height=302 ]      in this section , we briefly present some basic properties of legendre and chebyshev polynomials .",
    "those two sets are the usual choice for non periodic problems .",
    "the legendre polynomials , denoted by @xmath47 , constitute a family of orthogonal polynomials on @xmath48 $ ] with a measure @xmath49 .",
    "the fact that the measure is so simple is one of the main advantage of working with legendre polynomials , especially from the analytical point of view .    the scalar product of two @xmath47 is given by : _",
    "-1 ^ 1 p_n p_m dx = _ mn .",
    "the successive polynomials can be constructed by recurrence .",
    "indeed given that @xmath50 and @xmath51 , all the @xmath47 can be obtained by using [ rec_leg ] (n+1 ) p_n+1(x ) = (2n+1 ) x p_n(x ) - n p_n-1(x ) .",
    "it is then easy to see that the legendre polynomials have the following simple properties : i ) @xmath47 has the same parity as @xmath11 .",
    "ii ) @xmath47 is of degree @xmath11 .",
    "iii ) @xmath52 .",
    "iv ) @xmath47 has exactly @xmath11 zero on @xmath53 $ ] .",
    "the first polynomials are plotted on fig .",
    "[ f : legendre ] .",
    "the first legendre polynomials , from @xmath54 to @xmath55 .",
    ", height=302 ]    the values of the weights and collocation points can be written for the three usual quadratures :    * legendre - gauss : @xmath56 are the nodes of @xmath57 and @xmath58 ^ 2}$ ] .",
    "* legendre - gauss - radau : @xmath59 and the @xmath56 are the nodes of @xmath60 .",
    "the weights are given by @xmath61 and @xmath62 .",
    "* legendre - gauss - lobatto : @xmath59 , @xmath63 and @xmath56 are the nodes of @xmath64 .",
    "the weights are @xmath65 ^ 2}$ ] .    from the above formula",
    ", one can see that the position of the collocation points are not analytical and they have to be computed numerically , which is one of the main shortcoming of legendre polynomials .",
    "one can also derive the action of some linear operation in the coefficient space . consider a function @xmath8 given by its coefficients : @xmath66 and @xmath67 be a linear operator acting on @xmath8 so that @xmath68 .",
    "the relation between the @xmath69 and @xmath70 can be explicitly written in some cases .",
    "for example :    * if @xmath67 is the multiplication by @xmath71 then : b_n = a_n-1 + a_n+1(n 1 ) .",
    "* if @xmath67 is the derivation : b_n = (2n+1 ) _ p = n+1 , p+n odd^n a_p . *",
    "if @xmath67 is the second derivation : b_n = (n+1/2 ) _ p = n+2 , p+n even^n [p(p+1 ) - n (n+1 ) ] a_p .",
    "those kind of relations enable to represent the action of @xmath67 as a matrix acting on the vector of the @xmath69 , the product being the coefficients of @xmath72 , i.e. the @xmath70 .      the chebyshev polynomials @xmath73 are an orthogonal set on @xmath48 $ ] for the measure @xmath74 .",
    "more precisely one has _ -1 ^ 1 x = (1+_0n ) _ mn .",
    "chebyshev polynomials can be computed by knowing that @xmath75 , @xmath76 and by making use of the recurrence : [ rec_cheb ] t_n+1 (x ) = 2x t_n(x ) - t_n-1(x ) .",
    "it follows that i ) @xmath73 has the same parity as @xmath11 .",
    "ii ) @xmath73 is of degree @xmath11 .",
    "iii ) @xmath77 .",
    "iv ) @xmath73 has exactly @xmath11 zero on @xmath53 $ ] .",
    "the first polynomials are plotted on fig .",
    "[ f : cheby ] .",
    "the first chebyshev polynomials , from @xmath78 to @xmath79 .",
    ", height=302 ]    the weights and collocation points associated with chebyshev polynomials can be computed :    * chebyshev - gauss : @xmath80 and @xmath81 .",
    "* chebyshev - gauss - radau : @xmath82 .",
    "the weights are @xmath83 and @xmath84 * chebyshev - gauss - lobatto : @xmath85 .",
    "the weights are @xmath86 and @xmath87 .",
    "contrary to legendre polynomials , the position of the collocation points are completely analytical which can somewhat simplify the computational task . in all the following ,",
    "chebyshev polynomials are used , except when otherwise stated .    once again",
    ", one can find the relation between the coefficients @xmath69 of a function @xmath8 and the coefficients @xmath70 of @xmath72 , where @xmath67 is a linear operator .",
    "for example :    * if @xmath67 is the multiplication by @xmath71 then : b_n = [(1+_0 n-1 ) a_n-1 + a_n+1 ] (n 1 ) . * if @xmath67 is the derivation : b_n = _",
    "p = n+1 , p+n odd^n pa_p . * if @xmath67 is the second derivation : b_n = _",
    "p = n+2 , p+n even^n p(p^2-n^2 ) a_p .",
    "as already seen , for sufficiently regular functions , the difference between @xmath0 and its interpolant @xmath35 goes to zero exponentially .",
    "this statement can be made more precise .",
    "let us consider a @xmath88 function @xmath0 .",
    "upper bounds on the difference between @xmath0 and @xmath35 can be found for various norms and choice of polynomials .",
    "* for legendre :  i_nu - u _ l^2 _",
    "k=0^m u^(k)_l^2 . * for chebyshev :  i_nu - u _ l_w^2 _",
    "k=0^m u^(k)_l_w^2 .",
    " i_nu - u _ _",
    "k=0^m u^(k)_l_w^2 .    without going into to much details",
    ", one can note that the errors decay faster than some power of @xmath12 .",
    "the upper bounds decay faster for more regular function ( i.e. for higher @xmath89 ) .",
    "an interesting case is the one of a @xmath90 function . in such a case",
    ", the error decays faster than any power of @xmath12 and thus like an exponential .",
    "one talks of an _ evanescent error _ and this is the case previously observed .",
    "a limit case can be provided by a discontinuous function .",
    "consider a step function @xmath0 such that @xmath91 and @xmath92 .",
    "if one tries to interpolate this function the convergence theorems previously exposed can not ensure convergence .",
    "this reflects on fig .",
    "[ f : gibbs ] where the step function is presented with interpolants for various number of points . as the number of collocation points increases , the maximum difference between @xmath0 and @xmath35 stays constant ( the amplitude of the oscillations can be seen to be constant ) . however , more and more oscillations are present , as the chebyshev polynomials do their best to approximate the discontinuous step function . in a sense ,",
    "the approximation is still better and better , the support of the difference being smaller and smaller . in other words , the integrated error : @xmath93 still goes to zero when @xmath12 increases , as can be seen on fig .",
    "[ f : gibbs_int ] .",
    "however , this convergence follows only a very slow - decaying power - law .",
    "step function and some interpolants for @xmath94 , @xmath95 and @xmath96 .",
    "as @xmath12 increases the maximum difference stays constant and the number of oscillations increases .",
    ", height=302 ]     @xmath97 as a function of @xmath12 , @xmath0 being a step function .",
    "the convergence obeys a power - law which decays slower than @xmath98 .",
    ", height=302 ]    the simple example of the step function is a very general feature of the so - called _ gibbs phenomenon _ , which occurs anytime one tries to interpolate a function that is not @xmath90 .",
    "when this happens , the error is no longer evanescent and only converges as a power - law . in order to maintain evanescent errors",
    ", one should try to avoid any gibbs phenomenon . in some cases",
    ", as will be seen later , this can be achieved by using a multi - domain decomposition of space .",
    "the type of problems with are concerned with in the section are ordinary differential equations , in one dimension only , on a bounded domain at the boundaries of which some conditions are enforced on the solution .",
    "mathematically , one considers the following system : [ e : bulk ] lu(x ) = s(x ) & & xu + [ e : boundary ] bu(y ) = 0 & & y u where @xmath99 and @xmath100 are linear differential operators .",
    "a function @xmath0 is then an admissible solution of this system , if and only if i ) it satisfies eq .",
    "( [ e : boundary ] ) `` exactly '' ( i.e. to machine accuracy ) ii ) it makes the residual @xmath101 small . in order to quantify what this `` small '' means ,",
    "the weighted residual method relies on @xmath22 tests functions @xmath102 and one asks that the scalar product of @xmath103 with those functions is exactly zero : [ e : residu ] (_k , r ) = 0 , k n. of course as @xmath12 increases the obtained solution is closer and closer to the real one . depending on the choice of spectral basis and of test functions , one can generate various different types of spectral solvers . in the following ,",
    "three of the most used ones are presented and applied to a simple case .",
    "we propose to solve the equation : - 4 + 4u = (x ) + c , with @xmath104 $ ] and @xmath105 . as boundary conditions",
    ", we simply ask that the solution is zero at the boundaries : [ e : bound_example ] u(-1 ) = 0 u(1 ) = 0 .    under those conditions ,",
    "the solution is unique and analytical : u_sol(x ) = (x ) - (2x ) + .",
    "let us note that this solution is not a polynomial .    with our notations",
    "the linear operator @xmath99 is @xmath106 . using the elementary linear operations seen in sec.[poly ]",
    ", one can construct the matrix representation of @xmath99 , which will come handy in the implementation of the various solvers .",
    "let us recall that if @xmath107 then @xmath108 .    in this particular case , and for @xmath15",
    ", one finds : l_ij =  (    [ cols=\"^,^,^,^,^ \" , ]     ) once it is inverted , the solution is found in the configuration space :    * in domain 1 : @xmath109 * in domain 2 : @xmath110 .    as for the other methods ,",
    "the convergence properties are shown in the next section .    before finishing with the variational method",
    ", it may be worthwhile to explain why legendre polynomials were used .",
    "suppose one wants to use chebyshev polynomials .",
    "the measure is then @xmath111 .",
    "when one integrates the term in @xmath112 by part one then gets -u  f w dx = [-u f w ] + u f w d x because the measure is divergent at the boundaries , it is difficult , if not impossible , to isolate the term in @xmath43 . on the other hand",
    "this is precisely the term that is needed to impose the appropriate matching of the solution .",
    "there might be ways around this but this explains why the variational method has been presented with legendre polynomials .      on fig .",
    "[ f : errors_multi ] , we show the maximum relative difference between the numerical solution and the analytical one .",
    "contrary to the case of just one domain ( see fig . [",
    "f : tau_gibbs ] ) , exponential decay of the error is recovered , as expected , and machine accuracy is very rapidly reached .",
    "this illustrates the usefulness of a multi - domain decomposition .",
    "relative difference between the analytical solution and the numerical one , for the three multi - domain methods exposed .",
    "the error is evanescent .",
    ", height=302 ]    from a numerical point of view , the method based on the homogeneous solutions is different from the two others . indeed ,",
    "when using it , the problem is treated domain by domain .",
    "the number of system to be solved is equal to the number of domains but each of those systems is roughly of size @xmath113 only .",
    "this is to be compared to the tau - method and the variational one where only one global system is inverted but which is of size @xmath114 .",
    "so , if the number of domain is important , one may consider the homogeneous solution method instead of the two others , which would give rise to huge systems .    on the other hand , the tau - method and the variational one , do not require the knowledge of the homogeneous solutions which is a good point , especially when the differential operator @xmath99 is complicated .",
    "the variational method may seem a little more involved , especially as it requires to work with legendre polynomials but , from a mathematical point of view , it is the only one that has been prove to be optimal .",
    "our introductory tour of spectral methods is now over . after having presented the mathematical foundations of this class of method , along with two families of standard polynomials , legendre and chebyshev ones , two simple test problems were treated by means of various methods .",
    "three ways of solving an ordinary differential equation on a single domain were presented and tested : the tau method , the collocation one and the galerkin method . in the last part , a discontinuous source was treated by means of a multi - domain decomposition .",
    "again , three methods were implemented and discussed : a multi - domain tau method , one based on the knowledge of homogeneous solutions and a variational method using legendre polynomials .",
    "this simple examples were intended to illustrate the usefulness of spectral methods to reach very good accuracy with moderate computational resources .",
    "however , this work is far from covering the huge field of all spectral methods . among the many aspects that were ignored one can mention the use of fourier transform for periodic functions .",
    "we also have left out all issues concerning compactification of space , by means of auxiliary variables like @xmath115 .",
    "three dimensional problems were also left out as were non - linear problems . finally , we did not present any problem involving time evolution .",
    "should the reader feel the urge to learn more about all this , we recommend that he consults some of the books given in the bibliography ."
  ],
  "abstract_text": [
    "<S> this proceeding is intended to be a first introduction to spectral methods . </S>",
    "<S> it is written around some simple problems that are solved explicitly and in details and that aim at demonstrating the power of those methods . </S>",
    "<S> the mathematical foundation of the spectral approximation is first introduced , based on the gauss quadratures . </S>",
    "<S> the two usual basis of legendre and chebyshev polynomials are then presented . </S>",
    "<S> the next section is devoted to one dimensional equation solvers using only one domain . </S>",
    "<S> three different methods are described . </S>",
    "<S> techniques using several domains are shown in the last section of this paper and their various merits discussed . </S>"
  ]
}