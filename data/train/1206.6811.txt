{
  "article_text": [
    "convergence to the poisson distribution , for the number of occurrences of possibly dependent events , naturally arises in various applications . following the work of poisson ,",
    "there has been considerable interest in how well the poisson distribution approximates the binomial distribution .",
    "this approximation was treated by a limit theorem in ( * ? ? ?",
    "* chapter  8) , and later some non - asymptotic results have considered the accuracy of this approximation . among these old and interesting results",
    ", le cam s inequality @xcite provides an upper bound on the total variation distance between the distribution of the sum @xmath0 of @xmath1 independent bernoulli random variables @xmath2 , where @xmath3 , and a poisson distribution @xmath4 with mean @xmath5 .",
    "this inequality states that    @xmath6 so if , e.g. , @xmath7 for every @xmath8 ( referring to the case that @xmath9 is binomially distributed ) then this upper bound is equal to @xmath10 , thus decaying to zero as @xmath1 tends to infinity .",
    "this upper bound was later improved , e.g. , by barbour and hall ( see ( * ? ? ?",
    "* theorem  1 ) ) , replacing the above upper bound by @xmath11 and therefore improving it by a factor of @xmath12 when @xmath13 is large .",
    "this improved upper bound was also proved by barbour and hall to be essentially tight ( see ( * ? ? ? * theorem  2 ) ) with the following lower bound on the total variation distance :    @xmath14 so the upper and lower bounds on the total variation distance differ by a factor of at most 32 , irrespectively of the value of @xmath13 ( it is noted that in ( * ? ? ? * remark  3.2.2 ) , the factor @xmath15 in the lower bound was claimed to be improvable to @xmath16 with no explicit proof ) . the poisson approximation and later also the compound poisson approximation have been extensively treated in the literature ( see , e.g. , the reference list in @xcite and this paper ) .    among modern methods ,",
    "the chen - stein method forms a powerful probabilistic tool that is used to calculate error bounds when the poisson approximation serves to assess the distribution of a sum of ( possibly dependent ) bernoulli random variables @xcite .",
    "this method is based on the simple property of the poisson distribution where @xmath17 with @xmath18 if and only if @xmath19 - { \\ensuremath{\\mathbb{e}}}[z \\ , f(z)]= 0 $ ] for all bounded functions @xmath20 that are defined on @xmath21 .",
    "this method provides a rigorous analytical treatment , via error bounds , to the case where @xmath22 has approximately the poisson distribution @xmath4 where it can be expected that @xmath23 - { \\ensuremath{\\mathbb{e}}}[w \\ , f(w ) ] \\approx 0 $ ] for an arbitrary bounded function @xmath20 that is defined on @xmath24 .",
    "the interested reader is referred to several comprehensive surveys on the chen - stein method in @xcite , @xcite , ( * ? ? ?",
    "* chapter  2 ) , @xcite , ( * ? ? ?",
    "* chapter  2 ) and @xcite .    during the last decade ,",
    "information - theoretic methods were exploited to establish convergence to poisson and compound poisson limits in suitable paradigms .",
    "an information - theoretic study of the convergence rate of the binomial - to - poisson distribution , in terms of the relative entropy between the binomial and poisson distributions , was provided in @xcite , and maximum entropy results for the binomial , poisson and compound poisson distributions were studied in @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and @xcite .",
    "the law of small numbers refers to the phenomenon that , for random variables @xmath2 defined on @xmath24 , the sum @xmath25 is approximately poisson distributed with mean @xmath5 if ( qualitatively ) the following conditions hold : @xmath26 is close to  1 , @xmath27 is uniformly small , @xmath28 is negligible as compared to @xmath29 , and @xmath2 are weakly dependent ( see @xcite , @xcite and @xcite ) .",
    "an information - theoretic study of the law of small numbers was provided in @xcite via the derivation of upper bounds on the relative entropy between the distribution of the sum of possibly dependent bernoulli random variables and the poisson distribution with the same mean .",
    "an extension of the law of small numbers to a thinning limit theorem for convolutions of discrete distributions that are defined on @xmath24 was introduced in @xcite , followed by an analysis of the convergence rate and some non - asymptotic results .",
    "further work in this direction was studied in @xcite , and the work in @xcite provides an information - theoretic study for the problem of compound poisson approximation , which parallels the earlier study for the poisson approximation in @xcite . a recent follow - up to the works in @xcite and @xcite",
    "is provided in @xcite and @xcite , considering connections between stein characterizations and fisher information functionals .",
    "nice surveys on the line of work on information - theoretic aspects of the poisson approximation are introduced in ( * ? ? ?",
    "* chapter  7 ) and @xcite .",
    "furthermore , ( * ? ? ?",
    "* chapter  2 ) surveys some commonly - used metrics between probability measures with some pointers to the poisson approximation .",
    "this paper provides an information - theoretic study of the poisson approximation via the chen - stein method .",
    "the novelty of this paper is considered to be in the following aspects :    * consider the entropy of a sum of ( possibly dependent and non - identically distributed ) bernoulli random variables .",
    "upper bounds on the error that follows from an approximation of this entropy by the entropy of a poisson random variable with the same mean are derived via the chen - stein method ( see theorem  [ theorem : upper bound on the poisson approximation of the entropy ] and its related results in section  [ section : error bounds on the entropy of the sum of bernoulli random variables ] ) .",
    "the use of these new bounds is exemplified for some interesting applications of the chen - stein method in @xcite and @xcite .",
    "* improved lower bounds on the relative entropy between the distribution of a sum of independent bernoulli random variables and the poisson distribution with the same mean are derived ( see theorem  [ theorem : improved lower bound on the relative entropy ] in section  [ section : improved lower bounds on the total variation distance etc . ] ) .",
    "these new bounds are obtained by combining a derivation of some sharpened lower bounds on the total variation distance ( see theorem  [ theorem : improved lower bound on the total variation distance ] and some related results in section  [ section : improved lower bounds on the total variation distance etc . ] ) that improve the original lower bound in ( * ? ? ?",
    "* theorem  2 ) , and a probability - dependent refinement of pinsker s inequality @xcite .",
    "the new lower bounds are compared with existing upper bounds . *",
    "new upper and lower bounds on the chernoff information and bhattacharyya parameter are also derived in section  [ section : improved lower bounds on the total variation distance etc . ] via the introduction of new bounds on the hellinger distance and relative entropy .",
    "the use of the new lower bounds on the relative entropy and chernoff information is exemplified in the context of binary hypothesis testing .",
    "the impact of the improvements of these new bounds is studied as well .    to the best of our knowledge , among the publications of the _ ieee trans .",
    "on information theory _ , the chen - stein method for poisson approximation was used so far only in two occasions . in @xcite , this probabilistic method",
    "was used by a. j. wyner to analyze the redundancy and the distribution of the phrase lengths in one of the versions of the lempel - ziv data compression algorithm . in the second occasion , this method was applied in @xcite in the context of random networks . in @xcite , the authors relied on existing upper bounds on the total variation distance , applying them to analyze the asymptotic distribution of the number of isolated nodes in a random grid network where nodes are always active .",
    "the first part of this paper relies ( as well ) on some existing upper bounds on the total variation distance , with the purpose of obtaining error bounds on the poisson approximation of the entropy for a sum of ( possibly dependent ) bernoulli random variables , or more generally for a sum of non - negative , integer - valued and bounded random variables ( this work relies on stronger versions of the upper bounds in ( * ? ? ?",
    "* theorems  2.2 and  2.4 ) ) .",
    "the paper is structured as follows : section  [ section : error bounds on the entropy of the sum of bernoulli random variables ] forms the first part of this work where the entropy of the sum of bernoulli random variables is considered . section  [ section : improved lower bounds on the total variation distance etc .",
    "] provides the second part of this work where new lower bounds on the total variation distance and relative entropy between the distribution of the sum of independent bernoulli random variables and the poisson distribution are derived .",
    "the derivation of the new and improved lower bounds on the total variation distance relies on the chen - stein method for the poisson approximation , and it generalizes and tightens the analysis that was used to derive the original lower bound on the total variation distance in @xcite . the derivation of the new lower bound on the relative entropy follows from the new lower bounds on the total variation distance , combined with a distribution - dependent refinement of pinsker s inequality in @xcite .",
    "the new lower bound on the relative entropy is compared to a previously reported upper bound on the relative entropy from @xcite .",
    "upper and lower bounds on the bhattacharyya parameter , chernoff information and the hellinger , local and kolmogorov - smirnov distances between the distribution of the sum of independent bernoulli random variables and the poisson distribution with the same mean are also derived in section  [ section : improved lower bounds on the total variation distance etc . ] via some relations between these quantities with the total variation distance and the relative entropy .",
    "the analysis in this work combines elements of information theory with the chen - stein method for poisson approximation .",
    "the use of these new bounds is exemplified in the two parts of this work , partially relying on some interesting applications of the chen - stein method for the poisson approximation that were introduced in @xcite and @xcite .",
    "the bounds that are derived in this work are easy to compute , and their applicability is exemplified . throughout the paper , the logarithms",
    "are expressed on the natural base ( on base @xmath30 ) .",
    "this section considers the entropy of a sum of ( possibly dependent and non - identically distributed ) bernoulli random variables .",
    "section  [ subsection : first part of the review of some known results ] provides a review of some reported results on the poisson approximation , whose derivation relies on the chen - stein method , that are relevant to the analysis in this section .",
    "the original results of this section are introduced from section  [ subsection : a new result on the entropy of discrete random variables ] which provides an upper bound on the entropy difference between two discrete random variables in terms of their total variation distance .",
    "this bound is later in this section in the context of the poisson approximation .",
    "section  [ subsection : new error bounds on the entropy ] introduces some explicit upper bounds on the error that follows from the approximation of the entropy of a sum of bernoulli random variables by the entropy of a poisson random variable with the same mean .",
    "some applications of the new bounds are exemplified in section  [ subsection : examples for the use of the mew error bounds on the entropy ] , and these bounds are proved in section  [ subsection : proofs of the new bounds in the first part of this paper ] .",
    "finally , a generalization of these bounds is introduced in section  [ subsection : generalization of the bounds on the entropy for the sum of integer - valued random variables ] to address the case of the poisson approximation for the entropy of a sum of non - negative , integer - valued and bounded random variables .      throughout the paper ,",
    "we use the term ` distribution ' to refer to the discrete probability mass function of an integer - valued random variable . in the following ,",
    "we review briefly some known results that are used for the analysis later in this section .",
    "let @xmath31 and @xmath32 be two probability measures defined on a set @xmath33 .",
    "then , the total variation distance between @xmath31 and @xmath32 is defined by @xmath34 where the supermum is taken w.r.t . all the borel subsets @xmath35 of @xmath33 .",
    "if @xmath33 is a countable set then is simplified to @xmath36 so the total variation distance is equal to one - half of the @xmath37-distance between the two probability distributions .",
    "[ definition : total variation distance ]    the following theorem combines ( * ? ? ?",
    "* theorems  1 and 2 ) , and its proof relies on the chen - stein method :    let @xmath38 be a sum of @xmath1 independent bernoulli random variables with @xmath39 for @xmath8 , and @xmath40 . then , the total variation distance between the probability distribution of @xmath22 and the poisson distribution with mean @xmath13 satisfies @xmath41 where @xmath42 for every @xmath43 .",
    "[ theorem : bounds on the total variation distance - barbour and hall 1984 ]    the ratio between the upper and lower bounds in theorem  [ theorem : bounds on the total variation distance - barbour and hall 1984 ] is not larger than  32 , irrespectively of the values of @xmath44 .",
    "this shows that , for independent bernoulli random variables , these bounds are essentially tight .",
    "the upper bound in improves le cam s inequality ( see @xcite , @xcite ) ) which states that @xmath45 so the improvement , for large values of @xmath13 , is approximately by the factor @xmath12 .",
    "theorem  [ theorem : bounds on the total variation distance - barbour and hall 1984 ] provides a non - asymptotic result for the poisson approximation of sums of independent binary random variables via the use of the chen - stein method . in general , this method enables to analyze the poisson approximation for sums of dependent random variables .",
    "to this end , the following notation was used in @xcite and @xcite :    let @xmath46 be a countable index set , and for @xmath47 , let @xmath48 be a bernoulli random variable with @xmath49 let @xmath50 where it is assumed that @xmath18 .",
    "for every @xmath47 , let @xmath51 be a subset of @xmath46 that is chosen such that @xmath52 .",
    "this subset is interpreted in @xcite as the neighborhood of dependence for @xmath53 in the sense that @xmath48 is independent or weakly dependent of all of the @xmath54 for @xmath55 .",
    "furthermore , the following coefficients were defined in ( * ? ? ?",
    "* section  2 ) : @xmath56 & & b_2 \\triangleq \\sum_{\\alpha \\in i } \\sum_{\\beta \\in b_{\\alpha } \\setminus \\{\\alpha\\ } } p_{\\alpha , \\beta } , \\quad p_{\\alpha , \\beta } \\triangleq { \\ensuremath{\\mathbb{e}}}(x_{\\alpha } x_{\\beta } ) \\label{eq : b2 } \\\\[0.1 cm ] & & b_3 \\triangleq \\sum_{\\alpha \\in i } s_{\\alpha } , \\quad \\quad s_{\\alpha } \\triangleq { \\ensuremath{\\mathbb{e}}}\\bigl| { \\ensuremath{\\mathbb{e}}}(x_{\\alpha } - p_{\\alpha } \\ , | \\ , \\sigma(\\{x_{\\beta}\\})_{\\beta \\in i \\setminus b_{\\alpha } } ) \\bigr|   \\label{eq : b3}\\end{aligned}\\ ] ] where @xmath57 in the conditioning of denotes the @xmath58-algebra that is generated by the random variables inside the parenthesis . in the following ,",
    "we cite ( * ? ? ?",
    "* theorem  1 ) which essentially implies that when @xmath59 and @xmath60 are all small , then the total number @xmath22 of events is approximately poisson distributed .",
    "let @xmath61 be a sum of ( possibly dependent and non - identically distributed ) bernoulli random variables @xmath62 . then , with the notation in ",
    ", the following upper bound on the total variation distance holds : @xmath63 [ theorem : upper bound on the total variation distance by arratia et al . ]    a comparison of the right - hand side of with the bound in ( * ? ? ?",
    "* theorem  1 ) shows a difference in a factor of 2 between the two upper bounds .",
    "this follows from a difference in a factor of  2 between the two definitions of the total variation distance in ( * ? ? ?",
    "* section  2 ) and definition  [ definition : total variation distance ] here .",
    "it is noted , however , that definition  [ definition : total variation distance ] in this work is consistent , e.g. , with @xcite and @xcite .",
    "theorem  [ theorem : upper bound on the total variation distance by arratia et al . ]",
    "forms a generalization of the upper bound in theorem  [ theorem : bounds on the total variation distance - barbour and hall 1984 ] by choosing @xmath64 for @xmath65 ( note that , due to the independence assumption of the bernoulli random variables in theorem  [ theorem : bounds on the total variation distance - barbour and hall 1984 ] , the neighborhood of dependence of @xmath53 is @xmath53 itself ) . in this",
    "setting , under the independence assumption , @xmath66 which therefore gives , from , the upper bound on the right - hand side of .",
    "[ remark : generalization of theorem 1 of barbour and hall ]    before proceeding to this analysis , the following maximum entropy result of the poisson distribution is introduced .",
    "the poisson distribution @xmath4 has the maximal entropy among all probability distributions with mean @xmath13 that can be obtained as sums of independent bernoulli rvs : @xmath67 furthermore , since the supremum of the entropy over the set @xmath68 is monotonic increasing in @xmath1 , then @xmath69 for @xmath70 , the maximum entropy distribution in the class @xmath68 is the binomial distribution of the sum of @xmath1 i.i.d .",
    "bernoulli random variables @xmath71 , so @xmath72 [ theorem : maximum entropy result for the poisson distribution ]    theorem  [ theorem : maximum entropy result for the poisson distribution ] partially appears in ( * ? ? ?",
    "* proposition  2.1 ) ( see ( * ? ? ?",
    "* eq .  ( 2.20 ) ) ) .",
    "this theorem follows directly from ( * ? ? ?",
    "* theorems  7 and 8) .    the maximum entropy result for the poisson distribution in theorem  [ theorem : maximum entropy result for the poisson distribution ]",
    "was strengthened in @xcite by showing that the supermum on the right - hand side of can be extended to the larger set of ultra - log - concave probability mass functions ( that includes the binomial distribution ) .",
    "this result for the poisson distribution was generalized in @xcite and @xcite to maximum entropy results for discrete compound poisson distributions .",
    ": in the next sub - section , we consider the approximation of the entropy of a sum of bernoulli random variables by the entropy of a poisson random variable with the same mean . to this end , it is required to evaluate the entropy of @xmath17 .",
    "it is straightforward to verify that @xmath73 so the entropy of the poisson distribution ( in nats ) is given in terms of an infinite series that has no closed - form expression .",
    "sequences of simple upper and lower bounds on this entropy , which are asymptotically tight , were derived in @xcite . in particular , from ( * ? ? ?",
    "* theorem  2 ) , @xmath74 which gives tight bounds on the entropy of @xmath17 for large values of @xmath13 . for @xmath75 ,",
    "the entropy of @xmath76 is approximated by the average of its upper and lower bounds in , asserting that the relative error of this approximation is less than @xmath77 ( and it decreases like @xmath78 while increasing the value of @xmath13 ) .",
    "for @xmath79 , a truncation of the infinite series on the right - hand side of after its first @xmath80 terms gives an accurate approximation .",
    "the following theorem provides a new upper bound on the entropy difference between two discrete random variables in terms of their total variation distance .",
    "this theorem relies on the bound of ho and yeung in ( * ? ? ?",
    "* theorem  6 ) that forms an improvement over the previously reported bound in ( * ? ? ?",
    "* theorem  17.3.3 ) or ( * ? ? ?",
    "* lemma  2.7 ) .",
    "the following new bound is later used in this section in the context of the poisson approximation .",
    "let @xmath81 be a countable infinite set .",
    "let @xmath82 and @xmath83 be two discrete random variables where @xmath82 takes values from a finite set @xmath84 , for some @xmath85 , and @xmath83 takes values from the entire set @xmath86 .",
    "assume that @xmath87 for some @xmath88 , and let @xmath89 furthermore , let @xmath90 be set such that @xmath91 then @xmath92 where @xmath93 denote the binary entropy function .",
    "[ theorem : l1 bound on the entropy ]    let @xmath94 be a random variable that is defined to be equal to @xmath83 if @xmath95 , and it is set to be equal to @xmath96 if @xmath97 for some @xmath98 .",
    "hence , the probability mass function of @xmath94 is related to that of @xmath83 as follows @xmath99                            \\sum_{j = m}^{\\infty } p_y(a_j )   & \\mbox{if",
    "$ i = m$. }                            \\end{array }                            \\right .",
    "\\label{eq : probability mass function of y tilde}\\ ] ] since @xmath100 for every @xmath101 and @xmath102 , then it follows from that @xmath103 hence , @xmath82 and @xmath94 are two discrete random variables that take values from the set @xmath104 ( note that it includes the set @xmath33 ) and @xmath105 ( see and ) .",
    "the bound in ( * ? ? ?",
    "* theorem  6 ) therefore implies that if @xmath106 ( which is indeed the case , due to the way @xmath107 is defined in ) , then @xmath108 since @xmath94 is a deterministic function of @xmath83 then @xmath109 , and therefore and imply that @xmath110 & & = - \\sum_{i = m}^{\\infty } p_y ( a_i ) \\ , \\log p_y(a_i ) + \\left ( \\sum_{i = m}^{\\infty } p_y(a_i ) \\right ) \\log \\left ( \\sum_{i = m}^{\\infty } p_y(a_i ) \\right ) \\nonumber \\\\ & & \\leq - \\sum_{i = m}^{\\infty } p_y ( a_i ) \\ , \\log p_y(a_i ) \\nonumber \\\\ & & \\leq \\mu .",
    "\\label{eq : bound on the entropy difference of y and y tilde}\\end{aligned}\\ ] ] finally , the bound in follows from , and the triangle inequality .      the new bounds on the entropy of sums of bernoulli random variables are introduced in the following .",
    "their use is exemplified in section  [ subsection : examples for the use of the mew error bounds on the entropy ] , and their proofs appear in section  [ subsection : proofs of the new bounds in the first part of this paper ] .",
    "let @xmath46 be an arbitrary finite index set with @xmath111 . under the assumptions of theorem  [ theorem :",
    "upper bound on the total variation distance by arratia et al . ] and the notation used in eqs .  ",
    ", let @xmath112 & & m \\triangleq \\max \\left\\{n+2 , \\frac{1}{1-\\eta } \\right\\ } \\label{eq : function m in the upper bound on the poisson approximation of the entropy } \\\\[0.2 cm ] & & \\mu \\triangleq \\left [ \\bigl(\\lambda \\log \\bigl(\\frac{e}{\\lambda}\\bigr)\\bigr)_+ \\ , + \\lambda^2 + \\frac{6",
    "\\log(2\\pi ) + 1}{12 } \\right ] \\ , \\exp \\left\\{-\\left[\\lambda + ( m-2 ) \\ , \\log\\left(\\frac{m-2}{\\lambda e } \\right ) \\right ] \\right\\ } \\label{eq : function mu in the upper bound on the poisson approximation of the entropy}\\end{aligned}\\ ] ] where , in , @xmath113 for every @xmath114 .",
    "let @xmath17 be a poisson random variable with mean @xmath13 . if @xmath115 , then the difference between the entropies of @xmath76 and @xmath22 satisfies the following inequality : @xmath116 [ theorem : upper bound on the poisson approximation of the entropy ]    the following corollary refers to the entropy of a sum of independent bernoulli random variables :    consider the setting in theorem  [ theorem : upper bound on the poisson approximation of the entropy ] , and assume that the bernoulli random variables @xmath62 are also independent .",
    "then , the following inequality holds : @xmath117 where @xmath118 in is specialized to @xmath119 [ corollary : upper bound on the poisson approximation of the entropy for independent rvs ]    the following bound forms a possible improvement of the result in corollary  [ corollary : upper bound on the poisson approximation of the entropy for independent rvs ] .",
    "assume that the conditions in corollary  [ corollary : upper bound on the poisson approximation of the entropy for independent rvs ] are satisfied .",
    "then , inequality holds with the new parameter @xmath120 where @xmath121 [ proposition : a possibly improved error bound on the poisson approximation of the entropy for independent bernoulli rvs ]    from and , it follows that @xmath122 .",
    "the condition that @xmath115 is mild since it is a meaningful upper bound on the total variation distance ( which is bounded by 1 ) .",
    "proposition  [ proposition : a possibly improved error bound on the poisson approximation of the entropy for independent bernoulli rvs ] improves the bound in corollary  [ corollary : upper bound on the poisson approximation of the entropy for independent rvs ] only if @xmath123 is below a certain value that depends on @xmath13 .",
    "the maximal improvement that is obtained by proposition  [ proposition : a possibly improved error bound on the poisson approximation of the entropy for independent bernoulli rvs ] , as compared to corollary  [ corollary : upper bound on the poisson approximation of the entropy for independent rvs ] , is in the case where @xmath124 and @xmath125 , and the corresponding improvement in the value of @xmath118 is by a factor of @xmath126 .      in the following",
    ", the use of theorem  [ theorem : upper bound on the poisson approximation of the entropy ] is exemplified for the estimation of the entropy of sums of ( possibly dependent ) bernoulli random variables .",
    "it starts with a simple example where the summands are independent binary random variables , and some interesting examples from ( * ? ? ?",
    "* section  3 ) and ( * ? ? ?",
    "* section  4 ) are considered next .",
    "these examples are related to sums of dependent bernoulli random variables , where the use of theorem  [ theorem : upper bound on the poisson approximation of the entropy ] is exemplified for the calculation of error bounds on the entropy via the chen - stein method .",
    "let @xmath38 be a sum of @xmath1 independent bernoulli random variables where @xmath3 for @xmath127 .",
    "the calculation of the entropy of @xmath22 involves the numerical computation of the probabilities @xmath128 whose computational complexity is high for very large values of @xmath1 , especially if the probabilities @xmath129 are not the same .",
    "the bounds in corollary  [ corollary : upper bound on the poisson approximation of the entropy for independent rvs ] and proposition  [ proposition : a possibly improved error bound on the poisson approximation of the entropy for independent bernoulli rvs ] provide rigorous upper bounds on the accuracy of the poisson approximation for @xmath130 .",
    "lets exemplify this in the case where @xmath131 then @xmath132 and from @xmath133 the entropy of the poisson random variable @xmath17 is evaluated via the bounds in ( since @xmath134 , these bounds are tight ) , and they imply that @xmath135 . from corollary  [ corollary : upper bound on the poisson approximation of the entropy for independent rvs ] ( see eq .   where @xmath136 ) , it follows that @xmath137 , and proposition  [ proposition : a possibly improved error bound on the poisson approximation of the entropy for independent bernoulli rvs ] improves it to @xmath138 .",
    "hence , @xmath139 with a relative error of at most @xmath140    this problem , which appears in ( * ? ? ?",
    "* example  1 ) , is described as follows : on the cube @xmath141 , assume that each of the @xmath142 edges is assigned a random direction by tossing a fair coin .",
    "let @xmath143 be fixed , and denote by @xmath144 the random variable that is equal to the number of vertices at which exactly @xmath145 edges point outward ( so @xmath146 corresponds to the event where all @xmath1 edges , from a certain vertex , point inward ) .",
    "let @xmath46 be the set of all @xmath147 vertices , and @xmath48 be the indicator that vertex @xmath47 has exactly @xmath145 of its edges directed outward .",
    "then @xmath61 with @xmath148 this implies that @xmath149 ( since @xmath150 ) . clearly , the neighborhood of dependence of a vertex @xmath47 , denoted by @xmath51 , is the set of vertices that are directly connected to @xmath53 ( including @xmath53 itself since theorem  [ theorem : upper bound on the total variation distance by arratia et al . ]",
    "requires that @xmath52 ) .",
    "it is noted , however , that @xmath51 in ( * ? ? ?",
    "* example  1 ) was given by @xmath151 so it excluded the vertex @xmath53 . from",
    ", this difference implies that @xmath152 in their example should be modified to @xmath153 & & \\hspace*{0.4 cm } = 2^{-n } ( n+1 ) { { n}\\choose{k}}^2\\end{aligned}\\ ] ] so @xmath152 is larger than its value in @xcite by a factor of @xmath154 which has a negligible effect if @xmath155 . as is noted in @xcite ,",
    "if @xmath53 and @xmath156 are two vertices that are connected by an edge , then a conditioning on the direction of this edge gives that @xmath157 and therefore , from , @xmath158 finally , as is noted in ( * ? ? ?",
    "* example  1 ) , @xmath159 ( this is because the conditional expectation of @xmath48 given @xmath160 is , similarly to the un - conditional expectation , equal to @xmath161 ; i.e. , the directions of the edges outside the neighborhood of dependence of @xmath53 are irrelevant to the directions of the edges connecting the vertex @xmath53 ) .    in the following",
    ", theorem  [ theorem : upper bound on the poisson approximation of the entropy ] is applied to get a rigorous error bound on the poisson approximation of the entropy @xmath130 .",
    "table  [ table : a random graph problem ] presents numerical results for the approximated value of @xmath130 , and the maximal relative error that is associated with this approximation .",
    "note that , by symmetry , the cases with @xmath162 and @xmath163 are equivalent , so @xmath164    [ cols=\"^,^,^,^,^\",options=\"header \" , ]     table  [ table : the gaussian problem in this paper ] supports the following observations , which are first listed and then explained :    * for fixed values of @xmath1 and @xmath123 , the poisson approximation is improved by increasing the level @xmath165 .",
    "* for fixed values of @xmath1 and @xmath165 , the error bounds for the poisson approximation of the entropy improve when the value of @xmath123 is modified in a way that decreases the lag-1 auto - correlation @xmath166 in .",
    "* for fixed values of @xmath1 and @xmath165 , the effect of the tightened upper bound of @xmath167 ( see ) on the error bound of the entropy @xmath130 is more enhanced when @xmath166 is increased ( via a change in the value of @xmath123 ) . * for fixed values of @xmath123 and @xmath165 , the error bounds for the poisson approximation are weakly dependent on @xmath1 .",
    "the explanation of these observations is , respectively , as follows :    * for fixed values of @xmath1 and @xmath123 , by increasing the value of the positive level @xmath165 , the probability that a standard gaussian random variable @xmath168 ( for @xmath8 ) exceeds the value @xmath165 is decreased .",
    "the law of small numbers indicates on the enhancement of the accuracy of the poisson approximation for @xmath22 in this case . * for fixed values of @xmath1 and @xmath165 ,",
    "the expected value of @xmath22 ( i.e. , @xmath169 in ) is kept fixed , and so is the upper bound on @xmath152 in . however , if the correlation @xmath166 in is decreased ( by a proper change in the value of @xmath123 ) then the value of @xmath170 in is increased , and the upper bound on @xmath167 ( see ) is decreased . since the upper bounds on @xmath152 and @xmath60 are not affected by a change in the value of @xmath123 and the upper bound on @xmath167 is decreased , then the upper bound on the total variation distance in theorem  [ theorem : upper bound on the total variation distance by arratia et al . ]",
    "is decreased as well .",
    "this also decreases the error bound that refers to the poisson approximation of the entropy in theorem  [ theorem : upper bound on the poisson approximation of the entropy ] .",
    "note that table  [ table : the gaussian problem in this paper ] compares the situation for @xmath171 , which corresponds respectively to @xmath172 ( these are the two extreme values of @xmath166 ) .",
    "* when @xmath1 and @xmath165 are fixed , the balance between the upper bounds on @xmath152 and @xmath167 changes significantly while changing the value of @xmath123 . to exemplify this numerically ,",
    "let @xmath173 and @xmath174 be the length of the sequence of gaussian random variables and the considered level , respectively .",
    "if @xmath175 , the upper bounds on @xmath152 and @xmath167 in and are , respectively , equal to @xmath176 and @xmath177 ( the loosened bound on @xmath167 is equal to @xmath178 ) . in this case",
    ", @xmath167 dominates @xmath152 and therefore an improvement in the value of @xmath167 ( or its upper bound ) also improves the error bound for the poisson approximation of the entropy @xmath130 in theorem  [ theorem : upper bound on the poisson approximation of the entropy ] .",
    "consider now the case where @xmath179 ( while @xmath180 are kept fixed ) ; this changes the lag-1 autocorrelation @xmath166 in from its maximal value @xmath181 to its minimal value @xmath182 . in this case , the upper bound on @xmath152 does not change , but the new bound on @xmath167 is decreased from @xmath178 to @xmath183 ( and the loosened bound on @xmath167 , for @xmath184 , is equal to @xmath185 ) . in the latter case ,",
    "the situation w.r.t .",
    "the balance between the coefficients @xmath152 and @xmath167 is reversed , i.e. , the bound on @xmath152 dominates the bound on @xmath167 .",
    "hence , the upper bound on the total variation distance and the error bound that follows from the poisson approximation of the entropy @xmath130 are reduced considerably when @xmath123 changes from @xmath186 to @xmath187 .",
    "this is because , from theorem  [ theorem : upper bound on the total variation distance by arratia et al . ] , the upper bound on the total variation distance depends linearly on the sum @xmath188 when @xmath159 ) .",
    "a similar conclusion also holds w.r.t . the error bound on the entropy ( see theorem  [ theorem : upper bound on the poisson approximation of the entropy ] ) . in light of this comparison ,",
    "the tightened bound on @xmath167 affects the error bound for the poisson approximation of @xmath130 when @xmath175 , in contrast to the case when @xmath179 . *",
    "the numerical results in table  [ table : the gaussian problem in this paper ] show that the accuracy of the poisson approximation is weakly dependent on the length @xmath1 of the sequence @xmath189 .",
    "this is attributed to the fact that the probabilities @xmath190 , for @xmath8 , are not affected by @xmath1 but they are only affected by choice of the level @xmath165 .",
    "hence , the law of small numbers does not necessarily indicate on an enhanced accuracy of the poisson approximation for @xmath130 when the length of the sequence @xmath1 is increased .",
    "[ example : the entropy of the number of times that a sequence of gaussian random variables exceeds a given level ]        the random variable @xmath61 is a sum of bernoulli random variables where @xmath191 , then @xmath22 gets values from the set @xmath192 , and @xmath76 gets non - negative integer values . theorem  [ theorem : l1 bound on the entropy ]",
    "therefore implies that @xmath193 and we need in the following to calculate proper constants @xmath194 and @xmath107 for the poisson approximation .",
    "the cardinality of the set of possible values of @xmath22 is @xmath195 , so it follows from that @xmath107 is given by .",
    "the parameter @xmath118 , which serves as an upper bound on the total variation distance @xmath196 , is given in due to the result in theorem  [ theorem : upper bound on the total variation distance by arratia et al . ] .",
    "the last thing that is now required is the calculation of @xmath197 .",
    "let @xmath198 designate the probability distribution of @xmath17 , so @xmath197 is an upper bound on @xmath199 , which is an infinite sum that only depends on the poisson distribution .",
    "straightforward calculation gives that @xmath200 from stirling s formula , for every @xmath201 , the equality @xmath202 holds for some @xmath203 .",
    "this therefore implies that the third infinite sum on the right - hand side of satisfies @xmath204 + \\frac{1}{12 } \\sum_{k = m}^{\\infty } \\frac{\\pi_{\\lambda}(k)}{k } \\nonumber \\\\ & & \\leq \\frac{\\log(2\\pi)}{2 } \\sum_{k = m}^{\\infty } \\pi_{\\lambda}(k ) + \\sum_{k = m}^{\\infty } \\bigl\\{k(k-1 ) \\ , \\pi_{\\lambda}(k ) \\bigr\\ } + \\frac{1}{12 } \\sum_{k = m}^{\\infty } \\pi_{\\lambda}(k ) \\nonumber \\\\ & & \\stackrel{\\text{(a)}}{= } \\frac{\\log(2\\pi)}{2 } \\sum_{k = m}^{\\infty } \\pi_{\\lambda}(k ) + \\lambda^2 \\sum_{k = m-2}^{\\infty } \\pi_{\\lambda}(k ) + \\frac{1}{12 } \\sum_{k = m}^{\\infty } \\pi_{\\lambda}(k ) \\nonumber \\\\ & & \\leq \\left ( \\frac{6\\log(2\\pi)+1}{12 } + \\lambda^2 \\right ) \\sum_{k = m-2}^{\\infty } \\pi_{\\lambda}(k ) \\label{eq : bound on the second sum for the derivation of the error bound on the entropy - 2nd step}\\end{aligned}\\ ] ] where the equality in  ( a ) follows from the identity @xmath205 for every @xmath206 . by combining and",
    ", it follows that @xmath207 \\sum_{k = m-2}^{\\infty } \\pi_{\\lambda}(k ) .",
    "\\label{eq : bound on the second sum for the derivation of the error bound on the entropy - 3rd step}\\end{aligned}\\ ] ] based on chernoff s bound , since @xmath17 , @xmath208 & & = { \\ensuremath{\\mathbb{p}}}(z \\geq m-2 ) \\nonumber \\\\[0.1 cm ] & & \\leq \\inf_{\\theta \\geq 0 } \\left\\{e^{-\\theta ( m-2 ) } \\ , { \\ensuremath{\\mathbb{e}}}\\bigl[e^{\\theta z}\\bigr ] \\right\\ } \\nonumber \\\\ & & = \\inf_{\\theta \\geq 0 } \\left\\{e^{-\\theta ( m-2 ) } \\ , e^{\\lambda ( e^{\\theta}-1 ) } \\right\\ } \\nonumber \\\\ & & = \\exp\\left\\ { -\\left [ \\lambda + ( m-2 ) \\log \\bigl(\\frac{m-2}{\\lambda e}\\bigr ) \\right ] \\right\\ } \\label{eq : bound on the second sum for the derivation of the error bound on the entropy - 4th step}\\end{aligned}\\ ] ] where the last equality follows by substituting the optimized value @xmath209 in the exponent ( note that @xmath210 , so optimized value of @xmath123 is indeed non - negative ) .",
    "hence , by combining and , it follows that @xmath211 where the parameter @xmath197 is introduced in .",
    "this completes the proof of theorem  [ theorem : upper bound on the poisson approximation of the entropy ] .      for proving the right - hand side of , which holds under the assumption that the bernoulli random variables @xmath62 are independent , one chooses ( similarly to remark  [ remark : generalization of theorem 1 of barbour and hall ] ) the set @xmath212 as the neighborhood of dependence for every @xmath47 .",
    "note that this choice of @xmath51 is taken because @xmath213 is independent of @xmath48 . from ",
    ", this choice gives that @xmath214 and @xmath215 which therefore implies the right - hand side of as a special case of theorem  [ theorem : upper bound on the poisson approximation of the entropy ] .",
    "furthermore , due to the maximum entropy result of the poisson distribution ( see theorem  [ theorem : maximum entropy result for the poisson distribution ] ) , then @xmath216 .",
    "this completes the proof of corollary  [ corollary : upper bound on the poisson approximation of the entropy for independent rvs ] .      under the assumption that the bernoulli random variables @xmath62 are independent , we rely here on two possible upper bounds on the total variation distance between the distributions of @xmath22 and @xmath17",
    "the first bound is the one in ( * ? ? ?",
    "* theorem  1 ) , used earlier in corollary  [ corollary : upper bound on the poisson approximation of the entropy for independent rvs ] .",
    "this bound gets the form @xmath217 where @xmath123 is introduced in .",
    "the second bound appears in ( * ? ? ?",
    "* eq .  ( 30 ) ) , and it improves the bound in ( * ? ? ?",
    "* eq .  ( 10 ) ) ( see also ( * ? ? ?",
    "* eq .  ( 4 ) ) ) .",
    "this bound gets the form @xmath218 it therefore follows that @xmath219 where @xmath118 is defined in to be the minimum of the upper bounds on the total variation distance in and .",
    "the continuation of the proof of this proposition is similar to the proof of corollary  [ corollary : upper bound on the poisson approximation of the entropy for independent rvs ] .",
    "we introduce in the following a generalization of the bounds in section  [ subsection : new error bounds on the entropy ] to consider the accuracy of the poisson approximation for the entropy of a sum of non - negative , integer - valued and bounded random variables .",
    "the generalized version of theorem  [ theorem : upper bound on the poisson approximation of the entropy ] is first introduced , and it is then justified by relying on the proof of this theorem for sums of bernoulli random variables with the approach of serfling in ( * ? ? ?",
    "* section  7 ) .",
    "this approach enables to derive an explicit upper bound on the total variation distance between a sum of non - negative and integer - valued random variables and a poisson distribution with the same mean .",
    "the requirement that the summands are bounded random variables is used to obtain an upper bound on the accuracy of the poisson approximation for the entropy of a sum of non - negative , integer - valued and bounded random variables .",
    "the following proposition forms a generalized version of theorem  [ theorem : upper bound on the poisson approximation of the entropy ] .",
    "let @xmath46 be an arbitrary finite index set , and let @xmath111 .",
    "let @xmath62 be non - negative , integer - valued random variables , and assume that there exists some @xmath220 such that @xmath221 a.s .",
    "for every @xmath47 .",
    "let @xmath222 where @xmath223 and @xmath224 .",
    "furthermore , for every @xmath47 , let @xmath225 be a bernoulli random variable that is equal to  1 if @xmath226 , and let it be equal otherwise to zero . referring to these bernoulli random variables ,",
    "let @xmath227 & & b'_2 \\triangleq \\sum_{\\alpha \\in i } \\sum_{\\alpha \\neq \\beta \\in b_{\\alpha } } p'_{\\alpha , \\beta } , \\quad p'_{\\alpha , \\beta } \\triangleq { \\ensuremath{\\mathbb{e}}}(x'_{\\alpha } x'_{\\beta } ) \\label{eq : b2_prime } \\\\[0.1 cm ] & & b'_3 \\triangleq \\sum_{\\alpha \\in i } s'_{\\alpha } , \\quad \\quad s'_{\\alpha } \\triangleq { \\ensuremath{\\mathbb{e}}}\\bigl| { \\ensuremath{\\mathbb{e}}}(x'_{\\alpha } - p_{\\alpha } \\",
    ", | \\ , \\sigma(\\{x_{\\beta}\\})_{\\beta \\in i \\setminus b_{\\alpha } } ) \\bigr| \\label{eq : b3_prime}\\end{aligned}\\ ] ] where , for every @xmath47 , the subset @xmath228 is determined arbitrarily such that it includes the element @xmath53 .",
    "furthermore , let @xmath229 & & m_a \\triangleq \\max\\left\\{na+2 , \\frac{1}{1-\\eta_a } \\right\\ } \\\\[0.2 cm ] & & \\mu_a \\triangleq \\left [ \\bigl(\\lambda \\log \\bigl(\\frac{e}{\\lambda}\\bigr)\\bigr)_+ \\ , + \\lambda^2 + \\frac{6 \\log(2\\pi ) + 1}{12 } \\right ] \\ , \\exp \\left\\{-\\left[\\lambda + ( m_a-2 ) \\log\\left(\\frac{m_a-2}{\\lambda e } \\right ) \\right ] \\right\\ } \\label{eq : function mu_prime in the upper bound on the poisson approximation of the entropy}\\end{aligned}\\ ] ] provided that @xmath230 .",
    "then , the difference between the entropies ( to base  @xmath30 ) of @xmath22 and @xmath17 satisfies @xmath231 [ proposition : generalized upper bound on the poisson approximation of the entropy ]    following the approach in ( * ? ? ?",
    "* section  7 ) , let @xmath232 be a bernoulli random variable that is equal to the indicator function of the event @xmath233 and @xmath234 for every @xmath47 .",
    "let @xmath235 be the sum of the induced bernoulli random variables . from the chen - stein method ( see theorem  [ theorem : upper bound on the total variation distance by arratia et al . ] )",
    "@xmath236 with the constants @xmath237 and @xmath238 as defined in .",
    "furthermore , from ( * ? ? ?",
    "* eq .  ( 7.2 ) ) , it follows that @xmath239 & & \\leq { \\ensuremath{\\mathbb{p}}}(w ' \\neq w ) \\nonumber \\\\[0.1 cm ] & & \\leq \\sum_{\\alpha \\in i } { \\ensuremath{\\mathbb{p}}}\\bigl(x'_{\\alpha } \\neq x_{\\alpha}\\bigr ) \\nonumber \\\\[0.1 cm ] & & = \\sum_{\\alpha \\in i } { \\ensuremath{\\mathbb{p}}}\\bigl(x_{\\alpha } \\geq 2 ) \\nonumber \\\\ & & = \\sum_{\\alpha \\in i } q_{\\alpha } = q. \\label{eq : total variation between w and w prime is less than or equal to q}\\end{aligned}\\ ] ] it therefore follows from , and that @xmath240 the rest of this proof follows closely the proof of theorem  [ theorem : upper bound on the poisson approximation of the entropy ] ( note that @xmath241 for @xmath242 , so @xmath22 gets @xmath243 possible values ) .",
    "this completes the proof of proposition  [ proposition : generalized upper bound on the poisson approximation of the entropy ] .",
    "this section forms the second part of this work . as in the previous section ,",
    "the presentation starts in section  [ subsection : second part of the revision of some known results ] with a brief review of some reported results that are relevant to the analysis in this section . improved lower bounds on the total variation distance between the distribution of the sum of independent bernoulli",
    "random variables and the poisson distribution with the same mean are introduced in section  [ subsection : improved lower bounds on the total variation distance ] .",
    "these improvements are obtained via the chen - stein method , by a non - trivial refinement of the analysis that was used for the derivation of the original lower bound by barbour and hall ( see ( * ? ? ?",
    "* theorem  2 ) ) .",
    "furthermore , the improved tightness of the new lower bounds and their connection to the original lower bound are further considered .",
    "section  [ subsection : improved lower bounds on the relative entropy ] introduces an improved lower bound on the relative entropy between the above two distributions .",
    "the analysis that is used for the derivation of the lower bound on the relative entropy is based on the lower bounds on the total variation distance in section  [ subsection : improved lower bounds on the total variation distance ] , combined with the use of the distribution - dependent refinement of pinsker s inequality by ordentlich and weinberger @xcite ( where the latter is specialized to the poisson distribution ) .",
    "the lower bound on the relative entropy is compared to some previously reported upper bounds on the relative entropy by kontoyiannis et al .",
    "@xcite in the context of the poisson approximation .",
    "upper and lower bounds on the bhattacharyya parameter , chernoff information and hellinger distance between the distribution of the sum of independent bernoulli random variables and the poisson distribution are next derived in section  [ subsection : bounds on related quantities ] .",
    "the discussion proceeds in section  [ subsection : second part of applications of the new bounds ] by exemplifying the use of some of the new bounds that are derived in this section in the context of the classical binary hypothesis testing .",
    "finally , section  [ subsection : proofs of the results in the second part of this paper ] proves the new results that are introduced in sections  [ subsection : improved lower bounds on the relative entropy ] and [ subsection : bounds on related quantities ] .",
    "it is emphasized that , in contrast to the setting in section  [ section : error bounds on the entropy of the sum of bernoulli random variables ] where the bernoulli random variables may be dependent summands , the analysis in this section depends on the assumption that the bernoulli random variables are independent .",
    "this difference stems from the derivation of the improved lower bound on the total variation distance in section  [ subsection : improved lower bounds on the total variation distance ] , which forms the starting point for the derivation of all the subsequent results that are introduced in this section , assuming an independence of the summands .",
    "the following definitions of probability metrics are particularized and simplified to the case of our interest where the probability mass functions are defined on @xmath24 .",
    "let @xmath31 and @xmath32 be two probability mass functions that are defined on a same countable set @xmath33 .",
    "the hellinger distance and the bhattacharyya parameter between @xmath31 and @xmath32 are , respectively , given by @xmath244 & & \\text{bc}(p , q ) \\triangleq",
    "\\sum_{x \\in \\mathcal{x } } \\sqrt{p(x ) \\ , q(x ) } \\label{eq : bhattacharyya parameter}\\end{aligned}\\ ] ] so , these two probability metrics ( including the total variation distance in definition  [ definition : total variation distance ] ) are bounded between  0 and  1 .",
    "[ definition : probability metrics ]    in general , these probability metrics are defined in the setting where @xmath245 is a separable metric space .",
    "the interest in this work is in the specific case where @xmath246 and @xmath247 . in this case , the expressions of these probability metrics are simplified as above . for further study of probability metrics and their properties ,",
    "the interested reader is referred to , e.g. , ( * ? ? ?",
    "* appendix  a.1 ) , ( * ? ? ?",
    "* chapter  2 ) and ( * ? ? ?",
    "* section  3.3 ) .",
    "the hellinger distance is related to the bhattacharyya parameter via the equality @xmath248 [ remark : relation between the hellinger distance and bhattacharyya parameter ]    the chernoff information and relative entropy ( a.k.a .",
    "divergence or kullback - leibler distance ) between two probability mass functions @xmath31 and @xmath32 that are defined on a countable set @xmath33 are , respectively , given by @xmath249 } \\log \\left ( \\sum_{x \\in \\mathcal{x } } p^{\\theta}(x ) q^{1-\\theta}(x ) \\right ) \\label{eq : chernoff information } \\\\[0.1 cm ] & & d(p||q ) \\triangleq \\sum_{x \\in \\mathcal{x } } p(x ) \\log\\left(\\frac{p(x)}{q(x)}\\right ) \\label{eq : relative entropy}\\end{aligned}\\ ] ] so @xmath250 $ ] . throughout this paper ,",
    "the logarithms are on base  @xmath30 .",
    "[ definition : chernoff information and relative entropy ]    for two probability mass functions @xmath31 and @xmath32 that are defined on the same set @xmath33 @xmath251 [ proposition : known inequality that relates between the total variation , hellinger distance and relative entropy ]    the left - hand side of is proved in @xcite , and the right - hand side is proved in @xcite .    it is noted that the hellinger distance in the middle of is not multiplied by the square - root of  2 in @xcite , due to a small difference in the definition of this distance where the factor of one - half on the right - hand side of does not appear in the definition of the hellinger distance in @xcite .",
    "however , this is just a matter of normalization of this distance ( as otherwise , according to @xcite , the hellinger distance varies between 0 and @xmath252 instead of the interval @xmath253 $ ] ) .",
    "the definition of this distance in is consistent , e.g. , with @xcite .",
    "it makes the range of this distance to be between 0 and  1 , similarly to the total variation , local and kolmogorov - smirnov distances and also the bhattacharyya parameter that are considered in this paper .",
    "the chernoff information , @xmath254 , is the best achievable exponent in the bayesian probability of error for binary hypothesis testing ( see , e.g. , ( * ? ? ?",
    "* theorem  11.9.1 ) ) .",
    "furthermore , if @xmath255 are i.i.d .",
    "random variables , having distribution @xmath31 with prior probability @xmath256 and distribution @xmath32 with prior probability @xmath257 , the following upper bound holds for the best achievable overall probability of error : @xmath258      : pinsker s inequality provides a lower bound on the relative entropy in terms of the total variation distance between two probability measures that are defined on the same set .",
    "it states that @xmath259 in @xcite , a distribution - dependent refinement of pinsker s inequality was introduced for an arbitrary pair of probability distributions @xmath31 and @xmath32 that are defined on @xmath24 .",
    "it is of the form @xmath260 where @xmath261 and @xmath262 so @xmath263 is monotonic decreasing in the interval @xmath264 $ ] , @xmath265 where the latter limit implies that @xmath263 is left - continuous at one - half .",
    "note that it follows from that @xmath266 $ ] .    in section  [ subsection",
    ": improved lower bounds on the relative entropy ] , we rely on this refinement of pinsker s inequality and combine it with the new lower bound on the total variation distance between the distribution of a sum of independent bernoulli random variables and the poisson distribution with the same mean that is introduced in section  [ subsection : improved lower bounds on the total variation distance ] .",
    "the combination of these two bounds provides a new lower bound on the relative entropy between these two distributions .      in theorem  [",
    "theorem : bounds on the total variation distance - barbour and hall 1984 ] , we introduced the upper and lower bounds on the total variation distance in ( * ? ? ?",
    "* theorem  1 and  2 ) ( see also ( * ? ? ?",
    "* theorem  2.m and corollary  3.d.1 ) ) .",
    "this shows that these upper and lower bounds are essentially tight , where the lower bound is about @xmath15 of the upper bound .",
    "furthermore , it was claimed in ( * ? ? ?",
    "* remark  3.2.2 ) ( with no explicit proof ) that the constant @xmath15 in the lower bound on the left - hand side of can be improved to @xmath16 . in this section , we obtain further improvements of this lower bound where , e.g. , the ratio of the upper and new lower bounds on the total variation distance tends to 1.69 in the limit where @xmath267 , and this ratio tends to 10.54 in the limit where @xmath125 .",
    "as will be demonstrated in the continuation of section  [ section : improved lower bounds on the total variation distance etc .",
    "] , the effect of these improvements is enhanced considerably when considering improved lower bounds on the relative entropy and some other related information - theoretic measures .",
    "we further study later in this section the implications of the improvement in lower bounding the total variation distance , originating in this sub - section , and exemplify these improvements in the context of information theory and statistics .",
    "similarly to the proof of ( * ? ? ?",
    "* theorem  2 ) , the derivation of the improved lower bound is also based on the chen - stein method , but it follows from a significant modification of the analysis that served to derive the original lower bound in ( * ? ? ?",
    "* theorem  2 ) .",
    "the following upper bound on the total variation distance is taken ( as is ) from ( * ? ? ?",
    "* theorem  1 ) ( this bound also appears in theorem  [ theorem : bounds on the total variation distance - barbour and hall 1984 ] here ) . the motivation for improving the lower bound on the total variation distance is to take advantage of it to improve the lower bound on the relative entropy ( via pinsker s inequality or a refinement of it ) and some other related quantities , and then to examine the benefit of this improvement in an information - theoretic context .",
    "let @xmath38 be a sum of @xmath1 independent bernoulli random variables with @xmath39 for @xmath8 , and @xmath40 .",
    "then , the total variation distance between the probability distribution of @xmath22 and the poisson distribution with mean @xmath13 satisfies @xmath268 where @xmath269 is given by @xmath270 and @xmath271 & & \\hspace*{-0.5 cm } x_+ \\triangleq \\max\\{x , 0\\ } , \\quad x_+^2 \\triangleq \\bigl(x_+)^2 , \\quad \\forall \\ , x \\in { \\ensuremath{\\mathbb{r}}}\\\\[0.3 cm ] & & \\hspace*{-0.5 cm } g_{\\lambda}(\\alpha_1 , \\alpha_2 , \\theta ) \\triangleq \\max \\left\\ { \\ , \\left| \\left(1 + \\sqrt{\\frac{2}{\\theta \\lambda e } } \\cdot    & & \\hspace*{3.2 cm } \\left .",
    "\\left| \\left(2 e^{-\\frac{3}{2 } } + \\sqrt{\\frac{2}{\\theta \\lambda e } }   \\cdot    \\right\\ } \\label{eq : g in the lower bound on the total variation distance } \\\\[0.3 cm ] & & \\hspace*{-0.5 cm } x(u ) \\triangleq ( c_0 + c_1 u + c_2 u^2 ) \\ , \\exp(-u^2 ) , \\quad \\forall \\ , u \\in { \\ensuremath{\\mathbb{r}}}\\label{eq : function x } \\\\[0.2 cm ] & & \\hspace*{-0.5 cm } \\{u_i\\ } \\triangleq \\bigl\\ { u \\in { \\ensuremath{\\mathbb{r } } } : \\ , 2 c_2 u^3 + 2 c_1 u^2 - 2(c_2 - c_0 ) u - c_1 = 0\\bigr\\ } \\label{eq : zeros of a cubic polynomial equation } \\\\[0.1 cm ] & & \\hspace*{-0.5 cm } c_0 \\triangleq ( \\alpha_2 - \\alpha_1 ) ( \\lambda - \\alpha_2 ) \\label{eq : c0 } \\\\[0.1 cm ] & & \\hspace*{-0.5 cm } c_1",
    "\\triangleq \\sqrt{\\theta \\lambda } \\ , ( \\lambda + \\alpha_1 - 2 \\alpha_2 ) \\label{eq : c1 } \\\\[0.1 cm ] & & \\hspace*{-0.5 cm } c_2 \\triangleq -\\theta \\lambda .   \\label{eq : c2}\\end{aligned}\\ ] ] [ theorem : improved lower bound on the total variation distance ]    the upper and lower bounds on the total variation distance in scale like @xmath272 , similarly to the known bounds in theorem  [ theorem : bounds on the total variation distance - barbour and hall 1984 ] . the ratio of the upper and lower bounds in theorem  [ theorem : bounds on the total variation distance - barbour and hall 1984 ] tends to  32.00 when either @xmath13 tends to zero or infinity .",
    "it was obtained numerically that the ratio of the upper and lower bounds in theorem  [ theorem : improved lower bound on the total variation distance ] improves by a factor of 18.96 when @xmath267 , a factor of 3.04 when @xmath125 , and at least by a factor of 2.48 for all @xmath18 .",
    "alternatively , since the upper bound on the total variation distance in theorems  [ theorem : bounds on the total variation distance - barbour and hall 1984 ] and  [ theorem : improved lower bound on the total variation distance ] is common , it follows that the ratio of the upper bound and new lower bound on the total variation distance is reduced to 1.69 when @xmath267 , it is 10.54 when @xmath125 , and it is at most 12.91 for all @xmath18 .",
    "[ remark : improvement in the tightness of the new lower bound on the total variation distance ]    ( * ? ? ?",
    "* theorem  1.2 ) provides an asymptotic result for the total variation distance between the distribution of the sum @xmath22 of @xmath1 independent bernoulli random variables with @xmath39 and the poisson distribution with mean @xmath5 .",
    "it shows that when @xmath273 and @xmath274 as @xmath275 then @xmath276 this implies that the ratio of the upper bound on the total variation distance in ( * ? ? ?",
    "* theorem  1 ) ( see theorems  [ theorem : bounds on the total variation distance - barbour and hall 1984 ] here ) and this asymptotic expression is equal to @xmath277 .",
    "therefore , in light of the previous remark ( see remark  [ remark : improvement in the tightness of the new lower bound on the total variation distance ] ) , it follows that the ratio between the exact asymptotic value in and the new lower bound in is equal to @xmath278 .",
    "it therefore follows from remark  [ remark : improvement in the tightness of the new lower bound on the total variation distance ] that in the limit where @xmath267 , the new lower bound on the total variation in is smaller than the exact value by no more than 1.69 , and for @xmath134 , it is smaller than the exact asymptotic result by a factor of 2.55 .",
    "[ remark : more on the tightness of the new improved lower bound on the total variation distance ]    since @xmath279 in are zeros of a cubic polynomial equation with real coefficients , then the size of the set @xmath279 is either 1 or 3 .",
    "but since one of the values of @xmath280 is a point where the global maximum of @xmath281 is attained , and another value of @xmath280 is the point where its global minimum is attained ( note that @xmath282 and @xmath281 is differentiable , so the global maxima and minima of @xmath281 are attained at finite values where the derivative of @xmath281 is equal to zero ) , then the size of the set @xmath279 can not be  1 , which implies that it should be equal to  3 .",
    "[ remark : the size of the set of real zeros is equal to 3 ]    the optimization that is required for the computation of @xmath269 in w.r.t .",
    "the three parameters @xmath283 and @xmath284 is performed numerically .",
    "the numerical procedure for the computation of @xmath269 will be discussed later ( after introducing the following corollary ) .    in the following ,",
    "we introduce a closed - form lower bound on the total variation distance that is looser than the lower bound in theorem  [ theorem : improved lower bound on the total variation distance ] , but which already improves the lower bound in ( * ? ? ?",
    "* theorem  2 ) .",
    "this lower bound follows from theorem  [ theorem : improved lower bound on the total variation distance ] by the special choice of @xmath285 that is included in the optimization set for @xmath269 on the right - hand side of .",
    "following this sub - optimal choice , the lower bound in the next corollary follows by a derivation of a closed - form expression for the third free parameter @xmath284 .",
    "in fact , this was our first step towards the derivation of an improved lower bound on the total variation distance .",
    "after introducing the following corollary , we discuss it shortly , and suggest an optimization procedure for the computing @xmath269 on the left - hand side of .    under the assumptions in theorem  [",
    "theorem : improved lower bound on the total variation distance ] , then @xmath286 where    @xmath287 & & \\theta \\triangleq 3 + \\frac{7}{\\lambda } + \\frac{1}{\\lambda } \\cdot \\sqrt{(3\\lambda+7)\\bigl[(3 + 2e^{-1/2 } ) \\lambda + 7\\bigr]}. \\label{eq : optimal theta for alpha1 and alpha2 equal to lambda}\\end{aligned}\\ ] ] furthermore , the ratio of the upper and lower bounds on the total variation distance in tends to @xmath288 as @xmath267 , it tends to 10.539 as @xmath125 , and this ratio is monotonic decreasing as a function of @xmath18 ( see the upper plot in figure  [ figure : ratio ot upper and lower bounds on the total variation distance ] , and the calculation of the two limits in section  [ subsubsection : connection of the corollary with the improved lower bound on the total variation distance to the original lower bound of barbour and hall ] ) . [",
    "corollary : lower bound on the total variation distance ]    the lower bound on the total variation distance on the left - hand side of improves uniformly the lower bound in ( * ? ? ?",
    "* theorem  2 ) ( i.e. , the left - hand side of eq .   here ) .",
    "the improvement is by factors of 1.55 and 3.03 for @xmath267 and @xmath125 , respectively .",
    "note that this improvement is already remarkable since the ratio of the upper and lower bounds in ( * ? ? ?",
    "* theorems  1 and 2 ) ( theorem  [ theorem : bounds on the total variation distance - barbour and hall 1984 ] here ) is equal to 32 in these two extreme cases , and it is also uniformly upper bounded by  32 for all values of @xmath18 .",
    "furthermore , in light of remark  [ remark : improvement in the tightness of the new lower bound on the total variation distance ] , the improvement of the lower bound on the total variation distance in theorem  [ theorem : improved lower bound on the total variation distance ] over its loosened version in corollary  [ corollary : lower bound on the total variation distance ] is especially significant for small values of @xmath13 , but it is marginal for large values of @xmath13 ; this improvement is by a factor of 11.88 in the limit where @xmath267 , but asymptotically there is no improvement if @xmath125 where it even holds for @xmath75 ( see figure  [ figure : ratio ot upper and lower bounds on the total variation distance ] where all the curves in this plot merge approximately for @xmath75 ) .",
    "note , however , that even if @xmath125 , the lower bounds in theorem  [ theorem : improved lower bound on the total variation distance ] and corollary  [ corollary : lower bound on the total variation distance ] improve the original lower bound in theorem  [ theorem : bounds on the total variation distance - barbour and hall 1984 ] by a factor that is slightly above  3 .",
    "[ remark : a comparison between the improved , simplified and original lower bounds on the total variation distance ]    in light of corollary  [ corollary : lower bound on the total variation distance ] , a simplified algorithm is suggested in the following for the computation of @xmath269 in . in general ,",
    "what we compute numerically is a lower bound on @xmath269 ; but this is fine since @xmath269 is the coefficient of the lower bound on the left - hand side of , so its replacement by a lower bound still gives a valid lower bound on the total variation distance .",
    "the advantage of the suggested algorithm is its reduced complexity , as compared to a brute force search over the infinite three - dimensional region for @xmath289 ; the numerical computation that is involved with this algorithm takes less than a second on a standard pc .",
    "the algorithm proceeds as follows :    * it chooses the initial values @xmath285 , and @xmath123 as is determined on the right - hand side of .",
    "the corresponding lower bound on the total variation distance from theorem  [ theorem : improved lower bound on the total variation distance ] , for this sub - optimal selection of the three free parameters @xmath290 , is equal to the closed - form lower bound in corollary  [ corollary : lower bound on the total variation distance ] .",
    "* at this point , the algorithm performs several iterations where at each iteration , it defines a certain three - dimensional grid around the optimized point from the previous iteration ( the zeroth iteration refers to the initial choice of parameters from the previous item , and to the closed - form lower bound in corollary  [ corollary : lower bound on the total variation distance ] ) . at each iteration ,",
    "the algorithm searches for the optimized point on the new grid ( i.e. , it computes the maximum of the expression inside the supremum on the right - hand side of among all the points of the grid , and it also updates the new location of this point @xmath289 for the search that is made in the next iteration . note",
    "that , from , the grid should exclude points @xmath289 when either @xmath291 or @xmath292 .",
    "* at the beginning of this recursive procedure , the algorithm take a very large neighborhood around the point that was selected at the previous iteration ( or the initial selection of the point from the first item ) .",
    "the size of this neighborhood at each subsequent iteration shrinks , but the grid also becomes more dense around the new selected point from the previous iteration .    it is noted that numerically , the resulting lower bound on @xmath269 seems to be the exact value in and not just a lower bound",
    ". however , the reduction in the computational complexity of ( a lower bound on ) @xmath269 provides a very fast algorithm .",
    "the conclusions of the last two remarks ( i.e. , remarks  [ remark : a comparison between the improved , simplified and original lower bounds on the total variation distance ] and  [ remark : more on the improved lower bounds on the total variation distance ] are supported by figure  [ figure : ratio ot upper and lower bounds on the total variation distance ] . [",
    "remark : more on the improved lower bounds on the total variation distance ]      the following theorem relies on the new lower bound on the total variation distance in theorem  [ theorem : improved lower bound on the total variation distance ] , and the distribution - dependent refinement of pinsker s inequality in @xcite .",
    "their combination serves to derive a new lower bound on the relative entropy between the distribution of a sum of independent bernoulli random variables and a poisson distribution with the same mean .",
    "the following upper bound on the relative entropy was introduced in ( * ? ? ?",
    "* theorem  1 ) . together with the new lower bound on the relative entropy , it leads to the following statement :    in the setting of theorem  [ theorem : improved lower bound on the total variation distance ] , the relative entropy between the probability distribution of @xmath22 and the poisson distribution with mean @xmath293 satisfies the following inequality : @xmath294 where @xmath295 with @xmath269 from , and @xmath296 2 \\quad & \\mbox{if $ \\lambda \\geq \\log 2$. } \\end{array } \\right .",
    "\\label{eq : the refinement of pinsker 's inequality for the poisson distribution}\\ ] ] [ theorem : improved lower bound on the relative entropy ]    for the sake of simplicity , in order to have a bound in closed - form ( that is not subject to numerical optimization ) , the lower bound on the relative entropy on the left - hand side of can be loosened by replacing @xmath297 on the right - hand side of with @xmath298 in and . in light of remark  [ remark : a comparison between the improved , simplified and original lower bounds on the total variation distance ] , this possible loosening of the lower bound on the relative entropy has no effect if @xmath299 .",
    "[ remark : a possible loosening of the lower bound on the relative entropy ]    the distribution - dependent refinement of pinsker s inequality from @xcite yields that , when applied to a poisson distribution with mean @xmath13 , the coefficient @xmath300 in is larger than @xmath301 for @xmath302 , and it is approximately equal to @xmath303 for @xmath304 .",
    "hence , for @xmath304 , the refinement of pinsker s inequality in @xcite leads to a remarkable improvement in the lower bound that appears in  , which is by approximately a factor of @xmath305 . if , however , @xmath306 then there is no refinement of pinsker s inequality ( since @xmath307 in ) .",
    "[ remark : the effect of the refinement of pinsker s inequality ]    the combination of the original lower bound on the total variation distance from ( * ? ? ? * theorem  2 ) ( see ) with pinsker s inequality ( see ) gives the following lower bound on the relative entropy : @xmath308 in light of remarks  [ remark : improvement in the tightness of the new lower bound on the total variation distance ] and  [ remark : the effect of the refinement of pinsker s inequality ] , it is possible to quantify the improvement that is obtained by the new lower bound of theorem  [ theorem : improved lower bound on the relative entropy ] in comparison to the looser lower bound in .",
    "the improvement of the new lower bound on the relative entropy is by a factor of @xmath309 for @xmath304 , a factor of 9.22 for @xmath125 , and at least by a factor of 6.14 for all @xmath18 .",
    "the conclusions in the last two remarks ( i.e. , remark  [ remark : the effect of the refinement of pinsker s inequality ] and  [ remark : comparison to the lower bound on the relative entropy that follows from the original lower bound on the total variation distance and pinsker s inequality ] ) are supported by figure  [ figure : exact_and_bounds_for_the_relative_entropy_between_binomial_and_poisson_distributions ] that refers to the special case of the relative entropy between the binomial and poisson distributions . [",
    "remark : comparison to the lower bound on the relative entropy that follows from the original lower bound on the total variation distance and pinsker s inequality ]    in ( * ? ? ?",
    "* example  6 ) , it is shown that if @xmath310 then @xmath311 since @xmath312 then this lower bound on the relative entropy is not informative for the relative entropy @xmath313 .",
    "theorem  [ theorem : improved lower bound on the relative entropy ] and the loosened bound in are , however , informative in the studied case .    the author was notified in @xcite about the existence of another recently derived lower bound on the relative entropy @xmath314 in terms of the variance of a random variable @xmath82 with values in @xmath24 ( this lower bound appears in a currently un - published work ) .",
    "the two bounds were derived independently , based on different approaches . in the setting where @xmath315 is a sum of independent bernoulli random variables @xmath2 with @xmath39 and @xmath316 , the two lower bounds on the relative entropy scale like @xmath317 but with a different scaling factor .",
    "the following proposition introduces a sharpened version of proposition  [ proposition : known inequality that relates between the total variation , hellinger distance and relative entropy ] .",
    "let @xmath31 and @xmath32 be two probability mass functions that are defined on a same set @xmath33 .",
    "then , the following inequality suggests a sharpened version of the inequality in @xmath318 and @xmath319 [ proposition : sharpened inequality that relates between the total variation , hellinger distance , bhattacharyya parameter and relative entropy ]    a comparison of the upper and lower bounds on the hellinger distance in or the bhattacharyya parameter in gives the following lower bound on the relative entropy in terms of the total variation distance : @xmath320 it is noted that also follows from the combination of the last two inequalities in @xcite .",
    "it is tighter than pinsker s inequality ( see if @xmath321 , having also the advantage of giving the right bound for the relative entropy @xmath322 when the total variation distance approaches to  1 .",
    "however , is a slightly looser bound on the relative entropy in comparison to vajda s lower bound @xcite that reads : @xmath323    under the assumptions in theorem  [ theorem : improved lower bound on the total variation distance ] , the hellinger distance and bhattacharyya parameter satisfy the following upper and lower bounds : @xmath324 and @xmath325 where @xmath269 on the left - hand side of and the right - hand side of is introduced in .",
    "[ corollary : upper and lower bounds on the hellinger distance and bhattacharyya parameter in the context of poisson approximation ]    let @xmath326 be a sequence of random variables where @xmath327 is a sum of @xmath1 independent bernoulli random variables @xmath328 with @xmath329 ( note that , for @xmath330 , the binary random variables @xmath331 and @xmath332 may be dependent ) .",
    "assume that @xmath333 for some @xmath18 and every @xmath70 , and that there exist some fixed constants @xmath334 such that @xmath335 ( which implies that @xmath336 and @xmath337 , and @xmath338 if and only if the binary random variables @xmath328 are i.i.d . ) .",
    "then , the following asymptotic results hold : @xmath339 & & d_{\\text{tv}}\\bigl(p_{s_n } , \\text{po}(\\lambda ) \\bigr ) = o\\bigl(\\frac{1}{n}\\bigr ) \\label{eq : asymptotic scaling of the total variation distance } \\\\[0.1 cm ] & & d_{\\text{h}}\\bigl(p_{s_n } , \\text{po}(\\lambda ) \\bigr ) = o\\bigl(\\frac{1}{n}\\bigr ) \\label{eq : asymptotic scaling of the hellinger distance } \\\\[0.1 cm ] & & \\text{bc}\\bigl(p_{s_n } , \\text{po}(\\lambda ) \\bigr ) = 1 - o\\bigl(\\frac{1}{n^2}\\bigr ) \\label{eq : asymptotic scaling of the bhattacharyya parameter}\\end{aligned}\\ ] ] so , the relative entropy between the distribution of @xmath9 and the poisson distribution with mean @xmath13 scales like @xmath340 , the total variation and hellinger distances scale like @xmath341 , and the gap of the bhattacharyya parameter to  1 scales like  @xmath340 .",
    "[ corollary : asymptotic results of the relative entropy and related quantities for independent bernoulli sums ]      let @xmath31 and @xmath32 be two probability mass functions that are defined on a same set @xmath33 . then , the chernoff information between @xmath31 and @xmath32 is lower bounded in terms of the total variation distance as follows : @xmath342 [ proposition : lower bound on the chernoff information in terms of the total variation distance ]    under the assumptions in theorem  [ theorem : improved lower bound on the total variation distance ] , the chernoff information satisfies the following lower bound : @xmath343 where @xmath269 is introduced in . [ corollary : improved lower bound on the chernoff information between bernoulli sums of independent rvs and poisson distribution ]    remark  [ remark : a possible loosening of the lower bound on the relative entropy ] also applies to corollaries  [ corollary : upper and lower bounds on the hellinger distance and bhattacharyya parameter in the context of poisson approximation ] and  [ corollary : improved lower bound on the chernoff information between bernoulli sums of independent rvs and poisson distribution ] . [ remark : a possible loosening of the lower bounds on the related quantities ]    the combination of proposition  [ proposition : lower bound on the chernoff information in terms of the total variation distance ] with the lower bound on the total variation distance in ( * ? ? ? * theorem  2 ) ( see theorem  [ theorem : bounds on the total variation distance - barbour and hall 1984 ] here ) gives the following looser lower bound on the chernoff information : @xmath344 the impact of the tightened lower bound in , as compared to the bound in is exemplified in section  [ subsection : second part of applications of the new bounds ] in the context of the bayesian approach for binary hypothesis testing . [ remark : looser lower bound on the chernoff information between bernoulli sums of independent rvs and poisson distribution ]      in the following , we consider the use of the new bounds in section  [ section : improved lower bounds on the total variation distance etc . ] for binary hypothesis testing .",
    "the chernoff - stein lemma considers the asymptotic error exponent in binary hypothesis testing when one of the probabilities of error is held fixed , and the other one has to be made as small as possible ( see , e.g. , ( * ? ? ?",
    "* theorem  11.8.3 ) ) .",
    "let @xmath345 be a sequence of non - negative , integer - valued i.i.d .",
    "random variables with @xmath346 for some @xmath18 .",
    "let @xmath347 where we consider the following two hypothesis :    * @xmath348 : @xmath349 where @xmath350 , for @xmath351 , is a sum of @xmath1 binary random variables @xmath352 with @xmath353 and @xmath354 .",
    "it is assumed that the elements of the sequence @xmath355 are independent , and @xmath70 is fixed . *",
    "@xmath356 : @xmath357 is the poisson distribution with mean @xmath13 ( i.e. , @xmath358 ) .    note that in this case , if one of the @xmath350 exceeds the value @xmath1 then @xmath348 is rejected automatically , so one may assume that @xmath359 . more explicitly , if @xmath360 for @xmath351 , the probability of this event to happen is upper bounded ( via the union and chernoff bounds ) by @xmath361\\right\\ } \\label{eq : the probability for an element of the sequence y of poisson random variables to exceed the value n}\\ ] ] so , if @xmath362 , this probability is typically very small .    for an arbitrary @xmath363 ,",
    "let @xmath364 be an acceptance region for hypothesis  1 .",
    "using standard notation , let @xmath365 be the two types of error probabilities .",
    "following ( * ? ? ?",
    "* theorem  11.8.3 ) , for an arbitrary @xmath366 , let @xmath367 where @xmath368 is the alphabet that is associated with hypothesis @xmath348 .",
    "then , the best asymptotic exponent of @xmath369 in the limit where @xmath370 is @xmath371 from ( * ? ? ?",
    "( 11.206 ) , ( 11.207 ) and ( 11.227 ) ) , for the relative entropy typical set that is defined by @xmath372 then , it follows from the aep for relative entropy that @xmath373 for @xmath374 large enough ( see , e.g. , ( * ? ? ?",
    "* theorem  11.8.1 ) ) .",
    "furthermore , for every @xmath374 ( see , e.g , ( * ? ? ?",
    "* theorem  11.8.2 ) ) , @xmath375 the error probability of the second type @xmath376 is treated here separately from @xmath377 . in this case , a lower bound on the relative entropy @xmath378 gives an exponential upper bound on @xmath376 .",
    "let @xmath370 ( more explicitly , let @xmath379 be chosen to be small enough as compared to a lower bound on @xmath378 ) . in the following two simple examples , we calculate the improved lower bound in theorem  [ theorem : improved lower bound on the relative entropy ] , and compare it to the lower bound in .",
    "more importantly , we study the impact of theorem  [ theorem : improved lower bound on the relative entropy ] on the reduction of the number of samples @xmath374 that are required for achieving @xmath380 .",
    "the following two cases are used to exemplify this issue :    1 .",
    "let the probabilities @xmath381 ( that correspond to hypothesis  1 ) be given by @xmath382 for @xmath18 , in order to satisfy the equality @xmath354 then @xmath383 , and @xmath384 from theorem  [ theorem : improved lower bound on the relative entropy ] , the improved lower bound on the relative entropy reads @xmath385 where @xmath386 is introduced in , and the weaker lower bound in gets the form @xmath387 lets examine the two bounds on the relative entropy for @xmath388 and @xmath389 to find accordingly a proper value of @xmath374 such that @xmath390 , and choose @xmath391 .",
    "note that the probability of the event that one of the @xmath374 poisson random variables @xmath345 , under hypothesis @xmath356 , exceeds the value @xmath1 is upper bounded in by @xmath392 , so it is neglected for all reasonable amounts of samples @xmath374 . in this setting , the two lower bounds on the relative entropy in and , respectively , are equal to @xmath393 and @xmath394 nats . for these two lower bounds , the exponential upper bound in ensures that @xmath390 for @xmath395 and @xmath396 , respectively .",
    "hence , the improved lower bound on the relative entropy in theorem  [ theorem : improved lower bound on the relative entropy ] implies here a reduction in the required number of samples by a factor of 7.17 . 2 .",
    "in the second case , assume that the probabilities @xmath381 scale exponentially in @xmath397 ( instead of the linear scaling in the previous case ) .",
    "let @xmath398 and consider the case where @xmath399 for @xmath18 , in order to hold the equality @xmath354 then @xmath400 , and @xmath401 .",
    "hence , the improved lower bound in theorem  [ theorem : improved lower bound on the relative entropy ] and the other bound in imply respectively that @xmath402 and @xmath403 the choice @xmath404 , @xmath405 and @xmath389 , implies that the two lower bounds on the relative entropy in and are respectively equal to @xmath406 and @xmath407 .",
    "the exponential upper bound in therefore ensures that @xmath390 for @xmath408 and @xmath409 , respectively .",
    "hence , the improvement in theorem  [ theorem : improved lower bound on the relative entropy ] leads in this case to the conclusion that one can achieve the target error probability of the second type while reducing the number of samples @xmath345 by a factor of 155 .",
    "[ example : application of the chernoff - stein lemma and the new lower bound on the relative entropy ]    we turn to consider binary hypothesis testing with the bayesian approach ( see , e.g. , ( * ? ? ?",
    "* section  11.9 ) ) . in this",
    "setting , one wishes to minimize the overall probability of error while we refer to the two hypotheses in example  [ example : application of the chernoff - stein lemma and the new lower bound on the relative entropy ] .",
    "the best asymptotic exponent in the bayesian approach is the chernoff information ( see ) , and the overall error probability satisfies the following exponential upper bound : @xmath410 so , a lower bound on the chernoff information provides an upper bound on the overall error probability . in the following",
    ", the two lower bounds on the chernoff information in and , and the advantage of the former lower bound is studied in the two cases of example  [ example : application of the chernoff - stein lemma and the new lower bound on the relative entropy ] in order to examine the impact of its improved tightness on the reduction of the number of samples @xmath374 that are required to achieve an overall error probability below @xmath391 .",
    "we refer , respectively , to cases  1 and  2 of example  [ example : application of the chernoff - stein lemma and the new lower bound on the relative entropy ] .    1 .   in case  1 of example  [",
    "example : application of the chernoff - stein lemma and the new lower bound on the relative entropy ] , the two lower bounds on the chernoff information in corollary  [ corollary : improved lower bound on the chernoff information between bernoulli sums of independent rvs and poisson distribution ] and remark  [ remark : looser lower bound on the chernoff information between bernoulli sums of independent rvs and poisson distribution ] ( following the calculation of @xmath272 for these two cases ) are @xmath411 -\\frac{1}{2 } \\ , \\log \\left(1-\\frac{\\lambda^4}{2304 } \\ , \\min\\bigl\\{1 , \\frac{1}{\\lambda^2}\\bigr\\ } \\left(\\frac{2n+1}{n(n+1)}\\right)^2",
    "\\right ) & \\quad \\mbox{from eq.~\\eqref{eq : looser lower bound on the chernoff information between bernoulli sums of independent rvs and poisson distribution } ( remark~\\ref{remark : looser lower bound on the chernoff information between bernoulli sums of independent rvs and poisson distribution } ) . }",
    "\\end{array } \\right.\\ ] ] as in the first case of example  [ example : application of the chernoff - stein lemma and the new lower bound on the relative entropy ] , let @xmath388 and @xmath412",
    ". the lower bounds on the chernoff information are therefore equal to @xmath413 8.59 \\cdot 10^{-6 } & \\quad \\mbox{from eq.~\\eqref{eq : looser lower bound on the chernoff information between bernoulli sums of independent rvs and poisson distribution}. } \\end{array } \\right .",
    "\\label{eq : lower bounds on the chernoff information for case 1 of the example}\\ ] ] hence , in order to achieve the target @xmath414 for the overall error probability , the lower bounds on the chernoff information in and the exponential upper bound on the overall error probability in imply that @xmath415 2.68 \\cdot 10 ^ 6 & \\quad \\mbox{from eqs.~\\eqref{eq : looser lower bound on the chernoff information between bernoulli sums of independent rvs and poisson distribution } and \\eqref{eq : exponential upper bound on the overall probability in terms of chernoff information } } \\end{array } \\right .",
    "\\label{eq : lower bounds on n for case 1 of the example}\\ ] ] so , the number of required samples is approximately reduced by a factor of 7 . 2 .   for the second case in example  [ example : application of the chernoff - stein lemma and the new lower bound on the relative entropy ]",
    ", the lower bounds on the chernoff information in eqs .   and read @xmath416 -\\frac{1}{2 } \\ , \\log \\left(1-\\frac{\\lambda^4}{1024 } \\",
    ", \\min\\bigl\\{1 , \\frac{1}{\\lambda^2}\\bigr\\ } \\left(\\frac{1-\\alpha}{1+\\alpha } \\ , \\frac{1+\\alpha^n}{1-\\alpha^n}\\right)^2 \\right ) & \\quad \\mbox{from eq.~\\eqref{eq : looser lower bound on the chernoff information between bernoulli sums of independent rvs and poisson distribution } } \\end{array } \\right.\\ ] ] so , the same choice of parameters @xmath404 , @xmath405 and @xmath389 as in example  [ example : application of the chernoff - stein lemma and the new lower bound on the relative entropy ] implies that @xmath417 4.00 \\cdot 10^{-8 } & \\quad \\mbox{from eq.~\\eqref{eq : looser lower bound on the chernoff information between bernoulli sums of independent rvs and poisson distribution}. } \\end{array } \\right .",
    "\\label{eq : lower bounds on the chernoff information for case 2 of the example}\\ ] ]    for obtaining the target @xmath414 for the overall error probability , the lower bounds on the chernoff information in and the exponential upper bound on the overall error probability in imply that @xmath418 5.76 \\cdot 10 ^ 8 & \\quad \\mbox{from eqs.~\\eqref{eq : looser lower bound on the chernoff information between bernoulli sums of independent rvs and poisson distribution } and \\eqref{eq : exponential upper bound on the overall probability in terms of chernoff information } } \\end{array } \\right .",
    "\\label{eq : lower bounds on n for case 2 of the example}\\ ] ] so , the improved lower bound on the chernoff information implies in this case a reduction in the required number of samples @xmath374 by a factor of 123 .",
    "[ example : the application of the new lower bound on the chernoff information with the bayesian approach for binary hypothesis testing ]        the proof of theorem  [ theorem : improved lower bound on the total variation distance ] starts similarly to the proof of ( * ? ? ?",
    "* theorem  2 ) .",
    "however , it significantly deviates from the original analysis in order to derive an improved lower bound on the total variation distance . in the following ,",
    "we introduce the proof of theorem  [ theorem : improved lower bound on the total variation distance ] .",
    "let @xmath2 be independent bernoulli random variables with @xmath39 .",
    "let @xmath419 , @xmath420 for every @xmath8 , and @xmath17 with mean @xmath421 . from the basic equation of the chen - stein method , the equality @xmath422 = 0 .",
    "\\label{eq : basic equation of the chen - stein method for poisson approximation}\\ ] ] holds for an arbitrary bounded function @xmath423 .",
    "furthermore @xmath424 \\nonumber \\\\ & & = \\sum_{j=1}^n p_j \\ , { \\ensuremath{\\mathbb{e}}}\\bigl[f(w+1)\\bigr ] - \\sum_{j=1}^n { \\ensuremath{\\mathbb{e}}}\\bigl[x_j f(w ) \\bigr ] \\nonumber \\\\ & & = \\sum_{j=1}^n p_j \\ , { \\ensuremath{\\mathbb{e}}}\\bigl[f(w+1)\\bigr ] - \\sum_{j=1}^n p_j \\ , { \\ensuremath{\\mathbb{e}}}\\bigl[f(v_j+1 ) \\ , | \\ , x_j = 1 \\bigr ] \\nonumber \\\\ & &   \\stackrel{(\\text{a})}{= } \\sum_{j=1}^n p_j \\ , { \\ensuremath{\\mathbb{e}}}\\bigl[f(w+1 ) -",
    "f(v_j+1 ) \\bigr ] \\nonumber \\\\ & & = \\sum_{j=1}^n p_j^2 \\ , { \\ensuremath{\\mathbb{e}}}\\bigl[f(w+1 ) - f(v_j+1 ) \\ , | \\ , x_j = 1 \\bigr ] \\nonumber \\\\ & & = \\sum_{j=1}^n p_j^2 \\ , { \\ensuremath{\\mathbb{e}}}\\bigl[f(v_j+2 ) - f(v_j+1 ) \\ , | \\ , x_j = 1 \\bigr ] \\nonumber \\\\ & &   \\stackrel{(\\text{b})}{= } \\sum_{j=1}^n p_j^2 \\ , { \\ensuremath{\\mathbb{e}}}\\bigl[f(v_j+2 ) - f(v_j+1 ) \\bigr ] \\label{eq : first step in the derivation of the improved lower bound on total variation distance}\\end{aligned}\\ ] ] where equalities  ( a ) and ( b ) hold since @xmath425 and @xmath426 are independent random variables for every @xmath427 . by subtracting from",
    ", it follows that for an arbitrary bounded function @xmath423 @xmath428 - { \\ensuremath{\\mathbb{e}}}\\bigl[\\lambda f(z+1 ) - z f(z)\\bigr ] = \\sum_{j=1}^n p_j^2 \\ , { \\ensuremath{\\mathbb{e}}}\\bigl[f(v_j+2 ) - f(v_j+1 ) \\bigr ] .",
    "\\label{eq : second step in the derivation of the improved lower bound on total variation distance}\\ ] ] in the following , an upper bound on the left - hand side of is derived , based on total variation distance between the two distributions of @xmath22 and @xmath76 .",
    "@xmath424 - { \\ensuremath{\\mathbb{e}}}\\bigl[\\lambda",
    "f(z+1 ) - z f(z)\\bigr ] \\nonumber \\\\ & & = \\sum_{k=0}^{\\infty } \\ , \\bigl ( \\lambda f(k+1 ) - k f(k ) \\bigr ) \\ , \\bigl ( { \\ensuremath{\\mathbb{p}}}(w = k ) - { \\ensuremath{\\mathbb{p}}}(z = k ) \\bigr ) \\nonumber \\\\ & & \\leq \\sum_{k=0}^{\\infty } \\ ,",
    "\\bigl| \\lambda f(k+1 ) - k f(k ) \\bigr| \\ , \\bigl| { \\ensuremath{\\mathbb{p}}}(w = k ) - { \\ensuremath{\\mathbb{p}}}(z = k ) \\bigr| \\label{eq : intermediate step that is used later to derive a lower bound on the local distance } \\\\ & & \\leq \\sup_{k \\in { \\ensuremath{\\mathbb{n}}}_0 } \\bigl| \\lambda f(k+1 ) - k f(k ) \\bigr| \\ , \\sum_{k=0}^{\\infty } \\bigl| { \\ensuremath{\\mathbb{p}}}(w = k ) - { \\ensuremath{\\mathbb{p}}}(z = k ) \\bigr| \\nonumber \\\\ & & = 2 d_{\\text{tv}}(p_w , \\ , \\text{po}(\\lambda ) ) \\ , \\sup_{k \\in { \\ensuremath{\\mathbb{n}}}_0 } \\bigl| \\lambda f(k+1 ) - k f(k ) \\bigr| \\label{eq : third step in the derivation of the improved lower bound on total variation distance}\\end{aligned}\\ ] ] where the last equality follows from .",
    "hence , the combination of and gives the following lower bound on the total variation distance : @xmath429 \\bigr\\}}{2 \\ , \\sup_{k \\in { \\ensuremath{\\mathbb{n}}}_0 } \\bigl| \\lambda f(k+1 ) - k f(k ) \\bigr| } \\label{eq : fourth step in the derivation of the improved lower bound on total variation distance}\\end{aligned}\\ ] ] which holds , in general , for an arbitrary bounded function @xmath423 .    at this point , we deviate from the proof of ( * ? ? ?",
    "* theorem  2 ) by generalizing and refining ( in a non - trivial way ) the original analysis .",
    "the general problem with the current lower bound in is that it is not calculable in closed form for a given @xmath20 , so one needs to choose a proper function @xmath20 and derive a closed - form expression for a lower bound on the right - hand side of . to this end , let @xmath430 where @xmath283 and @xmath284 are fixed constants ( note that @xmath123 in needs to be positive for @xmath20 to be a bounded function ) . in order to derive a lower bound on the total variation distance , we calculate a lower bound on the numerator and an upper bound on the denominator of the right - hand side of for the function @xmath20 in . referring to the numerator of the right - hand side of with @xmath20 in , for every @xmath427 , @xmath431 & & = \\int_{v_j + 1 - \\alpha_2}^{v_j + 2 - \\alpha_2 } \\frac{\\mathrm{d}}{\\mathrm{d}u } \\left ( ( u+\\alpha_2-\\alpha_1 ) \\ , \\exp\\bigl(-\\frac{u^2}{\\theta \\lambda}\\bigr ) \\right ) \\",
    ", \\mathrm{d}u   \\nonumber \\\\[0.2 cm ] & & = \\int_{v_j + 1 - \\alpha_2}^{v_j + 2 - \\alpha_2 } \\left(1 - \\frac{2u ( u+\\alpha_2-\\alpha_1)}{\\theta \\lambda } \\right ) \\exp\\bigl(-\\frac{u^2}{\\theta \\lambda}\\bigr ) \\ , \\mathrm{d}u \\nonumber \\\\[0.2 cm ] & & = \\int_{v_j + 1 - \\alpha_2}^{v_j + 2 - \\alpha_2 } \\left(1 - \\frac{2 u^2}{\\theta \\lambda}\\right ) \\ , \\exp\\bigl(-\\frac{u^2}{\\theta \\lambda}\\bigr ) \\ , \\mathrm{d}u - \\frac{2(\\alpha_2 - \\alpha_1)}{\\theta \\lambda } \\int_{v_j + 1 - \\alpha_2}^{v_j + 2 - \\alpha_2 }",
    "u \\ , \\exp\\bigl(-\\frac{u^2}{\\theta \\lambda}\\bigr ) \\ , \\mathrm{d}u \\nonumber \\\\[0.2 cm ] & & = \\int_{v_j + 1 - \\alpha_2}^{v_j + 2 - \\alpha_2 } \\left(1 - \\frac{2 u^2}{\\theta \\lambda}\\right ) \\ , \\exp\\bigl(-\\frac{u^2}{\\theta \\lambda}\\bigr ) \\ , \\mathrm{d}u \\nonumber \\\\ & & \\hspace*{0.4 cm } - ( \\alpha_2 - \\alpha_1 ) \\left [ \\exp\\biggl(-\\frac{(v_j + 2 - \\alpha_2)^2}{\\theta \\lambda}\\biggr ) - \\exp\\biggl(-\\frac{(v_j + 1 - \\alpha_2)^2}{\\theta \\lambda}\\biggr ) \\right ] .",
    "\\label{eq : fifth step in the derivation of the improved lower bound on total variation distance}\\end{aligned}\\ ] ] we rely in the following on the inequality @xmath432 applying it to the integral on the right - hand side of gives that @xmath431 & & \\geq \\int_{v_j + 1 - \\alpha_2}^{v_j + 2 - \\alpha_2 } \\left(1 - \\frac{3 u^2}{\\theta \\lambda}\\right ) \\",
    ", \\mathrm{d}u - ( \\alpha_2 - \\alpha_1 ) \\left [ \\exp\\biggl(-\\frac{(v_j + 2 - \\alpha_2)^2}{\\theta \\lambda}\\biggr ) - \\exp\\biggl(-\\frac{(v_j + 1 - \\alpha_2)^2}{\\theta \\lambda}\\biggr ) \\right ] \\nonumber \\\\[0.1 cm ] & & \\geq 1 - \\frac{\\bigl(v_j + 2 - \\alpha_2\\bigr)^3 - \\bigl(v_j + 1 - \\alpha_2\\bigr)^3}{\\theta \\lambda } \\nonumber \\\\[0.1 cm ] & & \\hspace*{0.4 cm } - \\bigl|\\alpha_2 - \\alpha_1\\bigr| \\cdot \\left| \\exp\\biggl(-\\frac{(v_j + 2 - \\alpha_2)^2}{\\theta \\lambda}\\biggr ) - \\exp\\biggl(-\\frac{(v_j + 1 - \\alpha_2)^2}{\\theta \\lambda}\\biggr ) \\right| .",
    "\\label{eq : sixth step in the derivation of the improved lower bound on total variation distance}\\end{aligned}\\ ] ] in order to proceed , note that if @xmath433 then ( based on the mean - value theorem of calculus ) @xmath434 \\\\[0.1 cm ] & & \\leq e^{-\\min\\{x_1 , x_2\\ } } \\ , |x_1 - x_2|\\end{aligned}\\ ] ] which , by applying it to the second term on the right - hand side of , gives that for every @xmath427 @xmath435 & & \\leq \\exp\\left(-\\frac{\\min\\bigl\\{(v_j+2-\\alpha_2)^2 , \\ , ( v_j+1-\\alpha_2)^2 \\bigr\\}}{\\theta \\lambda}\\right ) \\cdot \\left(\\frac{(v_j+2-\\alpha_2)^2-(v_j+1-\\alpha_2)^2}{\\theta \\lambda}\\right ) \\ , .",
    "\\label{eq : seventh step in the derivation of the improved lower bound on total variation distance}\\end{aligned}\\ ] ] since @xmath436 then @xmath437 & & \\geq \\left\\ { \\begin{array}{cl } 0 \\quad & \\mbox{if $ \\alpha_2 \\geq 1 $ } \\\\[0.1 cm ] ( 1-\\alpha_2)^2 \\quad & \\mbox{if $ \\alpha_2 < 1 $ } \\end{array } \\right .",
    "\\nonumber \\\\[0.1 cm ] & & = \\bigl(1-\\alpha_2\\bigr)_{+}^2 \\label{eq : 8th step in the derivation of the improved lower bound on total variation distance}\\end{aligned}\\ ] ] where @xmath438 hence , the combination of the two inequalities in  gives that @xmath435 & & \\leq \\exp\\left(-\\frac{(1-\\alpha_2)_+^2}{\\theta \\lambda } \\right ) \\cdot \\left(\\frac{\\left|(v_j + 2 - \\alpha_2)^2 - ( v_j + 1 - \\alpha_2)^2 \\right|}{\\theta \\lambda } \\right ) \\nonumber \\\\[0.1 cm ] & & = \\exp\\left(-\\frac{(1-\\alpha_2)_+^2}{\\theta \\lambda } \\right ) \\cdot \\frac{\\left|2v_j + 3 - 2 \\alpha_2 \\right|}{\\theta \\lambda } \\nonumber \\\\[0.1 cm ] & & \\leq \\exp\\left(-\\frac{(1-\\alpha_2)_+^2}{\\theta \\lambda } \\right ) \\cdot \\frac{2v_j + \\left|3 - 2 \\alpha_2 \\right|}{\\theta \\lambda } \\label{eq : ninth step in the derivation of the improved lower bound on total variation distance}\\end{aligned}\\ ] ] and therefore , a combination of the inequalities in and gives that @xmath431 & & \\geq 1 - \\frac{\\bigl(v_j + 2 - \\alpha_2\\bigr)^3 - \\bigl(v_j + 1 - \\alpha_2\\bigr)^3}{\\theta \\lambda } \\nonumber \\\\[0.1 cm ] & & \\hspace*{0.4 cm } - \\bigl|\\alpha_2 - \\alpha_1\\bigr| \\cdot \\exp\\left(-\\frac{(1-\\alpha_2)_+^2}{\\theta \\lambda } \\right ) \\cdot \\frac{2v_j + \\left|3 - 2 \\alpha_2 \\right|}{\\theta \\lambda } \\ ; .",
    "\\label{eq : 10th step in the derivation of the improved lower bound on total variation distance}\\end{aligned}\\ ] ] let @xmath439 , then @xmath431 & & \\geq 1 - \\frac{\\bigl(u_j + \\lambda + 2 - \\alpha_2\\bigr)^3 - \\bigl(u_j + \\lambda + 1 - \\alpha_2\\bigr)^3}{\\theta \\lambda } \\nonumber \\\\[0.1 cm ] & & \\hspace*{0.4 cm } - \\bigl|\\alpha_2 - \\alpha_1\\bigr| \\cdot \\exp\\left(-\\frac{(1-\\alpha_2)_+^2}{\\theta \\lambda } \\right ) \\cdot \\frac{2u_j + 2\\lambda + \\left|3 - 2 \\alpha_2 \\right|}{\\theta \\lambda } \\nonumber \\\\[0.1 cm ] & & = 1 - \\frac{3 u_j^2 + 3 \\bigl(3 - 2\\alpha_2 + 2 \\lambda\\bigr ) u_j + ( 2-\\alpha_2+\\lambda)^3 - ( 1-\\alpha_2+\\lambda)^3}{\\theta \\lambda } \\nonumber \\\\[0.1 cm ] & & \\hspace*{0.4 cm } - \\bigl|\\alpha_2 - \\alpha_1\\bigr| \\cdot \\exp\\left(-\\frac{(1-\\alpha_2)_+^2}{\\theta \\lambda } \\right ) \\cdot \\frac{2u_j + 2\\lambda + \\left|3 - 2 \\alpha_2 \\right|}{\\theta \\lambda } \\ ; .",
    "\\label{eq : 11th step in the derivation of the improved lower bound on total variation distance}\\end{aligned}\\ ] ] in order to derive a lower bound on the numerator of the right - hand side of , for the function @xmath20 in , we need to calculate the expected value of the right - hand side of . to this end",
    ", the first and second moments of @xmath440 are calculated as follows : @xmath441 & & = { \\ensuremath{\\mathbb{e}}}\\bigl ( ( v_j - \\lambda)^2 \\bigr ) \\nonumber \\\\[0.1 cm ] & & = { \\ensuremath{\\mathbb{e}}}\\left [ \\ , \\left(\\sum_{i \\neq j } ( x_i - p_i ) - p_j \\right)^2 \\ , \\right ] \\nonumber \\\\ & & \\stackrel{(\\text{a})}{= } \\sum_{i \\neq j } { \\ensuremath{\\mathbb{e}}}\\bigl[(x_i - p_i)^2\\bigr ] + p_j^2 \\nonumber \\\\ & & \\stackrel{(\\text{b})}{= } \\sum_{i \\neq j } p_i(1-p_i ) + p_j^2 \\nonumber \\\\ & & = \\sum_{i \\neq j } p_i - \\sum_{i \\neq j } p_i^2 + p_j^2 \\nonumber \\\\ & & = \\lambda - p_j - \\sum_{i \\neq j } p_i^2 + p_j^2 .",
    "\\label{eq : second moment of u_j}\\end{aligned}\\ ] ] where equalities  ( a ) and  ( b ) hold since , by assumption , the binary random variables @xmath2 are independent and @xmath442 , @xmath443 . by taking expectations on both sides of",
    ", one obtains from and that @xmath444 \\nonumber \\\\[0.2 cm ] & & \\geq 1 - \\frac{3 \\bigl(\\lambda - p_j - \\sum_{i \\neq j } p_i^2 + p_j^2\\bigr ) + 3 \\bigl(3 - 2\\alpha_2 + 2 \\lambda\\bigr ) \\bigl(-p_j \\bigr ) + ( 2-\\alpha_2+\\lambda)^3 - ( 1-\\alpha_2+\\lambda)^3}{\\theta \\lambda } \\nonumber \\\\[0.2 cm ] & & \\hspace*{0.4 cm } - \\bigl|\\alpha_2 - \\alpha_1\\bigr| \\cdot \\exp\\left(-\\frac{(1-\\alpha_2)_+^2}{\\theta \\lambda } \\right ) \\cdot \\left(\\frac{-2p_j + 2\\lambda + \\left|3 - 2 \\alpha_2 \\right|}{\\theta \\lambda } \\right ) \\nonumber \\\\[0.2 cm ] & & = 1 - \\frac{3 \\lambda + ( 2-\\alpha_2+\\lambda)^3 - ( 1-\\alpha_2+\\lambda)^3 - \\bigl[3p_j(1-p_j ) + 3 \\sum_{i \\neq j } p_i^2 + 3 \\bigl(3 - 2\\alpha_2 + 2 \\lambda\\bigr ) p_j \\bigr]}{\\theta \\lambda } \\nonumber \\\\[0.2 cm ] & & \\hspace*{0.4 cm } - \\biggl(\\frac{\\bigl|\\alpha_2 - \\alpha_1\\bigr| \\ , \\bigl(2\\lambda - 2 p_j+ \\left|3 - 2 \\alpha_2 \\right| \\bigr)}{\\theta \\lambda } \\biggr ) \\cdot \\exp\\left(-\\frac{(1-\\alpha_2)_+^2}{\\theta \\lambda } \\right ) \\nonumber \\\\[0.2 cm ] & & \\geq 1 - \\frac{3 \\lambda + ( 2-\\alpha_2+\\lambda)^3 - ( 1-\\alpha_2+\\lambda)^3 - \\bigl(9 - 6\\alpha_2 + 6 \\lambda\\bigr ) p_j}{\\theta \\lambda } \\nonumber \\\\[0.2 cm ] & & \\hspace*{0.4 cm } - \\biggl(\\frac{\\bigl|\\alpha_2 - \\alpha_1\\bigr| \\ , \\bigl(2\\lambda+ \\left|3 - 2 \\alpha_2 \\right| \\bigr)}{\\theta \\lambda } \\biggr ) \\cdot \\exp\\left(-\\frac{(1-\\alpha_2)_+^2}{\\theta \\lambda } \\right ) \\ ; .",
    "\\label{eq : 12th step in the derivation of the improved lower bound on total variation distance}\\end{aligned}\\ ] ] therefore , from , the following lower bound on the right - hand side of holds @xmath445 \\bigr\\ } \\geq \\left ( \\frac{3\\bigl(3 - 2\\alpha_2 + 2\\lambda\\bigr)}{\\theta \\lambda } \\right ) \\sum_{j=1}^n p_j^3 \\nonumber \\\\ & & \\hspace*{-1.2 cm } + \\left(1-\\frac{3 \\lambda + ( 2-\\alpha_2+\\lambda)^3 - ( 1-\\alpha_2+\\lambda)^3 + |\\alpha_1 - \\alpha_2| \\bigl(2\\lambda+|3 - 2\\alpha_2|\\bigr ) \\exp\\left(-\\frac{(1-\\alpha_2)_+^2}{\\theta \\lambda } \\right)}{\\theta \\lambda } \\right ) \\sum_{j=1}^n p_j^2 \\ , .",
    "\\label{eq : 12.5th step in the derivation of the improved lower bound on total variation distance}\\end{aligned}\\ ] ] note that if @xmath446 , which is a condition that is involved in the maximization of , then the first term on the right - hand side of can be removed , and the resulting lower bound on the numerator of the right - hand side of gets the form @xmath447 \\bigr\\ } \\geq \\bigl(1 - h_{\\lambda}(\\alpha_1 , \\alpha_2 , \\theta ) \\bigr ) \\sum_{j=1}^n p_j^2 \\label{eq : 13th step in the derivation of the improved lower bound on total variation distance}\\ ] ] where the function @xmath448 is introduced in .",
    "we turn now to derive an upper bound on the denominator of the right - hand side of .",
    "therefore , we need to derive a closed - form upper bound on @xmath449 with the function @xmath20 in . for every @xmath450 @xmath451 + ( \\lambda - k ) \\ , f(k ) .",
    "\\label{eq : 14th step in the derivation of the improved lower bound on total variation distance}\\ ] ] in the following , we derive bounds on each of the two terms on the right - hand side of , and we start with the first term .",
    "let @xmath452 then @xmath453 for every @xmath450 , and by the mean value of calculus @xmath454 \\nonumber \\\\ & & = \\left(1-\\frac{2 c_k^2}{\\theta \\lambda}\\right ) \\",
    ", \\exp\\left(-\\frac{c_k^2}{\\theta \\lambda}\\right ) + \\left(\\frac{2(\\alpha_1-\\alpha_2 ) c_k}{\\theta \\lambda}\\right ) \\ , \\exp\\left(-\\frac{c_k^2}{\\theta \\lambda}\\right ) \\ , .",
    "\\label{eq : 15th step in the derivation of the improved lower bound on total variation distance}\\end{aligned}\\ ] ] by referring to the first term on the right - hand side of , let @xmath455 then the global maximum and minimum of @xmath456 over the non - negative real line are obtained at @xmath457 and @xmath458 , respectively , and therefore @xmath459 let @xmath460 , then it follows that the first term on the right - hand side of satisfies the inequality @xmath461 furthermore , by referring to the second term on the right - hand side of , let @xmath462 then the global maximum and minimum of @xmath463 over the real line are obtained at @xmath464 and @xmath465 , respectively , and therefore @xmath466 let this time @xmath467 , then it follows that the second term on the right - hand side of satisfies @xmath468 hence , by combining the equality in with the two inequalities in and , it follows that the first term on the right - hand side of satisfies @xmath469 \\leq \\lambda + \\sqrt{\\frac{2\\lambda}{\\theta e } } \\cdot |\\alpha_1 - \\alpha_2| \\ , , \\quad \\forall \\ , k \\in { \\ensuremath{\\mathbb{n}}}_0 . \\label{eq : 18th step in the derivation of the improved lower bound on total variation distance}\\ ] ] we continue the analysis by a derivation of bounds on the second term of the right - hand side of . for the function @xmath20 in , it is equal to @xmath470 \\ , \\bigl [ ( k-\\alpha_2)+(\\alpha_2-\\alpha_1 ) \\bigr ] \\",
    ", \\exp\\biggl(-\\frac{(k-\\alpha_2)^2}{\\theta \\lambda}\\biggr ) \\nonumber \\\\ & & = \\bigl [ ( \\lambda-\\alpha_2 ) ( k-\\alpha_2 ) + ( \\alpha_2-\\alpha_1 ) ( \\lambda-\\alpha_2 ) - ( k-\\alpha_2)^2 + ( \\alpha_1 - \\alpha_2 ) ( k-\\alpha_2 ) \\bigr ] \\ , \\exp\\biggl(-\\frac{(k-\\alpha_2)^2}{\\theta \\lambda}\\biggr ) \\nonumber \\\\[0.1 cm ] & & = \\bigl [ \\sqrt{\\theta \\lambda } \\ , ( \\lambda-\\alpha_2 ) \\ , v_k - \\theta \\lambda \\ , v_k^2 - \\sqrt{\\theta \\lambda } \\ , ( \\alpha_2 - \\alpha_1 ) \\ , v_k + ( \\alpha_2 - \\alpha_1 ) ( \\lambda - \\alpha_2 ) \\bigr ] \\ , e^{-v_k^2 } \\ , , \\quad v_k \\triangleq \\frac{k-\\alpha_2}{\\sqrt{\\theta \\lambda } } \\ ; \\ ; \\forall \\ , k \\in { \\ensuremath{\\mathbb{n}}}_0 \\nonumber \\\\ & & = ( c_0 + c_1 v_k + c_2 v_k^2 ) \\ , e^{-v_k^2 }",
    "\\label{eq : 19th step in the derivation of the improved lower bound on total variation distance}\\end{aligned}\\ ] ] where the coefficients @xmath471 and @xmath472 are introduced in eqs .   ,",
    "respectively . in order to derive bounds on the left - hand side of , lets find the global maximum and minimum of the function @xmath281 in : @xmath473 note that @xmath282 and @xmath281 is differentiable over the real line , so the global maximum and minimum of @xmath281 are attained at finite points and their corresponding values are finite . by setting the derivative of @xmath281 to zero , the candidates for the global maximum and minimum of @xmath281 over the real line are the real zeros @xmath279 of the cubic polynomial equation in .",
    "note that by their definition in , the values of @xmath279 are _ independent _ of the value of @xmath450 , and also the size of the set @xmath279 is equal to  3 ( see remark  [ remark : the size of the set of real zeros is equal to 3 ] ) .",
    "hence , it follows from that @xmath474 where these bounds on the second term on the right - hand side of are independent of the value of @xmath450 .    in order to get bounds on the left - hand side of ,",
    "note that from the bounds on the first and second terms on the right - hand side of ( see and , respectively ) then for every @xmath450 @xmath475 & & \\leq \\lambda \\ , f(k+1 ) - k \\ , f(k ) \\nonumber \\\\[0.1 cm ] & & \\leq \\max_{i \\in \\{1 , 2 , 3\\ } } \\{x(u_i)\\ } + \\lambda + \\sqrt{\\frac{2\\lambda}{\\theta e } } \\cdot |\\alpha_1 - \\alpha_2| \\label{eq : 21st step in the derivation of the improved lower bound on total variation distance}\\end{aligned}\\ ] ] which yields that the following inequality is satisfied : @xmath476 where the function @xmath477 is introduced in . finally , by combining the inequalities in eqs .  , and , the lower bound on the total variation distance in follows .",
    "the existing upper bound on the total variation distance in was derived in ( * ? ? ?",
    "* theorem  1 ) ( see theorem  [ theorem : bounds on the total variation distance - barbour and hall 1984 ] here ) .",
    "this completes the proof of theorem  [ theorem : improved lower bound on the total variation distance ] .",
    "corollary  [ corollary : lower bound on the total variation distance ] follows as a special case of theorem  [ theorem : improved lower bound on the total variation distance ] when the proposed function @xmath20 in is chosen such that two of its three free parameters ( i.e. , @xmath478 and @xmath479 ) are determined sub - optimally , and its third parameter ( @xmath123 ) is determined optimally in terms of the sub - optimal selection of the two other parameters .",
    "more explicitly , let @xmath478 and @xmath479 in be set to be equal to @xmath13 ( i.e. , @xmath285 ) . from ",
    ", this setting implies that @xmath480 and @xmath481 ( since @xmath482 ) .",
    "the cubic polynomial equation in , which corresponds to this ( possibly sub - optimal ) setting of @xmath478 and @xmath479 , is @xmath483 whose zeros are @xmath484 .",
    "the function @xmath281 in therefore gets the form @xmath485 so @xmath486 and @xmath487 .",
    "it implies that @xmath488 and therefore @xmath448 and @xmath477 in and , respectively , are simplified to @xmath489 this sub - optimal setting of @xmath478 and @xmath479 in implies that the coefficient @xmath269 in is replaced with a loosened version @xmath490 let @xmath491 , then is simplified to @xmath492 .",
    "it therefore follows from , and  that @xmath493 where @xmath494 and , in general , @xmath495 due to the above restricted constraint on @xmath123 ( see versus ) .",
    "differentiation of the function inside the supremum w.r.t .",
    "@xmath123 and by setting its derivative to zero , one gets the following quadratic equation in @xmath123 : @xmath496 whose positive solution is the optimized value of @xmath123 in .",
    "furthermore , it is clear that this value of @xmath123 in is larger than , e.g. , 3 , so it satisfies the constraint in .",
    "this completes the proof of corollary  [ corollary : lower bound on the total variation distance ] .      as was demonstrated in the previous sub - section , theorem  [ theorem : improved lower bound on the total variation distance ]",
    "implies the satisfiability of the lower bound on the total variation distance in corollary  [ corollary : lower bound on the total variation distance ] . in the following",
    ", it is proved that corollary  [ corollary : lower bound on the total variation distance ] implies the lower bound on the total variation distance in ( * ? ? ?",
    "* theorem  2 ) ( see also theorem  [ theorem : bounds on the total variation distance - barbour and hall 1984 ] here ) , and the improvement in the tightness of the lower bound in corollary  [ corollary : lower bound on the total variation distance ] is explicitly quantified in the two extreme cases where @xmath267 and @xmath125 .",
    "the observation that corollary  [ corollary : lower bound on the total variation distance ] provides a tightened lower bound , as compared to ( * ? ? ?",
    "* theorem  2 ) , is justified by the fact that the lower bound in with the coefficient @xmath298 in was loosened in the proof of ( * ? ? ?",
    "* theorem  2 ) by a sub - optimal selection of the parameter @xmath123 which leads to a lower bound on @xmath298 ( the sub - optimal selection of @xmath123 in the proof of ( * ? ? ?",
    "* theorem  2 ) is @xmath497 ) . on the other hand",
    ", the optimized value of @xmath123 that is used in provides an exact closed - form expression for @xmath298 in , and it leads to the derivation of the bound in corollary  [ corollary : lower bound on the total variation distance ] . this therefore justifies the observation that the lower bound on the total variation distance in corollary  [ corollary : lower bound on the total variation distance ] implies the original lower bound in ( * ? ? ?",
    "* theorem  2 ) .    from (",
    "* theorems  1 and 2 ) , the ratio between the upper and lower bounds on the total variation distance ( these bounds also appear in ) is equal to 32 in the two extreme cases where @xmath267 or @xmath125 . in order to quantify the improvement that is obtained by corollary  [ corollary : lower bound on the total variation distance ] ( that follows by the optimal selection of the parameter @xmath123 ) , we calculate in the following the ratio of the same upper bound and the new lower bound in this corollary at these two extreme cases . in the limit where one lets @xmath13 tend to infinity , this ratio tends to @xmath498 & & = 2 \\lim_{\\lambda \\rightarrow \\infty } \\frac{2 e^{-{3/2}}+\\theta \\ , e^{-1}}{1-\\frac{3\\lambda+7}{\\lambda \\theta } } \\nonumber \\\\[0.1 cm ] & & = \\frac{2}{e } \\ , \\lim_{\\lambda \\rightarrow \\infty } \\frac{\\theta \\bigl(2 e^{-{1/2}}+\\theta \\bigr)}{\\theta-\\bigl(3+\\frac{7}{\\lambda}\\bigr ) } \\nonumber \\\\[0.1 cm ] & & \\stackrel{\\text{(a)}}{= } \\frac{2 \\left(3+\\sqrt{3(3 + 2e^{-1/2 } ) } \\right ) \\ , \\left(3 + 2e^{-1/2}+\\sqrt{3(3 + 2e^{-1/2 } ) } \\right)}{e \\ , \\sqrt{3(3 + 2e^{-1/2 } ) } } \\nonumber \\\\[0.1 cm ] & & = \\frac{2}{e } \\ , \\left(3+\\sqrt{3(3 + 2e^{-1/2 } ) } \\right ) \\ , \\left(1+\\sqrt{1+\\frac{2}{3 } \\cdot e^{-1/2 } } \\right ) \\nonumber \\\\[0.1 cm ] & & = \\frac{6}{e } \\ , \\left(1+\\sqrt{1+\\frac{2}{3 } \\cdot e^{-1/2 } } \\right)^2 \\approx 10.539 \\label{eq : limit of the ratio between the upper and lower bounds on the total variation distance when lambda tends to infinity}\\end{aligned}\\ ] ] where equality  ( a ) holds since , from , @xmath499 .",
    "furthermore , the limit of this ratio when @xmath13 tends to zero is equal to @xmath500 & & = 2 \\ ,   \\lim_{\\lambda \\rightarrow 0 } \\left(\\frac{\\lambda \\theta \\ , ( 2 e^{-{3/2}}+\\theta \\ , e^{-1})}{\\theta-\\bigl(3+\\frac{7}{\\lambda}\\bigr)}\\right ) \\nonumber \\\\[0.1 cm ] & & \\stackrel{\\text{(a)}}{= } \\frac{28}{e } \\ , \\lim_{\\lambda \\rightarrow 0 } \\left(\\frac{2 e^{-{1/2}}+\\theta)}{\\theta-\\bigl(3+\\frac{7}{\\lambda}\\bigr)}\\right ) \\nonumber \\\\ & & \\stackrel{\\text{(b)}}{= } \\frac{56}{e } \\approx 20.601 \\label{eq : limit of the ratio between the upper and lower bounds on the total variation distance when lambda tends to zero}\\end{aligned}\\ ] ] where equalities  ( a ) and ( b ) hold since , from , it follows that @xmath501 .",
    "note that the two limits in and are indeed consistent with the limits of the upper curve in figure .",
    "[ figure : ratio ot upper and lower bounds on the total variation distance ] ( see p.  ) .",
    "this implies that corollary  [ corollary : lower bound on the total variation distance ] improves the original lower bound on the total variation distance in ( * ? ? ?",
    "* theorem  2 ) by a factor of @xmath502 in the limit where @xmath125 , and it improves it by a factor of @xmath503 in the other extreme case where @xmath267 while still having a closed - form expression lower bound in corollary  [ corollary : lower bound on the total variation distance ] where the only reason for this improvement that is related to the optimal choice of the free parameter @xmath123 , versus its sub - optimal choice in the proof of ( * ? ? ?",
    "* theorem  2 ) , shows a sensitivity of the resulting lower bound to the selection of @xmath123 .",
    "this observation in fact motivated us to further improve the lower bound on the total variation distance in theorem  [ theorem : improved lower bound on the total variation distance ] by introducing the two additional parameters @xmath283 of the proposed function @xmath20 in ( which , according to the proof in the previous sub - section , are set to be both equal to @xmath13 ) .",
    "the further improvement in the lower bound at the expense of a feasible increase in computational complexity is shown in the plot of figure .",
    "[ figure : ratio ot upper and lower bounds on the total variation distance ] ( by comparing the upper and lower curves of this plot which correspond to the ratio of the upper bound in ( * ? ? ?",
    "* theorem  1 ) and new lower bounds in corollary  [ corollary : lower bound on the total variation distance ] and theorem  [ theorem : improved lower bound on the total variation distance ] , respectively .",
    "it is interesting to note that no improvement is obtained however in theorem  [ theorem : improved lower bound on the total variation distance ] , as compared to corollary  [ corollary : lower bound on the total variation distance ] , for @xmath75 , as is shown in figure  [ figure : ratio ot upper and lower bounds on the total variation distance ] ( since the the upper and lower curves in this plot merge for @xmath75 , and their common limit in the extreme case where @xmath125 is given in ; this therefore implies that the two new lower bounds in theorem  [ theorem : improved lower bound on the total variation distance ] and corollary  [ corollary : lower bound on the total variation distance ] coincide for these values of @xmath13 ; however , for this range of values of @xmath13 , the lower bound on the total variation distance in corollary  [ corollary : lower bound on the total variation distance ] has the advantage of being expressed in closed form ( i.e. , there is no need for a numerical optimization of this bound ) . due to",
    "the above discussion , another important reasoning for our motivation to improve the lower bound on the total variation distance in theorem  [ theorem : improved lower bound on the total variation distance ] and corollary  [ corollary : lower bound on the total variation distance ] is that the factors of improvements that are obtained by these lower bounds ( as compared to the original bound ) are squared , according to pinsker s inequality , when one wishes to derive lower bounds on the relative entropy , and this improvement becomes significant in many inequalities in information theory and statistics where the relative entropy appears in the error exponent ( as is exemplified in section  [ subsection : second part of applications of the new bounds ] ) .",
    "finally , it is noted that the reason for introducing this type of discussion , which partially motivates our paper , in a sub - section that refers to proofs ( of the second half of this work ) is because this kind of discussion follows directly from the proofs of theorem  [ theorem : improved lower bound on the total variation distance ] and corollary  [ corollary : lower bound on the total variation distance ] , and therefore it was introduced here .      in the following we prove theorem  [ theorem : improved lower bound on the relative entropy ] by obtaining a lower bound on the relative entropy between the distribution @xmath504 of a sum of independent bernoulli random variables @xmath2 with @xmath3 and the poisson distribution @xmath4 with mean @xmath421 .",
    "a first lower bound on the relative entropy follows from a combination of pinsker s inequality ( see eq .  ) with the lower bound on the total variation distance between these distributions ( see theorem  [ theorem : improved lower bound on the total variation distance ] ) .",
    "the combination of the two gives that @xmath505 this lower bound can be tightened via the distribution - dependent refinement of pinsker s inequality in @xcite , which is introduced shortly in section  [ subsection : second part of the revision of some known results ] .",
    "following the technique of this refinement , let @xmath506 be the probability mass function that corresponds to the poisson distribution @xmath4 , i.e. , @xmath507 if @xmath508 then @xmath509 .",
    "hence , from , the maximization of @xmath510 over all the subsets @xmath511 is obtained for @xmath512 ( or , symmetrically , for @xmath513 ) which implies that , if @xmath508 , one gets from eqs .  , and that @xmath514 where @xmath515 and , since @xmath516 then @xmath517 & & \\hspace*{1.2 cm } = \\left ( \\frac{1}{2 e^{-\\lambda}-1 } \\right ) \\ ; \\log \\left(\\frac{1}{e^{\\lambda}-1 } \\right ) \\ , .",
    "\\label{eq : pi_q for the poisson distribution with mean lambda}\\end{aligned}\\ ] ] hence , the combination of , , and gives the lower bound on the relative entropy in theorem  [ theorem : improved lower bound on the relative entropy ] ( see eqs .  , and in this theorem ) .",
    "the upper bound on the considered relative entropy is a known result ( see ( * ? ? ?",
    "* theorem  1 ) ) , which is cited here in order to have both upper and lower bounds in the same inequality ( see eq .  ) .",
    "this completes the proof of theorem  [ theorem : improved lower bound on the relative entropy ] .",
    "we start by proving the tightened upper and lower bounds on the hellinger distance in terms of the total variation distance and relative entropy between the two considered distributions . these refined bounds in",
    "improve the original bounds in .",
    "it is noted that the left - hand side of is proved in @xcite , and the right - hand side is proved in @xcite .",
    "the following is the proof of the refined bounds on the hellinger distance in .    lets start with the proof of the left - hand side of . to this end , let @xmath31 and @xmath32 be two probability mass functions that are defined on a same set @xmath33 . from , and the cauchy - schwartz inequality @xmath518 & & = \\frac{1}{2 } \\ , \\sum_{x \\in \\mathcal{x } } \\left| p(x ) - q(x ) \\right| \\nonumber \\\\[0.1 cm ] & & = \\frac{1}{2 } \\ , \\sum_{x \\in \\mathcal{x } } \\left| \\sqrt{p(x ) } - \\sqrt{q(x ) } \\ , \\right| \\ , \\left ( \\sqrt{p(x ) } + \\sqrt{q(x ) } \\ , \\right ) \\nonumber \\\\[0.1 cm ] & & \\leq \\frac{1}{2 } \\ , \\left ( \\sum_{x \\in \\mathcal{x } } \\left ( \\sqrt{p(x ) } - \\sqrt{q(x ) } \\ , \\right)^2 \\right)^{\\frac{1}{2 } } \\ , \\left ( \\sum_{x \\in \\mathcal{x } } \\left ( \\sqrt{p(x ) } + \\sqrt{q(x ) } \\ , \\right)^2 \\right)^{\\frac{1}{2 } } \\nonumber \\\\[0.1 cm ] & & = d_{\\text{h}}(p , q ) \\cdot \\left ( 1 + \\sum_{x \\in \\mathcal{x } } \\sqrt{p(x ) \\ , q(x ) } \\right)^{\\frac{1}{2 } } \\nonumber \\\\[0.1 cm ] & & = d_{\\text{h}}(p , q ) \\ , \\bigl(2-\\bigl(d_{\\text{h}}(p , q)\\bigr)^2 \\bigr)^{\\frac{1}{2 } } \\ , .",
    "\\label{eq : 1st step in the derivation of the refined bounds for hellinger distance}\\end{aligned}\\ ] ] let @xmath519 and @xmath520 , then it follows by squaring both sides of that @xmath521 , which therefore implies that @xmath522 the right - hand side of is satisfied automatically since @xmath523 implies that @xmath524 .",
    "the left - hand side of gives the lower bound on the left - hand side of .",
    "next , we prove the upper bound on the right - hand side of . by jensen s inequality    @xmath525 & & = \\frac{1}{2 } \\",
    ", \\sum_{x \\in \\mathcal{x } } \\left\\ { \\left ( \\sqrt{p(x ) } - \\sqrt{q(x ) } \\ , \\right)^2 \\right\\ } \\nonumber \\\\[0.1 cm ] & & = 1 - \\sum_{x \\in \\mathcal{x } } \\sqrt{p(x ) \\ , q(x ) } \\nonumber \\\\[0.1 cm ] & & = 1 - \\sum_{x \\in \\mathcal{x } } p(x ) \\ , \\sqrt{\\frac{q(x)}{p(x ) } } \\nonumber \\\\[0.1 cm ] & & = 1 - \\sum_{x \\in \\mathcal{x } } p(x ) \\ , e^{\\frac{1}{2 } \\ , \\log \\left(\\frac{q(x)}{p(x)}\\right ) } \\nonumber \\\\[0.1 cm ] & & \\leq 1 - e^{\\frac{1}{2 } \\sum_{x \\in \\mathcal{x } } p(x ) \\ , \\log \\left(\\frac{q(x)}{p(x)}\\right ) } \\nonumber \\\\[0.1 cm ] & & = 1 - e^{-\\frac{1}{2 } \\ , d(p||q)}\\end{aligned}\\ ] ] which completes the proof of .",
    "the other bound on the bhattacharyya parameter in follows from and the simple relation in between the bhattacharyya parameter and hellinger distance .",
    "this completes the proof of proposition  [ proposition : sharpened inequality that relates between the total variation , hellinger distance , bhattacharyya parameter and relative entropy ] .",
    "the weaker bounds in , proved in @xcite , follow from their refined version in by using the simple inequalities @xmath526 \\\\ \\mbox{and } \\\\ & & e^{-x } \\geq 1-x , \\quad \\forall \\ , x \\geq 0.\\end{aligned}\\ ] ]      this corollary is a direct consequence of theorems  [ theorem : improved lower bound on the total variation distance ] and  [ theorem : improved lower bound on the relative entropy ] , and proposition  [ proposition : sharpened inequality that relates between the total variation , hellinger distance , bhattacharyya parameter and relative entropy ] .      under the conditions in corollary  [",
    "corollary : asymptotic results of the relative entropy and related quantities for independent bernoulli sums ] , the asymptotic scaling of the total variation distance , relative entropy , hellinger distance and bhattacharyya parameter follow from their ( upper and lower ) bounds in theorems  [ theorem : improved lower bound on the total variation distance ] and [ theorem : improved lower bound on the relative entropy ] and eqs .   and , respectively .",
    "this completes the proof of corollary  [ corollary : asymptotic results of the relative entropy and related quantities for independent bernoulli sums ] .",
    "let @xmath31 and @xmath32 be two arbitrary probability mass functions that are defined on a same set @xmath33 .",
    "we derive in the following the lower bound on the chernoff information in terms of the total variation distance between @xmath31 and @xmath32 , as is stated in .",
    "@xmath527 where inequality  ( a ) follows by selecting the possibly sub - optimal choice @xmath528 in , equality  ( b ) holds by definition of the bhattacharyya parameter ( see ) , equality  ( c ) follows from the equality in that relates the hellinger distance and bhattacharyya parameter , and inequality  ( d ) follows from the lower bound on the hellinger distance in terms of the total variation distance ( see ) .",
    "this completes the proof of proposition  [ proposition : lower bound on the chernoff information in terms of the total variation distance ] .",
    "this corollary is a direct consequence of the lower bound on the total variation distance in theorem  [ theorem : improved lower bound on the total variation distance ] , and the lower bound on the chernoff information in terms of the total variation distance in proposition  [ proposition : lower bound on the chernoff information in terms of the total variation distance ] .",
    "i thank ioannis kontoyiannis for inviting me to present part of this work at the 2012 information theory workshop ( itw 2012 ) in lausanne , switzerland , september 2012 .",
    "i also thank louis h. y. chen for expressing his interest in this work during the 2012 international workshop on applied probability that took place in june 2012 in jerusalem , israel .",
    "these two occasions were stimulating for the writing of this paper .",
    "i am thankful to peter harremos for personal communications during the 2012 international symposium on information theory ( isit 2012 ) at mit , and to abraham j. wyner for notifying me ( during isit 2012 as well ) about his related work to the chen - stein method and poisson approximation in the context of pattern recognition and the lempel - ziv algorithm @xcite .",
    "a. d. barbour , o. johnson , i. kontoyiannis and m. madiman , `` compound poisson approximation via information functionals , '' _ electronic journal of probability _ , vol .",
    "15 , paper no .",
    "42 , pp .  13441369 , august 2010 .",
    "l. holst and s. janson , `` poisson approximation using the stein - chen method and coupling : number of exceedances of gaussian random variables , '' _ annals of probability _ , vol .  18 , no .  2 , pp .",
    "713723 , april 1990 .",
    "o. johnson , i. kontoyiannis and m. madiman , `` a criterion for the compound poisson distribution to be maximum entropy , '' _ proceedings 2009 ieee international symposium on information theory _ , pp .  18991903 , seoul , south korea , july 2009 .",
    "o. johnson , i. kontoyiannis and m. madiman , `` log - concavity , ultra - log concavity , and a maximum entropy property of discrete compound poisson measures , '' to appear in _ discrete applied mathematics _ , 2012 .",
    "online available from http://arxiv.org/pdf/0912.0581v2.pdf .",
    "i. kontoyiannis , p. harremos , o. johnson and m. madiman , `` information - theoretic ideas in poisson approximation and concentration , '' slides of a short course ( available from the homepage of the first co - author ) ,",
    "september 2006 .",
    "l. a. shepp and i. olkin , `` entropy of the sum of independent bernoulli random variables and the multinomial distribution , '' _ contributions to probability _ , pp .  201206 , academic press , new york , 1981 ."
  ],
  "abstract_text": [
    "<S> the first part of this work considers the entropy of the sum of ( possibly dependent and non - identically distributed ) bernoulli random variables . </S>",
    "<S> upper bounds on the error that follows from an approximation of this entropy by the entropy of a poisson random variable with the same mean are derived via the chen - stein method . </S>",
    "<S> the second part of this work derives new lower bounds on the total variation distance and relative entropy between the distribution of the sum of independent bernoulli random variables and the poisson distribution . </S>",
    "<S> the starting point of the derivation of the new bounds in the second part of this work is an introduction of a new lower bound on the total variation distance , whose derivation generalizes and refines the analysis by barbour and hall ( 1984 ) , based on the chen - stein method for the poisson approximation . </S>",
    "<S> a new lower bound on the relative entropy between these two distributions is introduced , and this lower bound is compared to a previously reported upper bound on the relative entropy by kontoyiannis et al . </S>",
    "<S> ( 2005 ) . the derivation of the new lower bound on the relative entropy follows from the new lower bound on the total variation distance , combined with a distribution - dependent refinement of pinsker s inequality by ordentlich and weinberger ( 2005 ) . </S>",
    "<S> upper and lower bounds on the bhattacharyya parameter , chernoff information and hellinger distance between the distribution of the sum of independent bernoulli random variables and the poisson distribution with the same mean are derived as well via some relations between these quantities with the total variation distance and the relative entropy . the analysis in this work combines elements of information theory with the chen - stein method for the poisson approximation . </S>",
    "<S> the resulting bounds are easy to compute , and their applicability is exemplified .    </S>",
    "<S> chen - stein method , chernoff information , entropy , error bounds , error exponents , poisson approximation , relative entropy , total variation distance .    </S>",
    "<S> * ams 2000 subject classification * : primary 60e07 , 60e15 , 60g50 , 94a17 . </S>"
  ]
}