{
  "article_text": [
    "high - dimension , low - sample - size ( hdlss ) data situations occur in many areas of modern science such as genetic microarrays , medical imaging , text recognition , finance , chemometrics , and so on . in recent years ,",
    "substantial work has been done on hdlss asymptotic theory , where the sample size @xmath0 is fixed or @xmath1 as the data dimension @xmath2 .",
    "@xcite , @xcite , @xcite and @xcite explored several types of geometric representations of hdlss data .",
    "@xcite showed inconsistency properties of the sample eigenvalues and eigenvectors in the hdlss context .",
    "@xcite developed the noise - reduction methodology to give consistent estimators of both the eigenvalues and eigenvectors together with principal component ( pc ) scores in the hdlss context .",
    "@xcite also gave several asymptotic properties of the sample pc scores in the hdlss context .",
    "on the other hand , the asymptotic behavior of the sample eigenvalues was studied by @xcite and several literatures in high - dimension , large sample size data situations such as @xmath3 .",
    "the hdlss asymptotic theory was created under the assumption either the population distribution is gaussian or the random variables in a sphered data matrix have a @xmath4-mixing dependency .",
    "however , @xcite developed a hdlss asymptotic theory without such assumptions .",
    "moreover , they created a new principal component analysis ( pca ) called the cross - data - matrix methodology that is applicable to constructing an unbiased estimator in hdlss nonparametric settings .",
    "meanwhile , pca is quite popular for clustering high dimensional data .",
    "see section 9.2 in @xcite for details . for clustering hdlss gene expression data ,",
    "see @xcite and @xcite .",
    "@xcite and @xcite gave binary split type clustering methods for hdlss data .",
    "given this background , we decided to focus on high - dimensional structures of multiclass mixture models . in this paper , we consider asymptotic properties of pc scores for high - dimensional mixture models to apply to cluster analysis in hdlss settings .",
    "the main contribution of this paper is that we give theoretical reasons why pca is effective for clustering hdlss data .",
    "suppose there are independent and @xmath5-variate populations , @xmath6 , having an unknown mean vector @xmath7 and unknown covariance matrix @xmath8 for each @xmath9 .",
    "we do not assume @xmath10 .",
    "the eigen - decomposition of @xmath11 is given by @xmath12 , where @xmath13 having eigenvalues @xmath14 and @xmath15 is an orthogonal matrix of the corresponding eigenvectors .",
    "we consider a mixture model to classify a data set into @xmath16 groups .",
    "we assume that any sample is taken with mixing proportions @xmath17s from @xmath18s , where @xmath19 and @xmath20 but _ the label of the population is missing_. we assume that @xmath17s are independent of @xmath5 .",
    "we consider a mixture model whose probability density function ( or probability function ) is given by @xmath21 where @xmath22 and @xmath23 is a @xmath5-dimensional probability density function ( or probability function ) of @xmath18 having a mean vector @xmath24 and covariance matrix @xmath25 .",
    "suppose we have a @xmath26 data matrix @xmath27 , where @xmath28 , are independently taken from ( [ 1.1 ] ) .",
    "we assume @xmath29 .",
    "let @xmath30 and @xmath31 for @xmath32 , where @xmath33 denotes the number of elements in a set @xmath34 .",
    "we assume that @xmath0 and @xmath35s are independent of @xmath5 .",
    "let @xmath36 and @xmath37 be the mean vector and the covariance matrix of ( [ 1.1 ] ) .",
    "then , we have that @xmath38 and @xmath39 .",
    "we note that @xmath40 and @xmath41 for @xmath32 .",
    "we denote the eigen - decomposition of @xmath37 by @xmath42 , where @xmath43 having eigenvalues @xmath44 and @xmath45 is an orthogonal matrix of the corresponding eigenvectors .",
    "let @xmath46 for @xmath47 .",
    "then , @xmath48 is a sphered data vector from a distribution with the identity covariance matrix .",
    "the @xmath9th true pc score of @xmath49 is given by @xmath50 ( hereafter called @xmath51 ) .",
    "we note that @xmath52 for all @xmath53 .",
    "let @xmath54 and @xmath55 for @xmath56 , where @xmath57 denotes the euclidean norm .",
    "let @xmath58 .",
    "we note that @xmath59 when @xmath60 . since the sign of an eigenvector is arbitrary , we assume that @xmath61 for @xmath62 , without loss of generality .",
    "in addition , for the largest eigenvalue @xmath63s , we assume the following condition as necessary :    [ con1 ] @xmath64  as @xmath65 .",
    "we consider clustering @xmath66 into one of @xmath18s in hdlss situations .",
    "when @xmath60 , @xcite gave the following result : we denote the angle between two vectors @xmath67 and @xmath68 by @xmath69 .",
    "under condition 1 , it holds that as @xmath65 @xmath70 furthermore , for the normalized first pc score @xmath71 , it follows that @xmath72 -\\sqrt{\\varepsilon_1/\\varepsilon_2 } & \\mbox { when } { \\mbox{\\boldmath { $ x$}}}_j\\in \\pi_2   \\end{array } \\right .   \\label{1.3}\\end{aligned}\\ ] ] for @xmath47 . here , ` @xmath73 ' denotes the convergence in probability .",
    "one would be able to classify @xmath49s into two groups if @xmath74 is accurately estimated in hdlss situations .    in this paper , we consider asymptotic properties of sample pc scores for ( [ 1.1 ] ) in the hdlss context such as @xmath65 while @xmath0 is fixed . in section 2 , we first derive a geometric representation of hdlss data taken from the two - class mixture model . with the help of the geometric representation ,",
    "we give geometric consistency properties of sample pc scores in the hdlss context .",
    "we show that pca can classify hdlss data under certain conditions in a surprisingly explicit way . in section 3",
    ", we investigate asymptotic behaviors of true pc scores for the @xmath75-class mixture model and provide geometric consistency properties of sample pc scores when @xmath76 . in section 4 , we demonstrate the performance of clustering based on sample pc scores by using microarray data sets .",
    "we show that the real hdlss data sets hold the geometric consistency properties .",
    "the sample covariance matrix is given by @xmath77 , where @xmath78 and @xmath79 with @xmath80 .",
    "then , we define the @xmath81 dual sample covariance matrix by @xmath82 .",
    "we note that @xmath83 .",
    "let @xmath84 be the eigenvalues of @xmath85 .",
    "then , we define the eigen - decomposition of @xmath85 by @xmath86 , where @xmath87 denotes a unit eigenvector corresponding to @xmath88 .",
    "since the sign of @xmath89s is arbitrary , we assume @xmath90 for all @xmath9 without loss of generality , where @xmath91 is defined by @xmath92 .",
    "note that @xmath93 and @xmath85 share the non - zero eigenvalues .",
    "let @xmath94 for @xmath95 .",
    "we note that @xmath96 is an estimate of @xmath97 for @xmath95 from the facts that @xmath98 and @xmath99 if @xmath100 , where @xmath101 denotes a unit eigenvector of @xmath93 corresponding to @xmath88 .",
    "let @xmath102 and @xmath103 , where @xmath104 denotes the @xmath0-square identity matrix .",
    "we note that @xmath105 .",
    "we consider the sphericity condition : @xmath106 as @xmath65 .",
    "when one can assume that @xmath107 is gaussian or @xmath108 is @xmath4-mixing , @xcite and @xcite gave a geometric representation as follows : @xmath109    @xcite showed that ( [ 2.1 ] ) holds under the sphericity condition and @xmath110 as @xmath65 .    from ( [ 2.1 ] ) , we observe that the eigenvalue becomes deterministic as the dimension grows while the eigenvector of @xmath111 does not uniquely determine the direction . we note that ( [ 1.1 ] ) does not satisfy the assumption that @xmath107 is gaussian or @xmath112 is @xmath4-mixing .",
    "see section 4.1.1 in @xcite for details .",
    "we will find a geometric representation for ( [ 1.1 ] ) and the finding is completely different from ( [ 2.1 ] ) .",
    "we assume the following conditions :    [ con2 ] @xmath113  as @xmath65 .",
    "[ con3 ] @xmath114  as @xmath65 .",
    "[ con4 ] @xmath115  as @xmath65 for all @xmath56 .",
    "if @xmath18s are gaussian , it holds that @xmath116 for @xmath32 , so that condition 3 holds under condition 2 .",
    "on the other hand , condition 2 is stronger than condition 1 since @xmath117 for @xmath32 .",
    "we define @xmath118 according to @xmath119 for @xmath47 .",
    "the following result gives a geometric representation for ( [ 1.1 ] ) when @xmath60 .",
    "[ thm1 ] assume @xmath120 as @xmath65 . under conditions 2 to 4",
    ", it holds @xmath121 where @xmath122 .    from ( [ 2.2 ] ) , the first eigenvector of @xmath111 uniquely determines the direction .",
    "in fact , by noting @xmath123 , we have the following results for the first eigenvector and pc scores when @xmath60 . by using corollary [ cor1 ]",
    ", one can classify @xmath49s into two groups by the sign of @xmath124s :    [ cor1 ] under conditions 2 to 4 , it holds that for @xmath125 @xmath126 -\\sqrt{\\eta_1/\\eta_2 } & \\mbox { when } { \\mbox{\\boldmath { $ x$}}}_j\\in \\pi_2   \\end{array } \\right .",
    "\\mbox { for $ j=1, ... ,n$}.\\end{aligned}\\ ] ]    we considered an easy example such as @xmath127 , with @xmath128 , @xmath129 , @xmath130 and @xmath131 , where @xmath132 $ ] .",
    "we note that @xmath133 and @xmath134 but @xmath135 .",
    "then , conditions 2 to 4 hold .",
    "we set @xmath136 and @xmath137 .",
    "we took @xmath138 samples as @xmath139 and @xmath140 . in fig . 1",
    ", we displayed scatter plots of 20 independent pairs of @xmath141 when ( a ) @xmath142 , ( b ) @xmath143 , ( c ) @xmath144 and ( d ) @xmath145 .",
    "we denoted @xmath146 by the solid line and @xmath147 by the dotted line .",
    "we note that @xmath148 when @xmath149 .",
    "we observed that all the plots of @xmath150 gather on the surface of the orthogonal complement of @xmath151 .",
    "also , the plots appeared close to @xmath152 as @xmath5 increases . thus one can classify @xmath49s into two groups by the sign of @xmath124s . if one can not assume condition 3 or 4 , we recommend to estimate pc scores by using the cross - data - matrix methodology given by @xcite .",
    "see @xcite for the details .",
    "toy example to illustrate the geometric representation of @xmath150 on the unit sphere when @xmath60 and @xmath138 .",
    "we plotted 20 independent pairs of @xmath150 when @xmath139 and @xmath140 .",
    "the solid line denotes @xmath146 and the dotted line denotes @xmath147.,title=\"fig : \" ] + ( a ) @xmath142   ( b ) @xmath143 ( c ) @xmath144 ( d ) @xmath145",
    "we consider pc scores for the @xmath75-class mixture model .",
    "let @xmath154 and @xmath155 for @xmath32 .",
    "we assume the condition :    [ con5 ] @xmath156  and  @xmath157  as  @xmath65  for @xmath158 .",
    "we note that @xmath159 as @xmath65 under condition 5 .",
    "then , we have the following results .    [ thm2 ] under conditions 1 and 5",
    ", it holds that for @xmath160 @xmath161 \\sqrt { ( 1-\\varepsilon_{(i)})/\\{\\varepsilon_i(1-\\varepsilon_{(i-1)})\\ } } & \\mbox { when } { \\mbox{\\boldmath { $ x$}}}_j \\in   \\pi_i , \\\\[1 mm ] -\\sqrt { \\varepsilon_{i}/\\{(1-\\varepsilon_{(i)})(1-\\varepsilon_{(i-1)})\\ } } & \\mbox { when } { \\mbox{\\boldmath { $ x$}}}_j\\in \\bigcup_{m = i+1}^{k}\\pi_m .",
    "\\end{array } \\right .",
    "\\label{3.1}\\end{aligned}\\ ] ]    ( [ 1.3 ] ) is equivalent to ( [ 3.1 ] ) with @xmath60 and @xmath162 .",
    "[ cor2 ] under conditions 1 and 5 , it holds that for @xmath62 @xmath163    for example , when @xmath164 , from ( [ 3.1 ] ) we have that for @xmath47 @xmath165 -\\sqrt{\\varepsilon_1/(1-\\varepsilon_1 ) } & \\mbox { when } { \\mbox{\\boldmath { $ x$}}}_j\\notin \\pi_1   \\end{array } \\right . \\\\",
    "\\mbox{and}\\quad   & { \\mathop{\\rm plim}\\limits}_{d\\to \\infty } \\frac{s_{2j } } { \\lambda_2^{1/2}}= \\left\\ { \\begin{array}{ll } 0 & \\mbox { when } { \\mbox{\\boldmath { $ x$}}}_j\\in \\pi_1 ,   \\\\[1 mm ] \\sqrt { \\varepsilon_3/\\{\\varepsilon_2(1-\\varepsilon_1)\\ } } & \\mbox { when } { \\mbox{\\boldmath { $ x$}}}_j \\in   \\pi_2 , \\\\[1 mm ] -\\sqrt{\\varepsilon_2/\\{\\varepsilon_3(1-\\varepsilon_1)\\ } } & \\mbox { when } { \\mbox{\\boldmath { $ x$}}}_j \\in   \\pi_3 .",
    "\\end{array } \\right.\\end{aligned}\\ ] ] one can check whether @xmath166 or not by the first pc score . if @xmath167 , one can check whether @xmath168 or @xmath169 by the second pc score . in general",
    ", one can classify @xmath49s by using at most the first @xmath170 pc scores .",
    "we considered a toy example such as @xmath171 , where @xmath172 , @xmath173 whose first @xmath174 elements are @xmath175 , @xmath176 whose first @xmath177 elements are @xmath175 , and @xmath178 . here",
    ", @xmath179 denotes the ceiling function .",
    "we set @xmath130 , @xmath131 , @xmath180 and @xmath181 , where @xmath182 is defined in section 2.2 .",
    "then , conditions 1 and 5 hold .",
    "we first considered the case when @xmath183 , having @xmath184 .",
    "we set @xmath185 and @xmath186 . from theorem  [ thm2",
    "] one can expect that @xmath187 becomes close to @xmath188 when @xmath189 , @xmath190 when @xmath191 , and @xmath192 when @xmath193 . in fig .",
    "2 , we displayed scatter plots of @xmath194 , @xmath47 , when ( a ) @xmath195 , ( b ) @xmath196 and ( c ) @xmath197 .",
    "we observed that the scatter plots appear close to those three vertices as @xmath5 increases .    .",
    "we plotted @xmath194 which is denoted by small circles when @xmath189 , by small triangles when @xmath191 , and by small squares when @xmath193 .",
    "the dashed triangle consists of three vertices , @xmath188 , @xmath190 and @xmath192 , which are theoretical convergent points .",
    ", title=\"fig : \" ] + ( a ) @xmath195 ( b ) @xmath196 ( c ) @xmath197    next , we considered the case when @xmath198 , having @xmath199 .",
    "we set @xmath185 and @xmath200 . in fig .",
    "3 , we displayed scatter plots of @xmath201 , @xmath47 , when ( a ) @xmath195 , ( b ) @xmath196 and ( c ) @xmath197 . from theorem  [ thm2 ] , we displayed the triangular pyramid given by ( [ 3.1 ] ) with @xmath202 . as expected",
    "theoretically , we observed that the scatter plots appear close to four vertices of the triangular pyramid as @xmath5 increases .",
    "they seemed to converge slower in fig .",
    "3 than in fig .",
    "this is probably because the conditions of theorem  [ thm2 ] become strict as @xmath203 increases .    .",
    "we plotted @xmath201 .",
    "the dashed triangular pyramid was given by ( [ 3.1 ] ) with @xmath202 .",
    ", title=\"fig : \" ] + ( a ) @xmath195 ( b ) @xmath196 ( c ) @xmath197      let @xmath204 and @xmath205 for @xmath32 .",
    "we assume the condition :    @xmath206  as  @xmath65 .",
    "as for the estimated pc scores , we have the following result . from theorem [ thm3 ] , one can classify @xmath49s into @xmath203 groups by the elements of @xmath207 :    [ thm3 ] under conditions 2 to 6 , it holds that for @xmath208 , @xmath32 @xmath209 \\sqrt{(1-\\eta_{(i)})/\\{\\eta_i(1-\\eta_{(i-1)})\\ } } & \\mbox { when } { \\mbox{\\boldmath { $ x$}}}_j \\in   \\pi_i , \\\\[1 mm ] -\\sqrt{\\eta_{i}/\\{(1-\\eta_{(i)})(1-\\eta_{(i-1)})\\ } } & \\mbox { when } { \\mbox{\\boldmath { $ x$}}}_j \\in \\bigcup_{m = i+1}^{k}\\pi_m   \\end{array } \\right .",
    "\\label{3.2}\\end{aligned}\\ ] ] for @xmath160 .",
    "we analyzed gene expression data by @xcite in which the data set consists of @xmath210 genes and @xmath211 samples .",
    "the data set has two tumor cellular subtypes , @xmath212 b - cell ( 95 samples ) and @xmath213 t - cell ( 33 samples ) .",
    "refer to @xcite as well .",
    "we considered three cases : ( a ) @xmath214 samples consist of the first 5 samples both from @xmath215 and @xmath216 ( i.e. @xmath217 and @xmath218 ) ; ( b ) @xmath219 samples consist of the first 20 samples both from @xmath215 and @xmath216 ( i.e. @xmath220 and @xmath221 ) ; and ( c ) @xmath222 samples consist of @xmath223 samples from @xmath215 and @xmath224 samples from @xmath216 . in the top panels of fig .",
    "4 , we displayed scatter plots of the first two pc scores , @xmath225s , for ( a ) , ( b ) and ( c ) . from corollary  [ cor1 ] , we denoted @xmath226 and @xmath227 by dotted lines . for ( a )",
    ", we observed that the estimated pc scores give good performances .",
    "the first pc scores gathered around @xmath226 or @xmath227 . for ( b ) , the estimated pc scores gave adequate performances except for the two points from @xmath216 .",
    "those two samples , which are the ninth and twentieth samples of @xmath216 , are probably outliers .",
    "in fact , the two points are far from the cluster of @xmath216 .",
    "the other @xmath228 samples were perfectly classified into the two groups by the sign of the first pc scores . as for ( c ) , although there seemed to be two clusters except for the two samples , we could not classify the data set by the sign of the first pc scores .",
    "this is probably because @xmath229 and @xmath230 are unbalanced and @xmath0 is large . from ( [ 1.2 ] ) , when the mixing proportions are unbalanced , @xmath231 becomes small .",
    "the first eigenspace was possibly affected by the other eigenspaces so that the first pc scores appear in the wrong direction .",
    "we tested the clustering except for the outlying two samples .",
    "we used the remaining @xmath232 samples for @xmath216 .",
    "we considered three cases for samples from @xmath215 : ( d ) the first @xmath233 samples from @xmath215 , so that @xmath234 and @xmath235 ; ( e ) the first @xmath232 samples from @xmath215 , so that @xmath236 and @xmath237 ; and ( f ) the first @xmath238 samples from @xmath215 , so that @xmath239 and @xmath240 . in the bottom panels of fig .",
    "4 , we displayed scatter plots of @xmath225s for ( d ) , ( e ) and ( f ) . for ( d ) and ( e ) , we observed that the estimated pc scores give good performances . as for ( f ) , although there seemed to be two clusters , we could not classify the data set by the sign of the first pc scores . @xmath229 and @xmath230 are unbalanced in ( d ) and ( f ) .",
    "even though ( d ) is an unbalanced case , the estimated pc scores worked well for the case .",
    "we had an estimate of the ratio of the first eigenvalues , @xmath241 , as @xmath242 by the noise - reduction methodology given by @xcite .",
    "the first eigenspace of @xmath37 in ( d ) is less affected by the first eigenspace of @xmath25s than in ( f ) since @xmath243 .",
    "this is probably the reason why the estimated pc scores gave good performances even in ( d ) .     in the data",
    "set of @xcite .",
    "we denoted them by small circles when @xmath189 and by small triangles when @xmath191 .",
    "the theoretical convergent points , @xmath226 and @xmath227 , are denoted by dotted lines .",
    "the two samples , encircled by dots in ( b ) and ( c ) , are probably outliers .",
    ", title=\"fig : \" ] + ( a ) @xmath244 ( b ) @xmath245 ( c ) @xmath246 +   in the data set of @xcite .",
    "we denoted them by small circles when @xmath189 and by small triangles when @xmath191 .",
    "the theoretical convergent points , @xmath226 and @xmath227 , are denoted by dotted lines .",
    "the two samples , encircled by dots in ( b ) and ( c ) , are probably outliers . , title=\"fig : \" ] + ( d ) @xmath247 ( e ) @xmath248 ( f ) @xmath249      we analyzed gene expression data by @xcite in which the data set consists of five brain tumor types . however , we only used @xmath250 classes given in the cran r package ` rda ' in which the data set consists of @xmath251 genes and @xmath252 samples .",
    "we set the four tumor types as @xmath212 medulloblastomas ( 10 samples ) , @xmath213 malignant gliomas ( 10 samples ) , @xmath253 normal cerebellums ( 4 samples ) and @xmath254 at / rt ( 10 samples ) .",
    "we first considered the case when @xmath183 , so that @xmath255 and @xmath256 . in the left panel of fig . 5",
    ", we displayed scatter plots of the first two pc scores , @xmath225s . from theorem  [ thm3 ] , we displayed the triangle given by ( [ 3.2 ] ) with @xmath164 .",
    "although there seemed to be three clusters , we could not observe that they gather around each vertex .",
    "this is probably because the rate of convergence is slow because of small @xmath5 compared to such large @xmath0 when @xmath76 .",
    "we tested the clustering with a small sample size : the first @xmath257 samples both from @xmath215 and @xmath216 and the last @xmath258 samples from @xmath259 , so that @xmath260 and @xmath261 .",
    "we displayed the results in the right panel of fig .",
    "5 . they seemed to be classified into three classes around each vertex .     in the data",
    "set of @xcite .",
    "we denoted them by small circles when @xmath189 , by small triangles when @xmath191 and by small squares when @xmath193 .",
    "the theoretical convergent points are denoted by the vertices of the triangle.,title=\"fig : \" ] + ( i ) @xmath262 ( ii ) @xmath263 +    next , we considered the case when @xmath198 , so that @xmath264 and @xmath265 . in fig .",
    "6 , we displayed scatter plots of the first three pc scores .",
    "although there seemed to be four clusters of each @xmath18 , the data set seemed not to hold the consistency property given by ( [ 3.2 ] ) in theorem  [ thm3 ] .",
    "this is probably because some of conditions 2 to 6 in theorem  [ thm3 ] are not met because of such large @xmath203 .     in the data",
    "set of @xcite.,title=\"fig : \" ] + ( i ) @xmath225 ( ii ) @xmath266 ( iii ) @xmath267      we analyzed gene expression data by @xcite in which the data set consists of three leukemia subtypes having @xmath268 genes .",
    "we used @xmath258 classes such as @xmath215 : acute lymphoblastic leukemia ( @xmath269 samples ) and @xmath216 : mixed - lineage leukemia ( @xmath270 samples ) , so that @xmath271 and @xmath272 . in fig .",
    "7 , we displayed scatter plots of the first three pc scores .",
    "in the data set of @xcite.,title=\"fig : \" ] + ( i ) @xmath225 ( ii ) @xmath266 ( iii ) @xmath273    we observed that the data set is perfectly separated by the sign of the second pc scores .",
    "this figure looks completely different from fig .",
    "this is probably because the largest eigenvalue , @xmath274 or @xmath275 , is too large .",
    "when @xmath60 , we give the following result to explain the reason of the phenomenon in fig . 7 . under the assumptions of proposition  [ pro1 ] , one can classify @xmath49s into two groups by some @xmath9-th pc score even when condition 1 is not met :    [ pro1 ] assume @xmath276 as @xmath65 .",
    "then , there exists some positive integer @xmath277 such that @xmath278 furthermore , assume that @xmath279 is distinct in the sense that @xmath280 for @xmath281 .",
    "then , if @xmath282 , it holds that angle@xmath283 as @xmath65 and for @xmath47 @xmath284 -\\sqrt{\\varepsilon_1/\\varepsilon_2 } & \\mbox { when } { \\mbox{\\boldmath { $ x$}}}_j\\in \\pi_2 .",
    "\\end{array } \\right .",
    "\\notag \\end{aligned}\\ ] ]    we estimated the largest eigenvalue by using the noise - reduction methodology given by @xcite .",
    "we estimated @xmath285 by using an unbiased estimator given by @xcite .",
    "then , we obtained the estimates of @xmath286 as @xmath287 , so that condition 1 is not met obviously .",
    "in addition , by estimating @xmath17s by @xmath288s , we had @xmath289 .",
    "thus , the first eigenspace of @xmath37 is probably the first eigenspace of @xmath290 since @xmath243 . we conclude that @xmath277 in proposition  [ pro1 ] must be @xmath258",
    "this is the reason why the data set can be separated by the sign of the second pc scores in fig 7 .",
    "in this paper , we considered the mixture model by ( [ 1.1 ] ) in the hdlss context such as @xmath65 while @xmath0 is fixed .",
    "we studied asymptotic properties both of the true pc scores and the sample pc scores for the mixture model .",
    "we gave theoretical reasons why pca is effective for clustering hdlss data and we showed that hdlss data can be classified by the sign of the first several pc scores theoretically .",
    "however , we have to say , in actual hdlss data analyses , one may encounter cases such as in figs .",
    "4(c ) and 7 where the data set is not always classified by the sign of the first several pc scores .",
    "several reasons should be considered : ( i ) actual hdlss data sets often include several outliers ; ( ii ) the regularity conditions are not met ; and ( iii ) @xmath5 is not sufficiently large .",
    "thus , we recommend the following three steps : ( i ) apply pca to hdlss data ; ( ii ) by using pc scores , map the data set onto a feature space such as the first three eigenspaces ; and ( iii ) apply general clustering methods such as the @xmath203-means method to the feature space .",
    "we are now investigating the theory further and hope to bring it closer to the results of actual analysis .",
    "research of the first author was partially supported by grant - in - aid for young scientists ( b ) , japan society for the promotion of science ( jsps ) , under contract number 26800078 .",
    "research of the second author was partially supported by grants - in - aid for scientific research ( b ) and challenging exploratory research , jsps , under contract numbers 22300094 and 26540010 .",
    "throughout , let @xmath291 , where @xmath292   \\sqrt{(1-\\eta_{(i)})/\\{n\\eta_i(1-\\eta_{(i-1)})\\ } } & \\mbox { when } { \\mbox{\\boldmath { $ x$}}}_j \\in   \\pi_i , \\\\[1 mm ] -\\sqrt{\\eta_{i}/\\{n(1-\\eta_{(i)})(1-\\eta_{(i-1 ) } ) \\ } } & \\mbox { when } { \\mbox{\\boldmath { $ x$}}}_j\\in \\bigcup_{m = i+1}^{k}\\pi_m   \\end{array } \\right.\\ ] ] for @xmath160 .",
    "let @xmath293 for @xmath32 .",
    "let @xmath294 , where @xmath295 according to @xmath296 for @xmath47 .",
    "note that @xmath297 .",
    "we define the eigen - decomposition of @xmath298 by @xmath299 from the fact that rank@xmath300 , where @xmath301 are eigenvalues of @xmath298 and @xmath302 is a unit eigenvector corresponding to @xmath303 for each @xmath9 .",
    "we assume @xmath304 for @xmath62 , without loss of generality .",
    "let @xmath306 .",
    "then , we can write that @xmath307 for @xmath308 . from the fact that @xmath309 , we have that @xmath310 as @xmath65 for @xmath308 under condition 2 . also , we have that @xmath311 for all @xmath312 and @xmath313 under condition 2 .",
    "then , by using chebyshev s inequality , for any @xmath314 , under condition 2 , it holds that for all @xmath312 and @xmath313 @xmath315 so that @xmath316 and @xmath317 when @xmath318 and @xmath319 ( @xmath312 ) .",
    "we note that @xmath320 .",
    "similar to ( [ 1 ] ) , under condition 3 , it holds that @xmath321 when @xmath318 for @xmath322 @xmath323 . by noting that @xmath324 under condition 4 , we have that @xmath325 under conditions 2 to 4 . by noting that @xmath326 and @xmath327 from @xmath328",
    ", we conclude the result .",
    "let @xmath332 be an arbitrary unit vector .",
    "since @xmath333 @xmath334 , it holds that as @xmath65 @xmath335 under condition 1 .",
    "note that @xmath336 for @xmath56 .",
    "thus it holds that @xmath337 from the fact that @xmath338 , by combining ( [ 2 ] ) with ( [ 3 ] ) , under conditions 1 and 5 , we have that @xmath339 hence , from the assumption that @xmath340 , it holds that @xmath341 .",
    "next , we consider @xmath342 and @xmath343 .",
    "note that @xmath344 and @xmath345 for @xmath158 under condition 5 .",
    "then , under conditions 1 and 5 , it holds that for @xmath346 @xmath347 from ( [ 2])-([3 ] ) and @xmath348 , so that for @xmath346 @xmath349 by combining ( [ 2 ] ) with ( [ 3 ] ) and ( [ 4 ] ) , we have that@xmath350 under conditions 1 and 5 .",
    "hence , from the assumption that @xmath351 , it holds that @xmath352 .",
    "next , we consider @xmath353 and @xmath354 .",
    "note that @xmath355 for @xmath356 from @xmath352 .",
    "then , under conditions 1 and 5 , we have that for @xmath356 @xmath357 from ( [ 2])-([4 ] ) , @xmath348 , @xmath358 and @xmath359 .",
    "then , by combining ( [ 6 ] ) and ( [ 7 ] ) , under conditions 1 and 5 , it holds that for @xmath356 @xmath360 similar to ( [ 5 ] ) , by combining ( [ 2 ] ) with ( [ 3 ] ) and ( [ 8 ] ) , under conditions 1 and 5 , we have that @xmath361 so that @xmath362 from the assumption that @xmath363 .    in a way similar to @xmath353 and @xmath354 , as for @xmath364 and @xmath365 @xmath366",
    ", we have that @xmath367 , @xmath368 and @xmath369 together with @xmath370 for @xmath371 @xmath372 under conditions 1 and 5 .",
    "it concludes the results .",
    "[ lem3 ] under conditions 1 and 5 , it holds that for @xmath62 @xmath373 \\sqrt{(1-\\varepsilon_{(i)})/\\{\\varepsilon_{i}(1-\\varepsilon_{(i-1)})\\ } } & \\mbox{when } i ' = i , \\\\[1 mm ] -\\sqrt{\\varepsilon_{i}/\\{(1-\\varepsilon_{(i)})(1-\\varepsilon_{(i-1)})\\ } } & \\mbox{when } i ' > i.   \\end{array}\\right.\\end{aligned}\\ ] ]    we write that @xmath374 by using lemma  [ lem2 ] , under conditions 1 and 5 , we have that as @xmath65 @xmath375 from ( [ 9 ] ) . also , by using lemma  [ lem2 ] , under conditions 1 and 5 , we have that for @xmath376 @xmath377 thus , from lemma  [ lem2 ] , we can conclude the results .",
    "we have that @xmath380 as @xmath65 for @xmath322 @xmath381 @xmath382 , under condition 6 .",
    "also , from the fact that @xmath383 , we have that @xmath384 for @xmath322 @xmath385 under condition 2 .",
    "then , similar to ( [ 1 ] ) , under conditions 2 and 6 , it holds that @xmath386 when @xmath387 for @xmath388 .",
    "in addition , under conditions 2 and 3 , we can claim that @xmath389 and @xmath390 when @xmath391 and @xmath392 for all @xmath393 and @xmath394 . here , we write that @xmath395 for @xmath47 ; @xmath32 , where @xmath396 .",
    "then , by noting ( [ 9 ] ) with @xmath397 and @xmath398 , @xmath32 , under conditions 2 , 3 and 6 , we have that @xmath399 when @xmath391 and @xmath392 for all @xmath393 and @xmath394 .",
    "thus , under conditions 2 , 3 , 4 and 6 , it holds that @xmath400 let @xmath401 be an arbitrary random unit vector such that @xmath402 .",
    "we note that @xmath403 .",
    "then , by noting @xmath404 , under ( [ 10 ] ) , conditions 2 , 3 , 4 and 6 , we have that @xmath405 from ( [ 11 ] ) .",
    "we note that @xmath406 for @xmath62 in case of rank@xmath407 . also , we note that @xmath408 , are distinct under condition 5 and ( [ 10 ] ) for a sufficiently large @xmath5 . thus , if @xmath409 for @xmath62 , we have that @xmath410 for @xmath62 .",
    "it concludes the result .      by noting ( [ 9 ] ) with @xmath397 and @xmath398 , @xmath32 ,",
    "we can write that @xmath413 we have the eigen - decomposition of @xmath414 by @xmath415 , where @xmath416 is a unit eigenvector corresponding to @xmath303 for each @xmath9 .",
    "we note that @xmath417 for @xmath411 .",
    "then , by noting lemmas  [ lem2]-[lem3 ] and the fact that ( [ 13 ] ) is same as ( [ 3 ] ) with @xmath418 , under condition 5 , we have that for @xmath62 @xmath419 if @xmath420 .",
    "we note that @xmath421 from the fact that @xmath422 for @xmath62 .",
    "hence , we can conclude the result .",
    "next , we consider the proof of corollary  1 . from the fact that @xmath424 , it holds that @xmath425 when @xmath426 , so that @xmath427",
    ". also , note that @xmath123 .",
    "then , by using lemma  [ lem1 ] , under conditions 2 to 4 , it holds that @xmath428 as @xmath65 .",
    "hence , from ( 3 ) and the assumption that @xmath429 , we have that @xmath430 as @xmath65 for @xmath208 , @xmath323 . in view of the elements of @xmath152",
    ", we can conclude the result of corollary  1 .",
    "we write that @xmath431 for @xmath47 ; @xmath32 .",
    "we note that @xmath432 as @xmath65 under condition 1 for @xmath433 , where @xmath332 is an arbitrary unit vector .",
    "then , under condition 1 , when @xmath119 , it holds that as @xmath65 @xmath434 then , by using lemmas  [ lem2 ] and [ lem3 ] , we can conclude the result of theorem  2 .",
    "let @xmath435 .",
    "then , we define the eigen - decomposition of @xmath436 by @xmath437 , where @xmath438 are eigenvalues of @xmath436 and @xmath439 is a unit eigenvector corresponding to @xmath440 for each @xmath9 .",
    "let @xmath441 .",
    "then , from @xmath442 , under @xmath443 as @xmath65 , it holds that @xmath444 as @xmath65 , so that @xmath445 where @xmath446 .",
    "let @xmath447 for @xmath448 .",
    "for a sufficiently large @xmath5 , when @xmath449 , there exists some positive integer @xmath450 such that @xmath451 .",
    "then , from ( [ 14 ] ) , we have that @xmath452 , so that @xmath453 with @xmath454 .",
    "when @xmath455 for a sufficiently large @xmath5 , it holds that @xmath453 with @xmath456 .",
    "in addition , under @xmath457 for @xmath281 , it holds that @xmath458 from @xmath459 .",
    "then , from the fact that @xmath460 as @xmath65 for @xmath323 , in a way similar to ( [ 1 ] ) , we have that @xmath461 when @xmath119 for @xmath308",
    ". we can conclude the results .",
    "ahn , j. , lee , m.  h. and yoon , y.  j. ( 2012 ) clustering high dimension , low sample size data using the maximal data piling distance . _ statist .",
    "_ , * 22 * , 443464 .",
    "aoshima , m. and yata , k. ( 2014 ) a distance - based , misclassification rate adjusted classifier for multiclass , high - dimensional data . _",
    "_ , * 66 * , 9831010 .",
    "armstrong , s.  a. , staunton , j.  e. , silverman , l.  b. , pieters , r. den  boer , m.  l. , minden , m.  d. , sallan , s.  e. , lander , e.  s. , golub , t.  r. and korsmeyer , s.  j. ( 2002 ) mll translocations specify a distinct gene expression profile that distinguishes a unique leukemia .",
    "_ nature genetics _",
    ", * 30 * , 4147 .",
    "chiaretti , s. , li , x. , gentleman , r. , vitale , a. , vignetti , m. , mandelli , f. , ritz , j. and foa , r. ( 2004 ) gene expression profile of adult t - cell acute lymphocytic leukemia identifies distinct subsets of patients with different response to therapy and survival .",
    "_ blood _ , * 103 * , 27712778 .",
    "pomeroy , s.  l. , tamayo , p. , gaasenbeek , m. , sturla , l.  m. , angelo , m. , mclaughlin , m.  e. , kim , j.  y. , goumnerova , l.  c. , black , p.  m. , lau , c. et al .",
    "( 2002 ) prediction of central nervous system embryonal tumour outcome based on gene expression .",
    "_ nature _ , * 415 * , 436442 ."
  ],
  "abstract_text": [
    "<S> in this paper , we consider clustering based on principal component analysis ( pca ) for high - dimension , low - sample - size ( hdlss ) data . </S>",
    "<S> we give theoretical reasons why pca is effective for clustering hdlss data . </S>",
    "<S> first , we derive a geometric representation of hdlss data taken from a two - class mixture model . with the help of the geometric representation , </S>",
    "<S> we give geometric consistency properties of sample principal component scores in the hdlss context . </S>",
    "<S> we develop ideas of the geometric representation and geometric consistency properties to multiclass mixture models . </S>",
    "<S> we show that pca can classify hdlss data under certain conditions in a surprisingly explicit way . </S>",
    "<S> finally , we demonstrate the performance of the clustering by using microarray data sets . </S>",
    "<S> +   + * keywords : * clustering ; consistency ; geometric representation ; hdlss ; microarray ; pc score    0.5 cm * kazuyoshi yata and makoto aoshima * + institute of mathematics , university of tsukuba , ibaraki , japan + </S>"
  ]
}