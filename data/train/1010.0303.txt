{
  "article_text": [
    "fisher introduced the concept of likelihood in 1921 for inferences from statistical models involving two kinds of objects , namely observed random variables ( data ) and unknown fixed parameters .",
    "pearson ( @xcite ) points out a limitation of fisher likelihood for the prediction of unobserved future observations .",
    "fisher s likelihood can not be used to make inferences about .",
    "there has been an effort to extend likelihood inferences to models with unobservables by eliminating them via integration .",
    "however , with a few exceptions such as the copula ( joe , @xcite ) , marginal distributions for counts and proportions are not available in explicit forms , and this restricts the scope of the classical likelihood approach .    in longitudinal studies , generalized estimating equations ( gees )",
    "are widely used .",
    "they give an estimation method for regression coefficients constructed directly to describe marginal means with the covariance structure regarded as contributing nuisance parameters only .",
    "however , gees can not ( generally ) be integrated to obtain a likelihood function ( mccullagh and nelder , @xcite ) and therefore may not have a probabilistic or likelihood basis . these estimation methods for marginal ( or population - average ) means",
    "are often contrasted with conditional ( or subject - specific ) models which include the modeling of unobservables .",
    "jansen  et  al .",
    "( @xcite ) review the use of gee methods and conditional models for analysis of missing data and discuss the choice between them .",
    "however , we believe that such a choice is inappropriate because the choice of an estimation method for a particular parameterization ( marginal parameter ) should not pre - empt the process of model selection . recently",
    ", lee and nelder ( @xcite ) have shown that alleged differences in the behavior of parameters between gee methods and conditional models are based on a failure to compare like with like .",
    "we dislike the use of estimation methods without a probabilistic basis because , for example , inferences for joint and conditional probabilities are not possible .    recently , broad classes of new probabilistic models with unobserved random variables ( unobservables ) have been proposed , such as generalized linear models ( glms ) with random effects ( lee and nelder , @xcite ) , latent processes ( skrondal and rabe  hesketh , @xcite ) , models for missing data ( little and rubin , @xcite ) , prediction ( bjrnstad , @xcite ) and for potential outcomes in causality ( rubin , @xcite ) , etc . in the statistical literature",
    "unobservables appear with various names such as random effects , latent processes , factor , missing data , unobserved future observations , potential outcomes , and so on .",
    "random effects in the mean model have been proposed to account for within - subject correlation in longitudinal studies ( @xcite , @xcite ) ( for smooth spatial data , see besag and higdon , @xcite ; for spline - type function fitting , see eilers and marx , @xcite ; and for factor analysis , see bartholomew , @xcite , etc . ) while random effects in the dispersion model ( lee and nelder , @xcite ) can account for heteroskedasticity , giving heavy - tailed distributions that allow robust modeling ( noh and lee , @xcite ) .",
    "modeling of unobservables is the key to these new models .",
    "however , because of difficulties in making likelihood inferences about unobservables , some authors use the fisher likelihood for inferences about fixed unknown parameters while for inferences about unobservables they use the empirical bayesian ( eb ) approach or the full bayesian ( fb ) inference .",
    "recently , zhao et al .",
    "( @xcite ) have used an fb approach with which they claim to have an advantage over the frequentist version ( eb ) in that it is _ computationally simpler _ to obtain variance estimates of the random - effect estimates .",
    "( note that the word `` prediction '' has often been used to denote the estimation of random effects .",
    "however , we believe that it is clearer to use _ prediction _ when we estimate future observations ( unobservables ) and _ estimation _ for the estimation of random effects in the data already observed . )",
    "discussing the controversy between fisher and neyman , rubin ( @xcite ) maintains that models with unobservables arose most naturally in causal inference within an fb framework . from lindley and smith ( @xcite ) onwards , fb has become dominant for the analysis of models with unobservables .",
    "the availability of markov - chain monte carlo , which implements fb procedures , has made fb inferences popular .    by contrast",
    "we believe that modeling of unobservables is natural within an extended likelihood framework .",
    "recently , for general inferences from models involving unobservables , lee and nelder ( @xcite ) propose the use of the hierarchical ( or h-)likelihood .",
    "the h - likelihood plays a key role in the synthesis of the likelihood inferential tools needed for a broad class of new models having unobservables .",
    "the h - likelihood approach takes into account the uncertainty in the estimation of random effects , so that inferences about unobservables are possible without resorting to an eb framework .    in the next section",
    "we review some models with unobservables and discuss related modeling issues .",
    "we review the h - likelihood procedure for the estimation of random effects and compare them with the bayesian approach in section  [ sec3 ] ; likelihood inferences from such models are demonstrated with examples in section  [ sec4 ] , followed by conclusions in section [ sec5 ] .",
    "multivariate distributions for non - gaussian models can be produced by probabilistic modeling of unobservables without requiring explicit multivariate generalizations of non - gaussian distributions . using hierarchical likelihood , inferences from these new classes can be made .",
    "hglms allow a synthesis of glms , random - effect models and structured - dispersion models .",
    "consider a glm with random effects where the response @xmath0 follows the glm , conditioning on random effects @xmath1 , @xmath2 with a linear predictor , @xmath3 where @xmath4 for some monotonic function @xmath5 .",
    "when @xmath1 are normal the models are called generalized linear mixed models ( glmms ) . the use of other distributions for the random effects enriches the class of models .",
    "lee and nelder ( @xcite ) introduce hglms in which the distribution of the random components is extended to an arbitrary conjugate distribution of a glm family with an appropriate link , not necessarily that of the conjugate pair . above we suppress the indices to mean that our discussion covers various models having single or multiple random effects with nested , crossed , combined structures , etc .",
    "we write indices if necessary .        to allow various patterned associations among random effects lee and nelder ( @xcite ) propose adding an additional feature to hglms as follows : let @xmath6 with @xmath7 being random effects with a diagonal covariance matrix @xmath8 to give @xmath9 the last equation can be a spectral decomposition with an orthogonal matrix @xmath10 or a choleski decomposition with upper or lower triangular matrix @xmath11 zhao et al .",
    "( @xcite ) note that the full generality of the glmm requires using general design matrices for both fixed and random components . with",
    "fixed @xmath12 not depending upon unknown parameters , we have models for longitudinal studies , intrinsic autoregressive models , various spline models , etc .",
    "with parameter - dependent @xmath13 we have random - slope models , autoregressive models , antedependence models , markov - random - field models , and so on ( lee and nelder , @xcite ) .",
    "these models are also able to handle a great range of complications in regression - type analysis , for instance , within - subject correlation in longitudinal data , scatterplot smoothing , generalized additive models , kriging , function estimation and non - parametric regression models such as generalized additive models and varying - coefficient models ( zhao et al . , @xcite ) .",
    "example 1 : consider the model from item - response theory ( irt ) such that @xmath14 where @xmath15 is the intrinsic difficulty of the @xmath16th item , and @xmath17 is the @xmath18th subject s ability for the @xmath16th item . if @xmath19 with @xmath20 it becomes a one - parameter irt model ( rasch , @xcite )",
    ". an appealing feature of this model is that items and subjects ( examinees ) can be placed on a common scale .",
    "differences in both difficulty between items and ability of subjects is assumed to remain the same . in this model , for a given item , the probability of a correct response increases monotonically with ability as in figure [ fig1 ] .",
    "if @xmath21 with @xmath22 and @xmath23 fixed unknown , we have a two - parameter irt model .",
    "let @xmath24 and @xmath25 giving @xmath26 where @xmath27 is a one - by - one matrix .",
    "this model allows for correlations among items for each subject . in this model @xmath28",
    "is called the discriminant parameter and @xmath29 the difficulty parameter ( skrondal and rabe - hesketh , @xcite ) .",
    "this two - parameter irt model may lack the monotonicity property in that one item can be easier than another for one subject , while being more difficult for another ; this is described by the item - subject interaction @xmath30 .",
    "this example shows how a particular modeling of the ( singular ) covariance matrix @xmath31 can give an interesting interpretation of the parameters .",
    "example 2 : when @xmath32 with @xmath33 we have autoregressive random effects of order 1 .",
    "when @xmath34 we have the random - walk model which gives a singular precision matrix . this random - walk model for temporal correlation",
    "has been extended to spatially - correlated models via intrinsic autoregressive models with a singular fixed - precision matrix ( besag and kooperberg , @xcite ) .",
    "splines can be viewed as smoothing via random effects which also have a singular fixed - precision matrix ( green and silverman , @xcite ) .",
    "example 3 : skrondal and rabe - hesketh ( @xcite ) propose generalized linear latent and mixed models ( gllamms ) as a means of unifying factor models , linear structural - relations models and covariate measurement - error models .",
    "they point out thatgllamms consist of two building blocks , a response model and a structural model .",
    "for the responsemodel , they use the hglm shown in equation ( [ eq : hglm ] ) . for the structural model ,",
    "the random effect itself satisfies a regression model of the form @xmath35 where @xmath36 is a matrix of structural parameters relating the latent dependent variables to each other , @xmath37 is a matrix of structural parameters relating the latent dependent variable to the latent explanatory variables and @xmath7 is a vector of disturbances . from this",
    "we have @xmath38 thus , the gllamms can be represented as anhglm with two random components , @xmath39 where @xmath40 and @xmath41 ingllamms the parametrization using @xmath36 , @xmath37 , @xmath42 and @xmath43 gives a useful interpretation .",
    "another class of widely used models with unobservables is nonlinear mixed - effect models in population pharmacokinetics and pharmacodynamics , models for missing data and models for potential outcomes .",
    "lee and nelder ( @xcite ) introduce double hglms ( dhglms ) which allow random effects for the dispersion .",
    "this gives a systematic way of generating heavy - tailed distributions for various types of data such as counts , proportions , and so on .",
    "random effects in the mean affect the first two cumulants of the distribution of responses while those in the dispersion affect the third and fourth cumulants , so that by allowing random effects in both mean and dispersion we can generate models with various patterns in the first four cumulants . castillo and lee ( @xcite ) show that dhglms provide a general treatment of levy - process models in financial modeling while noh and lee ( @xcite ) show that this new class allows robust modeling of glm classes with bounded influence .",
    "yun and lee ( @xcite ) show how to model abrupt changes in the behavior of schizophrenics .",
    "glidden and liang ( @xcite ) show that sensitivity of estimators for @xmath44 from hglms become more serious when the data form a selected sample . however , noh et al .",
    "( @xcite ) show that by using a heavy - tailed distribution for the random effects , such a sensitivity in the estimators can be avoided .      without introducing random effects",
    "the gee can be used to obtain maximum likelihood ( ml ) estimators when responses are normal .",
    "estimates of regression coefficients from gees have been claimed to be consistent under various model misspecifications .",
    "it is often called the population - averaged model ( zeger et al . , @xcite ) or the marginal model ( jansen et al . ,",
    "@xcite ) for a particular parameterization [ regression coefficients for marginal means e(@xmath45 . for correlated non - normal responses , given a gee @xmath46 ( let us say ) , the mixed derivatives may not be the same ( mccullagh and nelder , @xcite , page 337 ) , that is , @xmath47 if so there is no probabilistic model leading to the gee @xmath48 .",
    "without such a basis the claim of consistency is meaningless ( for more discussion see crowder , @xcite and chaganty and joe , @xcite ) .",
    "it is of interest to study the class of marginal models , allowing estimating equations .",
    "various marginal models have been proposed by molenberghs andlesaffre ( @xcite ) , molenberghs et al .",
    "( @xcite ) and heagerty and zeger ( @xcite ) .",
    "heagerty and zeger ( @xcite ) claimed that the parameter estimates from theirmarginal models were less sensitive to the misspecification of the distribution of random effects .",
    "lee and nelder ( @xcite ) show that if one compares like with like the differences between the results from the two models are not great .",
    "all that we can say is that certain parameterizations are less sensitive under certain probabilistic models so that it could be recommended to use such a parameterization if it also met scientific requirements . for further controversies on parameterizations see lindsey and lambert ( @xcite ) .",
    "gee is an estimating _ method _ , not a model .",
    "thus we do not believe that a useful comparison can be made between a probabilistic model such as a hglm and an estimating method such as gee .",
    "we see the analysis of data as consisting of three main activities : the first two are model fitting and model checking which aim to find parsimonious well - fitting models , and together comprise model selection ; the third is model prediction , where parameter estimates from selected models are used to predict quantities of interest and their uncertainties . in our view , inferences about margins and individual subjects responses and a choice of an estimation method such as the gee , ml , etc .",
    ", both belong to the prediction phase of the analysis .",
    "in this paper we shall not consider gee further because the method does not allow inferences about unobservables .",
    "besides the observed data and fixed unknown parameters in fisher likelihood , an additional type of object , namely unobservable random variables @xmath49 is often of interest in making statistical inferences .",
    "example 4 : suppose that we have the number of epileptic seizures in an individual for five weeks , @xmath50 .",
    "suppose that these counts are i.i.d . from a poisson distribution with mean @xmath51",
    "now we want to have a predictive probability function for the seizure counts for the next week @xmath52 . here , @xmath53 @xmath54",
    "so that the plug - in technique gives the predictive distribution for the seizure count @xmath1 of the next week @xmath55 pearson ( @xcite ) points out the limitation of fisher likelihood using the plug - in method because it can not account for uncertainty in estimating @xmath56    example 5 : suppose that the data @xmath57 are collected from the statistical model @xmath58 suppose also that some of the intended observations in @xmath57 are unobservable because they are missing .",
    "we write @xmath59 for @xmath60 the observed and @xmath61 for the missing components .",
    "let @xmath7 be missing data indicators such that @xmath62 this leads to a probability function @xmath63 here @xmath64 are the observed data , and @xmath61 are the unobservables .    from these models , likelihood inferences can be made using the h - likelihood defined by @xmath65\\\\[-8pt ] & = & \\log f_{\\theta } ( y , v)=m+\\log f_{\\theta}(v|y)\\nonumber\\end{aligned}\\ ] ] where @xmath66 is the marginal log - likelihood @xmath67 with @xmath68 this is the ( log ) h - likelihood which plays the same role as thelog - likelihood @xmath66 in fisher s likelihood inference for models without unobservables .",
    "in forming the h - likelihood the choice of the scale for @xmath1 is important ( lee et al . , @xcite ) because the mode and its curvature are used for inferences as we shall discuss .    throughout this paper",
    "we use @xmath69 to denote probability functions of random variables with fixed parameters @xmath70 ; the arguments within the brackets can be either conditional or unconditional . thus @xmath71 and @xmath72 have different functional forms though we use the same @xmath69 to mean probability functions with parameters @xmath73 .",
    "if we assume a prior @xmath74 on parameters @xmath70 , bayesian inferences can be made .",
    "the posterior is @xmath75 where @xmath76 and @xmath77 here @xmath70 is also unobservable and is eliminated by integration .",
    "let @xmath78 forbayesian inferences the following various marginal or conditional posteriors have been used:@xmath79 in this paper full bayesian ( fb ) inference is assumed to use the marginal posteriors @xmath80 and @xmath81 while empirical bayesian ( eb ) inference ( morris , @xcite ) uses the conditional posteriors @xmath82 where @xmath53 are the ml estimators maximizing the likelihood @xmath83 under the uniform prior @xmath84      the likelihood principle of birnbaum ( @xcite ) states that fisher s marginal likelihood @xmath85 carries all the ( relevant experimental ) information in the data about the fixed parameters @xmath70 so that @xmath86 should be used for inferences about @xmath70 ( see also berger and wolpert , @xcite ) . for estimating fixed parameters @xmath70 we follow the likelihood principle by using the ml estimator from @xmath87 we view the marginal likelihood as an adjusted profile likelihood eliminating nuisance unobservables @xmath1 from the h - likelihood .",
    "however , the computation of ml estimators can be a complex task because of intractable integration .",
    "for example , in the salamander data ( mccullagh and nelder , @xcite ) marginal - likelihood inference , based upon numerical integration using gauss ",
    "hermite quadrature , is not feasible since a@xmath88-dimensional integral is required .",
    "let @xmath89 be the ( log- ) marginal likelihood .",
    "let @xmath90 be a likelihood , either a marginal likelihood @xmath91 or an hierarchical likelihood @xmath92 with nuisance parameters @xmath93 lee and nelder ( @xcite ) introduce a function , @xmath94 , defined by @xmath95\\\\[-8pt ] & & \\quad = \\biggl[l-\\frac{1}{2}\\log\\det\\{d(l,\\alpha)/(2\\pi ) \\}\\biggr]\\bigg|_{\\alpha=\\tilde{\\alpha}},\\nonumber\\end{aligned}\\ ] ] where @xmath96 and @xmath97 solves @xmath98 .",
    "these @xmath99 functions define adjusted profile h - likelihoods ( aphls ) .",
    "if @xmath100 the bayesian posterior is identical to the h - likelihood , @xmath101 thus aphls can have a bayesian interpretation ; for example @xmath102 is the laplace approximation to the marginal posterior @xmath103 eliminating ( @xmath104 by integration@xmath105 when @xmath100 , it is not a probability if the domain is the whole real line or the positive real line .",
    "however , as long as the marginal posterior is proper ( finite ) , @xmath81 would be considered as a valid posterior ( berger , @xcite ) .    aphls also allow a likelihood interpretation . here",
    "@xmath106 is the laplace approximation to the marginal likelihood @xmath91 obtained by integrating over unobservables @xmath1 ( lee and nelder , @xcite ) ; its maximum gives approximate ( marginal ) ml estimators for @xmath107 in likelihood inferences fixed parameters are eliminated by conditioning ( if available ) or profiling ( in general ) .",
    "now suppose that the parameters in a model can be divided into location parameters @xmath44 and dispersion parameters @xmath108 note that @xmath109 is an adjusted profile likelihood that approximates the conditional log - likelihood obtained by conditioning on the marginal ml estimator @xmath110 to eliminate the fixed unknown parameter @xmath44 ( cox and reid , @xcite ) .",
    "a well - known exact example of this is the use of restricted likelihood in linear mixed models .",
    "furthermore,@xmath111 is davison s ( @xcite ) predictive likelihood for @xmath1 , eliminating nuisance fixed parameters @xmath70 .",
    "the aphl @xmath102 eliminates @xmath112 by integration and @xmath70 by conditioning on @xmath113 when orthogonality does not hold between parameters we use a profile likelihood to eliminate nuisance parameters . to simplify the notation we sometimes suppress arguments",
    "; for example we use @xmath114 instead of @xmath115 if this does not lead to ambiguity .",
    "lee and nelder ( @xcite , @xcite , @xcite ) propose maximizing the h - likelihood @xmath116 for the estimation of @xmath1 , the marginal likelihood @xmath91 for the ml estimators for @xmath44 and the restricted likelihood @xmath117 for the dispersion parameters @xmath118 .",
    "thus our position is consistent with the likelihood principle by using the marginal likelihood for inferences about @xmath70 .",
    "however , when @xmath91 is numerically hard to obtain , we propose to use adjusted profile h - likelihoods ( aphls ) @xmath119 and @xmath120 as approximations to @xmath91 and @xmath121",
    "@xmath120 approximates the restricted log - likelihood .",
    "second - order laplace approximations may sometimes be useful to improve accuracy .",
    "many numerical studies on h - likelihood have shown that this development gives practically satisfactory estimates of parameters in many models where the ml estimators are hard to compute . for binary data noh and lee ( @xcite )",
    "show numerically that the h - likelihood estimator for @xmath70 has less bias and mean square error than various other methods developed by schall ( @xcite ) , breslow and clayton ( @xcite ) , drum and mccullagh ( @xcite ) , shun and mccullagh ( @xcite ) , lin and breslow ( @xcite ) and shun ( @xcite ) : see also the simulation studies of frailty models ( ha and lee , @xcite ) and of mixed linear models with censoring ( ha et al . , @xcite ) .",
    "in the salamander data , among other methods considered , the mcem of vaida and meng ( @xcite ) gives the closest estimates to the h - likelihood estimators .",
    "little and rubin ( @xcite ) provide an extensive review of the analysis of missing data and claim that h - likelihood methods are inappropriate for the estimation of @xmath70 in missing - value settings such as that in example  5 .",
    "they appear to wrongly equate h - likelihood estimation to a joint maximization of mean and dispersion parameters .",
    "yun et al .",
    "( @xcite ) show , in contrast to this assertion , that when applied appropriately h - likelihood methods are both valid and efficient in such settings . in non - linear mixed - effect models",
    "the h - likelihood can also improve on existing methods ( noh and lee , @xcite ) .          in the bayesian approach ,",
    "simulation techniques such as mcmc are often used to compute themarginal posteriors .",
    "consider the epil example of the openbugs manual , volume 1 ( thomas et al . , @xcite ) .",
    "the data come from a clinical trial of 59 epileptic patients .",
    "each patient @xmath18 is randomized to a new drug ( @xmath122 or a placebo ( @xmath123 ) .",
    "the observations for each patient @xmath124 are the number of seizures during the 2 weeks before each of four visits .",
    "the covariates are age ( @xmath125 ) , the baseline seizure counts ( @xmath126 ) and an indicator variable for the fourth clinic visit ( @xmath127 ) .",
    "consider the hglm , @xmath128 using centered covariates with @xmath129 and@xmath130 in discussing the paper by rue et al .",
    "( @xcite ) on bayesian inferences based on priors @xmath131 lee shows figure [ fig2 ] ( of this paper ) for the marginal posteriors , @xmath132 @xmath133 and @xmath134 from openbugs ( thomas et al . , @xcite ) and the corresponding aphls , @xmath135 @xmath136 and @xmath137where @xmath138 are the ml estimators of remaining @xmath44 and the reml estimators for the dispersion parameters at @xmath139 and @xmath140 is the reml estimators of @xmath141 at @xmath142 figure  [ fig2 ] shows almost identical plots for both random and fixed effects . however , the plots for the dispersion components are different because the inverse - gamma prior of rue et al .",
    "( @xcite ) is informative .",
    "this leads to biases when dispersion parameters are not random but are fixed unknowns , as in disease mappings ( jang et al . , @xcite ) .",
    "thus without mcmc samplings similar information could be obtained from the extended likelihood unless the assumed prior is informative .",
    "thus , likelihood inferences can be made without the necessity of inventing priors for parameters .",
    "the extended likelihood principle of bjrnstad(@xcite ) shows that extended likelihood , of which h - likelihood is a special case , carries all the information in the data about the unobserved quantities @xmath1 and @xmath51 bedrick and hill ( @xcite ) study the use of extended likelihood as a summary function forunobservables . in this paper",
    "we discuss its use as an estimating tool .",
    "consider the prediction problem in example 4 where the plug - in technique @xmath143 can be viewed as the eb .",
    "with jeffreys prior , @xmath144 @xmath145 the resulting marginal posterior @xmath146 gives a predictive probability with higher probabilities for larger @xmath0 .",
    "pawitan ( @xcite ) considers the h - likelihood , proportional to @xmath147 here @xmath148 then the normalized profile likelihood @xmath149 gives the predictive distribution of mathiasen ( @xcite ) almost identical to pearson s but without assuming a prior on @xmath70 ( figure [ fig3 ] ) ( for more discussion , see bjrnstad , @xcite ) .",
    "this example shows that standard methods for likelihood inferences can be used for the prediction problem . in the next section",
    "we illustrate how to use standard likelihood methods to overcome a drawback of eb method .          because the fisher likelihood @xmath85 in ( [ eq : h - likelihood ] ) does not involve @xmath49 the other component ( the conditional posterior ) @xmath150 seems to carry all the information in the data about the unobservables .",
    "thus an inference would be based solely upon the estimated posterior , @xmath151 where @xmath53 are usually the ml estimators ( carlin and louis , @xcite ) .",
    "using @xmath152 to make inferences about @xmath1 is naive , and bjrnstad ( @xcite ) shows how badly it performs in measuring the true uncertainty in estimating @xmath1 .",
    "note that maximization of the h - likelihood ( [ eq : h - likelihood ] ) yields eb - mode estimators for @xmath1 without computing @xmath153 .",
    "however , the hessian matrix based upon the estimated posterior @xmath152 gives a naive variance estimate for the prediction @xmath154 because it does not properly account for the uncertainty caused by estimating @xmath70 .",
    "note that the marginal posterior variance is @xmath155\\nonumber \\\\[-8pt]\\\\[-8pt ] & & { } + \\operatorname{var}% _ { \\theta|y}[e(v_{i}|y,\\theta)].\\nonumber\\end{aligned}\\ ] ] carlin and gelfand ( @xcite ) note that the naive eb variance estimate only approximates the first term in the equation above .",
    "laird and louis ( @xcite ) and carlin and gelfand ( @xcite ) propose to use the bootstrap method to estimate the second term . in this paper",
    "the fb method uses the marginal posterior @xmath156    up to now most studies on h - likelihood methods have been about the efficiencies of parameter estimates . here",
    "we discuss how to compute the variance of estimated random effects .",
    "we see that inferences about random effects can not be made by using only @xmath72 as the eb method does . because @xmath72 involves the fixed parameters @xmath157 we should use the whole h - likelihood to reflect the uncertainty in estimating @xmath158 it is the other component @xmath85 which carries the information about this . by using the h - likelihood ,",
    "complete likelihood inferences can be made not only for @xmath70 but also for @xmath1 and their combinations .",
    "given @xmath70 let @xmath159 be a random - effect estimator solving @xmath160 as a variance of random - effect estimators booth and hobert ( @xcite ) recommend using the conditional mean square error ( cmse ) defined by @xmath161\\\\[-8pt ] & = & \\operatorname{var}_{\\theta}(v|y)+d(\\theta),\\nonumber\\end{aligned}\\ ] ] where @xmath162and @xmath163 is the inflation of the cmse caused by estimating @xmath51 the eb estimator , the inverse of the hessian matrix from @xmath164 gives an estimator for the first term@xmath165 in ( [ eq : cmsep])@xmath166 thus it could give severe underestimation if @xmath167 is large .",
    "lee and nelder ( @xcite ) note that in hglms ( [ eq : hglm ] ) , the location parameters ( @xmath168 ) and dispersion parameters @xmath169(@xmath170 ) are orthogonal so that we need consider only the variance inflation caused by estimating @xmath107 the hessian matrix of @xmath44 and @xmath171 is given by @xmath172 here the eb variance estimator is given by @xmath173 .",
    "lee and ha ( @xcite ) show that in general the inverse of the hessian matrix ( [ eq : hessian ] ) gives an approximation to the cmse ( [ eq : cmsep ] ) . before we discuss the general use of this method we investigate a simple example which shows issues related to this problem .",
    "bayarri et al .",
    "( @xcite ) try to show by an example that likelihood inference is not possible for general models with unobservables .",
    "suppose that there is a single fixed parameter @xmath70 , a single unobservable random quantity @xmath174 and a single observable quantity @xmath175 an unobserved random variable @xmath174 has a probability function @xmath176 and an observable random variable @xmath0 has conditional probability function @xmath177 free of @xmath70 .",
    "besides @xmath178 they considered the following two additional possibilities for an extended likelihood for models with these three kinds of objects : @xmath179 the marginal log - likelihood @xmath67 gives the ml estimator for @xmath70 but is totally uninformative about the unknown value of @xmath174 .",
    "the conditional likelihood @xmath180 is uninformative about @xmath70 and loses the relationship between @xmath174 and @xmath70 reflected in @xmath181 finally , the extended likelihood @xmath182 yields , if maximized jointly with respect to @xmath70 and @xmath174 , the useless estimators @xmath183 and @xmath184 .",
    "bayarri et al .",
    "( @xcite ) therefore conclude that none is useful as a likelihood for complete inferences , so that bayes is the only method for inferences from general models .",
    "the h-(log)-likelihood is given by @xmath185 where @xmath186 with @xmath1 being the canonical scale in which the joint maximization of @xmath116 with respect to @xmath70 and @xmath174 gives the ml estimator of @xmath70 ( lee et al . ,",
    "suppose that the marginal likelihood is hard to obtain .",
    "the laplace approximation is proportional to @xmath187 and gives the ml estimator @xmath188 and its variance estimator @xmath189 given @xmath190 the estimating equation @xmath191 gives the best estimator of @xmath174 ( robinson , @xcite ) , @xmath192 from which we have @xmath193 furthermore , we have @xmath194 note here that @xmath195 so that eb gives @xmath196 here @xmath197 so that , following booth and hobert ( @xcite ) , if we estimate @xmath198 by var(@xmath199 we have @xmath200 thus the estimator for the cmse is @xmath201 which can be obtained from the corresponding element in the hessian matrix @xmath202 an alternative justification is that the h - likelihood variance estimator is estimating the unconditional mean - square error because@xmath203 from @xmath204 ( lee et al . , @xcite , page 116 ) .    with this small example we illustrate how the h - likelihood gives complete likelihood inferences , giving the ml inference for @xmath70 and improved eb inference by accounting for the uncertainty caused by estimating @xmath51          the example shows that between extended likelihoods @xmath182 and @xmath205 the mode of theh - likelihood @xmath206 gives a meaningful estimator for @xmath1 , while that of @xmath182 gives a meaningless one .",
    "given that extended likelihoods should serve as the basis for statistical inferences of a general nature , we want to find a particular scale whose mode gives meaningful inferences about unobservables .",
    "under the canonical scale the example shows that the mode gives the best estimator of @xmath207 e@xmath208 .",
    "however , the canonical scale does not exist in general . in hglms",
    "lee and nelder ( @xcite ) show that maintaining invariance of inference from extended likelihood for trivial re - expressions of the underlying model leads to a unique definition of the h - likelihood ; we call this the weak canonical scale in which @xmath1 appears in the linear predictor .    in section [ sec3.3 ]",
    "we show that aphls are often similar to marginal posteriors . given ( marginal ) posteriors ,",
    "a bayesian would use a decision - theoretic approach to choose estimators while we use the mode of the h - likelihood ( an extended likelihood on a particular scale ) or its aphls .",
    "thus the choice of the scale in defining the h - likelihood is important to guarantee the meaningfulness of the mode estimation .",
    "lee and ha ( @xcite ) show that the standard error estimators from the hessian matrix ( [ eq : hessian ] ) give the first - order approximation to ( [ eq : bayes ] ) with @xmath100 ( kass and steffey , @xcite ) and to the cmse ( booth and hobert , @xcite ) .",
    "let @xmath209 for some monotone function @xmath210 .",
    "ha and lee ( @xcite ) show conditions when the approximation becomes better .",
    "one such condition is that @xmath211 follows the normal distribution . in glmms",
    "when @xmath1 is normal we may expect @xmath212 to be approximately normal . if normal the laplace approximation is exact",
    "; we expect that proposed h - likelihood method works well .",
    "figure  [ sec2 ] shows how to check the normality of the conditional distribution by using the aphl .      for disease mapping , leroux et al .",
    "( @xcite ) and macnab et al .",
    "( @xcite ) consider the conditional autoregressive ( car ) model for the relative risk @xmath213 which satisfies @xmath214 where @xmath215 @xmath216 @xmath118 is a dispersion parameter reflecting the overall heterogeneity of the underlying risks , and @xmath217 is a dispersion parameter for the spatial autocorrelation , @xmath218.$ ] the neighborhood matrix @xmath219 has the @xmath16th diagonal element equal to the number of neighbors of the corresponding local region while the off - diagonal elements in each row are equal to @xmath220 if the corresponding regions are neighbors and @xmath221 otherwise .",
    "the data consist of the number of infant deaths and aggregated mid - year estimates of the population sizes of infants for @xmath222 local health areas .",
    "population size @xmath223 varies from @xmath224 to @xmath225 for these data lee et al .",
    "( @xcite ) compare inferences from the h - likelihood with the full bayes ( fb ) analysis . for the fb approach , they set priors @xmath226 and @xmath227 .",
    "initial values are set as @xmath228 , @xmath229 and @xmath230 , and they obtain a posterior sample of 10,000 , setting thinning at 10 using winbugs ( macnab et al . , @xcite ) .",
    "the coverage probability is calculated by 95% wald confidence intervals based upon asymptotic normality for the relative risks ( @xmath1 ) using eb and h - likelihood , and in the fb method by equal - tail 95% credible intervals , the interval between the 2.5th and 97.5th percentiles of the posterior distribution as given by winbugs .",
    "for the fb method we use 10,000 iterations after a burn - in of 2000 .",
    "lee et al .",
    "( @xcite ) did a simulation study , assuming @xmath223 and neighborhood structures identical to those in the bc infant mortality ; the data were generated based on ( 1.1 ) and ( 3.1 ) with @xmath231 @xmath232 and @xmath233 . using a graph similar to figure  [ fig4 ] , they showed that the eb coverage probability decreases dramatically as the population size @xmath223 increases , but that both the h - likelihood and fb methods improve the eb method substantially by accounting for the uncertainty in estimating fixed parameters .",
    "however , the coverage probability of fb also decreases as @xmath223 increases while the h - likelihood maintains the stated level of confidence .",
    "when @xmath223 becomes larger the priors for the dispersion parameters in the fb may cause problems in frequentist coverage probability .",
    "the h - likelihood procedure maintains the frequentist coverage probabilities better in this problem .",
    "the h - likelihood method is superior to ainsworth and dean s ( @xcite ) penalized quasi - likelihood ( lee et al .",
    ", @xcite ) for spatial glmms and ma and jorgensen s ( @xcite ) orthodox blupmethod ( lee and ha , @xcite ) for nonnormal tweedie models .",
    "the joint model for @xmath206 leads to a marginal model @xmath234 for the observed data .",
    "we regard @xmath206 as the fundamental model from which the marginal model can be made .",
    "however , different models for unobservables in @xmath206 can lead to the samemarginal model @xmath85 so that care is necessary in making inferences about unobservables .",
    "some model assumptions can be checked from the data while some can not .",
    "this could be an advantage of objective inference with the likelihood , where uncheckable model assumptions can not be identifiable .",
    "in bayesian analysis , priors can give information on unidentifiable model assumptions so that it is hard to know whether the information is coming entirely from the uncheckable priors .",
    "in the modeling of incomplete data we may assume the missing data to be `` missing not at random '' ( mnar ) or `` assume random missingness''(mar ) .",
    "here assumptions for the missing mechanism can not be checked by using observed data [ rubin ( @xcite ) ] .",
    "molenberghs et al .",
    "( @xcite ) further show that an empirical distinction between mar and mnar is not possible because each mnar model fits to a set of observed data can be reproduced exactly by its counterpart .",
    "such a pair of models will produce identical estimates for the observed data but give different estimates for the unobservables ( missing data ) .",
    "assumptions about unobservables ( missing data ) are not checkable without additional information .",
    "unless we have a side - study to determine whether the observation process depends on what would be observed , all we have is a model - based assessment .",
    "as a referee has pointed out , it will contain some unverifiable assumptions .    in hglms model assumptions for unobservables",
    "are often verifiable , that is , checkable , by using the data because the unobservables are latent variables for observed data .",
    "consider the one - way random - effect model , @xmath235 where @xmath236 and @xmath237 @xmath238 with @xmath213 and @xmath239 uncorrelated .",
    "with more than one observation in each group the within - group error components @xmath213 and @xmath239 are separately estimable , providing variance - component estimates for the dispersion parameters . here",
    "model parameters @xmath240 and @xmath217 connect the observed data and unobservables .",
    "lee and nelder(@xcite ) show that if there are different random - effect models giving the same induced marginal model for the observed data , then the h - likelihood inferences give equivalent inferences for equivalent pairs of objects , including unobservables .",
    "this model leads to a marginal model , namely the following compound - symmetric model : @xmath241 a compound - symmetry model with negative correlation @xmath242 is perfectly natural in a variety of settings ( nelder , @xcite ) which can be tested by the marginal likelihood ( or aphl ) .",
    "such a model can be covered by hglms if we allow a negative variance , but then many unanswered questions arise , such as estimability of random effects , etc . ; these require further research .    wilk and kempthorne ( @xcite ) and cox ( @xcite )",
    "study the randomization theory of the latin square , paying particular attention to the effects on the interpretation of the conventional analysis of variance ( anova ) of the absence of unit - treatment additivity , a point first raised by neyman ( @xcite ) .",
    "consider a model for the latin - square design , @xmath243\\\\[-8pt ] & & { } + ( rc)_{ij}+(rt)_{ik}+(ct)_{jk}+e_{ij(k)}.\\nonumber\\end{aligned}\\ ] ] suppose that the main effects are regarded as fixed .",
    "when the interactions @xmath244 are fixed a test for the main effect is irrelevant because it makes no sense to postulate that either of the two main effects is null when their interaction is not assumed zero ( nelder , @xcite ) .",
    "however , if the interactions are regarded as random the associated main effects can tested without any difficulty from the anova table .",
    "permutation from  a  finite population is a way of generating distributions for random effects .",
    "wilk and kempthorne ( @xcite ) put constraints @xmath245 nelder ( @xcite ) points out that such constraints make no sense either with fixed or random effects . with fixed effects",
    "the choice of constraints to give the least - square equations a solution is essentially arbitrary .",
    "however , with random effects symmetric constraints on estimates of the parameters of the form @xmath246 arise naturally ( lee and nelder , @xcite , @xcite ) .",
    "however , here only fractions of combinations are used to make the combined error component @xmath247 to form a sum of independent errors .",
    "thus model ( [ eq : latin1 ] ) gives an identical marginal model to the conventional model for latin squares with main effects only @xmath248 from lee and nelder ( @xcite ) the two models lead to identical inferences about both fixed parameters and random effects , giving @xmath249 thus in ( [ eq : latin1 ] ) individual error components can not be separated by the observed data .",
    "if a method can identify individual components then it must be based upon uncheckable model assumptions such as priors .",
    "consider the following model : @xmath250 where @xmath251 and @xmath252 and @xmath253 are random with zero means .",
    "this model assumes unit - treatment interaction and can be interpreted to have the average treatment effects such that @xmath254 then we can test that the average treatment effects are the same ( lee and nelder , @xcite ) .",
    "thus with unobservables there are different methods of interpretation : we may consider @xmath252 and @xmath255 to be either error components or random treatment - unit interactions .",
    "these give equivalent inferences for equivalent quantities .",
    "there have been many alleged examples similar to that of bayarri et al .",
    "( @xcite ) and little and rubin [ ( @xcite ) , chapter 6.3 ] , purporting to show that an extension of the fisher likelihood to three kinds of objects is not possible .",
    "lee and nelder ( @xcite ) refute those of bayarri et al . and",
    "yun et al . ( @xcite ) those of little and rubin .",
    "these complaints are , we believe , resolved by the h - likelihood framework .",
    "zhao et al .",
    "( 2006 ) claim that the bayesian analysis is computationally simpler for obtaining variance estimators for the random - effect estimates compared with its frequentist counterpart ; however with the extended likelihood framework this may not be so , at least in the analysis of the disease - mapping areas in section [ sec4.3.1 ] .",
    "the h - likelihood ( [ eq : h - likelihood ] ) gives a new definition of conjugate families ( lee and nelder , @xcite ) , showing that the likelihood for a conjugate family for @xmath256 takes the form of a glm .",
    "it is the sum of component likelihoods , @xmath256 and @xmath257 both representable as glm likelihoods .",
    "this means that an extended class of models can be decomposed into component glms ( lee and nelder , @xcite , @xcite ) and that these extended models can be fitted as an interconnected set of component glms .",
    "this greatly facilitates the development of model - checking techniques for the whole class ( lee and nelder , @xcite ) .",
    "a single algorithm , iterative weighted least squares , can be used throughout all this extended class of models and requires neither prior distributions of parameters nor multi - dimensional quadrature .",
    "the h - likelihood plays a key role in the of the computational algorithms needed for this extended class of models .",
    "this formulation means that a great variety of models can be fitted by a single algorithm and compared using extensions of standard glm procedures .",
    "thus we can change the link function , allow various types of term in the linear predictor and use model - selection methods for adding or deleting terms .",
    "furthermore , various model assumptions can be checked by applying glm model - checking procedures to the appropriate component glms .",
    "this establishes , we believe , algorithmic _",
    "wiseness _ in the sense of efron ( @xcite ) .",
    "we have shown that a broad class of new models with wide applications can be generated by the probabilistic modeling of unobservables .",
    "there has been an attempt using the gee method to make inferences from general non - normal multivariate models without modeling unobservables .",
    "it pre - empts model selection by claiming to make inferences about population averages or marginal means .",
    "we do not disagree with the need to make marginal predictions after choosing a model but believe that such a need does not require , and indeed should not use , prediction methods at the model - selection stage .",
    "we dislike the pre - emption of the model selection stage by a particular prediction method .",
    "furthermore , these population , marginal and subject - specific averages are parameterizations in the probabilistic model.when a prediction method lacks a probabilistic model basis it is not possible to connect these parameters and compare them .",
    "we do not object to the use of fisher s likelihood for inferences about fixed parameters .",
    "the fisher likelihood framework has advantages such as generality of application , statistical and computational efficiency , etc . , and we agree with its use .",
    "however , it can not deal with inferences from models having unobservables because there is always a problem of inference about those unobservables .",
    "h - likelihood gives a powerful and practical framework for statistical inference of general model class with unobservables , maintaining the advantages of the original likelihood framework for fixed parameters .",
    "we believe that more new classes of models will be developed and that the h - likelihood will become widely used for inference from them .",
    "the h - likelihood uses the mode and its curvature for inferences about unobservables .",
    "thus , in defining the h - likelihood the scale of unobservables must be carefully chosen to make a valid inferences .",
    "the ( weak ) canonical scale in hglms leads to an invariance of a certain extended likelihood .",
    "however , in general the validity of such a scale has not been established .",
    "the conditional normality in section  [ sec4.3 ] would be a promising condition to determine the scale , which can be checked by plotting the aphl .",
    "further studies are required on the scale in defining the h - likelihood under general situations beyond dhglms . for fixed parameter estimation",
    "we use the marginal likelihood .",
    "but it is often hard to compute , so that we have proposed using the laplace approximation .",
    "however , this approximation gives nonnegligible biases in binary data .",
    "we have found that the second - order approximation is effective in eliminating such biases .",
    "however , it becomes very hard to implement as the number of random components increases .",
    "so it would be of interest to find an approximation which can be implemented under general situations .",
    "the authors thank professors jan bjrnstad , martin crowder , harry joe , jaeyong lee , yudi pawitan and roger payne for their helpful comments . this work was supported by brain korea 21 .",
    "bayarri , m. j. , degroot , m. h. and kadane , j. b. ( 1988 ) . what is the likelihood function ?",
    "( with discussion ) . in",
    "_ statistical decision theory and related topics iv .",
    "vol . 1 . _ ( s. s. gupta and j. o. berger , eds . ) .",
    "springer , new york .",
    "leroux , b. g. , lin , x. and breslow , n. ( 1999 ) . estimation of disease rates in small areas : a new mixed model for spatial dependence . in _ statistical models in epidemiology , the environment and clinical trials _ ( m. e. halloran and d. berry , eds . ) 135178 .",
    "springer , new york .",
    "molenberghs , g. , beunckens , c. , sotto , c. and kenward , m. g. ( 2008 ) .",
    "every missingness not at random model has a missingness at random counterpart with equal fit . _",
    "soc . ser .",
    "b _ * 70 * 371388 .",
    "pawitan , y. ( 2001 ) .",
    "_ in all likelihood : statistical modelling and inference using likelihood",
    ". _ clarendon press , oxford .",
    "pearson , k. ( 1920 ) .",
    "the fundamental problems of practical statistics .",
    "_ biometrika _ * 13 * 116 .            rue , h. , martino , s. and chopin , n. ( 2009 ) .",
    "approximate bayesian inference for latent gaussian models by using integrated nested laplace approximations ( with discussion ) .",
    "b _ * 71 * 319392 .        shun , z. and mccullagh , p. ( 1995 ) .",
    "laplace approximation of high - dimensional integrals .",
    "b _ * 57 * 749760 .",
    "skrondal , a. and rabe - hesketh , s. ( 2004 ) .",
    "_ generalized latent variable modeling : multilevel , longitudinal , and structural equation models_. chapman and hall , london .",
    "vaida , f. and meng x. l. ( 2004 ) .",
    "mixed linear models and the em algorithm . in",
    "_ applied bayesian and causal inference from an incomplete data _ _ perspective _ ( a. gelman and x. l. meng , eds . ) .",
    "wiley , new york ."
  ],
  "abstract_text": [
    "<S> there have been controversies among statisticians on ( i ) what to model and ( ii ) how to make inferences from models with unobservables . </S>",
    "<S> one such controversy concerns the difference between estimation methods for the marginal means not necessarily having a probabilistic basis and statistical models having unobservables with a probabilistic basis . another concerns likelihood - based inference for statistical models with unobservables . </S>",
    "<S> this needs an extended - likelihood framework , and we show how one such extension , hierarchical likelihood , allows this to be done . </S>",
    "<S> modeling of unobservables leads to rich classes of new probabilistic models from which likelihood - type inferences can be made naturally with hierarchical likelihood .    . </S>"
  ]
}