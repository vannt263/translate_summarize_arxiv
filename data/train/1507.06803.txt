{
  "article_text": [
    "learning algorithms for deep multilayer neural networks have been known for a long time @xcite , though they usually could not outperform simpler , shallow networks . in this way",
    ", deep multilayer networks were not widely used to solve large scale real - world problems until the last decade  @xcite . in 2006 , deep belief networks ( dbns ) @xcite came out as a real breakthrough in this field , since the learning algorithms proposed ended up being a feasible and practical method to train deep networks , with spectacular results @xcite .",
    "dbns have restricted boltzmann machines ( rbms ) @xcite as their building blocks .",
    "rbms are topologically constrained boltzmann machines ( bms ) with two layers , one of hidden and another of visible neurons , and no intralayer connections .",
    "this property makes working with rbms simpler than with regular bms , and in particular the stochastic computation of the log - likelihood gradient may be performed more efficiently by means of gibbs sampling @xcite .    in 2002 ,",
    "the _ contrastive divergence _ ( cd ) learning algorithm was proposed as an efficient training method for product - of - expert models , from which rbms are a special case @xcite .",
    "it was observed that using cd to train rbms worked quite well in practice .",
    "this fact was important for deep learning since some authors suggested that a multilayer deep neural network is better trained when each layer is pre - trained separately as if it were a single rbm @xcite .",
    "thus , training rbms with cd and stacking up them seems to be a good way to go when designing deep learning architectures .    however , the picture is not as nice as it looks , since cd is not a flawless training algorithm . despite cd being an approximation of the true log - likelihood gradient @xcite ,",
    "it is biased and it may not converge in some cases @xcite .",
    "moreover , it has been observed that cd , and variants such as persistent cd @xcite or fast persistent cd @xcite can lead to a steady decrease of the log - likelihood during learning @xcite .",
    "therefore , the risk of learning divergence imposes the requirement of a stopping criterion .",
    "there are two main methods used to decide when to stop the learning process .",
    "one is based on the monitorization of the _ reconstruction error _",
    "the other is based on the estimation of the log - likelihood with _",
    "annealed importance sampling _",
    "( ais )  @xcite .",
    "the reconstruction error is easy to compute and it has been often used in practice , though its adequacy remains unclear because of monotonicity  @xcite .",
    "ais seems to work better than the reconstruction error in most cases , though it is considerably more expensive to compute , and may also fail @xcite .    in this work we approach this problem from a completely different perspective .",
    "based on the fact that the energy is a continuous and smooth function of its variables , the close neighborhood of the high - probability states is expected to acquire also a significant amount of probability . in this sense , we argue that the information contained in the neighborhood of the training data is valuable , and that it can be incorporated in the learning process of rbms .",
    "in particular , we propose to use it in the monitorization of the log - likelihood of the model by means of a new quantity that depends on the information contained in the training set and its neighbors . furthermore , and in order to make it computationally tractable ,",
    "we build it in such a way that it becomes independent of the partition function of the model . in this way",
    ", we propose a neighborhood - based stopping criterion for cd and show its performance in several data sets .",
    "energy - based probabilistic models define a probability distribution from an energy function , as follows : @xmath0 where @xmath1 and @xmath2 stand for ( typically binary ) visible and hidden variables , respectively .",
    "the normalization factor @xmath3 is called partition function and reads @xmath4    since only @xmath1 is observed , one is interested in the marginal distribution @xmath5 but the evaluation of the partition function @xmath3 is computationally prohibitive since it involves an exponentially large number of terms . in this way",
    ", one can not measure directly @xmath6 .",
    "the energy function depends on several parameters @xmath7 , that are adjusted at the learning stage .",
    "this is done by maximizing the likelihood of the data . in energy - based models ,",
    "the derivative of the log - likelihood can be expressed as @xmath8 } \\nonumber \\\\ { } & \\ \\ \\ \\ \\ \\ \\ \\ -\\ e_{p(\\xx ) } \\left[e_{p(\\h|\\xx)}\\left[\\frac{\\partial\\text{energy}(\\xx,\\h)}{\\partial\\theta}\\right ]    \\right ] \\ , \\end{aligned}\\ ] ] where the first term is called the positive phase and the second term the negative phase . similar to ( [ pdf - energy - x - sumh ] ) , the exact computation of the derivative of the log - likelihood is usually unfeasible because of the negative phase in ( [ dlog - likelihood ] ) , which comes from the derivative of the partition function .",
    "restricted boltzmann machines are energy - based probabilistic models whose energy function is : @xmath9 rbms are at the core of dbns @xcite and other deep architectures that use rbms for unsupervised pre - training previous to the supervised step @xcite .",
    "the consequence of the particular form of the energy function is that in rbms both @xmath10 and @xmath11 factorize . in this way it is possible to compute @xmath10 and @xmath11 in one step , making it possible to perform gibbs sampling efficiently , in contrast to more general models like boltzmann machines  @xcite .",
    "the most common learning algorithm for rbms uses an algorithm to estimate the derivative of the log - likelihood of a product of experts model .",
    "this algorithm is called contrastive divergence  @xcite .",
    "contrastive divergence cd@xmath12 estimates the derivative of the log - likelihood for a given point @xmath1 as @xmath13 } \\nonumber \\\\ { } & \\ \\ \\ \\ \\ \\ \\ \\ -\\ e_{p(\\h|\\x_{n})}\\left[\\frac{\\partial\\text{energy}(\\x_{n},\\h)}{\\partial\\theta}\\right ] \\",
    ".\\end{aligned}\\ ] ] where @xmath14 is the last sample from the gibbs chain starting from @xmath1 obtained after @xmath15 steps :    * @xmath16 * @xmath17 * ... * @xmath18 * @xmath19  .    usually , @xmath20 $ ] can be easily computed .",
    "several alternatives to cd@xmath12 are persistent cd @xcite , fast persistent cd @xcite or parallel tempering @xcite .",
    "learning in rbms is a delicate procedure involving a lot of data processing that one seeks to perform at a reasonable speed in order to be able to handle large spaces with a huge amount of states . in doing so",
    ", drastic approximations that can only be understood in a statistically averaged sense are performed  @xcite .",
    "one of the most relevant points to consider at the learning stage is to find a good way to determine whether a good solution has been found or not , and so to decide when the learning process should stop .",
    "one of the most widely used criteria for stopping is based on the monitorization of the reconstruction error , which is a measure of the capability of the network to produce an output that is consistent with the data at input . since rbms are probabilistic models , the reconstruction error of a data point @xmath21 is computed as the probability of @xmath21 given the expected value of @xmath2 for @xmath21 : @xmath22\\right ) \\ , \\ ] ] which is a probabilistic extension of the sum - of - squares reconstruction error for deterministic networks @xmath23    some authors have shown that , in some cases , learning induces an undesirable decrease in likelihood that goes undetected by the reconstruction error @xcite .",
    "it has been shown @xcite that the reconstruction error defined in ( [ reconstruction - error - rbm - probability ] ) usually decreases monotonically .",
    "since no increase in the reconstruction error takes place during training there is no apparent way to detect the change of behavior of the log - likelihood for cd@xmath12 .",
    "the proposed stopping criterion is based on the monitorization of the ratio of two quantities : the geometric average of the probabilities of the training set , and the sum of probabilities of points in a given neighbourhood of the training set .",
    "more formally , what we monitor is @xmath24^{1/n } } { { 1\\over |d|}\\sum_{j\\in d}p(\\y^{(j ) } ) } \\ , \\ ] ] where @xmath25 is a subset of points at a hamming distance from the training set less or equal than @xmath26 .",
    "the idea behind the definition is that the evolution of @xmath27 at the learning stage is expected to closely resemble that of the log - likelihood for certain values of @xmath26 and @xmath25 .",
    "for that reason we propose as the stopping criterion to find the maximum of @xmath27 , which will be close to the one shown by the log - likelihood of the data , as shown by the experiments in the next sections .",
    "the reason for that is twofold . on one hand",
    "the numerator and denominator monitor different things .",
    "the numerator , which is essentially the likelihood of the data , is sensitive to the accumulation of most of the probability mass by a reduced subset of the training data , a typical feature of cd@xmath12 . for continuity reasons ,",
    "the denominator is strongly correlated with the sum of probabilities of the training data . once the problem has been learnt , the probabilities in a close neighborhood of the training set will be high . the value of @xmath27 results from a delicate equilibrium between these two quantities ( see section  [ experiments ] ) , which we propose to use as a stopping criterion for learning .",
    "on the other hand , due to the structure of @xmath27 , the partition functions @xmath3 involved in both the numerator and denominator cancels out , which is a necessary condition in the design of the quantity being monitorized . in other words ,",
    "the computation of @xmath27 can be equivalently defined as @xmath28^{1/n } }         { { 1\\over|d|}\\sum_{j\\in d}\\sum_{\\h}\\e^{-\\text{energy}(\\y^{(j)},\\h ) } } \\ .\\ ] ] the particular topology of rbms allows to compute @xmath29 efficiently .",
    "this fact dramatically decreases the computational cost involved in the calculation , which would otherwise become unfeasible in most real - world problems where rbms could been successfully applied .    while the numerator in @xmath27 is directly evaluated from the data in the training set , the problem of finding suitable values for @xmath30 still remains .",
    "indeed , the set of points at a given hamming distance @xmath26 from the training set is independent of the weights and bias of the network . in this way",
    ", it can be built once at the very beginning of the process and be used as required during learning .",
    "therefore , two issues have to be sorted out before the criterion can be applied .",
    "the first one is to decide a suitable value of @xmath26 .",
    "experiments with different problems show that this is indeed problem dependent , as is illustrated in the experimental section below .",
    "the second one is the choice of the subset @xmath25 , which strongly depends on the size of the space being explored . for small spaces one can safely use the complete set of points at a distance less than or equal to @xmath26 , but that can be forbiddingly large in real world problems .",
    "for this reason we explore two possibilities : one including all points and another including only a random subset of the same size as the training set , which is only as expensive as dealing with the training set .",
    "we performed several experiments to explore the aforementioned criterion defined in section  [ proposed - stopping - criterion ] and study the behavior of @xmath27 in comparison with the log - likelihood and the reconstruction error of the data in several problems .",
    "we have explored problems of a size such that the log - likelihood can be exactly evaluated and compared with the proposed @xmath27 parameter .",
    "the first problem , denoted _ bars and stripes _",
    "( bs ) , tries to identify vertical and horizontal lines in 4@xmath314 pixel images .",
    "the training set consists in the whole set of images containing all possible horizontal or vertical lines ( but not both ) , ranging from no lines ( blank image ) to completely filled images ( black image ) , thus producing @xmath32 different images ( avoiding the repetition of fully back and fully white images ) out of the space of @xmath33 possible images with black or white pixels . the second problem , named _ labeled shifter ensemble _ ( lse ) , consists in learning 19-bit states formed as follows : given an initial 8-bit pattern , generate three new states concatenating to it the bit sequences 001 , 010 or 100 .",
    "the final 8-bit pattern of the state is the original one shifting one bit to the left if the intermediate code is 001 , copying it unchanged if the code is 010 , or shifting it one bit to the right if the code is 100 .",
    "one thus generates the training set using all possible @xmath34 states that can be created in this form , while the system space consists of all possible @xmath35 different states one can build with 19 bits .",
    "these two problems have already been explored in @xcite and are adequate in the current context since , while still large , the dimensionality of space allows for a direct monitorization of the partition function and the log - likelihood during learning . for the sake of completeness",
    ", we have also tested the proposed criterion on randomly generated problems with different space dimensions , where the number of states to be learnt is significantly smaller than the size of the space .",
    "in particular , we have generated four different data sets ( ran10 , ran12 , ran14 and ran16 ) consisting of @xmath36 binary input units and @xmath37 examples to be learnt , as suggested in  @xcite .    in the following",
    "we discuss the learning processes of these problems with binary rbms , employing the contrastive divergence algorithm cd@xmath12 with @xmath38 and @xmath39 as described in section  [ cd ] . in the bs case",
    "the rbm had 16 visible and 8 hidden units , while in the lse problem these numbers were 19 and 10 , respectively .",
    "for the random data sets we have used 10 hidden units in each case .",
    "every simulation was carried out for a total of 50000 epochs , with measures being taken every 50 epochs . moreover ,",
    "every point in the subsequent plots was the average of ten different simulations starting from different random values of the weights and bias .",
    "other parameters affecting the results that were changed along the analysis are the learning rates involved in the weight and bias update rules .",
    "no weight decay was used , and momentum was set to 0.8 .",
    "the learning rates were chosen in order to make sure that the log - likelihood degenerates , in such a way that it presents a clear maximum that should be detected by @xmath27 .    in the following",
    "we perform two series of experiments that are reported in the next two subsections . in the first one ( section  [ complete ] )",
    "we analyze the case where all states in @xmath25 are included . in the second one  ( section  [ uncomplete ] ) we relax the computational cost of the evaluation of @xmath27 by selecting only a small subset of all the states in @xmath25 .      [ cols=\"^,^,^,^,^ \" , ]      despite the success of the criterion built as proposed , it is clear that for large spaces it can be unpractical if the number of states in the neighborhood of the training set is very large .",
    "for that reason , we have tested the criterion on randomly selected subsets @xmath40 of the same size as the training set , which is always computationally tractable . in this sense , we denote by @xmath41 the evaluation of @xmath27 on @xmath42 .",
    "figure  [ fig_bs_lse_sales ] shows @xmath41 compared with @xmath27 from the previous figures for the bs ( first row ) and lse ( second row ) problems .",
    "more precisely , the first column shows the log - likelihood of the data along the training process , while the rest of the columns plot both @xmath43 and @xmath27 for @xmath44 and @xmath45 .",
    "notice that the absolute scales of @xmath27 and @xmath43 may vary mainly due to the value of the sum of probabilities in the denominators .",
    "however , since the precise value of these quantities is irrelevant , we have decided to scale them properly for the sake of comparison .",
    "although @xmath43 is built from a much smaller set than @xmath27 , it captures all the significant features of @xmath27 and can therefore be used instead of it . in this sense",
    ", @xmath43 provides a good stopping criterion for cd@xmath46 , although it is not as robust as @xmath27 due to the strong reduction of states contributing to @xmath43 as compared with those entering in @xmath27 .",
    "this reduction is illustrated in table  [ number - of - neighbours ] , where we show the number of neighboring states to the data set at different distances for the bs and lse problems . by increasing the number of states included in @xmath43 , convergence to @xmath27",
    "is expected at the expense of an increase in computational cost .",
    "however , the present results indicate that , at least for the problems at hand , a number of examples similar to that of the training set in the evaluation of @xmath43 is enough to detect the maximum of the log - likelihood of the data .",
    "all the results presented up to this point show the goodness of the proposed stopping criterion for learning in cd@xmath46 .",
    "however , the underlying idea can be applied to different learning algorithms that try to maximize the log - likelihood of the data . in this way we have repeated all the previous experiments for cd@xmath47 with very similar results to the ones above . as an example",
    ", figure  [ fig_lse_cd10 ] shows the log - likelihood , @xmath27 and @xmath41 with @xmath48 and cd@xmath47 for the lse data set , which is the largest one analyzed in this work . as it is clearly seen , the quality of the results is very similar to the cd@xmath46 case , thus stressing the robustness of the criterion .    as a final remark",
    ", we note that for the bs problem the trained rbm stopped using the proposed criterion is able to qualitatively generate samples similar to those in the training set .",
    "we show in figure  [ barritas ] the complete training set ( two upper rows ) and the same number of generated samples ( two lower rows ) obtained from the rbm trained with cd@xmath46 and stopped after 5000 epochs , around the maximum shown by @xmath49 , which approximately coincides with the optimal value of the log - likelihood .",
    "it is important to realize that , ultimately , the quality of the model is a direct measure of the quality of cd@xmath46 learning , and that the model used to generate the plots is the one with largest @xmath43 , which is quite close to the one with largest likelihood .",
    "in this work we have introduced the contribution of neighboring points to the training set to build a stopping criterion for learning in cd@xmath46 .",
    "we have shown that not only the training set but also the neighboring states contain valuable information that can be used to follow the evolution of the network along training .",
    "based on the fact that learning tries to increase the contribution of the relevant states while decreasing the contribution of the rest , continuity and smoothness of the energy function assigns more probability to states close to the training data .",
    "this is the key idea behind the proposed stopping criterion .",
    "in fact , two different but related estimators ( depending on the number of states used to compute them ) have been proposed and tested experimentally .",
    "the first one includes all states close to the training set , while the second one takes only a fraction of these states as small as the size of the training set .",
    "the first estimator is robust but may require from the use of a forbiddingly large amount of states , while the second one is always tractable and captures most of the features of the first one , thus providing a suitable stopping learning criterion .",
    "this second estimator could be used in larger data set problems , where an exact computation of the log - likelihood is not possible .",
    "additionally , the main idea of proximity to the training set will be explored in other aspects related to learning in future work .",
    "er : this research is partially funded by spanish research project tin2012 - 31377 .            a.  fischer and c.  igel , `` empirical analysis of the divergence of gibbs sampling based learning algorithms for restricted boltzmann machines , '' in _ international conference on artificial neural networks ( icann ) _ , vol .  3 , 2010 , pp .",
    "208217 .",
    "d.  e. rumelhart , g.  e. hinton , and r.  j. williams , `` learning internal representations by error propagation , '' in _",
    "parallel distributed processing : explorations in the microstructure of cognition , volume 1 : foundations _",
    ", d.  e. rumelhart , j.  l. mcclelland , and the pdp  research group . ,",
    "eds.1em plus 0.5em minus 0.4emmit press , 1986 .",
    "h.  lee , r.  grosse , r.  ranganath , and a.  y. ng , `` convolutional deep belief networks for scalable unsupervised learning of hierarchical representations , '' in _ international conference on machine learning _ , 2009 , pp .",
    "609616 .",
    "q.  v. le , m.  a. ranzato , r.  monga , m.  devin , k.  chen , g.  s. corrado , and a.  y. ng , `` building high - level features using large scale unsupervised learning , '' in _ 29th international conference on machine learning _ , 2012 .",
    "p.  smolensky , `` information processing in dynamical systems : foundations of harmony theory , '' in _",
    "parallel distributed processing : explorations in the microstructure of cognition ( vol .",
    "1 ) _ , d.  e. rumelhart and j.  l. mcclelland , eds.1em plus 0.5em minus 0.4emmit press , 1986 , pp .",
    "194281 .",
    "s.  geman and d.  geman , `` stochastic relaxation , gibbs distributions , and the bayesian restoration of images , '' _ ieee transactions on pattern analysis and machine intelligence _ , vol .  6 , no .  6 , pp .",
    "721741 , 1984 .",
    "y.  bengio , p.  lamblin , d.  popovici , and h.  larochelle , `` greedy layer - wise training of deep networks , '' in _ advances in neural information processing ( nips06 ) _ , vol .",
    "19.1em plus 0.5em minus 0.4emmit press , 2007 , pp .",
    "153160 .",
    "g.  desjardins , a.  courville , y.  bengio , p.  vincent , and o.  delalleau , `` parallel tempering for training of restricted boltzmann machines , '' in _ 13th international conference on artificial intelligence and statistics ( aistats ) _ , 2010 , pp . 145152 ."
  ],
  "abstract_text": [
    "<S> restricted boltzmann machines ( rbms ) are general unsupervised learning devices to ascertain generative models of data distributions . rbms are often trained using the contrastive divergence learning algorithm ( cd ) , an approximation to the gradient of the data log - likelihood . </S>",
    "<S> a simple reconstruction error is often used as a stopping criterion for cd , although several authors  @xcite have raised doubts concerning the feasibility of this procedure . in many cases </S>",
    "<S> the evolution curve of the reconstruction error is monotonic while the log - likelihood is not , thus indicating that the former is not a good estimator of the optimal stopping point for learning . </S>",
    "<S> however , not many alternatives to the reconstruction error have been discussed in the literature . in this manuscript </S>",
    "<S> we investigate simple alternatives to the reconstruction error , based on the inclusion of information contained in neighboring states to the training set , as a stopping criterion for cd learning . </S>"
  ]
}