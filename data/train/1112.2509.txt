{
  "article_text": [
    "in functional linear regression the dependence of a real - valued response @xmath0 on the variation of a random function @xmath1 is studied .",
    "typically the functional regressor @xmath1 is assumed to be square - integrable or more generally to take its values in a separable hilbert space @xmath2 with inner product @xmath3 and norm .",
    "furthermore , we suppose that @xmath0 and @xmath1 are centered , which simplifies the notations , and that the dependence between @xmath0 and @xmath1 is linear in the sense that @xmath4 for some slope function @xmath5 and error term @xmath6 with mean zero and variance one . assuming an independent and identically distributed ( i.i.d . )",
    "sample of @xmath7 , the objective of this paper is the construction of a fully data driven estimation procedure of the slope function @xmath8 which still can attain minimax - optimal rates of convergence .",
    "functional linear models have become very important in a diverse range of disciplines , including medicine , linguistics , chemometrics as well as econometrics ; see , for instance ,  @xcite and @xcite , for several case studies , or more specific , @xcite and  @xcite for applications in economics .",
    "the main class of estimation procedures of the slope function studied in the statistical literature is based on principal components regression ; see , for example , @xcite or  @xcite in the context of generalized linear models .",
    "the second important class of estimators relies on minimizing a penalized least squares criterion which can be seen as generalization of the ridge regression ; cf .",
    "@xcite and @xcite .",
    "more recently an estimator based on dimension reduction and threshold techniques has been proposed by cardot and johannes  @xcite which borrows ideas from the inverse problems community ( @xcite and @xcite ) .",
    "it is worth noting that all the proposed estimation procedures rely on the choice of at least one tuning parameter , which in turn , crucially influences the attainable accuracy of the constructed estimator .",
    "it has been shown , for example , in  @xcite , that the attainable accuracy of an estimator of the slope @xmath8 is essentially determined by a priori conditions imposed on both the slope function and the covariance operator @xmath9 associated to the random function @xmath1 ( defined below ) .",
    "these conditions are usually captured by suitably chosen classes @xmath10 and @xmath11 of slope functions and covariance operators , respectively .",
    "typically , the class @xmath12 characterizes the level of smoothness of the slope function , while the class @xmath13 specifies the decay of the sequence of eigenvalues of @xmath9 .",
    "for example ,  @xcite or @xcite consider differentiable slope functions and a polynomial decay of the eigenvalues of @xmath9 .",
    "furthermore , given a weighted norm @xmath14 and the completion @xmath15 of @xmath2 with respect to @xmath14 we shall measure the performance of an estimator @xmath16 of @xmath8 by its maximal @xmath17-risk over a class @xmath18 of slope functions and a class @xmath13 of covariance operators , that is , @xmath19:=\\sup_{{\\beta}\\in{{{\\mathcal}f}}}\\sup_{{\\gamma}\\in{{{\\mathcal}g}}}\\ex \\| { \\widehat{\\beta}}-{\\beta}\\|_{{\\omega}}^2.\\ ] ] this general framework with appropriate choice of the weighted norm allows us to cover the prediction problem with respect to the mean squared prediction error ( see , e.g. , @xcite or @xcite ) and the estimation not only of the slope function ( see , e.g. ,  @xcite ) but also of its derivatives . for a detailed discussion",
    ", we refer to @xcite . having these applications in mind",
    "the additional condition @xmath18 only means that the estimation of a derivative of the slope function necessitates its existence . assuming an i.i.d .",
    "sample of @xmath7 of size @xmath20 obeying model ( [ introe1 ] ) cardot and johannes @xcite have derived a lower bound of the maximal weighted risk , that is , @xmath21\\leq c \\inf_{{\\widehat{\\beta}}}r_{\\omega}[{\\widehat{\\beta } } ; { { { \\mathcal}f}},{{{\\mathcal}g}}]\\ ] ] for some finite positive constant @xmath22 where the infimum is taken over all possible estimators @xmath23 .",
    "moreover , they have shown that a thresholded projection estimator @xmath24 in dependence of an optimally chosen tuning parameter @xmath25 can attain this lower bound up to a constant @xmath26 , @xmath27\\leq c r_{\\omega}^*[n;{{{\\mathcal}f } } , { { { \\mathcal}g}}]\\ ] ] for a variety of classes @xmath12 and @xmath13 .",
    "in other words , @xmath28 $ ] is the minimax rate of convergence and @xmath24 is minimax - optimal .",
    "the optimal choice @xmath29 of the tuning parameter , however , follows from a classical squared - bias - variance compromise and requires an a priori knowledge about the classes @xmath12 and @xmath13 , which is usually inaccessible in practice .    in this paper",
    "we propose a fully data driven method to select a tuning parameter @xmath30 in such a way that the resulting data - driven estimator @xmath31 can still attain the minimax - rate @xmath28 $ ] up to a constant over a variety of classes @xmath12 and @xmath13 .",
    "it is interesting to note that , considering a linear regression model with infinitely many regressors , goldenshluger and tsybakov @xcite propose an optimal data - driven prediction procedure allowing sharp oracle inequalities .",
    "however , a straightforward application of their results is not obvious to us since they assume a priori standardized regressors , which in turn , in functional linear regression necessitates the covariance operator @xmath9 to be fully known in advance .",
    "in contrast , given a jointly normally distributed regressor and error term , verzelen  @xcite establishes sharp oracle inequalities for the prediction problem in case the covariance operator is not known in advance . although , it is worth noting that considering the mean prediction error as risk eliminates the ill - posedness of the underlying problem , which in turn leads to faster minimax rates of convergences of the prediction error than , for example , the mean integrated squared error .",
    "cai and zhou  @xcite present a fully data - driven estimation procedure of the slope function which attains optimal rates of convergence with respect to the maximal mean integrated squared error . on the other hand , covering both of these two risks within the general framework discussed above , comte and johannes  @xcite consider functional linear regression with circular functional regressor which results in a partial knowledge of the associated covariance operator , that is , its eigenfunctions are known in advance , but the eigenvalues have to be estimated .",
    "in this situation , comte and johannes  @xcite have applied successfully a model selection approach which is inspired by the work of @xcite now extensively discussed in @xcite . in the circular case",
    ", it is possible to develop the unknown slope function in the eigenbasis of the covariance operator , which in turn , allows one to derive an orthogonal series estimator in dependence of a dimension parameter .",
    "this dimension parameter has been chosen fully data driven by a model selection approach , and it is shown that the resulting data - driven orthogonal series estimator can attain minimax - optimal rates of convergence up to a constant .",
    "although , the proof crucially relies on the possibility to write the orthogonal series estimator as a minimizer of a contrast .    in this paper we do not impose an a priori knowledge of the eigenbasis , and hence the orthogonal series estimator is no more accessible to us . instead , we consider the thresholded projection estimator @xmath32 as presented in  @xcite which we did not succeed to write as a minimizer of a contrast .",
    "therefore , our selection method combines model selection and lepski s method ( cf .",
    "@xcite and its recent review in  @xcite ) which is inspired by a bandwidth selection method in kernel density estimation proposed recently in @xcite .",
    "selecting the dimension parameter @xmath30 as minimizer of a stochastic penalized contrast function imitating lepski s method among a random collection of admissible values , we show that the fully data - driven estimator @xmath33 can attain the minimax - rate up to a constant @xmath26 , that is , @xmath34\\leq",
    "c\\cdot r_{\\omega}^\\star[n;{{{\\mathcal}f}},{{{\\mathcal}g}}]\\ ] ] for a variety of classes @xmath12 and @xmath13 .",
    "we shall emphasize that in contrast to the result obtained in  @xcite , we show that the proposed estimator can attain minimax - optimal rates without specifying in advance neither that the slope function belongs to a class of differentiable or analytic functions nor that the decay of the eigenvalues is polynomial or exponential .",
    "the only price for this flexibility is in term of the constant @xmath22 which is asymptotically not equal to one ; that is , the oracle inequality ( [ introe2 ] ) is not sharp .",
    "the paper is organized as follows : in section  [ secmet ] we briefly introduce the thresholded projection estimator @xmath35 as proposed in @xcite .",
    "we present the data driven method to select the tuning parameter and prove a first upper risk - bound for the fully data - driven estimator @xmath33 which emphasizes the key arguments . in section  [ secminimax ] we review the available minimax theory as presented in  @xcite . within this general framework",
    "we derive upper risk - bounds for the fully - data driven estimator imposing additional assumptions on the distribution of the functional regressor @xmath1 and the error term @xmath6 .",
    "namely , we suppose first that @xmath1 and @xmath6 are gaussian random variables and second that they satisfy certain moment conditions . in both cases",
    "the proof of the upper risk - bound employs the key arguments given in section  [ secmet ] , while more technical aspects are deferred to the .",
    "the results in this paper are illustrated considering different configurations of classes @xmath12 and @xmath13 .",
    "we recall the minimax - rates in this situations and show that up to a constant , these rates are attained by the fully - data driven estimator . a simulation study illustrating the reasonable performance of the fully data - driven estimation procedure is available at the supplementary material archive .",
    "consider the functional linear model ( [ introe1 ] ) where the random function @xmath1 and the error term @xmath6 are independent .",
    "let the centred random function @xmath1 , that is , @xmath36 for all @xmath37 , have a finite second moment , that is , @xmath38 .",
    "multiplying both sides in ( [ introe1 ] ) by @xmath39 and taking the expectation leads to the normal equation @xmath40= \\ex\\bigl[\\langle{\\beta},x\\rangle_{{\\mathbb h}}\\langle x , h\\rangle_{{\\mathbb h}}\\bigr]=:\\langle{\\gamma}{\\beta},h\\rangle_{{\\mathbb h}}\\qquad \\forall h\\in{{\\mathbb h}},\\ ] ] where @xmath41 belongs to @xmath2 , and @xmath9 denotes the covariance operator associated to the random function @xmath1 . throughout the paper",
    "we shall assume that there exists a solution @xmath5 of equation ( [ mete1 ] ) and that the covariance operator @xmath9 is strictly positive definite which ensures the identifiability of the slope function @xmath8 ; cf .",
    "however , due to the finite second moment of @xmath1 the associated covariance operator @xmath9 has a finite trace ; that is , it is nuclear .",
    "thereby , solving equation ( [ mete1 ] ) is an _ ill - posed inverse problem _ with the additional difficulty that @xmath9 is unknown and has to be estimated ; for a detailed discussion of ill - posed inverse problems in general we refer to  @xcite .      in this paper",
    ", we follow  @xcite and consider a linear galerkin approach to derive an estimator of the slope function @xmath8 . here and subsequently , let @xmath42 be a pre - specified orthonormal basis in @xmath2 which in general does not correspond to the eigenbasis of the operator @xmath9 defined in ( [ mete1 ] ) . with respect to this basis ,",
    "we consider for all @xmath43 the development @xmath44_j{\\psi}_j$ ] where the sequence @xmath45_j)_{j\\geq1}$ ] with generic elements @xmath46_j:=\\langle h,{\\psi}_j\\rangle_{{\\mathbb h}}$ ] is square - summable , that is , @xmath47_j^2<\\infty$ ] .",
    "moreover , given any strictly positive sequence of weights @xmath48 define the weighted norm @xmath49_j^2 $ ] .",
    "we will refer to any sequence as a whole by omitting its index as , for example , in `` the sequence of weights @xmath50 . '' furthermore , for @xmath51 let @xmath46_{{{\\underline{m}}}}:=([h]_1,\\ldots,[h]_m)^t$ ] ( where @xmath52 is the transpose of @xmath53 ) , and let @xmath54 be the subspace of @xmath2 spanned by @xmath55 .",
    "obviously , the norm of @xmath56 equals the euclidean norm of its coefficient vector @xmath46_{{{\\underline{m}}}}$ ] , that is , @xmath57_{{\\underline{m}}}^t[h]_{{\\underline{m}}})^{1/2}=:\\vert[h]_{{\\underline{m}}}\\vert$ ] with a slight abuse of notation .",
    "an element @xmath58 satisfying @xmath59 for all @xmath60 , is called a galerkin solution of equation ( [ mete1 ] ) . since the covariance operator @xmath9 is strictly positive definite , it follows that the covariance matrix @xmath61_{{{\\underline{m}}}}:=\\ex([x]_{{{\\underline{m}}}}[x]_{{{\\underline{m}}}}^t)$ ] associated with the @xmath62-dimensional random vector @xmath63_{{{\\underline{m}}}}$ ] is strictly positive definite too .",
    "consequently , the galerkin solution @xmath64 is uniquely determined by @xmath65_{{{\\underline{m}}}}=[{\\gamma}]_{{{\\underline{m}}}}^{-1}[g]_{{{\\underline{m}}}}$ ] and @xmath66_j=0 $ ] for all @xmath67 . although , it does generally not correspond to the orthogonal projection of @xmath8 onto the subspace @xmath54 and the approximation error @xmath68 does generally not converge to zero as @xmath69 . here and subsequently",
    ", however , we restrict ourselves to classes @xmath12 and @xmath13 of slope functions and covariance operators , respectively , which ensure the convergence .",
    "obviously , this is a minimal regularity condition for us since we aim to estimate the galerkin solution .    assuming a sample @xmath70 of @xmath7 of size @xmath20",
    ", it is natural to consider the estimators @xmath71 and @xmath72 for @xmath41 and @xmath9 , respectively .",
    "moreover , let @xmath73_{{{\\underline{m}}}}:=n^{-1}\\sum_{i=1}^n [ x_i]_{{{\\underline{m}}}}[x_i]_{{{\\underline{m}}}}^t$ ] and note that @xmath74_{{{\\underline{m}}}}= n^{-1}\\sum_{i=1}^n y_i[x_i]_{{{\\underline{m}}}}$ ] .",
    "replacing the unknown quantities by their empirical counterparts @xmath75 denotes a galerkin solution satisfying @xmath76 for all @xmath60 .",
    "observe that there exists always a solution @xmath77 , but it might not be unique . obviously , if @xmath73_{{{\\underline{m}}}}$ ] is nonsingular , then @xmath78_{{{\\underline{m}}}}=[{\\widehat{{\\gamma}}}]_{{{\\underline{m}}}}^{-1 } [ { \\widehat{g}}]_{{{\\underline{m}}}}$ ] .",
    "we shall emphasize the multiplication with the inverse of the random matrix @xmath73_{{{\\underline{m}}}}$ ] which may result in an unstable estimator even in case @xmath61_{{{\\underline{m}}}}$ ] is well conditioned .",
    "let @xmath79^{-1}_{{{\\underline{m}}}}\\vert_s\\leq n\\}}$ ] denote the indicator function which takes the value one if @xmath73_{{{\\underline{m}}}}$ ] is nonsingular with spectral norm @xmath80_{{{\\underline{m}}}}^{-1}\\vert_s:=\\sup_{\\vert z\\vert=1}\\vert[{\\widehat{{\\gamma}}}]_{{{\\underline{m}}}}^{-1}z\\vert$ ] of its inverse bounded by @xmath20 , and the value zero otherwise .",
    "the estimator of @xmath8 proposed in  @xcite consists of thresholding the estimated galerkin solution , that is , @xmath81^{-1}_{{{\\underline{m}}}}\\vert_s\\leq n\\}}.\\ ] ] in the next paragraph we introduce a data - driven method to select the dimension parameter @xmath82 .      given a random integer @xmath83 and a random sub sequence of penalties @xmath84 , we select the dimension parameter @xmath30 among the random collection of admissible values @xmath85 as minimizer of a penalized contrast criterion . to be precise , setting @xmath86 for a sequence @xmath87 with minimal value in @xmath88 , we define @xmath89 the data - driven estimator of @xmath8 is now given by @xmath33 , and below we derive an upper bound for its maximal @xmath17-risk .",
    "the choice of the @xmath15-risk as performance measure is reflected in the definition of the contrasts , that is , @xmath90 the construction of the random penalty sequence @xmath91 and the upper bound @xmath83 given below is guided by the key arguments used in the proof of the @xmath15-risk bound which we present first .",
    "a central step for our reasoning is the next assertion which employs essentially the particular choice of the contrast .",
    "[ elementarybound]consider the approximation errors @xmath92 , @xmath51 . if the sub sequence @xmath84 is nondecreasing , then we have @xmath93 for all @xmath94 , where @xmath95 .    from the definition of @xmath30 we deduce for all @xmath94 that @xmath96\\\\[-8pt ] & \\leq&6 { \\lbrace\\psi_m + \\hpen_m \\rbrace}+3 \\| { \\widehat{\\beta}}_{m}-{\\beta}\\|^2_{\\omega}. \\nonumber\\end{aligned}\\ ] ] first , employing an elementary triangular inequality allows us to write @xmath97 for all @xmath94 .",
    "second , since @xmath84 is nondecreasing and @xmath98 , @xmath94 , it is easily verified that @xmath99 combining the last two inequalities and ( [ prelementarybounde1 ] ) , we obtain the result .    keeping the last assertion in mind we decompose the @xmath15-risk with respect to an event on which the quantities @xmath100 and @xmath83 are close to some theoretical counterparts @xmath101 , @xmath102 and @xmath103 .",
    "more precisely , define the event @xmath104 and the corresponding risk decomposition @xmath105 consider the first right - hand side ( r.h.s . ) term .",
    "if @xmath84 is nondecreasing , then we may apply lemma  [ elementarybound ] which on the event @xmath106 implies @xmath107 where @xmath108 realizes a penalty - squared - bias compromise among the collection of admissible values @xmath109 . keeping in mind that @xmath108 should mimic the value of the optimal variance - squared - bias trade - off , we wish the upper bound @xmath102 to be as large as possible .",
    "in contrast , in order to control the remainder term , the second r.h.s .",
    "term , we are forced to use a rather small upper bound @xmath110 to ensure that the penalty term is uniformly bounded with increasing sample size .",
    "however , we bound the remainder term by imposing the following assumption , which though holds true for a wide range of classes @xmath12 and @xmath13 under reasonable assumptions on the distribution of @xmath6 and @xmath1 ; see propositions [ gaussp1 ] and  [ momp1 ] in section  [ secminimax ] .",
    "[ assremainder - i ] there exists a constant @xmath111 such that @xmath112    roughly speaking , the penalty term @xmath101 should provide an upper bound for the estimator s variation which allows us to establish a concentration inequality for the -norm of the corresponding empirical process . however , under assumption  [ assremainder - i ] we bound the first r.h.s .",
    "term in ( [ defdecomp ] ) by @xmath113 it remains to consider the second r.h.s .",
    "the conditions on the distribution of @xmath6 and @xmath1 presented in the next section are also sufficient to show that the following assumption holds true .",
    "[ assremainder - ii ] there exists a constant @xmath114 such that @xmath115    under assumption  [ assremainder - ii ] , @xmath83 and @xmath100 behave similarly to their theoretical counterparts with sufficiently high probability so as not to deteriorate the estimators risk .",
    "the next assertion provides an upper bound for the maximal @xmath15-risk over the classes @xmath12 and @xmath13 of the thresholded projection estimator @xmath31 with data - driven choice @xmath30 given by ( [ defwhm ] ) .",
    "[ methp1 ] suppose that @xmath84 is nondecreasing .",
    "if assumptions  [ assremainder - i ] and [ assremainder - ii ] hold true , then for all @xmath116 we have @xmath117\\leq582 \\sup_{{\\beta}\\in{{{\\mathcal}f}}}\\sup_{{\\gamma}\\in{{{\\mathcal}g } } } \\max\\{\\pen_{{m^\\diamond_n}},\\bias_{{m^\\diamond_n}}^2\\}+ ( 42 k_1+k_2 ) n^{-1}$ ] .",
    "keeping in mind the risk decomposition ( [ defdecomp ] ) the upper bound ( [ defdecompe1 ] ) and assumption  [ assremainder - ii ] imply the result .    the first r.h.s .",
    "term in the last upper risk - bound is strongly reminiscent of a variance - squared - bias decomposition of the @xmath15-risk for the estimator @xmath118 with dimension parameter @xmath108 . indeed , in many cases the penalty term @xmath119 is in the same order as the variance of the estimator @xmath118 ; cf .",
    "illust1][p - p ] and [ e - p ] below .",
    "consequently , in this situation the upper risk bound of the data - driven estimator is essentially given by @xmath120 $ ] .",
    "moreover , by balancing penalty and squared - bias @xmath108 just realizes the optimal trade - off between variance and squared - bias which in turn in many cases means that @xmath121 $ ] is of optimal order.=1    we complete this section by introducing our choice for the random upper bound @xmath83 and the random penalty @xmath100 which takes its inspiration from  @xcite .",
    "let us first define some auxiliary quantities required in the construction .",
    "for @xmath51 , let @xmath122_{{{\\underline{m}}}}$ ] denote the @xmath62-dimensional diagonal matrix with diagonal entries @xmath123 , and for any sequence @xmath124:=([k]_{{{\\underline{k}}}})_{k\\geq1}$ ] of matrices , define=1 @xmath125}&:=&\\max_{1\\leq k\\leq m}\\bigl\\| [ \\diag_{{\\omega}}]_{{{\\underline{k}}}}^{1/2}[k]_{{{\\underline{k}}}}^{-1 } [ \\diag_{{\\omega}}]_{{{\\underline{k}}}}^{1/2}\\bigr\\|_{s } \\quad\\mbox{and } \\nonumber\\\\[-8pt]\\\\[-8pt ] \\delta_m^{[k]}&:=&m \\delta_m^{[k ] } \\frac{\\log(\\delta_m^{[k]}\\vee ( m+2))}{\\log(m+2)}. \\nonumber\\end{aligned}\\]]=0 for @xmath116 , set @xmath126 with integer part @xmath127 of @xmath128 and @xmath129 . for any sequence @xmath130 let @xmath131 where we set @xmath132 if the defining set is empty .",
    "given the sequence of covariance matrices @xmath61=([{\\gamma}]_{{\\underline{m}}})_{m\\geq1}$ ] associated with the regressor @xmath1 , define@xmath133 } n^{-1 } \\qquad\\mbox{with } \\sigma_m^2:=2 \\bigl(\\ex y^2+[g ] _ { { { \\underline{m}}}}^t[{\\gamma}]_{{{\\underline{m}}}}^{-1}[g]_{{{\\underline{m } } } } \\bigr ) \\quad\\mbox{and } \\nonumber\\\\[-8pt]\\\\[-8pt ] m^{\\gamma}&:= & m_n(a ) \\qquad\\mbox{with } a:=\\bigl(\\bigl\\| [ { \\gamma}]_{{{\\underline{m}}}}^{-1}\\bigr\\|_{s}\\bigr)_{m\\geq1 } , \\nonumber\\end{aligned}\\ ] ] where @xmath134 is a positive numerical constant to be chosen below .",
    "roughly speaking the penalty term provides an upper bound of the variance of the estimator @xmath32 and is in many cases even in the same order .",
    "its construction , however , allows a deterioration to ensure that assumption  [ assremainder - i ] can be satisfied ; cf .",
    "illustration  [ illust1][p - e ] .",
    "moreover , for growing sample size @xmath20 the penalty sequence is uniformly bounded over the collection of admissible values @xmath135 .",
    "note that the penalty and the upper bound still depend on unknown quantities which , however , can easily be estimated , that is,=1 @xmath136 } n^{-1 } \\nonumber\\\\ & & \\qquad\\hspace*{-40pt}\\mbox{with } { \\widehat{\\sigma}}_m^2:=2 \\biggl ( \\frac { 1}{n}\\sum_{i=1}^n y^2_i+[{\\widehat{g}}]_{{{\\underline{m}}}}^t [ { \\widehat{{\\gamma}}}]_{{{\\underline{m}}}}^{-1}[{\\widehat{g}}]_{{{\\underline{m } } } } \\biggr)\\quad\\mbox{and}\\hspace*{-12pt } \\\\ { \\widehat{m}}&:= & m_n(a ) \\qquad\\mbox{with } a:=\\bigl(\\bigl\\|[\\widehat { { \\gamma}}]_{{{\\underline{m}}}}^{-1}\\bigr\\|_{s}\\bigr)_{m\\geq1}. \\nonumber\\end{aligned}\\]]=0 note that by construction @xmath84 is nondecreasing .",
    "indeed , the identity @xmath137_{{{\\underline{k}}}}^t[\\widehat{{\\gamma}}]_{{{\\underline{k}}}}^{-1}[{\\widehat{g}}]_{{{\\underline{k}}}}-[{\\widehat{g}}]_{{{\\underline{m}}}}^t[\\widehat{{\\gamma}}]_{{{\\underline{m}}}}^{-1}[{\\widehat{g}}]_{{{\\underline{m}}}}$ ] holds true for all @xmath138 .",
    "since @xmath139 is positive definite , @xmath74_{{{\\underline{m}}}}^t[\\widehat{{\\gamma}}]_{{{\\underline{m}}}}^{-1}[{\\widehat{g}}]_{{{\\underline{m}}}}\\leq[{\\widehat{g}}]_{{{\\underline{k}}}}^t[\\widehat{{\\gamma}}]_{{{\\underline{k}}}}^{-1}[{\\widehat{g}}]_{{{\\underline{k}}}}$ ] and @xmath140 which in turn implies the assertion .",
    "consequently , we may apply proposition [ methp1 ] if assumptions  [ assremainder - i ] and [ assremainder - ii ] hold true .",
    "in this section we recall first a general framework proposed by cardot and johannes  @xcite which allows us to derive minimax - optimal rates for the maximal @xmath15-risk , @xmath141 , over the classes @xmath12 and  @xmath13 .",
    "the classes @xmath12 and @xmath13 of slope functions and covariance operators , respectively , are characterized by different weighted norms in @xmath2 with respect to the pre - specified orthonormal basis @xmath142 . given a strictly positive sequence of weights @xmath143 and a radius @xmath144 , let @xmath145 be the completion of @xmath2 with respect to the weighted norm and the ellipsoid @xmath146 be the class of possible slope functions .",
    "furthermore , as usual in the context of ill - posed inverse problems , we link the mapping properties of the covariance operator @xmath9 and the regularity condition @xmath147 .",
    "denote by @xmath148 the set of all strictly positive nuclear operators defined on @xmath2 . given a strictly positive sequence of weights @xmath149 and a constant @xmath150",
    "define the class of covariance operators by@xmath151 where arithmetic operations on sequences are defined element - wise , for example , @xmath152 .",
    "let us briefly discuss the last definition . if @xmath153 , then we have @xmath154 , for all @xmath155 .",
    "consequently , the sequence @xmath149 is necessarily summable , because @xmath156 is nuclear .",
    "moreover , if @xmath157 denotes the sequence of eigenvalues of @xmath156 , then @xmath158 , for all @xmath155 .",
    "in other words the sequence @xmath149 characterizes the decay of the eigenvalues of @xmath153 .",
    "we do not specify the sequences of weights @xmath50 , @xmath143 and @xmath149 , but impose from now on the following minimal regularity conditions .",
    "[ assreg ] let @xmath50 , @xmath143 and @xmath149 be strictly positive sequences of weights with @xmath159 , and @xmath160 such that the sequences @xmath161 , @xmath162 , @xmath149 and @xmath163 are monotonically nonincreasing and converging to zero .",
    "the last assumption is fairly mild .",
    "for example , assuming that @xmath162 is nonincreasing , ensures that @xmath164 .",
    "furthermore , it is shown in @xcite that the minimax rate @xmath165 $ ] is of order @xmath166 for all sequences @xmath149 and @xmath50 such that @xmath163 is nondecreasing",
    ". we will illustrate all our results considering the following three configurations for the sequences @xmath50 , @xmath143 and @xmath149 .",
    "[ illust1 ] in all three cases , we take @xmath167 , @xmath155",
    ". moreover , let :    @xmath168 and @xmath169 , @xmath170 , with @xmath171 , @xmath172 and @xmath173 ;    @xmath174 and @xmath169 , @xmath155 , with @xmath171 , @xmath172 , @xmath175 ;    @xmath168 and @xmath176 , @xmath155 , with @xmath171 , @xmath177 , and @xmath178 ;    then assumption  [ assreg ] is satisfied in all cases .",
    "in the configurations [ p - p ] and [ e - p ] , the case @xmath179 can be interpreted as mean - prediction error ; cf .",
    "moreover , if @xmath180 is the trigonometric basis and the value of @xmath181 is an integer , then the weighted norm @xmath182 corresponds to the @xmath183-norm of the weak @xmath181th derivative of @xmath184 ; cf .  @xcite . in other words in this situation we consider as risk the mean integrated squared error when estimating the @xmath181th derivative of @xmath8 . in the configurations [ p - p ] and [ p - e ]",
    ", the additional condition @xmath185 means that the slope function has at least @xmath186 weak derivatives , while for a value @xmath187 in [ e - p ] , the slope function is assumed to be an analytic function ; cf .",
    "@xcite .",
    "let us first recall a lower bound of the maximal @xmath15-risk over the classes @xmath188 and @xmath189 due to  @xcite .",
    "given an i.i.d .",
    "sample of @xmath7 of size @xmath20 and sequences as in assumption  [ assreg ] , define @xmath190\\\\[-8pt ] { r^*_n}&:=&\\max \\biggl ( \\frac{{\\omega}_{{m^*_n}}}{b_{{m^*_n } } } , \\sum_{j=1}^{{m^*_n } } \\frac{{\\omega}_j}{n{\\gamma}_j } \\biggr).\\nonumber\\end{aligned}\\ ] ] if @xmath191 , then there exists a constant @xmath192 depending on @xmath193 and @xmath194 only such that@xmath195 \\geq c { r^*_n } \\qquad\\mbox{for all } n\\geq1.\\ ] ] on the other hand , considering the dimension parameter @xmath29 given in ( [ defmstarn ] ) cardot and johannes  @xcite have shown that the maximal risk @xmath196 $ ] of the estimator @xmath197 defined in ( [ introestimator ] ) is bounded by @xmath198 up to constant for a wide range of sequences @xmath50 , @xmath143 and @xmath149 , provided the random function @xmath1 and the error @xmath6 satisfy certain additional moment conditions .",
    "in other words @xmath199 $ ] is the minimax - rate in this situation , and the estimator @xmath200 is minimax optimal ; although , the definition of the dimension parameter @xmath29 necessitates an a priori knowledge of the sequences @xmath143 and @xmath149 . in the remaining part of this paper",
    "we show that the data - driven choice of the dimension parameter constructed in section [ secmet ] can automatically attain the minimax - rate @xmath198 for a variety of sequences @xmath50 , @xmath143 and @xmath149 .",
    "first , let us briefly illustrate the minimax result .",
    "[ illust2 ] considering the three configurations ( see illustration  [ illust1 ] ) , it has been shown in @xcite that the estimator @xmath200 with @xmath29 as given below attains the rate @xmath201 up to a constant .",
    "we write for two strictly positive sequences @xmath202 and @xmath203 that @xmath204 , if @xmath205 is bounded away from @xmath206 and infinity .    if @xmath207 , then @xmath208 , while @xmath209}$ ] for @xmath210",
    ". thus , @xmath211 for @xmath212 .",
    "if @xmath213 , then @xmath214}$ ] and @xmath215 .",
    "if @xmath207 , then @xmath216 and @xmath217 , while @xmath218 and @xmath219 for @xmath210 [ and @xmath220 for @xmath221 .    @xmath222 and @xmath223 .",
    "an increasing value of the parameter @xmath224 leads in all three cases to a slower rate @xmath198 , and hence it is called degree of ill - posedness ; cf .  @xcite .      consider the thresholded projection estimator @xmath31 with data - driven choice @xmath30 of the dimension parameter .",
    "supposing that the joint distribution of the random function @xmath1 and the error term @xmath6 satisfies certain additional conditions , we will prove below that assumptions [ assremainder - i ] and  [ assremainder - ii ] formulated in section  [ secmet ] hold true .",
    "these assumptions rely on the existence of sequences @xmath225 and @xmath226 which amongst others we define now referring only to the classes @xmath188 and @xmath189 .",
    "keep in mind the notation given in ( [ defdelta ] ) and ( [ defm ] ) . for @xmath227 and @xmath228=([\\diag_{\\gamma}]_{{{\\underline{m}}}})_{m\\geq1}$",
    "] define @xmath229}$ ] and @xmath230}$ ] , set @xmath231 and @xmath232 , and let @xmath233 where @xmath234 .",
    "let @xmath235 denote a finite constant such that @xmath236 which by construction always exists and depends on the class @xmath189 only .",
    "we illustrate below the last definitions by revisiting the three configurations for the sequences @xmath50 , @xmath143 and @xmath149 ( illustration [ illust1 ] ) .",
    "[ illust3]in the following we state the order of @xmath102 and @xmath237 which in turn are used to derive the order of @xmath108 and @xmath238 .",
    "@xmath239 , @xmath240 and for @xmath241 it follows @xmath242}$ ] and @xmath243}$ ] ;    @xmath239 , @xmath240 and for @xmath244 , @xmath245 and @xmath246/(2p)}$ ] ;    @xmath247 , @xmath248 and for @xmath241 , it follows @xmath249 and @xmath250 .",
    "we proceed by formalizing additional conditions on the joint distribution of @xmath6 and @xmath1 , allowing us to prove that assumptions  [ assremainder - i ] and  [ assremainder - ii ] hold true .",
    "let us first assume that @xmath1 is a centred gaussian @xmath2-valued random variable ; that is , for all @xmath251 and for all finite collections @xmath252 the joint distribution of the real valued random variables @xmath253 is gaussian with zero mean vector and covariance matrix with generic elements @xmath254 , @xmath255 .",
    "moreover , suppose that the error term is standard normally distributed .",
    "[ assgauss]the joint distribution of @xmath1 and @xmath6 is normal .",
    "the more involved proof of the next assertion is deferred to appendix [ appgauss ] .",
    "[ gaussp1]assume an i.i.d .",
    "@xmath20-sample of @xmath7 obeying ( [ introe1 ] ) and assumption  [ assgauss ] .",
    "consider sequences @xmath50 , @xmath143 and @xmath149 satisfying assumption  [ assreg ] and set @xmath256 in the definition ( [ defpen ] ) and ( [ defhpenmen ] ) of the penalty @xmath257 and @xmath91 , respectively .",
    "for the classes @xmath188 and @xmath189 there exist finite constants @xmath258 and @xmath259 depending on @xmath260 only such that assumptions [ assremainder - i ] and  [ assremainder - ii ] with @xmath261 and @xmath262 , respectively , holds true .    by taking the value",
    "@xmath256 the random penalty and upper bound given in ( [ defhpenmen ] ) depend indeed only on the data , and hence the choice @xmath30 in ( [ defwhm ] ) is fully data - driven .",
    "moreover , we can apply proposition  [ methp1 ] to prove the next upper risk - bound for the data - driven thresholded projection estimator @xmath31 .",
    "[ gaussth1]let the assumptions of proposition [ gaussp1 ] be satisfied .",
    "there exists a finite constant @xmath263 depending on @xmath260 only such that @xmath264\\leq k \\bigl(\\sigma^2+r\\bigr ) { \\bigl\\lbrace{r^\\diamond_n}+ \\sigma n^{-1 } \\bigr \\rbrace } \\qquad\\mbox{for all } n\\geq1.\\ ] ]    we shall provide in the among others , the two technical lemmas [ appprel1 ] and  [ appprel2 ] which are used in the following .",
    "moreover , we denote by @xmath263 a constant depending on @xmath260 only which changes from line to line . making use of proposition  [ gaussp1 ]",
    "we intend to apply proposition  [ methp1 ] . to this end , if @xmath265 and @xmath266 , then first from [ appprel1e4 ] in lemma  [ appprel1 ] it follows that @xmath267 because @xmath163 and @xmath162 are nonincreasing due to assumption  [ assreg ] .",
    "second , by combination of [ appprel2e1 ] and  [ appprel2e4 ] in lemma  [ appprel2 ] , it is easily verified that @xmath268 .",
    "consequently , @xmath269 for all @xmath116 by combination of the last two estimates and the definition of @xmath238 which in turn together with proposition  [ methp1 ] implies the assertion of the theorem .",
    "we now dismiss assumption [ assgauss ] and formalize in its place , conditions on the moments of the random function @xmath1 and the error term  @xmath6 .",
    "in particular we use that for all @xmath37 with @xmath270 , the random variable @xmath271 is standardized , that is , has mean zero and variance one .",
    "[ assmom]there exist a finite integer @xmath272 and a finite constant such that @xmath273 and that for all @xmath37 with @xmath270 the standardized random variable @xmath271 satisfies latexmath:[$\\ex     it is worth noting that for any gaussian random function @xmath1 with finite second moment , assumption  [ assmom ] holds true , since for all @xmath37 with @xmath270 the random variable @xmath275 is standard normally distributed and hence @xmath276 .",
    "the proof of the next assertion is again rather involved and deferred to appendix [ appmom ] .",
    "it follows , however , along the general lines of the proof of proposition  [ methp1 ] though it is not a straightforward extension .",
    "take as an example the concentration inequality for the random variable @xmath277^{1/2}_{{\\underline{m}}}([{\\widehat{g}}]_{{\\underline{m}}}- [ { \\widehat{{\\gamma}}}]_{{\\underline{m}}}[{\\beta}^{m}]_{{\\underline{m}}})\\|$ ] in lemma  [ appgaussl2 ] in appendix [ appgauss ] which due to assumption  [ assgauss ] is shown by employing elementary inequalities for gaussian random variables . in contrast , the proof of an analogous result under assumption [ assmom ] given in lemma  [ appgenerall2 ] in appendix  [ appmom ] is based on an inequality due to talagrand  @xcite ( proposition [ appgeneraltala ] in the states a version as presented in  @xcite ) .",
    "[ momp1]assume an i.i.d .",
    "@xmath20-sample of @xmath7 obeying ( [ introe1 ] ) and assumption  [ assmom ] .",
    "consider sequences as in assumption  [ assreg ] and set @xmath278 in the definition ( [ defpen ] ) and ( [ defhpenmen ] ) of the penalty @xmath257 and @xmath91 , respectively .",
    "for the classes @xmath188 and @xmath189 , there exist finite constants @xmath279 depending on @xmath280 , @xmath281 and the classes @xmath188 and @xmath189 only , and @xmath259 depending on @xmath260 only , such that assumptions  [ assremainder - i ] and  [ assremainder - ii ] with @xmath282 and @xmath283 , respectively , hold true .",
    "we remark on a change only in the constants when comparing the last proposition with proposition  [ gaussp1 ] .",
    "note further that we need a larger value for the constant @xmath134 than in proposition  [ gaussp1 ] although it is still a numerical constant and hence the choice @xmath30 given by ( [ defwhm ] ) is again fully data - driven .",
    "moreover , both values for the constant  @xmath134 , though convenient for deriving the theory , are far too large in practice . in our simulation study they are instead determined by means of preliminary simulations as proposed in @xcite , for example .",
    "the next assertion provides an upper risk - bound for the data - driven thresholded projection estimator @xmath31 when imposing moment conditions .",
    "[ momth1]let the assumptions of proposition [ momp1 ] be satisfied . there exist finite constants @xmath263 depending on @xmath260 only and @xmath284 depending on @xmath280 , @xmath285 and the classes @xmath188 and @xmath189 only such that @xmath264\\leq k \\bigl(\\sigma^2+r\\bigr ) { \\bigl\\lbrace{r^\\diamond_n}+ k ' \\eta^{64 } \\sigma n^{-1 } \\bigr\\rbrace } \\qquad\\mbox{for all } n\\geq1.\\ ] ]    taking into account proposition  [ momp1 ] rather than proposition  [ gaussp1 ] we follow line by line the proof of theorem  [ gaussth1 ] and we omit the details .",
    "a comparison of the upper bounds in both theorems  [ gaussth1 ] and [ momth1 ] with the lower bound displayed in ( [ deflower ] ) shows that the data - driven estimator @xmath31 attains up to a constant the minimax - rate @xmath286 only if @xmath287 has the same order as @xmath198 .",
    "note that , by construction , @xmath288 for all @xmath51 .",
    "the next assertion is an immediate consequence of theorems [ gaussth1 ] and  [ momth1 ] , and we omit its proof .",
    "[ adapcoro ] let the assumptions of either theorems [ gaussth1 ] or  [ momth1 ] be satisfied .",
    "if @xmath289 holds true , then @xmath290\\leq c \\cdot \\inf_{{\\widetilde{\\beta}}}r_{\\omega}[{\\widetilde{\\beta}};\\mathcal{f}^r_b,{{{\\mathcal}g}}_{{\\gamma}}^d]$ ] for all @xmath116 and a finite positive constant @xmath22 , where the infimum is taken over all possible estimators @xmath291 .    in the last assertion @xmath292 is , for example , satisfied if the following two conditions hold simultaneously true : ( i )  @xmath293 for all @xmath116 and ( ii ) @xmath294 and @xmath295 for all @xmath51 .",
    "observe that ( ii ) which implies @xmath296 is satisfied in case @xmath297 is in the order of a power of @xmath62 ( e.g. , illustration  [ illust2][p - p ] and [ e - p ] ) .",
    "if this term has an exponential order with respect to @xmath62 ( e.g. , illustration [ illust2][p - e ] ) , then a deterioration of the term @xmath237 compared to the variance term @xmath298 is possible .",
    "however , no loss in terms of the rate may occur , that is , @xmath299 , when the squared - bias term @xmath300 dominates the variance term @xmath301 ; for a detailed discussion in a deconvolution context , we refer to  @xcite .",
    "let us illustrate the performance of the data - driven thresholded projection estimator revisiting the three configurations presented in illustration [ illust1 ] .",
    "[ rate]assume an i.i.d .",
    "@xmath20-sample of @xmath7 satisfying ( [ introe1 ] ) and let either assumptions  [ assgauss ] or  [ assmom ] hold true where we set , respectively , @xmath256 or @xmath278 in ( [ defhpenmen ] ) .",
    "the data - driven estimator @xmath302 attains the minimax - rates @xmath198 , up to a constant , in the three cases given in illustration  [ illust1 ] , if we additionally assume @xmath303 in the cases and .    under the stated conditions it is easily verified that the assumptions of either theorems  [ gaussth1 ] or  [ momth1 ] are satisfied .",
    "moreover , the rates @xmath198 ( illustration  [ illust2 ] ) and @xmath238 ( illustration [ illust3 ] ) are of the same order if we additionally assume @xmath303 in the cases [ p - p ] and [ e - p ] .",
    "therefore , corollary  [ adapcoro ] applies , and we obtain the assertion .",
    "[ app ]    appendix    this section gathers preliminary technical results and the proofs of propositions  [ gaussp1 ] and  [ momp1 ] .",
    "we begin by defining and recalling notation to be used in all proofs . given @xmath51",
    ", @xmath54 denotes the subspace of @xmath2 spanned by the functions @xmath55 .",
    "@xmath304 and @xmath305 denote the orthogonal projections on @xmath54 and its orthogonal complement @xmath306 , respectively .",
    "if @xmath307 is an operator mapping @xmath2 to itself and if we restrict @xmath308 to an operator from @xmath54 to itself , then it can be represented by a matrix @xmath124_{{{\\underline{m}}}}$ ] with generic entries @xmath309_{j , l}$ ] for @xmath310 .",
    "the spectral norm of @xmath124_{{{\\underline{m } } } } $ ] is denoted by @xmath311_{{{\\underline{m}}}}\\|_s$ ] , and the inverse matrix of @xmath124_{{{\\underline{m}}}}$ ] by @xmath124_{{{\\underline{m}}}}^{-1}$ ] . furthermore , @xmath122_{{{\\underline{m}}}}$ ] and @xmath312}_{{{\\underline{m}}}}$ ] denote , respectively , the @xmath62-dimensional diagonal matrix with diagonal entries @xmath313 and the identity matrix . for @xmath56 it follows @xmath314^t_{{\\underline{m}}}[\\diag_{\\omega}]_{{{\\underline{m}}}}[h]_{{\\underline{m}}}=\\|[\\diag_{\\omega}]_{{{\\underline{m}}}}^{1/2}[h]_{{\\underline{m}}}\\|^2 $ ] . keeping in mind the notation given in ( [ defdelta])([defhpenmen ] ) we use for all @xmath51 in addition @xmath315}:=\\frac{\\log(\\delta_m^{[{\\gamma}]}\\vee ( m+2))}{\\log(m+2)}$ ] , @xmath316 and @xmath317}:=\\frac{\\log(\\delta_m^{[\\widehat{{\\gamma}}]}\\vee(m+2))}{\\log(m+2)}$ ] allowing us to write @xmath318}= m\\delta_m^{[{\\gamma}]}\\lambda_m^{[{\\gamma } ] } $ ] , @xmath319 and @xmath320}= m\\delta_m^{[\\widehat{{\\gamma}}]}\\lambda_m^{[\\widehat{{\\gamma}}]}$ ] . given a galerkin solution @xmath64 of equation ( [ introe2 ] ) , let @xmath321 and denote @xmath322 , @xmath323 and @xmath324_{{{\\underline{m}}}}^t[{\\gamma}]_{{{\\underline{m}}}}^{-1}[g]_{{{\\underline{m } } } } ) $ ] employing that @xmath6 and @xmath1 are uncorrelated .",
    "define the matrix @xmath325_{{{\\underline{m}}}}:= [ { \\gamma}]_{{{\\underline{m}}}}^{-1/2}[{\\widehat{{\\gamma}}}]_{{{\\underline{m}}}}[{\\gamma}]_{{{\\underline{m}}}}^{-1/2 } - { [ \\mathrm{id}]}_{{{\\underline{m}}}}$ ] and the vector @xmath326_{{{\\underline{m}}}}:= [ { \\widehat{g}}]_{{{\\underline{m}}}}- [ { \\widehat{{\\gamma}}}]_{{{\\underline{m } } } } [ { \\beta}^{m}]_{{{\\underline{m}}}}$ ] satisfying @xmath327_{{{\\underline{m}}}}= 0 $ ] and @xmath328_{{{\\underline{m } } } } = [ { \\gamma}({\\beta}-{\\beta}^{m})]_{{{\\underline{m}}}}=0 $ ] . let further @xmath329 and define the events @xmath330^{-1}_{{{\\underline{m } } } } \\bigr\\|_{s}\\leq n \\bigr\\},\\qquad { { \\mho_{m , n}}}:= \\bigl\\ { 8\\bigl\\| [ \\xi]_{{{\\underline{m}}}}\\bigr\\|_{s}\\leq1\\bigr\\ } , \\nonumber \\\\ { { \\mathcal{a}_{n}}}&:=&\\bigl\\{{1}/{2}\\leq{\\widehat{\\sigma}}_y^2/ \\sigma_y^2\\leq{3}/{2}\\bigr\\},\\qquad { { \\mathcal{b}_{n}}}:=\\bigl\\{\\bigl\\| [ \\xi]_{{\\underline{k}}}\\bigr\\|_{s}\\leq1/8,\\forall1\\leq k\\leq m^{{\\omega}}_n \\bigr\\},\\hspace*{-26pt } \\\\ { { \\mathcal{c}_{n}}}&:=&\\bigl\\{8[w]_{{\\underline{k}}}^t[{\\gamma}]_{{{\\underline{k}}}}^{-1}[w]_{{\\underline{k}}}\\leq \\bigl([g]_{{\\underline{k}}}^t[{\\gamma}]^{-1}_{{\\underline{k}}}[g]_{{\\underline{k}}}+ \\sigma_y^2\\bigr),\\forall 1\\leq k\\leq m^{{\\omega}}_n\\bigr\\ } , \\nonumber\\end{aligned}\\ ] ] and their complements @xmath331 , @xmath332 , @xmath333 , @xmath334 and @xmath335 , respectively .",
    "furthermore , we will denote by @xmath22 universal numerical constants and by @xmath336 constants depending only on the arguments . in both cases ,",
    "the values of the constants may change from line to line .",
    "this section gathers results exploiting assumption  [ assreg ] only .",
    "the proof of the next lemma can be found in  @xcite .",
    "[ appprel1 ] let @xmath266 with sequence @xmath149 as in assumption [ assreg ] .",
    "then we have :    [ appprel1e1]@xmath337_{{{\\underline{m}}}}^{-1}\\|_s \\}\\leq4d^3 $ ] ;    [ appprel1e2]@xmath338^{1/2}_{{{\\underline{m}}}}[{\\gamma}]_{{{\\underline{m}}}}^{-1}[\\diag_{{\\gamma}}]^{1/2}_{{{\\underline{m}}}}\\|_s\\leq 4d^3 $ ] ;    [ appprel1e3]@xmath339^{-1/2}_{{{\\underline{m}}}}[{\\gamma}]_{{{\\underline{m}}}}[\\diag_{\\gamma}]^{-1/2}_{{{\\underline{m}}}}\\|_s\\leq d$ ] .",
    "let in addition @xmath340 with sequence @xmath143 as in assumption  [ assreg ] .",
    "if @xmath341 denotes a galerkin solution of @xmath342 , then for each strictly positive sequence @xmath343 such that @xmath344 is nonincreasing and for all @xmath51 we obtain :    [ appprel1e4]@xmath345 ;    [ appprel1e5]@xmath346 and @xmath347 .",
    "[ appprel2 ] let assumption  [ assreg ] be satisfied . if @xmath266 and @xmath348 , then :    [ appprel2e1]@xmath349_{{\\underline{m}}}^{-1}\\|_s \\leq d$ ] , @xmath350}/\\delta^{\\gamma}_m\\leq d $ ] , @xmath351}/\\break\\lambda^{\\gamma}_m\\leq ( 1+\\log d)$ ] , and @xmath352}/\\delta^{\\gamma}_m\\leq d(1+\\log d)$ ] , for all @xmath51 ;    [ appprel2e2]@xmath353 and @xmath354 } \\leq n 4d^2(1 + 2\\log d)$ ] , for all @xmath116 ;    [ appprel2e3]@xmath355^{-1}_{{\\underline{m}}}\\|$ ] if @xmath356 and @xmath357 ;    [ appprel2e4]@xmath358 , for all @xmath51 , assuming in addition .    consider  [ appprel2e1 ] .",
    "from lemma  [ appprel1][appprel1e1 ] , [ appprel1e3 ] follows @xmath277_{{\\underline{m}}}^{-1}\\|_s\\leq4d^3{\\gamma}_m^{-1}$ ] and @xmath359_{{\\underline{m}}}^{-1}\\|_s$ ] which in turn imply @xmath360_{{\\underline{m}}}^{-1}\\|_s{\\gamma}_m",
    "\\leq d$ ] and @xmath361_{{\\underline{m}}}^{-1}\\|_s\\leq d$ ] due to the monotonicity of @xmath149 . from these estimates",
    "we conclude  [ appprel2e1 ] .",
    "consider [ appprel2e2 ] .",
    "observe that @xmath362 . in case",
    "@xmath363 the assertion follows from @xmath364 ( assumption  [ assreg ] ) .",
    "thus , let , then @xmath365 , and hence @xmath366 , @xmath367 , @xmath368}\\leq 4d^2n(1+\\log n)^{-1}$ ] and @xmath369}\\leq(1 + 2\\log d)(1+\\log n)$ ] .",
    "[ appprel2e2 ] follows now by combination of these estimates .",
    "consider [ appprel2e3 ] . by employing @xmath370_{{\\underline{m}}}^{-1}\\|$ ] , [ appprel2e3 ]",
    "follows from @xmath371 if @xmath363 , while for @xmath372 , we use @xmath373 .",
    "consider [ appprel2e4 ] .",
    "since @xmath6 and @xmath1 are centred the identity @xmath65_{{\\underline{m}}}=[{\\gamma}]_{{{\\underline{m}}}}^{-1}[g]_{{{\\underline{m}}}}$ ] implies @xmath374_{{{\\underline{m}}}}^t[{\\gamma}]_{{{\\underline{m}}}}^{-1}[g]_{{{\\underline{m } } } } ) = \\sigma_m^2 $ ] . by applying successively the inequality@xmath375 due to @xcite , assumption  [ assreg ] , that is , @xmath149 and @xmath161 are nonincreasing , and the identity @xmath376 follows @xmath377 furthermore , from  [ appprel1e3 ] and  [ appprel1e5 ] in lemma  [ appprel1 ]",
    ", we obtain @xmath378_{{{\\underline{m}}}}^t[{\\gamma}]_{{{\\underline{m}}}}^{-1}[g]_{{{\\underline{m } } } } \\leq d \\bigl\\|{\\beta}^{m}\\bigr\\|_{\\gamma}^2 \\leq34d^9r,\\ ] ] which together with ( [ appprel3e1 ] ) implies  [ appprel2e4 ] and completes the proof .",
    "[ appprel4]let @xmath266 with @xmath149 as in assumption  [ assreg ] . for all @xmath379",
    "holds @xmath380_{{\\underline{m}}}^{-1}\\|_s}{\\|[{\\gamma}]_{{\\underline{m}}}^{-1}\\|_s}\\leq4,\\forall 1\\leq m\\leq m^{{\\omega}}_n \\biggr\\rbrace}\\subset { \\bigl\\lbrace m^-_n\\leq{\\widehat{m}}\\leq m^+_n \\bigr\\rbrace}.\\ ] ]",
    "let @xmath381_{{\\underline{m}}}^{-1}\\|_s^{-1}$ ] and @xmath382_{{\\underline{m}}}^{-1}\\|_s^{-1}$ ] .",
    "we use below without further reference that @xmath383 due to lemma  [ appprel2][appprel2e1 ] .",
    "the result of the lemma follows by combination of the next two assertions , @xmath384 consider ( [ appprel4e1 ] ) which holds trivially true for @xmath385 .",
    "if @xmath386 , then @xmath387 implies @xmath388 and @xmath389 while @xmath390 which shows ( [ appprel4e1 ] ) because @xmath391 .",
    "consider ( [ appprel4e2 ] ) which holds trivially true for @xmath392 .",
    "if @xmath393 , then @xmath394 and ( [ appprel4e2 ] ) follows from @xmath395 and @xmath396 which completes the proof .",
    "[ appprel5 ] let @xmath397 , @xmath398 and @xmath399 as in ( [ apppree4 ] ) . for all @xmath116 it holds true that @xmath400 .",
    "let @xmath401 .",
    "if @xmath402_{{\\underline{k}}}\\|_s\\leq1/8 $ ] , that is , on the event @xmath398 , it is easily verified that @xmath403}_{{\\underline{k}}}+[\\xi]_{{\\underline{k}}})^{-1}-{[\\mathrm{id}]}_{{\\underline{k}}}\\|_s\\leq1/7 $ ] which we exploit to conclude @xmath404_{{{\\underline{k}}}}^{1/2}[{\\widehat{{\\gamma}}}]^{-1}_{{\\underline{k}}}[\\diag_{{\\omega}}]_{{{\\underline{k}}}}^{1/2}\\|_s}{\\|[\\diag_{{\\omega}}]_{{{\\underline{k}}}}^{1/2}[{\\gamma}]^{-1}_{{\\underline{k}}}[\\diag_{{\\omega}}]_{{{\\underline{k}}}}^{1/2}\\|_s}\\leq \\frac{8}{7},\\qquad \\frac{6}{7 } \\leq\\frac{\\|[{\\widehat{{\\gamma}}}]^{-1}_{{\\underline{k}}}\\|_s}{\\|[{\\gamma}]^{-1}_{{\\underline{k}}}\\|_s}\\leq \\frac{8}{7 } \\quad\\mbox{and } & \\nonumber\\\\[-8pt]\\\\[-8pt ] & \\displaystyle { 6 } x^t[{\\gamma}]^{-1}_{{\\underline{k}}}x\\leq 7x^t [ { \\widehat{{\\gamma}}}]^{-1}_{{\\underline{k}}}x \\leq8 x^t[{\\gamma}]^{-1}_{{\\underline{k}}}x\\qquad \\mbox{for all } x\\in{{\\mathbb r}}^k & \\nonumber\\end{aligned}\\ ] ] and , consequently @xmath405^t_{{\\underline{k } } } [ { \\gamma}]^{-1}_{{\\underline{k}}}[{\\widehat{g}}]_{{\\underline{k}}}\\leq [ { \\widehat{g}}]^t_{{\\underline{k}}}[{\\widehat{{\\gamma}}}]^{-1}_{{\\underline{k } } } [ { \\widehat{g}}]_{{\\underline{k}}}\\leq(8/7 ) [ { \\widehat{g}}]^t_{{\\underline{k } } } [ { \\gamma}]^{-1}_{{\\underline{k}}}[{\\widehat{g}}]_{{\\underline{k}}}.\\ ] ] moreover , from @xmath402_{{\\underline{k}}}\\|_{s}\\leq1/8 $ ] we obtain after some algebra , @xmath406^t_{{\\underline{k}}}[{\\gamma}]^{-1}_{{\\underline{k}}}[g]_{{\\underline{k}}}&\\leq&({1}/{16 } ) [ g]^t_{{\\underline{k}}}[{\\gamma}]^{-1}_{{\\underline{k}}}[g]_{{\\underline{k}}}+ 4[w]_{{\\underline{k}}}[{\\gamma}]_{{\\underline{k}}}^{-1}[w]_{{{\\underline{k } } } } + 2 [ { \\widehat{g}}]^t_{{\\underline{k}}}[{\\gamma}]^{-1}_{{\\underline{k } } } [ { \\widehat{g}}]_{{\\underline{k } } } , \\\\ { } [ { \\widehat{g}}]^t_{{\\underline{k}}}[{\\gamma}]^{-1}_{{\\underline{k } } } [ { \\widehat{g}}]_{{\\underline{k}}}&\\leq&({33}/{16 } ) [ g ] ^t_{{\\underline{k } } } [ { \\gamma}]^{-1}_{{\\underline{k}}}[g]_{{\\underline{k}}}+ 4[w]_{{\\underline{k } } } [ { \\gamma}]_{{\\underline{k}}}^{-1}[w]_{{{\\underline{k}}}}.\\end{aligned}\\ ] ] combining each of these estimates with ( [ appprel5e2 ] ) yields @xmath407^t_{{\\underline{k}}}[{\\gamma}]^{-1}_{{\\underline{k}}}[g]_{{\\underline{k}}}&\\leq & 4[w]_{{\\underline{k}}}[{\\gamma}]_{{\\underline{k}}}^{-1}[w]_{{{\\underline{k } } } } + ( 7/3)[{\\widehat{g}}]^t_{{\\underline{k}}}[{\\widehat{{\\gamma}}}]^{-1}_{{\\underline{k } } } [ { \\widehat{g}}]_{{\\underline{k } } } , \\\\ ( 7/8)[{\\widehat{g}}]^t_{{\\underline{k}}}[{\\widehat{{\\gamma}}}]^{-1}_{{\\underline{k } } } [ { \\widehat{g}}]_{{\\underline{k}}}&\\leq&(33/16 ) [ g ] ^t_{{\\underline{k}}}[{\\gamma}]^{-1}_{{\\underline{k}}}[g]_{{\\underline{k}}}+ 4[w]_{{\\underline{k } } } [ { \\gamma}]_{{\\underline{k}}}^{-1}[w]_{{{\\underline{k}}}}.\\end{aligned}\\ ] ] if in addition @xmath326_{{\\underline{k}}}^t[{\\gamma}]_{{{\\underline{k}}}}^{-1}[w]_{{\\underline{k}}}\\leq\\frac { 1}{8}([g]_{{\\underline{k}}}^t[{\\gamma}]^{-1}_{{\\underline{k}}}[g]_{{\\underline{k}}}+ \\sigma_y^2 ) $ ] , that is , on the event @xmath408 , then the last two estimates imply , respectively , @xmath409^t_{{\\underline{k}}}[{\\gamma}]^{-1}_{{\\underline{k}}}[g]_{{\\underline{k}}}+ \\sigma_y^2\\bigr ) & \\leq & ( 15/16)\\sigma_y^2 + ( 7/3 ) [ { \\widehat{g}}]^t_{{\\underline{k}}}[{\\widehat{{\\gamma}}}]^{-1}_{{\\underline{k } } } [ { \\widehat{g}}]_{{\\underline{k } } } , \\\\ ( 7/8)[{\\widehat{g}}]^t_{{\\underline{k}}}[{\\widehat{{\\gamma}}}]^{-1}_{{\\underline{k } } } [ { \\widehat{g}}]_{{\\underline{k}}}&\\leq&(41/16 ) [ g ] ^t_{{\\underline{k}}}[{\\gamma}]^{-1}_{{\\underline{k}}}[g]_{{\\underline{k}}}+ ( 1/2)\\sigma_y^2\\end{aligned}\\ ] ] and hence in case @xmath410 , that is , on the event @xmath411 , we obtain @xmath412^t_{{\\underline{k}}}[{\\gamma}]^{-1}_{{\\underline{k}}}[g]_{{\\underline{k}}}+ \\sigma_y^2\\bigr)&\\leq&(15/8 ) { \\widehat{\\sigma}}_y^2 + ( 7/3 ) [ { \\widehat{g}}]^t_{{\\underline{k}}}[{\\widehat{{\\gamma}}}]^{-1}_{{\\underline{k } } } [ { \\widehat{g}}]_{{\\underline{k } } } , \\\\ ( 7/8 ) \\bigl([{\\widehat{g}}]^t_{{\\underline{k}}}[{\\widehat{{\\gamma}}}]^{-1}_{{\\underline{k } } } [ { \\widehat{g}}]_{{\\underline{k}}}+ { \\widehat{\\sigma}}{}^2_y\\bigr)&\\leq&(41/16 ) [ g]^t_{{\\underline{k}}}[{\\gamma}]^{-1}_{{\\underline{k}}}[g]_{{\\underline{k}}}+ ( 29/16)\\sigma_y^2.\\end{aligned}\\ ] ] combining the last two estimates we have @xmath413^t_{{\\underline{k}}}[{\\gamma}]^{-1}_{{\\underline{k}}}[g]_{{\\underline{k}}}+2 \\sigma_y^2\\bigr)&\\leq & \\bigl(2[{\\widehat{g}}]^t_{{\\underline{k } } } [ { \\widehat{{\\gamma}}}]^{-1}_{{\\underline{k}}}[{\\widehat{g}}]_{{\\underline{k}}}+ 2{\\widehat{\\sigma}}{}^2_y \\bigr)\\\\ & \\leq&3 \\bigl(2[g ] ^t_{{\\underline{k}}}[{\\gamma}]^{-1}_{{\\underline{k}}}[g]_{{\\underline{k}}}+2 \\sigma_y^2\\bigr).\\end{aligned}\\ ] ] on @xmath414 the last estimate and ( [ appprel5e1 ] ) hold for all @xmath415 , hence @xmath416}_m}{\\delta_m^{[{\\gamma}]}}\\leq\\frac{8}{7 } , \\forall1\\leq m\\leq m^{{\\omega}}_n \\biggr\\}.\\ ] ] moreover it is easily seen that @xmath417}/ \\delta_m^{[{\\gamma}]}\\leq(8/7)$ ] implies @xmath418}/ \\lambda_m^{[{\\gamma } ] } \\leq \\bigl(1+\\log(8/7)\\bigr)\\leq3/2.\\ ] ] due to the last estimates the definitions of @xmath101 and @xmath100 imply @xmath419 on the other hand , by exploiting successively ( [ appprel5e1 ] ) and lemma  [ appprel4 ] , we have @xmath420^{-1}_{{\\underline{m}}}\\|_s}{\\|[{\\gamma}]^{-1}_{{\\underline{m}}}\\|_s}\\leq\\frac{8}{7 } , \\forall1 \\leq m\\leq m^{{\\omega}}_n \\biggr\\rbrace } \\subset { \\bigl\\lbrace m^-_n\\leq{\\widehat{m}}\\leq m^+_n \\bigr\\rbrace}.\\ ] ] the last display and ( [ appprel5e3 ] ) imply the assertion of the lemma .",
    "[ appprel6 ] for all @xmath421 with @xmath422^{-1}_{{{\\underline{m}}}}\\|_s$ ] we have @xmath423 .",
    "taking into account @xmath73_{{{\\underline{m}}}}= [ { \\gamma}]^{1/2}_{{{\\underline{m}}}}\\{{[\\mathrm{id}]}_{{{\\underline{m}}}}+[\\xi]_{{{\\underline{m}}}}\\}[{\\gamma}]^{1/2}_{{{\\underline{m}}}}$ ] observe that @xmath402_{{{\\underline{m}}}}\\|_s\\leq 1/8 $ ] and @xmath424^{-1}_{{{\\underline{m}}}}\\|_s$ ] imply @xmath425^{-1}_{{{\\underline{m}}}}\\|_s \\leq n$ ] due to a neumann series argument .",
    "hence , @xmath423 which proves the lemma .",
    "we will suppose throughout this section that the conditions of proposition  [ gaussp1 ] are satisfied which allow us to employ lemmas  [ appprel1][appprel6 ] .",
    "first , we show technical assertions ( lemmas  [ appnormal][appgaussl4 ] ) exploiting assumption  [ assgauss ] , that is , @xmath1 and @xmath6 are jointly normally distributed .",
    "they are used below to prove that assumptions  [ assremainder - i ] and [ assremainder - ii ] are satisfied ( propositions  [ appgaussp1 ] and  [ appgaussp2 ] , resp . ) , which is the claim of proposition  [ gaussp1 ] .",
    "we begin by recalling elementary properties due to assumption [ assgauss ] which are frequently used in this section .",
    "given @xmath426 the random variable @xmath427 is normally distributed with mean zero and variance @xmath428 . consider the galerkin solution @xmath429 and @xmath430 ; then @xmath431 and @xmath432 are independent .",
    "thereby , @xmath433 and @xmath63_{{\\underline{m}}}$ ] are independent , normally distributed with mean zero and , respectively , variance @xmath434 and covariance matrix  @xmath61_{{\\underline{m}}}$ ] .",
    "consequently , @xmath435_m^t[{\\gamma}]_m^{-1/2})$ ] is a vector with independent , standard normally distributed entries .",
    "the next assertion states elementary inequalities for gaussian random variables and its straightforward proof is omitted .",
    "[ appnormal]let @xmath436 be independent and standard normally distributed . for all @xmath437 and @xmath438",
    "we have :    [ appnormaleq1]@xmath439 ;    [ appnormaleq2]@xmath440 ;    [ appnormaleq2b]@xmath441 ;    and for all @xmath442 and @xmath443 we obtain :    [ appnormaleq3]@xmath444 ;    [ appnormaleq4]@xmath445 ;    [ appnormaleq5]@xmath446 .    [ appgaussl1 ] for all @xmath379",
    "we have :    [ appgaussl1e1]@xmath447_{{\\underline{m}}}\\|^4\\leq 6   ( \\ex\\| x\\|^2)^2 $ ] .    furthermore , there exist a numerical constant @xmath26 such that for all @xmath116 :    [ appgaussl1e2 ] @xmath448_{{\\underline{m}}}^t[{\\gamma}]_{{\\underline{m}}}^{-1}[w]_{{\\underline{m}}}}{\\rho_m^2}>\\frac{1}{16 } ) \\leq c$ ] ;    [ appgaussl1e3 ] @xmath449_{{\\underline{m}}}\\|_s > 1/8 ) } \\leq c$ ] ;    [ appgaussl1e4 ] @xmath450",
    ".    denote by @xmath451 an eigenvalue decomposition of @xmath61_{{\\underline{m}}}$ ] .",
    "define @xmath452 and @xmath453_{{{\\underline{m}}}})$ ] , @xmath454 , @xmath455 , where @xmath456 are independent and standard normally distributed .",
    "consider  [ appgaussl1e1 ] and [ appgaussl1e2 ] .",
    "taking into account @xmath457 and the identities @xmath458_{{\\underline{m}}}\\|^4 = ( \\sum_{j=1}^m \\lambda_j(\\sum_{i=1}^nu_iv_{ij})^2)^2 $ ] and @xmath459_{{\\underline{m}}}^t[{\\gamma}]_{{\\underline{m}}}^{-1}[w]_{{\\underline{m}}})/\\rho_m^2 = n^{-2 } \\sum_{j=1}^m ( \\sum_{i=1}^nu_iv_{ij})^2 $ ] , assertions  [ appgaussl1e1 ] and [ appgaussl1e2 ] follow , respectively , from in lemma [ appnormal][appnormaleq5 ] and  [ appnormaleq2b ] ( with @xmath460 ) . consider  [ appgaussl1e3 ] .",
    "since @xmath461_{{\\underline{m}}}\\|_{s}\\leq m\\max_{1\\leq j , l\\leq m}|\\sum_{i=1}^n(v_{ij}v_{il}-\\delta_{jl})|$ ] we obtain due to [ appnormaleq1 ] and  [ appnormaleq2 ] in lemma [ appnormal ] that for all @xmath437@xmath462_{{\\underline{m}}}\\bigr\\|_{s}\\geq\\eta\\bigr)\\\\[-1pt ] & & \\quad\\leq\\sum _",
    "{ 1\\leq j , l\\leq m}p\\biggl(\\biggl|n^{-1}\\sum _ { i=1}^n(v_{ij}v_{il}- \\delta_{jl})\\biggr|\\geq\\eta / m\\biggr ) \\\\[-1pt ] & & \\quad\\leq m^2\\max \\biggl\\ { p\\biggl(\\biggl|\\frac{1}{n}\\sum _ { i=1}^nv_{i1}v_{i2}\\biggr|\\geq \\frac{\\eta}{m}\\biggr ) , p\\biggl(\\biggl|\\frac { 1}{n^{1/2}}\\sum _ { i=1}^n\\bigl(v_{i1}^2 - 1 \\bigr)\\biggr|\\geq n^{1/2}\\frac{\\eta } { m}\\biggr ) \\biggr\\ } \\\\[-1pt ] & & \\quad\\leq m^2\\max \\biggl\\{\\biggl(1 + \\frac{m}{\\eta n^{1/2}}\\biggr ) \\exp \\biggl(-\\frac{n}{4}\\min\\biggl\\{\\frac{\\eta^2}{m^2},\\frac{1}{4}\\biggr \\ } \\biggr ) , 2 \\exp \\biggl(-\\frac{1}{8}\\frac{n\\eta^2/m^2}{1+\\eta / m } \\biggr ) \\biggr \\}.\\ ] ] keeping in mind that @xmath463 , the last bound implies [ appgaussl1e3 ] .",
    "consider  [ appgaussl1e4 ] .",
    "since @xmath464 are independent , standard , normally distributed and @xmath465 ,  [ appgaussl1e4 ] follows from lemma  [ appnormal][appnormaleq1 ] .",
    "[ appgaussl2 ] we have for all @xmath442 and @xmath379 @xmath466_{{\\underline{m}}}^t[{\\gamma}]_{{\\underline{m}}}^{-1}[w]_{{\\underline{m}}}}{\\rho_m^2 } - 4 c m \\biggr)_+\\leq16\\exp \\biggl ( \\frac{-c m}{16 } \\biggr ) + 32 \\frac { c m}{n } \\exp \\biggl(\\frac{-n}{16 } \\biggr).\\vspace*{-1pt}\\ ] ]    from @xmath467_{{\\underline{m}}}^{-1/2}[w]_{{\\underline{m}}}\\|^2\\rho_m^{-2 } = \\sum_{j=1}^m ( n^{-1/2}\\sum_{i=1}^n u_iv_{ij})^2 $ ] derived in the proof of lemma [ appgaussl1 ] and lemma  [ appnormal][appnormaleq4 ] follows the assertion .",
    "[ appgaussl3 ] there is a constant @xmath468 depending on @xmath260 such that for all @xmath116 , @xmath469 } \\ex \\biggl([w]_{{{\\underline{k}}}}^t [ { \\gamma}]_{{{\\underline{k}}}}^{-1}[w]_{{{\\underline{k } } } } - 4 \\sigma_k^2 \\frac{k \\lambda_k^{[{\\gamma}]}}{n } \\biggr)_+ \\leq c(d ) \\bigl(\\sigma^2+r\\bigr ) \\sigma n^{-1}.\\ ] ]    the key argument of the proof is lemma  [ appgaussl2 ] with @xmath470}$ ] .",
    "taking into account this bound and for all @xmath471 and @xmath472 that @xmath473}\\leq4d^3 \\delta_k^{\\gamma}$ ] , @xmath474}$ ] , @xmath475}\\leq n c d^6(1+\\log d)$ ] and @xmath476 [ lemma  [ appprel2](i ) , ( ii ) and ( iv ) , resp .",
    "] hold true , we obtain @xmath477 } \\ex \\biggl([w]_{{{\\underline{k}}}}^t[{\\gamma}]_{{{\\underline{k}}}}^{-1}[w]_{{{\\underline{k } } } } - 4 \\sigma_k^2 \\frac{k \\lambda_k^{[{\\gamma } ] } } { n } \\biggr)_+ \\\\[-1pt ] & & \\qquad\\leq\\sum _ { k=1}^{m^+_n}\\frac{\\sigma_k^2\\delta_{k}^{[{\\gamma}]}}{n } \\ex \\biggl(\\frac{n[w]_{{{\\underline{k}}}}^t[{\\gamma}]_{{{\\underline{k}}}}^{-1}[w]_{{{\\underline{k}}}}}{\\rho_k^2 } - 4 k \\lambda_k^{[{\\gamma } ] } \\biggr)_+ \\\\[-1pt ] & & \\qquad\\leq c(d ) \\bigl(\\sigma^2+r\\bigr)\\\\[-1pt ] & & \\qquad\\quad{}\\times n^{-1 } \\biggl\\ { \\sum _ { k=1}^{m^+_n } \\delta_k^{\\gamma}\\exp \\biggl(-\\frac { k\\lambda_k^{\\gamma}}{16(1+\\log d ) } \\biggr ) + m^+_n\\exp ( { -n}/{16 } ) \\biggr\\}.\\end{aligned}\\ ] ] finally , exploiting the constant @xmath478 satisfying ( [ defsigma ] ) and @xmath479 for all @xmath116 , we obtain the assertion of the lemma .",
    "[ appgaussl4 ] there exist a numerical constant @xmath22 and a constant @xmath468 only depending on @xmath260 such that for all @xmath116 , we have :    [ appgaussl4e1]@xmath480 ;    [ appgaussl4e2]@xmath481 ;    [ appgaussl4e3]@xmath482 .    since @xmath483 and @xmath484_{{{\\underline{m}}}}\\|>1/8 \\rbrace}$ ] assertion [ appgaussl4e1 ] follows from lemma  [ appgaussl1][appgaussl1e2 ] .",
    "consider [ appgaussl4e2 ] .",
    "let @xmath485 , and consequently @xmath486 for all @xmath487 .",
    "we distinguish in the following the cases @xmath488 and @xmath487 .",
    "first , let @xmath489 . obviously , @xmath490 since @xmath491 and @xmath492 depends on @xmath260 only . on the other hand , if @xmath493 , then lemma  [ appprel2](iii ) implies @xmath494^{-1}_{{\\underline{m}}}\\|$ ] , and hence @xmath495 for all @xmath496 by employing lemma  [ appprel6 ] .",
    "from  [ appgaussl4e1 ] we conclude@xmath497 . by combination of the two cases we obtain  [ appgaussl4e2 ] .",
    "it remains to show [ appgaussl4e3 ] .",
    "consider the events @xmath397 , @xmath398 and @xmath399 given in  ( [ apppree4 ] ) , where @xmath498 due to lemma  [ appprel5 ] .",
    "we have @xmath499 , @xmath500 , @xmath501 due to lemma [ appgaussl1][appgaussl1e4 ] ,  [ appgaussl1e3 ] , [ appgaussl1e2 ] , respectively ( keep in mind @xmath502 and @xmath503_{{\\underline{k}}}^t[{\\gamma}]_{{\\underline{k}}}^{-1}[g]_{{\\underline{k}}})=\\sigma_{k}^2\\geq\\rho_k^2 $ ] ) . combining these estimates",
    "implies  [ appgaussl4e3 ] .",
    "[ appgaussp1]let @xmath256 in definition ( [ defpen ] ) of the penalty @xmath257 .",
    "there exists a constant @xmath468 only depending on @xmath260 such that for all @xmath116 , @xmath504    since @xmath505_{{\\underline{k}}}= [ { \\widehat{{\\gamma}}}]^{-1}_{{\\underline{k}}}[w]_{{{\\underline{k}}}}{{\\mathbh1}}_{\\omega_{k , n } } - [ { \\beta}^{k } ] _ { { \\underline{k}}}{{\\mathbh1}}_{\\omega_{k , n}^c}$ ] it follows @xmath506_{{\\underline{k}}}^{1/2 } [ { \\widehat{{\\gamma}}}]^{-1}_{{\\underline{k}}}[w]_{{{\\underline{k}}}}\\bigr\\|^2 { { \\mathbh1}}_{\\omega_{k , n } } + \\bigl\\| { \\beta}^{k } \\bigr\\|^2_{\\omega}{{\\mathbh1}}_{\\omega_{k , n}^c}.\\ ] ] exploiting @xmath507}_{{{\\underline{k}}}}+[\\xi]_{{{\\underline{k}}}})^{-1}\\|_s{{\\mathbh1}}_{\\mho _ { k , n}}\\leq 2 $ ] , @xmath73_{{{\\underline{k}}}}= [ { \\gamma}]^{1/2}_{{{\\underline{k}}}}\\{{[\\mathrm{id}]}_{{{\\underline{k}}}}+[\\xi]_{{{\\underline{k}}}}\\ } [ { \\gamma}]^{1/2}_{{{\\underline{k}}}}$ ] and the definition of @xmath473}$ ] imply @xmath508^{1/2}_{{\\underline{k}}}[{\\widehat{{\\gamma}}}]^{-1}_{{\\underline{k}}}[w]_{{{\\underline{k}}}}\\|^2{{\\mathbh1}}_{\\mho_{k , n}}\\leq4 \\delta_k^{[{\\gamma } ] } \\|[{\\gamma}]^{-1/2}_{{\\underline{k}}}[w]_{{{\\underline{k}}}}\\|^2 $ ] . on the other hand",
    ", we have @xmath508^{1/2}_{{\\underline{k}}}[{\\widehat{{\\gamma}}}]^{-1}_{{\\underline{k}}}[w]_{{{\\underline{k}}}}\\|^2{{\\mathbh1}}_{\\omega_{k , n}}\\leq{\\omega}_{(k ) } n^2 \\|[w]_{{{\\underline{k}}}}\\|^2 $ ] . from these estimates and @xmath509 (",
    "@xmath162 is nonincreasing due to assumption  [ assreg ] ) we deduce for all @xmath251 , @xmath510}\\bigl\\|[{\\gamma}]_{{{\\underline{k}}}}^{-1/2}[w]_{{{\\underline{k } } } } \\bigr\\|^2+{\\omega}_{(k ) } n^2\\bigl\\|[w]_{{{\\underline{k } } } } \\bigr\\|^2{{\\mathbh1}}_{\\mho _ { k , n}^c } + \\bigl\\|{\\beta}^{k } \\bigr\\|^2_b{{\\mathbh1}}_{\\omega_{k , n}^c}.\\ ] ] this upper bound and @xmath511}\\lambda_k^{[{\\gamma } ] } n^{-1}$ ] imply @xmath512_{{{\\underline{k}}}}\\bigr\\|^4 \\bigr)^{1/2 } \\bigl(p\\bigl(\\mho_{k , n}^c\\bigr ) \\bigr)^{1/2 } + \\sum_{k={m^\\diamond_n}}^{m^+_n } \\bigl\\|{\\beta}^{k } \\bigr\\|^2_bp\\bigl(\\omega^c_{k , n } \\bigr)\\\\ & & \\qquad\\quad { } + 4\\sum _ { k={m^\\diamond_n}}^{m^+_n } \\delta_{k}^{[{\\gamma } ] } \\ex \\biggl(\\bigl\\|[{\\gamma}]_{{{\\underline{k}}}}^{-1/2}[w]_{{{\\underline{k } } } } \\bigr\\|^2 - 4 \\sigma_k^2 \\frac { k \\lambda_k^{[{\\gamma}]}}{n } \\biggr)_+.\\end{aligned}\\ ] ] by exploiting lemmas  [ appprel1][appprel1e5 ] and [ appgaussl1][appgaussl1e1 ] together with @xmath513 [ lemma  [ appprel2](iv ) ] the first and second r.h.s .",
    "term are bounded by @xmath514 combining this upper bound , the property @xmath515 and the estimates given in lemma [ appgaussl4 ] , we deduce for all @xmath265 and @xmath516 that @xmath517 } \\ex { \\biggl ( \\bigl\\|[{\\gamma}]_{{{\\underline{k}}}}^{-1/2}[w]_{{{\\underline{k } } } } \\bigr\\|^2 - 4 \\sigma_k^2 \\frac{k \\lambda_k^{[{\\gamma}]}}{n } \\biggr)_{+}}.\\end{aligned}\\ ] ] the result of the proposition follows now by replacing the last r.h.s .",
    "term by its upper bound given in lemma  [ appgaussl3 ] , which completes the proof .",
    "[ appgaussp2 ] let @xmath256 in definition ( [ defpen ] ) and ( [ defhpenmen ] ) of @xmath257 and @xmath91 .",
    "there exists a constant @xmath468 only depending on @xmath260 such that for all @xmath116 @xmath518    from the decomposition ( [ appgaussp1decomp ] ) and @xmath519^{1/2}_{{\\underline{k}}}[{\\widehat{{\\gamma}}}]^{-1}_{{\\underline{k}}}[w]_{{{\\underline{k}}}}\\|^2{{\\mathbh1}}_{\\omega _ { k , n}}\\leq\\delta^{\\omega}_k n^2 \\|[w]_{{{\\underline{k}}}}\\|^2 $ ] given in the proof of proposition [ appgaussp1 ] we conclude @xmath520_{{{\\underline{k } } } } \\bigr\\|^2 + 2 \\bigl\\|{\\beta}^{k}\\bigr\\|^2_{\\omega}+ 2 \\|{\\beta}\\|^2_{\\omega}\\qquad \\mbox{for all } k\\geq1.\\ ] ] by exploiting lemma  [ appprel1][appprel1e5 ] together with @xmath521 ( @xmath162 is nonincreasing due to assumption [ assreg ] ) we obtain for all @xmath265 and @xmath516@xmath520_{{{\\underline{k } } } } \\bigr\\|^2 + 2 \\bigl(34 d^8r+r\\bigr)\\qquad \\mbox{for all } k \\geq1.\\ ] ] since @xmath522 and @xmath523 it follows that @xmath524_{{{\\underline{k}}}}\\bigr\\|^4 \\bigr)^{1/2}\\bigl|p\\bigl({{{\\mathcal}e}}^c_n\\bigr)\\bigr|^{1/2}\\\\ & & { } + 70 d^8rm^{{\\omega}}_np\\bigl({{{\\mathcal}e}}^c_n \\bigr).\\end{aligned}\\ ] ] from lemma  [ appgaussl1][appgaussl1e1 ] together with @xmath525 ( lemma  [ appprel2 ] ) and @xmath526 , we conclude for all @xmath147 and @xmath266 that @xmath527 the result of the proposition follows now from @xmath528 and by replacing the probability @xmath529 by its upper bound @xmath530 given in lemma  [ appgaussl4 ] .    proof of proposition  [ gaussp1 ] the assertion follows from propositions  [ appgaussp1 ] and [ appgaussp2 ] , and we omit the details .",
    "we assume throughout this section that the conditions of proposition [ momp1 ] are satisfied which allows us to employ lemmas  [ appprel1][appprel6 ] .",
    "we formulate first preliminary results ( proposition  [ appgeneraltala ] and lemmas [ appgenerall1][appgenerall4 ] ) relying on the moment conditions ( assumption  [ assmom ] ) .",
    "they are used to prove that assumptions [ assremainder - i ] and  [ assremainder - ii ] are satisfied ( propositions  [ appgeneralp1 ] and  [ appgeneralp2 ] , resp . ) , which is the claim of proposition [ momp1 ] .",
    "we begin by gathering elementary bounds due to assumption [ assmom ] .",
    "let @xmath531 be given by assumption  [ assmom ] ; then for all @xmath51 we have @xmath532_{{\\underline{m}}}^{-1/2}[x]_{{\\underline{m}}}\\bigr)_j\\bigr|^{4k}\\leq\\eta^{4k } , & \\\\",
    "& \\displaystyle \\ex \\bigl|\\bigl\\langle{\\beta}-{\\beta}^{m},x\\bigr\\rangle_{{\\mathbb h}}\\bigr|^{4k}\\leq\\bigl\\| { \\gamma}^{1/2}\\bigl({\\beta}^{m}-{\\beta}\\bigr ) \\bigr\\|_{{\\mathbb h}}^{4k } \\eta^{4k},&\\\\ & \\displaystyle   \\ex \\bigl|[x]_{{\\underline{m}}}^t[{\\gamma}]_{{\\underline{m}}}^{-1}[x]_{{\\underline{m}}}\\bigr|^{2k}\\leq m^{2k}\\eta^{4k}.&\\end{aligned}\\ ] ] from @xmath533 , @xmath534 , under assumption  [ assmom ] follows@xmath535_{{\\underline{m}}}^t[{\\gamma}]_{{\\underline{m}}}^{-1}[x]_{{\\underline{m}}}\\bigr|^2 { { \\mathbh1}}_{\\{[x]_{{\\underline{m}}}^t[{\\gamma}]_{{\\underline{m}}}^{-1}[x]_{{\\underline{m}}}>m n^{1/3}\\ } } \\leq\\frac{\\eta^{32}}{n^{14/3}}m^{2 } & \\nonumber\\end{aligned}\\ ] ] for all @xmath421 , and by employing markov s inequality @xmath536\\\\[-8pt ] p\\bigl(\\bigl|\\bigl\\langle { \\beta}-{\\beta}^{m},x\\bigr \\rangle_{{\\mathbb h}}\\bigr|>\\bigl\\|{\\gamma}^{1/2}\\bigl({\\beta}^{m}-{\\beta}\\bigr)\\bigr\\|_{{\\mathbb h}}n^{1/6}\\bigr)&\\leq & \\frac { \\eta^{32}}{n^{16/3}}.\\nonumber\\end{aligned}\\ ] ] we exploit these bounds in the following proofs .",
    "the key argument used in the proof of lemma  [ appgenerall2 ] is the following inequality due to  @xcite ; see , for example ,  @xcite .",
    "[ appgeneraltala ] let @xmath537 be independent @xmath538-valued random variables and @xmath539   ] $ ] , for @xmath540 belonging to a countable class @xmath541 of measurable functions .",
    "then , for @xmath542 , @xmath543 with @xmath544 , @xmath545 , @xmath546 and @xmath22 a universal constant and where @xmath547\\leq h,\\qquad \\sup_{s\\in { { \\mathbb s } } } \\frac{1}{n}\\sum_{i=1}^n \\var\\bigl(\\nu_s(t_i)\\bigr)\\leq v.\\ ] ]    [ appgenerall1]there exist a numerical constant @xmath26 such that for all @xmath116 :    [ appgenerall1e1]@xmath548_{{\\underline{m}}}\\|^4\\leq c \\eta^8 ( \\ex\\| x\\|_{{\\mathbb h}}^2)^2 $ ] ;    [ appgenerall1e2]@xmath549_{{\\underline{m}}}^t[{\\gamma}]_{{\\underline{m}}}^{-1}[w]_{{\\underline{m}}}}{\\rho_m^2}>\\frac { 1}{16 } ) } \\leq c \\eta^{64}$ ] ;    [ appgenerall1e3]@xmath550_{{\\underline{m}}}\\|_s > 1/8 ) } \\leq c(\\eta)$ ] ;    [ appgenerall1e4]@xmath551 .    denote by @xmath451 an eigenvalue decomposition of @xmath61_{{\\underline{m}}}$ ] .",
    "define @xmath452 and @xmath453_{{{\\underline{m}}}})$ ] , @xmath454 , @xmath552 .",
    "keep in mind that @xmath553 , @xmath554 and @xmath555 for @xmath556 ( assumption  [ assmom ] ) and @xmath557 are independent , centred for @xmath552 .",
    "consider [ appgenerall1e1 ] ,  [ appgenerall1e2 ] where @xmath458_{{\\underline{m}}}\\|^4= ( \\sum_{j=1}^m\\lambda_j(\\sum_{i=1}^nu_iv_{ij})^2)^2 $ ] and @xmath459_{{\\underline{m}}}^t[{\\gamma}]_{{\\underline{m}}}^{-1}[w]_{{\\underline{m}}})/\\rho_m^2 = n^{-2 } \\sum_{j=1}^m ( \\sum_{i=1}^nu_iv_{ij})^2 $ ] . applying minkowski s ( resp .",
    ", jensen s ) inequality and theorem 2.10 in  @xcite , we have @xmath558_{{\\underline{m}}}\\bigr\\|^4 & \\leq & n^{-2 } \\biggl[\\sum _ { j=1}^m \\lambda_j \\biggl(\\ex \\biggl|\\sum _ { i=1}^nu_iv_{ij } \\biggr|^4 \\biggr)^{1/2 } \\biggr]^2 \\\\ & \\leq & c \\eta^8 \\biggl[\\sum_{j=1}^m \\lambda_j \\biggr]^2 ; \\\\ { n^{k } } { m^{-k}\\rho^{-2k}_m}\\ex\\bigl\\| [ { \\gamma}]_{{\\underline{m}}}^{-1/2}[w]_{{\\underline{m}}}\\bigr\\|^{2k } & \\leq & \\frac{1 } { m}\\sum_{j=1}^m n^{-k}\\ex \\biggl|\\sum_{i=1}^nu_iv_{ij } \\biggr|^{2k } \\leq c(k ) \\eta^{4k},\\end{aligned}\\ ] ] which , respectively , implies  [ appgenerall1e1 ] , since @xmath559 , and  [ appgenerall1e2 ] , by employing markov s inequality",
    ". proof of  [ appgenerall1e3 ] . since @xmath560 are independent , centred with @xmath561 , @xmath562 , theorem 2.10 in  @xcite implies @xmath563_{{\\underline{m}}}\\|_s^{2k}\\leq c(k ) \\eta^{4k}$ ] because @xmath402_m\\|_s^2\\leq\\sum_{1\\leq j , l\\leq m}|v_{ij}v_{il}-\\delta_{jl}|^2 $ ] . applying markov",
    "s inequality gives  [ appgenerall1e3 ] .",
    "proof of  [ appgenerall1e4 ] . since @xmath564 are independent , centred with @xmath565 theorem 2.10 in @xcite implies@xmath566 and @xmath567 employing markov s inequality .",
    "[ appgaussl1e4 ] follows now from @xmath568 let @xmath569 , @xmath51 .",
    "there exists a numerical constant @xmath22 such that for all @xmath570 we have @xmath571_{{{\\underline{m}}}}^{-1/2}[w_n]_{{{\\underline{m}}}}\\|^2}{\\varsigma_m^2 } -12\\frac{m\\lambda_m^{[{\\gamma}]}}{n } \\biggr)_+\\\\ & & \\qquad\\leq \\frac { c}{n } \\biggl\\ { \\exp \\biggl ( - \\frac{m\\lambda_m^{[{\\gamma}]}}{6 } \\biggr ) + \\exp \\biggl ( - \\frac{n^{1/6}}{100 } \\biggr ) + \\frac{\\eta^{32 } } { n^{2 } } \\biggr\\}.\\end{aligned}\\ ] ]    let @xmath572 .",
    "define @xmath573 , @xmath574 , @xmath575_{{\\underline{m}}}^t[{\\gamma}]_{{\\underline{m}}}^{-1}[x]_{{\\underline{m}}}\\leq m n^{1/3}\\}$ ] and @xmath576 . for @xmath577 , @xmath578 , @xmath579 set@xmath580_{{\\underline{m}}}^{-1/2}[x]_{{\\underline{m}}}{{\\mathbh1}}_{\\{e\\in{{{\\mathcal}e}}_n , x\\in{{{\\mathcal}x}}_n\\ } } , \\\\",
    "r_{s}(e , x)&:= & \\bigl(\\sigma e+ \\bigl\\langle{\\beta}-{\\beta}^{m},x \\bigr\\rangle_{{\\mathbb h}}\\bigr ) s^t[{\\gamma}]_{{\\underline{m}}}^{-1/2}[x]_{{\\underline{m}}}(1- { { \\mathbh1}}_{\\{e\\in{{{\\mathcal}e}}_n , x\\in{{{\\mathcal}x}}_n\\}}).\\end{aligned}\\ ] ] let @xmath581 and @xmath582 , then @xmath277_{{{\\underline{m}}}}^{-1/2}[w_n]_{{{\\underline{m}}}}\\|^2=\\sup_{s\\in{{\\mathbb s}}^m}|\\nu_{s}^ * + r_{s}^*|^2 $ ] and hence @xmath583_{{{\\underline{m}}}}^{-1/2}[w_n]_{{{\\underline{m } } } } \\bigr\\|^2 -12\\varsigma_m^2\\frac { m\\lambda_m^{[{\\gamma}]}}{n } \\biggr)_+ \\nonumber\\\\ & & \\qquad\\leq2\\ex \\biggl ( \\sup_{s\\in{{\\mathbb s}}^m}\\bigl|\\nu_{s}^ * \\bigr|^2 -6\\varsigma_m^2\\frac { m\\lambda_m^{[{\\gamma}]}}{n } \\biggr)_+ + 2\\ex\\sup_{s\\in{{\\mathbb s}}^m}\\bigl|r_{s}^*\\bigr|^2\\\\ & & \\qquad=:2\\ { t_1+t_2\\ } , \\nonumber\\end{aligned}\\ ] ] where we bound the r.h.s .",
    "terms @xmath584 and @xmath585 separately .",
    "consider first @xmath584 . we intend to apply talagrand s inequality . to this end , for @xmath586 , we have@xmath587_{{\\underline{m}}}^t[{\\gamma}]_{{\\underline{m}}}^{-1}[x]_{{\\underline{m}}}{{\\mathbh1}}_{\\{e\\in{{{\\mathcal}e}}_n , x\\in{{{\\mathcal}x}}_{n}\\ } } \\nonumber\\\\[-8pt]\\\\[-8pt ] & \\leq&\\bigl(\\sigma+ \\bigl\\|{\\gamma}^{1/2}\\bigl({\\beta}^{m}-{\\beta}\\bigr ) \\bigr\\|_{{\\mathbb h}}\\bigr)^2 n^{2/3 }",
    "m \\leq \\varsigma^2_m n^{2/3 } m=:h^2 . \\nonumber\\end{aligned}\\ ] ] by employing the independence of @xmath6 and @xmath1 it is easily seen that @xmath588^t_{{{\\underline{m } } } } [ { \\gamma}]_{{\\underline{m}}}^{-1}[x]_{{\\underline{m } } } , \\\\ \\sup _ { s\\in{{\\mathbb s}}^m } \\frac{1}{n } \\sum_{i=1}^n \\var\\bigl(\\nu_{s}({\\varepsilon}_i , x_i)\\bigr ) & \\leq&\\sigma^2 + \\sup _ { s\\in{{\\mathbb s}}^m } \\ex\\bigl|\\bigl\\langle{\\beta}- { \\beta}^{m},x\\bigr\\rangle_{{\\mathbb h}}\\bigr|^2\\bigl|s^t [ { \\gamma}]_{{\\underline{m}}}^{-1/2}[x]_{{\\underline{m}}}\\bigr|^2.\\end{aligned}\\ ] ] by applying the cauchy  schwarz inequality together with @xmath589_{{\\underline{m}}}^{-1/2}[x]_{{\\underline{m}}}\\|^{4}\\leq m^2\\eta^{4}$ ] and @xmath590}}{n}=:h^2,\\ ] ] and taking into account that @xmath591_{{\\underline{m}}}^{-1/2}[x]_{{\\underline{m}}}|^{4}\\leq\\eta^{4}$ ] , @xmath592 , we obtain @xmath593 due to ( [ appgenerall2e11])([appgenerall2e13 ] ) talagrand s inequality ( lemma  [ appgeneraltala ] with @xmath594 ) implies @xmath595}}{n } \\biggr)_+ \\leq c \\frac{\\varsigma_m^2}{n } \\biggl\\{\\exp \\biggl ( - \\frac{m\\lambda_m^{[{\\gamma } ] } } { 6 } \\biggr ) + \\exp \\biggl ( - \\frac{n^{1/6}}{100 } \\biggr ) \\biggr\\},\\hspace*{-35pt}\\ ] ] where we used that @xmath596 .",
    "consider @xmath585 . by employing @xmath63_{{\\underline{m}}}[{\\gamma}]_{{\\underline{m}}}^{-1}[x]_{{\\underline{m}}}\\*{{\\mathbh1}}_{\\{x\\in{{{\\mathcal}x}}_{2,n}\\ } } \\leq mn^{1/3}$ ] and @xmath597 we have @xmath598_{{\\underline{m}}}[{\\gamma}]_{{\\underline{m}}}^{-1}[x]_{{\\underline{m}}}{{\\mathbh1}}_{\\{x\\notin{{{\\mathcal}x}}_{2,n}\\ } } \\\\ & & { } + m n^{1/3}\\ex\\bigl(\\sigma{\\varepsilon}+",
    "\\bigl\\langle{\\beta}- { \\beta}^{m},x\\bigr\\rangle_{{\\mathbb h}}\\bigr)^2 ( { { \\mathbh1}}_{\\{{\\varepsilon}\\notin{{{\\mathcal}e}}_n\\ } } + { { \\mathbh1}}_{\\ { x\\notin{{{\\mathcal}x}}_{1n}\\}}).\\end{aligned}\\ ] ] since @xmath599 , @xmath600 and latexmath:[$\\ex    the independence of @xmath6 and @xmath1 implies @xmath602_{{\\underline{m}}}[{\\gamma}]_{{\\underline{m}}}^{-1}[x]_{{\\underline{m}}}\\bigr|^2 { { \\mathbh1}}_{\\{x\\notin{{{\\mathcal}x}}_{2,n}\\ } } \\bigr)^{1/2 } \\\\ & & { } + m n^{1/3 } \\bigl\\ { \\sigma^2 \\ex{\\varepsilon}^2 { { \\mathbh1}}_{\\{{\\varepsilon}\\notin { { { \\mathcal}e}}_n\\ } } + \\bigl\\|{\\gamma}^{1/2}\\bigl({\\beta}-{\\beta}^{m}\\bigr ) \\bigr\\|_{{\\mathbb h}}^2p({\\varepsilon}\\notin{{{\\mathcal}e}}_n ) \\\\ & & \\hspace*{42.8pt}{}+ \\sigma^2 p(x\\notin{{{\\mathcal}x}}_{1n})+ \\ex\\bigl|\\bigl\\langle{\\beta}- { \\beta}^{m},x\\bigr\\rangle_{{\\mathbb h}}\\bigr|^2 { { \\mathbh1}}_{\\{x\\notin{{{\\mathcal}x}}_{1n}\\ } } \\bigr\\}.\\end{aligned}\\ ] ] we exploit now the estimates given in ( [ appgenerale1 ] ) and ( [ appgenerale2 ] ) .",
    "thereby , we obtain @xmath603 where we used that @xmath596 . keeping in mind decomposition ( [ appgenerall2decomp ] ) , the last bound and ( [ appgenerall2e2 ] ) imply together the claim of lemma  [ appgenerall2 ] .",
    "[ appgenerall3 ] there exists a constant @xmath604 depending on @xmath280 , @xmath281 and the classes @xmath188 and @xmath189 only such that for all @xmath116 we have @xmath605}\\ex \\biggl(\\bigl\\|[{\\gamma}]_{{{\\underline{m}}}}^{-1/2}[w_n]_{{{\\underline{m } } } } \\bigr\\|^2 -\\frac{12\\sigma_m^2 m\\lambda_m^{[{\\gamma}]}}{n } \\biggr)_+\\leq k\\frac{\\eta^{32 } ( \\sigma^2+r ) \\sigma}{n}.\\ ] ]    there exists an integer @xmath606 depending on @xmath280 , @xmath285 and the classes @xmath188 and @xmath189 only such that for all @xmath493 and for all @xmath607 we have @xmath608_{{{\\underline{m}}}}^t[{\\gamma}]_{{{\\underline{m}}}}^{-1}[g]_{{{\\underline{m}}}})= 2(\\sigma^2_y + [ g]_{{{\\underline{m}}}}^t[{\\gamma}]_{{{\\underline{m}}}}^{-1}[g]_{{{\\underline{m}}}})=\\sigma_m^2 $ ] .",
    "indeed , we have @xmath609 as @xmath610 and @xmath611 as @xmath69 because @xmath612 and @xmath613 due to lemma  [ appprel1][appprel1e5 ] .",
    "first , consider @xmath488 .",
    "due to lemma [ appgenerall1][appgenerall1e1 ] and @xmath525 [ lemma  [ appprel2](iv ) ] we have for all @xmath51 @xmath614_{{{\\underline{m}}}}^{-1/2}[w_n]_{{{\\underline{m } } } } \\bigr\\|^2 -\\frac { 12\\sigma_m^2m\\lambda_m^{\\gamma}}{n } \\biggr)_+\\leq\\ex\\bigl\\| [ { \\gamma}]_{{{\\underline{m}}}}^{-1/2}[w_n]_{{{\\underline{m } } } } \\bigr\\|^2\\leq c \\frac{m}{n } \\eta^{4}\\bigl ( \\sigma^2+d^6r\\bigr).\\ ] ] hence , @xmath483 and @xmath615 [ lemma [ appprel2](ii ) ] imply @xmath616}\\ex \\biggl(\\bigl\\|[{\\gamma}]_{{{\\underline{m}}}}^{-1/2}[w_n]_{{{\\underline{m } } } } \\bigr\\|^2 -12\\sigma_m^2\\frac { m\\lambda_m^{[{\\gamma}]}}{n } \\biggr)_+\\\\ & & \\qquad\\leq c(d)\\frac { n_o^{5/4}\\eta^4(\\sigma^2+r)}{n},\\end{aligned}\\ ] ] which proves the lemma for all @xmath617 .",
    "consider now @xmath493 where @xmath618 for all @xmath607 .",
    "thereby , we can apply lemma  [ appgenerall2 ] , which gives @xmath619}\\ex \\biggl(\\bigl\\|[{\\gamma}]_{{{\\underline{m}}}}^{-1/2}[w_n]_{{{\\underline{m } } } } \\bigr\\|^2 -12\\sigma_m^2\\frac { m\\lambda_m^{[{\\gamma}]}}{n } \\biggr)_+ \\\\ & & \\qquad\\leq c\\sup_{{\\beta}\\mathcal{f}^r_b } \\sup_{{\\gamma}\\in{{{\\mathcal}g}}_{{\\gamma}}^d}\\sum _ { m={m^\\diamond_n}}^{m^+_n } \\frac { \\varsigma_m^2\\delta_m^{[{\\gamma } ] } } { n } \\biggl\\ { \\exp \\biggl ( - \\frac{m\\lambda_m^{[{\\gamma}]}}{6 } \\biggr ) + \\exp \\biggl ( - \\frac{n^{1/6}}{100 } \\biggr ) + \\frac{\\eta^{32 } } { n^{2 } } \\biggr\\}.\\end{aligned}\\ ] ] since @xmath473}\\leq4d^3 \\delta_k^{\\gamma}$ ] , @xmath620 } \\geq(1+\\log d)^{-1 } \\lambda_k^{\\gamma}$ ] , @xmath621}\\leq \\delta_{m^+_n}^{[{\\gamma}]}\\leq n c d^6(1+\\log d)$ ] and @xmath622 [ lemma  [ appprel2](i ) , ( ii ) , ( iv ) , resp . ] follows @xmath619}\\ex \\biggl(\\bigl\\|[{\\gamma}]_{{{\\underline{m}}}}^{-1/2}[w_n]_{{{\\underline{m } } } } \\bigr\\|^2 -12\\sigma_m^2\\frac { m\\lambda_m^{[{\\gamma}]}}{n } \\biggr)_+ \\\\ & & \\qquad\\leq c(d ) \\bigl(\\sigma^2+r\\bigr ) n^{-1 } \\\\ & & \\qquad\\quad{}\\times\\sup_{{\\beta}\\in\\mathcal{f}^r_b}\\sup_{{\\gamma}\\in{{{\\mathcal}g}}_{{\\gamma}}^d } \\biggl\\ { \\sum _ { m={m^\\diamond_n } } ^{m^+_n } \\delta_m^{\\gamma}\\exp \\biggl(-\\frac{m\\lambda_m^{\\gamma}}{6(1+\\log d ) } \\biggr ) + n\\exp \\biggl ( - \\frac{n^{1/6}}{100 } \\biggr ) + \\frac{\\eta^{32 } } { n } \\biggr\\}.\\end{aligned}\\ ] ] finally , @xmath623 as in ( [ defsigma ] ) and @xmath624 imply for @xmath493 @xmath619}\\ex \\biggl(\\bigl\\|[{\\gamma}]_{{{\\underline{m}}}}^{-1/2}[w_n]_{{{\\underline{m } } } } \\bigr\\|^2 -12\\sigma_m^2\\frac { m\\lambda_m^{[{\\gamma}]}}{n } \\biggr)_+ \\\\ & & \\qquad\\leq c(d)\\frac{\\eta^{32}(\\sigma^2+r)\\sigma}{n}.\\end{aligned}\\ ] ] combining the cases @xmath488 and @xmath487 completes the proof .",
    "[ appgenerall4 ] there exist a numerical constant @xmath22 and a constant @xmath468 only depending on @xmath260 such that for all @xmath116 we have :    [ appgenerall4e1]@xmath625 ;    [ appgenerall4e2]@xmath626 ;    [ appgenerall4e3]@xmath627 .    by employing lemma  [ appgenerall1 ] rather than lemma [ appgaussl1 ]",
    "the proof follows along the lines of the proof of lemma [ appgaussl4 ] , and we omit the details .",
    "[ appgeneralp1 ] let @xmath278 in the definition ( [ defpen ] ) of the penalty @xmath257 .",
    "there exists a constant @xmath628 depending on @xmath280 , @xmath285 and the classes @xmath188 and @xmath189 only such that for all @xmath116 , we have @xmath629    by employing lemmas  [ appgenerall1 ] ,  [ appgenerall3 ] and [ appgenerall4 ] rather than lemmas  [ appgaussl1 ] , [ appgaussl3 ] and  [ appgaussl4 ] the proof follows along the lines of the proof of proposition  [ appgaussp1 ] , and we omit the details .    [ appgeneralp2 ]",
    "let @xmath278 in definition ( [ defpen ] ) and ( [ defhpenmen ] ) of @xmath257 and @xmath91 .",
    "there exists a constant @xmath468 only depending on @xmath260 such that for all @xmath116 , @xmath630    taking into account lemmas  [ appgenerall1][appgenerall1e1 ] and  [ appgenerall4 ] rather than lemmas [ appgaussl1][appgaussl1e1 ] and  [ appgaussl4 ] the proof follows along the lines of the proof of proposition [ appgaussp2 ] , and we omit the details .",
    "proof of proposition  [ momp1 ] the result follows from propositions  [ appgeneralp1 ] and  [ appgeneralp2 ] , and we omit the details .",
    "we are grateful to two referees and the associate editor for constructive criticism and clear guidelines ."
  ],
  "abstract_text": [
    "<S> we consider the estimation of the slope function in functional linear regression , where scalar responses are modeled in dependence of random functions . </S>",
    "<S> cardot and johannes [ _ j . </S>",
    "<S> multivariate anal . _ </S>",
    "<S> * 101 * ( 2010 ) 395408 ] have shown that a thresholded projection estimator can attain up to a constant minimax - rates of convergence in a general framework which allows us to cover the prediction problem with respect to the mean squared prediction error as well as the estimation of the slope function and its derivatives . </S>",
    "<S> this estimation procedure , however , requires an optimal choice of a tuning parameter with regard to certain characteristics of the slope function and the covariance operator associated with the functional regressor . as </S>",
    "<S> this information is usually inaccessible in practice , we investigate a fully data - driven choice of the tuning parameter which combines model selection and lepski s method . </S>",
    "<S> it is inspired by the recent work of goldenshluger and lepski [ _ ann . </S>",
    "<S> statist . _ </S>",
    "<S> * 39 * ( 2011 ) 16081632 ] . </S>",
    "<S> the tuning parameter is selected as minimizer of a stochastic penalized contrast function imitating lepski s method among a random collection of admissible values . </S>",
    "<S> this choice of the tuning parameter depends only on the data and we show that within the general framework the resulting data - driven thresholded projection estimator can attain minimax - rates up to a constant over a variety of classes of slope functions and covariance operators . the results are illustrated considering different configurations which cover in particular the prediction problem as well as the estimation of the slope and its derivatives . </S>",
    "<S> a simulation study shows the reasonable performance of the fully data - driven estimation procedure . </S>"
  ]
}