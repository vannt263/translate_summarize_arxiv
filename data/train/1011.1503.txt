{
  "article_text": [
    "in recent years , there has been an explosion in the number of applications to which compressed ( or compressive ) sensing has been applied @xcite . from image and video",
    "capture to microarrays and other applications represents just a sub - spectrum of its possible uses .",
    "an important application that seems particularly promising in terms of being translated into practical circuit designs is _",
    "quantization_. if the original signal to be compressed is @xmath1 dimensional but is @xmath2-sparse ( i.e. , has at most @xmath2 non - zero entries ) , and @xmath3 , then there is a significant benefit in using a compressive sensing framework for quantization .",
    "indeed , compressive sensing in itself represents a nearly - lossless linear transformation on the original source , and thus , compressive sensing is a  good lossless \" compression mechanism for sparse vectors , reducing the length of the representation of the original source from an @xmath1 dimensional vector to @xmath4 dimensional vector , which is an order - wise optimal lossless compression of the source . in particular ,  sampling \" matrices @xmath5 of dimension @xmath6 have been shown to exist where @xmath7 such that the original source @xmath8 can be recovered losslessly with high probability .",
    "this is not surprising from a compression perspective , as optimal linear compressors are known to exist for lossless compression .",
    "our goal in this paper is to investigate if compressive sensing is good for lossy compression of continuous - valued sources .",
    "the answer to this question is not immediately obvious , as typical lossy compression algorithms involve non - linear transformations between the source and its compressed equivalent in the encoding step .",
    "there are two ways in which compressive sensing can be combined with quantization .",
    "the first is where the number of samples @xmath9 in ( [ eqn : cs ] ) is reduced from @xmath10 to an orderwise smaller quantity .",
    "the resulting lossy - compressed vector @xmath11 is then  inverted \" to obtain a compressed version of @xmath12 .",
    "this approach is studied in detail for the case when @xmath13 in @xcite .",
    "the second is to maintain the lossless compression process as prescribed by ( [ eqn : cs ] ) and then use a ( possibly non - linear ) quantizer on @xmath14 to obtain @xmath15 .",
    "subsequently , @xmath15 is transformed using a suitable algorithm to obtain @xmath16 , a lossy - compressed version of @xmath8 .",
    "this second approach has been studied recently in @xcite , and it forms the main quantization scheme studied in this paper .",
    "[ fig : coding ]     the chain comprising of sampling using compressive sensing , then quantization and finally , reconstruction of the compressed vector is depicted in fig .",
    "[ fig : coding ] . associated with this framework",
    "are two notions of rate , the sampling rate and the compression rate . while the sampling rate is the rate at which the quantizer needs to the sample the incoming signal , the compression rate is the number of bits per symbol needed to represent the sampled signal within a fidelity criterion .",
    "this chain is particularly useful in developing a / d converters for sparse sources - it reduces the sampling rate at which they must operate thus making their design simpler and the quantization operation more effective .",
    "what we desire to know in this paper is if this quantization mechanism , besides being practically efficient , is indeed optimal . in other words , if the source were to be directly quantized ( using the best quantizer available ) , would it suffer a lower distortion than being first filtered in accordance with ( [ eqn : cs ] ) and then compressed ? observe that the compression rate at which the quantizer in fig .",
    "1 operates is higher than the optimal quantizer so that the number of codeword indices ( or the cardinality of the reconstruction alphabet ) is kept equal .",
    "mathematically , the compression rate of the quantizer in fig.1 is @xmath17 , while the optimal quantizer operates at a compression rate of @xmath18 .",
    "the result of this paper may also be interpreted as trading off compression rate for sampling rate while still achieving the same distortion performance as the optimal quantizer . while there is prior literature in studying the performance of specific designs of fig .",
    "1 @xcite , we prove a conclusive result on when the framework is optimal .    in this paper ,",
    "our focus is on those quantization applications where we desire the support of @xmath16 and @xmath8 to be identical .",
    "this is especially important in applications where we desire that the quantization process not introduce  spurious \" signals . in sensing systems and other applications where the signal represents a change in state of the system",
    ", it is particularly important that the compression process retain the original ( sparse ) support of the original .",
    "a distortion in the sparsity pattern could lead to false activation resulting in undesirable consequences .",
    "the problem also has applications in dna microarrays for cancer diagnosis , where a wrong sparsity pattern could lead to faulty diagnosis .",
    "our main results are as follows :    1 .",
    "the coding architecture in fig . [ fig : coding ] is distortion - rate optimal when the reconstruction is also required to be sparse .",
    "we show this result when the distortion constraint is on any @xmath0-norm of the error between the source and reconstruction sequence , where @xmath19 .",
    "3 .   in order to prove such a general result",
    ", we study a modified restricted isometry property ( rip ) for matrices and show the existence of matrices that satisfy this property .",
    "the modified rip introduced in this paper is essential in order to prove the distortion rate optimality for @xmath0-norm distortion measures .",
    "the proof of existence of matrices satisfying the modified rip uses hoeffding s inequality . related to this work",
    "is the use of hoeffding s inequality to obtain rip bounds in a recent paper @xcite .",
    "also related are results on heavy tailed restricted isometries in @xcite and rip using tail bounds in @xcite . while hoeffding s inequality has been previously used in other contexts to obtain rip bounds , this paper uses it to prove the modified rip required for @xmath0-norm compression .",
    "the rest of this paper is organized as follows . in the next section ,",
    "we describe the system model . in section",
    "[ sec : mresult ] , we state the main results of the paper .",
    "we conclude the paper in section [ sec : conc ] .",
    "the proofs of the results are detailed in the appendix .",
    "consider the set @xmath20 of all @xmath2-sparse vectors of length @xmath1 where each non - zero entry takes any value in @xmath21 .",
    "the goal is to compress the sparse and real - valued @xmath8 to a vector @xmath16 within a distortion @xmath22 .",
    "note that in general , the rate distortion optimal quantizer does not ensure the reconstruction , @xmath16 , is sparse .",
    "since we desire the support of @xmath23 and @xmath24 be identical , we let @xmath23 belong to a @xmath2-sparse reconstruction space denoted by @xmath25 .",
    "let @xmath26 , be the indices such that @xmath27 for @xmath28 . observe that @xmath29 is a random set on account of @xmath24 being a random vector .",
    "let @xmath30 be the vector with components corresponding to indices in @xmath29 .",
    "@xmath31 is defined in a similar fashion .",
    "we begin by defining the distortion rate function of the optimal quantizer .",
    "let @xmath32 be the average distortion achieved by a code operating at rate @xmath18 for @xmath2-sparse source vectors distributed according to @xmath33 .",
    "mathematically , @xmath34 \\\\",
    "\\textrm{subject to } & \\lvert\\hat{\\mathcal{x}}^n_k\\rvert \\leq 2^{nr } \\textrm { and } x^n(t^c)=\\hat{x}^n(t^c).\\end{aligned}\\ ] ] we wish to point out out that the equality constraint in the optimization problem limits the reconstruction spaces to those that are @xmath2-sparse with the same sparsity pattern as the source .",
    "we now define the optimization problem concerning the quantizer in fig . 1 followed by the distortion rate function of the compression scheme in fig",
    "let @xmath35 and let @xmath36 denote the quantized version of @xmath37 .",
    "let @xmath38 be the @xmath39-th row of a matrix @xmath5 of dimension @xmath40 , @xmath41 and @xmath42 satisfy @xmath43 .",
    "define , @xmath44\\\\ \\textrm{subject to } & \\lvert\\hat{\\mathcal{x}}^n_k\\rvert \\leq 2^{nr } \\textrm { and } x^n(t^c)=\\hat{x}^n(t^c).\\end{aligned}\\ ] ] the quantity defined above represents the distortion achieved in @xmath37 corresponding to a particular distortion metric . since , we force the quantizer to search over quantized versions of the form @xmath36 , the optimization is again carried out over the reconstruction spaces of the form @xmath25 .",
    "note that we depart from the usual convention of denoting the compression rate as the argument of the distortion rate function in the definition of @xmath45 .",
    "the argument @xmath18 in @xmath45 denotes the fact that @xmath46 indices are used for quantization and the compression rate is in fact @xmath17 .",
    "let @xmath47 be the average distortion achieved in @xmath24 by the scheme consisting of compressive sensing followed by quantization and reconstruction .",
    "the chain shown in fig .",
    "1 uses @xmath46 codewords at a compression rate @xmath18 . in the next section",
    ", we present our main result relating @xmath47 and @xmath32 .",
    "we first briefly discuss the order wise optimality of compressed sensing for lossless compression before turning to the main results of this paper .",
    "let us assume for this discussion alone that the non - zero entries of @xmath24 are discrete random variables belonging to some alphabet @xmath48 with finite cardinality .",
    "let @xmath29 denote the sparsity pattern of @xmath24 , uniformly distributed among @xmath49 possiblities .",
    "mathematically , @xmath50 .",
    "if @xmath18 is the rate of compression , we have ,    @xmath51    here @xmath52 follows from fano s inequality where @xmath53 as @xmath54 .",
    "therefore compressed sensing is an order wise optimal lossless compression scheme .",
    "we now turn to the main results of the paper .",
    "we state the restricted isometry property ( rip ) for matrices @xcite .",
    "a matrix @xmath5 , is said to satisfy the @xmath55-rip if @xmath56 , @xmath57    we now state a modified version of the rip which is useful in proving the rate distortion result of the paper when the distortion constraint is on any @xmath0-norm on the error between the source and the reconstruction .",
    "a matrix @xmath5 , is said to satisfy the modified @xmath55-rip if @xmath56 , and @xmath19 , @xmath58    we show the existence of matrices satisfying the above modified rip through the following theorem .",
    "[ thm : mripproof ] let @xmath5 be a matrix of dimension @xmath40 containing entries chosen i.i.d . and supported in @xmath59 $ ] , where @xmath60 .",
    "for every @xmath61 , if @xmath62 , there exists a constant @xmath63 such that with probability greater than @xmath64 , @xmath65    the above theorem is proved in the appendix .",
    "we now show that there exist matrices that satisfy both the rip and the modified rip simultaneously with high probability through the following lemma",
    ". such a result will be useful in proving the main result of the paper .",
    "[ thm : mrip ] if @xmath66 , then there exists an @xmath40 matrix @xmath5 that satisfies the rip ( [ eqn : rip ] ) and the modified rip ( [ eqn : mrip ] ) with high probability .",
    "let the entries of @xmath5 be i.i.d . and distributed according to a bernoulli distribution taking values @xmath67 or @xmath68 with equal probability .",
    "then , it follows from @xcite that @xmath5 satisfies the rip ( [ eqn : rip ] ) .",
    "further , using theorem [ thm : mripproof ] , it follows that @xmath5 also satisfies the modified rip ( [ eqn : mrip ] ) with high probability since the entries of @xmath5 are i.i.d . and belong to @xmath69 $ ] . therefore , @xmath5 satisfies both ( [ eqn : rip ] ) and ( [ eqn : mrip ] ) with high probability .    the following theorem , which is the main result of the paper states that the coding architecture of fig .",
    "[ fig : coding ] achieves the same distortion rate function as the optimal compression scheme for the @xmath2-sparse source .    the coding architecture in fig .",
    "1 is distortion rate optimal when @xmath5 satisfies the rip ( [ eqn : rip ] ) and the modified rip ( [ eqn : mrip ] ) .",
    "mathematically , @xmath70 , with high probability , @xmath71    by lemma [ thm : mrip ] , there exists a @xmath5 that satisfies the rip ( [ eqn : rip ] ) and the modified rip ( [ eqn : mrip ] ) simultaneously .",
    "a candidate code for the quantization of @xmath37 can be described as follows .",
    "the optimal codebook for @xmath24 is multiplied by @xmath5 to obtain a codebook for @xmath37 .",
    "now , given a @xmath37 , the quantizer looks for that @xmath72 that minimizes the average distortion .",
    "since @xmath5 satisfies the modified rip with high probability , we have @xmath73 & \\leq \\sum_{i=1}^m\\mathbb{e}\\left[\\frac{1}{m}\\frac{\\lvert y_i-\\phi_i \\hat{x}^n\\rvert}{\\lvert\\phi_i\\rvert_q}\\right]\\nonumber\\\\ \\rightarrow ( 1-\\epsilon)d_2(r)&\\leq \\delta_2(r)\\label{eqn : step1}\\end{aligned}\\ ] ]    now , @xmath74 , let @xmath75 be the quantized value of @xmath24 according to the optimal quantizer that achieves @xmath76 .",
    "@xmath77 is a feasible solution to the problem @xmath45 .",
    "therefore , again by the modified rip , with high probability , @xmath78\\leq \\frac{1}{n}(1+\\epsilon)\\lvert x^n-\\tilde{x}^n\\rvert_p\\leq(1+\\epsilon)(d_1(r)+\\delta).\\ ] ] now , since @xmath79 for all @xmath80 , we get @xmath81 from equation ( [ eqn : step1 ] ) and ( [ eqn : step2 ] ) , we get @xmath71 we claim that the scheme in fig .",
    "1 achieves the optimal distortion rate function since @xmath82 for all @xmath61 .",
    "however , note that as @xmath83 , we require more and more number of measurements to satisfy the rip and modified rip with high probability . also , forcing @xmath36 where @xmath23 is sparse implies that recovery of the sparse vector is possible without any loss .",
    "in other words , @xmath23 may be exactly recovered from @xmath84 since @xmath5 satisfies the rip as well .    for the specific case of 2-norm distortion measures",
    ", the above theorem can be proved for matrices @xmath5 that just satisfy the rip alone .",
    "define @xmath85 \\\\",
    "\\textrm{subject to } & \\lvert\\hat{\\mathcal{x}}^n_k\\rvert \\leq 2^{nr } \\textrm { and } x^n(t^c)=\\hat{x}^n(t^c)\\end{aligned}\\ ] ] and @xmath86\\\\ \\textrm{subject to } & \\lvert\\hat{\\mathcal{x}}_k^n\\rvert \\leq 2^{nr } \\textrm { and } x^n(t^c)=\\hat{x}^n(t^c).\\end{aligned}\\ ] ] let @xmath87 be the distortion rate function achieved by the coding architecture of fig",
    ". 1 .    the coding architecture of fig .",
    "1 is distortion rate optimal when @xmath5 satisfies the rip ( [ eqn : rip ] ) .",
    "mathematically , @xmath70 , with high probability , @xmath88    the proof is similar to that of theorem 2 , where only the rip is used instead of the modified rip in steps ( [ eqn : step1 ] ) and ( [ eqn : step2 ] ) .",
    "we consider the problem of quantization of sparse signals using compressive sensing .",
    "we show that the chain comprising of compressive sampling followed by quantization and then reconstruction is rate distortion optimal when the reconstruction is also required to be sparse .",
    "the result is shown when the distortion metric is any @xmath0-norm , @xmath19 , on the error between the source and the reconstruction .",
    "the proof of the result requires the compressive sensing matrix to satisfy a new modified restricted isometry property and we also prove the existence of matrices satisfying this property .",
    "theorem [ thm : mrip ] is proved through the sequence of the following lemmas .",
    "the overall procedure closely follows the technique in @xcite with suitable changes as required .",
    "we first state a lemma about the concentration of measure around the mean for bounded random variables .",
    "let @xmath89 , where @xmath90 , @xmath41 , are independent bounded continuous random variables such that @xmath91 for each @xmath39 . also , let @xmath92=\\beta m$ ] .",
    "following the procedure in @xcite , we use the inequality @xmath96 for all @xmath97 to get @xmath98\\leq 1-\\lambda\\mathbb{e}[|y_i|]+\\lambda^2\\mathbb{e}[|y_i|^{2}],\\ ] ] for @xmath99 .",
    "further , since @xmath100 for all @xmath101 , we have @xmath98\\leq   e^{-(\\lambda\\mathbb{e}[|y_i|]-\\lambda^2\\mathbb{e}[|y_i|^{2}])}.\\ ] ] therefore , by markov s inequality , we obtain , @xmath102&=\\mathbb{p}\\left[e^{-\\lambda z}\\geq e^{-\\lambda(1-\\epsilon)\\beta m}\\right]\\\\ & \\leq e^{\\lambda(1-\\epsilon)\\beta m}\\mathbb{e}\\left[e^{-\\lambda z}\\right]\\\\ & = e^{\\lambda(1-\\epsilon)\\beta m}\\mathbb{e}\\left[e^{-\\sum_{i=1}^m\\lambda|y_i|}\\right]\\\\ & \\leq e^{-\\lambda\\left[\\sum_{i=1}^m\\mathbb{e}[|y_i|]-m(1-\\epsilon)\\beta -\\lambda\\sum_{i=1}^m\\mathbb{e}[|y_i|^{2}]\\right]}\\\\ & \\leq e^{-m\\lambda\\left[\\epsilon\\beta -\\lambda\\sum_{i=1}^m\\mathbb{e}[|y_i|^{2}]/m\\right]}. \\end{aligned}\\ ] ]    for the other side of the inequality , we use hoeffding s inequality as follows .",
    "now , @xmath103 & = \\mathbb{p}\\left[e^{\\lambda z}\\geq e^{\\lambda(1+\\epsilon)\\beta m}\\right]\\\\ & \\leq e^{-\\lambda(1+\\epsilon)\\beta m}\\mathbb{e}\\left[e^{\\lambda z}\\right]\\\\ & \\leq e^{-\\lambda(1+\\epsilon)\\beta m}\\left[e^{\\lambda^2c^2/8}\\right]^m\\\\ & = e^{-m\\lambda\\left[(1+\\epsilon)\\beta-\\lambda c^2/8\\right]},\\end{aligned}\\ ] ] where we use markov s inequality in the second step and hoeffding s inequality in the third step . choosing @xmath104},\\frac{(1+\\epsilon)8\\beta}{c^2}\\right\\}$ ] , we get @xmath105 > 1 - 2e^{-m\\gamma(\\epsilon)},\\ ] ] with @xmath106}{m},(1+\\epsilon)\\beta-\\lambda \\frac{c^2}{8}\\big\\}>0 $ ] .",
    "the following lemma states that for every given @xmath8 , there exists a matrix that satisfies the condition in the modified isometry property with high probability .",
    "note that this does not prove the statement of the theorem yet since we need to show the existence of a matrix that satisfies the condition for all @xmath107 .",
    "[ thm : probgivenx ] for every given @xmath8 , and @xmath61 , an @xmath40 matrix @xmath5 with i.i.d .",
    "entries supported in @xmath59 $ ] , where @xmath60 , satisfies @xmath108 < 2e^{-m\\gamma(\\epsilon)},\\ ] ] where @xmath95 and @xmath43 , @xmath109 .    since each entry of @xmath5 is i.i.d . and",
    "bounded in @xmath59 $ ] , by hlder s inequality , we have @xmath110 where @xmath111 satisfies @xmath43 .",
    "therefore , the random variable @xmath112 satisfies @xmath113 , for @xmath41 .",
    "let @xmath114=m\\beta$ ] .",
    "it follows that @xmath115 , since @xmath116 \\leq \\mathbb{e}\\left[\\frac{\\lvert x^n\\rvert_p\\lvert \\phi_i\\rvert_q}{\\lvert x^n\\rvert_p\\lvert \\phi_i\\rvert_q}\\right]= 1.\\ ] ] by applying lemma [ thm : concentration ] , we get @xmath117 & > 1 - 2e^{-m\\gamma(\\epsilon)}\\\\ \\rightarrow \\mathbb{p}\\left[(1-\\epsilon)\\leq \\sum_{i=1}^m\\frac{1}{m}|y_i|\\leq ( 1+\\epsilon)\\right ] & > 1 - 2e^{-m\\gamma(\\epsilon)},\\end{aligned}\\ ] ] since @xmath115 .",
    "the desired result follows from the above .",
    "we now present a lemma about the quantization of vectors in the unit @xmath0-norm ball .",
    "we characterize the size of the set required to represent every such vector within a prescribed @xmath0-norm error .",
    "@xmath123 is a set of quantization indices in @xmath1 dimensions with size @xmath124 .",
    "we now define @xmath125 , where @xmath126 is the unit ball in @xmath127 with @xmath128 norm as the distance metric .",
    "the size of @xmath129 , is then the ratio of the volumes of the unit ball @xmath126 to the unit cube times the size of @xmath123 .",
    "the volume of @xmath126 is given by @xmath130 therefore , @xmath131    we now specify the choice of @xmath2 that bounds @xmath132 as required .",
    "let @xmath133 be the quantization index for @xmath8 .",
    "mathematically , for @xmath134 , we choose , @xmath135 .",
    "therefore , @xmath136 and @xmath137 choosing @xmath138 , we satisfy @xmath139 , and obtain @xmath140 now , @xmath141      we now prove theorem 1 .",
    "@xmath5 is a matrix of dimension @xmath40 containing entries chosen i.i.d . and supported in @xmath59 $ ] , where @xmath60 .",
    "we need to show that for every @xmath61 , if @xmath62 , there exists a constant @xmath63 such that with probability greater than @xmath64 , we have @xmath65    without loss of generality , we consider @xmath8 such that @xmath145 . we first prove that for the set of @xmath2-sparse vectors @xmath8 with a given sparsity pattern , @xmath5 exists with probability greater than @xmath146 .",
    "now , we show that there exists a matrix @xmath5 such that @xmath149 with high probability . by lemma [ thm : probgivenx ] and",
    "union bounding argument , we have , @xmath150 \\leq |\\mathcal{q}|2e^{-m\\gamma(\\epsilon/2 ) } \\leq 2\\sqrt{\\frac{k}{2\\pi p}}(8c_1/\\epsilon)^ke^{-m\\gamma(\\epsilon/2)}.\\ ] ]      now , considering all @xmath154 , @xmath2 sparse vectors , the probability that there does not exist @xmath5 satisfying the modified rip is upper bounded by @xmath155+\\frac{1}{2}\\log\\frac{k}{2\\pi p}-m\\gamma(\\epsilon/2)+\\log2}.\\ ] ] therefore , if @xmath156+\\frac{1}{2}\\log\\frac{k}{2\\pi p}+\\log2\\right),\\ ] ] there exists @xmath63 , chosen smaller than @xmath157+\\frac{1}{2}\\log\\frac{k}{2\\pi p}+\\log2\\right)$ ] , such that probability that there does not exist a matrix satisfying the @xmath0-norm condition is upper bounded by @xmath158 .",
    "f.  parvaresh , h.  vikalo , s.  misra , and b.  hassibi , `` recovering sparse signals using sparse measurement matrices in compressed dna microarrays , '' _ ieee journ .",
    "topics in sig . process .",
    "issue on genomic and proteomic sig .",
    "_ , vol .  2 , pp",
    ". 275285 , jun . 2008 .",
    "j.  n. laska , p.  boufounos , m.  a. davenport , and r.  g. baraniuk , `` democracy in action : quantization , saturation , and compressive sensing , '' 2009 , submitted for publication .",
    "preprint available at http://www.ece.rice.edu/  richb / recentpapers.html .",
    "b.  nazer and r.  nowak , `` sparse interactions : identifying high - dimensional multilinear systems via compressed sensing , '' in _ proc . of 48th allerton conference on communication , control and computing _ , monticello , il 2010 ."
  ],
  "abstract_text": [
    "<S> the problem of compressing a real - valued sparse source using compressive sensing techniques is studied . </S>",
    "<S> the rate distortion optimality of a coding scheme in which compressively sensed signals are quantized and then reconstructed is established when the reconstruction is also required to be sparse . </S>",
    "<S> the result holds in general when the distortion constraint is on the expected @xmath0-norm of error between the source and the reconstruction . </S>",
    "<S> a new restricted isometry like property is introduced for this purpose and the existence of matrices that satisfy this property is shown . </S>"
  ]
}