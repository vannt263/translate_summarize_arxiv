{
  "article_text": [
    "in @xcite , we discussed the measurement of luminosity distances of events using the advanced and virgo ground - based interferometric gravitational - wave ( ) detectors . in the main letter ,",
    "an algorithm was introduced for rapidly extracting directionally dependent distance estimates from observations and illustrated the typical shape of volume reconstructions during early advanced .",
    "finally , we argued that the structure and distance information can be leveraged to guide searches of likely nearby host galaxies for x - ray , optical , and infrared counterparts of mergers .",
    "this supplement provides the following supporting material .",
    "first , in  [ sec : format ] , we document a file format for the rapid transmission of volume reconstructions in alerts . it is based on and is backward - compatible with the localization formation that we introduced in @xcite and that was employed in alerts that were sent in (;",
    "second , in ",
    "[ sec : data ] , we describe the online data release , which provides a browsable collection of simulated sky maps .",
    "third , in  [ sec : tutorial ] , we provide a python primer for performing basic operations on sky maps , all the way through selecting a list of the most likely host galaxies . in  [ sec : algorithm ] , we provide additional details of the position reconstruction algorithm . finally , in  [ sec : faithfulness ] , we show that the algorithm produces faithful representations of the full probability distributions .",
    "the reader who is interested in leveraging distance information for planning follow - up observations or performing archival research needs only consult  [ sec : format ] and  [ sec : tutorial ] .",
    "the localization for a single candidate is stored as a (;",
    "@xcite ) file .",
    "the file contains a single binary table @xcite that represents a (;",
    "@xcite ) all - sky image .",
    "the table has four floating - point columns , listed in table  [ table : fitscolumns ] , which represent four channels of the image .",
    "the first column , ` prob ` , is simply the probability that the source is contained within the pixel @xmath0 that is centered on the direction @xmath1 , the same as in the localization format . the second and third columns , ` distmu ` and `",
    "diststd ` , are the ansatz location and scale parameters , respectively . the fourth column , `",
    "distnorm ` , is the ansatz normalization coefficient , included for convenience .    in pixels on the sky that contain very little probability ,",
    "sometimes the conditional distance distribution can not be represented using the ansatz .",
    "this is signaled by ` distmu`=@xmath2 , ` distsigma`=@xmath3 , and ` distnorm`=0 .",
    "> llll prob & @xmath4 & pixel@xmath5 & probability that the source is contained in pixel @xmath0 , centered on the direction @xmath1 + distmu & @xmath6 & mpc & ansatz location parameter of conditional distance distribution in direction @xmath1 , or @xmath2 if invalid + distsigma & @xmath7 & mpc & ansatz scale parameter of conditional distance distribution in direction @xmath1 , or @xmath3 if invalid + distnorm & @xmath8 & mpc@xmath9 & ansatz normalization coefficient , or @xmath10 if invalid    the header , an example of which is shown in table  [ table : fitsheader ] , provides metadata including the utc time of the trigger and the list of instruments that contributed to the localization .",
    "the header also provides values for ` distmean ` and ` diststd ` , respectively , being the posterior mean and standard deviation of distance marginalized over the whole sky .",
    "> l > l > l simple & t & conforms to fits standard + bitpix & 8 & array data type + naxis & 0 & number of array dimensions + extend & t + xtension & bintable & binary table extension + bitpix & 8 & array data type + naxis & 2 & number of array dimensions + naxis1 & 16384 & length of dimension 1 + naxis2 & 3072 & length of dimension 2 + pcount & 0 & number of group parameters + gcount & 1 & number of groups + tfields & 4 & number of table fields + ttype1 & prob  + tform1 & 1024e  + tunit1 & pix-1  + ttype2 & distmu  + tform2 & 1024e  + tunit2 & mpc  + ttype3 & distsigma + tform3 & 1024e",
    " + tunit3 & mpc  + ttype4 & distnorm + tform4 & 1024e  + tunit4 & mpc-2 ",
    "+ pixtype & healpix  & healpix pixelisation + ordering & nested  & pixel ordering scheme , either ring or nested + coordsys & c  & ecliptic , galactic or celestial ( equatorial ) + extname & xtension & name of this binary table extension + nside & 512 & resolution parameter of healpix + firstpix & 0 & first pixel # ( 0 based ) + lastpix & 3145727 & last pixel # ( 0 based ) + indxschm & implicit & indexing : implicit or explicit + object & coinc_event : coinc_event_id:18951 & unique identifier for this event + instrume & h1,l1  & instruments that triggered this event + date - obs & 2010 - 09 - 03t06:12:26.60324 & utc date of the observation + mjd - obs & 55442.2586412414 & modified julian date of the observation + date & 2015 - 04 - 13t10:17:11 & utc date of file creation + creator & bayestar_localize_coincs.py & program that created this file + distmean & 68.54061620909769 & posterior mean distance in mpc + diststd & 17.14006463067744 & posterior standard deviation of distance in mpc +",
    "an online data release provides a browsable catalog of simulated localizations .",
    "one may select events from or .",
    "events may be sorted by detector network ( a one- or two - letter combination consisting of ` h ' for , ` l ' for , ` v ' for virgo ) , 90% credible volume in mpc@xmath11 , 90% credible area in deg@xmath12 , or . for each event",
    ", a or lalinference file may be downloaded .",
    "a screen shot of the data release is shown in fig .  [",
    "fig : screenshot ] .",
    "in this section , we provide some python sample code to perform some simple manipulations of sky maps .",
    "the triple greater - than signs ( ` > > > `  ) and triple - dots ( ` ... `  ) are the python interactive prompt ; the reader should type everything on the line _",
    "after _ these .",
    "these examples will work in python 2.7 and later on linux or unix systems .",
    "if the reader does not already have a python environment of preference , we suggest the anaconda python distribution for desktop use or the lightweight miniconda variant for computing clusters . only the astropy , healpy , and numpy packages are essential for working with the localizations , but the examples below will also use matplotlib , scipy , and astroquery .",
    "all of these packages can be installed with pip :    .... $ pip install astropy astroquery healpy matplotlib scipy ....      for all of the samples below , start by importing the healpy for working with files , the numpy for vector operations , matplotlib for plotting , and scipy for probability functions :    .... $ python > > > import healpy as hp > > > import numpy as np > > > from matplotlib import pyplot as plt >",
    "> > from scipy.stats import norm ....    next , select download an example sky map from the data release . in this example , we use the simulated event that is shown in figs .  1 and 2 of @xcite",
    ". a convenient way to download it is using astropy s ` download_file ` utility , which will retrieve the file and cache it locally :    .... > > > from astropy.utils.data import download_file > > > url = ( ' https://dcc.ligo.org/p1500071/public ' ... + ' /18951_bayestar.fits.gz ' ) > > > filename = download_file(url , cache = true ) ....    the new localization format is backward - compatible with the format introduced in @xcite . by default , when we read the healpix file with healpy ( or any other common - place healpix library or tool ) , we get just the first layer , the probability sky map :    .... > > > prob = hp.read_map(filename ) ....    to read both the probability layer and the three additional distance layers , we need to pass the optional ` field= ` parameter to healpy :    .... > > > prob , distmu , distsigma , distnorm = hp.read_map ( ... filename , field=[0 , 1 , 2 , 3 ] ) ....    or slightly more concisely :    .... > > > prob , distmu , distsigma , distnorm = hp.read_map ( ... filename , field = range(4 ) ) ....    last , it will be useful for subsequent healpy calls to have the resolution on hand :    .... > > > npix = len(prob ) > > > npix 3145728 > >",
    "> nside = hp.npix2nside(npix ) > > > nside 512 ....      in this example , we compute the probability per steradian or per deg@xmath12 that the source is in a given direction .",
    "let s take as an example the following equatorial coordinates :    .... > > >",
    "ra , dec = 137.8 , -39.9 ....    which , coincidentally , happen to be the true simulated position to the source .",
    "healpy uses ",
    "physicist s \" spherical coordinates ( @xmath13 ) , with @xmath14 $ ] being the colatitude from the north celestial pole in radians , and @xmath15 being the right ascension in radians .",
    "we convert    ....",
    "> > > theta = 0.5 * np.pi - np.deg2rad(dec ) > >",
    "> phi = np.deg2rad(ra ) ....",
    "next , we use healpy to look up the index of the pixel that contains that direction :    .... > > > ipix = hp.ang2pix(nside , theta , phi ) > > > ipix 2582288 ....    healpy will tell us the area per pixel in steradians at the current resolution :    .... > > > pixarea = hp.nside2pixarea(nside ) > > > pixarea 3.994741635118857e-06 ....    or in deg@xmath12 :    .... > > >",
    "pixarea_deg2 = hp.nside2pixarea(nside , degrees = true ) > > > pixarea_deg2 0.013113963206424481 ....    all that is left to do is look up the probability contained within pixel ` ipix ` and ( if desired ) divide by the area per pixel to obtain the probability per steradian :    .... > > > dp_da = prob[ipix ] / pixarea > > > dp_da 7.4387317043042076 ....    or the probability per deg@xmath12 :    .... > > >",
    "dp_da_deg2 = prob[ipix ] / pixarea_deg2 > > > dp_da_deg2 0.0022659672582507331 ....      next , we calculate the conditional distance distribution along a given line of sight , which is the probability per unit distance under the assumption that the source is in a given direction .",
    "we will use the same sky position as in the example above .",
    "we lay out a grid in distance along that line of sight :    .... > > > r = np.linspace(0 , 150 ) ....    then , we plug everything into the ansatz distribution :    .... > > >",
    "dp_dr = r**2 * distnorm[ipix ] * norm ( ... distmu[ipix ] , distsigma[ipix]).pdf(r ) ....    finally , we plot the result :    .... > > > plt.plot(r , dp_dr ) > > > plt.xlabel('distance ( mpc ) ' ) > > > plt.ylabel('prob mpc$^{-1}$ ' ) > > > plt.show ( ) ....          now , we calculate the probability density per mpc@xmath11 at a point .",
    "we will use the same right ascension and declination as above and a distance of 74.8mpc :    .... > > > r = 74.8 ....    finally ,    .... > > >",
    "dp_dv = prob[ipix ] * distnorm[ipix ] * norm ( ... distmu[ipix ] , distsigma[ipix]).pdf(r ) / pixarea > > > dp_dv 3.1173200109121657e-05 ....      as our next example , we compute the marginal distance distribution , the probability density per unit distance integrated over the entire sky :    .... > > > r = np.linspace(0 , 150 ) > > > dp_dr = [ np.sum(prob * rr**2 * distnorm ... * norm(distmu , distsigma).pdf(rr ) ) for rr in r ] ....    finally , we plot the result :    .... > > > plt.plot(r , dp_dr ) >",
    "> > plt.xlabel('distance ( mpc ) ' ) >",
    "> > plt.ylabel('prob mpc$^{-1}$ ' ) > > > plt.show ( ) ....          as our final example , we will generate a ranked list of galaxies .    for the purpose of this demonstration",
    ", we will use the (; @xcite ) because it is a flux - limited all - sky spectroscopic redshift catalog .",
    "this greatly simplifies the issues of completeness , sky coverage , and accuracy of redshift estimates .",
    "first , download the entire catalog from vizier @xcite using astroquery :    .... > > > from astroquery.vizier import vizier > > > vizier.row_limit = -1 > > > cat , = vizier.get_catalogs('j/apjs/199/26/table3 ' ) ....    according to @xcite , the luminosity function is well fit by a schechter function with a cutoff absolute magnitude of @xmath16 and a power - law index of @xmath17 .",
    "we find the maximum absolute magnitude @xmath18 for a completeness fraction of 0.5 :    .... > >",
    "> from scipy.special import gammaincinv > > > completeness = 0.5 > > > alpha = -1.0 >",
    "> > mk_star = -23.55 > > > mk_max = mk_star + 2.5 * np.log10 ( ... gammaincinv(alpha + 2 , completeness ) ) > >",
    "-23.947936347387156 ....    we select only galaxies with positive redshifts and absolute magnitudes greater than @xmath18 :    .... > >",
    "> from astropy.cosmology import wmap9 as cosmo > > > from astropy.table import column >",
    "> > import astropy.units as u > > > import astropy.constants as c > > > z = ( u.quantity(cat['cz ' ] ) / c.c).to ( ...",
    "u.dimensionless_unscaled ) > > > mk = cat['ktmag ' ] - cosmo.distmod(z ) > > > keep = ( z > 0 ) & ( mk < mk_max ) > > > cat = cat[keep ] > > > z = z[keep ] ....    then , we calculate the luminosity distance and index of each galaxy :    .... > > > r = cosmo.luminosity_distance(z).to('mpc').value > >",
    "> theta = 0.5 * np.pi - cat['_dej2000'].to('rad').value > > >",
    "phi = cat['_raj2000'].to('rad').value > > > ipix = hp.ang2pix(nside , theta , phi ) ....    we find the probability density per unit volume at the position of each galaxy :    .... > > >",
    "dp_dv = prob[ipix ] * distnorm[ipix ] * norm ( ... distmu[ipix ] , distsigma[ipix]).pdf(r ) / pixarea ....    finally , we sort the galaxies by descending probability density and take the top 50 :    .... > > >",
    "top50 = cat[np.flipud(np.argsort(dp_dv))][:50 ] > > > top50['_raj2000 ' , ' _ dej2000 ' , ' ktmag ' ] < table masked = true length=50 >   _",
    "raj2000   _",
    "dej2000   ktmag      deg        deg       mag     float64    float64   float32 --------- --------- ------- 344.01190   36.36136    8.772 343.81122   36.67177    9.958 137.19089 -38.60788    9.566 334.86545   29.39581    9.835 359.81589   46.88923    9.307",
    "0.00695   47.27456    9.499        ...        ...      ... 123.16494 -16.05073    9.727 341.26642   33.99616    9.799 339.33075   34.44790    9.204 137.27219 -35.90259   10.822 188.13953 -68.53886    9.609 339.01483   33.97575   10.032 ....",
    "the volume reconstruction algorithm consists of a computationally trivial postprocessing stage that is added to the two established ligo / virgo methods for localization of events , the rapid triangulation code @xcite , and the lalinference parameter estimation pipeline @xcite .",
    "the conditional mean and standard deviation of distance are extracted from as described in ",
    "[ sec : bayestar ] and from lalinference as explained in ",
    "[ sec : lalinference ] below .",
    "then , the mean and standard deviation are converted to the ansatz parameters as described in  [ sec : method - of - moments ] .",
    "@xcite is a rapid position reconstruction algorithm for mergers .",
    "its inputs are a trio of numbers for each detector : the matched - filter estimates of the arrival time , phase , and amplitude at each site .",
    "it marginalizes over polarization angle , inclination angle , coalescence time , and distance by performing low - order gaussian quadrature integration in a series of nested loops .",
    "the output is a all - sky map of posterior probability , consisting of @xmath19 equal - area pixels .",
    "the distance prior is a power law of @xmath20 , with @xmath21 being supplied by the user and normally set to @xmath22 for a spatially homogeneous source population . to evaluate the distances , we run two more times , with @xmath23 and @xmath24 .",
    "the resulting three sky maps are denoted @xmath25 , @xmath26 , and @xmath27 .",
    "the healpix - sampled marginal sky posterior @xmath28 , conditional mean distance @xmath29 , and conditional standard deviation of distance @xmath30 are then given by @xmath31    finally , the moments @xmath29 and @xmath30 are converted to the ansatz parameters @xmath32 , @xmath33 , and @xmath34 using the procedure described in  [ sec : method - of - moments ] below",
    ".    takes about a minute to run @xcite ; the conventional one - dimensional sky map is ready with a response time of a few minutes .",
    "since we will now run three times , the total number of computations will increase by about a factor of 3 .",
    "fortunately , since is able to make effective use of many cpu cores , we can offset the modest increase in computational cost by moving the analysis to a machine with more cores , resulting in a negligible overall change in running time .",
    "lalinference @xcite is the advanced bayesian parameter estimation library .",
    "it includes several algorithms that perform full modeling of the signal and stochastic sampling of the parameter space .",
    "the inputs to lalinference are the time series from all of the detectors .",
    "the output is a cloud of sample points drawn from the posterior .",
    "the samples are converted to a smooth multidimensional probability distribution by clustering them into @xmath35 disjoint sets , each consisting of @xmath36 spatially neighboring points , and building a for each cluster . in cartesian",
    "coordinates , the smoothed distribution is given by a double sum over the clusters and the samples within each cluster : @xmath37.\\end{gathered}\\ ] ] here , @xmath38 is a weight associated with cluster @xmath0 , and each cluster is described by its covariance @xmath39 and @xmath36 samples @xmath40 .",
    "the @xmath41 denotes the matrix determinant .",
    "the stochastic sampling takes hours to weeks depending on the sophistication of the waveform models that are used and on the treatment of uncertainty in detector calibration .",
    "it takes up to tens of minutes to build the .",
    "we can exactly calculate the conditional mean and standard deviation of the distance for the posterior .",
    "first , we evaluate eq .",
    "( [ eq : kde ] ) at the position @xmath42 : @xmath43 , \\intertext{with }      c_i & = ( \\boldsymbol{n } { \\ensuremath{^{^\\mathsf{t}}}}{\\boldsymbol c_i}^{-1 } \\boldsymbol{n})^{-1 } , \\\\      x_{ij } & = ( \\boldsymbol{n } { \\ensuremath{^{^\\mathsf{t}}}}{\\boldsymbol c_i}^{-1 } \\boldsymbol x_{ij } ) c_i , \\text { and } \\\\      w_{ij } & = \\frac{1}{2\\pi } \\sqrt{\\frac{c_i}{\\left| \\boldsymbol c_i\\right| } } \\exp\\left[\\frac{1}{2}\\left(\\frac{{x_{ij}}^2}{c_i } - \\boldsymbol x_{ij}{\\ensuremath{^{^\\mathsf{t}}}}\\boldsymbol{c_i}^{-1}\\boldsymbol x_{ij}\\right)\\right ] w_i.\\end{aligned}\\ ] ] we compute the integrals of 1 , @xmath44 , and @xmath45 , weighted by @xmath46 : @xmath47\\right ) , \\text { and } \\\\     b & = \\sqrt{\\frac{c_i}{2\\pi } } \\exp\\left[\\frac{-{x_{ij}}^2}{2c_i}\\right].\\end{aligned}\\ ] ]    then @xmath25 , @xmath26 , and @xmath27 are converted to @xmath48 , @xmath29 , and @xmath30 using equations  ( [ eq : bayestar - rho])([eq : bayestar - s ] ) .",
    "finally , @xmath49 and @xmath50 are converted to @xmath32 , @xmath33 , and @xmath51 using the procedure described in  [ sec : method - of - moments ] below .      for both and lalinference",
    ", the parameters of the ansatz distribution are extracted using the method of moments .",
    "the ansatz is that the conditional distribution of distance is described by the function @xmath52 r^2 \\\\",
    "\\text{for } r & \\geq 0 .",
    "\\nonumber\\end{aligned}\\ ] ] the @xmath53th moment of the distance ansatz is @xmath54 r^{2+n } \\ , { \\ensuremath{\\mathrm{d}}}r.\\ ] ] the conditional mean and standard deviation of the ansatz distribution are @xmath55 our task is , given the conditional mean @xmath49 and standard deviation @xmath50 as measured from the actual posterior probability distribution , to numerically solve the following system of equations for @xmath32 and @xmath33 : @xmath56    we can reduce this to a single equation by defining @xmath57 and @xmath58 .",
    "with this substitution , the moments can be written as @xmath59 the function @xmath60 is the upper tail of the normal distribution , @xmath61 is the normal distribution function , and @xmath62 is the hazard function .",
    "then equations  ( [ eq : hat - m ] ) and ( [ eq : hat - s ] ) become @xmath63    the derivative of the left - hand side , @xmath64 , is given by @xmath65 with @xmath66    we solve equations  ( [ eq : mom - f ] ) and ( [ eq : mom - fprime ] ) for @xmath67 using steffensen s method , an accelerated newton solver . starting from an initial value of @xmath68",
    ", the solution converges to machine precision in 10 iterations .",
    "finally , we calculate @xmath32 , @xmath33 , and @xmath35 as follows : @xmath69 in the rare event that the solution does not converge , or yields an invalid value such that @xmath70 , we set @xmath71",
    "the ansatz guarantees that the first two moments of distance are exactly reproduced along all .",
    "however , we must ask how accurately the ansatz represents the 3d posterior as a whole . the @xmath72@xmath72 plot graphical test , popularized in the parameter estimation literature by @xcite , compares two populations by plotting their cumulative distributions against each other .",
    "if the two distributions match , then the result should be a diagonal line .    in our case",
    ", we compare the to the lalinference posterior samples by projecting both the and the posterior samples along the distribution s three principal axes , yielding three @xmath72@xmath72 tests . as shown in fig .",
    "[ fig : kde - ansatz - comparison]a , the plot is nearly diagonal , indicating that the is a faithful representation of the posterior samples .",
    "we then compare the with the ansatz by drawing samples from the ansatz distribution ( fig .",
    "[ fig : kde - ansatz - comparison]b ) .",
    "some deviation is perceptible ; in the most extreme cases we find a maximum difference in credible levels of about 5% .",
    "@xmath72@xmath72 tests of the conditional distance distribution itself along individual generally also agree within 5% or better , except in directions of low probability ( small @xmath4 ) .",
    "we test the statistical self - consistency of the entire ensemble of simulated events in fig .",
    "[ fig : pp ] . here",
    ", we show a cumulative histogram of the number of simulated events whose true and coordinates are found within a given credible level .",
    "we find that both the sky maps and the ansatz are self - consistent within a binomial 95% tolerance band due to the finite sample size of 250 events .",
    "our interpretation is that the ansatz is a reasonable approximation of the full 3d posterior , in the sense that a stated 50% credible volume has a 50%@xmath73% chance of containing the source .",
    "the most obvious alternative to the ansatz is a densely sampled 3d grid , or a stack of 2d sky maps for a series of distance shells .",
    "either would be just as conceptually simple , but computationally cumbersome due to size .",
    "the is an accurate representation of the posterior , but is expensive to evaluate because it is a sum of @xmath74 gaussians . as a rapidly available data product and as a tool for real - time observation planning , the ansatz distribution is a reasonable compromise .    [ 2d]two - dimensional [ 2 + 1d]2 + 1dimensional [ 2mrs]2mass redshift survey [ 3d]three - dimensional [ 2mass]two micron all sky survey [ advirgo]advanced virgo [ ami]arcminute microkelvin imager [ agn]active galactic nucleus [ agn]active galactic nuclei [ aligo]advanced [ askap]australian pathfinder [ atca]australia telescope compact array [ atlas]asteroid terrestrial - impact last alert system [ bat]burst alert telescope [ batse]burst and transient source experiment [ bayestar]bayesian triangulation and rapid localization [ bbh]binary black hole [ bhbh] [ bh]black hole [ bns]binary neutron star [ carma]combined array for research in millimeter - wave astronomy [ casa]common astronomy software applications [ cfh12k]canada  france ",
    "hawaii @xmath75 pixel ccd mosaic [ crts]catalina real - time transient survey [ ctio]cerro tololo inter - american observatory [ cbc]compact binary coalescence [ ccd]charge coupled device [ cdf]cumulative distribution function [ cgro]compton gamma ray observatory [ cmb]cosmic microwave background [ crlb]cramr  rao lower bound [ wb]coherent waveburst [ daswg]data analysis software working group [ dbsp]double spectrograph [ dct]discovery channel telescope [ decam]dark energy camera [ des]dark energy survey [ dft]discrete fourier transform [ em]electromagnetic [ er8]eighth engineering run [ fd]frequency domain [ far]false alarm rate [ fft]fast fourier transform [ fir]finite impulse response [ fits]flexible image transport system [ flops]floating point operations per second [ fov]field of view [ fov]fields of view [ ftn]faulkes telescope north [ fwhm]full width at half - maximum [ gbm]gamma - ray burst monitor [ gcn]gamma - ray coordinates network [ gmos]gemini multi - object spectrograph [ grb]gamma - ray burst [ gsc]gas slit camera [ gsl]gnu scientific library [ gtc]gran telescopio canarias [ gw]gravitational wave [ hawc]high - altitude water erenkov gamma - ray observatory [ hct]himalayan chandra telescope [ healp]hierarchical equal area isolatitude pixelization [ heasarc]high energy astrophysics science archive research center [ hete]high energy transient explorer [ hfosc]himalaya faint object spectrograph and camera [ hmxb]high - mass x - ray binary [ hmxb]high - mass x - ray binaries [ hsc]hyper suprime - cam [ iact]imaging atmospheric erenkov telescope [ iir]infinite impulse response [ imacs]inamori - magellan areal camera & spectrograph [ imr]inspiral - merger - ringdown [ ipac]infrared processing and analysis center [ ipn]interplanetary network [ ptf]intermediate [ ism]interstellar medium [ iss]international space station [ kagra]kamioka gravitational - wave observatory [ kde]kernel density estimator [ kn]kilonova [ kne]kilonovae [ lat]large area telescope [ lcogt]las cumbres observatory global telescope [ lho ] hanford observatory [ lib]lalinference burst [ ligo]laser interferometer observatory [ grb]low - luminosity [ lloid]low latency online inspiral detection [ llo ] livingston observatory [ lmi]large monolithic imager [ lofar]low frequency array [ los]line of sight [ loss]lines of sight [ lmc]large magellanic cloud [ lsb]long , soft burst [ lsc ] scientific collaboration [ lso]last stable orbit [ lsst]large synoptic survey telescope [ lt]liverpool telescope [ lti]linear time invariant [ map]maximum a posteriori [ mbta]multi - band template analysis [ mcmc]markov chain monte carlo [ mle ] estimator [ ml]maximum likelihood [ mou]memorandum of understanding [ mous]memoranda of understanding [ mwa]murchison widefield array [ ned]nasa / ipac extragalactic database [ nsbh]neutron star  black hole [ nsbh] [ nsf]national science foundation [ nsns] [ ns]neutron star [ o1 ] s first observing run [ o2 ] s second observing run [ lib]omicron+ [ ot]optical transient [ p48]palomar 48  inch oschin telescope [ p60]robotic palomar 60  inch telescope [ p200]palomar 200  inch hale telescope [ pc]photon counting [ pessto]public eso spectroscopic survey of transient objects [ psd]power spectral density [ psf]point - spread function [ ps1]pan - starrs  1 [ ptf]palomar transient factory [ quest]quasar equatorial survey team [ raptor]rapid telescopes for optical response [ reu]research experiences for undergraduates [ rms]root mean square [ rotse]robotic optical transient search [ s5 ] s fifth science run [ s6 ] s sixth science run [ saa]south atlantic anomaly [ shb]short , hard burst [ shgrb]short , hard [ ska]square kilometer array [ smt]slewing mirror telescope [ s / n]signal - to - noise ratio [ ssc]synchrotron self - compton [ sdss]sloan digital sky survey [ sed]spectral energy distribution [ sgrb]short [ sn]supernova [ sn]supernova [ i]type  ia [ i]type  ic [ i - bl]broad - line type  ic [ i - bl]broad - line type  ic [ svd]singular value decomposition [ tarot]tlescopes  action rapide pour les objets transitoires [ tdoa]time delay on arrival [ tdoa]time delays on arrival [ td]time domain [ toa]time of arrival [ toa]times of arrival [ too]target - of - opportunity [ too]targets of opportunity [ uffo]ultra fast flash observatory [ uhe]ultra high energy [ uvot]uv / optical telescope [ vhe]very high energy [ vista@eso]visible and infrared survey telescope [ vla]karl g. jansky very large array [ vlt]very large telescope [ vst@eso ] survey telescope [ wam]wide - band all - sky monitor [ wcs]world coordinate system [ w.s.s.]wide - sense stationary [ xrf]x - ray flash [ xrf]x - ray flashes [ xrt]x - ray telescope [ ztf]zwicky transient facility    we thank the aspen center for physics and nsf grant # 1066293 for hospitality during the conception , writing , and editing of this paper . we thank p  shawhan and f  tombesi for detailed feedback on the manuscript .",
    "the online data release is available at https://dcc.ligo.org/p1500071/public/html .",
    "this is document p1500071-v7 ."
  ],
  "abstract_text": [
    "<S> this is an online supplement to the letter of @xcite , in which we demonstrated a rapid algorithm for obtaining joint estimates of sky location and luminosity distance from observations of mergers with advanced and virgo . we argued that combining the reconstructed volumes with positions and redshifts of possible host galaxies can provide large - aperture but small instruments with a manageable list of targets to search for optical or infrared emission . in this supplement </S>",
    "<S> , we document the new -based file format for localizations of gravitational - wave transients . </S>",
    "<S> we include python sample code to show the reader how to perform simple manipulations of the sky maps and extract ranked lists of likely host galaxies . </S>",
    "<S> finally , we include mathematical details of the rapid volume reconstruction algorithm .    </S>",
    "<S> = -1 </S>"
  ]
}