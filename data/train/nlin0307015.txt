{
  "article_text": [
    "a complex system , roughly speaking , is one with many parts , whose behaviors are both highly variable and strongly dependent on the behavior of the other parts .",
    "clearly , this includes a large fraction of the universe !",
    "nonetheless , it is not vacuously all - embracing : it excludes both systems whose parts just can not do very much , and those whose parts are really independent of each other .",
    "`` complex systems science '' is the field whose ambition is to understand complex systems .",
    "of course , this is a broad endeavor , overlapping with many even larger , better - established scientific fields .",
    "having been asked by the editors to describe its methods and techniques , i begin by explaining what i feel does _ not _ fall within my charge , as indicated by figure [ figure : quadrangle ] .    at the top of figure",
    "[ figure : quadrangle ] i have put `` patterns '' . by this",
    "i mean more or less what people in software engineering do @xcite : a pattern is a recurring theme in the analysis of many different systems , a cross - systemic regularity .",
    "for instance : bacterial chemotaxis can be thought of as a way of resolving the tension between the exploitation of known resources , and costly exploration for new , potentially more valuable , resources ( figure [ fig : chemotaxis ] ) . this same tension is present in a vast range of adaptive systems . whether the exploration - exploitation trade - off arises among artificial agents , human decision - makers or colonial organisms , many of the issues are the same as in chemotaxis , and solutions and methods of investigation that apply in one case",
    "can profitably be tried in another @xcite . the pattern `` trade - off between exploitation and exploration ''",
    "thus serves to orient us to broad features of novel situations .",
    "there are many other such patterns in complex systems science : `` stability through hierarchically structured interactions '' @xcite , `` positive feedback leading to highly skewed outcomes '' @xcite , `` local inhibition and long - rate activation create spatial patterns '' @xcite , and so forth .    at the bottom of the quadrangle",
    "is `` foundations '' , meaning attempts to build a basic , mathematical science concerned with such topics as the measurement of complexity @xcite , the nature of organization @xcite , the relationship between physical processes and information and computation @xcite and the origins of complexity in nature and its increase ( or decrease ) over time .",
    "there is dispute whether such a science is possible , if so whether it would be profitable .",
    "i think it is both possible and useful , but most of what has been done in this area is very far from being applicable to _ biomedical _ research .",
    "accordingly , i shall pass it over , with the exception of a brief discussion of some work on measuring complexity and organization which is especially closely tied to data analysis .",
    "`` topics '' go in the left - hand corner . here",
    "are what one might call the `` canonical complex systems '' , the particular systems , natural , artificial and fictional , which complex systems science has traditionally and habitually sought to understand . here we find networks ( wuchty , ravasz and barabsi , this volume ) , turbulence @xcite , physio - chemical pattern formation and biological morphogenesis @xcite , genetic algorithms @xcite , evolutionary dynamics @xcite , spin glasses @xcite , neuronal networks ( see part iii , 4 , this book ) , the immune system ( see part iii , 5 , this book ) , social insects , ant - like robotic systems , the evolution of cooperation , evolutionary economics , etc .",
    "these topics all fall within our initial definition of `` complexity '' , though whether they are studied together because of deep connections , or because of historical accidents and tradition , is a difficult question . in any event",
    ", this chapter will _ not _ describe the facts and particular models relevant to these topics .",
    "instead , this chapter is about the right - hand corner , `` tools '' .",
    "some are procedures for analyzing data , some are for constructing and evaluating models , and some are for measuring the complexity of data or models . in this chapter",
    "i will restrict myself to methods which are generally accepted as valid ( if not always widely applied ) , _ and _ seem promising for biomedical research .",
    "these still demand a book , if not an encyclopedia , rather than a mere chapter !",
    "accordingly , i will merely try to convey the essentials of the methods , with pointers to references for details .",
    "the goal is for you to have a sense of which methods would be good things to try on your problem , rather than to tell you everything you need to know to implement them .      as mentioned above ,",
    "the techniques of complex systems science can , for our purposes , be divided into three parts : those for analyzing data ( perhaps without reference to a particular model ) , those for building and understanding models ( often without data ) , and those for measuring complexity as such .",
    "this chapter will examine them in that order .",
    "the first part , on * * data , opens with the general ideas of * * statistical learning and data mining (  [ sec : data - mining ] ) , namely developments in statistics and machine learning theory that extend statistical methods beyond their traditional domain of low - dimensional , independent data .",
    "we then turn to * * time series analysis (  [ sec : time - series ] ) , where there are two important streams of work , inspired by statistics and nonlinear dynamics .",
    "the second part , on * * modeling , considers the most important and distinctive classes of models in complex systems . on the vital area of * * nonlinear dynamics , let the reader consult socoloar ( this volume ) .",
    "* * cellular automata (  [ sec : cas ] ) allow us to represent spatial dynamics in a way which is particularly suited to capturing strong local interactions , spatial heterogeneity , and large - scale aggregate patterns .",
    "complementary to cellular automata are * * agent - based models (  [ sec : abms ] ) , perhaps the most distinctive and most famous kind of model in complex systems science . a general section ( [ sec : evaluation ] ) on * * evaluating complex models , including analytical methods , various sorts of simulation , and testing , closes this part of the chapter .",
    "the third part of the chapter considers ways of measuring complexity . as",
    "a necessary preliminary ,  [ sec : info - theory ] introduces the concepts of * * information theory , with some remarks on its application to biological systems . then ",
    "[ sec : complexity ] treats * * complexity measures , describing the main kinds of complexity measure , their relationships , and their applicability to empirical questions .",
    "the chapter ends with a guide to further reading , organized by section .",
    "these emphasize readable and thorough introductions and surveys over more advanced or historically important contributions .",
    "complex systems , we said , are those with many strongly interdependent parts .",
    "thanks to comparatively recent developments in statistics and machine learning , it is now possible to infer reliable , predictive models from data , even when the data concern thousands of strongly dependent variables . such *",
    "* data mining is now a routine part of many industries , and is increasingly important in research . while not , of course , a substitute for devising valid theoretical models , data mining _ can _ tell us what kinds of patterns are in the data , and so guide our model - building .",
    "the basic goal of any kind of data mining is prediction : some variables , let us call them @xmath0 , are our inputs .",
    "the output is another variable or variables @xmath1 .",
    "we wish to use @xmath0 to predict @xmath1 , or , more exactly , we wish to build a machine which will do the prediction for us : we will put in @xmath0 at one end , and get a prediction for @xmath1 out at the other .",
    "`` prediction '' here covers a lot of ground .",
    "if @xmath1 are simply other variables like @xmath0 , we sometimes call the problem * * regression",
    ". if they are @xmath0 at another time , we have * * forecasting , or prediction in a strict sense of the word .",
    "if @xmath1 indicates membership in some set of discrete categories , we have * * classification .",
    "similarly , our predictions for @xmath1 can take the form of distinct , particular values ( * * point predictions ) , of ranges or intervals we believe @xmath1 will fall into , or of entire probability distributions for @xmath1 , i.e. , guesses as to the conditional distribution @xmath2 .",
    "one can get a point prediction from a distribution by finding its mean or mode , so distribution predictions are in a sense more complete , but they are also more computationally expensive to make , and harder to make successfully .",
    "whatever kind of prediction problem we are attempting , and with whatever kind of guesses we want our machine to make , we must be able to say whether or not they are good guesses ; in fact we must be able to say just how much bad guesses cost us .",
    "that is , we need a * * loss function for predictions .",
    "we suppose that our machine has a number of knobs and dials we can adjust , and we refer to these parameters , collectively , as @xmath3 .",
    "the predictions we make , with inputs @xmath0 and parameters @xmath3 , are @xmath4 , and the loss from the error in these predictions , when the actual outputs are @xmath1 , is @xmath5 . given _",
    "values @xmath6 and @xmath7 , we have the empirical loss @xmath8 , or @xmath9 for short , etc . ) denote random variables , and lower - case ones particular values or realizations  so @xmath0 = the role of a die , whereas @xmath10 ( say ) . ] .",
    "now , a natural impulse at this point is to twist the knobs to make the loss small : that is , to select the @xmath3 which minimizes @xmath9 ; let s write this @xmath11 .",
    "this procedure is sometimes called * * empirical risk minimization , or erm .",
    "( of course , doing that minimization can itself be a tricky nonlinear problem , but i will not cover optimization methods here . )",
    "the problem with erm is that the @xmath12 we get from _ this _ data will almost surely not be the same as the one we d get from the _ next _ set of data .",
    "what we really care about , if we think it through , is not the error on any particular set of data , but the error we can _ expect _ on new data , @xmath13}$ ] .",
    "the former , @xmath9 , is called the * * training or * * in - sample or * * empirical error ; the latter , @xmath13}$ ] , the * * generalization or * * out - of - sample or * * true error .",
    "the difference between in - sample and out - of - sample errors is due to sampling noise , the fact that our data are not _ perfectly _ representative of the system we re studying .",
    "there will be quirks in our data which are just due to chance , but if we minimize @xmath14 blindly , if we try to reproduce every feature of the data , we will be making a machine which reproduces the random quirks , which do not generalize , along with the predictive features .",
    "think of the empirical error @xmath9 as the generalization error , @xmath13}$ ] , plus a sampling fluctuation , @xmath15 .",
    "if we look at machines with low empirical errors , we will pick out ones with low true errors , which is good , but we will also pick out ones with large negative sampling fluctuations , which is not good . even if the sampling noise @xmath15 is very small , @xmath12 can be very different from @xmath16 .",
    "we have what optimization theory calls an * * ill - posed problem @xcite .",
    "having a higher - than - optimal generalization error because we paid too much attention to our data is called * * over - fitting .",
    "just as we are often better off if we tactfully ignore our friends and neighbors little faults , we want to ignore the unrepresentative blemishes of our sample .",
    "much of the theory of data mining is about avoiding over - fitting .",
    "three of the commonest forms of tact it has developed are , in order of sophistication , * * cross - validation , * * regularization ( or * * penalties ) and * * capacity control",
    ".      we would never over - fit if we _",
    "knew _ how well our machine s predictions would generalize to new data . since our data is never perfectly representative , we always have to estimate the generalization performance .",
    "the empirical error provides one estimate , but it s biased towards saying that the machine will do well ( since we built it to do well on that data ) . if we had a second , independent set of data , we could evaluate our machine s predictions on it , and that would give us an unbiased estimate of its generalization .",
    "one way to do this is to take our original data and divide it , at random , into two parts , the * * training set and the * * test set or * * validation set .",
    "we then use the training set to fit the machine , and evaluate its performance on the test set .",
    "( this is an instance of * * resampling our data , which is a useful trick in many contexts . ) because we ve made sure the test set is independent of the training set , we get an unbiased estimate of the out - of - sample performance .    in * * cross - validation , we divide our data into random training and test sets many different ways , fit a different machine for each training set , and compare their performances on their test sets , taking the one with the best test - set performance .",
    "this re - introduces some bias  it could happen by chance that one test set reproduces the sampling quirks of its training set , favoring the model fit to the latter .",
    "but cross - validation generally _ reduces _ over - fitting , compared to simply minimizing the empirical error ; it makes more _ efficient _ use of the data , though it can not get rid of sampling noise altogether .",
    "i said that the problem of minimizing the error is * * ill - posed , meaning that small changes in the errors can lead to big changes in the optimal parameters . a standard approach to ill - posed problems in optimization theory",
    "is called * * regularization . rather than trying to minimize @xmath9 alone ,",
    "we minimize @xmath17 where @xmath18 is a * * regularizing or * * penalty function .",
    "remember that @xmath19 } + \\epsilon$ ] , where @xmath15 is the sampling noise . if the penalty term is well - designed , then the @xmath3 which minimizes @xmath20 } + \\epsilon + \\lambda d(\\theta ) \\label{eqn : regularization - incl - noise}\\ ] ] will be close to the @xmath3 which minimizes @xmath13}$ ]",
    " it will cancel out the effects of favorable fluctuations .",
    "as we acquire more and more data , @xmath21 , so @xmath22 , too , goes to zero at an appropriate pace , the penalized solution will converge on the machine with the best possible generalization error .",
    "how then should we design penalty functions ?",
    "the more knobs and dials there are on our machine , the more opportunities we have to get into mischief by matching chance quirks in the data .",
    "if one machine with fifty knobs , and another fits the data just as well but has only a single knob , we should ( the story goes ) chose the latter  because it s _ less _ flexible , the fact that it does well is a good indication that it will still do well in the future .",
    "there are thus many regularization methods which add a penalty proportional to the number of knobs , or , more formally , the number of parameters .",
    "these include the akaike information criterion or aic @xcite and the bayesian information criterion or bic @xcite .",
    "other methods penalized the `` roughness '' of a model , i.e. , some measure of how much the prediction shifts with a small change in either the input or the parameters ( * ? ? ?",
    "a smooth function is less flexible , and so has less ability to match meaningless wiggles in the data .",
    "another popular penalty method , the * * minimum description length principle of rissanen , will be dealt with in  [ sec : stat - compl ] below .",
    "usually , regularization methods are justified by the idea that models can be more or less complex , and more complex ones are more liable to over - fit , all else being equal , so penalty terms should reflect complexity ( figure [ fig : overfit ] ) .",
    "there s something to this idea , but the usual way of putting it does not really work ; see  [ sec : razor ] below .",
    "empirical risk minimization , we said , is apt to over - fit because we do not know the generalization errors , just the empirical errors .",
    "this would not be such a problem if we could _ guarantee _ that the in - sample performance was close to the out - of - sample performance .",
    "even if the exact machine we got this way was not particularly close to the optimal machine , we d then be guaranteed that our _ predictions _ were nearly optimal .",
    "we do not even need to guarantee that _ all _ the empirical errors are close to their true values , just that the _ smallest _ empirical error is close to the smallest generalization error .",
    "recall that @xmath19 } + \\epsilon$ ] .",
    "it is natural to assume that as our sample size @xmath23 becomes larger , our sampling error @xmath15 will approach zero .",
    "( we will return to this assumption below . )",
    "suppose we could find a function @xmath24 to bound our sampling error , such that @xmath25 . then we could guarantee that our choice of model was * * approximately correct ; if we wanted to be sure that our prediction errors were within @xmath15 of the best possible , we would merely need to have @xmath26 data - points .",
    "it should not be surprising to learn that we can not , generally , make approximately correct guarantees . as the eminent forensic statistician c. chan remarked , `` improbable events permit themselves the luxury of occurring '' @xcite , and one of these indulgences could make the discrepancy between @xmath9 and @xmath13}$ ] very large .",
    "but if something like the law of large numbers holds , or the ergodic theorem (  [ sec : time - series - properties ] ) , then for every choice of @xmath3 , @xmath27}\\right| >",
    "\\epsilon\\right ) & \\rightarrow & 0 ~,\\end{aligned}\\ ] ] for every positive @xmath15 . to its mean value . for a practical introduction to such convergence properties , the necessary and sufficient conditions for them to obtain , and some thoughts about what one can do , statistically , when they do not , see @xcite . ]",
    "we should be able to find some function @xmath28 such that @xmath27}\\right| > \\epsilon\\right ) & \\leq & \\delta(n,\\epsilon,\\theta ) ~,\\end{aligned}\\ ] ] with @xmath29 .",
    "then , for any particular @xmath3 , we could give * * probably approximately correct @xcite guarantees , and say that , e.g. , to have a 95% confidence that the true error is within 0.001 of the empirical error requires at least 144,000 samples ( or whatever the precise numbers may be ) .",
    "if we can give probably approximately correct ( pac ) guarantees on the performance of one machine , we can give them for any _ finite _ collection of machines .",
    "but if we have infinitely many possible machines , might not there always be _ some _ of them which are misbehaving ?",
    "can we still give pac guarantees when @xmath3 is continuous ?    the answer to this question depends on how flexible the set of machines is  its * * capacity .",
    "we need to know how easy it is to find a @xmath3 such that @xmath4 will accommodate itself to any @xmath1 .",
    "this is measured by a quantity called the vapnik - chervonenkis ( vc ) dimension @xcite .",
    "if the vc dimension @xmath30 of a class of machines is finite , one can make a pac guarantee which applies to _ all _ machines in the class simultaneously : @xmath31}\\right| } \\geq \\eta(n , d,\\delta)\\right ) & \\leq & \\delta \\label{vc - pac}\\end{aligned}\\ ] ] where the function @xmath32 expresses the rate of convergence .",
    "it depends on the particular kind of loss function involved .",
    "for example , for binary classification , if the loss function is the fraction of inputs mis - classified , @xmath33 notice that @xmath3 is not an argument to @xmath34 , and does not appear in ( [ classification - bound ] ) .",
    "the rate of convergence is the same across all machines ; this kind of result is thus called a * * uniform law of large numbers .",
    "the really remarkable thing about ( [ vc - pac ] ) is that it holds no matter what the sampling distribution is , so long as samples are independent ; it is a * * distribution - free result .",
    "the vc bounds lead to a very nice learning scheme : simply apply empirical risk minimization , for a fixed class of machines , and then give a pac guarantee that the one picked is , with high reliability , very close to the actual optimal machine .",
    "the vc bounds also lead an appealing penalization scheme , where the penalty is equal to our bound on the over - fitting , @xmath34 . specifically , we set the @xmath35 term in ( [ eqn : regularization ] ) equal to the @xmath34 in ( [ vc - pac ] ) , ensuring , with high probability , that the @xmath15 and @xmath35 terms in ( [ eqn : regularization - incl - noise ] ) cancel each other .",
    "this is * * structural risk minimization ( srm ) .",
    "it s important to realize that the vc dimension is not the same as the number of parameters . for some classes of functions , it is much _ lower _ than the number of parameters , and for others it s much _",
    "( there are examples of one - parameter classes of functions with infinite vc dimension . )",
    "determining the vc dimension often involves subtle combinatorial arguments , but many results are now available in the literature , and more are appearing all the time .",
    "there are even schemes for experimentally estimating the vc dimension @xcite .",
    "two caveats are in order .",
    "first , because the vc bounds are distribution - free , they are really about the rate of convergence under the worst possible distribution , the one a malicious adversary out to foil our data mining would choose .",
    "this means that in practice , convergence is often much faster than ( [ vc - pac ] ) would indicate .",
    "second , the usual proofs of the vc bounds all assume independent , identically - distributed samples , though the relationship between @xmath0 and @xmath1 can involve arbitrarily complicated dependencies . recently , there has been much progress in proving uniform laws of large numbers for dependent sequences of samples , and structural risk minimization has been extended to what are called `` mixing '' processes @xcite , in effect including an extra term in the @xmath36 function appearing in ( [ vc - pac ] ) which discounts the number of observations by their degree of mutual dependence .      the basic idea of data mining is to fit a model to data with minimal assumptions about what the correct model should be , or how the variables in the data are related .",
    "( this differs from such classical statistical questions as testing _",
    "specific _ hypotheses about specific models , such as the presence of interactions between certain variables . )",
    "this is facilitated by the development of extremely flexible classes of models , which are sometimes , misleadingly , called * * non - parametric ; a better name would be * * megaparametric .",
    "the idea behind megaparametric models is that they should be capable of approximating any function , at least any well - behaved function , to any desired accuracy , given enough capacity .",
    "the polynomials are a familiar example of a class of functions which can perform such universal approximation . given any smooth function @xmath37 ,",
    "we can represent it by taking the taylor series around our favorite point @xmath38 .",
    "truncating that series gives an approximation to @xmath37 : @xmath39 in fact , if @xmath37 is an @xmath40 order polynomial , the truncated series is exact , not an approximation .    to see why this is",
    "_ not _ a reason to use only polynomial models , think about what would happen if @xmath41 .",
    "we would need an _",
    "infinite _ order polynomial to completely represent @xmath37 , and the generalization properties of finite - order approximations would generally be lousy : for one thing , @xmath37 is bounded between -1 and 1 everywhere , but any finite - order polynomial will start to zoom off to @xmath42 or @xmath43 outside some range . of course , this @xmath37 would be really easy to approximate as a superposition of sines and cosines , which is another class of functions which is capable of universal approximation ( better known , perhaps , as fourier analysis ) .",
    "what one wants , naturally , is to chose a model class which gives a good approximation of the function at hand , _ at low order_. we want low order functions , both because computational demands rise with model order , _ and _ because higher order models are more prone to over - fitting ( vc dimension generally rises with model order ) .    to adequately describe all of the _ common _ model classes , or * * model architectures , used in the data mining literature would require another chapter .",
    "( @xcite and @xcite are good for this . )",
    "instead , i will merely name a few .    * * * splines are piecewise polynomials , good for regression on bounded domains ; there is a very elegant theory for their estimation @xcite . * * * neural networks or * * multilayer perceptrons have a devoted following , both for regression and classification @xcite .",
    "the application of vc theory to them is quite well - advanced @xcite , but there are many other approaches , including ones based on statistical mechanics @xcite .",
    "it is notoriously hard to understand _ why _ they make the predictions they do . * * * classification and regression trees ( cart ) , introduced in the book of that name @xcite , recursively sub - divide the input space , rather like the game of `` twenty questions '' ( `` is the temperature above 20 centigrade ?",
    "if so , is the glucose concentration above one millimole ? '' , etc . )",
    "; each question is a branch of the tree .",
    "all the cases at the end of one branch of the tree are treated equivalently .",
    "the resulting decision trees are easy to understand , and often similar to human decision heuristics @xcite . * * * kernel machines @xcite apply nonlinear transformations to the input , mapping it to a much higher dimensional `` feature space '' , where they apply linear prediction methods .",
    "the trick works because the vc dimension of linear methods is low , even in high - dimensional spaces .",
    "kernel methods come in many flavors , of which the most popular , currently , are * * support vector machines @xcite .",
    "predictive and descriptive models both are not necessarily causal .",
    "pac - type results give us reliable prediction , _ assuming _ future data will come from the _ same _ distribution as the past . in a causal model , however , we want to know how _ changes _ will propagate through the system .",
    "one difficulty is that these relationships are one - way , whereas prediction is two - way ( one can predict genetic variants from metabolic rates , but one can not change genes by changing metabolism ) . the other is that it is hard ( if not impossible ) to tell if the predictive relationships we have found are * * confounded by the influence of other variables and other relationships we have neglected . despite these difficulties , the subject of * * causal inference from data is currently a very active area of research , and many methods have been proposed , generally under assumptions about the absence of feedback @xcite .",
    "when we have a causal or generative model , we can use very well - established techniques to infer the values of the hidden or latent variables in the model from the values of their observed effects @xcite .",
    "often , regularization methods are thought to be penalizing the _ complexity _ of the model , and so implementing some version of occam s razor .",
    "just as occam said `` entities are not to be multiplied beyond necessity '' , we say `` parameters should not be multiplied beyond necessity '' , or , `` the model should be no rougher than necessary '' .",
    "this takes complexity to be a property of an _ individual _ model , and the hope is that a simple model which can predict the training data will also be able to predict new data .",
    "now , under many circumstances , one can prove that , as the size of the sample approaches infinity , regularization will converge on the correct model , the one with the best generalization performance @xcite .",
    "but one can often prove exactly the same thing about erm without any regularization or penalization at all ; this is what the vc bounds ( [ vc - pac ] ) accomplish . while regularization methods often do well in practice , so , too , does straight erm .",
    "if we compare the performance of regularization methods to straight empirical error minimization on artificial examples , where we can calculate the generalization performance exactly , regularization conveys _ no clear advantage at all _ @xcite .",
    "contrast this with what happens in structural risk minimization .",
    "there our complexity penalty depends solely on the vc dimension of the _ class _ of models we re using . a simple , inflexible model which we find only because we re looking at a complex , flexible class is penalized just as much as the most wiggly member of that class . experimentally ,",
    "srm _ does _ work better than simple erm , or than traditional penalization methods .",
    "a simple example may help illuminate why this is so .",
    "suppose we re interested in binary classification , and we find a machine @xmath3 which correctly classifies a million independent data points .",
    "if the real error rate (= generalization error ) for @xmath3 was one in a hundred thousand , the chance that it would correctly classify a million data points would be @xmath44 .",
    "if @xmath3 was the very first parameter setting we checked , we could be quite confident that its true error rate was much less than @xmath45 , no matter how complicated the function @xmath4 looked .",
    "but if we ve looked at ten million parameter settings before finding @xmath3 , then the odds are quite good that , among the machines with an error rate of @xmath45 , we d find several which correctly classify all the points in the training set , so the fact that @xmath3 does is not good evidence that it s the best machine . what matters is not how much algebra is involved in making the predictions once we ve chosen @xmath3 , but how many alternatives to @xmath3 we ve tried out and rejected .",
    "the vc dimension lets us apply this kind of reasoning rigorously and without needing to know the details of the process by which we generate and evaluate models .",
    "the upshot is that the kind of complexity which matters for learning , and so for occam s razor , is the complexity of _ classes of models _ , not of individual models nor of the system being modeled .",
    "it is important to keep this point in mind when we try to measure the complexity of systems (  [ sec : complexity ] ) .",
    "complex systems scientists often regard the field of statistics as irrelevant to understanding such systems .",
    "this is understandable , since the exposure most scientists have to statistics ( e.g. , the `` research methods '' courses traditional in the life and social sciences ) typically deal with systems with only a few variables and with explicit assumptions of independence , or only very weak dependence . the kind of modern methods we have just seen , amenable to large systems and strong dependence , are rarely taught in such courses , or even mentioned . considering the shaky grasp many students have on even the basic principles of statistical inference ,",
    "this is perhaps wise .",
    "still , it leads to even quite eminent researchers in complexity making disparaging remarks about statistics ( e.g. , `` statistical hypothesis testing , that substitute for thought '' ) , while actually re - inventing tools and concepts which have long been familiar to statisticians .    for their part , many statisticians tend to overlook the very _ existence _ of complex systems science as a separate discipline .",
    "one may hope that the increasing interest from both fields on topics such as bioinformatics and networks will lead to greater mutual appreciation .",
    "there are two main schools of time series analysis . the older one , which has a long pedigree in applied statistics @xcite , and is prevalent among statisticians , social scientists ( especially econometricians ) and engineers .",
    "the younger school , developed essentially since the 1970s , comes out of physics and nonlinear dynamics .",
    "the first views time series as samples from a stochastic process , and applies a mixture of traditional statistical tools and assumptions ( linear regression , the properties of gaussian distributions ) and the analysis of the fourier spectrum .",
    "the second school views time series as distorted or noisy measurements of an underlying dynamical system , which it aims to reconstruct .",
    "the separation between the two schools is in part due to the fact that , when statistical methods for time series analysis were first being formalized , in the 1920s and 1930s , dynamical systems theory was literally just beginning .",
    "the real development of nonlinear dynamics into a powerful discipline has mostly taken place since the 1960s , by which point the statistical theory had acquired a research agenda with a lot of momentum . in turn ,",
    "many of the physicists involved in experimental nonlinear dynamics in the 1980s and early 1990s were fairly cavalier about statistical issues , and some happily reported results which should have been left in their file - drawers .    there are welcome signs , however , that the two streams of thought are coalescing .",
    "since the 1960s , statisticians have increasingly come to realize the virtues of what they call `` state - space models '' , which are just what the physicists have in mind with their dynamical systems .",
    "the physicists , in turn , have become more sensitive to statistical issues , and there is even now some cross - disciplinary work . in this section ,",
    "i will try , so far as possible , to use the state - space idea as a common framework to present both sets of methods .      a vector - valued function of time , @xmath46 , the * * state . in discrete time",
    ", this evolves according to some map , @xmath47 where the map @xmath48 is allowed to depend on time @xmath49 and a sequence of independent random variables @xmath50 . in continuous time",
    ", we do not specify the evolution of the state directly , but rather the rates of change of the components of the state , @xmath51 since our data are generally taken in discrete time , i will restrict myself to considering that case from now on ; almost everything carries over to continuous time naturally .",
    "the evolution of @xmath7 is so to speak , self - contained , or more precisely markovian : all the information needed to determine the future is contained in the _ present _ state @xmath46 , and earlier states are irrelevant .",
    "( this is basically how physicists _ define _ `` state '' @xcite . ) indeed , it is often reasonable to assume that @xmath48 is independent of time , so that the dynamics are * * autonomous ( in the terminology of dynamics ) or * * homogeneous ( in that of statistics ) .",
    "if we could look at the series of states , then , we would find it had many properties which made it very convenient to analyze .    sadly , however , we do not observe the state @xmath7 ; what we observe or measure is @xmath6 , which is generally a noisy , nonlinear function of the state : @xmath52 , where @xmath53 is measurement noise .",
    "whether @xmath6 , too , has the convenient properties depends on @xmath54 , and usually @xmath6 is _ not _ convenient .",
    "matters are made more complicated by the fact that we do not , in typical cases , know the observation function @xmath54 , nor the state - dynamics @xmath48 , nor even , really , what space @xmath7 lives in .",
    "the goal of time - series methods is to make educated guess about all these things , so as to better predict and understand the evolution of temporal data .    in the ideal case , simply from a knowledge of @xmath6 , we would be able to identify the state space , the dynamics , and the observation function . as a matter of pure mathematical possibility",
    ", this can be done for essentially arbitrary time - series @xcite .",
    "nobody , however , knows how to do this with complete generality in practice .",
    "rather , one makes certain assumptions about , say , the state space , which are strong enough that the remaining details can be filled in using @xmath6 .",
    "then one checks the result for accuracy and plausibility , i.e. , for the kinds of errors which would result from breaking those assumptions @xcite .",
    "subsequent parts of this section describe classes of such methods .",
    "first , however , i describe some of the general properties of time series , and general measurements which can be made upon them .",
    "[ [ notation ] ] notation + + + + + + + +    there is no completely uniform notation for time - series . since it will be convenient to refer to sequences of consecutive values .",
    "i will write all the measurements starting at @xmath55 and ending at @xmath49 as @xmath56 .",
    "further , i will abbreviate the set of all measurements up to time @xmath49 , @xmath57 , as @xmath58 , and the future starting from @xmath49 , @xmath59 , as @xmath60",
    ".      one of the most commonly assumed properties of a time - series is * * stationarity , which comes in two forms : * * strong or * * strict stationarity , and * * weak , * * wide - sense or _ second - order _ stationarity .",
    "strong stationarity is the property that the probability distribution of sequences of observations does not change over time .",
    "that is , @xmath61 for all lengths of time @xmath54 and all shifts forwards or backwards in time @xmath62 . when a series is described as `` stationary '' without qualification , it depends on context whether strong or weak stationarity is meant .",
    "weak stationarity , on the other hand , is the property that the first and second moments of the distribution do not change over time .",
    "@xmath63 } & = & { \\mathbf{e}\\left[y_{t+\\tau}\\right]}\\\\ { \\mathbf{e}\\left[y_t y_{t+h}\\right ] } & = & { \\mathbf{e}\\left[y_{t+\\tau } y_{t+\\tau+h}\\right]}\\end{aligned}\\ ] ] if @xmath1 is a gaussian process , then the two senses of stationarity are equivalent .",
    "note that both sorts of stationarity are statements about the true distribution , and so can not be simply read off from measurements .",
    "strong stationarity implies a property called * * ergodicity , which is much more generally applicable . roughly speaking ,",
    "a series is ergodic if any sufficiently long sample is representative of the entire process .",
    "more exactly , consider the * * time - average of a well - behaved function @xmath37 of @xmath1 , @xmath64 this is generally a random quantity , since it depends on where the trajectory started at @xmath65 , and any random motion which may have taken place between then and @xmath66 .",
    "its distribution generally depends on the precise values of @xmath65 and @xmath66 .",
    "the series @xmath1 is ergodic if almost all time - averages converge eventually , i.e. , if @xmath67 for some constant @xmath68 independent of the starting time @xmath49 , the starting point @xmath69 , or the trajectory @xmath70 . *",
    "* ergodic theorems specify conditions under which ergodicity holds ; surprisingly , even completely deterministic dynamical systems can be ergodic .",
    "ergodicity is such an important property because it means that statistical methods are very directly applicable . simply by waiting long enough",
    ", one can obtain an estimate of any desired property which will be closely representative of the future of the process . statistical inference _ is _ possible for non - ergodic processes , but it is considerably more difficult , and often requires multiple time - series @xcite .",
    "one of the most basic means of studying a time series is to compute the * * autocorrelation function ( acf ) , which measures the linear dependence between the values of the series at different points in time .",
    "this starts with the * * autocovariance function : @xmath71}\\right)\\left(y_t - { \\mathbf{e}\\left[y_t\\right]}\\right)\\right ] } ~.\\end{aligned}\\ ] ] ( statistical physicists , unlike anyone else , call _ this _ the `` correlation function '' . )",
    "the autocorrelation itself is the autocovariance , normalized by the variability of the series : @xmath72 @xmath73 is @xmath74 when @xmath75 is a linear function of @xmath76 .",
    "note that the definition is symmetric , so @xmath77 . for stationary or weakly - stationary processes",
    ", one can show that @xmath73 depends only on the difference between @xmath62 @xmath49 and @xmath55 .",
    "in this case one just writes @xmath78 , with one argument .",
    "@xmath79 , always .",
    "the time @xmath80 such that @xmath81 is called the * * ( auto)correlation time of the series .",
    "the correlation function is a * * time - domain property , since it is basically about the series considered as a sequence of values at distinct times .",
    "there are also * * frequency - domain properties , which depend on re - expressing the series as a sum of sines and cosines with definite frequencies . a function of time @xmath6 has a fourier transform which is a function of frequency , @xmath82 .",
    "@xmath83 assuming the time series runs from @xmath84 to @xmath85 .",
    "( rather than separating out the sine and cosine terms , it is easier to use the complex - number representation , via @xmath86 . )",
    "the inverse fourier transform recovers the original function : @xmath87 the fourier transform is a linear operator , in the sense that @xmath88 .",
    "moreover , it represents series we are interested in as a sum of trigonometric functions , which are themselves solutions to linear differential equations .",
    "these facts lead to extremely powerful frequency - domain techniques for studying linear systems . of course",
    ", the fourier transform is always _ valid _ , whether the system concerned is linear or not , and it may well be useful , though that is not guaranteed .    the squared absolute value of the fourier transform , @xmath89 , is called the * * spectral density or * * power spectrum . for stationary processes ,",
    "the power spectrum @xmath90 is the fourier transform of the autocovariance function @xmath91 ( a result called the wiener - khinchin theorem ) .",
    "an important consequence is that a gaussian process is completely specified by its power spectrum . in particular , consider a sequence of independent gaussian variables , each with variance @xmath92 . because they are perfectly uncorrelated , @xmath93 , and @xmath94 for any @xmath95 .",
    "the fourier transform of such a @xmath91 is just @xmath96 , independent of @xmath97  every frequency has just as much power .",
    "because white light has equal power in every color of the spectrum , such a process is called * * white noise .",
    "correlated processes , with uneven power spectra , are sometimes called * * colored noise , and there is an elaborate terminology of red , pink , brown , etc .",
    "noises ( * ? ? ?",
    "* ch .  3 ) .",
    "the easiest way to estimate the power spectrum is simply to take the fourier transform of the time series , using , e.g. , the fast fourier transform algorithm @xcite .",
    "equivalently , one might calculate the autocovariance and fourier transform that .",
    "either way , one has an estimate of the spectrum which is called the * * periodogram .",
    "it is unbiased , in that the expected value of the periodogram at a given frequency is the true power at that frequency .",
    "unfortunately , it is not consistent  the variance around the true value does not shrink as the series grows . the easiest way to overcome",
    "this is to apply any of several well - known smoothing functions to the periodogram , a procedure called * * windowing @xcite .",
    "( standard software packages will accomplish this automatically . )",
    "the fourier transform takes the original series and decomposes it into a sum of sines and cosines .",
    "this is possible because _ any _ reasonable function can be represented in this way .",
    "the trigonometric functions are thus a * * basis for the space of functions .",
    "there are many other possible bases , and one can equally well perform the same kind of decomposition in any other basis . the trigonometric basis is particularly useful for stationary time series because the basis functions are themselves evenly spread over all times ( * ? ? ?",
    "* ch .  2 ) .",
    "other bases , localized in time , are more convenient for non - stationary situations . the most well - known of these alternate bases , currently , are wavelets @xcite , but there is , literally , no counting the other possibilities .      the traditional statistical approach to time series is to represent them through linear models of the kind familiar from applied statistics .",
    "the most basic kind of model is that of a * * moving average , which is especially appropriate if @xmath7 is highly correlated up to some lag , say @xmath98 , after which the acf decays rapidly .",
    "the moving average model represents @xmath7 as the result of smoothing @xmath99 independent random variables .",
    "specifically , the ma(@xmath98 ) model of a weakly stationary series is @xmath100 where @xmath101 is the mean of @xmath6 , the @xmath102 are constants and the @xmath103 are white noise variables .",
    "@xmath98 is called the * * order of the model .",
    "note that there is no direct dependence between successive values of @xmath6 ; they are all functions of the white noise series @xmath104 .",
    "note also that @xmath76 and @xmath105 are completely independent ; after @xmath98 time - steps , the effects of what happened at time @xmath49 disappear .",
    "another basic model is that of an * * autoregressive process , where the next value of @xmath6 is a linear combination of the preceding values of @xmath6 . specifically , an ar(@xmath106 ) model is @xmath107 where @xmath108 are constants and @xmath109 .",
    "the order of the model , again is @xmath106 .",
    "this is the multiple regression of applied statistics transposed directly on to time series , and is surprisingly effective . here , unlike the moving average case",
    ", effects propagate indefinitely  changing @xmath76 can affect all subsequent values of @xmath6 .",
    "the remote past only becomes irrelevant if one controls for the last @xmath106 values of the series .",
    "if the noise term @xmath103 were absent , an ar(@xmath106 ) model would be a @xmath110 order linear difference equation , the solution to which would be some combination of exponential growth , exponential decay and harmonic oscillation . with noise , they become oscillators under stochastic forcing @xcite .",
    "the natural combination of the two types of model is the * * autoregressive moving average model ,",
    "arma(@xmath111 ) : @xmath112 this combines the oscillations of the ar models with the correlated driving noise of the ma models .",
    "an ar(@xmath106 ) model is the same as an arma(@xmath113 ) model , and likewise an ma(@xmath98 ) model is an arma(@xmath114 ) model .",
    "it is convenient , at this point in our exposition , to introduce the notion of the * * back - shift operator @xmath115 , @xmath116 and the * * ar and ma polynomials , @xmath117 respectively .",
    "then , formally speaking , in an arma process is @xmath118 the advantage of doing this is that one can determine many properties of an arma process by algebra on the polynomials .",
    "for instance , two important properties we want a model to have are * * invertibility and * * causality .",
    "we say that the model is invertible if the sequence of noise variables @xmath103 can be determined uniquely from the observations @xmath76 ; in this case we can write it as an ma(@xmath42 ) model .",
    "this is possible just when @xmath119 has no roots inside the unit circle .",
    "similarly , we say the model is causal if it can be written as an ar(@xmath42 ) model , without reference to any _ future _ values .",
    "when this is true , @xmath120 also has no roots inside the unit circle .",
    "if we have a causal , invertible arma model , with known parameters , we can work out the sequence of noise terms , or * * innovations @xmath103 associated with our measured values @xmath76 . then",
    ", if we want to forecast what happens past the end of our series , we can simply extrapolate forward , getting predictions @xmath121 , etc .",
    "conversely , if we knew the innovation sequence , we could determine the parameters @xmath122 and @xmath3 . when both are unknown , as is the case when we want to fit a model , we need to determine them jointly @xcite . in particular , a common procedure is to work forward through the data , trying to predict the value at each time on the basis of the past of the series ; the sum of the squared differences between these predicted values @xmath123 and the actual ones @xmath76 forms the empirical loss : @xmath124 for this loss function , in particular , there are very fast standard algorithms , and the estimates of @xmath122 and @xmath3 converge on their true values , provided one has the right model order .",
    "this leads naturally to the question of how one determines the order of arma model to use , i.e. , how one picks @xmath106 and @xmath98 .",
    "this is precisely a model selection task , as discussed in ",
    "[ sec : data - mining ] .",
    "all methods described there are potentially applicable ; cross - validation and regularization are more commonly used than capacity control .",
    "many software packages will easily implement selection according to the aic , for instance .",
    "the power spectrum of an arma(@xmath111 ) process can be given in closed form : @xmath125 thus , the parameters of an arma process can be estimated directly from the power spectrum , if you have a reliable estimate of the spectrum .",
    "conversely , different hypotheses about the parameters can be checked from spectral data .",
    "all arma models are weakly stationary ; to apply them to non - stationary data one must transform the data so as to make it stationary .",
    "a common transformation is * * differencing , i.e. , applying operations of the form @xmath126 which tends to eliminate regular trends . in terms of the back - shift operator , @xmath127 and higher - order differences",
    "are @xmath128 having differenced the data to our satisfaction , say @xmath30 times , we then fit an arma model to it .",
    "the result is an * * autoregressive integrated moving average model , arima(@xmath129 ) @xcite , given by @xmath130    as mentioned above (  [ sec : state - space - picture ] ) , arma and arima models can be re - cast in state space terms , so that our @xmath6 is a noisy measurement of a hidden @xmath7 @xcite . for these models ,",
    "both the dynamics and the observation functions are linear , that is , @xmath131 and @xmath132 , for some matrices @xmath133 and @xmath134 .",
    "the matrices can be determined from the @xmath3 and @xmath122 parameters , though the relation is a bit too involved to give here .",
    "it is often possible to describe a nonlinear dynamical system through an effective linear statistical model , provided the nonlinearities are cooperative enough to appear as noise @xcite .",
    "it is an under - appreciated fact that this is at least sometimes true even of turbulent flows @xcite ; the generality of such an approach is not known .",
    "certainly , if you care only about predicting a time series , and not about its structure , it is always a good idea to try a linear model first , even if you _ know _ that the real dynamics are highly nonlinear .",
    "while standard linear models are more flexible than one might think , they do have their limits , and recognition of this has spurred work on many extensions and variants . here",
    "i briefly discuss a few of these .",
    "[ [ long - memory ] ] long memory + + + + + + + + + + +    the correlations of standard arma and arima models decay fairly rapidly , in general exponentially ; @xmath135 , where @xmath136 is the correlation time . for some series",
    ", however , @xmath136 is effectively infinite , and @xmath137 for some exponent @xmath138 .",
    "these are called * * long - memory processes , because they remain substantially correlated over very long times .",
    "these can still be accommodated within the arima framework , formally , by introducing the idea of _ fractional _ differencing , or , in continuous time , fractional derivatives @xcite . often long - memory processes are self - similar , which can simplify their statistical estimation @xcite .    [ [ volatility ] ] volatility + + + + + + + + + +    all arma and even arima models assume constant variance",
    ". if the variance is itself variable , it can be worthwhile to model it . * * autoregressive conditionally heteroscedastic ( arch ) models assume a fixed mean value for @xmath139 , but a variance which is an auto - regression on @xmath140 . *",
    "* generalized arch ( garch ) models expand the regression to include the ( unobserved ) earlier variances .",
    "arch and garch models are especially suitable for processes which display * * clustered volatility , periods of extreme fluctuation separated by stretches of comparative calm .",
    "[ [ nonlinear - and - nonparametric - models ] ] nonlinear and nonparametric models + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    nonlinear models are obviously appealing , and when a particular parametric form of model is available , reasonably straight - forward modifications of the linear machinery can be used to fit , evaluate and forecast the model @xcite .",
    "however , it is often impractical to settle on a good parametric form beforehand . in these cases",
    ", one must turn to nonparametric models , as discussed in ",
    "[ sec : architectures ] ; neural networks are a particular favorite here @xcite .",
    "the so - called * * kernel smoothing methods are also particularly well - developed for time series , and often perform almost as well as parametric models @xcite .",
    "finally , information theory provides * * universal prediction methods , which promise to asymptotically approach the best possible prediction , starting from exactly no background knowledge .",
    "this power is paid for by demanding a long initial training phase used to infer the structure of the process , when predictions are much worse than many other methods could deliver @xcite .",
    "the younger approach to the analysis of time series comes from nonlinear dynamics , and is intimately bound up with the state - space approach described in  [ sec : state - space - picture ] above .",
    "the idea is that the dynamics on the state space can be determined _ directly _ from observations , at least if certain conditions are met .",
    "the central result here is the takens embedding theorem @xcite ; a simplified , slightly inaccurate version is as follows .",
    "suppose the @xmath30-dimensional state vector @xmath46 evolves according to an unknown but continuous and ( crucially ) deterministic dynamic .",
    "suppose , too , that the one - dimensional observable @xmath6 is a smooth function of @xmath7 , and `` coupled '' to all the components of @xmath7 .",
    "now at any time we can look not just at the present measurement @xmath141 , but also at observations made at times removed from us by multiples of some lag @xmath62 : @xmath142 , @xmath143 , etc .",
    "if we use @xmath144 lags , we have a @xmath144-dimensional vector .",
    "one might expect that , as the number of lags is increased , the motion in the lagged space will become more and more predictable , and perhaps in the limit @xmath145 would become deterministic .",
    "in fact , the dynamics of the lagged vectors become deterministic at a finite dimension ; not only that , but the deterministic dynamics are completely equivalent to those of the original state space ! ( more exactly , they are related by a smooth , invertible change of coordinates , or * * diffeomorphism . ) the magic * * embedding dimension @xmath144 is at most @xmath146 , and often less .    given an appropriate reconstruction via embedding , one can investigate many aspects of the dynamics . because the reconstructed space is related to the original state space by a smooth change of coordinates ,",
    "any geometric property which survives such treatment is the same for both spaces .",
    "these include the dimension of the attractor , the lyapunov exponents ( which measure the degree of sensitivity to initial conditions ) and certain qualitative properties of the autocorrelation function and power spectrum ( `` correlation dimension '' ) .",
    "also preserved is the relation of `` closeness '' among trajectories  two trajectories which are close in the state space will be close in the embedding space , and vice versa .",
    "this leads to a popular and robust scheme for nonlinear prediction , the * * method of analogs : when one wants to predict the next step of the series , take the current point in the embedding space , find a similar one with a known successor , and predict that the current point will do the analogous thing .",
    "many refinements are possible , such as taking a weighted average of nearest neighbors , or selecting an analog at random , with a probability decreasing rapidly with distance .",
    "alternately , one can simply fit non - parametric predictors on the embedding space .",
    "( see @xcite for a review . )",
    "closely related is the idea of * * noise reduction , using the structure of the embedding - space to filter out some of the effects of measurement noise .",
    "this can work even when the statistical character of the noise is unknown ( see @xcite again ) .",
    "determining the number of lags , and the lag itself , is a problem of model selection , just as in  [ sec : data - mining ] , and can be approached in that spirit .",
    "an obvious approach is to minimize the in - sample forecasting error , as with arma models ; recent work along these lines @xcite uses the minimum description length principle ( described in  [ sec : mdl ] below ) to control over - fitting . a more common procedure for determining the embedding dimension , however , is the * * false nearest neighbor method @xcite .",
    "the idea is that if the current embedding dimension @xmath144 is sufficient to resolve the dynamics , @xmath147 would be too , and the reconstructed state space will not change very much . in particular , points which were close together in the dimension-@xmath144 embedding should remain close in the dimension-@xmath147 embedding .",
    "conversely , if the embedding dimension is too small , points which are really far apart will be brought artificially close together ( just as projecting a sphere on to a disk brings together points on the opposite side of a sphere ) .",
    "the particular algorithm of kennel et al . , which has proved very practical , is to take each point in the @xmath144-dimensional embedding , find its nearest neighbor in that embedding , and then calculate the distance between them .",
    "one then calculates how much further apart they would be if one used a @xmath147-dimensional embedding .",
    "if this extra distance is more than a certain fixed multiple of the original distance , they are said to be `` false nearest neighbors '' .",
    "( ratios of 2 to 15 are common , but the precise value does not seem to matter very much . )",
    "one then repeats the process at dimension @xmath147 , stopping when the proportion of false nearest neighbors becomes zero , or at any rate sufficiently small . here ,",
    "the loss function used to guide model selection is the number of false nearest neighbors , and the standard prescriptions amount to empirical risk minimization .",
    "one reason simple erm works well here is that the problem is intrinsically finite - dimensional ( via the takens result ) .",
    "unfortunately , the data required for calculations of quantities like dimensions and exponents to be reliable can be quite voluminous .",
    "approximately @xmath148 data - points are necessary to adequately reconstruct an attractor of dimension @xmath149 @xcite .",
    "( even this is more optimistic than the widely - quoted , if apparently pessimistic , calculation of @xcite , that attractor reconstruction with an _ embedding _ dimension of @xmath144 needs @xmath150 data - points ! ) in the early days of the application of embedding methods to experimental data , these limitations were not well appreciated , leading to many calculations of low - dimensional deterministic chaos in eeg and ekg series , economic time series , etc . , which did not stand up to further scrutiny .",
    "this in turn brought some discredit on the methods themselves , which was not really fair .",
    "more positively , it also led to the development of ideas such as * * surrogate - data methods .",
    "suppose you have found what seems like a good embedding , and it appears that your series was produced by an underlying deterministic attractor of dimension @xmath149 .",
    "one way to test this hypothesis would be to see what kind of results your embedding method would give if applied to similar but _ non_-deterministic data . concretely",
    ", you find a stochastic model with similar statistical properties ( e.g. , an arma model with the same power spectrum ) , and simulate many time - series from this model .",
    "you apply your embedding method to each of these * * surrogate data series , getting the approximate distribution of apparent `` attractor '' dimensions when there really is no attractor .",
    "if the dimension measured from the original data is not significantly different from what one would expect under this null hypothesis , the evidence for an attractor ( at least from this source ) is weak . to apply surrogate data tests well , one must be very careful in constructing the null model , as it is easy to use over - simple null models , biasing the test towards apparent determinism .",
    "a few further cautions on embedding methods are in order . while _ in principle _ any lag @xmath62 is suitable , in practice very long or very short lags both lead to pathologies .",
    "a common practice is to set the lag to the autocorrelation time ( see above ) , or the first minimum of the mutual information function ( see  [ sec : info - theory ] below ) , the notion being that this most nearly achieves a genuinely `` new '' measurement @xcite .",
    "there is some evidence that the mutual information method works better @xcite . again , while in principle almost any smooth observation function will do , given enough data , in practice some make it much easier to reconstruct the dynamics ; several * * indices of observability try to quantify this @xcite .",
    "finally , it strictly applies only to deterministic observations of deterministic systems .",
    "embedding approaches are reasonably robust to a degree of noise in the observations",
    ". they do not cope at all well , however , to noise in the dynamics itself . to anthropomorphize a little , when confronted by apparent non - determinism ,",
    "they respond by adding more dimensions , and so distinguishing apparently similar cases .",
    "thus , when confronted with data which really are stochastic , they will infer an infinite number of dimensions , which is correct in a way , but definitely not helpful .",
    "these remarks should not be taken to belittle the very real power of nonlinear dynamics methods .",
    "applied skillfully , they are powerful tools for understanding the behavior of complex systems , especially for probing aspects of their structure which are not directly accessible .",
    "suppose we have a state - space model for our time series , and some observations @xmath6 , can we find the state @xmath7 ?",
    "this is the problem of * * filtering or * * state estimation .",
    "clearly , it is not the same as the problem of finding a model in the first place , but it is closely related , and also a problem in statistical inference .    in this context , a * * filter is a function which provides an estimate @xmath151 of @xmath46 on the basis of observations up to and including values as well ; this is a * * non - causal or * * smoothing filter .",
    "this is clearly not suitable for estimating the state in real time , but often gives more accurate estimates when it is applicable .",
    "the discussion in the text generally applies to smoothing filters , at some cost in extra notation . ]",
    "time @xmath49 : @xmath152 .",
    "a filter is * * recursive if it estimates the state at @xmath49 on the basis of its estimate at @xmath153 and the new observation : @xmath154 .",
    "recursive filters are especially suited to on - line use , since one does not need to retain the complete sequence of previous observations , merely the most recent estimate of the state . as with prediction in general ,",
    "filters can be designed to provide either point estimates of the state , or distributional estimates .",
    "ideally , in the latter case , we would get the conditional distribution , @xmath155 , and in the former case the conditional expectation , @xmath156",
    ".    given the frequency with which the problem of state estimation shows up in different disciplines , and its general importance when it does appear , much thought has been devoted to it over many years .",
    "the problem of optimal _ linear _",
    "filters for stationary processes was solved independently by two of the `` grandfathers '' of complex systems science , norbert wiener and a. n. kolmogorov , during the second world war @xcite . in the 1960s ,",
    "kalman and bucy @xcite solved the problem of optimal recursive filtering , assuming linear dynamics , linear observations and additive noise . in the resulting * * kalman filter ,",
    "the new estimate of the state is a weighted combination of the old state , extrapolated forward , and the state which would be inferred from the new observation alone . the requirement of linear dynamics can be relaxed slightly with what s called the `` extended kalman filter '' , essentially by linearizing the dynamics around the current estimated state .",
    "nonlinear solutions go back to pioneering work of stratonovich @xcite and kushner @xcite in the later 1960s , who gave optimal , recursive solutions .",
    "unlike the wiener or kalman filters , which give point estimates , the stratonovich - kushner approach calculates the complete conditional distribution of the state ; point estimates take the form of the mean or the most probable state @xcite . in most circumstances ,",
    "the strictly optimal filter is hopelessly impractical numerically .",
    "modern developments , however , have opened up some very important lines of approach to practical nonlinear filters @xcite , including approaches which exploit the geometry of the nonlinear dynamics @xcite , as well as more mundane methods which yield tractable numerical approximations to the optimal filters @xcite .",
    "noise reduction methods (  [ sec : nld - approach ] ) and hidden markov models (  [ sec : symbolic - dyn ] ) can also be regarded as nonlinear filters .",
    "the methods we have considered so far are intended for time - series taking continuous values .",
    "an alternative is to break the range of the time - series into discrete categories ( generally only finitely many of them ) ; these categories are sometimes called * * symbols , and the study of these time - series * * symbolic dynamics .",
    "modeling and prediction then reduces to a ( perhaps more tractable ) problem in discrete probability , and many methods can be used which are simply inapplicable to continuous - valued series @xcite . of course , if a bad discretization is chosen , the results of such methods are pretty well meaningless , but sometimes one gets data which is already nicely discrete  human languages , the sequences of bio - polymers , neuronal spike trains , etc .",
    "we shall return to the issue of discretization below , but for the moment , we will simply consider the applicable methods for discrete - valued , discrete - time series , however obtained .",
    "formally , we take a continuous variable @xmath157 and * * partition its range into a number of discrete * * cells , each labeled by a different symbol from some * * alphabet ; the partition gives us a discrete variable @xmath158 .",
    "a * * word or * * string is just a sequence of symbols , @xmath159 .",
    "a time series @xmath160 naturally generates a string @xmath161 . in general ,",
    "not every possible string can actually be generated by the dynamics of the system we re considering .",
    "the set of allowed sequences is called the * * language . a sequence which is never generated",
    "is said to be * * forbidden . in a slightly inconsistent metaphor , the rules which specify the allowed words of a language are called its * * grammar . to each grammar there",
    "corresponds an abstract machine or * * automaton which can determine whether a given word belongs to the language , or , equivalently , generate all and only the allowed words of the language .",
    "the generative versions of these automata are stochastic , i.e. , they generate different words with different probabilities , matching the statistics of @xmath120 .    by imposing restrictions on the forms",
    "the grammatical rules can take , or , equivalently , on the memory available to the automaton , we can divide all languages into four nested classes , a hierarchical classification due to chomsky @xcite . at the bottom",
    "are the members of the weakest , most restricted class , the * * regular languages generated by automata within only a fixed , finite memory for past symbols ( * * finite state machines ) . above them are the * * context free languages , whose grammars do not depend on context ; the corresponding machines are * * stack automata , which can store an unlimited number of symbols in their memory , but on a strictly first - in , first - out basis",
    ". then come the * * context - sensitive languages ; and at the very top , the unrestricted languages , generated by universal computers .",
    "each stage in the hierarchy can simulate all those beneath it .",
    "we may seem to have departed very far from dynamics , but actually this is not so .",
    "because different languages classes are distinguished by different kinds of memories , they have very different correlation properties (  [ sec : time - series - properties ] ) , mutual information functions (  [ sec : info - theory ] ) , and so forth  see @xcite for details .",
    "moreover , it is often easier to determine these properties from a system s grammar than from direct examination of sequence statistics , especially since specialized techniques are available for grammatical inference @xcite .",
    "the most important special case of this general picture is that of regular languages .",
    "these , we said , are generated by machines with only a finite memory .",
    "more exactly , there is a finite set of states @xmath7 , with two properties :    1 .",
    "the distribution of @xmath76 depends solely on @xmath46 , and 2 .",
    "the distribution of @xmath162 depends solely on @xmath46 .",
    "that is , the @xmath7 sequence is a markov chain , and the observed @xmath6 sequence is noisy function of that chain .",
    "such models are very familiar in signal processing @xcite , bioinformatics @xcite and elsewhere , under the name of * * hidden markov models ( hmms ) .",
    "they can be thought of as a generalization of ordinary markov chains to the state - space picture described in  [ sec : state - space - picture ] .",
    "hmms are particularly useful in filtering applications , since very efficient algorithms exist for determining the most probable values of @xmath7 from the observed sequence @xmath6 .",
    "the * * expectation - maximization ( em ) algorithm @xcite even allows us to simultaneously infer the most probable hidden states and the most probable parameters for the model .",
    "the main limitation of ordinary hmms methods , even the em algorithm , is that they assume a fixed * * architecture for the states , and a fixed relationship between the states and the observations .",
    "that is to say , they are not geared towards inferring the structure of the model .",
    "one could apply the model - selection techniques of  [ sec : data - mining ] , but methods of direct inference have also been developed .",
    "a popular one relies on * * variable - length markov models , also called * * context trees or * * probabilistic suffix trees @xcite .",
    "a suffix here is the string at the end of the @xmath6 time series at a given time , so e.g. the binary series @xmath163 has suffixes @xmath164 , @xmath165 , @xmath166 , @xmath167 , etc .",
    ", but not @xmath168 .",
    "a suffix is a * * context if the future of the series is independent of its past , given the suffix .",
    "context - tree algorithms try to identify contexts by iteratively considering longer and longer suffixes , until they find one which seems to be a context . for instance",
    ", in a binary series , such an algorithm would first try whether the suffices @xmath169 and @xmath164 are contexts , i.e. , whether the conditional distribution @xmath170 can be distinguished from @xmath171 , and likewise for @xmath172 .",
    "it could happen that @xmath169 is a context but @xmath164 is not , in which case the algorithm will try @xmath173 and @xmath165 , and so on .",
    "if one sets @xmath46 equal to the context at time @xmath49 , @xmath46 is a markov chain .",
    "this is called a _",
    "variable - length _ markov model because the contexts can be of different lengths .",
    "once a set of contexts has been found , they can be used for prediction .",
    "each context corresponds to a different distribution for one - step - ahead predictions , and so one just needs to find the context of the current time series .",
    "one could apply state - estimation techniques to find the context , but an easier solution is to use the construction process of the contexts to build a decision tree (  [ sec : architectures ] ) , where the first level looks at @xmath69 , the second at @xmath174 , and so forth .",
    "variable - length markov models are conceptually simple , flexible , fast , and frequently more accurate than other ways of approaching the symbolic dynamics of experimental systems @xcite .",
    "however , not every regular language can be represented by a finite number of contexts .",
    "this weakness can be remedied by moving to a more powerful class of models , discussed next .      in discussing the state - space picture in  [ sec : state - space - picture ] above",
    ", we saw that the state of a system is basically defined by specifying its future time - evolution , to the extent that it can be specified .",
    "viewed in this way , a state @xmath175 corresponds to a distribution over future observables @xmath176 .",
    "one natural way of finding such distributions is to look at the _ conditional _ distribution of the future observations , given the previous history , i.e. , @xmath177 . for a given stochastic process or dynamical system",
    ", there will be a certain characteristic family of such conditional distributions .",
    "one can then consider the distribution - valued process generated by the original , observed process .",
    "it turns out that the former has is always a markov process , and that the original process can be expressed as a function of this markov process plus noise .",
    "in fact , the distribution - valued process has all the properties one would want of a state - space model of the observations .",
    "the conditional distributions , then , can be treated as states .",
    "this remarkable fact has lead to techniques for modeling discrete - valued time series , all of which attempt to capture the conditional - distribution states , and all of which are strictly more powerful than vlmms .",
    "there are at least three : the * * causal - state models or * * causal - state machines ( csms ) , and so called the resulting models `` @xmath15-machines '' .",
    "this name is unfortunate : that is usually a bad way of discretizing data (  [ sec : generating - partitions ] ) , the quantity @xmath15 plays no role in the actual theory , and the name is more than usually impenetrable to outsiders . while i have used it extensively myself , it should probably be avoided . ] introduced by crutchfield and young @xcite , the * * observable operator models ( ooms ) introduced by jaeger @xcite , and the * * predictive state representations ( psrs ) introduced by littman , sutton and singh @xcite .",
    "the simplest way of thinking of such objects is that they are vlmms where a context or state can contain more than one suffix , adding expressive power and allowing them to give compact representations of a wider range of processes .",
    "( see @xcite for more on this point , with examples . )",
    "all three techniques  csms , ooms and psrs  are basically equivalent , though they differ in their formalisms and their emphases .",
    "csms focus on representing states as classes of histories with the same conditional distributions , i.e. , as suffixes sharing a single context .",
    "( they also feature in the `` statistical forecasting '' approach to measuring complexity , discussed in  [ sec : comp - mech ] below . )",
    "ooms are named after the operators which update the state ; there is one such operator for each possible observation .",
    "psrs , finally , emphasize the fact that one does not actually need to know the probability of every possible string of future observations , but just a restricted sub - set of key trajectories , called `` tests '' . in point of fact , all of them can be regarded as special cases of more general prior constructions due to salmon ( `` statistical relevance basis '' ) @xcite and knight ( `` measure - theoretic prediction process '' ) @xcite , which were themselves independent .",
    "( this area of the literature is more than usually tangled . )    efficient * * reconstruction algorithms or * * discovery procedures exist for building csms @xcite and ooms @xcite directly from data .",
    "( there is currently no such discovery procedure for psrs , though there are parameter - estimation algorithms @xcite . )",
    "these algorithms are reliable , in the sense that , given enough data , the probability that they build the wrong set of states becomes arbitrarily small .",
    "experimentally , selecting an hmm architecture through cross - validation never does better than reconstruction , and often much worse @xcite .",
    "while these models are more powerful than vlmms , there are still many stochastic processes which can not be represented in this form ; or , rather , their representation requires an infinite number of states @xcite .",
    "this is mathematically unproblematic , though reconstruction will then become much harder .",
    "( for technical reasons , it seems likely to be easier to carry through for ooms or psrs than for csms . ) in fact , one can show that these techniques would work straight - forwardly on continuous - valued , continuous - time processes , if only we knew the necessary conditional distributions @xcite .",
    "devising a reconstruction algorithm suitable for this setting is an extremely challenging and completely unsolved problem ; even parameter estimation is difficult , and currently only possible under quite restrictive assumptions @xcite .      so far , everything has assumed that we are either observing truly discrete quantities , or that we have a fixed discretization of our continuous observations . in the latter case , it is natural to wonder how much difference the discretization makes .",
    "the answer , it turns out , is _ quite a lot _ ;",
    "changing the partition can lead to completely different symbolic dynamics @xcite .",
    "how then might we choose a _ good _ partition ?",
    "nonlinear dynamics provides an answer , at least for deterministic systems , in the idea of a * * generating partition @xcite .",
    "suppose we have a continuous state @xmath7 and a deterministic map on the state @xmath48 , as in ",
    "[ sec : state - space - picture ] . under a partitioning @xmath122 ,",
    "each point @xmath7 in the state space will generate an infinite sequence of symbols , @xmath178 , as follows : @xmath179 .",
    "the partition @xmath122 is generating if each point @xmath7 corresponds to a _",
    "unique _ symbol sequence , i.e. , if @xmath180 is invertible .",
    "thus , no information is lost in going from the continuous state to the discrete symbol sequence ) of the symbol sequences : a generating partition is one which maximizes the entropy rate , which is the same as maximizing the extra information about the initial condition @xmath7 provided by each symbol of the sequence @xmath178 . ] . while one must know the continuous map @xmath48 to determine exact generating partitions , there are reasonable algorithms for approximating them from data , particularly in combination with embedding methods @xcite .",
    "when the underlying dynamics are stochastic , however , the situation is much more complicated @xcite .",
    "* * cellular automata are one of the more popular and distinctive classes of models of complex systems",
    ". originally introduced by von neumann as a way of studying the possibility of mechanical self - reproduction , they have established niches for themselves in foundational questions relating physics to computation in statistical mechanics , fluid dynamics , and pattern formation . within that",
    "last , perhaps the most relevant to the present purpose , they have been extensively and successfully applied to physical and chemical pattern formation , and , somewhat more speculatively , to biological development and to ecological dynamics .",
    "interesting attempts to apply them to questions like the development of cities and regional economies lie outside the scope of this chapter .",
    "take a board , and divide it up into squares , like a chess - board or checker - board .",
    "these are the cells .",
    "each cell has one of a finite number of distinct colors  red and black , say , or ( to be patriotic ) red , white and blue .",
    "( we do not allow continuous shading , and every cell has just one color . )",
    "now we come to the `` automaton '' part .",
    "sitting somewhere to one side of the board is a clock , and every time the clock ticks the colors of the cells change .",
    "each cell looks at the colors of the nearby cells , and its own color , and then applies a definite rule , the * * transition rule , specified in advance , to decide its color in the next clock - tick ; and all the cells change at the same time .",
    "( the rule can say `` stay the same . '' ) each cell is a sort of very stupid computer  in the jargon , a * * finite - state automaton  and so the whole board is called a * * cellular automaton , or ca .",
    "to run it , you color the cells in your favorite pattern , start the clock , and stand back .",
    "let us follow this concrete picture with one more technical and abstract .",
    "the cells do not have to be colored , of course ; all that s important is that each cell is in one of a finite number of states at any given time . by custom",
    "they re written as the integers , starting from 0 , but any `` finite alphabet '' will do . usually the number of states is small , under ten , but in principle any finite number is allowed .",
    "what counts as the `` nearby cells '' , the * * neighborhood , varies from automaton to automaton ; sometimes just the four cells on the principle directions , sometimes the corner cells , sometimes a block or diamond of larger size ; in principle any arbitrary shape .",
    "you do not need to stick to a chess - board ; you can use any regular pattern of cells which will fill the plane ( or `` tessellate '' it ; an old name for cellular automata is * * tessellation structures ) .",
    "and you do not have to stick to the plane ; any number of dimensions is allowed .",
    "there are various tricks for handling the edges of the space ; the one which has `` all the advantages of theft over honest toil '' is to assume an infinite board .",
    "[ [ cellular - automata - as - parallel - computers ] ] cellular automata as parallel computers + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    ca are synchronous massively parallel computers , with each cell being a finite state transducer , taking input from its neighbors and making its own state available as output . from this perspective , the remarkable thing about ca is that they are computationally universal , able to calculate any ( classically ) computable function ; one can use finite - state machines , the least powerful kind of computer , to build devices equivalent to turing machines , the most powerful kind of computer .",
    "the computational power of different physically - motivated ca is an important topic in complex systems @xcite , though it must be confessed that ca with very different computational powers can have very similar behavior in most respects .",
    "[ [ cellular - automata - as - discrete - field - theories ] ] cellular automata as discrete field theories + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    from the perspective of physics , a ca is a `` digitized '' classical field theory , in which space , time and the field ( state ) are all discrete . thus fluid mechanics , continuum mechanics , and electromagnetism can all be simulated by ca typically , however , the physical relevance of a ca comes not from accurately simulating some field theory at the microscopic level , but from the large - scale phenomena they generate .",
    "take , for example , simulating fluid mechanics , where ca are also called * * lattice gases or * * lattice fluids . in the `` hpp '' @xcite rule , a typical lattice gas with a square grid ,",
    "there are four species of `` fluid particle '' , which travel along the four principal directions . if two cells moving in opposite directions try to occupy the same location at the same time , they collide , and move off at right angles to their original axis ( figure [ fig : lattice - gas ] ) .",
    "each cell thus contains only an integer number of particles , and only a discrete number of values of momentum are possible .",
    "if one takes averages over reasonably large regions , however , then density and momentum approximately obey the equations of continuous fluid mechanics .",
    "numerical experiments show that this rule reproduces many fluid phenomena , such as diffusion , sound , shock - waves , etc .",
    "however , with this rule , the agreement with fluid mechanics is only approximate .",
    "in particular , the square lattice makes the large - scale dynamics anisotropic , which is unphysical .",
    "this in turn can be overcome in several ways  for instance , by using a hexagonal lattice @xcite .",
    "the principle here  get the key parts of the small - scale `` microphysics '' right , and the interesting `` macrophysics '' will take care of itself  is extensively applied in studying pattern formation , including such biologically - relevant phenomena as phase separation @xcite , excitable media @xcite , and the self - assembly of micelles @xcite .",
    "if there is any _ one _ technique associated with complex systems science , it is agent - based modeling .",
    "an agent - based model is a computational model which represents individual agents and their collective behavior .",
    "what , exactly , do we mean by `` agent '' ?",
    "stuart kauffman has offered the following apt definition : `` an agent is a thing which does things to things '' .",
    "that is , an agent is a persistent thing which has some _ state _ we find worth representing , and which interacts with other agents , mutually modifying each others states .",
    "the components of an agent - based model are a collection of agents and their states , the rules governing the interactions of the agents , and the environment within which they live .",
    "( the environment need not be represented in the model if its effects are constant . )",
    "the state of an agent can be arbitrarily simple , say just position , or the color of a cell in a ca .",
    "( at this end , agent - based models blend with traditional stochastic models . )",
    "states can also be extremely complicated , including , possibly , sophisticated internal models of the agent s world .    here is an example to make this concrete . in epidemiology , there is a classic kind of model of the spread of a disease through a population called an `` sir '' model @xcite .",
    "it has three classes of people  the susceptible , who have yet to be exposed to the disease ; the infected , who have it and can pass it on ; and the resistant or recovered , who have survived the disease and can not be re - infected . a traditional approach to an sir model would have three variables , namely the number of people in each of the three categories , @xmath181 , and would have some deterministic or stochastic dynamics in terms of those variables .",
    "for instance , in a deterministic sir model , one might have @xmath182i(t)\\\\ \\label{eqn : sir - r } r(t+1 ) - r(t ) & = & bi(t)\\end{aligned}\\ ] ] which we could interpret by saying that ( i ) the probability of a susceptible person being infected is proportional to the fraction of the population which is already infected , ( ii ) infected people get better at a rate @xmath164 and ( iii ) infected people die at a rate @xmath183 .",
    "( this is not a particularly _ realistic _ sir model . ) in a stochastic model , we would treat the right hand sides of ( [ eqn : sir - s])([eqn : sir - r ] ) as the mean changes in the three variables , with ( say ) poisson - distributed fluctuations , taking care that , e.g. , the fluctuation in the @xmath184 term in ( [ eqn : sir - s ] ) is the same as that in ( [ eqn : sir - i ] ) .",
    "the thing to note is that , whether deterministic or stochastic , the whole model is cast in terms of the aggregate quantities @xmath185 , @xmath186 and @xmath187 , and those aggregate variables are what we would represent computationally .    in an agent - based model of the same dynamics , we would represent _ each _ individual in the population as a distinct agent , which could be in one of three states , s , i and r. a simple interaction rule would be that at each time - step , each agent selects another from the population entirely at random .",
    "if a susceptible agent ( i.e. , one in state s ) picks an infectious agent ( i.e. , one in state i ) , it becomes infected with probability @xmath169 .",
    "infectious agents die with probability @xmath164 and recover with probability @xmath183 ; recovered agents never change their state .",
    "so far , we have merely reproduced the stochastic version of ( [ eqn : sir - s])([eqn : sir - r ] ) , while using many more variables .",
    "the power of agent - based modeling only reveals itself when we implement more interesting interaction rules .",
    "for instance , it would be easy to assign each agent a position , and make two agents more likely to interact if they are close .",
    "we could add visible symptoms which are imperfectly associated with the disease , and a tendency not to interact with symptomatic individuals .",
    "we could make the degree of aversion to symptomatic agents part of the agents state .",
    "all of this is easy to implement in the model , even in combination , but _ not _ easy to do in a more traditional , aggregated model .",
    "sometimes it would be all but impossible ; an excellent case in point is the highly sophisticated model of hiv epidemiology produced by jacquez , koopman , simon and collaborators @xcite , incorporating multiple routes of transmission , highly non - random mixing of types , and time - varying infectiousness .",
    "agent - based models steer you towards representing individuals , their behaviors and their interactions , rather than aggregates and their dynamics . whether this is a good thing depends , of course , on what you know , and what you hope to learn .",
    "if you know a lot about individuals , agent - based models can help you leverage that knowledge into information about collective dynamics .",
    "this is particularly helpful if the population is heterogeneous , since you can represent the different types of individuals in the population by different states for agents .",
    "this requires a bit of effort on your part , but often not nearly so much as it would to represent the heterogeneity in an aggregated model .",
    "conversely , if you think you have the collective dynamics down , an abm will let you check whether a candidate for an individual - level mechanism really will produce them . ( but see  [ sec : evaluation ] , below . )",
    "ideally , there are no `` mass nouns '' in an abm , nothing represented by a smeared - out `` how much '' : everything should be represented by some definite number of distinctly - located agents . at most , some aggregate variables may be stuffed into the environment part of the model , but only simple and homogeneous ones . of course , the _ level _ of disaggregation at which it is useful to call something an agent is a matter for particular applications , and need not be the same for every agent in a model .",
    "( e.g. , one might want to model an entire organ as a single agent , while another , more interesting organ is broken up into multiple interacting agents , along anatomical or functional lines . )",
    "sometimes it s just not practical to represent everything which we know is an individual thing by its own agent : imagine trying to do chemical thermodynamics by tracking the interactions of a mole of molecules .",
    "such cases demand either giving up on agent - based modeling ( fortunately , the law of mass action works pretty well in chemistry ) , or using fictitious agents that represent substantial , but not too large , collections of individuals .",
    "models describing the collective dynamics of aggregate variables are sometimes called `` equation - based models '' , in contrast to agent - based models .",
    "this is sloppy , however : it is always possible , though generally tedious and unilluminating , to write down a set of equations which describe the dynamics of an agent - based model . rather than drawing a false contrast between agents and equations",
    ", it would be better to compare abms to `` aggregate models '' , `` collective models '' or perhaps `` factor models '' .",
    "the nicest way to computationally implement the commitment of distinctly representing each agent , is to make agents * * objects , which are , to over - simplify slightly , data structures which have internal states , and interact with each other by passing messages . while objects are not necessary for agent - based models , they do make programming them _ much _ easier , especially if the agents have much more state than , say , just a position and a type . if you try to implement models with sophisticated agents without using objects , the odds are good that you will find yourself re - inventing well - known features of object - oriented programming .",
    "( historically , object - oriented programming _ began _ with languages for simulation modeling @xcite . )",
    "you might as well save your time , and do those things _ right _",
    ", by using objects in the first place",
    ".    generally speaking , computational implementations of abms contain many non - agent objects , engaged in various housekeeping tasks , or implementing the functions agents are supposed to perform .",
    "for instance , an agent , say a rat , might be supposed to memorize a sequence , say of turns in a maze .",
    "one way of implementing this would be to use a linked list , which is an object itself .",
    "such objects do not represent actual features of the _ model _ , and it should be possible to vary them without interfering with the model s behavior . which objects are picked out as agents is to some degree a matter of convenience and taste .",
    "it is common , for instance , to have mobile agents interacting on a static environment .",
    "if the environment is an object , modelers may or may not speak of it as an `` environment agent , '' and little seems to hinge on whether or not they do .",
    "there are several programming environments designed to facilitate agent - based modeling .",
    "perhaps the best known of these is swarm ( www.swarm.org ) , which works very flexibly with several languages , is extensively documented , and has a large user community , though it presently ( 2004 ) lacks an institutional home .",
    "repast , while conceptually similar , is open - source ( repast.sourceforge.net ) and is associated with the university of chicago .",
    "starlogo , and it successor , netlogo ( ccl.sesp.northwestern.edu/netlogo ) , are extensions of the popular logo language to handle multiple interacting `` turtles '' , i.e. , agents . like logo",
    ", children can learn to use them @xcite , but they are fairly easy for adults , too , and certainly give a feel for working with abms .      not everything which involves the word `` agent '' is connected to agent - based modeling .",
    "* * representative agent models are not abms . in these models , the response of a population to environment conditions is found by picking out a _ single _ typical or representative agent , determining their behavior , and assuming that everyone else does likewise .",
    "this is sometimes reasonable , but it s clearly diametrically opposed to what an abm is supposed to be .    *",
    "* software agents are not abms .",
    "software agents are a very useful and rapidly developing technology @xcite ; an agent , here , is roughly a piece of code which interacts with other software and with pieces of the real world autonomously .",
    "agents index the web for search engines , engage in automated trading , and help manage parts of the north american electrical power grid , among other things .",
    "some agent software systems are _ inspired _ by abms @xcite .",
    "when one wants to model their behavior , an abm is a natural tool ( but not the only one by any means : see @xcite ) .",
    "but a set of software agents running the michigan power grid is not a _ model _ of anything , it s _ doing _ something .",
    "finally , * * multi - agent systems @xcite and * * rational agents @xcite in artificial intelligence are not abms .",
    "the interest of this work is in understanding , and especially _ designing _ , systems capable of sophisticated , autonomous cognitive behavior ; many people in this field would restrict the word `` agent '' to apply only to things capable , in some sense , of having `` beliefs , desires and intentions '' .",
    "while these are certainly complex systems , they are not usually intended to be _ models _ of anything else .",
    "one can , of course , press them into service as models @xcite , but generally this will be no more than a heuristic device .",
    "one striking feature of agent - based models , and indeed of complex systems models in general , is how _ simple _ they are .",
    "often , agents have only a few possible states , and only a handful of kinds of interaction .",
    "this practice has three motivations .",
    "( i ) a model as detailed as the system being studied would be as hard to understand as that system .",
    "( ii ) many people working in complex systems science want to show that a certain set of mechanisms are sufficient to generate some phenomenon , like cooperation among unrelated organisms , or the formation of striped patterns . hence using simple models , which contain only those mechanisms ,",
    "makes the case .",
    "( iii ) statistical physicists , in particular , have a long tradition of using highly simplified models as caricatures of real systems .",
    "all three motives are appropriate , in their place .",
    "( i ) is completely unexceptionable ; abstracting away from irrelevant detail is always worthwhile , so long as it really is irrelevant .",
    "( ii ) is also fair enough , though one should be careful that the mechanisms in one s model can still generate the phenomenon when they interact with _ other _ effects as well .",
    "( iii ) works very nicely in statistical physics itself , where there are powerful mathematical results relating to the renormalization group @xcite and bifurcation theory @xcite which allow one to extract certain kinds of _ quantitative _ results from simplified models which share certain _ qualitative _ characteristics with real systems .",
    "( we have seen a related principle when discussing cellular automata models above . )",
    "there is , however , little reason to think that these universality results apply to most complex systems , let alone ones with adaptive agents !",
    "we do not build models for their own sake ; we want to see what they do , and we want to compare what they do both to reality and to other models . this kind of evaluation of models is a problem for all areas of science , and as such little useful general advice can be given . however , there are some issues which are peculiar to models of complex systems , or especially acute for them , and i will try to provide some guidance here , moving from figuring out just what your model does , to comparing your model to data , to comparing it to other models .",
    "the most basic way to see what your model does is to run it ; to do a simulation . even though a model is entirely a human construct ,",
    "every aspect of its behavior following logically from its premises and initial conditions , the frailty of human nature is such that we generally can not perceive those consequences , not with any accuracy .",
    "if the model involves a large number of components which interact strongly with each other  if , that is to say , it s a good model of a complex system  our powers of deduction are generally overwhelmed by the mass of relevant , interconnected detail .",
    "computer simulation then comes to our aid , because computers have no trouble remembering large quantities of detail , nor in following instructions .",
    "direct simulation  simply starting the model and letting it go  has two main uses .",
    "one is to get a sense of the typical behavior , or of the range of behavior . the other , more quantitative , use is to determine the distribution of important quantities , including time series .",
    "if one randomizes initial conditions , and collects data over multiple runs , one can estimate the distribution of desired quantities with great accuracy .",
    "this is exploited in the time series method of surrogate data ( above ) , but the idea applies quite generally .",
    "individual simulation runs for models of complex systems can be reasonably expensive in terms of time and computing power ; large numbers of runs , which are really needed to have confidence in the results , are correspondingly more costly .",
    "few things are more dispiriting than to expend such quantities of time and care , only to end up with ambiguous results .",
    "it is almost always worthwhile , therefore , to carefully think through what you want to measure , and why , before running anything .",
    "in particular , if you are trying to judge the merits of competing models , effort put into figuring out how and where they are _ most _ different will generally be well - rewarded .",
    "the theory of experimental design offers extensive guidance on how to devise informative series of experiments , both for model comparison and for other purposes , and by and large the principles apply to simulations as well as to real experiments .",
    "* * monte carlo is the name of a broad , slightly indistinct family for using random processes to estimate deterministic quantities , especially the properties of probability distributions .",
    "a classic example will serve to illustrate the basic idea , on which there are many , many refinements .",
    "consider the problem of determining the area @xmath188 under an curve given by a known but irregular function @xmath189 . in principle , you could integrate @xmath37 to find this area , but suppose that numerical integration is infeasible for some reason .",
    "( we will come back to this point presently . )",
    "a monte carlo solution to this problem is as follows : pick points at random , uniformly over the square .",
    "the probability @xmath106 that a point falls in the shaded region is equal to the fraction of the square occupied by the shading : @xmath190 .",
    "if we pick @xmath191 points independently , and @xmath7 of them fall in the shaded region , then @xmath192 ( by the law of large numbers ) , and @xmath193 .",
    "@xmath194 provides us with a stochastic estimate of the integral .",
    "moreover , this is a probably approximately correct (  [ sec : pac - and - srm ] ) estimate , and we can expect , from basic probability theory , that the standard deviation of the estimate around its true value will be proportional to @xmath195 , which is not bad , so asymptotically @xmath196 has a gaussian distribution with mean @xmath106 and standard deviation @xmath197 .",
    "a non - asymptotic result comes from chernoff s inequality @xcite , which tells us that , for all @xmath191 , @xmath198 . ] .",
    "however , when faced with such a claim , one should always ask what the proportionality constant is , and whether it is the best achievable . here",
    "it is not : the equally simple , if less visual , scheme of just picking values of @xmath7 uniformly and averaging the resulting values of @xmath189 always has a smaller standard deviation @xcite .",
    "this example , while time - honored and visually clear , does not show monte carlo to its best advantage ; there are few one - dimensional integrals which can not be done better by ordinary , non - stochastic numerical methods .",
    "but numerical integration becomes computationally intractable when the domain of integration has a large number of dimensions , where `` large '' begins somewhere between four and ten .",
    "monte carlo is much more indifferent to the dimensionality of the space : we could replicate our example with a 999-dimensional hyper - surface in a 1000-dimensional space , and we d still get estimates that converged like @xmath195 , so achieving an accuracy of @xmath199 will require evaluating the function @xmath37 only @xmath200 times .",
    "our example was artificially simple in another way , in that we used a uniform distribution over the entire space .",
    "often , what we want is to compute the expectation of some function @xmath189 with a non - uniform probability @xmath201 .",
    "this is just an integral , @xmath202 , so we could sample points uniformly and compute @xmath203 for each one .",
    "but if some points have very low probability , so they only make a small contribution to the integral , spending time evaluating the function there is a bit of a waste .",
    "a better strategy would be to pick points according to the actual probability distribution .",
    "this can sometimes be done directly , especially if @xmath201 is of a particularly nice form .",
    "a very general and clever indirect scheme is as follows @xcite .",
    "we want a whole sequence of points , @xmath204 .",
    "we pick the first one however we like , and after that we pick successive points according to some markov chain : that is , the distribution of @xmath205 depends only on @xmath206 , according to some fixed function @xmath207 . under some mild conditions ,",
    "the distribution of @xmath46 approaches a stationary distribution @xmath208 at large times @xmath49 . if we could ensure that @xmath209 , we know that the markov chain was converging to our distribution , and then , by the ergodic theorem , averaging @xmath189 along a trajectory would give the expected value of @xmath189 .",
    "one way to ensure this is to use the `` detailed balance '' condition of the invariant distribution , that the total probability of going from @xmath7 to @xmath6 must equal the total probability of going the other way : @xmath210 so now we just need to make sure that ( [ eqn : detailed - condition ] ) is satisfied .",
    "one way to do this is to set @xmath211 ; this was the original proposal of metropolis _",
    "et al_. another is @xmath212 .",
    "this method is what physicists usually mean by `` monte carlo '' , but statisticians call it * * markov chain monte carlo , or `` mcmc '' .",
    "while we can now estimate the properties of basically arbitrary distributions , we no longer have independent samples , so evaluating the accuracy of our estimates is no longer a matter of _ trivial _",
    "probability for large enough @xmath23 , but determining what s `` large enough '' is trickier . ] .",
    "an immense range of refinements have been developed over the last fifty years , addressing these and other points ; see the further reading section for details .",
    "keep in mind that monte carlo is a stochastic simulation method only in a special sense  it simulates the probability distribution @xmath201 , _ not _ the mechanism which generated that distribution .",
    "the dynamics of markov chain monte carlo , in particular , often bear no resemblance whatsoever to those of the real system . since the point of monte carlo is to tell us about the properties of @xmath201 ( what is the expectation value of this function ? what is the probability of configurations with this property ? etc . )",
    ", the actual trajectory of the markov chain is of no interest .",
    "this point sometimes confuses those more used to direct simulation methods .",
    "naturally enough , analytical techniques are not among the tools which first come to mind for dealing with complex systems ; in fact , they often do not come to mind at all .",
    "this is unfortunate , because a lot of intelligence has been devoted to devising approximate analytical techniques for classes of models which include many of those commonly used for complex systems .",
    "a general advantage of analytical techniques is that they are often fairly insensitive to many details of the model . since any model we construct of a complex system is almost certainly much simpler than the system itself , a great many of its details are just wrong .",
    "if we can extract non - trivial results which are insensitive to those details , we have less reason to worry about this .",
    "one particularly useful , yet neglected , body of approximate analytical techniques relies on the fact that many complex systems models are markovian .",
    "in an agent - based model , for instance , the next state of an agent generally depends only on its present state , and the present states of the agents it interacts with .",
    "if there is a fixed interaction graph , the agents form a markov random field on that graph .",
    "there are now very powerful and computationally efficient methods for evaluating many properties of markov chains @xcite , markov random fields @xcite , and ( closely related ) graphical models @xcite _ without _ simulation .",
    "the recent books of peyton young @xcite and sutton @xcite provide nice instances of using analytical results about markov processes to solve models of complex social systems , without impractical numerical experiments .",
    "we can only compare particular aspects of a model of a system to particular kinds of data about that system .",
    "the most any experimental test can tell us , therefore , is how similar the model is to the system _ in that respect_. one may think of an experimental comparison as a test for a _",
    "particular _ kind of _ error _ , one of the infinite number of mistakes which we could make in building a model .",
    "a good test is one which is very likely to alert us to an error , if we have made it , but not otherwise @xcite .",
    "these ought to be things every school - child knows about testing hypotheses .",
    "it is very easy , however , to blithely ignore these truisms when confronted with , on the one hand , a system with many strongly interdependent parts , and , on the other hand , a model which tries to mirror that complexity .",
    "we must decide which features of the model _",
    "ought _ to be similar to the system , and how similar .",
    "it is important not only that our model be able to adequately reproduce those phenomena , but that it not entail badly distorted or non - existent phenomena in other respects .",
    "let me give two examples from very early in the study of complex systems , which nicely illustrate some fundamental points .",
    "the first has to do with pattern formation in chemical oscillators @xcite .",
    "certain mixtures of chemicals in aqueous solution , mostly famously the belusov - zhabotinsky reagent , can not only undergo cyclic chemical reactions , but will form rotating spiral waves , starting from an initial featureless state .",
    "this is a visually compelling example of self - organization , and much effort has been devoted to understanding it .",
    "one of the more popular early models was the `` brusselator '' advanced by prigogine and his colleagues at the free university of brussels ; many similarly - named variants developed .",
    "brusselator - type models correctly predicted that these media would support spiral waves .",
    "they all , further , predicted that the spirals would form only when the homogeneous configuration was unstable , and that then they would form spontaneously .",
    "it proved very easy , however , to prepare the belusov - zhabotisnky reagent in such a way that it was `` perfectly stable in its uniform quiescence '' , yet still able to produce spiral waves if excited ( e.g. , by being touched with a hot wire ) @xcite .",
    "the brusselator and its variants were simply unable to accommodate these phenomena , and had to be discarded in favor of other models .",
    "the fact that these were qualitative results , rather than quantitative ones , if anything made it more imperative to get rid of the brusselator .",
    "the second story concerns the work of varela and maturana on `` autopoesis '' . in a famous paper @xcite",
    ", they claimed to exhibit a computational model of a simple artificial chemistry where membranes not only formed spontaneously , but a kind of metabolism self - organized to sustain the membranes .",
    "this work influenced not just complex systems science but theoretical biology , psychology , and even sociology @xcite .",
    "when , in the 1990s , mcmullin made the first serious effort to reproduce the results , based on the description of the model in the paper , that description proved _ not _ to match the published simulation results .",
    "the discrepancy was only resolved by the fortuitous rediscovery of a mass of papers , including fortran code , that varela had left behind in chile when forced into exile by the fascist regime .",
    "these revealed a crucial change in one particular reaction made all the difference between successful autopoesis and its absence .",
    "( for the full story , see @xcite . ) many similar stories could be told of other models in complex systems @xcite ; this one is distinguished by mcmullin s unusual tenacity in trying to replicate the results , varela s admirable willingness to assist him , and the happy ending .",
    "the story of autopoesis is especially rich in morals .",
    "( 1 ) replication is essential .",
    "( 2 ) it is a good idea to share not just data but programs . ( 3 ) _ always _ test the robustness of your model to changes in its parameters .",
    "( this is fairly common . ) ( 4 ) _ always _ test your model for robustness to small changes in qualitative assumptions . if your model calls for a given effect , there are usually several mechanisms which could accomplish it .",
    "if it does not matter which mechanism you actually use , the result is that much more robust .",
    "conversely , if it does matter , the over - all adequacy of the model can be tested by checking whether _ that _ mechanism is actually present in the system .",
    "altogether too few people perform such tests .",
    "data are often available only about large aggregates , while models , especially agent - based models , are about individual behavior .",
    "one way of comparing such models to data is to compute the necessary aggregates , from direct simulation , monte carlo , etc .",
    "the problem is that many different models can give the same aggregated behavior , so this does not provide a powerful test between different models .",
    "ideally , we d work back from aggregate data to individual behaviors , which is known , somewhat confusingly , as * * ecological inference .",
    "in general , the ecological inference problem itself does not have a unique solution .",
    "but the aggregate data , if used intelligently , can often put fairly tight constraints on the individual behaviors , and micro - scale can be directly checked against those constraints .",
    "much of the work here has been done by social scientists , especially american political scientists concerned with issues arising from the voting rights act @xcite , but the methods they have developed are very general , and could profitably be applied to agent - based models in the biological sciences , though , to my knowledge , they have yet to be .      are there other ways of generating the data ?",
    "there generally are , at least if `` the data '' are some very gross , highly - summarized pattern .",
    "this makes it important to look for differential signatures , places where discrepancies between different generative mechanisms give one some _",
    "leverage_. given two mechanisms which can both account for our phenomenon , we should look for some _ other _ quantity whose behavior will be different under the two hypotheses . ideally , in fact , we would look for the statistic on which the two kinds of model are _ most _ divergent .",
    "the literature on experimental design is relevant here again , since it considers such problems under the heading of * * model discrimination , seeking to maximize the power of experiments ( or simulations ) to distinguish between different classes of models @xcite .",
    "perhaps no aspect of methodology is more neglected in complex systems science than this one . while it is always perfectly legitimate to announce a new mechanism as _ a _ way of generating a phenomenon ,",
    "it is far too common for it to be called _ the _ way to do it , and vanishingly rare to find an examination of how it _ differs _ from previously - proposed mechanisms .",
    "newman and palmer s work on extinction models @xcite stands out in this regard for its painstaking examination of the ways of discriminating between the various proposals in the literature .",
    "information theory began as a branch of communications engineering , quantifying the length of codes needed to represent randomly - varying signals , and the rate at which data can be transmitted over noisy channels .",
    "the concepts needed to solve these problems turn out to be quite fundamental measures of the uncertainty , variability , and the interdependence of different variables .",
    "information theory thus is an important tool for studying complex systems , and in addition is indispensable for understanding complexity measures (  [ sec : complexity ] ) .",
    "our notation and terminology follows that of cover and thomas s standard textbook @xcite .",
    "given a random variable @xmath0 taking values in a discrete set @xmath213 , the * * entropy or * * information content @xmath214 $ ] of @xmath0 is @xmath215 & \\equiv & -\\sum_{a\\in\\mathcal{a}}{{\\mathrm{pr}}(x = a)\\log_2{{\\mathrm{pr}}(x = a ) } } ~.\\end{aligned}\\ ] ] @xmath214 $ ] is the expectation value of @xmath216 .",
    "it represents the uncertainty in @xmath0 , interpreted as the mean number of binary distinctions ( bits ) needed to identify the value of @xmath0 . alternately , it is the minimum number of bits needed to encode or describe @xmath0 .",
    "note that @xmath214 = 0 $ ] if and only if @xmath0 is ( almost surely ) constant .    the * * joint entropy @xmath217 $ ] of two variables @xmath0 and @xmath1 is the entropy of their joint distribution : @xmath218 & \\equiv & -\\sum_{a\\in\\mathcal{a},b\\in\\mathcal{b}}{{\\mathrm{pr}}(x = a , y = b)\\log_2{{\\mathrm{pr}}(x = a , y = b ) } } ~.\\end{aligned}\\ ] ]    the * * conditional entropy of @xmath0 given @xmath1 is @xmath219 & \\equiv & h[x , y ] - h[y ] ~.\\end{aligned}\\ ] ] @xmath220 $ ] is the average uncertainty remaining in @xmath0 , given a knowledge of @xmath1 .",
    "the * * mutual information @xmath221 $ ] between @xmath0 and @xmath1 is @xmath222 & \\equiv &   h[x ] - h[x|y ] ~.\\end{aligned}\\ ] ] it gives the reduction in @xmath0 s uncertainty due to knowledge of @xmath1 and is symmetric in @xmath0 and @xmath1 .",
    "we can also define higher - order mutual informations , such as the third - order information @xmath223 $ ] , @xmath224 & \\equiv & h[x ] + h[y ] + h[z ] - h[x , y , z]\\end{aligned}\\ ] ] and so on for higher orders .",
    "these functions reflect the joint dependence among the variables .",
    "mutual information is a special case of the * * relative entropy , also called the * * kullback - leibler divergence ( or * * distance ) . given two _ distributions _ ( not variables ) , @xmath225 and @xmath226 , the entropy of @xmath226 relative to @xmath225 is @xmath227 @xmath149 measures how far apart the two distributions are , since @xmath228 , and @xmath229 implies the two distributions are equal almost everywhere .",
    "the divergence can be interpreted either in terms of codes ( see below ) , or in terms of statistical tests @xcite .",
    "roughly speaking , given @xmath191 samples drawn from the distribution @xmath225 , the probability of our accepting the false hypothesis that the distribution is @xmath226 can go down no faster than @xmath230 .",
    "the mutual information @xmath221 $ ] is the divergence between the joint distribution @xmath231 , and the product of the marginal distributions , @xmath232 , and so measures the departure from independence .",
    "some extra information - theoretic quantities make sense for time series and stochastic processes .",
    "supposing we have a process @xmath233 + @xmath234 , we can define its * * mutual information function by analogy with the autocovariance function ( see  [ sec : time - series - properties ] ) , @xmath235\\\\ i_{\\bar{x}}(\\tau ) & = & i[x_t;x_{t+\\tau}]\\end{aligned}\\ ] ] where the second form is valid only for strictly stationary processes .",
    "the mutual information function measures the degree to which different parts of the series are dependent on each other .",
    "the * * entropy rate @xmath54 of a stochastic process @xmath236}\\\\ & = & h[x_0|x_{-\\infty}^{-1 } ] ~.\\end{aligned}\\ ] ] ( the limit always exists for stationary processes . ) @xmath54 measures the process s unpredictability , in the sense that it is the uncertainty which remains in the next measurement even given complete knowledge of its past .",
    "in nonlinear dynamics , @xmath54 is called the * * kolmogorov - sinai ( ks ) entropy .    for continuous variables",
    ", one can define the entropy via an integral , @xmath215 & \\equiv & -\\int{p(x ) \\log{p(x ) } dx } ~,\\end{aligned}\\ ] ] with the subtlety that the continuous entropy not only can be negative , but depends on the coordinate system used for @xmath7 .",
    "the relative entropy also has the obvious definition , @xmath237 but is coordinate - independent and non - negative .",
    "so , hence , is the mutual information .",
    "[ [ optimal - coding ] ] optimal coding + + + + + + + + + + + + + +    one of the basic results of information theory concerns codes , or schemes for representing random variables by bit strings .",
    "that is , we want a scheme which associates each value of a random variable @xmath0 with a bit string .",
    "clearly , if we want to keep the average length of our code - words small , we should give shorter codes to the more common values of @xmath0 .",
    "it turns out that the average code - length is minimized if we use @xmath238 bits to encode @xmath7 , and it is always possible to come within one bit of this .",
    "then , on average , we will use @xmath239 } = h[x]$ ] bits .",
    "this presumes we know the true probabilities .",
    "if we think the true distribution is @xmath226 when it is really @xmath225 , we will , on average , use @xmath240 } \\geq h[x]$ ] .",
    "this quantity is called the * * cross - entropy or * * inaccuracy , and is equal to @xmath214 + d({\\mathrm{p}}\\|{\\mathrm{q}})$ ] .",
    "thus , finding the correct probability distribution is equivalent to minimizing the cross - entropy , or the relative entropy @xcite .",
    "[ [ the - khinchin - axioms - and - rnyi - information ] ] the khinchin axioms and rnyi information + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in 1953 , a. i. khinchin published a list of four reasonable - looking axioms for a measure of the information @xmath214 $ ] associated with a random variable @xmath0 @xcite .",
    "he then proved that the shannon information was the unique functional satisfying the axioms , up to an over - all multiplicative constant .",
    "( the choice of this constant is equivalent to the choice of the base for logarithms . )",
    "the axioms were as follows .",
    "* the information is a functional of the probability distribution of @xmath0 , and not on any of its other properties . in particular ,",
    "if @xmath37 is any invertible function , @xmath214 = h[f(x)]$ ]",
    ". * the information is maximal for the uniform distribution , where all events are equally probable . *",
    "the information is unchanged by enlarging the probability space with events of zero probability . *",
    "if the probability space is divided into two sub - spaces , so that @xmath0 is split into two variables @xmath1 and @xmath241 , the total information is equal to the information content of the marginal distribution of one sub - space , plus the mean information of the conditional distribution of the other sub - space : @xmath214 = h[y ] + { \\mathbf{e}\\left[h(z|y)\\right]}$ ] .",
    "a similar axiomatic treatment can be given for the mutual information and the relative entropy .",
    "while the first three of khinchin s axioms are all highly plausible , the fourth is somewhat awkward .",
    "it is intuitively more plausible to merely require that , if @xmath1 and @xmath241 are independent , then @xmath242 = h[y ] + h[z]$ ] . if the fourth axiom is weakened in this way , however , there is no longer only a single functional satisfying the axioms . instead , any of the infinite family of entropies introduced by rnyi satisfies the axioms .",
    "the * * rnyi entropy of order @xmath138 , with @xmath138 any non - negative real number , is @xmath243 & \\equiv & \\frac{1}{1 - \\alpha}{\\log{\\sum_{i : p_i >        0}{p_i^\\alpha}}}\\end{aligned}\\ ] ] in the discrete case , and the corresponding integral in the continuous case .",
    "the parameter @xmath138 can be thought of as gauging how strongly the entropy is biased towards low - probability events . as @xmath244 , low - probability events count more , until at @xmath245 , all possible events receive equal weight .",
    "( this is sometimes called the * * topological entropy . ) as @xmath246 , only the highest - probability event contributes to the sum .",
    "one can show that , as @xmath247 , @xmath248 \\rightarrow h[x]$ ] , i.e. , one recovers the ordinary shannon entropy in the limit .",
    "there are entropy rates corresponding to all the rnyi entropies , defined just like the ordinary entropy rate . for dynamical systems , these are related to the fractal dimensions of the attractor @xcite .",
    "the * * rnyi divergences bear the same relation to the rnyi entropies as the kullback - leibler divergence does to the shannon entropy .",
    "the defining formula is @xmath249 and similarly for the continuous case .",
    "once again , @xmath250 . for all @xmath251 , @xmath252 , and",
    "is equal to zero if and only if @xmath225 and @xmath226 are the same .",
    "( if @xmath253 , then a vanishing rnyi divergence only means that the supports of the two distributions are the same . )",
    "the rnyi entropy @xmath248 $ ] is non - increasing as @xmath138 grows , whereas the rnyi divergence @xmath254 is non - decreasing .",
    "[ [ estimation - of - information - theoretic - quantities ] ] estimation of information - theoretic quantities + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in applications , we will often want to estimate information - theoretic quantities , such as the shannon entropy or the mutual information , from empirical or simulation data .",
    "restricting our attention , for the moment , to the case of discrete - valued variables , the empirical distribution will generally converge on the true distribution , and so the entropy ( say ) of the empirical distribution ( `` sample entropy '' ) will also converge on the true entropy . however , it is not the case that the sample entropy is an _ unbiased _ estimate of the true entropy .",
    "the shannon ( and rnyi ) entropies are measures of variation , like the variance , and sampling tends to reduce variation .",
    "just as the sample variance is a negatively biased estimate of the true variance , sample entropy is a negatively - biased estimate of the true entropy , and so sample mutual information is a positively - biased estimate of true information . understanding and controlling the bias , as well as the sampling fluctuations , can be very important .",
    "victor @xcite has given an elegant method for calculating the bias of the sample entropy ; remarkably , the leading - order term depends only on the alphabet size @xmath144 and the number of samples @xmath23 , and is @xmath255 .",
    "higher - order terms , however , depend on the true distribution .",
    "recently , kraskov et al .",
    "@xcite have published an adaptive algorithm for estimating mutual information , which has very good properties in terms of both bias and variance .",
    "finally , the estimation of entropy _ rates _ is a somewhat tricky matter .",
    "the best practices are to either use an algorithm of the type given by @xcite , or to fit a properly dynamical model .",
    "( for discrete data , variable - length markov chains , discussed in ",
    "[ sec : vlmms ] above , generally work very well , and the entropy rate can be calculated from them very simply . )",
    "another popular approach is to run one s time series through a standard compression algorithm , such as ` gzip ` , dividing the size in bits of the output by the number of symbols in the input @xcite .",
    "this is an absolutely horrible idea ; even under the circumstances under which it gives a consistent estimate of the entropy rate , it converges much more slowly , and runs more slowly , than employing either of the two techniques just mentioned @xcite .      beyond its original home in communications engineering",
    ", information theory has found a multitude of applications in statistics @xcite and learning theory @xcite .",
    "scientifically , it is very natural to consider some biological systems as communications channels , and so analyze their information content ; this has been particularly successful for biopolymer sequences @xcite and especially for neural systems , where the analysis of neural codes depends vitally on information theory @xcite",
    ". however , there is nothing prohibiting the application of information theory to systems which are not designed to function as communications devices ; the concepts involved require only well - defined probability distributions .",
    "for instance ,",
    "in nonlinear dynamics @xcite information - theoretic notions are very important in characterizing different kinds of dynamical system ( see also  [ sec : symbolic - dyn ] ) . even more closely tied to complex",
    "systems science is the literature on `` physics and information '' or `` physics and computation '' , which investigates the relationships between the mechanical principles of physics and information theory , e.g. , landauer s principle , that erasing ( but not storing ) a bit of information at temperature @xmath256 produces @xmath257 joules of heat , where @xmath258 is boltzmann s constant .",
    "we have already given some thought to complexity , both in our initial rough definition of `` complex system '' and in our consideration of machine learning and occam s razor .",
    "in the latter , we saw that the relevant sense of `` complexity '' has to do with families of models : a model class is complex if it requires large amounts of data to reliably find the best model in the class . on the other hand , we initially said that a complex system is one with many highly variable , strongly interdependent parts . here",
    ", we will consider various proposals for putting some mathematical spine into that notion of a system s complexity , as well as the relationship to the notion of complexity of learning .",
    "most measures of complexity for systems formalize the intuition that something is complex if it is difficult to describe adequately . the first mathematical theory based on this idea",
    "was proposed by kolmogorov ; while it is _ not _ good for analyzing empirical complex systems , it was very important historically , and makes a good point of entry into the field .",
    "consider a collection of measured data - values , stored in digitized form on a computer .",
    "we would like to say that they are complex if they are hard to describe , and measure their complexity by the difficulty of describing them .",
    "the central idea of kolmogorov complexity ( proposed independently by solomonoff @xcite and chaitin ) is that one can describe the data set by writing a program which will reproduce the data .",
    "the difficulty of description is then measured by the length of the program .",
    "anyone with much experience of other people s code will appreciate that it is always possible to write a longer , slower program to do a given job , so what we are really interested in is the shortest program that can exactly replicate the data .    to introduce some symbols ,",
    "let @xmath7 be the data , and @xmath259 their size in bits .",
    "the kolmogorov or algorithmic complexity of @xmath7 , @xmath260 , is the length of the shortest program that will output @xmath7 and then stop .",
    "clearly , there is always some program which will output @xmath7 and then stop , for instance , `` ` print(x ) ; end ` '' . thus @xmath261 ",
    "then @xmath7 is highly complex . some data , however , is highly compressible .",
    "for instance , if @xmath7 consists of the second quadrillion digits of @xmath262 , a very short program suffices to generate it to arbitrary accuracy , and the length of these programs does not grow as the number of digits calculated does .",
    "so one could run one of these programs until it had produced the first two quadrillion digits , and then erase the first half of the output , and stop . ] .    as you may already suspect , the number of simple data sets is quite limited",
    ". suppose we have a data set of size @xmath191 bits , and we want to compress it by @xmath144 bits , i.e. , find a program for it which is @xmath263 bits long .",
    "there are at most @xmath264 programs of that length , so of all the @xmath265 data sets of size @xmath191 , the fraction which can be compressed by @xmath144 bits is at most @xmath266",
    ". the precise degree of compression does not matter  when we look at large data sets , almost all of them are highly complex . if we pick a large data set _ at random _",
    ", then the odds are very good that it will be complex",
    ". we can state this more exactly if we think about our data as consisting of the first @xmath191 measurements from some sequence , and let @xmath191 grow .",
    "that is , @xmath267 , and we are interested in the asymptotic behavior of @xmath268 .",
    "if the measurements @xmath206 are independent and identically distributed ( iid ) , then @xmath269 almost surely ; iid sequences are * * incompressible .",
    "if @xmath7 is a realization of a stationary ( but not necessarily iid ) random process @xmath270 , then @xcite @xmath271 } } & = & h(\\bar{x } ) ~,\\end{aligned}\\ ] ] the entropy rate (  [ sec : info - theory ] ) of @xmath270 .",
    "thus , random data has high complexity , and the complexity of a random process grows at a rate which just measures its unpredictability .",
    "this observation goes the other way : complex data looks random .",
    "the heuristic idea is that if there were any regularities in the data , we could use them to shave at least a little bit off the length of the minimal program .",
    "what one can show formally is that incompressible sequences have _ all _ the properties of iid sequences  they obey the law of large numbers and the central limit theorem , pass all statistical tests for randomness , etc .",
    "in fact , this possibility , of defining `` random '' as `` incompressible '' , is what originally motivated kolmogorov s work @xcite .",
    "kolmogorov complexity is thus a very important notion for the foundations of probability theory , and it has extensive applications in theoretical computer science @xcite and even some aspects of statistical physics @xcite .",
    "unfortunately , it is quite useless as a measure of the complexity of natural systems .",
    "this is for two reasons .",
    "first , as we have just seen , it is maximized by _ independent _ random variables ; we want _ strong dependence_. second , and perhaps more fundamental , it is simply not possible to calculate kolmogorov complexity . for deep reasons related to gdel s theorem , there can not be any procedure for calculating @xmath260 , nor are there any accurate approximation procedures @xcite .",
    "many scientists are strangely in denial about the kolmogorov complexity , in that they think they can calculate it .",
    "apparently unaware of the mathematical results , but aware of the relationship between kolmogorov complexity and data compression , they reason that file compression utilities should provide an estimate of the algorithmic information content . thus one finds many papers which might be titled `` ` gzip ` as a measure of complexity '' , and the practice is even recommended by some otherwise - reliable sources ( e.g. , @xcite ) . however , this is simply a confused idea , with absolutely nothing to be said in its defense .",
    "we saw just now that algorithmic information is really a measure of randomness , and that it is maximized by collections of independent random variables .",
    "since complex systems have many strongly dependent variables , it follows that the kolmogorov notion is not the one we really want to measure .",
    "it has long been recognized that we really want something which is small both for systems which are strongly ordered ( i.e. , have only a small range of allowable behavior ) and for those which are strongly disordered ( i.e. , have independent parts ) .",
    "many ways of modifying the algorithmic information to achieve this have been proposed ; two of them are especially noteworthy .",
    "bennett @xcite proposed the notion of the * * logical depth of data as a measure of its complexity . roughly speaking ,",
    "the logical depth @xmath272 of @xmath7 is the number of computational steps the minimal program for @xmath7 must execute . for incompressible data ,",
    "the minimal program is ` print(x ) ` , so @xmath273 . for periodic data , the minimal program cycles over printing out one period over and over , so @xmath274 again .",
    "for some compressible data , however , the minimal program must do non - trivial computations , which are time - consuming .",
    "thus , to produce the second quadrillion digits of @xmath262 , the minimal program is one which _ calculates _ the digits , and this takes considerably more time than reading them out of a list .",
    "thus , @xmath262 is deep , while random or periodic data are shallow .",
    "while logical depth is a clever and appealing idea , it suffers from a number of drawbacks .",
    "first , real data are not , so far as we know , actually produced by running their minimal programs , and the run - time of that program has no known _ physical _ significance , and that s not for lack of attempts to find one @xcite .",
    "second , and perhaps more decisively , there is still no procedure for finding the minimal program .",
    "perhaps the most important modification of the kolmogorov complexity is that proposed by gcs , tromp and vitanyi @xcite , under the label of `` algorithmic statistics '' .",
    "observe that , when speaking of the minimal program for @xmath7 , i said nothing about the inputs to the program ; these are to be built in to the code .",
    "it is this which accounts for the length of the programs needed to generate random sequences : almost all of the length of ` print(x ) ` comes from @xmath7 , not @xmath275 .",
    "this suggests splitting the minimal program into two components , a `` model '' part , the program properly speaking , and an `` data '' part , the inputs to the program .",
    "we want to put all the regularities in @xmath7 into the model , and all the arbitrary , noisy parts of @xmath7 into the inputs . just as in probability theory",
    "a `` statistic '' is a function of the data which summarizes the information they convey , gcs _ et al . _",
    "regard the model part of the program as an * * algorithmic statistic , summarizing its regularities . to avoid the trivial regularity of @xmath275",
    "when possible , they define a notion of a * * sufficient algorithmic statistic , based on the idea that @xmath7 should be in some sense a typical output of the model ( see their paper for details ) .",
    "they then define the complexity of @xmath7 , or , as they prefer to call it , the * * sophistication , as the length of the shortest sufficient algorithmic statistic .",
    "like logical depth , sophistication is supposed to discount the purely random part of algorithmic complexity .",
    "unlike logical depth , it stays within the confines of description in doing so ; programs , here , are just a particular , mathematically tractable , kind of description .",
    "unfortunately , the sophistication is still uncomputable , so there is no real way of applying algorithmic statistics .",
    "the basic problem with algorithmic complexity and its extensions is that they are all about finding the shortest way of exactly describing a single configuration .",
    "even if we could compute these measures , we might suspect , on the basis of our discussion of over - fitting in  [ sec : data - mining ] above , that this is not what we want .",
    "many of the details of any particular set of data are just noise , and will not generalize to other data sets obtained from the same system . if we want to characterize the complexity of the system , it is precisely the generalizations that we want , and not the noisy particulars .",
    "looking at the sophistication , we saw the idea of picking out , from the overall description , the part which describes the regularities of the data .",
    "this idea becomes useful and operational when we abandon the goal of _ exact _ description , and allow ourselves to recognize that the world is full of noise , which is easy to describe statistically ; we want a statistical , and not an algorithmic , measure of complexity .",
    "i will begin with what is undoubtedly the most widely - used statistical measure of complexity , rissanen s * * stochastic complexity , which can also be considered a method of model selection .",
    "then i will look at three attempts to isolate the complexity of the system as such , by considering how much information would be required to predict its behavior , _ if _ we had an optimal statistical model of the system .",
    "suppose we have a statistical model with some parameter @xmath3 , and we observe the data @xmath7 .",
    "the model assigns a certain likelihood to the data , @xmath276 .",
    "one can make this into a loss function by taking its negative logarithm : @xmath277 .",
    "maximum likelihood estimation minimizes this loss function .",
    "we also learned , in ",
    "[ sec : info - theory ] , that if @xmath278 is the correct probability distribution , the optimal coding scheme will use @xmath279 bits to encode @xmath7 . thus , maximizing the likelihood can also be thought of as minimizing the encoded length of the data .",
    "however , we do not yet have a complete description : we have an encoded version of the data , but we have not said what the encoding scheme , i.e. , the model , is .",
    "thus , the total description length has two parts : @xmath280 where @xmath281 is the number of bits we need to specify @xmath3 from among the set of all our models @xmath282 .",
    "@xmath283 represents the `` noisy '' or arbitrary part of the description , the one which will not generalize ; the model represents the part which does generalize .",
    "if @xmath284 gives short codes to simple models , we have the desired kind of trade - off , where we can reduce the part of the data which looks like noise only by using a more elaborate model .",
    "the * * minimum description length principle @xcite enjoins us to pick the model which minimizes the description length , and the * * stochastic complexity of the data is that minimized description - length : @xmath285 under not - too - onerous conditions on the underlying data - generating process and the model class @xmath282 @xcite , as we provide more data @xmath286 will converge on the model in @xmath282 which minimizes the generalization error , which here is just the same as minimizing the kullback - leibler divergence from the true distribution , find the minimum would , once again , be incomputable .",
    "this restriction to a definite , perhaps hierarchically organized , class of models is vitally important . ] .",
    "regarded as a principle of model selection , mdl has proved very successful in many applications , even when dealing with quite intricate , hierarchically - layered model classes .",
    "( @xcite is a nice recent application to a biomedical complex system ; see  [ sec : nld - approach ] for applications to state - space reconstruction . )",
    "it is important to recognize , however , that most of this success comes from carefully tuning the model - coding term @xmath287 so that models which do not generalize well turn out to have long encodings .",
    "this is perfectly legitimate , but it relies on the tact and judgment of the scientist , and often , in dealing with a complex system , we have no idea , or at least no _ good _ idea , what generalizes and what does not .",
    "if we were malicious , or short - sighted , we can always insure that the particular data we got have a stochastic complexity of just one bit , @xmath283 is either 0 ( if @xmath7 is what that model happens to generate ) or @xmath42 . then , once we have our data , and find a @xmath3 which generates that and nothing but that , re - arrange the coding scheme so that @xmath288 ; this is always possible .",
    "thus , @xmath289 bit . ] . the model which gives us this complexity will then have absolutely horrible generalization properties .",
    "whatever its merits as a model selection method , stochastic complexity does not make a good measure of the complexity of natural systems .",
    "there are at least three reasons for this .    1 .",
    "the dependence on the model - encoding scheme , already discussed .",
    "the log - likelihood term , @xmath283 in @xmath290 can be decomposed into two parts , one of which is related to the entropy rate of the data - generating process , and so reflects its intrinsic unpredictability . the other ,",
    "however , indicates the degree to which even the most accurate model in @xmath282 is misspecified .",
    "thus it reflects our ineptness as modelers , rather than any characteristic of the process .",
    "finally , the stochastic complexity reflects the need to specify some particular model , and to represent this specification .",
    "while this is necessarily a part of the modeling process for us , it seems to have no _ physical _ significance ; the system does not need to _ represent _ its organization , it just _ has _ it .      [ [ forecast - complexity - and - predictive - information ] ] forecast complexity and predictive information + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    motivated in part by concerns such as these",
    ", grassberger @xcite suggested a new and highly satisfactory approach to system complexity : complexity is the amount of information required for optimal prediction .",
    "let us first see why this idea is plausible , and then see how it can be implemented in practice .",
    "( my argument does not follow that of grassberger particularly closely . also",
    ", while i confine myself to time series , for clarity , the argument generalizes to any kind of prediction @xcite . )",
    "we have seen that there is a limit on the accuracy of any prediction of a given system , set by the characteristics of the system itself ( limited precision of measurement , sensitive dependence on initial conditions , etc . ) .",
    "suppose we had a model which was maximally predictive , i.e. , its predictions were at this limit of accuracy .",
    "prediction , as i said , is always a matter of mapping inputs to outputs ; here the inputs are the previous values of the time series .",
    "however , not all aspects of the entire past are relevant . in the extreme case of independent , identically - distributed values , _ no _ aspects of the past are relevant . in the case of periodic sequences with period @xmath106 ,",
    "one only needs to know which of the @xmath106 phases the sequence is in .",
    "if we ask how _ much _ information about the past is relevant in these two cases , the answers are clearly 0 and @xmath291 , respectively .",
    "if one is dealing with a markov chain , only the present state is relevant , so the amount of information needed for optimal prediction is just equal to the amount of information needed to specify the current state .",
    "one thus has the nice feeling that both highly random ( iid ) and highly ordered ( low - period deterministic ) sequences are of low complexity , and more interesting cases can get high scores .",
    "more formally , any predictor @xmath37 will translate the past of the sequence @xmath292 into an effective state , @xmath293 , and then make its prediction on the basis of @xmath55 .",
    "( this is true whether @xmath37 is formally a state - space model or not . )",
    "the amount of information required to specify the state is @xmath294 $ ] . we can take this to be the complexity of @xmath37 . now ,",
    "if we confine our attention to the set @xmath295 of maximally predictive models , we can define what grassberger called the `` true measure complexity '' or `` forecast complexity '' of the process as the minimal amount of information needed for optimal prediction : @xmath296}\\end{aligned}\\ ] ]    grassberger did not provide a procedure for finding the maximally predictive models , nor for minimizing the information required among them .",
    "he did , however , make the following observation .",
    "a basic result of information theory , called the * * data - processing inequality , says that @xmath297 \\geq i[f(a);b]$ ] , for any variables @xmath188 and @xmath115  we can not get more information out of data by processing it than was in there to begin with .",
    "since the state of the predictor is a function of the past , it follows that @xmath298 \\geq i[f(x^-);x^+]$ ] .",
    "presumably , for optimal predictors , the two informations are equal  the predictor s state is just as informative as the original data .",
    "( otherwise , the model would be missing some potential predictive power . ) but another basic inequality is that @xmath299 \\geq i[a;b]$ ]  no variable contains more information about another than it does about itself .",
    "so , for optimal models , @xmath300 \\geq i[x^-;x^+]$ ] . the latter quantity , which grassberger called the * * effective measure complexity , can be estimated purely from data , without intervening models .",
    "this quantity  the mutual information between the past and the future  has been rediscovered many times , in many contexts , and called * * excess entropy ( in statistical mechanics ) , * * stored information @xcite , * * complexity @xcite or * * predictive information @xcite ; the last name is perhaps the clearest .",
    "since it quantifies the degree of statistical dependence between the past and the future , it is clearly appealing as a measure of complexity .",
    "[ [ the - grassberger - crutchfield - young - statistical - complexity ] ] the grassberger - crutchfield - young statistical complexity + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the forecasting complexity notion was made fully operational by crutchfield and young @xcite , who provided an effective procedure for finding the minimal maximally predictive model and its states .",
    "they began by defining the * * causal states of a process , as follows .",
    "for each history @xmath292 , there is some conditional distribution of future observations , @xmath301 .",
    "two histories @xmath302 and @xmath303 are equivalent if @xmath304 .",
    "write the set of all histories equivalent to @xmath292 as @xmath305 $ ] .",
    "now we have a function @xmath15 which maps each history into its equivalence class : @xmath306 $ ] . clearly , @xmath307 .",
    "crutchfield and young accordingly proposed to forget the particular history and retain only its equivalence class , which they claimed would involve no loss of predictive power ; this was later proved to be correct ( * ? ? ?",
    "* theorem 1 ) .",
    "they called the equivalence classes the `` causal states '' of the process , and claimed that these were the simplest states with maximal predictive power ; this is also was right ( * ? ? ?",
    "* theorem 2 ) .",
    "finally , one can show that the causal states are the _ unique _ optimal states ( * ? ? ?",
    "* theorem 3 ) ; any other optimal predictor is really a disguised version of the causal states . accordingly , they defined the * * statistical complexity of a process @xmath308 as the information content of its causal states .",
    "because the causal states are purely an objective property of the process being considered , @xmath308 is too ; it does not depend at all on our modeling or means of description .",
    "it is equal to the length of the shortest description of the past which is _ relevant _ to the actual dynamics of the system . as we argued",
    "should be the case above , for iid sequences it is exactly 0 , and for periodic sequences it is @xmath291 .",
    "one can show ( * ? ? ?",
    "* theorems 5 and 6 ) that the statistical complexity is always at least as large as the predictive information , and generally that it measures how far the system departs from statistical independence .",
    "the causal states have , from a statistical point of view , quite a number of desirable properties .",
    "the maximal prediction property corresponds exactly to that of being a sufficient statistic @xcite ; in fact they are minimal sufficient statistics @xcite . the sequence of states of the process form a markov chain .",
    "referring back to our discussion of filtering and state estimation (  [ sec : filtering ] ) , one can design a recursive filter which will eventually estimate the causal state without any error at all ; moreover , it is always clear whether the filter has `` locked on '' to the correct state or not .",
    "all of these properties of the causal states and the statistical complexity extend naturally to spatially - extended systems , including , but not limited to , cellular automata @xcite .",
    "each point in space then has its own set of causal states , which form not a markov chain but a markov field , and the local causal state is the minimal sufficient statistic for predicting the future of that point .",
    "the recursion properties carry over , not just temporally but spatially : the state at one point , at one time , helps determine not only the state at that same point at later times , but also the state at neighboring points at the same time .",
    "the statistical complexity , in these spatial systems , becomes the amount of information needed about the past of a given point in order to optimally predict its future .",
    "systems with a high degree of local statistical complexity are ones with intricate spatio - temporal organization , and , experimentally , increasing statistical complexity gives a precise formalization of intuitive notions of self - organization @xcite .",
    "crutchfield and young were inspired by analogies to the theory of abstract automata , which led them to call their theory , somewhat confusingly , * * computational mechanics .",
    "their specific initial claims for the causal states were based on a procedure for deriving the minimal automaton capable of producing a given family of sequences ) . ] known as nerode equivalence classing @xcite .",
    "in addition to the theoretical development , the analogy to nerode equivalence - classing led them to describe a procedure @xcite for estimating the causal states and the @xmath15-machine from empirical data , at least in the case of discrete sequences . this crutchfield - young algorithm has actually been successfully used to analyze empirical data , for instance , geomagnetic fluctuations @xcite .",
    "the algorithm has , however , been superseded by a newer algorithm which uses the known properties of the causal states to guide the model discovery process @xcite ( see  [ sec : causal - state - models ] above ) .",
    "let me sum up .",
    "the grassberger - crutchfield - young statistical complexity is an objective property of the system being studied .",
    "it reflects the _ intrinsic _ difficulty of predicting it , namely the amount of information which is actually relevant to the system s dynamics .",
    "it is low both for highly disordered and trivially - ordered systems .",
    "above all , it is calculable , and has actually been calculated for a range of natural and mathematical systems . while the initial formulation was entirely in terms of discrete time series , the theory can be extended straightforwardly to spatially - extended dynamical systems @xcite , where it quantifies self - organization @xcite , to controlled dynamical systems and transducers , and to prediction problems generally @xcite .      over the last decade or so",
    ", it has become reasonably common to see people ( especially physicists ) claiming that some system or other is complex , because it exhibits a power law distribution of event sizes . despite its popularity , this is simply a fallacy .",
    "no one has demonstrated any relation between power laws and any kind of formal complexity measure . nor is there any link tying power laws to our intuitive idea of complex systems as ones with strongly interdependent parts .",
    "it is true that , _ in equilibrium statistical mechanics _",
    ", one does not find power laws _ except _ near phase transitions @xcite , when the system _ is _ complex by our standard .",
    "this has encouraged physicists to equate power laws as such with complexity . despite this",
    ", it has been known for half a century @xcite that there are many , many ways of generating power laws , just as there are many mechanisms which can produce poisson distributions , or gaussians .",
    "perhaps the simplest one is that recently demonstrated by reed and hughes @xcite , namely exponential growth coupled with random observation times .",
    "the observation of power laws alone thus says nothing about complexity ( except in thermodynamic equilibrium ! ) , and certainly is not a reliable sign of some specific favored mechanism , such as self - organized criticality @xcite or highly - optimized tolerance @xcite .",
    "the statistics of power laws are poorly understood within the field of complex systems , to a degree which is quite surprising considering how much attention has been paid to them . to be quite honest ,",
    "there is little reason to think that many of the things claimed to be power laws actually _ are _ such , as opposed to some other kind of heavy - tailed distribution .",
    "this brief section will attempt to inoculate the reader against some common mistakes , most of which are related to the fact that a power law makes a straight line on a log - log plot . since it would be impractical to cite all papers which commit these mistakes , and unfair to cite only some of them i will omit references here ; interested readers will be able to assemble collections of their own very rapidly .    [",
    "[ parameter - estimation ] ] parameter estimation + + + + + + + + + + + + + + + + + + + +    presuming that something is a power law , a natural way of estimating its exponent is to use linear regression to find the line of best fit to the points on the log - log plot .",
    "this is actually a consistent estimator , if the data really do come from a power law .",
    "however , the loss function used in linear regression is the sum of the squared distances between the line and the points ( `` least squares '' ) . in general",
    ", the line minimizing the sum of squared errors is _ not _ a valid probability distribution , and so this is simply not a reliable way to estimate the _",
    "distribution_.    one is much better off using maximum likelihood to estimate the parameter . with a discrete variable ,",
    "the probability function is @xmath309 , where @xmath310 is the riemann zeta function , which ensures that the probability is normalized .",
    "thus the maximum likelihood estimate of the exponent is obtained by minimizing the negative log - likelihood , @xmath311 , i.e. , @xmath312 is our loss function . in the continuous case , the probability density is @xmath313 , with @xmath314 .",
    "[ [ error - estimation ] ] error estimation + + + + + + + + + + + + + + + +    most programs to do linear regression also provide an estimate of the standard error in the estimated slope , and one sometimes sees this reported as the uncertainty in the power law .",
    "this is an entirely unacceptable procedure .",
    "those calculations of the standard error assume that measured values having gaussian fluctuations around their true means . here",
    "that would mean that the log of the empirical relative frequency is equal to the log of the probability plus gaussian noise .",
    "however , by the central limit theorem , one knows that the relative frequency is equal to the probability plus gaussian noise , so the former condition does not hold .",
    "notice that one can obtain asymptotically reliable standard errors from maximum likelihood estimation .",
    "[ [ validation - r2 ] ] validation ; @xmath315 + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    perhaps the most pernicious error is that of trying to validate the assumption of a power law distribution by either eye - balling the fit to a straight line , or evaluating it using the @xmath315 statistic , i.e. , the fraction of the variance accounted for by the least - squares regression line . unfortunately , while these procedures are good at confirming that something is a power law , if it really is ( low type i error , or high statistical significance ) , they are very bad at alerting you to things that are _ not _ power laws ( they have a very high rate of type ii error , or low statistical power ) .",
    "the basic problem here is that _ any _ smooth curve looks like a straight line , if you confine your attention to a sufficiently small region  and for some non - power - law distributions , such `` sufficiently small '' regions can extend over multiple orders of magnitude .    to illustrate this last point , consider figure [ fig : simulation ] , made by generating 10,000 random numbers according to a log - normal distribution , with a mean log of 0 and a standard deviation in the log of 3 .",
    "restricting attention to the `` tail '' of random numbers @xmath316 , and doing a usual least - squares fit , gives the line shown in figure [ fig : simulation - tail ] .",
    "one might hope that it would be easy to tell that this data does not come from a power law , since there are a rather large number of observations ( 5,112 ) , extending over a wide domain ( more than four orders of magnitude ) .",
    "nonetheless , @xmath315 is 0.962 .",
    "this , in and of itself , constitutes a demonstration that getting a high @xmath315 is not a reliable indicator that one s data was generated by a power law",
    ". actually increases , to 0.994 . ]",
    "[ [ an - illustration - blogging ] ] an illustration : blogging + + + + + + + + + + + + + + + + + + + + + + + + +    an amusing empirical illustration of the difficulty of distinguishing between power laws and other heavy - tailed distributions is provided by political weblogs , or `` blogs ''  websites run by individuals or small groups providing links and commentary on news , political events , and the writings of other blogs . a rough indication of the prominence of a blog is provided by the number of other blogs linking to it  its * * in - degree .",
    "( for more on network terminology , see wuchty , ravasz and barabsi , this volume . )",
    "a widely - read essay by shirky claimed that the distribution of in - degree follows a power law , and used that fact , and the literature on the growth of scale - free networks , to draw a number of conclusions about the social organization of the blogging community @xcite . a more recent paper by drenzer and farrell @xcite , in the course of studying the role played by blogs in general political debate , re - examined the supposed power - law distribution .",
    "using a large population of inter - connected blogs , they found a definitely heavy - tailed distribution which , on a log - log plot , was quite noticeably concave ( [ fig : least - squares - fit ] ) ; nonetheless , @xmath315 for the conventional regression line was 0.898 .",
    "maximum likelihood fitting of a power law distribution gave @xmath317 , with a negative log - likelihood of @xmath318 . similarly fitting a log - normal distribution gave @xmath319 } = 2.60 \\pm 0.02 $ ] and @xmath320 , with a negative log - likelihood of 17,218.22 .",
    "as one can see from figure [ fig : combined - fit ] , the log - normal provides a very good fit to almost all of the data , whereas even the best fitting power - law distribution is not very good at all .    a rigorous application of the logic of error testing @xcite would now consider the probability of getting at least this good a fit to a log - normal if the data were actually generated by a power law .",
    "however , since in this case the data were @xmath321 million times more likely under the log - normal distribution , any sane test would reject the power - law hypothesis .      considerations of space preclude an adequate discussion of further complexity measures .",
    "it will have to suffice to point to some of the leading ones .",
    "the * * thermodynamic depth of lloyd and pagels @xcite measures the amount of information required to specify a trajectory leading to a final state , and is related both to departure from thermodynamic equilibrium and to retrodiction @xcite .",
    "huberman and hogg @xcite , and later wolpert and macready @xcite proposed to measure complexity as the _ dissimilarity _ between different levels of a given system , on the grounds that self - similar structures are actually very easy to describe .",
    "( say what one level looks like , and then add that all the rest are the same ! ) wolpert and macready s measure of self - dissimilarity is , in turn , closely related to a complexity measure proposed by sporns , tononi and edelman @xcite for biological networks , which is roughly the amount of information present in higher - order interactions between nodes which is not accounted for by the lower - order interactions .",
    "badii and politi @xcite propose a number of further * * hierarchical scaling complexities , including one which measures how slowly predictions converge as more information about the past becomes available .",
    "other interesting approaches include the * * information fluctuation measure of bates and shepard @xcite , and the predictability indices of the `` school of rome '' @xcite .",
    "why measure complexity at all ?",
    "suppose you are interested in the patterns of gene expressions in tumor cells and how they differ from those of normal cells .",
    "why should you care if i analyze your data and declare that ( say ) healthy cells have a more complex expression pattern ? assuming you are not a numerologist , the only reason you _",
    "should _ care is if you can learn something from that number  if the complexity i report tells you something about the thermodynamics of the system , how it responds to fluctuations , how easy it is to control , etc .",
    "a good complexity measure , in other words , is one which is _ relevant _ to many other aspects of the system measured .",
    "a bad complexity measure lacks such relevance ; a really bad complexity measure would be positively misleading , lumping together things with no real connection or similarity just because they get the same score .",
    "my survey here has focused on complexity measures which have some claim to relevance , deliberately avoiding the large number of other measures which lack it @xcite .",
    "there is no systematic or academically - detailed survey of the `` patterns '' of complex systems , but there are several sound informal discussions : axelrod and cohen @xcite , flake @xcite , holland @xcite and simon @xcite .",
    "the book by simon , in particular , repays careful study .    on the `` topics '' , the only books i can recommend are the ones by boccara @xcite and flake @xcite . the former emphasizes topics from physics , chemistry , population ecology and epidemiology , along with analytical methods , especially from nonlinear dynamics .",
    "some sections will be easier to understand if one is familiar with statistical mechanics at the level of , e.g. , @xcite , but this is not essential .",
    "it does not , however , describe any models of adaptation , learning , evolution , etc .",
    "many of those topics are covered in flake s book , which however is written at a much lower level of mathematical sophistication .",
    "on foundational issues about complexity , the best available surveys @xcite both neglect the more biological aspects of the area , and assume advanced knowledge of statistical mechanics on the part of their readers .",
    "there are now two excellent introductions to statistical learning and data mining , @xcite and @xcite .",
    "the former is more interested in computational issues and the initial treatment of data ; the latter gives more emphasis to pure statistical aspects .",
    "both are recommended unreservedly .",
    "baldi and brunak @xcite introduces machine learning via its applications to bioinformatics , and so may be especially suitable for readers of this book .    for readers seriously interested in understanding the theoretical basis of machine learning , @xcite is a good starting point .",
    "the work of vapnik @xcite is fundamental ; the presentation in his @xcite is enlivened by many strong and idiosyncratic opinions , pungently expressed .",
    "@xcite describes the very useful class of models called `` support vector machines '' , as well as giving an extremely clear exposition of key aspects of statistical learning theory .",
    "those interested in going further will find that most of the relevant literature is still in the form of journals  _ machine learning _",
    ", _ journal of machine learning research _ ( free on - line at www.jmlr.org ) , _ neural computation _  and especially annual conference proceedings  computational learning theory ( colt ) , international conference on machine learning ( icml ) , uncertainty in artificial intelligence ( uai ) , knowledge discovery in databases ( kdd ) , neural information processing systems ( nips ) , and the regional versions of them ( eurocolt , pacific kdd , etc . ) .",
    "much of what has been said about model selection could equally well have been said about what engineers call * * system identification , and in fact _ is _ said in good modern treatments of that area , of which @xcite may be particularly recommended .    in many respects ,",
    "data mining is an extension of exploratory data analysis ; the classic work by tukey @xcite is still worth reading .",
    "no discussion of drawing inferences from data would be complete without mentioning the beautiful books by tufte @xcite .",
    "perhaps the best all - around references for the nonlinear dynamics approach are @xcite and @xcite .",
    "the former , in particular , succeeds in integrating standard principles of statistical inference into the nonlinear dynamics method .",
    "@xcite , while less advanced than those two books , is a model of clarity , and contains an integrated primer on chaotic dynamics besides .",
    "ruelle s little book @xcite is _ much _ more subtle than it looks , full of deep insights .",
    "the sfi proceedings volumes @xcite are very worthwhile .",
    "the journals _ physica d _ , _ physical review e _ and _ chaos _ often have new developments .    from the statistical wing , one of the best recent textbooks is @xcite ; there are many , many others . that by durbin and koopman @xcite is particularly strong on the state - space point of view .",
    "the one by @xcite azencott and dacunha - castelle is admirably clear on both the aims of time series analysis , and the statistical theory underlying classical methods ; unfortunately it typography is less easy to read than it should be .",
    "@xcite provides a comprehensive and up - to - date view of the statistical theory for modern models , including strongly non - linear and non - gaussian models . while many of the results are directly useful in application , the proofs rely on advanced theoretical statistics , in particular the geometric approach pioneered by the japanese school of amari _ et al . _",
    "@xcite , this * * information geometry has itself been applied by ay to the study of complex systems @xcite .    at the interface between the statistical and the dynamical points of view",
    ", there is an interesting conference proceedings @xcite and a useful book by tong @xcite .",
    "pearson s book @xcite on discrete - time models is very good on many important issues related to model selection , and exemplifies the habit of control theorists of cheerful stealing whatever seems helpful .",
    "@xcite provides a readable introduction to optimal nonlinear estimation , draws interesting analogies to non - equilibrium statistical mechanics and turbulence , _ and _ describes a reasonable approximation scheme .",
    "@xcite is an up - to - date textbook , covering both linear and nonlinear methods , and including a concise exposition of the essential parts of stochastic calculus .",
    "the website run by r. w. r. darling , www.nonlinearfiltering.webhop.net , provides a good overview and extensive pointers to the literature",
    ".      on symbolic dynamics , formal languages and hidden markov models generally , see @xcite .",
    "@xcite is a good first course on formal languages and automata theory .",
    "charniak is a very readable introduction to grammatical inference .",
    "@xcite is an advanced treatment of symbolic dynamics emphasizing applications ; by contrast , @xcite focuses on algebraic , pure - mathematical aspects of the subject .",
    "@xcite is good on the stochastic properties of symbolic - dynamical representations .",
    "gershenfeld @xcite gives a good motivating discussion of hidden markov models , as does baldi and brunak @xcite , while @xcite describes advanced methods related to statistical signal processing .",
    "open - source code for reconstructing causal - state models from state is available from http://bactra.org/cssr .",
    "there is unfortunately no completely satisfactory unified treatment of cellular automata above the recreational .",
    "ilachinski @xcite attempts a general survey aimed at readers in the physical sciences , and is fairly satisfactory on purely mathematical aspects , but is more out of date than its year of publication suggests .",
    "chopard and droz @xcite has good material on models of pattern formation missing from ilachinski , but the english is often choppy .",
    "toffoli and margolus @xcite is inspiring and sound , though cast on a piece of hardware and a programming environment which are sadly no longer supported .",
    "much useful material on ca modeling has appeared in conference proceedings @xcite .",
    "the evolution of ca begins in @xcite , continues in @xcite , and is brought up to the modern era in @xcite ; the last is a beautiful , thought - provoking and modest book , sadly out of print .",
    "the modern era itself opens with @xcite .",
    "@xcite is a good introduction , @xcite somewhat more advanced .",
    "the pair of proceedings edited by doolen @xcite describe many interesting applications , and contain useful survey and pedagogical articles .",
    "( there is little overlap between the two volumes . )",
    "there do not seem to be any useful textbooks or monographs on agent - based modeling .",
    "the _ artificial life _ conference proceedings , starting with @xcite , were a prime source of inspiration for agent - based modeling , along with the work of axelrod @xcite .",
    "@xcite is also worth reading .",
    "the journal _ artificial life _ continues to be relevant , along with the _ from animals to animats _ conference series .",
    "epstein and axtell s book @xcite is in many ways the flagship of the `` minimalist '' approach to abms ; while the arguments in its favor ( e.g. , @xcite ) are often framed in terms of social science , many apply with equal force to biology .",
    "@xcite illustrates how abms can be combined with extensive real - world data .",
    "other notable publications on agent - based models include @xcite , spanning social science and evolutionary biology ; @xcite on agent - based models of morphogenesis ; and @xcite on biological self - organization .",
    "@xcite introduces object - oriented programming and the popular java programming language at the same time ; it also discusses the roots of object - orientation in computer simulation .",
    "there are many , many other books on object - oriented programming .",
    "honerkamp @xcite is great , but curiously almost unknown .",
    "gershenfeld @xcite is an extraordinary readable encyclopedia of applied mathematics , especially methods which can be used on real data .",
    "gardiner @xcite is also useful .      the old book by hammersley and handscomb @xcite is concise , clear , and has no particular prerequisites beyond a working knowledge of calculus and probability .",
    "@xcite and @xcite are both good introductions for readers with some grasp of statistical mechanics .",
    "there are also very nice discussions in @xcite .",
    "beckerman @xcite makes monte carlo methods the starting point for a fascinating and highly unconventional exploration of statistical mechanics , markov random fields , synchronization and cooperative computation in neural and perceptual systems .",
    "information theory appeared in essentially its modern form with shannon s classic paper @xcite , though there had been predecessors in both communications @xcite and statistics , notably fisher ( see kullback @xcite for an exposition of these notions ) , and similar ideas were developed by wiener and von neumann , more or less independently of shannon @xcite .",
    "cover and thomas @xcite is , deservedly , the standard modern textbook and reference ; it is highly suitable as an introduction , and handles almost every question most users will , in practice , want to ask .",
    "@xcite is a more mathematically rigorous treatment , now free on - line . on neural information theory , @xcite is seminal , well - written , still very valuable , and largely self - contained . on the relationship between physics and information ,",
    "the best reference is still the volume edited by zurek @xcite , and the thought - provoking paper by margolus .",
    "the best available survey of complexity measures is that by badii and politi @xcite ; the volume edited by peliti and vulpiani @xcite , while dated , is still valuable .",
    "edmonds @xcite is an online bibliography , fairly comprehensive through 1997 .",
    "@xcite has an extensive literature review .    on kolmogorov complexity ,",
    "see li and vitanyi @xcite .",
    "while the idea of measuring complexity by the length of descriptions is usually credited to the trio of kolmogorov , solomonoff and chaitin , it is implicit in von neumann s 1949 lectures on the `` theory and organization of complicated automata '' ( * ? ? ?",
    "* part i , especially pp .",
    "4256 ) .",
    "this work has been supported by a grant from the james s. mcdonnell foundation , and was finished while enjoying the hospitality of the institut des systmes complexes , the laboratoire de linformatique du parallisme and the exystence thematic institute on discrete and computational aspects of complex systems at the cole normale superiere de lyon .",
    "i am grateful to michel morvan and cris moore , organizers of the `` science et gastronomie 2003 '' workshop , for allowing me to present some of this material there , and to my fellow participants for their critical comments .",
    "it is a pleasure to acknowledge discussions with dave albers , satinder singh baveja , philippe binder , sven bruckener , sandra chapman , markus christen , michael cohen , jim crutchfield , gunther eble , dave feldman , walter fontana , peter grassberger , rob haslinger , john holland , herbert jaeger , jrgen jost , michael lachmann , alex lancaster , norm margolus , cris moore , mark newman , scott page , mitchell porter , kristina shalizi , carl simon , eric smith , ricard sol , bill tozier , erik van nimwegen and nick watkins .",
    "special thanks to prof .",
    "cohen for permission to take a triangle he drew and add a corner to it ; to profs .",
    "drenzer and farrell for permission to use their data on weblogs ; to bill tozier for suggesting that the leitmotifs of complex systems are analysis patterns , and for advice on agents ; to kara kedi for daily validation of von neumann s remarks on the complexity of cats ; to mathew dafilis and nigel phillips for spotting misprints ; to kristina shalizi for assistance on linear models of time series and for careful reading of a draft ; and to kristina and rob for resisting the idea that there _ is _ such a thing as complex systems science . finally , i wish to acknowledge the patience of the editors ."
  ],
  "abstract_text": [
    "<S> in this chapter , i review the main methods and techniques of complex systems science . as a first step , </S>",
    "<S> i distinguish among the broad patterns which recur across complex systems , the topics complex systems science commonly studies , the tools employed , and the foundational science of complex systems . </S>",
    "<S> the focus of this chapter is overwhelmingly on the third heading , that of tools . </S>",
    "<S> these in turn divide , roughly , into tools for analyzing data , tools for constructing and evaluating models , and tools for measuring complexity . </S>",
    "<S> i discuss the principles of statistical learning and model selection ; time series analysis ; cellular automata ; agent - based models ; the evaluation of complex - systems models ; information theory ; and ways of measuring complexity . throughout </S>",
    "<S> , i give only rough outlines of techniques , so that readers , confronted with new problems , will have a sense of which ones might be suitable , and which ones definitely are not . </S>"
  ]
}