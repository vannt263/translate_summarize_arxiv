{
  "article_text": [
    "adaptation techniques are useful when the labeled training data are from a source domain and the test ones are from a target domain . samples of the two domains are collected under different conditions , thus have different distributions . labeling samples in the target domain to develop new prediction models is often labor - intensive and time - consuming .",
    "therefore , domain adaptation or transfer learning is needed to improve the performance in the target domain by leveraging unlabeled ( and maybe a few labeled ) target samples @xcite .",
    "this topic is receiving increasing attention in recent years due to its broad applications such as computer vision @xcite and text classification @xcite .",
    "it is also important in the field of sensors and measurement . because of",
    "the variations in the fabrication of sensors and devices , the responses to the same signal source may not be identical for different instruments .",
    "furthermore , the sensing characteristics of the sensors , the operating condition , or even the signal source itself , can change over time . as a result ,",
    "the prediction model trained with the samples from the initial device in an earlier time period ( source domain ) is not suitable for new devices or in a latter time ( target domains ) .",
    "a typical application plagued by this problem is machine olfaction , which uses electronic noses ( e - noses ) and pattern recognition algorithms to predict the type and concentration of odors @xcite .",
    "the applications of machine olfaction range from agriculture and food to environmental monitoring , robotics , biometrics , and disease analysis @xcite .",
    "however , owing to the nature of chemical sensors , many e - noses are prone to instrumental variation and time - varying drift mentioned above @xcite , which greatly hamper their usage in real - world applications .",
    "traditional methods for calibration transfer ( compensating instrumental variation ) and drift correction ( compensating time - varying drift ) require a set of predefined gas samples as transfer samples .",
    "they should be collected with each device and in each time period so as to provide mapping information between the source and the target domains @xcite .",
    "then , a widely - used method is to map the features in the target domain to the source domain with regression algorithms @xcite .",
    "nevertheless , collecting transfer samples is demanding for non - professional e - nose users because standard gases need to be prepared and much effort has to be made .",
    "in such cases , domain adaptation techniques with unlabeled target samples are desirable .",
    "an intuitive idea is to reduce the difference of distributions among domains in the feature level , i.e.  to learn domain - invariant feature representation @xcite .",
    "for example , pan et al .",
    "@xcite proposed transfer component analysis ( tca ) , which finds a latent feature space that minimizes the difference of distributions between two domains in the sense of maximum mean discrepancy .",
    "more related methods will be introduced in section [ sec : relwork ] . when applied to calibration transfer and drift correction ,",
    "however , existing domain adaptation algorithms are faced with two difficulties .",
    "first , they are designed to handle discrete source and target domains . in time",
    "- varying drift , samples come in a stream , so the change in data distribution is often continuous . one solution is to split data into several batches , but it will lost the temporal order information .",
    "second , because of the variation in the sensitivity of chemical sensors , the same signal in different conditions may indicate different concepts . in other words , the conditional probability @xmath0 may change for samples with different backgrounds , where `` background '' means when and with which device a sample was collected .",
    "methods like tca project all samples to a common subspace , hence the samples with similar appearance but different concepts can not be distinguished .    in this paper",
    ", we present a simple yet effective algorithm called maximum independence domain adaptation ( mida ) .",
    "the algorithm first defines `` domain features '' for each sample to describe its background .",
    "then , it finds a latent feature space in which the samples and their domain features are maximally independent in the sense of hilbert - schmidt independence criterion ( hsic ) @xcite .",
    "thus , the discrete and continuous change in distribution can be handled uniformly . in order to project samples according to their backgrounds ,",
    "feature augmentation is performed by concatenating the original feature vector with the domain features .",
    "we also propose semi - supervised mida ( smida ) to exploit the label information with hsic .",
    "mida and smida are both very flexible .",
    "( 1 ) they can be applied in situations with single or multiple source or target domains thanks to the use of domain features .",
    "in fact , the notion `` domain '' has been extended to `` background '' which can carry more information about the background of a sample .",
    "( 2 ) although they are designed for unsupervised domain adaptation problems ( no labeled sample in target domains ) , the proposed methods naturally allow both unlabeled and labeled samples in any domains , thus can be applied in semi - supervised ( both unlabeled and labeled samples in target domains ) and supervised ( only labeled samples in target domains ) problems as well .",
    "( 3 ) the label information can be either discrete ( binary- or multi - class classification ) or continuous ( regression ) .",
    "this advantage is inherited from hsic .    to illustrate the effect of our algorithms , we first evaluate them on several synthetic datasets .",
    "then , calibration transfer and drift correction experiments are performed on two e - nose datasets and one spectroscopy dataset .",
    "note that spectrometers suffer the same instrumental variation problem as e - noses @xcite .",
    "finally , a domain adaptation experiment is conducted on a well - known object recognition benchmark @xcite .",
    "results confirm the effectiveness of the proposed algorithms .",
    "the rest of the paper is organized as follows .",
    "related work on unsupervised domain adaptation and hsic is briefly reviewed in section [ sec : relwork ] .",
    "section [ sec : method ] describes domain features , mida , and smida in detail .",
    "the experimental configurations and results are presented in section [ sec : exp ] , along with some discussions .",
    "section [ sec : conclusion ] concludes the paper .",
    "two good surveys on domain adaptation can be found in @xcite and @xcite . in this section ,",
    "we focus on typical methods that extract domain - invariant features . compared to model - level adaptation methods ,",
    "feature - level ones are easier to use because the extracted features can be applied to various prediction models . in order to reduce the difference of distributions among domains while preserving useful information ,",
    "researchers have developed many strategies .",
    "some algorithms project all samples to a common latent space @xcite .",
    "transfer component analysis ( tca ) @xcite tries to learn transfer components across domains in a reproducing kernel hilbert space ( rkhs ) using maximum mean discrepancy .",
    "it is further extended to semi - supervised tca ( sstca ) to encode label information and preserve local geometry of the manifold .",
    "shi et al .",
    "@xcite measured domain difference by the mutual information between all samples and their binary domain labels , which can be viewed as a primitive version of the domain features used in this paper .",
    "they also minimized the negated mutual information between the target samples and their cluster labels to reduce the expected classification error .",
    "the low - rank transfer subspace learning ( ltsl ) algorithm presented in @xcite is a reconstruction guided knowledge transfer method .",
    "it aligns source and target data by representing each target sample with some local combination of source samples in the projected subspace .",
    "the label and geometry information can be retained by embedding different subspace learning methods into ltsl .",
    "another class of methods first project the source and the target data into separate subspaces , and then build connections between them @xcite .",
    "fernando et al .",
    "@xcite utilized a transformation matrix to map the source subspace to the target one , where a subspace was represented by eigenvectors of pca .",
    "the geodesic flow kernel ( gfk ) method @xcite measures the geometric distance between two different domains in a grassmann manifold by constructing a geodesic flow .",
    "an infinite number of subspaces are combined along the flow in order to model a smooth change from the source to the target domain .",
    "liu et al .",
    "@xcite adapted gfk for drift correction of e - noses .",
    "a sample stream is first split into batches according to the acquisition time .",
    "the first and the last batches ( domains ) are then connected through every intermediate batch using gfk .",
    "another improvement of gfk is domain adaptation by shifting covariance ( dasc ) @xcite . observing that modeling one domain as a subspace is not sufficient to represent the difference of distributions",
    ", dasc characterizes domains as covariance matrices and interpolates them along the geodesic to bridge the domains .",
    "hsic is used as a convenient method to measure the dependence between two sample sets @xmath1 and @xmath2 .",
    "let @xmath3 and @xmath4 be two kernel functions associated with rkhss @xmath5 and @xmath6 , respectively .",
    "@xmath7 is the joint distribution .",
    "hsic is defined as the square of the hilbert - schmidt norm of the cross - covariance operator @xmath8 @xcite : @xmath9 + \\mathbf{e}_{xx'}[k_x(x , x')]\\mathbf{e}_{yy'}[k_y(y , y ' ) ] \\\\          & - 2\\mathbf{e}_{xy}[\\mathbf{e}_{x'}[k_x(x , x')]\\mathbf{e}_{y'}[k_y(y , y')]].\\end{aligned}\\ ] ] here @xmath10 is the expectation over independent pairs @xmath11 and @xmath12 drawn from @xmath7 .",
    "it can be proved that with characteristic kernels @xmath13 and @xmath14 , hsic@xmath15 is zero if and only if @xmath16 and @xmath17 are independent @xcite .",
    "a large hsic suggests strong dependence with respect to the choice of kernels .",
    "hsic has a biased empirical estimate .",
    "suppose @xmath18 , @xmath19 are the kernel matrices of @xmath1 and @xmath2 , respectively , then @xcite : @xmath20 where @xmath21 is the centering matrix .",
    "due to its simplicity and power , hsic has been adopted for feature extraction @xcite and feature selection @xcite .",
    "researchers typically use it to maximize the dependence between the extracted / selected features and the label . however , to our knowledge , it has not been utilized in domain adaptation to reduce the dependence between the extracted features and the domain features .",
    "we aim to reduce the dependence between the extracted features and the background information . a sample s background information should ( 1 ) naturally exists , thus can be easily obtained ;",
    "( 2 ) has different distributions in training and test samples ; ( 3 ) correlates with the distribution of the original features .",
    "the domain label ( which domain a sample belongs ) in common domain adaptation problems is an example of such information . according to these characteristics ,",
    "the information clearly interferes the testing performance of a prediction model .",
    "thus , minimizing the aforementioned dependence is desirable .",
    "first , a group of new features need to be designed to describe the background information .",
    "the features are called `` domain features '' . from the perspective of calibration transfer and drift correction , there are two main types of background information : the device label ( with which device the sample was collected ) and the acquisition time ( when the sample was collected ) .",
    "we can actually encode more information such as the place of collection , the operation condition , and so on , which will be useful in other domain adaptation problems .",
    "formally , if we only consider the instrumental variation , an one - hot coding scheme can be used .",
    "suppose there are @xmath22 devices , which result in @xmath22 different but related domains .",
    "the domain feature vector is thus @xmath23 , where @xmath24 if the sample is from the @xmath25th device and 0 otherwise .",
    "this scheme also applies to traditional domain adaptation problems with several discrete domains . if the time - varying drift is also considered , the acquisition time can be further added . if a sample was collected from the @xmath25th device at time @xmath26 , then @xmath27 , where @xmath28    according to ( [ eq : hsicemp ] ) , the kernel matrix @xmath29 of the domain features needs to be computed for hsic .",
    "we apply the linear kernel .",
    "suppose @xmath30 \\in { \\bf{r}}^{m_d\\times n } $ ] , @xmath31 is the dimension of a domain feature vector .",
    "then @xmath32 @xmath33 is 0 if samples @xmath34 and @xmath35 are from different devices ; otherwise , it is 1 when time - varying drift is not considered , or @xmath36 when it is considered .",
    "feature augmentation is used in this paper to learn background - specific subspaces . in @xcite ,",
    "the author proposed a feature augmentation strategy for domain adaptation : if a sample @xmath37 is from the source domain , then its augmented feature vector is @xmath38 \\in { \\bf{r}}^{3 m } $ ] ; if it is from the target domain , then @xmath39 \\in { \\bf{r}}^{3 m } $ ] .",
    "the augmented labeled source and target samples are then used jointly to train one prediction model . in this way",
    ", the learned model can be viewed as two different models for the two domains . meanwhile ,",
    "the two models share a common component @xcite .",
    "however , this strategy requires that data lie in discrete domains and can not deal with time - varying drift .",
    "we propose a more general and efficient feature augmentation strategy : concatenating the original features and the domain features , i.e. @xmath40\\in { \\bf{r}}^{m+m_d}.\\ ] ]    the role of this strategy can be demonstrated through a linear dimensionality reduction example .",
    "suppose a projection matrix @xmath41 has been learned for the augmented feature vector .",
    "@xmath42 is the dimension of the subspace .",
    "@xmath43 has two parts : @xmath44 , w_x\\in { \\bf{r}}^{m\\times h } , w_d\\in { \\bf{r}}^{m_d\\times h } $ ] .",
    "the embedding of @xmath45 can be expressed as @xmath46 , which means that a background - specific bias @xmath47 has been added to each dimension @xmath34 of the embedding .",
    "one may argue that a bias may not always be enough for alignment",
    ". it may be better to have domain - specific affine transformation matrices .",
    "however , with the absence of target labels , learning such matrices can be prone to overfitting . from another perspective",
    ", the feature augmentation strategy maps the samples to an augmented space with higher dimension before projecting them to a subspace .",
    "it will be easier to find a projection direction in the augmented space to align the samples well in the subspace .",
    "the effect of feature augmentation will be illustrated on several synthetic datasets in section [ subsec : synexp ] .",
    "take machine olfaction for example , there are situations when the conditional probability @xmath0 changes along with the background .",
    "for instance , the sensitivity of chemical sensors often decays over time . a signal that indicates low concentration in an earlier time actually suggests high concentration in a later time . in such cases , feature augmentation is important , because it allows samples with similar appearance but different concepts to be treated differently by the background - specific bias .",
    "the strategy also helps to align the domains better in each projected dimension .      in this section",
    ", we introduce the formulation of mida in detail .",
    "suppose @xmath48 is the matrix of @xmath49 samples .",
    "the training and the test samples are pooled together .",
    "more importantly , we do not have to explicitly differentiate which domain a sample is from .",
    "the feature vectors has been augmented , but we use the notations @xmath50 and @xmath51 instead of @xmath52 and @xmath53 for brevity . a linear or nonlinear mapping function @xmath54 can be used to map @xmath1 to a new space .",
    "based on the kernel trick , we need not know the exact form of @xmath54 , but the inner product of @xmath55 can be represented by the kernel matrix @xmath56 . then , a projection matrix @xmath57 is applied to project @xmath55 to a subspace with dimension @xmath42 , leading to the projected samples @xmath58 .",
    "similar to other kernel dimensionality reduction algorithms @xcite , the key idea is to express each projection direction as a linear combination of all samples in the space , namely @xmath59 .",
    "@xmath60 is the projection matrix to be actually learned .",
    "thus , the projected samples are @xmath61 with the kernel matrix @xmath62 intuitively , if the projected features are independent of the domain features , then we can not distinguish the background of a sample by its projected features , suggesting that the difference of distributions among domains is diminished in the subspace .",
    "therefore , by substituting ( [ eq : kz ] ) into the empirical hsic ( [ eq : hsicemp ] ) and omit the scaling factor , we get the expression to be minimized : @xmath63",
    ".    in domain adaptation , the goal is not only minimizing the difference of distributions , but also preserving important properties of data , such as the variance @xcite .",
    "it can be achieved by maximizing the trace of the covariance matrix of the project samples .",
    "the covariance matrix is @xmath64 where @xmath65 is the same as that in ( [ eq : hsicemp ] ) .",
    "an orthonormal constraint is further added on @xmath43 .",
    "the learning problem then becomes @xmath66 where @xmath67 is a trade - off hyper - parameter .",
    "to solve ( [ eq : midaobj ] ) , we can use its lagrangian : @xmath68 where @xmath69 is a matrix containing the lagrange multipliers . setting the derivative of ( [ eq : midalagr ] ) with respect",
    "to @xmath43 to zero , we get @xmath70 consequently , @xmath43 is the eigenvectors of @xmath71 corresponding to the @xmath42 largest eigenvalues .",
    "note that a conventional constraint is requiring @xmath57 to be orthonormal as in @xcite , which will lead to a generalized eigenvector problem .",
    "however , we find that this strategy is inferior to the proposed one in both adaptation accuracy and training speed in practice , so it is not used .    when computing @xmath72 , a proper kernel function needs to be selected .",
    "common kernel functions include linear ( @xmath73 ) , polynomial ( @xmath74 ) , gaussian radial basis function ( rbf , @xmath75 ) , and so on .",
    "different kernels indicate different assumptions on the type of dependence in using hsic @xcite . according to @xcite , the polynomial and rbf kernels map the original features to a higher or infinite dimensional space ,",
    "thus are able to detect more types of dependence . however , choosing a suitable kernel width parameter ( @xmath76 ) is also important for these more powerful kernels @xcite",
    ". when looking for a linear projection matrix to align the domains , first mapping the original features to a higher dimensional space may also be helpful , see the experiment on a synthetic dataset ( section [ subsec : synexp ] , fig .",
    "[ fig : synexp_nonlin ] ) for an example .    the maximum mean discrepancy ( mmd ) criterion is used in tca @xcite to measure the difference between two distributions",
    ". song et al .",
    "@xcite showed that when hsic and mmd are both applied to measure the dependence of features and labels in a binary - class classification problem , they are identical up to a constant factor if the label kernel matrix in hsic is properly designed .",
    "however , tca is feasible only when there are two discrete domains . on the other hand",
    ", mida can deal with a variety of situations including multiple domains and continuous distributional change . the stationary subspace analysis ( ssa ) algorithm @xcite is able to identify temporally stationary components in multivariate time series .",
    "however , ssa only ensures that the mean and covariance of the components are stationary , while they may not be suitable for preserving important properties in data .",
    "concept drift adaptation algorithms @xcite are able to correct continuous time - varying drift .",
    "however , most of them rely on newly arrived labeled data to update the prediction models , while mida works unsupervisedly .",
    "mida aligns samples with different backgrounds without considering the label information .",
    "however , if the labels of some samples are known , they can be incorporated into the subspace learning process , which may be beneficial for prediction .",
    "therefore , we extend mida to semi - supervised mida ( smida ) . since we do not explicitly differentiate the domain labels of the samples , both unlabeled and labeled samples can exist in any domain .",
    "similar to @xcite , hsic is adopted to maximize the dependence between the projected features and the labels .",
    "the biggest advantage of this strategy is that all types of labels can be exploited , such as the discrete labels in classification and the continuous ones in regression .",
    "the label matrix @xmath2 is defined as follows .",
    "for @xmath77-class classification problems , the one - hot coding scheme can be used , i.e.  @xmath78 if @xmath79 is labeled and belongs to the @xmath35th class ; 0 otherwise . for regression problems , the target values can be centered first .",
    "then , @xmath80 equals to the target value of @xmath79 if it is labeled ; 0 otherwise .",
    "the linear kernel function is chosen for the label kernel matrix , i.e. @xmath81    the objective of smida is @xmath82 where @xmath83 is a trade - off hyper - parameter .",
    "its solution is the eigenvectors of @xmath84 corresponding to the @xmath42 largest eigenvalues .",
    "the outline of mida and smida is summarized in algorithm [ algo : smida ] .",
    "the statements in brackets correspond to those specialized for smida .",
    "the matrix of all samples @xmath1 and their background information ; [ the labels of some samples ] ; the kernel function for @xmath1 ; @xmath85 $ ] .",
    "the projected samples @xmath86 .",
    "construct the domain features according to the background information , e.g.  section [ subsec : domainft ] .",
    "augment the original features with the domain features ( [ eq : domainft ] ) .    compute the kernel matrices",
    "@xmath87 $ ] .",
    "obtain @xmath43 , namely the eigenvectors of @xmath71 @xmath88 $ ] corresponding to the @xmath42 largest eigenvalues .",
    "@xmath89 .",
    "besides variance and label dependence , another useful property of data is the geometry structure , which can be preserved by manifold regularization ( mr ) @xcite . the manifold structure is modeled by a data adjacency graph .",
    "mr can be conveniently incorporated into smida by adding a regularizer @xmath90 into ( [ eq : smidaobj ] ) , where @xmath91 is the graph laplacian matrix @xcite , @xmath92 is a trade - off hyper - parameter . in our experiments , adding mr generally increases the accuracy slightly .",
    "however , it also brings three more hyper - parameters , including @xmath93 , the number of nearest neighbors , and the kernel width when computing the data adjacency graph .",
    "consequently , the experimental results in the next section were obtained without mr .",
    "it can still be an option in applications where geometry structure is important .",
    "in this section , we first conduct experiments on some synthetic datasets to verify the effect of the proposed methods . then , calibration transfer and drift correction experiments are performed on two e - nose datasets and a spectroscopy dataset . to show the universality of the proposed methods , we further evaluate them on a visual object recognition dataset .",
    "comparison is made between them and recent unsupervised feature - level domain adaptation algorithms .      in fig .",
    "[ fig : synexp_twodomain ] , tca @xcite and mida are compared on a 2d dataset with two discrete domains . for both methods ,",
    "the linear kernel was used on the original features and the hyper - parameter @xmath94 was set to 1 . in order to quantitatively assessing the effect of domain adaptation , logistic regression models were trained on the labeled source data and tested on the target data .",
    "the accuracies are displayed in the caption , showing that the order of performance is mida @xmath95 tca @xmath95 original feature .",
    "tca aligns the two domains only on the first projected dimension .",
    "however , we can find that the two classes have large overlap on that dimension .",
    "this is because the direction for alignment is different from that for discrimination . incorporating the label information of the source domain ( sstca ) did no help . on the contrary",
    ", mida can align the two domains well in both projected dimensions , in which the domain - specific bias on the second dimension brought by feature augmentation played a key role .",
    "thus , good accuracy can be obtained by using the two dimensions for classification .    in fig .",
    "[ fig : synexp_time ] , ssa @xcite and mida are compared on a 2d dataset with continuous distributional change , which resembles time - varying drift in machine olfaction .",
    "the chronological order of a sample is indicated by color ranging from blue to red .",
    "samples in both classes drift to the upper right .",
    "the parameter setting of mida was the same with those in fig .",
    "[ fig : synexp_twodomain ] , whereas the number of stationary components in ssa was set to 1 .",
    "the classification accuracies were obtained by training a logistic regression model on the first halves of the data in both classes , and testing them on the last halves .",
    "ssa succeeds in finding a direction ( @xmath96 ) that is free from time - varying drift ( @xmath97 ) .",
    "however , the two classes can not be well separated in that direction . in plot ( c ) , the randomly scattered colors suggest that the time - varying drift is totally removed in the subspace .",
    "mida first mapped the 2d data into a 3d space with the third dimension being time .",
    "then , the augmented data were projected to a 2d plane that is orthogonal to the direction of drift in the 3d space .",
    "the projection direction was decided so that the independence of the projected data and time is maximized , meanwhile class separation was achieved by properly exploiting the background information .",
    "no label information was used in the last two experiments .",
    "if keeping the label dependence in the subspace is a priority , smida can be adopted instead of mida . in the 3d synthetic dataset in fig .",
    "[ fig : synexp_label ] , the best direction to align the two domains ( @xmath98 ) also mixes the two classes , which results in the output of mida in plot ( b ) . for smida ,",
    "the weights for variance ( @xmath94 ) and label dependence ( @xmath99 ) were both set to 1 .",
    "the labels in the source domain were used when learning the subspace . from plot ( c )",
    ", we can observe that the classes are separated .",
    "in fact , class separation can still be found in the third dimension of the space learned by mida .",
    "however , for the purpose of dimensionality reduction , we generally hope to keep the important information in the first few dimensions .",
    "nonlinear kernels are often applied in machine learning algorithms when data is not linearly separable . besides , they are also useful in domain adaptation when domains are not linearly `` alignable '' , as shown in fig .",
    "[ fig : synexp_nonlin ] .",
    "as can be found in plot ( a ) , the inter - domain changes in distributions are different for the two classes .",
    "hence , it is difficult to find a linear projection direction to align the two domains , even with the domain - specific biases of mida .",
    "actually , domain - specific rotation matrices are needed .",
    "since the target labels are not available , the rotation matrices can not be obtained accurately . however",
    ", a nonlinear kernel can be used to map the original features to a space with higher dimensions , in which the domains may be linearly alignable .",
    "we applied an rbf kernel with width @xmath100 .",
    "although the domains are not perfectly aligned in plot ( c ) , the classification model trained in the source domain can be better adapted to the target domain .",
    "the gas sensor array drift dataset collected by vergara et al .",
    "@xcite is dedicated to research in drift correction .",
    "a total of 13910 samples were collected by an e - nose with 16 gas sensors over a course of 36 months .",
    "there are six different kinds of gases ( ammonia , acetaldehyde , acetone , ethylene , ethanol , and toluene ) at different concentrations .",
    "they were split into 10 batches by the authors according to their acquisition time .",
    "table  [ tbl : ucids ] shows the period of collection and the number of samples of different type of gases in each batch .",
    "we aim to classify the type of gases , despite their concentrations .    [ cols=\"<,<,<,<,<,<,<,<,<\",options=\"header \" , ]     for gfk , low - rank transfer subspace learning ( ltsl ) , and domain adaptation by shifting covariance ( dasc ) , we copied the best results reported in the original papers @xcite , @xcite , @xcite . for other methods tested ,",
    "the hyper - parameters were tuned for the best accuracy .",
    "logistic regression was adopted as the classifier .",
    "the polynomial kernel with degree 2 was used in kpca , tca , sstca , mida , and smida .",
    "mida and smida achieve the best average accuracies in both unsupervised and semi - supervised visual object recognition experiments .",
    "we observe that tca and sstca have comparable performance with mida and smida , which may be explained by the fact that the hsic criterion used in mida and mmd used in tca are identical under certain conditions when there are one source and one target domain @xcite .",
    "besides , the feature augmentation strategy in mida is not crucial in this dataset because there is no change in conditional probability . on the other hand , tca and sstca can only handle one source and one target domains .",
    "sstca uses the manifold regularization strategy to preserve local geometry information , hence introduces three more hyper - parameters than smida .",
    "moreover , computing the data adjacency graph in sstca and the matrix inversion operation in tca and sstca make them slower than mida and smida .",
    "we compared their speed on the domain adaptation experiment ca .",
    "they were run on a server with intel xeon 2.00 ghz cpu and 128 gb ram .",
    "no parallel computing was used .",
    "the codes of the algorithms were written in matlab r2014a .",
    "on average , the running times of each trial of mida , smida , tca , and sstca were 2.4 s , 2.5 s , 3.0 s , and 10.2 s , respectively .",
    "therefore , mida and smida are more practical to use than tca and sstca .",
    "besides , they were initially designed for calibration transfer and drift correction .",
    "this dataset is used to show their universality .",
    "in this paper , we introduced maximum independence domain adaptation ( mida ) to learn domain - invariant features .",
    "the main idea of mida is to reduce the discrepancy among domains by maximizing the independence between the learned features and the domain features of the samples .",
    "the domain features describe the background information of each sample , such as the domain label in traditional domain adaptation problems . in the field of sensors and measurement ,",
    "the device label and acquisition time of the each collected sample can be expressed by the domain features , so that unsupervised calibration transfer and drift correction can be achieved by using mida .",
    "the feature augmentation strategy proposed in this paper adds domain - specific biases to the learned features , which helps mida to align domains .",
    "it is also useful when there is a change in conditional probability . finally , to incorporate the label information , semi - supervised mida ( smida ) is further presented .",
    "mida and smida are flexible algorithms . with the design of the domain features and the use of the hsic criterion",
    ", they can be applied in all kinds of domain adaptation problems , including discrete or continuous distributional change , supervised / semi - supervised / unsupervised , multiple domains , classification or regression , etc .",
    "thus , they have a wide range of potential applications .",
    "they are also easy to implement and fast , requiring to solve only one eigenvalue decomposition problem .",
    "experimental results on various types of datasets proved their effectiveness .",
    "although initially designed for calibration transfer and drift correction in machine olfaction , they also performed well in spectroscopy and vision datasets .",
    "future directions may include further extending the definition of the domain features for other applications . besides",
    ", this paper mainly focus on reducing the difference of the marginal distribution @xmath101 among domains , while it may be better if the conditional distribution @xmath0 among domains can also be approximately aligned @xcite .",
    "the authors would like to thank the providers of the datasets used in this paper .",
    "l.  zhang , f.  tian , c.  kadri , b.  xiao , h.  li , l.  pan , and h.  zhou , `` on - line sensor calibration transfer among electronic nose instruments for monitoring volatile organic chemicals in indoor air quality , '' _ sens .",
    "actuators : b. chem .",
    "160 , no .  1 ,",
    "pp . 899909 , 2011 .",
    "k.  yan , d.  zhang , d.  wu , h.  wei , and g.  lu , `` design of a breath analysis system for diabetes screening and blood glucose level prediction , '' _ ieee trans .",
    "_ , vol .",
    "61 , no .  11 , pp . 27872795 , 2014 .",
    "s.  di  carlo and m.  falasconi , _ drift correction methods for gas chemical sensors in artificial olfaction systems : techniques and challenges_. 1em plus 0.5em minus 0.4emintech , 2012 , ch .",
    "305326 .",
    "b.  fernando , a.  habrard , m.  sebban , and t.  tuytelaars , `` unsupervised visual domain adaptation using subspace alignment , '' in _ proceedings of the ieee international conference on computer vision _ , 2013 , pp .",
    "29602967 .",
    "b.  gong , k.  grauman , and f.  sha , `` learning kernels for unsupervised domain adaptation with applications to visual object recognition , '' _ international journal of computer vision _ ,",
    "109 , no . 1 - 2 , pp .",
    "327 , 2014 .",
    "a.  gretton , o.  bousquet , a.  smola , and b.  schlkopf , `` measuring statistical dependence with hilbert - schmidt norms , '' in _ algorithmic learning theory_.1em plus 0.5em minus 0.4emspringer , 2005 , pp . 6377 .",
    "b.  gong , y.  shi , f.  sha , and k.  grauman , `` geodesic flow kernel for unsupervised domain adaptation , '' in _ computer vision and pattern recognition ( cvpr ) , 2012 ieee conference on_.1em plus 0.5em minus 0.4emieee , 2012 , pp .",
    "20662073 .",
    "e.  barshan , a.  ghodsi , z.  azimifar , and m.  z. jahromi , `` supervised principal component analysis : visualization , classification and regression on subspaces and submanifolds , '' _ pattern recogn . _ ,",
    "44 , no .  7 , pp .",
    "13571371 , 2011 ."
  ],
  "abstract_text": [
    "<S> when the distributions of the source and the target domains are different , domain adaptation techniques are needed . </S>",
    "<S> for example , in the field of sensors and measurement , discrete and continuous distributional change often exist in data because of instrumental variation and time - varying sensor drift . in this paper , we propose maximum independence domain adaptation ( mida ) to address this problem . </S>",
    "<S> domain features are first defined to describe the background information of a sample , such as the device label and acquisition time . </S>",
    "<S> then , mida learns features which have maximal independence with the domain features , so as to reduce the inter - domain discrepancy in distributions . a feature augmentation strategy is designed so that the learned projection is background - specific . </S>",
    "<S> semi - supervised mida ( smida ) extends mida by exploiting the label information . </S>",
    "<S> the proposed methods can handle not only discrete domains in traditional domain adaptation problems but also continuous distributional change such as the time - varying drift . </S>",
    "<S> in addition , they are naturally applicable in supervised / semi - supervised / unsupervised classification or regression problems with multiple domains . </S>",
    "<S> this flexibility brings potential for a wide range of applications . </S>",
    "<S> the effectiveness of our approaches is verified by experiments on synthetic datasets and four real - world ones on sensors , measurement , and computer vision .    </S>",
    "<S> submitted to ieee transactions on cybernetics    dimensionality reduction , domain adaptation , drift correction , hilbert - schmidt independence criterion , machine olfaction , transfer learning </S>"
  ]
}