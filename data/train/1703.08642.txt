{
  "article_text": [
    "blind deconvolution is the task of estimating two unknown functions from their convolution .",
    "while it is a highly ill - posed bilinear inverse problem , blind deconvolution is also an extremely important problem in signal processing  @xcite , communications engineering  @xcite , imaging processing  @xcite , audio processing  @xcite , etc . in this paper",
    ", we deal with an even more difficult and more general variation of the blind deconvolution problem , in which we have to extract multiple convolved signals mixed together in one observation signal .",
    "this joint blind deconvolution - demixing problem arises in a range of applications such as acoustics  @xcite , dictionary learning  @xcite , and wireless communications  @xcite .",
    "we briefly discuss one such application in more detail .",
    "blind deconvolution / demixing problems are expected to play a vital role in the future internet - of - things . the internet - of - things will connect billions of wireless devices , which is far more than the current wireless systems can technically and economically accommodate .",
    "one of the many challenges in the design of the internet - of - things will be its ability to manage the massive number of sporadic traffic generating devices which are most of the time inactive , but regularly access the network for minor updates with no human interaction  @xcite .",
    "this means among others that the overhead caused by the exchange of certain types of information between transmitter and receiver , such as channel estimation , assignment of data slots , etc , has to be avoided as much as possible .",
    "focusing on the underlying mathematical challenges , we consider a _ multi - user communication _ scenario where many different users / devices communicate with a common base station , as illustrated in figure  [ fig : demix ] .",
    "suppose we have @xmath2 users and each of them sends a signal @xmath3 through an unknown channel ( which differs from user to user ) to a common base station , .",
    "we assume that the @xmath4-th channel , represented by its impulse response @xmath5 , does not change during the transmission of the signal @xmath3 .",
    "therefore @xmath5 acts as convolution operator , i.e. , the signal transmitted by the @xmath4-th user arriving at the base station becomes @xmath6 , where  @xmath7 \" denotes convolution .",
    "users sends a signal @xmath3 through an unknown channel @xmath5 to a common base station .",
    "the base station measures the superposition of all those signals , namely , @xmath8 ( plus noise ) .",
    "the goal is to extract all pairs of @xmath9 simultaneously from @xmath10.,width=529 ]    the antenna at the base station , instead of receiving each individual component @xmath11 , is only able to record the superposition of all those signals , namely , @xmath12 where @xmath13 represents noise .",
    "we aim to develop a fast algorithm to simultaneously extract all pairs @xmath9 from @xmath10 ( i.e. , estimating the channel / impulse responses @xmath5 and the signals @xmath3 jointly ) in a numerically efficient and robust way , while keeping the number of required measurements as small as possible .",
    "a thorough theoretical analysis concerning the solvability of demixing problems via convex optimization can be found in  @xcite .",
    "there , the authors derive explicit sharp bounds and phase transitions regarding the number of measurements required to successfully demix structured signals ( such as sparse signals or low - rank matrices ) from a single measurement vector . in principle",
    "we could recast the blind deconvolution / demixing problem as the demixing of a sum of rank - one matrices , see  .",
    "as such , it seems to fit into the framework analyzed by mccoy and tropp .",
    "however , the setup in  @xcite differs from ours in a crucial manner .",
    "mccoy and tropp consider as measurement matrices ( see the matrices @xmath14 in  ) full - rank random matrices , while in our setting the measurement matrices are rank - one .",
    "this difference fundamentally changes the theoretical analysis .",
    "the findings in  @xcite are therefore not applicable to the problem of joint blind deconvolution / demixing .",
    "the compressive principal component analysis in  @xcite is also a form of demixing problem , but its setting is only vaguely related to ours .",
    "there is a large amount of literature on demixing problems , but the vast majority does not have a `` blind deconvolution component '' , therefore this body of work is only marginally related to the topic of our paper .",
    "blind deconvolution / demixing problems also appear in convolutional dictionary learning , see e.g.  @xcite .",
    "there , the aim is to factorize an ensemble of input vectors into a linear combination of overcomplete basis elements which are modeled as shift - invariant  the latter property is why the factorization turns into a convolution .",
    "the setup is similar to  , but with an additional penalty term to enforce sparsity of the convolving filters . the existing literature on convolutional dictionary learning is mainly focused on empirical results , therefore there is little overlap with our work .",
    "but it is an interesting challenge for future research to see whether the approach in this paper can be modified to provide a fast and theoretically sound solver for the sparse convolutional coding problem .",
    "there are numerous papers concerned with blind deconvolution / demixing problems in the area of wireless communications .",
    "but the majority of these papers assumes the availability of multiple measurement vectors , which makes the problem significantly easier .",
    "those methods however can not be applied to the case of a single measurement vector , which is the focus of this paper .",
    "thus there is essentially no overlap of those papers with our work .",
    "our previous paper  @xcite solves   under subspace conditions , i.e. , assuming that both @xmath5 and @xmath3 belong to known linear subspaces .",
    "this contributes to generalizing the pioneering work by ahmed , recht , and romberg  @xcite from the  single - user \" scenario to the  multi - user \" scenario .",
    "both  @xcite and @xcite employ a two - step convex approach : first  lifting \"  @xcite is used and then the lifted version of the original bilinear inverse problems is relaxed into a semi - definite program .",
    "an improvement of the theoretical bounds in  @xcite was announced in  @xcite .",
    "while the convex approach is certainly effective and elegant , it can hardly handle large - scale problems .",
    "this motivates us to apply a nonconvex optimization approach  @xcite to this blind - deconvolution - blind - demixing problem .",
    "the mathematical challenge , when using non - convex methods , is to derive a rigorous convergence framework with conditions that are competitive with those in a convex framework .    in the last few years",
    "several excellent articles have appeared on provably convergent nonconvex optimization applied to various problems in signal processing and machine learning , e.g. , matrix completion  @xcite , phase retrieval  @xcite , blind deconvolution  @xcite , dictionary learning  @xcite and low - rank matrix recovery  @xcite . in this paper",
    "we derive the first nonconvex optimization algorithm to solve   fast and with rigorous theoretical guarantees concerning exact recovery , convergence rates , as well as robustness for noisy data .",
    "our work can be viewed as a generalization of blind deconvolution  @xcite @xmath15 to the multi - user scenario @xmath16 .    the idea behind our approach is strongly motivated by the nonconvex optimization algorithm for phase retrieval proposed in  @xcite .",
    "in this foundational paper , the authors use a two - step approach : ( i ) construct a good initial guess with a numerically efficient algorithm ; ( ii ) starting with this initial guess , prove that simple gradient descent will converge to the true solution .",
    "our paper follows a similar two - step scheme .",
    "however , the techniques used here are quite different from  @xcite . like the matrix completion problem  @xcite , the performance of the algorithm relies heavily and inherently on how much the ground truth signals are aligned with the design matrix . due to this so - called  incoherence \" issue",
    ", we need to impose extra constraints , which results in a different construction of the so - called  _ basin of attraction_. therefore , influenced by  @xcite , we add penalty terms to control the incoherence and this leads to the regularized gradient descent method , which forms the core of our proposed algorithm .    to the best of our knowledge ,",
    "our algorithm is the first algorithm for the blind deconvolution / blind demixing problem that is numerically efficient , robust against noise , and comes with rigorous recovery guarantees .      for a matrix @xmath17",
    ", @xmath18 denotes its operator norm and @xmath19 is its the frobenius norm . for a vector @xmath20",
    ", @xmath21 is its euclidean norm and @xmath22 is the @xmath23-norm . for both matrices and vectors , @xmath24 and",
    "@xmath25 denote their complex conjugate transpose .",
    "@xmath26 is the complex conjugate of @xmath20 .",
    "we equip the matrix space @xmath27 with the inner product defined by @xmath28 for a given vector @xmath20 , @xmath29 represents the diagonal matrix whose diagonal entries are @xmath20 . for any @xmath30 , let @xmath31",
    "obviously , without any further assumption , it is impossible to solve  . therefore , we impose the following subspace assumptions throughout our discussion  @xcite .    * * channel subspace assumption : * each finite impulse response @xmath32 is assumed to have _ maximum delay spread _ @xmath33 , i.e. , @xmath34 * * signal subspace assumption : * let @xmath35 be the outcome of the signal @xmath36 encoded by a matrix @xmath37 with @xmath38 , where the encoding matrix @xmath39 is known and assumed to have full rank instead of @xmath40 because it will simplify our notation in later derivations . ] .",
    "both subspace assumptions are common in various applications .",
    "for instance in wireless communications , the channel impulse response can always be modeled to have finite support ( or maximum delay spread , as it is called in engineering jargon ) due to the physical properties of wave propagation  @xcite ; and the signal subspace assumption is a standard feature found in many current communication systems )  @xcite , including cdma ( where @xmath39 is known as spreading matrix ) and ofdm ( where @xmath39 is known as precoding matrix .",
    "the specific choice of the encoding matrices @xmath39 depends on a variety of conditions . in this paper",
    ", we derive our theory by assuming that @xmath39 is a complex gaussian random matrix , i.e. , each entry in @xmath39 is i.i.d .",
    "this assumption , while sometimes imposed in the wireless communications literature , is somewhat unrealistic in practice , due to the lack of a fast algorithm to apply @xmath39 and due to storage requirements . in practice",
    "one would rather choose @xmath39 to be something like the product of a hadamard matrix and a diagonal matrix with random binary entries .",
    "we hope to address such more structured encoding matrices in our future research .",
    "our numerical simulations ( see section  [ s : numerics ] ) show no difference in the performance of our algorithm for either choice .    under the two assumptions above , the model actually has a simpler form in the  _ frequency _ domain .",
    "we assume throughout the paper that the convolution of finite sequences is circular convolution . by applying the discrete fourier transform dft ) to   along with the two assumptions , we have @xmath42 where @xmath43 is the @xmath44 normalized unitary dft matrix with @xmath45 .",
    "the noise is assumed to be additive white complex gaussian noise with @xmath46 where @xmath47 , and @xmath48 is the ground truth .",
    "we define @xmath49 and assume without loss of generality that @xmath50 and @xmath51 are of the same norm , i.e. , @xmath52 . in that way , @xmath53 actually is a measure of snr ( signal to noise ratio ) .",
    "let @xmath54 be the first @xmath33 nonzero entries of @xmath5 and @xmath55 be a low - frequency dft matrix ( the first @xmath33 columns of an @xmath44 unitary dft matrix ) .",
    "then a simple relation holds , @xmath56 we also denote @xmath57 and @xmath58 . due to the gaussianity , @xmath59 also possesses complex gaussian distribution and so does @xmath60 from now on , instead of focusing on the original model , we consider ( with a slight abuse of notation ) the following equivalent formulation throughout our discussion : @xmath61 where @xmath62 .",
    "our goal here is to estimate all @xmath63 from @xmath64 and @xmath65 . obviously , this is a bilinear inverse problem , i.e. , if all @xmath66 are given , it is a linear inverse problem ( the ordinary demixing problem ) to recover all @xmath67 , and vice versa .",
    "we note that there is a scaling ambiguity in all blind deconvolution problems that can not be resolved by any reconstruction method without further information .",
    "namely , if the pair @xmath68 is a solution then so is @xmath69 for any @xmath70 .",
    "therefore , when we talk about exact recovery in the following , then this is understood modulo such a trivial scaling ambiguity",
    ".    0.25 cm before proceeding to our proposed algorithm we introduce some notation to facilitate a more convenient presentation of our approach .",
    "let @xmath71 be the @xmath72-th column of @xmath73 and @xmath74 be the @xmath72-th column of @xmath75 . based on our assumptions",
    "the following properties hold : @xmath76 moreover , inspired by the well - known  _ lifting _ idea  @xcite , we define the useful matrix - valued linear operator @xmath77 and its adjoint @xmath78 by @xmath79 for each @xmath80 under canonical inner product over @xmath81 therefore ,   can be written in the following equivalent form @xmath82 hence , we can think of @xmath10 as the observation vector obtained from taking _ linear _ measurements with respect to a set of rank-1 matrices @xmath83 in fact , with a bit of linear algebra ( and ignoring the noise term for the moment ) , the @xmath72-th entry of @xmath10 in   equals the inner product of two block - diagonal matrices : @xmath84 where @xmath85 in other words , we aim to recover such a block - diagonal matrix ( the left - hand side in the inner product  ) from @xmath86 linear measurements with block structure if @xmath87    by stacking all @xmath88 ( and @xmath89 ) into a long column , we let @xmath90 we define @xmath91 as a bilinear operator which maps a pair @xmath92 into a block diagonal matrix in @xmath93 , i.e. , @xmath94 let @xmath95 and @xmath96 where @xmath97 is the ground truth .",
    "define @xmath98 as @xmath99 where @xmath100 therefore , @xmath101 and @xmath102 the adjoint operator @xmath103 is defined naturally as @xmath104 which is a linear map from @xmath105 to @xmath106 to measure the approximation error of @xmath97 given by @xmath107 , we define @xmath108 as the global relative error : @xmath109 where @xmath110 is the relative error within each component : @xmath111 note that @xmath112 and @xmath113 are functions of @xmath114 and @xmath115 respectively and in most cases , we just simply use @xmath112 and @xmath113 if no possibility of confusion exists .",
    "as indicated in  , joint blind deconvolution - demixing can be recast as the task to recover a rank-@xmath2 block - diagonal matrix from linear measurements . in general ,",
    "such a low - rank matrix recovery problem is np - hard . in order to take advantage of the low - rank property of the ground truth , it is natural to adopt convex relaxation by solving a convenient nuclear norm minimization program , i.e. , @xmath116    the question of when the solution of   yields exact recovery is answered in our previous work  @xcite , whose main theoretical result is informally summarized in the following theorem .",
    "[ thm : convex ] suppose that @xmath59 are @xmath117 i.i.d .",
    "complex gaussian matrices and @xmath118 is an @xmath119 partial dft matrix with @xmath120 .",
    "then solving   gives exact recovery if the number of measurements @xmath86 yields @xmath121 with probability at least @xmath122 where @xmath123 is an absolute scalar only depending on @xmath124 linearly .",
    "numerical simulations in  @xcite show that the semidefinite program ( sdp ) in   is able to estimate all pairs of @xmath63 even when @xmath86 is very close to @xmath125 , i.e. , the degree of freedom for the unknowns , although @xmath86 depends on @xmath2 quadratically in our theory .",
    "however , the computational cost for solving an sdp already become challenging for moderate size problems and too expensive for large scale problems .",
    "therefore , we try to look for a more efficient nonconvex approach , which hopefully is also reinforced by theory .",
    "it seems quite natural to achieve the goal by minimizing the following  _ nonlinear _ least squares objective function with respect to @xmath126 @xmath127 in particular , if @xmath128 we write @xmath129 as also pointed out in  @xcite , this is a highly nonconvex optimization problem .",
    "many of the commonly used algorithms , such as gradient descent or alternating minimization , may not necessarily yield convergence to the global minimum , so that we can not always hope to obtain the desired solution .",
    "often , those simple algorithms might get stuck in local minima .",
    "motivated by several excellent recent papers of nonconvex optimization on various signal processing and machine learning problem , we propose our two - step algorithm : ( i )  compute an initial guess carefully ; ( ii )  apply gradient descent to the objective function , starting with the carefully chosen initial guess .",
    "one difficulty of understanding nonconvex optimization consists in how to construct the so - called  _ basin of attraction _ , i.e. , if the starting point is inside this basin of attraction , the iterates will always stay inside the region and converge to the global minimum .",
    "the construction of the basin of attraction varies for different problems  @xcite . for this problem ,",
    "similar to  @xcite , the construction follows from the following three observations .",
    "each of these observations suggests the definition of a certain _ neighborhood _ and the basin of attraction is then defined as the intersection of these three neighborhood sets @xmath130    1 .",
    "* ambiguity of solution * : in fact , we can only recover @xmath115 up to a scalar since @xmath131 and @xmath115 are both solutions for @xmath132 . from a numerical perspective , we want to avoid the scenario when @xmath133 and @xmath134 while @xmath135 is fixed , which potentially leads to numerical instability . to balance both the norm of @xmath136 and @xmath137 for all @xmath80 , we define @xmath138 which is a convex set .",
    "* incoherence * : the performance depends on how large / small the incoherence @xmath139 is , where @xmath140 is defined by @xmath141 the idea is that :  _ the smaller the @xmath139 is , the better the performance is .",
    "_ let s consider an extreme case : if @xmath142 is highly sparse or spiky , we lose much information on those zero / small entries and can not hope to get satisfactory recovered signals .",
    "+ a similar quantity is also introduced in the matrix completion problem  @xcite .",
    "the larger @xmath139 is , the more @xmath143 is aligned with one particular row of @xmath144 to control the incoherence between @xmath145 and @xmath146 , we define the second neighborhood , @xmath147 where @xmath148 is a parameter and @xmath149 .",
    "note that @xmath150 is also a convex set .",
    "* close to the ground truth * : we also want to construct an initial guess such that it is close to the ground truth , i.e. , @xmath151 where @xmath152 is a predetermined parameter in @xmath153 $ ] .    to ensure @xmath154 , it suffices to ensure @xmath155 where @xmath156 .",
    "this is because @xmath157 which implies @xmath158    when we say @xmath159 or @xmath160 , it means for all @xmath161 we have @xmath162 , @xmath150 or @xmath160 respectively . in particular , @xmath163      to implement the first two observations , we introduce the regularizer @xmath164 , defined as the sum of @xmath2 components @xmath165 for each component @xmath166 , we let @xmath167 , @xmath168 , @xmath169 for all @xmath80 and @xmath170,\\end{aligned}\\ ] ] where @xmath171 . here",
    "both @xmath172 and @xmath173 are data - driven and well approximated by our spectral initialization procedure ; and @xmath174 is a tuning parameter which could be estimated if we assume a specific statistical model for the channel ( for example , in the widely used rayleigh fading model , the channel coefficients are assumed to be complex gaussian ) .",
    "the idea behind @xmath175 is quite straightforward though the formulation is complicated . for each @xmath175 in",
    ", the first two terms try to force the iterates to lie in @xmath176 and the third term tries to force the iterates to lie in @xmath177 what about the neighborhood @xmath160 ?",
    "a proper choice of the initialization along with gradient descent ( keeping the objective function decrease ) will ensure that the iterates lie in @xmath160 .",
    "0.25 cm finally , we consider the objective function as the sum of nonlinear least squares objective function @xmath178 in   and the regularizer @xmath179 , @xmath180    note that the input of the function @xmath181 consists of complex variables , but the output is real - valued ( so do @xmath178 and @xmath179 ) and thus simple relations hold @xmath182    therefore , to minimize this function , it suffices to consider only the gradient of @xmath183 with respect to @xmath184 and @xmath185 , which is also called wirtinger derivative  @xcite .",
    "the wirtinger derivatives of @xmath178 and @xmath179 w.r.t .",
    "@xmath184 and @xmath185 can be easily computed as follows @xmath186 , \\label{eq : wgh } \\\\",
    "\\nabla g_{{\\boldsymbol{x}}_i } & = \\frac{\\rho}{2d_i } g'_0\\left ( \\frac{\\|{\\boldsymbol{x}}_i\\|^2}{2d_i}\\right ) { \\boldsymbol{x}}_i , \\label{eq : wgx}\\end{aligned}\\ ] ] where @xmath187 and @xmath103 is defined in  .",
    "in short , we denote @xmath188 similar definitions hold for @xmath189 and @xmath190 .",
    "it is easy to see that @xmath191 and @xmath192 .",
    "as mentioned before , the first step is to find a good initial guess @xmath193 such that it is inside the basin of attraction .",
    "the initialization follows from this key fact : @xmath194 where we use @xmath195 , @xmath196 and @xmath197 therefore , it is natural to extract the leading singular value and associated left and right singular vectors from each @xmath198 and use them as ( a hopefully good ) approximation to @xmath199 this idea leads to algorithm  [ initial ] , the proof of which is given in section  [ s : init ] .",
    "the second step of the algorithm is just to apply gradient descent to @xmath183 with the initial guess @xmath200 or @xmath201 , where @xmath202 stems from stacking all @xmath203 into one long vector .",
    "compute @xmath204 find the leading singular value , left and right singular vectors of @xmath198 , denoted by @xmath205 .",
    "solve the following optimization problem for @xmath80 : @xmath206 set @xmath207 .",
    "output : @xmath200 or @xmath208 .",
    "obtain @xmath208 via algorithm  [ initial ] .",
    "@xmath209 , @xmath210 ,    for algorithm  [ agd ] , we can rewrite each iteration into @xmath211 where @xmath212 and @xmath213 are in  , and @xmath214      our main findings are summarized as follows : theorem  [ thm : init ] shows that the initial guess given by algorithm  [ initial ] indeed belongs to the basin of attraction .",
    "moreover , @xmath215 also serves as a good approximation of @xmath216 for each @xmath4 .",
    "theorem  [ thm : main ] demonstrates that the regularized wirtinger gradient descent will guarantee the linear convergence of the iterates and the recovery is exact in the noisefree case and stable in the presence of noise .",
    "[ thm : init ] the initialization obtained via algorithm  [ initial ] satisfies @xmath217 and @xmath218 holds with probability at least @xmath219 if the number of measurements satisfies @xmath220 here @xmath152 is any predetermined constant in @xmath153 $ ] , and @xmath123 is a constant only linearly depending on @xmath124 with @xmath221 .",
    "[ thm : main ] starting with the initial value @xmath222 satisfying  , the algorithm  [ agd ] creates a sequence of iterates @xmath223 which converges to the global minimum linearly , @xmath224 with probability at least @xmath219 and @xmath225 if the number of measurements @xmath86 satisfies @xmath226 in particular , with probability at least @xmath219 , there holds @xmath227    our previous work  @xcite shows that the convex approach via semidefinite programming ( see  ) requires @xmath228 to ensure exact recovery . later ,  @xcite claimed to improve this result to the near - optimal bound @xmath229 up to some @xmath230-factors .",
    "the difference between nonconvex and convex methods lies in the appearance of the condition number @xmath231 in  .",
    "this is not just an artifact of the proof  empirically we also observe that the value of @xmath231 affects the convergence rate of our nonconvex algorithm , see figure  [ fig : snr - kappa ] .",
    "our theory suggests @xmath232-dependence for the number of measurements @xmath86 , although numerically @xmath86 in fact depends on @xmath2 linearly , as shown in section  [ s : numerics ] .",
    "the reason for @xmath232-dependence will be addressed in details in section  [ s : outline ] .    in the theoretical analysis",
    ", we assume that @xmath233 is a gaussian random matrix .",
    "numerical simulations suggest that this assumption is clearly not necessary .",
    "for example , @xmath39 may be chosen to be a hadamard - type matrix which is more appropriate and favorable for communications .    if @xmath128   shows that @xmath223 converges to the ground truth at a linear rate .",
    "on the other hand , if noise exists , @xmath223 is guaranteed to converge to a point within a small neighborhood of @xmath234 more importantly , if the number of measurements @xmath86 gets larger , @xmath235 decays at the rate of @xmath236 .",
    "in this section we present a range of numerical simulations to illustrate and complement different aspects of our theoretical framework . we will empirically analyze the number of measurements needed for perfect joint deconvolution / demixing to see how this compares to our theoretical bounds .",
    "we will also study the robustness for noisy data . in our simulations we use gaussian encoding matrices , as in our theorems .",
    "but we also more more realistic structured encoding matrices , that are more reminiscent of what one might come across in wireless communications .    while theorem  [ thm : main ]",
    "says that the number of measurements @xmath86 depends _ quadratically _ on the number of sources @xmath2 , numerical simulations suggest near - optimal performance .",
    "figure  [ fig : l - vs - s - gaussian ] demonstrates that @xmath86 actually depends linearly on @xmath2 , i.e. , the boundary between success ( white ) and failure ( black ) is approximately a linear function of @xmath2 . in the experiment , @xmath237 are fixed , all @xmath59 are complex gaussians and all @xmath115 are standard complex gaussian vectors . for each pair of @xmath238 ,",
    "25 experiments are performed and we treat the recovery as a success if @xmath239 for our algorithm , we use backtracking to determine the stepsize and the iteration stops either if @xmath240 or if the number of iterations reaches 500 .",
    "the backtracking is based on the armijo - goldstein condition  @xcite .",
    "the initial stepsize is chosen to be @xmath241 . if @xmath242 , we just divide @xmath243 by two and use a smaller stepsize .",
    "we see from figure  [ fig : l - vs - s - gaussian ] that the number of measurements for the proposed algorithm to succeed not only seems to depend linearly on the number of sensors , but it is actually rather close to the information - theoretic limit @xmath125 . indeed , the green dashed line in figure  [ fig : l - vs - s - gaussian ] , which represents the empirical boundary for the phase transition between success and failure corresponds to a line with slope about @xmath244 .",
    "it is interesting to compare this empirical performance to the sharp theoretical phase transition bounds one would obtain via convex optimization  @xcite .",
    "considering the convex approach based on lifting in  @xcite , we can adapt the theoretical framework in  @xcite to the blind deconvolution / demixing setting , but with one modification .",
    "the bounds in  @xcite rely on gaussian widths of tangent cones related to the measurement matrices @xmath14 . since simply analytic formulas for these expressions seem to be out of reach for the structured rank - one measurement matrices used in our paper , we instead compute the bounds for full - rank gaussian random matrices , which yields a sharp bound of about @xmath245 ( the corresponding bounds for rank - one sensing matrices will likely have a constant larger than 3 ) .",
    "note that these sharp theoretical bounds predict quite accurately the empirical behavior of convex methods .",
    "thus our empirical bound for using a non - convex methods compares rather favorably with that of the convex approach .     where @xmath246 are fixed .",
    "black region : failure ; white region : success .",
    "the red solid line depicts the number of degrees of freedom and the green dashed line shows the empirical phase transition bound for algorithm  [ agd].,width=680 ]    similar conclusions can be drawn from figure  [ fig : l - vs - s - hadamard ] ; there all @xmath59 are in the form of @xmath247 where @xmath43 is the unitary @xmath44 dft matrix , all @xmath248 are independent diagonal binary @xmath249 matrices and @xmath250 is an @xmath117 fixed partial deterministic hadamard matrix .",
    "the purpose of using @xmath248 is to enhance the incoherence between each channel so that our algorithm is able to tell apart each individual signal and channel .",
    "as before we assume gaussian channels , i.e. , @xmath251 therefore , our approach does not only work for gaussian encoding matrices @xmath59 but also for the matrices that are interesting to real - world applications , although no satisfactory theory has been derived yet for that case .",
    "moreover , due to the structure of @xmath59 and @xmath118 , fast transform algorithms are available , potentially allowing for real - time deployment .     when @xmath246 are fixed .",
    ", width=283 ]    figure  [ fig : snr - gaussian ] shows the robustness of our algorithm under different levels of noise .",
    "we also run 25 samples for each level of snr and different @xmath86 and then compute the average relative error .",
    "it is easily seen that the relative error scales linearly with the snr and one unit of increase in snr ( in db ) results in one unit of decrease in the relative error .",
    ".,width=283 ]    .,width=283 ]    theorem  [ thm : main ] suggests that the performance and convergence rate actually depend on the condition number of @xmath252 , i.e. , on @xmath253 where @xmath254 .",
    "next we demonstrate that this dependence on the condition number is not an artifact of the proof , but is indeed also observed empirically . in this experiment , we let @xmath255 and set for the first component @xmath256 and for the second one @xmath257 for @xmath258 . here",
    ", @xmath259 means that the received signals of both sensors have equal power , whereas @xmath260 means that the signal received from the second sensor is considerably stronger .",
    "the initial stepsize is chosen as @xmath261 , followed by the backtracking scheme .",
    "figure  [ fig : snr - kappa ] shows how the relative error decays with respect to the number of iterations @xmath262 under different condition number @xmath231 and @xmath263    the larger @xmath231 is , the slower the convergence rate is , as we see from figure  [ fig : snr - kappa ] .",
    "this may result from two reasons : our spectral initialization may not be able to give a good initial guess for those weak components ; moreover , during the gradient descent procedure , the gradient directions for the weak components could be totally dominated / polluted by the strong components .",
    "currently , we still have no effective way of how to deal with this issue of slow convergence when @xmath231 is not small .",
    "we have to leave this topic for future investigations .",
    ".,width=283 ]    .,width=283 ]",
    "our convergence analysis relies on the following four conditions where the first three of them are local properties .",
    "we will also briefly discuss how they contribute to the proof of our main theorem .",
    "note that our previous work  @xcite on blind deconvolution is actually a special case @xmath15 of  .",
    "therefore , the proof of theorem  [ thm : main ] follows in part the main ideas in  @xcite .",
    "however , there are still many key differences since we are now dealing with a more complicated scenario and thus many technical details are much more involved . during the presentation",
    ", we will clearly point out both the similarities to and differences from  @xcite .",
    "* local regularity condition:*[cond : reg ] let @xmath264 and @xmath265 , then @xmath266_+\\ ] ] for @xmath267 where @xmath268 and @xmath269    we will prove condition  [ cond : reg ] in section  [ s : lrc ] .",
    "condition  [ cond : reg ] tells that @xmath270 if @xmath271 and @xmath272 , i.e. , all the stationary points inside the basin of attraction are global minima .    * local smoothness condition:*[cond : smooth ] let @xmath273 and @xmath274 and there holds @xmath275 for @xmath276 and @xmath20 inside @xmath277 where @xmath278 the convergence rate is governed by @xmath279 .",
    "the proof of condition  [ cond : smooth ] can be found in section  [ s : smooth ] .",
    "* local restricted isometry property:*[cond : rip ] denote @xmath280 and @xmath281",
    ". there holds @xmath282 uniformly all for @xmath283 .",
    "condition  [ cond : rip ] will be proven in section  [ s : lrip ] .",
    "this condition says that the convergence of the objective function implies the convergence of the iterates .    *",
    "robustness condition:*[cond : robust ] let @xmath284 be a predetermined constant .",
    "we have @xmath285 where @xmath286 if @xmath287    we will prove condition  [ cond : robust ] in section  [ s : init ] .",
    "we now extract one useful result based on conditions  [ cond : rip ] and  [ cond : robust ] . from these two conditions , we are able to produce a good approximation of @xmath288 for all @xmath283 in terms of @xmath112 in  . for @xmath283 ,",
    "the following inequality holds @xmath289 note that   simply follows from @xmath290 note that   implies @xmath291 .",
    "thus it suffices to estimate the cross - term , @xmath292 where @xmath293 and @xmath294 are a pair of dual norms and @xmath235 comes from  .",
    "0.5 cm      for the ease of proof , we introduce another neighborhood : @xmath295 moreover , another reason to consider @xmath296 is based on the fact that gradient descent  _ only _ allows one to make the objective function decrease . in other words , all the iterates @xmath297 generated by gradient descent are inside @xmath296 as long as @xmath298    on the other hand , it is crucial to note that the decrease of the objective function does not necessarily imply the decrease of the relative error of the iterates .",
    "therefore , we want to construct an initial guess in @xmath299 so that @xmath300 is sufficiently close to the ground truth and then analyze the behavior of @xmath297 .",
    "0.25 cm    in the rest of this section , we basically try to prove the following relation : @xmath301    now we give a more detailed explanation of the relation above , which constitutes the main structure of the proof :    1 .",
    "we will show @xmath302 in the proof of theorem  [ thm : main ] in section  [ s : mainthm ] , which is quite straightforward .",
    "2 .   lemma  [ lem : betamu ] explains why it holds that @xmath303 and where the @xmath232-bottleneck comes from .",
    "lemma  [ lem : line_section ] implicitly tells us that the iterates @xmath297 will remain in @xmath304 if the initial guess @xmath300 is inside @xmath299 and @xmath305 is monotonically decreasing .",
    "lemma  [ lem : induction ] makes this observation explicit by showing that @xmath306 implies @xmath307 if the stepsize @xmath243 obeys @xmath308 .",
    "moreover , lemma  [ lem : induction ] guarantees sufficient decrease of @xmath305 in each iteration , which paves the road towards the proof of linear convergence of @xmath305 and thus @xmath309    0.25 cm remember that @xmath176 and @xmath150 are both convex sets , and the purpose of introducing regularizers @xmath310 is to approximately project the iterates onto @xmath311 moreover , we hope that once the iterates are inside @xmath160 and inside a sublevel subset @xmath296 , they will never escape from @xmath312 .",
    "those ideas are fully reflected in the following lemma .",
    "[ lem : betamu ] assume @xmath313 and @xmath314 , there holds @xmath315 ; moreover , under conditions  [ cond : rip ] and  [ cond : robust ] , we have @xmath316 .",
    "if @xmath317 , by the definition of @xmath318 in  , at least one component in @xmath318 exceeds @xmath319 .",
    "we have @xmath320 where @xmath167 , @xmath321 and @xmath322 this implies @xmath323 and hence @xmath315 .",
    "+ now we have @xmath324 if @xmath325 .",
    "applying   gives @xmath326 which implies that @xmath327 by definition of @xmath112 in  , there holds @xmath328 which gives @xmath329 and",
    "@xmath330    the @xmath232-bottleneck comes from  . if @xmath331 is small , we can not guarantee that each @xmath113 is also smaller than @xmath152 . just consider the simplest case when all @xmath216 are the same : then @xmath332 and there holds @xmath333 obviously , we can not conclude that @xmath334 but only say that @xmath335 this is why we require @xmath336 to ensure @xmath337 , which gives @xmath232-dependence in @xmath263    [ lem : line_section ] denote @xmath338 and @xmath339 .",
    "let @xmath340 .",
    "if @xmath341 and @xmath342 for all @xmath343 $ ] , we have @xmath344",
    ".    we prove it by contradiction based on @xmath345 in lemma  [ lem : betamu ] .",
    "suppose that @xmath346 and @xmath347 , and there exists @xmath348 for some @xmath349 $ ] , such that @xmath350 .",
    "therefore , @xmath351 and implies @xmath352 , which contradicts @xmath350 .",
    "[ lem : induction ] let the stepsize @xmath308 , @xmath353 and @xmath279 be the lipschitz constant of @xmath354 over @xmath277 in  .",
    "if @xmath306 , we have @xmath355 and @xmath356 where @xmath357",
    "this lemma tells us that once @xmath358 , the next iterate @xmath359 is also inside @xmath304 as long as the stepsize @xmath308 . in other words , @xmath304 is in fact a stronger version of the basin of attraction .",
    "moreover , the objective function will decay sufficiently in each step as long as we can control the lower bound of the @xmath360 , which is guaranteed by the local regularity condition  [ cond : rip ] .",
    "let @xmath361 , @xmath362 and consider the following quantity : @xmath363 where @xmath364 is the largest stepsize such that the objective function @xmath365 evaluated at any point over the whole line segment @xmath366 is not greater than @xmath305 .",
    "now we will show @xmath367 . obviously , if @xmath368 , it holds automatically .",
    "consider @xmath369 and assume @xmath370 .",
    "first note that , @xmath371 by the definition of @xmath364 , there holds @xmath372 since @xmath373 is a continuous function w.r.t .",
    "@xmath374 . lemma  [ lem : line_section ] implies @xmath375 now we apply lemma  [ lem : dsl ] , the modified descent lemma , and obtain @xmath376 where @xmath377 in other words , @xmath378 contradicts @xmath372 .",
    "therefore , we conclude that @xmath367 . for any @xmath308 , lemma  [ lem : line_section ] implies @xmath379 and applying lemma  [ lem : dsl ] gives @xmath380      combining all the considerations above , we now prove theorem  [ thm : main ] to conclude this section .",
    "the proof consists of three parts :    [ [ part - i - proof - of - boldsymbolz0-boldsymbolu0-boldsymbolv0-in - mathcaln_epsiloncapmathcaln_widetildef . ] ] part i : proof of @xmath381 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    from the assumption of theorem  [ thm : main ] , @xmath382 first we show @xmath383 : for @xmath384 and the definition of @xmath176 and @xmath150 , @xmath385 where @xmath386 , @xmath387 and @xmath388 therefore @xmath389 for all @xmath390 and @xmath391    for @xmath392 , we have @xmath393 by  , there holds @xmath394 and @xmath383 , @xmath395 and hence @xmath396    [ [ part - ii - the - linear - convergence - of - the - objective - function - widetildefboldsymbolzt . ] ] part ii : the linear convergence of the objective function @xmath305 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    denote @xmath397 note that @xmath398 , lemma  [ lem : induction ] implies @xmath399 for all @xmath400 by induction if @xmath308 .",
    "moreover , combining condition  [ cond : reg ] with lemma  [ lem : induction ] leads to @xmath401_+ , \\quad t\\geq 1\\ ] ] with @xmath402 and @xmath403 .",
    "therefore , by induction , we have @xmath404_+ \\leq ( 1 - \\eta\\omega )   \\left [   { \\widetilde{f}}({\\boldsymbol{z}}^{(t-1 ) } )   - c \\right]_+ \\leq   \\left(1 - \\eta\\omega\\right)^t \\left [ { \\widetilde{f}}({\\boldsymbol{z}}^{(0 ) } ) - c\\right]_+ \\leq \\frac{{\\varepsilon}^2 d_0 ^ 2}{3s\\kappa^2 } ( 1 - \\eta\\omega)^{t}\\ ] ] where @xmath405 and @xmath406_+ \\leq    \\left [ \\frac{1}{3s\\kappa^2}{\\varepsilon}^2 d_0 ^ 2 - a\\|{\\mathcal{a}}^*({\\boldsymbol{e}})\\|^2 \\right]_+ \\leq \\frac{{\\varepsilon}^2 d_0 ^ 2}{3s\\kappa^2}.$ ] now we conclude that @xmath407_+$ ] converges to @xmath408 linearly .    [ [ part - iii - the - linear - convergence - of - the - objective - function - widetildefboldsymbolzt . ] ] part iii : the linear convergence of the objective function @xmath305 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    denote @xmath409 note that @xmath410 and over @xmath277 , there holds @xmath411 which follows from local rip condition in   and @xmath412 defined in  .",
    "moreover @xmath413 where @xmath414 and the second inequality follows from  . there holds @xmath415_+ \\leq \\frac { { \\varepsilon}^2 d_0 ^ 2}{3s\\kappa^2}(1 - \\eta\\omega)^t\\ ] ] and equivalently , @xmath416 solving the inequality above for @xmath417 , we have @xmath418 where @xmath419",
    "let @xmath420 for @xmath421 by   and triangle inequality , we immediately obtain @xmath422",
    "this section is devoted to proving the four key conditions introduced in section  [ s : converge ] .",
    "the  _ local smoothness condition _ and the  _ robustness condition _ are relatively less challenging to deal with .",
    "the more difficult part is to show the  _ local regularity condition _ and the  _ local isometry property_. the key to solve those problems is to understand how the matrix - valued linear operator @xmath423 in   behaves on block - diagonal matrices , such as @xmath424 , @xmath425 and @xmath426 in particular , when @xmath427 , all those matrices become rank-1 matrices , which have been well discussed in our previous work  @xcite .    0.25 cm first of all , we define the linear subspace @xmath428 along with its orthogonal complement for @xmath80 as @xmath429 where @xmath430 in particular , @xmath431 for all @xmath80 .",
    "0.25 cm the proof also requires us to consider block - diagonal matrices whose @xmath4-th block belongs to @xmath432 ( or @xmath433 ) . let @xmath434 be a block - diagonal matrix and say @xmath435 if @xmath436 and @xmath437",
    "if @xmath438 where both @xmath439 and @xmath440 are subsets in @xmath93 and @xmath441    0.25 cm now we take a closer look at a special case of block - diagonal matrices , i.e. , @xmath442 and calculate its projection onto @xmath439 and @xmath440 respectively and it suffices to consider @xmath443 and @xmath444 . for each block @xmath445 and @xmath80",
    ", there are unique orthogonal decompositions @xmath446 where @xmath447 and @xmath448 .",
    "it is important to note that @xmath449 and @xmath450 and thus @xmath451 and @xmath452 are functions of @xmath146 and @xmath40 respectively .",
    "immediately , we have the following matrix orthogonal decomposition for @xmath445 onto @xmath432 and @xmath433 , @xmath453 where the first three components are in @xmath432 while @xmath454 .      from the decomposition in   and  ,",
    "we want to analyze how @xmath455 , @xmath456 , @xmath451 and @xmath452 depend on @xmath457 if @xmath458 .",
    "the following lemma answers this question , which can be viewed as an application of singular value / vector perturbation theory  @xcite applied to rank-1 matrices . from the lemma",
    "below , we can see that if @xmath445 is close to @xmath459 , then @xmath444 is in fact very small ( of order @xmath460 ) .",
    "* ( lemma 5.9 in  @xcite ) * [ lem : orth_decomp ] recall that @xmath52 .",
    "if @xmath461 , we have the following useful bounds @xmath462 and @xmath463 moreover , if @xmath464 and @xmath465 , i.e. , @xmath466 , we have @xmath467 .",
    "now we start to focus on several results related to the linear operator @xmath423 .    *",
    "( operator norm of @xmath423).*[lem : a - upbd ] for @xmath423 defined in  , there holds @xmath468 with probability at least @xmath469    note that @xmath470 in  .",
    "lemma 1 in  @xcite implies @xmath471 with probability at least @xmath472 by taking the union bound over @xmath80 , @xmath473 with probability at least @xmath474    0.25 cm    for @xmath423 defined in  , applying the triangle inequality gives @xmath475 where @xmath476 therefore , @xmath477 with probability at least @xmath122 .    0.25 cm    * ( restricted isometry property for @xmath423 on @xmath439 ) . *",
    "[ lem : ripu ] @xmath423 restricted on @xmath439 is well - conditioned ,",
    "i.e. , @xmath478 where @xmath479 is the projection operator from @xmath93 onto @xmath439 , given @xmath480 with probability at least @xmath469    here @xmath481 and @xmath482 are defined as @xmath483 respectively where @xmath17 is a block - diagonal matrix and @xmath484    from corollary 5.3 and 5.8 in  @xcite , we know that @xmath485 with probability at least @xmath219 if @xmath486    for any block diagonal matrix @xmath487 and @xmath488 , @xmath489 using  , the following two inequalities hold , @xmath490    finally , we show how @xmath423 behaves when applied to block - diagonal matrices @xmath491 .",
    "( * @xmath423 restricted on block - diagonal matrices with rank-1 blocks ) . *",
    "[ lem : key ]    consider @xmath280 and @xmath492 conditioned on  , we have @xmath493 uniformly for any @xmath494 and @xmath495 with probability at least @xmath496 if @xmath497 . here",
    "@xmath498    here are a few more explanations and facts about @xmath499 .",
    "note that @xmath500 is the sum of @xmath86 sub - exponential random variables , i.e. , @xmath501 here @xmath502 corresponds to the largest expectation of all those components in @xmath500 .    for @xmath502 , without loss of generality , we assume @xmath503 for @xmath80 and let @xmath494 be a unit vector , i.e. , @xmath504 .",
    "the bound @xmath505 follows from @xmath506    moreover , @xmath507 and @xmath508 are both lipschitz functions w.r.t .",
    "@xmath509 now we want to determine their lipschitz constants . first note that for @xmath503 , @xmath508 equals @xmath510 where ",
    "@xmath511 \" denotes kronecker product .",
    "let @xmath512 be another unit vector and we have @xmath513 where @xmath514 for @xmath515 @xmath516    without loss of generality , let @xmath503 and @xmath517 .",
    "it suffices to prove @xmath518 for all @xmath519 in   where @xmath520 is defined as @xmath521    [ [ part - i - bounds - of - mathcalaboldsymbolx2-for - any - fixed - boldsymbolhboldsymbolx . ] ] part i : bounds of @xmath500 for any fixed @xmath114 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    from  , we already know that @xmath522 where @xmath523 are i.i.d . @xmath524 random variables and @xmath525 .",
    "more precisely , we can determine @xmath526 as @xmath527 because @xmath528 .    by the bernstein inequality",
    ", there holds @xmath529 where @xmath530 in order to apply the bernstein inequality , we need to estimate @xmath531 and @xmath532 as follows , @xmath533 applying   gives @xmath534 in particular , by setting @xmath535 we have @xmath536 so far , we have shown that @xmath537 with probability at least @xmath538 for a fixed pair of @xmath539    [ [ part - ii - covering - argument . ] ] part ii : covering argument .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    now we will use a covering argument to extend this result for all @xmath126 and thus prove that @xmath540 uniformly for all @xmath126 .",
    "we start with defining @xmath541 and @xmath542 as @xmath543-nets of @xmath544 and @xmath545 for @xmath546 and @xmath547 , respectively .",
    "the bounds @xmath548 and @xmath549 follow from the covering numbers of the sphere ( lemma 5.2 in  @xcite ) . here",
    "we let @xmath550 by taking the union bound over @xmath551 we have that @xmath552 holds uniformly for all @xmath553 with probability at least @xmath554 for any @xmath555 , we can find a point @xmath556 satisfying @xmath557 and @xmath558 for all @xmath80 . conditioned on  , we know that @xmath559    now we aim to evaluate @xmath560 .",
    "first we consider @xmath561 .",
    "since @xmath562 if @xmath563 for @xmath80 , we have @xmath564 where the first inequality is due to @xmath565 for any @xmath566 .",
    "we proceed to estimate @xmath567 by using   and  , @xmath568 where   and   give @xmath569    0.25 cm therefore , if @xmath570 , there holds @xmath571 for all @xmath114 uniformly with probability at least @xmath572 by letting @xmath573 with @xmath123 reasonably large and @xmath221 , we have @xmath574 and with probability at least @xmath575 .      [ lem : rip ] conditioned on   and  , the following rip type of property holds : @xmath576 uniformly for all @xmath577 with @xmath149 and @xmath578 if @xmath579 for some numerical constant @xmath123 .",
    "the main idea of the proof follows two steps : decompose @xmath580 onto @xmath439 and @xmath440 , then apply   and   to @xmath581 and @xmath582 respectively .",
    "0.25 cm for any @xmath583 with @xmath584 , we can decompose @xmath585 as the sum of two block diagonal matrices @xmath586 and @xmath587 where each pair of @xmath588 corresponds to the orthogonal decomposition of @xmath589 , @xmath590 which has been briefly discussed in   and  .",
    "note that @xmath591 and @xmath592 therefore , it suffices to have a two - side bound for @xmath593 and an upper bound for @xmath594 where @xmath595 and @xmath596 in order to establish the local isometry property .",
    "[ [ estimation - of - mathcalaboldsymbolu ] ] estimation of @xmath593 : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for @xmath593 , we know from   that @xmath597 and hence we only need to compute @xmath598 by , there also hold @xmath599 and @xmath600 , i.e. , @xmath601 with @xmath602 , it is easy to get @xmath603 . combined with  , we get @xmath604    [ [ estimation - of - mathcalaboldsymbolv ] ] estimation of @xmath594 : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    note that @xmath605 is a block - diagonal matrix with rank-1 block .",
    "so applying gives us @xmath606 where @xmath607 and @xmath608 it suffices to get an estimation of @xmath609 and @xmath610 to bound @xmath594 in  .",
    "lemma  [ lem : orth_decomp ] says that @xmath611 if @xmath612 .",
    "moreover , @xmath613 if @xmath114 belongs to @xmath130 for @xmath609 , @xmath614 now we aim to get an upper bound for @xmath615 by using  , @xmath616 by substituting the estimations of @xmath609 and @xmath615 into   @xmath617 by letting @xmath618 with @xmath619 sufficiently large and combining and , we have @xmath620 which gives @xmath621",
    "we first introduce a few notations : for all @xmath622 , consider @xmath623 and @xmath624 defined in and define @xmath625 where @xmath626 with @xmath627    the function @xmath628 is defined for each block of @xmath629 the particular form of @xmath630 serves primarily for proving the lemma  [ lem : regg ] , i.e. , local regularity condition of @xmath164 .",
    "we also define @xmath631 the following lemma gives bounds of @xmath632 and @xmath633 .",
    "[ lem : dxdh ] for all @xmath634 with @xmath578 , there hold @xmath635 moreover , if we assume @xmath636 additionally , we have @xmath637 .",
    "we only consider @xmath638 and @xmath639 , and the other case is exactly the same due to the symmetry .",
    "for both @xmath633 and @xmath632 , by definition , @xmath640 where @xmath641 and @xmath642 come from the orthogonal decomposition in  .",
    "0.25 cm we start with estimating @xmath643 note that @xmath644 and @xmath645 since @xmath646 . by , we have @xmath647    then we calculate @xmath648 : from  , we have @xmath649 where gives @xmath650 for @xmath651 .",
    "so it suffices to estimate @xmath652 , which satisfies @xmath653 lemma  [ lem : orth_decomp ] implies that @xmath654 , and   gives @xmath655 and @xmath656 if @xmath657 substituting   into   gives @xmath658 then we have @xmath659    finally , we try to bound @xmath660 gives @xmath661 and @xmath662 . combining them along with  ,  ,   and",
    ", we have @xmath663 by symmetry , similar results hold for the case @xmath664 and @xmath665 0.25 cm next , under the additional assumption @xmath666 , we now prove @xmath667 : + case 1 : @xmath638 and @xmath668 . by gives",
    "@xmath662 , which implies @xmath669 case 2 : @xmath664 and @xmath670 . using the same argument as   gives @xmath671 therefore , @xmath672    * ( local regularity for @xmath178)*[lem : regf ] conditioned on   and  , the following inequality holds @xmath673 uniformly for any @xmath674 with @xmath578 if @xmath675 for some numerical constant @xmath676 .    first note that for @xmath677 for each component , recall that   and",
    ", we have @xmath678 define @xmath679 and @xmath680 as @xmath681 here @xmath680 does not necessarily belong to @xmath682 from the way of how @xmath633 , @xmath632 , @xmath679 and @xmath680 are constructed , two simple relations hold : @xmath683 define @xmath684 and @xmath685 .",
    "@xmath686 can be simplified to @xmath687 now we will give a lower bound for @xmath688 and an upper bound for @xmath689 so that the lower bound of @xmath690 is obtained . by the cauchy - schwarz inequality",
    ", @xmath688 has the lower bound @xmath691 in the following , we will give an upper bound for @xmath594 and a lower bound for @xmath593 .",
    "[ [ upper - bound - for - mathcalaboldsymbolv ] ] upper bound for @xmath594 : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    note that @xmath605 is a block - diagonal matrix with rank-1 blocks , and applying results in @xmath692 by using lemma  [ lem : dxdh ] , we have @xmath693 and @xmath694 . substituting them into @xmath695 gives @xmath696 for @xmath609 , note that @xmath697 and thus @xmath698    then by @xmath699 and letting @xmath700 for a sufficiently large numerical constant @xmath676 , there holds @xmath701    [ [ lower - bound - for - mathcalaboldsymbolu ] ] lower bound for @xmath593 : + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    by the triangle inequality , @xmath702 if @xmath578 since @xmath703 .",
    "since @xmath595 , by , there holds @xmath704 with the upper bound of @xmath705 in  , the lower bound of @xmath706 in , and  , we get @xmath707    now let us give an upper bound for @xmath689 , @xmath708 where @xmath294 and @xmath293 are a pair of dual norms and @xmath709 combining the estimation of @xmath688 and @xmath689 above leads to @xmath710    * ( local regularity for @xmath179*[lem : regg ] for any @xmath711 with @xmath578 and @xmath712 , @xmath713 , the following inequality holds uniformly @xmath714 where @xmath715 immediately , we have @xmath716    for the local regularity condition for @xmath164 , we use the results from  @xcite when @xmath427 .",
    "this is because each component @xmath717 only depends on @xmath115 by definition and thus the lower bound of @xmath718 is completely determined by @xmath115 and @xmath719 , and is independent of @xmath2 .    for each @xmath720 , @xmath721 ( or @xmath722 ) only depends on @xmath146 ( or @xmath40 ) and there holds @xmath723 which follows exactly from lemma 5.17 in  @xcite . for  , by definition of @xmath724 and @xmath725 in  , @xmath726 where @xmath727    * ( proof of the local regularity condition ) * [ lem : reg ] conditioned on  , for the objective function @xmath181 in  , there exists a positive constant @xmath728 such that @xmath729_+\\ ] ] with @xmath730 and @xmath268 for all @xmath674 . here",
    "we set @xmath715    following from lemma  [ lem : regf ] and lemma  [ lem : regg ] , we have @xmath731 for all @xmath674 where @xmath732 and @xmath733 . adding them together gives @xmath734 on the left side",
    ". moreover , cauchy - schwarz inequality implies @xmath735 where both @xmath736 and @xmath737 are bounded by @xmath738 in lemma  [ lem : dxdh ] since @xmath739 therefore , @xmath740    dividing both sides of   by @xmath741 , we obtain @xmath742 + \\frac{\\delta d_0}{24 } - 2\\sqrt{s}\\|{\\mathcal{a}}^*({\\boldsymbol{e}})\\| \\end{aligned}\\ ] ] where the local rip condition   implies @xmath743 and hence @xmath744 , where @xmath745 is defined in  .",
    "note that   gives @xmath746_+ } \\leq \\sqrt { 2\\sqrt{2s } \\|{\\mathcal{a}}^*({\\boldsymbol{e}})\\| \\delta d_0 } \\leq \\frac{\\sqrt{6}\\delta d_0}{4 } + \\frac{4\\sqrt{s}}{\\sqrt{6}}\\|{\\mathcal{a}}^*({\\boldsymbol{e}})\\|.\\ ] ] by   and @xmath747_+ + g({\\boldsymbol{h } } , { \\boldsymbol{x}})$ ] , there holds @xmath748_+ } + \\sqrt{g({\\boldsymbol{h } } , { \\boldsymbol{x}})}\\right ) \\\\ & & + \\frac{\\delta d_0}{24 } - \\frac{1}{6\\sqrt{6 } } \\left ( \\frac{\\sqrt{6}\\delta d_0}{4 } + \\frac{4\\sqrt{s}}{\\sqrt{6}}\\|{\\mathcal{a}}^*({\\boldsymbol{e}})\\|\\right ) - 2\\sqrt{s}\\|{\\mathcal{a}}^*({\\boldsymbol{e}})\\|\\\\ & \\geq & \\frac{1}{6\\sqrt{6 } } \\left [ \\sqrt { \\left[{\\widetilde{f}}({\\boldsymbol{h } } , { \\boldsymbol{x } } ) - \\|{\\boldsymbol{e}}\\|^2\\right]_+ } -   \\sqrt{1000s}\\|{\\mathcal{a}}^*({\\boldsymbol{e}})\\|\\right].\\end{aligned}\\ ] ]    for any nonnegative real numbers @xmath749 and @xmath750 , we have @xmath751_+ + b \\geq \\sqrt{(x - a)_+ } $ ] and it implies @xmath752_+^2 + b^2 ) \\longrightarrow [ \\sqrt{(x - a)_+ } - b ] _ + ^2   \\geq \\frac{(x - a)_+}{2 } - b^2.\\ ] ] therefore , by setting @xmath753 and @xmath754 , there holds @xmath755_+ \\\\ & \\geq & \\frac{d_0}{7000 } \\left [ { \\widetilde{f}}({\\boldsymbol{h } } , { \\boldsymbol{x } } ) - ( \\|{\\boldsymbol{e}}\\|^2 + 2000s \\|{\\mathcal{a}}^*({\\boldsymbol{e}})\\|^2 ) \\right]_+.\\end{aligned}\\ ] ]      conditioned on  ,   and  , for any @xmath756 and @xmath757 such that @xmath20 and @xmath758 , there holds @xmath759 with @xmath760 where @xmath167 and @xmath761 holds with probability at least @xmath122 from lemma  [ lem : a - upbd ] .    in particular , @xmath762 and @xmath763 follows from @xmath764 and  .",
    "therefore , @xmath279 can be simplified to @xmath765 by choosing @xmath766    by , we know that both @xmath767 and @xmath768 .",
    "note that @xmath769 where  ,  ,   and   give @xmath770 and @xmath725 .",
    "it suffices to find out the lipschitz constants for all of those four functions .",
    "[ [ step-1 ] ] step 1 : + + + + + + +    we first estimate the lipschitz constant for @xmath771 and the result can be applied to @xmath772 due to symmetry . @xmath773 \\\\ & = { \\mathcal{a}}^ * ( { \\mathcal{a}}({\\mathcal{h}}({\\boldsymbol{h}}+{\\boldsymbol{u } } , { \\boldsymbol{x}}+ { \\boldsymbol{v } } ) - { \\mathcal{h}}({\\boldsymbol{h}},{\\boldsymbol{x } } ) ) ) ( { \\boldsymbol{x}}+ { \\boldsymbol{v } } ) \\\\ & \\quad + { \\mathcal{a}}^*{\\mathcal{a } } ( { \\mathcal{h}}({\\boldsymbol{h}},{\\boldsymbol{x } } ) - { \\mathcal{h}}({\\boldsymbol{h}}_0,{\\boldsymbol{x}}_0 ) ) { \\boldsymbol{v}}- { \\mathcal{a}}^*({\\boldsymbol{e } } ) { \\boldsymbol{v}}\\\\ & = { \\mathcal{a}}^ * ( { \\mathcal{a } } (    { \\mathcal{h}}({\\boldsymbol{h}}+{\\boldsymbol{u}},{\\boldsymbol{v } } ) + { \\mathcal{h}}({\\boldsymbol{u } } , { \\boldsymbol{x } } ) ) ) ( { \\boldsymbol{x}}+ { \\boldsymbol{v } } ) \\\\ & \\quad + { \\mathcal{a}}^*{\\mathcal{a } } ( { \\mathcal{h}}({\\boldsymbol{h}},{\\boldsymbol{x } } ) - { \\mathcal{h}}({\\boldsymbol{h}}_0,{\\boldsymbol{x}}_0 ) ) { \\boldsymbol{v}}- { \\mathcal{a}}^*({\\boldsymbol{e } } ) { \\boldsymbol{v}}.\\end{aligned}\\ ] ] note that @xmath774 and @xmath775 directly implies @xmath776 where @xmath777 moreover ,   implies @xmath778 since @xmath779 combined with @xmath780 in   and @xmath781 , we have @xmath782 due to the symmetry between @xmath771 and @xmath772 , we have , @xmath783 in other words , @xmath784 where @xmath785    [ [ step-2 ] ] step 2 : + + + + + + +    we estimate the upper bound of @xmath786 .",
    "implied by lemma  5.19 in  @xcite , we have @xmath787    [ [ step-3 ] ] step 3 : + + + + + + +    we estimate the upper bound of @xmath788 .",
    "denote @xmath789 } _ { { \\bm{j}}_1 } \\\\",
    "& \\underbrace{+ \\frac{\\rho l}{8d_i\\mu^2 } \\sum_{l=1}^l \\left[g'_0\\left(\\frac{l|{\\boldsymbol{b}}_l^*({\\boldsymbol{h}}_i + { \\boldsymbol{u}}_i)|^2}{8d_i\\mu^2}\\right ) { \\boldsymbol{b}}_l^*({\\boldsymbol{h}}_i + { \\boldsymbol{u}}_i ) - g'_0\\left(\\frac{l|{\\boldsymbol{b}}_l^*{\\boldsymbol{h}}_i|^2}{8d_i\\mu^2}\\right ) { \\boldsymbol{b}}_l^*{\\boldsymbol{h}}_i \\right]{\\boldsymbol{b}}_l}_{{\\bm{j}}_2}.\\end{aligned}\\ ] ] following the same estimation of @xmath790 and @xmath791 in lemma 5.19 of  @xcite , we have @xmath792 therefore , combining   and gives @xmath793 in summary , the lipschitz constant @xmath279 of @xmath365 has an upper bound as follows : @xmath794      in this section , we will prove the robustness condition   and also theorem  [ thm : init ] . to prove",
    ", it suffices to show the following lemma , which is a more general version of  .",
    "[ lem : denoise ] consider a sequence of gaussian independent random variable @xmath795 where @xmath796 with @xmath797 .",
    "moreover , we assume @xmath14 in   is independent of @xmath798",
    ". then there holds @xmath799 with probability at least @xmath122 if @xmath800    it suffices to show that @xmath801 . for each fixed @xmath720 , @xmath802",
    "the key is to apply the matrix bernstein inequality   and we need to estimate @xmath803 , and the variance of @xmath804 for each @xmath72 , @xmath805 follows from  .",
    "moreover , the variance of @xmath806 is bounded by @xmath807 since @xmath808 & = \\sum_{l=1}^l \\operatorname{\\mathbb{e}}(|c_l|^2 \\|{\\boldsymbol{a}}_{il}\\|^2){\\boldsymbol{b}}_l{\\boldsymbol{b}}_l^ * = \\frac{n}{l } \\sum_{l=1}^l \\lambda_l^2{\\boldsymbol{b}}_l{\\boldsymbol{b}}_l^ * \\preceq \\frac{\\lambda^2 n}{l } ,",
    "\\\\ \\operatorname{\\mathbb{e } } [ ( { \\mathcal{a}}_i^*({\\boldsymbol{c } } ) ) ^ * ( { \\mathcal{a}}_i^*({\\boldsymbol{c } } ) )   ] & = \\sum_{l=1}^l \\|{\\boldsymbol{b}}_l\\|^2 \\operatorname{\\mathbb{e}}(|c_l|^2 { \\boldsymbol{a}}_{il}{\\boldsymbol{a}}_{il}^ * ) = \\frac{k}{l^2 }   \\sum_{l=1}^l \\lambda_i^2 { \\boldsymbol{i}}_n \\preceq \\frac{\\lambda^2 k}{l}.\\end{aligned}\\ ] ] letting @xmath809 and applying   leads to @xmath810 therefore , by taking the union bound over @xmath80 , @xmath811 with probability at least @xmath122 if @xmath812 .    the robustness condition is an immediate result of lemma  [ lem : denoise ] by setting @xmath813 and @xmath814    [ cor : ae]*[robustness condition ] * for @xmath815 @xmath816 with probability at least @xmath122 if @xmath817 .",
    "[ lem : ay - hx ] for @xmath815 , there holds @xmath818 with probability at least @xmath122 if @xmath819    the success of the initialization algorithm completely relies on the lemma above .",
    "as mentioned in section  [ s : thm ] , @xmath820 and lemma  [ eq : ay - hx ] confirms that @xmath198 is close to @xmath459 in operator norm and hence the spectral method is able to give us a reliable initialization .",
    "note that @xmath821 where @xmath822 is independent of @xmath823 the proof consists of two parts : 1 .",
    "show that @xmath824 ; 2 .",
    "prove that @xmath825 .",
    "[ [ part - i ] ] part i : + + + + + + +    following from the definition of @xmath14 and @xmath826 in  , @xmath827 where @xmath828 the sub - exponential norm of @xmath829 is bounded by @xmath830 where @xmath831 , @xmath832 and @xmath833 follows from  .",
    "we proceed to estimate the variance of @xmath834 by using   and  : @xmath835 \\right\\|   \\leq \\frac{kd_{i0}^2}{l}.\\end{aligned}\\ ] ] therefore , the variance of @xmath836 is bounded by @xmath837 . by applying matrix bernstein inequality   and taking",
    "the union bound over all @xmath4 , we prove that @xmath838 holds with probability at least @xmath839 if @xmath840    [ [ part - ii ] ] part ii : + + + + + + + +    for each @xmath390 , the @xmath72-th entry of @xmath841 in  , i.e. , @xmath842 , is independent of @xmath843 and obeys @xmath844 .",
    "here @xmath845 this gives @xmath846 thanks to the independence between @xmath841 and @xmath14 , applying lemma  [ lem : denoise ] results in @xmath847 with probability @xmath848 if @xmath849    0.5 cm    therefore , combining   with  , we get @xmath850 for all @xmath80 with probability at least @xmath219 if @xmath851 where @xmath852    before moving to the proof of theorem  [ thm : init ] , we introduce a property about the projection onto a closed convex set .",
    "[ lem : kmc ] let @xmath853 be a closed nonempty convex set",
    ". there holds @xmath854 where @xmath855 is the projection of @xmath20 onto @xmath856 .    with this lemma",
    ", we can easily see @xmath857 for all @xmath858 and @xmath859 .",
    "it means that projection onto nonempty closed convex set is non - expansive .",
    "now we present the proof of theorem  [ thm : init ] .    by choosing @xmath860",
    ", we have @xmath861 where @xmath862 .    by applying the triangle inequality to  , it is easy to see that @xmath863 which gives @xmath713 where @xmath864 according to algorithm  [ initial ] .",
    "[ [ part - i - proof - of - boldsymbolu0boldsymbolv0in - frac1sqrt3mathcaln_dcap - frac1sqrt3mathcaln_mu ] ] part i : proof of @xmath865 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    note that @xmath866 where @xmath867 is the leading right singular vector of @xmath204 therefore , @xmath868 which implies @xmath869    now we will prove that @xmath870 by lemma  [ lem : kmc ] . by algorithm  [ initial ] , @xmath871 is the minimizer to the function @xmath872 over @xmath873 obviously , by definition , @xmath871 is the projection of @xmath874 onto @xmath875 .",
    "note that @xmath876 implies @xmath877 and hence @xmath878    moreover , due to  , there holds @xmath879 in particular , let @xmath880 and immediately we have @xmath881 in other words , @xmath882 .    [",
    "[ part - ii - proof - of - boldsymbolu0boldsymbolv0in - mathcaln_frac2varepsilon5sqrtskappa ] ] part ii : proof of @xmath883 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we will show @xmath884 for @xmath80 so that @xmath885 .",
    "0.25 cm    first note that @xmath886 for all @xmath887 , which follows from weyl s inequality  @xcite for singular values where @xmath888 denotes the @xmath889-th largest singular value of @xmath198 .",
    "hence there holds @xmath890 on the other hand , for any @xmath4 , @xmath891 { \\hat{\\boldsymbol{x}}}_{i0}{\\hat{\\boldsymbol{h}}}_{i0}^ * \\right\\| \\\\ & =   \\frac{1}{d_{i0 } } \\|    { \\mathcal{a}}_i^*({\\boldsymbol{y } } )   - { \\boldsymbol{h}}_{i0}{\\boldsymbol{x}}_{i0}^ * \\| + \\left|\\frac{d_i}{d_{i0}}-1\\right| \\leq 2\\xi \\end{aligned}\\ ] ] where @xmath892 and @xmath893 .",
    "therefore , we have @xmath894 where @xmath895 and @xmath896 . if we substitute @xmath897 by @xmath898 into  , @xmath899 where @xmath898 follows from @xmath900 .",
    "0.5 cm now we are ready to estimate @xmath901 as follows , @xmath902 here @xmath903 because @xmath904 and @xmath905 follows from   and  . for @xmath906 , there holds @xmath907 which follows from  . therefore , @xmath908",
    "[ lem : dsl ] if @xmath909 is a continuously differentiable real - valued function with two complex variables @xmath20 and @xmath26 , ( for simplicity , we just denote @xmath909 by @xmath910 and keep in the mind that @xmath910 only assumes real values ) for @xmath911 .",
    "suppose that there exists a constant @xmath279 such that @xmath912 for all @xmath913 and @xmath914 such that @xmath915 and @xmath916 .",
    "then @xmath917 where @xmath918 is the complex conjugate of @xmath919 .",
    "we define the matrix @xmath920-norm via @xmath921 \\leq 2 \\}.\\ ] ]    [ thm : bern1 ] *  @xcite * consider a finite sequence of @xmath829 of independent centered random matrices with dimension @xmath922 .",
    "assume that @xmath923 and introduce the random matrix @xmath924 compute the variance parameter @xmath925 then for all @xmath926 @xmath927 with probability at least @xmath928 where @xmath929 is an absolute constant .",
    "[ lem : multiple1 ] let @xmath930 , then @xmath931 and @xmath932 and @xmath933 let @xmath934 be any deterministic vector , then the following properties hold    @xmath935    @xmath936 = \\|{\\boldsymbol{q}}\\|^2 { \\boldsymbol{i}}_n.\\ ] ]    let @xmath937 be a complex gaussian random vector in @xmath938 , independent of @xmath939 , then @xmath940",
    "s.ling would like to thank felix krahmer and dominik stger for the discussion about  @xcite , and also thank ju sun for pointing out the connection between convolutional dictionary learning and this work .",
    "j.  liu , j.  xin , y.  qi , f .-",
    "zheng , et  al . a time domain algorithm for blind separation of convolutive sound mixtures and @xmath941 constrained minimization of cross correlations .",
    ", 7(1):109128 , 2009 .",
    "d.  stger , p.  jung , and f.  krahmer .",
    "blind deconvolution and compressed sensing . in _ compressed sensing theory and its applications to radar , sonar and remote sensing ( cosera ) , 2016 4th international workshop on _ , pages 2427 .",
    "ieee , 2016 .",
    "s.  tu , r.  boczar , m.  simchowitz , m.  soltanolkotabi , and b.  recht .",
    "low - rank solutions of linear matrix equations via procrustes flow . in _ proceedings of the 33rd international conference on machine learning _ , pages 964973 , 2016 .",
    "r.  vershynin .",
    "introduction to the non - asymptotic analysis of random matrices . in y.  c. eldar and g.  kutyniok , editors , _ compressed sensing : theory and applications _ , chapter  5 . cambridge university press , 2012 ."
  ],
  "abstract_text": [
    "<S> we study the question of extracting a sequence of functions @xmath0 from observing only the sum of their convolutions , i.e. , from @xmath1 . </S>",
    "<S> while convex optimization techniques are able to solve this joint blind deconvolution - demixing problem provably and robustly under certain conditions , for medium - size or large - size problems we need computationally faster methods without sacrificing the benefits of mathematical rigor that come with convex methods . in this paper </S>",
    "<S> we present a non - convex algorithm which guarantees exact recovery under conditions that are competitive with convex optimization methods , with the additional advantage of being computationally much more efficient . </S>",
    "<S> our two - step algorithm converges to the global minimum linearly and is also robust in the presence of additive noise . while the derived performance bounds are suboptimal in terms of the information - theoretic limit , numerical simulations show remarkable performance even if the number of measurements is close to the number of degrees of freedom . </S>",
    "<S> we discuss an application of the proposed framework in wireless communications in connection with the internet - of - things . </S>"
  ]
}