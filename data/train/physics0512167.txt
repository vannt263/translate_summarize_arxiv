{
  "article_text": [
    "simulated tempering was introduced in ref .",
    "@xcite , parallel tempering , also known as replica exchange monte carlo , in ref .",
    "@xcite and both have been widely used ( see e.  g. refs .",
    "@xcite ) to make markov chain monte carlo faster . for an introduction to both methods",
    "see ref .",
    "@xcite .",
    "propp and wilson introduced the coupling from the past ( cftp ) method to draw exact samples , i.e. samples which are guaranteed to be uncorrelated and to obey the desired distribution in ref .",
    "applications of this method to continuous degrees of freedom and cluster algorithms exist , see refs . @xcite and @xcite .",
    "a small modification of the simulated tempering algorithm likewise allows to obtain uncorrelated samples , see ref .",
    "the first two sections give an introduction to this procedure and the resulting error estimates .",
    "we then examine the tempering markov matrix and the autocorrelation time in and give indications about the needed parameters .",
    "we investigate the tempering algorithm for multidimensional continuous distributions and find a polynomial dependence on the dimension .",
    "finally , we compare exact sampling with simulated tempering to the cftp method for the two dimensional ising model and find a quadratic dependence on the system size for simulated tempering , while cftp needs exponential time for cold but finite temperatures .",
    "besides speeding simulations up , simulated tempering provides an alternative way to obtain exact samples from arbitrary probability density functions @xcite .",
    "[ fig : walk_bins ] shows the principle for a multi - modal distribution @xmath0 consisting of two gaussians , but it does not depend on the specified example and can thus be applied to a variety of probability distributions .",
    "we want to draw exact samples from the distribution @xmath0 , which we can not sample directly , where @xmath1 can be a discrete or continuous quantity of arbitrary dimension . in order to do so",
    ", we introduce an additional parameter @xmath2 and the joint probability @xmath3 .",
    "we have large freedom in choosing @xmath4 , for the simulation depicted in fig .",
    "[ fig : walk_bins ] , we chose : @xmath5 where @xmath6 is the overall normalization , @xmath7 is a constant depending on @xmath8 , which determines @xmath9 .",
    "the additional variable @xmath2 was allowed to take @xmath10 discrete values @xmath8 with @xmath11 and @xmath12 .",
    "@xmath13 should be chosen in a way to allow generating exact samples easily ; in our example , it was a single broad gaussian peak .",
    "furthermore , its range in @xmath1-space should be broad enough to cover all structures of @xmath0 .",
    "for application to physical systems , @xmath14 would of course be chosen as @xmath15 ( with @xmath16 denoting the energy ) and @xmath17 , which yields @xmath18 . in this case , @xmath19 is not chosen to be one , but can take any other value @xmath20 .",
    "we then do markov chain monte carlo in the @xmath21-space , where we alternate a couple of sweeps in @xmath1-space with moves in @xmath2-direction . in @xmath2-direction , @xmath22 with @xmath23 is proposed with equal probability and accepted according to the metropolis scheme . in @xmath1-space and for @xmath24 ,",
    "usual metropolis updates are employed .",
    "a special case arises for @xmath1-moves at @xmath25 . in this case",
    "@xmath26 and we are able to draw a new exact sample @xmath27 distributed according to @xmath13 , which gives us a sample @xmath27 _ uncorrelated _ from @xmath1 .",
    "an example of the resulting random walk is depicted on the floor of fig .",
    "[ fig : walk_bins ] .",
    "whenever this random walk reaches @xmath28 , a new exact sample from @xmath29 is drawn independent from the current state of the markov chain so that the walk forgets its past .",
    "the mc time needed for one exact sample is thus given by the time needed by the markov chain to travel from @xmath30 to @xmath12 and back again .",
    "+    a plain mcmc run would instead be trapped in one of the two peaks and rarely tunnel to the other .",
    "repeating several plain mcmc runs and taking their average would give the wrong expectation value @xmath31 , because the different weight of the peaks would not be accounted for .",
    "as the @xmath21-samples obtained by the simulation obey @xmath4 , the @xmath1 drawn at a given temperature @xmath8 obeys @xmath32 .",
    "expectation values for @xmath12 are therefore calculated from all ( correlated and uncorrelated ) samples obtained at a given temperature : @xmath33 the @xmath34 are the measurements obtained at the desired temperature @xmath12 , their index @xmath35 was broken into @xmath36 and @xmath37 with @xmath36 denoting the independent and uncorrelated bins and @xmath37 labeling the correlated measurements within one bin , see fig .",
    "[ fig : walk_bins ] .",
    "@xmath38 is the number of independent bins which contain at least one sample drawn at @xmath19 , and @xmath39 the number of measurements within the @xmath36-th bin .",
    "@xmath40 is the total number of times the simulation has visited the desired temperature @xmath12 .",
    "a bar denotes the sample mean obtained in the monte carlo run , @xmath41 denotes an expectation value over all samples .",
    "the sample mean is obviously unbiased .",
    "it is worth noting that measuring the bin averages does not give the same result , because the probability for a move in @xmath2-direction , and thus the number of measurements ( @xmath39 ) taken in a bin before the walk returns to @xmath28 , is a random variable and depends on the current sample @xmath1 : @xmath42 here , @xmath43 is the average number of measurements per bin . for the same reason ,",
    "taking only the first sample of each bin does not give correct results .",
    "for a multi - modal @xmath0 with a different height ( and/or width ) of the peaks as in fig .",
    "[ fig : walk_bins ] , the markov chain may visit the smaller peak very often , but it will stay at the larger one longer .",
    "the independent samples provide a way to analyze correlations and to calculate reliable error estimates @xcite . when calculating the variance of the estimate @xmath44 , the new labels @xmath36 and @xmath37 become useful as it is now important to distinguish between correlated and uncorrelated samples : @xmath45    where @xmath46 for @xmath47 , because the measurements are from different bins , @xmath48 is independent of @xmath36 , because all bins are equivalent . from eq .",
    "[ eq : squ_mean ] , it follows for the variance @xmath49 the unknown expectation value @xmath50 is estimated from the monte carlo run , thus @xmath51 . however , the variance depends on the determination of the above expectation value , so it can only be correct , if all modes of @xmath14 have been sampled sufficiently .",
    "similar formulae can be derived for the expectation values and error estimates of more complex observables ( e.g. of the covariance ) , where correlations between the measured parameters can thus be taken into account .",
    "although nobody would think of using monte carlo simulation for one dimensional problems , as much more efficient approaches are available , it is interesting to examine the markov matrix for a simulated tempering simulation in the two - dimensional @xmath1-@xmath2-space with discretized @xmath1 .",
    "the probability density @xmath52 for @xmath53 was chosen to consist of two gaussians well separated from each other and @xmath54 was chosen to be constant . for simulated tempering ,",
    "the number of @xmath2-slices was varied from two ( just @xmath28 with @xmath55 and @xmath53 with @xmath56 ) to five .",
    "the intermediate @xmath2-values were chosen so as to give approximately the same transition rate between all pairs of adjacent @xmath2-values .",
    "autocorrelation and thermalization are largely determined by the second largest eigenvalue ( @xmath57 ) of the markov matrix , i.e. the one with magnitude closest to one . the autocorrelation time was approximately calculated as @xmath58 .",
    "the distance is measured in multiples of the width @xmath59 of the gaussian.[fig : mini_sim],scaledwidth=60.0% ]    fig .",
    "[ fig : mini_sim ] shows this autocorrelation time as a function of the distance of the two peaks .",
    "one sees that more @xmath2-slices become necessary as the distance increases . for plain markov chain monte carlo in the one - dimensional discrete @xmath1-space ,",
    "the autocorrelation time far exceeded the range plotted in fig .",
    "[ fig : mini_sim ] even for a distance of @xmath60 ( @xmath61 ) and its calculation is numerically instable for larger distances .",
    "the columns of the tempering markov matrix which correspond to @xmath28 are identical , which means just that whenever the current state of the chain is at @xmath28 , the outcome of the next move will not depend on the current position in @xmath1-space .",
    "another method similar to simulated tempering parallel tempering , also called exchange monte carlo , see refs .  @xcite . in this method , we have @xmath62 copies of @xmath1 at the @xmath62 values for @xmath2 . instead of the space @xmath63 as in simulated tempering",
    ", we now consider the product space @xmath64 where the configuration @xmath65 is at the temperature @xmath8 . at every @xmath8",
    ", there is _ exactly one _ configuration @xmath1 , denoted by @xmath65 and obeying the distribution @xmath32 .",
    "the probability for the total product space is given by the product of the individual probabilities .",
    "we now do markov chain monte carlo again with this product probability . in @xmath1-space ,",
    "metropolis monte carlo updates are performed for all @xmath2s independently .",
    "new configurations @xmath66 are obtained with the usual metropolis random walk for @xmath67 , while a new sample is drawn directly from @xmath13 for @xmath28 .",
    "alternated with the updates in @xmath1-space , metropolis moves to swap configurations @xmath65 and @xmath68 at adjacent @xmath2-values are performed .    during the monte",
    "carlo run , all @xmath69 will eventually get swapped to @xmath28 , where a new sample is drawn .",
    "this time , however , the random walk does not completely forget its past , which can be inferred from the markov matrix for a similar toy situation as for simulated tempering above .",
    "suppose , we have three @xmath2-values @xmath70 , and the following temperature swaps occur in the markov chain monte carlo :    @xmath71    where a tilde means that an exact sample is drawn from @xmath29 .",
    "all configurations have now been at @xmath28 , but the columns of the matrix corresponding to the above sequence of swaps are still not equal , which means that the current state of the markov chain still depends on its initial state",
    ". however , these correlations are small after an initial thermalization and autocorrelation times are short .",
    "in order to do simulated or parallel tempering , we have to adjust the values for the @xmath8 and the @xmath7 , see eq .. the @xmath8-values have to be dense enough to give a considerable overlap of @xmath32 and @xmath72 .",
    "on the other hand , we want to have as few @xmath2-values as possible between @xmath53 and @xmath28 .",
    "the @xmath2-values can be adjusted in a parallel tempering prerun , where a new value is inserted whenever the swapping rate between adjacent @xmath2s is too low .",
    "the ideal @xmath7 needed for simulated tempering would make all @xmath2-values equally likely .",
    "this prevents the markov chain from spending too much time at on single temperature and thus speeds travel from @xmath28 to @xmath53 and back again .",
    "this leads to : @xmath73 for physical systems , the weight @xmath74 gives the partition function , which can only be determined in terms of @xmath75 . for problems in data analysis , it is the model evidence , i.e. the probability for the chosen model integrated over all possible parameter values .",
    "the weights can be obtained from the visiting frequency for the @xmath2-values in simulated tempering preruns , but this is rather difficult , because they may differ by orders of magnitude .",
    "they are not needed for parallel tempering , where they cancel out , but the integral can still be calculated with a procedure similar to thermodynamical integration , see ref .",
    "@xcite . with the random samples produced at @xmath8",
    ", we can estimate @xmath76 for @xmath77 : @xmath78 where @xmath79 denotes an expectation value calculated at @xmath8 .",
    "the integral @xmath80 is the product of all the measured ratios : @xmath81 care must be taken in evaluating this quantity , because the configurations are interchanged between @xmath2-values and the measurements obtained for the different @xmath2-values are therefore heavily correlated and the same applies to using parallel tempering data for multihistogrmming ( ref .",
    "@xcite ) , as was proposed in ref .  @xcite .",
    "in this section , we examine the behavior of the tempering algorithm in higher dimensions .",
    "we chose @xmath29 as one single broad gaussian with width @xmath82 centered at @xmath83 and the wanted probability @xmath14 consisted of two gaussians of width @xmath84 centered at @xmath85 and @xmath86 , which were multiplied by 5000 , so as to yield a norm @xmath87 .",
    "100 sweeps were performed between @xmath2-moves , the @xmath8 and @xmath7 were adjusted in a parallel tempering prerun .",
    "the geometric mean was used to insert new slices , except for finding the second lowest @xmath88 , where the old value was just divided by a constant , if the swapping rate was too low . as the number of needed @xmath2 values @xmath89 depends on the logarithm of the ratio of the volumes of @xmath14 and @xmath29 @xmath90 , the dependence on the dimension @xmath91",
    "is expected to be linear , which is indeed approximately the case , as can be seen in fig .",
    "[ fig : d_beta ] .    figure [ fig : d_moves ] shows the number of mc updates needed for one independent sample .",
    "one sees that the increase in needed samples with the dimension of the problem approximately obeys a power law in contrast to the behavior found for the cftp algorithm ( ref .",
    "@xcite ) , which has an exponential dependence on the dimension and generally similar performance as the rejection method , see ref .  @xcite . for all presented dimensions ,",
    "the results for the norm were consistent with the errorbars ( see fig . [",
    "fig : d_av_z ] ) and likewise the average for @xmath1 , i.  e. the simulation found both peaks .     of needed @xmath2-values needed for various dimensions d.[fig : d_beta],scaledwidth=45.0% ]    .[fig",
    ": d_moves],scaledwidth=45.0% ]     for various dimensions @xmath91.[fig : d_av_z],scaledwidth=45.0% ]",
    "although other choices could be more promising @xcite , we chose constant transition ( simulated tempering ) and swapping rates ( parallel tempering , see ref .",
    "@xcite ) between adjacent temperatures . this leads to : @xmath92 where @xmath93 is the energy , @xmath94 the specific heat and @xmath95 the boltzmann constant .",
    "this relation shows that we need denser @xmath2-values where @xmath94 is large , i.  e. near a ( second order ) phase transition . as the specific heat is small for low temperatures again , further cooling below the phase transition",
    "the specific heat is an extensive quantity , the number of needed temperatures is therefore expected to grow as @xmath96 with @xmath97 the number of spins . in this case , we used the arithmetic men to insert new @xmath2-values .",
    "as expected , the number of needed temperatures scales proportional to the linear system size @xmath98 , see fig .",
    "[ fig : nl_n ] .",
    "[ fig : nl_n ]    [ fig : sim_pw ]    figure [ fig : sim_pw ] shows the time per independent sample for exact tempering and for the coupling from the past ( cftp ) method with single spin flips introduced by propp and wilson in ref .",
    "for cftp , the time needed for an independent sample grows on a logarithmic scale with the system sizes for temperatures above the critical @xmath99 , obeys a power law at @xmath99 and grows exponentially for the ordered phase below @xmath99 .",
    "there are cftp schemes for the swendsen wang algorithm , but its runtime scales exponentially with the system size except at @xmath100 and it is _ much _ slower than the single spin flip algorithm around @xmath99 , see ref .",
    "@xcite .",
    "exact tempering with the swendsen wang algorithm , on the other hand , gives linear scaling for all temperatures .",
    "although this is slower than cftp for high temperatures , one does in fact get the high temperature results for free , because they are sampled anyway in a tempering run for low temperatures .",
    "the critical exponent for exact tempering is two for both the swendsen wang ( @xmath101 at the lowest temperature ) and the wolff ( @xmath102 ) algorithm .",
    "the reason for this is , that the time for an independent sample is determined by the number of steps needed to go from @xmath28 to @xmath103 and back again and not by the algorithm used for the spin updates .",
    "this random walk in the temperatures scales proportional to the square of their number @xmath89 , which gives    @xmath104    this dependence @xmath105 breaks down for the single spin flip algorithm .",
    "exact tempering then becomes much slower , because the spin configurations at the critical temperature can not be sampled as efficiently .",
    "this effect becomes more severe for first order transitions , where the algorithm does not manage the transition from the disordered to the ordered phase at @xmath99 . ` tempering ' of a model parameter which carries the transition from first to second order might then be a solution .",
    "this was introduced in ref .",
    "@xcite for the swendsen - wang algorithm applied to the potts model , where the variable ` tempering ' parameter was the number of states @xmath106 .",
    "exact tempering is also applicable to this variant , because the percolation problem for @xmath107 can be sampled exactly .",
    "this paper provides a discussion of exact sampling with simulated tempering @xcite and compares it to exact sampling via the propp - wilson method @xcite .",
    "the former is found to be advantageous in most cases . simulated tempering provides a way to draw exact , i.e. completely uncorrelated samples from arbitrary distributions in high dimensions .",
    "the peaks of multimodal densities are sampled with their respective weights .",
    "the parameters @xmath8 and @xmath7 needed for the simulated tempering run can be adjusted in a parallel tempering prerun . while the parallel tempering algorithm itself does not provide perfectly uncorrelated samples ,",
    "its autocorrelation time is small . for practical purposes , it is a robust alternative , because it does not need the parameters @xmath7 .",
    "both methods allow to calculate the integral over the probability density , i.e. partition functions or model evidences .",
    "this work has been supported by the austrian science fund ( fwf ) , project no .",
    "this document has been typeset using the -latex  packages ."
  ],
  "abstract_text": [
    "<S> multimodal structures in the sampling density ( e.g. two competing phases ) can be a serious problem for traditional markov chain monte carlo ( mcmc ) , because correct sampling of the different structures can only be guaranteed for infinite sampling time . </S>",
    "<S> samples may not decouple from the initial configuration for a long time and autocorrelation times may be hard to determine .    </S>",
    "<S> we analyze a suitable modification @xcite of the simulated tempering idea  @xcite , which has orders of magnitude smaller autocorrelation times for multimodal sampling densities and which samples all peaks of multimodal structures according to their weight . </S>",
    "<S> the method generates * exact * , i.e. uncorrelated , samples and thus gives access to reliable error estimates . * </S>",
    "<S> exact tempering * is applicable to arbitrary ( continuous or discreet ) sampling densities and moreover presents a possibility to calculate integrals over the density ( e.g. the partition function for the boltzmann distribution ) , which are not accessible by usual mcmc .     </S>",
    "<S> address = institute for theoretical and computational physics , tu graz , austria     address = institute for theoretical and computational physics , tu graz , austria     address = institute for theoretical and computational physics , tu graz , austria     address = institute for theoretical and computational physics , tu graz , austria </S>"
  ]
}