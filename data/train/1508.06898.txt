{
  "article_text": [
    "complicated patterns which evolve with time occur in a variety of applied contexts , and quantifying or even just distinguishing such patterns can pose serious challenges . over the last decade ,",
    "computational topology has emerged as a tool which on the one hand allows for significant data reduction , while at the same time focusing on and retaining essential connectivity information of the studied patterns .",
    "one particularly interesting application area is materials science , where complex evolving patterns are frequently created through complicated phase separation processes .",
    "computational topology encompasses a wide variety of possible tools  @xcite , and most of them in one way or another are based on homology theory .",
    "the deeper reason for this can be found in the inherent computability of homology , and in recent years powerful algorithms have been devised which enable fast homology computations for very large data sets , see for example  @xcite , as well as the references therein .",
    "homology has been used in a number of materials science contexts .",
    "earlier studies have made use of the easily computable euler characteristic , see for example the references in the recent survey  @xcite .",
    "however , it has been pointed out that in some situations the information retained by the euler characteristic is not enough to distinguish certain important pattern features . in contrast",
    ", the betti numbers , which are associated with the homology groups and will be described below , provide a finer metric .",
    "they were used for example in  @xcite to relate the pattern complexity evolution as described by averaged betti number evolution curves to the amount of stochasticity inherent in a phase field model for binary phase separation in metal alloys .",
    "this study was the first to use homology information in the context of model validation .",
    "based on available experimental data it was shown that if the noise in the system is too low , the observed betti number evolution curves are qualitatively different from the experimental ones .",
    "in addition , it was demonstrated in  @xcite that while betti numbers can be used to separate bulk from boundary behavior , the averaged euler characteristic can only describe the boundary effects .",
    "similar materials science studies in the context of polycrystals can be found in  @xcite .",
    "while the first of these papers uses homology to study the connectivity properties of grain - boundary networks in planar sections of polycrystals , the second paper employs betti numbers as a means to describe the thermal - elastic response of calcite - based polycrystalline materials such as marble .",
    "more precisely , in  @xcite homological techniques are used to characterize not only the elastic energy density and maximum principal stress response fields in a polycrystal , but also the respective grain - boundary misorientation distributions which generated these response fields .",
    "it was shown that this topological analysis can quantitatively distinguish between different types of grain - boundary misorientations , and relate them to differences in the resulting response fields .    in all of the applications described so far",
    ", the numerical or experimental data is given in the form of a field , or in other words , a real - valued function  @xmath0 defined on some domain  @xmath1 .",
    "the associated patterns are subsets of the domain  @xmath2 , and usually created through a thresholding process . for example , after selecting a suitable threshold  @xmath3 , one can consider the sub- and super - level sets @xmath4 which in some contexts are called nodal domains of  @xmath5 . as subsets of  @xmath1 ,",
    "the sets  @xmath6 have well - defined singular homology groups , and if the function  @xmath5 is sufficiently regular , these groups can be computed precisely ; see for example  @xcite for the two - dimensional case @xmath7 .",
    "however , if the function  @xmath5 is not smooth , or if the thresholding process involves a field created from experimental or noisy data , then the above thresholding process may not capture the correct topology of the actual underlying pattern .",
    "moreover , in certain applications the thresholding approach itself might not be appropriate , for example if there is no obvious or physically relevant choice of threshold .",
    "an extension of the concept of homology to situations involving noise or the lack of a clear thresholding process has been proposed some fifteen years ago and is called persistent homology  @xcite . while this extension is described in more detail in the next section , persistent homology is a dimension reduction technique which provides a topological description of the evolution of sublevel sets of a mapping  @xmath5 as a function of the thresholding level  @xmath3 .",
    "it gives rise to intervals  @xmath8 $ ] over which certain topological features persist , and the length  @xmath9 can in some sense be viewed as a measure of importance of the specific feature .",
    "thus , topological features with small interval lengths are usually considered as `` noise '' or `` unimportant , '' while features with long intervals are deemed `` significant . ''",
    "needless to say , the precise meaning of these notions will change from application to application .",
    "the concept of persistent homology has already been used in a number of materials science contexts , such as for example in the analysis of granular media  @xcite , in the study of protein compressibility  @xcite , as well as in the classification of amorphous structures  @xcite and glass  @xcite .",
    "efficient algorithms for computing persistent homology are described in  @xcite .    despite the success of the above uses of computational topology in applications , one immediate question is the extent of the resulting dimension reduction . through homology , patterns or microstructures",
    "are basically reduced to a finite set of integers , and it is therefore natural to wonder what information is still encoded in this reduced measurement , beyond the obvious connectivity information .",
    "it was pointed out in  @xcite that even small betti number counts , which taken in isolation do not provide much in terms of pattern differentiation , can provide significant information when viewed in a stochastic , i.e. , averaged setting .",
    "more precisely , it was shown in  @xcite that during the phase separation process called nucleation , averaged droplet counts on small domains give extremely precise projections for the observed averaged droplet counts on large domains , where the droplet count is just the zero - dimensional betti number .",
    "( 14.9,7.3 ) ( 0.0,3.8 ) ) with white noise forcing .",
    "the images show snapshots of solutions which originate at the homogeneous state  @xmath10 , and for parameter values @xmath11 and @xmath12 . from top",
    "left to bottom right the images are for mass  @xmath13 , where @xmath14.,title=\"fig:\",width=132 ] ( 3.8,3.8 ) ) with white noise forcing .",
    "the images show snapshots of solutions which originate at the homogeneous state  @xmath10 , and for parameter values @xmath11 and @xmath12 . from top",
    "left to bottom right the images are for mass  @xmath13 , where @xmath14.,title=\"fig:\",width=132 ] ( 7.6,3.8 ) ) with white noise forcing .",
    "the images show snapshots of solutions which originate at the homogeneous state  @xmath10 , and for parameter values @xmath11 and @xmath12 . from top",
    "left to bottom right the images are for mass  @xmath13 , where @xmath14.,title=\"fig:\",width=132 ] ( 11.4,3.8 ) ) with white noise forcing .",
    "the images show snapshots of solutions which originate at the homogeneous state  @xmath10 , and for parameter values @xmath11 and @xmath12 . from top",
    "left to bottom right the images are for mass  @xmath13 , where @xmath14.,title=\"fig:\",width=132 ] ( 0.0,0.0 ) ) with white noise forcing .",
    "the images show snapshots of solutions which originate at the homogeneous state  @xmath10 , and for parameter values @xmath11 and @xmath12 . from top",
    "left to bottom right the images are for mass  @xmath13 , where @xmath14.,title=\"fig:\",width=132 ] ( 3.8,0.0 ) ) with white noise forcing .",
    "the images show snapshots of solutions which originate at the homogeneous state  @xmath10 , and for parameter values @xmath11 and @xmath12 . from top",
    "left to bottom right the images are for mass  @xmath13 , where @xmath14.,title=\"fig:\",width=132 ] ( 7.6,0.0 ) ) with white noise forcing .",
    "the images show snapshots of solutions which originate at the homogeneous state  @xmath10 , and for parameter values @xmath11 and @xmath12",
    ". from top left to bottom right the images are for mass  @xmath13 , where @xmath14.,title=\"fig:\",width=132 ] ( 11.4,0.0 ) ) with white noise forcing .",
    "the images show snapshots of solutions which originate at the homogeneous state  @xmath10 , and for parameter values @xmath11 and @xmath12 . from top",
    "left to bottom right the images are for mass  @xmath13 , where @xmath14.,title=\"fig:\",width=132 ]    in the present paper , we demonstrate that when viewed in a stochastic and time evolving framework , topological information encodes considerably more than anticipated .",
    "this will be accomplished in the setting of instantaneous phase separation in binary metal alloys , as modelled by the cahn - hilliard theory of spinodal decomposition  @xcite .",
    "this phase separation phenomenon is initiated immediately after a high - temperature melt of two uniformly mixed metal components is quenched , i.e. , rapidly cooled . depending on the concentrations of the involved components",
    ", they will quickly separate to form complicated microstructures which contain some apparent element of randomness .",
    "some of the resulting patterns in two space dimensions are shown in figure  [ fig : chcpatt ] .",
    "these patterns evolve with time , and after the initial phase separation process a coarsening stage sets in , during which the characteristic length scale of the patterns increases .    the first mathematical model for spinodal decomposition was introduced by cahn and hilliard  @xcite , who proposed a nonlinear evolution equation for the relative concentration difference @xmath15 , where  @xmath16 and  @xmath17 denote the relative concentrations of the two components , i.e. , @xmath18 .",
    "their model is based on the ginzburg - landau free energy given by @xmath19 where  @xmath1 is a bounded domain , and the positive parameter  @xmath20 models interaction distance .",
    "the bulk free energy  @xmath21 is a double well potential , which for the purposes of this paper is taken as @xmath22 taking the variational derivative @xmath23 of the ginzburg - landau free energy  ( [ defenergy ] ) with respect to the concentration variable  @xmath5 , one then obtains first the chemical potential @xmath24 , and then the associated cahn - hilliard equation @xmath25 , i.e. , the fourth - order partial differential equation @xmath26 subject to homogeneous neumann boundary conditions for both  @xmath27 and  @xmath5 . due to these boundary conditions ,",
    "any mass flux through the boundary is prohibited , and therefore mass is conserved .",
    "we generally consider initial conditions for  ( [ ch ] ) which are small - amplitude random perturbations of a spatially homogeneous state , i.e. , we assume that @xmath28 for all @xmath29 , as well as @xmath30 , where we use the standard abbreviation @xmath31 .",
    "one can easily see that such initial conditions lead to instantaneous phase separation in the cahn - hilliard equation as long as the conserved total mass  @xmath32 satisfies @xmath33 . in the context of  ( [ deff ] ) , this condition is equivalent to  @xmath34 .    while the deterministic cahn - hilliard model  ( [ ch ] ) provides a basic qualitative model for spinodal decomposition , it can not produce microstructures whose evolution during the initial phase separation stage agrees with experiments  @xcite . as it turns out , this deficiency is due to the fact that the original model  ( [ ch ] ) completely ignores thermal fluctuations . to remedy this ,",
    "cook  @xcite extended the model by adding a random fluctuation term  @xmath35 , i.e. , he considered the stochastic cahn - hilliard - cook model @xmath36 where the expected value and the correlation of the noise process satisfy both @xmath37 here @xmath38 is a measure for the intensity of the fluctuation and  @xmath39 describes the spatial correlation of the noise .",
    "thus , the noise is always uncorrelated in time , and for the special case  @xmath40 we obtain space - time white noise . in general",
    ", however , the noise process will exhibit spatial correlations . for more details",
    "we refer the reader to the discussions in  @xcite .",
    "as mentioned before , while both the deterministic and the stochastic model generate microstructures which are qualitatively similar to the ones observed during spinodal decomposition  @xcite , only the stochastic model can in principle lead to a more quantitative agreement  @xcite .",
    "recent mathematical results for the models  ( [ ch ] ) and  ( [ chc ] ) have identified the observed microstructures as certain random superpositions of eigenfunctions of the laplacian , and were able to explain the dynamics of the decomposition process in more detail  @xcite .",
    "based on the insight obtained in  @xcite , our studies in the present paper concentrate exclusively on the stochastic cahn - hilliard - cook model  ( [ chc ] ) . for this model , we show that persistent homology averages suffice to accurately deduce the total mass  @xmath32 , and therefore the total alloy component concentrations , as well as the actual stage of the decomposition process .",
    "both of these observations are somewhat surprising , since there is no obvious link between the connectivity information retained by homology and either of these quantities .",
    "our results demonstrate , however , that during the phase separation process , the evolution of averaged persistent homology information in some sense is a very detailed encoding of these system parameters .",
    "expressed differently , while each mass value  @xmath32 uniquely determines an averaged persistent homology evolution for randomly selected solutions originating close to the homogeneous state  @xmath32 , the topology encoded in these averaged evolutions is sufficiently different to allow for the solution of an inverse problem .",
    "a similar statement can be made for the stage of the decomposition .",
    "moreover , we wold like to point out that unlike the betti numbers , persistent homology information can not easily be averaged directly .",
    "we therefore make use of the recent concept of persistence landscapes  @xcite , which will be described in more detail below . in our studies",
    ", we are also using the idea of topological process , which will be introduced later in this paper .",
    "it allows us to deal with a processes that produces a sequence of topological spaces .",
    "the remainder of this paper is organized as follows . in section  [ sec : topology ] we survey the topological background for this paper , to keep our presentation self - contained and accessible for applied scientists . after introducing cubical homology in section  [ sec : introcubhom ] , we discuss the concept of persistent homology in section  [ sec : intropers ] . since for our study the averaging of persistence information is crucial , we then turn our attention to persistence landscapes in section  [ sec : intropl ] . after these preparations , the topological analysis of microstructures created by the cahn - hilliard - cook model  ( [ chc ] )",
    "is the subject of section  [ sec : chc ] .",
    "we begin by presenting the basic methodology in section  [ sec : basicmethod ] , turn our attention to the topological determination of the total mass  @xmath32 in section  [ sec : chcmass ] , and show in section  [ sec : chctime ] that even the time of a solution snapshot can be recovered accurately from the persistence information during the spinodal decomposition and the initial coarsening regime .",
    "finally , section  [ sec : future ] contains a brief summary and draws some conclusions .",
    "in order to keep this paper as self - contained as possible , this section provides background information on homology and persistence .",
    "more precisely , we recall the basic definition of cubical homology , introduce persistent homology , and finally describe the concept of persistence landscapes .",
    "homology theory associates discrete objects with topological spaces in such a way that continuous deformations of the space do not change the objects .",
    "in other words , homology is a topological invariant of the underlying topological space .",
    "we will see below that the discrete objects are abelian groups , and depending on the form in which the topological space is given , a number of different homology theories have been developed over the years to compute them .",
    "if a given space can be treated within several different homology theories , then the homology groups assigned by these different homology theories will be isomorphic . for the purposes of this paper ,",
    "we restrict our attention to one specific homology theory , namely _",
    "cubical homology_. for this , we assume that our topological space is given as a specific union of cubical sets of possibly varying dimensions , leading to the concept of a _ cubical complex_.    to be more precise , define an _ elementary interval _ as a compact real interval of the type  @xmath41 $ ] or  @xmath42 $ ] , where  @xmath43 denotes an integer . while the first interval type is referred to as _ non - degenerate _ , the second one is called a _ degenerate",
    "interval_. an _ elementary cube _",
    "@xmath44 is a product of elementary intervals @xmath45 , and its _ dimension _ equals the number of non - degenerate intervals  @xmath46 in this product .",
    "furthermore , the total number  @xmath47 of intervals in the product is called the _ embedding dimension _ of the cube  @xmath44 . for example , figure  [ fig : cubhom]_(a ) _ depicts an elementary cube of dimension one in black , and a two - dimensional elementary cube in blue , and both cubes have embedding dimension two .",
    "( 14.5,5.0 ) ( 0.0,4.5 ) ( 1.5,0.0 )   \\times [ 1,2]$ ] and  @xmath48 \\times [ 2,3]$ ] , whose dimensions are one and two , respectively . their boundaries can easily be computed as @xmath49 \\times [ 1,1 ] + [ 1,1 ] \\times [ 2,2]$ ] , as well as @xmath50 \\times [ 2,3 ] + [ 3,3 ] \\times [ 2,3 ]             + [ 2,3 ] \\times [ 2,2 ] + [ 2,3 ] \\times [ 3,3]$ ] .",
    "note that in both cases , the formal notion of boundary is consistent with the intuitive boundary of that cube .",
    "_ ( b ) cycles _ : consider the cubical complex shown in gray in the right image , together with all contained edges and vertices .",
    "then both the collection of edges shown in red , and the collection shown in green form one - dimensional chains which are cycles , i.e. , their boundaries vanish .",
    "this is due to the fact that every cell of dimension zero in the support of each chain occurs in the boundary of exactly two cells , and therefore cancels out in the chain s boundary.,title=\"fig:\",height=188 ] ( 8.0,4.5 ) ( 9.5,0.0 )   \\times [ 1,2]$ ] and  @xmath48 \\times [ 2,3]$ ] , whose dimensions are one and two , respectively . their boundaries can easily be computed as @xmath49 \\times [ 1,1 ] + [ 1,1 ] \\times [ 2,2]$ ] , as well as @xmath50 \\times [ 2,3 ] + [ 3,3 ] \\times [ 2,3 ]             + [ 2,3 ] \\times [ 2,2 ] + [ 2,3 ] \\times [ 3,3]$ ] .",
    "note that in both cases , the formal notion of boundary is consistent with the intuitive boundary of that cube . _",
    "( b ) cycles _ : consider the cubical complex shown in gray in the right image , together with all contained edges and vertices",
    ". then both the collection of edges shown in red , and the collection shown in green form one - dimensional chains which are cycles , i.e. , their boundaries vanish .",
    "this is due to the fact that every cell of dimension zero in the support of each chain occurs in the boundary of exactly two cells , and therefore cancels out in the chain s boundary.,title=\"fig:\",height=188 ]    while elementary cubes form the building blocks of the cubical complexes introduced below , we first need to introduce a few algebraic concepts based on them .",
    "a _ chain _",
    "@xmath51 is a formal sum @xmath52 in this formula , @xmath21 denotes an arbitrary field .",
    "however , for the purposes of this paper , we always consider the field  @xmath53 which consists of the two elements  @xmath54 and  @xmath55 , as well as addition and multiplication modulo two .",
    "thus , for the chain  @xmath51 in  ( [ def : chain ] ) a cube  @xmath56 is either present or absent , depending on whether @xmath57 or @xmath58 , respectively .",
    "furthermore , the _ support _ of the chain  @xmath51 is the collection of all elementary cubes  @xmath56 which have nonzero coefficient  @xmath59 in  @xmath51 .",
    "two one - dimensional chains consisting of four one - dimensional elementary cubes each are shown in green and red in figure  [ fig : cubhom]_(b)_.    next we have to introduce the _ algebraic boundary _ of a chain . for an elementary interval @xmath60 $ ] , its boundary is the chain supported by the two degenerate intervals  @xmath61 $ ] and  @xmath62 $ ] , and this immediately implies both @xmath63 = [ n+1,n+1 ] + [ n , n ]    \\qquad\\mbox { as well as } \\qquad    \\partial [ n ,",
    "n ] = 0 \\ ; , \\ ] ] depending on whether @xmath64 or @xmath65 , respectively .",
    "note that if the interval  @xmath66 is degenerate , we have used the fact that @xmath67 = [ n , n ] + [ n , n ] = ( 1 + 1 ) [ n , n ] = 0 $ ] in the field  @xmath53 . combining the definition of the boundary for elementary intervals with the definition of elementary cubes , one",
    "can then lift the boundary definition to the latter using linearity .",
    "more precisely , let @xmath68 denote an elementary cube .",
    "then we define @xmath69 where @xmath70 \\times \\ldots \\times i_n = i_1 \\times \\ldots \\times [ m+1,m+1 ] \\times \\ldots \\times i_n + i_1 \\times \\ldots \\times [ m , m ] \\times \\ldots \\times i_n$ ] in the nondegenerate case , and @xmath71 \\times \\ldots \\times i_n = 0 $ ] in the degenerate one .",
    "finally , the boundary operator extends linearly from elementary cubes to chains , i.e. , for a chain @xmath72 one defines its boundary via @xmath73 .",
    "these concepts are illustrated further in figure  [ fig : cubhom ] .",
    "the boundary operator formalizes the intuitive notion of the boundary of a @xmath74-dimensional elementary cube . for @xmath75 ,",
    "such a cube is a point , and its boundary is zero , while for @xmath76 the cube is an interval with the boundary consisting of its endpoints .",
    "note , however , that this notion does not depend on the embedding dimension of the cube , in contrast to the topological boundary . in other words ,",
    "the boundary  @xmath77 is determined from the intrinsic dimension of the cube .",
    "this latter fact also lies at the heart of the fundamental property of the boundary operator , which states that @xmath78 one can easily see why this statement is true for elementary cubes .",
    "if  @xmath44 is an elementary cube of dimension  @xmath74 , then its boundary is a union of elementary cubes of dimension  @xmath79 .",
    "this implies that the chain  @xmath80 consists of cubes of dimension  @xmath81 .",
    "every one of these @xmath81-dimensional cubes is contained in the boundary of exactly two @xmath79-dimensional cubes in  @xmath77 , i.e. , the chain  @xmath80 contains every elementary cube exactly twice . since we are considering the field  @xmath53 , these cubes cancel out and yield a zero boundary .",
    "this is illustrated in figure  [ fig : cubhom]_(b)_.    as we mentioned earlier , elementary cubes are the fundamental building blocks for the topological spaces which can be treated using cubical homology .",
    "loosely speaking , we will consider subsets of  @xmath82 which can be obtained as unions of elementary cubes of embedding dimension  @xmath47 .",
    "more precisely , we say that a finite collection  @xmath83 of elementary cubes with embedding dimension  @xmath47 is a _ cubical complex _ , if for every cube  @xmath84 all elementary cubes in its boundary  @xmath77 are also contained in  @xmath83 . for a given cubical complex , we can then talk about its associated chains .",
    "for this , let  @xmath85 be fixed .",
    "then chains which consist exclusively of elementary cubes in  @xmath83 with dimension  @xmath74 are called _",
    "@xmath74-chains _ of  @xmath83 , and the set of all @xmath74-chains is denoted by  @xmath86 .",
    "one can easily see that  @xmath86 is an additive group , where again we use coefficients in  @xmath53 , and we will therefore refer to  @xmath86 as the _ @xmath87 chain group_. furthermore , if the boundary operator  @xmath88 is applied to a @xmath74-chain  @xmath51 , then  @xmath89 is clearly a @xmath79-chain .",
    "in fact , one can show that the map  @xmath90 is a group homomorphism .",
    "since we consider coefficients in the field  @xmath53 , even more is true : the chain groups  @xmath86 are vector spaces over  @xmath91 , and the maps  @xmath92 are all linear .",
    "we are now finally in a position to describe _",
    "cubical homology_. for this , let  @xmath83 denote a cubical complex",
    ". intuitively speaking , homology is used to count the number of @xmath74-dimensional holes in the cubical complex , for every @xmath85 . as a first step towards this goal",
    ", we call a @xmath74-chain @xmath93 a _ @xmath74-cycle _ , if its boundary vanishes , i.e. , if we have @xmath94 .",
    "the set of all @xmath74-cycles is denoted by  @xmath95 , and one can easily see that it is a subgroup of the @xmath87 chain group .",
    "for example , in figure  [ fig : cubhom]_(b ) _ , two @xmath55-cycles are shown in red and green .",
    "it is clear from this image , that cycles alone can not be used to detect holes in the cubical complex  @xmath83 .",
    "while the red @xmath55-cycle in the figure encloses a hole , the green one does not .    in order to distinguish between cycles which enclose holes and cycles which do not",
    ", we need to introduce one final concept . for this ,",
    "assume as before that  @xmath83 is a cubical complex and that @xmath85 .",
    "then a @xmath74-cycle is called a _",
    "@xmath74-boundary _ , if it is equal to the boundary of some @xmath96-chain .",
    "we denote the set of all @xmath74-boundaries by  @xmath97 , and since this set forms a subgroup of the @xmath87 chain group , it is called the _ @xmath87 boundary group_. in fact , due to the fundamental property  @xmath98 of the boundary operator , one has the inclusion @xmath99 , i.e. , the @xmath74-boundaries form a subgroup of the @xmath74-cycles .",
    "we now return to the two @xmath55-cycles shown in figure  [ fig : cubhom]_(b)_. notice that the green cycle is the boundary of the @xmath100-chain which consists of all elementary cubes in the interior of the green loop . in other words ,",
    "the green cycle is bounding a region of @xmath100-dimensional cubes in the cubical complex , which completely fill its interior . on the other hand",
    ", one can show that the red @xmath55-cycle can not be represented as the boundary of a @xmath100-chain , and this cycle does enclose a hole in the cubical set .",
    "the last example captures the essence of homology . in order to detect holes in a cubical complex",
    ", one needs to consider @xmath74-cycles , as they are the @xmath74-chains without boundary . on the other hand , one needs to make sure that such a @xmath74-cycle actually encloses a hole , and for this one has to make sure that the cycle is not the boundary of a @xmath96-chain in  @xmath83 .",
    "otherwise , this @xmath96-chain would fill up the holes created by the cycle .",
    "notice , however , that just counting the @xmath74-cycles which are not @xmath74-boundaries does not give the correct number of holes .",
    "in fact , in figure  [ fig : cubhom]_(b ) _ one can easily find @xmath55-cycles which are different from the red one , but which still enclose the same hole .",
    "while not quite as intuitive , one can show that two cycles enclose the same holes , if their difference happens to be a @xmath74-boundary .    from a mathematical point of view",
    ", counting the number of holes seems to be related to counting equivalence classes .",
    "call two @xmath74-cycles _ homologous _ , if their difference is contained in the boundary group  @xmath97 .",
    "this defines an equivalence relation on the set of all @xmath74-cycles .",
    "furthermore , a cycle is is homologous to zero if and only if it is a @xmath74-boundary , i.e. , if it does not enclose a hole .",
    "based on this discussion , it should not come as a surprise that the quotient groups @xmath101 capture information about @xmath74-dimensional holes in the cubical complex  @xmath83 .",
    "these groups are called _ homology groups _ , and since we consider coefficients in the field @xmath53 , they are in fact quotient vector spaces .",
    "the homology groups defined above are abstract algebraic objects which can be assigned to a cubical complex .",
    "but what exactly is their relation to the number of @xmath74-dimensional holes in  @xmath83 . to see this ,",
    "notice that if we have two @xmath74-cycles  @xmath102 and  @xmath103 which enclose to different holes in the cubical complex , then the equivalence class in  @xmath104 determined by their sum  @xmath105 encloses both holes . in other words , two different holes in  @xmath83 give rise to three different equivalence classes in the @xmath87 homology group .",
    "in fact , what we need are the _ independent _ classes that can be found in  @xmath104 , i.e. , we need to determine the size of a basis in the quotient vector space  @xmath104 , which is the same as the rank of the quotient group . if we now define @xmath106 then the number of @xmath74-dimensional holes in the cubical complex is given by  @xmath107 .",
    "furthermore , the @xmath87 homology group is isomorphic to  @xmath108 . intuitively , the zero - dimensional betti number counts the number of connected components of the cubical complex , while the one - dimensional betti number counts the number of tunnels or one - dimensional holes .",
    "similarly , the two - dimensional betti number counts the number of voids or cavities in the complex , and this classification of holes continues for higher dimensions . for example , the cubical complex in figure  [ fig : cubhom ] is connected and has one hole , but as a flat object no cavities .",
    "this implies both  @xmath109 and  @xmath110 , while  @xmath111 .",
    "the cubical homology theory introduced above provides a very rough quantitative measurement for the topological complexity of a complex  @xmath83 and its underlying topological space . to make this more precise ,",
    "let  @xmath112 denote the union of all elementary cubes in  @xmath83 .",
    "then one can show that the information provided by the homology groups is _ invariant _ under continuous transformations of  @xmath113 . specifically , if two cubical complexes  @xmath114 and  @xmath115 are such that their underlying topological spaces  @xmath116 and  @xmath117 are homeomorphic , then the homology groups  @xmath118 and  @xmath119 are isomorphic .",
    "in fact , this is still true even if the spaces  @xmath116 and  @xmath117 are only homotopy equivalent . in other words",
    ", homology captures connectivity and hole information that is invariant under continuous deformations .",
    "the homology groups introduced in the last section are topological invariants which can be assigned to a fixed topological space  @xmath113 . in applications , finding the `` correct '' space to study is often a straightforward task , but sometimes the situation is less clear .",
    "consider for example a gray - scale image , and suppose we are trying to use homology to detect holes in the image .",
    "one simple instance of this is shown in figure  [ fig : filtrationofanimage]_(a)_. in this @xmath120-pixel image , the lighter - colored pixels seem to enclose a hole given by the solid black pixel .",
    "if we are trying to use homology to detect this hole , we need to create a cubical complex which consists of the `` correct pixels '' of the image .",
    "this can be accomplished for example by choosing a gray - scale threshold , and then defining  @xmath83 to consist of all pixels with intensity less than that threshold , together with all their edges and vertices . depending on the threshold ,",
    "one obtains the cubical complexes shown in figure  [ fig : filtrationofanimage]_(b ) _ , and the third and fourth of these complexes do correctly identify the hole visible in the image .",
    "note , however , that the remaining three complexes fail to detect the hole .",
    "( 14.5,5.5 ) ( 0.0,4.0 ) ( 1.0,1.0 )   ( 5.5,4.7 ) ( 6.5,3.0 )   ( 5.5,1.7 ) ( 6.5,0.0 )     but what about a situation in which the image contains two holes , whose interiors occur at different gray - scale levels ? depending on the intensities of the respective surrounding pixels , it is conceivable that one can not find a threshold value which detects both holes at the same time . in situations like this ,",
    "finding one threshold level for the whole image clearly is the wrong goal .",
    "rather , it would be nice to have an automatic way which allows for the detection of holes at different levels  and using homology in its original form alone makes this a difficult task .    to address problems of the above type",
    ", the concept of _ persistent homology _ has been proposed , and it will be described in more detail in the following . for a literature review on the subject ,",
    "we refer the reader to  @xcite and the references therein .",
    "the fundamental idea behind persistent homology can easily be motivated by the complexes shown in figure  [ fig : filtrationofanimage ] . rather than singling out a specific cubical complex in  _ ( b ) _ , persistent homology deals with all of these complexes using the concept of a _",
    "filtration_. in the situation of the figure , the underlying filtration is the increasing ( with respect to set inclusion ) sequence of the five cubical complexes shown in  _ ( c ) _ , indexed by the threshold levels indicated by  ( 1 ) through  ( 5 ) . in other words ,",
    "a filtration encodes how the full image is assembled by adding pixels according to their gray - scale level , where pixels of the same level will be added simultaneously .",
    "even though we only mention the pixels , which are two - dimensional cubes , we implicitly assume that at every stage of the filtration we do have cubical complexes as defined in the last section .",
    "this means that the edges and vertices of the pixels have to be added at the same time as the pixel .",
    "this is indicated in figure  [ fig : filtrationofanimage]_(c ) _ by highlighting the one- and zero - dimensional cubes .",
    "this implies for example that at stage  ( 1 ) the shown cubical complex has four vertices , four edges , and one square , while at stage  ( 2 ) we have eight vertices , ten edges , and three squares .",
    "the goal of persistent homology is to detect and keep track of holes in this sequence of cubical complexes , as they grow from complex  ( 1 ) to complex  ( 5 ) . in the simple example of figure  [ fig : filtrationofanimage ]",
    ", persistent homology will allow us to deduce that a single one - dimensional hole can be observed starting with the third complex , and that it will disappear again at level  ( 5 ) .",
    "the example involving gray - scale images can easily be generalized .",
    "formally , a _ filtration of cubical complexes _ is a nested sequence of cubical complexes @xmath121 notice that since we require every  @xmath122 to be a cubical complex , if a cube of dimension  @xmath74 appears for the first time in  @xmath122 , all of its boundary elements have to appear at the latest in  @xmath122 , but they could appear earlier . while filtrations could be specified explicitly , one often convenient , and in fact equivalent , way of defining them is based on the concept of a _ filtering function_. this is a function  @xmath123 which assigns every cube  @xmath44 in the final cubical complex an integer between  @xmath55 and  @xmath124 in such a way that @xmath125 if and only if @xmath126 and @xmath127 , where we define  @xmath128 .",
    "one can readily see that for this filtering function we have @xmath129 equivalently , but more useful for practical applications , assume that  @xmath83 denotes a cubical complex and that we are given a function  @xmath130 such that the following holds : if  @xmath131 are any two cubes such that  @xmath132 is contained in the boundary of  @xmath133 , then the function  @xmath134 satisfies @xmath135 . in this case , we call  @xmath134 a _ filtering function _ on  @xmath83 , and  ( [ def : filteringfcomplex ] ) defines a filtration of cubical complexes with  @xmath136 .",
    "we would like to point out that the above monotonicity property of  @xmath134 from a cube to its boundary is all that is needed to ensure that every  @xmath122 is indeed a cubical complex .",
    "the above concept of a filtering function can easily be related to our image example from figure  [ fig : filtrationofanimage ] . in this case , let  @xmath83 denote the cubical complex consisting of all sixteen pixels , together with their edges and vertices . furthermore , suppose that the gray - scale intensities are given by the five values  @xmath137 .",
    "then for every two - dimensional pixel  @xmath133 in  @xmath83 we define  @xmath138 , if its gray - scale intensity is given by  @xmath139 . for an edge  @xmath140 ,",
    "we let  @xmath141 be the smallest value of  @xmath134 on a pixel which contains  @xmath132 as an edge , and for a vertex  @xmath84 , the value  @xmath142 is the smallest value of  @xmath134 on an edge which contains  @xmath44 .",
    "then  ( [ def : filteringfcomplex ] ) leads to the filtration shown in figure  [ fig : filtrationofanimage]_(c)_. this example also shows that choosing  @xmath143 as the index set for a filtration of cubical complexes can be done without loss of generality  through a simple transformation , any finite and discrete subset of  @xmath144 can be mapped onto this set . alternatively , we could keep the values of the filtering function in the set  @xmath145 , and index the cubical complexes by these distinct real numbers .",
    "this extension will be useful later on .",
    "having introduced the concept of a filtration , we now turn our attention to persistent homology .",
    "suppose we are given a filtration of cubical complexes as in  ( [ def : filtration ] ) .",
    "then for each dimension  @xmath74 and each complex in the sequence we obtain a homology group  @xmath146 .",
    "note that these homology groups are in general not ordered by inclusion , despite the fact that the underlying cubical complexes are .",
    "however , if  @xmath51 denotes an arbitrary @xmath74-cycle in  @xmath122 , then one can readily see that  @xmath51 is also a @xmath74-cycle in  @xmath147 .",
    "this implies that we can define a map from  @xmath146 to  @xmath148 by mapping the equivalence class of  @xmath51 in the first homology group to the equivalence class of  @xmath51 in the second one . in order words , the inclusions in  ( [ def : filtration ] ) lead to a sequence of maps @xmath149 and",
    "these maps are in fact group homomorphisms .",
    "it was shown in the last section that if  @xmath150 is a nontrivial homology class , and if  @xmath51 is a @xmath74-chain representing this class , then  @xmath51 encloses a @xmath74-dimensional hole in  @xmath122 .",
    "now consider the class  @xmath151 induced by  @xmath51 in  @xmath148 .",
    "if this equivalence class is nontrivial , then  @xmath51 still encloses a hole in  @xmath147 . on the other hand ,",
    "if the equivalence class  @xmath151 is trivial , then the hole has been filled in by cubes that were added to  @xmath122 during the formation of  @xmath147 . in other words ,",
    "the homomorphisms induced by inclusion allow us to detect the disappearance of holes .",
    "note , however , that holes can also disappear in a second way . rather than being mapped to a trivial homology class , it is also possible that two different homology classes  @xmath152 are mapped to the same nontrivial class in  @xmath148 , and this case will be discussed in more detail later .    what about the appearance of holes ?",
    "suppose again that  @xmath153 is a nontrivial homology class representing a @xmath74-dimensional hole in  @xmath122 .",
    "one can easily see that if this hole was already present in  @xmath154 , then  @xmath155 has to be in the image of the inclusion - induced homomorphism from  @xmath156 to  @xmath146 .",
    "in other words , the hole  @xmath155 appears for the first time in  @xmath122 , if and only if it is not in the range of this homomorphism .",
    "being able to track the appearance and disappearance of holes in the filtration is the main idea behind _",
    "persistent homology_. while it is usually defined via the so - called _ persistent homology groups _ , which are just the images of the above - mentioned maps between homology groups induced by inclusion",
    ", we proceed right away to an equivalent formulation which is based on the _ lifespan of homology classes_. we say that a @xmath74-dimensional homology class  @xmath157 is _ born _ in the complex  @xmath158 , if we have both @xmath159 and @xmath160 , where  @xmath161 is induced by inclusion .",
    "moreover , we say that a @xmath74-dimensional homology class  @xmath157 _ dies _ in the complex  @xmath162 , if  @xmath163 is the first homology group where  @xmath157 either became trivial , or where it became identical to another class which was born earlier in the filtration .",
    "note that the first of these cases , corresponds to the disappearance of a hole discussed above , while the second case addresses the issue of two nontrivial homology classes merging  in this case , the class that was created later in the filtration dies , while the one which was created earlier survives .",
    "in other words , merging effects can be treated in a unique way , through a process called the _",
    "elder rule_.    using the definitions of the last paragraph , persistent homology provides us with a list of nontrivial and independent homology classes , together with their birth times and death times .",
    "if  @xmath155 is one of these classes and if its birth and death times are given by  @xmath164 and  @xmath43 , respectively , then the interval  @xmath165 is called the _ persistence interval _ associated with  @xmath155 , and the integer  @xmath166 is called its _",
    "lifespan_. if there are classes which are born , but never die , then we set @xmath167 . in typical applications ,",
    "persistent homology classes with relatively long lifespan are considered to be more important than the ones that have shorter lifespan .",
    "( 12,7 ) ( 0.0,0.0 ) , two connected components are born , i.e. , two nontrivial zero - dimensional homology classes .",
    "( 2 )  in  @xmath115 , another two connected components are born .",
    "( 3 )  in the complex  @xmath168 , the two connected components born in  @xmath115 die , since each of them merges with a component which was born in  @xmath114  these latter two components still live on . (",
    "4 )  in  @xmath169 , however , the remaining two components merge , so one dies and the other survives .",
    "since both were born at the same time , one can randomly choose which one survives .",
    "( 5 )  in the complex  @xmath170 a @xmath55-dimensional cycle is created , which gives rise to a nontrivial homology class .",
    "( 6 )  this @xmath55-dimensional class dies in the final complex  @xmath171.,title=\"fig:\",height=264 ]    the lifespans of homology classes are illustrated in figure  [ fig : persistence ] .",
    "as described in the caption , this figure shows a filtration of six cubical complexes in the plane .",
    "persistent homology yields five different homology classes .",
    "four of these are in dimension zero , and their persistence intervals are given by  @xmath172 , @xmath173 , @xmath174 , and again  @xmath174 , respectively .",
    "the one nontrivial class in dimension one has persistence interval  @xmath175 .",
    "the persistence intervals provide information about when homology classes are born , evolve , and die in the filtration .",
    "but they also can be used to recover the homologies of the involved cubical complexes .",
    "it can be shown that the @xmath87 betti number of the complex  @xmath122 in the filtration is given by the number of persistence intervals in dimension  @xmath74 which contain  @xmath176 .",
    "for example , in the situation of figure  [ fig : persistence ] , the complex  @xmath168 has betti number  @xmath177 , since only the intervals  @xmath172 and  @xmath173 contain the index  @xmath178 .",
    "we would like to pint out again , however , that persistent homology provides considerably more information than the sequence of betti numbers of the complexes  @xmath122 , as it allows us to track for how long specific homology classes survive .",
    "( 14,7 ) ( 0.0,0.0 ) .",
    "the left image shows the persistence diagram in dimension zero , which contains the points  @xmath179 and @xmath180 once , and the point  @xmath181 twice .",
    "these multiplicities are indicated by the numbers adjacent to the points .",
    "the right image contains the diagram for dimension one , which only contains the single point  @xmath182.,title=\"fig:\",height=264 ]    while the persistence intervals encode all of the information provided by persistent homology , dealing with the intervals directly is fairly cumbersome . in practice , these intervals are visualized in a _ persistence diagram _ as follows .",
    "fix a dimension  @xmath74 .",
    "then the persistence diagram in dimension  @xmath74 is a finite collection of points ( possibly of multiplicity larger than one ) in the plane such that  @xmath183 is a point in the @xmath87 diagram if and only if persistent homology provides a nontrivial @xmath74-dimensional homology class with interval  @xmath184 . in other words ,",
    "a persistence diagram is a multiset of points in  @xmath185 which corresponds to the persistence intervals .",
    "notice that all of these points lie above the diagonal in the first quadrant of  @xmath185 .",
    "for the filtration in figure  [ fig : persistence ] the persistence diagrams in dimensions zero and one are shown in figure  [ fig : persistencediagram ] .",
    "for our special situation of a filtration indexed by integers as in  ( [ def : filtration ] ) , the points in a persistence diagram always have integer coordinates . however , if one considers a cubical complex  @xmath83 together with a filtering function @xmath186 , one can consider the associated filtration @xmath187 in this more general setting , persistent homology produces persistence diagrams in a completely analogous way , but now with points of the form  @xmath188 which lie above the main diagonal . as mentioned before",
    ", the values  @xmath189 could be the gray - scale intensities in an underlying image . what happens if these values are slightly perturbed ?",
    "one would expect that new persistence diagram is only a slight perturbation of the old one , provided the perturbations are small enough .",
    "one of the main features of persistent homology is that this perturbation idea can be quantified in a most satisfying way . in order to do this",
    ", one needs to introduce a metric on the space of persistence diagrams , and there are a number of ways for doing this  @xcite . for the purposes of this paper , we focus on the so - called _ bottleneck distance _ , which is closely related to the earth mover s distance discussed in  @xcite . to define this distance ,",
    "consider two persistence diagrams  @xmath190 and  @xmath191 , both in the same dimension .",
    "note , however , that these persistence diagrams could be created from two completely different filtrations .",
    "let us now assume that in addition to the finitely many points corresponding to persistence intervals , we add all points of the form  @xmath192 with  @xmath193 to both diagrams , each of these points with infinite multiplicity .",
    "while this might seem strange at first glance , it is necessary for comparing persistence diagrams with different number of points .",
    "we denote the resulting new diagrams by  @xmath194 and  @xmath195 , respectively .",
    "after these preparations , we can finally define the bottleneck distance of the diagrams  @xmath190 and  @xmath191 , which will be denoted by  @xmath196 and is defined as @xmath197 in other words , the bottleneck distance tries to find a matching @xmath198 which minimizes the largest distance between a point  @xmath199 and its image  @xmath200 .",
    "note that obtaining matchings of this type is made possible only through extending the persistence diagrams by the points on the diagonals , since  @xmath190 and  @xmath191 could have very different numbers of points . since points on the diagonal would correspond to homology classes which die immediately when born , this should not alter the information content of the persistence diagrams .",
    "intuitively , the bottleneck distance between two diagrams  @xmath190 and  @xmath191 is of size  @xmath20 , if it is possible to move every point of  @xmath190 to a point in  @xmath191 or the diagonal by less than distance  @xmath20 , as well as vice versa .",
    "this is illustrated in figure  [ fig : bottelneckdistance ] . as we mentioned before , typically , persistent homology classes with relatively long lifespan",
    "are considered to be more important than the ones that have shorter lifespan  and the latter ones give rise to points in the persistence diagram which are closer to the diagonal .",
    "this implies that these are the points that can easily be moved onto the diagonal in a matching .",
    "( 17.0,5.0 ) ( 0.0,0.0 )   and  @xmath191 shown in the first two images , the third image contains a matching @xmath201 in which points have to be moved only over fairly short distances.,title=\"fig:\",height=188 ] ( 12.0,0.0 )   and  @xmath191 shown in the first two images , the third image contains a matching @xmath201 in which points have to be moved only over fairly short distances.,title=\"fig:\",height=188 ]    we now return to filtrations which are generated through a filtering function  @xmath202 as defined in  ( [ def : filteringfcomplex2 ] ) .",
    "how do perturbations of  @xmath134 affect the resulting persistence diagrams ? to describe this , let us consider two filtering functions  @xmath203 and  @xmath204 on the same cubical complex  @xmath83 , but we do not assume that the finite ranges of  @xmath203 and  @xmath204 are the same .",
    "if we further denote the persistence diagrams in dimension  @xmath74 associated with  @xmath203 and  @xmath204 by  @xmath205 and  @xmath206 , respectively , then one can prove a _ stability theorem _ in the form of the estimate @xmath207 where  @xmath208 denotes the usual maximum norm .",
    "see  @xcite for more details .",
    "in other words , the bottleneck distance of the two persistence diagrams is bounded by the maximal function value difference of the two filtering functions .",
    "the stability theorem encapsulated by  ( [ th : stabilitytheorem ] ) is one of the central properties of persistence diagrams , which is invaluable in applications .",
    "it implies , for example , that if observational data is perturbed by noise which is bounded by a constant  @xmath20 , then the persistence diagram of the noisy data will be at most  @xmath20 away from the persistence diagram of a true underlying data . in other words , persistence is a robust topological metric . in the context of this paper , this robustness property allows us to compute persistence diagrams of finite - dimensional spectral approximations to the true solution  @xmath5 of the cahn - hilliard - cook equation  ( [ chc ] ) , which are basically indistinguishable from the true underlying persistence diagrams due to spectral accuracy .    in closing , and for the sake of completeness , we would like to point out that the bottleneck distance is only one possibility for measuring the distance between persistence diagrams .",
    "in fact , in some situations its insistence on only registering the largest move distance  @xmath209 is too stringent . in such cases",
    "one can make use of the _ wasserstein distance _",
    "@xcite , which is defined as @xmath210 for @xmath211 .",
    "this distance has the advantage of taking all line segments in a matching into account , but it comes at the price of no longer satisfying the stability result  ( [ th : stabilitytheorem ] ) .",
    "however , more elaborate stability estimates which are valid for the wasserstein distance are available in a number of situations , and we refer the reader to  @xcite for more information .      by moving from the concept of homology groups to the concept of persistent homology , we have made the transition from thresholding a gray - scale image at one specific level to considering the fine structure of the image as we pass through all possible shades of gray . as we mentioned in the introduction ,",
    "the goal of this paper is to show that the topology of material microstructures such as the ones in figure  [ fig : chcpatt ] encodes interesting facts about the underlying dynamics .",
    "after suitable discretization these microstructures can be represented as gray - scale images , with the value of the solution  @xmath5 of  ( [ chc ] ) serving as the filtration parameter .",
    "recall , however , that the cahn - hilliard - cook model is a stochastic partial differential equation , and therefore with probability one no two realizations of the dynamics will lead to identical patterns . in order to uncover this `` typical '' behavior , one would therefore like to consider statistical properties , in particular , averages of the topological information .",
    "while averages of betti numbers are easily accessible , and have in fact been used for example in  @xcite , it is not immediately clear how the average of persistence diagrams can be computed .",
    "one of the first attempts at defining the mean of two persistence diagrams is the notion of _ frchet mean _ which was explored in  @xcite . without going into too much detail ,",
    "assume we are given two persistence diagrams  @xmath190 and  @xmath191 , and assume that we have found a matching  @xmath212 which realizes the @xmath213-wasserstein distance  @xmath214 , for some @xmath211 .",
    "then the frchet mean of the diagrams  @xmath190 and  @xmath191 is defined as the collection of midpoints of the line segments between  @xmath215 and  @xmath216 , for all @xmath199 . despite the fact that the wasserstein distance between  @xmath190 and  @xmath191 takes the length of all matching distances @xmath217 into account",
    ", one can easily see that the above notion of frchet mean is not well - defined .",
    "consider for example a diagram  @xmath190 which consists of the points  @xmath218 and  @xmath219 , while the diagram  @xmath191 has the points  @xmath220 and  @xmath221 , and where we assume that @xmath222 .",
    "then there are two minimal matchings which involve these points : one matching maps along two horizontal line segments of length one , while the other one uses two vertical line segments of length one .",
    "this implies that in the former case the frchet mean consists of the points  @xmath223 and  @xmath224 , whereas in the latter case it is given by the points  @xmath225 and  @xmath226 .",
    "it is not surprising that this obvious non - uniqueness of the frchet mean makes a statistical analysis of persistence diagrams which involves this concept nontrivial . from a mathematical point of view",
    ", these problems can be resolved , see for example  @xcite .",
    "unfortunately , however , these ideas have not yet been implemented algorithmically , as they pose serious technical and computational difficulties . for the purposes of the present paper",
    "we therefore pursue a different approach , which uses the concept of _ persistence landscapes_. these objects were introduced in  @xcite , and they prove to be an algorithmically feasible tool for the statistical analysis of persistence diagrams .    in order to motivate the definition of persistence landscapes , we return one final time to the frchet mean .",
    "its non - uniqueness stems from the fact that it operates on a discrete set of points . in some sense",
    ", this situation strongly resembles the problem of computing the mean of two integers within the ring of integers .",
    "also in this case , the mean is often not unique . in the integer case",
    ", however , the mean problem can be solved easily by introducing rational numbers .",
    "we will see below that the viewpoint behind persistence landscapes is very similar  it is based on the idea of embedding persistence diagrams into the larger space of piecewise linear functions , where the mean is uniquely defined . for the remainder of this section , we present an intuitive introduction to persistence landscapes , rather than a formal definition . for a more detailed discussion",
    "we refer the reader to  @xcite .",
    "important for our applications is the fact that persistence landscapes can be computed and manipulated efficiently , and algorithms for this can be found in  @xcite .    to begin with ,",
    "suppose we are given a point  @xmath227 in a persistence diagram , i.e. , the point corresponds to a homology class with birth time  @xmath228 and death time  @xmath229 . with this point , we associate a piecewise linear function  @xmath230 , which is defined as @xmath231              x - b & \\mbox { if } & x \\in \\left ( b , \\frac{b+d}{2 }                \\right ] \\ ; , \\\\[2ex ]              d - x & \\mbox { if } & x \\in \\left(\\frac{b+d}{2 } ,                d \\right ) \\ ; .",
    "\\end{array } \\right.\\ ] ] in other words , the function  @xmath232 is a hat function , whose maximal value is  @xmath233 , i.e. , half the lifespan of the homology class represented by the point  @xmath234",
    ". moreover , this global maximum of  @xmath232 occurs at the center of the associated persistence interval , and on either side the function decays linearly towards zero with slopes  @xmath235 .",
    "in fact , there is another , more geometric way of describing  @xmath232 . for this , map the point  @xmath234 in the persistence diagram via the linear map @xmath236 and then draw lines with slopes  @xmath235 from the image of the point towards the horizontal axis .",
    "clearly , the function  @xmath232 encodes the original data provided by the point  @xmath234 , but in a more convenient way : the support of the function equals the closure of the persistence interval  @xmath237 , while the height of the function is half the lifespan . note also that the map  ( [ def : pltransformation ] ) transforms the main diagonal into the horizontal axis .",
    "this procedure is illustrated in figure  [ fig : persistencelandscapeillustration]_(a ) _ , _ ( b ) _ , and  _",
    "( c)_.    ( 15.0,10.0 ) ( 0.0,0.0 ) ) .",
    "( c )  depicts the piecewise linear functions  @xmath232 defined in  ( [ eq : basicland ] ) for every one of the transformed points .",
    "finally , the graph in ( d )  illustrates the first persistence landscape function  @xmath238 in red , which is the upper envelope of the graphs from the previous picture .",
    "the second and third persistence landscape functions  @xmath239 and  @xmath240 are shown in green and blue in  ( e ) and  ( f ) , respectively , and they follow the second- and third - largest function values of the functions  @xmath232 in image  ( c ) . all subsequent persistence landscape functions  @xmath241 are identically zero.,title=\"fig:\",height=377 ]    after these preparations , we can now introduce the concept of persistence landscape .",
    "as was mentioned earlier , in many applications a homology class is considered important , if its lifespan  @xmath242 is large .",
    "since the heights of the functions  @xmath232 shown in figure  [ fig : persistencelandscapeillustration]_(c ) _ are proportional to the lifespan , the functions close to the top are therefore considered more significant than the ones closer to the horizontal axis . motivated by this observation , the _ persistence landscape _ of the birth - death pairs  @xmath243 , where @xmath244 , which constitute the given persistence diagram is the sequence of functions @xmath245 for @xmath246 , where  @xmath247 denotes the @xmath87 largest value of the numbers  @xmath248 , for @xmath249 , and we define @xmath250 if @xmath251 .",
    "equivalently , this sequence of functions can be combined into a single function @xmath252 of two variables , if we define @xmath253 .",
    "see again figure  [ fig : persistencelandscapeillustration ] for an illustration of these concepts .",
    "one can easily see that the persistence landscape  @xmath254 encodes exactly the same information as the underlying persistence diagram . yet , in some sense , this information is presented in a way which is mathematically more accessible .",
    "first of all , it creates a filtered representation of the persistence diagram which is premised on the assumption that homology classes with large lifespans are more important than ones with shorter lifespans .",
    "this is reflected in part by the monotonicity property @xmath255 secondly , and most importantly , it represents the persistence data as a piecewise linear function  @xmath254 , and averaging functions of this type results again in a piecewise linear function .",
    "moreover , the monotonicity property  ( [ lmonotone ] ) is transferred from the involved landscapes to the average .",
    "thus , the introduction of persistence landscapes allows us to easily talk about the average of persistence information .",
    "assume we are given persistence landscapes  @xmath256 .",
    "then the _ averaged persistence landscape _ is defined by  @xmath257 , and it is again a piecewise linear function which satisfies  ( [ lmonotone ] ) .",
    "in addition , it is clearly uniquely defined . but considering the persistence information in the form of a real - valued function defined on  @xmath258 has another advantage . for such functions",
    ", we can use the standard @xmath259-metric to quantify how `` different '' two landscape functions are .",
    "in fact , it was shown in  @xcite that persistence landscapes are stable with respect to the @xmath259-metric , i.e. , small changes in the filtering function which generates the underlying persistence diagram leads to small changes in the resulting persistence landscape with respect to the @xmath259-norm .    also from a computational point of view persistence landscapes",
    "are useful .",
    "the construction of persistence landscapes has been implemented by the first author of this paper in a library called persistence landscape toolbox , see  @xcite for more details .",
    "the library is publicly available and can be downloaded from the first author s webpage .",
    "it implements various manipulations of persistence landscapes , including the statistical classifier which will be heavily used in the analysis of the present paper , and which will be described in more detail in the next section .",
    "we now turn to the analysis of the cahn - hilliard - cook model using persistence landscapes .",
    "we begin in section  [ sec : basicmethod ] by introducing the basic methodology , including a description of the persistence landscapes based statistical classifier and of the numerical simulation techniques .",
    "after that , section  [ sec : chcmass ] is concerned with showing that the topology evolution of  ( [ chc ] ) encodes the central mass parameter  @xmath32 , while section  [ sec : chctime ] illustrates that even the decomposition stage can be recovered accurately from the topological information .      in order to generate the microstructures for the persistence landscape analysis",
    ", we simulated the cahn - hilliard - cook model  ( [ chc ] ) at the parameter values @xmath260 and on the square domain  @xmath261 .",
    "all simulations were performed in  c++ using a linearly implicit spectral method .",
    "more precisely , the solution  @xmath262 for @xmath263 and @xmath264 is approximated using the fourier expansion @xmath265 where the constants  @xmath266 are chosen in such a way that the functions @xmath267 form a complete orthonormal set in  @xmath268 , for @xmath269 .",
    "also the white noise process  @xmath35 is approximated in this way , i.e. , we use cut - off noise which acts independently on all  @xmath270 fourier modes in the representation  ( [ def : spectralapprox ] ) .",
    "recall that the noise is assumed to be mass - conserving , so it does not act on the constant mode . throughout this paper ,",
    "the simulations use @xmath271 , and to avoid aliasing effects , the necessary fourier coefficients of the nonlinearity  @xmath272 are computed using the two - dimensional discrete cosine transform on a grid of size  @xmath273 .    for a variety of different mass values  @xmath32",
    ", we then performed the following simulations .",
    "let  @xmath274 denote a randomly chosen initial condition with average  @xmath32 which satisfies @xmath275 then the above numerical method is used to compute the solution  @xmath5 of the cahn - hilliard - cook model  ( [ chc ] ) with @xmath276 for times @xmath277 where we recall that @xmath278 .",
    "while the choice of simulation endtime might seem strange at first sight , it takes into account the different instability strengths of the homogeneous state  @xmath32 as a function of the mass .",
    "in fact , by choosing the endtime  @xmath279 as above every simulation leads to basically the same phase separation horizon .",
    "the time interval is discretized using  @xmath280 steps , and solution snapshots are recorded at a variety of times , as will be described in more detail below .",
    "( 16.0,9.9 ) ( 0.0,6.6 ) ) . from top to bottom",
    ", the rows correspond to mass @xmath281 , @xmath282 , and  @xmath283 , respectively . from left to right , the respective columns use solution snapshots at times @xmath284 , @xmath285 , and  @xmath279.,title=\"fig:\",width=188 ] ( 5.5,6.6 ) ) . from top to bottom , the rows correspond to mass @xmath281 , @xmath282 , and  @xmath283 , respectively . from left to right , the respective columns use solution snapshots at times @xmath284 , @xmath285 , and  @xmath279.,title=\"fig:\",width=188 ] ( 11.0,6.6 ) ) . from top to bottom , the rows correspond to mass @xmath281 , @xmath282 , and  @xmath283 , respectively . from left to right , the respective columns use solution snapshots at times @xmath284 , @xmath285 , and  @xmath279.,title=\"fig:\",width=188 ] ( 0.0,3.3 ) ) . from top to bottom , the rows correspond to mass @xmath281 , @xmath282 , and  @xmath283 , respectively . from left to right , the respective columns use solution snapshots at times @xmath284 , @xmath285 , and  @xmath279.,title=\"fig:\",width=188 ] ( 5.5,3.3 ) ) . from top to bottom , the rows correspond to mass @xmath281 , @xmath282 , and  @xmath283 , respectively . from left to right , the respective columns use solution snapshots at times @xmath284 , @xmath285 , and  @xmath279.,title=\"fig:\",width=188 ] ( 11.0,3.3 ) ) . from top to bottom , the rows correspond to mass @xmath281 , @xmath282 , and  @xmath283 , respectively . from left to right , the respective columns use solution snapshots at times @xmath284 , @xmath285 , and  @xmath279.,title=\"fig:\",width=188 ] ( 0.0,0.0 ) ) . from top to bottom , the rows correspond to mass @xmath281 , @xmath282 , and  @xmath283 , respectively . from left to right , the respective columns use solution snapshots at times @xmath284 , @xmath285 , and  @xmath279.,title=\"fig:\",width=188 ] ( 5.5,0.0 ) ) . from top to bottom , the rows correspond to mass @xmath281 , @xmath282 , and  @xmath283 , respectively . from left to right , the respective columns use solution snapshots at times @xmath284 , @xmath285 , and  @xmath279.,title=\"fig:\",width=188 ] ( 11.0,0.0 ) ) . from top to bottom , the rows correspond to mass @xmath281 , @xmath282 , and  @xmath283 , respectively . from left to right , the respective columns use solution snapshots at times @xmath284 , @xmath285 , and  @xmath279.,title=\"fig:\",width=188 ]    ( 16.0,9.9 ) ( 0.0,6.6 ) ) . from top to bottom , the rows correspond to mass @xmath281 , @xmath282 , and  @xmath283 , respectively . from left to right , the respective columns use solution snapshots at times @xmath284 , @xmath285 , and  @xmath279.,title=\"fig:\",width=188 ] ( 5.5,6.6 ) ) . from top to bottom , the rows correspond to mass @xmath281 , @xmath282 , and  @xmath283 , respectively . from left to right , the respective columns use solution snapshots at times @xmath284 , @xmath285 , and  @xmath279.,title=\"fig:\",width=188 ] ( 11.0,6.6 ) ) . from top to bottom , the rows correspond to mass @xmath281 , @xmath282 , and  @xmath283 , respectively . from left to right , the respective columns use solution snapshots at times @xmath284 , @xmath285 , and  @xmath279.,title=\"fig:\",width=188 ] ( 0.0,3.3 ) ) . from top to bottom , the rows correspond to mass @xmath281 , @xmath282 , and  @xmath283 , respectively . from left to right , the respective columns use solution snapshots at times @xmath284 , @xmath285 , and  @xmath279.,title=\"fig:\",width=188 ] ( 5.5,3.3 ) ) . from top to bottom , the rows correspond to mass @xmath281 , @xmath282 , and  @xmath283 , respectively . from left to right , the respective columns use solution snapshots at times @xmath284 , @xmath285 , and  @xmath279.,title=\"fig:\",width=188 ] ( 11.0,3.3 ) ) . from top to bottom , the rows correspond to mass @xmath281 , @xmath282 , and  @xmath283 , respectively . from left to right , the respective columns use solution snapshots at times @xmath284 , @xmath285 , and  @xmath279.,title=\"fig:\",width=188 ] ( 0.0,0.0 ) ) . from top to bottom , the rows correspond to mass @xmath281 , @xmath282 , and  @xmath283 , respectively . from left to right , the respective columns use solution snapshots at times @xmath284 , @xmath285 , and  @xmath279.,title=\"fig:\",width=188 ] ( 5.5,0.0 ) ) . from top to bottom , the rows correspond to mass @xmath281 , @xmath282 , and  @xmath283 , respectively . from left to right , the respective columns use solution snapshots at times @xmath284 , @xmath285 , and  @xmath279.,title=\"fig:\",width=188 ] ( 11.0,0.0 ) ) . from top to bottom , the rows correspond to mass @xmath281 , @xmath282 , and  @xmath283 , respectively . from left to right , the respective columns use solution snapshots at times @xmath284 , @xmath285 , and  @xmath279.,title=\"fig:\",width=188 ]    the solution snapshots  @xmath286 obtained through this procedure are then analyzed using persistence landscapes . as a first step ,",
    "the spatial domain is discretized into  @xmath273 equal - size subsquares , and on each square the function  @xmath5 is approximated using its function value at the center of the subsquare . in this way , we obtain a pixelated image version of the microstructure , as shown in figure  [ fig : chcpatt ] , where the function values of  @xmath5 take the role of the gray - scales in the previous section .",
    "next , the solution range  @xmath287 $ ] is discretized into  @xmath288 equal - sized intervals , and the persistence diagrams are computed by considering sublevel sets of  @xmath286 as the threshold parameter increases in  @xmath288 steps from  @xmath289 to  @xmath55 .",
    "notice that since the solution  @xmath5 could take values slightly larger than one , we actually consider the function  @xmath290 instead of  @xmath5 . in the language of section",
    "[ sec : intropers ] , we consider the cubical complex  @xmath83 which consists of  @xmath273 equal squares to form the unit square  @xmath2 ( where one unit in the cubical complex representation corresponds to  @xmath291 units in the @xmath2-scale ) , and the filtering function  @xmath292 $ ] evaluated at the centers of the subsquares . together with  ( [ def : filteringfcomplex2 ] ) this leads to a filtration  @xmath293 , where the thresholds  @xmath139 are chosen uniformly between  @xmath289 and  @xmath294 .",
    "the persistence diagrams are computed using the software phat  @xcite , together with an implementation of cubical complexes .",
    "those computations can also be performed with perseus  @xcite .",
    "the associated persistence landscapes are then determined using  @xcite .",
    "some of the resulting persistence landscapes can be found in figures  [ fig : pldchdim0 ] and  [ fig : pldchdim1 ] for dimensions zero and one , respectively . as we pointed out in the last section",
    ", none of the patterns discussed in our setting will have non - trivial two - dimensional homology , as the underlying two - dimensional cubical complexes are flat .",
    "due to the stochastic nature of the cahn - hilliard - cook model , the above simulations are repeated many times in a monte - carlo type fashion . in this way",
    ", we created three data sets that will then be analyzed further :    * the first data set considers the  @xmath295 mass values @xmath296 for @xmath297 , and in each case five different solution paths of  ( [ chc ] ) , i.e. , five different initial conditions  @xmath298 .",
    "along the solution , snapshots are saved at times @xmath299 for @xmath300 . *",
    "the second data set considers the  @xmath301 mass values @xmath302 for @xmath303 , and in each case fifty different solution paths of  ( [ chc ] ) . along",
    "all solution paths , snapshots are saved at times @xmath299 for @xmath300 . * the third and last data set considers the  @xmath295 mass values @xmath296 for @xmath297 , and in each case  @xmath304 different solution paths of  ( [ chc ] ) .",
    "along each solution , snapshots are saved at times @xmath305 for @xmath306 .",
    "thus , in each of the three data sets we consider  @xmath307 different mass values  @xmath32 , where @xmath308 , respectively , and for each simulation we obtain solution snapshots at  @xmath309 different times , where  @xmath310 , respectively .",
    "every one of these solution snapshots leads to two associated persistence landscapes , one each in dimensions zero and one .    we would like to briefly comment on the fact that all of our data sets consider non - negative mass values  @xmath311 .",
    "based on our thresholding process described above , we study sublevel sets of the function  @xmath5 as the threshold increases from  @xmath289 to  @xmath294 . in figure",
    "[ fig : chcpatt ] , these sublevel sets for the threshold  @xmath312 are shown in red . thus , for mass values  @xmath313 the sublevel sets always have nontrivial @xmath55-dimensional homology , which in fact measures the bulk or interior behavior of the material , as described in  @xcite .",
    "in contrast , one can easily see that for mass values @xmath314 the sublevel sets usually have trivial first homology , i.e. , the first homology dimension is useless for topological classification .",
    "based on the above topological data , our goal in the next two sections is a statistical study to solve inverse problems for the total mass  @xmath32 and the decomposition stage . while this could be done in a number of ways , we concentrate on one statistical classification method which has been implemented in  @xcite and which we recall now .",
    "suppose we are given  @xmath307 different classes  @xmath315 of persistence landscapes .",
    "assume further that each class  @xmath316 consists of  @xmath317 different landscapes  @xmath318 , for @xmath319 .",
    "more precisely , in every class  @xmath316 we have  @xmath317 different persistence landscapes  @xmath320 , @xmath319 , in dimension  @xmath74 , where  @xmath321 .",
    "our classification scheme will be based on the previously mentioned @xmath259-distances between persistence landscapes , and therefore we choose and fix a real number  @xmath322 $ ] .",
    "suppose we are given a new persistence landscape  @xmath254 and would like to decide which of the classes  @xmath315 it belongs to , based only on the persistence information in dimension @xmath321 .",
    "then we use the following classification scheme :    * suppose  @xmath307 classes of persistence landscapes are given as above . then for each @xmath323 we define the _ average classifier _",
    "@xmath324 of the  @xmath325 class in dimension  @xmath74 via @xmath326 now let  @xmath254 be any other persistence landscape which we would like to classify .",
    "then we say that _ @xmath254 has been classified to belong to class  @xmath316 using dimension  @xmath74 _ , if there exists a unique index @xmath327 such that @xmath328 if no such unique index  @xmath47 exists , then we say that _ the classifier fails to classify  @xmath254_.    intuitively , the classification scheme  ( ck ) is easy to understand .",
    "based on the different persistence landscapes available in a given class , we compute their average to determine the `` typical '' persistence behavior in this class .",
    "given a new persistence landscape  @xmath254 , one then uses the average classifier which lies closest to  @xmath254 to determine the class which most likely contains  @xmath254 .",
    "the above classification scheme  ( ck ) in fact describes a whole family of classification schemes , indexed by the homology dimension  @xmath74 .",
    "in our situation , only the cases  @xmath75 and  @xmath76 lead to nontrivial persistence landscapes , and therefore we will use only schemes  ( c0 ) and  ( c1 ) in the following .",
    "note , however , that in some situations one might like to use all available topological information , i.e. , all dimensions  @xmath74 , at the same time to determine the most likely class which contains a given persistence landscape  @xmath254 .",
    "this can be achieved by the following simple scheme .",
    "* suppose  @xmath307 classes of persistence landscapes are given as above , and that the average classifier  @xmath324 of the  @xmath325 class in dimension  @xmath74 has been defined as in  ( [ aveclassifierk ] ) , for @xmath323 and @xmath321 .",
    "as before , let  @xmath254 be any other persistence landscape which we would like to classify .",
    "+ for each dimension  @xmath321 , one can then order the  @xmath307 possible classification classes according to monotone increasing values of the distances  @xmath329 , and we let  @xmath330 denote the first  @xmath164 class indices in this sequence , for @xmath331 . note that @xmath332 is a singleton set which contains a class whose averaged classifier  @xmath324 is closest to  @xmath333 , while we have @xmath334 for all @xmath321 .",
    "furthermore , let  @xmath335 be the smallest number such that @xmath336 .",
    "+ then we say that _ @xmath254 has been classified to belong to class  @xmath316 using all dimensions _ , if any ordering of the classes as above leads to @xmath337 if this intersection contains more than one element , or if there are ties in the above orderings due to equidistant average classifiers , then we say that _ the classifier fails to classify  @xmath254_.    this classification scheme can be useful if no clear choice of dimension  @xmath74 in  ( ck ) is evident a - priori , and we will use it also in our further studies .    for our application to the cahn - hilliard - cook equation in section  [ sec : chcmass ]",
    "we are using a slight extension of the above classification schemes .",
    "for each simulation , our input data consists of a sequence of  @xmath309 different persistence landscapes which correspond to microstructure patterns at times  @xmath338 .",
    "such a sequence of persistence landscapes will be called a _",
    "topological process_. if we define the average of topological processes @xmath339 as the topological process whose @xmath340 persistence landscape in dimension  @xmath74 is the average of all the @xmath340 persistence landscapes in dimension  @xmath74 of the processes @xmath339 , then we can easily talk about an averaged classifier as in  ( ck ) .",
    "furthermore , if we define the @xmath259-distance between topological processes  @xmath341 and  @xmath342 as the sum of the @xmath259-distances between the persistence landscapes in the  @xmath340 positions of the processes  @xmath341 and  @xmath342 , for @xmath343 , then one can easily reformulate  ( [ classifierk ] ) for the case of topological processes .",
    "the total classification scheme  ( ca ) can be extended analogously .",
    "the resulting schemes will be used for the classification of the cahn - hilliard - cook simulations described in  ( d1 ) , ( d2 ) , and  ( d3 ) .      as described in the last section ,",
    "our simulations of the cahn - hilliard - cook model  ( [ chc ] ) provide snapshots of a solution realization along  @xmath309 equidistant timesteps  @xmath338 . for each time",
    "@xmath344 , computational topology is then used to create the associated persistence landscapes in dimensions  @xmath54 and  @xmath55 . in other words , for each simulation of  ( [ chc ] ) we obtain a topological process which captures the persistence evolution of the created microstructure over the time interval  @xmath345 $ ] . in this sense , our topological data is a refinement of the topology evolution curves which were considered in  @xcite .    what does this refined topology evolution capture ? in the present section ,",
    "we demonstrate that it is in fact possible to determine the total mass  @xmath32 of the underlying simulation extremely accurately .",
    "this implies the somewhat surprising observation that the total mass  @xmath32 has quite a significant influence on the pattern formation process  to the extent that persistence information alone , which of course only depends on @xmath74-dimensional connectivity measurements , but not on size information , suffices to accurately infer  @xmath32 .",
    "we demonstrate this mass - topology dependence using the three data sets  ( d1 ) , ( d2 ) , and  ( d3 ) described in section  [ sec : basicmethod ] . the simulations in each of these data sets give rise to topological processes of length  @xmath309 , and",
    "they can be divided into  @xmath307 different mass classes . while in  ( d1 ) and  ( d3 ) we considered the @xmath346 mass values @xmath296 for @xmath347 , the data set  ( d2 ) considers the @xmath348 mass values @xmath302 , where now we have @xmath349 .",
    "furthermore , the length of the topological processes is given by @xmath350 for  ( d1 ) and  ( d2 ) , and @xmath351 for data set  ( d3 ) .",
    "for each mass value  @xmath32 , a data set contains  @xmath352 stochastically independent repetitions of the simulation . for  ( d1 ) , ( d2 ) , and  ( d3 ) we have @xmath353 , @xmath354 , and @xmath355 , respectively .    based on the composition of the data sets , we apply the classification schemes  ( ca ) , as well as  ( c0 ) and  ( c1 ) , from the previous section for mass value classes  @xmath315 , which correspond to the chosen  @xmath307 mass values  @xmath32 in increasing order .",
    "then we proceed as follows .",
    "* based on  @xmath356 training runs each from the  @xmath307 considered mass values , we determine the averaged classifier  @xmath324 of the  @xmath325 class in dimension  @xmath74 as defined in  ( [ aveclassifierk ] ) with @xmath357 , but extended for the case of topological processes .",
    "* we then try to classify each of the remaining  @xmath358 simulations in the data set , by either using the classification scheme  ( ca ) , or one of the schemes  ( c0 ) or  ( c1 ) .    in each case",
    ", we note the index  @xmath47 obtained from the classification scheme , and compare it to the actual mass index  @xmath359 of the underlying simulation .",
    "if  @xmath360 , then we have obtained a `` hit '' , otherwise we say that the classification `` missed by  @xmath361 '' .",
    "note that the integer value  @xmath361 is directly proportional to the mass difference incurred by the classification .",
    "for the data sets  ( d1 ) and  ( d3 ) the associated proportionality factor is  @xmath282 , while for  ( d2 ) it is  @xmath362 .",
    "batch no .",
    "& hits & missed by @xmath55 & missed by @xmath100 & missed by @xmath178 & missed by @xmath363 & wrong + @xmath55 & @xmath364 & @xmath365 & @xmath54 & @xmath54 & @xmath55 & @xmath54 + @xmath100 & @xmath366 & @xmath367 & @xmath55 & @xmath54 & @xmath55 & @xmath54 +   + batch no .",
    "& hits & missed by @xmath55 & missed by @xmath100 & missed by @xmath178 & missed by @xmath363 & wrong + @xmath55 & @xmath368 & @xmath369 & @xmath55 & @xmath54 & @xmath55 & @xmath54 + @xmath100 & @xmath370 & @xmath371 & @xmath55 & @xmath54 & @xmath55 & @xmath54 +   + batch no .",
    "& hits & missed by @xmath55 & missed by @xmath100 & missed by @xmath178 & missed by @xmath363 & wrong + @xmath55 & @xmath372 & @xmath373 & @xmath374 & @xmath55 & @xmath55 & @xmath55 + @xmath100 & @xmath375 & @xmath373 & @xmath376 & @xmath54 & @xmath54 & @xmath55 +    we now turn to the first data set  ( d1 ) , which consists of @xmath353 simulations each for @xmath346 different mass values . in this case",
    ", we took the first @xmath377 data sets for each mass value as training sets , and their average constitutes the averaged classifier in each case . classifying the remaining @xmath378 topological process using scheme  ( ca ) as described above then leads to the results shown in table  [ tabled1ca ] , in the rows denoted `` batch 1 '' .",
    "if instead we use the third and fourth simulations as two - element training set , one obtains the lines labeled as `` batch 2 '' . for the classifications",
    ", we used the @xmath259-norm with @xmath379 .",
    "the results in the table are remarkable . if one uses the @xmath380-norm for the classification , then in over  80% of the cases the classification scheme finds the correct mass value class .",
    "if one allows for an error of the form @xmath381 , then the classification succeeds in at least 98% of the cases .",
    "while the behavior for the @xmath382-norm is only slightly worse , the @xmath383-norm does not seem to be quite as accurate an estimator .",
    "the latter norm gives correct classifications only in at least 50% of the cases , errors of the form @xmath381 in at least  88% , as well as errors @xmath384 in at least 98% of the cases .",
    "in addition , the two classifications appearing in the `` wrong '' column have index differences of  @xmath385 , i.e. , they are significant misclassifications . despite the worse performance of the @xmath383-norm estimator",
    ", these results clearly indicate how powerful the above topological classification scheme is .",
    "we would like to point out that one can not expect the classifier to have  100% accuracy .",
    "on the one hand , these classifications used training sets which consist of _ two _ topological processes , which clearly is an extremely small batch size . moreover",
    ", we are classifying pattern evolutions created by a stochastic partial differential equation which originate at random initial conditions .",
    "given these uncertainties , the presented topological classification method is highly effective in distinguishing the pattern evolutions from the cahn - hilliard - cook equation  ( [ chc ] ) .",
    "norm & hits & missed by @xmath55 & missed by @xmath100 & missed by @xmath178 & missed by @xmath363 & missed by @xmath386 & wrong + @xmath380 & @xmath368 & @xmath387 & @xmath54 & @xmath54 & @xmath54 & @xmath54 & @xmath55 + @xmath382 & @xmath304 & @xmath388 & @xmath363 & @xmath54 & @xmath54 & @xmath54 & @xmath55 + @xmath383 & @xmath389 & @xmath390 & @xmath391 & @xmath178 & @xmath55 & @xmath54 & @xmath363 +   + norm & hits & missed by @xmath55 & missed by @xmath100 & missed by @xmath178 & missed by @xmath363 & missed by @xmath386 & wrong + @xmath380 & @xmath392 & @xmath393 & @xmath55 & @xmath54 & @xmath55 & @xmath54 & @xmath54 + @xmath382 & @xmath394 & @xmath365 & @xmath178 & @xmath54 & @xmath55 & @xmath54 & @xmath54 + @xmath383 & @xmath395 & @xmath396 & @xmath397 & @xmath363 & @xmath178 & @xmath55 & @xmath54 +    for the above classifications we have used the classification scheme  ( ca ) which simultaneously considers persistence information in dimensions  @xmath75 and  @xmath76 .",
    "how do the dimension - dependent classification schemes  ( c0 ) and  ( c1 ) fare ?",
    "the results for `` batch 1 '' as above are collected in table  [ tabled1c01 ] .",
    "as before , both the @xmath380- and the @xmath382-norm classifications lead to precise mass value classifications , while the classification based on the @xmath383-norm is worse .",
    "note , however , that the performance of the  ( c1 ) scheme is consistently better than the one of the  ( c0 ) scheme .",
    "we believe that this can be explained as follows . in our simulations",
    ", we considered mass values  @xmath311 .",
    "this implies that in the droplet forming regime  @xmath313 , most threshold values lead to a connected cubical complex with a number of holes , which correspond to the droplets of one material forming in the background matrix consisting of the second material . in this situation , the first betti number only counts the _ interior droplets",
    "_ , i.e. , the ones that do not touch the boundary . in other words , in our setup , the @xmath55-dimensional betti number of the considered sublevel set is equal to the number of interior components of the minority phase .",
    "in contrast , the @xmath54-dimensional betti number counts both the interior _ and _ the boundary components of the majority phase .",
    "thus , it seems reasonable to attribute the better performance of the scheme  ( c1 ) to its exclusive focus on the _ bulk behavior _ of the material , while  ( c0 ) measures the combination of both bulk and boundary phenomena . as was pointed out in  @xcite , these phenomena exhibit different scaling behaviors , and can therefore affect the results for certain values of the parameter @xmath398 .",
    "( 16.75,10.5 ) ( 0.0,5.5 )   of the  @xmath325 mass value class for the data set  ( d1 ) , where @xmath399 .",
    "the horizontal and vertical axes correspond to the mass value index  @xmath47 , and the sum of the distances for dimensions @xmath75 and @xmath76 , as defined in  ( [ def : heatmapdist ] ) , are color - coded as shown in the colorbars . from top left to bottom middle the images are for the @xmath380- , @xmath382- , and @xmath383-norm , respectively.,title=\"fig:\",height=188 ] ( 8.75,5.5 )   of the  @xmath325 mass value class for the data set  ( d1 ) , where @xmath399 .",
    "the horizontal and vertical axes correspond to the mass value index  @xmath47 , and the sum of the distances for dimensions @xmath75 and @xmath76 , as defined in  ( [ def : heatmapdist ] ) , are color - coded as shown in the colorbars . from top left to bottom middle the images are for the @xmath380- , @xmath382- , and @xmath383-norm , respectively.,title=\"fig:\",height=188 ] ( 4.375,0.0 )   of the  @xmath325 mass value class for the data set  ( d1 ) , where @xmath399 .",
    "the horizontal and vertical axes correspond to the mass value index  @xmath47 , and the sum of the distances for dimensions @xmath75 and @xmath76 , as defined in  ( [ def : heatmapdist ] ) , are color - coded as shown in the colorbars . from top",
    "left to bottom middle the images are for the @xmath380- , @xmath382- , and @xmath383-norm , respectively.,title=\"fig:\",height=188 ]    before we move on to the other two data sets , we would like to elaborate a bit more on the relative behavior of the @xmath259-classifiers for @xmath400 in the above setting .",
    "one would expect good classification results only if the averaged classifiers  @xmath324 defined in  ( [ aveclassifierk ] ) , but extended to the case of topological processes , are suitably far apart with respect to the considered distance . more precisely , one would expect that the distances @xmath401 should be smoothly increasing as the distance  @xmath402 becomes larger .",
    "the distances defined in  ( [ def : heatmapdist ] ) for @xmath400 are shown as heat maps in figure  [ fig : distancesheatmap ] , from top left to bottom middle .",
    "notice that while in all three cases this `` monotonicity '' can be observed , the level of smoothness of the heat maps decreases with increasing  @xmath213 .",
    "the first of these observations is responsible for the fact that all three choices of  @xmath213 lead to acceptable classification schemes , while the second observation explains the relative difference in their performances .",
    "we now turn our attention to the second data set  ( d2 ) . in this case",
    ", we consider @xmath354 simulations each for @xmath348 different mass values , and the length of each topological process is still given by @xmath350 .",
    "we increase the size of the training sets to @xmath403 , and classify the remaining @xmath404 simulations using the classification schemes  ( ca ) , ( c0 ) , and  ( c1 ) .",
    "regardless of the choice of @xmath259-norm , where again we use @xmath400 , the classification schemes yields 100% accuracy .",
    "we have also performed cross validations by choosing different training sets of size ten , which again leads to perfect classifications .",
    "these results certainly owe to the fact that the discrete mass values  @xmath32 considered in the data set are spaced further apart .",
    "however , while in the case  ( d1 ) the @xmath383-classification led to isolated misclassifications with large deviations , this was not observed in the data set  ( d2 ) .",
    "we believe that these outliers are controlled due to the increased size of the training sets .",
    "batch no .",
    "& hits & missed by @xmath55 & missed by @xmath100 & missed by @xmath178 & missed by @xmath363 & wrong + @xmath55 & @xmath405 & @xmath406 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath100 & @xmath407 & @xmath408 & @xmath54 & @xmath54 & @xmath54 & @xmath54 +   + batch no .",
    "& hits & missed by @xmath55 & missed by @xmath100 & missed by @xmath178 & missed by @xmath363 & wrong + @xmath55 & @xmath409 & @xmath410 & @xmath55 & @xmath54 & @xmath54 & @xmath54 + @xmath100 & @xmath411 & @xmath412 & @xmath55 & @xmath54 & @xmath54 & @xmath54 +   + batch no .",
    "& hits & missed by @xmath55 & missed by @xmath100 & missed by @xmath178 & missed by @xmath363 & wrong + @xmath55 & @xmath413 & @xmath414 & @xmath415 & @xmath391 & @xmath100 & @xmath54 + @xmath100 & @xmath416 & @xmath417 & @xmath418 & @xmath391 & @xmath100 & @xmath54 +     norm & hits & missed by @xmath55 & missed by @xmath100 & missed by @xmath178 & missed by @xmath363 & missed by @xmath386 & wrong + @xmath380 & @xmath419 & @xmath420 & @xmath363 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath382 & @xmath421 & @xmath422 & @xmath423 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath383 & @xmath424 & @xmath425 & @xmath426 & @xmath427 & @xmath428 & @xmath178 & @xmath363 +   + norm & hits & missed by @xmath55 & missed by @xmath100 & missed by @xmath178 & missed by @xmath363 & missed by @xmath386 & wrong + @xmath380 & @xmath429 & @xmath430 & @xmath54 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath382 & @xmath431 & @xmath432 & @xmath433 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath383 & @xmath434 & @xmath435 & @xmath436 & @xmath437 & @xmath438 & @xmath439 & @xmath428 +    the results so far have used fairly fine sampling in the temporal direction , which has led to two data sets with topological processes of length  @xmath350 .",
    "it is natural to wonder whether the classification schemes work as well if this sampling size is decreased . to study this",
    ", we consider the data set  ( d3 ) , which consists of @xmath355 simulations each for @xmath346 different mass values , but only saves @xmath351 solution snapshots in the interval  @xmath345 $ ] . as a first experiment , we use two different training sets of size  @xmath440 , and in each case",
    "classify the remaining @xmath441 simulations .",
    "the two different sets are referred to as `` batch no .  1 '' and `` batch no .  2 '' , respectively , the results for the classifier  ( ca ) are collected in table  [ tabled3aca ] , while the results for  ( c0 ) and  ( c1 ) can be found in table  [ tabled3ac01 ] .",
    "batch no .",
    "& hits & missed by @xmath55 & missed by @xmath100 & missed by @xmath178 & missed by @xmath363 & missed by @xmath386 & wrong + @xmath55 & @xmath442 & @xmath443 & @xmath54 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath100 & @xmath444 & @xmath445 & @xmath54 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath178 & @xmath446 & @xmath447 & @xmath54 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath363 & @xmath448 & @xmath449 & @xmath54 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath386 & @xmath450 & @xmath451 & @xmath54 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath301 & @xmath452 & @xmath453 & @xmath54 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath428 & @xmath454 & @xmath455 & @xmath54 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath456 & @xmath457 & @xmath458 & @xmath54 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath459 & @xmath460 & @xmath443 & @xmath55 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath439 & @xmath461 & @xmath462 & @xmath54 & @xmath54 & @xmath54 & @xmath54 & @xmath54 +   + batch no .",
    "& hits & missed by @xmath55 & missed by @xmath100 & missed by @xmath178 & missed by @xmath363 & missed by @xmath386 & wrong + @xmath55 & @xmath463 & @xmath464 & @xmath376 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath100 & @xmath465 & @xmath466 & @xmath301 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath178 & @xmath467 & @xmath468 & @xmath439 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath363 & @xmath469 & @xmath470 & @xmath301 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath386 & @xmath471 & @xmath472 & @xmath456 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath301 & @xmath473 & @xmath474 & @xmath363 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath428 & @xmath475 & @xmath476 & @xmath178 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath456 & @xmath477 & @xmath417 & @xmath178 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath459 & @xmath478 & @xmath479 & @xmath363 & @xmath54 & @xmath54 & @xmath54 & @xmath54 + @xmath439 & @xmath480 & @xmath481 & @xmath178 & @xmath54 & @xmath54 & @xmath54 & @xmath54 +   + batch no .",
    "& hits & missed by @xmath55 & missed by @xmath100 & missed by @xmath178 & missed by @xmath363 & missed by @xmath386 & wrong + @xmath55 & @xmath482 & @xmath483 & @xmath484 & @xmath391 & @xmath363 & @xmath55 & @xmath54 + @xmath100 & @xmath485 & @xmath486 & @xmath487 & @xmath367 & @xmath386 & @xmath54 & @xmath54 + @xmath178 & @xmath488 & @xmath489 & @xmath490 & @xmath374 & @xmath100 & @xmath54 & @xmath54 + @xmath363 & @xmath491 & @xmath492 & @xmath493 & @xmath374 & @xmath386 & @xmath54 & @xmath54 + @xmath386 & @xmath494 & @xmath495 & @xmath496 & @xmath497 & @xmath363 & @xmath54 & @xmath54 + @xmath301 & @xmath498 & @xmath499 & @xmath500 & @xmath367 & @xmath386 & @xmath55 & @xmath54 + @xmath428 & @xmath501 & @xmath502 & @xmath503 & @xmath504 & @xmath178 & @xmath54 & @xmath54 + @xmath456 & @xmath505 & @xmath506 & @xmath507 & @xmath374 & @xmath178 & @xmath54 & @xmath54 + @xmath459 & @xmath508 & @xmath509 & @xmath510 & @xmath497 & @xmath100 & @xmath54 & @xmath54 + @xmath439 & @xmath511 & @xmath512 & @xmath513 & @xmath367 & @xmath363 & @xmath55 & @xmath54 +    for the classification scheme  ( ca ) , the simulation results are convincing . if one uses the @xmath380-norm for the classification , then in over  91% of the cases the classification scheme finds the correct mass value class , and all classifications are correct up to an error of the form @xmath381 .",
    "as before , the @xmath382-norm classifier performs slightly worse , with correct classifications in over 84% of the cases , and errors of  @xmath381 in over 99% of the cases .",
    "the maximally observed errors where of the form @xmath384 .",
    "while the integral norms perform extremely well , the maximum norm @xmath383-estimator again does not seem to be quite as accurate .",
    "it leads to correct classifications only in at least 64% of the cases , errors of the form @xmath381 in at least  94% , as well as errors @xmath384 in at least 99% of the cases .",
    "we would like to point out , however , that in all of the above situations , the performance exceeded the one observed in table  [ tabled1ca ] for the data set  ( d1 ) , despite the fact that now our topological processes have only length @xmath351 .",
    "this increased accuracy is due to the larger training set size @xmath440 .",
    "how do the results change if we use the dimension - dependent classification schemes  ( c0 ) or  ( c1 ) ?",
    "the corresponding results are presented in table  [ tabled3ac01 ] , and they largely confirm the observations made in the context of table  [ tabled1c01 ] . using only one dimension for the classification yields still acceptable , but definitely worse results .",
    "while now the @xmath380-classification is clearly the most accurate , the @xmath382- and @xmath383-norm classifications come in second and third , respectively . finally , the classification using dimension @xmath76 performs better than the one in dimension zero , at least for the case of @xmath380-classifications .",
    "these results show , however , that in general it is preferable to use all available topological information for the classification .    as a final experiment",
    ", we again consider data set  ( d3 ) , but this time the @xmath355 simulations for each mass value  @xmath32 are partitioned into ten different training sets of size  @xmath403 each .",
    "these training sets are referred to as `` batch no .",
    "@xmath43 '' for @xmath514 , and for each training set the remaining @xmath515 topological processes are then classified using scheme  ( ca ) .",
    "the results of these computations are shown in table  [ tabled3bca ] , and they confirm our previous observations . for classifications using the @xmath380- , @xmath382- , and @xmath383-norm",
    "one obtains exact hits in at least 90% , 81% , and 63% of the cases , maximal errors of the form @xmath381 are observed in at least 99% , 99% , and 94% of the cases , and finally maximal errors @xmath384 occur in at least 100% , 100% , and 99% of the cases , respectively . for the @xmath380- , @xmath382- , and @xmath383-norm classifications , the maximal observed classification errors where @xmath384 , @xmath384 , and @xmath516 , respectively . while all of these numbers are slightly decreased from the ones shown in table  [ tabled3aca ] , this is to be expected due to the smaller training set size @xmath403 .",
    "nevertheless , especially the @xmath380-norm classification performs amazingly accurate .",
    "we have seen in the last section that if one considers all the topological information encoded in the microstructure evolution of a solution path of the cahn - hilliard - cook model  ( [ chc ] ) , then one can use the classification schemes  ( ca ) and  ( ck ) to recover the total mass  @xmath32 with high accuracy .",
    "of course the total mass is only one of several parameters in  ( [ chc ] ) which affect the topology , and it would be interesting to also determine how strong the influence of these other parameters on the topology is .    rather than pursuing this line of inquiry , we now consider a different question .",
    "for this , consider again the simulations of the cahn - hilliard - cook model , but now for a fixed and a - priori known mass value  @xmath32 .",
    "then a simulation of  ( [ chc ] ) produces solution snapshots along  @xmath309 equidistant timesteps  @xmath338 , and for each time  @xmath344 , computational topology can be used to create the associated persistence landscapes in dimensions  @xmath54 and  @xmath55 .",
    "suppose now that we pick up one of these persistence landscapes at random .",
    "can we recover the actual time  @xmath344 which corresponds to this landscape purely from the encoded topological information ?",
    "in other words , for fixed mass value  @xmath32 , is it possible to determine the _ time at which a solution snapshot has been taken _",
    ", purely based on topological information ?",
    "we will see in the following that this can in fact be done , and in many cases with surprising accuracy .",
    "as we saw in the previous section , our classification technique can only recover mass information up to a certain tolerance , and the situation is no different in the snapshot time case .",
    "therefore , we only use the data set  ( d3 ) in the present section , which produces  @xmath351 times  @xmath344 in the interval  @xmath345 $ ] .",
    "recall that this data set considers the @xmath346 mass values @xmath296 for @xmath347 , and for each mass value one has  @xmath355 stochastically independent repetitions of the simulation .    based on the composition of the data set  ( d3 ) ,",
    "we apply the classification schemes  ( c0 ) and  ( c1 ) from section  [ sec : basicmethod ] for every fixed mass value  @xmath32 to the snapshot time classes  @xmath517 , which correspond to the  @xmath309 snapshot times  @xmath338 in increasing order",
    ". then we proceed as follows .",
    "* based on  @xmath518 training runs each from the  @xmath351 considered snapshot times  @xmath344 , we determine the averaged classifier  @xmath519 of the  @xmath520 class in dimension  @xmath74 as defined in  ( [ aveclassifierk ] ) with @xmath357 and @xmath521 .",
    "* we then try to classify each of the remaining  @xmath522 simulations in the data set for fixed mass value  @xmath32 , by either using the classification scheme  ( c0 ) or  ( c1 ) .",
    "as before , in each case we note the index  @xmath124 obtained from the classification scheme , and compare it to the snapshot time index  @xmath523 of the underlying simulation .",
    "if  @xmath524 , then we have obtained a `` hit '' , otherwise the classification `` missed by  @xmath525 '' .",
    "clearly the integer value  @xmath525 is directly proportional to the time difference incurred by the classification , with the proportionality factor being given by  @xmath526 .",
    "( 16.75,16.0 ) ( 0.0,11.0 ) -norm and training set size  @xmath440 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 8.75,11.0 ) -norm and training set size  @xmath440 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 0.0,5.5 ) -norm and training set size  @xmath440 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 8.75,5.5 ) -norm and training set size  @xmath440 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 0.0,0.0 ) -norm and training set size  @xmath440 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 8.75,0.0 ) -norm and training set size  @xmath440 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ]    ( 16.75,16.0 ) ( 0.0,11.0 ) -norm and training set size  @xmath440 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 8.75,11.0 ) -norm and training set size  @xmath440 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 0.0,5.5 ) -norm and training set size  @xmath440 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 8.75,5.5 ) -norm and training set size  @xmath440 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 0.0,0.0 ) -norm and training set size  @xmath440 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 8.75,0.0 ) -norm and training set size  @xmath440 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ]    rather than performing an exhaustive study as in the previous section , we use the already obtained insight to focus our approach .",
    "since the @xmath383-norm classification consistently performed significantly worse than the @xmath380- and @xmath382-norm classifications , we only run simulations for the latter two .",
    "however , in contrast to the experiments presented in section  [ sec : chcmass ] , it turns out that for recovering the snapshot time both  ( c0 ) and  ( c1 ) are superior to  ( ca ) . for this reason",
    ", we only consider the dimension - dependent classification schemes below .    for the case of @xmath380-norm classifications using the schemes  ( c0 ) and  ( c1 ) , and for training set size  @xmath527 , the results can be found in figure  [ fig : time1 ] .",
    "for the sake of brevity , we have presented the data in the form of heat maps , rather than providing precise tables .",
    "each panel in the figure represents frequencies which are indicated by colors , and which correspond to the ranges of the adjacent colorbars .",
    "the horizontal axes correspond to the  @xmath346 mass values  @xmath32 , while the vertical axes are for the  @xmath351 snapshot times , both in increasing order .",
    "panels in the left column are for classification scheme  ( c0 ) , while the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of _ exact hits _ , _ misses by exactly one _ , and _ misses by at least two_. analogous results for the case of the @xmath382-norm are depicted in figure  [ fig : time2 ] . in each of the cases",
    "we have cross - validated the results by choosing different training sets of the same size , and the results are quantitatively similar .",
    "notice that in this setting , for each mass value  @xmath32 we are classifying @xmath528 microstructures according to their snapshot time class .",
    "a first glance at the heat maps in figures  [ fig : time1 ] and  [ fig : time2 ] , especially the ones in the respective bottom rows , confirms our statement from the beginning of this section .",
    "averaged persistence landscapes can indeed be used to accurately deduce the snapshot time at which a given microstructure occurs . in most cases ,",
    "the match is an exact hit , while in a few cases one is off by one index . in more detail",
    ", one can draw the following conclusions from the data in these figures :    * for every considered mass value  @xmath32 , there exists an interval of the form  @xmath529 \\subset ( 0,t_e]$ ] over which the snapshot time can be determined exactly with probability close to one . *",
    "the times  @xmath530 are small , and more or less constant with  @xmath32 . only at the beginning of the simulations",
    "does it seem to be difficult to pin down the exact snapshot time .",
    "the volatile timeframe  @xmath531 corresponds to the initial rapid smoothing regime in the parabolic stochastic partial differential equation  ( [ chc ] ) , during with the high - frequency terms in the random initial conditions decay due to dissipativity . *",
    "the value of  @xmath532 is close to about  60% of the endtime  @xmath279 for mass values  @xmath32 close to zero , and decreases as  @xmath32 increases . at the final mass value @xmath533",
    "one has @xmath534 . for larger simulation times",
    ", i.e. , times in the interval  @xmath535 $ ] , it appears to be more difficult to precisely locate the snapshot time , and this becomes more pronounced as the mass  @xmath32 increases .",
    "this timeframe is well into the coarsening regime , where microstructures , particularly of droplet type , only move slightly , and no longer change their topology frequently .",
    "thus , the classification results are not surprising .",
    "notice that in all panels of the two figures , the probabilities seem to jump significantly along the topmost line .",
    "in other words , the classification results for the time  @xmath536 are significantly different from the one for  @xmath537 .",
    "this is due to the fact that for all interior times  @xmath538 one can miss by one by either classifying the time as  @xmath539 or  @xmath540 , while at the last time , anything that would have been misclassified as a larger time will most likely be classified as  @xmath536 .",
    "( 16.75,16.0 ) ( 0.0,11.0 ) -norm and training set size  @xmath541 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 8.75,11.0 ) -norm and training set size  @xmath541 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 0.0,5.5 ) -norm and training set size  @xmath541 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 8.75,5.5 ) -norm and training set size  @xmath541 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 0.0,0.0 ) -norm and training set size  @xmath541 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 8.75,0.0 ) -norm and training set size  @xmath541 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ]    ( 16.75,16.0 ) ( 0.0,11.0 ) -norm and training set size  @xmath541 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 8.75,11.0 ) -norm and training set size  @xmath541 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 0.0,5.5 ) -norm and training set size  @xmath541 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 8.75,5.5 ) -norm and training set size  @xmath541 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 0.0,0.0 ) -norm and training set size  @xmath541 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ] ( 8.75,0.0 ) -norm and training set size  @xmath541 .",
    "each panel contains a heat map , whose horizontal axis corresponds to the  @xmath346 mass values  @xmath32 , and the vertical axis to the  @xmath351 snapshot times , in increasing order .",
    "color indicates frequency of observation , as indicated by the colorbar .",
    "panels in the left column are for scheme  ( c0 ) , the right column corresponds to  ( c1 ) . from top to bottom , the three rows show the likelihoods of exact hits , misses by exactly one , and misses by at least two.,title=\"fig:\",height=188 ]    for the above results we used different norms , but kept the training set size fixed at  @xmath440 . in order to see how this size affects the results , we have repeated the simulations , but this time with smaller training sets of size  @xmath541 .",
    "the corresponding results are shown in figures  [ fig : time3 ] and  [ fig : time4 ] .",
    "notice that despite the smaller training set size , and the thereby induced larger simulation size of @xmath542 microstructures which have to be classified , the results are almost the same .",
    "this indicates again that the topological information which is captured by the random microstructures and their evolution carries enormous information , enough to accurately detect the snapshot time , and therefore the stage of material decomposition .",
    "the only apparent exception to this is the case of large mass values  @xmath543 and time values well into the coarsening regime . in the first case ,",
    "the microstructures are of moving droplet type as shown in the bottom row of figure  [ fig : chcpatt ] , and in the latter case microstructure evolve slowly since there is less energy to dissipate , and therefore there are fewer topology changes . but during the initial spinodal decomposition regime , and well into the coarsening regime , topological information can be used to find the snapshot time .",
    "homology is a way of quantifying complicated topological spaces through a sequence of discrete objects , in fact , through a sequence of integers in its most reduced form .",
    "this information is invariant under continuous transformations of the underlying space , and in particular , it does not contain any size or `` metric '' information of the considered object .",
    "the only structures that `` count '' , in the true sense of the word , are @xmath74-dimensional holes , which can not be contracted within the space . in this",
    "form , homology has long been used in classical mathematics to determine when two spaces are different within a certain category , be it that they are not homeomorphic or not homotopy equivalent . for data analysis ,",
    "however , often the opposite question is of interest . can homology be used to say that two objects are similar , and if so , how similar are they ?    at its core",
    ", the above question is concerned with the amount of information that is retained by homology . in the present paper ,",
    "we study this problem in a well - defined and important situation , namely phase separation dynamics in binary metal alloys as described by the stochastic cahn - hilliard - cook model  ( [ chc ] ) .",
    "the equation can be used to create realistic evolutions of phase separating microstructures , which due to their inherent complexity can not easily be classified using standard techniques . by using persistence landscapes ,",
    "we could obtain a sequence of discrete objects , which encapsulates the topology evolution of a solution path .",
    "since the cahn - hilliard - cook model is stochastic in nature , and in addition , since the initial alloy state is never known precisely , one can only hope to capture typical behavior , and we do this by averaging these persistence landscapes . it was demonstrated in this paper , that these averaged landscapes encode enough information to make the following decision problems solvable :    * given only topological information about a specific microstructure evolution , can we determine the total mass  @xmath32 of the underlying experiment ?",
    "* given only topological information about a specific microstructure at a given mass value  @xmath32 , can we determine the snapshot time during the decomposition process at which this pattern was observed ?",
    "these questions can be answered by comparing the provided topological information to the nearest averaged classifiers , which are created through training sets .",
    "we would like to stress that we did not set out to develop a method which determines the total mass  @xmath32 from topological information  that can obviously be done easier by just finding the average gray level of any solution snapshot .",
    "rather , we wanted to demonstrate that the seemingly stark reduction in information , which happens during the passage from a complicated microstructure to its persistent homology , still retains an incredible amount of information about the underlying dynamical process . yet ,",
    "due to its inherently discrete nature , this topological approach shows considerable promise for analyzing and relating experimental data to numerical models , and this avenue has to be explored further in future work .",
    "the first author was supported by the advanced grant of the european research council gudhi ( geometric understanding in higher dimensions ) , by darpa grant fa9550 - 12 - 1 - 0416 , by afosr grant fa9550 - 14 - 1 - 0012 , and by a foundation for polish science start scholarship .",
    "the second author was partially supported by national science foundation grants dms-1114923 and dms-1407087 .",
    "the numerical simulations for this work were run on argo , a research computing cluster provided by the office of research computing at george mason university , va .",
    "( url : http://orc.gmu.edu )    99 u.  bauer , m.  kerber , j.  reininghaus , and h.  wagner , _ phat  persistent homology algorithms toolbox _ , mathematical software ",
    "icms 2014 , lecture notes in computer science , vol .  8592 , springer - verlag , new - york , pp .",
    "137143 , 2014 .",
    "d.  blmker , s.  maier - paape , and t.  wanner , _ spinodal decomposition for the cahn - hilliard - cook equation _ , communications in mathematical physics 223 ( 3 ) : 553582 , 2001 .",
    "d.  blmker , s.  maier - paape , and t.  wanner , _ phase separation in stochastic cahn - hilliard models _ , in : _ mathematical methods and models in phase transitions _ , ( alain miranville , editor ) , nova science publishers , new york , pp .  141 , 2005 .",
    "d.  blmker , s.  maier - paape , and t.  wanner , _ second phase spinodal decomposition for the cahn - hilliard - cook equation _ , transactions of the american mathematical society 360 ( 1 ) : 449489 , 2008 .",
    "p.  bubenik , _ statistical topology using persistence landscapes _ , journal of machine learning research 16 : 77102 , 2015 .",
    "p.  bubenik , p.  dotko , _ a persistence landscapes toolbox for topological statistics _ ,",
    "cahn , _ free energy of a nonuniform system .",
    "thermodynamic basis _",
    ", journal of chemical physics 30 : 11211124 , 1959 .",
    "cahn , _ phase separation by spinodal decomposition in isotropic systems _ , journal of chemical physics 42 : 9399 , 1965 .",
    "cahn , _ spinodal decomposition _",
    ", transactions of the metallurgical society of aime 242 : 166180 , 1968 . j.w .  cahn and j.e .",
    "hilliard , _ free energy of a nonuniform system i. interfacial free energy _ , journal of chemical physics 28 : 258267 , 1958 .",
    "cochran , t.  wanner , and p.  dotko , _ a randomized subdivision algorithm for determining the topology of nodal sets _ , siam journal on scientific computing 35 ( 5 ) : b1034b1054 , 2013 . h.  cook , _",
    "brownian motion in spinodal decomposition _",
    ", acta metallurgica 18 : 297306 , 1970 . s.  day , w.d .",
    "kalies , and t.  wanner , _ verified homology computations for nodal domains _ , siam journal on multiscale modeling & simulation 7 ( 4 ) : 16951726 , 2009 .",
    "desi , h.  edrees , j.  price , e.  sander , and t.  wanner , _ the dynamics of nucleation in stochastic cahn - morral systems _ , siam journal on applied dynamical systems 10 ( 2 ) : 707743 , 2011 .",
    "p.  dotko , t.  kaczynski , m.  mrozek , and t.  wanner , _",
    "coreduction homology algorithm for regular cw - complexes _ , discrete & computational geometry 46 ( 2 ) : 361388 , 2011 .",
    "m.  gameiro , y.  hiraoka , s.  izumi , m.  kramar , k.  mischaikow , and v.  nanda , _ a topological measurement of protein compressibility _",
    ", japan journal of industrial and applied mathematics 32 ( 1 ) : 117 , 2014 . m.  gameiro , k.  mischaikow , and t.  wanner , _ evolution of pattern complexity in the cahn - hilliard theory of phase separation _ , acta materialia 53 ( 3 ) : 693704 , 2005 . h.  edelsbrunner , j.  harer , _ computational topology  an introduction _ , american mathematical society , providence , 2010",
    ". t.  kaczynski , k.  mischaikow , and m.  mrozek , _ computational homology _ , springer - verlag , new york , 2004 .",
    "l.  kondic , a.  goullet , c.s .",
    "ohern , m.  kramar , k.  mischaikow , and r.p .",
    "behringer , _ topology of force networks in compressed granular media _ , epl 97 ( 5 ) : 54001 , 2012 .",
    "m.  kramar , a.  goullet , l.  kondic , and k.  mischaikow , _ persistence of force networks in compressed granular media _ , physical review  e 87 : 042207 , 2013 .",
    "langer , _ theory of spinodal decomposition in alloys _ ,",
    "annals of physics 65 : 5386 , 1971 .",
    "mallows , _ a note on asymptotic joint normality _ , annals of mathematical statistics 43 ( 2 ) : 508515 , 1972 .",
    "y.  mileyko , s.  mukherjee , and j.  harer , _ probability measures on the space of persistence diagrams _ , inverse problems 27 ( 12 ) : 124007 , 2011 .",
    "m.  mrozek and t.  wanner , _ coreduction homology algorithm for inclusions and persistent homology _ , computers & mathematics with applications 60 ( 10 ) : 28122833 , 2010 .",
    "e.  munch , p.  bendich , k.  turner , s.  mukherjee , j.  mattingly , and j.  harer , _ probabilistic frchet means and statistics on vineyards _ ,",
    "t.  nakamura , y.  hiraoka , a.  hirata , e.g.  escolar , k.  matsue , and y.  nishiura , _ description of medium - range order in amorphous structures by persistent homology _",
    ", preprint .",
    "t. nakamura , y. hiraoka , a. hirata , e. g. escolar , and y. nishiura , _ persistent homology and many - body atomic structure for medium - range order in the glass _ , preprint .",
    "g.s .  rohrer and h.m .",
    "miller , _ topological characteristics of plane sections of polycrystals _ , acta materialia 58 ( 10 ) : 38053814 , 2010 .",
    "e.  sander and t.  wanner , _ monte carlo simulations for spinodal decomposition _ , journal of statistical physics 95 ( 56 ) : 925948 , 1999 .",
    "e.  sander and t.  wanner , _ unexpectedly linear behavior for the cahn - hilliard equation _ , siam journal on applied mathematics 60 ( 6 ) : 21822202 , 2000 .",
    "v.  nanda , the perseus software project , www.math.rutgers.edu/@xmath544vidit/perseus .",
    "t.  wanner , _ maximum norms of random sums and transient pattern formation _ , transactions of the american mathematical society 356 ( 6 ) : 22512279 , 2004 .",
    "t.  wanner , _ topological analysis of the diblock copolymer equation _ , in : mathematical challenge to a new phase of materials science , springer proceedings in mathematics & statistics , to appear , 2015 .",
    "t.  wanner , e.r .",
    "fuller jr . , and d.m .",
    "saylor , _ homological characterization of microstructure response fields in polycrystals _ , acta materialia 58 ( 1 ) : 102110 , 2010 ."
  ],
  "abstract_text": [
    "<S> phase separation mechanisms can produce a variety of complicated and intricate microstructures , which often can be difficult to characterize in a quantitative way . in recent years </S>",
    "<S> , a number of novel topological metrics for microstructures have been proposed , which measure essential connectivity information and are based on techniques from algebraic topology . </S>",
    "<S> such metrics are inherently computable using computational homology , provided the microstructures are discretized using a thresholding process . </S>",
    "<S> however , while in many cases the thresholding is straightforward , noise and measurement errors can lead to misleading metric values . in such situations , </S>",
    "<S> persistence landscapes have been proposed as a natural topology metric . </S>",
    "<S> common to all of these approaches is the enormous data reduction , which passes from complicated patterns to discrete information . </S>",
    "<S> it is therefore natural to wonder what type of information is actually retained by the topology . in the present paper , we demonstrate that averaged persistence landscapes can be used to recover central system information in the cahn - hilliard theory of phase separation . more precisely , we show that topological information of evolving microstructures alone suffices to accurately detect both concentration information and the actual decomposition stage of a data snapshot . considering that persistent homology only measures discrete connectivity information , </S>",
    "<S> regardless of the size of the topological features , these results indicate that the system parameters in a phase separation process affect the topology considerably more than anticipated . </S>",
    "<S> we believe that the methods discussed in this paper could provide a valuable tool for relating experimental data to model simulations . </S>"
  ]
}