{
  "article_text": [
    "multiple instance learning ( mil ) is a variation of classical learning methods that can be used to solve problems in which the labels are assigned to bags , i.e. , a set of instances , rather than individual instances .",
    "mil was originally introduced to solve the problem of drug activity prediction and polymorphism ambiguity @xcite .",
    "then , it has been applied to several problems such as protein - protein interactions ( ppi ) @xcite and image regions classification in computer vision @xcite .",
    "for example , in drug activity prediction problem , each drug molecule is represented by a bag , and the alternative low - energy shapes of the molecule are represented by the instances in the bag . in image regions classification , each image can be treated as a bag of segments that are modeled as instances , and the concept point representing the target object can be learned through mil algorithms .",
    "one major assumption of most existing mil methods is that each bag contains a set of instances that are independently and identically distributed .",
    "but , many real world applications such as bioinformatics , web mining , and text mining have to deal with sequential and temporal data . when the tackled problem can be formulated as a mil problem",
    ", each instance of each bag may have structural and/or temporal relation with other instances in other bags . considering this issue ,",
    "thus , the problem we want to solve in this work is the mil problem in sequence data that present structural dependencies between instances of different bags . in this context , the learning data consist of a set of bags where each bag contains a set of sequences that are expressed differently for every bag .    a variety of mil algorithms",
    "have been developed such as diverse density @xcite , citation - knn @xcite , mi - svm @xcite and hydr - mi @xcite .",
    "however , most of these algorithms are not suitable for the problem of sequence data prediction since they require an attribute - value format for their processed data .",
    "in addition , the major assumption of most existing mil methods is that instances of different bags are independently and identically distributed .",
    "nevertheless , in many applications , the dependencies between across bag instances naturally exist and if incorporated in classification models , they can potentially improve the prediction performance significantly @xcite .    in this paper , we deal with mil problem for sequence data and we consider the case of data that present dependencies between instances of different bags .",
    "we first provide a formalization of the problem of multiple instance learning in sequence data .",
    "then , we present two approaches that take into account relational structure information among across bag instances / sequences .",
    "the first approach is the _ abclass _ approach which performs first a preprocessing step of the input sequences that consists in extracting motifs from the set of sequences .",
    "these motifs will be used as attributes / features to construct a binary table where each row corresponds to a sequence . then a discriminative classifier is applied to the sequences of an unknown bag in order to assign its label .",
    "the second approach is the _ absim _ approach which uses a similarity measure between each sequence of an unknown bag and the corresponding sequences in the learning bags .",
    "we applied the proposed approaches to the problem of prediction of irr in bacteria using mil techniques introduced and described in @xcite .",
    "the problem of irr prediction in bacteria consists in learning a classifier that classifies a bacterium to either irrb or irsb .",
    "we describe an implementation of our algorithms and we present an experimental study that evaluates the performance of the proposed approaches in the case of irr prediction in bacteria .",
    "the remainder of this paper is organized as follows .",
    "section [ background ] defines the problem of mil for sequence data . in section [ related ]",
    ", we present an overview of some related works dealing with mil problems . in section [ approach ] , we describe the proposed mil - based approaches for sequence data . in section [ experiments ] , we describe our experimental environment and we discuss the obtained results . concluding points make the body of section [ conclusion ] .",
    "in this section , we present the background information related to mil in sequence data .",
    "we first describe the terminology and our problem formulation . then , we introduce a simple use case that serves as a running example throughout this paper .",
    "a * sequence * is an ordered list of events .",
    "an event can be represented as a symbolic value , a numerical value , a vector of values or a complex data type @xcite .",
    "there are many types of sequences including symbolic sequences , simple time series and multivariate time series . in our work",
    ", we are interested in symbolic sequences since the protein sequences are described using symbols ( amino acids ) .",
    "we denote @xmath0 an _ alphabet _ defined as a finite set of characters or symbols . a * symbolic sequence * is defined as an ordered list of symbols .",
    "a sequence @xmath1 with length @xmath2 is written as : @xmath3 , where @xmath4 is a symbol at position @xmath5 .",
    "let @xmath6 be a learning database that contains a set of @xmath7 labeled bags @xmath8 where @xmath9 is the label of the bag @xmath10 .",
    "instances in @xmath10 are sequences and are denoted by @xmath11 .",
    "formally @xmath12 , where @xmath13 is the total number of instances in this bag .",
    "we note that there is a * relation * @xmath14 between instances of different bags denoted across bag sequences relation .",
    "each instance @xmath11 of a bag @xmath10 is related by @xmath14 to the instance @xmath15 of an other bag @xmath16 in @xmath6 .",
    "the problem investigated in this work is to learn a multiple instance classifier from @xmath6 .",
    "given a query bag @xmath17 , where @xmath18 is the total number of instances in @xmath19 , the classifier should use sequential data in this bag and in each bag of @xmath6 to predict the label of @xmath19 .      in order to illustrate our proposed approaches , we rely on the following running example .",
    "let @xmath20 be an alphabet .",
    "let @xmath21 @xmath22 a learning database that contains four bags .",
    "positive bags consists of @xmath23 and @xmath24 and negative bags consists of @xmath25 and @xmath26 .",
    "@xmath27    @xmath28    @xmath29    @xmath30    we note that @xmath31 , @xmath32 , @xmath33 and @xmath34 are related by an across bag relation @xmath14 .",
    "the same applies to @xmath35 , @xmath36 , @xmath37 and @xmath38 .",
    "we need to predict the class label of an unknown bag @xmath39 where : @xmath40",
    "existing sequence classification methods can be divided into three large categories @xcite : ( 1 ) feature based classification , ( 2 ) sequence distance based classification and ( 3 ) model based classification . in feature based classification , a sequence is transformed into a feature vector .",
    "this representation scheme could lead to very high - dimensional feature spaces .",
    "the feature extraction step is very important since it would impact on the classification results .",
    "this step should deal with many parameters such as the criteria used for selecting features ( e.g. frequency and length ) and the matchings type ( i.e. exact or inexact with gaps ) . after adapting the input data format ,",
    "a conventional classification method is applied .    in sequence distance",
    "based classification , a distance function should be defined to measure the similarity between a pair of sequences .",
    "then an existing classification method could be used such as k nearest neighbor ( knn ) or svm .",
    "the distance function determines the quality of the classification significantly @xcite . in a recent work @xcite",
    ", authors propose algorithms to learn sequential classifiers from long and noisy discrete - event sequences .",
    "the algorithms use a lightweight and flexible subsequence matching function and a subsequence enumeration strategy called _ pattern silhouettes_.    model based classification methods define a classification model based on the probability distribution of the sequences over the different classes .",
    "this model is then used to classify unknown sequences .",
    "naive bayes is a simple model based classifier that makes the assumption that the features of the sequences are independent .",
    "markov model and hidden markov model ( hmm ) could be used to model the dependencies among sequences .      in multiple instance learning",
    ", the training set is composed of @xmath7 labeled bags .",
    "each bag @xmath41 in the training set contains @xmath13 instances and has a bag label @xmath42 .",
    "we notice that instance @xmath43 of each bag has label @xmath44 , but this label is not known during training .",
    "the most common assumption in this field is that a bag is labeled positive if at least one of its instances is positive , which can be expressed as follows :    @xmath45    the task of mil is to learn a classifier from the training set that correctly predicts unseen bags .",
    "although mil is quite similar to traditional supervised learning , the main difference between the two approaches can be found in the class labels provided by the data . according to the specification given by dietterich et al .",
    "@xcite , in a traditional setting of machine learning , an object is represented by a feature vector ( an instance ) , which is associated to a label .",
    "however , in a multiple instance setting , each object may have various instances .",
    "recently , several mil algorithms have been proposed including diverse density @xcite , citation - knn @xcite , mi - svm @xcite , and hydr - mi @xcite .",
    "diverse density ( dd ) was proposed in @xcite as a general framework for solving multiple instance learning problems .",
    "diverse density uses a probabilistic approach to maximize a measure of the intersection of the positive bags minus the union of the negative bags in feature space .",
    "the key point of dd approach is to find a concept point in the feature space that are close to at least one instance from every positive bag and meanwhile far away from instances in negative bags .",
    "the optimal concept point is defined as the one with the maximum diversity density , which is a measure of how many different positive bags have instances near the point , and how far the negative instances are away from that point .    _ mi - svm",
    "_ @xcite is an adaptation of support vector machines ( _ svm _ ) to the mil problem .",
    "the approach adopted by _",
    "mi - svm _ explicitly treats the label instance labels as unobserved hidden variables subject to constraints defined by their bag labels .",
    "the goal is to maximize the usual instance margin jointly over the unknown instance labels and a linear or kernelized discriminant function .    in @xcite ,",
    "the authors present two variants of the k - nearest neighbor algorithm called bayesian - knn and citation - knn .",
    "the bayesian method computes the posterior probabilities of the label of an unknown bag based on labels of its neighbors .",
    "citation - knn algorithm classifies a bag based on the labels of both the references and citers of that bag .    in @xcite , _ hydr - mi _ ( which stands for hybrid dimensionality reduction method for multiple instance learning )",
    "is proposed as a feature subset selection method for mil algorithms .",
    "the hybrid consists of the filter component based on an extension of the _ relieff _ algorithm @xcite developed to work with mil and the wrapper component based on a genetic algorithm that optimizes the search for the best feature subset from a reduced set of features , output by the filter component .    in @xcite ,",
    "the authors present an svm - based algorithm via approximate box counting .",
    "they reformulate gmil-1 algorithm using a kernel for a support vector machine to reduce its time complexity from exponential to polynomial .",
    "computing the kernel is equivalent to counting the number of axis - parallel boxes in a discrete , bounded space that contain at least one point from each of the two multisets .    in @xcite , an optimization algorithm that deals with multiple instance learning on structured data ( milsd ) is proposed . in milsd",
    "there exists rich dependency / structure information between instances / bags that may be used to improve the performance of existing mil algorithms .",
    "this additional information is represented using a graph that depicts the structure between either bags or instances .",
    "the proposed formulation deals with two sets of constraints caused by learning on instances within individual bags and learning on structured data and has a non - convex optimization problem . to solve this problem , authors present an iterative method based on constrained concave - convex procedure ( cccp ) .",
    "it is an optimization method that deals with the concave convex objective function with concave convex constraints @xcite .",
    "however , in many real world applications , the number of the labeled bags as well as the number of links between bags are huge . to solve the problem efficiently , the cutting plane method @xcite is used .",
    "the authors of @xcite have present a novel adaption of the cutting plane method that can handle the two sets of constraints simultaneously : the goal is to find two small subsets of constraints from a larger constraint sets .",
    "they also summarize three scenarios of the structure information in mil :    * i - milsd : the relational structures are on the instance level ( either in the same bag or across bags ) .",
    "* b - milsd : the structure information is available on the bag level . * bi - milsd : the structure information is available on both instance level and bag level .    applying the above presented algorithms on our irr problem",
    "leads to two problems .",
    "the first problem is that these algorithms can be used when the processed data can be simply represented by bags of instances and the labels are assigned to bags rather than individual instances .",
    "if applied on sequences , the input data should be presented in attribute - value format whereas we aim to use the input data without any adaptation of the format .",
    "for example , in @xcite , the empirical evaluation is done on three datasets : ( 1 ) musk dataset , ( 2 ) corel dataset for image annotation and ( 3 ) trec9 dataset for document categorisation .",
    "the last dataset contains sequence data .",
    "terms are used to present the text .",
    "this leads to an extremely sparse and high dimensional attribute - value representation of the processed text .",
    "in@xcite and @xcite , a dataset of protein sequences was used in the empirical evaluation .",
    "the goal is to identify trx - fold proteins . in each protein",
    "s primary sequence , the primary sequence motif ( typically cxxc ) that is known to exist in all trx - fold proteins are found .",
    "then , they extract a window of size 214 around each motif ( 20 residues upstream , 180 downstream ) .",
    "each primary protein sequence is considered as a bag , and some of its subsequences ( the extracted windows ) are considered as instances .",
    "these subsequences are aligned mapped to a 8-dimensional feature space : 7 numeric properties @xcite and an 8@xmath46 feature that represents the residue s position .",
    "so we obtain an attribute - value format description of the dataset .",
    "the second problem is that they do nt deal with the across bag relations that may exists between instances , except the algorithms in @xcite . in @xcite ,",
    "the alignment score is used to identify the additional structure information between proteins : if the score between a pair of proteins exceed 25 , then authors consider that there exists a link between them .",
    "only the b - milsd algorithm was used in experiments .",
    "in this section , we present the proposed approaches for mil in sequence data . we also present the naive approach to deal with the problem of mil in sequence data .      the simplest way to solve",
    "the problem of mil for sequence data is to use standard mil classifiers .",
    "however , most commonly used mil algorithms require a uniform attribute - value format description of all instances of different bags . the naive approach for mil in sequence data",
    "consists of a two step approach .",
    "the first step is a preprocessing step that transforms the set of sequences to an attribute - value matrix where each row corresponds to a sequence and each column corresponds to an attribute .",
    "the second step consists in applying an existing mil classifier .",
    "figure [ fignaive ] illustrates the naive approach for mil in sequence data .        in the case of sequence data ,",
    "the most used technique to transform data to an attribute - value format is to extract motifs that serve as attributes .",
    "we note that finding a uniform description of all instances using a set of motifs is not always an easy task .",
    "since our naive approach takes into account the across bag relations between instances , the preprocessing step extracts motifs from each set of related instances . the union of these extracted motifs is then used as features to construct an attribute - value matrix where each row corresponds to a sequence . the presence or the absence of an attribute in a sequence",
    "is respectively denoted by 1 or 0 . using this approach",
    ", we obtain an attribute - value matrix that contains a large number of motifs .",
    "it is worthwhile to mention that only a subset of the used attributes is representative for each processed sequence .",
    "therefore , we may have a big sparse matrix when trying to present the whole sequence data using an attribute value format .",
    "we apply the naive approach to our running example .",
    "let @xmath47 be the list of features extracted from the instances @xmath31 , @xmath32 , @xmath33 and @xmath34 .",
    "let @xmath48 be the list of features extracted from the instances @xmath35 , @xmath36 , @xmath37 and @xmath38 .",
    "the union of @xmath49 and @xmath50 produces the list @xmath51 . in order to encode the learning sequence data ,",
    "we generate the following attribute - value matrix denoted @xmath52 : @xmath53    the sparsity percentage of @xmath52 is 60% .",
    "we note that if we have a big learning database , @xmath52 could result to a huge and very sparse matrix .      in order to avoid the use of one large vector of features to describe sequence data , we present _ abclass _ , a novel approach that takes into account the across bag relations .",
    "each set of related instances will be presented by its own motifs vector .",
    "this reduces the number of attributes that are not representative for the processed sequence .",
    "instead of using a classifier that uses a large vector to describe all the sequences data , every vector of motifs will be used to produce a prediction result .",
    "these results will be then aggregated to have a final result . based on the formalization",
    ", we propose an algorithm that discriminates bags by applying a classification model to each instance of the query bag .",
    "for each set of across bag sequences , we extract motifs and we construct a classification model . during the execution of the _ abclass _ algorithm",
    ", we will use the following variables :    * a matrix @xmath52 to store the encoded data of the learning database . * a vector @xmath54 to store the encoded data of the query bag instances . * a vector @xmath55 to store prediction results of the classification .",
    "the algorithm works as follows ( see algorithm [ algo1 ] ) .",
    "learning database @xmath56 , query bag @xmath57 prediction result @xmath2 @xmath58 @xmath59 @xmath60 @xmath61 @xmath62 @xmath63 @xmath2    the function is illustrated in algorithm [ algo2 ] .",
    "sequence index @xmath64 , learning database @xmath56 a set of sequences @xmath65",
    "@xmath66 @xmath67 @xmath65    informally , the main steps of the _ abclass _ algorithm are :    1 .   for each instance sequence @xmath68 in the query bag @xmath19 , the related instances among bags of the learning database are grouped into a list ( lines 1 to 3 ) .",
    "the algorithm extracts motifs from the list of grouped instances .",
    "these motifs are used to encode instances in order to create a discriminative model ( lines 4 and 5 ) .",
    "abclass _ uses the extracted motifs to represent the instance @xmath68 of the unknown bag into a vector @xmath54 , then it compares it with the corresponding model .",
    "the comparison results are stored in a vector @xmath55 ( lines 6 and 7 ) .",
    "_ abclass _ applies an aggregation method to @xmath55 in order to compute the final prediction result @xmath2 ( line 9 ) , which consists in a positive or a negative class label .",
    "we notice that the proposed approach can be simply evaluated by the accuracy of its prediction result .",
    "another option can be used is the rate of classification models that contributes to the prediction result .",
    "we apply the _ abclass _ approach to our running example .",
    "since the query bag contains two instances @xmath69 and @xmath70 , we will have two iterations followed by an aggregation step .",
    "first , the algorithm groups the set of bags that are related and extract the corresponding motifs .",
    "@xmath71 + @xmath72 +    then , it generates the attribute - value matrix @xmath52 describing the data .",
    "@xmath73    the sparsity percentage of @xmath52 is 33% .",
    "we note that the sparsity percentage of the produced matrix is reduced because there is no need to use the motifs extracted from instances @xmath74 to describe instances @xmath75 . a model",
    "is then created using the encoded data ( @xmath76 ) . using the features list ,",
    "a vector @xmath54 is generated to describe the first instance @xmath69 of the query bag .",
    "@xmath77    by applying the model to the vector @xmath54 , we obtain the first prediction result and we store it into the vector @xmath55 .",
    "@xmath78 +    the second iteration concerns the second instance @xmath70 of the query bag .",
    "we do the same instructions described in the first iteration .",
    "@xmath79 + @xmath80    @xmath81    the sparsity percentage of @xmath52 is reduced to 25% .",
    "a model is then created using the encoded data ( @xmath76 ) . using the features list ,",
    "a vector @xmath54 is generated to describe the second instance @xmath70 of the query bag .",
    "@xmath82 by applying the model to the vector @xmath54 , we obtain the second prediction result and we store it into the vector @xmath55 .",
    "@xmath83 +    the aggregation step is finally used to generate the final prediction result using the prediction vector @xmath55 .",
    "@xmath84      according to the specificity of the processed data , a similarity measure can be defined and used to discriminate instances .",
    "we propose an algorithm that focuses on discriminating bags by measuring the similarity between each instance sequence in the query bag and corresponding related sequences in the different bags of the learning database .",
    "we note @xmath52 a matrix used to store similarity measurement score vectors during the execution of the algorithm .",
    "the _ absim _",
    "algorithm works as follows :    learning database @xmath85 , query bag @xmath86prediction result @xmath2    @xmath87    @xmath88 @xmath2    informally , the algorithm is described as follows :    1 .   for each instance sequence @xmath68 in the query bag @xmath19",
    ", it computes the corresponding similarity measure scores ( line 1 to 4 ) .",
    "similarity scores of all instances of the query bag are grouped into a matrix @xmath52 ( line 3 ) .",
    "element @xmath89 corresponds to the similarity score between @xmath68 of @xmath19 and @xmath90 of @xmath10 .",
    "an aggregation method is applied to @xmath52 in order to compute the final prediction result @xmath2 ( line 6 ) . according to the aggregation result",
    ", a class label is associated to the query bag .    in our work , we define two aggregation methods : ( 1 ) sum of maximum scores ( sms ) and ( 2 ) weighted average of maximum scores ( wams )",
    ". algorithms [ algosms ] and [ algowams ] illustrate the sms and wams aggregation methods .",
    "similarity matrix @xmath91 and @xmath92 a prediction result @xmath2 @xmath93 @xmath94 @xmath95 @xmath96 @xmath97 @xmath98 @xmath99 @xmath100 @xmath101 @xmath102 @xmath2    for each sequence in the query bacterium , we scan the corresponding line of @xmath52 , which contains the obtained scores against all the other bags of the training database .",
    "the @xmath103 method selects the maximum score among the similarity scores against bags that belong to the positive class label ( which we call @xmath104 ) and the maximum score among the similarity scores against bags that belong to the negative class label ( which we call @xmath105 ) .",
    "it then compares these scores .",
    "if @xmath104 is greater than @xmath105 , it adds @xmath104 to the total score of the positive class label ( which we denote @xmath106 ) .",
    "otherwise , it adds @xmath105 to the total score of the negative class label ( which we denote @xmath107 ) .",
    "when all selected sequences were processed , the @xmath103 method compares total scores of positive class label and negative class label .",
    "if @xmath106 is greater than @xmath107 , the prediction output is the positive class label . otherwise , the prediction output is the negative class label .",
    "similarity matrix @xmath91 and @xmath108 , weight vector @xmath109    a prediction result @xmath2 @xmath93 @xmath94 @xmath110 @xmath111 @xmath95 @xmath96 @xmath97 @xmath98 @xmath112 @xmath113 @xmath114 @xmath115 @xmath116 @xmath117 @xmath101 @xmath102 @xmath2    using the @xmath118 method , each sequence @xmath119 has a given weight @xmath120 . for each sequence in the query bag",
    ", we scan the corresponding line of @xmath52 , which contains the obtained scores against all other bags of the training database .",
    "the @xmath118 method selects the maximum score among the similarity scores against bags that belong to positive class label ( which we denote @xmath121 ) and the maximum score among the similarity scores against bags that belong to the negative class label ( which we denote @xmath122 ) .",
    "it then compares these scores .",
    "if the @xmath121 is greater than @xmath122 , it adds @xmath121 multiplied by the weight of the sequence to the total score of the positive class label and it increments the number of positive bags having a max score .",
    "otherwise , it adds @xmath122 multiplied by the weight of the sequence to the total score of the negative class label and it increments the number of negative bags having a max score .",
    "when all the selected sequences were processed , we compare the average of total scores of positive class labels ( which we denote @xmath123 ) and the average of total scores of negative class labels ( which we denote @xmath124 ) .",
    "if @xmath123 is greater than @xmath124 , the prediction output is the positive class label . otherwise , the prediction output is the negative class label . in the case of irr prediction in bacteria",
    "the positive ( respectively negative ) class label corresponds to irrb ( respectively irsb ) .    in order to apply the _ absim _ approach to our running example , we need to use a similarity measure between sequences .",
    "suppose that we use a very simple similarity measure that consists in the number of common symbols between two sequences .",
    "the first iteration computes the common symbols between the instance @xmath69 of the query bag and the four instances @xmath31 , @xmath32 , @xmath33 and @xmath34 and stores the result in the first column of the matrix @xmath52 .",
    "@xmath125    the second iteration computes the common symbols between the instance @xmath70 of the query bag and the four instances @xmath35 , @xmath36 , @xmath37 and @xmath38 and stores the result in the second column of the matrix @xmath52 .",
    "@xmath126    the aggregation step applies @xmath103 or @xmath118 aggregation algorithm on the matrix @xmath52 in order to generate the final prediction result . using the @xmath103 aggregation method",
    ", we have the following results :    @xmath127 + @xmath128 +    the query bag @xmath19 is finally classified as positive .    using the @xmath118 aggregation method , it is needed to specify a weight value for each instance .",
    "we suppose that all sequences have the same weight value , then we have the following results :    @xmath129 + @xmath130 +    the query bag @xmath19 is finally classified as positive .",
    "we apply the proposed approaches to the problem of phenotype prediction of bacterial ionizing radiation resistance ( irr ) that can be formulated as a mil problem for sequence data @xcite .",
    "bacteria represent the bags and primary structure of basal dna repair proteins represent the sequences . in this context",
    ", an unknown bacterium is affiliated to either ionizing radiation resistant bacteria ( irrb ) or ionizing radiation sensitive bacteria ( irsb ) . for our tests",
    ", we used the dataset described in @xcite .",
    "this dataset consists of 28 bags ( 14 irrb and 14 irsb ) .",
    "each bacterium / bag contains 25 to 31 instances that correspond to proteins implicated in basal dna repair in irrb @xcite .",
    "additional and more detailed information about our datasets and our experiments in general can be found in the following link : http://fc.isima.fr/~aridhi / mil/.      computations were carried out on a i7 cpu 2.49 ghz pc with 6 gb memory , operating on linux ubuntu .",
    "we used weka @xcite data mining tool in order to apply existing multiple instance classifiers when using the naive approach . to deal with the _ absim _ approach",
    ", we used a local alignment technique as a similarity measure . in our tests , basic local alignment search tool ( blast ) @xcite was used for computing local alignments .      in our context ,",
    "the leave - one - out ( loo ) technique is considered to be the most objective test technique compared to the other techniques such as hold - out and cross validation .",
    "this can be explained by the fact that our training set contains a small number of bags . for each dataset",
    "( comprising @xmath7 bags ) , only one bag is kept for the test and the remaining bags are used for the training .",
    "this action is repeated @xmath7 times .    in order to evaluate the naive approach and the across bag sequences classification approach , we first encode the protein sequences of each bag using a set of features / motifs generated by an existing motif extraction method .",
    "then , we apply an existing classifier to the encoded data . in our tests",
    ", we used dms @xcite as a motif extraction method .",
    "dms allows building motifs that can discriminate a family of proteins from other ones .",
    "it first identifies motifs in the protein sequences .",
    "the extracted motifs are then filtered in order to keep only the discriminative and minimal ones .",
    "a substring is considered to be discriminative between the family @xmath131 and the other families if it appears in @xmath131 significantly more than in the other families .",
    "dms extracts discriminative motifs according to @xmath132 and @xmath133 thresholds where @xmath132 is the minimum rate of motif occurrences in the sequences of a family @xmath131 and @xmath133 is the maximum rate of motif occurrences in all sequences except those of the family @xmath131 . in the following",
    ", we present the used motif extraction settings according to the values of @xmath132 and @xmath133 :    * * s1 * ( @xmath134 and @xmath135 ) : used to extract frequent motifs with medium discrimination . * * s2 * ( @xmath134 and @xmath136 ) : used to extract frequent motifs without discrimination . * * s3 * ( @xmath137 and @xmath136 ) : used to extract motifs having medium frequencies without discrimination . * * s4 : * ( @xmath138 and @xmath136 ) : used to extract infrequent and non discriminative motifs . * * s5 : * ( @xmath139 and @xmath140 ) : used to extract frequent and strictly discriminative motifs .      in this section ,",
    "we first provide accuracy and quality results of the proposed approaches .",
    "then , we present a comparison of runtime values of both naive approach , _ abclass _ and _ absim_.      in order to use standard multiple instance classifiers , we apply a preprocessing technique that consists in extracting motifs from each set of protein sequences using the dms method . table [ tabmotifs ] presents , for each value of @xmath132 and @xmath133 , the number of extracted motifs from each set of orthologous protein sequences .",
    "llllll & + & s1 & s2 & s3 & s4 & s5 +    p1 & 397 & 459 & 856 & 2579 & 244 + p2 & 19 & 322 & 1505 & 5550 & 0 + p3 & 10 & 223 & 997 & 4752 & 0 + p4 & 7 & 143 & 726 & 4131 & 0 + p5 & 4 & 33 & 299 & 2036 & 0 + p6 & 14 & 135 & 619 & 3756 & 0 + p7 & 9 & 142 & 663 & 4297&1 + p8 & 5 & 221 & 793 & 4301&0 + p9 & 19 & 227 & 1107 & 4599 & 0 + p10 & 31 & 289 & 1027 & 4210 & 0 + p11 & 8 & 74 & 403 & 3138 & 0 + p12 & 8 & 74 & 328 & 2236 & 0 + p13 & 4 & 26 & 268 & 1882 & 0 + p14 & 5 & 62 & 343 & 2815 & 0 + p15 & 5 & 144 & 514 & 2405 & 0 + p16 & 8 & 69 & 408 & 3031 & 0 + p17 & 10 & 130 & 600 & 3086 & 0 + p18 & 0 & 25 & 229 & 2081 & 0 + p19 & 14 & 148 & 731 & 4185&0 + p20 & 6 & 110 & 593 & 3589 & 0 + p21 & 25 & 286 & 1206 & 4970 & 1 + p22 & 19 & 383 & 1110 & 4295 & 1 + p23 & 11 & 205 & 843 & 4545 & 0 + p24 & 8 & 191 & 822 & 4246 & 0 + p25 & 14 & 84 & 504 & 2933 & 0 + p26 & 31 & 221 & 1077 & 3730 & 2 + p27 & 17 & 54 & 324 & 2006 & 0 + p28 & 18 & 288 & 866 & 3680&1 + p29 & 4 & 56 & 432 & 3114 & 0 + p30 & 11 & 174 & 501 & 2178 & 0 + p31 & 1 & 20 & 228 & 2422&0 +    * total * & 742&5018 & 20922 & 106778 & 250 +    for setting 5 ( @xmath141 and @xmath142 ) , there is no frequent and strictly discriminative motifs for most proteins .",
    "this is why we will not use these values of @xmath132 and @xmath133 for our next experiments .",
    "we note that the number of extracted motifs increases for high values of @xmath133 and low values of @xmath132 .",
    "as presented in table [ tabmotifs ] , the number of infrequent and non discriminative motifs is very high .    in order to encode data ,",
    "the union of the extracted motifs from each protein is used .",
    "these motifs are used as attributes / features to construct a binary table where each row corresponds to sequence .",
    "the presence or the absence of an attribute in a sequence is denoted by @xmath143 or @xmath144 , respectively .",
    "this binary table is called an attribute - value matrix .",
    "it is worthwhile to mention that the number of used motifs in the encoding step is huge .",
    "consequently , the attribute - value matrix representing the data becomes large and sparse since only a small subset of the used motifs is representative for each protein .",
    "we show in table [ tabarff ] the sparsity of the attribute - value matrix which measure the fraction of zero elements over the total number of elements .",
    "lll motif extraction setting & total number of motifs & sparsity ( % ) +    s1 & 671&73.9 + s2 & 1490&73.8 + s3 & 4562&85.7 + s4 & 8077&91.1 +    the sparsity of our attribute - value matrix is generally proportional to the number of used motifs .",
    "for example , the sparsity of the matrix goes from 73.9% with 671 motifs to 91.1% with 8077 motifs .",
    "figure [ figsacc ] shows the accuracy values obtained using naive approach , _ abclass _ approach and _ absim _ approach .",
    "figures [ figaccweka ] and [ figaccmot ] show the impact of the set of motifs used in the preprocessing step on the results of the _ abclass _ and the naive approaches .",
    "for example , using misvm classifier , the accuracy varies from 53.5% to 78.5% .",
    "although the motifs extracted using s1 motif extraction setting are discriminative , naive approach does not provide good accuracy results for this setting except for the mismo classifier .",
    "the reason could be that the number of discriminative motifs for some proteins is limited to at most 10 as stressed in table [ tabmotifs ] . using the naive approach ,",
    "the best accuracy is always provided by mismo classifier ( 92.8% ) .",
    "accuracy of other used multiple instance classifiers depends on the used motifs .",
    "most of them provide good accuracy using the s3 setting ( non discriminative motifs with medium frequencies ) .",
    "the number of extracted motifs per protein using this setting is between 228 to 1505 which is an acceptable number of motifs used to encode a protein sequence . both _",
    "abclass _ and _ absim _ approaches provide good overall accuracy results since the least accuracy percentage is 89.2% .",
    "this clearly shows that our proposed approaches are efficient . using _ absim _",
    "approach with the sms aggregation method provides a better accuracy result compared to the _ wams _ aggregation method .",
    "the best result was reached using _ abclass _ approach , j48 classifier and the motif extraction settings s3 and s4 .",
    "using these two settings , a large number of non discriminative motifs are extracted ( see table [ tabmotifs ] ) .",
    "lllllll & & + & smo & j48 & naive bayes & smo & j48 & naive bayes +    b1 & * 44 * & * 60 * & * 36 * & * 60 * & * 64 * & * 68 * + b2 & 100 & 100 & 100 & 100 & 100 & 100 + b3 & 100 & 90.3 & 100 & 100 & 90.3 & 100 + b4 & 100 & 96.6 & 100 & 100 & 93.3 & 100 + b5 & 100 & 90 & 100 & 100 & 90 & 100 + b6 & 100 & 83.3 & 100 & 100 & 83.3 & 100 + b7 & 100 & 93.5 & 100 & 100 & 93.5 & 100 + b8 & 96.5 & 96.5 & 96.5 & 100 & 93.1 & 100 + b9 & 100 & 84 & 100 & 100 & 84 & 100 + b10 & 100 & 82.1 & 92.8 & 100 & 82.1 & 100 + b11 & * 17.8 * & * 50 * & * 17.8 * & * 21.4 * & * 50 * & * 35.7 * + b12 & 100 & 92.8 & 96.4 & 100 & 92.8 & 100 + b13 & 88.8 & 66.6 & 70.3 & 88.8 & 66.6 & 77.7 + b14 & 90 & 73.3 & 100 & 93.3 & 70 & 96.6 + b15 & * 3.5 * & * 32.1 * & * 35.7 * & * 0 * & * 32.1 * & * 14.2 * + b16 & 100 & 96.6 & 96.6 & 100 & 96.6 & 100 + b17 & 96.2 & 96.2 & 96.2 & 96.2 & 96.2 & 96.2 + b18 & 100 & 100 & 100 & 100 & 100 & 100 + b19 & 100 & 100 & 100 & 100 & 100 & 100 + b20 & 89.6 & 62 & 96.5 & 82.7 & 62 & 86.2 + b21 & 96.5 & 82.7 & 96.5 & 93.1 & 82.7 & 93.1 + b22 & 100 & 100 & 96.6 & 100 & 96.6 & 100 + b23 & 100 & 96.7 & 93.5 & 100 & 96.7 & 100 + b24 & 100 & 100 & 93.5 & 100 & 100 & 100 + b25 & 100 & 96.7 & 93.5 & 100 & 96.7 & 100 + b26 & 100 & 100 & 93.5 & 100 & 100 & 100 + b27 & 100 & 100 & 100 & 100 & 100 & 100 + b28 & 96.6 & 100 & 96.6 & 96.6 & 93.3 & 96.6 +    table [ tabconf ] presents the rate of classification models that contribute to predict the true class of each bacterium using _ abclass _ approach .",
    "we present this rate for the two motif extraction settings that already provided the best accuracy values i.e. , s3 and s4 .",
    "the rate of successful classification models for b1 , b11 and b15 are marked with bold text because these three bacteria generate always low rates compared to the rate of successful classification models of the other bacteria .",
    "b1 presents variable rates that reach 68% .",
    "although b11 is sometimes successfully classified , its higher successful classification models rate does not exceed 50%",
    ". the rate of b15 does not reach 50% which makes this bacterium always misclassified .",
    "these results may help to understand some characteristics of the studied bacteria .",
    "in particular , _ m. radiotolerans _ ( b11 ) and _ b. abortus _ ( b15 ) that present the lowest rates .",
    "it means that in most cases , _",
    "m. radiotolerans _ is predicted as irsb and _ b. abortus _ is predicted as irrb ; the former is an intracellular parasite @xcite and the latter is an endosymbiont of most plant species @xcite .",
    "a probable explanation for these two failed predictions is the increased rate of sequence evolution in endosymbiotic bacteria @xcite . as our training set",
    "is composed mainly of members of the phylum _",
    "deinococcus_-_thermus _ ; expectedly , the _ deinococcus _ bacteria ( b2-b7 ) present a very high rate of successful classification models .",
    "figure [ fig_time ] shows the runtime of our approaches using different settings .",
    "it is worthwhile to mention that the runtime of our approaches varies in each loo iteration according to the number of proteins contained in the query bacterium and the length of protein sequences of the learning set .",
    "the indicated runtime in figure [ fig_time ] is the average runtime of all the 28 iterations of the loo evaluation technique .",
    "we note that when using the naive approach , the motif extraction is done once in the preprocessing step , whereas when using the _ abclass _ approach it is executed in every loo iteration .",
    "the runtime values of the naive approach ( figures [ fig_miltime1 ] , [ fig_miltime2 ] , [ fig_miltime3 ] , [ fig_miltime4 ] and [ fig_miltime5 ] ) and the _ abclass _ approach ( figures [ fig_mot1 ] , [ fig_mot2 ] and [ fig_mot3 ] ) are mainly composed of two parts : the motif extraction runtime and the classification time .",
    "generally , these two parts are inversely proportional .",
    "for example , a large number of non discriminative motifs is extracted in about 9 seconds using the s4 motif extraction setting , then a larger runtime is needed in order to learn a classifier using such high number of motifs .",
    "we notice that the motif extraction runtime increases considerably when discriminative motifs are required in the preprocessing step . for example",
    ", it goes from 10 seconds for infrequent and non discriminative motifs to about 10 hours for the frequent and discriminative motifs .",
    "when the number of extracted motifs is low ( frequent and discriminative motifs ) the motif extraction time is high but the classification time is still reasonable . as stressed in figure [ fig_timeblast ] , the similarity based approach is much faster than both the naive approach and the _ abclass _ method with most motif extraction settings ( s1 , s2 and s3 ) .",
    "this is particularly due to the fact that the _ absim _ approach does not need a preprocessing step such as a motif extraction step .",
    "taking into account the accuracy and the total runtime , the best result is obtained using the _ abclass _ approach , j48 classifier and s4 motif extraction setting .",
    "we notice that midd which is an implementation of the diverse density algorithm @xcite has the longer execution time with low accuracy compared to all other classifiers . according the above presented results",
    ", we mention that both _ abclass _ and _ absim _ are efficient to perform mil on sequence data that have dependencies between instances across bags .",
    "it is also important to recommend the use of _ absim _ approach when a similarity measure can be easily defined and to use _ abclass _ approach when the data is already preprocessed .",
    "in this paper , we addressed the issue of multiple instance learning ( mil ) in the case of sequence data .",
    "we focused on data that present dependencies between instances of different bags .",
    "we have described two novel approaches for mil in sequence data : ( 1 ) _ abclass _ and ( 2 ) _ absim_. we applied the proposed approaches to the problem of prediction of ionizing radiation resistance ( irr ) in bacteria . by running experiments",
    ", we have shown that the proposed approaches are efficient .",
    "we are able to successfully predict irr of most bacteria , but we do not reach a 100% accuracy percentage using the different experimental settings with all proteins .    in the future work , we will study how the use of _ a priori _ knowledge can improve the efficiency of our algorithm .",
    "we specifically want to define weights for sequences by using _ a priori _ knowledge in the learning phase .",
    "d.  n. fedorov , g.  a. ekimova , n.  v. doronina , and y.  a. trotsenko .",
    "1-aminocyclopropane-1-carboxylate ( acc ) deaminases from methylobacterium radiotolerans and methylobacterium nodulans with higher specificity for acc . , 343(1):7076 , 2013 .",
    "s.  m. halling , b.  d. peterson - burch , b.  j. bricker , r.  l. zuerner , z.  qing , l .- l .",
    "li , v.  kapur , d.  p. alt , and s.  c. olsen .",
    "completion of the genome sequence of brucella abortus and comparison to the highly similar genomes of brucella melitensis and brucella suis .",
    ", 187(8):27152726 , 2005 .",
    "j.  kim , e.  n. moriyama , c.  g. warr , p.  j. clyne , and j.  r. carlson .",
    "identification of novel multi - transmembrane proteins from genomic databases using quasi - periodic structural properties .",
    ", 16(9):767775 , 2000 .",
    "q.  tao , s.  scott , n.  vinodchandran , and t.  t. osugi .",
    "svm - based generalized multiple - instance learning via approximate box counting .",
    "in _ proceedings of the twenty - first international conference on machine learning _",
    ", page 101 .",
    "acm , 2004 .",
    "j.  wang and j .- d .",
    "zucker . solving the multiple - instance problem : a lazy learning approach . in _ proceedings of the seventeenth international conference on machine learning _",
    ", icml 00 , pages 11191126 , san francisco , ca , usa , 2000 .",
    "morgan kaufmann publishers inc .",
    "h.  yamakawa , k.  maruhashi , and y.  nakao . predicting types of protein - protein interactions using a multiple - instance learning model .",
    "in t.  washio , k.  satoh , h.  takeda , and a.  inokuchi , editors , _ new frontiers in artificial intelligence _ ,",
    "volume 4384 of _ lecture notes in computer science _ , pages 4253 .",
    "springer berlin heidelberg , 2007 ."
  ],
  "abstract_text": [
    "<S> in multiple instance learning ( mil ) problem for sequence data , the learning data consist of a set of bags where each bag contains a set of instances / sequences . in many real world applications such as bioinformatics , </S>",
    "<S> web mining , and text mining , comparing a random couple of sequences makes no sense . </S>",
    "<S> in fact , each instance of each bag may have structural and/or temporal relation with other instances in other bags . </S>",
    "<S> thus , the classification task should take into account the relation between _ semantically related _ instances across bags . in this paper </S>",
    "<S> , we present two novel mil approaches for sequence data classification : ( 1 ) _ abclass _ and ( 2 ) _ absim_. in _ abclass _ , each sequence is represented by one vector of attributes . for each sequence of the unknown bag , </S>",
    "<S> a discriminative classifier is applied in order to compute a partial classification result . </S>",
    "<S> then , an aggregation method is applied to these partial results in order to generate the final result . in _ absim _ , we use a similarity measure between each sequence of the unknown bag and the corresponding sequences in the learning bags . </S>",
    "<S> an unknown bag is labeled with the bag that presents more similar sequences . </S>",
    "<S> we applied both approaches to the problem of bacterial ionizing radiation resistance ( irr ) prediction . </S>",
    "<S> we evaluated and discussed the proposed approaches on well known ionizing radiation resistance bacteria ( irrb ) and ionizing radiation sensitive bacteria ( irsb ) represented by primary structure of basal dna repair proteins . </S>",
    "<S> the experimental results show that both _ </S>",
    "<S> abclass _ and _ absim _ approaches are efficient . + </S>"
  ]
}