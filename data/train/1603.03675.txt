{
  "article_text": [
    "this paper is motivated by the observation that empirical research in economics increasingly involves the collection of original data through laboratory or field experiments ( see , e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* among others ) .",
    "this observation carries with it a call and an opportunity for research to provide econometrically sound guidelines for data collection .",
    "we consider the decision problem faced by a researcher designing the survey for a randomized control trial ( rct ) .",
    "we assume that the goal of the researcher is to obtain precise estimates of the average treatment effect and/or a powerful t - test of the hypothesis of no treatment effect , using the experimental data .",
    "data collection is costly and the researcher is restricted by a budget , which limits how much data can be collected .",
    "we focus on optimally trading off the number of individuals included in the rct and the choice of covariates elicited as part of the data collection process .",
    "there are , of course , other factors potentially influencing the choice of covariates to be collected in a survey for an rct .",
    "for example , one may wish to learn about the mechanisms through which the rct is operating , check whether treatment or control groups are balanced , or measure heterogeneity in the impacts of the intervention being tested . in practice ,",
    "researchers place implicit weights on each of the main objectives they consider when designing surveys , and consider informally the different trade - offs involved in their choices .",
    "we show that there is substantial value to making this decision process more rigorous and transparent through the use of data - driven tools that optimize a well - defined objective . instead of attempting to formalize the whole research design process , we focus on one particular trade - off that we think is of first - order importance and particularly conducive to data - driven procedures",
    ".    we assume the researcher has access to pre - experimental data from the population from which the experimental data will be drawn or at least from a population that shares similar second moments of the variables to be collected .",
    "the data set includes all the potentially relevant variables that one would consider collecting for the analysis of the experiment .",
    "the researcher faces a fixed budget for implementing the survey for the rct .",
    "given this budget , the researcher chooses the survey s sample size and set of covariates to optimize the resulting treatment effect estimator s precision and/or the corresponding t - test s power .",
    "this choice takes place before the implementation of the rct and could , for example , be part of a pre - analysis plan in which , among other things , the researcher specifies outcomes of interest , covariates to be selected , and econometric techniques to be used .    in principle",
    ", the trade - offs involved in this choice involve basic economic reasoning .",
    "for each possible covariate , one should be comparing the marginal benefit and marginal cost of including it in the survey , which in turn , depend on all the other covariates included in the survey . as we discuss below , in simple settings it is possible to derive analytic and intuitive solutions to this problem .",
    "although these are insightful , they only apply in unrealistic formulations of the problem .    in general",
    ", for each covariate , there is a discrete choice of whether to include it or not , and for each possible sample size , one needs to consider all possible combinations of covariates within the budget .",
    "this requires a solution to a computationally difficult combinatorial optimization problem .",
    "this problem is especially challenging when the set of potential variables to choose from is large , a case that is increasingly encountered in today s big data environment .",
    "fortunately , with the increased availability of high - dimensional data , methods for the analysis of such data sets have received growing attention in several fields , including economics @xcite .",
    "this literature makes available a rich set of new tools , which can be adapted to our study of optimal survey design .    in this paper",
    ", we propose the use of a computationally attractive algorithm based on the orthogonal greedy algorithm ( oga )  also known as the orthogonal matching pursuit ; see , for example , @xcite and @xcite among many others . to implement the oga ,",
    "it is necessary to specify the stopping rule , which in turn generally requires a tuning parameter .",
    "one attractive feature of our algorithm is that , once the budget constraint is given , there is no longer the need to choose a tuning parameter to implement the proposed method , as the budget constraint plays the role of a stopping rule . in other words ,",
    "we develop an automated oga that is tailored to our own decision problem .",
    "furthermore , it performs well even when there are a large number of potential covariates in the pre - experimental data set .",
    "there is a large and important body of literature on the design of experiments , starting with @xcite .",
    "there also exists an extensive body of literature on sample size ( power ) calculations ; see , for example , @xcite for a practical guide .",
    "both bodies of this literature are concerned with the precision of treatment effect estimates , but neither addresses the problem that concerns us . for instance , @xcite have developed methods to choose the sample size when cost constraints are binding , but they neither consider the issue of collecting covariates nor its trade - off with selecting the sample size .",
    "both our paper and the standard literature on power calculations rely on the availability of information in pre - experimental data .",
    "the calculations we propose can be seen as a substantive reformulation and extension of the more standard power calculations , which are an important part of the design of any rct .",
    "when conducting power calculations , one searches for the sample size that allows the researcher to detect a particular effect size in the experiment .",
    "the role of covariates can be accounted for if one has pre - defined the covariates that will be used in the experiment , and one knows ( based on some pre - experimental data ) how they affect the outcome .",
    "then , once the significance level and power parameters are determined ( specifying the type i and type ii errors one is willing to accept ) , all that matters is the impact of the sample size on the variance of the treatment effect .",
    "suppose that , instead of asking what is the minimum sample size that allows us to detect a given effect size , we asked instead how small an effect size we could detect with a particular sample size ( this amounts to a reversal of the usual power calculation ) . in this simple",
    "setting with pre - defined covariates , the sample size would define a particular survey cost , and we would essentially be asking about the minimum size of the variance of the treatment effect estimator that one could obtain at this particular cost , which would lead to a question similar to the one asked in this paper .",
    "therefore , one simple way to describe our contribution is that we adapt and extend the information in power calculations to account for the simultaneous selection of covariates and sample size , explicitly considering the costs of data collection .    to illustrate the application of our method we examine two recent experiments for which we have detailed knowledge of the process and costs of data collection .",
    "we ask two questions .",
    "first , if there is a single hypothesis one wants to test in the experiment , concerning the impact of the experimental treatment on one outcome of interest , what is the optimal combination of covariate selection and sample size given by our method , and how much of an improvement in the precision of the impact estimate can we obtain as a result ?",
    "second , what are the minimum costs of obtaining the same precision of the treatment effect as in the actual experiment , if one was to select covariates and sample size optimally ( what we call the `` equivalent budget '' ) ?",
    "we find from these two examples that by adopting optimal data collection rules , not only can we achieve substantial increases in the precision of the estimates ( statistical importance ) for a given budget , but we can also accomplish sizeable reductions in the equivalent budget ( economic importance ) . to illustrate the quantitative importance of the latter ,",
    "we show that the optimal selection of the set of covariates and the sample size leads to a reduction of about 45 percent ( up to 58 percent ) of the original budget in the first ( second ) example we consider , while maintaining the same level of the statistical significance as in the original experiment .    to the best of our knowledge",
    ", no paper in the literature directly considers our data collection problem .",
    "some papers address related but very different problems ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "they study some issues of data measurement , budget allocation or efficient estimation ; however , they do not consider the simultaneous selection of the sample size and covariates for the rcts as in this paper . because our problem is distinct from the problems studied in these papers ,",
    "we give a detailed comparison between our paper and the aforementioned papers in section [ sec : literature ] .",
    "more broadly , this paper is related to a recent emerging literature in economics that emphasizes the importance of micro - level predictions and the usefulness of machine learning for that purpose .",
    "for example , @xcite argue that prediction problems are abundant in economic policy analysis , and recent advances in machine learning can be used to tackle those problems .",
    "furthermore , our paper is related to the contemporaneous debates on pre - analysis plans which demand , for example , the selection of sample sizes and covariates before the implementation of an rct ; see , for example , @xcite and @xcite for the advantages and limitations of the pre - analysis plans .",
    "the remainder of the paper is organized as follows . in section [ sec : problem ] , we describe our data collection problem in detail . in section [ sec : algorithm ] , we propose the use of a simple algorithm based on the oga . in section",
    "[ sec : cost ] , we discuss the costs of data collection in experiments . in section [ sec : app ] , we present two empirical applications , in section [ sec : literature ] , we discuss the existing literature , and in section [ sec : conclusion ] , we give concluding remarks .",
    "appendices provide details that are omitted from the main text .",
    "suppose we are planning an rct in which we randomly assign individuals to either a treatment ( @xmath0 ) or a control group ( @xmath1 ) with corresponding potential outcomes @xmath2 and @xmath3 , respectively .",
    "after administering the treatment to the treatment group , we collect data on outcomes @xmath4 for both groups so that @xmath5 .",
    "we also conduct a survey to collect data on a potentially very high - dimensional vector of covariates @xmath6 ( e.g. from a household survey covering demographics , social background , income etc . )",
    "that predicts potential outcomes .",
    "these covariates are a subset of the universe of predictors of potential outcomes , denoted by @xmath7 .",
    "random assignment of @xmath8 means that @xmath8 is independent of potential outcomes and of @xmath7 .",
    "our goal is to estimate the average treatment effect @xmath9 $ ] as precisely as possible , where we measure precision by the finite sample mean - squared error ( mse ) of the treatment effect estimator , and/or produce a powerful t - test of the hypothesis @xmath10 . instead of simply regressing @xmath4 on @xmath8 ,",
    "we want to make use of the available covariates @xmath6 to improve the precision of the resulting treatment effect estimator .",
    "therefore , we consider estimating @xmath11 in the regression @xmath12 where @xmath13 is a vector of parameters to be estimated and @xmath14 is an error term .",
    "the implementation of the rct requires us to make two decisions that may have a significant impact on the estimation of and inference on the average treatment effect :    1",
    ".   which covariates @xmath6 should we select from the universe of potential predictors @xmath7 ?",
    "2 .   from how many individuals ( @xmath15 )",
    "should we collect data on @xmath16 ?    obviously , a large experimental sample size @xmath15 reduces the variance of the treatment effect estimator .",
    "similarly , collecting more covariates , in particular strong predictors of potential outcomes , reduces the variance of the residual @xmath14 which , in turn , also improves the variance of the estimator . at the same time",
    "collecting data from more individuals and on more covariates is costly so that , given a finite budget , we want to find a combination of sample size @xmath15 and covariate selection @xmath6 that leads to the most precise treatment effect estimator possible .    in this section ,",
    "we propose a procedure to make this choice based on a pre - experimental data set on @xmath4 and @xmath7 , such as a pilot study or a census from the same population from which we plan to draw the rct sample .",
    "the combined data collection and estimation procedure can be summarized as follows :    1 .",
    "obtain pre - experimental data @xmath17 on @xmath18 .",
    "2 .   use data in @xmath17 to select the covariates @xmath6 and sample size @xmath15 .",
    "3 .   implement the rct and collect the experimental data @xmath19 on @xmath16 .",
    "4 .   estimate the average treatment effect using @xmath19 .",
    "5 .   compute standard errors .",
    "we now describe the five steps listed above in more detail .",
    "the main component of our procedure consists of a proposal for the optimal choice of @xmath15 and @xmath6 in step 2 , which is described more formally in section  [ sec : algorithm ] .",
    "* step * .",
    "* obtain pre - experimental data .",
    "* we assume the availability of data on outcomes @xmath20 and covariates @xmath21 for the population from which we plan to draw the experimental data .",
    "we denote the pre - experimental sample of size @xmath22 by @xmath23 .",
    "our framework allows the number of potential covariates , @xmath24 , to be very large ( possibly much larger than the sample size @xmath22 ) .",
    "typical examples would be census data , household surveys , or data from other , similar experiments .",
    "another possible candidate is a pilot experiment that was carried out before the larger - scale role out of the main experiment , provided that the sample size @xmath22 of the pilot study is large enough for our econometric analysis in step 2 .",
    "* optimal selection of covariates and sample size .",
    "* we want to use the pre - experimental data to choose the sample size , and which covariates should be in our survey .",
    "let @xmath25 be a vector of ones and zeros of the same dimension as @xmath7 .",
    "we say that the @xmath26th covariate ( @xmath27 ) is selected if @xmath28 , and denote by @xmath29 the subvector of @xmath7 containing elements that are selected by @xmath30 .",
    "for example , consider @xmath31 and @xmath32 .",
    "then @xmath33 .",
    "for any vector of coefficients @xmath34 , let @xmath35 denote the nonzero elements of @xmath36 and @xmath37 .",
    "we can then rewrite as @xmath38 where @xmath34 and @xmath39 . for a given @xmath36 and sample size @xmath15 ,",
    "we denote by @xmath40 the ols estimator of @xmath11 in a regression of @xmath41 on a constant and @xmath8 , using a random sample @xmath42 .",
    "we also consider the two - sided t - test of @xmath43 using the t - statistic @xmath44 where @xmath45 is the residual variance and @xmath46 the number of individuals in the treatment group divided by the sample size @xmath15 .",
    "data collection is costly and therefore constrained by a budget of the form @xmath47 , where @xmath48 are the costs of collecting the variables given by selection @xmath30 from @xmath15 individuals , and @xmath49 is the researcher s budget .",
    "we assume the researcher is interested in collecting data so as to ensure good statistical properties of the resulting treatment effect estimator and the corresponding t - test .",
    "we consider two criteria , the mse of @xmath40 and the power of the t - test that employs @xmath50 .",
    "we now briefly argue that minimizing the mse of @xmath40 and maximizing power of the t - test lead to equivalent optimization problems for selecting the optimal collection of covariates and sample size .",
    "subsequently , we directly consider that optimization problem and the approximation of its solution , thereby transparently covering both objectives at the same time .",
    "first , consider choosing the experimental sample size @xmath15 and the covariate selection @xmath30 so as to minimize the finite sample mse of @xmath40 , i.e. , we want to choose @xmath15 and @xmath36 to minimize @xmath51.\\ ] ] subject to the budget constraint .",
    "[ ass : random sampling ] ( i ) @xmath52 is an i.i.d .",
    "sample from the distribution of @xmath53 such that @xmath8 is completely randomized .",
    "( ii ) @xmath54 for all @xmath34 .    part ( i ) of this assumption is standard .",
    "there are other assignment mechanisms such as re - randomization , but we focus on the simplest case in the paper . part ( ii ) is a homoskedasticity assumption that is common in standard power calculations and requires the residual variance to be the same across the treatment and control group .",
    "this assumption is satisfied , for example , when the treatment effect is constant across individuals in the experiment .",
    "if the researcher feels uncomfortable with this assumption , it is necessary to collect a pilot study that produces pre - experimental data from the joint distribution of @xmath55 .",
    "the power of the homoskedasticity assumption is that , as we discuss in more detail below , data on @xmath8 is not required for the optimal choice of @xmath15 and @xmath30 .",
    "the following lemma characterizes the finite sample mse of the estimator under the above assumption .",
    "[ lem : equiv mse ] under assumption  [ ass : random sampling ] , for any @xmath34 , @xmath56    the proof of this lemma can be found in the appendix .",
    "note that for each ( @xmath57 ) , the mse is minimized by the equal splitting between the treatment and control groups . hence , suppose that the treatment and control groups are of exactly the same size ( i.e. , @xmath58 ) . by lemma  [ lem : equiv mse ] , minimizing the mse of the treatment effect estimator subject to the budget constraint , @xmath59 is equivalent to minimizing the residual variance in a regression of @xmath4 on @xmath7 , divided by the sample size , @xmath60    now , consider choosing the experimental sample size @xmath15 and the covariate selection @xmath30 so as to maximize power of the two - sided t - test based on @xmath50 .",
    "denote by @xmath61 and @xmath62 the @xmath63-quantile and cumulative distribution function of the standard normal distribution , respectively .",
    "the following lemma calculates the test s finite sample power under the assumption of joint normality of @xmath4 and @xmath7 .",
    "[ lem : equiv power ] suppose assumption  [ ass : random sampling ] holds and that @xmath64 are jointly normal .",
    "then , for any @xmath65 , @xmath66 , and @xmath34 , @xmath67 where @xmath68 denotes probabilities under the assumption that @xmath69 is the true coefficient in front of @xmath8 .",
    "furthermore , @xmath70 is decreasing in @xmath71 .",
    "the lemma shows that , under the normality assumption and for any alternative @xmath66 and size @xmath63 , the power of the two - sided t - test is a decreasing transformation of @xmath72 .",
    "therefore , assigning as many individuals to the treatment as to the control group , besides minimizing the mse above also maximizes power .",
    "therefore , assuming again @xmath73 , maximizing power subject to the budget constraint , @xmath74 is also equivalent to minimizing the residual variance in a regression of @xmath4 on @xmath7 , divided by the sample size , as in .",
    "notice that even when @xmath64 are not jointly normal , the power expression in lemma  [ lem : equiv power ] may be approximately correct because the berry - esseen bound guarantees that the t - statistic s distribution is close to normal as long as @xmath15 is not too small .",
    "having motivated the optimization problem in in terms of minimization of the mse of the treatment effect estimator as well as in terms of maximization of power of the corresponding t - test , we now discuss how to approximate the solution to in a given finite sample .",
    "importantly , notice that the optimization problem depends on the data only through the residual variance @xmath75 , which , under assumption  [ ass : random sampling ] , can be estimated before the randomization takes place , i.e. using the pre - experimental sample @xmath17 .",
    "therefore , employing the standard sample variance estimator of @xmath75 , the sample counterpart of our population optimization problem is @xmath76 the problem , which is based on the pre - experimental sample , approximates the population problem for the experiment if the second moments in the pre - experimental sample are close to the second moments in the experiment ( which holds , for example , if the population in the pre - experimental sample is the same as the population in the experiment ) .    in section  [ sec : algorithm ] , we describe a computationally attractive oga that approximates the solution to .",
    "the oga has been studied extensively in the signal extraction literature and is implemented in most statistical software packages .",
    "appendices a and d show that this algorithm possesses desirable theoretical and practical properties .",
    "the basic idea of the algorithm ( in its simplest form ) is straightforward .",
    "fix a sample size @xmath15 .",
    "start by finding the covariate that has the highest correlation with the outcome .",
    "regress the outcome on that variable , and keep the residual .",
    "then , among the remaining covariates , find the one that has the highest correlation with the residual .",
    "regress the outcome onto both selected covariates , and keep the residual . again , among the remaining covariates , find the one that has the highest correlation with the new residual , and proceed as before .",
    "we iteratively select additional covariates up to the point when the budget constraint is no longer satisfied .",
    "finally , we repeat this search process for alternative sample sizes , and search for the combination of sample size and covariate selection that minimizes the residual variance . denote the oga solution by @xmath77 and",
    "let @xmath78 denote the selected covariates .",
    "see section  [ sec : algorithm ] for more details .",
    "note that , generally speaking , the oga requires us to specify how to terminate the iterative procedure .",
    "one attractive feature of our algorithm is that the budget constraint plays the role of the stopping rule , without introducing any tuning parameters",
    ".    * experiment and data collection . * given the optimal selection of covariates @xmath79 and sample size @xmath80 , we randomly assign @xmath80 individuals to either the treatment or the control group ( with equal probability ) , and collect the covariates @xmath81 from each of them .",
    "this yields the experimental sample @xmath82 from @xmath83 .",
    "* estimation of the average treatment effect .",
    "* we regress @xmath84 on @xmath85 using the experimental sample @xmath19 .",
    "the ols estimator of the coefficient on @xmath86 is the average treatment effect estimator @xmath87 .",
    "* computation of standard errors .",
    "* assuming the two samples @xmath17 and @xmath19 are independent , and that treatment is randomly assigned , the presence of the covariate selection step 2 does not affect the asymptotic validity of the standard errors that one would use in the absence of step 2 . therefore , asymptotically valid standard errors of @xmath87 can be computed in the usual fashion ( see , e.g. , * ? ? ?",
    "* ) .      in this subsection",
    ", we discuss some of conceptual and practical properties of our proposed data collection procedure .",
    "[ [ availability - of - pre - experimental - data . ] ] availability of pre - experimental data .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    as in standard power calculations , pre - experimental data provide essential information for our procedure .",
    "the availability of such data is very common , ranging from census data sets and other household surveys to studies that were conducted in a similar context as the rct we are planning to implement .",
    "in addition , if no such data set is available , one may consider running a pilot project that collects pre - experimental data .",
    "we recognize that in some cases it might be difficult to have the required information readily available .",
    "however , this is a problem that affects any attempt to a data - driven design of surveys , including standard power calculations . even when pre - experimental data are imperfect , such calculations provide a valuable guide to survey design , as long as the available pre - experimental data are not very different from the ideal data .",
    "in particular , our procedure only requires second moments of the pre - experimental variables to be similar to those in the population of interest .",
    "[ [ the - optimization - problem - in - a - simplified - setup . ] ] the optimization problem in a simplified setup .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in general , the problem in does not have a simple solution and requires joint optimization problem over the sample size @xmath15 and the coefficient @xmath36 . to gain some intuition about the trade - offs in this problem , in appendix c we consider a simplified setup in which all covariates are orthogonal to each other , and the budget constraint has a very simple form . in this case , the constraint can be substituted into the objective and the optimization becomes univariate and unconstrained .",
    "we show that if all covariates have the same price , then one wants to choose covariates up to the point where the percentage increase in survey costs equals the percentage reduction in the residual variance from the last covariate .",
    "furthermore , the elasticity of the residual variance with respect to changes in sample size should equal the elasticity of the residual variance with respect to an additional covariate .",
    "if the costs of data collection vary with covariates , then this conclusion is slightly modified .",
    "if we organize variables by type according to their contribution to the residual variance , then we want to choose variables of each type up to the point where the percent marginal contribution of each variable to the residual variance equals its percent marginal contribution to survey costs .",
    "[ [ imbalance - and - re - randomization . ] ] imbalance and re - randomization .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in rcts , covariates typically do not only serve as a means to improving the precision of treatment effect estimators , but also for checking whether the control and treatment groups are balanced .",
    "see , for example , @xcite for practical issues concerning randomization and balance . to rule out large biases due to imbalance , it is important to carry out balance checks for strong predictors of potential outcomes .",
    "our procedure selects the strongest predictors as long as they are not too expensive ( e.g. household survey questions such as gender , race , number of children etc . ) and we can check balance for these covariates .",
    "however , in principle , it is possible that our procedure does not select a strong predictor that is very expensive ( e.g. baseline test scores ) .",
    "such a situation occurs in our second empirical application ( section  [ sec : school grants ] ) . in this case , in step 2 , we recommend running the oga a second time , forcing the inclusion of such expensive predictors . if the mse of the resulting estimate is not much larger than that from the selection without the expensive predictor , then we may prefer the former selection to the latter so as to reduce the potential for bias due to imbalance at the expense of slightly larger variance of the treatment effect estimator .",
    "an alternative approach to avoiding imbalance considers re - randomization until some criterion capturing the degree of balance is met ( e.g. , @xcite , @xcite and @xcite ) .",
    "our criterion for the covariate selection procedure in step 2 can readily be adapted to this case ; however , the details are not worked out here .",
    "it is an interesting future research topic to fully develop a data collection method for re - randomization based on the modified variance formulae in @xcite and @xcite , which account for the effect of re - randomization on the treatment effect estimator .",
    "[ [ expensive - strong - predictors . ] ] expensive , strong predictors .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + +    when some covariates have similar predictive power , but respective prices that are substantially different , our covariate selection procedure may produce a suboptimal choice .",
    "for example , if the covariate with the highest price is also the most predictive , oga selects it first even when there are other covariates that are much cheaper but only slightly less predictive . in section [ sec : school grants ] , we encounter an example of such a situation and propose a simple robustness check for whether removing an expensive , strong predictor may be beneficial .    [ [ properties - of - the - treatment - effect - estimator . ] ] properties of the treatment effect estimator .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    since the treatment indicator is assumed independent of @xmath7 , standard asymptotic theory of the treatment effect estimator continues to hold for our estimator ( despite the addition of a covariate selection step ) .",
    "for example , it is unbiased , consistent , asymptotically normal , and adding the covariates @xmath7 in the regression in can not increase the asymptotic variance of the estimator .",
    "in fact , inclusion of a covariate strictly reduces the estimator s asymptotic variance as long as the corresponding true regression coefficient is not zero .",
    "all these results hold regardless of whether the true conditional expectation of @xmath4 given @xmath8 and @xmath7 is in fact linear and additive separable as in or not . in particular ,",
    "in some applications one may want to include interaction terms of @xmath8 and @xmath7 ( see , e.g. , * ? ? ? * ) .",
    "finally , the treatment effect can be allowed to be heterogeneous ( i.e. vary across individuals @xmath88 ) in which case our procedure estimates the average of those treatment effects .    [",
    "[ an - alternative - to - regression . ] ] an alternative to regression .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + +    step 4 consists of running the regression in .",
    "there are instances when it is desirable to modify this step .",
    "for example , if the selected sample size @xmath80 is smaller than the number of selected covariates , then the regression in is not feasible . however , if the pre - experimental sample @xmath17 is large enough , we can instead compute the ols estimator @xmath89 from the regression of @xmath4 on @xmath90 in @xmath17 . then use @xmath4 and @xmath6 from the experimental sample @xmath91 to construct the new outcome variable @xmath92 and compute the treatment effect estimator @xmath87 from the regression of @xmath93 on @xmath94 .",
    "this approach avoids fitting too many parameters when the experimental sample is small and has the additional desirable property that the resulting estimator is free from bias due to imbalance in the selected covariates .",
    "[ [ multivariate - outcomes . ] ] multivariate outcomes .",
    "+ + + + + + + + + + + + + + + + + + + + + +    it is straightforward to extend our data collection method to the case when there are multivariate outcomes .",
    "appendix g provides details regarding how to deal with a vector of outcomes when we select the common set of regressors for all outcomes .",
    "in practice , the vector @xmath7 of potential covariates is typically high - dimensional , which makes it challenging to solve the optimization problem . in this section ,",
    "we propose a computationally feasible algorithm that is both conceptually simple and performs well in our simulations .",
    "in particular , it requires only running many univariate , linear regressions and can therefore easily be implemented in popular statistical packages such as stata .",
    "we split the joint optimization problem in over @xmath15 and @xmath36 into two nested problems .",
    "the outer problem searches over the optimal sample size @xmath15 , which is restricted to be on a grid @xmath95 , while the inner problem determines the optimal selection of covariates for each sample size @xmath15 : @xmath96 to convey our ideas in a simple form , suppose for the moment that the budget constraint has the following linear form , @xmath97 where @xmath98 denotes the number of non - zero elements of @xmath36 .",
    "note that the budget constraint puts the restriction on the number of selected covariates , that is , @xmath99 .",
    "it is known to be np - hard ( non - deterministic polynomial time hard ) to find a solution to the inner optimization problem in subject to the constraint that @xmath36 has @xmath100 non - zero components , also called an @xmath100-term approximation , where @xmath100 is the integer part of @xmath101 in our problem . in other words , solving directly is not feasible unless the dimension of covariates , @xmath24 , is small @xcite",
    ".    there exists a class of computationally attractive procedures called greedy algorithms that are able to approximate the infeasible solution .",
    "see @xcite for a detailed discussion of greedy algorithms in the context of approximation theory .",
    "@xcite , @xcite , @xcite , @xcite , @xcite , @xcite , and @xcite , among many others , demonstrate the usefulness of greedy algorithms for signal recovery in information theory , and for the regression problem in statistical learning .",
    "we use a variant of oga that can allow for selection of groups of variables ( see , for example , @xcite ) .    to formally define our proposed algorithm",
    ", we introduce some notation . for a vector @xmath102 of @xmath22 observations @xmath103 ,",
    "let @xmath104 denote the empirical @xmath105-norm and let @xmath106 .",
    "suppose that the covariates @xmath27 , @xmath107 , are organized into @xmath108 pre - determined groups @xmath109 , where @xmath110 indicates the covariates of group @xmath111 .",
    "we denote the corresponding matrices of observations by bold letters ( i.e. , @xmath112 is the @xmath113 matrix of observations on @xmath114 , where @xmath115 denotes the number of elements of the index set @xmath116 ) . by a slight abuse of notation ,",
    "we let @xmath117 be the column vector of observations on @xmath118 when @xmath111 is a scalar .",
    "one important special case is that in which each group consists of a single regressor .",
    "furthermore , we allow for overlapping groups ; in other words , some elements can be included in multiple or even all groups .",
    "the group structure occurs naturally in experiments where data collection is carried out through surveys whose questions can be grouped in those concerning income , those concerning education , and so on .",
    "this can also occur naturally when we consider multivariate outcomes . see appendix g for details .",
    "suppose that the largest group size @xmath119 is small , so that we can implement orthogonal transformations _ within each group _ such that @xmath120 , where @xmath121 is the @xmath122-dimensional identity matrix .",
    "in what follows , assume that @xmath123 without loss of generality .",
    "let @xmath124 denote the @xmath125 norm .",
    "the following procedure describes our algorithm .",
    "set the initial sample size @xmath126 .",
    "group oga for a given sample size @xmath15 :    1 .",
    "initialize the inner loop at @xmath127 and set the initial residual @xmath128 , the initial covariate indices @xmath129 and the initial group indices @xmath130 ; 2 .",
    "separately regress @xmath131 on each group of regressors in @xmath132 ; call @xmath133 the group of regressors with the largest @xmath125 regression coefficients , @xmath134 add @xmath133 to the set of selected groups , @xmath135 ; 3 .",
    "regress @xmath136 on the covariates @xmath137 where @xmath138 ; call the regression coefficient @xmath139 and the residual @xmath140 ; 4 .",
    "increase @xmath111 by one and continue with ( b ) as long as @xmath141 is satisfied ; 5 .   let @xmath142 be the number of selected groups ; call the resulting submatrix of selected regressors @xmath143 and @xmath144 , respectively .",
    "set @xmath15 to the next sample size in @xmath145 , and go to step 2 until ( and including ) @xmath146 .    set @xmath80 as the sample size that minimizes the residual variance : @xmath147",
    "the algorithm above produces the selected sample size @xmath80 , the selection of covariates @xmath148 with @xmath149 selected groups and @xmath150 selected regressors . here , @xmath151 is the corresponding coefficient vector on the selected regressors @xmath6 .",
    "theorem  [ thm : risk bound ] in appendix  a gives a finite - sample bound on the criterion function resulting from our oga method and , thus , also for the mse of the resulting treatment effect estimator .",
    "the natural target for this residual variance is an infeasible residual variance when @xmath152 is known _ a priori_. theorem  [ thm : risk bound ] establishes conditions under which the difference between the residual variance resulting from our method and the infeasible residual variance decreases at a rate of @xmath153 as @xmath111 increases , where @xmath111 is the number of the steps in the oga .",
    "it is known in a simpler setting than ours that this rate @xmath153 can not generally be improved ( see , e.g. , * ? ? ?",
    "* ) . in this sense",
    ", we show that our proposed method has a desirable property .",
    "see appendix  a for further details .",
    "there are many important reasons for collecting covariates , such as checking whether randomization was carried out properly and identifying heterogeneous treatment effects , among others .",
    "if a few covariates are essential for the analysis , we can guarantee their selection by including them in every group @xmath116 , @xmath154 .    in a simple model such as the one in appendix c , the optimal combination of covariates equalizes the percent marginal contribution of an additional variable to the residual variance with the percent marginal contribution of the additional variable to the costs per interview .",
    "step 2 of the oga selects the next covariate as the one that has the highest predictive power independent of its cost . outside a class of very simple models as in appendix c , it is difficult to determine an oga approximation to the optimum that jointly takes into account both predictive power as it requires comparison of all possible covariate combinations . in our empirical application of section  v.b ,",
    "we study a case with heterogeneous costs and propose a sensitivity analysis that assesses whether the oga solution significantly changes with perturbations of the set of potential covariates .",
    "in this section , we discuss the specification of the cost function @xmath48 that defines the budget constraint of the researcher . in principle , it is possible to construct a matrix containing the value of the costs of data collection for every possible combination of @xmath30 and @xmath15 without assuming any particular form of relationship between the individual entries .",
    "however , determination of the costs for every possible combination of @xmath30 and @xmath15 is a cumbersome and , in practice , probably infeasible exercise .",
    "therefore , we consider the specification of cost functions that capture the costs of all stages of the data collection process in a more parsimonious fashion .    we propose to decompose the overall costs of data collection into three components : administration costs @xmath155 , training costs @xmath156 , and interview costs @xmath157 , so that @xmath158 in the remainder of this section , we discuss possible specifications of the three types of costs by considering fixed and variable cost components corresponding to the different stages of the data collection process .",
    "the exact functional form assumptions are based on the researcher s knowledge about the operational details of the survey process . even though this section s general discussion is driven by our experience in the empirical applications of section  [ sec : app ] ,",
    "the operational details are likely to be similar for many surveys , so we expect the following discussion to provide a useful starting point for other data collection projects .",
    "we start by specifying survey time costs .",
    "let @xmath159 , @xmath107 , be the costs of collecting variable @xmath26 for one individual , measured in units of survey time .",
    "similarly , let @xmath160 denote the costs of collecting the outcome variable , measured in units of survey time .",
    "then , the total time costs of surveying one individual to elicit the variables indicated by @xmath30 are @xmath161      a data collection process typically incurs costs due to administrative work and training prior to the start of the actual survey . examples of such tasks are developing the questionnaire and the program for data entry , piloting the questionnaire , developing the manual for administration of the survey , and organizing the training required for the enumerators .    fixed costs , which depend neither on the size of the survey nor on the sample size of survey participants , can simply be subtracted from the budget .",
    "we assume that @xmath49 is already net of such fixed costs .",
    "most administrative and training costs tend to vary with the size of the questionnaire and the number of survey participants .",
    "administrative tasks such as development of the questionnaire , data entry , and training protocols are independent of the number of survey participants , but depend on the size of the questionnaire ( measured by the number of positive entries in @xmath30 ) as smaller questionnaires are less expensive to prepare than larger ones .",
    "we model those costs by @xmath162 where @xmath163 and @xmath63 are scalars to be chosen by the researcher .",
    "we assume @xmath164 , which means that marginal costs are positive but decline with survey size .",
    "training of the enumerators depends on the survey size , because a longer survey requires more training , and on the number of survey participants , because surveying more individuals usually requires more enumerators ( which , in turn , may raise the costs of training ) , especially when there are limits on the duration of the fieldwork .",
    "we therefore specify training costs as @xmath165 where @xmath166 is some function of the number of survey participants .",
    "depends not only on @xmath15 but also on @xmath167 .",
    "we model it this way for simplicity , and because it is a sensible choice in the applications we discuss below . ]",
    "training costs are typically lumpy because , for example , there exists only a limited set of room sizes one can rent for the training , so we model @xmath168 as a step function : @xmath169 here , @xmath170 is a sequence of scalars describing the costs of sample sizes in the ranges defined by the cut - off sequence @xmath171 .",
    "enumerators are often paid by the number of interviews conducted , and the payment increases with the size of the questionnaire .",
    "let @xmath172 denote fixed costs per interview that are independent of the size of the questionnaire and of the number of participants .",
    "these are often due to travel costs and can account for a substantive fraction of the total interview costs .",
    "suppose the variable component of the interview costs is linear so that total interview costs can be written as @xmath173 where @xmath167 should now be interpreted as the average time spent per interview , and @xmath108 is the average price of one unit of survey time .",
    "we employ the specification with  when studying the impact of free day - care on child development in section  [ sec : daycare ] .",
    "because we always collect the outcome variable , we incur the fixed costs @xmath174 and the variable costs @xmath175 even when no covariates are collected .",
    "non - financial costs are difficult to model , but could in principle be added .",
    "they are primarily related to the impact of sample and survey size on data quality .",
    "for example , if we design a survey that takes more than four hours to complete , the quality of the resulting data is likely to be affected by interviewer and interviewee fatigue .",
    "similarly , conducting the training of enumerators becomes more difficult as the survey size grows .",
    "hiring high - quality enumerators may be particularly important in that case , which could result in even higher costs ( although this latter observation could be explicitly considered in our framework ) .",
    "in many experiments , randomization is carried out at a cluster level ( e.g. , school level ) , rather than at an individual level ( e.g. , student level ) . in this case ,",
    "training costs may depend not only on the ultimate sample size @xmath176 , where @xmath177 and @xmath178 denote the number of clusters and the number of participants per cluster , respectively , but on a particular combination ( @xmath179 ) , because the number of required enumerators may be different for different ( @xmath179 ) combinations .",
    "therefore , training costs ( which now also depend on @xmath177 and @xmath178 ) may be modeled as @xmath180 the interaction of cluster and sample size in determining the number of required enumerators and , thus , the quantity @xmath181 , complicates the modeling of this quantity relative to the case without clustering .",
    "let @xmath182 denote the number of required survey enumerators for @xmath177 clusters of size @xmath178 .",
    "as in the case without clustering , we assume that the training costs is lumpy in the number of enumerators used : @xmath183 the number of enumerators required , @xmath182 , may also be lumpy in the number of interviewees per cluster , @xmath178 , because there are bounds to how many interviews each enumerator can carry out . also , the number of enumerators needed for the survey typically increases in the number of clusters in the experiment .",
    "therefore , we model @xmath182 as @xmath184 where @xmath185 denotes the integer part , @xmath186 for some constant @xmath187 ( i.e. , @xmath188 is assumed to be linear in @xmath177 ) , and @xmath189    in addition , while the variable interview costs component continues to depend on the overall sample size @xmath15 as in , the fixed part of the interview costs is determined by the number of clusters @xmath177 rather than by @xmath15 . therefore , the total costs per interview become @xmath190 where @xmath191 is some function of the number of clusters @xmath177 .      in randomized experiments ,",
    "the data collection process often differs across blocks of covariates .",
    "for example , the researcher may want to collect outcomes of psychological tests for the members of the household that is visited . these tests may need to be administered by trained psychologists , whereas administering a questionnaire about background variables such as household income , number of children , or parental education , may not require any particular set of skills or qualifications other than the training provided as part of the data collection project .",
    "partition the covariates into two blocks , a high - cost block ( e.g. , outcomes of psychological tests ) and a low - cost block ( e.g. , standard questionnaire ) .",
    "order the covariates such that the first @xmath192 covariates belong to the low - cost block , and the remaining @xmath193 together with the outcome variable belong to the high - cost block .",
    "let @xmath194 be the total time costs per individual of surveying all low - cost and high - cost covariates , respectively .",
    "then , the total time costs for all variables can be written as @xmath195 .",
    "because we require two types of enumerators , one for the high - cost covariates and one for the low - cost covariates , the financial costs of each interview ( fixed and variable ) may be different for the two blocks of covariates .",
    "denote these by @xmath196 and @xmath197 , respectively .",
    "the fixed costs for the high - cost block are incurred regardless of whether high - cost covariates are selected or not , because we always collect the outcome variable , which here is assumed to belong to this block .",
    "the fixed costs for the low - cost block , however , are incurred only when at least one low - cost covariate is selected ( i.e. , when @xmath198 ) . therefore , the total interview costs for all covariates can be written as @xmath199 the administration and training costs can also be assumed to differ for the two types of enumerators . in that case , @xmath200 we employ specification with ",
    "when , in section  [ sec : school grants ] , we study the impact on student learning of cash grants which are provided to schools .",
    "in this section , we re - examine the experimental design of @xcite , who evaluate the impact of access to free day - care on child development and household resources in rio de janeiro . in their dataset ,",
    "access to care in public day - care centers , most of which are located in slums , is allocated through a lottery , administered to children in the waiting lists for each day - care center .    just before the 2008 school year , children applying for a slot at a public day - care center were put on a waiting list . at this time , children were between the ages of 0 and 3 .",
    "for each center , when the demand for day - care slots in a given age range exceeded the supply , the slots were allocated using a lottery ( for that particular age range ) .",
    "the use of such an allocation mechanism means that we can analyze this intervention as if it was an rct , where the offer of free day - care slots is randomly allocated across potentially eligible recipients .",
    "@xcite compare the outcomes of children and their families who were awarded a day - care slot through the lottery , with the outcomes of those not awarded a slot .",
    "the data for the study were collected mainly during the second half of 2012 , four and a half years after the randomization took place .",
    "most children were between the ages of 5 and 8 .",
    "a survey was conducted , which had two components : a household questionnaire , administered to the mother or guardian of the child ; and a battery of health and child development assessments , administered to children .",
    "each household was visited by a team of two field workers , one for each component of the survey .",
    "the child assessments took a little less than one hour to administer , and included five tests per child , plus the measurement of height and weight .",
    "the household survey took between one and a half and two hours , and included about 190 items , in addition to a long module asking about day - care history , and the administration of a vocabulary test to the main carer of each child .",
    "as we explain below , we use the original sample , with the full set of items collected in the survey , to calibrate the cost function for this example .",
    "however , when solving the survey design problem described in this paper we consider only a subset of items of these data , with the original budget being scaled down properly . this is done for simplicity , so that we can essentially ignore the fact that some variables are missing for part of the sample , either because some items are not applicable to everyone in the sample , or because of item non - response .",
    "we organize the child assessments into three indices : cognitive tests , executive function tests , and anthropometrics ( height and weight ) .",
    "these three indices are the main outcome variables in the analysis .",
    "however , we use only the cognitive tests and anthropometrics indices in our analysis , as we have fewer observations for executive function tests .",
    "we consider only 40 covariates out of the total set of items on the questionnaire .",
    "the variables not included can be arranged into four groups : ( i ) variables that can be seen as final outcomes , such as questions about the development and the behavior of the children in the household ; ( ii ) variables that can be seen as intermediate outcomes , such as labor supply , income , expenditure , and investments in children ; ( iii ) variables for which there is an unusually large number of missing values ; and ( iv ) variables that are either part of the day - care history module , or the vocabulary test for the child s carer ( because these could have been affected by the lottery assigning children to day - care vacancies ) .",
    "we then drop four of the 40 covariates chosen , because their variance is zero in the sample .",
    "the remaining @xmath201 covariates are related to the respondent s age , literacy , educational attainment , household size , safety , burglary at home , day care , neighborhood , characteristics of the respondent s home and its surroundings ( the number of rooms , garbage collection service , water filter , stove , refrigerator , freezer , washer , tv , computer , internet , phone , car , type of roof , public light in the street , pavement , etc . ) .",
    "we drop individuals for whom at least one value in each of these covariates is missing , which leads us to use a subsample with 1,330 individuals from the original experimental sample , which included 1,466 individuals .    [",
    "[ calibration - of - the - cost - function . ] ] calibration of the cost function .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we specify the cost function with components  to model the data collection procedure as implemented in @xcite .",
    "we calibrate the parameters using the actual budgets for training , administrative , and interview costs in the authors implementation .",
    "the contracted total budget of the data collection process was r$665,000 .    for the calibration of the cost function",
    ", we use the originally planned budget of r$665,000 , and the original sample size of 1,466 . as mentioned above , there were @xmath202 variables collected in the household survey , together with a day - care module and a vocabulary test . in total",
    ", this translates into a total of roughly 240 variables .",
    "appendix  b provides a detailed description of all components of the calibrated cost function .",
    "[ [ implementation . ] ] implementation .",
    "+ + + + + + + + + + + + + + +    in implementing the oga , we take each single variable as a possible group ( i.e. , each group consists of a singleton set ) .",
    "we studentized all covariates to have variance one . to compare the oga with alternative approaches , we also consider lasso and post - lasso for the inner optimization problem in step 2 of our procedure .",
    "the lasso solves @xmath203 with a tuning parameter @xmath204 .",
    "the post - lasso procedure runs an ols regression of @xmath84 on the selected covariates ( non - zero entries of @xmath36 ) in .",
    "@xcite , for example , provide a detailed description of the two algorithms .",
    "it is known that lasso yields biased regression coefficient estimates and that post - lasso can mitigate this bias problem . together with the outer optimization over the sample size using the lasso or post - lasso solutions in the inner loop may lead to different selections of covariate - sample size combinations .",
    "this is because post - lasso re - estimates the regression equation which may lead to more precise estimates of @xmath36 and thus result in a different estimate for the mse of the treatment effect estimator .    in both lasso implementations ,",
    "the penalization parameter @xmath187 is chosen so as to satisfy the budget constraint as close to equality as possible .",
    "we start with a large value for @xmath187 , which leads to a large penalty for non - zero entries in @xmath36 , so that few or no covariates are selected and the budget constraint holds .",
    "similarly , we consider a very small value for @xmath187 which leads to the selection of many covariates and violation of the budget .",
    "then , we use a bisection algorithm to find the @xmath187-value in this interval for which the budget is satisfied within some pre - specified tolerance .",
    ".day - care ( outcome : cognitive test ) [ cols=\"<,^,^,^,^,^,^\",options=\"header \" , ]     lcccccc method & @xmath80 & @xmath205 & cost / b & rmse & eqb & relative eqb +   +   + experiment & 1,824 & 142 & 1 & 0.0082721 & $ 27,523.74 & 1",
    "+ oga & 2,618.4 & 1.2 & 0.99823 & 0.0044229 & $ 16,609.53 & 0.603 + lasso & 2,658 & 0 & 0.9991 & 0.004445 & $ 16,621.60 & 0.604 + post - lasso & 2,638.2 & 1.2 & 0.99927 & 0.0044291 & $ 16,651.77 & 0.605 + [ 6pt ]     + [ 3pt ] experiment & 609 & 143 & 1 & 0.0098756 & $ 48,856.20 & 1 + oga & 6,132 & 0 & 0.99885 & 0.0028432 & $ 14,664.85 & 0.300 + lasso & 6,132 & 0 & 0.99885 & 0.0028432 & $ 14,664.85 & 0.300 + post - lasso & 6,132 & 0 & 0.99885 & 0.0028432 & $ 14,664.85 & 0.300 + [ 6pt ]     + experiment & 609 & 143 & 1 & 0.0098756 & $ 48,856.20 & 1 + oga & 6,092.8 & 0.8 & 0.99893 & 0.0027807 & $ 14,571.10 & 0.298 + lasso & 6,000.8 & 5.4 & 0.99905 & 0.002795 & $ 14,651.75 & 0.300 + post - lasso & 6,040.4 & 2.4 & 0.99887 & 0.0027623 & $ 14,532.12 & 0.297 + [ 6pt ]     + experiment & 609 & 143 & 1 & 0.0098756 & $ 48,856.20 & 1 + oga & 2,035.2 & 2.4 & 0.90783 & 0.0041647 & $ 24,918.89 & 0.510 + lasso & 2,494 & 1 & 0.99623 & 0.0046439 & $ 25,522.72 & 0.522 + post - lasso & 2,494 & 1 & 0.99623 & 0.0034893 & $ 23,651.72 & 0.484 +",
    "in this section , we consider an extension to the case of multivariate outcomes . if data on a particular regressor is collected , then the regressor is automatically available for regressions involving any of the outcomes .",
    "therefore , it is natural to select one common set of regressors for all outcomes .",
    "hence , our regression problem corresponds to the special case of seemingly unrelated regressions ( sur ) such that the vector of regressors is identical for each equation . in this case",
    ", it is well known that the ols and gls estimators are algebraically identical . in other words , there is no loss of efficiency in using the single - equation ols estimator even if regression errors are correlated .",
    "suppose there are @xmath206 outcome variables of interest , say @xmath207 .",
    "then a multivariate analog of can be written as @xmath208 in other words , the stacked version of the ols problem is equivalent to regressing @xmath209 on @xmath210 conditional on the budget constraint , where @xmath211 , @xmath212 is the @xmath206-dimensional identity matrix , and @xmath213 is @xmath214 dimensional matrix whose @xmath88th row is @xmath215 .",
    "therefore , the oga applies to this case as well with minor modifications .",
    "first , we need to redefine the outcome vector and the design matrix with the stacked @xmath216 and the enlarged design matrix @xmath210 .",
    "suppose that a variable selection problem is on individual components of @xmath217 .",
    "then note that because of the nature of the stacked regressions , we need to apply a group oga with each group consisting of @xmath206 columns of @xmath218_k$ ] , where @xmath219 @xmath220 for each @xmath221 ."
  ],
  "abstract_text": [
    "<S> in a randomized control trial , the precision of an average treatment effect estimator and the power of the corresponding t - test can be improved either by collecting data on additional individuals , or by collecting additional covariates that predict the outcome variable . </S>",
    "<S> we propose the use of pre - experimental data such as a census , or a household survey , to inform the choice of both the sample size and the covariates to be collected . </S>",
    "<S> our procedure seeks to minimize the resulting average treatment effect estimator s mean squared error or the corresponding t - test s power , subject to the researcher s budget constraint . </S>",
    "<S> we rely on a modification of an orthogonal greedy algorithm that is conceptually simple and easy to implement in the presence of a large number of potential covariates , and does not require any tuning parameters . in two empirical applications , we show that our procedure can lead to substantial gains of up to 58% , measured either in terms of reductions in data collection costs or in terms of improvements in the precision of the treatment effect estimator .    </S>",
    "<S> * jel codes * : c55 , c81 .    </S>",
    "<S> * key words * : randomized control trials , big data , data collection , optimal survey design , orthogonal greedy algorithm , survey costs . </S>"
  ]
}