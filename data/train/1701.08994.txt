{
  "article_text": [
    "the increased complexity of models posed in fields such as biology , ecology , and epidemiology ( to name a few ) has led many practitioners to adopt bayesian methodologies .",
    "this trend is not necessarily motivated by philosophical underpinnings , rather no alternative machinery capable of fitting posed models exists .",
    "thus , in a way , bayesian methods have become more or less mainstream , and this has led to an increased need for model assessment metrics that are quickly calculated and easily interpreted .",
    "a welcome metric to practitioners would be one that is able to guide decisions in the model building process by providing a quick assessment of the level of agreement or influence that each component of bayes theorem has on inference and predictions .    assessing the influence that prior distributions and/or likelihoods have on posterior inference has been a topic of research for some time .",
    "one commonly used ad - hoc method suggests simply fitting a bayes model using a few competing priors , then visually ( or numerically ) assessing changes in the posterior as a whole or using some pre - specified posterior summary .",
    "more rigorous approaches have also been developed .",
    "@xcite developed a framework to assess sensitivity of posterior inference to sampling distribution ( likelihood ) and the priors .",
    "@xcite introduced the concept of bayesian robustness which includes perturbation models ( see also @xcite ) .",
    "more recently , @xcite have compared information available in two competing priors . related to this work ,",
    "@xcite advocates the use of so - called weakly informative priors that purposely incorporate less information than available as a means of regularizing .",
    "work has also been dedicated to the so - called prior - data conflict which aims to assess the level of agreement between prior and likelihood ( see @xcite , @xcite , @xcite ) .",
    "such conflict can be of interest in a wealth of situations , such as for assessing how much an expert agrees with the data , or for evaluating how much prior and likelihood information are at odds at the node level in a hierarchical model ( see * ? ? ?",
    "* and references therein ) . regarding sensitivity of the posterior distribution to prior specifications , @xcite provide a fairly accessible overview",
    "we argue that a geometric representation of the prior , likelihood , and posterior distribution encourages understanding of their interplay .",
    "considering bayes methodologies from a geometric perspective is not new , but none of the existing geometric perspectives has been designed with the goal of providing a summary on the agreement or impact that each component of bayes theorem has on inference and predictions .",
    "@xcite used a geometric perspective to build intuition behind each component of bayes theorem .",
    "@xcite defined a manifold on which a bayesian perturbation analysis can be carried out by perturbing data , prior and likelihood simultaneously , @xcite used a geometric approach to draw conditional distributions in arbitrary coordinate systems , and @xcite argued that conjugate priors of posterior distributions belong to the same geometry giving an appealing interpretation of hyperparameters .",
    "the novel contribution we aim to make here is the development of easily computed metrics that provide an informative preliminary ` snap - shot ' regarding comparisons between prior and likelihood ( to assess the level of agreement between prior and data ) , prior and posterior ( to determine the influence that prior has on inference ) , and prior versus prior ( to compare ` informativeness'i.e .",
    ", a density s peakedness  and / or congruence of two competing priors ) . to this end",
    ", we treat each component of bayes theorem as an element of a geometry formally constructed using concepts from hilbert spaces and tools from abstract geometry . because of this",
    ", it is possible to calculate norms , inner products , and angles between vectors . not only do each of these numeric summaries have intuitively appealing individual interpretations , but they may also be combined to construct a unitless measure of compatibility , which can be used to assess how much the prior agrees with the likelihood , to gauge the sensitivity of the posterior to the prior , and to quantify the coherency of the opinions of two experts .",
    "further , estimating our measures of ` similarity ' is straightforward and can actually be carried out within an mcmc algorithm as is typically employed in a bayesian analysis .    to facilitate the illustration of ideas , concepts , and methods we reference the following simple example ( which is found in @xcite ) through the first few sections of this article .",
    "+   +    on - the - job drug usage toy example + suppose interest lies in estimating the proportion @xmath1 $ ] of us transportation industry workers that use drugs on the job .",
    "suppose @xmath2 workers were selected and tested with the 2nd and 7th testing positive .",
    "let @xmath3 with @xmath4 denoting that the @xmath5th worker tested positive and @xmath6 otherwise . a natural data model for these data would be @xmath7 .",
    "a prior distribution that is typically used in this situation is @xmath8 for @xmath9 and produces the following posterior distribution @xmath10 with @xmath11 and @xmath12 .",
    "+ some natural questions one may ask and that we aim to quantify are : how compatible is the likelihood with this prior choice ?",
    "how similar are the posterior and prior distributions ?",
    "how does the choice of @xmath13 compare to other possible prior distributions ?",
    "we provide a unified treatment to answer the questions above .",
    "while the drug usage example provides a recurring backdrop that we consistently call upon , additional examples are used throughout the paper to illustrate our methods .",
    "the rest of the article is organized as follows .",
    "section 2 introduces the basic geometric framework in which we work and provides definitions and interpretations of norms and inner - products .",
    "section 3 generalizes how bayes theorem employs a likelihood to recast a prior density to obtain a posterior density .",
    "section 4 contains computational details .",
    "section 5 provides a regression example illustrating utility of our metric .",
    "section 6 conveys some concluding remarks .",
    "proofs are given in the appendix .",
    "suppose the inference of interest is over a parameter @xmath14 which takes values on @xmath15 .",
    "we consider the space of square integrable functions @xmath16 , and use the geometry of the hilbert space @xmath17 , with inner - product @xmath18 the fact that @xmath19 is an hilbert space is often known in mathematical parlance as the riesz  fischer theorem ; for a proof see @xcite .",
    "borrowing geometric terminology from linear spaces , we refer to the elements of @xmath16 as vectors , and assess their ` magnitudes ' through the use of the norm induced by the inner product in , i.e. , @xmath20 .    the starting point for constructing our geometry",
    "is the observation that bayes theorem can be written using the inner - product in as follows @xmath21 where @xmath22 denotes the likelihood , @xmath23 is a prior density , @xmath24 is the posterior density and @xmath25 is the so - called marginal likelihood or integrated likelihood .",
    "the inner product in naturally leads to considering @xmath26 and @xmath27 that are in @xmath16 , which is compatible with a wealth of parametric models and proper priors . by considering @xmath28 , @xmath26 , and @xmath27 as vectors with different magnitudes and directions ,",
    "bayes theorem simply indicates how one might recast the prior vector so to obtain the posterior vector .",
    "the likelihood vector is used to enlarge / reduce the magnitude and suitably tilt the direction of the prior vector in a sense that will be made precise below .",
    "the marginal likelihood @xmath29 is simply the inner product between the likelihood and the prior , and hence can be understood as a natural measure of agreement between the prior and the likelihood . to make this more concrete ,",
    "define the _ angle measure _ between the prior and the likelihood as @xmath30 since @xmath26 and @xmath27 are nonnegative , the angle between the prior and the likelihood can only be acute or right , i.e. , @xmath31 $ ] .",
    "the closer @xmath32 is to @xmath33 , the greater the agreement between the prior and the likelihood .",
    "conversely , the closer @xmath32 is to @xmath34 , the greater the disagreement between prior and likelihood . in the pathological case",
    "where @xmath35 ( which requires the prior and the likelihood to have all of their mass on disjoint sets ) , we say that the prior is orthogonal to the likelihood .",
    "bayes theorem is incompatible with a prior being completely orthogonal to the likelihood as @xmath36 indicates that @xmath37 , thus leading to a division by zero in .",
    "similar to the correlation coefficient for random variables in @xmath38with @xmath39 denoting the borel sigma - algebra over the sample space @xmath40 , our target object of interest is given by a standardized inner product @xmath41 the quantity @xmath42 quantifies how much an expert s opinion agrees with the data , thus providing a natural measure of prior - data compatibility . as it will become apparent , @xmath42 can be used as a simple alternative to the procedure developed by @xcite to assess prior - data conflict ( see also @xcite ) , but with some differences that will be highlighted in the discussion below about figure  [ kappa ] .",
    "although not with the express interest of making comparisons between two functions , it should be noted that the idea of angles between functions has appeared in the context of functional data analysis @xcite .    before exploring more fully by providing interpretations and properties",
    "we concretely define how the term ` geometry ' will be used throughout the paper .",
    "the following definition of abstract geometry can be found in @xcite .",
    "an abstract geometry @xmath43 consists of a pair @xmath44 , where the elements of set @xmath45 are designed as points , and the elements of the collection @xmath46 are designed as lines , such that :    1 .   for every two points @xmath47 , there is a line @xmath48 .",
    "every line has at least two points .",
    "our abstract geometry of interest is @xmath49 , where @xmath50 and the set of all lines is @xmath51 hence , in our setting points can be , for example , prior densities , posterior densities , or likelihoods , as long as they are in @xmath16 .",
    "lines are elements of @xmath46 , as defined in , so that for example if @xmath52 and @xmath53 are densities , line segments in our geometry consist of all possible mixture distributions which can be obtained from @xmath52 and @xmath53 , i.e. , @xmath54\\}.\\ ] ] vectors in @xmath49 are defined through the difference of elements in @xmath50 . for example , let @xmath55 and let @xmath56 .",
    "then @xmath57 , and hence @xmath52 can be regarded both as a point and as a vector .",
    "if @xmath58 are vectors then we say that @xmath52 and @xmath53 are collinear if there exists @xmath59 , such that @xmath60 .",
    "put differently , we say @xmath52 and @xmath53 are collinear if @xmath61 , for all @xmath62 .",
    "for any two points in the geometry under consideration , we define their compatibility as a standardized inner product ( with being a particular case ) .",
    "[ comp.def ] the compatibility between points in the geometry under consideration is the mapping @xmath63 $ ] defined as @xmath64    the concept of compatibility in definition  [ comp.def ] is based on the same construction principles as the pearson correlation coefficient , which would be based however on the inner product @xmath65 instead of the inner product in . for a few selected @xmath26 s ,",
    "@xmath66 can be used to gauge the sensitivity of the posterior to the prior specification .",
    "also , @xmath67 might quantify the compatibility of different priors , and hence it can be used to assess the coherency of the opinions of two experts . as an illustration",
    "consider the following simple example .",
    "[ simple.example]consider the following densities @xmath68 , @xmath69 , @xmath70 , and @xmath71 .",
    "note that @xmath72 , @xmath73 , and ; further , @xmath74 , thus implying that @xmath75 . note further that @xmath76 , and hence @xmath77 .    as can be observed in example  [ simple.example ] , @xmath78 is a natural measure of distinctiveness of two densities .",
    "in addition , example  [ simple.example ] shows us how different distributions can be associated to the same norm and angle .",
    "hence , as expected , any cartesian representation @xmath79 , will only allow us to represent some features of the corresponding distributions , but will not allow us to identify the distributions themselves .     +    to begin building intuition regarding the values produced by @xmath80 , we provide figure  [ kappa ] . in the figure , @xmath27 is set to @xmath81 while @xmath82 varies according to @xmath83 and @xmath84 .",
    "the left plot corresponds to fixing @xmath85 and varying @xmath83 while in the right plot @xmath86 is fixed and @xmath84 varies .",
    "notice that in plot ( i ) @xmath87 corresponds to distributions whose means are approximately 3 standard deviations apart while a @xmath88 corresponds to distributions whose means are approximately 0.65 standard deviations apart . connecting specific values of @xmath89 to specific standard deviation distances between means seems like a natural way to quickly get a rough idea of relative differences between two distributions . in plot ( ii ) it appears that if both distributions are centered at the same value , then one distribution must be very disperse relative to the other to produce @xmath89 values that are small ( e.g. , @xmath90 ) .",
    "this makes sense as there always exists some mass intersection between the two distributions considered . in this scenario",
    " an especially diffuse prior@xcite would conclude that no prior - data conflict exists .",
    "their method relies on determining how extreme the data are relative to the induced prior - predictive distribution , and an increasingly disperse prior would make any given data set seem increasingly less extreme .",
    "we consider @xmath42 more a measure of prior - data _ compatibility _ than prior - data _ conflict _ inasmuch as it can be small not only when there are differences in the locations of the prior and likelihood but also when there are differences in the peakedness of the distributions .",
    "some further comments regarding our geometry are in order :    * two different densities @xmath91 and @xmath92 can not be collinear : if @xmath93 , then @xmath94 , otherwise @xmath95 . * a density can be collinear to a likelihood : if the prior is uniform @xmath96 , and hence the posterior is collinear to the likelihood , i.e. , in such case the posterior simply consists of a renormalization of the likelihood .",
    "* our geometry is compatible with having two likelihoods be collinear , and thus it can be used to rethink the strong likelihood principle @xcite .",
    "let @xmath27 and @xmath97 be the likelihoods based on observing @xmath98 and @xmath99 , respectively .",
    "the strong likelihood principle states that if @xmath100 , then the _ same _ inference should be drawn from both samples . according to our geometry , this would mean that likelihoods with the same direction should yield the same inference .      as @xmath101 is comprised of function norms , we dedicate some exposition to how one might interpret these quantities .",
    "we start by noting that in some cases the norm of a density is linked to the precision parameter , as can be seen in the following example .",
    "[ unif.normal]let @xmath102 and let @xmath103 denote its corresponding density . then , it holds that @xmath104 , where the precision of @xmath105 is @xmath106 .",
    "next , consider a normal model @xmath107 with known precision @xmath108 and let @xmath109 denote its corresponding density .",
    "it can be shown that @xmath110 which is a function of @xmath108 .",
    "the following proposition further explores the connection between norms and precision suggested by example  [ unif.normal ]    [ norminterpretation ] let @xmath111 with @xmath112 where @xmath113 denotes the lebesgue measure .",
    "consider @xmath26 : @xmath114 a probability density with @xmath115 and let @xmath116 denote a uniform density on @xmath117 , then @xmath118    since @xmath119 is constant , @xmath120 increases as @xmath26 s mass becomes more concentrated ( or less uniform ) .",
    "thus , as can be seen from , @xmath121 is a measure of how much @xmath26 differs from a uniform distribution over @xmath117 .",
    "this interpretation can not be applied to @xmath117 s that are not finite measurable as there is no corresponding proper uniform distribution .",
    "nonetheless , the notion that the norm of a density is a measure of its peakedness may be applied whether or not @xmath117 is finite measurable .",
    "therefore , @xmath122 can be seen as very simple alternative to that proposed in @xcite to compare the ` informativeness ' of two competing priors with @xmath123 indicating that @xmath91 is less informative .",
    "further reinforcing the idea that the norm is related to the peakedness of a distribution , there is an interesting connection between @xmath124 and the ( differential ) entropy ( denoted by @xmath125 ) which is described in the following theorem .",
    "[ expansion ] suppose @xmath115 is a continuous density on a compact @xmath126 , and that @xmath23 is differentiable on @xmath127 .",
    "let @xmath128 .",
    "then , it holds that @xmath129 for some @xmath130 .    the expansion in hints that the norm of a density and the entropy should be negatively related , and hence as the norm of a density increases , its mass becomes more concentrated . in terms of priors",
    ", this suggests that priors with a large norm should be more ` peaked ' relative to priors with a smaller norm .",
    "therefore , the magnitude of a prior appears to be linked to its peakedness ( as is demonstrated in and in example  [ unif.normal ] ) .",
    "while this might also be viewed as ` informativeness , ' the @xmath13 density has a higher norm if @xmath131 than if @xmath132 , possibly placing this interpretation at odds with the notion that @xmath133 and @xmath134 represent ` prior successes ' and ` prior failures ' in the beta - binomial setting .",
    "as can be seen from , the connection between entropy and @xmath124 is an approximation at best .",
    "just as a first order taylor expansion provides a poor polynomial approximation for points that are far from the point under which the expansion is made , the expansion in will provide a poor entropy approximation when @xmath26 is not similar to a standard uniform - like distribution @xmath135 .",
    "however , since @xmath136 , the approximation is exact for a standard uniform - like distribution .",
    "we end this discussion by noting that integrals related to @xmath137 also appear in physical models on @xmath138-spaces and they are usually interpreted as the total energy of a physical system @xcite .",
    "now , to illustrate the information that @xmath122 and @xmath89 provide , we consider the example described in the introduction .",
    "+    [ ex2]from the example in the introduction we have @xmath139 with @xmath140 and @xmath141 .",
    "the norm of the prior , posterior , and likelihood are respectively given by @xmath142 with @xmath143 , and @xmath144^{1/2},\\ ] ] where @xmath145 .",
    "figure  [ norms ] @xmath146 plots @xmath147 and figure  [ norms ] @xmath148 plots @xmath149 as functions of @xmath133 and @xmath134 .",
    "we highlight the prior values @xmath150 which were employed by @xcite . because prior densities with large norms will be more peaked relative to priors with small norms",
    ", @xmath151 is more peaked than @xmath152 ( uniform prior ) indicating that @xmath153 is more ` informative ' than @xmath154 .",
    "the norm of the posterior for these same pairs is @xmath155 and @xmath156 , meaning that the posteriors will have mass more concentrated than the corresponding priors .",
    "in fact , the lines found in figure  [ norms ] @xmath148 represent boundary lines such that all @xmath157 pairs that fall outside of the boundary produce @xmath158 which indicates that the prior is more peaked than the posterior ( typically an undesirable result ) .",
    "if we used an extremely peaked prior , say @xmath159 , then we would get @xmath160 and @xmath161 indicating that the peakedness of the prior and posterior densities is essentially the same .",
    "considering @xmath42 , it follows that @xmath162    figure  [ kappaprpokappprpr ] @xmath146 plots values of @xmath89 as a function of prior parameters @xmath133 and @xmath134 with @xmath163 being highlighted indicating a great deal of agreement with the likelihood . in this example a lack of prior - data compatibility would occur ( e.g. , @xmath164 ) for priors that are very peaked at @xmath165 or for priors that place substantial mass at @xmath166 .",
    "the values of the hyperparameters @xmath157 which , according to @xmath42 , are more compatible with the data ( i.e. , those that maximize @xmath89 ) are given by @xmath167 and are highlighted with a star ( * * ) in figure  [ kappaprpokappprpr ] @xmath146 . in section  [ maxcompatible ] we provide some connections between this prior and maximum likelihood estimators .",
    "+      as mentioned , we are not restricted to use @xmath89 only to compare @xmath26 and @xmath27 .",
    "in fact , angles between different densities , and between likelihoods and densities or even between two likelihoods are available .",
    "we explore these options further using the example provided in the introduction .",
    "[ exkpip ] extending example  [ ex2 ] and equation   we calculate @xmath168 and for @xmath169 and @xmath170 , @xmath171 to visualize how the hyperparameters influence @xmath66 and @xmath172 we provide figures  [ kappaprpokappprpr ] @xmath148 and @xmath173 .",
    "figure  [ kappaprpokappprpr ] @xmath148 again highlights the prior used in with @xmath174 ; see solid dot ( @xmath175 ) .",
    "this value of @xmath66 implies that both prior and posterior are concentrated on essentially the same subset of @xmath176 $ ] , indicating a large amount of agreement between them .",
    "disagreement between prior and posterior takes place with priors concentrated on high probabilities of @xmath177 being greater than 0.8 . in figure  [ kappaprpokappprpr ] @xmath173",
    ", @xmath172 is largest when @xmath92 is close to @xmath178 ( the distribution of @xmath91 ) and gradually drops off as @xmath92 becomes more peaked and/or less symmetric .    in the next example",
    ", we utilize another small data set and demonstrate the application of @xmath89 to a two - parameter model .",
    "@xcite used a data set of nine midge wing lengths ( originally reported by @xcite ) .",
    "the nine measurements were assumed to be conditionally iid with a normal likelihood .",
    "the prior distribution for @xmath179 and @xmath180 was decomposed as a normal - inverse gamma distribution , i.e. , @xmath181 and @xmath182 ; we refer to this conjugate prior distribution as @xmath183 .",
    "as noted by @xcite , this parametrization affords appealing interpretations for the hyperparameters : @xmath184 and @xmath185 as the mean and sample size of ` prior observations'for inference on @xmath186and @xmath187 and @xmath188 as the sample size and variance of ` prior observations'for inference on @xmath180 . in comparing two normal  inverse gamma distributions , @xmath189 and @xmath190 , @xmath172",
    "may be expressed using the normal - inverse gamma density with three different sets of hyperparameters , each evaluated at @xmath191 , i.e. , @xmath192 in this form , @xmath193 represents the @xmath194 density , @xmath195 the @xmath196 density , and @xmath197 the @xmath198 density .",
    "+    [ normig ]    in particular , may be used not only for assessing agreement between two normal - inverse gamma priors , but also between the prior and the posterior distribution .",
    "the hyperparameters for the posterior relate to the prior specification as follows ( see also * ? ? ?",
    "* ) : @xmath199 for the midge data application , hoff chose as hyperparameters @xmath200 , @xmath201 , @xmath202 , and @xmath203 , while @xmath204 and @xmath205 , producing @xmath206 .",
    "the agreement between the prior and posterior is not particularly strong .",
    "figure  [ nnigpripos ] @xmath146 displays the prior - posterior compatibility , @xmath66 , for these data as a function of @xmath184 and @xmath188 while fixing @xmath202 and @xmath201 . to evaluate how @xmath66 is affected by @xmath187 and @xmath185 , the sample sizes of ` prior observations , ' the analogous plot is displayed as figure  [ nnigpripos ] @xmath148 when these values are fixed at @xmath207 and @xmath208 ; these alternative values for @xmath187 and @xmath185 are those which allow the compatibility between the prior and likelihood to be maximized .",
    "it is apparent from these plots that a somewhat larger value of @xmath188 would have increased @xmath209 substantially , and a simultaneous increase of @xmath187 and @xmath185 would further propel this increase .      in example",
    "[ ex2 ] we briefly alluded to a connection between priors maximizing prior - likelihood compatibility @xmath80to be termed as max - compatible priors  and maximum likelihood ( ml ) estimators , on which we now elaborate . in the following we use the notation @xmath210 to denote a prior on @xmath62 , and where @xmath211 are hyperparameters .",
    "( think of the beta - binomial model , where @xmath212 , and @xmath213 . ) below , let @xmath214 and @xmath215 .",
    "let @xmath216 , and let @xmath217 be a family of priors for @xmath218 .",
    "if there exists @xmath219 , such that @xmath220 , the prior @xmath221 is said to be _ max - compatible _ , and @xmath222 is said to be a max - compatible hyperparameter .",
    "[ max - compatibility ]    the max - compatible hyperparameter , @xmath222 , is by definition a random vector , and thus a max - compatible prior density is a random function .",
    "geometrically , a prior is max - compatible iff it is collinear to the likelihood in the sense that @xmath220 iff @xmath223 , for all @xmath62 .",
    "the following example suggests there could be a connection between the ml estimator of @xmath218 and the max - compatibility parameter @xmath222 .",
    "let @xmath224 , and suppose @xmath225 .",
    "let @xmath226 where @xmath227 it can be shown that the max - compatible prior is @xmath228 , where @xmath229 , and @xmath230 , so that @xmath231 [ beta-binomial.ex ]    a natural question is whether there always exists a function @xmath232 , linking the max - compatible parameter with the ml estimator , as in ? the following theorem addresses this question .",
    "[ max - thm ] let @xmath233 , and let @xmath217 be a family of priors for @xmath218 .",
    "suppose there exists a max - compatible prior @xmath221 , which we assume to be unimodal .",
    "then , @xmath234    theorem  [ max - thm ] states that the mode of the max - compatible prior coincides with the ml estimator .",
    "note that in example  [ beta-binomial.ex ] , @xmath235 is indeed the mode of a beta prior .",
    "the next examples illustrate further this result .",
    "[ exp - gamma1 ] in this case the max - compatible prior is given by @xmath236 , where @xmath237 .",
    "the connection with the ml estimator is the following @xmath238    in this case the max - compatible prior is given by @xmath236 , where @xmath239 .",
    "the max - compatible hyperparameter in this case is different from the one in example  [ exp - gamma1 ] , but still a similar connection holds @xmath240    the preceding examples and theorem suggest a possible connection between max - compatible priors and empirical bayes priors .",
    "it is true that both are data - driven prior distributions but save a few special cases the max - compatible prior does not coincide with an empirical bayes prior .",
    "theorem 2 highlights a situation when they do .",
    "if an ml estimator is employed to provide prior parameter values in an empirical bayes prior and the prior distribution is symmetric so that the mean and median are equal , then the max - compatible prior and empirical bayes prior will be the same ( e.g. , a @xmath241 prior with @xmath242 and @xmath180 known ) .",
    "therefore , the max - compatible prior can be thought of as an alternative data - based prior construction .",
    "by rewriting bayes theorem as in , it is natural to pose the question : do other inner products exist that can be used to mimic the geometric principles described in section  [ geometry ] and yet produce inference different but related to the bayesian paradigm ? as we shall see next , the answer to this question is positive , and we will refer to such approaches as posterior schemes .",
    "below , let @xmath243 .",
    "[ bayes.scheme ] let @xmath244 be a mapping .",
    "let @xmath245 be an inner product space , such that @xmath246 .",
    "a posterior scheme is a mapping @xmath247 , defined as @xmath248    the simplest posterior scheme is defined through bayes theorem ; by setting @xmath249 , and thus @xmath250 .",
    "thus , we obtain @xmath251 , and hence the posterior scheme corresponding to bayes theorem ( i.e. , corresponding to the standard @xmath138 inner product ) is simply the posterior density .    note",
    "that a posterior scheme provides a well defined probability distribution for @xmath252 as it always integrates to one when we integrate over @xmath117 , i.e. , @xmath253 it is clear from definition  [ bayes.scheme ] that to construct posterior schemes , all that one needs is to plug into inner products that can be expressed as integrals .",
    "in addition , the construction of bayes - type estimators , @xmath254 based on @xmath255 , can be performed by minimizing the expected posterior scheme loss @xmath256 where @xmath257 , is a loss function and @xmath258 is the space of all decision rules . just as non - euclidean distances find their application in geometry , we argue that posterior schemes other than the posterior density could provide other sensible ways to update the prior with data .",
    "one possible example is discussed next .    [",
    "weighted ] an alternative posterior scheme to bayes theorem can be constructed by using the weighted inner product @xmath259 , where @xmath260 is a weighted function @xcite , and it is given by @xmath261 a particularly appealing interpretation for this scheme is as a model for combining expert opinion in prior elicitation .",
    "if @xmath262 represents a prior obtained from a second expert , then the posterior scheme in provides a natural model for combining two independent priors @xmath91 and @xmath92 , i.e. , @xmath263 which is similar to bayes theorem , but where the posterior based on @xmath91 ( @xmath264 ) replaces the likelihood . to aid in the interpretation of ,",
    "suppose that priors arrive sequentially , with @xmath91 arriving firstly and @xmath92 secondly .",
    "thus in the second stage of the learning process associated with this posterior scheme it can be see from how the state of knowledge is updated . note in addition that @xmath265 where @xmath266 is the posterior based on @xmath92 , and hence the order of the learning based on this scheme is irrelevant .",
    "an alternative way of interpreting this scheme could be through bayes theorem itself , which follows from observing that    @xmath267    where @xmath268 . in terms of sampling , the connection in shows that updating the posterior scheme is equivalent to updating the posterior based on @xmath269 .",
    "more generally , if @xmath270 independent sources of prior information , @xmath271 , are available the posterior scheme is given as @xmath272 where @xmath273 .",
    "for example , if @xmath270 researchers each have independent @xmath274 priors for @xmath177 in the bernoulli likelihood model , the posterior scheme in is equivalent to the standard posterior scheme that utilizes a @xmath275 prior distribution .",
    "if @xmath276 and @xmath277 are interpreted as the @xmath278th researcher s equivalent prior observations of successes and failures , this prior reflects the aggregation of the disparate prior data .",
    "example  [ weighted ] suggests that posterior schemes can be regarded as an alternative way to redirect the prior vector using data : the scheme in obeys the strong likelihood principle , is a valid probability model ( in the sense that it integrates to one ) , and obeys similar geometric principles to the ones discussed in section  [ geometry ] .",
    "this leads us to the following concept .    a bayes geometry @xmath279 consists of an abstract geometry equipped with a posterior scheme .",
    "in addition , we say that @xmath280 is the canonical bayes geometry if the posterior scheme @xmath281 is simply a posterior density , i.e. @xmath282 .",
    "as mentioned above , to generate new posterior schemes , one simply needs to plug into an inner product that can be written as an integral .",
    "thus , beyond the canonical bayes geometry and the bayes geometry based on the weighted inner product , there are multitudinous possibilities for how one might use posterior schemes to recast the prior vector ( update information ) using the likelihood vector .",
    "in many situations closed form estimators of @xmath89 and @xmath122 are not available .",
    "this leads to considering algorithmic techniques to obtain estimates .",
    "as most bayes methods resort to using mcmc methods it would be appealing to express @xmath283 and @xmath122 as functions of posterior expectations and employ mcmc iterates to estimate them . for example",
    ", @xmath209 can be expressed as @xmath284^{-1/2},\\ ] ] where @xmath285 is the expected value with respect to the posterior density .",
    "a natural monte carlo estimator would then be @xmath286^{-1/2},\\ ] ] where @xmath287 denotes the @xmath134th mcmc iterate of @xmath288 .",
    "consistency of such an estimator follows trivially by the ergodic theorem and the continuous mapping theorem , but there is an important issue regarding its stability .",
    "unfortunately , includes an expectation that contains @xmath289 in the denominator and therefore inherits the undesirable properties of the so - called harmonic mean estimator @xcite .",
    "it has been shown that even for simple models this estimator may have infinite variance ( @xcite ) , and has been harshly criticized for , among other things , converging extremely slowly . indeed , as argued by @xcite :    _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ `` the reduction of monte carlo sampling error by a factor of two requires increasing the monte carlo sample size by a factor of @xmath290 , or in excess of @xmath291 when @xmath292 , rendering [ the harmonic mean estimator ] entirely untenable . ''",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    making things a bit more difficult is the fact that contains a _ root _ of @xmath293 , which renders corrections like those found in @xcite and @xcite unsuitable .",
    "an alternate strategy is to avoid writing @xmath209 as a function of harmonic mean estimators and instead express it as a function of posterior and prior expectations .",
    "for example , consider @xmath294^{-1/2},\\ ] ] where @xmath295 . now",
    "the monte carlo estimator is @xmath296^{-1/2},\\ ] ] where @xmath297 denotes the @xmath134th draw of @xmath252 from @xmath298 , which can also be sampled within the mcmc algorithm .",
    "representations and are somewhat less elegant than and as they require draws from the posterior and the prior , but they behave much better in practice . to see this , figure  [ khkt ] contains running estimates of @xmath209 using and for example [ ex2 ] with three prior parameter specifications , namely : @xmath299 , @xmath300 , and @xmath301 ; the true @xmath66 for each prior specification is also provided .",
    "it is fairly clear that @xmath302 displays slow convergence and large variance , while @xmath303 converges quickly .",
    "the next proposition contains prior and posterior mean - based representations of geometric quantities for the canonical bayes geometry , that can be readily used for constructing monte carlo estimators .",
    "notice that metrics that include the prior only ( e.g. , @xmath124 ) are expressed entirely as functions of prior expectations .",
    "this allows comparing competing prior densities prior to any model fitting .",
    "( we briefly note that there is an enormous frequentist literature on the estimation of the integral @xmath304 , especially due to its appearance in some variance  covariance structures ; see for instance @xcite , and the references therein . )",
    "[ postmean ] let @xmath280 be the canonical bayes geometry and let @xmath89 denote compatibility in this geometry .",
    "let @xmath305 and @xmath306 be the posterior and prior means .",
    "the following equalities hold : @xmath307\\nonumber          \\kappa_{\\pi_1,\\pi_2 } & = e_{\\pi_1}\\ , \\pi_2(\\bm{\\theta } )          \\bigg[e_{\\pi_1}\\ , \\pi_1(\\bm{\\theta } ) \\ , e_{\\pi_2}\\ ,          \\pi_2(\\bm{\\theta } ) \\bigg]^{-1/2 } , \\quad          \\kappa_{\\pi,\\ell }   = e_{\\pi } \\ , \\ell({\\boldsymbol{\\theta } } ) \\bigg[e_{\\pi } \\ , \\pi(\\bm{\\theta } ) \\ , e_{\\pi } \\ , \\ell({\\boldsymbol{\\theta } } ) \\ , e_p\\left\\{\\frac{\\ell({\\boldsymbol{\\theta}})}{\\pi(\\bm{\\theta})}\\right\\}\\bigg]^{-1/2 } , \\\\          \\kappa_{\\pi , p } & = e_{p}\\ , \\pi(\\bm{\\theta } )          \\bigg[\\frac{e_{\\pi } \\ , \\pi(\\bm{\\theta})}{e_{\\pi}\\ ,            \\ell({\\boldsymbol{\\theta}})}e_p\\left\\{\\ell({\\boldsymbol{\\theta}})\\pi(\\bm{\\theta})\\right\\}\\bigg]^{-1/2 } ,          \\quad \\kappa_{\\ell , p } = e_{p } \\ , \\ell({\\boldsymbol{\\theta } } ) \\ ,",
    "\\bigg[e_{p}\\left\\{\\frac{\\ell({\\boldsymbol{\\theta}})}{\\pi(\\bm{\\theta})}\\right\\}e_p\\left\\{\\ell({\\boldsymbol{\\theta}})\\pi(\\bm{\\theta})\\right\\}\\bigg]^{-1/2 } ,          \\\\[0.2cm]\\nonumber \\kappa_{\\ell_1,\\ell_2 } & = e_{\\pi } \\ ,          \\ell_2(\\bm{\\theta } ) \\ ,          e_{p_2}\\left\\{\\frac{\\ell_1(\\bm{\\theta})}{\\pi(\\bm{\\theta})}\\right\\}\\bigg[e_{\\pi}\\{\\ell_1(\\bm{\\theta})\\ }          e_{p_1}\\left\\{\\frac{\\ell_1(\\bm{\\theta})}{\\pi(\\bm{\\theta})}\\right\\}e_{\\pi }          \\ , \\ell_2(\\bm{\\theta } ) \\",
    ",         e_{p_2}\\left\\{\\frac{\\ell_2(\\bm{\\theta})}{\\pi(\\bm{\\theta})}\\right\\}\\bigg]^{-1/2}.        \\end{split}\\ ] ]    in the next section we provide an example that requires the use of proposition  [ postmean ] to estimate @xmath89 and @xmath122 .",
    "the linear regression model is ubiquitous in applied statistics . in vector form ,",
    "the model is commonly written as @xmath308 where @xmath309 , @xmath310 is a @xmath311 design matrix , @xmath312 is a @xmath28-vector of regression coefficients , and @xmath180 is an unknown idiosyncratic variance parameter . we consider two competing prior distributions for @xmath312 , gaussian and laplace .",
    "these two priors are often employed as shrinkage priors that perform some type of regularization .",
    "connections between the regularization via ridge and lasso penalization and that from using gaussian and laplace prior distributions are now well documented ( @xcite , @xcite ) . estimating ridge regression coefficients amounts to minimizing @xmath313 subject to @xmath314 and",
    "it has been shown that assigning @xmath315 produces the same regularization on @xmath312 .",
    "similarly , estimating lasso coefficients amounts to minimizing @xmath313 subject to @xmath316 and produces the same regularization as assigning @xmath317 with @xmath318 . in what follows",
    "we will use @xmath91 to denote a gaussian prior and @xmath92 a laplace .",
    "further , to make reasonable comparisons between the two priors , we set @xmath319 which ensures that @xmath320 for all @xmath278 . to develop intuition regarding these two priors we first present some results from a small synthetic dataset generated using a simple linear regression model .",
    "then we consider the prostate cancer data example found in @xcite that was used to illustrate differences between the ridge and lasso regularization .",
    "using @xmath321 with @xmath322 , @xmath323 , and @xmath324 as a data generating mechanism we generated 25 @xmath325 pairs . using these 25 observations we fit the no - intercept simple linear regression model using @xmath326 as a prior in addition to considering both @xmath327 and @xmath328 .",
    "then , for each value of @xmath329 we collected 10000 mcmc draws from posteriors associated with @xmath91 and @xmath92 and employed them to compute @xmath330 and @xmath331 using proposition  [ postmean ] .",
    "results can be found in figure  [ regexp1 ] where the top left plot represents an extremely peaked prior for both @xmath91 and @xmath92 which produces posterior distributions that are very similar to the respective priors resulting in @xmath66 values that are close to one . as the value of @xmath332 increases ,",
    "the priors become less ` informative ' and agreement between prior and posterior decreases . however , in becoming more flat , the priors reach a point where the intersection of prior and posterior mass increases which is depicted by an increase in @xmath66 in the bottom right plot in figure  [ regexp1 ] .",
    "we now turn our attention to the prostate cancer data example found in @xcite . in this example",
    "the response variable is the level of prostate - specific antigens measured on 97 males .",
    "eight other clinical measurements ( such as age and log prostate weight ) were also measured and are used as covariates .",
    "thus , @xmath333 in this example .    ) and lasso ( laplace , @xmath92 ) regularization in regression models in terms of @xmath121 and @xmath172 .",
    "the left plot depicts @xmath122 as a function of @xmath332 for both @xmath91 and @xmath92 .",
    "the right compares @xmath172 values as a function of @xmath332 when @xmath91 and @xmath92 are centered at zero to that when the center of @xmath91 moves away from zero.,width=264 ]    ) and lasso ( laplace , @xmath92 ) regularization in regression models in terms of @xmath121 and @xmath172 .",
    "the left plot depicts @xmath122 as a function of @xmath332 for both @xmath91 and @xmath92 .",
    "the right compares @xmath172 values as a function of @xmath332 when @xmath91 and @xmath92 are centered at zero to that when the center of @xmath91 moves away from zero.,width=264 ]    before proceeding with model fit , we first evaluate the ` informativeness ' of the two priors for the eight regression coefficients by computing @xmath334 and @xmath335 and then assess their compatibility ( or mass intersection ) by computing @xmath172 .",
    "all calculations employ proposition  [ postmean ] .",
    "each metric is calculated for a sequence of @xmath332 values with results provided in figure  [ mvnlapnormpriokap ] .",
    "the left plot of figure  [ mvnlapnormpriokap ] provides @xmath122 of each prior for a sequence of @xmath332 values . for small values of the scale parameter @xmath336 , indicating that the laplace prior is more peaked than the normal .",
    "thus , even though the laplace has thicker tails , it is more ` informative ' relative to the gaussian .",
    "this corroborates the lasso penalization s ability to shrink coefficients to zero ( something ridge regulation lacks ) .",
    "as @xmath332 increases the two norms converge as both spread their mass more uniformly .",
    "the right plot of figure  [ mvnlapnormpriokap ] depicts @xmath172 as a function of @xmath332 .",
    "when @xmath91 is centered at zero , then @xmath172 is constant over values of @xmath332 which means that mass intersection when both priors are centered at zero is not influenced by tail thickness .",
    "compare this to @xmath89 values when @xmath91 is not centered at zero [ i.e. , @xmath337 or @xmath338 . for the former , @xmath89 increases as intersection of prior and posterior mass increases . for the latter , @xmath332 must be greater than two for there to be any substantial mass intersection as @xmath172 remains essentially at zero .",
    "now that the ` informativeness ' of the two priors has been explored , we fit model to the cancer data . within the mcmc algorithm we compute @xmath339 and @xmath340 along with @xmath341 and @xmath331 using proposition  [ postmean ] . without loss of generality we centered the @xmath98 so that @xmath312 does not include an intercept and standardized each of the eight covariates to have mean zero and standard deviation one .",
    "we employ @xmath342 as a prior .",
    "the resulting @xmath89 for a range of @xmath332 values is provided in figure  [ regressionkappa ] .    ) for linear regression model in , with shrinkage priors , applied to the prostrate cancer data from @xcite .",
    "the @xmath89 estimates were computed using proposition  [ postmean ] . ]    from figure  [ regressionkappa ] it appears that prior - data agreement is very small for both priors indicating the existence of prior - data incompatibility .",
    "however , for small values of @xmath332 , @xmath343 indicating more compatibility between prior and data for the gaussian prior .",
    "as an aside , the procedure found in @xcite would conclude that no prior - data conflict exists in this example as the prior for @xmath84 is sufficiently diffuse to render the data plausible per the prior predictive distribution .",
    "prior - posterior compatibility ( @xmath66 ) is very similar for both priors with that for @xmath92 being slightly smaller when @xmath332 is close to @xmath344 .",
    "interestingly , @xmath66 for both priors appears to asymptote at around 0.4 as @xmath332 decreases .",
    "the value of the asymptote is an artifact of the prior on @xmath84 .",
    "the slightly higher @xmath66 value for the gaussian prior implies that it has slightly more influence on the posterior than the laplace , and @xmath345 communicates a similar story , mainly that the gaussian prior has more influence on the posterior than the laplace .",
    "the thicker tails of the laplace prior seem to produce larger @xmath346 values than that of the gaussian prior indicating a larger amount of posterior - data compatibility .",
    "additionally , @xmath347 approaches one quicker than @xmath348 .",
    "this may be a result of the ability that the laplace prior has to shrink coefficients to zero .",
    "overall , it appears that the gaussian prior has more influence on the resulting posterior distribution relative to the laplace when updating knowledge via bayes theorem .",
    "we discussed a natural geometric framework to bayesian inference which motivated a simple , intuitively appealing measure of the agreement between priors , likelihoods , and posteriors : compatibility ( @xmath89 ) . in this geometric framework",
    ", we also discuss a related measure of the ` informativeness ' of a distribution , @xmath349 .",
    "in addition , in section  [ estimation ] we developed mcmc - based estimators of these metrics that are easily computable and , by avoiding the estimation of harmonic means , are reasonably stable .",
    "therefore at virtually no cost , practitioners can easily produce metrics that assess the degree of prior - data and prior - posterior compatibility .",
    "overall , we believe that the procedures developed in this paper should be a valuable contribution to the applied bayesian modeling community .    in theory , one may argue that compatibility as defined in section  [ geometry ] is grounded in the same construction principles as pearson correlation , in the sense that both consist of standardized inner products .",
    "however , compatibility is defined for priors , posteriors , and likelihoods in @xmath16 equipped with the inner product , whereas pearson correlation works with random variables in @xmath350 equipped with the inner product .",
    "our concept of compatibility can be used to evaluate how much the prior agrees with the likelihood , to measure the sensitivity of the posterior to the prior , and to quantify the level of agreement of elicited priors .",
    "one practical drawback with our geometric construction is that it has been developed for priors which are on @xmath16 , and thus some cases will not be handled by our setting ; a simple example is that of the jeffreys prior for the beta - binomial , @xmath351 , whose norm will be infinity .",
    "one possible approach to being able to consider densities not in @xmath16 , but not explored further here , is to work directly with @xmath352 , for @xmath353 this approach would result in a compatibility measure , @xmath354 , that continues being a metric that measures agreement between two elements of a geometry , though it loses direct connection with bayes theorem .",
    "interestingly , @xmath354 coincides with the so - called hellinger affinity @xcite    in addition to assessing agreement between the three components of bayes theorem , @xmath89 can also be used to perform model comparison by comparing competing likelihoods .",
    "it can also be employed to assess sensitivity to potential influential points by comparing same likelihoods and/or posteriors with and without the data points under consideration .",
    "the question of how one might gauge the compatibility between prior and likelihood in hierarchical models is a natural one .",
    "for example , to complete the hierarchy in section  [ reg ] one could consider assigning a prior to @xmath332 .",
    "without fully specifying the prior distributions on parameters that appear in the likelihood ( i.e. , introducing a process model in a hierarchy ) , @xmath80 would become a direct function of the process model parameters . to assess compatibility between prior and likelihood",
    ", it would then be natural to consider @xmath355 , where @xmath356 this , however , would increase the computational cost of estimating @xmath89 considerably . an alternative approach might be to employ a plug - in summary of @xmath89 via @xmath357 where @xmath358 corresponds to either the posterior mean or the max - compatible hyperparameter . assessing the value of this approach and considering @xmath89 in hierarchical models",
    "is the topic of current research .",
    "a possible avenue of research which is only briefly explored here is that of using our geometry for devising new probabilistic models for recasting the prior vector using the likelihood vector . indeed ,",
    "as mentioned in section  [ schemes_geom ] , new posterior schemes may be generated by plugging into alternative inner products which can be written as an integral .",
    "thus , the sky really is the limit in how one might use posterior schemes to recast the prior vector using the likelihood vector .",
    "developing innovative ways of recasting the prior vector is an area of ongoing research .",
    "another possibility for future research is an exploration of how the dimensionality of @xmath117 should affect the interpretation of @xmath89 , if at all .",
    "we would anticipate that as the dimensionality increases , there is increased potential for disagreement between two distributions .",
    "consequently , @xmath89 would generally diminish as additional parameters are added , _",
    "ceteris paribus_. a suitable offsetting transformation of @xmath89 , if it exists , could result in a measure of ` per parameter ' agreement .",
    "the proof follows by combining a taylor expansion with the first mean value theorem for integrals @xcite .",
    "just note that @xmath359      \\,{\\ensuremath{\\mathrm{d}}}\\boldsymbol{\\theta } \\\\      & = -\\int_{\\theta}\\pi^2(\\boldsymbol{\\theta } ) \\,{\\ensuremath{\\mathrm{d}}}{\\boldsymbol{\\theta}}+\\int_{\\theta}\\pi(\\boldsymbol{\\theta } ) { \\ensuremath{\\mathrm{d}}}{\\boldsymbol{\\theta}}\\\\      & \\hspace{0.4cm}-\\int_{\\theta } \\pi({\\boldsymbol{\\theta}})o\\{\\pi({\\boldsymbol{\\theta}})-1\\ } \\,{\\ensuremath{\\mathrm{d}}}\\boldsymbol{\\theta } \\\\      & = 1-\\|\\pi\\|^2 + o\\{\\pi({\\boldsymbol{\\theta}}^*)-1\\}\\int_{\\theta } \\pi({\\boldsymbol{\\theta } } ) \\,{\\ensuremath{\\mathrm{d}}}{\\boldsymbol{\\theta}}\\\\      & = 1-\\|\\pi\\|^2 + o\\{\\pi({\\boldsymbol{\\theta}}^*)-1\\ } ,    \\end{split}\\ ] ] for some @xmath360 , from where the final result follows .",
    "grogan , w. and wirth , w. ( 1981 ) a new american genus of predaceous midges related to palpomyia and bezzia ( diptera : ceratopogonidae ) .",
    "_ proceedings of the biological society of washington _ , * 94 * , pp .  12791305 .",
    "raftery , a.  e. , newton , m.  a. , satagopan , j.  m. and krivitsky , p.  n. ( 2007 ) estimating the integrated likelihood via posterior simulation using the harmonic mean identity . in _",
    "bayesian statistics _ , eds . bernardo , j.  m. , bayarri , m.  j. , berger , j.  o. , dawid , a.  p. , heckerman , d. , smith , a. f.  m. and west , m. , oxford university press , vol .  8 .",
    "shortle , j.  f. and mendel , m.  b. ( 1996 ) the geometry of bayesian inference , in _",
    "bayesian statistics_. eds .",
    "bernardo , j.  m. , berger , j.  o. , dawid , a.  p. and smith , a. f.  m. , oxford university press , vol .  5 , pp . 739746 ."
  ],
  "abstract_text": [
    "<S> we provide a geometric interpretation to bayesian inference that allows us to introduce a natural measure of the level of agreement between priors , likelihoods , and posteriors . the starting point for the construction of our geometry </S>",
    "<S> is the simple observation that the marginal likelihood can be regarded as an inner product between the prior and the likelihood . </S>",
    "<S> a key concept in our geometry is that of compatibility , a measure which is based on the same construction principles as pearson correlation , but which can be used to assess how much the prior agrees with the likelihood , to gauge the sensitivity of the posterior to the prior , and to quantify the coherency of the opinions of two experts . </S>",
    "<S> estimators for all the quantities involved in our geometric setup are discussed , which can be directly computed from the posterior simulation output . </S>",
    "<S> some examples are used to illustrate our methods , including data related to on - the - job drug usage , midge wing length , and prostate cancer . </S>",
    "<S> + keywords : bayesian inference ; geometry ; harmonic mean estimator ; hilbert spaces ; marginal likelihood ; normalizing constant ; prior - data conflict .    </S>",
    "<S> = 1    @xmath0 </S>"
  ]
}