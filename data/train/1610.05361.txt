{
  "article_text": [
    "recently end - to - end neural network speech recognition systems has shown promising results @xcite .",
    "further improvement were reported by using more advanced models such as attention - based recurrent sequence generator ( arsg ) as part of an end - to - end speech recognition system @xcite .",
    "although these new techniques help to decrease the word error rate ( wer ) on the automatic speech recognition system ( asr ) , distant speech recognition ( dsr ) remains a challenging task due to the reverberation and overlapping acoustic signals , even with sophisticated front - end processing techniques such as state - of - the - art beamforming @xcite .",
    "it is reported that using multiple microphones with signal preprocessing techniques like beamforming will improve the performance of the dsr @xcite .",
    "however the performance of such techniques are suboptimal since they are depended heavily on the microphone array geometry and the location of target source @xcite .",
    "other works have shown that deep neural networks with multichannel inputs can be used for learning a suitable representation for distant speech recognition without any front - end preprocessing @xcite .    in this paper",
    ", we propose an extension to the current attention - based recurrent sequence generator ( arsg ) that can handle multichannel inputs .",
    "we use @xcite as the baseline while extend its single channel input to multiple channel .",
    "we also integrate the language model in the decoder of asrg with weighted finite state transducer ( wfst ) framework as it was purposed in @xcite . to avoid slowness in convergence , we use highway lstm @xcite in our model which allows us to have stacked lstm layers without having gradient vanishing problem .",
    "rest of the paper is organized as follows . in section 2",
    "we briefly discuss related work . in section 3",
    "we introduce our proposed model and describe integration of language model with attention - based recurrent sequence generator .",
    "finally , conclusions are drawn in section 4 .",
    "kim et al . already proposed an attention - based approach with multichannel input @xcite . in their work",
    "an attention mechanism is embeded within a recurrent neural network based acoustic model in order to weigh channel inputs for a number of frames .",
    "the more reliable channel input will get higher score . by calculating phase difference between microphone pairs",
    ", they also incorporated spatial information in their attention mechanism to accelerate learning of auditory attention .",
    "they reported that this model achieve comparable performance to beamforming techniques .",
    "one advantage of using such model is that no prior knowledge of microphone array geometry is required and real time processing is done much faster than models with front - end preprocessing techniques .",
    "however , by using conventional dnn - hmm acoustic model , they did not utilize the full potential of attention mechanism in their proposed model .",
    "the feature vectors , weighted by attention mechanism , can help to condition the generation of the next element of the output sequence . in our model ,",
    "attention mechanism is similar to @xcite but it i also connected to the args to produce an end - to - end results .",
    "our model consists of three parts : an encoder , an attention mechanism and a decoder .",
    "the model takes multichannel input x and stochastically generates an output sequence ( @xmath0 , @xmath1 , ... , @xmath2 ) .",
    "the input x consists of n channel inputs x=\\{@xmath3 , @xmath4 , ... , @xmath5",
    "} where each channel @xmath6 is a sequence of small overlapping window of audio frames @xmath6=\\{@xmath7 , @xmath8 , ... ,",
    "@xmath9}. the encoder transforms the input into another representation by using recurrent neural network ( rnn ) . then the attention mechanism weighs elements of new representation based on their relevance to the output @xmath10 at each time step .",
    "finally , the decoder which is a rnn in our model , produces an output sequence ( @xmath0 , @xmath1 , ... , @xmath2 ) one character at time by using weighted representation produced by attention mechanism and the hidden state of the decoder . in the literature , combination of the attention mechanism and the decoder",
    "is called attention recurrent sequence generator ( arsg ) .",
    "arsg is proposed to address the problem of learning variable - length input and output sequences since in the conventional rnns , the length of the sequence of hidden state vectors is always equal to the length of the input sequence .    in the following subsections",
    "we will discuss these three parts in detail .",
    "we concatenate frames of all channels and feed them as an input to the encoder which is a rnn , at each time step t. another approach to handle multichannel input as kim et al .",
    "proposed in @xcite , is to feed each channel separately in to the rnn at each time step .",
    "training such rnn in this approach can be very slow for an end - to - end scenario since the attention mechanism should weigh frames of each channel at every time step .",
    "moreover , phase difference calculation between microphone pairs are needed for scoring frames .",
    "because of these computational complexities , we decided to concatenate all channel features and feed them to the network as liu et al .",
    "have done @xcite in their model .",
    "we used bidirectional highway long short - term memory ( bhlstm ) in the encoder .",
    "zhang et al . showed that bhlstm with dropout achieved state - of - the - art performance in the ami ( sdm ) task @xcite .",
    "this model addresses the gradient vanishing problem caused by stacking lstm layers .",
    "thus this model allows the network to go much deeper @xcite .",
    "bhlstm is based on long short - term memory projected ( lstmp ) which originally proposed by @xcite .",
    "the operation of lstmp network follows the equations    @xmath11    f_t & = &   \\sigma(\\mathbf{w_{xf}}x_t + \\mathbf{w_{mf}}r_{t-1 } + \\mathbf{w_{cf}}c_{t-1 } + b_f ) \\\\[4pt ]    c_t & = &   f_t \\odot c_{t-1 } + i_t \\odot tanh(\\mathbf{w_{xc}}x_t + \\mathbf{w_{mc}}r_{t-1 } + b_c ) \\label{has : one } \\\\[4pt ]     o_t & = &   \\sigma(\\mathbf{w_{xo}}x_t + \\mathbf{w_{mo}}r_{t-1 } + \\mathbf{w_{co}}c_{t } + b_o ) \\\\[4pt ]    m_t & = &   o_t \\odot tanh(c_t)\\\\[4pt ]    r_t & = &   \\mathbf{w_{rm}}m_t\\\\[4pt ]    y_t & = &   \\sigma(\\mathbf{w_{yr}}r_t + b_y ) \\end{aligned}\\ ] ]    iteratively from @xmath12 to @xmath13 , where @xmath14 term denote weight matrices .",
    "@xmath15 , @xmath16 , @xmath17 are diagonal weight matrices for peephole connections .",
    "the @xmath18 terms denote bias vectors , @xmath19 denotes the element - wise product , @xmath20 , @xmath21 , @xmath22 , @xmath23 and @xmath24 are input gate , forget gate , output gate , cell activation and cell output activation vector respectively . @xmath25 and @xmath26 denotes input and output to the layer at time step t. @xmath27 is sigmoid activation fuction .",
    "finally @xmath28 denotes the recurrent unit activations .",
    "a lstmp unit can be seen as combination of standard lstm unit with peephole that its cell output activation @xmath24 is transformed by a linear projection layer .",
    "this architecture , shown in the figure [ fig : lstmp ] , converges faster than standard lstm and it has less parameters than standard lstm while keeping the performance @xcite .",
    "+        the hlstm rnn is illustrated in figure [ fig : hlstm ] .",
    "it is an extension to the lstmp model .",
    "it has an extra gate , called carry gate , between memory cells of adjacent layers .",
    "the carry gate controls how much information can flow between cells in adjacent layers .",
    "hlstm architecture can be orbtained by modification of eq .",
    "( [ has : one ] ) :    @xmath29 d_t^l & = &   \\sigma(\\mathbf{w_{xd}^l}x_t^l + w_{d1}^l \\odot c_{t-1}^l + w_{d2}^l \\odot c_{t}^{l-1 } + b_d^l ) \\end{aligned}\\ ] ]    if the predecessor layer @xmath30 is also an lstm layer , otherwise : @xmath31 d_t^l & = &   \\sigma(\\mathbf{w_{xd}^l}x_t^l + w_{d1}^l \\odot c_{t-1}^l   + b_d^l ) \\end{aligned}\\ ] ]    where @xmath32 is a bias vector , @xmath33 and @xmath34 are weight vectors , @xmath35 is the weight matrix connecting the carry gate to the input of this layer . in this study",
    ", we also extend the hlstm rnns from unidirection to bidirection .",
    "note that the backward layer follows the same equations used in the forward layer except that @xmath36 is replaced by @xmath37 to exploit future frames and the model operates from @xmath38 to 1 .",
    "the output of the forward and backward layers are concatenated to form the input to the next layer .",
    "-3 cm       attention mechanism is a subnetwork that weighs the encoded inputs @xmath39 .",
    "it selects the temporal locations on input sequence that is useful for updating hidden state of the decoder .",
    "specifically , at each time step @xmath20 , the attention mechanism computes scalar energy @xmath40 for each time step of encoded input @xmath41 and hidden state of decoder @xmath42 .",
    "the scalar energy will be converted into probability distribution , which is called alignment , over time steps using softmax function . finally , the context vector @xmath43 is calculated by linear combination of elements of encoded inputs @xmath41 @xcite .",
    "@xmath44    \\alpha_{i , l } & = & \\frac{exp(e_{i , l})}{\\sum_{l=1}^l exp(e_i)}\\\\[4pt ]   c_{i } & = & \\sum_{l } \\alpha_{i , l}h_l\\end{aligned}\\ ] ]    where @xmath14 and @xmath45 are matrix parameters and @xmath46 and @xmath18 are vector parameters .",
    "note that the @xmath47 and @xmath48 .",
    "this method is called context - based .",
    "bahdanau et al .",
    "showed that using context - based scheme for attention model is prone to error since similar elements of embedded input @xmath39 is scored equally @xcite . to alleviate this problem , bahdanau et al .",
    "suggested to use previous alignment @xmath49 by convolving it along time axis for calculation of the scalar energy .",
    "thus by changing eq .",
    "( [ eq : energy ] ) we have :    @xmath50    e_{i , l } & = &   w^ttanh(\\mathbf{w}s_{i } + \\mathbf{v}h_l + \\mathbf{u}f_l + b)\\end{aligned}\\ ] ]    where @xmath51 , @xmath52 are parameter matrices , * denotes convolution and @xmath53 are feature vectors of matrix @xmath54 . a disadvantage of this model is the complexity of the training procedure .",
    "the alignments @xmath55 should be calculated for all pairs of input and output position which is @xmath56 .",
    "chorowski et al . suggested a windowing approach which limits the number of embedded inputs for computation of alignments @xcite .",
    "this approach reduces the complexity to @xmath57 .",
    "the task of the decoder which is a rnn in arsg framework , is to produce probability distribution over the next character conditioned on all the characters that already has been seen .",
    "this distribution is generated with a mlp and a softmax by the hidden state of the decoder @xmath42 and the context vector of attention mechanism @xmath43 .",
    "@xmath58    the hidden state of decoder @xmath42 is a function of previous hidden state @xmath59 , the previously emitted character @xmath60 and context vector of attention mechanism @xmath61 :    @xmath62    where @xmath21 is a single layer of standard lstm .",
    "the arsg with encoder can be trained jointly for end - to - end speech recognition .",
    "the objective function is to maximize the log probability of each character sequence condition on the previous characters .    @xmath63    where @xmath64 is the network parameter and @xmath65 is the ground truth of previous characters .",
    "to make it more resilient to the bad predictions during test , chen et al .",
    "suggested to sample from previous predicted characters with the rate of 10% and use them as input for predicting next character instead of ground truth transcript @xcite .",
    "chen et al . showed that end - to - end models for speech recognition can achieve good results without using language model but for reducing word error rate ( wer ) , language model is essential @xcite .",
    "one way for integrating language model with the arsg framework , as bahdanau et al .",
    "suggested , is to use weighted finite state transducer ( wfst ) framework @xcite . by composition of grammar and lexicon",
    ", wfst defines the log probability for a character sequence ( see @xcite ) . with the combination of wfst and args",
    ", we can look for transcripts that maximize :    @xmath66    in decoding time .",
    "where @xmath67 is tunable parameter .",
    "( see @xcite ) .",
    "there are also other models for integrating language model to args .",
    "shallow fusion and deep fusion are two of them that are proposed by gulcehre et al .",
    "@xcite .",
    "integrating shallow fusion is similar to the wfst framework but it uses recurrent neural network language model ( rnnlm ) for producing log probability of sequences . in deep fusion , the rnnlm is integrated with the decoder subnetwork in arsg framework .",
    "specifically , the hidden state of rnnlm is used as input for generating the probability distribution on the next character in eq .",
    "( [ mlpdist ] ) .",
    "however , in our work we opt to use wfst framework for the sake of simplicity and training tractability of the model .",
    "in this work , we show the end - to - end models that are based only on neural networks can be applied to the distant speech recognition task with concatenation of multichannel inputs . also better accuracy is expected with the usage of highway lstms in the encoder network . for the future work , we will study integration of recurrent neural network language model ( rnnlm ) with attention - based recurrent sequence generator ( arsg ) with highway lstm .",
    "@xmath68 with @xmath69 a convex function of @xmath70 , going to @xmath71 when @xmath72 .",
    "we shall first consider the question of nontriviality , within the general framework of @xmath75-subquadratic hamiltonians . in the second subsection",
    ", we shall look into the special case when @xmath76 is @xmath77-subquadratic , and we shall try to derive additional information .",
    "theorem  [ ghou : pre ] tells us that if @xmath82 , the boundary - value problem : @xmath83 has at least one solution @xmath84 , which is found by minimizing the dual action functional : @xmath85 dt\\ ] ] on the range of @xmath86 , which is a subspace @xmath87 with finite codimension . here",
    "@xmath88 is a convex function , and @xmath89                  assume @xmath76 is @xmath112 and @xmath113-subquadratic at infinity . let @xmath114 be the equilibria , that is , the solutions of @xmath115 .",
    "denote by @xmath116 the smallest eigenvalue of @xmath117 , and set : @xmath118 if : @xmath119",
    "<    \\frac{t}{2\\pi}\\omega    \\label{eq : three}\\ ] ] then minimization of @xmath109 yields a non - constant @xmath13-periodic solution @xmath84 .",
    "we recall once more that by the integer part @xmath120 $ ] of @xmath121 , we mean the @xmath122 such that @xmath123 . for instance , if we take @xmath124 , corollary 2 tells us that @xmath84 exists and is non - constant provided that :                            to understand the nontriviality conditions , such as the one in formula ( [ eq : four ] ) , one may think of a one - parameter family @xmath149 , @xmath150 of periodic solutions , @xmath151 , with @xmath149 going away to infinity when @xmath152 , which is the period of the linearized system at 0 .",
    "assume also that @xmath76 is @xmath112 , and @xmath160 is positive definite everywhere .",
    "then there is a sequence @xmath161 , @xmath162 , of @xmath163-periodic solutions of the system @xmath164 such that , for every @xmath162 , there is some @xmath165 with : @xmath166    [ external forcing ] consider the system : @xmath167 where the hamiltonian @xmath76 is @xmath77-subquadratic , and the forcing term is a distribution on the circle : @xmath168 where @xmath169 .",
    "for instance , @xmath170 where @xmath171 is the dirac mass at @xmath172 and @xmath173 is a constant , fits the prescription .",
    "this means that the system @xmath174 is being excited by a series of identical shocks at interval @xmath13 .",
    "if @xmath187 and @xmath188 , with @xmath189 , we shall say that @xmath76 is @xmath113-subquadratic at infinity . as an example , the function @xmath190 , with @xmath191 , is @xmath154-subquadratic at infinity for every @xmath97 .",
    "similarly , the hamiltonian @xmath192 is @xmath193-subquadratic for every @xmath97 .",
    "note that , if @xmath194 , it is not convex .",
    "the first results on subharmonics were obtained by rabinowitz in @xcite , who showed the existence of infinitely many subharmonics both in the subquadratic and superquadratic case , with suitable growth conditions on @xmath195 . again the duality approach enabled clarke and ekeland in @xcite to treat the same problem in the convex - subquadratic case , with growth conditions on @xmath76 only .",
    "recently , michalek and tarantello ( see @xcite and @xcite ) have obtained lower bound on the number of subharmonics of period @xmath163 , based on symmetry considerations and on pinching estimates , as in sect .  5.2 of this article .",
    "5 a. y. hannun , a. l. maas , d. jurafsky , and a. y. ng , `` first - pass large vocabulary continuous speech recognition using bi - directional recurrent dnns , '' arxiv , pp .",
    "1-7 , 2014 .",
    "a. graves , s. fernandez , f. gomez , and j. schmidhuber , `` connectionist temporal classification  : labelling unsegmented sequence data with recurrent neural networks , '' proc .",
    ", pp . 369-376 , 2006 .",
    "k. kumatani , j. mcdonough , and b. raj , `` microphone array processing for distant speech recognition : from close - talking microphones to far - field sensors , '' ieee signal process . mag .",
    "29 , no . 6 , pp . 127-140 , 2012 .        y. zhang , g. chen , d. yu , k. yaco , s. khudanpur , and j. glass , `` highway long short - term memory rnns for distant speech recognition , '' in icassp , ieee international conference on acoustics , speech and signal processing - proceedings , 2016 , vol .",
    "2016-may , pp . 5755-5759 .",
    "s. kim and i. lane , `` recurrent models for auditory attention in multi - microphone distance speech recognition , '' proc .",
    "speech commun .",
    "interspeech , pp . 1-9 , 2016 . y. liu , p. zhang , and t. hain , `` using neural network front - ends on far field multiple microphones based speech recognition , '' in icassp , ieee international conference on acoustics , speech and signal processing - proceedings , 2014 , pp .",
    "5542-5546 .",
    "w. chan , n. jaitly , q. le , and o. vinyals , `` listen , attend and spell : a neural network for large vocabulary conversational speech recognition , '' in icassp , ieee international conference on acoustics , speech and signal processing - proceedings , 2016 , vol .",
    "2016-may , pp . 4960-4964 .",
    "j. chorowski , d. bahdanau , k. cho , and y. bengio , `` end - to - end continuous speech recognition using attention - based recurrent nn : first results , '' deep learn .",
    "work . nips 2014 , pp .",
    "1-10 , 2014 . m. mohri , f. pereira , and m. riley , `` weighted finite - state transducers in speech recognition , '' comput .",
    "speech lang .",
    "16 , pp . 69-88 , 2002 .",
    "c. gulcehre , o. firat , k. xu , k. cho , l. barrault , , h. c. lin , f. bougares , h. schwenk and y. bengio , `` on using monolingual corpora in neural machine translation , '' arxiv prepr .",
    "arxiv1503.03535 , p. 9"
  ],
  "abstract_text": [
    "<S> end - to - end attention - based models have been shown to be competitive alternatives to conventional dnn - hmm models in the speech recognition systems . in this paper , we extend existing end - to - end attention - based models that can be applied for distant speech recognition ( dsr ) task . </S>",
    "<S> specifically , we propose an end - to - end attention - based speech recognizer with multichannel input that performs sequence prediction directly at the character level . to gain a better performance </S>",
    "<S> , we also incorporate highway long short - term memory ( hlstm ) which outperforms previous models on ami distant speech recognition task . </S>"
  ]
}