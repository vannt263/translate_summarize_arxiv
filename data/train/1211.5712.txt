{
  "article_text": [
    "ellipse detection is one of the most important problems in image processing .",
    "it has been researched using a good variety of methods , see i.e. tsuji and matsumoto @xcite , davies @xcite .",
    "most of the existing techniques use the hough transform @xcite  that is very memory and time consuming .    in this paper",
    "a new approach will be presented and its advantages and disadvantages will be discussed .",
    "we show the results of the algorithm on the pictures from fig .",
    "[ fig : infull ] .        the algorithm discussed in this paper :",
    "* is easily adaptable , ie .",
    "if we know the expected shape of the object sought , or its position ( orientation ) in space , by little calculation we can prepare a proper configuration for its detection ; * can detect simultaneously multiple type of objects , ex .",
    "we can look for matches and coins at the same time ; * is rather insensitive to the disturbance of the picture ( such as bluring , contrast and illumination modification , etc ) ; * can be used for classification ( we can detect specified shapes ) and for clustering ( we can use it for exploring the data structure ) .",
    "the acceptable disadvantage of the presented method is that to work well we need the beforehand knowledge that on the picture we study there are no other objects than ellipse - like shapes .",
    "consequently , our approach is well - adapted for example to the following tasks :    * count the number of ellipses on the picture ; * divide the shapes into circles of different radiuses ; * count the number of vertical and horizontal ellipses .",
    "our idea uses a cross - entropy clustering @xcite ( cec ) , which from the practical point of view can be seen as joining of the k - means method with the model approach used in expectation maximization ( em ) .",
    "em @xcite is one of the basic and most important applications of maximal likelihood in the density estimations @xcite .",
    "em , or its variations like classification em @xcite are often applied in clustering . although em approach is quite general , and gives good results , to apply it we usually need to first perform complicated computations",
    ". moreover , to accomplish the m step one commonly needs numerically consuming minimization techniques , and consequently em is relatively slow and can not deal well with large data .",
    "our aim in this paper is to show that cec is well - adapted to classification and detection of ellipses and ellipsoids .",
    "the advantage of cec over em is simplicity and speed  in the case of typical gaussian families we do not need the m - step , which enables us in particular to use fast and efficient hartigans approach .",
    "moreover , as the use of every cluster in cec has its cost , contrary to classification em , cec reduces on - line clusters which carry no information , which in practice implies that our algorithm can find the `` right '' number of ellipses on the picture .",
    "let us discuss the contents of the paper .",
    "in the first part of our work we briefly describe the cec algorithm . in the next section we present the basic models we use ( compare with @xcite ) .",
    "we also present results of numerical experiments .",
    "then we describe the procedure for finding toothpicks in the image ( see fig .",
    "[ fig : infull ] ) .",
    "in appendix we provide the proof of the only cross - entropy formula from section which is essentially new . in our opinion",
    "its proof is worth including as in fact it given a method which can be easily used in search for cross - entropy in other gaussian subfamilies .",
    "in this section we give a short introduction to cec , for more detailed explanation we refer the reader to @xcite .",
    "to explain cec we need to introduce the `` energy function '' we want to minimize . by the cross - entropy of the probability measure @xmath0 ( which represent the data - set we study ) with respect to density @xmath1 we understand @xmath2 the above cross - entropy corresponds to the theoretical code - length of compression of @xmath0-randomly chosen element of @xmath3 with the code optimized for density @xmath1 @xcite . in a more general case when one is interested in ( best ) coding for @xmath0 by densities chosen from family @xmath4",
    ", we arrive at _ the cross - entropy of @xmath0 with respect to a family of coding densities @xmath4",
    "_ @xmath5 in the case of splitting of @xmath3 into pairwise disjoint sets @xmath6 such that elements of @xmath7 we `` code '' by optimal density from family @xmath8 , the mean code - length of randomly chosen element @xmath9 equals @xmath10 where @xmath11 denotes the normalized restriction of @xmath0 to the set @xmath12 and is given by @xmath13 .",
    "the aim of cec is to find splitting of @xmath3 into pairwise disjoint sets @xmath7 which minimize the function given in . in this paper",
    "we restrict for the sake of simplicity to clusters generated by gaussian densities ( although one can easily use any density family for which mle can be performed ) .",
    "now we proceed with discussion of the gaussian models we will use in cec .",
    "we consider following density families :    1 .",
    "@xmath14  gaussian densities with covariance @xmath15 .",
    "the clustering will have the tendency to divide the data into clusters resembling the unit circles in the mahalanobis distance given by @xmath16 .",
    "its particular important subfamily is given by @xmath17 , where @xmath18 is fixed ( in this case we will have tendency to divide the data into `` circles '' with approximate radius of @xmath19 ) .",
    "2 .   @xmath20  spherical gaussian densities , which covariance is proportional to identity .",
    "the clustering will try to divide the data into circles of arbitrary sizes .",
    "3 .   @xmath21  gaussians with diagonal covariance .",
    "the clustering will try to divide the data into ellipsoid with radiuses parallel to coordinate axes .",
    "@xmath22  all gaussian densities . in this case",
    "we divide dataset into ellipsoid - like clusters without any preferences concerning the size or shape or position in space of the ellipsoid .",
    "we need a result which says what is the cross - entropy of the probability measure @xmath0 with respect to coding adapted for the respective gaussian subfamilies .",
    "a basic role is played by the following observation .    [ ob ] let @xmath0 be a discrete or continuous probability measure in @xmath3 with well - defined mean @xmath23 and covariance matrix @xmath24 .",
    "let a fixed positive - definite symmetric matrix @xmath15 be given .",
    "then @xmath25 , where @xmath26 denotes the probability measure with gaussian density of the same mean and covariance as @xmath0 .",
    "consequently @xmath27    by applying the above proposition",
    "one can easily deduce the formulas for cross - entropy given the table [ tab1:cec ] .",
    "@xmath28 +    @xmath14 & @xmath15 & @xmath29 +    @xmath17 & @xmath30 & @xmath31 +    @xmath32 & @xmath33 & @xmath34 +    @xmath21 & @xmath35 & @xmath36 +    @xmath22 & @xmath37 & @xmath38 +    in the second column we give the formula for the covariance matrix of the gaussian density which realizes the desired minimum of cross - entropy ( obviously the mean is always the mean of the measure ) .",
    "simple applications of the formulas given above can be found on the figure [ fig : ir ] .",
    "let us explain the method on the following simple problem : assume that we want to count the toothpicks on the fig .",
    "[ fig : irisx ] . to do",
    "so we take a particular object and compute its covariance matrix .",
    "we have obtained a covariance with eigenvalues @xmath39 since we want to allow the toothpick to have any position in space , we introduce the set @xmath40 to consist of all gaussian densities on the plane with covariance matrix having eigenvalues @xmath41 and @xmath42 ( observe that this set is rotation and translation invariant , but not scale invariant ) .",
    "consider now a probability measure @xmath0 , representing our data , with covariance @xmath37 , with eigenvalues @xmath43 . by applying proposition [ bas ] ( see appendix ) jointly with observation [ ob ] we easily conclude that the best approximation ( understood in the maximal likelihood or equivalently cross - entropy , sense ) of @xmath0 in @xmath40 is given by the gaussian density with covariance matrix with the same eigenvectors as @xmath37 and eigenvalues @xmath41 and @xmath42 .",
    "+     consequently , the cross - entropy , which plays the role of energy , @xmath44 thanks to is given by @xmath45 by applying hartigan approach we can now find the splitting of the data into pairwise disjoint sets @xmath46 which minimizes the value of",
    ". results of our method can be seen on figure [ fig : irisx ] ( we omit here the natural preliminary binarization procedure ) .    to visualize the found clusters ,",
    "we draw the boundary of an ellipse with the same mean and covariance as a given density estimator is given by @xmath47 $ ] , that is we draw the ellipse with radiuses @xmath48 . ] .",
    "we have proposed a new method , which uses cross - entropy clustering approach , to classification and detection of ellipse - like shapes .",
    "the main advantage of the method lies in the fact that it can be easily adapted to finding ellipses of desired shape and position in space .",
    "the basic disadvantage is that in current algorithm configuration ( basic approach ) we can deal only with pictures which contain only ellipse - like shapes ( for example we can not discover ellipses in a picture with ellipses and rectangles ) .",
    "our further work will consist on elimination of this inconvenience .",
    "the situation is very simple if we search for the mle , or in other words for the minimum in in the class of diagonal matrices ( subclass consisting of gaussians with independent variables ) .",
    "a more requiring and difficult question is to find the desired minimum in the class of all gaussians .",
    "below we present an approach which allows to do this .",
    "given @xmath53 by @xmath54 we denote the set of all symmetric matrices with eigenvalues @xmath55 .",
    "the following proposition plays the basic role in the search for optimal gaussian densities , as it reduces the search from all symmetric matrices to search in the set of eigenvalues .",
    "since its proof is short , we provide it for the sake of completeness .        to prove the inverse inequality we will use the von neumann trace inequality . let @xmath64 be arbitrary .",
    "we apply the inequality for @xmath65 , @xmath66 .",
    "since @xmath67 and @xmath68 are symmetric nonnegatively defined matrices , their eigenvalues @xmath69 and @xmath70 coincide with singular values , and therefore by @xmath71 since @xmath72 , from inequality we obtain that @xmath73 ."
  ],
  "abstract_text": [
    "<S> the problem of finding elliptical shapes in an image will be considered . </S>",
    "<S> we discuss the solution which uses cross - entropy clustering . </S>",
    "<S> the proposed method allows the search for ellipses with predefined sizes and position in the space . </S>",
    "<S> moreover , it works well for search of ellipsoids in higher dimensions . </S>"
  ]
}