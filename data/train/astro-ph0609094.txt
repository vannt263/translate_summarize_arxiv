{
  "article_text": [
    "clustering and visualisation are two widespread unsupervised data analysis techniques , with applications in numerous fields of science and engineering .",
    "two key strategies can be distinguished in the literature : ( 1 ) one is to produce a compression of the data first and then use that to visually detect distinct clusters .",
    "a wide range of linear and nonlinear dimensionality reduction techniques developed in machine learning follow this route , including pca , gtm @xcite , etc .",
    "( 2 ) the alternative strategy is to cluster the data first and visualise the resulting cluster assignments afterwards .",
    "this is more popular in the data mining community @xcite . a recently proposed method , termed parametric embedding ( pe ) @xcite proposes to take class posteriors produced by a mixture - based clustering algorithm and project them in 2d for visualisation .",
    "let us observe however , that for both of these data exploration strategies the two objectives  clustering and visualisation  are decoupled and essentially one is entirely subordinated to the other .",
    "this is worrying in that inevitably , the errors accumulated in the first stage can not be corrected in a subsequent stage and may essentially compromise the process of understanding the data .    in this paper",
    "we consider the class visualisation problem , as in @xcite and we identify a setting where a more fruitful coupling between clustering and visualisation can be achieved by integrating them both into a consistent probabilistic model .",
    "as we shall see , this is particularly useful in high - dimensional problems , where the number of data dimensions exceeds the number of observations .",
    "such cases are encountered in modern scientific data analysis , e.g. in gene expression analysis , or the analysis of special objects in astronomy .",
    "our approach is based on a multi - objective formulation in a probabilistic formalism .",
    "as we shall see , our model can be decoupled and understood as a sum of two objectives : one term is responsible for clustering and one other for a pe - like projection of the estimated class assignments .",
    "these two steps are now interdependent , so that in high dimensional problems the projection step fulfils a regularisation role , guarding against the curse of dimensionality problem .",
    "we use both synthetic data and two real - world high - dimensional data sets in our experiments : observed spectra ( of rare quality and coverage ) of early - type galaxies and a benchmark gene expression array data set are used to demonstrate the working of the proposed approach .",
    "we find that in both cases we obtain not only a visualisation of the mixture posteriors , but a more predictive mixture model , as measured by out of sample test likelihood , as a result of appropriately combining the objectives of clustering and class projection .",
    "the remainder of the paper is organised as follows : we begin with presenting our probabilistic model in section 2 .",
    "the interpretation by which this can be understood as a joint model for mixture based clustering and pe - like projection will become evident in section 3 , where the em @xcite methodology is used to derive a maximum a posteriori ( map ) estimation algorithm for our model .",
    "we then extend this work to take into account additional available knowledge on measurement uncertainties for real - world data analysis .",
    "section 4 presents the application two very different experiments involving galaxy spectra and gene expression .",
    "the results are summarised in the concluding section .",
    "consider @xmath0 independent , @xmath1-dimensional data points .",
    "the @xmath2-th point is denoted by @xmath3 having features @xmath4 .",
    "we seek a 2d mapping of this data into points @xmath5 in the euclidean space such as to reflect topological relationships based on some cluster structure that may be present in the data set . in building up our model ,",
    "we begin by making the common assumption of conditional independence , in order to enforce the dependences among data features to be captured in the latent space . @xmath6",
    "further , in order to capture complicated density shapes , including possibly distinct clusters , we model the conditional probabilities of the data features as a mixtures of gaussians .",
    "the mixing coefficients of these mixtures are instance - specific , so that each measurement that belongs to the same object will have the same mixing coefficient .",
    "this will ensure that the various features of an instance are likely to be allocated to the same ( set of ) mixture components . @xmath7",
    "observe we do not impose that each data point must belong to exactly one mixture component .",
    "this allows us to model the relationships between clusters .    assuming that we work with real - valued observations , and @xmath8 is a gaussian , then @xmath9 are the mean and precision parameters respectively .",
    "other choices are however possible as appropriate .",
    "@xmath10    the second factor in ( [ mixt ] ) is a nonlinear function that projects a point @xmath11 from the euclidean space onto a probability simplex . a parameterised softmax function can be used for this purpose . @xmath12",
    "our goal is then to determine @xmath11 for each @xmath13 .",
    "in addition , we also need to estimate the parameters @xmath14 and @xmath15 .    in order to somewhat narrow down the search space",
    ", we add smoothing priors , similarly to @xcite : @xmath16 in addition , the inverse variance parameters ( precisions ) are given exponential priors to prevent them produce singularities and encourage the extinction of unnecessary model complexity . @xmath17",
    "the hyperparameters @xmath18 and @xmath19 must all have strictly positive values .",
    "here we derive map estimates for our model specified in the previous section . the complete data log likelihood is proportional to the posterior over all hidden variables and this is the following .",
    "@xmath20 where we used jensen s inequality and @xmath21 represent variational parameters that can be obtained from maximising ( [ q ] ) : @xmath22 we can also regard @xmath23 as the outcome of a hidden class variable and @xmath24 are in fact true class posterior probabilities of this class variable , cf .",
    "bayes rule .",
    "the re - writing ( [ q]-[r ] ) is convenient for deriving the estimation algorithm for the parameters of the model . before proceeding ,",
    "let us rearrange ( [ q ] ) in two main terms , so that the interpretation of our model as a combination of mixture - based clustering and a pe - like class projection becomes evident .",
    "@xmath25 now , the first term can be recognised as a clustering model , essentially an instance of modular mixtures @xcite or an aspect - mixture of gaussians @xcite , which is known to be advantageous in high - dimensional clustering problems @xcite . the second term , in turn , is a pe - like objective @xcite , which minimises the kullback - leibler ( kl ) divergence between the class - posteriors and their projections .",
    "evidently , these two objectives are now interdependent .",
    "it remains to be seen in which cases their coupling is advantageous .",
    "carrying out the optimisation of ( [ q ] ) yields the following maximum likelihood estimates for the means and maximum a posteriori estimates for the precisions .",
    "@xmath26    for the remaining parameters , there is no closed form solution , we employ numerical optimisation using the gradients ( see appendix ) : @xmath27    as expected , the form of parameter updates also reflects the interdependence of our two objectives : ( [ pars ] ) is formally identical with the updates in @xcite ( up to the variation implied by the use of the prior for precisions ) is formally identical to the inverse of their variance estimates  so",
    "we now see the expression can be derived with the use of a proper exponential prior on the precisions . ] ) . the gradients ( [ latent1])-([latent2 ] ) are in turn , as expected , very similar to the updates in pe @xcite .",
    "having estimated the model , the empirical bayesian estimate @xcite of the goodness of fit for new points is given by integrating over the empirical distribution @xmath28 .",
    "this is the following .",
    "@xmath29      the 2d coordinates @xmath30 provide a visual summary of the data density . in addition ,",
    "label markers ( or colouring information ) , to aid the visual analysis , are obtained directly from @xmath31 .",
    "this is a handy feature of our method , as opposed to techniques based on dimensionality reduction methods ( such as e.g. pca ) , where detecting meaningful clusters from the projection plot is not straightforward .",
    "the class labels may also serve as an interface to the domain expert , who may wish to specify and fix the labels of certain points in order to explore the structure of the data interactively",
    ".    for accommodating new data points on a visualisation produced from a training set , a fast folding in @xcite procedure can be used .",
    "this is to compute @xmath32 with all model parameters kept fixed to their estimated values . conveniently , this optimisation task is convex , i.e. with all other parameters kept fixed , the hessian w.r.t .",
    "@xmath33 is positive semidefinite @xmath34 for the same reasons as in the case of pe @xcite .",
    "therefore the projection of test points is unique .",
    "however , pe @xcite makes no mention of how to accommodate new points on an existing visualisation plot .",
    "it is often the case that data from science domains ( e.g. astronomy ) come with known observational errors . in this section",
    "we modify our algorithm to take these into account .",
    "let @xmath35 be the standard deviation of the known measurement error of the @xmath36-th feature of instance @xmath2 .",
    "we handle this by considering @xmath4 as a hidden variable which stands for the clean data , and in addition we have @xmath37 .    assuming that we are dealing with real valued data , @xmath38 was defined as a mixture of gaussians , and so the integration over the unseen clean data variable @xmath4 gives the following likelihood term for component @xmath39 of feature @xmath36 : @xmath40 in other words , the variance of the data likelihood now has two terms , one coming from the measurement error and one other coming from the modelling error .",
    "the latter needs to be estimated .",
    "the estimation equations in this case modify as follows : @xmath41 the update equation of @xmath42 becomes @xmath43 and the updates of @xmath11 and @xmath15 remain unchanged .    for the precision parameters @xmath44 there is no closed form solution and so numerical optimisation may be employed , e.g. a conjugate gradient w.r.t .",
    "@xmath45 , since then the optimisation is unconstrained .",
    "@xmath46 observe that when @xmath47 , all equations of this subsection reduce to those presented for the noise - free case in sec .",
    "yet another alternative is to treat @xmath4 as hidden variables and take a hierarchical em approach .",
    "it should be highlighted , that although many non - probabilistic methods simply ignore the measurement errors even when these are known , due to our probabilistic framework a principled treatment is possible .",
    "this prevents finding interesting patterns in the visualisation plot as a result of measurement errors , at least in the cases when such errors are known .",
    "furthermore , there are cases when further refinement of the noise model will be needed , e.g. in many cases the recorded error values are uncertain or known to be optimistic .",
    "the first set of experiments is meant to demonstrate the working of our method and to highlight in which situations it is advantageous over the fully disjoint and sequential application of a mixture - based clustering and subsequent visualisation strategy .",
    "illustrative cases are shown and these are important for knowing in what kind of problems is the method appropriate to use .    throughout , we used smoothness hyperparameters @xmath48 and @xmath19 was determined by cross - validation under an initial assumption of a large ( k=10 ) number of clusters .",
    "the priors on the precision parameters favour the extinction of unnecessary components and even if there are remaining redundant components , a good - enough @xmath19 parameter can be located .",
    "the typical value obtained was of the order of @xmath49 .",
    "then @xmath19 is fixed and a further cross - validation is run to determine the optimal number of clusters @xmath50 ( less or equal to the number of non - empty clusters found in the previous step ) . we noted the optimisation is very sensitive to initialisation and starting @xmath51 from k - means is beneficial . to alleviate problems with local optima , each run",
    "was repeated 20 times and the model that found better maximum of the model likelihood was retained for further testing .",
    "two sets of generated data were created .",
    "for the first set , 300 points were drawn from a 6-dimensional mixture of 5 independent gaussians ( 60 points in each class ) .",
    "the second set was sampled from a 300-dimensional mixture of 5 independent gaussians ( again , 60 points per class ) .",
    "1.a ) shows the test likelihood averaged over 20 repeated runs , having generated the test data from the same model as the training data . the test likelihood obtained with a mixture of gaussians ( mog )",
    "is superimposed for comparison .",
    "we see that for the relatively low dimensional data set the proposed joint model has little ( no ) advantage .",
    "this is simply because in this case there is enough data to reliably estimate a mog .",
    "the obtained mixture posteriors could then safely be fed into e.g. a pe @xcite for class visualisation .    for the case of high dimensional data",
    ", however the situation is different .",
    "the mog overfits badly and is therefore unable to identify the clusters or to provide reliable input to a subsequent visualisation method .",
    "this is the situation when our proposed model is of use .",
    "the projection part of the objective guards against overfitting  as we can see from the test likelihood on fig  1.b .",
    "[ here ]        fig  1.c shows the visualisation of the 300-dimensional data set .",
    "each point is the 2d representation ( @xmath11 ) of the corresponding 300-d datum point .",
    "the markers correspond to the maximum argument of the softmax outputs @xmath52 , so they represent labels automatically assigned by the model . in this case , the estimated labels are identical with the true labels .",
    "we also see that the true number of classes has been correctly recovered .",
    "in addition , we noted that in experiments where the number of classes was deliberately chosen larger than the true number of clusters , some of the unnecessary clusters progressively become empty indeed , due to the employed prior on the precision parameters , while others split a true cluster .",
    "however , notably , the visual image of the true cluster split by several components tends to remain compact .",
    "such an example is seen on fig  1.d .",
    "we apply the method developed above to a sample of measured spectra ( in the ultraviolet to optical range of radiation ) of 21 well - studied early - type ( elliptical or lenticular ) galaxies . in a previous work @xcite",
    ", we had studied this data set using various factor analysis techniques . here , we seek to obtain a visual analysis of the data .",
    "each of these spectra represent flux measurements at 348 values of wavelength in the range 2000 - 8000 , in equal bins , for all spectra .",
    "observational errors are associated with each value , which we take into account as described in section 3.3 .",
    "thus , the clustering and class visualisation of these spectra is a high - dimensional problem .",
    "this represents a pilot data set for an important study in the evolution of galaxies .",
    "it is generally believed that all early - type galaxies formed all their stars in the early universe , and then have evolved more - or - less passively until the present day- so one expects to find their spectrum to correspond to a collection of stars all of the same age . however , detailed observations in the last decade indicate a wealth of complex detail in a significant fraction of such galaxies , including evidence of a sub - population of very young stars in many cases . how common this effect is largely unknown , and",
    "can only be addressed through data mining of large spectral archives . even though many @xmath53 galaxy spectra are being assembled in large public archives ( e.g. www.sdss.org ) , a sample as detailed as ours is rare and difficult to assemble , particularly with such wide a coverage in wavelength , which requires combining observations from both ground and space based observatories ( see details in @xcite ) . from this small sample ,",
    "we would attempt to isolate those galaxies which have young stars from those that do nt .",
    "needless to say , the fluxes are all positive values . in order to be interpretable",
    ", our method needs to ensure the estimated parameters ( cluster prototypes ) are also positive . in our previous work",
    ", we built in explicit constraints to ensure this @xcite .",
    "here , since each @xmath42 in ( [ pars ] ) is just a weighted average of positive data , its positivity is automatically satisfied .    the leave - one - out test likelihood of our model is shown on the left plot of fig .  2 .",
    "the peak at @xmath54 indicates that two clusters have been identified .",
    "a mixture of gaussians test likelihood is superimposed for comparison , showing the overfitting problem due to the high dimensionality .",
    "the mog is therefore unable to identify any clusters in the data .",
    "hence , a class visualisation of the data based on mixture posteriors would be clearly compromised in this case .",
    "the right hand plot shows the grouping of the actual data around the two identified prototypes @xmath55 of our model .",
    "the latter are superimposed with thick lines .",
    "these can be recognised and interpreted as the prototype of the spectrum of a ` young ' and ` mature ' stellar population respectively .",
    "thus , in this case , the clusters have a physical interpretation in astrophysical terms .    [ here ! ]     produced by our method indicates two clusters whereas a mixture of gaussians overfits and therefore fails to identify clusters in the data .",
    "right : the actual data , clustered around the two prototypes identified by our model .",
    "the parameters @xmath51 for @xmath56 are superimposed with thick lines .",
    "they are interpretable as a young and an old prototypical spectrum respectively .",
    "the identification number that marks some of the spectra correspond to those on fig .",
    "the marked spectra are the instances that apart from their overall shape present some features of the young category too . , title=\"fig:\",width=211,height=170 ]   produced by our method indicates two clusters whereas a mixture of gaussians overfits and therefore fails to identify clusters in the data .",
    "right : the actual data , clustered around the two prototypes identified by our model .",
    "the parameters @xmath51 for @xmath56 are superimposed with thick lines .",
    "they are interpretable as a young and an old prototypical spectrum respectively .",
    "the identification number that marks some of the spectra correspond to those on fig .",
    "the marked spectra are the instances that apart from their overall shape present some features of the young category too .",
    ", title=\"fig:\",width=219,height=170 ]    identification numbers mark some of the spectra clustered in the ` mature ' category on the left lower plot of fig .  2 .",
    "these are the galaxies that have a significantly non - zero class membership for either cluster , and they indeed include some morphological aspects of the ` young ' category as well ( the emission lines at @xmath57   and the slope of the spectral continuum in the range 6000 - 8000 ) . physically , this indicates the presence of a significant population of young ( @xmath58  gyr old ) stars , whereas the rest of the stars are @xmath59  gyr old .    [ here ! ]    , the 2d latent representation @xmath11 is plotted .",
    "the markers are those assigned by the model , as shown on the right.,width=359,height=170 ]    the identification numbers are the same as those on fig .  3 , where we see the 2d visualisation of the sample on the left . for each spectrum @xmath60 ,",
    "the 2d latent representation @xmath11 is plotted .",
    "the markers represent cluster labels automatically assigned by the model , as detailed in the right hand plot .",
    "we see the two clusters are well separated on the image and the ` hybrid ' galaxies are indeed placed in between those that are clearly cluster members . of these , the one marked as 18 represents a galaxy ( ngc 3605 ) for which recent detailed physical analyses have been made @xcite .",
    "it turns out that although more than 85% of its stellar mass is associated with an old ( 912 gyr ) stellar population , it does contain a younger stellar population too , at @xmath61 gyr @xcite .",
    "we therefore conclude that it is possible to have an intuitive visual summary of a few hundreds of measurements per galaxy in just two coordinates with the application of our method .      in a brief final experiment",
    "we show the potential use of our approach for the visual analysis of high dimensional gene expression arrays .",
    "oligonucleotide arrays can provide a means of studying the state of a cell , by monitoring the expression level of thousands of genes at the same time @xcite and have been the focus of extensive research .",
    "the main difficulty is that the number of examples is typically of the order of tens while the number of genes is of the order of thousands . even after eliminating genes that have little variation ,",
    "we are still left with at least hundreds of data dimensions .",
    "straightforward mixture based clustering runs into the well - know curse of dimensionality problem .",
    "here we apply our method to the coloncancer data set , having 40 tumour and 22 normal colon tissue samples @xcite .",
    "this is a benchmark data set , used in many previous classification and clustering studies .",
    "our input matrix consisted of the 500 genes with highest overall variation @xmath62 the 62 samples .    fig .",
    "[ colon ] shows the visualisation obtained in a purely unsupervised manner .",
    "the markers now correspond to the true labels ( not used by the algorithm ) , and are given for the ease of visual evaluation of the representation produced .",
    "the separation of cancerous from noncancerous tissues is most apparent on the plot .",
    "the potential of such a visualisation tool lies mainly in that it would allow a domain expert to interactively explore the structure of the sample . additionally , the gene - specific posteriors @xmath24 provide quantitative gene - level class information .    [ here ]",
    "we proposed and investigated a model for class visualisation of explicitly high dimensional data .",
    "we have shown this model relates closely to pe @xcite in that it represents a probabilistic integration of the clustering and visualisation objectives into a single model .",
    "we derived empirical bayesian estimates for our model which make this multi - objective interpretation easy to follow .",
    "although this work may potentially further be enhanced by a fuller bayesian estimation scheme , the empirical bayesian methodology has been appropriate for our purposes @xcite and it allows us to estimate an empirical latent density from a given example set of data and to reason about previously unseen data relative to that .",
    "we demonstrated gains in terms of the predictive capabilities of the proposed model over the fully modular and sequential approach to clustering and class visualisation in the case of high dimensional data .",
    "this research is funded by pparc grant pp / c503138/1 , ` designer algorithms for astronomical data mining ' .",
    "ak also acknowledges partial support from a wellcome trust vip award ( project 10835 ) .",
    "the terms containing @xmath11 are the following .",
    "@xmath63 the gradient is then : @xmath64 renaming @xmath65 by @xmath39 and replacing the expression of @xmath66 the following is obtained .",
    "@xmath67    u alon , n barkai , d notterman , k gish , s ybarra , d mack , a levine",
    ". broad patterns of gene expression revealed by clustering analysis of tumour and normal colon cancer tissues probed by oligonucleotide arrays .",
    "cell biol .",
    "96 , 67456750 .",
    "h attias . learning in high dimension : modular mixture models .",
    "proc . artificial intelligence and statistics , 2001 .",
    "c.m bishop .",
    "neural networks for pattern recognition .",
    "oxford university press , inc .",
    ", new york , ny , 1995 .",
    "c.m bishop , m svensen and c.k.i williams .",
    "gtm : the generative topographic mapping . neural computation ,",
    "10(1 ) , 1998 .",
    "b.p carlin and t.a louis .",
    "bayes and empirical bayes methods for data analysis . chapman and hall , 2000 .",
    "th hofmann .",
    "gaussian latent semantic models for collaborative filtering .",
    "26th annual international acm sigir conference , 2003 .",
    "t iwata , k saito , n ueda , s stromsten , t.l griffiths , j.b tenenbaum .",
    "parameteric embedding for class visualisation .",
    "information processing systems 17 , 2005 .",
    "a kabn , l nolan and s raychaudhury .",
    "finding young stellar populations in elliptical galaxies from independent components of optical spectra .",
    "siam intl conf on data mining ( sdm05 ) , pp .",
    "l nolan , m harva , a kabn and s raychaudhury .",
    "a data - driven bayesian approach to finding young stellar populations in early - type galaxies from their ultraviolet - optical spectra , mon . not . of the royal astron .",
    "366,321 - 338 , 2006 .",
    "l nolan , j.s dunlop , b panter , r jimenez , a heavens , g smith .",
    "the star - formation histories of elliptical galaxies across the fundamental plane , submitted to mnras .",
    "reflections on scma iii . in : statistical challenges in astronomy .",
    "eds : e.c feigelson and g.j babu . springer .",
    "s rogers , m girolami , c campbell , r breitling . the latent process decomposition of cdna microarray datasets .",
    "ieee / acm transact .",
    "biol . bioinformatics . 2 : 143156 .",
    "t soukup and i davidson .",
    "visual data mining : techniques and tools for data visualisation and mining .",
    "wiley , 2002 ."
  ],
  "abstract_text": [
    "<S> parametric embedding ( pe ) has recently been proposed as a general - purpose algorithm for class visualisation . </S>",
    "<S> it takes class posteriors produced by a mixture - based clustering algorithm and projects them in 2d for visualisation . </S>",
    "<S> however , although this fully modularised combination of objectives ( clustering and projection ) is attractive for its conceptual simplicity , in the case of high dimensional data , we show that a more optimal combination of these objectives can be achieved by integrating them both into a consistent probabilistic model . in this way </S>",
    "<S> , the projection step will fulfil a role of regularisation , guarding against the curse of dimensionality . as a result </S>",
    "<S> , the tradeoff between clustering and visualisation turns out to enhance the predictive abilities of the overall model . </S>",
    "<S> we present results on both synthetic data and two real - world high - dimensional data sets : observed spectra of early - type galaxies and gene expression arrays . </S>"
  ]
}