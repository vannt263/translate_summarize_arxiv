{
  "article_text": [
    "the analytical model encodes the low- and high - temperature phases of the ising model through their magnetization .",
    "the hidden layer contains 3 perceptrons ( a neuron with a heaviside step nonlinearity ) ; the first two perceptrons activate when the input states are mostly polarized , while the third one activate if the states are polarized up or unpolarized .",
    "notice that the third neuron can also be choosen to activate if the states are polarized down or unpolarized .",
    "the resulting outcomes are recombined in the output layer and produce the desired classification of the state .",
    "the hidden layer is parametrized through a weight matrix and bias vector given by @xmath34 where @xmath35 is the only free parameter of the model .",
    "the arguments of the three hidden layer neurons , in terms of the weight matrix , bias vector , and a particular ising configuration @xmath36 , are given by @xmath37 where @xmath38 is the magnetization of the ising configuration . in figure",
    "[ fig : activations](a ) we display the components of the @xmath39 vector as a function of the magnetization of the ising state @xmath40 .",
    "the first and second neuron activate when the state is predominantly polarized , i.e. , when @xmath41 or @xmath42 .",
    "the third neuron activates if the state has a magnetization @xmath43 , which means that , in the limit where @xmath44 , it activates when the state is either polarized or unpolarized .",
    "the parameter @xmath45 is thus a threshold value of the magnetization that helps deciding whether the state is considered polarized or not .    .",
    "( a ) hidden layer arguments for our toy model .",
    "( b ) and ( c ) show the arguments for a neural network with 3 sigmoid neurons before and after training , respectively .",
    ", width=624 ]    the output layer is parametrized through a weight matrix and bias vector given by @xmath46    where these arbitrary choices ensure that the ordered , low-@xmath6 output neuron @xmath47 is active when either the spins polarize mostly @xmath48 or @xmath49 . on the other hand , when the @xmath50 neuron is active but the @xmath48 is not , then the high - temperature output neuron @xmath51 , symbolizing a high - temperature state .    to illustrate what the effects of the training on the parameters @xmath52 and @xmath53 are , we consider the numerical training of a fully - connected neural network with only 3 neurons using the same setup and training / test data used in our ferromagnetic model ( figure  [ fig : isingneural ] ) for the @xmath17 system . in figure",
    "[ fig : activations](b ) we display the argument @xmath54 of the input layer at training iteration @xmath55 for configurations @xmath56 in the test set as a function of the magnetization of the configurations @xmath40 .",
    "the weights have been randomly initialized at @xmath55 from a normal distribution with zero mean and unit standard deviation .",
    "as the training proceeds , the parameters adjust such that the components of the vector @xmath57 approximately become linear functions of the magnetization @xmath40 , as shown in figure  [ fig : activations](c ) , in agreement with the assumptions of our toy model .",
    "these results clearly support our claim about the neural network s ability encode and _ learn _ the magnetization in the hidden layer .",
    "a strategy to gain intuition for how these neural networks operate is to produce a low - dimensional visualization of data used in the training .",
    "we consider the t - distributed stochastic neighbor embedding ( t - sne ) technique  @xcite where high - dimensional data is embedded in two or three dimensions so that data points close to each other in the original space are also positioned close to each other in the embedded low - dimensional space .",
    "figure  [ fig : tsneising ] displays a t - sne visualization of the ising configurations used the training of our ferromagnetic model .",
    "the two low - temperature blue regions correspond to the two ordered states with spins polarized either up or down .",
    "the high - temperature red region identifies the paramagnetic state .",
    "the resulting neural networks , which are functions defined over the high - dimensional state space , become trained so that the low - temperature output neuron takes a high value in the cool region ( and vice versa ) , crossing over to a low value as the system is warmed through the orange hyperplane .",
    "this allows the classification of a state in terms of the neuron values .     colored according to temperature .",
    "the orange line represents a hyperplane separating the low- from high - temperatures states .",
    ", width=384 ]",
    "the exact architecture of the convolutional neural network ( cnn )  @xcite , schematically described in figure  [ fig : neural ] , is as follows .",
    "the input layer is a two - dimensional ising spin configuration with @xmath58 spins , where @xmath59 .",
    "the first hidden layer convolves 64 @xmath60 filters on each of the two sublattices of the model with a unit stride , no padding , with periodic boundary conditions , followed by rectified linear unit ( relu ) .",
    "the final hidden layer is a fully - connected layer with 64 relu units , while the output is a softmax layer with two outputs ( correponding to @xmath26 and @xmath31 states ) . to prevent overfitting",
    ", we apply a dropout regularization in the fully - connected layer  @xcite .",
    "our model has been implemented using tensorflow  @xcite .",
    "since our ccn correctly classifies @xmath26 and @xmath31 states with @xmath32 accuracy , we would like to scrutinize the origin of its discriminiative power by asking whether it discerns the states by the presence ( or absence ) of the local hamiltonian constraints or the extended closed - loop structure .",
    "our strategy consists in the construction of new test sets with modified low temperature states , as detailed below .",
    "first , we consider transformations that do not destroy the topological order  @xcite of the @xmath26 state but change the local constraints of the original ising lattice gauge theory . as",
    "shown in the configurations in figure  [ fig : toruscuts](a ) and ( b ) , we consider transformations where a spin is flipped every @xmath61 ( a ) ( @xmath62 ( b ) ) plaquettes",
    ". the positions of the flipped spins are marked with red crosses .",
    "after optimizing the cnn using the original training set , the neural network classifies most of the transformed @xmath26 states as high - temperature ones , resulting in an overall test accuracy of @xmath30 and @xmath63 for @xmath61 and @xmath62 , respectively .",
    "this reveals that the neural network relies on the presence of satisfied local constraints of the original ising lattice gauge theory , and not on the topological order of the state , in deciding whether a state is considered low or high temperature .",
    "second , we consider a new test set where the @xmath26 states retain most local constraints but disrupt non - local features like the extended closed - loop structure .",
    "we consider dividing the original states into 4 pieces as shown in figure  [ fig : toruscuts](c ) and then reshuffling the 4 pieces among different states , subsequently stitching them to form new `` low '' temperature configurations .",
    "the new configurations will contain defects along the dashed lines in figure  [ fig : toruscuts](c ) , thus disrupting the extended closed - loop picture , but preserving the local constraints everywhere else in the configuration .",
    "we find that the trained cnn recognizes such states as ground states with high confidence , suggesting that the cnn does not use the extended closed - loop structure and indicating that local constraints are the only features that the cnn relies on for classification of the ground state .",
    "spins per plaquette ( a ) and @xmath64 spins per plaquette ( b ) are flipped .",
    "the red crossess symbolize the flipped spins .",
    "( c ) cuts / stitches ( yellow dashed lines ) performed on the ground state configurations in order to produce a new test set from mixing the 4 resulting pieces among different ground states.,width=624 ]    in view of the conclusion above , we now present a toy model that uses a streamlined version of our original cnn constructed to explicitly detect satisfied energetic local constraints .",
    "the convolutional layer contains 16 2@xmath652 filters per sublattice with unit stride in both directions and periodic boundary conditions . the convolutional layer is fully connected to two perceptron neurons in the output layer , as described below .",
    "a schematic representation of the toy cnn is presented in figure  [ fig : toyisinggauge ] .",
    "the values of the filters @xmath66 are presented in table  [ w - table ] , where @xmath56 and @xmath67 represent the spatial indices of the convolution , and @xmath68 and @xmath69 label the sublattice and the filter , respectively .",
    "the purpose of the filters is to individually process each plaquette in the spin configuration and determine whether its energetic constraints are satisfied or not .",
    "the ising gauge theory contains @xmath70 different spin configurations per plaquette , of which 8 satisfy the energetic constraints of the hamiltonian .",
    "the first group of 16 filters @xmath66 ( @xmath71 thorugh @xmath72 , left blue column in table  [ w - table ] ) detect satisfied plaquettes , while the remaining 16 ( @xmath73 through @xmath74 , right red column in table  [ w - table ] ) detect unsatisfied plaquettes .",
    "the bias of the convolutional layer is a 16-dimensional vector given by @xmath75 , where @xmath76 is a small parameter .",
    "the outcome of the convolutional layer consists of 16 two - dimensional arrays of size @xmath77 processed through perceptrons . here",
    "the total number of spins in the configuration is @xmath78 , where @xmath79 is the linear size of the system .",
    "the outcome of the convolutional layer is reshaped into a @xmath80-dimensional vector such that the first @xmath81 entries correspond to the outcome of the first group of filters ( @xmath71 thorugh @xmath72 ) while the remaining @xmath81 correspond to the last group of filters ( @xmath73 through @xmath74 ) .",
    "the output layer , which is fully connected to the the convolutional layer , contains two perceptron neurons denoted by @xmath82 and @xmath83 for zero- and high - temperature states , respectively .",
    "it is parametrized through a weight matrix and a bias vector given by       1 & @xmath841 & 0\\\\[-0.8em ] 1&0\\end{pmatrix*}$ ] & @xmath84 1&1 \\\\[-0.8em ] 0 & 0\\end{pmatrix*}$ ] & 9 & @xmath84 1 & 0\\\\[-0.8em ] 1&0\\end{pmatrix*}$ ] & @xmath84 1&-1 \\\\[-0.8em ] 0 & 0\\end{pmatrix*}$ ] + 2 & @xmath84 - 1 & 0\\\\[-0.8em]-1&0\\end{pmatrix*}$ ] & @xmath84 - 1&-1 \\\\[-0.8em ] 0 & 0\\end{pmatrix*}$ ] & 10 & @xmath84 1 & 0\\\\[-0.8em ] 1&0\\end{pmatrix*}$ ] & @xmath84 - 1 & 1 \\\\[-0.8em ] 0 & 0\\end{pmatrix*}$ ] + 3 & @xmath84 1 & 0\\\\[-0.8em ] 1&0\\end{pmatrix*}$ ] & @xmath84 - 1&-1 \\\\[-0.8em ] 0 & 0\\end{pmatrix*}$ ] & 11 & @xmath84 1 & 0\\\\[-0.8em]-1&0\\end{pmatrix*}$ ] & @xmath84 1 & 1 \\\\[-0.8em ] 0 & 0\\end{pmatrix*}$ ] + 4 & @xmath84 1 & 0\\\\[-0.8em]-1&0\\end{pmatrix*}$ ] & @xmath84 - 1 & 1 \\\\[-0.8em ] 0 & 0\\end{pmatrix*}$ ] & 12 & @xmath84 - 1 & 0\\\\[-0.8em ] 1&0\\end{pmatrix*}$ ] & @xmath84 1 & 1 \\\\[-0.8em ] 0 & 0\\end{pmatrix*}$ ] + 5 & @xmath84 - 1 & 0\\\\[-0.8em]-1&0\\end{pmatrix*}$ ] & @xmath84 1 & 1 \\\\[-0.8em ] 0 & 0\\end{pmatrix*}$ ] & 13 & @xmath84 - 1 & 0\\\\[-0.8em]-1&0\\end{pmatrix*}$ ] & @xmath84 - 1 & 1 \\\\[-0.8em ] 0 & 0\\end{pmatrix*}$ ] + 6 & @xmath84 - 1 & 0\\\\[-0.8em ] 1&0\\end{pmatrix*}$ ] & @xmath84 1&-1 \\\\[-0.8em ] 0 & 0\\end{pmatrix*}$ ] & 14 & @xmath84 - 1 & 0\\\\[-0.8em]-1&0\\end{pmatrix*}$ ] & @xmath84 1&-1 \\\\[-0.8em ] 0 & 0\\end{pmatrix*}$ ] + 7 & @xmath84 1 & 0\\\\[-0.8em]-1&0\\end{pmatrix*}$ ] & @xmath84 1&-1 \\\\[-0.8em ] 0 & 0\\end{pmatrix*}$ ] & 15 & @xmath84 - 1 & 0\\\\[-0.8em ] 1&0\\end{pmatrix*}$ ] & @xmath84 - 1&-1 \\\\[-0.8em ] 0 & 0\\end{pmatrix*}$ ] + 8 & @xmath84 - 1 & 0\\\\[-0.8em ] 1&0\\end{pmatrix*}$ ] & @xmath84 - 1 & 1 \\\\[-0.8em ] 0 & 0\\end{pmatrix*}$ ] & 16 & @xmath84 - 1 & 0\\\\[-0.8em]-1&0\\end{pmatrix*}$ ] & @xmath84 - 1&-1 \\\\[-0.8em ] 0 & 0\\end{pmatrix*}$ ] +      these choices ensure that whenever an unsatisfied plaquette is encountered by the convolutional layer , the zero - temperature neuron is @xmath86 and the high - temperature @xmath87 , while only if _ all _ energetic constraints are satisfied @xmath88 and @xmath89 , thus allowing the classification of the states .",
    "when used on our test sets , the model performs the classification task with a @xmath32 accuracy , which means that all the high temperature states in the test set contain least one unsatisfied plaquette .",
    "note that the classification error for this task is expected to be exponentially small in the volume of the system , since at infinte temperature the ground states appear with exponentially small probability .",
    "having distilled the model s basic ingredients , we proceed to train an analogue model _ numerically _ starting from random weights and biases @xmath90 , @xmath91 , @xmath92 , and @xmath93 .",
    "further , we replace the perceptron nonlinearities by relu units and a softmax output layer to enable a reliable numerical training . after the training",
    ", the model performs the classification task with a @xmath32 accuracy on the test sets , as expected .    as a consequence of the classification scheme provided by the analytical toy model",
    ", we observe that the values of the zero - temperature neuron @xmath82 behave exactly like the amplitudes of one of the ground states of the toric code written in the @xmath94 basis  @xcite .",
    "the ground state described by @xmath82 is a linear combination of all 4 ground states with well defined parity on the torus .",
    "more precisely , such a state can be written as @xmath95 , where the spin configurations @xmath96 , and @xmath97 corresponds to the value of @xmath82 after a feed - forward pass of the neural network for a given a input configuration @xmath98 .",
    "our model bears resemblance with the construction of the ground state of the toric code in terms of projected entangled pair states in that local tensors project out states containing plaquettes with odd parity  @xcite .",
    "these observations suggest that convolutional neural networks have the potential to represent ground states with topological order .",
    "x.  wen , _ quantum field theory of many - body systems : from the origin of sound to an origin of light and electrons : from the origin of sound to an origin of light and electrons _ , oxford graduate texts ( oup oxford , 2004 ) ."
  ],
  "abstract_text": [
    "<S> neural networks can be used to identify phases and phase transitions in condensed matter systems via supervised machine learning . </S>",
    "<S> readily programmable through modern software libraries , we show that a standard feed - forward neural network can be trained to detect multiple types of order parameter directly from raw state configurations sampled with monte carlo . </S>",
    "<S> in addition , they can detect highly non - trivial states such as coulomb phases , and if modified to a convolutional neural network , topological phases with no conventional order parameter . </S>",
    "<S> we show that this classification occurs within the neural network without knowledge of the hamiltonian or even the general locality of interactions . </S>",
    "<S> these results demonstrate the power of machine learning as a basic research tool in the field of condensed matter and statistical physics .    </S>",
    "<S> condensed matter physics is the study of the collective behavior of massively complex assemblies of electrons , nuclei , magnetic moments , atoms or qubits  @xcite . </S>",
    "<S> this complexity is reflected in the size of the classical or quantum state space , which grows exponentially with the number of particles . </S>",
    "<S> this exponential growth is reminiscent of the `` curse of dimensionality '' commonly encountered in machine learning . </S>",
    "<S> that is , a target function to be learned requires an amount of training data that grows exponentially in the dimension ( e.g.  the number of image features ) . despite this curse , </S>",
    "<S> the machine learning community has developed a number of techniques with remarkable abilities to recognize , classify , and characterize complex sets of data . in light of this success , </S>",
    "<S> it is natural to ask whether such techniques could be applied to the arena of condensed - matter physics , particularly in cases where the microscopic hamiltonian contains strong interactions , where numerical simulations are typically employed in the study of phases and phase transitions  @xcite . </S>",
    "<S> we demonstrate that modern machine learning architectures , such as fully - connected and convolutional neural networks  @xcite , can provide a complementary approach to identifying phases and phase transitions in a variety of systems in condensed matter physics . </S>",
    "<S> the training of neural networks on data sets obtained by monte carlo sampling provides a particularly powerful and simple framework for the supervised learning of phases and phase boundaries in physical models , and can be easily built from readily - available tools such as theano  @xcite or tensorflow  @xcite libraries .    </S>",
    "<S> conventionally , the study of phases in condensed matter systems is performed with the help of tools that have been carefully designed to elucidate the underlying physical structures of various states . among the most powerful </S>",
    "<S> are monte carlo simulations , which consist of two steps : a stochastic importance sampling over state space , and the evaluation of estimators for physical quantities calculated from these samples  @xcite . </S>",
    "<S> these estimators are constructed based on a variety of physical impetuses ; e.g.  the ready availability of an analogous experimental measure like a specific heat ; or , the encoding of some more abstract theoretical device , like an order parameter  @xcite . </S>",
    "<S> however , unique and technologically important states of matter may not be straightforwardly identified with standard estimators . indeed , for some highly - coveted phases such as topologically - ordered states  @xcite , positive identification may require prohibitively expensive ( and experimentally challenging  @xcite ) measures such as the entanglement entropy  @xcite .     of the model in the thermodynamic limit , @xmath0  @xcite . </S>",
    "<S> , width=624 ]    machine learning , already explored as a tool in condensed matter and materials research  @xcite , provides an alternate paradigm to the above approach . the ability of modern machine learning techniques to classify , identify , or interpret massive data sets like images , videos , genome sequences , internet traffic statistics , natural language recordings , etc .  </S>",
    "<S> foreshadows their suitability to provide physicists with similar analyses on the exponentially large data sets embodied in the state space of condensed matter systems . </S>",
    "<S> we first demonstrate this on the prototypical example of the square - lattice ferromagnetic ising model , @xmath1 . </S>",
    "<S> we set the energy scale @xmath2 ; the ising variables @xmath3 so that for @xmath4 lattice sites , the state space is of size @xmath5 . </S>",
    "<S> standard monte carlo techniques can efficiently provide samples of configurations for any temperature @xmath6 , weighted by the boltzmann distribution . </S>",
    "<S> the existence of a well - understood phase transition at temperature @xmath7  @xcite , between a high - temperature paramagnetic phase and a low - temperature ferromagnetic phase , allows us the opportunity to attempt to classify the two different types of configurations without the use of monte carlo estimators ( e.g. the magnetization ) . </S>",
    "<S> instead , we construct a fully connected feed - forward neural network , implemented with tensorflow  @xcite , to perform supervised learning directly on the raw configurations sampled by a monte carlo simulation ( see figure  [ fig : isingneural ] ) . </S>",
    "<S> the neural network is composed of an input layer with values determined by the spin configurations , @xmath8-unit hidden layer of sigmoid neurons , and an analogous output layer . </S>",
    "<S> we use a cross - entropy cost function supplemented with an @xmath9 regularization term to prevent overfitting . </S>",
    "<S> the neural network is trained using the adam method for stochastic optimization  @xcite . </S>",
    "<S> as illustrated in figure  [ fig : isingneural](a ) through ( c ) , when trained on a broad range of temperatures above and below @xmath7 , the neural network is able to correctly classify @xmath10 of uncorrelated data provided in a test set , at the same temperatures as in the training set , for a system of @xmath11 spins . </S>",
    "<S> the classification accuracy improves as the system size is increased ( as high as @xmath12 for @xmath13 ) , as inferred from figure  [ fig : isingneural](c ) , indicating that this training / testing paradigm is capable of systematically narrowing in on the true thermodynamic value of @xmath7 in a way analogous to the direct measurement of the magnetization in a conventional monte carlo simulation . in fact , due to the simplicity of the underlying order parameter ( a bulk polarization of ising spins below @xmath7 ) , one can understand the training of the network through a simple toy model involving a hidden layer of only three analytically `` trained '' perceptrons , representing the possible combinations of high and low - temperature magnetic states exclusively based on their magnetization . </S>",
    "<S> as illustrated in figure  [ fig : isingneural](d ) through ( f ) , it performs the classification task with remarkably high accuracy . </S>",
    "<S> we emphasize that the toy model has no _ a priori _ knowledge of the critical temperature . </S>",
    "<S> further details about the toy model , as well as a low - dimensional visualization of the training data to gain intuition for how these neural networks operate , are discussed in the supplementary materials . </S>",
    "<S> similar results and success rates occur if the model is modified to have anti - ferromagnetic couplings , @xmath14 , illustrating that the neural network is not only useful in identifying a global spin polarization , but an order parameter with a more complicated ordering wave vector ( here @xmath15 , where @xmath16 is the lattice spacing ) .    </S>",
    "<S> clearly , such a framework does not provide the same quantitive understanding as a direct monte carlo measurement of the order parameter , which sits on a solid bedrock of decades of statistical mechanics theory . </S>",
    "<S> however , the power of neural networks lies in their ability to generalize to tasks beyond their original design . </S>",
    "<S> for example , what if one was presented with a data set of ising configurations from an unknown hamiltonian , where the lattice structure ( and therefore its @xmath7 ) is not known ? </S>",
    "<S> we illustrate this scenario by taking our above feed - forward neural network , already trained on configurations for the square - lattice ferromagnetic ising model , and feed it a test set produced by monte carlo simulations of the triangular lattice ferromagnetic ising hamiltonian . </S>",
    "<S> the network has no information about the hamiltonian , the lattice structure , or even the general locality of interactions . in figure  </S>",
    "<S> [ fig : triangle ] we present the output layer neurons averaged over the test set as a function of temperature for @xmath17 . </S>",
    "<S> we estimate the critical temperature based on the crossing point of the low- and high - temperature outputs to be @xmath18 , which is close to the exact thermodynamic @xmath19 @xcite  a discrepancy easily attributed to finite - size effects . </S>",
    "<S> further , the same strategy can be repeated , using instead our toy neural network . </S>",
    "<S> again , without any knowledge of the critical temperatures on the square or triangular lattices , we estimate @xmath20 , differing from the true thermodynamic critical @xmath7 by less than @xmath21 .    . </S>",
    "<S> the orange line signals the triangular ising model @xmath22 , while the blue dashed line represents our estimate @xmath18.,width=480 ]    we turn to the application of such techniques to problems of greater interest in modern condensed matter , such as disordered or topological phases , where no conventional order parameter exists . </S>",
    "<S> coulomb phases , for example , are states of frustrated lattice models where local energetic constraints lead to extensively degenerate classical ground states , which are highly - correlated `` spin liquids '' without a bulk magnetization or other local order parameter . </S>",
    "<S> we consider a two - dimensional _ square ice _ hamiltonian given by @xmath23 where the charge at vertex @xmath24 is @xmath25 , and the ising variables located in the lattice bonds as shown in figure  [ fig : ice_tcode ] . </S>",
    "<S> in a conventional condensed - matter approach , the ground states and the high - temperature states are distinguished by their spin - spin correlation functions : power - law decay in the coulomb phase at @xmath26 , and exponential decay at high temperature . </S>",
    "<S> instead we use supervised learning , feeding raw monte carlo configurations to train a fully - connected neural network ( figure  [ fig : isingneural](a ) ) to distinguish ground states from high - temperature states . </S>",
    "<S> figure  [ fig : ice_tcode](a ) and figure  [ fig : ice_tcode](b ) display high- and low - temperature snapshots of the configurations used in the training of the model . for a square ice system with @xmath27 spins , we find that a standard fully - connected neural network with 100 hidden units successfully distinguishes the states with a @xmath12 accuracy . </S>",
    "<S> the network does so solely based on spin configurations , with no information about the underlying lattice  </S>",
    "<S> a feat difficult for the human eye , even if supplemented with a layout of the underlying hamiltonian locality .        </S>",
    "<S> these results indicate that the learning capabilities of neural networks go beyond the simple ability to encode order parameters , extending to the detection of subtle differences in higher - order correlations functions . as a final demonstration of this </S>",
    "<S> , we examine an ising lattice gauge theory , one of the most prototypical examples of a topological phase of matter @xcite . </S>",
    "<S> the hamiltonian is given by @xmath28 where the ising spins live on the bonds of a two - dimensional square lattice with plaquettes @xmath29 , as shown in the inset of figure  [ fig : ice_tcode](c ) . </S>",
    "<S> the ground state is again a degenerate manifold @xcite ( figure  [ fig : ice_tcode](c ) ) , with exponentially - decaying spin - spin correlations that makes it much more difficult to distinguish from the high temperature phase .     </S>",
    "<S> filters with the spin configuration on each sublattice , followed by rectified linear units ( relu ) . </S>",
    "<S> the outcome is followed by fully - connected layer with 64 units and a softmax output layer . </S>",
    "<S> the green line represents the sliding of the maps across the configuration.,width=624 ]    just as in the square ice model , we have made an attempt to use the neural network in figure  [ fig : isingneural](a ) to classify the high- and low- temperature states in the ising gauge theory . </S>",
    "<S> a straightforward implementation of supervised training fails to classify a test set containing samples of the two states to an accuracy over @xmath30  equivalent to simply guessing . </S>",
    "<S> such failures typically occur because the neural network overfits to the training set . to overcome this difficulty we consider a convolutional neural network ( cnn )  @xcite which readily takes advantage of the two - dimensional structure of the input configurations , as well as the translational invariance of the model . </S>",
    "<S> the cnn in figure  [ fig : neural ] is detailed in the supplementary materials . </S>",
    "<S> we optimize the cnn using monte carlo configurations drawn from the partition function of the ising gauge theory at @xmath26 and @xmath31 . using this setting , </S>",
    "<S> the cnn successfully discriminates high - temperature from ground states with an accuracy of @xmath32 on a test set with @xmath33 configurations , in spite of the lack of an order parameter or qualitative differences in the spin - spin correlations . through the generation of new test sets that violate an extensive fraction of the local energetic constraints of the theory , we conclude that the discriminative power of the cnn relies on the detection of these satisfied constraints . </S>",
    "<S> furthermore , test sets with defects that retain most local constraints but disrupt non - local features , like the extended closed - loop gas picture or the associated topological degeneracy @xcite , indicate that local constraints are the only features that the cnn relies on for classification of the ground state . in view of these observations , </S>",
    "<S> we construct a simplified analytical toy model of our original cnn designed to explicitly exploit local constraints in the classification task . </S>",
    "<S> such a model discriminates high - temperature from ground states with an accuracy of @xmath32 . </S>",
    "<S> details of the behavior of the cnn with various test sets , as well as the details of the analytical model , are contained in the supplementary material .    </S>",
    "<S> we have shown that neural network technology , developed for engineering applications such as computer vision and natural language processing , can be used to encode phases of matter and discriminate phase transitions in correlated many - body systems . in particular , we have argued that neural networks encode information about conventional ordered phases by learning the order parameter of the phase , without knowledge of the energy or locality conditions of hamiltonian . </S>",
    "<S> furthermore , we have shown that neural networks can encode basic information about the ground states of unconventional disordered models , such as square ice model and the ising lattice gauge theory , where they learn local constraints satisfied by the spin configurations in the absence of an order parameter . </S>",
    "<S> these results indicate that neural networks have the potential to faithfully represent ground state wave functions . </S>",
    "<S> for instance , ground states of the toric code  @xcite can be represented by convolutional neural networks akin to the one in figure  [ fig : neural ] ( see the supplementary materials for details ) . </S>",
    "<S> we thus anticipate adoption to the field of quantum technology  @xcite , such as quantum error correction protocols and quantum state tomography  @xcite . </S>",
    "<S> the ability of machine learning algorithms to generalize to situations beyond their original design anticipates future applications such as the detection of phases and phase transitions in models vexed with the monte carlo sign problem  @xcite , as well as in experiments with single - site resolution capabilities such as the modern quantum gas microscopes  @xcite . </S>",
    "<S> as in all other areas of `` big data '' , we expect the rapid adoption of machine learning techniques as a basic research tool in condensed matter and statistical physics in the near future .    _ </S>",
    "<S> acknowledgments_. we would like to thank ganapathy baskaran , claudio castelnovo , anushya chandran , lauren e. hayward sierens , bohdan kulchytskyy , david schwab , miles stoudenmire , giacomo torlai , guifre vidal , and yuan wan for discussions and encouragement . </S>",
    "<S> we thank adrian del maestro for a careful reading of the manuscript . </S>",
    "<S> this research was supported by nserc of canada , the perimeter institute for theoretical physics , the john templeton foundation , and the shared hierarchical academic research computing network ( sharcnet ) . </S>",
    "<S> r.g.m .  acknowledges support from a canada research chair . </S>",
    "<S> research at perimeter institute is supported through industry canada and by the province of ontario through the ministry of research & innovation . </S>"
  ]
}