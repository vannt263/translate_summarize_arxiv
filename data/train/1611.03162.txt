{
  "article_text": [
    "the autonomous learning conjecture for the design of dynamical systems with predefined functionalities has been previously proposed by the authors @xcite .",
    "it extends the dynamics of a given system to a new one where the parameters are transformed to dynamical variables .",
    "the extended dynamical system then operates decreasing some cost function or error by varying the parameters through dynamics that include a delayed feedback and noise .",
    "the central feature of this idea is that the original variables and the parameters evolve simultaneously on different time scales . because of an intimate connection with the previous publication",
    ", we do not provide here a detailed introduction and literature review , all of which can be found in ref .",
    "@xcite .    the original formulation of the autonomous learning scheme works properly for systems where the cost functions are defined at all times during their evolution .",
    "however , many dynamical systems can not satisfy this restriction since they need a finite time interval in order to process a piece of information and produce its response . as a result ,",
    "the errors of such systems with respect to target functions are not defined during these processing intervals .",
    "examples of these systems are feed - forward neural networks @xcite , spiking neural networks @xcite , gene regulatory systems with adaptive responses @xcite , signal transduction networks @xcite , etc .    in order to treat these kinds of systems",
    "we propose to define an iterative map for the evolution of their parameters . between successive iterations ,",
    "a dynamical system is allowed to evolve during a long enough transient time for its error function to be evaluated . with the error",
    "thus computed we update the parameters in the next iteration according to our proposed autonomous learning scheme .",
    "this approach can be seen as an adiabatic realization of the original learning scheme for continuous - time evolution .    to illustrate the application of this new formulation we consider two systems : a feed - forward neural network and a kuramoto system of phase oscillators . with the former , we have a classic example of a system that needs a time interval to process some input signal and produce its corresponding output . since the system may classify several input patterns , the network does not only need a time to process each signal , it also needs to restart the process for each one of them .",
    "we show that this system can not only learn to classify a set of patterns but also to be robust against structural damages , namely , deletion of one of its nodes in the processing layer . in the case of the kuramoto system ,",
    "we repeat the problem of synchronization treated in our previous work @xcite , now using the new formalism .",
    "we then proceed to compare some aspects of the discrete and continuous - time approaches .",
    "the work is organized as follows . in section two",
    "we write down the original continuous - time autonomous learning scheme , followed by the proposed discrete - time approach . in section three",
    "we describe the two systems where we apply the new scheme . in section four",
    "we present our numerical investigations of these systems .",
    "finally , in the last section we discuss our results and present our conclusions .",
    "the continuous - time version of the autonomous learning scheme considers a simple dynamical system with @xmath0 variables @xmath1 , and @xmath2 parameters @xmath3 ) with dynamics given by    @xmath4    and an output function @xmath5 which defines the task the system is responsible of executing .",
    "we further define a cost function or error @xmath6 between the output function and some target performance @xmath7 as    @xmath8    finally , to allow for a self - directed ( or autonomous ) minimization of the deviation @xmath6 , we extend the original system ( [ equ_dynamical_system ] ) by defining a dynamics for the parameters @xmath9 as follows :    @xmath10    in this expression @xmath11 and @xmath12 are temporal differences at time @xmath13 and @xmath14 , where @xmath15 is a time delay .",
    "the constant @xmath16 fixes the time scale of the evolution of @xmath9 . in the last term",
    ", @xmath17 plays the role of a noise intensity , and @xmath18 are independent random white noises with @xmath19 and @xmath20 .    taken together ,",
    "equations ( [ equ_dynamical_system ] ) and ( [ equ_dynamical_system_parameters ] ) define an extended dynamical system with combined variables @xmath21 and @xmath22 , evolving according to two different characteristic times . in effect , if the time scale of subsystem ( [ equ_dynamical_system ] ) is taken as unity and @xmath23 , the dynamics of subsystem ( [ equ_dynamical_system_parameters ] ) will be slower .",
    "additionally , the time delay @xmath15 must satisfy @xmath24 .",
    "our conjecture is that , under an appropriate choice of @xmath16 , @xmath15 and @xmath17 , this autonomous system will evolve along an orbit in the space of variables @xmath22 which minimizes the system s deviation @xmath6 from the target performance . in @xcite",
    "we show that this learning method works properly for systems of oscillators where different levels of synchronization must be reached .",
    "equation ( [ equ_dynamical_system_parameters ] ) has been designed in order to perform the weight evolution aiming to reduce the error @xmath6 .",
    "the interpretation of the first term on the right side of this equation is the following .",
    "if , as a consequence of the delayed feedback ( memory ) , the system performance improves , i.e. @xmath25 , with the weight correction satisfying @xmath26 , then the weight @xmath27 should next decrease . if instead @xmath28 , @xmath27 should increase .",
    "the opposite behavior is obtained in the case that the system performance decays with @xmath29 .",
    "the second term on the right side of eq . ( [ equ_dynamical_system_parameters ] ) is a noise proportional to the error @xmath6 .",
    "its function is to keep the dynamics from being trapped in local minima , vanishing when the error is zero .",
    "another interpretation of eq .",
    "( [ equ_dynamical_system_parameters ] ) is that the weight evolution is controlled by a drift term ( first term on the right ) whose corrections are given by the memory ( feedback ) of the system .",
    "the second term is a stochastic exploration in parameter space proportional to the error @xmath6 . as a result , this dynamics can be seen as a competition between a drift term of intensity @xmath30 and a stochastic term of intensity @xmath31 . as we will later show , a proper balance between these two terms is needed in order to obtain successful evolutions .      for the application of ( [ equ_dynamical_system ] ) and ( [ equ_dynamical_system_parameters ] ) it is necessary that the error @xmath32 be defined for all @xmath13 . as we mentioned in the introduction",
    ", this restriction can not be satisfied by systems which need a finite amount of time @xmath33 to execute the task given by @xmath34 and , consequently , to yield a corresponding value for @xmath6 .",
    "our discrete - time autonomous learning approach for the optimization of such systems considers that during each time interval @xmath33 the system s parameters @xmath9 are fixed . after this transient",
    ", the system function @xmath35 assumes a value , allowing the error function @xmath6 to be evaluated and the parameters @xmath9 to be accordingly updated . taking each iteration step as comprising one complete processing interval @xmath33",
    ", we define an iterative map analogous to eq .",
    "( [ equ_dynamical_system_parameters ] ) as :    @xmath36    here , @xmath37 is the iteration index , @xmath38 and @xmath39 , with @xmath40 a delay given by a certain number of iterations .",
    "the other quantities are analogous to those of eq .",
    "( [ equ_dynamical_system_parameters ] ) .",
    "the constant @xmath41 determines the characteristic time for the evolution of the parameters and is equivalent to @xmath42 . in this way ,",
    "the new formulation replaces ( [ equ_dynamical_system_parameters ] ) with an iterative map as the new dynamics for @xmath9 , and allows the system to evolve according to ( [ equ_dynamical_system ] ) during an interval of time @xmath33 between successive iterations .",
    "in this section we present the two models we set out to design through optimization with the discrete - time autonomous learning scheme .",
    "we note that the goal of these models in this work is only to serve as examples of application and we do not pretend to analyze their properties in detail nor to compare our procedure with other methods of optimization .",
    "we focus only on the autonomous learning procedure .",
    "the first example we consider is a classical feed - forward artificial neural network @xcite able to classify bitmaps .",
    "this kind of system is prototypic for our interest .",
    "the neural network must process different signals with a fixed set of weights , requiring a finite time interval to compute the input information and retrieve its response . during this processing interval",
    "the error is not defined , making it difficult to implement the original scheme of autonomous learning .",
    "our model is defined as follows : a network @xmath43 has @xmath44 nodes in the input layer , @xmath45 nodes in the hidden layer and @xmath46 nodes in the output layer . a weight @xmath47 is associated with a directed connection from node @xmath48 in layer @xmath49 to node @xmath50 in layer @xmath51 .",
    "the dynamics @xmath52 of node @xmath50 in layer @xmath51 is    @xmath53    where the activation function @xmath54 is given by @xmath55 . for each node @xmath50 in layer @xmath51",
    ", we include a threshold value in the activation function by adding a weight @xmath56 from a threshold node @xmath57 to the node in question .",
    "the threshold node is always activated with @xmath58 .",
    "the neural network operates with a discrete - time dynamics . in the first iteration the input neurons read an input pattern @xmath59 and get their values . in the second iteration the neurons in the hidden layer",
    "compute their state as a function of the states of the input neurons . finally , in the third iteration the output nodes compute their states using the states of the neurons in the hidden layer . as a result ,",
    "the output layer yields the response of the network after it processes the input pattern @xmath59 .",
    "a network may repeat this @xmath60 times in order to compute the set of patterns @xmath61 .",
    "we thus define the error of the network as    @xmath62    in this expression @xmath63 and @xmath64 are the ideal and actual responses ( respectively ) of the output node @xmath50 when the pattern @xmath65 is processed by the network .      in order to construct functional networks ,",
    "i.e. , networks with small @xmath6 , we use our new discrete - time formalism of autonomous learning .",
    "we set the iterative map ( [ equ_discreta_parametros ] ) as the evolution law for the weights of the neural network . in order to use this equation we sort all the weights @xmath47 and @xmath66 in one linear array @xmath22 .",
    "we are also interested on retaining the functionality of a network when destructive mutations or damages alter the network structure .",
    "this property of structural robustness is a key feature of neural networks @xcite and several biological systems @xcite .",
    "our aim is to employ the autonomous learning scheme as well to improve the robustness of a given system .",
    "the following ideas concerning robustness have already been applied to flow processing networks @xcite .    consider a network @xmath43 with error @xmath6 . if we delete one if its @xmath45 nodes in the hidden layer , we get a new network @xmath67 with error @xmath68 . in general",
    ", the damage introduced will worsen the functionality of the original network @xmath43 , that is , we will have @xmath69 .",
    "we say that a damaged network is no longer functional if @xmath70 , where @xmath71 is thus defined as the maximum error below which a network is still considered functional .",
    "we can repeat this process of deletion for all nodes in the hidden layer to define the robustness @xmath72 of network @xmath43 as the ratio between the number of damaged functional networks and the total number of possible damages @xmath45 :    @xmath73 .",
    "\\label{equ_nn_robustnez}\\ ] ]    here , @xmath74 is the heaviside function with @xmath75 if @xmath76 and @xmath77 if @xmath78 .",
    "since our goal is the construction of functional and robust networks , we must aim for @xmath79 and @xmath80 .",
    "we propose the simultaneous optimization of these two quantities through the use of a biparametric autonomous learning scheme for the weights as follows :    @xmath81 \\nonumber \\\\                         & &    + \\big [ \\epsilon(n ) s_{\\epsilon }   + \\big(1-\\rho(n)\\big ) s_{\\rho } \\big]\\bm{\\xi}(n ) .",
    "\\label{equ_nn_pesos_robustnez }      \\end{aligned}\\ ] ]    in this expression , we have two constants @xmath82 and @xmath83 related to the drift term , and two constants @xmath84 and @xmath85 related to the noise intensity .",
    "it is directly seen that this biparametric prescription for the evolution of @xmath9 is essentially a superposition of ( [ equ_discreta_parametros ] ) as applied individually to the optimization of @xmath6 and @xmath86 .",
    "note that the robustness must be calculated in each iteration , and therefore the @xmath45 nodes are removed one by one in each iteration .      we take as a second example of a dynamical system the kuramoto model studied in our previous article @xcite , now applying the discrete - time autonomous learning prescription for the evolution of the coupling weights .",
    "the kuramoto model @xcite of coupled phase oscillators is described by equations    @xmath87    where @xmath88 is the phase and @xmath89 is the natural frequency of oscillator @xmath50 .",
    "the interactions are characterized by weights @xmath90 .",
    "they are symmetric , i.e. , @xmath91 , and can be positive or negative .    the synchronization of the system is quantified by the kuramoto order parameter    @xmath92    due to time fluctuations , we work with the mean order parameter @xmath93 defined as    @xmath94    where @xmath33 is the time interval we consider for its calculation .",
    "@xmath93 can vary between zero and one . in the case of full phase synchronization ,",
    "@xmath95 .",
    "the aim of this example is to construct a system of oscillators able to autonomously learn to reach a target order parameter @xmath96 .",
    "the error associated to this function is defined as    @xmath97    the discrete - time autonomous learning scheme dictates that the system ( [ equ_kuramoto ] ) evolve for a time @xmath33 after each learning iteration .",
    "the iterative map for the weights is a variation of eq .",
    "( [ equ_discreta_parametros ] ) for our system of oscillators :    @xmath98    note that we compute the weights @xmath90 ( with @xmath99 ) only for @xmath100 , because of the symmetry of the interactions .    as in our previous work ,",
    "we add a term to control the total interaction in the system . the mean absolute weight @xmath101 is defined as    @xmath102    the control parameter @xmath103 determines how strong the redistribution of the weights is , @xmath104 being the ideal absolute value of @xmath105 .    as a result of this dynamics",
    "the system of oscillators evolves to a mean order parameter @xmath106 by redistributing the weights and maintaining the total absolute value @xmath107 .",
    "we present several numerical studies of the proposed systems .",
    "we show examples of evolutions where the autonomous learning scheme is able to lead the system to the target states and we analyze particular aspects of it which arise in connection with each particular model .",
    "the neural network we work with must learn to classify the vowels , i.e. , there are @xmath108 input patterns @xmath109 .",
    "the letters are given as matrices of binary pixels , as shown in fig .",
    "[ fig_figure1].a .",
    "each pixel acts on an input neuron , with black squares indicating activation and white ones inactivation of the associated neuron .",
    "the network has @xmath110 input neurons , one hidden layer with @xmath111 nodes and @xmath112 output neurons . a schematic representation of the network is shown in fig .",
    "[ fig_figure1].b .",
    "our first experiment consists in the realization of a full evolution through the iterative map ( [ equ_discreta_parametros ] ) to construct a functional network able to classify the letters .",
    "figure [ fig_figure2].a presents a typical evolution of the error as function of the number of iterations ( blue curve ) .",
    "we observe that the error decreases to a relatively small value at the end of the simulation , indicating a proper average classification of the patterns .",
    "the learning parameters used in the simulation were @xmath113 , @xmath114 , and @xmath115 . as the initial conditions for the weights we set @xmath116 and @xmath117 .    as a matter of comparison , we add a standard back - propagation realization for a supervised learning for this system in fig .",
    "[ fig_figure2]a .",
    "we observe that the error as a function of the number of iterations ( black dashed curve ) converges to small values much faster than for the discrete - time autonomous learning scheme ( blue curve ) .",
    "this important difference in performance is due to the deterministic character of the back - propagation algorithm in contrast with the stochastic searching of the autonomous learning .",
    "the learning factor used in the back - propagation realization was @xmath118 .      as we have seen from the interpretations of eqs .",
    "[ equ_discreta_parametros ] and [ equ_dynamical_system_parameters ] , there is a competition between the drift term controlled by @xmath41 and the stochastic term of exploration controlled by @xmath17 . as a result ,",
    "there exist certain combinations of these two values @xmath119 and @xmath17 where the learning is optimum .",
    "generally , combinations which differ from such optimum ones result in failed evolutions where the system can not learn .",
    "the second numerical experiment is related to finding these optimum values for @xmath41 and @xmath17 , that is , those values for which the evolutions converge to the smallest values of @xmath6 in a fixed number of iterations . in order to find them ,",
    "we run several simulations with ensembles of @xmath120 networks , fixing for each ensemble the value of @xmath41 and @xmath17 , and evaluating the mean error @xmath121 over the ensemble after @xmath122 iterations .",
    "the results are shown in fig .",
    "3a , where it can be seen that there is a clear minimum of @xmath121 at @xmath123 .",
    "the previous study considered only five input patterns for classification .",
    "now , in order to get more robust results against the number of patterns to be classified , we consider @xmath124 input patterns with the same network characteristics .",
    "this set of @xmath124 patterns consists of the five vowels shown in fig .",
    "[ fig_figure1]a and the ten numerical digits from @xmath125 to @xmath126 .",
    "the number of output nodes is now @xmath124 . the mean error @xmath121 as a function of @xmath41 and @xmath17 is shown in fig .",
    "[ fig_figure3]b .",
    "we observe that almost the same error surface is found as in the previous study , with the same optimum values for @xmath41 and @xmath17 .",
    "we now consider the biparametric weight evolution given by eq .",
    "[ equ_nn_pesos_robustnez ] that aims to minimize the error @xmath6 and maximize the robustness @xmath86 .",
    "we set the values of @xmath82 and @xmath84 as the ones that guarantee the best convergence in the optimization of functionality , i.e. , those found in the previous study . performing a similar study to find the optimum parameters associated with the optimization of robustness",
    ", we found the minimum of @xmath127 at @xmath128 .",
    "for the functionality threshold we take @xmath129 .",
    "an evolution for this case is shown in fig .",
    "[ fig_figure2].b .",
    "we observe that at the beginning of the learning process the error @xmath6 is high and , as a result , the network has zero robustness . when the error is reduced and close to the threshold @xmath71 the learning of robustness is automatically turned on , owing to the fact that a damaged functional network has more chances to possess an error lower than the threshold as compared with a nonfunctional network with high error .",
    "as the evolution progresses , the error is kept below @xmath71 and the robustness increases until it reaches its optimum value .",
    "thus , the resulting network is functional and robust .     of the errors for four ensembles of networks , all normalized to unity .",
    "( a ) histogram @xmath130 for @xmath120 networks optimized only with respect to functionality .",
    "( b ) histogram @xmath130 for @xmath120 networks optimized with respect to both functionality and robustness .",
    "( c ) histogram @xmath131 for the ensemble of networks obtained by removal of hidden nodes from functional networks .",
    "( d ) histogram @xmath131 for the ensemble of networks obtained by removal of hidden nodes from robust networks .",
    "the vertical dashed lines indicate the functionality threshold value @xmath129 . ]    in order to fix the threshold value @xmath71 we proceed as in article @xcite .",
    "we optimize an ensemble of @xmath120 networks only by functionality ( testing ensemble ) , computing the resulting final errors and those of the associated damaged networks .",
    "the corresponding histograms of the errors @xmath6 are shown in fig .",
    "[ fig_figure4].a ( original ensemble ) and [ fig_figure4].c ( associated damaged ensemble ) respectively .",
    "we choose @xmath71 as the value for which the functional ensemble has a mean robustness @xmath132 .",
    "hence , increasing the mean robustness from @xmath132 to @xmath133 will represent a @xmath134 increase in robustness with respect to the testing ensemble .",
    "figures [ fig_figure4].b and [ fig_figure4].d show the histograms for an ensemble of @xmath120 networks ( original and damaged , respectively ) optimized to be functional _ and _ robust through the application of the biparametric autonomous learning scheme during @xmath135 iterations .",
    "the mean robustness of this ensemble is @xmath136 .",
    "this notable increase in robustness can be clearly seen by comparing the histograms corresponding to the damaged set of networks in each case ( fig .",
    "[ fig_figure4].c and [ fig_figure4].d ) . in the case of the ensemble of networks optimized solely with respect to functionality , the error window below the threshold",
    "@xmath71 is much less populated than in the case for the ensemble optimized to be both functional and robust .",
    "the relative increment in robustness of the latter with respect to the testing ensemble is approximately @xmath137 .",
    "this system presents an interesting characteristic concerning the error function .",
    "this function is evaluated by using the mean order parameter @xmath93 from eq .",
    "( [ equ_kuramoto_mean_order_parameter ] ) , that is , a mean value over a time interval @xmath33 . in the previous formulation for continuous - time parameter dynamics",
    "the error can be evaluated at any time . however , we are then forced to make a prescription concerning the set of weights which are most responsible for this mean value . in effect , during the interval @xmath33 the weights are continuously changing and it is therefore not clear which set of assumed values during @xmath33 are the effective ones in determining the error @xmath32 .",
    "the prescription we used then was that the error @xmath32 be related to the weights at time @xmath138 , that is , we considered that the weights at the beginning of the time interval @xmath33 are the ones responsible for the behavior of the system at the end of the interval .",
    "of course , we can always use a different prescription such as , for example , using the mean value of the weights over @xmath33 .",
    "this problem has been previously considered in a different implementation of reinforcement learning for spiking neural networks @xcite .",
    "a second problem related to this situation is that in the case of large enough @xmath33 , the correlation between the weights and the error is missing . at the same time , we need a long enough period @xmath33 in order to minimize the variations of @xmath93 .",
    "this problem can be avoided by using our new formulation with discrete - time evolution for the parameters .",
    "we take a full connected system with @xmath139 phase oscillators with natural frequencies @xmath140 ( @xmath141 ) .",
    "the integer delay is set to @xmath115 , and the target values to @xmath142 and @xmath143 .",
    "as the initial conditions , we set @xmath144 and the initial phases @xmath145 uniformly distributed between 0 and @xmath146 , which altogether results in the system s order parameter @xmath147 having an approximate initial value of @xmath148 .",
    "our aim is to increase the synchronization level to @xmath134 .",
    "the system is numerically integrated using an euler algorithm with time step @xmath149",
    ". note that between iterations of the learning algorithm the phases @xmath150 preserve their values , therefore the initial conditions are not restarted as in our previous example .",
    "we use this protocol to accelerate the simulations and to avoid transients as well as cases of multistability .",
    "figure [ fig_figure5 ] illustrates a typical successful evolution for this system .",
    "the weights of the system evolve according to eq .",
    "( [ equ_kuramoto_pesos ] ) . in this simulation",
    "we use @xmath151 , @xmath152 and @xmath153 . the time interval to compute the mean value @xmath93 is @xmath154 . in fig .",
    "[ fig_figure5].a we plot the error as a function of the number of iterations .",
    "we observe that it decreases from @xmath155 to @xmath156 in @xmath157 iterations .",
    "not all the realizations converge to small errors in this given number of iterations .",
    "a study of the learning efficiency is presented later on in this work .",
    "figure [ fig_figure5].b displays the mean absolute weight @xmath101 as a function of the number of iterations .",
    "we see that at the beginning of the learning process there are relatively strong fluctuations around the target weight @xmath104 .",
    "when the system is reaching low error values , these fluctuations vanish almost entirely as @xmath158 .     as a function of time .",
    "( b ) mean absolute weight @xmath101 as a function of time .",
    "the target weight value @xmath143 is shown with a black dashed line .",
    "( c ) and ( d ) order parameters @xmath159 as a function of time for the initial and final systems respectively .",
    "black dashed lines show the target order parameter value @xmath142 . ]",
    "figures fig .",
    "[ fig_figure5].c and fig .",
    "[ fig_figure5].d show the order parameter @xmath159 as a function of time for the initial and the final systems , respectively .",
    "the simulation is performed for a time interval longer than @xmath33 .",
    "we observe the relative improvement in the behavior of the order parameter @xmath159 between the initial and final systems .",
    "the dashed black line shows the target value @xmath96 prescribed for the learning evolution .",
    "we see that , for the final system , @xmath159 varies consistently around @xmath96 .",
    "controlling the total absolute weight @xmath101 imposes a strong restriction on the learning process .",
    "effectively , the corresponding correcting term implies that the difference @xmath160 is distributed in proportion to the strength of the connections .",
    "this way of redistributing the weights opposes their differentiation and , in general , resists the heterogeneities needed in order to find a solution .",
    "the stronger the correction by @xmath103 , the more difficult it is to reach small errors .     (",
    "a ) and mean absolute weight @xmath161 ( b ) as a function of @xmath103 for ensembles of networks after the learning process .",
    "errors bars indicate the dispersion of the distributions . ]",
    "figure [ fig_figure6].a shows the mean order parameter @xmath162 as a function of @xmath103 for ensembles of @xmath120 networks after the learning process .",
    "each evolution is done with @xmath163 iterations , @xmath164 , @xmath165 and @xmath152 . in figure",
    "[ fig_figure6].b we show the mean absolute weight @xmath161 of these ensembles as a function of @xmath103 . we observe that , for large values of @xmath103 , the learning scheme can not find good solutions and the mean value @xmath162 is far from the target value @xmath96 . however , the mean absolute weight @xmath161 is near its target @xmath104 with very small dispersion .",
    "the opposite situation is found for small values of @xmath103 .",
    "there , the mean order parameter @xmath162 is close to the target value @xmath96 with small dispersion , but the mean absolute weight @xmath161 is much larger than @xmath104 . as a compromise between these two tendencies , we may settle with @xmath166 .",
    "in such a case , we find that @xmath167 of the optimized networks have @xmath168 .",
    "for this sub - ensemble of networks , we have @xmath169 and @xmath170 .",
    "thus , the efficiency of the learning process to find acceptable solutions is about @xmath171 .",
    "we now study the dependence of the learning process on the time interval @xmath33 .",
    "as done in the previous analysis , we optimize several ensembles of networks , this time fixing @xmath153 and varying @xmath33 .",
    "we keep the previous values for the rest of the learning parameters .",
    "figures [ fig_figure7].a and [ fig_figure7].b show , respectively , the mean order parameter @xmath162 and the mean absolute weight @xmath161 as a function @xmath33 .",
    "the average values are computed only with successful learning cases , i.e. , systems with @xmath168 .",
    "thus , the number of averaged systems can be different for any two points , but approximately close to @xmath172 .",
    "( a ) and mean absolute weight @xmath161 ( b ) as a function of @xmath33 for ensembles of networks after the learning process .",
    "errors bars indicate the dispersion in each ensemble . ]",
    "we observe that , for any value of @xmath33 , we can find networks with order parameters close to the target @xmath96 .",
    "however , for short periods of time @xmath33 the fluctuations are strong and the weight restriction does not work properly . as a result",
    ", the total absolute weight @xmath161 grows to values much larger than @xmath104 .",
    "when we increase @xmath33 we observe that the weight control operates correctly and all the solutions approach @xmath173 .",
    "additionally , it is interesting to note that the learning scheme works well for relatively short time windows of @xmath174 , considering that in our previous work @xcite with the continuous - time version of autonomous learning we worked with a much longer time interval @xmath164 .",
    "this result indicates that the discrete - time formulation is more efficient than the continuous version in terms of convergence speed .",
    "this fact can be understood by considering that the weights values are fixed during the interval @xmath33 in the new formulation and the error can be better estimated that way .",
    "in this work we presented a new formulation for the autonomous learning scheme by defining an iterative map for the evolution of the parameters of a dynamical system .",
    "the utility of this discrete - time formulation resides in including within the scope of application of the autonomous learning scheme those systems which need an intrinsic time interval of finite duration to process a unit amount of information and therefore can not measure a cost function at all times during their dynamics .",
    "the first system treated , a feed - forward neural network responsible for classifying several input patterns , is a typical example of a system subject to this restriction .",
    "we showed that our learning scheme works properly for this system , with the resulting networks able to classify the assigned patterns .",
    "furthermore , we showed that we can implement the discrete - time scheme in biparametric form by setting a double feed - back signal in the evolution prescription for the weights , allowing us to optimize the networks with respect to the error @xmath6 and the robustness @xmath86 simultaneously .",
    "our results show that the final systems can in this manner improve their robustness @xmath137 with respect to a testing ensemble .",
    "it is important to mention that from the point of view of machine learning @xcite , the autonomous learning scheme can be classified as a type of reinforcement learning .",
    "these methods are characterized by their slow convergence , mainly due to the stochastic exploration of parameter space . in our case , this exploration is carried out by the multiplicative noise . as a result , our method is found to converge slower than the classical back - propagation algorithm , which constitutes a form of supervised learning by gradient descent .    in the second example",
    ", a system of phase oscillators , we showed that the discrete - time formulation of autonomous learning can help to avoid the inherent fluctuations of the error function generated by the dynamics of a continuous - time dynamical system ."
  ],
  "abstract_text": [
    "<S> we present a discrete - time formulation for the autonomous learning conjecture . </S>",
    "<S> the main feature of this formulation is the possibility to apply the autonomous learning scheme to systems in which the errors with respect to target functions are not well - defined for all times . </S>",
    "<S> this restriction for the evaluation of functionality is a typical feature in systems that need a finite time interval to process a unit piece of information . </S>",
    "<S> we illustrate its application on an artificial neural network with feed - forward architecture for classification and a phase oscillator system with synchronization properties . </S>",
    "<S> the main characteristics of the discrete - time formulation are shown by constructing these systems with predefined functions . </S>"
  ]
}