{
  "article_text": [
    "with the dramatic increase of smart mobile devices , as well as diversified services and applications , we are in the era of data deluge @xcite . meanwhile , with the emerging applications empowered by the internet of things ( iot ) and tactile internet , massive devices will need to get connected , which calls for ultra - low latency , high availability , reliability and security communications @xcite .",
    "however , with the low latency and high data rate requirements , the communication systems are placed under tremendous pressure to accommodate increasingly large data sets and to efficiently deliver the content . to resolve the big data challenge in communication networks , side information plays a pivotal role for both the wired and wireless communication links to deliver messages to users @xcite .",
    "that is , users can access to the messages as the side information that requested by other users . for instance",
    ", this scenario arises in the cache enabled fog radio access networks ( fog - ran ) @xcite . in this network architecture",
    ", the content can be stored in the caches or other storage elements , e.g. , the fog data center , radio access points and the mobile devices .",
    "the cached content may be requested by other users or in the further , thereby providing side information for message delivery in wired and wireless communications @xcite .",
    "index coding provides a powerful framework to model the communication scenarios with side information @xcite . although it has been shown that the index coding problem is related to many challenging problems ( e.g. , distributed storage , topological interference management @xcite and network coding @xcite ) , the index coding problem itself remains open .",
    "most of works on index coding focus on how to exploit the fixed side information , thereby designing efficient message delivering strategies , e.g. , the interference alinement approach @xcite .",
    "in particular , in caching networks @xcite , the side information ( i.e. , massage placement ) can be designed , followed by the message delivery . however , the amount of the side information in caching networks is limited by the storage capacity in caching networks .    in this paper",
    ", we put forth a different viewpoint on the index coding problem by investigating the fundamental tradeoff between the amount of the side information and the achievable data rate .",
    "that is , the higher data rate comes from at the price of high storage size , yielding more side information . to achieve this goal",
    ", we propose a novel sparse and low - rank optimization framework to minimize the amount of side information to meet a data rate requirement .",
    "specifically , the sparsity of this model represents the amount of side information , while the low - rankness of this model represents the number of channel uses , i.e. , blocklength , which equals the inverse of the achievable data rate .",
    "although the sparse and low - rank models have recently been well - studied in signal processing and machine learning @xcite , the presented model for index coding is novel and can help reveal the fundamental tradeoff between the amount of side information and the achievable data rate .",
    "unfortunately , the resulting sparse and low - rank optimization problem raises a unique challenge due to a non - convex objective function ( @xmath0 ) and non - convex constraint ( rank ) . although the convex relaxation approach based on convex surrogates ",
    "@xmath1-norm and nuclear norm  can provide polynomial time complexity algorithms @xcite , this approach is inapplicable in our problem as it always return the identity matrix .",
    "another approach is based on alternating minimization by factorizing a fixed - rank matrix @xcite , accompanied with @xmath1-norm relaxation",
    ". however , this approach fails to yield good performance by inducing a less sparse solution and is computationally expensive using the off - the - shelf parser / solver cvx @xcite .    to address the limitations of the above methods",
    ", we propose a riemannian optimization algorithm @xcite to solve the resulting sparse and low - rank optimization problem .",
    "in particular , by exploiting the quotient manifold geometry of fixed - rank matrices @xcite , the riemannian optimization algorithm was proposed to solve the low - rank matrix completion problem for topological interference management @xcite .",
    "however , this algorithm can not be applied in our problem due to the additional affine constraint preserving the desired signals and the non - convex sparsity inducing objective .",
    "we thus propose a smooth sparsity inducing surrogate and regularize the affine constraint as a smooth least - squares term .",
    "the second - order trust - region method @xcite is further applied to the resulting optimization problem with smooth objective over fixed - rank manifold constraint .",
    "the proposed algorithm , which is implemented in the manifold optimization toolbox manopt @xcite , outperforms the alternating minimization algorithm in terms of implementation complexity and performance .",
    "simulation results demonstrate the appealing tradeoff between the sparsity and low - rankness of the model , thereby revealing the tradeoff between the amount of side information and the achievable data rate .",
    "we consider the communication networks ( e.g. , caching network @xcite ) with side information to help message delivery . to investigate the tradeoff between the amount of side information and the achievable data rate",
    ", we introduce an index coding modeling framework for communications with side information @xcite . specifically , we consider a multiple unicast index coding problem consists of a set of @xmath2 independent messages @xmath3 , and a set of @xmath2 destination nodes .",
    "the @xmath4-th destination desires message @xmath5 with side information index as @xmath6 and @xmath7 .",
    "let @xmath8 be the choice of a finite alphabet .",
    "the coding function @xmath9 for all the messages is given by @xmath10 , where @xmath11 is the sequence of symbols transmitted over @xmath12 channel uses . here",
    ", each message @xmath5 is a random variable uniformly distributed over the set @xmath13 with @xmath14 as an integer . at destination @xmath4 , the decoding function @xmath15 for the desired message @xmath5 is given by @xmath16 .",
    "the probability of decoding error is given by @xmath17 .",
    "define the above coding scheme as @xmath18 .",
    "if for every @xmath19 , for some @xmath8 and @xmath12 , there exists a coding scheme @xmath20 , such that @xmath21 , and the error probability @xmath22 , then the rate tuple @xmath23 is said to be achievable . note that the index coding capacity does not dependent on the field specification @xcite . in this paper ,",
    "the achievable scheme is restricted to the real field @xmath24 for linear coding schemes design to construct index codes over real field .",
    "consider a scalar linear index coding scheme , which sends one symbol for each message over @xmath12 channel uses .",
    "let @xmath25 and @xmath26 be the precoding vector and the decoding vector , respectively .",
    "the transmitted symbol sequence @xmath27 over @xmath12 channel uses in a linear coding scheme is given by @xmath28 , where @xmath29 is one symbol from @xmath24 representing @xmath5 .",
    "the decoding operation for message @xmath30 at destination @xmath31 is given by @xmath32    the above decoding operation is achieved by the following interference alignment condition @xcite : @xmath33 if the above interference alignment conditions ( [ c1 ] ) and ( [ c2 ] ) are satisfied over @xmath12 channel uses , the following data rate vector @xmath34 , can be achieved @xcite .",
    "therefore , the achievable sum data rate is given by @xmath35 .",
    "we shall give an example of caching network to illustrate the amount of side information and achievable data rate .",
    "specifically , we assume that the file library is a set of @xmath2 messages @xmath36 , where each has entropy @xmath37 bits .",
    "given the side information @xmath6 s after content placement , the amount of side information is @xmath38 bits , which measures the total storage size . in this case , we characterize the tradeoff between the following two important metrics :    * the amount of side information : @xmath39 ( normalized by @xmath37 ) ; * the achievable data rate : @xmath40 ( normalized by @xmath2 ) .    in general , the more side information is available , the higher data rate can be achieved .",
    "the index coding problem has shown to be related to the network coding problem @xcite , topological interference management problem @xcite , caching problem @xcite , as well as distributed storage problem .",
    "this paper thus can provide principles for all these important design problems by characterizing the tradeoff between the amount of the side information and the achievable data rate .",
    "in this section , we propose a unified sparse and low - rank modeling framework to investigate the tradeoffs between the amount of side information @xmath41 and the achievable data rate @xmath42 in the index coding problem .",
    "this is achieved by rewriting the interference alignment conditions ( [ c1 ] ) and ( [ c2 ] ) into a sparse minimization problem with a fixed - rank constraint and an affine constraint .",
    "let @xmath43 .",
    "define the @xmath44 matrix @xmath45 $ ] , we have the rank of matrix @xmath46 as @xmath47 . the achievable data rate ( normalized by @xmath2 ) is given by @xmath48 additionally , the sparsity of the matrix @xmath46 is given by @xmath49 .",
    "finally , the amount of side information ( normalized by @xmath37 ) is given by @xmath50 an example of the sparsity and low - rankness of the matrix @xmath46 for the index coding problem is shown in fig .",
    "[ tim_example ] . in this case , the amount of side information is given by @xmath51 , which equals @xmath52 by assuming that the unknown entries in the associated incomplete matrix are non - zero .    .",
    "( b ) the associated incomplete matrix representing the interference alignment conditions ( [ c1 ] ) and ( [ c2 ] ) . ]    from ( [ rate ] ) and ( [ side ] ) , we can see that , to characterize the tradeoff between the amount of side information ( i.e. , storage size ) and the achievable data rate , it is equivalent to characterize the tradeoff between the sparsity and low - rankness of the modeling matrix @xmath46 .",
    "specifically , we propose to solve the following sparse and low - rank optimization problem : @xmath53 where @xmath54 is a fixed rank value of matrix @xmath46 . by solving a sequence of the optimization problem @xmath55 via varying @xmath54 from @xmath56 to @xmath2 , we can reveal the tradeoff between the sparsity and low - rankness of matrix @xmath46 .",
    "the widely used @xmath1-norm and nuclear - norm relaxation method provides a computationally tractable algorithm for the sparse and low - rank optimization as follows @xcite : @xmath57 where @xmath58 is a regularized parameter , @xmath59 , and @xmath60 is the nuclear norm of @xmath61 , i.e. , it is defined as the summation of the singular values of @xmath61 . @xmath62 and @xmath60 are popular convex surrogates of @xmath63 and the rank constraint , respectively . unfortunately , since @xmath64 @xcite and @xmath65 , the problem ( [ slr ] ) always returns @xmath66 as solution , which clearly is not low rank .",
    "another approach is based on alternating minimization by factorizing the rank-@xmath54 matrix @xmath46 as @xmath67 , where @xmath68 and @xmath69 are full column rank matrices .",
    "consequently , problem @xmath55 is further relaxed as follows : @xmath70_{ii}=1 , \\forall i=1,\\dots , k , \\end{array}\\ ] ] where @xmath71_{ij}$ ] denotes the @xmath72-entry of a matrix .",
    "the alternating minimization algorithm for problem ( [ l1norm_factorized ] ) consists of alternatively solving for @xmath73 and @xmath74 while fixing the other factor . however , the alternating minimization algorithm fails to exploit the second - order information to improve the performance , i.e. , enhance sparsity in matrix @xmath46 .    in this paper , in order to enhance sparsity via exploiting the second - order information , we propose a riemannian optimization algorithm to approximately solve problem @xmath55 .",
    "in this section , we propose a riemannian optimization algorithm to solve problem @xmath55 . specifically , the @xmath0-norm is relaxed to the @xmath1-norm , resulting in the optimization problem : @xmath75 however , the intersection of rank constraint and the affine constraint is challenging to characterize .",
    "we , therefore , propose to solve ( [ l1norm ] ) in two steps . in the first step , we find a good sparsity pattern by considering a regularized version of ( [ l1norm ] ) . in the second step ,",
    "we refine the estimate obtained in the first step . in both of these steps ,",
    "the underlying step is an optimization problem over the set of fixed - rank matrices .",
    "the overall algorithm is presented in table [ tab : overall_algorithm ] .      in the first step ,",
    "we reformulate problem ( [ l1norm ] ) as the _ regularized _ problem : @xmath76 where @xmath77 is the regularization parameter and @xmath78 is the parameter that approximates @xmath79 with the smooth term @xmath80 that makes the objective function _ differentiable_.",
    "a very small @xmath78 leads to ill - conditioning of the objective function in ( [ eq : regularized_formulation ] ) .",
    "similarly , a larger @xmath81 induces more sparsity in @xmath46 .",
    "since we intend to obtain the sparsity pattern of the optimal @xmath82 , we set @xmath78 to a high value , e.g. , @xmath83 , to make the problem ( [ eq : regularized_formulation ] ) well conditioned .",
    "if @xmath84 $ ] is the solution of ( [ eq : regularized_formulation ] ) , then the sparsity pattern matrix @xmath85 $ ] is of size @xmath44 such that @xmath86 if @xmath87 and @xmath88 otherwise .",
    "once the sparsity pattern @xmath89 is determined by solving ( [ eq : regularized_formulation ] ) , the _ refining step _ translates into solving a rank - constrained _ matrix completion _ problem . to see this , note that we know the positions of zeros in the solution matrix ( from @xmath90 ) and that the diagonal entries are all @xmath56s",
    "consequently , computing the entries at other positions is _ equivalent _ to the problem @xmath91 where @xmath92 is the _ frobenius _ norm of a matrix and @xmath93 is the element - wise multiplication of the matrices @xmath89 and @xmath61 .",
    "additionally , the algorithm for ( [ eq : refining ] ) is initialized from @xmath94 , which is the solution of ( [ eq : regularized_formulation ] ) .",
    "|p8.2cm|    * finding sparsity partition : we solve the",
    "_ regularized _ formulation ( [ eq : regularized_formulation ] ) to identify a good _ sparsity _ pattern @xmath89 , which is a binary matrix of size @xmath95 with @xmath56s at non - zero positions and @xmath96s at zero positions . *",
    "refining : once the sparsity pattern @xmath89 is determined , we solve the matrix completion problem ( [ eq : refining ] ) with rank constraint to refine the estimate obtained from the regularized formulation solution . * both ( [ eq : regularized_formulation ] ) and ( [ eq : refining ] ) are solved with a riemannian trust - region algorithm on the set of fixed - rank matrices .",
    "+      the optimization problems ( [ eq : regularized_formulation ] ) and ( [ eq : refining ] ) are regularized _",
    "least - square _ optimization problems over the set of fixed - rank matrices .",
    "a rank-@xmath54 matrix @xmath97 is factorized as @xmath98 , where @xmath99 and @xmath100 are full column - rank matrices .",
    "such a factorization , however , is not unique as @xmath61 remains unchanged under the transformation of the factors @xmath101 for all non - singular matrices @xmath102 , the set of @xmath103 non - singular matrices .",
    "equivalently , @xmath104 for all non - singular matrices @xmath105 . as a result",
    ", the local minima of an objective function parameterized with @xmath106 and @xmath107 are not isolated on @xmath108 .    the classical remedy to remove this indeterminacy requires further ( triangular - like ) structure in the factors @xmath106 and @xmath107 . for example",
    ", lu decomposition is a way forward .",
    "in contrast , we encode the invariance map ( [ eq : symmetry_gh ] ) in an abstract search space by optimizing directly over a set of equivalence classes @xmath109 : = \\ { ( { { \\bm u}}{{\\bm m}}^{-1},{{\\bm v}}{{\\bm m}}^{t } ) : { { \\bm m } } \\in\\mathrm{gl}(r ) \\}.\\ ] ] the set of equivalence classes is termed as the _ quotient space _ and is denoted by @xmath110 where the total space @xmath111 is the product space @xmath112",
    ".    consequently , if an element @xmath113 has the matrix characterization @xmath114 , then ( [ eq : regularized_formulation ] ) and ( [ eq : refining ] ) are of the form @xmath115",
    "\\in \\mathcal{m}_r } & & f([x]),\\\\ \\end{array}\\ ] ] where @xmath116=[({{\\bm u}},{{\\bm v } } ) ] $ ] is defined in ( [ eq : equivalence - classes - balanced ] ) and @xmath117 is a _ smooth _ function on @xmath118 , but now induced ( with slight abuse of notation ) on the quotient space @xmath119 ( [ eq : quotient - balanced ] ) .",
    "the quotient space @xmath119 has the structure of a smooth _ riemannian _ quotient manifold of @xmath118 by @xmath120 @xcite .",
    "the riemannian structure conceptually transforms a rank - constrained optimization problem into an _ unconstrained _ optimization problem over the non - linear manifold @xmath119 .",
    "additionally , it allows to compute objects like gradient ( of an objective function ) and develop a riemannian trust - region algorithm on @xmath119 that uses second - order information for faster convergence @xcite .",
    "consider an equivalence relation @xmath121 in the _ total _ ( computational ) space @xmath122 .",
    "the quotient manifold @xmath123 generated by this equivalence property consists of elements that are _ equivalence classes _ of the form @xmath124 =   \\ { { y } \\in   { \\mathcal m } :   { y } \\sim   { x}\\}$ ] .",
    "equivalently , if @xmath116 $ ] is an element in @xmath125 , then its matrix representation in @xmath118 is @xmath126 .",
    "figure [ fig : manifold_optimization ] shows a schematic viewpoint of optimization on a quotient manifold .",
    "particularly , we need the notion of `` linearization '' of the search space , `` search '' direction and a way `` move '' on a manifold .",
    "below we show the concrete development of these objects that allow to do develop a second - order trust - regions algorithm on manifolds .",
    "since the manifold @xmath125 is an abstract space , the elements of its tangent space @xmath127 } ( \\mathcal{m}/\\sim)$ ] at @xmath116 $ ] also call for a matrix representation in the tangent space @xmath128 that respects the equivalence relation @xmath121 .",
    "equivalently , the matrix representation of @xmath127 } ( \\mathcal{m}/\\sim)$ ] should be restricted to the directions in the tangent space @xmath129 on the total space @xmath130 at @xmath131 that do not induce a displacement along the equivalence class @xmath116 $ ] .",
    "this is realized by decomposing @xmath132 into complementary subspaces , the _ vertical _ and _ horizontal _ subspaces such that @xmath133 .",
    "the vertical space @xmath134 is the tangent space of the equivalence class @xmath116 $ ] . on the other hand",
    ", the horizontal space @xmath135 , which is any complementary subspace to @xmath134 in @xmath136 , provides a valid matrix representation of the abstract tangent space @xmath127 } ( \\mathcal{m}/\\sim)$ ] ( * ? ? ?",
    "* section  3.5.8 ) .",
    "an abstract tangent vector @xmath137 } \\in t_{[x ] } ( \\mathcal{m}/\\sim)$ ] at @xmath138 $ ] has a unique element in the horizontal space @xmath139 that is called its _",
    "horizontal lift_. our specific choice of the horizontal space is the subspace of @xmath140 that is the _ orthogonal complement _ of @xmath134 in the sense of a riemannian metric ( an inner product ) .",
    "a particular riemannian metric on the total space @xmath111 that takes into account the symmetry ( [ eq : symmetry_gh ] ) imposed by the factorization model , and that is well suited to a least - squares objective @xcite , is @xmath141 where @xmath142 and @xmath143 .",
    "it should be noted that the tangent space @xmath136 has the matrix characterization @xmath144 .",
    "consequently , @xmath145 ( and similarly @xmath146 ) has the matrix representation @xmath147 .",
    "motivation for the metric ( [ eq : metric_gh ] ) comes from the fact that it is induced from a block approximation of the hessian of a least - squares objective function .",
    "similar idea has also been exploited in @xcite .",
    "once the metric ( [ eq : metric_gh ] ) is defined on @xmath148 , the development of the geometric objects required for second - order optimization follow @xcite .",
    "the matrix characterizations of the tangent space @xmath140 , vertical space @xmath149 , and horizontal space @xmath150 are straightforward with the expressions : @xmath151    apart from the characterization of the horizontal space , we need a linear mapping @xmath152 that projects vectors from the tangent space onto the horizontal space . projecting an element @xmath153 onto the horizontal space is accomplished with the operator @xmath154 where @xmath155 is uniquely obtained by ensuring that @xmath156 belongs to the horizontal space characterized in ( [ eq : horizontal_space_gh ] ) . finally , the expression of @xmath157 is @xmath158 .",
    "\\end{array}\\ ] ]      the choice of the metric ( [ eq : metric_gh ] ) and of the horizontal space ( as the orthogonal complement of @xmath159 ) turns the quotient manifold @xmath125 into a _",
    "riemannian submersion _ of @xmath160 ( * ? ? ?",
    "* section  3.6.2 ) .",
    "as shown in @xcite , this special construction allows for a convenient matrix representation of the gradient ( * ? ? ?",
    "* section  3.6.2 ) and the hessian ( * ? ? ?",
    "* proposition  5.3.3 ) on the quotient manifold @xmath125 .",
    "the riemannian gradient @xmath161 } f$ ] of @xmath9 on @xmath125 is uniquely represented by its horizontal lift in @xmath111 which has the matrix representation @xmath162 } f } \\\\ \\qquad \\quad = { \\mathrm{grad}}_x   f =   ( \\frac{\\partial f}{\\partial { { \\bm u } } } ( { { \\bm v}}^t{{\\bm v}})^{-1 } ,   \\frac{\\partial f}{\\partial { { \\bm v } } } ( { { \\bm u}}^t{{\\bm u}})^{-1 } ) , \\end{array}\\ ] ] where @xmath163 is the gradient of @xmath9 in @xmath118 and @xmath164 and @xmath165 are the _ partial derivatives _ of @xmath9 with respect to @xmath106 and @xmath107 , respectively .",
    "in addition to the riemannian gradient computation ( [ eq : riemannian_gradient ] ) , we also require the directional derivative of the gradient along a search direction .",
    "this is captured by a _ connection _",
    "@xmath166 , which is the _ covariant derivative _ of vector field @xmath145 with respect to the vector field @xmath146 .",
    "the riemannian connection @xmath167 } } \\eta_{[x]}$ ] on the quotient manifold @xmath125 is uniquely represented in terms of the riemannian connection @xmath168 in the total space @xmath111 ( * ? ? ?",
    "* proposition  5.3.3 ) which is @xmath169 } } { \\eta _ { [ x ] } } } = \\pi_{{x } } ( { { \\nabla}}_{{\\xi}_{x } } { \\eta}_{x}),\\ ] ] where @xmath137}$ ] and @xmath170}$ ] are vector fields in @xmath125 and @xmath171 and @xmath172 are their horizontal lifts in @xmath111 . here",
    "@xmath173 is the projection operator defined in ( [ eq : projection_gh ] ) .",
    "it now remains to find out the riemannian connection in the total space @xmath111 .",
    "we find the matrix expression by invoking the _ koszul _ formula ( * ? ? ?",
    "* theorem  5.3.1 ) . after a routine calculation ,",
    "the final expression is @xcite @xmath174 + \\left ( { { \\bm a}}_{{{\\bm u } } } , { { \\bm a}}_{{{\\bm v } } } \\right ) , \\   { \\rm where } \\\\",
    "{ { \\bm a}}_{{{\\bm u } } }   =   { \\eta}_{{{\\bm u}}}{{\\mathrm{sym } } } ( { \\xi}_{{{\\bm v } } } ^t { { \\bm v } } ) ( { { \\bm v}}^t{{\\bm v}})^{-1 }      + { \\xi}_{{{\\bm u}}}{{\\mathrm{sym } } } ( { \\eta}_{{{\\bm v } } } ^t { { \\bm v } } ) ( { { \\bm v}}^t{{\\bm v}})^{-1}\\\\    \\quad \\qquad - { { \\bm u}}{{\\mathrm{sym } } } ( { \\eta}_{{{\\bm v } } } ^t { \\xi}_{{{\\bm v } } } ) ( { { \\bm v}}^t{{\\bm v}})^{-1 } \\\\ { { \\bm a}}_{{{\\bm v } } }   =   { \\eta}_{{{\\bm v}}}{{\\mathrm{sym } } } ( { \\xi}_{{{\\bm u } } } ^t { { \\bm u } } ) ( { { \\bm u}}^t{{\\bm u}})^{-1 }      + { \\xi}_{{{\\bm v}}}{{\\mathrm{sym } } } ( { \\eta}_{{{\\bm u } } } ^t { { \\bm u } } ) ( { { \\bm u}}^t{{\\bm u}})^{-1 }   \\\\",
    "\\quad \\qquad - { { \\bm v}}{{\\mathrm{sym } } } ( { \\eta}_{{{\\bm u } } } ^t \\overline{\\xi}_{{{\\bm u } } } ) ( { { \\bm u}}^t{{\\bm u}})^{-1 }    \\end{array}\\ ] ] and @xmath175 $ ] is the euclidean directional derivative @xmath175 : = \\lim_{t \\rightarrow 0 } { ( { \\xi}_{{x } + t { \\eta}_{\\bar x } } - { \\xi}_{x})}/{t}$ ] . @xmath176 extracts the symmetric part of a square matrix , i.e. , @xmath177 .",
    "the directional derivative of the riemannian gradient in the direction @xmath137}$ ] is given by the _ riemannian hessian operator _",
    "@xmath178 } f [ \\xi _ { [ x]}]$ ] which is now directly defined in terms of the riemannian connection @xmath179",
    ". based on ( [ eq : riemannian_connection ] ) and ( [ eq : connection_total_space ] ) , the horizontal lift of the riemannian hessian in @xmath180 has the matrix expression : @xmath181 } f [ \\xi _ { [ x ] } ] = \\pi_{{x } } (   { { \\nabla}}_{{\\xi}_{x } } { { \\mathrm{grad } } _ { x } f }    ) , \\ ] ] where @xmath137 } \\in t_{[x ] } ( \\mathcal{m}/\\sim)$ ] and its horizontal lift @xmath182 .",
    "@xmath183 is the projection operator defined in ( [ eq : projection_gh ] ) .",
    "an iterative optimization algorithm involves computing a search direction ( e.g. , negative gradient ) and then `` moving in that direction '' .",
    "the default option on a riemannian manifold is to move along geodesics , leading to the definition of the _ exponential map_. because the calculation of the exponential map can be computationally demanding , it is customary in the context of manifold optimization to relax the constraint of moving along geodesics . to this end , we define _ retraction",
    "_ @xmath184 ( * ? ? ?",
    "* definition  4.1.1 ) . a natural update on the manifold",
    "@xmath118 is , therefore , based on the update formula @xmath185 , i.e. , defined as @xmath186 where @xmath187 is a search direction and @xmath188 .",
    "it translates into the update @xmath189 = [ r_x(\\xi_x)]$ ] on @xmath125 .",
    "analogous to trust - region algorithms in the euclidean space ( * ? ? ?",
    "* chapter  4 ) , trust - region algorithms on a riemannian quotient manifold with guaranteed superlinear rate convergence and global convergence have been proposed in ( * ? ? ?",
    "* chapter  7 ) . at each iteration",
    "we solve the _ trust - region sub - problem _ on the quotient manifold @xmath125 .",
    "the trust - region sub - problem is formulated as the minimization of the _ locally - quadratic _ model of the objective function .",
    "the concrete matrix characterizations of riemannian gradient ( [ eq : riemannian_gradient ] ) , riemannian hessian ( [ eq : riemannian_hessian ] ) , projection operator ( [ eq : projection_gh ] ) , and retraction ( [ eq : retraction_gh ] ) allow to use an _ off - the - shelf _ trust - region implementation on manifolds , e.g. , in manopt @xcite .",
    "in this section , we compare the proposed riemannian optimization algorithm in table [ tab : overall_algorithm ] with with the alternating minimization algorithm based on ( [ l1norm_factorized ] ) for the sparse and low - rank optimization problem @xmath55 . for the alternating minimization algorithm",
    ", we need to solve a sequence of subproblems with non - smooth @xmath1-norm objective and an affine constraint ( i.e. , linear programming problem ) for which we use cvx @xcite .",
    "the maximum number of iterations of the proposed alternating minimization algorithm is set to be @xmath190 .",
    "for the proposed riemannian algorithm , we set @xmath78 to a high value of @xmath83 . a good choice of @xmath81 is @xmath191 and is obtained by cross - validation . the riemannian algorithm in table [ tab : overall_algorithm ]",
    "is implemented in manopt @xcite .",
    "the maximum number of trust - region iterations is set to @xmath192 .",
    "the matlab codes are available at https://bamdevmishra.com/codes/indexcoding .",
    "consider a sparse and low - rank optimization problem @xmath55 with @xmath193 .",
    "( we consider a smaller size instance as cvx is too computationally expensive to run larger ones . )",
    "the achievable normalized data rate equals @xmath194 , and the amount of normalized side information equals @xmath195 , which measures the cache size .",
    "therefore , the sparsity and low - rankness tradeoff in figure [ slopt ] reveals the tradeoff between the amount of side information and the achievable data rate in the corresponding index coding problem .",
    "furthermore , figure [ slopt ] demonstrates that , by encoding the second - order information in the algorithm design , the trust - region riemannian algorithm can achieve sparser solutions than the alternating minimization algorithm .    , where sparsity is given by @xmath195 and the low - rankness is given by @xmath196 .",
    "in this paper , we proposed a new sparse and low - rank optimization modeling framework to characterize the tradeoff between the amount of the side information and the achievable data rate by revealing the sparsity and low - rankness tradeoff in the modeling matrix .",
    "a trust - region riemannian optimization algorithm was proposed to improve the performance by encoding the second - order information , as well as the quotient manifold geometry of the fixed - rank matrices in the search space .",
    "this is achieved by relaxing the @xmath0-norm as a smooth @xmath1-norm surrogate and regularizing the affine constraint with least - squares objective .",
    "simulation results revealed the fundamental tradeoff between the amount of side information and the achievable data rate in index coding problem .",
    "our framework is useful for important system design problems , e.g. , cache size allocation .",
    "a promising and interesting future research direction is theoretically characterizing the fundamental tradeoffs between storage size and the achievable data rate , i.e. , the sparsity and low - rankness tradeoff in the proposed modeling matrix .",
    "s.  oymak , a.  jalali , m.  fazel , y.  eldar , and b.  hassibi , `` simultaneously structured models with application to sparse and low - rank matrices , '' _ ieee trans .",
    "inf . theory _ ,",
    "61 , pp .  28862908 , may 2015 .",
    "z.  wen , w.  yin , and y.  zhang , `` solving a low - rank factorization model for matrix completion by a nonlinear successive over - relaxation algorithm , '' _ math .",
    "_ , vol .  4 , no .  4 , pp .",
    "333361 , 2012 ."
  ],
  "abstract_text": [
    "<S> side information provides a pivotal role for message delivery in many communication scenarios to accommodate increasingly large data sets , e.g. , caching networks . </S>",
    "<S> although index coding provides a fundamental modeling framework to exploit the benefits of side information , the index coding problem itself still remains open and only a few instances have been solved . in this paper , we propose a novel sparse and low - rank optimization modeling framework for the index coding problem to characterize the tradeoff between the amount of side information and the achievable data rate . </S>",
    "<S> specifically , sparsity of the model measures the amount of side information , while low - rankness represents the achievable data rate . </S>",
    "<S> the resulting sparse and low - rank optimization problem has non - convex sparsity inducing objective and non - convex rank constraint . to address the coupled challenges in objective and constraint , we propose a novel riemannian optimization framework by exploiting the quotient manifold geometry of fixed - rank matrices , accompanied by a smooth sparsity inducing surrogate . </S>",
    "<S> simulation results demonstrate the appealing sparsity and low - rankness tradeoff in the proposed model , thereby revealing the tradeoff between the amount of side information and the achievable data rate in the index coding problem . </S>"
  ]
}