{
  "article_text": [
    "image deblurring is a classical ill - conditioned problem in many fields of applied sciences , including astronomy imaging and biomedical imaging . during the recording of a digital image",
    ", blurring artifacts always arise due to some unavoidable causes , e.g. , the optical imaging system in a camera lens may be out of focus , in astronomy imaging the incoming light in the telescope may be slightly bent by turbulence in the atmosphere , and the same problem appears due to the diffraction of light in the fluorescence microscopy .",
    "mathematically , image blurring process in such applications can often be described as follows . for simplification",
    "we denote the @xmath0 image as a one - dimensional vector in @xmath1 by concatenating their columns .",
    "let @xmath2 be the original image .",
    "the degradation model is described by @xmath3 where @xmath4 is the observed image and @xmath5 is a linear blurring operator .",
    "since the linear operator @xmath6 can not be inverted , and @xmath7 is also possibly contaminated by random noises , the recovery of @xmath8 from the noisy version @xmath9 of the blurred observation @xmath7 is a ill - posed problem .",
    "variational image restoration methods based on the regularization technique are the most popular approach for solving this problem .",
    "typically , the variational model corresponds to solving the following minimization problem    @xmath10    where @xmath11 is a data fidelity term which is derived from the the noise distribution , and @xmath12 is a regularization term for imposing the prior on the unknown image @xmath8 .    generally , the data fidelity term controls the closeness between the original image @xmath8 and the observed image @xmath9 .",
    "it takes different forms depending on the type of noise being added .",
    "for example , it is well known that the @xmath13-norm fidelity    @xmath14    is used for the additive white gaussian noise .",
    "such fidelity term is mostly considered in literature for its good characterization of noise of a optical imaging system .",
    "however , non - gaussian noises are also presented in the real imaging , e.g. , poisson noise is generally observed in photon - limited images such as electronic microscopy @xcite , positron emission tomography @xcite and single photon emission computerized tomography @xcite . due to its important applications in medical imaging ,",
    "linear inverse problems in presence of poisson noise have received much interest in literature @xcite .",
    "the likelihood probability of the poisson noisy data @xmath9 is given by    @xmath15    based on the statistics of poisson noise and maximum a posterior ( map ) likelihood estimation approach , a generalized kullback - leibler ( kl)-divergence @xcite arises as the fidelity term for poisson deconvolution variational model , i.e. ,    @xmath16    besides the fidelity term , a regularization term is also needed to restrain the noise amplification and avoid other artifacts in the recovered image .",
    "a simple but efficient idea is to use sparse representation in some transform domain of the unknown image @xmath8 .",
    "the choice of transform domain is crucial to obtain a suitable solution , and one popular choice is the total variation ( tv ) @xcite due to the strong edge - preserving ability . in this case , we obtain the classical tv - kl model for poisson image deblurring :    @xmath17    where @xmath18 is a regularization parameter , and @xmath19 with @xmath20 is the total variation regularization .",
    "since the pixel values of images represent the number of discrete photons incident over a given time interval in this application , we demand that @xmath21 in model ( [ equ1.6 ] ) .",
    "another selection for the regularizer term is the wavelet tight framelets @xcite , which have also been proved to be efficient but may need more computational cost associated with the wavelet transform and inverse transform . in the last several years , the relationship between the total variation and wavelet framelet has also been revealed @xcite .    in this paper , we focus our attention on the tv - kl model ( [ equ1.6 ] ) . due to the complex form of the fidelity term ( [ equ1.5 ] ) , the ill - posed inverse problem in presence of poisson noise has attracted less interest in literature than their gaussian counterpart .",
    "recently , sawatzky et al .",
    "@xcite proposed an em - tv algorithm for poisson image deblurring which has been shown to be more efficient than earlier methods , such as tv penalized richardson - lucy algorithm @xcite .",
    "s. bonettini et al .",
    "@xcite also developed gradient projection methods for tv - based image restoration .",
    "later on , the augmented lagrangian framework @xcite , which has been successfully applied to various image processing tasks , has been used for solving the tv - kl model .",
    "in particular , in @xcite a very effective alternating direction method of multipliers ( admm ) called pidal was proposed for image deblurring in presence of poisson noise , where a tv denoising problem is solved by chambolle s algorithm in each iteration .",
    "it has been proved to be more efficient than the admm algorithm proposed in @xcite .",
    "the relation between the two admms with and without nested iteration has been analyzed in @xcite .",
    "although the augmented lagrangian methods have been shown to be very useful , inner iterations or inverse operators involving the linear operator @xmath6 and laplacian operator are required in each iteration . besides , at least three auxiliary variables , which may reduce the convergence speed of the iterative algorithm , need to be introduced in the augmented lagrangian method due to the fidelity term is non - quadratic . in order to further improve the efficiency of the augmented lagrangian method",
    ", alternating direction minimization methods based on the linearized technique have been widely investigated @xcite very recently .",
    "the key idea of these methods is to use the proximal linearized term instead of the whole or part of the augmented lagrangian function . as a result ,",
    "sub - minimization problems which have closed solutions are obtained in the iteration process . in literature @xcite , an efficient optimization algorithm using the linearized alternating direction method was proposed , and further applied to solve the tv minimization problem for multiplicative gamma noise removal .",
    "numerical examples demonstrate that it is more efficient than the augmented lagrangian algorithms in this application .",
    "the primal - dual hybrid gradient ( pdhg ) method proposed by zhu et al .",
    "@xcite is another efficient iterative algorithm .",
    "the core idea is to alternately update the primal and dual variables by the gradient descent scheme and the gradient ascend scheme .",
    "the recent study on variants of the original pdhg algorithm , and on the connection with the linearized version of admm reveals the equivalence relation between the two algorithms framework . for more details",
    "refer to @xcite and the references cited therein .",
    "however , in the previous linearized alternating direction methods , the second - order derivative ( or the hessian matrix ) of the objective function of the sub - minimization problem is just approximated by an identity matrix multiplied by some constant .",
    "this approximation is obviously not exact in most cases , and therefore may reduce the convergence speed of the iterative algorithms .",
    "in fact , from the numerical comparison shown in section [ sec4 ] we find that the convergence rate of the linearized alternating direction method proposed in @xcite is obviously influenced by the inexact linearized approximation while applying it to solve the tv - kl model for poisson image deblurring .",
    "it is observed that the computational efficiency of the linearized alternating direction is even lower than the previous augmented lagrangian algorithms such as the pidal algorithm .",
    "refer to the experiments below for details .",
    "the main contribution of this work is to propose a novel inexact alternating direction method utilizing the second - order information of the objective function .",
    "specifically , in one sub - minimization problem of the proposed algorithm , the solution is obtained by a one - step iteration , in a way reminiscent of newton descent methods .",
    "in other words , the second - order derivative of the corresponding objective function in the sub - minimization problem is just approximated by a proximal hessian matrix which can be computed easily , rather than a constant multiplied by the identity matrix .",
    "the improved iterative algorithm is proved to be more efficient than the current state - of - the - art methods with application to poisson image deblurring , including the pidal algorithm and the linearized alternating direction methods .",
    "the rest of this paper is organized as follows . in section [ sec2 ]",
    "we briefly review the recently proposed proximal linearized alternating direction ( plad ) method @xcite .",
    "in section [ sec3 ] , in order to overcome the drawback of the previous linearized alternating direction method , we develop an inexact alternating direction method based on the newton descent algorithm .",
    "the updating strategy of the proximal hessian matrix in the newton descent algorithm is also discussed , and then the convergence of the proposed algorithms is further investigated under certain conditions . in section [ sec4 ] the numerical examples on poisson image deblurring problem are reported to compare the proposed algorithms with the recent state - of - the - art algorithms .",
    "in this section , we briefly review the plad method proposed in @xcite , and through further investigation we find that the plad method can be regarded as a linearized version of another widely used iterative algorithm-primal - dual hybrid gradient algorithm ( pdhg ) , which was firstly proposed by zhu et.al @xcite .",
    "fist of all , we consider the following tv regularized minimization problem    @xmath22    where @xmath23^{n}$ ] .",
    "note that ( [ equ2.1 ] ) can also be reformulated as a constrained optimization problem as follows    @xmath24    the augmented lagrangian function for ( [ equ2.2 ] ) is given by    @xmath25    therefore , the well - known admm for solving ( [ equ2.2 ] ) can be formulated as    @xmath26    the solution @xmath27 of the first subproblem in ( [ equ2.4 ] ) satisfies the first - order optimality condition , i.e. , it is the solution of the following nonlinear system of equations : @xmath28 which has no closed solution .",
    "note that we have @xmath29 .",
    "therefore , the linearization of the convex function @xmath30 is adopted and the first subproblem of ( [ equ2.4 ] ) is simplified as    @xmath31    substituting the first subproblem in ( [ equ2.4 ] ) with ( [ equ2.5 ] ) , we obtain the plad algorithm proposed in @xcite , which is given by    @xmath32    note that the second - order information of the objective function is just approximated by @xmath33 , which is obviously inexact .",
    "choose @xmath34 in ( [ equ2.1 ] ) .",
    "then we obtain the plad algorithm for solving the tv - kl model as follows :    @xmath35    where @xmath36 denotes the projection onto the set @xmath37 . in order to avoid the special case of @xmath38 in the @xmath8-iteration step , we choose @xmath39 in the following experiments , which is also adopted for the proposed algorithms .",
    "the shrinkage operator is componentwise , i.e. , it is defined by @xmath40 where @xmath41 .",
    "the plad algorithm can also be regarded as a linearized version of the state - of - the - art pdhg proposed in @xcite .",
    "consider the minimization problem @xmath42 where both @xmath43 and @xmath44 are convex and lower - semicontinuous ( l.s.c . ) functions , and @xmath45 is a linear operator .",
    "the original pdhgmp in @xcite can be expressed as follows    @xmath46    where @xmath47 is the convex conjugate of @xmath44 .",
    "let @xmath48 , @xmath49 , @xmath50 and @xmath51 . then utilizing the celebrated moreau s identity @xcite @xmath52 equations ( [ equ2.7 ] )",
    "can be reformulated as @xmath53 choose @xmath54 , @xmath55 , @xmath56 , and replace @xmath11 by its linearized version @xmath57 .",
    "then we can easily deduce the plad algorithm shown in ( [ equ2.9 ] ) .",
    "consider the @xmath8-subproblem in ( [ equ2.4 ] ) .",
    "let @xmath58 it is easy to observe from the first formulas of ( [ equ2.9 ] ) and ( [ equ2.6 ] ) that the solution @xmath27 in the plad algorithm is obtained by    @xmath59    which implies that an approximate solution of the @xmath8-subproblem in ( [ equ2.4 ] ) is obtained by the projection gradient descent algorithm , which only utilizes the first - order information of the objective function , typically have a sub - linear convergence rate .",
    "it is well - known that newton or quasi - newton methods , which further utilize the hessian matrix of the objective function , have been presented with a super - linear convergence rate .",
    "this fact motivates us to design a more efficient algorithm based on the newton methods to obtain an approximate solution of the @xmath8-subproblem .",
    "here we adopt the expression of the newton descent algorithm , i.e. , the solution @xmath27 is obtained by an one - step projection newton descent algorithm as follows .",
    "@xmath60    where @xmath61 $ ] is the relaxed parameter .",
    "the iterative formula ( [ equ3.2rev2 ] ) can be reformulated as @xmath62 here we assume that the hessian matrix @xmath63 is inverse .    in what follows",
    ", we consider the special case of tv - kl model for poisson image deblurring . in this case",
    ", we have @xmath34 , and ( [ equ3.2 ] ) can be reformulated as    @xmath64    however , the computation of the inverse of the operator @xmath65 is difficult in the update formula ( [ equ3.3 ] ) . one simple strategy is to use a proximal hessian matrix @xmath66 , which is a block - circulant matrix with periodic boundary conditions and hence can be easily computed by fast fourier transforms ( ffts ) , instead of the original operator @xmath67 . in this situation , we obtain the following inexact alternating direction method based on the newton descent algorithm with adaptive parameters ( iadmnda ) :    @xmath68    in the proposed iadmnda algorithm ( [ equ3.4 ] ) , a parameter @xmath69 is used to approximate the term @xmath70 in the operator @xmath67 , and therefore @xmath67 is replaced by a simple block - circulant matrix @xmath71 .    in what follows",
    ", we further discuss the selection of the parameters @xmath69 and @xmath72 in the iadmnda algorithm .",
    "for the relaxed parameter @xmath72 , we choose it to satisfy that    @xmath73    for guaranteeing the convergence of the proposed algorithm . here",
    "@xmath74 in the poisson image deblurring problem , we always choose @xmath75 , and hence the condition ( [ equ3.3rev2 ] ) comes into existence while choosing @xmath72 monotone non - increasing and small enough . in this setting",
    ", the projection operator @xmath36 can be removed from the first formula of ( [ equ3.4 ] ) .    for the parameter @xmath69 ,",
    "one strategy is to update its value in the iterative step according to the widely used barzilai - borwein ( bb ) spectral approach @xcite .",
    "let @xmath76 , @xmath77 , and @xmath78 .",
    "the parameter @xmath69 is chosen such that @xmath79 mimics the hessian matrix @xmath80 over the most recent step .",
    "specifically , we require that    @xmath81    and immediately get    @xmath82    the whole process of the proposed algorithm is summarized as algorithm 1 .",
    "it is observed that the update of @xmath69 introduces the extra convolution operation including in @xmath83 .",
    "therefore , one simple strategy is to use a unchanged value for @xmath69 during the iteration , i.e. , @xmath84 , where @xmath85 is a constant . in this setting",
    ", we abbreviate the proposed algorithm as iadmnd .    observation @xmath9 ; regularization parameter @xmath86 ; parameters @xmath87 and @xmath88 ; inner iteration number @xmath89 . + * initialization * : @xmath90 ; @xmath91 ; @xmath92 ; @xmath93 ; @xmath94 ; @xmath95 .",
    "+ * iteration * : +  ( 1 ) update @xmath8 : +  @xmath96 ; +  update @xmath72 according to ( [ equ3.3rev2 ] ) ; +  @xmath97 ; +  ( 2 ) update @xmath98 : +  @xmath99 ; +  ( 3 ) update @xmath100 : +  @xmath101 ; +  update @xmath102 according to ( [ equ3.6 ] ) ; +  ( 4 ) @xmath103 ; + until some stopping criterion is satisfied . +",
    "* output * the recovered image @xmath104 .      in this subsection",
    ", we further investigate the global convergence of the proposed iadmnd(a ) algorithms for poisson image deblurring under certain conditions .",
    "the bound constrained tv regularized minimization problem ( [ equ2.2 ] ) can be reformulated as    @xmath105    where @xmath106 denotes the indicator function of set @xmath37 ,",
    "i.e. , @xmath107 if @xmath108 and @xmath109 otherwise .",
    "assume that @xmath110 is one solution of the above bound constrained optimization problem corresponding to tv - kl model with @xmath34 , and @xmath111 is the corresponding lagrangian multiplier .",
    "then the point @xmath112 is a karush - kuhn - tucker ( kkt ) @xcite point of problem ( [ equ3.50c ] ) , i.e. , it satisfies the following conditions :    @xmath113    where @xmath114 denotes the set of the subdifferential of @xmath106 at @xmath115 , and @xmath116 denotes the set of the subdifferential of @xmath117 at @xmath118 . from literature",
    "@xcite we know that @xmath114 is also equal to the normal cone @xmath119 at @xmath115 .",
    "besides , assume that the convex function @xmath11 satisfies : @xmath120 for any @xmath121 , where @xmath122 and @xmath123 are two positive constants ( the estimation of @xmath122 and @xmath123 is discussed at the end of this section ) .",
    "[ the1 ] ( the convergence of the proposed iadmnda algorithm ) let @xmath124 be the sequence generated by the iadmnda algorithm with @xmath125 , @xmath126 , @xmath127 and @xmath128 .",
    "then @xmath129 converges to a solution of the minimization problem ( [ equ2.1 ] ) .",
    "denote @xmath130 . according to the iterative formula with respect to @xmath8 we know that @xmath27 is the solution of the minimization problem @xmath131 therefore , the sequence @xmath124 generated by the iadmnda algorithm satisfies    @xmath132    where @xmath133 denotes the set of the subdifferential of @xmath117 at @xmath134 .",
    "due to @xmath135 is one solution of ( [ equ3.50c ] ) , it is also the kkt point that satisfies :    @xmath136    denote the errors by @xmath137 , @xmath138 , and @xmath139 .",
    "subtracting ( [ equ3.11 ] ) from ( [ equ3.10 ] ) , and taking the inner product with @xmath140 , @xmath141 and @xmath142 on both sides of the three equations , we obtain that    @xmath143    where @xmath144 , @xmath145 , @xmath146 , and @xmath147 .",
    "utilizing @xmath148 @xmath149 , and @xmath101 , we can reformulate the third equation of ( [ equ3.12 ] ) as @xmath150 therefore , summing three formulas in ( [ equ3.12 ] ) we can obtain that    @xmath151    due to @xmath152 ( @xmath153 $ ] , with @xmath154 $ ] denoting the line segment between @xmath115 and @xmath155 ) , we have @xmath156 besides , we also have @xmath157 , and @xmath158 based on the above three relations , expression ( [ equ3.14 ] ) can be reformulated as    @xmath159    due to @xmath160 , by the convexity of @xmath106 and the definition of the subdifferential we conclude that @xmath161 . similarly , by the convexity of the function @xmath117 we also have that @xmath162 . due to @xmath163 , we get    @xmath164    therefore , removing the first two non - negative terms in ( [ equ3.15 ] ) , and utilizing the inequalities ( [ equ3.17 ] ) we obtain that    @xmath165    according to the definition of @xmath72 in algorithm 1 , we know that @xmath72 is monotone non - increasing , and hence there exists @xmath166 such that @xmath167 . denote @xmath168 . by the boundedness of @xmath69 and @xmath167 we have that @xmath169 since @xmath170",
    ", we know that @xmath171 according to the condition ( [ equ3.21 ] ) . therefore , by the definition of @xmath172 we further have    @xmath173    based on ( [ equ3.18 ] ) and ( [ equ3.16 ] ) we immediately get    @xmath174    according to ( [ equ3.6rev2 ] ) we can easily conclude that , there exists some @xmath175 such that @xmath176 in the next , summing ( [ equ3.18 ] ) from some @xmath175 to @xmath177 we obtain that    @xmath178    which implies that    @xmath179    due to @xmath180 , we get @xmath181 due to @xmath128 , we further have @xmath182 , which implies that @xmath129 converges to a solution of the minimization problem ( [ equ2.1 ] )    [ the2 ] ( the convergence of the proposed iadmnd algorithm ) let @xmath124 be the sequence generated by the iadmnd algorithm with @xmath183 , and @xmath128 .",
    "then @xmath129 converges to a solution of the minimization problem ( [ equ2.1 ] ) .",
    "the proof is analogous to that presented in theorem [ the1 ] , and the only difference lies in that @xmath69 is replaced by a constant @xmath85 .",
    "here we neglect the proof due to limited space .    in the above proof , we observe that the constants @xmath122 and @xmath123 in ( [ equ3.21 ] ) are crucial for the convergence of the proposed algorithms , since they decide the range of the parameters @xmath69 in iadmnda algorithm and @xmath85 in iadmnd algorithm respectively .",
    "for the poisson image deblurring problem , we have @xmath34 .",
    "then we can obtain that    @xmath184    where @xmath185 denotes a diagonal matrix with diagonal elements of the components of @xmath8 .",
    "due to @xmath186 and @xmath187 are upper and lower bounds of @xmath188 , they are also some estimation of @xmath123 and @xmath122 . in this extreme case ,",
    "the value of @xmath123 is too large and the value of @xmath122 is too small , and thus the parameter @xmath69 can be too large , and the  step size \" @xmath189 can be too small .",
    "this may cause the proposed algorithms converge very slowly .",
    "therefore , similarly to @xcite , we use the average of the second derivative @xmath190 instead of the worst estimation of @xmath123 and @xmath122 , which implies that a smaller @xmath69 can be selected during the implementation of the proposed algorithms .",
    "assume that @xmath191 is a collection of the image region with @xmath192 .",
    "when @xmath193 is sufficiently close to the unknown image @xmath8 , @xmath194 where the third approximation equation uses the relation of @xmath195 , and the last approximation is obtained by the second - order taylor expansion of the function @xmath196 .",
    "the rough estimation of @xmath123 and @xmath122 shown in ( [ equ3.23 ] ) depends on the mean and variance of the unknown blurring image @xmath197 .",
    "however , in general , we have @xmath198 .",
    "therefore , @xmath123 and @xmath122 can be simply approximated by @xmath199 due to @xmath200 , which implies that larger @xmath85 is demanded for images with smaller image intensity ( corresponding to images with higher noise level ) .    besides , in the proof of the convergence of the proposed iadmnda algorithm , we demand that @xmath127 and @xmath201 for any @xmath202 .",
    "this condition can be satisfied by modifying the update formula of @xmath69 as follows : @xmath203 where @xmath204 is computed by the formula ( [ equ3.6 ] ) in the @xmath205-th iteration , and @xmath206 is a large positive number . however , in our experiments we observe that the iadmnda algorithm still converges without the monotone decreasing condition @xmath127 .",
    "in fact , the iadmnda algorithm using the new update strategy in ( [ equ3.24e ] ) can not obviously improve the convergence speed compared with the counterpart using the original update strategy in ( [ equ3.6 ] ) .",
    "in this section , we evaluate the performance of the proposed algorithms by numerical experiments on poissonian image deblurring problem .",
    "first , the convergence of the proposed algorithms , which has been investigated in section [ sec3 ] under certain conditions , is further verified through several experiment examples , and meanwhile the influence of the parameter @xmath69 on the rate of convergence is also investigated .",
    "second , the proposed algorithms are compared with the widely used augmented lagrangian methods for poisson image restoration @xcite and the recently proposed plad algorithm @xcite , which can also be understood from the view of the linearized pdhg algorithm .    the codes of proposed algorithms and methods used for comparison are written entirely in matlab , and all the numerical examples are implemented under windows xp and matlab 2009 running on a laptop with an intel core i5 cpu ( 2.8 ghz ) and 8 gb memory . in the following experiments , six standard nature images ( see figure [ fig4.1 ] ) , which consist of complex components in different scales and with different patterns , are used for our test . among them , the size of the boat image is @xmath207 , and the size of other images is @xmath208 .",
    "+      in the proposed iadmnda algorithm , there are two parameters needed to be manually adjusted .",
    "one is the regularization parameter @xmath86 , the other is the penalty parameter @xmath88 .",
    "it is well - known that @xmath86 is decided by the noise level , and the value of @xmath88 does influence the convergence speed of the proposed algorithm .",
    "here we use the strategy similarly to that adopted in @xcite to choose @xmath88 , i.e. , we set @xmath209 , where @xmath210 denotes the maximum intensity of the original image .",
    "moreover , for the step parameter @xmath72 , we choose it to be the largest value to satisfy the condition ( [ equ3.3rev2 ] ) .    in what follows",
    ", we further investigate influence of the parameter @xmath85 on the rate of convergence of proposed iadmnd algorithm .",
    "two images named  cameraman \" and  barbara \" ( see figure [ fig4.1 ] ) are used for the test . here",
    "we consider two types of blur effects with different levels of poisson noise : the cameraman image is scaled to a maximum value of @xmath211 and @xmath212 respectively , and blurred with a @xmath213 uniform blur kernel ; the barbara image is scaled to the same range and convoluted by a @xmath214 gaussian kernel of unit variance .",
    "then the blurred images are contaminated by poisson noise .",
    "figure [ fig4.2 ] depicts the evolution curves of the relative error @xmath215 with different @xmath85 values .",
    "it is observed that the value of the parameter @xmath85 does influence the convergence speed . generally speaking ,",
    "the proposed algorithm with a small @xmath85 converges faster .",
    "however , if @xmath85 is too small , the convergence can not be guaranteed . in our experiments , we observe that @xmath216 is not suitable for images with @xmath217 , and in this case the iadmnd algorithm become unstable .",
    "this is also consistent with the analysis in section [ subsec3.2 ] . besides",
    ", the plots in figure [ fig4.2 ] implicitly verify that the convergence of the proposed iadmnd algorithm is really guaranteed with suitable values of @xmath85 .    in the proposed iadmnda algorithm",
    ", we use the formula ( [ equ3.6 ] ) to update the parameter @xmath69 in each iteration , and hence we further discuss the selection of the initial value @xmath218 .",
    "two images called  cameraman \" and  bridge \" ( see figure [ fig4.1 ] ) are adopted here .",
    "figure [ fig4.3 ] shows the evolution curves of snr ( signal - to - noise ratio ) values with different @xmath87 .",
    "note that there is almost no difference between the results with different values of @xmath87 , except the snr values in the first several iteration steps .",
    "therefore , we set @xmath87 to be a fixed constant in the following experiments .    finally , we compare the performance of the iadmnda algorithms which use the update formulas ( [ equ3.6 ] ) and ( [ equ3.24e ] ) for @xmath69 respectively . in the formula ( [ equ3.24e ] ) , the values of @xmath122 and @xmath123 are estimated by @xmath219 table [ tab4.3 ] lists the snr values and the iteration number of the iadmnda algorithms with different update formulas for @xmath69 .",
    "here we use the parameters setting in table [ tab4.1 ] for the iadmnda algorithms , and the stopping criterion is defined such that the relative error is below some small constant , i.e. ,    @xmath220    here we choose @xmath221 . in this table ,",
    "the serial numbers  1 \" and  2 \" denote the results of the iadmnda algorithms with the update formulas ( [ equ3.6 ] ) and ( [ equ3.24e ] ) respectively , and @xmath222 denotes the snr values , iteration numbers in sequence .",
    "it is observed that the performances of algorithms with both update formulas are almost the same .",
    "therefore , in the following compared experiments , we use the update formulas ( [ equ3.6 ] ) for the iadmnda algorithm .",
    "[ htbp ]    [ tab4.3 ]      in this subsection , we further compare the proposed algorithms with the current state - of - the - art algorithms , including the pidal algorithm proposed in @xcite , and the recently proposed plad algorithm @xcite .",
    "note that several parameters are required to be manually adjusted in the compared algorithms : the regularization parameter @xmath86 , the penalty parameter @xmath88 for all these algorithms ; the step parameter @xmath85 ( see ( [ equ2.6 ] ) ) for the plad algorithm , and the parameter @xmath85 for the proposed iadmnd algorithm . through",
    "many trials we use the rules of thumb : @xmath88 in the pidal algorithm is set to @xmath223 , while in other algorithms it is chosen to be @xmath224 ; the initial value @xmath87 in the proposed iadmnda algorithm is fixed as @xmath225 ; the other parameters setting is summarized in table [ tab4.1 ] , found to guarantee the convergence and achieve satisfactory results . moreover",
    ", a rudin - osher - fatemi ( rof ) denoising problem is included in each iteration of the pidal algorithm , and it is solved by using a small and fixed number of iterations ( just 5 ) of chambolle s algorithm . for more details refer to @xcite .",
    "[ htbp ]    [ tab4.1 ]    in the following numerical experiments , the stopping criterion in ( [ equ4.2 ] ) is used for all the algorithms here .",
    "table [ tab4.2 ] lists the snr values , the number of iterations and cpu time of different algorithms for images with different blur kernels and noise levels . in this table ,  gaussian \" and  uniform \" denote a @xmath214 gaussian kernel of unit variance and a @xmath213 uniform blur kernel respectively .",
    "the two cases can be seen as examples of mild blur and strong blur . besides ,",
    " @xmath226 \" denotes the snr values , iteration numbers and cpu time in sequence . note that the iteration numbers of the pidal algorithm represent the outer iteration numbers .    from the results in table [ tab4.2 ] we observe that the proposed algorithms are much faster than the pidal and plad algorithms , and meanwhile the snr values of the recovered images achieved with the proposed algorithms are comparable to those achieved with the pidal and plad algorithms .",
    "therefore , it is verified that the strategy of using the proximal hessian matrix to approximate the second - order derivatives is more efficient than the simple approximation of an identity matrix multiplied by some constant in the plad algorithm .",
    "it is also noted that the iteration numbers of the iadmnda algorithm are the least in most cases .",
    "however , the update of @xmath69 generates extra computational cost in the iadmnda algorithm , which makes its implementation time longer than the iadmnd algorithm in some cases .",
    "figures [ fig4.4][fig4.9 ] show the recovery results of the pidal methods , the plad algorithm and the proposed iadmnd and iadmnda algorithms with respect to the cameraman , bridge and boat images respectively .",
    "it is observed that the visual qualities of images generated by these algorithms are more or less the same .    finally , we consider two mri images called  rkknee \" and  chest \" .",
    "table [ tab4.4 ] lists the snr values , the number of iterations and cpu time of different algorithms for images with different blur kernels and noise levels .",
    "the regularization parameter @xmath86 is set to @xmath227 and @xmath228 for images with @xmath229 and convoluted by gaussian and uniform blur kernels respectively .",
    "similarly , we notice that the proposed algorithms are the most efficient in the computational time .",
    "some recovery results are shown in figures [ fig4.10 ] .",
    "we find that the quality of recovery images by these algorithms is very similar .",
    "[ htbp ]    [ tab4.2 ]    [ htbp ]    [ tab4.4 ]",
    "in this article , through further analyze the drawback of the recently proposed linearization techniques for image restoration , we develop an inexact alternating direction method based on the proximal hessian matrix . compared with the existing algorithms ,",
    "the main difference is that the second - order derivative of the objective function is just approximated by a proximal hessian matrix in the proposed algorithm , rather than a identity matrix multiplied by a constant . besides",
    ", we also propose a strategy for updating the proximal hessian matrix .",
    "the convergence of the proposed algorithms is further investigated under certain conditions , and numerical experiments demonstrate that the proposed algorithms outperform the widely used linearized augmented lagrangian methods in the computational time .",
    "the work was supported in part by the national natural science foundation of china under grant 61271014 and 61401473 ."
  ],
  "abstract_text": [
    "<S> the recovery of images from the observations that are degraded by a linear operator and further corrupted by poisson noise is an important task in modern imaging applications such as astronomical and biomedical ones . </S>",
    "<S> gradient - based regularizers involve the popular total variation semi - norm have become standard techniques for poisson image restoration due to its edge - preserving ability . </S>",
    "<S> various efficient algorithms have been developed for solving the corresponding minimization problem with non - smooth regularization terms . in this paper , motivated by the idea of the alternating direction minimization algorithm and the newton s method with upper convergent rate </S>",
    "<S> , we further propose inexact alternating direction methods utilizing the proximal hessian matrix information of the objective function , in a way reminiscent of newton descent methods . </S>",
    "<S> besides , we also investigate the global convergence of the proposed algorithms under certain conditions . </S>",
    "<S> finally , we illustrate that the proposed algorithms outperform the current state - of - the - art algorithms through numerical experiments on poisson image deblurring .    image deblurring ; proximal hessian matrix ; inexact alternating direction method ; total variation ; poisson noise </S>"
  ]
}