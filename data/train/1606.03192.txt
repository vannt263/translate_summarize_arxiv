{
  "article_text": [
    "word embedding has gained popularity as an important unsupervised natural language processing ( nlp ) technique in recent years .",
    "the task of word embedding is to derive a set of vectors in a euclidean space corresponding to words which best fit certain statistics derived from a corpus .",
    "these vectors , commonly referred to as the _ embeddings _ , capture the semantic / syntactic regularities between the words .",
    "word embeddings can supersede the traditional one - hot encoding of words as the input of an nlp learning system , and can often significantly improve the performance of the system .",
    "there are two lines of word embedding methods .",
    "the first line is neural word embedding models , which use softmax regression to fit bigram probabilities and are optimized with stochastic gradient descent ( sgd ) .",
    "one of the best known tools is word2vec @xcite .",
    "the second line is low - rank matrix factorization ( mf)-based methods , which aim to reconstruct certain bigram statistics matrix extracted from a corpus , by the product of two low rank factor matrices .",
    "representative methods / toolboxes include hyperwords @xcite , glove @xcite , singular @xcite , and sparse @xcite .",
    "all these methods use two different sets of embeddings for words and their context words , respectively .",
    "svd based optimization procedures are used to yield two singular matrices .",
    "only the left singular matrix is used as the embeddings of words . however , svd operates on @xmath0 , which incurs information loss in @xmath1 , and may not correctly capture the _ signed correlations _ between words .",
    "an empirical comparison of popular methods is presented in @xcite .",
    "the toolbox presented in this paper is an implementation of our previous work @xcite .",
    "it is a new mf - based method , but is based on eigendecomposition instead .",
    "this toolbox is based on @xcite , where we estabilish a bayesian generative model of word embedding , derive a weighted low - rank positive semidefinite approximation problem to the pointwise mutual information ( pmi ) matrix , and finally solve it using eigendecomposition .",
    "eigendecomposition avoids the information loss in based methods , and the yielded embeddings are of higher quality than svd - based methods .",
    "however eigendecomposition is known to be difficult to scale up . to make our method scalable to large vocabularies",
    ", we exploit the sparsity pattern of the weight matrix and implement a divide - and - conquer approximate solver to find the embeddings incrementally .",
    "our toolbox is named _ positive - semidefinite vectors ( psdvec)_. it offers the following advantages over other word embedding tools :    1 .",
    "the incremental solver in psdvec has a time complexity @xmath2 and space complexity @xmath3 , where @xmath4 is the total number of words in a vocabulary , @xmath5 is the specified dimensionality of embeddings , and @xmath6 is the number of specified core words . note the space complexity does not increase with the vocabulary size @xmath4 .",
    "in contrast , other mf - based solvers , including the core embedding generation of psdvec , are of @xmath7 time complexity and @xmath8 space complexity .",
    "hence asymptotically , psdvec takes about @xmath9 of the time and @xmath10 of the space of other mf - based solvers , and space complexity @xmath11 , where @xmath12 is the number of word occurrences in the input corpus , and @xmath13 is the number of negative sampling words , typically in the range @xmath14 . ]",
    "; 2 .   given the embeddings of an original vocabulary , psdvec is able to learn the embeddings of new words incrementally . to our best knowledge ,",
    "none of other word embedding tools provide this functionality ; instead , new words have to be learned together with old words in batch mode .",
    "a common situation is that we have a huge general corpus such as english wikipedia , and also have a small domain - specific corpus , such as the nips dataset . in the general corpus , specific terms",
    "may appear rarely .",
    "it would be desirable to train the embeddings of a general vocabulary on the general corpus , and then incrementally learn words that are unique in the domain - specific corpus .",
    "then this feature of incremental learning could come into play ; 3 .   on word similarity / analogy benchmark sets and common natural language processing ( nlp ) tasks , psdvec produces embeddings that has the best average performance among popular word embedding tools ; 4 .",
    "psdvec is established as a bayesian generative model @xcite .",
    "the probabilistic modeling endows psdvec clear probabilistic interpretation , and the modular structure of the generative model is easy to customize and extend in a principled manner . for example , global factors like topics can be naturally incorporated , resulting in a hybrid model @xcite of word embedding and latent dirichlet allocation @xcite . for such extensions",
    ", psdvec would serve as a good prototype . while in other methods , the regression objectives are usually heuristic , and other factors are difficult to be incorporated .",
    "psdvec implements a low - rank mf - based word embedding method .",
    "this method aims to fit the @xmath15 using @xmath16 , where @xmath17 and @xmath18 are the empirical unigram and bigram probabilities , respectively , and @xmath19 is the embedding of @xmath20 .",
    "the regression residuals @xmath21 are penalized by a monotonic transformation @xmath22 of @xmath18 , which implies that , for more frequent ( therefore more important ) bigram @xmath23 , we expect it is better fitted .",
    "the optimization objective in the matrix form is @xmath24 where @xmath1 is the pmi matrix , @xmath25 is the embedding matrix , @xmath26 is the bigram probabilities matrix , @xmath27 is the @xmath28-weighted frobenius - norm , and @xmath29 are the tikhonov regularization coefficients .",
    "the purpose of the tikhonov regularization is to penalize overlong embeddings .",
    "the overlength of embeddings is a sign of overfitting the corpus .",
    "our experiments showed that , with such regularization , the yielded embeddings perform better on all tasks .",
    "is to find a weighted low - rank positive semidefinite approximation to @xmath1 .",
    "prior to computing @xmath1 , the bigram probabilities @xmath30 are smoothed using jelinek - mercer smoothing .",
    "a block coordinate descent ( bcd ) algorithm @xcite is used to approach , which requires eigendecomposition of @xmath1 .",
    "the eigendecomposition requires @xmath7 time and @xmath8 space , which is difficult to scale up . as a remedy",
    ", we implement an approximate solution that learns embeddings incrementally .",
    "the incremental learning proceeds as follows :    1 .",
    "partition the vocabulary @xmath31 into @xmath32 consecutive groups @xmath33 .",
    "take @xmath34 as an example .",
    "@xmath35 consists of the most frequent words , referred to as the * core words * , and the remaining words are * noncore words * ; 2 .   accordingly",
    "partition @xmath1 into @xmath36 blocks as @xmath37 partition @xmath28 in the same way .",
    "@xmath38 correspond to * core - core bigrams * ( consisting of two core words ) .",
    "partition @xmath25 into @xmath39[l]{$\\smash{\\smallunderbrace{\\phantom {     \\begin{matrix}\\boldsymbol{v}_{1}\\end{matrix}}}_{\\text{$\\boldsymbol{s}_{1}$}}}$}\\boldsymbol{v}_{1 } } & \\negthickspace {   \\makebox[0pt][l]{$\\smash{\\smallunderbrace{\\phantom {     \\begin{matrix}\\;\\boldsymbol{v}_{2}\\end{matrix}}}_{\\text{$\\boldsymbol{s}_{2}$}}}$}\\;\\boldsymbol{v}_{2 } } & \\negthickspace {   \\makebox[0pt][l]{$\\smash{\\smallunderbrace{\\phantom {     \\begin{matrix}\\;\\boldsymbol{v}_{3}\\end{matrix}}}_{\\text{$\\boldsymbol{s}_{3}$}}}$}\\;\\boldsymbol{v}_{3}}\\end{pmatrix}\\\\ \\rule{0pt}{15pt } \\end{array}$ ] ; 3 .   for core words ,",
    "set @xmath40 , and solve @xmath41 using eigendecomposition , obtaining core embeddings @xmath42 ; 4 .",
    "set @xmath43 , and find @xmath44 that minimizes the total penalty of the @xmath45-th and 21-th blocks ( the 22-th block is ignored due to its high sparsity ) : @xmath46 the columns in @xmath47 are independent , thus for each @xmath19 , it is a separate weighted ridge regression problem , which has a closed - form solution @xcite ; 5 .   for any other set of noncore words @xmath48 ,",
    "find @xmath49 that minimizes the total penalty of the @xmath50-th and @xmath51-th blocks , ignoring all other @xmath52-th and @xmath53-th blocks ; 6 .",
    "combine all subsets of embeddings to form @xmath54 . here",
    "our toolbox consists of 4 python / perl scripts : ` extractwiki.py ` , ` gramcount.pl ` , ` factorize.py ` and ` evaluate.py ` .",
    "figure 1 presents the overall architecture .",
    "= [ trapezium , trapezium left angle=75 , trapezium right angle=105 , minimum width=2 cm , minimum height=1 cm , text centered , draw = black , outer sep=1pt , inner xsep=-4pt , fill = blue!30 ] = [ rectangle , minimum width=3 cm , minimum height=1 cm , text centered , draw = black , outer sep=0 , inner sep=5pt , fill = orange!30 , rounded corners ] = [ diamond , minimum width=1 cm , minimum height=1 cm , text centered , inner sep=-2pt , draw = black , fill = green!30 ] = [ thick,->,>=stealth ]    ( corpus ) [ io ] corpus ; ( gramcount ) [ process , right of = corpus , xshift=2 cm , align = center ] extractwiki.py + gramcount.pl ; ( bigram ) [ io , below of = gramcount , yshift=0.3 cm ] bigram statistics ; ( factorize ) [ process , below of = bigram , yshift=0.3 cm , align = center ] factorize.py : we_factorize_em ( ) + factorize core block ; ( core ) [ io , right of = factorize , xshift=4 cm ] core embeddings ; ( more ) [ decision , below of = factorize , yshift=-0.7cm , text width=2 cm ] more noncore words ? ; ( concat ) [ process , below of = more , yshift=-1.2cm , text width=3 cm ] concatenate all embeddings ; ( gennoncore ) [ process , right of = more , xshift=3.5cm , text width=4 cm , align = center ] factorize.py : + block_factorize ( ) + solve noncore blocks ; ( noncore ) [ io , below of = gennoncore , yshift=-0.2 cm , minimum width=2 cm , inner xsep=-14pt , align = center ] noncore embeddings ; ( vec ) [ io , below of = concat , yshift=0.2 cm ] save to .vec ; ( eval ) [ process , xshift=3 cm , right of = vec , align = center ] evaluate.py + evaluate ; ( testset ) [ io , xshift=3 cm , right of = eval , xshift=-1 cm , align = center ] 7 datasets ; ( corpus )  ( gramcount ) ; ( gramcount )  ( bigram ) ; ( bigram )  ( factorize ) ; ( factorize )  ( core ) ; ( factorize )  ( more ) ; ( [ xshift=2.8em]core )  ( gennoncore ) ; ( more ) ",
    "node[anchor = east ] yes ( concat ) ; ( more ) ",
    "node[anchor = south ] no ( gennoncore ) ; ( concat ) ",
    "( vec ) ; ( gennoncore )  ( noncore ) ; ( noncore ) |- ( [ yshift=0.5em]concat ) ; ( core.east ) |- ( [ yshift=-1.5em]concat ) ; ( vec )  ( eval ) ; ( testset )  ( eval ) ; = [ rectangle , minimum width=3 cm , minimum height=1 cm , text centered , text width=3 cm , draw = black , fill = orange!30 ]    1 .   `",
    "extractwiki.py ` first receives a wikipedia snapshot as input ; it then removes non - textual elements , non - english words and punctuation ; after converting all letters to lowercase , it finally produces a clean stream of english words ; 2 .   `",
    "counts the frequencies of either unigrams or bigrams in a word stream , and saves them into a file . in the unigram mode ( ` -m1 ` ) , unigrams that appear less than certain frequency threshold are discarded . in the bigram mode ( ` -m2 ` ) , each pair of words in a text window ( whose size is specified by ` -n ` ) forms a bigram .",
    "bigrams starting with the same leading word are grouped together in a row , corresponding to a row in matrices @xmath26 and @xmath1 ; 3 .   `",
    "factorize.py ` is the core module that learns embeddings from a bigram frequency file generated by ` gramcount.pl ` .",
    "a user chooses to split the vocabulary into a set of core words and a few sets of noncore words . `",
    "can : 1 ) in function ` we_factorize_em ( ) ` , do bcd on the pmi submatrix of core - core bigrams , yielding _ core embeddings _ ; 2 ) given the core embeddings obtained in 1 ) , in ` block_factorize ( ) ` , do a weighted ridge regression w.r.t . _ noncore embeddings _ to fit the pmi submatrices of core - noncore bigrams .",
    "the tikhonov regularization coefficient @xmath29 for a whole noncore block can be specified by ` -t ` . a good",
    "rule - of - thumb for setting @xmath29 is to increase @xmath29 as the word frequencies decrease , i.e. , give more penalty to rarer words , since the corpus contains insufficient information of them ; 4 .   ` evaluate.py ` evaluates a given set of embeddings on 7 commonly used testsets , including 5 similarity tasks and 2 analogy tasks .",
    "the python scripts use numpy for the matrix computation .",
    "numpy automatically parallelizes the computation to fully utilize a multi - core machine .    the perl script ` gramcount.pl ` implements an embedded c++ engine to speed up the processing with a smaller memory footprint .",
    "our competitors include : * word2vec * , * ppmi * and * svd * in hyperwords , * glove * , * singular * and * sparse*. in addition , to show the effect of tikhonov regularization on `` psdvec '' , evaluations were done on an unregularized psdvec ( by passing `` ` -t 0 ` '' to ` factorize.py ` ) , denoted as * psd - unreg*. all methods were trained on an 12-core xeon 3.6ghz pc with 48 gb of ram .",
    "we evaluated all methods on two types of testsets .",
    "the first type of testsets are shipped with our toolkit , consisting of 7 word similarity tasks and 2 word analogy tasks ( luong s rare words is excluded due to many rare words contained ) . 7 out of the 9 testsets are used in @xcite .",
    "the hyperparameter settings of other methods and evaluation criteria are detailed in @xcite .",
    "the other 2 tasks are toefl synonym questions ( * tfl * ) @xcite and rubenstein & goodenough ( * rg * ) dataset @xcite . for these tasks ,",
    "all 7 methods were trained on the apri 2015 english wikipedia .",
    "all embeddings except `` sparse '' were 500 dimensional .",
    "`` sparse '' needs more dimensionality to cater for vector sparsity , so its dimensionality was set to 2500 .",
    "it used the embeddings of word2vec as the input . in analogy tasks ` google ` and ` msr ` , embeddings were evaluated using 3cosmul @xcite .",
    "the embedding set of psdvec for these tasks contained 180,000 words , which was trained using the blockwise online learning procedure described in section 5 , based on 25,000 core words .",
    "the second type of testsets are 2 practical nlp tasks for evaluating word embedding methods as used in @xcite , i.e. , named entity recognition ( * ner * ) and noun phrase chunking ( * chunk * ) .",
    "following settings in @xcite , the embeddings for nlp tasks were trained on reuters corpus , volume 1 @xcite , and the embedding dimensionality was set to 50 ( `` sparse '' had a dimensionality of 500 ) .",
    "the embedding set of psdvec for these tasks contained 46,409 words , based on 15,000 core words .",
    "[ cols=\"^,^,^,^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]",
    "in this example , we train embeddings on the english wikipedia snapshot in april 2015 .",
    "the training procedure goes as follows :    1 .",
    "use ` extractwiki.py ` to cleanse a wikipedia snapshot , and generate ` cleanwiki.txt ` , which is a stream of 2.1 billion words ; 2 .",
    "use ` gramcount.pl ` with ` cleanwiki.txt ` as input , to generate ` top1grams-wiki.txt ` ; 3 .",
    "use ` gramcount.pl ` with ` top1grams-wiki.txt ` and ` cleanwiki.txt ` as input , to generate ` top2grams-wiki.txt ` ; 4 .",
    "use ` factorize.py ` with ` top2grams-wiki.txt ` as input , to obtain 25000 core embeddings , saved into ` 25000-500-em.vec ` ; 5 .",
    "use ` factorize.py ` with ` top2grams-wiki.txt ` and ` 25000-500-em.vec ` as input , and tikhonov regularization coefficient set to 2 , to obtain 55000 noncore embeddings .",
    "the word vectors of totally 80000 words is saved into ` 25000-80000-500-blkem.vec ` ; 6 .",
    "repeat step 5 twice with tikhonov regularization coefficient set to 4 and 8 , respectively , to obtain extra @xmath56 noncore embeddings . the word vectors are saved into ` 25000-130000-500-blkem.vec ` and ` 25000-180000-500-blkem.vec ` , respectively ; 7 .",
    "use ` evaluate.py ` to test ` 25000-180000-500-blkem.vec ` .",
    "we have developed a python / perl toolkit ` psdvec ` for learning word embeddings from a corpus .",
    "this open - source cross - platform software is easy to use , easy to extend , scales up to large vocabularies , and can learn new words incrementally without re - training the whole vocabulary .",
    "the produced embeddings performed stably on various test tasks , and achieved the best average score among 7 state - of - the - art methods .",
    "this research is supported by the national research foundation singapore under its interactive digital media ( idm ) strategic research programme .",
    "10    david  m blei , andrew  y ng , and michael  i jordan .",
    "latent dirichlet allocation .",
    ", 3:9931022 , 2003 .",
    "manaal faruqui , yulia tsvetkov , dani yogatama , chris dyer , and noah  a. smith . sparse overcomplete word vector representations . in _ proceedings of acl _ ,",
    "thomas  k landauer and susan  t dumais .",
    "a solution to plato s problem : the latent semantic analysis theory of acquisition , induction , and representation of knowledge .",
    ", 104(2):211 , 1997 .    omer levy and yoav goldberg .",
    "neural word embeddings as implicit matrix factorization . in _ proceedings of nips 2014 _ , 2014 .",
    "omer levy , yoav goldberg , and ido dagan .",
    "improving distributional similarity with lessons learned from word embeddings .",
    ", 3:211225 , 2015 .",
    "omer levy , yoav goldberg , and israel ramat - gan .",
    "linguistic regularities in sparse and explicit word representations . in _ proceedings of conll-2014",
    "_ , page 171 , 2014 .",
    "david  d lewis , yiming yang , tony  g rose , and fan li .",
    "rcv1 : a new benchmark collection for text categorization research . , 5:361397 , 2004 .",
    "shaohua li , tat - seng chua , jun zhu , and chunyan miao .",
    "topic embedding : a continuous representation of documents . in _ proceedings of the the 54th annual meeting of the association for computational linguistics ( acl ) _ , 2016 .",
    "shaohua li , jun zhu , and chunyan miao . a generative word embedding model and its low rank positive semidefinite solution . in _ proceedings of the 2015 conference on empirical methods in natural language processing",
    "pages 15991609 , lisbon , portugal , september 2015 .",
    "association for computational linguistics .",
    "tomas mikolov , ilya sutskever , kai chen , greg  s corrado , and jeff dean . distributed representations of words and phrases and their compositionality . in _ proceedings of nips 2013",
    "_ , pages 31113119 , 2013 .",
    "jeffrey pennington , richard socher , and christopher  d manning .",
    "glove : global vectors for word representation .",
    ", 12 , 2014 .",
    "herbert rubenstein and john  b. goodenough .",
    "contextual correlates of synonymy . , 8(10):627633 , october 1965 .",
    "nathan srebro , tommi jaakkola , et  al . weighted low - rank approximations . in _ proceedings of icml 2003",
    "_ , volume  3 , pages 720727 , 2003 .",
    "karl stratos , michael collins , and daniel hsu .",
    "model - based word embeddings from decompositions of count matrices . in _ proceedings of acl _ , 2015 .",
    "joseph turian , lev ratinov , and yoshua bengio .",
    "word representations : a simple and general method for semi - supervised learning . in",
    "_ proceedings of the 48th annual meeting of the association for computational linguistics _ , pages 384394 .",
    "association for computational linguistics , 2010 .",
    "ancillary data table required for subversion of the codebase .",
    "kindly replace examples in right column with the correct information about your current code , and leave the left column as it is .",
    "c1 & current code version & 0.4 +   c2 & permanent link to code / repository used of this code version & https://github.com/askerlee/topicvec +   c3 & legal code license & gpl-3.0 +   c4 & code versioning system used & git +   c5 & software code languages , tools , and services used & python , perl , ( inline ) c++ +   c6 & compilation requirements , operating environments & dependencies & python : numpy , scipy , psutils ; perl : inline::cpp ; c++ compiler +   c7 & if available link to developer documentation / manual & n / a +   c8 & support email for questions & shaohua@gmail.com +"
  ],
  "abstract_text": [
    "<S> psdvec is a python / perl toolbox that learns word embeddings , i.e. the mapping of words in a natural language to continuous vectors which encode the semantic / syntactic regularities between the words . </S>",
    "<S> psdvec implements a word embedding learning method based on a weighted low - rank positive semidefinite approximation . to scale up the learning process </S>",
    "<S> , we implement a blockwise online learning algorithm to learn the embeddings incrementally . </S>",
    "<S> this strategy greatly reduces the learning time of word embeddings on a large vocabulary , and can learn the embeddings of new words without re - learning the whole vocabulary . on 9 word similarity / analogy benchmark sets and 2 natural language processing ( nlp ) tasks , psdvec produces embeddings that has the best average performance among popular word embedding tools . </S>",
    "<S> psdvec provides a new option for nlp practitioners .    </S>",
    "<S> word embedding , matrix factorization , incremental learning </S>"
  ]
}