{
  "article_text": [
    "the remarkable feature that power - law distributions are commonly encountered in a huge variety of seemingly very different systems has a long history : the first discovery seems to go back to pareto s paper from 1896 concerning the uneven distribution of incomes  @xcite .",
    "some twenty years later auerbach in ref .",
    "@xcite found the same power - law distributions for city sizes .",
    "subsequently , george kingsley zipf found power - law distributions for the word frequency in written texts and this empirical finding became known as zipf s law  @xcite , although the first discovery for the case of word frequencies was made some twenty years earlier by j.  k. estroup  @xcite . by now the literature related to zipf",
    "s law is immense and spans basically all fields : economy , sociology , linguistics , physics , mathematical statistics , to mention a few .",
    "a short history can be found in ref .",
    "@xcite .    a large amount of the literature on zipf s law",
    "is concerned with empirically finding systems which obey zipf s law  @xcite , the precise mathematical form of the distribution which should be associated with zipf s empirical law  @xcite , statistical methods for establishing whether or not one mathematical form fits the empirical data better than another , and last but not least various methods for data analysis in order to find the precise value of the power - law exponent for the alleged power law .",
    "this is _ not _ a concern of the present paper .",
    "instead we focus on the question `` why ? ''",
    ": why does zipf s law give a good description of data from seemingly completely unrelated phenomena ?",
    "the present work stems from a specific remark made by herbert simon in ref .",
    "@xcite ` _ _ no one supposes that there is any connection between horse - kicks suffered by soldiers in the german army and blood cells on a microscopic slide other than that the same urn scheme provides a satisfactory abstract model for both phenomena__. ' this is precisely the view taken in the present paper : if a vast amount of seemingly unrelated phenomena share a common characteristic , this characteristic can not depend on the details of the system but must be traced to a global feature .",
    "simon s explicit attempt to find such an abstract model , the simon growth model , was criticized by benoit mandelbrot in ref .",
    "@xcite , who instead argued that a growth model was not adequate and that the common feature should be associated with information and entropy  @xcite .",
    "these opposite view - points led to a heated argument between simon and mandelbrot  @xcite . from our perspective , both were right : the common element must be a global abstract model and information is the shared quantity which decides the characteristics .",
    "the basic element proposed here is that the abstract common feature is the division into groups . the generic model for this is numbered balls divided into boxes .",
    "as examples , we take people ( balls ) divided into cities ( boxes ) , people ( balls ) divided into family names ( boxes ) , and words from a novel ( balls ) divided into number of occurrences in the text ( boxes ) .",
    "we then use information theory to obtain the best ( bayesian ) prediction of the box - size distribution based on maximum mutual information .    in , we present the empirical data to which we compare our predictions . in the following ,",
    "we describe and explain the random group formation ( rgf ) model . in",
    ", the empirical data is directly compared with the explicit predictions of the rgf model .",
    "the reason for a systematic size change of the power - law exponent is explained and exemplified by data from novels .",
    "in , the connection between the equilibrium maximum - entropy distribution and gibrat s growth model  @xcite is discussed and it is explained that the power - law distribution @xmath5 is indeed an equilibrium feature rather than a growth feature . finally , contains a summary and concluding remarks .",
    "in addition , various more detailed clarifications are relegated to three appendices .",
    "@rrrrr & @xmath6 & @xmath7 & @xmath8 & @xmath9 + us counties & 277  537  173 & 2  445 & 9  519  338 & @xmath10 + french communes & 51  107  816 & 9  011 & 852  395 & @xmath11 + us family names & 242  121  073 & 151  671 & 2  376  206 & @xmath12 + korean family names & 45  974  571 & 244 & 9  925  949 & @xmath12 + hardy & 1  342  258 & 30  744 & 74  165 & @xmath13 + melville & 743  666 & 30  122 & 49  136 & @xmath13 +                 the question we want to address is best illustrated by explicit examples .",
    "three seemingly completely unrelated phenomena are chosen : the city - size distribution of a country , family - name frequencies for a country , and the word - frequency distribution in novels .",
    "two examples are given in each case .",
    "shows the county - size distribution in united states ( us ) for year 2000  @xcite and in this case the total population is @xmath14 , the number of counties @xmath15 and the largest county is los angeles with @xmath16 inhabitants ( see ) .",
    "( a ) gives the average number of counties having @xmath0 inhabitants and since only rarely two counties have precisely the same number , basically all data points fall on the line @xmath17 .",
    "however , smaller counties are much more common than very large and in ( b ) this feature is clearly displayed by instead plotting the number of counties which have population larger than @xmath0 .",
    "this is usually called the cumulative distribution , denoted by @xmath18 , and normalized such that @xmath19 , where @xmath9 is the size of the smallest county in the dataset .",
    "the interesting thing to note is the broadness of the distribution : this type of distribution is often called `` fat - tailed '' .",
    "( c ) illustrates the same feature by log - binning the raw data .",
    "the resulting distribution @xmath20 is also `` fat - tailed '' and , since @xmath18 is related to @xmath20 by @xmath21 , ( b ) and ( c ) basically carry the same information .",
    "@xmath20 is called the frequency distribution and is normalized such that @xmath22 .",
    "the first thing one may ask is if this fat - tailed feature is specific to county sizes in us .",
    "the answer that it indeed is a typical feature of city distributions was first noted by auerbach in 1913  @xcite and has since then been amply verified .",
    "we illustrate this in by including the communal sizes of france  @xcite . in this case",
    "the total population is @xmath23 , the number of communes @xmath24 and the largest commune is marseille with @xmath25 inhabitants ( see ) .",
    "the data is displayed in the same way and the similarity of the shape of the fat - tailed distribution is striking .",
    "what is the reason for this similarity ?",
    "the first thought might be that it must be connected to some specific human endeavor of creating towns for reasons related to fertility , immigration , economics , commerce and defense and hindered by factors like epidemics , emigration , war , famine and earthquakes .",
    "however , this thought is to some extent superseded by , which shows that the family names in us are distributed in a very similar way ( data from us census 2000  @xcite ) . in this case",
    "the total number of persons in the dataset is @xmath26 , the number of family names is @xmath27 , and the most common name smith has @xmath28 carriers .",
    "thus `` fat tails '' are not exclusive for city - size distributions .",
    "the second example of family names is from korea ( data taken from 2000 south korea census  @xcite ) . as apparent from",
    ", this distribution also has a `` fat tail '' .",
    "however , the fall - off in the log - log plot is slower than for the us family names",
    ". nevertheless , a common feature is the fat tails .",
    "perhaps one could argue that the common factor between city sizes and family names is that in both cases the basic entity are people and so that the reason could be linked to some human sociology  @xcite .",
    "however , and show that the same fat - tailed feature remains true for the word - frequency distribution of words of an author . shows the data compiled from a large set of thomas hardy s novels ( the data set is taken from table i of ref .",
    "@xcite and is obtained by adding together novels by hardy into a single giant novel ) . in this case , the total number of words is @xmath29 , the number of distinct words is @xmath30 and the most common word is ` the ' which appears @xmath31 times .",
    "again the same type of `` fat tailed '' distribution , as for the previous cases , is obtained .",
    "the second example of word - frequency of an authors is herman melville ( the data is obtained in the same way as for hardy and is taken from table i of ref .",
    "this time @xmath32 , @xmath30 and the number of ` the ' is @xmath33 .",
    "the result is very much the same as for the other cases .    .",
    "the horizontal axis is plotted as @xmath34 where @xmath9 is the size of the smallest group in respective dataset .",
    "although the functional forms are clearly different in detail for all the cases , the `` fat tails '' are a shared feature.,scaledwidth=50.0% ]    gives a direct comparison between the raw data for the six cases by using the cumulative representation @xmath35 where @xmath9 is the size of the smallest group of the dataset in each case .",
    "the distributions in all cases are `` fat tailed '' .",
    "however the precise functional form differs in every case . of the pairs for the three different phenomena , the word frequencies for hardy and melville come closest to be similar . nevertheless , they are clearly different .",
    "the two city - size distributions are also rather similar but not as close as the two word - frequency distributions .",
    "finally the two family - name distributions are quite different .",
    "on the other hand , the us family names and the melville s word frequency have a substantial overlap for smaller @xmath34 .",
    "the full drawn straight line in is the prediction of zipf s law @xmath36  @xcite .",
    "as seen from the figure , zipf s law does not give a particular good mathematical description of the data .",
    "all the data sets fall on convex curves in a log - log plot and only the french commune data follow zipf s law over a limited interval for smaller @xmath0 .",
    "you can argue that all the data sets to some extent follow a power - law @xmath37 with different power - law indices @xmath38 in limited regions for smaller @xmath0 .",
    "in such a case it is only the zipf value @xmath39 which is too restricted , so that the more general power - law form could still be a significant feature .",
    "however , the undeniable fact is that all the curves are somewhat convex and hence that the power - law form does not give a complete mathematical representation of the data .",
    "nevertheless , one can argue that zipf s law catches the essential fact that the distributions are broad and have fat tails and that the broadness to a first approximation can be estimated by power - law distributions , albeit with different power - law indices .",
    "however , from such a view point , it is really the broadness which is the essential thing : the power - law approximations _ per se",
    "_ may not have any direct bearing on the understanding .",
    "there are two possible hypotheses you can start from : either one argues that the `` fat - tailed '' distributions are essentially system - specific and that the similarity of the distributions is just accidental and hence of no particular significance , or you can side with herbert simon in ref .",
    "@xcite and argue that the similarities imply an underlying system - independent stochastic process which accounts for the `` fat tails '' . in this paper",
    ", we pursue the latter possibility .",
    "a common feature of all the data - sets in is that they on an abstract level can be described as objects divided into categories .",
    "the point made here is that , considering the immense variety of systems which display the same type of `` fat tails '' , it seems hard to imagine any other general shared feature .",
    "the question addressed here is what can be deduced about the size of the categories solely based on this common feature .",
    "the starting point for the rgf model is as follows : you have @xmath6 numbered balls and @xmath7 boxes .",
    "the box sizes are @xmath40 , which means that a box of size @xmath0 has @xmath0 distinct slots which a ball can occupy .",
    "there are hence in total @xmath6 distinct slots and since you have no other knowledge , you assume that the probability of finding a specific ball at any address is equal .",
    "this is the bayesian assumption .",
    "this means that the chance of finding a specific ball at a specific address is @xmath41 . to make it less abstract",
    ", one can consider people divided into towns .",
    "a town of size @xmath0 has @xmath0 addresses where a person can live .",
    "you can imagine that the persons move around so as to come close to friends and suitable job opportunities .",
    "however , the motivation and initiative differ quite a lot .",
    "but if you do not have any information of these system - specific driving forces , your best guess is that any person has an equal probability to live at any address .    under this bayesian assumption of equal address probability , what is the best estimate you can make for the distribution @xmath40 ?",
    "one may then note that @xmath42 can be viewed as a probability distribution .",
    "this means that , if you know @xmath20 , then a typical expected @xmath40 is obtained by randomly drawing @xmath7 box - sizes from the probability distribution @xmath20 .",
    "the most likely @xmath20 corresponds to the maximum entropy of @xmath43=-\\sum_kp(k ) \\ln p(k)$ ] under the constraints that @xmath6 and @xmath7 are given , together with the constraint given by the condition @xmath41 .",
    "the last constraint can be handled by maximum mutual information  @xcite or equivalently by minimum information cost , as explained in [ app : max ] .",
    "the information cost enters as follows : the information to localize a ball with no additional knowledge is @xmath44 ( in nats ) .",
    "the information needed to localize a ball , if you know that it is contained in a box of size @xmath0 , is @xmath45 $ ] .",
    "this means that if you draw a value @xmath0 from the probability function @xmath20 , then @xmath46 $ ] is the information it will cost you to localize a specific ball belonging to this @xmath0 value : the information cost is defined as the additional info which on the average is needed to localize a ball if you know the box size , @xmath47 $ ] .",
    "the best estimate of @xmath20 is obtained by minimizing the information cost subject to the @xmath6 and @xmath7 constraints , i.e. , minimizing @xmath48 $ ] @xmath49)=i_{\\rm cost}[p(k)]+c_1\\sum_kp(k)+c_2\\sum_kkp(k )                \\label{eq : g}\\ ] ] where @xmath50 and @xmath51 are positive constants .",
    "it is interesting to note that @xmath48)$ ] can be regarded as the total information cost : each additional constraint means that information is added to the specification of the system and hence adds to the cost .",
    "the variational solution is @xmath52 where @xmath53 and @xmath54 are determined by the conditions @xmath55 and @xmath56 .",
    "this distribution is hence the most likely distribution provided that you randomly place @xmath6 numbered balls into @xmath7 boxes under the condition that the chance of finding a specific ball in a specific slot is equal .",
    "a crucial observation is that equal chance means no preference and that any preference means additional knowledge .",
    "additional knowledge means larger _ a priori _ knowledge which means smaller entropy for the distribution @xmath20 .",
    "thus any additional _ a priori _ knowledge means smaller entropy .",
    "this observation makes it possible to go one step further without losing generality .    in case of the real systems described in the preceding section",
    ", one can think of innumerable processes involved in creating the data . among these",
    "there are likely to be processes which breaks the no preference condition and hence lower the entropy of the distribution @xmath20 .",
    "if we _ a priori _ assume that the entropy is lowered by the amount @xmath57 , caused by the combined effect of all such unknown non - preference breaking processes , then this can simply be incorporated into the variational estimate as yet another lagrangian constraint    @xmath49)=i_{cost}([p(k)])+c_1\\sum_kp(k)+c_2\\sum_kkp(k)+c_3 s[p(k ) ]   \\label{eq : gs}\\ ] ]    where @xmath58 is an additional lagrangian multiplier and the solution is @xmath59 where @xmath60 and the multipliers are determined by the three conditions @xmath55 , @xmath61 and @xmath62 .    to turn this into a predictive estimate",
    ", one also needs an estimate of @xmath57 . here",
    "the particular functional form of @xmath20 given by provides a convenient estimate of @xmath57 ; since @xmath57 lowers the entropy of @xmath20 , it always makes the `` fat tail '' less broad .",
    "this means that it also lowers the value @xmath8 .",
    "the value of the member of the largest group is well defined for each dataset and can hence be used as an input parameter .",
    "the connection between @xmath8 and @xmath20 given by can be obtained as follows : determine the value @xmath63 for which @xmath64 which means that there is on the average precisely one box in the interval @xmath65 $ ] .",
    "the average size of this box is given by @xmath66 ; @xmath67 is the best possible estimate of @xmath8 for a dataset generated by the probability distribution @xmath20 .",
    "this means that given @xmath6 , @xmath7 , and @xmath8 , the rgf model provides you with a unique prediction for @xmath20 , where @xmath20 is obtained from a set of self - consistent equations .",
    "more details on the rgf model are given in [ app : self ] .",
    "for a fixed @xmath68 : @xmath6 is given on the horizontal axis and @xmath69 on the horizontal right axis , while @xmath70 is color coded .",
    "the figure illustrates that if you know any three of the four parameters @xmath6 , @xmath7 , @xmath71 and @xmath2 , the fourth is uniquely determined by the rgf function .",
    ", scaledwidth=50.0% ]     on the total number of elements @xmath6 for fixed @xmath72.,scaledwidth=45.0% ]     on the number of elements in the largest group , @xmath71 , for fixed @xmath72 .",
    ", scaledwidth=45.0% ]    shows the possible solution for a specific value of @xmath2 as a function of @xmath6 , @xmath72 , and @xmath8 ( where @xmath8 is color coded ) .",
    "one notes that for any given @xmath6 and @xmath69 , there is a whole range of possible @xmath8 values and this range depends explicitly on the system size @xmath6 . illustrates how @xmath2 depends on @xmath6 for fixed @xmath69 whereas shows how @xmath2 depends on @xmath8 for fixed @xmath69 .",
    "is particularly illuminating because it shows that the power - law index @xmath2 , associated with the power law which described the distribution for _ small _ @xmath0 , is in fact determined by the number of elements in the _ largest _ group .",
    "in other words , the small @xmath0 behavior is determined by the non - power - law - like behavior for large @xmath0 .",
    "an interesting consequence of this coupling between the small and large @xmath0-dependence is that the power - law index @xmath2 of the group distribution increases if one randomly remove a fraction of the original elements .",
    "this will be further discussed in the following section .    as seen in ,",
    "the data for the group distributions of real systems often follows a slightly convex function in a log - log - plot .",
    "however , the rgf phenomena in itself does not have this restriction , but it all depends on the relation between the parameters .",
    "pure power - laws are just special cases of rgf , which correspond to particular relations between @xmath6 , @xmath7 and @xmath8 . also slightly concave distributions are possible within the general rgf description .",
    "the rgf model leads to a general distribution associated with a minimal information cost from which the group sizes are drawn .",
    "this is somewhat reminiscent of the gauss distribution , which is likewise general and system - independent .",
    "one may then ask what the entropy is for a gaussian ( or poisson ) distribution for a given @xmath6 and @xmath7 .",
    "the answer is that the gaussian entropy is smaller than the rgf entropy because the width of a gaussian ( or poission ) distribution is always smaller or equal to @xmath73 , whereas the reverse is true for the `` fat tailed '' .",
    "this means that the gaussian ( or poisson ) form corresponds to a larger information cost ; the process leading to a gauss curve requires more _ a priori _ information to be specified .",
    "from this perspective , the difference in the shapes of the two probability curves partly stems from the difference in the amount of _ a priori _ information needed to specify the global structure of the problem at hand .",
    "gives the values of @xmath6 , @xmath7 and @xmath8 for the raw - data of the six real examples described in .",
    "this is precisely the raw data needed for uniquely determining the rgf prediction for each case . in to , these predictions are plotted .",
    "the agreements between the real data and and rgf predictions are remarkably good in all the cases .",
    "it is important to note two things : first , the rgf curves are predictions based on minimal information .",
    "they are _ not _ any best fits to the data with some prescribed functional form .",
    "it is an important distinction , because a _ prediction _ of how the data should be distributed is conceptually quite different from just fitting a function to a given data set .",
    "the second important thing to note is that the parameter @xmath2 , which is the counterpart of the power - law index in the cruder power - law description of the data , is different for different datasets and is in the range @xmath74 $ ] , where the korean surname distribution has the smallest ( @xmath75 ) and the french communes the largest ( @xmath76 ) . _",
    "these @xmath2 values are * not * fitted values , but are obtained directly from the data in . _",
    "if you are an orthodox believer in power - laws and zipf s law , you might argue that the data in to are essentially power laws except for uninteresting cutoffs at higher @xmath0 values .",
    "then the thing to note is that the @xmath2 values for the rgf curves in to are determined by the cutoffs @xmath8 . in other words ,",
    "the data for french communes , according to the rgf prediction , falls on a power law with exponent @xmath77 for small @xmath0 values because the large-@xmath0 cutoff , marseille , has about @xmath78 inhabitants . in order for the french communes to have the same @xmath79 as the counties in us",
    ", marseille would be required to have about @xmath80 inhabitants instead . as explained in the cutoff",
    "is as essential for the description as is the number of communes and the total population .",
    "another consequence of the rgf is that the exponent @xmath2 for a given complete dataset with @xmath6 elements in fact depends on the number of elements @xmath81 of this dataset which you include in your analysis . to illustrate this , we choose the word - frequency data from thomas hardy :",
    "shows this data , where the full dataset has @xmath82 words and @xmath83 specific words and the number of the word ` the ' is @xmath84 ( compare ) .",
    "this information gives @xmath85 and , as seen in ( b ) and ( c ) , the rgf prediction gives a very good representation of the data .",
    "next we randomly remove 99% of the words so that the total number of words is instead @xmath86 .",
    "the simplest method is just to randomly remove the words using a computer .",
    "it corresponds to a well - defined mathematical transformation , which in the present context can be called the random book transformation ( rbt )  @xcite : let the word distributions before and after the transformation , @xmath87 and @xmath88 , be expressed as two column matrices with @xmath7 elements numerated by @xmath0 , then @xmath89 where @xmath90 is a triangular matrix with elements @xmath91 and @xmath92 is the binomial coefficient .",
    "the coefficient @xmath93 is @xmath94     words and is shown in .",
    "the reduced data - set contains 1% of the words and are plotted as the full - drawn line in the cumulative plot @xmath18 .",
    "this line is an average over many random removals of 99% of the words .",
    "the rgf prediction is given by the dashed line .",
    "the size transformation causes the power - law index @xmath2 to change from @xmath95 ( see ) to @xmath96 when only 1% of the words remain.,scaledwidth=40.0% ]    more details on the rbt are given in [ app : rbt ] .",
    "the transformed hardy has @xmath97 , @xmath98 and the number of ` the ' is @xmath99 .",
    "note that the transformations of @xmath6 and @xmath8 are trivial : both are reduced by a factor of hundred .",
    "however , the transformation of @xmath7 is nontrivial : the chance of removing a specific word which occurs @xmath0 times in the original dataset depends in a nontrivial way on @xmath0 .",
    "the three values @xmath100 for the transformed book give a corresponding rgf prediction for the distribution .",
    "this prediction gives @xmath101 .",
    "thus the prediction is that @xmath2 , because of the size transformation , increases from @xmath95 to @xmath96 .",
    "this is confirmed by the actual data for the smaller dataset since the rgf prediction again gives a very good representation of the data . however , there are some small deviations .",
    "these small deviations are also reflected in a small difference of the entropy for the transformed data and the rgf prediction : the reduced data given by the full - drawn curve in corresponds to @xmath102 , while the rgf prediction corresponds to @xmath103 .",
    "this means that the process of randomly removing words imposes some further tiny constraint in addition to what is absorbed into the rgf prediction .",
    "one should note that , from a system - specific perspective , these small deviations from the rgf are really the interesting thing , because they do reflect something system - specific . in the present case ,",
    "the additional constraint is a consequence of randomly removing data .",
    "however , the most striking thing is how well the rgf describes the transformation : the removal 99% of the words is really a substantial reduction .     as a function of text length @xmath6",
    "is obtained by randomly transforming the data down to a given length .",
    "since the transformation of @xmath8 is trivial , the knowledge of @xmath69 suffices for obtaining the rgf prediction . ( a ) and ( b ) show that the rgf parameters @xmath104 and @xmath2 are both to good approximation linear functions of @xmath105 .",
    "this makes it possible to extrapolate to @xmath106 .",
    "the extrapolated value of @xmath2 in the limit @xmath106 is in this case @xmath107.,title=\"fig:\",scaledwidth=40.0% ]   as a function of text length @xmath6 is obtained by randomly transforming the data down to a given length . since the transformation of @xmath8 is trivial , the knowledge of @xmath69 suffices for obtaining the rgf prediction . (",
    "a ) and ( b ) show that the rgf parameters @xmath104 and @xmath2 are both to good approximation linear functions of @xmath105 .",
    "this makes it possible to extrapolate to @xmath106 .",
    "the extrapolated value of @xmath2 in the limit @xmath106 is in this case @xmath107.,title=\"fig:\",scaledwidth=40.0% ]     as a function of text length @xmath6 is obtained by randomly transforming the data down to a given length . since the transformation of @xmath8 is trivial , the knowledge of @xmath69 suffices for obtaining the rgf prediction . ( a ) and ( b ) show that the rgf parameters @xmath104 and @xmath2 are both to good approximation linear functions of @xmath105 .",
    "this makes it possible to extrapolate to @xmath108 .",
    "the extrapolated value of @xmath2 is @xmath109 .",
    ", title=\"fig:\",scaledwidth=40.0% ]   as a function of text length @xmath6 is obtained by randomly transforming the data down to a given length .",
    "since the transformation of @xmath8 is trivial , the knowledge of @xmath69 suffices for obtaining the rgf prediction .",
    "( a ) and ( b ) show that the rgf parameters @xmath104 and @xmath2 are both to good approximation linear functions of @xmath105 .",
    "this makes it possible to extrapolate to @xmath108 .",
    "the extrapolated value of @xmath2 is @xmath109 .",
    ", title=\"fig:\",scaledwidth=40.0% ]    the fact that the power - law index @xmath2 increases , when the total number of elements is reduced , also means that @xmath2 decreases when the number of elements is increased .",
    "one may then ask if @xmath2 acquires some special value in the limit @xmath108 .",
    "the fact that @xmath2 decreases means that the effect of preferential processes diminishes , and from this perspective one might suspect that the limit value is the non - preferential value @xmath110 .",
    "we have tried to estimate this limit in case of the word - frequency data by hardy and melville : starting from the data in and , we first transform the data to smaller sizes , by randomly removing words , and obtain the @xmath2 and @xmath54 for the corresponding rgf .",
    "these are plotted as @xmath2 versus @xmath105 and @xmath111 versus @xmath105 in and for the data from hardy and melville , respectively .",
    "the reason why @xmath112 is a natural variable is explained in [ app : rbt ] .",
    "as seen in and for hardy and melville , the two quantities @xmath2 and @xmath104 scale linearly with @xmath105 to a very good approximation . from this",
    ", the limit value of @xmath2 can be directly estimated : for hardy , the value @xmath107 is obtained in the limit @xmath108 and for melville @xmath109 .",
    "this shows that @xmath2 does indeed decrease in a systematic way with increasing size and , furthermore , that the limit value comes close to the non - preferential value @xmath110 .",
    "we note in passing that in case of a novel written by an author , one may ask how @xmath2 changes if one analyzes a small part of the text within the novel . as shown in ref .",
    "@xcite , the result is very similar to the random removal of words because , to very good approximation , the chance that a picked word belongs to the frequency class @xmath0 is on the average independent of the position in the book .",
    "most earlier attempts to explain the broad distributions for word frequencies , towns and family names are based on growth models  @xcite .",
    "the basic goal of these attempts focuses on explaining why the data follows a power law with an exponent close to 2 . as seen from the data in",
    ", such a power law does rarely give a good description of the data : the data only approximately follows a power law for small @xmath0 and the power - law exponent is usually significantly smaller than 2 .",
    "furthermore , these attempts completely miss the fact that , as shown here , the number of members of the largest group determines the power - law index of the power law which approximately describes the data for small @xmath0",
    ". however , the growth models are also problematic for two conceptual reasons .",
    "the first is that a real growth model is history - dependent .",
    "this is problematic because history and memory are usually system - specific features and any description which contains such features is less ubiquitous .",
    "the second is the relation between growth , steady state , and maximum entropy which makes the definition of a growth model rather flexible .    in order to make the connection between the rgf and growth models , we first construct a dynamical model which directly leads to the maximum - entropy solution of the equal - address - probability rgf given by .",
    "a simple dynamical model which achieves this is the following : start with @xmath7 boxes and @xmath6 balls and the condition that all boxes must always contain at least one ball . then , at each time step , you pick two balls randomly with equal probability and move one of the balls to the same box as the other .",
    "any move which attempts to empty a box is abandoned .",
    "this dynamical update has the @xmath113 distribution given by as its steady state solution  @xcite .",
    "next imagine that you watch this dynamical process from the vantage point of a single box .",
    "this box will then have a fluctuating number , @xmath0 , of balls between 1 and @xmath6 following a trajectory in time @xmath114 .",
    "since the maximum entropy dynamics is completely ergodic , it follows that @xmath115 @xmath116 for @xmath117 and furthermore that @xmath118 for @xmath117 .",
    "after start and the average of many such outcomes gives @xmath20 .",
    "the distribution @xmath20 is close to log - normal .",
    "the parameters chosen are @xmath119 and @xmath120 .",
    "( b ) the same model with the same parameters but with the restriction that the number of balls in the box can not exceed @xmath6 .",
    "this turns the model into an ergodic model with the steady - state distribution given by @xmath121 .",
    "( c ) every @xmath0 can change by @xmath122 . at this point",
    "the model ceases to be a growth model and @xmath123 ( d ) every @xmath0 can change by @xmath124 or @xmath125 .",
    "this is an ergodic non - growing model and @xmath126 with @xmath127 ( e ) the same parameter as in ( a ) and ( b ) , but every time one subtracts a number of balls corresponding to the average increase , i.e. , @xmath128 . in this way ,",
    "the stochastic part of the model is turned into the non - growing gibrat model given in ( c ) with the same distribution @xmath123.,title=\"fig:\",scaledwidth=40.0% ]   after start and the average of many such outcomes gives @xmath20 .",
    "the distribution @xmath20 is close to log - normal .",
    "the parameters chosen are @xmath119 and @xmath120 .",
    "( b ) the same model with the same parameters but with the restriction that the number of balls in the box can not exceed @xmath6 .",
    "this turns the model into an ergodic model with the steady - state distribution given by @xmath121 .",
    "( c ) every @xmath0 can change by @xmath122 . at this point",
    "the model ceases to be a growth model and @xmath123 ( d ) every @xmath0 can change by @xmath124 or @xmath125 .",
    "this is an ergodic non - growing model and @xmath126 with @xmath127 ( e ) the same parameter as in ( a ) and ( b ) , but every time one subtracts a number of balls corresponding to the average increase , i.e. , @xmath128 . in this way ,",
    "the stochastic part of the model is turned into the non - growing gibrat model given in ( c ) with the same distribution @xmath123.,title=\"fig:\",scaledwidth=40.0% ]   after start and the average of many such outcomes gives @xmath20 .",
    "the distribution @xmath20 is close to log - normal .",
    "the parameters chosen are @xmath119 and @xmath120 .",
    "( b ) the same model with the same parameters but with the restriction that the number of balls in the box can not exceed @xmath6 .",
    "this turns the model into an ergodic model with the steady - state distribution given by @xmath121 .",
    "( c ) every @xmath0 can change by @xmath122 . at this point",
    "the model ceases to be a growth model and @xmath123 ( d ) every @xmath0 can change by @xmath124 or @xmath125 .",
    "this is an ergodic non - growing model and @xmath126 with @xmath127 ( e ) the same parameter as in ( a ) and ( b ) , but every time one subtracts a number of balls corresponding to the average increase , i.e. , @xmath128 . in this way ,",
    "the stochastic part of the model is turned into the non - growing gibrat model given in ( c ) with the same distribution @xmath123.,title=\"fig:\",scaledwidth=40.0% ]   after start and the average of many such outcomes gives @xmath20 .",
    "the distribution @xmath20 is close to log - normal .",
    "the parameters chosen are @xmath119 and @xmath120 .",
    "( b ) the same model with the same parameters but with the restriction that the number of balls in the box can not exceed @xmath6 . this turns the model into an ergodic model with the steady - state distribution given by @xmath121 .",
    "( c ) every @xmath0 can change by @xmath122 . at this point",
    "the model ceases to be a growth model and @xmath123 ( d ) every @xmath0 can change by @xmath124 or @xmath125 .",
    "this is an ergodic non - growing model and @xmath126 with @xmath127 ( e ) the same parameter as in ( a ) and ( b ) , but every time one subtracts a number of balls corresponding to the average increase , i.e. , @xmath128 . in this way ,",
    "the stochastic part of the model is turned into the non - growing gibrat model given in ( c ) with the same distribution @xmath123.,title=\"fig:\",scaledwidth=40.0% ]   after start and the average of many such outcomes gives @xmath20 .",
    "the distribution @xmath20 is close to log - normal .",
    "the parameters chosen are @xmath119 and @xmath120 .",
    "( b ) the same model with the same parameters but with the restriction that the number of balls in the box can not exceed @xmath6 .",
    "this turns the model into an ergodic model with the steady - state distribution given by @xmath121 .",
    "( c ) every @xmath0 can change by @xmath122 . at this point",
    "the model ceases to be a growth model and @xmath123 ( d ) every @xmath0 can change by @xmath124 or @xmath125 .",
    "this is an ergodic non - growing model and @xmath126 with @xmath127 ( e ) the same parameter as in ( a ) and ( b ) , but every time one subtracts a number of balls corresponding to the average increase , i.e. , @xmath128 . in this way ,",
    "the stochastic part of the model is turned into the non - growing gibrat model given in ( c ) with the same distribution @xmath123.,title=\"fig:\",scaledwidth=40.0% ]    does there exist a corresponding _ single box _",
    "stochastic process which yields the same @xmath20 as the maximum - entropy dynamical process described above ?",
    "a particular class of stochastic models are the stochastic _ growth _ models , where the box on the average grows proportional to the size of the box .",
    "the generic type can be described as follows : start with one ball in the box . then at each time step , with probability 1/2 you either increase the box by adding @xmath129 balls or you subtract @xmath130 balls , where @xmath131 and @xmath132 .",
    "the precise meaning is that you pick the balls in the box consecutively and with chance @xmath133 you add an additional ball and similarly for the subtraction .",
    "the boundary condition is that the box has at least one ball .",
    "thus if @xmath130 is too large to be compatible with the boundary condition , you only remove all balls but one . at each time step , the box increases on the average with @xmath134 balls .",
    "this is a generic discretized model for growth proportional to the box size .",
    "this model is in the continuum limit called the gibrat model and the corresponding @xmath20 has a log - normal distribution which is distinct from a power law . shows the average @xmath135 for the discretized model for the values @xmath136 and @xmath137 and @xmath138 .",
    "this is clearly not a power law but is close to a log - normal distribution . comparing this to the word - frequency data in and shows that the log - normal distribution produced by the stochastic growth model does not match the data .",
    "consequently , growth models producing log - normal distributions are not contenders for an ubiquitous explanation of the `` fat tailed '' distributions presented in .    the model can be turned into an ergodic version by imposing a maximal size @xmath6 ; any attempt to increase the box beyond this size is abandoned and a new attempt is made at this time step .",
    "this is just like having a fixed number of balls which you try to put into the box .",
    "the balls which are not in the box are outside on the table and you choose randomly from them when adding balls to the box . every time you remove balls from the box you put them together with the ones on the table .",
    "( b ) shows that the stochastic steady state is @xmath139 .",
    "this means that a model , which grows proportional to the box size at the same time obeys the condition @xmath140 , has a steady - state version which corresponds to the maximum - entropy solution .",
    "this is just saying that also for the steady - state single - box - growth model , the chance of finding a specific ball in the box when it has size @xmath0 is independent of @xmath0 .",
    "the reason for this can be traced to the particular stochastic update which in a logarithmic scale corresponds to @xmath141 .",
    "thus the system wanders randomly among the values @xmath142 within the interval @xmath143 $ ] and , since there is no preference , the probability to find the system in any of these points are equal ( modulo a slight correction imposed by the boundary points ) , from which @xmath139 follows .",
    "next we consider the situation when @xmath144 but still a growth model , so that @xmath145 .",
    "this means that @xmath146 . in this case",
    ", the steady - state solution instead becomes @xmath147 where @xmath148 .",
    "the limit case @xmath149 is no longer a growth model , but is an equilibrium model with a well - defined non - growing average size @xmath69 . in this case , the exponent is @xmath150 as illustrated in ( c ) .",
    "when @xmath151 the generic model is a non - growing steady - state model with the solution @xmath147 with @xmath152 as illustrated in ( d ) .",
    "the gibrat model is often connected to the non - growing steady - state solution @xmath153 by changing it into a non - stochastic growing part on top of which a stochastic non - growing part is added  @xcite : at each time - step one subtracts the number of balls which corresponds to the average increase during one time step , i.e. , @xmath134 . in this way the growing model is transformed into a steady average growth on top of which",
    "is added the stochastic model with @xmath154 .",
    "this changes the log - normal distribution into the non - growing steady - state distribution @xmath153 , as is illustrated in ( e ) .",
    "it is interesting to note that the distribution @xmath153 has little to do with the _ growing _ proportional to the size , but is in fact associated with the corresponding equilibrium non - growing situation .",
    "thus from a conceptual point the difference between the log - normal and the @xmath155 distributions is precisely the difference between a stochastically _ growing _ model and a stochastically non - growing steady - state solution .",
    "it is also interesting to note that one could equally well transform the gibrat model by instead subtracting @xmath156 balls at each time step where @xmath157 .",
    "this again yields a model consisting of a steady average growth and a stochastic non - growing part .",
    "however now the distribution becomes @xmath158 with @xmath152 .",
    "thus the steady - state solutions of the stochastic growth model does give rise to power laws with a wide range of power law indices , but the actual _ growth _ is not responsible for this . as illustrated in , starting from the gibrat model with a log - normal distribution , you can , by manipulating the boundary conditions , turn it into effective steady - state solutions which are of power - law forms and can have a broad range of power - law indices . however , a general principle , of which manipulation connects to which set of real data , is lacking .",
    "the gibrat models are a model for size - proportional stochastic growth of an _ independent _ box .",
    "the simon model is a model of proportional growth for interdependent boxes  @xcite .",
    "it is associated with suggestive descriptions like `` rich - gets - richer '' models and `` preferential attachment '' models  @xcite . in the context of written texts",
    "the generic form can be described as follows  @xcite : when you write a text , you either choose a new word or you repeat one of the words you have used earlier in the text .",
    "the simon assumption is that you with probability @xmath159 write a new word and with probability @xmath160 you repeat an old word chosen uniformly among the words already written . within the box - and - ball model",
    ", each new word defines a new box and a word is added to an existing box in proportion to its size .",
    "the average size of a box becomes @xmath161 and the distribution for large @xmath6 is a power law with exponent @xmath162  @xcite .",
    "for the texts by hardy and melville presented in and , the simon model predicts the power - law indices @xmath163 and @xmath164 , respectively .",
    "as seen from the figures , the simon model fails to describe the word - frequency data .",
    "only the data for the french communes in could be argued to be partially described by a power law with a @xmath2 close to 2 in the region of small @xmath0 .",
    "but since the simon model fails for the us county data and all the other datasets in , our conclusion is that the simon model does not have the ubiquitous generality necessary to explain the `` fat tail '' phenomena .",
    "the lack of generality of the simon model is to some extent reflected in ref .",
    "@xcite by mandelbrot s comment that `` _ _ this is a fairly reasonable assumption in the case of word frequencies , since a text is indeed generated word by word . but a national income is surely * not * distributed dollar by dollar _ _ ''",
    ". however , the simon model is in fact also conceptually unreasonable for texts .",
    "this is because it is a true growth model and hence forces a history dependence on the text which is incompatible with real texts  @xcite : since new words are added and old words repeated at each time - step , the consequence is that the words in a simon book which occur only a few number of times in the book occurs more often at the end of the book . in a typical text , about half",
    "the words only occur once , and in the simon book , these words are with larger probability found at the end . in a real text ,",
    "the words of any frequency group are to good approximation randomly spread through the book : the history dependence of the simon model is a too strong system dependent assumption to make it a contender for ubiquity  @xcite .",
    "`` fat tails '' are common features of datasets encountered in very different contexts .",
    "the question is then , if there is a different system - specific explanation in each case , or if the `` fat tails '' represent an ubiquitous non - system - specific feature . in this paper",
    ", we present evidence for a ubiquitous explanation based on a random group formation ( rgf ) phenomena .",
    "the rgf phenomena lead to an explicit prediction of the group sizes for given values of the total number of elements , groups and the number of elements in the largest group . as a consequence ,",
    "the power - law index of the power law , which approximately describes the data for small @xmath0 , is in fact determined by the size of the largest group .",
    "these predictions were tested against six large datasets for three system types , i.e. , population distributions , surname distributions and word - frequency distributions .",
    "two datasets for each type was chosen in order to be able to compare inter- and intra differences between the datasets .",
    "in addition , the datasets were chosen to be very large in order to get good statistics .",
    "the rgf prediction was found to describe the data very well in all the cases .",
    "the rgf phenomena were also found to be consistent with a systematic change in the power - law index with system size .",
    "this system - size dependence was explicitly demonstrated in case of the word - frequency distributions .",
    "it was also pointed out that alternative attempts to explain the `` fat tails '' based on growth models , like the simon model or the gibrat model , give power - law indices larger than 2 , whereas the data presented typically have smaller values .",
    "in addition , the growth models can neither explain the coupling between the largest group and the power - law index , nor the fact that the power - law index changes in a systematic way with the system size . the growth models typically give size - independent power - law indices . the problem with system - specific memory effects for growth models , like the simon model ,",
    "was also pointed out .",
    "the present investigation leads to the conclusion that a ubiquitous explanation must account for the fact that the largest group determines the power - law index describing the small @xmath0 part of the distribution , as well as the fact that the power - law index in a systematic way depends on the system size .",
    "for example , a short novel written by an author has a different power - law index than a much longer novel  @xcite .",
    "this leaves the critical reader with two options : either one could argue that the agreements found in the present paper are purely accidental and that there is indeed no ubiquitous explanation of the `` fat tails '' .",
    "or you could argue that there is a ubiquitous explanation but it is not given by the rgf . in the latter case , one would then have to come up with an alternative explanation which accounts for the fact that the size of the largest group determines the power - law index for small @xmath0 and which , at the same time , is consistent with a systematic size dependence .",
    "for our part , we think that the evidence in favor of the rgf explanation is entirely convincing . furthermore , since the rgf gives explicit predictions , its validity is open to further tests .",
    "we are grateful for support from the swedish research council through grant no 621 - 2008 - 4449 .",
    "10    v.  pareto . .",
    "droz , geneva , 1896 .",
    "f.  auerbach .",
    "das gesetz der bevolkerungskonzentration . , 59 , 1913 .",
    "g.  zipf . .",
    "harvard university press , cambridge , massachusetts , 1932 .",
    "g.  zipf . .",
    "mifflin company , boston , massachusetts , 1935 .",
    "g.  zipf . .",
    "addison - wesley , reading , massachusetts , 1949 .    j.  b. estroup . .",
    "institut stenographique de france , paris , 4 edition , 1916 .",
    "m.  mitzenmacher . a brief history of generative models for power law and lognormal distributions . ,",
    "1:226 , 2003 .",
    "m.  e.  j. newman .",
    "power laws , pareto distributions and zipf s law .",
    ", 46:323 , 2005 .",
    "a.  clauset , c.  r. shalizi , and m.  e.  j. newman .",
    "power - law distributions in empirical data . , 51:661703 , 2009 .",
    "h.  simon . on a class of skew distribution functions .",
    ", 42:425 , 1955 .",
    "b.  mandelbrot . a note on a class of skew distribution functions : analysis and critique of a paper by h. a. simon .",
    ", 2:90 , 1959 .",
    "b.  mandelbrot . .",
    "butterworth , woburn , massachusetts , 1953 .",
    "r.  gibrat .",
    "une loi des rparations conomiques : leffet proportionnel . , 19:469 , 1930 .",
    "r.  gibrat . .",
    "libraire du recueil sirey , paris , 1931",
    ".    http://www.census.gov/population/www/cen2000/briefs/phc-t4/index.html .",
    "http://www.citypopulation.de/france.html .",
    "http://www.census.gov/genealogy/www/data/2000surnames/index.html .",
    "http://en.wikipedia.org/wiki/list@xmath165of@xmath165korean@xmath165family@xmath165names .",
    "s.  bernhardsson , l.  e.  c. da  rocha , and p.  minnhagen .",
    "the meta book and size - dependent properties of written language . , 11:123015 , 2009 .",
    "d.  h. zanette and s.  c. marunbia .",
    "vertical transmission of culture and the distribution of family names .",
    ", 295:1 , 2001 .",
    "s.  k. baek , h.  a.  t. kiet , and b.  j. kim",
    ". family name distributions : master equation approach . , 76:046113 , 2007 .",
    "t.  m. cover and a.  t. joy . .",
    "wiley , hoboken , new jersey , 2 edition , 2006 .",
    "s.  bernhardsson , l.  e.  c. da  rocha , and p.  minnhagen .",
    "size dependent word frequencies and the translational invariance of books .",
    ", 389:330 , 2010 .",
    "r.  h. baayen . .",
    "kluwer academic publisher , dordrecht , the netherlands , 2001 .",
    "x.  gabaix .",
    "zipf law for cities : an explanation . , 114:739 , 1999 .",
    "p.  minnhagen , s.  bernhardsson , and b.  j. kim .",
    "scale - freeness for networks as a degenerate ground state : a hamiltonian formulation . , 78:28004 , 2007 .    a .-",
    "barabsi , r.  albert , and h.  jeong .",
    "emergence of scaling in random networks .",
    ", 286:509 , 1999 .",
    "m.  newman , a .-",
    "barabsi , and d.  watts . .",
    "princeton university press , princeton and oxford , 2006 .",
    "suppose you have two variables @xmath166 and @xmath167 distributed according to the corresponding probability functions @xmath168 and @xmath169 , respectively .",
    "the total entropy @xmath170 $ ] is then given by @xmath171=-\\sum_{x}\\sum_{y}p(x , y)\\ln p(x , y),\\ ] ] where @xmath172 is the joint probability for the variables @xmath166 and @xmath167 .",
    "if the two distributions are independent ( so that the probability for a value @xmath166 is independent of the value @xmath167 ) , then @xmath172 reduces to @xmath173 and the entropy reduces to @xmath170=h[p(x)]+h[p(y)]$ ] where @xmath174=-\\sum_{x}p(x)\\ln p(x)$ ] . in many situations , @xmath168 and @xmath169 are dependent so that @xmath175 or equivalently the constrained probability @xmath176 ( the probability for a @xmath166 for a fixed given @xmath167 ) is in fact not equal to @xmath168 i.e. @xmath177 ( note the general relation @xmath178 .",
    "we here consider the special case when the distribution @xmath169 is _ a priori _ known .",
    "in such a case , the maximum entropy @xmath179 $ ] is obtained by minimizing the constrained entropy @xmath180=-\\sum_{x}\\sum_{y}p(x , y)\\ln p(y|x)\\ ] ] this follows from the general maximum - mutual - information principle  @xcite .",
    "the mutual information is defined by @xmath181=\\sum_{x}\\sum_{y}p(x , y)\\ln \\frac{p(x , y)}{p(x)p(y)}\\ ] ] and the @xmath168 corresponding to maximum entropy is obtained by maximizing the mutual information for the given @xmath169 .",
    "however , the mutual information can also be expressed as @xmath182=-i[p(x),p(y)]+h[p(y)]$ ] and since @xmath169 is _ a priori _ known it follows that maximizing @xmath183 $ ] is equivalent to minimizing the constrained entropy @xmath184 $ ] .",
    "this constrained entropy we term the information cost @xmath185=h[p(y),p(x)]$ ] . in the case of the equal - address rgf ,",
    "the information cost is given by @xmath186=h[p_{i}|p(k)]=-\\sum_{k}\\sum_{i}p(k , i)\\ln p(i|k ) \\label{cost}\\ ] ]    the _ a priori _ known distribution is the equal probability for each of the @xmath6 addresses , so that @xmath187 .",
    "this means that @xmath188 and consequently @xmath189 .",
    "inserting this in gives @xmath190 the conceptual advantage with the quantity @xmath191 is that it has a simple interpretation : if you know that a specific ball can be found among the boxes which contain @xmath0 balls , then @xmath192 is the information ( in nats ) needed to specify at which specific address the ball can be found .",
    "the average information cost needed for localizing a ball with a known @xmath0-value is hence @xmath193 .",
    "this is an information cost in the sense that the additional info needed to specify the outcome of the @xmath194 is no longer available for the entropy associated with @xmath20 .",
    "the rgf - curve is obtained by minimizing the total information cost @xmath195 $ ] given by . the minimum condition @xmath196}{\\partial p(k)}=0 $ ] leads to the condition @xmath197 with the solution given in @xmath198 with @xmath199 , @xmath200 and @xmath201\\}$ ] .",
    "the constants @xmath53 , @xmath54 and @xmath2 are determined by simultaneously fulfilling three conditions .",
    "the first two of these conditions are @xmath202 and @xmath203 , i.e. , @xmath204 where @xmath9 is the size of the smallest box .",
    "the natural limit is @xmath205 , but can equally well be generalized to an arbitrary @xmath9 .",
    "this means that the constants @xmath2 and @xmath54 are interdependent through the relation @xmath206 the third condition is determined by requiring a specific average value for the size of the largest box @xmath71 . in the direct comparison with a single dataset",
    ", this value is approximated with the actual value of @xmath8 for the dataset .",
    "the calculation of @xmath67 is made in two steps : first a value @xmath63 is determined by the condition that @xmath207 .",
    "this means that on the average there is precisely one box in the interval @xmath65 $ ] .",
    "the second step is to calculate the average size of a box in the interval @xmath65 $ ] , i.e. , @xmath208 thus the three requirements turn into a set of self - consistent equations : one starts by assuming a certain value for @xmath2 and then one obtains @xmath54 by using . thus the two basic constraints yields @xmath20 from the trial @xmath2 .",
    "next this trial @xmath20 is inserted in .",
    "if is not satisfied within a predefined precision , we repeat this procedure with a new trial @xmath2 . in this way , the correct values of @xmath53 , @xmath54 and @xmath2 can be self - consistently determined .",
    "suppose we want the distribution for an @xmath209th part of a book which has the word - frequency distribution @xmath20 .",
    "the chance that a picked word is part of the @xmath209th part is @xmath210 and the chance that it is not is @xmath211 .",
    "consequently the rbt gives  @xcite @xmath212 where @xmath213 is the binomial coefficient and the normalization is appropriate because @xmath214\\\\ & = & \\frac{1}{1-p_{\\frac{1}{n}}(0 ) } \\left[1-p_{\\frac{1}{n}}(0)\\right]=1\\end{aligned}\\ ] ]    the rbt can be analytically obtained in two limiting cases .",
    "these are the equal - address rgf distribution @xmath113 and the limit distribution @xmath215 $ ] .",
    "the transformed solution in the first case is given by @xmath216\\}}{k}\\ ] ] and , since @xmath217 for small @xmath54 , it reduces to @xmath218}{k } \\propto \\frac{\\exp(-knb)}{k}.\\ ] ] this means that the exponential cutoff increases linearly with @xmath219 , or in other words , the size dependence is to good approximation given by @xmath220 where @xmath104 is a constant .",
    "this result just reflects the fact that the exponential cuts off the distribution at the system size @xmath221 .",
    "an important consequence is that the functional form @xmath121 is invariant under the rbt .",
    "this is a very special property and @xmath121 is presumably the only nontrivial invariant functional form with a finite value at @xmath222 .",
    "the typical situation is that the shape of @xmath20 becomes less broad under the transformation , e.g. , @xmath223 with @xmath224 will have an increasing @xmath2 with decreasing size .",
    "the functional form @xmath225 $ ] transforms as @xmath226 one notes that the exponent transforms in the same way and has the form @xmath227 .",
    "one also notes that the form @xmath228 $ ] is invariant but that it is infinite for @xmath222 .",
    "the point is that if you start from @xmath223 then the transformed @xmath229 approaches the limit form @xmath230 $ ] .",
    "it is also interesting to note that , for @xmath0 values not too small , the limiting function is a power law with exponent @xmath231 .",
    "one may then ask what happens if we instead followed the transformation in the reverse direction towards larger books .",
    "since @xmath121 is invariant under the transformation , it seems likely that it is also the limiting function in the reverse direction so that @xmath232 .",
    "this suggests that a book approaches this word - frequency distribution in the limit of infinite size .",
    "as seen from the data analysis in , this expectation seems to have some support in the actual data .",
    "one may perhaps also speculate that since @xmath110 for the upper limit and @xmath231 for the lower , it should not be a surprise that @xmath2 values within @xmath233 are often found in real data ."
  ],
  "abstract_text": [
    "<S> why does zipf s law give a good description of data from seemingly completely unrelated phenomena ? </S>",
    "<S> here it is argued that the reason is that they can all be described as outcomes of a ubiquitous random group division : the elements can be citizens of a country and the groups family names , or the elements can be all the words making up a novel and the groups the unique words , or the elements could be inhabitants and the groups the cities in a country , and so on . </S>",
    "<S> a random group formation ( rgf ) is presented from which a bayesian estimate is obtained based on minimal information : it provides the best prediction for the number of groups with @xmath0 elements , given the total number of elements , groups , and the number of elements in the largest group . for each specification of these three values , </S>",
    "<S> the rgf predicts a unique group distribution @xmath1 , where the power - law index @xmath2 is a unique function of the same three values . the universality of the result is made possible by the fact that no system specific assumptions are made about the mechanism responsible for the group division . </S>",
    "<S> the direct relation between @xmath2 and the total number of elements , groups , and the number of elements in the largest group , is calculated . </S>",
    "<S> the predictive power of the rgf model is demonstrated by direct comparison with data from a variety of systems . </S>",
    "<S> it is shown that @xmath2 usually takes values in the interval @xmath3 and that the value for a given phenomena depends in a systematic way on the total size of the data set . </S>",
    "<S> the results are put in the context of earlier discussions on zipf s and gibrat s laws , @xmath4 and the connection between growth models and rgf is elucidated . </S>"
  ]
}