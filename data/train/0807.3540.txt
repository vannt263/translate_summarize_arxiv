{
  "article_text": [
    "the classical deconvolution problem consists of estimation of the density @xmath4 of a random variable @xmath2 based on the i.i.d .",
    "copies @xmath13 of @xmath14 which are corrupted by an additive measurement error .",
    "more precisely , let @xmath0 be i.i.d.observations , where @xmath15 and the @xmath2 s and @xmath3 s are independent .",
    "assume that the @xmath2 s are unobservable and that they have the density @xmath4 and also that the @xmath3 s have a known density @xmath5 such a model of measurements contaminated by an additive measurement error has numerous applications in practice and arises in a variety of fields , see for instance @xcite .",
    "notice that the @xmath16 s have a density @xmath17 which is equal to the convolution of @xmath4 and @xmath5 the deconvolution problem consists in estimation of the density @xmath4 based on the sample @xmath10    a popular estimator of @xmath4 is the deconvolution kernel density estimator , which was proposed in @xcite and @xcite , see also pp .",
    "231233 in @xcite for an introduction .",
    "additional recent references can be found e.g.in @xcite .",
    "let @xmath18 be a kernel and @xmath19 a bandwidth .",
    "the deconvolution kernel density estimator @xmath20 is constructed as @xmath21 where @xmath22 denotes the empirical characteristic function , i.e.  @xmath23 @xmath24 and @xmath25 are fourier transforms of functions @xmath18 and @xmath26 respectively , and @xmath27 depending on the rate of decay of the characteristic function @xmath25 at plus and minus infinity , deconvolution problems are usually divided into two groups , ordinary smooth deconvolution problems and supersmooth deconvolution problems . in the first case",
    "it is assumed that @xmath25 decays to zero at plus and minus infinity algebraically ( an example of such @xmath28 is the laplace density ) and in the second case the decay is essentially exponential ( in this case @xmath28 can be e.g.  a standard normal density ) . in general",
    ", the faster @xmath25 decays at plus and minus infinity ( and consequently smoother the density @xmath28 is ) , the more difficult the deconvolution problem becomes , see e.g.@xcite .",
    "the usual smoothness condition imposed on the target density @xmath4 is that it belongs to the class @xmath29 where @xmath30 @xmath31 ( the integer part of @xmath32 ) and @xmath33 are known constants , cf.@xcite .",
    "then , if @xmath28 is ordinary smooth of order @xmath34 ( see e.g.  assumption c ( ii ) below for a definition ) , the optimal rate of convergence for the estimator @xmath35 with the mean square error used as the performance criterion is @xmath36 while if @xmath28 is supersmooth of order @xmath37 ( see assumption b ( ii ) ) , the optimal rate of convergence is @xmath38 see @xcite .",
    "the latter convergence rate is rather slow and it suggests that the deconvolution problem is not practically feasible in the supersmooth case , since it seems samples of very large size are required to obtain reasonable estimates .",
    "hence at first sight it appears that the nonparametric deconvolution with e.g.  the gaussian error distribution ( a popular choice in practice ) can not lead to meaningful results for moderate sample sizes and is practically irrelevant .",
    "however , it was demonstrated by exact @xmath39 ( mean integrated square error ) computations in @xcite that , despite the slow convergence rate in the supersmooth case , the deconvolution kernel density estimator performs well for reasonable sample sizes , if the noise level measured by the noise - to - signal ratio @xmath40(\\var[y])^{-1}100\\%,$ ] cf.@xcite , is not too high .",
    "clearly , an ` ideal case ' in a deconvolution problem would be that not only the sample size @xmath7 is large , but also that the error term variance is small .",
    "this leads one to an idealised model @xmath41 where now @xmath42=1 $ ] and @xmath6 depends on @xmath7 and tends to zero as @xmath43 the idea to consider @xmath44 was already proposed in @xcite and was further developed in @xcite .",
    "we refer to these works for additional motivation .",
    "these papers deal mainly with the mean integrated square error of the estimator of @xmath45 here we will study its asymptotic normality .",
    "asymptotic normality of the deconvolution kernel density estimator in the deconvolution problem with fixed error term variance was derived in @xcite and @xcite . for a practical situation where @xmath8 can arise , see e.g.  section 4.2 of @xcite , where an example of measurement of sucrase in intestinal tissues is considered and inference is drawn on the density of the sucrase content .",
    "sucrase is a name of several enzymes that catalyse the hydrolisis of sucrose to fructose and glucose .",
    "it trivially follows from that the deconvolution kernel density estimator for the model that we consider , i.e.@xmath1 with @xmath8 as @xmath46 is defined as @xmath47 where @xmath48 @xmath49 and @xmath25 now denotes the characteristic function of the random variable @xmath3 with a density @xmath5 we will also use @xmath50 and in this case we will denote the function @xmath51 by @xmath52 observe that if @xmath18 is symmetric , will be real - valued .    to get a consistent estimator , we need to control the bandwidth @xmath11 the usual condition to get consistency in kernel density estimation is that the bandwidth @xmath53 depends on @xmath7 and is such that @xmath54 see e.g.  theorem 6.27 in @xcite . since in our model",
    "we assume @xmath55 additional assumptions on @xmath56 which relate it to @xmath57 are needed .",
    "in essence we distinguish two cases : @xmath58 with @xmath59 or @xmath60 conditions on the target density @xmath61 the density @xmath28 of @xmath3 and kernel @xmath18 will be tailored to these two cases .",
    "the remaining part of the paper is organised as follows : in section [ results ] we will present the obtained results .",
    "section [ simulations ] contains several simulation examples illustrating the results from section [ results ] .",
    "all the proofs are given in section [ proofs ] .",
    "we first consider the case when @xmath63 we will need the following conditions on @xmath61 @xmath64 @xmath28 and @xmath11    * assumption a. *    \\(i ) the density @xmath4 is such that @xmath65 is integrable .",
    "\\(ii ) @xmath66 for all @xmath67 and @xmath25 has a bounded derivative .",
    "\\(iii ) the kernel @xmath18 is symmetric , bounded and continuous . furthermore , @xmath24 has support @xmath68,$ ] @xmath69 @xmath24 is differentiable and @xmath70    \\(iv ) the bandwidth @xmath53 depends on @xmath7 and we have @xmath71    \\(v ) @xmath8 and @xmath72 where @xmath63    notice that assumption a ( i ) implies that @xmath4 is continuous and bounded .",
    "assumption @xmath66 for all @xmath67 is standard in kernel deconvolution and is unavoidable when using the fourier inversion approach to deconvolution .",
    "furthermore , a variety of kernels satisfy assumption a ( iii ) , see e.g.  examples in @xcite . also notice that @xmath18 is not necessarily a density , since it may take on negative values .",
    "observe that in assumption a ( v ) we do not exclude the case @xmath73    the following theorem establishes asymptotic normality in this case .",
    "[ thman1 ] let assumption a hold and let the estimator @xmath20 be defined by .",
    "then @xmath74)\\convd { \\mathcal n}\\left(0\\ , , \\ , f(x)\\negthinspace{\\int_{-\\infty}^{\\infty}}|w_r(u)|^2du\\right)\\ ] ] as @xmath9    notice that unlike the asymptotic normality theorem for the deconvolution kernel density estimator in the supersmooth deconvolution problem with fixed @xmath75 that was obtained in @xcite , the asymptotic variance in now depends on @xmath4 .",
    "when @xmath76 for all @xmath77 we recover the asymptotic normality theorem for an ordinary kernel density estimator , see @xcite .",
    "we turn to the case @xmath79 in this case we have to make the distinction between the ordinary smooth and supersmooth deconvolution problems .",
    "we first consider the supersmooth case .",
    "we will need the following condition .",
    "* assumption b. *    \\(i ) the density @xmath4 is such that @xmath65 is integrable .",
    "\\(ii ) @xmath66 for all @xmath67 and @xmath80 for some constants @xmath81 and real constants @xmath82 and @xmath83",
    "\\(iii ) @xmath18 is a bounded , symmetric and continuous function .",
    "furthermore , @xmath24 is supported on @xmath68,$ ] @xmath84 and @xmath70 moreover , @xmath85 as @xmath86 where @xmath87 and @xmath88 are some numbers .",
    "\\(iv ) the bandwidth @xmath53 depends on @xmath7 and we have @xmath71    \\(v ) @xmath8 and @xmath89    assumption b ( i)-(iv ) correspond to those in @xcite .",
    "assumption b ( v ) is stronger than @xmath90 but it is essential in the proof of theorem [ thman3 ] .",
    "denote @xmath91 the following theorem holds true .",
    "[ thman3 ] let assumption b hold and let the estimator @xmath20 be defined by . furthermore",
    ", assume that @xmath92<\\infty$ ] and @xmath93<\\infty.$ ] then @xmath94)\\convd { \\mathcal n } \\left(0,\\frac{a^2}{2\\pi^2c^2}\\left(\\frac{\\mu } { \\lambda}\\right)^{2 + 2\\alpha}(\\gamma(\\alpha+1))^2\\right)\\ ] ] as @xmath9    when @xmath95 for all @xmath77 the arguments given in the proof of this theorem are still valid , and hence we can also recover the asymptotic normality theorem of @xcite for the deconvolution kernel density estimator in the supersmooth deconvolution problem .    finally , we consider the ordinary smooth case .",
    "* assumption c. *    \\(i ) the density @xmath4 is such that @xmath65 is integrable .",
    "\\(ii ) @xmath66 for all @xmath67 and @xmath96 as @xmath97 where @xmath98 and @xmath99 are some constants .",
    "\\(iii ) @xmath24 is symmetric and continuously differentiable .",
    "furthermore , @xmath24 is supported on @xmath68,$ ] @xmath100 and @xmath101    \\(iv ) the bandwidth @xmath53 depends on @xmath7 and we have @xmath71    \\(v ) @xmath8 and @xmath60    for the discussion on assumption c ( i)(iv ) see @xcite .    [ thman2 ] let assumption c hold and let the estimator @xmath20 be defined by",
    ". then @xmath102)\\convd { \\mathcal n } \\left(0,\\frac{f(x)}{{2\\pi c^2}}{\\int_{-\\infty}^{\\infty}}|t|^{2\\beta}|\\phi_w(t)|^2dt\\right)\\ ] ] as @xmath9    when @xmath103 we recover the asymptotic normality theorem of @xcite for a deconvolution kernel density estimator in the ordinary smooth deconvolution problem .    as a general conclusion ,",
    "we notice that theorems [ thman1][thman2 ] demonstrate that the asymptotics of @xmath35 depend in an essential way on the relationship between the sequences @xmath6 and @xmath11 in case @xmath104 the asymptotics are similar to those in the direct density estimation , while when @xmath105 they resemble those in the classical deconvolution problem .",
    "in this section we consider several simulation examples for the supersmooth deconvolution case covered by theorems [ thman1 ] and [ thman3 ] .",
    "we do not pretend to produce an exhaustive simulation study .",
    "our examples serve as a mere illustration of the asymptotic results from the previous section .",
    "it follows from theorems [ thman1][thman2 ] that for a fixed point @xmath106 and a large enough @xmath77 a suitably centred and normalised estimator @xmath35 is approximately normally distributed with mean and standard deviation given in these three theorems .",
    "suppose we have fixed the sample size @xmath7 and the bandwidth @xmath56 generated a sample of size @xmath77 evaluated the estimate @xmath35 and have repeated this procedure @xmath107 times , where @xmath107 is sufficiently large .",
    "this will give us @xmath107 values of @xmath108 we then can evaluate the sample mean and the sample standard deviation of this set of values @xmath108 under appropriate conditions these should be close to the ones predicted by theorems [ thman1 ] and [ thman3 ] . in particular , in the setting of theorem [ thman1 ] , the mean @xmath109 and the standard deviation @xmath110 must be approximately given by @xmath111 while in the setting of theorem [ thman3 ] they are approximately equal to @xmath112    we first concentrate on theorem [ thman1 ] .",
    "let @xmath4 and @xmath28 be standard normal densities , let @xmath113 and suppose @xmath114 the noise level measured by the noise - to - signal ratio is thus rather low and equals @xmath115 suppose that a kernel @xmath18 is given by @xmath116 its corresponding fourier transform is given by @xmath117}(t).$ ] here @xmath118 and @xmath119 a good performance of this kernel in deconvolution context was established in @xcite .",
    "assume that the number of replications @xmath120 before we proceed any further , we need to fix the bandwidth .",
    "we opted for a theoretically optimal bandwidth , i.e.  the bandwidth that minimises",
    "@xmath121=\\ex \\left[{\\int_{-\\infty}^{\\infty}}(f_{nh_n}(x)-f(x))^2dx \\right],\\ ] ] the mean - squared error of the estimator @xmath122 to find this optimal bandwidth , we considered a sequence of bandwidths @xmath123 where @xmath124 is a large enough integer , passed to the fourier transforms in via parseval s identity , cf .",
    "@xcite , and then used the numerical integration .",
    "this procedure resulted in @xmath125 for real data the above method does not work , because depends on the unknown @xmath61 and we refer to @xcite for data - dependent bandwidth selection methods .",
    "however , once again we stress the fact that in order to reach a specific goal of these simulation examples , the bandwidth @xmath53 must be the same for all @xmath107 replications .",
    "this excludes the use of a data - dependent procedure . to speed up the computation of the estimates ,",
    "binning of observations was used , see e.g.  @xcite and @xcite for related ideas in kernel density estimation .    under these assumptions we evaluated the sample means and",
    "standard deviations of @xmath35 for @xmath106 from a grid on the interval @xmath126 $ ] with mesh size @xmath127 these then were plotted in figure [ thm1fig1msd ] together with the theoretical values from .",
    "we notice that the sample means match the theoretical values very well .",
    "this can be also explained by the fact that the bandwidth @xmath53 is quite small .",
    "the match between the sample standard deviations and the theoretical standard deviations is slightly less satisfactory .",
    "it also turns out that theorem [ thman3 ] is clearly not applicable in this case : an evaluation of the theoretical standard deviation @xmath110 in yields a very large value @xmath128 which grossly overestimates the sample standard deviation for any point @xmath129 the reason for this seems to be that both the sample size @xmath7 and the error variance @xmath130 appear to be too small for the setting of theorem [ thman3 ] .",
    "( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    at this point the following remark is in order .",
    "reviewing the proof of theorem [ thman3 ] , one sees that the following asymptotic equivalence is used : @xmath131ds \\sim a \\gamma ( \\alpha+1 ) \\left(\\frac{\\mu}{\\lambda}h^{\\lambda}\\right)^{1+\\alpha } e^{1/(\\mu h^{\\lambda})}\\ ] ] as @xmath132 this explains the shape of the normalising constant in theorem [ thman3 ] .",
    "however , the direct numerical evaluation of the integral in ( with the same parameters and the kernel as in our example above ) shows that the approximation in is good only for very small values of @xmath133 and that it is quite inaccurate for larger values of @xmath134 see a discussion in @xcite .",
    "obviously , one can correct for the poor approximation of the sample standard deviation by the theoretical standard deviation by using the left - hand side of instead of its approximation .",
    "nevertheless , this still leads to a very large ( compared to the sample standard deviation ) value of the theoretical standard deviation for our particular example , namely @xmath135    in our second example we left @xmath136 and @xmath28 the same as above , but as @xmath4 we took a mixture of two normal densities with means @xmath137 and @xmath138 and equal variance @xmath139 the mixing probability was taken to be equal to @xmath140 the density @xmath4 is bimodal and is plotted in figure [ bimodalfig ] .",
    "the simulation results for this density are reported in figure [ add4 ] .",
    "the conclusions are the same as for the first example .",
    "one can easily recognise a bimodal shape of the target density @xmath4 by looking at the sample standard deviation .",
    "( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    in our third example we again considered the standard normal density , but we increased the sample size to @xmath141 the results are reported in figure [ thm1fig2msd ] .",
    "as can be seen , the match between the sample standard deviations and the theoretical standard deviations as computed using theorem [ thman1 ] is less satisfactory than in the previous example .",
    "the explanation lies in the fact that , even though the noise level is low when judged by itself , it is still a bit large compared to the sample size that we have in this case .",
    "also theorem [ thman3 ] remains unapplicable , as it still produces considerably larger values of the theoretical standard deviation compared to the sample standard deviation ( @xmath142 after the necessary correction using ) .",
    "( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    in the next three examples we kept the standard normal densities @xmath4 and @xmath26 but increased the sample size @xmath7 to @xmath143 the error variance @xmath130 was consecutively taken to be @xmath144 and @xmath145 i.e. we considered three different noise levels , @xmath146 and @xmath147 a transition from the asymptotics described by theorem [ thman1 ] to those described by theorem [ thman3 ] is clearly visible in the resulting plots , see figures [ add1][add3 ] .",
    "figure [ add1 ] also indicates that there exist intermediate situations not immediately covered by either of the two theorems .",
    "notice that figure [ add3 ] seems to confirm a general , albeit not intuitive message of theorem [ thman3 ] , which says that the asymptotic standard deviation does not depend on a point @xmath148 but only on the error density @xmath149 there is a large neighbourhood around zero for which the sample standard deviation is almost constant .",
    "( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    in our final example we considered the case when the density @xmath4 is again a mixture of two normal densities ( see above for details ) .",
    "the simulation results for this density are reported in figure [ add2 ] .    ( 5.5,4.0 ) = 5.5 cm    ( 5.5,4.0 ) = 5.5 cm    in this last example the bandwidth @xmath150 was on purpose not selected as a minimiser of @xmath151,$ ] but was taken to be the same as when estimating a standard normal density ( see figure [ add3 ] above ) .",
    "notice that the sample standard deviation is almost constant in the neighbourhood of the origin and is of the same magnitude as the one depicted in figure [ add3 ] .",
    "this seems to provide an additional confirmation of the statement of theorem [ thman3 ] , which says that the limit variance of the estimator @xmath20 does not depend on the target density @xmath45 also notice that because of the fact that @xmath53 is relatively large , the smoothed version of @xmath61 i.e.  @xmath152 is unimodal instead of being bimodal .    as a preliminary conclusion ( we also considered some other examples not reported here )",
    ", our simulation examples seem to suggest that the asymptotics given by theorem [ thman3 ] correspond to the less realistic scenarios of high noise level and very large sample size .",
    "this provides further motivation for the study of deconvolution problems under the assumption @xmath8 as @xmath9",
    "to prove theorem [ thman1 ] , we will need the following modification of bochner s lemma , see @xcite for the latter .",
    "[ fanlemma ] suppose that for all @xmath153 we have @xmath154 as @xmath155 and that @xmath156 where the function @xmath157 is such that @xmath158 and @xmath159 furthermore , suppose that @xmath160 is a sequence of densities , such that @xmath161 for some sequence @xmath162 such that @xmath163 as @xmath155 for a sequence @xmath164 then @xmath165    the proof follows the same lines as the proof of lemma 2.1 in @xcite .",
    "we have @xmath166 notice that @xmath167 converges to zero by the dominated convergence theorem .",
    "we turn to @xmath168 splitting the integration region into the sets @xmath169 and @xmath170 for some @xmath171 we obtain that @xmath172 for @xmath173 we have @xmath174 by the right - hand side of the above expression vanishes as @xmath9 now we consider @xmath175 using the fact that @xmath160 is a density ( and hence that it is positive and integrates to one ) , we have latexmath:[\\ ] ] hence the contribution of ( [ epsint ] ) minus its expectation is of order @xmath213 by comparing this to the normalising constant in , by slutsky s lemma we see that can be neglected when considering the asymptotic normality of @xmath108    the term can be written as @xmath214 observe that both and are real .",
    "expression equals @xmath215 by formula ( 21 ) of @xcite @xmath216 where @xmath217 is a remainder term satisfying @xmath218 where @xmath219 by and lemma 5 of @xcite the latter expression can be bounded as @xmath220 hence @xmath221\\leq \\ex [ \\tilde{r}_{n , j}^2]=o\\left(\\frac{1}{\\sigma_n^2 h_n^2 } \\rho_n^{2(\\lambda(2+\\alpha)+\\lambda_0 - 1)}(\\zeta(\\rho_n))^2\\right).\\ ] ] here we used the fact that @xmath92+\\ex[z^2_j]<\\infty$ ] together with the fact that being convergent , the sequence @xmath6 is bounded , which implies that @xmath222 $ ] is bounded uniformly in @xmath195 by chebyshev s inequality it follows that @xmath223 after multiplication of this term by the normalising factor from we obtain that the resulting expression is of order @xmath224 assumption b ( v ) and slutsky s lemma then imply that the remainder term can be neglected when considering the asymptotic normality of @xmath108    the variance of can be bounded by @xmath225 where the function @xmath226 is given by @xmath227 this function is bounded on @xmath228 where @xmath178 is an arbitrary positive number .",
    "it follows that @xmath229 is also bounded and tends to zero for all fixed @xmath230 with @xmath231 as @xmath232 hence the variance of is of smaller order compared to the variance of , which can be shown by the dominated convergence theorem via an argument similar to the one in the proof of lemma 5 of @xcite .",
    "therefore by slutsky s lemma can be neglected when considering asymptotic normality of .",
    "combination of the above observations yields that it suffices to study @xmath233 where @xmath234\\right).\\ ] ] observe that @xmath235 and that by the same arguments as in the proof of lemma 6 in @xcite , both @xmath236 and @xmath237 converge in distribution to a random variable with a uniform distribution on @xmath238.$ ] furthermore , these two random variables are independent .",
    "now notice that for two independent random variables @xmath239 and @xmath240 the sum @xmath241 equals in distribution @xmath242 moreover , if @xmath239 and @xmath240 are uniformly distributed on @xmath238,$ ] then also @xmath241 is uniformly distributed on @xmath238,$ ] see @xcite . using these two facts , by exactly the same arguments as in the proof of lemma 6 of @xcite we finally obtain that @xmath243 the latter in conjunction with entails .",
    "the proof employs an approach similar to the proof of theorem 2.1 of @xcite .",
    "we have @xmath244={\\int_{-\\infty}^{\\infty}}\\frac{1}{h_n^2}\\left|w_{{\\rho}_n}\\left(\\frac{x - y}{h_n}\\right)\\right|^2g_n(y)dy.\\ ] ] by equation ( 3.1 ) of @xcite ( with @xmath53 replaced by @xmath245 ) we have @xmath246 where @xmath247 is a positive integrable function . hence by the dominated convergence theorem @xmath248 furthermore , again by equation ( 3.1 ) of @xcite we have @xmath249 for some constant @xmath194 independent of @xmath7 and @xmath250",
    "while equation ( 2.7 ) of @xcite implies that @xmath251 combination of these two bounds gives latexmath:[\\[\\label{rhob1 }    since the fact that @xmath160 satisfies can be shown exactly as in the proof of theorem [ thman1 ] , by lemma [ fanlemma ] we then obtain that @xmath253 & \\sim \\frac{f(x)}{h_n\\rho_n^{2\\beta}}{\\int_{-\\infty}^{\\infty}}\\left[\\frac{1}{2\\pi c}\\int_{-1}^{1}e^{-ity}t^{\\beta}\\phi_w(t)dt\\right]^2dy\\\\ & = \\frac{1}{h_n\\rho_n^{2\\beta}}\\frac{f(x)}{2\\pi c^2}\\int_{-1}^{1}|t|^{2\\beta}|\\phi_w(t)|^2dt , \\end{split}\\ ] ] where the last equality follows from parseval s identity .",
    "furthermore , by fubini s theorem and the dominated convergence theorem we have @xmath254&=\\frac{1}{h_n}\\frac{1}{2\\pi}{\\int_{-\\infty}^{\\infty}}\\exp\\left(-\\frac{i{t}x}{h_n}\\right)\\ex\\left[\\exp\\left(\\frac{i{t}x_j}{h_n}\\right)\\right ] \\frac{\\phi_w(t)}{\\phi_k(t/\\rho_n)}dt\\\\ & = \\frac{1}{h_n}\\frac{1}{2\\pi}{\\int_{-\\infty}^{\\infty}}e^{-i{t}x / h}\\phi_f\\left(\\frac{t}{h_n}\\right)\\phi_w(t)dt\\\\ & = \\frac{1}{2\\pi}{\\int_{-\\infty}^{\\infty}}e^{-i{t}x}\\phi_f({t})\\phi_w(h_nt)dt\\\\ & \\rightarrow f(x ) .",
    "\\end{split}\\ ] ] the dominated convergence theorem is applicable because of assumption b ( i ) and ( iii ) . finally , let us consider @xmath255.$ ] writing @xmath256={\\int_{-\\infty}^{\\infty}}\\frac{1}{h_n^{2+\\delta}}\\left|w_{{\\rho}_n}\\left(\\frac{x - y}{h_n}\\right)\\right|^{2+\\delta}g_n(y)dy,\\ ] ] and using and lemma 1 , we obtain that @xmath257=o(h_n^{-1-\\delta}\\rho_n^{-\\beta(2+\\delta)}).\\ ] ] combination of , and yields that lyapunov s condition is fulfilled and hence that @xmath35 is asymptotically normal .",
    "formula then follows from and .",
    "this completes the proof ."
  ],
  "abstract_text": [
    "<S> let @xmath0 be i.i.d .  observations , where @xmath1 and the @xmath2 s and @xmath3 s are independent . </S>",
    "<S> assume that the @xmath2 s are unobservable and that they have the density @xmath4 and also that the @xmath3 s have a known density @xmath5 furthermore , let @xmath6 depend on @xmath7 and let @xmath8 as @xmath9 we consider the deconvolution problem , i.e.  the problem of estimation of the density @xmath4 based on the sample @xmath10 a popular estimator of @xmath4 in this setting is the deconvolution kernel density estimator . </S>",
    "<S> we derive its asymptotic normality under two different assumptions on the relation between the sequence @xmath6 and the sequence of bandwidths @xmath11 we also consider several simulation examples which illustrate different types of asymptotics corresponding to the derived theoretical results and which show that there exist situations where models with @xmath8 have to be preferred to the models with fixed @xmath12 + _ keywords : _ asymptotic normality , deconvolution , fourier inversion , kernel type density estimator . </S>",
    "<S> + _ ams subject classification : _ </S>",
    "<S> primary 62g07 ; secondary 62g20 + _ running title : _ deconvolution kernel density estimator </S>"
  ]
}