{
  "article_text": [
    "there is a large literature on the problem of estimating the modes of a density .",
    "known results include minimax rates of convergence , limiting distributions and the validity of bootstrap inference [ @xcite ( @xcite ) ] .",
    "the purpose of the current paper is to establish similar results for the estimation of _ ridges _ , an extension of modes to higher dimensions .",
    "intuitively , an @xmath0-ridge of a density is an @xmath0-dimensional set of high - density concentration .",
    "modes are just @xmath1-ridges .",
    "a density s ridges provide a useful summary of its structure and are features of interest in a variety methods and applications .",
    "figure  [ fig : ridgeestimation ] shows some one - dimensional density ridges .",
    "figure  [ fig::ex2 ] shows two simple datasets and estimates of the ridges .    in this paper",
    ", we consider the @xmath2 case , and we study the large - sample behavior of the plug - in ridge estimator based on a kernel estimator for the underlying density .",
    "let @xmath3 be a density , and let @xmath4 be a kernel estimator with bandwidth @xmath5 .",
    "the mean @xmath6 is a smoothed version of the density .",
    "we let @xmath7 denote the ridge of a density @xmath3 , defined formally in section  [ sec::ridges ] .",
    "we define @xmath8 as the estimated ridge and @xmath9 as the smoothed ridge .    [ cols=\"^,^ \" , ]      define the projection from one point @xmath10 onto a set @xmath11 by @xmath12 we define the projection vector from @xmath10 onto a set @xmath11 as @xmath13 the projection vector may not be unique .",
    "a condition related to the uniqueness of the projection is called the _",
    "reach _ and will be formally introduced in section  [ sec::as ] . the projection distance from @xmath10 onto @xmath11",
    "is @xmath14    the hausdorff distance between two subsets of @xmath15 is defined by @xmath16 where @xmath17 and @xmath18 .",
    "we also define the quasi - hausdorff distance @xmath19 as @xmath20 so that @xmath21 note that @xmath22    now we introduce some norms and semi - norms characterizing the smoothness of the density @xmath3 .",
    "a vector @xmath23 of nonnegative integers is called a multi - index with @xmath24 , and the corresponding derivative operator is @xmath25 where @xmath26 is often written as @xmath27 . for @xmath28 , define @xmath29 when @xmath30 , we have the infinity norm of @xmath3 ; for @xmath31 , these are semi - norms .",
    "we also define @xmath32 it is easy to verify that this is a norm .",
    "we define the _ local uncertainty _ by @xmath33    we estimate the local uncertainty measure by the bootstrap .",
    "let @xmath34 be the given observations .",
    "we define @xmath35 as the estimated ridge based on the bootstrap [ @xcite ] sample of @xmath36 .",
    "more precisely , let @xmath37 be a bootstrap sample from the empirical distribution @xmath38 .",
    "let @xmath39 be the kde based on the bootstrap sample .",
    "the bootstrap ridge is defined as @xmath40 we define @xmath41 as the estimated local uncertainty . algorithm  [ alg::lu ] gives a pseudo - code for estimating @xmath42 by the bootstrap .",
    "* input : * data @xmath43 .",
    "1 . estimate the ridges from @xmath43 ; denote the estimate by @xmath44 .",
    "2 . generate @xmath45 bootstrap samples : @xmath46 for @xmath47 .",
    "3 . for each bootstrap sample , estimate the ridges , yielding @xmath48 for @xmath47 .",
    "4 . for each @xmath49 , calculate @xmath50 , @xmath51 .",
    "5 . define @xmath52 .",
    "* output : * @xmath53 .      for making inferences about ridges , we focus on constructing a confidence set for @xmath54 , ignoring the bias @xmath55 . for suitable @xmath5",
    ", @xmath54 has essentially the same shape as @xmath56 and thus serves as a useful target .",
    "we call @xmath57 a valid @xmath58 confidence set if @xmath59 let @xmath60 be the value such that @xmath61 thus @xmath62 where @xmath63 although @xmath64 is unknown , we can estimate it by the bootstrap .",
    "we define @xmath65 where @xmath66 and where @xmath67 is constructed from an i.i.d .",
    "sample @xmath68 from the empirical distribution @xmath69 .",
    "algorithm  [ alg::ci ] provides a pseudo - code for constructing the confidence sets and theorem  [ thmm::gp5 ] shows its consistency .",
    "* input : * data @xmath70 , significance level @xmath71 .",
    "1 . estimate the ridge from @xmath43 ; denote this by @xmath44 .",
    "2 . generates bootstrap samples @xmath72 for @xmath47 .",
    "3 . for each bootstrap sample , estimate the ridge , call this @xmath73 .",
    "4 . for @xmath74 , calculate @xmath75 .",
    "5 . let @xmath76 be the @xmath71-upper quantile of @xmath77 . *",
    "output : * @xmath78 .",
    "for a vector @xmath79 , @xmath80 is the usual @xmath81 norm for the vector , and @xmath82 is the supremum norm for @xmath83 ; that is , @xmath84 for a matrix @xmath85 , let @xmath86 .",
    "when @xmath85 is symmetric , we define @xmath87 .",
    "we define @xmath88 to be the collection of @xmath89-times continuously differentiable functions . for a vector value function @xmath90",
    ", we define the gradient @xmath91 as a @xmath92 matrix given by @xmath93      we begin by defining the tangent vector @xmath94 to @xmath54 at each @xmath95 .",
    "let @xmath96 which is a @xmath97 matrix .",
    "we define @xmath94 to be the eigenvector corresponding to the largest eigenvalue of @xmath98 . as long as @xmath99 has rank @xmath100",
    ", @xmath94 is unique .",
    "[ lem::ns0 ] assume the matrix @xmath99 has rank @xmath100",
    ". then @xmath94 , the first eigenvector of @xmath101 is tangent to @xmath54 at @xmath95 .",
    "the column space of @xmath99 is normal to @xmath54 at each @xmath95 .",
    "the proof can be found in the supplementary material [ @xcite ] . by lemma  [ lem::ns0 ] ,",
    "the vector @xmath94 defined as above is always tangent to @xmath54 whenever @xmath95 .",
    "later we will see in claim 4 of lemma  [ lem::ns ] , condition ( p1 ) with smoothness on @xmath102 [ guaranteed by conditions ( k1)(k2 ) ] implies lemma  [ lem::ns0 ] .    with the above notation",
    ", we now formally describe our assumptions .",
    "the kernel @xmath103 is in @xmath104 and @xmath105 .",
    "let @xmath106 where @xmath107 is defined in ( [ eq::d1 ] ) , and let @xmath108 .",
    "we assume that @xmath109 is a vc - type class ; that is , there exist constants @xmath110 and a constant envelope @xmath111 such that @xmath112 where @xmath113 is the @xmath114-covering number for an semi - metric set @xmath115 with metric @xmath116 , and @xmath117 is the @xmath118 norm with respect to the probability measure @xmath119 .",
    "there exist constants @xmath120 such that @xmath121 for all @xmath122 .",
    "we call @xmath123 the _ gap_. note that @xmath124 defined in equation  ( [ eq::sr ] ) .    for each @xmath95 , @xmath125 where @xmath94 is the direction of @xmath54 at point @xmath95 defined in lemma  [ lem::ns0 ]",
    ".    conditions ( p1 ) , ( p2 ) hold for all small @xmath5 .",
    "now we discuss the conditions .",
    "( k1 ) is needed since the definition of density ridge requires twice differentiability .",
    "we need additional smoothness for making sure the estimated ridges are smooth .",
    "( k2 ) regularizes the complexity of kernel functions and its partial derivatives .",
    "this is to ensure the fourth derivatives of the kde will converge ; we need the fourth derivative since the reach of @xmath44 depends on the fourth derivative of @xmath126 by claim 7 in lemma  [ lem::ns ] .",
    "note that similar conditions to ( k2 ) appear in @xcite ( @xcite ) , @xcite .",
    "the gaussian kernel satisfies this condition .",
    "( p1 ) is the eigen condition which also appears in @xcite .",
    "this implies that the projected gradient near the ridge is smooth .",
    "this leads to a well - defined local normal coordinate along ridges ; see lemma  [ lem::ns ] .",
    "we require a slightly stronger condition ( existence of @xmath127 ) than @xcite .",
    "we use ( p2 ) to make sure the density ridge is also a generalized local mode in the normal space ; see lemma  [ lulem3 ] .",
    "note that whenever @xmath128 for some @xmath95 , @xmath10 must be a local mode in the normal space of @xmath54 at @xmath10 since all eigenvalues are negative .",
    "( p3 ) is required if we allow @xmath129 ; otherwise we do not need to assume it . note that if we say a density @xmath3 satisfies ( p1 ) or ( p2 ) , we mean that the condition holds for @xmath102 .",
    "finally , we consider the following assumption that will not be assumed in our main result but is useful and frequently assumed in working lemmas .    the density @xmath130 and has uniformly bounded derivatives to the fourth order .",
    "this condition will not be assumed in our main results since conditions ( k1)(k2 ) imply ( a1 ) for @xmath102 .      in this section , we show that under suitable conditions , for each point @xmath10 on the density ridge we can construct a matrix @xmath131 whose columns span the normal space of the density ridge at @xmath10 .",
    "let @xmath132 be a @xmath133 matrix with orthonormal columns .",
    "for such an @xmath132 , we define the _ subspace derivative _ by @xmath134 , which in turn gives the _ subspace gradient _ @xmath135 and the _ subspace hessian _",
    "@xmath136 thus @xmath137 and @xmath138 are the gradient and hessian generated by the partial derivatives along columns of @xmath132 ; this is the partial derivative in the subspace spanned by columns of @xmath132 .",
    "if @xmath132 is a unit vector , then @xmath139 is the _ directional derivative _ along @xmath132 .",
    "now we construct a local normal coordinate for the ridge .",
    "note in this subsection , all notation with subscript @xmath140 ( e.g. , @xmath141 ) denote the quantities defined for the smooth density @xmath140 . for any smooth density @xmath140 ,",
    "let @xmath142 denote the gradient and hessian of @xmath140 .",
    "for simplicity , we denote the eigenvectors and eigenvalues of @xmath143 using the same notation as before .",
    "let @xmath144 be the eigenvectors of @xmath143 corresponding to eigenvalues @xmath145 .",
    "as before , the ridge set @xmath146 is defined as the collection of @xmath10 such that @xmath147 with @xmath148 . by lemma  [ lem::ns0 ] ,",
    "the gradient of @xmath149 forms a matrix whose columns space spans the normal space to @xmath150 at each @xmath151 .",
    "define @xmath152 $ ] which is a @xmath97 matrix .",
    "@xcite ( page 65 ) shows that @xmath153 where @xmath154 is the @xmath155 identity matrix .",
    "the columns of @xmath156 span the normal space to @xmath150 at @xmath10",
    ". however , the columns of @xmath156 are not orthonormal .",
    "thus we perform an orthonormalization to @xmath156 to construct @xmath157 by the following steps : we have that @xmath158 .",
    "there exists a lower triangular matrix @xmath159 such that @xmath160 we then define @xmath161^{-1}.\\ ] ] note that @xmath156 might not be unique since the eigenvalues of @xmath143 can have multiplicities .",
    "when @xmath143 has multiplicities , any choice of linearly independent eigenvectors for @xmath143 will work in the above construction . as will be shown later , what we need is the smoothness of @xmath162 or @xmath163 , which is unaffected by multiplicities .",
    "the _ reach _ [ @xcite ] for a set @xmath11 , denoted by @xmath164 , is the largest real number @xmath89 such that each @xmath165 has a unique projection onto @xmath11 .",
    "the reach measures the smoothness of a set .",
    "[ lem::ns ] let @xmath140 be a density that satisfies and , and denote @xmath166 .",
    "let @xmath167 where @xmath123 is the gap defined in .",
    "let @xmath168 be constructed from ( [ eq::ns3 ] ) .",
    "then :    @xmath169 and @xmath170 have the same column space .",
    "also , @xmath171^{-1}m_q(x)^t . \\label{eq::ns5}\\ ] ] that is , @xmath162 is the projection matrix onto columns of @xmath156 .",
    "the columns of @xmath157 are orthonormal to each other .    for @xmath151 ,",
    "the column space of @xmath157 is normal to the direction of @xmath150 at  @xmath10 .    for all @xmath151 , @xmath172",
    "moreover , @xmath150 is a @xmath173-dimensional manifold that contains no intersection and no endpoints .",
    "namely , @xmath150 is a finite union of connected , closed curves .    when @xmath174 is sufficiently small and @xmath175 , @xmath176 for some constant @xmath177 .",
    "assume @xmath178 also satisfies and and @xmath179 is sufficiently small .",
    "then @xmath180 for some constant @xmath181 .",
    "the reach of @xmath150 satisfies @xmath182 for some constant @xmath183 .",
    "the proof can be found the supplementary material [ @xcite ] .",
    "we call @xmath157 the _ normal matrix _ since by claims 2 and 3 of the lemma , the columns of @xmath157 span the normal space to @xmath184 at @xmath10 . by claim 4 ,",
    "the ridge is a 1-dimensional manifold and by claim 13 and lemma  [ lem::ns0 ] , at each @xmath95 , the column space of @xmath185 spans the normal space to @xmath54 at  @xmath10 .",
    "claim  4 avoids cases in density ridges that are not well defined : endpoints and intersections .",
    "the eigenvectors near endpoints or intersections will be ill - defined .",
    "claim  5 proves that the projection matrix , @xmath162 , changes smoothly near @xmath150 .",
    "claim  6 shows that when two density functions are sufficiently close , the column space of @xmath157 will also be close .",
    "claim  7 gives the smoothness of @xmath150 in terms of the reach .    in the following sections , we work primarily on the ridge generated from @xmath102 and @xmath126 , so for simplicity we define @xmath186      let @xmath187 which is the subspace hessian matrix in the normal space along @xmath54 at @xmath10 .",
    "recall that @xmath131 is not uniquely defined ( due to possible multiplicities of eigenvalues ) , but any choice of @xmath131 constructed from ( [ eq::ns3 ] ) can be used in the definition of @xmath188 .",
    "lemma  [ lem::sh ] guarantees this invariance .",
    "let @xmath189 be the class of vector valued functions defined by @xmath190 define the empirical process @xmath191 where @xmath192    [ lu2 ] assume , .",
    "suppose that @xmath193 . if @xmath194 , then we further assume .",
    "then for all @xmath95 , when @xmath195 is sufficiently small , @xmath196 and @xmath197 , where @xmath198    we used theorem  [ lulem2 ] to convert the rate @xmath199 into @xmath200 in the first equality . an intuitive explanation for the approximation error rate @xmath201 comes from difference in normal matrices @xmath202 and @xmath203 by claim 6 in lemma  [ lem::ns ] .    for a fixed @xmath10 , @xmath204 is a vector and converges to a mean @xmath1 multivariate - normal distribution with covariance matrix @xmath205 having rank @xmath100 .",
    "this theorem also shows the asymptotic result for the local uncertainty measure @xmath206 . the matrix @xmath205 determines the behavior of @xmath206 and depends on three quantities : the normal matrix @xmath131 , the inverse of subspace hessian @xmath207 and the kernel function @xmath208 .",
    "the normal matrix comes from the fact that @xmath209 is asymptotically in the normal space of @xmath54 at @xmath10 .",
    "the inverse of subspace hessian @xmath210 plays the same role as the inverse hessian to a local mode .",
    "we will discuss its properties later .",
    "the last term comes from the kernel density estimator that depends on the kernel function we use .",
    "theorem  [ lu2 ] shows that the uncertainty measure has a limiting distribution that is similar to kde for estimating the gradient .",
    "the difference is the matrix @xmath211 whose properties are given in the following lemma .",
    "[ lem::sh ] assume .",
    "let @xmath212 then :    for any other @xmath97 matrix @xmath213 such that @xmath214 and @xmath215 , @xmath216 when @xmath122 .    when @xmath217 is sufficiently small , @xmath218 for some constant @xmath219 .",
    "assume another density @xmath140 satisfies and , and let @xmath220 be the counterpart of @xmath221 for density @xmath140 .",
    "when @xmath222 is sufficiently small , @xmath223 for some constant @xmath224 .",
    "the proof can be found the supplementary material [ @xcite ] .",
    "the first result shows that the matrix @xmath225 is the same for any orthonormal matrix @xmath213 whose column space spans the same space .",
    "this shows that @xmath221 is unaffected if multiplicity of eigenvalues occur .",
    "the second result gives the smoothness for @xmath226 , and the third result shows stability under small perturbation on the density .",
    "now we show that the uncertainty measure @xmath206 can be estimated by the bootstrap .",
    "given the observed data @xmath227 , we generate the bootstrap sample @xmath228 .",
    "we use the bootstrap sample to construct the bootstrap kde @xmath229 the bootstrap ridge is @xmath230 let @xmath231 be the bootstrap estimate to the local uncertainty measure .",
    "[ bt ] assume , for all large @xmath232 the following is true . there exists an event @xmath233 such that @xmath234 for some constant @xmath235 , and for @xmath236 , when @xmath195 is sufficiently small , for all @xmath95 :    the set @xmath237 .",
    "the estimated ridge satisfies : @xmath238 .",
    "suppose that @xmath193 .",
    "the estimated local uncertainty measure is consistent in the sense that for any @xmath239 , @xmath240    [ if we allow @xmath129 , we need to assume . ]    note that we need the above set - based argument because @xmath241 and @xmath242 are defined on different supports : @xmath241 is defined on @xmath54 while @xmath242 is defined on @xmath44 .",
    "this theorem shows that as @xmath44 is approaching @xmath54 , the estimated local uncertainty on @xmath44 will converge to the local uncertainty defined on @xmath54 .      in this section ,",
    "we derive the limiting distribution of the hausdorff distance .",
    "let @xmath243 be a centered , tight gaussian process defined on @xmath244 with covariance function @xmath245 - \\mathbb{e}\\bigl[f_1(x_i)\\bigr ] \\mathbb{e } \\bigl[f_2(x_i)\\bigr].\\ ] ] such gaussian processes exists if @xmath244 is pre - gaussian .",
    "the kernel functions and its derivatives of order less than four are pre - gaussian by assumption ( k2 ) .",
    "[ thmm::gp ] assume conditions , and that @xmath193",
    ". then there exists a gaussian process @xmath243 defined on a function space @xmath246 [ see equation  ( [ eq::ch1 ] ) ] such that , when @xmath232 is sufficiently large , @xmath247 we can replace @xmath248 with @xmath249 in the above . if we allow @xmath129 , we need to assume .",
    "here we provide an intuitive explanation . from theorem",
    "[ lu2 ] , the local uncertainty vector @xmath209 can be approximated by an empirical process .",
    "recall from equations  ( [ eq::pr1 ] ) , ( [ eq::pr2 ] ) , ( [ eq::ahaus ] ) , we have @xmath250 the hausdorff distance and the quasi - hausdorff distance will be the same when the two ridges are close enough ; see lemma  [ lem::ah ] .",
    "the above argument shows the connection between hausdorff distance and the empirical process .",
    "the rest of the proof of theorem  [ thmm::gp ] establishes the approximation of the empirical process by the gaussian process and applies an anti - concentration argument due to @xcite to construct the berry  esseen type bound .",
    "as a referee points out , the hausdorff distance is usually unstable .",
    "here we obtain a nice concentration because of assumption ( p1)(p2 ) along with the fact that @xmath102 has fourth derivatives .",
    "these conditions ensure the density near ridges is well behaved .",
    "to show our confidence set is consistent , we need to show that @xmath251 has the same limit as @xmath252    [ thmm::gpb ] assume conditions and and that @xmath193 . for all large @xmath232",
    "the following is true .",
    "there exists an event @xmath233 such that @xmath234 for some constant @xmath235 , and for @xmath236 , there exists a gaussian process @xmath243 defined on a space @xmath246 [ see equation  ( [ eq::ch1 ] ) ] such that @xmath253 a similar result also holds when replacing @xmath254 by @xmath255 .",
    "note that if we allow @xmath129 , we need to assume .",
    "the above result , together with theorem  [ thmm::gp ] , establishes a berry  esseen result for the bootstrap estimate for the distribution of @xmath248 .",
    "theorem  [ thmm::gpb ] gives the rate for the bootstrap case .",
    "one might expect the rate to be @xmath256 in light of theorem  [ thmm::gp ] .",
    "the second term @xmath257 comes from the difference in support of the two ridges @xmath258 .",
    "the rate is related to the rate estimating the third derivative of a density , which contributes to the difference in normal spaces between points of @xmath54 and @xmath44 .",
    "we now have the following result on the coverage of the confidence set .    [ thmm::gp5 ]",
    "assume , and that @xmath259 .",
    "let @xmath260 .",
    "then @xmath261 if we allow @xmath129 , we need to assume .",
    "this theorem is a direct result of theorems [ thmm::gp ] and [ thmm::gpb ] , so we omit the proof . note that here @xmath76 differs to the one defined in algorithm [ alg::ci ] ( and section  [ sec::ci ] ) by a factor @xmath262 .",
    "this is because we rescale @xmath263 when defining @xmath264 .    as a referee points out",
    ", one can use @xmath265 as a replacement for @xmath266 and use the bootstrap to construct a confidence set .",
    "this is a variance - stabilizing version for the original confidence set .",
    "this confidence set is also valid by a simple modification of theorems  [ thmm::gp][thmm::gp5 ] .",
    "we consider two simulation settings : the circle data and the smoothed box data .",
    "for all simulations , we use a sample size of @xmath267 .",
    "we choose the bandwidth @xmath5 using silverman s rule [ @xcite ] .",
    "the first dataset is the circle data .",
    "see figure  [ fig::cc1 ] .",
    "we show the true smoothed ridge ( red ) and the estimated ridge ( blue ) along with the @xmath268 confidence sets ( gray regions ) .",
    "the second dataset is the box data ; see figure  [ fig::cc1 ] .",
    "notice that the original box data has corners that violate condition ( p1 ) , but the ridge of the smoothed density @xmath102 obeys ( p1 ) .",
    "we show the @xmath268 confidence sets .",
    "the box data has a large angle near its corner , but our confidence set still has good behavior over these regions .",
    "confidence sets for the circle data ( left ) and box data ( right ) .",
    "the red curve is the smoothed ridge @xmath54 , and the gray regions are confidence sets . ]",
    "we prove the main theorems in this section .",
    "the proofs for the lemmas ( including those used for proving the main theorems ) are given in the supplementary material ; see and @xcite .",
    "before we prove theorem  [ lu2 ] , we state three useful lemmas .",
    "[ lulem3 ] let @xmath56 be the ridge of a density @xmath3 .",
    "for @xmath269 , let the hessian at @xmath10 be @xmath270 with eigenvectors @xmath271 $ ] and eigenvalues @xmath272 .",
    "consider any subspace @xmath273 spanned by a basis @xmath274 $ ] with @xmath275 in the normal direction of that subspace .",
    "then a sufficient condition for @xmath10 being a local mode of @xmath3 constrained to @xmath273 is @xmath276    the proof can be found in the supplementary material [ @xcite ] .",
    "the following lemma is a uniform bound for the kde .",
    "[ lulem2 ] assume and that @xmath277 for some @xmath278 .",
    "then we have @xmath279 for @xmath280 .",
    "in particular , if we consider the smoothed version of density , @xmath102 , for the same kernel function , then we have @xmath281 for @xmath280 .    [ lulem4 ]",
    "then we have @xmath282 for @xmath280 .",
    "this lemma follows directly from talagrand s inequality  [ @xcite ] , which proves an exponential concentration inequality for random variable @xmath283 .",
    "thus the second moment is bounded at the specified rate .    in the next proof",
    ", we will frequently use the following theorem that links the uniform derivative difference to the hausdorff distance .",
    "[ lu0 ] assume condition , for two densities @xmath284 . when @xmath285 is sufficiently small",
    ", we have @xmath286 .",
    "proof of theorem  [ lu2 ] theorem  [ lu2 ] makes two claims : the first claim is an empirical approximation @xmath287 and the second claim is the limiting behavior for the uncertainty measure @xmath206 .",
    "we prove the empirical approximation first and then use it to show the asymptotic theory for the uncertainty measure .    _",
    "proof for the empirical approximation_. let @xmath288 and @xmath289 , and define @xmath290 to be the normal space at @xmath95 and @xmath49 , respectively .",
    "note that when @xmath291 is sufficiently small , we have ( p1 ) for  @xmath126 .",
    "this implies that @xmath292 can be defined ( but they are not necessarily unique ) for points near @xmath258 by claim 3 of lemma  [ lem::ns ] .",
    "condition ( p3 ) ensures that the constants in ( p1 ) and the reach of @xmath102 have positive lower bound as @xmath129 for @xmath102 .    by ( p2 ) and lemma  [ lulem3 ] , the ridges are the local modes in the subspace @xmath131 . note that despite the fact that @xmath131 may not be unique , the column space of @xmath131 is unique by claim 5 in lemma  [ lem::ns ] .",
    "hence we have @xmath293 for all @xmath95 and @xmath294 .",
    "this shows that ridges are generalized local modes with respect to their local normal coordinate .",
    "let @xmath295 . when @xmath296 is smaller than the reach of @xmath44 , the projection @xmath297 is unique",
    "by claim 7 in lemma  [ lem::ns ] and the fact that @xmath298 from theorem  [ lu0 ] , the reach of @xmath44 and the reach of @xmath54 will be close once @xmath195 is sufficiently small .",
    "accordingly , @xmath299 is unique once @xmath300 is sufficiently small .",
    "this leads to @xmath301 we use the fact that @xmath302 has bounded derivatives from ( k1 ) .",
    "accordingly , @xmath303 converges to @xmath302 .",
    "hence , when @xmath291 is sufficiently small , @xmath304 by lemma  [ lulem3 ] . since @xmath304 , @xmath305,\\ ] ] which leads to @xmath306&=&-n(x)^t\\bigl[\\hat { g}_n(x)-g_h(x)\\bigr ] \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & = & -n(x)^t \\bigl[\\hat{g}_n(x)-\\mathbb{e } \\bigl ( \\hat{g}_n(x ) \\bigr)\\bigr].\\end{aligned}\\ ] ] we used @xmath307 in the last equality . since @xmath296 is small due to theorem  [ lu0 ] , and @xmath308 is small , we use taylor s theorem for the first term which yields @xmath309\\nonumber \\\\ & & \\qquad= n(x)^t \\int_{0}^1 \\hat{h}_n\\bigl(x+(\\tilde{x}-x)t\\bigr)\\,dt ( \\tilde{x}-x ) \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & \\qquad= n(x)^t h(x ) \\bigl(1+o\\bigl(\\|\\hat{p}_h - p_h \\|^*_{\\infty,2}\\bigr)+o\\bigl(\\|\\tilde { x}-x\\|\\bigr ) \\bigr ) ( \\tilde{x}-x ) \\\\ & & \\qquad= n(x)^t h(x ) ( \\tilde{x}-x ) \\bigl(1+o\\bigl(\\|\\hat { p}_h - p_h\\|^*_{\\infty , 2}\\bigr ) \\bigr).\\nonumber\\end{aligned}\\ ] ] we use the fact that @xmath310 in the second equality and apply theorem  [ lu0 ] to absorb @xmath311 into the other term . by claims 5 , 6 in lemma  [ lem::ns ] and",
    "the fact that the line segment joining @xmath312 and @xmath10 is contained in @xmath313 by ( p1 ) , we have @xmath314    now @xmath315 . combining this with equations  ( [ eq::lu2pf2 ] ) , ( [ eq::lu2pf3 ] ) and ( [ eq::lu2pf4 ] ) we obtain @xmath316\\nonumber \\\\ & & \\qquad= n(x)^t\\bigl[\\hat{g}_n(\\tilde{x})- \\hat{g}_n(x)\\bigr]\\nonumber \\\\ & & \\qquad = n(x)^t h(x ) ( \\tilde{x}-x ) \\bigl(1+o\\bigl(\\|\\hat{p}_h - p_h \\|^*_{\\infty , 2}\\bigr)\\bigr ) \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & \\qquad = n(x)^t h(x)\\hat{n}_n(\\tilde{x})\\hat{n}_n ( \\tilde{x})^t ( \\tilde { x}-x ) \\bigl(1+o\\bigl(\\|\\hat{p}_h - p_h \\|^*_{\\infty,2}\\bigr)\\bigr ) \\\\ & & \\qquad = n(x)^t h(x)n(x)n(x)^t ( \\tilde{x}-x ) \\bigl(1+o\\bigl ( \\|\\hat { p}_h - p_h\\| ^*_{\\infty,3}\\bigr)\\bigr)\\nonumber \\\\ & & \\qquad = h_n(x ) n(x)^t ( \\tilde{x}-x ) \\bigl(1+o\\bigl(\\| \\hat{p}_h - p_h\\|^*_{\\infty,3}\\bigr)\\bigr),\\nonumber\\end{aligned}\\ ] ] where @xmath317 in the fourth equality , we used  ( [ eq::lu2pf4 ] ) . multiplying the matrix @xmath318 to the left of both sides and moving @xmath199 to the other side , @xmath319 \\\\[-8pt ] \\nonumber & & \\qquad= -h_n(x)^{-1}n(x)^t \\bigl [ \\hat{g}_n(x)-\\mathbb{e}\\bigl(\\hat{g}_n(x)\\bigr)\\bigr ] \\bigl(1+o\\bigl(\\|\\hat{p}_h - p_h\\|^*_{\\infty,3}\\bigr ) \\bigr).\\end{aligned}\\ ] ]    we multiply by @xmath131 and use ( [ eq::lu2pf4 ] ) again to obtain @xmath320 \\\\[-8pt ] \\nonumber & = & ( \\tilde{x}-x ) \\bigl(1+o\\bigl(\\|\\hat{p}_h - p_h \\|^*_{\\infty,3}\\bigr)\\bigr).\\end{aligned}\\ ] ]",
    "let @xmath321 , and define @xmath322 . combining ( [ eq::lu2pf5 ] ) and ( [ eq::lu2pf5 - 1 ] ) , @xmath323 \\bigl(1+o\\bigl(\\|\\hat{p}_h - p_h\\|^*_{\\infty,3}\\bigr )",
    "\\label{eq::lu2pf6}\\ ] ] notice that the kde can be expressed in terms of the empirical process via @xmath324 \\\\[-8pt ] \\nonumber & & \\qquad=\\frac{1}{\\sqrt{n}}\\mathbb{g}_n(\\tau_x ) , \\ ] ] where @xmath325 . from equation  ( [ eq::lu2eq1 ] ) , @xmath326 for all @xmath95 .",
    "hence , multiplying ( [ eq::lu2pf6 ] ) by @xmath327 and using ( [ eq::lu2pf7 ] ) and ( [ eq::lu2pf8 ] ) , @xmath328 for each @xmath95 .",
    "note that the bound @xmath199 is independent of @xmath10 and the above construction is valid for all @xmath329 .",
    "hence @xmath330 this proves the approximation for @xmath209 .    _",
    "proof for the uncertainty measures_. we first prove that the local uncertainty measure @xmath331 converges to @xmath332 .",
    "then we show the limiting behavior for @xmath333 .",
    "we have @xmath334 \\\\[-8pt ] \\nonumber & & \\hspace*{76pt}\\quad=\\mathbb{e}\\bigl{\\vert}nh^{d+2}\\bigl\\|\\mathbf{d}(x,\\hat{r}_h ) \\bigr\\|^2-\\bigl\\|\\mathbb { g}_n(f_x)\\bigr\\|^2 \\bigr{\\vert}\\\\ & & \\hspace*{76pt}\\quad= \\mathbb{e } \\bigl{\\vert}(d_n - g_n ) ^t ( d_n+g_n ) \\bigr{\\vert}\\nonumber \\\\ & & \\mbox{(cauchy -- schwarz)}\\leq\\sqrt{\\mathbb{e } \\bigl ( \\|d_n - g_n \\| ^2 \\bigr)\\mathbb{e } \\bigl ( \\|d_n+g_n \\|^2 \\bigr)},\\nonumber\\end{aligned}\\ ] ] where @xmath335 and @xmath336 .",
    "now by ( [ eq::lu2pf9 ] ) and lemma  [ lulem4 ] , @xmath337 note @xmath338 , which implies @xmath339 . taking expectation on both sides and using the fact that @xmath340 is @xmath81 norm for the random variable @xmath11 , @xmath341 \\\\[-8pt ] \\nonumber & \\leq & \\bigl(\\sqrt{\\mathbb{e}\\bigl(\\|2g_n\\|^2\\bigr ) } + \\sqrt{\\mathbb { e}\\bigl(\\|d_n - g_n\\|^2\\bigr ) } \\bigr)^2.\\end{aligned}\\ ] ] again by ( [ eq::lu2pf9 ] ) and",
    "lemma  [ lulem4 ] , @xmath342    here we derive @xmath343 .",
    "recall that @xmath344 , where @xmath345 for each @xmath95 by ( [ eq::lu2eq1 ] ) .",
    "note that @xmath346 and @xmath347 .",
    "hence @xmath348 \\\\[-8pt ] \\nonumber & = & \\operatorname{trace } \\bigl({\\operatorname{cov}}(g_n ) \\bigr ) \\\\ & = & \\operatorname{trace } \\bigl(\\sigma(x ) \\bigr ) , \\nonumber\\end{aligned}\\ ] ] where @xmath349 is bounded .",
    "thus by ( [ eq::lu2pf10])([eq::lu2pf12 ] ) we conclude that @xmath350    thus the uncertainty measure @xmath241 can be approximated by @xmath351 .",
    "now by ( [ eq::lu2pf12 ] ) , the result follows .",
    "before we prove the bootstrap result , we need the following lemma .",
    "[ lem::pb ] let @xmath102 be the smoothed density and @xmath54 be the associated ridges .",
    "let @xmath126 be the kde based on the observed data @xmath352 and @xmath44 be the estimated ridge .",
    "consider these two conditions :    holds for @xmath126 .",
    "@xmath353 for a small constant @xmath354 .",
    "let @xmath355 . then , when @xmath232 is sufficiently large , @xmath356 for some constant @xmath235 .",
    "the proof can be found in the supplementary material [ @xcite ] .",
    "proof of theorem  [ bt ] to prove the bootstrap result , we use a technique of @xcite by first considering a sequence of nonrandom distributions @xmath357 . in the last step ,",
    "we replace @xmath358 by the empirical distribution @xmath38 .",
    "let @xmath359 be the density of the smoothed distribution @xmath360 where @xmath361 is the kernel function used in the kde and @xmath362 is the convolution operator .",
    "if we replace @xmath358 with the sample distribution @xmath363 , the smoothed distribution has density @xmath102 .",
    "if we replace @xmath358 with the empirical distribution @xmath364 , we obtain the kde @xmath126 .",
    "we assume that each smoothed density @xmath359 satisfies conditions ( p1)(p2 ) and @xmath365 , and @xmath366 is sufficiently small for all @xmath367 .",
    "let @xmath368 .",
    "let @xmath369 where @xmath370 .",
    "let @xmath371 be the kde based on @xmath372 , and let @xmath373 .",
    "let @xmath374 for @xmath375 be the local uncertainty measure .",
    "when @xmath376 is sufficiently small , we can apply theorem  [ lu2 ] to @xmath377 so that @xmath378 where @xmath379\\ ] ] for @xmath375 .",
    "note that although we do not assume ( p3 ) for @xmath359 , theorem  [ lu2 ] is still valid once the gap constants in ( p1 ) have positive lower bound . in this case , because @xmath201 is sufficiently small and we assume ( p3 ) for @xmath102 , the gap constants have a lower bound for @xmath359 as @xmath359 approaching @xmath102 .",
    "now we proceed with the proof .",
    "claims 1 and 2 are trivially true by the definition of hausdorff distance .",
    "now we prove claim 3 .",
    "when @xmath376 is sufficiently small , @xmath380 for any point @xmath95 , and any @xmath381 , @xmath382    since @xmath383 is on @xmath377 , the local uncertainty @xmath384 is well defined .",
    "then @xmath385 since the terms in @xmath205 involve only the derivatives of the smoothed density up to the third order and since @xmath386 , we conclude that @xmath387 the @xmath388 term does not depend on @xmath10 so that this can be taken uniformly for all @xmath95 .",
    "this proves claim 3 .    for the bootstrap case ,",
    "we replace @xmath358 by @xmath364 .",
    "thus @xmath359 is replaced by @xmath126 so that we obtain the result .",
    "notice that we require that @xmath126 satisfies ( p1)(p2 ) and that @xmath195 be sufficient small .",
    "recall that @xmath233 is the collection of @xmath36 such that @xmath126 satisfies conditions ( p1)(p2 ) and @xmath195 is sufficiently small .",
    "applying lemma  [ lem::pb ] we conclude that @xmath389 for some constant @xmath235 .",
    "before we prove the gaussian approximation , we need the following lemma that links the quasi - hausdorff distance to the hausdorff distance .",
    "[ lem::ah ] let @xmath390 be two closed , nonself - intersecting curves with positive reach . if @xmath391 then @xmath392    the proof can be found in the supplementary material [ @xcite ] .",
    "proof of theorem  [ thmm::gp ] our proof consists of three steps .",
    "the first step establishes a coupling between the hausdorff distance @xmath248 and the supremum of an empirical process .",
    "the second step shows that the distribution of the maxima of the empirical process can be approximated by the maxima of a gaussian process .",
    "the last step uses anti - concentration to bound the distributions between @xmath248 and the maxima of a gaussian process .",
    "_ step empirical process approximation . _",
    "recall that @xmath393 is the empirical process defined by @xmath394 \\\\[-8pt ] \\nonumber \\operatorname{cov } \\bigl(\\mathbb{g}_n(f_1 ) , \\mathbb{g}_n(f_2)\\bigr ) & = & \\mathbb { e } \\bigl(f_1(x_1)f_2(x_1)\\bigr)\\end{aligned}\\ ] ] for any two functions @xmath395 .",
    "we also recall the function @xmath396 in  ( [ eq::lu2eq1 ] ) , @xmath397 note that @xmath398 is a vector .",
    "let @xmath399 by theorem  [ lu2 ] , @xmath330 since the @xmath81 norm is bounded by @xmath400 times the infinity norm for a vector , @xmath401 for any vector @xmath402 , @xmath403 where @xmath404 .",
    "hence @xmath405    define @xmath406 .",
    "recall that the asymptotic hausdorff distance is @xmath407 .",
    "then @xmath408 this shows that the quasi - hausdorff distance can be approximated by the supremum of an empirical process over the functional space @xmath246 .    when @xmath195 is sufficiently small , the reach of @xmath44 is close to the reach of @xmath54 by claim 7 of lemma  [ lem::ns ] , and the hausdorff distance is much smaller than the reach . by lemma  [ lem::ah ] ,",
    "the quasi - hausdorff distance is the same as the hausdorff distance so that @xmath409 equation ( [ eq::ah1 ] ) is the coupling between hausdorff distance and the supremum of an empirical process and is the main result for step 1 .",
    "note that a sufficient condition for @xmath195 being small is that @xmath193 .",
    "this is the bandwidth condition we require .",
    "_ step gaussian approximation . _    in this step , we use a theorem of  @xcite to show that the supremum of the empirical process can be approximated by the supremum of a centered , tight gaussian process @xmath243 defined on @xmath246 with covariance function @xmath410 - \\mathbb{e}\\bigl[f_1(x_i)\\bigr]\\mathbb{e } \\bigl[f_2(x_i)\\bigr]\\ ] ] for @xmath411 .",
    "we first recall the theorem of  @xcite .",
    "[ thmm::gpa1 ] let @xmath412 be a collection of functions that is a vc - type class [ see condition ] with a constant envelope function @xmath413 .",
    "let @xmath414 be a constant such that @xmath415\\leq\\sigma^2\\leq b^2 $ ] .",
    "let @xmath243 be a centered , tight gaussian process defined on @xmath412 with covariance function @xmath416 - \\mathbb{e}\\bigl[g_1(x_i)\\bigr]\\mathbb{e } \\bigl[g_2(x_i)\\bigr],\\ ] ] where @xmath417 .",
    "then for any @xmath418 as @xmath232 is sufficiently large , there exists a random variable @xmath419 such that @xmath420 where @xmath421 are two universal constants .",
    "now we show that @xmath412 in theorem  [ thmm::gpa1 ] can be linked to @xmath422 with a proper scaling . from condition ( k2 ) ,",
    "the collection @xmath423 is a vc - type pre - gaussian class with a constant envelope @xmath111 .",
    "recall equation  ( [ eq::lu2eq1r ] ) : @xmath424 this function will not be uniformly bounded as @xmath129 , so we consider @xmath425 \\\\[-8pt ] \\nonumber & = & n(x ) h_n^{-1}(x ) n(x)^t ( \\nabla k ) \\biggl(\\frac { x - y}{h } \\biggr),\\qquad x\\in r_h.\\end{aligned}\\ ] ] note that each element of the vector @xmath426 is uniformly bounded .",
    "this is because @xmath427 for some universal constant since @xmath131 is generated by the derivatives of @xmath102 with order less than four and by ( k1 ) is uniformly bounded .",
    "now we define @xmath428 \\\\[-8pt ] \\nonumber & = & \\bigl\\{\\sqrt{h^{d+2 } } f\\dvtx f\\in\\mathcal{f}_{h}\\bigr \\}.\\end{aligned}\\ ] ] since @xmath429 and @xmath430 and @xmath111 is a constant envelope for the partial derivatives of kernel functions , @xmath431 is a constant envelope for @xmath432 and @xmath432 is a vc - type class .",
    "in addition , @xmath433\\leq h^{d+2 } b_1 ^ 2\\leq b_1 ^ 2\\ ] ] as @xmath434 .",
    "so we can choose @xmath435 in theorem  [ thmm::gpa1 ] .",
    "applying theorem  [ thmm::gpa1 ] and ( [ eq::gp1 - 2 ] ) , there exist random variables @xmath436 \\\\[-8pt ] \\nonumber \\mathbf{b}_2 & \\stackrel{d } { = } & \\|\\mathbb{b}\\|_{\\mathcal{f}_h}\\end{aligned}\\ ] ] such that @xmath437 \\\\[-8pt ] \\nonumber { \\mathbb{p}}\\biggl ( \\bigl|\\|\\mathbb{g}_n\\|_{\\mathcal{f}_h } - \\mathbf{b}_2 \\bigr|>a_1\\frac{b_1\\log^{2/3 } n}{\\gamma^{1/3 } ( nh^{d+2})^{1/6 } } \\biggr)&\\leq & a_2 \\gamma\\end{aligned}\\ ] ] for two universal constants , when @xmath232 is sufficiently large and @xmath438 . the second result comes from the one - to - one correspondence between @xmath432 and @xmath246 with a constant scaling",
    ".    now recall ( [ eq::ah1 ] ) from the end of step 1 : @xmath439 which implies that there exists a constant @xmath440 such that for any @xmath441 , @xmath442 for some constant @xmath443 as @xmath232 is sufficiently large . note",
    "that we apply talagrand s inequality ( see lemma  [ lem::pb ] ) in the last inequality .",
    "choose @xmath444 in ( [ eq::gp1-tala ] ) , combine it with ( [ eq::gp1 - 3 ] ) and use the fact that @xmath445 converges much faster than @xmath446 , to conclude that @xmath447 for some constants",
    ". we can replace @xmath181 by @xmath219 and @xmath183 by @xmath224 to absorb the extra small terms from ( [ eq::gp1-tala ] ) and the envelope @xmath449 .",
    "_ step anti - concentration bound . _    to convert the above result into a berry  esseen type bound , we use the anti - concentration inequality in ( corollary  2.1 ) in  @xcite ; a similar result appears in chernozhukov , chetverikov and= kato ( @xcite ) .",
    "here we use a modification of the anti - concentration inequality .",
    "[ [ modification of corollary  2.1 in  @xcite ] ] [ lem::gpa3 ] let @xmath450 be a gaussian process with index @xmath451 , and with semi - metric @xmath116 such that @xmath452 for all @xmath451 .",
    "assume that @xmath453 a.s .",
    "and there exists a random variable @xmath454 such that @xmath455 . if @xmath456 , then @xmath457 for some constant @xmath458 .",
    "this lemma is a direct application of corollary  2.1 of chernozhukov , chetverikov and kato ( @xcite ) , so we omit the proof .",
    "we apply lemma  [ lem::gpa3 ] to equation  ( [ eq::gp1 - 4 ] ) which yields @xmath459 \\\\[-8pt ] \\nonumber & & \\qquad = a_6 \\biggl(a_3\\frac{\\log^{2/3 } n}{\\gamma^{1/3 } ( nh^{d+2})^{1/6 } } + a_4 \\gamma \\biggr),\\end{aligned}\\ ] ] where @xmath460 is a constant .",
    "we use the fact that @xmath461 and @xmath462 have the same distribution .",
    "choosing @xmath463 completes the proof .    for the case of @xmath266 ,",
    "the result follows by using ( [ eq::ahaus2 ] ) rather than ( [ eq::ah1 ] ) in the empirical approximation .",
    "proof of theorem  [ thmm::gpb ] the proof for the bootstrap result is very similar to the previous theorem .",
    "the major difference is that the estimated ridges and the smoothed ridges have different supports .",
    "this makes the functional spaces different .",
    "our strategy for proving this theorem has three steps .",
    "first , we show that the hausdorff distance @xmath464 conditioned on the observed data can be approximated by an empirical process .",
    "this is the same as step 1 in proof of theorem  [ thmm::gp ] .",
    "second , we apply the result of theorem  [ thmm::gp ] to bound the difference between the distributions of @xmath464 and a gaussian process defined on the @xmath44 .",
    "this uses the second and the third steps of the previous proof .",
    "the last step shows that the gaussian process defined on @xmath44 is asymptotically the same as being defined on  @xmath54 .",
    "let @xmath236 , and recall that by lemma  [ lem::pb ] , @xmath465 .",
    "_ step empirical approximation .",
    "_    let @xmath466 be the observed data .",
    "let @xmath467 .",
    "let @xmath468 be the bootstrap kde ( kde based on the bootstrap sample ) .    in the following ,",
    "we assume that @xmath236 and treat @xmath36 as fixed .",
    "hence , @xmath126 and @xmath44 are fixed . in this case ,",
    "theorem  [ lu2 ] can be applied to the local uncertainty vector , that is , @xmath469 where @xmath470 note that @xmath471 is the matrix with column space equal to the normal space of @xmath44 at @xmath10 , and @xmath472 is the corresponding subspace hessian matrix of the space spanned by columns of @xmath471 .",
    "define @xmath473 then by the same argument as in the paragraph before the proof of theorem  [ thmm::gp ] , we have a similar result to ( [ eq::ah1 ] ) , @xmath474    _ step gaussian approximation . _",
    "we use the same proof as in theorem  [ thmm::gp ] .",
    "we apply theorem  [ thmm::gp ] to conclude that @xmath475 \\\\[-8pt ] \\nonumber & & \\qquad = o \\biggl ( \\biggl(\\frac{\\log^4 n}{nh^{d+2 } } \\biggr)^{1/8 } \\biggr).\\end{aligned}\\ ] ]    _ step support approximation . _    in the previous step , the approximating distribution is a gaussian process over the function space @xmath476 , which is not the same as @xmath246 .",
    "now we apply lemma  [ lem::gcr ] and the fact that @xmath477 to get @xmath478 combining ( [ eq::gpb-4 ] ) , ( [ eq::gpb-5 ] ) and the fact that @xmath479 , we conclude @xmath480    consider two densities @xmath284 satisfying conditions ( a1 ) , ( p1)(p2 ) .",
    "let @xmath390 be the density ridges for @xmath284 , respectively .",
    "we assume conditions ( k1)(k2 ) .",
    "define @xmath481 where @xmath482 note that we have two indices @xmath483 for each element in @xmath484 and @xmath485 .",
    "the first index @xmath10 is the _ location _ , and the second index @xmath486 is the _",
    "direction_. @xmath487 is the normal matrix ( as @xmath488 , its column space is the normal space ) defined by lemma  [ lem::ns ] at @xmath10 , and @xmath489 is the subspace hessian in the columns space of @xmath487 .",
    "[ lem::gcr ] when @xmath490 is sufficiently small , we have @xmath491    the proof can be found in the supplementary material [ @xcite ] .",
    "we thank the reviewers for helpful comments . we also thank alessandro rinaldo for useful comments about lemma  [ lulem3 ] ."
  ],
  "abstract_text": [
    "<S> the large sample theory of estimators for density modes is well understood . in this paper </S>",
    "<S> we consider density ridges , which are a higher - dimensional extension of modes . </S>",
    "<S> modes correspond to zero - dimensional , local high - density regions in point clouds . </S>",
    "<S> density ridges correspond to @xmath0-dimensional , local high - density regions in point clouds . </S>",
    "<S> we establish three main results . </S>",
    "<S> first we show that under appropriate regularity conditions , the local variation of the estimated ridge can be approximated by an empirical process . </S>",
    "<S> second , we show that the distribution of the estimated ridge converges to a gaussian process . </S>",
    "<S> third , we establish that the bootstrap leads to valid confidence sets for density ridges </S>",
    "<S> .    ./style / arxiv - general.cfg    , </S>"
  ]
}