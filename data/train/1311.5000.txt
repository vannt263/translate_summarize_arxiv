{
  "article_text": [
    "let @xmath6 and @xmath7 be an @xmath8 matrix of i.i.d .",
    "( independent and identically distributed ) complex random variables with mean 0 and variance 1 .",
    "we consider , a class of sample covariance matrices @xmath9 where @xmath10 denotes the conjugate transpose of the data matrix @xmath11 . the empirical spectral distribution ( esd ) @xmath12 of @xmath13 is then defined as @xmath14 where @xmath15 are the eigenvalues of @xmath13 in ascending order and @xmath16 is the conventional indicator function .",
    "marenko and pastur in @xcite proved that with probability 1 , @xmath12 converges weakly to the standard marenko ",
    "pastur distribution @xmath17 with density function @xmath18 where @xmath19 and @xmath20 . here",
    "the positive constant @xmath21 is the limit of dimension to sample size ratio when both @xmath1 and @xmath2 tend to infinity .    in applications of asymptotic theorems of spectral analysis of large dimensional random matrices ,",
    "one of the important problems is the convergence rate of the esd .",
    "the kolmogorov distance between the expected esd of @xmath13 and the marenko ",
    "pastur distribution @xmath17 is defined as @xmath22 as well as the distance between two distributions @xmath12 and @xmath17 , @xmath23 notice that , for any constant @xmath24 , @xmath25 thus , @xmath26 measures the rate of convergence in probability .",
    "bai in @xcite firstly tackled the problem of convergence rate and established three berry  esseen type inequalities for the difference of two distributions in terms of their stieltjes transforms .",
    "gtze and tikhomirov in @xcite further improved the berry  esseen type inequlatiy and showed the convergence rate of @xmath12 is @xmath0 in probability under finite 8th moment condition .",
    "more recently , a sharper bound is obtained by pillai and yin in @xcite , under a stronger condition , that is , the sub - exponential decay assumption .",
    "it is shown that the difference between eigenvalues of @xmath13 and the marenko ",
    "pastur distribution is of order @xmath27 in probability .    in the literature ,",
    "research on limiting properties of eigenvectors of large dimensional sample covariance matrices is much less developed than that of eigenvalues , due to the cumbersome formulation of the eigenvectors .",
    "some great achievements have been made in proving the properties of eigenvectors for large dimensional sample covariance matrices , such as @xcite , and that for wigner matrices , such as @xcite .    however , the eigenvectors of large sample covariance matrices play an important role in high - dimensional statistical analysis .",
    "in particular , due to the increasing availability of high - dimensional data , principal component analysis ( pca ) has been favorably recognized as a powerful technique to reduce dimensionality .",
    "the eigenvectors corresponding to the leading eigenvalues are the directions of the principal components .",
    "johnstone @xcite proposed the spiked eigenvalue model to test the existence of principal component .",
    "paul @xcite discussed the length of the eigenvector corresponding to the spiked eigenvalue .    in pca ,",
    "the eigenvectors ( @xmath28 ) of population covariance matrix @xmath29 determine the directions in which we project the observed data and the corresponding eigenvalues ( @xmath30 ) determine the proportion of total variability loaded on each direction of projections . in practice , the ( sample ) eigenvalues ( @xmath31 ) and eigenvectors ( @xmath32 ) of the sample covariance matrix @xmath13 are used in pca . in @xcite , anderson has shown the following asymptotic distribution for the sample eigenvectors @xmath33 when the observations are from a multivariate normal distribution of covariance matrix @xmath29 with distinct eigenvalues : @xmath34 where @xmath35    however , this is a large sample result when the dimension @xmath1 is fixed and low . in particular , if @xmath36 , then the eigenmatrix ( matrix of eigenvectors ) should be asymptotically isotropic when the sample size is large .",
    "that is , the eigenmatrix should be asymptotically haar , under some minor moment conditions . however , when the dimension is large ( increasing ) , the haar property is not easy to formulate .    motivated by the orthogonal iteration method , @xcite proposed an iterative thresholding method to estimate sparse principal subspaces ( spanned by the leading eigenvectors of @xmath29 ) in high dimensional and spiked covariance matrix setting .",
    "the convergence rates of the proposed estimators are provided . by reducing the sparse pca problem to a high - dimensional regression problem",
    ", @xcite established the optimal rates of convergence for estimating the principal subspace with respect to a large collection of spiked covariance matrices .",
    "see the reference therein for more literature on sparse pca and spiked covariance matrices .    to perform the test of existence of spiked eigenvalues",
    ", one has to investigate the null properties of the eigenmatrices , that is , when @xmath36 ( i.e. , nonspiked ) .",
    "then the eigenmatrix should be asymptotically isotropic , when the sample size is large .",
    "that is , the eigenmatrix should be asymptotically haar . however , when the dimension is large , the haar property is not easy to formulate",
    "the recent development in random matrix theory can help us investigate the large dimension and large sample properties of eigenvectors .",
    "we will adopt the vesd , defined later in the paper , to characterize the asymptotical haar property so that if the eigenmatrix is haar , then the process defined the vesd tends to a brownian bridge .",
    "conversely , if the process defined by the vesd tends to a brownian bridge , then it indicates a similarity between the haar distribution and that of the eigenmatrix .",
    "therefore , studying the large sample and large dimensional results of the vesd can assist us in better examining spiked covariance matrix as assumed by @xcite and @xcite among many others .",
    "let @xmath37 denote the spectral decomposition of @xmath13 , where @xmath38 and @xmath39 is a unitary matrix consisting of the corresponding orthonormal eigenvectors of @xmath13 .",
    "for each @xmath1 , let @xmath40 , @xmath41 be nonrandom and let @xmath42 , where @xmath43 denotes euclidean norm of @xmath44 .",
    "define a stochastic process @xmath45 by @xmath46 } \\biggl(|d_j|^2-\\frac1n \\biggr),\\qquad[a]\\mbox { denotes the greatest integer}\\leq a.\\ ] ] if @xmath47 is haar distributed over the orthogonal matrices , then @xmath48 would be uniformly distributed over the unit sphere in @xmath49 , and the limiting distribution of @xmath45 is a unique brownian bridge @xmath50 when @xmath1 tends to infinity . in this paper",
    ", we use the behavior of @xmath45 for all @xmath44 to reflect the uniformity of @xmath47 .",
    "the process @xmath45 is considerably important for us to understand the behavior of the eigenvectors of @xmath13 .    motivated by silverstein s ideas in @xcite",
    ", we want to examine the limiting properties of @xmath47 through stochastic process @xmath45 .",
    "we claim that @xmath47 is `` asymptotically haar distributed , '' which means @xmath45 converges to a brownian bridge @xmath50 . in @xcite , it showed that the weak convergence of @xmath45 converging to a brownian bridge @xmath50 is equivalent to @xmath51 converging to @xmath52 .",
    "we therefore consider transforming @xmath45 to @xmath53 where @xmath12 is the esd of @xmath13 .",
    "we define the eigenvector empirical spectral distribution ( vesd ) @xmath54 of @xmath13 as follows : latexmath:[\\[\\label{esed } h^{\\mathbf{s}_n}(x)=\\sum_{i=1}^n    @xmath54 in ( [ esed ] ) and @xmath12 in ( [ esd ] ) , we notice that there is no difference except the coefficient associated with each indicator function such that @xmath56 henceforth , the investigation of @xmath45 is converted to that of the difference between two empirical distributions @xmath54 and @xmath12 .",
    "the authors in @xcite proved that @xmath54 and @xmath12 have the same limiting distribution , the marenko ",
    "pastur distribution @xmath17 , where @xmath57 and @xmath58 .",
    "before we present the main theorems , let us introduce the following notation : @xmath59 and @xmath60 we denote @xmath61 and @xmath62 if , for any @xmath63 , there exist a large positive constant @xmath64 and a positive random variable @xmath65 , such that @xmath66 respectively .    in this paper",
    ", we follow the work in @xcite and establish three types of convergence rates of @xmath54 to @xmath67 in the following theorems .",
    "[ theorem1 ] suppose that @xmath68 , @xmath69 , @xmath70 are i.i.d .",
    "complex random variables with @xmath71 , @xmath72 and @xmath73 . for any fixed unit vector @xmath74 , and @xmath75",
    ", it then follows that @xmath76 where @xmath77 as it is defined in ( [ mplaw ] ) and @xmath78 denotes the marenko ",
    "pastur distribution function with an index @xmath79 .    from the proof of theorem [ theorem1 ] , it is clear that the condition @xmath73 is required only in the truncation step in the next section .",
    "we therefore believe that the condition @xmath73 can be replaced by @xmath80 in theorems [ theorem2 ]  and  [ theorem3 ] .    because the convergence rate of @xmath81 depends on the convergence rate of @xmath82",
    ", we only consider the convergence rate of .    as @xmath77",
    ", we can characterize the closeness between @xmath79 and 1 through @xmath83 .",
    "in particular , when @xmath79 is away from 1 ( or @xmath84 ) , the convergence rate of @xmath85 is @xmath0 , which we believe is the optimal convergence rate .",
    "this is because we observe in @xcite that for an analytic function  @xmath86 , @xmath87 converges to a gaussian distribution .",
    "while in @xcite , bai and silverstein proved that the limiting distribution of @xmath88 is also a gaussian distribution .",
    "we therefore conjecture that the optimal rate of @xmath54 should be @xmath0 and @xmath89 for @xmath12 .",
    "although @xmath12 and @xmath54 converge to the same limiting distribution , there exists a substantial difference between @xmath12 and @xmath54 .",
    "notice that two matrices @xmath90 and @xmath91 share the same set of nonzero eigenvalues . however , these two matrices do not always share the same set of eigenvectors .",
    "especially when @xmath92 , the eigenvectors of @xmath13 corresponding to 0 eigenvalues can be arbitrary . as a result , the limit of @xmath93 may not exist or heavily depends on the choice of unit vector @xmath44 .",
    "therefore , we only consider the case of @xmath94 in this paper and leave the case of @xmath95 as a future research problem .",
    "the rates of convergence in probability and almost sure convergence of the vesd are provided in the next two theorems .",
    "[ theorem2 ] under the assumptions in theorem [ theorem1 ] except that we now only require @xmath96 , we have @xmath97    as an application of theorem [ theorem2 ] , in @xcite we extended the clt of the linear spectral statistics @xmath98 established in @xcite to the case where the kernel function @xmath86 is continuously twice differentiable provided that the sample covariance matrix @xmath13 satisfies the assumptions of theorem [ theorem2 ] .",
    "this result is useful in testing johnstone s hypothesis when normality is not assumed .",
    "[ theorem3 ] under the assumptions in theorem [ theorem2 ] , for any @xmath3 , we have @xmath99    in this paper , we will use the following notation :    * @xmath10 denote the conjugate transpose of a matrix ( or vector ) @xmath11 ; * @xmath100 denote the ( ordinary ) transpose of a matrix ( or vector ) @xmath11 ; * @xmath101 denote the euclidean norm for any vector @xmath102 ; * @xmath103 , the spectral norm ; * @xmath104 for any function @xmath105 ; * @xmath106 denote the conjugate of a complex number @xmath107 .",
    "the rest of the paper is organized as follows . in section  [ sec2 ] ,",
    "we introduce the main tools used to prove theorems [ theorem1 ] , [ theorem2 ] and [ theorem3 ] , including stieltjes transform and a berry  esseen type inequality .",
    "the proofs of these three theorems are presented in sections  [ sec3][sec6 ] .",
    "several important results which are repeatedly employed throughout sections  [ sec3][sec6 ] are proved in appendix  [ sec7 ] .",
    "appendix  [ sec8 ] contains some existing results in the literature .",
    "finally , preliminaries on truncation , centralization and rescaling are postponed to the last section .",
    "the stieltjes transform is an essential tool in random matrix theory and our paper .",
    "let us now briefly review the stieltjes transform and some important and relevant results . for a cumulative distribution function @xmath108 ,",
    "its stieltjes transform @xmath109 is defined as @xmath110 where @xmath111 denotes the imaginary part of a complex number .",
    "the stieltjes transforms of the esd @xmath12 and the vesd @xmath54 are @xmath112 and @xmath113 respectively . here",
    "@xmath114 denotes the @xmath115 identity matrix . for simplicity of notation ,",
    "we use @xmath116 and @xmath117 to denote @xmath118 and @xmath119 , respectively .",
    "notice that although the eigenmatrix @xmath47 may not be unique , the stieltjes transform @xmath117 of @xmath93 depends on @xmath13 for any @xmath44 rather than @xmath47 .",
    "let @xmath120 denote the companion matrix of @xmath13 .",
    "as @xmath13 and @xmath121 share the same set of nonzero eigenvalues , it can be shown that stieltjes transforms of @xmath12 and @xmath122 satisfy the following equality : @xmath123 where @xmath124 denotes the stieltjes transform of @xmath122 .",
    "moreover , @xcite and @xcite claimed that @xmath125 converges , almost surely , to a nonrandom distribution function @xmath126 with stieltjes transform @xmath127 such that @xmath128 where @xmath129 denotes the stieltjes transform of the marenko ",
    "pastur distribution with index @xmath21 .",
    "using ( 6.1.4 ) in @xcite , we also obtain the relationship between two limits @xmath129 and @xmath127 as follows : @xmath130      [ main ] let @xmath54 and @xmath67 be the vesd of @xmath13 and the marenko  pastur distribution with index @xmath79 , respectively .",
    "denote their corresponding stieltjes transforms by @xmath117 and @xmath131 , respectively .",
    "then there exist large positive constants @xmath132 and @xmath133 , such that for @xmath134 , @xmath135 where @xmath136 is a complex number with positive imaginary part ( i.e. , @xmath137 ) .    lemma [ main ] can be proved using lemma [ ll ] .",
    "to prove theorem [ theorem1 ] , we apply lemma [ main ] .",
    "in addition , we prove theorems [ theorem2 ] and [ theorem3 ] by replacing @xmath138 , @xmath139 with @xmath54 and @xmath117 , respectively .",
    "under the condition of @xmath140 , we can choose a sequence of @xmath141 with @xmath142 and @xmath143 as @xmath144 , such that @xmath145 furthermore , without loss of generality , we can assume that every @xmath146 is bounded by @xmath147 and has mean 0 and variance 1 .",
    "see appendix  [ sec9 ] for details on truncation , centralization and rescaling .",
    "we introduce some notation before start proving theorem [ theorem1 ] . throughout the paper",
    ", we use @xmath148 and @xmath149 for @xmath150 to denote positive constant numbers which are independent of @xmath2 and may take different values at different appearances . let @xmath151 denote the @xmath152th column of the data matrix @xmath11 . let @xmath153 so that @xmath154 and let @xmath155    it is easy to show that @xmath156 for any @xmath157 , we can also show that @xmath158 due to the fact that @xmath159 from ( 2.2 ) in @xcite , we can write @xmath124 in terms of @xmath160 as follows : @xmath161 we proceed with the proof of theorem [ theorem1 ] : @xmath162\\mathbf{x}_n \\\\ & = & \\bigl(z\\underline{m}(z)+z \\bigr)^{-1}\\mathbf{x}_n^ * \\mathrm{e } \\bigl [ \\bigl(z\\underline{m}(z)+z \\bigr)\\mathbf{a}^{-1}(z)+ \\mathbf{i}_n \\bigr]\\mathbf{x}_n \\\\ & = & \\bigl(z\\underline{m}(z)+z \\bigr)^{-1}\\mathbf{x}_n^ * \\mathrm{e } \\bigl [ \\bigl(z\\mathbf{i}_n+\\mathbf{a}(z ) \\bigr ) \\mathbf{a}^{-1}(z)+z\\underline{m}(z)\\mathbf{a}^{-1}(z ) \\bigr ] \\mathbf{x}_n \\\\ & = & \\bigl(z\\underline{m}(z)+z \\bigr)^{-1}\\mathbf{x}_n^ * \\mathrm{e } \\biggl[\\sum_{j=1}^n \\mathbf{r}_j\\mathbf{r}_j^*\\mathbf{a}^{-1}(z)+z \\bigl(\\mathrm{e}\\underline{m}_n(z ) \\bigr)\\mathbf{a}^{-1}(z ) \\\\ & & \\hspace*{85pt } { } -z \\bigl(\\mathrm{e}\\underline{m}_n(z ) \\bigr ) \\mathbf{a}^{-1}(z)+z\\underline{m}(z)\\mathbf{a}^{-1}(z ) \\biggr]\\mathbf{x}_n \\\\ & = & \\bigl(z\\underline{m}(z)+z \\bigr)^{-1}\\mathbf{x}_n^ * \\mathrm{e } \\biggl[\\sum_{j=1}^n \\beta_j(z)\\mathbf{r}_j\\mathbf{r}_j^ * \\mathbf{a}_j^{-1}(z)- \\bigl(-z\\mathrm{e } \\underline{m}_n(z ) \\bigr)\\mathbf{a}^{-1}(z ) \\\\[-2pt ] & & \\hspace*{150pt } { } - \\bigl(z\\mathrm{e}\\underline{m}_n(z)-z\\underline{m}(z)\\bigr)\\mathbf{a}^{-1}(z ) \\biggr]\\mathbf{x}_n \\\\ & = & \\bigl(z\\underline{m}(z)+z \\bigr)^{-1}\\mathbf{x}_n^ * \\mathrm{e } \\biggl[\\sum_{j=1}^n \\beta_j(z)\\mathbf{r}_j\\mathbf{r}_j^ * \\mathbf{a}_j^{-1}(z)- \\biggl(\\frac{1}{n}\\sum _ { j=1}^n\\mathrm{e}\\beta_j(z ) \\biggr)\\mathbf{a}^{-1}(z ) \\biggr]\\mathbf{x}_n \\\\ & & { } - \\bigl(z\\underline{m}(z)+z \\bigr)^{-1}\\mathbf{x}_n^*\\bigl(z\\mathrm{e}\\underline{m}_n(z)-z\\underline{m}(z ) \\bigr ) \\bigl ( \\mathrm{e}\\mathbf{a}^{-1}(z ) \\bigr)\\mathbf{x}_n \\\\ & = & \\bigl(z\\underline{m}(z)+z \\bigr)^{-1}\\mathbf{x}_n^ * \\biggl[\\sum_{j=1}^n\\mathrm{e } \\beta_j(z ) \\biggl(\\mathbf{r}_j\\mathbf{r}_j^ * \\mathbf{a}_j^{-1}(z)-\\frac{1}{n}\\mathrm{e } \\mathbf{a}^{-1}(z ) \\biggr ) \\biggr]\\mathbf{x}_n \\\\ & & { } + m(z ) \\bigl(z\\mathrm{e}\\underline{m}_n(z)-z\\underline{m}(z)\\bigr)\\mathrm{e}m_n^h(z ) \\\\ & = : & \\delta_1+\\delta_2,\\end{aligned}\\ ] ] where @xmath163\\mathbf{x}_n , \\\\ \\delta_2&=&m_y(z ) \\bigl(z\\mathrm{e}\\underline{m}_n(z)-z \\underline{m}(z ) \\bigr)\\mathrm{e}m_n^h(z).\\end{aligned}\\ ] ]    [ deltah ] if @xmath164 holds for some constants @xmath165 and @xmath166 , when @xmath167 , under the conditions of theorem [ theorem1 ] , there exists a constant @xmath148 such that @xmath168 .    according to lemma [ main ] , @xmath169 from lemmas [ b1 ] and [ ap7 ] , we know that there exists a positive constant @xmath148 , such that @xmath170 notice that @xmath171 lemma [ ap0 ] , ( [ mnmn ] ) and ( [ aa ] ) imply that @xmath172 and @xmath173 from ( 2.3 ) in @xcite",
    ", we have @xmath174 where @xmath175 denotes the stieltjes transform of the semicircle law , see ( 3.2 ) in  @xcite .",
    "therefore @xmath176 is bounded by a constant , for @xmath177 , see ( 3.3 ) in  @xcite .",
    "combined with lemma [ b7 ] , there exist constants @xmath178 , @xmath179 , such that @xmath180 given @xmath167 , for @xmath181 , we have @xmath182 . for a large enough @xmath165 such that @xmath183 , we have @xmath184 as @xmath185 and @xmath167 ,",
    "if @xmath186 , we then have @xmath187\\\\[-8pt ] & \\leq&\\frac{\\delta^h}{2k_1}+\\frac{cv}{v_y}.\\nonumber\\end{aligned}\\ ] ] thus , from lemma [ main ] , equations ( [ 1 ] ) and ( [ 2n3 ] ) , we conclude that there exists a  constant @xmath148 , such that @xmath188 the proof is complete .",
    "to finish the proof of theorem [ theorem1 ] , we choose @xmath189 such that @xmath190 according to lemma [ deltah ] , we know that @xmath191 if @xmath192 , @xmath193 .    if @xmath194 , @xmath195 .",
    "thus , the proof of theorem [ theorem1 ] is complete .",
    "in this section , we are going to show that when @xmath167 , @xmath196 is indeed bounded by @xmath197 , as required by lemma [ deltah ] .    from @xmath198 , we can further write @xmath199 , where @xmath200 \\\\ & = & n \\bigl(z\\underline{m}(z)+z \\bigr)^{-1 } \\mathrm{e } \\bigl ( \\beta_1(z)\\alpha_1(z ) \\bigr ) , \\\\",
    "\\delta_{12}&= & \\bigl(z\\underline{m}(z)+z \\bigr)^{-1 } \\mathrm{e } \\bigl[\\beta_1(z)\\mathbf{x}_n^ * \\bigl ( \\mathbf{a}_1^{-1}(z)-\\mathbf{a}^{-1}(z ) \\bigr ) \\mathbf{x}_n \\bigr ] , \\\\",
    "\\delta_{13}&= & \\bigl(z\\underline{m}(z)+z \\bigr)^{-1 } \\mathrm{e } \\bigl[\\beta_1(z)\\mathbf{x}_n^ * \\bigl ( \\mathbf{a}^{-1}(z)-\\mathrm{e}\\mathbf{a}^{-1}(z ) \\bigr ) \\mathbf{x}_n \\bigr].\\end{aligned}\\ ] ]    according to ( [ underlinemm ] ) and lemma [ b7 ] , @xmath201 for some constant @xmath148 . using identity ( [ betabxi ] ) three times , we have @xmath202 notice that @xmath203 and @xmath204 is bounded by a constant ( due to lemma  [ ap2 ] ) , we then have @xmath205\\\\[-8pt ] & \\leq & \\frac{cn}{v_y } \\bigl ( \\bigl|\\mathrm{e}\\xi_1(z ) \\alpha_1(z ) \\bigr|+ \\bigl|\\mathrm{e}\\xi_1 ^ 2(z ) \\alpha_1(z ) \\bigr|+ \\bigl|\\mathrm{e}\\beta_1(z)\\xi_1 ^ 3(z ) \\alpha_1(z ) \\bigr| \\bigr).\\nonumber\\end{aligned}\\ ] ]    let us start with the first term in the above upper bound of @xmath206 as in ( [ eq33 ] ) . note that @xmath207 and @xmath208 are independent .",
    "therefore , for any integer @xmath209 we have @xmath210 and @xmath211 denote @xmath212 , @xmath213 , and @xmath214 be the @xmath215th canonical basis vector , that is , the @xmath1-vector whose coordinates are all 0 except that the @xmath215th coordinate is 1",
    ". then lemmas [ b2 ] , [ ap4 ] , [ ap5 ] and the inequality @xmath216 imply that @xmath217 in the above , we use the following two results , which can be proved by applying lemmas [ ap4 ] and [ ap5 ] : @xmath218 hence , we have shown that @xmath219    let us denote @xmath220 the conjugate transpose of @xmath221 , that is , @xmath222 .",
    "then we can rewrite the second term in the upper bound of @xmath223 as @xmath224 \\\\ & = & \\frac{1}{n^3}\\mathrm{e } \\biggl [ \\biggl\\ { \\biggl(\\sum _ { i\\ne j}a_{ij}\\bar{x}_{i1}x_{j1 } \\biggr)^2 + 2 \\biggl(\\sum_{i } \\bigl(a_{ii}\\bigl|x_{1i}^2\\bigr|-\\mathrm{e}a_{ii } \\bigr )",
    "\\biggr ) \\\\ & & \\hspace*{33pt}{}\\times\\biggl(\\sum_{i\\ne j}a_{ij } \\bar{x}_{i1}x_{j1 } \\biggr)+ \\biggl(\\sum _ { i}a_{ii}\\bigl|x_{1i}^2\\bigr|- \\mathrm{e}a_{ii } \\biggr)^2 \\biggr\\ } \\\\ & & \\hspace*{64pt}{}\\times\\biggl\\{\\sum_{i\\ne j}b_{ij } \\bar{x}_{i1}x_{j1}+\\sum_{i}b_{ii } \\bigl(\\bigl|x_{1i}^2\\bigr|-1\\bigr ) \\biggr\\ } \\biggr ] \\\\ & \\leq&\\frac{c}{n^3}\\mathrm{e } \\biggl(\\sum_i\\bigl|a_{ii}^2b_{ii}\\bigr|+ \\sum_i|\\mathrm{e}a_{ii}|^2|b_{ii}| + \\sum_{i\\neq j}\\bigl|a_{ij}^2b_{ii}\\bigr| \\\\ & & \\hspace*{28pt}{}+ \\sum_{i\\neq j}\\bigl|a_{ij}^2b_{ij}\\bigr|+ \\sum_{i\\neq j}|a_{ij}a_{ii}b_{jj}| + \\sum_{i\\neq j}|a_{ii}a_{ij}b_{ij}| \\biggr ) \\\\ & & { } + \\frac{c}{n^3}\\sum_{\\iota,\\tau}\\biggl\\vert \\sum_{i\\neq j\\ne k}\\mathrm{e}a_{ij}^{\\iota}a_{jk}^{\\tau } b_{ik}\\biggr\\vert,\\end{aligned}\\ ] ] where @xmath225 and @xmath226 denote @xmath227 or @xmath228 . by following the similar proofs in establishing  ( [ delta111 ] ) , we are able to show that @xmath229^{3/2}.\\end{aligned}\\ ] ]    the cauchy ",
    "schwarz inequality implies that @xmath230 & \\leq & \\biggl(\\sum_i\\mathrm{e}\\bigl| \\mathbf{x}_n^*\\mathbf{a}_1^{-1}(z)\\mathbf { e}_i\\bigr|^2 \\biggr)^{1/2 } \\biggl ( \\sum _ i\\bigl|\\mathbf{x}_n^*\\mathbf{e}_i\\bigr|^2 \\mathrm{e } \\biggl(\\sum_j\\bigl|a_{ij}^2\\bigr| \\biggr)^2 \\biggr)^{1/2 } \\\\[-1.8pt ] & = & \\bigl(\\mathrm{e}\\mathbf{x}^*_n\\bigl(\\mathbf{a}_1^{-1}(z ) \\mathbf{a}_1^{-1}(\\bar{z})\\bigr)\\mathbf{x}_n \\bigr)^{1/2 } \\bigl(\\mathrm{e}\\bigl|\\bigl(\\mathbf{a}_1^{-1}(z ) \\mathbf{a}_1^{-1}(\\bar{z})_{11 } \\bigr)^2\\bigr| \\bigr)^{1/2 } \\\\[-1.8pt ] & \\leq&\\frac{c}{v^{3/2 } } \\biggl(\\frac1{v_y}+\\frac{\\delta^h}{v } \\biggr)^{3/2 } , \\\\[-1.8pt ] \\mathrm{e}\\sum_{i\\ne j}a_{ij}^2b_{ij } & \\leq & \\biggl[\\sum_{ij}\\mathrm{e}\\bigl|a_{ij } \\mathbf{x}_n^*\\mathbf{a}_1^{-1}(z)\\mathbf{e}_i\\bigr|^2 \\sum_{ij}\\mathrm{e}\\bigl|a_{ij}\\mathbf{x}_n^ * \\mathbf{e}_j\\bigr|^2 \\biggr]^{1/2 } \\\\[-1.8pt ] & \\leq & \\biggl[\\sum_{i}\\mathrm{e}\\bigl| \\bigl ( \\mathbf{a}_1^{-1}(z)\\mathbf{a}_1^{-1}(\\bar{z } ) \\bigr)_{ii}\\bigl(\\mathbf{x}_n^*\\mathbf{a}_1^{-1}(z ) \\mathbf{e}_i\\bigr)^2 \\bigr| \\\\[-1.8pt ] & & \\hspace*{21pt}{}\\times\\sum_{j}\\mathrm{e}\\bigl| \\bigl(\\mathbf{a}_1^{-1 } ( \\bar{z})\\mathbf{a}_1^{-1}(z ) \\bigr)_{jj}\\bigl ( \\mathbf{x}_n^*\\mathbf{e}_j\\bigr)^2 \\bigr| \\biggr]^{1/2 } \\\\[-1.8pt ] & \\leq & \\frac{c}{v^2 } \\biggl(\\frac1{v_y}+\\frac{\\delta^h}{v } \\biggr)^{3/2 } , \\\\[-1.8pt ] \\mathrm{e}\\sum_{i\\ne j}|a_{ii}a_{ij}b_{jj}| & \\leq & \\biggl[\\sum_{ij}\\mathrm{e}\\bigl|a_{ii } \\mathbf{x}_n^*\\mathbf{a}_1^{-1}(z)\\mathbf{e}_j\\bigr|^2 \\sum_{ij}\\mathrm{e}\\bigl|a_{ij}\\mathbf{x}_n^ * \\mathbf{e}_j\\bigr|^2 \\biggr]^{1/2 } \\\\[-1.8pt ] & \\leq & \\biggl[\\sum_{i}\\mathrm{e}\\bigl|a_{ii}^2\\bigr| \\mathbf{x}_n^*\\mathbf{a}_1^{-1}(z)\\mathbf{a}_1^{-1 } ( \\bar{z})\\mathbf{x}_n \\\\[-1.8pt ] & & \\hspace*{5pt}{}\\times\\sum_{j } \\mathrm{e } \\bigl ( \\mathbf{a}_1^{-1}(z)\\mathbf{a}_1^{-1}(\\bar{z } ) \\bigr)_{jj}\\bigl|\\mathbf{x}_n^*\\mathbf{e}_j\\bigr|^2 \\biggr]^{1/2 } \\\\[-1.8pt ] & \\leq&c\\frac{\\sqrt{n}}{v^2 } \\biggl(\\frac1{v_y}+\\frac{\\delta^h}{v}\\biggr ) , \\\\[-1.8pt ] \\mathrm{e}\\sum_{i\\neq j}|a_{ii}a_{ij}b_{ij}| & \\leq & \\biggl[\\sum_{ij}\\mathrm{e}\\bigl|a_{ii } \\mathbf{x}_n^*\\mathbf{a}_1^{-1}\\mathbf{e}_j\\bigr|^2 \\sum_{ij}\\mathrm{e}\\bigl|a_{ij}\\mathbf{x}_n^ * \\mathbf{e}_i\\bigr|^2 \\biggr]^{1/2 } \\\\[-1.8pt ] & \\leq&c\\frac{\\sqrt{n}}{v^2 } \\biggl(\\frac1{v_y}+\\frac{\\delta^h}{v } \\biggr).\\end{aligned}\\ ] ] finally , we establish @xmath231 by inclusive  exclusive principle and what we have just proved , it remains to show that @xmath232 notice that @xmath233 is the @xmath234-element of @xmath235 .",
    "we obtain @xmath236 similarly , one can prove the other terms of ( [ eqb1 ] ) share this common bound .",
    "in summary , when @xmath167 it holds that @xmath237    for the last term in the upper bound of @xmath223 , we apply lemma [ ap3 ] and the cauchy  schwarz inequality again . in particular , for any fixed @xmath238 , we have that @xmath239 the last inequality in ( [ delta113 ] ) is due to lemmas [ ap1 ] and [ ap6 ] . therefore , for any @xmath240 , ( [ delta111 ] ) , ( [ delta112 ] ) and ( [ delta113 ] ) lead us to @xmath241    to establish the upper bound for @xmath242 , we will make use of the following equality : @xmath243 note that ( [ mz ] ) implies that @xmath244\\\\[-8pt ] & \\leq & \\frac{c}{nv_y } \\bigl|\\mathrm{e}\\operatorname{tr } \\bigl(\\mathbf{a}_1^{-1}(z ) \\mathbf{x}_n \\mathbf{x}_n^*\\mathbf{a}_1^{-1}(z ) \\bigr ) \\bigr|\\qquad(\\mbox{see lemma \\ref{b4}})\\nonumber \\\\ & = & \\frac{c}{nv_y } \\bigl|\\mathrm{e}\\mathbf{x}_n^*\\mathbf { a}_1^{-2}(z)\\mathbf{x}_n \\bigr|\\nonumber \\\\ & \\leq & \\frac{c}{nvv_y } \\biggl(\\frac1{v_y}+\\frac{\\delta^h}{v } \\biggr).\\nonumber\\end{aligned}\\ ] ]    at last , we establish the upper bound for @xmath245 .    by ( [ mz ] ) , lemma [ ap2 ] , and the fact @xmath246 we obtain @xmath247 from @xmath248 inequality ( see love @xcite )",
    ", we have @xmath249 , where @xmath250    it should be noted that @xmath251 , and @xmath207 and @xmath252 are independent .",
    "then we have @xmath253= 0.\\end{aligned}\\ ] ] by the results in ( [ aa1 ] ) and ( [ bbeta ] ) , we have @xmath254 where @xmath255 the above inequality follows from lemmas [ b2 ] and [ ap2 ] . by lemma [ ap3 ] and the cauchy ",
    "schwarz inequality , it holds that @xmath256 hence , we have shown that @xmath257    moreover , lemmas [ ap1 ] and [ ap4 ] , and the cauchy ",
    "schwarz inequality lead us to the following : @xmath258    therefore , it follows that @xmath259    as it has been shown in ( [ delta11 ] ) , ( [ delta22 ] ) and ( [ delta33 ] ) , we conclude that @xmath260",
    "from lemma [ main ] , and by replacing @xmath138 and @xmath139 by @xmath54 and @xmath117 , respectively , we have @xmath261 as the convergence rate of @xmath262 has already been established in theorem [ theorem1 ] , we only focus on the convergence rate of @xmath263 .    by lemma [ ap5 ] and the cauchy ",
    "schwarz inequality , it follows that @xmath264 together with lemma [ deltah ] that @xmath168 , when @xmath265 , we have @xmath266 by choosing @xmath267 , we obtain @xmath268 the proof of theorem [ theorem2 ] is complete .",
    "notice that the proof of theorem [ theorem3 ] is almost the same as that of theorem [ theorem2 ] .                  since @xmath283\\\\[-8pt ] & = & \\frac{1}{n}\\sum_{k=1}^n \\mathrm{e}\\frac{1}{\\epsilon + 1-y_n - z - y_nz\\mathrm{e}m_n(z ) } \\nonumber \\\\ & = & -\\frac{1}{z+y_n-1+y_nz\\mathrm{e}m_n(z)}+\\delta_n,\\nonumber\\end{aligned}\\ ] ] where @xmath284 where @xmath285 is the @xmath286 matrix obtained from @xmath11 with its @xmath287th row removed and @xmath288 is the @xmath287th row of @xmath11 .",
    "it has proved that one of the roots of equation ( [ 8.3.7 ] ) is ( see ( 3.1.7 ) in @xcite ) @xmath289 the stieltjes transform of the marenko ",
    "pastur distribution  with index @xmath21 is given by ( see ( 2.3 ) in @xcite ) @xmath290 thus , @xmath291.\\end{aligned}\\ ] ] let us define by convention @xmath292 if @xmath293 , then the real parts of @xmath294 and @xmath295 have the same sign .",
    "since they both have positive imaginary parts , it follows that @xmath296 thus , @xmath297 if @xmath298 , we have @xmath299 .",
    "in addition , the following relevant result which is involved in the proof of lemma [ deltah ] is presented here : @xmath302\\\\[-8pt ] & & \\quad\\qquad{}+\\int_{|u - y_n-1|<1/(5(a+1)),|u|\\leq a}\\bigl\\vert\\mathrm{e}m_n(z)-m_y(z ) \\bigr\\vert\\,du \\nonumber \\\\ & & \\qquad \\leq cv.\\nonumber\\end{aligned}\\ ] ]      by the @xmath248-inequality ( see love @xcite ) , it follows that @xmath305 from lemmas [ b6 ] , [ b8 ] , [ b12 ] and the @xmath248-inequality with @xmath167 , we have @xmath306 under finite 8th moment assumption , for @xmath307 we have @xmath308 it can be shown that @xmath309 , @xmath310 , and @xmath311 hence , by lemma [ b4 ] , @xmath312 the last inequality is due to lemmas [ b6 ] , [ b7 ] , [ b8 ] and @xmath313 here",
    "let @xmath314 , by integration by parts and lemma [ b12 ] , we have @xmath315        from ( 2.3 ) in @xcite , we have @xmath320 where the square root of a complex number is defined as the one with positive imaginary part .",
    "it then can be verified that @xmath321 where @xmath322 denotes the stieltjes transform of the semicircular law . as @xmath323",
    ", we conclude that @xmath324 by the relationship between @xmath325 and @xmath326 , we have @xmath327 when @xmath165 is chosen large enough , by lemma [ ap0 ] , for all large @xmath2 we have @xmath328 and consequently we obtain @xmath329 thus , the proof is complete .",
    "note that if @xmath332 , by lemma [ ap2 ] , we get @xmath333 as a result , @xmath334 by the @xmath248-inequality , lemmas [ b8 ] and [ b10 ] , for some @xmath335 and @xmath336 , we have @xmath337 for any fixed @xmath238 , when @xmath2 is large enough so that @xmath338 , it can be shown that @xmath339 we finish the proof .",
    "recall that @xmath342 by lemmas [ ap3 ] and [ b4 ] , it holds that @xmath343 \\\\ & & \\quad\\qquad { } + \\frac{c}{n^{2l}}\\mathrm{e}\\bigl\\vert \\mathbf{x}_n^*\\mathbf{a}_1^{-2}(z ) \\mathbf{x}_n\\bigr\\vert^{2l } \\\\ & & \\qquad \\leq\\frac{c}{n^{l+2}v^{2l}}\\mathrm{e}\\bigl\\vert m_{h^{\\mathbf { b}_1}}(z)\\bigr\\vert^{2l}.\\end{aligned}\\ ] ] the last step follows the fact that @xmath344 for @xmath167 , which implies that @xmath345 .",
    "choose @xmath165 and @xmath2 large enough , such that @xmath346 .",
    "further , by @xmath248-inequality , we obtain @xmath347 that is , @xmath348 , for some constant @xmath148 .",
    "this finishes the proof .",
    "write @xmath350 as the conditional expectation given @xmath351 .",
    "it can then be shown that @xmath352 where @xmath353 therefore , by lemmas [ b3](b ) , we have @xmath354 using lemmas [ ap3 ] and [ b4 ] , we have @xmath355 by the fact that @xmath356 and @xmath357 , we have @xmath358 on the other side , @xmath359 thus , we obtain @xmath360 further @xmath361 for @xmath345 , choose @xmath165 large enough , such that @xmath362 . and using integration by parts , it is easy to find that @xmath363 where @xmath364 .",
    "[ ll ] let @xmath105 be a distribution function and let @xmath372 be a function of bounded variation satisfying @xmath373 .",
    "denote their stieltjes transforms by @xmath374 and @xmath375 , respectively .",
    "then @xmath376 where @xmath136 is a complex variable , @xmath377 , @xmath378 , @xmath379 , @xmath380 and @xmath381 are positive constants such that @xmath382 , @xmath383 and @xmath384        [ b3 ] let @xmath393 be a complex martingale difference sequence with respect to the increasing @xmath394-field @xmath395 , and let @xmath396 denote the conditional expectation with respect to @xmath397",
    ". then we have :        [ b4 ] let @xmath404 be an @xmath115 nonrandom matrix and @xmath405 be random vector of independent complex entries .",
    "assume that @xmath390 , @xmath391 and @xmath406 .",
    "then for any @xmath407 , @xmath408 where @xmath402 is a constant depending on @xmath403 only .          [ b10 ] suppose that @xmath422 , @xmath69 , are independent , with @xmath390 , @xmath391 , @xmath423 and @xmath424 with @xmath3 .",
    "assume that @xmath411 is a complex matrix .",
    "then for any given @xmath403 such that @xmath425 and @xmath426 , we have @xmath427 where @xmath428 .              [ b14 ]",
    "assume that the entries of @xmath437 is a double array of i.i.d .",
    "complex random variables with mean zero , variance @xmath438 and finite 4th moment .",
    "let @xmath439 be the @xmath8 matrix of the upper - left corner of the double array . if @xmath440 , then , with probability one , we have @xmath441 and @xmath442                note that the data matrix @xmath439 consists of i.i.d .",
    "complex random variables with mean 0 and variance 1 . in",
    "what follows , we will further assume that every @xmath146 is bounded by @xmath147 for some carefully selected @xmath141 .",
    "the proofs presented in the following three steps jointly justify such a convenient assumption .",
    "choose @xmath142 and @xmath143 as @xmath144 such that @xmath457 let @xmath458 denote the truncated data matrix whose entry on the @xmath215th row and @xmath152th column is @xmath459 , @xmath460 , @xmath461 .",
    ". then @xmath463      choose @xmath464 and @xmath143 as @xmath144 such that @xmath465 let @xmath458 denote the truncated data matrix whose entry on the @xmath215th row and @xmath152th column is @xmath459 , @xmath460 , @xmath461 .",
    "define @xmath462 .",
    "then @xmath466 the last equality is due to ( [ eta2 ] ) .",
    "the centralization procedures for three theorems are identical , only 8th moment is required and thus we treat them uniformly .",
    "let @xmath467 denote the centralized version of @xmath458 .",
    "more explicitly , on the @xmath215th row and @xmath152th column of @xmath467 , the entry is @xmath468      to establish both the weak and the strong convergence rates of the vesd to the marenko  pastur distribution , this @xmath475 suffices .",
    "moreover , for the convergence rate presented in theorem [ theorem1 ] , we shall prove the following .",
    "let @xmath129 denotes the stieltjes transform of the marenko ",
    "pastur distribution , thus @xmath129 is bounded by @xmath476 in lemma [ b7 ] .",
    "then @xmath477 besides @xmath478 can be considered as a stieltjes transform of some vesd function .",
    "so , we have @xmath479 thus , @xmath480      the rescaling procedures for the three theorems are exactly the same , and only 8th moment is required .",
    "thus , we treat them uniformly .",
    "write @xmath481 , where @xmath482 notice that @xmath483 tends to @xmath484 as @xmath2 goes to @xmath485 .",
    "define @xmath486 , which is the sample covariance matrix of @xmath487 .",
    "we shall show that @xmath488 and @xmath13 are asymptotically equivalent , that is , the vesd of @xmath488 and @xmath13 have the same limit if either one limit exists . for @xmath345 , @xmath489"
  ],
  "abstract_text": [
    "<S> the eigenvector empirical spectral distribution ( vesd ) is adopted to investigate the limiting behavior of eigenvectors and eigenvalues of covariance matrices . in this paper </S>",
    "<S> , we shall show that the kolmogorov distance between the expected vesd of sample covariance matrix and the marenko  </S>",
    "<S> pastur distribution function is of order @xmath0 . given that data dimension @xmath1 to sample size @xmath2 ratio is bounded between 0 and 1 , this convergence rate is established under finite 10th moment condition of the underlying distribution . </S>",
    "<S> it is also shown that , for any fixed @xmath3 , the convergence rates of vesd are @xmath4 in probability and @xmath5 almost surely , requiring finite 8th moment of the underlying distribution .    , </S>"
  ]
}