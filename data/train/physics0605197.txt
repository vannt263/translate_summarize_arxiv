{
  "article_text": [
    "histograms are used extensively as nonparametric density estimators both to visualize data and to obtain summary quantities , such as the entropy , of the underlying density . however in practice , the values of such summary quantities depend on the number of bins chosen for the histogram , which given the range of the data dictates the bin width .",
    "the idea is to choose a number of bins sufficiently large to capture the major features in the data while ignoring fine details due to ` random sampling fluctuations ' .",
    "several rules of thumb exist for determining the number of bins , such as the belief that between 5 - 20 bins is usually adequate ( for example , ` matlab ` uses 10 bins as a default ) .",
    "@xcite and derived formulas for the optimal bin width by minimizing the integrated mean squared error of the histogram model @xmath0 of the true underlying density @xmath1 , @xmath2 for @xmath3 data points , the optimal bin width @xmath4 goes as @xmath5 , where @xmath6 is a constant that depends on the form of the underlying distribution . assuming that the data are normally distributed with a sample variance @xmath7 gives @xmath8 ( scott , 1979 ) , and @xmath9 given a fixed range @xmath10 for the data",
    ", the number of bins @xmath11 then goes as @xmath12 freedman and diaconis report similar results , however they suggest choosing @xmath6 to be twice the interquartile range of the data .",
    "while these appear to be useful estimates for unimodal densities similar to a gaussian distribution , they are known to be suboptimal for multimodal densities .",
    "this is because they were derived assuming particular characteristics of the underlying density .",
    "in particular , the result by freedman and diaconis is not valid for some densities , such as the uniform density , since it derives from the assumption that the density @xmath13 satisfies @xmath14 .",
    "@xcite chooses to minimize @xmath15 and obtains a rule where one chooses the bin width @xmath4 to minimize @xmath16 where @xmath11 is the number of bins and @xmath17 are the bin probabilities .",
    "@xcite obtains a similar rule by applying cross - validation techniques with a kullback - leibler risk function .",
    "we approach this problem from a different perspective .",
    "since the underlying density is not known , it is not reasonable to use an optimization criterion that relies on the error between our density model and the true density .",
    "instead , we consider the histogram to be a piecewise - constant model of the underlying probability density .",
    "using bayesian probability theory we derive a straightforward algorithm that computes the posterior probability of the number of bins for a given data set .",
    "this enables one to objectively select an optimal piecewise - constant model describing the density function from which the data were sampled .",
    "we begin by considering the histogram as a piecewise - constant model of the probability density function from which @xmath3 data points were sampled .",
    "this model has @xmath11 bins with each bin having width @xmath18 , where @xmath19 is used to index the bins .",
    "we further assume that the bins have equal width @xmath20 for all @xmath19 , and together they encompass an entire width @xmath21 .",
    "is the width of the @xmath22 bin . in the case of a multi - dimensional histogram",
    ", this will be a multi - dimensional volume . ]",
    "each bin has a `` height '' @xmath23 , which is the constant probability density over the region of the bin .",
    "integrating this constant probability density @xmath23 over the width of the bin @xmath18 leads to a total probability mass of @xmath24 for the bin .",
    "this leads to the following piecewise - constant model @xmath0 of the unknown probability density function @xmath1 @xmath25 where @xmath23 is the probability density of the @xmath22 bin with edges defined by @xmath26 and @xmath27 , and @xmath28 is the boxcar function where @xmath29 our density model can be re - written in terms of the bin probabilities @xmath30 as @xmath31 it is important to keep in mind that the piecewise - constant density model is not a histogram in the usual sense .",
    "we use the bin heights to represent the probability density and not the probability mass of the bin .",
    "furthermore , we will show that the mean value of the probability mass of a bin is computed in a slightly different way .",
    "given @xmath11 bins and the normalization condition that the integral of the probability density equals unity , we are left with @xmath32 bin probabilities : @xmath33 , each describing the probability that samples will be drawn from each of the @xmath11 bins .",
    "the normalization condition requires that @xmath34 . for simplicity , we assume that the bin alignment is fixed so that extreme data points lie precisely at the center of the extreme bins of the histogram ( that is , the smallest sample is at the center of the leftmost bin , and similarly for the largest sample ) .",
    "as we will show , this technique is easily extended to multi - dimensional densities of arbitrarily high dimension .",
    "the likelihood function is a probability density that when multiplied by @xmath35 describes probability that a datum point @xmath36 is found to have a value in the infinitesimal range between @xmath37 and @xmath38 . for @xmath36 to have a value of @xmath37 occurring in the @xmath22 bin ,",
    "the likelihood is simply the probability density in the region defined by the bin @xmath39 where @xmath40 represents our prior knowledge about the problem , which includes the range of the data and the bin alignment . for equal width bins",
    ", the likelihood density reduces to @xmath41 for @xmath3 independently sampled data points , the joint likelihood is given by @xmath42 where @xmath43 and @xmath44 .",
    "equation ( [ eq : likelihood ] ) is data - dependent and describes the likelihood that the hypothesized piecewise - constant model accounts for the data .",
    "individuals who recognize this as having the form of the multinomial distribution may be tempted to include its familiar normalization factor .",
    "however , it is important to note that this likelihood function is properly normalized as is , which we now demonstrate . for a single datum point @xmath45 , the likelihood that it will take the value @xmath37 is @xmath46 where we have written @xmath47 . multiplying the probability density by @xmath35",
    "to get the probability and integrating over all possible values of @xmath37 we have @xmath48      for the prior probability of the number of bins , we assign a uniform density @xmath49 where @xmath50 is the maximum number of bins to be considered .",
    "this could reasonably be set to the range of the data divided by smallest non - zero distance between any two data points .",
    "we assign a non - informative prior for the bin parameters @xmath51 , the possible values of which lie within a simplex defined by the corners of an @xmath11-dimensional hypercube with unit side lengths @xmath52^{-1/2}.\\ ] ] equation ( [ eq : prior - for - pi s ] ) is the jeffreys s prior for the multinomial likelihood ( [ eq : likelihood ] ) , and has the advantage in that it is also the conjugate prior to the multinomial likelihood .",
    "using bayes theorem , the posterior probability of the histogram model is proportional to the product of the priors and the likelihood @xmath53 ) , ( [ eq : prior - for - m ] ) , and ( [ eq : prior - for - pi s ] ) gives the joint posterior probability for the piecewise - constant density model @xmath54 where @xmath55 is absorbed into the implicit proportionality constant with the understanding that we will only consider a reasonable range of bin numbers .",
    "the goal is to obtain the posterior probability for the number of bins @xmath11 . to do this we integrate the joint posterior over all possible values of @xmath33 in the simplex .",
    "the expression we desire is written as a series of nested integrals over the @xmath32 dimensional parameter space of bin probabilities @xmath56 in order to write this more compactly , we first define @xmath57 and note the recursion relation @xmath58 these definitions greatly simplify the sum in the last term as well as the limits of integration @xmath59 to solve the set of nested integrals in ( [ eq : nonsimplified - nested - integrals ] ) , consider the general integral @xmath60 where @xmath61 and @xmath62 .",
    "this integral can be re - written as @xmath63 setting @xmath64 we have @xmath65 where @xmath66 is the beta function with @xmath67 to solve all of the integrals we rewrite @xmath68 in ( [ eq : intsoln ] ) using the recursion formula ( [ eqn : recursion ] ) @xmath69 by defining @xmath70 we find @xmath71 finally , integrating ( [ eq : nestedints ] ) gives @xmath72 which can be simplified further by expanding the beta functions using ( [ eq : beta - ito - gamma ] ) @xmath73 using the recursion relation ( [ eq : b - recursion ] ) for the @xmath74 , we see that the general term @xmath75 in each numerator , except the last , cancels with the denominator in the following term .",
    "this leaves @xmath76 where we have used ( [ eq : b - recursion ] ) to observe that @xmath77 .",
    "last , again using the recursion relation in ( [ eq : b - recursion ] ) we find that @xmath78 , which results in our marginal posterior probability @xmath79 the normalization of this posterior probability density depends on the actual data used .",
    "for this reason , we will work with the unnormalized posterior , and shall refer to it as the relative posterior .    in optimization problems",
    ", it is often easier to maximize the logarithm of the posterior @xmath80 where @xmath81 represents the sum of the volume term and the logarithm of the implicit proportionality constant",
    ". the optimal number of bins @xmath82 is found by identifying the mode of the logarithm of the marginal posterior @xmath83 such a result is reassuring , since it is independent of the order in which the bins are counted .",
    "many software packages are equipped to quickly compute the log of the gamma function . however , for more basic implementations , the following definitions from can be used for integer  @xmath84 .",
    "@xmath85 equation ( [ eq : log - posterior - for - m ] ) allows one to easily identify the number of bins @xmath11 which optimize the posterior .",
    "we call this technique the ` optbins ` algorithm and provide a ` matlab ` code implementation in appendix 1 .",
    "in order to obtain the posterior probability for the probability mass of a particular bin , we begin with the joint posterior ( [ eq : joint - posterior ] ) and integrate over all the other bin probability masses . since we can consider the bins in any order ,",
    "the resulting expression is similar to the multiple nested integral in ( [ eq : nonsimplified - nested - integrals ] ) except that the integral for one of the @xmath32 bins is not performed . treating the number of bins as a given",
    ", we can use the product rule to get @xmath86 where the numerator is given by ( [ eq : joint - posterior ] ) and the denominator by ( [ eq : posterior - for - m ] ) . since the bins can be treated in any order",
    ", we derive the marginal posterior for the first bin and generalize the result for the @xmath22 bin .",
    "the marginal posterior is @xmath87 evaluating the integrals and substituting ( [ eq : posterior - for - m ] ) into the denominator we get @xmath88 cancelling terms and explicitly writing @xmath89 , the marginal posterior for @xmath90 is @xmath91 which can easily be verified to be normalized by integrating @xmath90 over its entire possible range from 0 to 1 . since the bins can be considered in any order , this is a general result for the @xmath22 bin @xmath92    the mean bin probability mass can be found from its expectation @xmath93 which substituting ( [ eq : posterior - bin - height ] ) gives @xmath94 the integral again gives a beta function , which when written in terms of gamma functions is @xmath95 using the fact that @xmath96 and cancelling like terms , we find that @xmath97 the mean probability density for bin @xmath19 ( the bin height ) is simply @xmath98 it is an interesting result that bins with no counts still have a non - zero probability .",
    "this makes sense since no lack of evidence can ever prove conclusively that an event occurring in a given bin is impossible  just less probable .",
    "the jeffrey s prior effectively places one - half of a datum point in each bin .",
    "the variance of the height of the @xmath22 bin is found similarly by @xmath99 which gives @xmath100 thus , given the optimal number of bins found by maximizing ( [ eq : log - posterior - for - m ] ) , the mean and variance of the bin heights are found from ( [ eq : mean - of - prob - density ] ) and ( [ eq : variance - of - prob - density ] ) , which allow us to construct an explicit histogram model of the probability density and perform computations and error analysis .",
    "note that in the case where there is one bin ( [ eq : variance - of - prob - density ] ) gives a zero variance .",
    "in this section we demonstrate the utility of this method for determining the optimal number of bins in a histogram model of several different data sets . here",
    "we consider 1000 data points sampled from four different probability density functions .",
    "the optimal histogram for 1000 data points drawn from a gaussian distribution @xmath101 is shown in figure 1a , where it is superimposed over a 100-bin histogram showing the density of the sampled points .",
    "figure 1b shows that the logarithm of the posterior probability ( [ eq : log - posterior - for - m ] ) peaks at 14 bins .",
    "figure 1c shows the optimal binning for data sampled from a 4-step piecewise - constant density .",
    "the logarithm of the posterior ( figure 1d ) peaks at 4 bins , which indicates that the algorithm can correctly detect the 4-step structure . in figures 1e and f",
    ", we see that samples drawn from a uniform density were best described by a single bin .",
    "this result is significant , since entropy estimates computed from these data would be biased if multiple bins were used .",
    "last , we consider a density function that consists of a mixture of three sharply - peaked gaussians with a uniform background ( figure 1 g ) . the posterior peaks at 52 bins indicating that the data warrant a detailed model ( figure 1h ) .",
    "the spikes in the log posterior are due to the fact that the bin edges are fixed .",
    "the log posterior is large at values of @xmath11 where the bins happen to line up with the gaussians , and small when they are misaligned .",
    "this last example demonstrates one of the weaknesses of the equal bin - width model , as many bins are needed to describe the uniform density between the three narrow peaks .",
    "we now compare our results with those obtained using scott s rule , stone s rule , and the akaike model selection criterion ( @xcite ) . since freedman and diaconis ( f&d ) method has the same functional form as scott s rule , the results using f&d are not presented here .",
    "their technique leads to a greater number of bins than scott s rule , which , as we will show , already prescribes more bins than are warranted by the data .",
    "akaike s method , applied to histograms in @xcite , balances the logarithm of the likelihood of the model against the number of model parameters . the number of bins is chosen to maximize @xmath102    since scott s rule was derived to be asymptotically optimal for gaussian distributions , we limited our comparison to gaussian - distributed data .",
    "we tested data sets with @xmath103 different numbers of samples including @xmath104 . for each @xmath3 we tested @xmath105 different histograms , each with @xmath3 samples drawn from a gaussian distribution @xmath101 , for a total of @xmath106 histograms in this analysis .",
    "the optimal number of bins was found using scott s rule ( [ eq : scott ] ) , stone s rule ( [ eq : stone ] ) , akaike s aic ( [ eq : aic ] ) and the present ` optbins ` algorithm ( [ eq : log - posterior - for - m ] ) .",
    "the quality of fit was quantified using the integrated square error ( ise ) , which is the criterion for which scott s rule is asymptotically optimized .",
    "this is @xmath107 where @xmath108 is the piecewise - constant histogram model with @xmath11 bins .",
    "figure [ fig:02]a shows the mean number of bins @xmath11 found using each of four methods for the values of the number of samples @xmath3 tested .",
    "the present algorithm ` optbins ` tends to choose the least number of bins , and does so consistently for @xmath109 .",
    "more importantly , figure [ fig:02]b shows the mean ise between the modelled density function and the correct gaussian distribution @xmath101 . since scott s rule was derived to asymptotically optimize the ise , it may be surprising to see that ` optbins ` outperforms each of these methods with respect to this measure . while scott s rule may be optimal as @xmath110 ,",
    "` optbins ` , which is based on a bayesian solution , is optimal for finite @xmath3 .",
    "furthermore if a log probability ( or log - odds ratio ) criterion were to be used , ` optbins ` would be optimal by design .",
    "it is instructive to observe how this algorithm behaves in situations involving small sample sizes .",
    "we begin by considering the extreme case of two data points @xmath111 . in the case of a single bin , @xmath112",
    ", the posterior probability reduces to @xmath113 so that the log posterior is zero . for @xmath114 ,",
    "the two data points lie in separate bins , resulting in @xmath115 figure [ fig : small - samples]a shows the log posterior which starts at zero for a single bin , drops to @xmath116 for @xmath117 and then increases monotonically approaching zero in the limit as @xmath11 goes to infinity .",
    "the result is that a single bin is the most probable solution for two data points .    for three data points in a single bin ( @xmath118 and @xmath112 ) ,",
    "the posterior probability is one , resulting in a log posterior of zero . in the @xmath114 case where there are two data points in one bin and one datum point in another ,",
    "the posterior probability is @xmath119 and for each point in a separate bin we have @xmath120 while the logarithm of the posterior in ( [ eq : n=3,2|1 ] ) can be greater than zero , as @xmath11 increases , the data points eventually fall into separate bins .",
    "this causes the posterior to change from ( [ eq : n=3,2|1 ] ) to ( [ eq : n=3,1|1|1 ] ) resulting in a dramatic decrease in the logarithm of the posterior , which then asymptotically increases to zero as @xmath121 .",
    "this behavior is shown in figure [ fig : small - samples]b .",
    "the result is that either one or two bins will be optimal depending on the relative positions of the data points .",
    "more rich behavior can be seen in the case of @xmath122 data points .",
    "the results again ( figure [ fig : small - samples]c ) depend on the relative positions of the data points with respect to one another . in this case",
    "the posterior probability switches between two types of behavior as the number of bins increase depending on whether the bin positions force two data points together in the same bin or separate them into two bins .",
    "the ultimate result is a ridiculous _ maximum a posteriori _ solution of 57 bins .",
    "clearly , for a small number of data points , the mode depends sensitively on the relative positions of the samples in a way that is not meaningful . in these cases there",
    "are too few data points to model a density function .    with a larger number of samples , the posterior probability shows a well - defined mode indicating a well - determined optimal number of bins . in the general case of @xmath123 where each of the @xmath3 data points is in a separate bin",
    ", we have @xmath124 which again results in a log posterior that asymptotically approaches zero as @xmath121 .",
    "figure [ fig : small - samples]d demonstrates these two effects for @xmath125 .",
    "this also can be compared to the log posterior for @xmath126 gaussian samples in figure [ fig:01]b .",
    "this investigation on the effects of small sample size raises the question as to how many data points are needed to estimate the probability density function .",
    "the general shape of a healthy log posterior reflects a sharp initial rise to a well - defined peak , and a gradual fall - off as the number of bins @xmath11 increases from one ( eg . fig .",
    "[ fig:01]b , fig .",
    "[ fig : small - samples]d . with small sample sizes",
    ", however , one finds that the bin heights have large error bars ( figure [ fig : sufficient]a ) so that @xmath127 , and that the log posterior is multi - modal ( figure [ fig : sufficient]b ) with no clear peak .",
    "we tested our algorithm on data sets with @xmath128 different sample sizes from @xmath129 to @xmath130 .",
    "one thousand data sets were drawn from a gaussian distribution for each value of @xmath3 .",
    "the standard deviation of the number of bins obtained for these 1000 data sets at a given value if @xmath3 was used as an indicator of the stability of the solution .",
    "figure [ fig : sufficient]c shows a plot of the standard deviation of the number of bins selected for the 1000 data sets at each value of @xmath3 . as we found above , with two data points , the optimal solution is always one bin giving a standard deviation of zero .",
    "this increases dramatically as the number of data points increases , as we saw in our example with @xmath131 and @xmath132 .",
    "this peaks around @xmath133 and slowly decreases as @xmath3 increases further .",
    "the standard deviation of the number of bins decreased to @xmath134 for @xmath135 , and stabilized to @xmath136 for @xmath137 .    while 30 samples may be sufficient for estimating the mean and variance of a density function known to be gaussian",
    ", it is clear that more samples are needed to reliably estimate the shape of an unknown density function . in the case where the data are described by a gaussian , it would appear that at least @xmath138 samples , and preferentially @xmath139 samples , are required to accurately and consistently infer the shape of the density function . by examining the shape of the log posterior",
    ", one can easily determine whether one has sufficient data to estimate the density function .",
    "in the event that there are too few samples to perform such estimates , one can either incorporate additional prior information or collect more data .",
    "due to the way that computers represent data , all data are essentially represented by integers . in some cases , the data samples have been intentionally rounded or truncated , often to save storage space or transmission time .",
    "it is well - known that any non - invertible transformation , such as rounding , destroys information .",
    "here we investigate how severe losses of information due to rounding or truncation affects the ` optbins ` algorithm .",
    "when data are digitized via truncation or rounding , the digitization is performed so as to maintain a resolution that we will denote by @xmath140 .",
    "that is , if the data set has values that range from 0 to 1 , and we represent these numbers with an 8 bits , the minimum resolution we can maintain is @xmath141 . for a sufficiently large data set ( in this example @xmath142 ) it will be impossible for every datum point to be in its own bin when the number of bins is greater than a critical number , @xmath143 , where @xmath144 and @xmath145 is the range of the data considered . once @xmath146 the number of populated bins @xmath147 will remain unchanged since the bin width @xmath148 for @xmath143 will be smaller than the digitization resolution , @xmath149 .    for all bin numbers",
    "@xmath143 , there will be @xmath147 populated bins with populations @xmath150 .",
    "this leads to a form for the marginal posterior probability for @xmath11 ( [ eq : posterior - for - m ] ) that depends only on the number of instances of each discrete value that was recorded , @xmath151 .",
    "since these values do not vary for @xmath146 , the marginal posterior can be viewed solely as a function of @xmath11 @xmath152 where the product over @xmath153 is over populated bins only . comparing this to ( [ eq : m > n ] ) ,",
    "the function on the right - hand side asymptotically approaches a value greater than one  so that its logarithm increases asymptotically to a value greater than zero .    as the number of bins @xmath11 increases , the point is reached where the data can not be further separated ; call this point @xmath154 .",
    "in this situation , there are @xmath155 data points in the @xmath156 bin and the posterior probability can be written as @xmath157 where @xmath158 denotes the double factorial . for @xmath159 , as @xmath160 , the log posterior asymptotes to @xmath161 , which can be further simplified to @xmath162    to test for excessive rounding or truncation , the mode of @xmath163 for @xmath164 should be compared to ( [ eq : picket - fence - log - asymptote ] ) above .",
    "if the latter is larger , than the discrete nature of the data is a more significant feature than the general shape of the underlying probability density function .",
    "when this is the case , a reasonable histogram model of the density function can still be obtained by adding a uniformly - distributed random number , with a range defined by the resolution @xmath140 , to each datum point . while this will produce the best histogram possible given the data",
    ", this will not recover the lost information .",
    "in this section , we demonstrate that our method can be extended naturally to multi - dimensional histograms .",
    "we begin by describing the method for a two - dimensional histogram .",
    "the constant - piecewise model @xmath165 of the two - dimensional density function @xmath166 is @xmath167 where @xmath168 , @xmath145 is the total area of the histogram , @xmath169 indexes the bin labels along @xmath37 , and @xmath19 indexes them along @xmath170 . since the @xmath171",
    "all sum to unity , we have @xmath32 bin probability density parameters as before , where @xmath11 is the total number of bins .",
    "the likelihood of obtaining a datum point @xmath36 from bin @xmath172 is still simply @xmath173 the previous prior assignments result in the posterior probability @xmath174 where @xmath175 is @xmath176 minus the sum of all the other bin probabilities .",
    "the order of the bins in the marginalization does not matter , which gives a result similar in form to the one - dimensional case @xmath177 where @xmath168 .    for a d - dimensional histogram ,",
    "the general result is @xmath178 where @xmath179 is the number of bins along the @xmath180 dimension , @xmath11 is the total number of bins , @xmath145 is the @xmath181-dimensional volume of the histogram , and @xmath182 indicates the number of counts in the bin indexed by the coordinates @xmath183 .",
    "note that the result in ( [ eq : log - posterior - for - m ] ) can be used directly for a multi - dimensional histogram simply by relabelling the multi - dimensional bins with a single index .",
    "figure [ fig:06 ] demonstrates the procedure on a data set sampled from a two - dimensional gaussian . in this example",
    ", 10000 samples were drawn from a two - dimensional gaussian density .",
    "figure [ fig:06]a shows the relative logarithm of the posterior probability plotted as a function of the number of bins in each dimension . the same surface is displayed as contour plot in figure [ fig:06]b , where we find the optimal number of bins to be @xmath184 .",
    "figure [ fig:06]c shows the optimal two - dimensional histogram model . note that modelled density function is displayed in terms of the number of counts rather than the probability density , which can be easily computed using ( [ eq : mean - of - prob - density ] ) with error bars computed using ( [ eq : variance - of - prob - density ] ) . in figure [ fig:06]d , we show the histogram obtained using stone s method , which results in a @xmath185 array of bins .",
    "this is clearly a sub - optimal model since random sampling variations are easily visible .",
    "the optimal binning algorithm presented in this paper , ` optbins ` , relies on finding the mode of the marginal posterior probability of the number of bins in a piecewise - constant density function .",
    "this posterior probability originates as a product of the likelihood of the density parameters given the data and the prior probability of those same parameter values .",
    "as the number of bins increases , the model can better fit the data , which leads to an increase in the likelihood function .",
    "however , by introducing additional bins , the prior probability , which must sum to unity , is now spread out over a parameter space of greater dimensionality resulting in a decrease of the prior probability .",
    "thus a density model with more bins will result in a greater likelihood , but also in a smaller prior probability .",
    "since the posterior is a product of these two functions , the maximum of the posterior probability occurs at a point where these two opposing factors are balanced .",
    "this interplay between the likelihood and the prior probability effectively implements occam s razor by selecting the most simple model that best describes the data .",
    "the quality of this algorithm was demonstrated by testing it against several other popular bin selection techniques .",
    "the ` optbins ` algorithm outperformed all techniques investigated in the sense that it consistently resulted in the minimum integrated square error between the estimated density model and the true density function .",
    "this was true even in the case of scott s technique , which is designed to asymptotically minimize this error for gaussian - distributed data .",
    "our algorithm also can be readily applied to multi - dimensional data sets , which we demonstrated with a two - dimensional data set . in practice",
    ", we have been applying ` optbins ` to three - dimensional data sets with comparable results .",
    "it should be noted that we are working with a piecewise - constant model of the density function , and _ not _ a histogram .",
    "the distinction is subtle , but important . given the full posterior probability for the model parameters and a selected number of bins , one can estimate the mean bin probabilities and their associated standard deviations .",
    "this is extremely useful in that it quantifies uncertainties in the density model , which can be used in subsequent calculations . in this paper",
    ", we demonstrated that with small numbers of data points the magnitude of the error bars on the bin heights is on the order of the bin heights themselves .",
    "such a situation indicates that too few data exist to infer a density function .",
    "this can also be determined by examining the marginal posterior probability for the number of bins . in cases where there are too few data points",
    ", the posterior will not possess a well - defined mode . in our experiments with gaussian - distributed data",
    ", we found that approximately 150 data points are needed to accurately estimate the density model when the functional form of the density is unknown .",
    "this approach has an additional advantage in that we can use the ` optbins ` algorithm to identify data sets where the data have been excessively truncated .",
    "this will be explored further in future papers @xcite .",
    "as always , there is room for improvement .",
    "the ` optbins ` algorithm , as currently implemented , performs a brute force search of the number of bins .",
    "this can be slow for large data sets that require large numbers of bins , or multi - dimensional data sets that have multiple bin dimensions .",
    "we have also made some simplifying assumptions in designing the algorithm .",
    "first , the endpoints of the density model are defined by the data , and are not allowed to vary during the analysis .",
    "second , we use the marginal posterior to select the optimal number of bins and then use this value to estimate the mean bin heights .",
    "this neglects the fact that we actually possess uncertainty about the number of bins .",
    "thus the uncertainty in the number of bins is not quantified .",
    "last , equally - spaced bins can be very inefficient in describing multi - modal density functions ( as in fig .",
    "[ fig:01]g . ) in such cases , variable bin - width models such as ` bayesian blocks ` @xcite or the markov chain monte carlo implementations that we have experimented with may prove to be more useful .    for most applications ,",
    "` optbins ` efficiently delivers an appropriate number of bins that both maximizes the depiction of the shape of the density function while minimizing the appearance of random fluctuations .",
    ".... % optbins computes the optimal number of bins for a given one - dimensional % data set .",
    "this optimization is based on the posterior probability for % the number of bins % % usage : %            optm = optbins(data , minm , maxm ) ; % % where : %            data is a ( 1,n ) vector of data points %            minm is the minimum number of bins to consider %            maxm is the maximum number of bins to consider % % this algorithm uses a brute - force search trying every possible bin number % in the given range .",
    "this can of course be improved .",
    "% generalization to multidimensional data sets is straightforward .",
    "% % created by kevin h. knuth on 17 april 2003 % modified by kevin h. knuth on 15 march 2006        % simply loop through the different numbers of bins % and compute the posterior probability for each .",
    "logp = zeros(1,maxm ) ; for m = minm : maxm      n = hist(data , m ) ;   % bin the data ( equal width bins here )",
    "part1 = n*log(m ) + gammaln(m/2 ) - gammaln(n+m/2 ) ;      part2 = - m*gammaln(1/2 ) + sum(gammaln(n+0.5 ) ) ;      logp(m ) = part1 + part2 ; end"
  ],
  "abstract_text": [
    "<S> histograms are convenient non - parametric density estimators , which continue to be used ubiquitously . </S>",
    "<S> summary quantities estimated from histogram - based probability density models depend on the choice of the number of bins . in this paper </S>",
    "<S> we introduce a straightforward data - based method of determining the optimal number of bins in a uniform bin - width histogram . using the bayesian framework </S>",
    "<S> , we derive the posterior probability for the number of bins in a piecewise - constant density model given the data . </S>",
    "<S> the most probable solution is determined naturally by a balance between the likelihood function , which increases with increasing number of bins , and the prior probability of the model , which decreases with increasing number of bins . </S>",
    "<S> we demonstrate how these results outperform several well - accepted rules for choosing bin sizes . </S>",
    "<S> in addition , we examine the effects of small sample sizes and digitized data . </S>",
    "<S> last , we demonstrate that these results can be applied directly to multi - dimensional histograms . </S>"
  ]
}