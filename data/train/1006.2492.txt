{
  "article_text": [
    "the study of systems arising in different areas , from signal processing and chemical kinetics to econometrics and finance ( see e.g. @xcite ) often requires the sampling of paths of stochastic differential equations ( sdes ) subject to initial and endpoint conditions . while unconditional path sampling of sdes is straightforward , albeit expensive for high dimensional systems of sdes , conditional path sampling can be difficult even for low dimensional systems",
    "this is because we need to produce sample paths of the sde which respect both the dynamics of the sde and the initial and endpoint conditions .",
    "an analogous situation arises in ordinary differential equations , where it can be considerably more difficult to create solutions to boundary value problems than it is to construct solutions to initial value problems ( see e.g. ch . 8 in @xcite ) .",
    "the problem of conditional path sampling of sdes has been a subject of active research in recent years and some very interesting approaches have already been developed ( see e.g. @xcite ) .",
    "the dynamics of a sde are governed by the deterministic term ( drift ) and the stochastic term ( noise ) . instead of producing conditional paths directly from the original sde",
    ", one can consider a sequence of sdes with modified drifts .",
    "the modified drifts should be chosen so that it is easier to produce sample paths which satisfy the initial and endpoint conditions .",
    "also , the sequence of modified drifts converges to the drift of the original sde .",
    "we construct a simple markov chain monte carlo ( mcmc ) algorithm which samples , in sequence , conditional paths from the modified sdes , by taking the last sampled path at each level of the sequence as an initial condition for the sampling at the next level in the sequence .",
    "we have used the drift relaxation algorithm to modify a popular filtering method called particle filter @xcite .",
    "a particle filter is a sequential importance sampling algorithm which is based on the recursive ( online ) bayesian updating of the values of samples ( called particles ) to incorporate information from noisy observations of the state of a dynamic model . while the particle filter is a very versatile method it may require a very large number of samples to approximate accurately the conditional density of the state of the model .",
    "this has led to considerable research ( see e.g. @xcite ) into how one can modify a particle filter to make it more efficient ( see also @xcite for a different approach to particle filtering ) . as an application of the drift relaxation algorithm",
    "we show in section [ particle_filtering ] how it can be used to construct a more efficient particle filter .",
    "the paper is organized as follows .",
    "section [ drift - relaxation ] presents the drift relaxation algorithm for an sde conditional path sampling problem .",
    "section [ particle_filtering ] shows how to use the algorithm to modify a particle filter .",
    "section [ numerical ] contains numerical results for the application of the modified particle filter to the standard example of filtering a diffusion in a double - well potential ( more elaborate examples will be presented in @xcite ) .",
    "finally , section [ discussion ] discusses the results as well as current and future work .",
    "suppose that we are given a system of stochastic differential equations ( sdes ) @xmath0 suppose also that we want to construct , in the time interval @xmath1,$ ] sample paths from such that the endpoints are distributed according to the densities @xmath2 and @xmath3 respectively .",
    "equation can be discretized in the interval @xmath1 $ ] by some numerical approximation scheme @xcite .",
    "suppose that we have discretized the interval @xmath1 $ ] using a stepsize @xmath4 to construct conditional paths of we can start by sampling initial conditions from the density @xmath5 for each initial condition @xmath6 we can then produce the desired conditional paths by sampling the density @xmath7 where @xmath8 is the transition probability from @xmath9 at time @xmath10 to the point @xmath11 at time @xmath12 the density can be sampled using mcmc assuming that the transition densities @xmath8 can be evaluated . however , the major issue with the mcmc sampling is whether it can be performed efficiently ( see e.g. @xcite ) . instead of mcmc sampling",
    "directly from the density i.e. , starting from an arbitrary initial path and modifying it to become a path corresponding to , we can aid the mcmc sampling process by providing the mcmc sampler of the density with a better initial condition .    to this end , consider an sde system with modified drift @xmath13 where @xmath14 can be suitably chosen to facilitate the conditional path sampling problem .    also , consider the collection of @xmath15 modified sde systems @xmath16 where @xmath17 , \\ ; l=0,\\ldots , l,$ ] with @xmath18 @xmath19 and @xmath20 instead of sampling directly a ( conditional ) path from the sde , one can sample a path from the modified sde and gradually morph the path into a path of .    * ( @xmath21 ) begin with a sample path from the modified sde .",
    "* sample through mcmc the density @xmath22 * for @xmath23 take the last sample path from the ( @xmath24)st sde and use it as in initial condition for mcmc sampling the density @xmath25 at the @xmath26th level .",
    "* keep the last sample path at the @xmath27th level .",
    "the drift relaxation algorithm is similar to simulated annealing ( sa ) used in equilibrium statistical mechanics @xcite .",
    "however , instead of modifying a temperature as in sa , here we modify the drift of the system .",
    "we show in this section how the drift relaxation algorithm can be applied to particle filtering with the aim of bringing the samples closer to the observations .",
    "suppose that we are given an sde system and that we also have access to noisy observations @xmath28 of the state of the system at specified instants @xmath29 the observations are functions of the state of the system , say given by @xmath30 where @xmath31 are mutually independent random variables . for simplicity ,",
    "let us assume that the distribution of the observations admits a density @xmath32 i.e. , @xmath33    the filtering problem consists of computing estimates of the conditional expectation @xmath34,$ ] i.e. , the conditional expectation of the state of the system given the ( noisy ) observations .",
    "equivalently , we are looking to compute the conditional density of the state of the system given the observations @xmath35 there are several ways to compute this conditional density and the associated conditional expectation but for practical applications they are rather expensive .",
    "particle filters fall in the category of importance sampling methods . because computing averages with respect to the conditional density involves the sampling of the conditional density which can be difficult , importance sampling methods proceed by sampling a reference density @xmath36 which can be easily sampled and then compute the weighted sample mean @xmath37 \\approx \\frac{1}{n } \\sum_{n=1}^n f(x^n_{t_k})\\frac{p(x^n_{t_k}|\\{z_{t_j}\\}^{k}_{j=1})}{q(x^n_{t_k}|\\{z_{t_j}\\}^{k}_{j=1})}\\ ] ] or the related estimate @xmath38 \\approx \\frac{\\sum_{n=1}^n f(x^n_{t_k } ) \\frac{p(x^n_{t_k}|\\{z_{t_j}\\}^{k}_{j=1})}{q(x^n_{t_k}|\\{z_{t_j}\\}^{k}_{j=1})}}{\\sum_{n=1}^n \\frac{p(x^n_{t_k}|\\{z_{t_j}\\}^{k}_{j=1})}{q(x^n_{t_k}|\\{z_{t_j}\\}^{k}_{j=1})}},\\ ] ] where @xmath39 has been replaced by the approximation @xmath40 particle filtering is a recursive implementation of the importance sampling approach .",
    "it is based on the recursion @xmath41 if we set @xmath42 then from we get @xmath43 the approximation in expression becomes @xmath44 \\approx",
    "\\frac{\\sum_{n=1}^n f(x^n_{t_k})g(x^n_{t_k},z_{t_k})}{\\sum_{n=1}^n g(x^n_{t_k},z_{t_k})}\\ ] ] from we see that if we can construct samples from the predictive distribution @xmath45 then we can define the ( normalized ) weights @xmath46 use them to weigh the samples and the weighted samples will be distributed according to the posterior distribution @xmath35    in many applications , most samples will have a negligible weight with respect to the observation , so carrying them along does not contribute significantly to the conditional expectation estimate ( this is the problem of degeneracy @xcite ) . to create larger diversity",
    "one can resample the weights to create more copies of the samples with significant weights .",
    "the particle filter with resampling is summarized in the following algorithm due to gordon _",
    "_ @xcite .    1 .",
    "begin with @xmath39 unweighted samples @xmath47 from @xmath48 2 .",
    "* prediction * : generate @xmath39 samples @xmath49 from @xmath50 3 .",
    "* update * : evaluate the weights @xmath51 4 .",
    "* resampling * : generate @xmath39 independent uniform random variables @xmath52 in @xmath53 for @xmath54 let @xmath55where @xmath56 where @xmath57 can range from @xmath58 to @xmath59 5 .",
    "set @xmath60 and proceed to step 1 .    the particle filter algorithm is easy to implement and adapt for different problems since the only part of the algorithm that depends on the specific dynamics of the problem is the prediction step .",
    "this has led to the particle filter algorithm s increased popularity @xcite . however , even with the resampling step , the particle filter can still need a lot of samples in order to describe accurately the conditional density @xmath35 snyder _ et al . _",
    "@xcite have shown how the particle filter can fail in simple high dimensional problems because one sample dominates the weight distribution .",
    "the rest of the samples are not in statistically significant regions . even worse , as we will show in the numerical results section , there are simple examples where not even one sample is in a statistically significant region . in the next subsection",
    "we present how drift relaxation can be used to push samples closer to statistically significant regions .",
    "several authors ( see e.g. @xcite ) have suggested the use of a mcmc step after the resampling step ( step 4 ) in order to move samples away from statistically insignificant regions .",
    "there are many possible ways to append an mcmc step after the resampling step in order to achieve that objective .",
    "the important point is that the mcmc step must preserve the conditional density @xmath35 in the current section we show that the mcmc step constitutes a case of conditional path sampling .",
    "we begin by noting that one can use the resampling step ( step 4 ) in the particle filter algorithm to create more copies not only of the good samples according to the observation , but also of the values ( initial conditions ) of the samples at the previous observation .",
    "these values are the ones who have evolved into good samples for the current observation ( see more details in @xcite ) .",
    "the motivation behind producing more copies of the pairs of initial and final conditions is to use the good initial conditions as starting points to produce statistically more significant samples according to the current observation .",
    "this process can be accomplished in two steps .",
    "first , step 4 of the particle filter algorithm is replaced by    : generate @xmath39 independent uniform random variables @xmath52 in @xmath53 for @xmath54 let @xmath61where @xmath56 also , through bayes rule @xcite one can show that the posterior density @xmath62 is preserved if one samples from the density @xmath63 where @xmath64 are given by the modified resampling step .",
    "this is a problem of conditional path sampling for ( continuous - time or discrete ) stochastic systems .",
    "the important issue is to perform the necessary sampling efficiently @xcite .",
    "we propose to do that here using drift relaxation ( see section [ drift - relaxation ] ) .",
    "the particle filter with mcmc step algorithm is given by    1 .",
    "begin with @xmath39 unweighted samples @xmath47 from @xmath48 2 .",
    "* prediction * : generate @xmath39 samples @xmath49 from @xmath50 3 .",
    "* update * : evaluate the weights @xmath51 4 .",
    "* resampling * : generate @xmath39 independent uniform random variables @xmath52 in @xmath53 for @xmath54 let @xmath61 where @xmath56 where @xmath57 can range from @xmath58 to @xmath59 5 .",
    "* mcmc step * : for @xmath54 choose a modified drift ( possibly different for each @xmath65 ) .",
    "construct a path for the sde with the modified drift starting from @xmath66 construct through drift relaxation a markov chain for @xmath67 with stationary distribution @xmath68 6 .",
    "set @xmath69 7 .",
    "set @xmath60 and proceed to step 1 .",
    "since the samples @xmath70 are constructed by starting from different sample paths , they are independent . also , note that the samples @xmath71 are unweighted . however , we can still measure how well these samples approximate the posterior density by comparing the effective sample sizes of the particle filter with and without the mcmc step . for a collection of @xmath39 samples the effective sample size @xmath72 is defined by @xmath73 where @xmath74 the effective sample size can be interpreted as that the @xmath39 weighted samples are worth of @xmath75 i.i.d .",
    "samples drawn from the target density , which in our case is the posterior density . by definition , @xmath76 if the samples have uniform weights , then @xmath77 on the other hand , if all samples but one have zero weights , then @xmath78",
    "we present numerical results of the particle filter algorithm with mcmc step for the standard problem of diffusion in a double - well potential ( more elaborate applications of the method will be presented elsewhere @xcite ) .",
    "our objective here is to show how the generic particle filter s performance can be significantly improved by incorporating the mcmc step via drift relaxation .",
    "the problem of diffusion in a double well potential is described by the scalar sde @xmath79 the deterministic part ( drift ) describes a gradient flow for the potential @xmath80 which has two minima , at @xmath81 in the notation of section [ particle_filtering ] we have @xmath82 and @xmath83 if the stochastic term is zero the solution wanders around one of the minima depending on the value of the initial condition .",
    "a weak stochastic term leads to rare transitions between the minima of the potential .",
    "we have chosen the coefficient @xmath84 to make the stochastic term rather weak .",
    "this is done because we plan to enforce the observations to alternate among the minima , and thus check if the particle filter can track these transitions .",
    "the sde is discretized by the euler - maruyama @xcite scheme with step size @xmath85 which is small enough to guarantee stability of the scheme .",
    "the initial condition is set to @xmath86 and there is a total of @xmath87 observations at @xmath88 the observations are given by @xmath89 where @xmath90 for @xmath91 for this choice of observation noise , the observation density ( also called likelihood ) is given by @xmath92\\ ] ] the observations alternate between @xmath58 and @xmath93 in particular , for @xmath94 we have @xmath95 if @xmath96 is odd and @xmath97 if @xmath96 is even .    in order to apply the mcmc step with drift relaxation we need to define the modified drift @xmath14 for the process @xmath98 given by @xmath99 the modified drift can be the same for all the samples or different for each sample . since the difficulty in tracking the observations comes from the inability of the original sde to make frequent transitions between the two minima of the double well , an intuitively appealing choice for @xmath14 is @xmath100 where @xmath101 this drift corresponds to the potential @xmath102 the potential @xmath103 has its minima also located at @xmath104 however , the value of the potential at the minima is @xmath105 instead of @xmath86 for the potential @xmath106 this means that the wells corresponding to the minima of @xmath103 are shallower than the wells corresponding to the minima of @xmath106 this makes the transitions between the two wells for the process @xmath98 more frequent than for the original process @xmath107 for the numerical experiments we have chosen @xmath108    the sequence of modified sdes for the drift relaxation algorithm with @xmath27 levels is given by @xmath109 where @xmath17 , \\ ; l=0,\\ldots , l,$ ] with @xmath18 @xmath19 and @xmath20 for our numerical experiments we chose @xmath110 and @xmath111    recall that the density we want to sample during the mcmc step is given by @xmath112 where @xmath113 is the transition probability between @xmath64 and @xmath114 for many applications , sampling directly from @xmath113 may be impossible .",
    "thus , one needs to resort to some numerical approximation scheme which approximates the path between @xmath64 and @xmath115 by a discretized path .",
    "however ( see @xcite for details ) , even the evaluation of the discretized path s density may not be efficient . instead , by using the fact that each brownian path in gives rise to a unique path for @xmath116 @xcite , we can replace the sampling of @xmath117 by sampling from the density @xmath118   \\prod_{i=0}^{i-1 }",
    "\\exp \\biggl [ - \\frac{(\\delta b^n_{i})^2}{2*\\delta t }   \\biggr ] = \\\\",
    "\\exp \\biggl [ - \\biggl (   \\frac { ( z_{t}-x^n_{t}(\\{\\delta b^n_{i } \\}_{i=0}^{i-1}))^2}{2 * 0.01 }   +   \\sum_{i=0}^{i-1 } \\frac{(\\delta b^n_{i})^2}{2*\\delta t }   \\biggr ) \\biggr]\\end{gathered}\\ ] ] where @xmath119 are the brownian increments of the discretized path connecting @xmath64 and @xmath114 also , note that the final point @xmath115 has now become a function of the entire brownian path @xmath120 for the numerical experiments we have chosen @xmath121 which , since @xmath122 gives @xmath123    we use drift relaxation to produce samples from the density .",
    "the markov chain at each level of the drift relaxation algorithm is constructed using hybrid monte carlo ( hmc ) @xcite . at the @xmath26th level ,",
    "we can discretize , say with the euler - maruyama scheme , and the points on the path will be given by @xmath124 for @xmath125 we can use more sophisticated schemes than the euler - maruyama scheme for the discretization of the simplified sde at the cost of making the expression for the density more complicated .    we can define a potential @xmath126 for the variables @xmath127 the potential is given by @xmath128 and the density to be sampled can be written as @xmath129.\\ ] ] the subscript @xmath130 is to denote the dependence of the potential on the drift relaxation parameter @xmath131 in hmc one considers the variables on which the potential depends as the position variables of a hamiltonian system . in our case",
    "we have @xmath132 position variables so we can define a @xmath132-dimensional position vector @xmath133 the next step is to augment the position variables vector by a vector of associated momenta @xmath134 together they form a hamiltonian system with hamiltonian given by @xmath135 where @xmath136 is the momenta vector .",
    "thus , the momenta variables are gaussian distributed random variables with mean zero and variance 1 .",
    "the equations of motion for this hamiltonian system are given by hamilton s equations @xmath137 note that the hamiltonian depends also on the initial condition for each sample @xmath138 and we have written an equation for the evolution of @xmath139 as well its associated momentum @xmath140 since the @xmath138 are fixed by the resampling procedure we do not evolve them .",
    "however , note that the brownian increment @xmath141 needs to be evolved because it affects the evolution of @xmath142    hmc proceeds by assigning initial conditions to the momenta variables ( through sampling from @xmath143 ) , evolving the hamiltonian system in fictitious time @xmath144 for a given number of steps of size @xmath145 and then using the solution of the system to perform a metropolis accept / reject step ( more details in @xcite ) .",
    "after the metropolis step , the momenta values are discarded . the most popular method for solving the hamiltonian system , which is the one we also used , is the verlet leapfrog scheme . in our numerical implementation , we did not attempt to optimize the performance of the hmc algorithm . for the sampling at each level of the drift relaxation process we used @xmath87 metropolis",
    "accept / reject steps and @xmath58 hmc step of size @xmath146 to construct a trial path .",
    "a detailed study of the drift relaxation / hmc algorithm for conditional path sampling problems outside of the context of particle filtering will be presented in a future publication .    for the chosen values of the parameters for the drift relaxation and hmc steps , the particle filter with mcmc step is about @xmath147 times more expensive per sample ( particle ) than the generic particle filter .",
    "however , we show that this increase in cost per sample is worthwhile .",
    "figure [ plot_comparison_varying ] compares the performance of the particle filter with mcmc step with @xmath87 samples and the generic particle filter with @xmath148 samples .",
    "it is obvious that the particle filter with mcmc step follows accurately all the transitions between the two minima of the double - well . on the other hand ,",
    "the generic particle filter captures accurately only every other observation .",
    "it fails to performs the transitions between the two minima of the double - well .",
    "of course , since the particle filter with mcmc step uses only @xmath87 samples the conditional expectation estimate of the hidden signal is not as smooth as the estimate of the generic particle filter which uses @xmath148 samples .",
    "however , the particle filter with mcmc step allows a much better resolution of the conditional density ( conditioned on the observations ) .",
    "this can be seen by computing the effective sample size for the two filters .",
    "figure [ plot_comparison_ess_varying ] shows the effective sample size for both filters . because of the different number of samples used in the two filters we have plotted the effective sample size for each filter as a percentage of the respective sample size .",
    "we see that the particle filter with mcmc step has overall a much better sample size than the generic particle filter .",
    "the generic particle filter has a wildly fluctuating effective sample size . in particular",
    ", since the generic particle filter misses every other observation , the corresponding effective sample size dips down to @xmath58 sample for the missed observations . note from figure [ plot_comparison_varying ] that even this one sample which dominates the observation weight does not come close to the observation .",
    "for the observations that the generic particle filter does capture , its effective sample size is still lower than the effective sample size of the particle filter with mcmc step .",
    "we have presented an algorithm for conditional path sampling of sdes .",
    "the proposed algorithm is based on drift relaxation which allows to sample conditional paths from a modified drift equation .",
    "the conditional paths of the modified drift equation are then morphed into conditional paths of the original equation .",
    "we have called this process of gradually enforcing the drift of the original equation drift relaxation .",
    "the algorithm has been used to create a modified particle filter for sdes .",
    "we have shown that the modified particle filter s performance is significantly better than the performance of a generic particle filter .    in the current work ,",
    "we have examined the application of drift relaxation to the filtering problem of diffusion in a double - well potential which is a standard example in the filtering literature .",
    "the same algorithm can be applied to the problem of tracking a single target .",
    "a problem of great practical interest is that of tracking not only one but multiple moving targets @xcite .",
    "the multi - target tracking problem is much more difficult than the single - target problem due to the combinatorial explosion of the number of possible target - observation association arrangements . in this context",
    ", the accurate tracking of each target becomes crucial .",
    "suppose that only one of the targets is of interest and the rest act as decoys @xcite .",
    "the inability to track each potential target accurately can lead to ambiguity about the targets movement if the observations for different targets are close .",
    "we have already applied the drift relaxation modified particle filter to multi - target tracking problems with very encouraging results which will appear elsewhere @xcite .",
    "i am grateful to profs .",
    "v. maroulas and j. weare for many helpful discussions and comments .",
    "i would also like to thank for its hospitality the institute for mathematics and its applications ( i m a ) at the university of minnesota where the current work was completed ."
  ],
  "abstract_text": [
    "<S> we present an algorithm for the efficient sampling of conditional paths of stochastic differential equations ( sdes ) . while unconditional path sampling of sdes is straightforward , albeit expensive for high dimensional systems of sdes , conditional path sampling can be difficult even for low dimensional systems . </S>",
    "<S> this is because we need to produce sample paths of the sde which respect both the dynamics of the sde and the initial and endpoint conditions . </S>",
    "<S> the dynamics of a sde are governed by the deterministic term ( drift ) and the stochastic term ( noise ) . instead of producing conditional paths directly from the original sde </S>",
    "<S> , one can consider a sequence of sdes with modified drifts . </S>",
    "<S> the modified drifts should be chosen so that it is easier to produce sample paths which satisfy the initial and endpoint conditions . </S>",
    "<S> also , the sequence of modified drifts converges to the drift of the original sde . </S>",
    "<S> we construct a simple markov chain monte carlo ( mcmc ) algorithm which samples , in sequence , conditional paths from the modified sdes , by taking the last sampled path at each level of the sequence as an initial condition for the sampling at the next level in the sequence . </S>",
    "<S> the algorithm can be thought of as a stochastic analog of deterministic homotopy methods for solving nonlinear algebraic equations or as a sde generalization of simulated annealing . </S>",
    "<S> the algorithm is particularly suited for filtering / smoothing applications . </S>",
    "<S> we show how it can be used to improve the performance of particle filters . </S>",
    "<S> numerical results for filtering of a stochastic differential equation are included . </S>"
  ]
}