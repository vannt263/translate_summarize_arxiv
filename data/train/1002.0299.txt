{
  "article_text": [
    "a key challenge across many domains of science and engineering is to understand the behavior of complex systems in terms of dynamical interactions among their component parts .",
    "a common way to address this challenge is by analysis of time series data acquired simultaneously from multiple system components .",
    "increasingly , such analysis aims to draw inferences about _ causal _ interactions among system variables @xcite , as a complement to standard assessments of undirected functional connectivity as revealed by coherence , correlation , and the like .    a first step in any dynamical analysis is to identify target variables .",
    "typically , subsequent analysis then assumes that functional ( causal ) interactions take place among these variables .",
    "however , in the general case it may be that explanatorily relevant causal interactions take place among _ groups _ , or `` ensembles '' , of variables @xcite .",
    "it is important to account for this possibility for at least two reasons .",
    "first , identification of target variables is usually based on _ a priori _ system knowledge or technical constraints , which may be incomplete or arbitrary , respectively .",
    "second , even given appropriate target variables , it is possible that relevant interactions may operate at multiple scales within a system , with larger scales involving groups of variables .",
    "consider an example from functional neuroimaging . in a typical fmri study",
    ", the researcher may identify _ a priori _ several `` regions - of - interest '' ( roi ) in the brain , each represented in the fmri dataset by multiple voxels , where each voxel is a variable comprising a single time series reflecting changes in the underlying metabolic signal . assuming that the objective of the study is to assess the causal connectivity among the rois , a standard approach is to derive a single time series for each roi either by averaging or by extracting a principal component @xcite ; alternatively",
    ", repeated pairwise analysis can be performed on each pair of voxels . a more appropriate approach",
    ", however , may be to consider causal interactions among the multivariate groups of voxels comprising each roi .",
    "similar scenarios could be concocted in a very wide range of application areas , including economics , biology , climate science , among others .    in this paper",
    ", we describe a principled approach to assessing causal interactions among multivariate groups of variables .",
    "our approach is based on the concept of granger causality ( g - causality ) @xcite , a statistical notion of causality which originated in econometrics but which has since found widespread application in many fields , with a particular concentration in the neurosciences @xcite .",
    "g - causality is an example of time series inference on stochastic processes and is usually implemented via autoregressive modeling of multivariate time series .",
    "the basic idea is simple : one variable ( or time series ) can be called `` causal '' to another if the ability to predict the second variable is improved by incorporating information about the first .",
    "more precisely , given inter - dependent variables @xmath0 and @xmath1 , it is said that `` @xmath1 g - causes @xmath0 '' if , in a statistically suitable manner , @xmath1 assists in predicting the future of @xmath0 beyond the degree to which @xmath0 already predicts its own future .",
    "it is straightforward to extend g - causality to the conditional case @xcite , where @xmath1 is said to g - cause @xmath0 , conditional on @xmath2 , if @xmath1 assists in predicting the future of @xmath0 beyond the degree to which @xmath0 and @xmath2 together already predict the future of @xmath0 .",
    "importantly , conditional g - causality is orthogonal to the notion of inferring causality among groups of variables , which is the focus of the present paper and which we term _ multivariate _ g - causality . in the multivariate case ,",
    "the above description of g - causality is generalized to interactions among _ sets _ of interdependent variables @xmath3 , @xmath4 , and ( in the conditional multivariate case ) @xmath5 .",
    "the generalization we propose was originally introduced in the field of econometrics by geweke in 1982 @xcite , but has since been almost totally overlooked . indeed a different measure has recently appeared @xcite . in the following ,",
    "we derive several justifications for preferring geweke s measure , some of which we examine numerically .",
    "we go on to explore a series of implications for the analysis of complex systems in general , with a particular focus on applications in neuroscience .    after laying out our conventions in section 2 , in section 3",
    "we introduce two alternative measures of multivariate g - causality .",
    "the formulations differ according to their treatment of the covariance matrices of residuals in the underlying autoregressive models : geweke s measure uses the _ determinant _ of this matrix ( the generalized variance ) , while the other uses the _ trace _ ( the total variance ) .",
    "section 4 explores several advantageous properties of the determinant formulation as compared to the trace formulation . in brief , the determinant formulation is fully equivalent to transfer entropy @xcite under gaussian assumptions , is invariant under a wider range of variable transformations , is expandable as a sum of standard univariate g - causalities , and admits a satisfactory spectral decomposition .",
    "numerically , we show that geweke s measure is just as stable as is the alternative measure based on the total variance .",
    "section 5 extends the determinant formulation to the important case of `` partial '' g - causality which provides some measure of control with respect to unmeasured latent or exogenous variables .",
    "section 6 extends a previously defined measure of `` causal density '' @xcite which reflects the overall dynamical complexity of causal interactions sustained by a system . in section 7",
    "we show how multivariate g - causality can enhance a measure of `` autonomy '' ( or `` self - causation '' ) based on g - causality @xcite , and section 8 carries the discussion towards the identification of macroscopic variables via the notion of causal independence .",
    "section 9 provides a general discussion and summary of contributions .",
    "we use a mathematical vector / matrix notation in which bold type generally denotes vector quantities and upper - case type denotes matrices or random variables , according to context .",
    "all vectors are considered to be _ column _ vectors . `",
    "@xmath6 ' denotes _ vertical concatenation _ of vectors , so that for @xmath7 and @xmath8 , @xmath9 is the vector @xmath10 of dimension @xmath11 , where the symbol ` @xmath12 ' denotes the transpose operator .",
    "we also write @xmath13 for the determinant and @xmath14 for the trace of a square matrix .",
    "given jointly distributed multivariate random variables (  random vectors ) @xmath15 , we denote by @xmath16 the @xmath17 matrix of covariances @xmath18 and by @xmath19 the @xmath20 matrix of cross - covariances @xmath21 .",
    "we then use @xmath22 to denote the @xmath17 matrix @xmath23 defined when @xmath24 is invertible .",
    "@xmath22 appears as the covariance matrix of the residuals of a linear regression of @xmath3 on @xmath4 ( c.f .",
    "eq .   below ) ; thus , by analogy with _ partial correlation _ @xcite we term @xmath22 the _ _ partial covariance _ _ of @xmath3 given @xmath4 .",
    "similarly , given another jointly distributed variable @xmath5 , we define the _ partial cross - covariance _ @xmath25    the following identity @xcite will be useful for deriving certain properties of multivariate g - causality : @xmath26    suppose we have a multivariate stochastic process @xmath27 in discrete time (  the random variables @xmath28 are jointly distributed ) .",
    "we use the notation @xmath29 to denote @xmath3 itself , along with @xmath30 _ lags _ , so that for each @xmath31 , @xmath32 is a random vector of dimension @xmath33 . given the lag @xmath34 , we also often use the shorthand notation @xmath35 for the lagged variable .",
    "g - causality analysis is concerned with the comparison of different linear regression models of data .",
    "thus , let us consider the ( multivariate ) linear regression of one random vector @xmath3 , the predictee , on another random vector @xmath4 , the predictor : @xmath36 where the @xmath20 matrix @xmath37 contains the regression coefficients and the random vector @xmath38 comprises the residuals .",
    "the coefficients of this model are uniquely specified by imposing zero correlation between the residuals @xmath39 and the regressors ( predictors ) @xmath4 . via the yule - walker procedure",
    "@xcite one obtains @xmath40 and finds the covariance matrix of the residuals to be given by @xmath41 with @xmath22 defined as in .",
    "suppose now we have three jointly distributed , stationary multivariate stochastic processes @xmath42 .",
    "then to measure the g - causality from @xmath4 to @xmath3 given @xmath5 , one wants to compare the following two multivariate autoregressive ( mvar ) models for the processes @xcite : @xmath43 thus the predictee variable @xmath3 is regressed firstly on the previous @xmath34 lags of itself plus @xmath44 lags of the conditioning variable @xmath5 and secondly , in addition , on @xmath45 lags of the predictor variable @xmath4 ( in theory , if not in practice , @xmath34 , @xmath45 and @xmath44 could be infinite ) . the conditioning variable . in practice",
    "it is the more useful form ; for the non - conditional version , @xmath5 may simply be omitted . ]",
    "the standard measure of g - causality used in the literature is defined only for _ univariate _ predictor and predictee variables @xmath1 and @xmath0 , and is given by the log of the ratio of the residual variances for the regressions . in our notation , and @xmath1 are univariate , the _ lagged _ variables @xmath46 and @xmath47 will generally be multivariate ( at least if @xmath48 ) ; hence they are written in bold type . ]",
    "@xmath49 where the last equality follows from the general formula . by stationarity",
    "this expression does not depend on time @xmath31 .",
    "note that the residual variance of the first regression will always be larger than or equal to that of the second , so that @xmath50 always .",
    "as regards statistical inference , it is known that the corresponding maximum likelihood estimator - statistic for the regressions rather than @xmath51 itself @xcite ; the quantities are in any case related by a monotonic transformation . ]",
    "@xmath52 will have ( asymptotically for large samples ) a @xmath53-distribution under the null hypothesis @xmath54 @xcite , and a non - central @xmath53-distribution under the alternative hypothesis @xmath55 @xcite .",
    "we now consider the case where predictee and predictor variables are no longer constrained to be univariate , i.e.  multivariate g - causality . for a multivariate predictor , eq .   above",
    "( with @xmath1 replaced by the bold - type @xmath4 ) is a valid and consistent formula for g - causality .",
    "however , for the case of a multivariate predictee there is not yet a standard definition for g - causality .",
    "one possibility is to simply use the multivariate mean square error ( i.e.  total variance , or expected squared length of the multivariate residual ) , leading to @xmath56 we call this the trace version of multivariate g - causality ( trvmvgc ) . as recently noted by ladroue and colleagues @xcite trvmvgc appears to be a natural extension of g - causality to the multivariate case because total variance is a common choice for a measure of goodness - of - fit or prediction error for a multivariate regression .",
    "moreover , the measure is always non - negative , reduces to when the predictee variable is univariate , and the regression matrix coefficients that render the residuals uncorrelated with the regressors also minimize the total variance ( this is just the `` ordinary least squares '' procedure , minimizing mean square error ) .",
    "nonetheless , an alternative originally proposed by geweke @xcite uses instead the _ generalized variance _",
    "@xmath57 , which quantifies the _ volume _ in which the residuals lie .",
    "this leads to the measure @xmath58 like trvmvgc , this measure is always non - negative , reduces to when the predictee variable is univariate , and is consistent with the autoregressive approach inasmuch as the yule - walker regression matrix coefficients minimize the generalized variance , @xmath59 , as well as the total variance , ( see appendix  [ apx : mindet ] for a proof ) .",
    "geweke @xcite lists a number of motivations for taking @xmath60 as given in eq .   as the natural extension of g - causality to the multivariate case .",
    "these include : ( i ) that the generalized variance version is invariant under ( linear ) transformation of variables ( see section  [ sec : invariance ] ) ; and ( ii ) that the maximum likelihood estimator of this quantity , @xmath61 , is asymptotically @xmath53-distributed for large samples . in the following section we further justify this choice . since we advocate the use of geweke s measure of multivariate g - causality we abbreviate this simply as mvgc henceforth .",
    "as remarked previously , the expression defines _",
    "conditional _ mvgc .",
    "geweke @xcite gives the following intuitively appealing expression for @xmath60 in terms of unconditional mvgcs : @xmath62 that is , the extent to which @xmath4 and @xmath5 together cause @xmath3 less the extent that @xmath5 on its own causes @xmath3 . note that this identity also holds for trvmvgc .",
    "in the following subsections we discuss some properties of mvgc and further motivate geweke s definition of this measure .",
    "when all variables are gaussian distributed , the mvgc @xmath60 is fully equivalent to the transfer entropy @xmath63 , an information - theoretic notion of causality @xcite , with a simple factor of 2 relating the two quantities , @xmath64 transfer entropy @xcite is defined by the difference in entropies @xmath65 and quantifies the degree to which knowledge of the past of @xmath4 reduces uncertainty in the future of @xmath3 .",
    "the equivalence stems from the entropy of a gaussian distribution being directly proportional to the logarithm of the determinant of its covariance matrix ; and , furthermore , from any conditional entropy involving gaussian variables being directly proportional to the logarithm of the determinant of the appropriate corresponding _ partial _ covariance matrix ( see @xcite for details ) . due to the use of the determinant being crucial for this relationship , for trvmvgc the equivalence holds only in the more restricted situation when the predictee variable is univariate .",
    "in addition to motivating mvgc over trvmvgc , the equivalence also provides a justification for the use of linear regression models in measuring causality .",
    "transfer entropy is naturally sensitive to nonlinearities in the data , a property which is rightly seen as desirable for measures of causality and which has motivated the development of several nonlinear extensions to standard g - causality @xcite .",
    "however , when data are gaussian , the two linear regressions capture all of the entropy difference that defines transfer entropy , which implies that non - linear extensions to g - causality are of no additional utility . indeed for two multivariate gaussian variables @xmath3 and @xmath4 , the partial covariance @xmath66 , which is the same quantity as the residual covariance under linear regression , can be simply thought of as the conditional covariance of @xmath3 given @xmath4 , because @xmath67 for all @xmath68 .",
    "hence , for gaussian data , linear regression accounts for all the dependence of the regressee on the regressor .    to demonstrate formally that a stationary gaussian ar process must be _ linear _ , consider a general stationary multivariate gaussian process @xmath27 satisfying @xmath69 where @xmath70 is some sufficiently well - behaved , possibly nonlinear function and the @xmath71 are independent of @xmath72 for @xmath73 . for any @xmath31 then , @xmath74 is independent of @xmath75 , so that , in particular , for any value @xmath76 taken by @xmath75 , the conditional expectation @xmath77 does not depend on @xmath76 and nor , by stationarity , on @xmath31 .",
    "but since by assumption @xmath27 and @xmath75 are jointly multivariate gaussian , by a well - known result @xmath78 depends linearly on @xmath76 , and from it follows that @xmath79 must be a linear function of @xmath76 .",
    "the partial covariance @xmath66 transforms in a simple way under linear transformation of variables .",
    "if @xmath80 and @xmath81 are respective matrices for linear transformations on @xmath3 and @xmath4 then we have that @xmath82 using this formula , and the properties of the determinant and trace operators , we can find the respective groups of linear transformations under which mvgc and trvmvgc are invariant . for mvgc , we find that the most general transformation that @xmath60 is invariant under is given by @xmath83 where the matrices @xmath84 , @xmath85 and @xmath86 on the diagonal are non - singular .",
    "all these symmetries are desirable properties for a causality measure .",
    "there ought to be invariance under redefinition of the individual variables within each of @xmath3 , @xmath4 and @xmath5 , ( i.e.  under the diagonal components @xmath84 , @xmath85 and @xmath86 of eq .  ) , because mvgc is designed to measure causality between unified wholes rather than between arbitrarily defined constituent elements",
    ". the `` off - diagonal '' components @xmath87 , @xmath88 and @xmath89 are also intuitive .",
    "adding components of @xmath5 or @xmath3 to the predictor @xmath4 should not change the value of mvgc , because mvgc is designed to measure the ability of @xmath4 at predicting @xmath3 _ over and above _ @xmath5 and @xmath3 . similarly , adding components of @xmath3 onto @xmath5 should not make a difference because the predictee @xmath3 could already be thought of as a conditional variable before transformation .",
    "trvmvgc has an invariance under a similar group of transformations but with one significant restriction , namely that the matrix @xmath84 must be _ conformal _ ( angle - preserving ) , that is @xmath84 must satisfy @xmath90 for some constant @xmath91 .",
    "this difference can have practical consequences .",
    "the broader invariance of mvgc ( under _ all _ linear transformations @xmath84 ) means that this measure , but not trvmvgc , is insensitive to certain common inaccuracies of data collection , namely those in which variables within a given set @xmath3 are contaminated by contributions from other variables ( see discussion ) . to put this point another way ,",
    "if one wishes to infer mvgc between _ hidden _ variables by analyzing mvgc between observed variables , these two quantities are actually the same if the relationship between hidden and observed variables is linear and can be written in the form given in eq .  .",
    "one may also wish to measure the mvgc from the independent components of the predictor to the independent components of the predictee .",
    "again , the invariance properties of mvgc mean that one does not need to explicitly find these independent components ; one can simply compute mvgc between observed components .",
    "these observations indicate that mvgc takes into account correlation between variables in a principled way .",
    "we see this explicitly in section [ sec : expand ] .",
    "the restriction @xmath90 for trvmvgc further implies that an uneven _ rescaling _ of the components of the predictee variable may change the value of @xmath92 .",
    "this too has practical implications , namely that trvmvgc but not mvgc can be affected by magnitude differences in the components of @xmath3 , perhaps resulting from these components reflecting underlying mechanisms that are differently amplified or differentially accessible to the measuring equipment , a common situation in many neuroscience contexts ( see discussion ) .",
    "this sensitivity is undesirable because causal connectivity should be based on the information content of signals ( c.f .  section  [ sec : gauss ] ) , and not on their respective magnitudes .",
    "it is worth noting that for transfer entropy the symmetry group can be extended to include all non - singular ( not necessarily linear ) transformations of the predictee variable , since the entropies are invariant under such transformations .",
    "since g - causality is essentially a linear version of transfer entropy , the former should at least be invariant under the linear subgroup of transformations .",
    "mvgc is expandable as a sum of g - causalities over all combinations of _ univariate _ predictor and predictee variables contained within the multivariate composites .",
    "the existence of this expansion depends on the fact that determinants are decomposable into products , and that logarithms of products are decomposable into sums of logarithms .",
    "no such decomposition exists for the logarithm of a trace , and so there is no obvious way of expanding trvmvgc into combinations of univariate components .",
    "the expansion of mvgc is not entirely straightforward because different terms in the sum involve conditioning on the past and present of different subsets of variables .",
    "however each predictor / predictee combination appears precisely once in the sum , and each term can be explained intuitively . the general formula may be written as @xmath93 where the superscript ` @xmath94 ' indicates conditioning on the present ( in addition to the past ) of the corresponding variables .",
    "thus , in the term for causality from @xmath95 to @xmath96 one conditions on ( i ) the past of the entire multivariate conditional variable @xmath5 , ( ii ) the past of the entire multivariate predictee variable @xmath3 , ( iii ) the past of all predictor variables @xmath97 with @xmath98 and ( iv ) the present of all predictee variables @xmath99 with @xmath100 .",
    "the derivation of the expansion is given in appendix [ expandproof ] .",
    "for the case of a multivariate predictor and a univariate predictee we have @xmath101 this formula is consistent with the intuitive idea that the total degree to which the multivariate @xmath4 helps predict the univariate @xmath0 is : the degree to which @xmath102 predicts @xmath0 , plus the degree to which @xmath103 helps predict @xmath0 over and above the information already present in @xmath102 , and so on .",
    "for the case of a multivariate predictee and a univariate predictor we have @xmath104 this formula supports the intuition that the total degree to which the univariate @xmath1 helps predict the multivariate @xmath3 is : the degree to which the past of @xmath1 helps predict the current value of @xmath105 over and above the degree to which the past of the whole of @xmath3 predicts the current value of @xmath105 , plus the degree to which the past of @xmath1 helps predict the current value of @xmath106 over and above the degree to which the past of the whole of @xmath3 and the current value of @xmath105 predicts the current value of @xmath106 , and so on .",
    "we remark on two implications of the expansion of mvgc .",
    "first , ladroue and colleagues suggested that use of generalized residual variance for causal inference on high - dimensional data might suffer from problems of numerical stability .",
    "however , the expansion of mvgc into low - dimensional , univariate g - causalities suggests that there should be no problem ( see section [ sec : stability ] for numerical evidence of this ) .",
    "second , the expansion indicates that mvgc controls for , to some extent , the influence of unmeasured latent / exogenous variables ( see also section [ sec : partial ] ) . by conditioning on the present of certain appropriate predictee variables for each term of the expansion , only the effects of each predictor on independent components of the predictees enter the equation .",
    "this property stems from the fact that the determinant of the residual covariance matrix reflects not just residual variances , but also the extent to which these residual variances are independent of each other .",
    "this is another advantage of the mvgc measure over trvmvgc , which does not depend on residual correlations .",
    "we tested numerically our claim ( section [ sec : expand ] ) that mvgc should not be less stable than trvmvgc .",
    "we studied mvar@xmath107 processes whose dynamics are given by @xmath108 where @xmath3 contains 8 variables , the sum of each row of @xmath37 ( i.e.  total afferent to each element ) is 0.5 , all components in a given row of @xmath37 are equal and positive , and each component of @xmath71 is an independent gaussian random variable of mean 0 and variance 1 .",
    "we generated 30 random `` connectivity '' matrices ( or systems ) @xmath109 , @xmath110 , each with an average of 2 non - zero components per row .",
    "for each @xmath109 we obtained 10 sets of 3000 ( post equilibrium ) data points via eq .  .",
    "for each set , we computed the mvgc across each bipartition of the system corresponding to @xmath109 .",
    "we then calculated , for each bipartition , the standard deviation of the mvgc across the 10 data sets and ( excluding bipartitions with standard deviation less than 0.01 ) the corresponding coefficient of variation ( cov , standard deviation divided by mean ) .",
    "this procedure allowed us to obtain , for each @xmath109 , a maximum cov .",
    "figure [ fig : stability1](a ) shows that the maximum cov is generally very small and never large , confirming the stability of mvgc .    to compare the stability of mvgc with that of trvmvgc , for each @xmath109 and for each bipartition we divided the cov for mvgc by the cov for trvmvgc .",
    "figure [ fig : stability1](b ) shows the distribution of the average of this ratio across all bipartitions .",
    "the clustering of this distribution at @xmath1111 , with no outliers , confirms that mvgc and trvmvgc have similar stability properties , at least in the systems we have simulated .    to generalize these results we next used a genetic algorithm ( ga ) @xcite to see if we could find a network for which mvgc becomes unstable .",
    "the ga was initialized using a population composed of the 30 random systems @xmath109 described above .",
    "we ran the ga for 130 generations . in each generation , we computed the fitness of each system as the maximum cov of mvgc .",
    "systems were selected to proceed to subsequent generations using stochastic rank - based selection .",
    "mutations enabled the adding of new non - zero components to @xmath109 , the removal of existing non - zero components , or the swapping of components , followed by renormalization of each row to sum to 0.5 again ; two mutations were applied per system .",
    "after 130 generations ( sufficient for fitness to asymptote ) the average fitness ( i.e.  maximum cov ) in the population was @xmath1110.25 , and the maximum was 0.39 , which is still a low value . for the @xmath109 that gave this highest value , we compared the cov obtained using mvgc with that obtained using trvmvgc following the procedure described above .",
    "the average ratio ( across all bipartitions ) was @xmath1111.00 , ( maximum value 1.12 ) , indicating that mvgc and trvmvgc had similar stability properties even for systems optimized to be unstable with respect to mvgc .",
    "further , we examined some @xmath109 for which the sums of the rows differed ( i.e.  having heterogeneous afferent connectivity ) ; these systems had similar stability properties to those described above .",
    "finally , stability properties were unaffected when computations were based on 1000 ( rather than 3000 ) data points .    taken together",
    ", these simulation results confirm that mvgc is numerically stable , and is not appreciably different from trvmvgc in terms of stability properties .          in this section",
    "we review the spectral decomposition of g - causality @xcite . for simplicity",
    "we limit ourselves to the unconditional case , although the procedure may be readily extended to the conditional case ( as described in  refs .",
    "we assume multivariate predictor and predictee variables , and show that mvgc but not trvmvgc has a satisfactory spectral decomposition .",
    "consider the stationary mvar @xmath112 we may write this as @xmath113 where @xmath114 denotes the ( single time step ) lag operator , and @xmath115 eq .",
    "may be solved as @xmath116 where @xmath117 . transforming into the frequency domain via the discrete - time fourier",
    "transform @xmath118 yields @xmath119 ( replace @xmath114 by @xmath120 ) , so that @xmath121 where @xmath122 is the _ transfer matrix_. the ( power ) _ spectral density _ of @xmath3 is then given by @xmath123 from a standard result @xcite , since @xmath124 is a square matrix lag operator with the identity matrix as leading term , we have @xmath125 provided that all roots of the characteristic polynomial @xmath126 lie outside the unit circle , which is a necessary condition for the existence of the stationary process . from we",
    "may then derive the relation @xcite @xmath127    consider now the stationary mvar @xmath128 with coefficients matrix @xmath129 and residuals covariance matrix @xmath130 let us split the corresponding transfer matrix @xmath131 as @xmath132 and the spectral density as @xmath133 then @xmath134 is just the spectral density of @xmath3 , which from is given by @xmath135 the idea is that we wish to decompose this expression into a part reflecting the effect of @xmath3 itself and a part reflecting the causal influence of @xmath4 .",
    "the problem is that , due to the presence of the `` cross '' term , @xmath134 does not split cleanly into an @xmath3 and a @xmath4 part .",
    "geweke @xcite addresses this issue by introducing the transformation @xmath136 where @xmath137 note that this transformation leaves the g - causality @xmath138 invariant ( c.f .",
    "section [ sec : invariance ] ) and , for the transformed regression , we have @xmath139 ; that is , the residuals @xmath140 are uncorrelated .",
    "thus , assuming the transformation has been pre - applied , eq .",
    "becomes @xmath141 whereby the spectral density of @xmath3 splits into an `` intrinsic '' part and a `` causal '' part .",
    "the spectral g - causality of @xmath142 at frequency @xmath143 is now defined to be @xmath144 or , in terms of the _ untransformed _ variables , @xmath145 with @xmath134 as in and @xmath146 .",
    "geweke ( ref .",
    "@xcite , theorem 2 ) then establishes the fundamental motivating relationship between frequency and time domain g - causality : @xmath147 provided that all roots of @xmath148 lie outside the unit circle .",
    ", the exact _ restricted _ regression of @xmath3 on its own past will generally require an infinite number of lags @xcite .",
    "thus in theory , for exact equality in , an infinite number of lags is required to calculate the term @xmath149 which appears in @xmath138 ( using a finite number of lags will generally result in an overestimate of @xmath138 , since residual errors will be larger than for the exact regression ) . as applied to empirical data ,",
    "it is in any case good practice to choose `` sufficient '' lags for all regressions so as to model the data adequately without overfitting @xcite.[fn : gcint ] ] the proof of this relation relies crucially on the result which , we note , involves the _ determinant _ of the transfer matrix . thus if the trace , rather than the determinant , were to be used in the definition for @xmath150 then we could not expect to obtain a relation corresponding to , since ( i ) the trace of the spectral density in eq",
    ".   does not factorize , ( ii ) there is no trace analogue to eq .  , and",
    "thus ( iii ) no analogue to eq .  .",
    "this would seem to preclude a satisfactory spectral decomposition for the trace version of g - causality .",
    "similar remarks apply to conditional g - causality in the spectral domain .    in ref .",
    "@xcite , however , it is conjectured that a trace analogue of eq .",
    "does indeed hold . to test this conjecture",
    "we performed the following experiment : we simulated @xmath151 mvar@xmath107 processes of the form @xmath152 where @xmath3 has dimension @xmath153 and @xmath1 dimension @xmath154 .",
    "residuals @xmath155 were completely uncorrelated , with unit variance (  @xmath156 was the @xmath157 identity matrix ) so that , in particular , the geweke transformation was unnecessary .",
    "for each trial the @xmath157 coefficients matrix @xmath37 was chosen at random with elements uniform on @xmath158 $ ] , and the process simulated for @xmath159 stationary time steps ( the occasional unstable process was rejected ) .",
    "time domain causalities @xmath160 and frequency domain causalities @xmath161 were calculated in sample using @xmath162 lags .",
    "( as noted previously,@xmath163 equality in is only assured in the limit of infinite lags ; @xmath164 lags was found empirically to achieve good accuracy without overfitting the data . ) relative errors of integrated spectral mvgc with respect to time - domain mvgc , expressed as a percentage , were defined as @xmath165 for mvgc and trvmvgc respectively .",
    "( the integrals were computed by standard numerical quadrature . )",
    "results , displayed in table  [ tab : speccomp ] , confirm to good accuracy the theoretical prediction of eq .   for mvgc",
    "( the small negative bias on @xmath166 is due to the finite number of lags ) , while for trvmvgc relative errors are several orders of magnitude larger and furthermore were not decreased by choosing longer stationary sequences and/or more lags .",
    ".comparison of relative errors of integrated spectral mvgc and trvmvgc with respect to time domain mvgc and trvmvgc , for a random sample of mvar@xmath107 processes .",
    "top row shows mvgc , bottom row shows trvmvgc .",
    "see text for details .",
    "figures in the `` abs .",
    "mean '' column are the means of the absolute values @xmath167 and @xmath168 . [ cols=\"^,^,^,^\",options=\"header \" , ]     the full distribution of relative errors is also displayed as a histogram in fig .  [",
    "fig : speccomp_hist ] .",
    "we also repeated the experiment with higher order mvar@xmath169 processes , higher dimensional predictee and predictor variables and correlated residuals @xmath170 . in all cases , results confirmed the accuracy of for mvgc and yielded large relative errors for trvmvgc .",
    "we remark that _ qualitative _ differences (  aside from differences of scale ) between spectral mvgc and trvmvgc could be substantial ( fig .  [",
    "fig : speccomp_single ] ) . these differences ,",
    "furthermore , appeared in general to be exaggerated by the presence of residual correlations ; this is consonant with the sensitivity of mvgc as contrasted with the lack of sensitivity of trvmvgc to residual correlations ( see sections [ sec : expand ] and [ sec : partial ] ) .",
    "it is straightforward to show that @xmath150 is invariant under the same group of linear transformations as @xmath138 ; again , @xmath171 will in general be invariant only under the restricted group with @xmath84 conformal ; this extends to the conditional case .",
    "recently , a _ partial g - causality _ measure has been introduced @xcite which exploits a parallel with the concept of _ partial coherence _ @xcite in order to control for latent / exogenous influences on standard g- causality .",
    "partial g - causality modifies the standard g - causality measure by including terms based on residual correlations between the predictee variable and the conditional variables .",
    "consider , in addition to the regressions , the following regressions of the _ conditioning _ variable @xmath172 : @xmath173 here the roles of the predictee and conditioning variables are reversed .",
    "then for univariate predictor and predictee the partial g - causality of @xmath1 on @xmath0 given @xmath5 is defined by conditioning the respective residual covariances for the regressions of @xmath0 on the corresponding residuals for the regressions of @xmath5 : @xmath174 this extends naturally to the fully multivariate case ( c.f .",
    "and we define partial mvgc ( pmvgc ) as @xmath175 where the rhs follows from the identity derived in appendix [ apx : pgc ] , ( with @xmath176 and @xmath177 for the numerator and denominator terms respectively ) . comparing with we",
    "see thus that pmvgc differs from mvgc in the inclusion of the _ present _ of the conditioning variable @xmath5 in the respective regressions .",
    "seen in this form , it is clear that , as is the case for mvgc , pmvgc is always non - negative .",
    "one could alternatively express pmvgc as ( non - partial ) mvgc conditioned on a `` forward lagged '' version of @xmath5 : defining @xmath178 we have @xmath179 , or @xmath180 ( note the additional lag on @xmath181 ) , so that , from eq .  , @xmath182    as noted in section [ sec : expand ] , ( non - partial ) mvgc to some extent already controls for the influence of latent / exogenous variables because the generalized variance is sensitive to residual correlations .",
    "however , pmvgc takes into account even more correlations with the explicit aim of controlling for latent / exogenous influences .",
    "pmvgc may therefore be preferable when such influences are expected to be ( a ) strong and ( b ) relatively uniform in their influence on the measured system .",
    "indeed , pmvgc ( and the original measure of partial g - causality ) can only be effective in compensating for latent / exogenous variables that affect _ all _ modeled variables (  predictee , predictor and conditioning ) to a roughly equal degree @xcite .",
    "it is interesting to note that pmvgc may be expressed in terms of non - partial mvgcs as @xmath183 by straightforward application of eq .  .",
    "as expected , includes a term with a mandatory multivariate predictee , since it is only in this case that residual correlation can make a difference .",
    "it is interesting that @xmath5 appears as a _ predictee _ variable ; this might be understood as pmvgc using the conditioning variable @xmath5 as a `` proxy '' by which to assess the influence of latent or exogenous variables .",
    "a `` trace '' version of pmvgc may be defined analogously to .",
    "again by eq .   of appendix [ apx :",
    "pgc ] , the identity corresponding to will hold , as will the trace analogue of . however , the analogue of will _ not _ hold in general , since the traces of the partial covariance matrices will in general not factorize appropriately .    from it is straightforward to derive a spectral decomposition @xmath184 for pmvgc , which will integrate correctly to the time - domain pmvgc @xmath185 .",
    "again , a spectral decomposition for the corresponding trace version is likely to be problematic , insofar as it will fail in general to integrate correctly to the time - domain value ( c.f .",
    "section [ sec : sdecomp ] ) .",
    "a straightforward application of mvgc is to measures of _ causal density _ , the overall level of causal interactivity sustained by a multivariate system @xmath3 . a previous measure of causal density",
    "@xcite has been defined as the average of all pairwise ( and hence univariate ) g - causalities between system elements , conditioned on the remaining system elements : bounded alternative can be defined as the fraction of all pairwise conditional causalities that are statistically significant at a given significance level . ]",
    "@xmath186 } } } } \\label{cd1}\\ ] ] where @xmath187}$ ] denotes the subsystem of @xmath3 with variables @xmath96 and @xmath99 omitted , and @xmath188 is the total number of variables .",
    "causal density provides a useful measure of the dynamical `` complexity '' of a system inasmuch as elements that are completely independent will have zero causal density , as will elements that are completely integrated in their dynamics . exemplifying standard intuitions about complexity @xcite",
    ", high causal density will only be achieved when elements behave somewhat differently from each other , in order to contribute novel potential predictive information , and at the same time are globally integrated , so that the potential predictive information is in fact useful @xcite .",
    "using mvgc , various extensions to can be suggested , based on the various possible interactions between multivariate predictors , predictees and conditional variables .",
    "these extensions may provide a more principled measure of complexity by analyzing a target system at multiple scales .",
    "first we define the causal density from size @xmath189 to size @xmath44 , @xmath190 , as the average mvgc from a subset of size @xmath189 to a subset of size @xmath44 , conditioned on the rest of the system : @xmath191 where @xmath192 denotes the @xmath193 of the @xmath194 distinct tripartitions of @xmath3 into disjoint sub - systems of respective sizes @xmath189 , @xmath44 and @xmath195 . then using this",
    ", one could define the _ bipartition causal density _ ( bcd ) as the average of @xmath196 over predictor size @xmath189 , @xmath197 interestingly , this quantity is closely related to the popular tononi - sporns - edelman `` neural complexity '' measure @xcite which averages ( contemporaneous ) _ mutual information _ across bipartitions ; ( we are currently exploring this in work in preparation ) .",
    "it could also be interesting to compare causal density at different scales of predictor plus predictee size ; thus we define @xmath198 then the original causal density measure of eq .",
    "is just @xmath199 and bcd is @xmath200 .",
    "the average of this over all scales can be used to define a complete _ tripartition causal density _ ( tcd ) : @xmath201 a comparison of the properties of all versions of causal density , as well as related complexity measures , is in progress .",
    "we remark that it is straightforward to define spectral versions of these causal density measures .",
    "g - causality has recently been adapted to provide an operational measure of `` autonomy '' in complex systems @xcite .",
    "a variable @xmath3 can be said to be `` g - autonomous '' with respect to a ( multivariate ) set of external variables @xmath5 if its own past states help predict its future states over and above predictions based on @xmath5 .",
    "this definition rests on the intuition of autonomy as `` self determination '' or `` self causation '' . we can formalize this notion along the lines of mvgc as follows .",
    "consider the regressions @xmath202 which differ from eqs .",
    "primarily because the predictee variable @xmath3 is _ not _ regressed on itself in one of the equations .",
    "the g - autonomy of @xmath3 is then given by @xmath203 the extension of g - autonomy to the multivariate case is important because it accommodates situations in which groups of elements may be jointly autonomous ( self - determining , self - causing ) , even though the activity of individual elements within the group may be adequately predicted by combinations of activities of other elements in the group .",
    "univariate formulations of g - autonomy @xcite would fail in these cases . consider as a trivial example an element @xmath105 which is g - autonomous with respect to a background @xmath5 .",
    "if @xmath105 is now duplicated by the element @xmath106 it will no longer appear as g - autonomous within the multivariate system @xmath204",
    ". however , the multivariate variable @xmath205 will be ( jointly ) g - autonomous with respect to @xmath5 .    as discussed in @xcite g - autonomy",
    "also provides the basis for a notion of `` g - emergence '' as applied to the relation between _ macroscopic _ variables `` emerging '' from the activity of _ microscopic _ constituents .",
    "g - emergence operationalizes the intuition that a macro - level variable is emergent to the extent that it is simultaneously _",
    "autonomous from _ and _ dependent upon _ its micro - level constituents @xcite .",
    "extension of g - emergence to the multivariate case using mvgc is straightforward , allowing consideration of multivariate micro- and macro - variables .",
    "given the ability to assess multivariate causal interactions , a second challenge arises : the identification of relevant groupings of variables into multivariate ensembles .",
    "one approach to this challenge adopts the perspective of statistical mechanics on the emergence of novel macroscopic variables , given a microscopic description of a system @xcite . here , we suggest that mvgc may furnish a useful method for macro - variable identification in this context .",
    "let us assume that @xmath172 represents a set of microscopic variables defining a complex ( possibly stochastic ) dynamical system , and @xmath206 a set of macroscopic variables functionally ( possibly deterministically ) dependent on the microscopic variables .",
    "there is then a sense in which @xmath3 represents a `` parsimonious '' high - level description of the system , to the extent that it predicts its own dynamical evolution without recourse to the low level of description of the system represented by @xmath5 ; that is , to the extent that @xmath3 exhibits strong _ causal independence _ with respect to @xmath5 . in this view",
    ", @xmath207 furnishes a natural measure of the _ lack _ of this causal independence , which might then be used to _ identify _ parsimonious macroscopic variables by minimizing @xmath208 over candidate functions @xmath70 .",
    "the multivariate formulation mvgc would appear to be significant in this context for reasons similar to the g - autonomy case .",
    "specifically , it may be that a set of macroscopic variables @xmath3 may _ jointly _ have high causal independence with respect to the microscopic variables @xmath5 , while the component variables @xmath96 may individually have lower causal independence .    the notions of g - autonomy , g - emergence , and causal independence are distinct but related . in short , g - autonomy measures",
    "`` self - causation '' , causal independence measures the _ absence _ of useful predictive information between microscopic and macroscopic descriptions of a system , and g - emergence measures a combination of macro - level autonomy and micro - to - macro causal _",
    "dependence_. it is possible , and is left as an objective of future work , that all three measures could be applied usefully to systems that avail multiple levels of descriptions , ( i ) to identify relevant groupings of observables at each level , ( ii ) to decompose causal interactions within each level , and finally ( iii ) to quantitatively characterize inter - level relationships .",
    "we have described and motivated a measure of multivariate causal interaction that is a natural extension of the standard g - causality measure .",
    "the measure , originally introduced by geweke @xcite but almost totally overlooked since , uses the generalized variance ( the determinant of the residual covariance matrix ) and we have termed it _",
    "multivariate g - causality _",
    "it contrasts with another recent proposal @xcite for addressing the same problem which uses instead the total variance ( the trace of the residual covariance matrix ) . in this paper , we have presented several theoretical justifications , augmented by numerical modeling , for preferring mvgc over the trace version , which we summarize below .",
    "we have also extended mvgc to address novel challenges in the analysis of complex dynamical systems , including quantitative characterization of `` causal density '' , `` autonomy '' , and the identification of novel macroscopic variables via causal independence .      in many analyses of complex systems , particularly in neuroscience and biology , there may be no simple or principled relationship between observed variables and explanatorily relevant collections , or ensembles , of these variables . in the introduction we already remarked on fmri , where explanatorily relevant rois are each composed of multiple observables ( voxels ) which are arbitrarily demarcated with respect to underlying neural mechanisms .",
    "other non - invasive neuroimaging methods share similar varieties of arbitrariness : both electroencephalography ( eeg ) and magnetoencephalography ( meg ) provide signals which are complex convolutions of underlying neural sources . in these and similar cases , multivariate causal analysis , and mvgc in particular , can be used to aggregate univariate observables into meaningful multivariate ( ensemble ) variables .",
    "it bears emphasizing that mvgc is fundamentally different from conditional g - causality @xcite , which assesses the causal connectivity between two univariate variables , conditioned on a set of other variables .",
    "even when it is possible to measure directly the activity of variables of interest , it is still important to consider multivariate interactions . continuing with the neuroscience example , it may be that multiple rois act _ jointly _ to influence other rois , or cognitive and/or behavioral outputs .",
    "in single cell recordings this point is even more pressing : since hebb @xcite it has been increasingly appreciated that neurons act as ensembles , rather than singly , in the adaptive function of the brain @xcite .",
    "mvgc is well suited to disclosing causal relationships among these ensembles as a window onto underlying principles of brain operation .",
    "of course , the application of mvgc is not limited to neuroscience .",
    "multivariate interactions are likely to be important in a very broad range of application areas . for example",
    ", genetic , metabolic , and transcriptional regulatory networks may be usefully decomposed into multivariate ensembles influencing other such ensembles @xcite .",
    "indeed , multivariate interactions may be important in any system , natural or artificial , which can be described in terms of multiple simultaneously acquired time series .",
    "a different approach to multivariate causal analysis was recently proposed by ladroue and colleagues @xcite .",
    "this involved a measure ( which we call trvmvgc ) based on the trace of the residual covariance matrix ( the total variance ) , rather than the determinant ( the generalized variance ) .",
    "geweke @xcite provided the original justifications for the determinant form , but did not explicitly discuss the trace form .",
    "as noted in section 3 of ref .",
    "@xcite , geweke s motivations included ( i ) mvgc is invariant under ( linear ) transformations of variables , and ( ii ) the maximum likelihood estimator of mvgc is asymptotically @xmath53-distributed for large samples ; ( there is no standard test statistic for trvmvgc ) . in this paper we have substantially enhanced this list , in each case comparing mvgc explicitly with trvmvgc . in summary : ( iii ) mvgc is fully equivalent to transfer entropy under gaussian assumptions , whereas for trvmvgc this equivalence only holds for the univariate case ; ( iv ) mvgc is invariant under _ all _ ( non - singular ) linear transformations of the predictee variable , while trvmvgc is invariant only under conformal linear transformations ( see below ) ; ( v ) only mvgc is expandable as a sum of univariate g - causalities ; ( vi ) mvgc but not trvmvgc admits a satisfactory spectral decomposition , inasmuch as it guarantees a consistent relationship with the corresponding time - domain formulation ; ( vii ) only mvgc depends on residual correlations , and through these accommodates in a natural way the influence of exogenous or latent variables , and ( viii ) the partial version of mvgc , pmvgc is decomposable in terms of non - partial mvgcs , but this is not true in general for trvmvgc .",
    "all the above factors suggest that mvgc should be preferred to trvmvgc . taken individually",
    "they may differ in their significance but taken together they emphasize that mvgc , but not trvmvgc , provides a comprehensive and _ theoretically consistent _ extension of standard g - causality to the multivariate case . while this consistency is the most important reason to prefer mvgc to trvmvgc ,",
    "let us consider further three of the individual properties .",
    "first , the equivalence with transfer entropy is important because it justifies the use of linear modeling for multivariate causal analysis , at least where gaussian assumptions are reasonable .",
    "second , the broader range of invariance is important because it means that mvgc is robust to a wider range of common inaccuracies during data collection , in particular those in which univariate variables are contaminated by contributions from other variables and in which different components of multivariate ensembles are differently scaled by measurement constraints .",
    "it is likely that this additional robustness will have significant practical importance in many experimental applications , for example in eeg and meg where individual sensors detect signals from multiple neural sources and may differentially amplify these sources according to their distance from the sensors and their alignment with the cortical surface .",
    "finally , the lack of a satisfactory spectral version of trvmvgc , which we establish both theoretically and numerically ( section [ sec : sdecomp ] and figures [ fig : speccomp_hist ] and [ fig : speccomp_single ] ) , implies that frequency - domain results obtained using trvmvgc are unreliable , both in their magnitude and in their spectral profile .",
    "ladroue _ et al . _",
    "@xcite note geweke s form ( i.e.  mvgc ) and suggest trvmvgc is preferable in view of possible numerical instabilities attending the computation of determinants for high - dimensional data .",
    "however the existence of an expansion of mvgc in terms of univariate g - causalities seems to counter this claim , since the univariate causalities would not be expected to be unstable . numerical simulations ( section [ sec : stability ] and figure [ fig : stability1 ] ) confirm our view .      in the second part of the paper we used mvgc to derive several novel measures that have the potential to shed substantial new light on complex system dynamics .",
    "first , mvgc leads immediately to a series of redefinitions of our previous `` causal density '' measure @xcite , which aims to capture the dynamical complexity of a system s dynamics in terms of coexisting integration and differentiation .",
    "extension to the multivariate case allows causal density to be evaluated at multiple levels of description thus furnishing a more principled measure of dynamical complexity .",
    "causal density has been suggested as a measure of neural dynamics that captures certain aspects of consciousness @xcite .",
    "it has been shown to increase in response to perceived stimuli as compared to non - perceived stimuli in a visual masking task @xcite , and it captures the complex dynamics of small - world networks more effectively than does a prominent competing measure , neural complexity @xcite .",
    "multivariate causal density has the potential to further strengthen and generalize these contributions .",
    "second , mvgc can be used to generalize the concept of g - autonomy , which operationalizes the notion of autonomy as `` self causation '' @xcite .",
    "multivariate g - autonomy is a significant enhancement because it deals with the case in which a group of variables may be jointly autonomous even though , individually , no variable is autonomous .",
    "our results therefore pave the way to informative application of this measure to complex systems .",
    "third , mvgc can be helpful in considering relations between microscopic and macroscopic levels of description of a system .",
    "one approach is to consider how _ causally independent _ a macroscopic variable is , with respect to its set of constituent micro - variables . we have suggested that this notion can be used to identify parsimonious macro - variables by maximizing causal independence over a space of functions relating micro- and macro - variables . alternatively , the concept of g - emergence operationalizes the idea that an emergent macro - variable is both _",
    "autonomous from _ and _ causally dependent _ on its underlying micro - level constituents . unlike the `` causal independence ''",
    "view , g - emergence may be better suited to characterizing the degree of emergence as opposed to identifying prospective macro - variables ; g - emergence also explicitly measures micro - to - macro causal dependence rather than assuming that it is present .    finally , the concepts of redundancy and synergy amongst variables have been recently introduced , via the use of a variant of the trvmvgc measure @xcite . these quantities aim at detecting functionally relevant partitions of a system by grouping variables according to their summed causal influences . because of the advantages of mvgc over trvmvgc , we suggest it may be useful to redefine redundancy and synergy in terms of mvgc .",
    "models of complex systems typically contain large numbers of variables .",
    "having a measure for directed interactions between groups of variables , as opposed to just single variables , provides a useful tool for the analysis of such systems .",
    "we have demonstrated that mvgc is such a measure , and we have provided a series of justifications , theoretical and numerical , to prefer it over a related measure , trvmvgc .",
    "like all measures of directed interaction based on g - causality , mvgc can be measured for freely collected data , without perturbing or providing inputs to the system .",
    "finally , in contrast to alternative approaches such as structural equation modeling @xcite or dynamic causal modeling @xcite , mvgc can be applied with very little prior knowledge of the system under consideration .",
    "aks is supported by epsrc leadership fellowship ep / g007543/1 , which also supports the work of abb .",
    "support is also gratefully acknowledged from the dr .",
    "mortimer and theresa sackler foundation .",
    "we wish to show that minimizing the determinant @xmath59 , where @xmath209 as specified in , leads to the same values for the regression coefficients @xmath37 .",
    "we thus solve for @xmath37 in the simultaneous equations @xmath210 where @xmath211 runs from @xmath212 , @xmath213 from @xmath214 and @xmath215 is given by @xmath216 we use the formula for an invertible square matrix @xmath217 @xmath218 assuming @xmath215 invertible and setting @xmath219 we have @xmath220 after gathering terms and simplifying , and eq .   follows .",
    "here we prove eq .  .",
    "we consider the case of there being no conditional third variable , since the extension to this case is trivial .",
    "we first expand in terms of predictor variables according to @xmath221 to expand in terms of predictees we use the expansion @xmath222 which follows from repeated application of eq .  .",
    "we obtain @xmath223 and similar for the other components of the sum in eq .  , from which the result follows .",
    "given the regressions @xmath224 where the regression coefficients @xmath225 are derived from an ordinary least squares , yule - walker or equivalent procedure , we show that @xmath226 assuming that all ( partial ) covariance matrices which appear below are invertible .",
    "we have @xmath227 thus we may calculate that @xmath228 using the block matrix inversion formula for @xmath229 , we may also calculate that @xmath230 now expanding the @xmath231 term in , we find using that is equivalent to @xmath232 or , rearranging and factorizing , @xmath233 now the term in square brackets on the rhs of simplifies to @xmath234 so that , factoring out @xmath235 , is equivalent to @xmath236 we now show that the term in the square brackets in is zero ;  that @xmath237 thus proving . rearranging and factoring out @xmath238 , becomes @xmath239 or , multiplying through on the right by @xmath240 , @xmath241 expanding @xmath240 , factorizing and rearranging again , we get @xmath242 or , since the term in square brackets on the lhs is just @xmath243 , @xmath244 we now show that , again , the term in square brackets is zero ;  that @xmath245 multiplying through on the left by @xmath246 , is equivalent to @xmath247 which follows immediately on expanding @xmath246 and @xmath240 , thus establishing",
    ".      m.  ding , y.  chen and s.  bressler .",
    "granger causality : basic theory and application to neuroscience . in s.",
    "schelter , m.  winterhalder , and j.  timmer , editors , _ handbook of time series analysis _ , 438460 .",
    "wiley , wienheim , 2006 ."
  ],
  "abstract_text": [
    "<S> granger causality analysis is a popular method for inference on directed interactions in complex systems of many variables . </S>",
    "<S> a shortcoming of the standard framework for granger causality is that it only allows for examination of interactions between single ( univariate ) variables within a system , perhaps conditioned on other variables . </S>",
    "<S> however , interactions do not necessarily take place between single variables , but may occur among groups , or `` ensembles '' , of variables . in this study </S>",
    "<S> we establish a principled framework for granger causality in the context of causal interactions among two or more multivariate sets of variables . </S>",
    "<S> building on geweke s seminal 1982 work , we offer new justifications for one particular form of multivariate granger causality based on the generalized variances of residual errors . taken together , our results support a comprehensive and theoretically consistent extension of granger causality to the multivariate case . </S>",
    "<S> treated individually , they highlight several specific advantages of the generalized variance measure , which we illustrate using applications in neuroscience as an example . </S>",
    "<S> we further show how the measure can be used to define `` partial '' granger causality in the multivariate context and we also motivate reformulations of `` causal density '' and `` granger autonomy '' . </S>",
    "<S> our results are directly applicable to experimental data and promise to reveal new types of functional relations in complex systems , neural and otherwise . + </S>",
    "<S> pacs numbers : 87.19.l- , 87.10.mn , 89.75.-k , 87.19.lj + keywords : granger causality , causal inference , multivariate statistics , generalized variance </S>"
  ]
}