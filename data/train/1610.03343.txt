{
  "article_text": [
    "molecular dynamics ( md ) simulation is one of the most important numerical tools in investigating various physical properties of materials . many applications using md simulation demand high performance computing . in the past decade , the computational power of general - purpose graphics processing units ( gpus ) has been exploited to accelerate many md simulations .",
    "not only existing md codes and libraries , such as amber @xcite , gromacs @xcite , lammps @xcite , namd @xcite , and openmm @xcite have been benefited from utilizing gpus as accelerators , but also new codes , such as hoomd - blue @xcite , halmd @xcite , and rumd @xcite , have been built from the ground up to achieve high performance using one or more gpus .",
    "most of the previous relevant works have only considered pairwise potentials , or a special many - body potential , namely , the embedded atom method @xcite , which are relatively simple to implement on gpus .",
    "gpu - acceleration of many - body potentials such as the tersoff @xcite and stillinger - weber @xcite potentials , which play an important role in modelling various materials , is more challenging and has only attracted some attention recently @xcite . taking three - body interaction as an example",
    ", a naive implementation of the force evaluation function , as usually done in a serial cpu code , requires accumulating the forces on three different atoms within a single thread . in a gpu kernel with many threads , each atom is usually associated with one thread and the force accumulation for an atom from the thread it belongs to will conflict with the force accumulation for the same atom from another thread .",
    "this causes a problem called write conflict where two threads try to write data simultaneously into the same global memory @xcite . one way to avoid write",
    "conflict is to use atomic operations , which are usually quite slow and can also introduce randomness in the computation , which is undesirable for debugging .",
    "there have been some proposals to avoid using atomic operations .",
    "@xcite proposed an algorithm for implementing the tersoff potential on a gpu , which has achieved impressive performance , but requires using a special fixed neighbour list and is thus not quite flexible .",
    "brown and yamada @xcite proposed a flexible gpu - implementation of the stillinger - weber potential within lammps .",
    "atomic operations are avoided , but redundant calculations and other overheads are introduced .",
    "a similar proposal was given by knizhnik _",
    "recently , hhnerbach _ et al . _",
    "@xcite developed a vectorization scheme to achieve performance portability across various parallel computing platforms for the tersoff potential within lammps .    in this work ,",
    "we propose a new algorithm of force evaluation for many - body potentials and present details of its gpu - implementation and performance .",
    "the new force evaluation algorithm is based on an explicit pairwise force expression for many - body potentials derived recently @xcite . in this approach",
    ", the force , virial stress , and heat current for a given atom are well defined and can be accumulated within a single thread .",
    "therefore , write conflict is absent by construction and no redundant calculations are introduced . to be specific ,",
    "we discuss the algorithm explicitly in terms of the tersoff potential , but performance evaluation is made for both the tersoff potential and the stillinger - weber potential .",
    "the implementation is done based on a previously work @xcite , and the resulting code , which we call gpumd ( graphics processing units molecular dynamics ) , will be made public soon . using silicon crystal as a test system , we measure the performance of gpumd and compare it with lammps .",
    "although the method to be introduced is applicable to any many - body potential , it is beneficial to start with an explicit example , which is taken as the widely used tersoff potential .",
    "generalizations to other many - body potentials will be discussed later .",
    "the total potential energy for a system of @xmath2 atoms described by the tersoff potential can be written as @xcite @xmath3 where @xmath4 @xmath5 @xmath6 @xmath7 here , @xmath8 , @xmath9 , @xmath10 , @xmath11 , and @xmath12 are material - specific parameters and @xmath13 is the angle formed by @xmath14 and @xmath15 , which implies that @xmath16 our convention is that @xmath17 represents the position difference pointing from atom @xmath18 to atom @xmath19 .",
    "the magnitude of @xmath14 is denoted as @xmath20 .    as in many empirical potentials ,",
    "the energy @xmath21 consists of a repulsive part @xmath22 and an attractive part @xmath23 .",
    "the many - body nature of the tersoff potential is embodied in the bond order function @xmath24 appearing in the attractive part , the value of which depends not only on @xmath25 and @xmath26 , but also on the positions of other atoms near atom @xmath18 .    the function @xmath27 is a cutoff function , which is only nonzero when @xmath20 is less than a cutoff distance .",
    "therefore , a verlet neighbour list can be used to speed up the force evaluation . for uniform cutoff ,",
    "the standard cell list method is very efficient , although more sophisticated methods perform better for systems with large size disparities @xcite .      due to the three - body nature of the tersoff potential , the conventional method for evaluating the interatomic forces",
    "is significantly different from that in the case of a simple two - body potential .",
    "algorithm [ algorithm : conventional_algorithm ] presents a pseudo code for the conventional method as implemented in most existing md codes such as lammps @xcite .",
    "the following symbols are used :    * @xmath2 : number of atoms * @xmath28 : potential energy of atom @xmath18 * @xmath29 : total force on atom @xmath18 * @xmath30 : per - atom virial stress of atom @xmath18 * nn@xmath31 : number of neighbour atoms of atom @xmath18 * nl@xmath32 : index of the @xmath33th neighbour atom of atom @xmath18 * @xmath34 : per - atom heat current    initialise @xmath28 , @xmath29 , and @xmath30 to zero @xmath35 @xmath36 @xmath37 @xmath38 @xmath39 @xmath40 continue @xmath41 @xmath42 @xmath43 @xmath44 @xmath45 @xmath46 @xmath47    in algorithm [ algorithm : conventional_algorithm ] , the potential energy @xmath48 is accumulated in line 7 , the two - body parts of the force and per - atom virial stress are accumulated in lines 8 - 9 and 10 , respectively , and the many - body parts of the force and per - atom virial stress are accumulated in lines 16 - 18 and 19 - 21 , respectively .",
    "last , in line 26 , the per - atom heat current is calculated from the per - atom virial stress and velocity .",
    "the forces defined in the pseudo code can be explicitly written as @xmath49 @xmath50 @xmath51 @xmath52    @xmath53    here , only the repulsive part of @xmath54 [ the first term on the right hand side of eq .",
    "( [ equation : f_ij_i ] ) ] is pairwise ; all the other force expressions are not .",
    "newton s third law could be exploited to reduce calculations regarding the pairwise part , but this would not result in a noticeable improvement of the overall performance , because the pairwise part only takes up a tiny fraction of the whole force evaluation .",
    "therefore , newton s third law has not been used in algorithm [ algorithm : conventional_algorithm ] . regardless of using newton s third law for the pairwise part or",
    "not , the above conventional method can be straightforwardly implemented on the cpu .",
    "newton s third law has also not been used on the gpu implementation @xcite of two - body potentials due to the problem of concurrent writes ( write conflict ) to the same location in global memory .",
    "when two or more threads in the same warp write to the same location in global memory , only one thread performs the write and it is not defined which thread does it @xcite . to understand why this feature of cuda restricts the use of newton s third law on the gpu",
    ", we recall that in the commonly used force - evaluation kernel @xcite for two - body potentials , which we call the thread - scheme @xcite , one thread is devoted to the calculation of the total force on one atom . using newton s",
    "third law would require accumulating the total force on atom @xmath18 by two threads .",
    "if these two threads are in the same warp , the total force on atom @xmath18 would not be correctly accumulated .",
    "one may try to use atomic operations to solve this problem , but this would hardly result in a gain of performance .    in view of the above discussion , one would immediately realise the difficulty of implementing algorithm [ algorithm : conventional_algorithm ] on the gpu : the partial accumulations of the forces and stresses on atoms @xmath19 and @xmath55 by the thread associated with atom @xmath18 ( cf .",
    "lines 9 , 17 - 18 , 20 - 21 in algorithm [ algorithm : conventional_algorithm ] ) would conflict with those by the threads associated with atoms @xmath19 and @xmath55 . therefore , algorithm [ algorithm : conventional_algorithm ] is not suitable for gpu - implementation .",
    "this difficulty has also been realised previously and some strategies are proposed to circumvent it @xcite .",
    "below , we present a different approach than these .      despite the many - body nature of the tersoff potential ,",
    "a pairwise force expression that complies with newton s third law has been derived recently @xcite : @xmath56 @xmath57 which can be simplified in terms of the per - atom potential to be @xmath58 an explicit expression for @xmath59 can be found in ref .",
    "@xcite .",
    "one may wonder whether the new pairwise force expression @xmath60 produces per - atom forces @xmath29 that are equivalent to those obtained by the conventional method .",
    "the answer should be definitely yes ; otherwise , either the conventional or our new method of force evaluation is wrong . to show this equivalence ,",
    "let us note that according to algorithm [ algorithm : conventional_algorithm ] , the total force acting on atom @xmath18 can be written as : @xmath61 plugging in eqs .",
    "( [ equation : f_ij_i])-([equation : f_ijk_k ] ) , and changing the absolute positions @xmath25 to relative ones @xmath14 using the chain rule , we get @xmath62 in this equation , the sum of the 1st and 4th lines is @xmath63 , the sum of the 2nd and 5th lines is @xmath64 , the 3rd line , with the dummy indices @xmath19 and @xmath55 interchanged , is @xmath65 , and the 6th line is @xmath66",
    ". therefore , the conventional expression eq .",
    "( [ equation : f_i_lammps ] ) is equivalent to eqs .",
    "( [ equation : f_i_new ] ) and ( [ equation : f_ij_new ] ) .",
    "while there is no difference between the force calculations based on the conventional and the new methods , the same can not be said for some other quantities such as the virial stress and heat current .",
    "the total virial stress tensor is defined as @xmath67 in md simulations with periodic boundary conditions in one or more directions , the absolute positions cause problems and in the case of two - body potentials , one can change them to relative positions using newton s third law .",
    "the resulting virial stress tensor takes a simple form : @xmath68 here , @xmath69 is the pairwise force acting on atom @xmath18 by atom @xmath19",
    ". one can also decompose the total virial stress into per - atom ones : @xmath70 @xmath71 since pairwise forces @xmath60 also exist for the tersoff potential , the per - atom virial stress tensor for the tersoff potential takes the same simple form as in the case of two - body potentials : @xmath72 however , the existence of a pairwise force expression has not been widely recognised and the virial stress tensor in standard md packages such as lammps is not implemented in this way . referring to algorithm [ algorithm : conventional_algorithm ] , the per - atom virial stress tensor as implemented in lammps takes a rather complicated form : @xmath73 this expression is not likely equivalent to eq .",
    "( [ equation : w_i_tersoff ] ) .",
    "for example , the first line in this equation suggests that the force component @xmath54 has been taken to be pairwise , which is not true because @xmath74 .",
    "a related quantity is the potential part of the heat current @xmath75 . for two - body potentials ,",
    "it can be written as @xmath76 since @xmath77 , we can also express @xmath75 in terms of the per - atom virial stress tensor : @xmath78 this is the heat current expression implemented in lammps .",
    "however , as pointed out in ref .",
    "@xcite , this stress - based formula does not apply to many - body potentials .",
    "the correct potential part of the heat current formula for many - body potentials reads @xmath79 similar to the case of virial stress , one can also decompose the total heat current into per - atom ones : @xmath80 @xmath81    @xmath82 is the block index @xmath83 is the thread index @xmath84 is the block size @xmath85 initialize @xmath28 , @xmath29 , @xmath30 , and @xmath34 to zero read in @xmath25 from global memory read in @xmath86 from global memory @xmath35 read in @xmath26 from global memory and calculate @xmath14 @xmath87 minimum image of @xmath14 calculate @xmath21 , @xmath88 and @xmath89 @xmath90 @xmath91 @xmath92 @xmath93 save the per - atom quantities @xmath28 , @xmath29 , @xmath30 , and @xmath34 to global memory    from the above discussion , we see that all the relevant quantities have a simple per - atom expression .",
    "this is exactly what one needs for an efficient gpu - implementation : the per - atom quantities ( @xmath28 , @xmath29 , @xmath94 , and @xmath34 ) can be accumulated solely by the thread associated with atom @xmath18 and no write conflict would occur . in algorithm [",
    "algorithm : new ] , we present a pseudo code for the force evaluation kernel on the gpu .",
    "this algorithm is much simpler than algorithm [ algorithm : conventional_algorithm ] and can be straightforwardly implemented using cuda or opencl .",
    "the above algorithm for the tersoff potential also applies to other many - body potentials . in ref .",
    "@xcite , it has been shown that for any many - body potential , the force , virial stress tensor , and heat current have the following per - atom forms : @xmath95 @xmath96 @xmath97 thanks to these per - atom expressions , one can construct a force evaluation cuda kernel for any many - body potential without worrying about write conflicts .",
    "we have implemented the force evaluation kernels for the tersoff and stillinger - weber potentials into our gpumd code using cuda , and the code has already been used to study various problems in large - scale ( up to a few million atoms ) graphene - based materials @xcite .",
    "systematic applications on thermal transport calculations have been presented in ref .",
    "@xcite . here",
    ", we measure the performance of gpumd and compare it with that of lammps , which is the standard production code for simulations with many - body potentials .",
    "figure [ figure : speed ] shows the scaling of the computational speed of gpumd with respect to the number of atoms for the tersoff and stillinger - weber potentials , with either double- or single - precision , using a tesla k40 gpu .",
    "the test system is silicon crystal of cubic domain with the number of atoms varying from @xmath98 to @xmath99 . for each domain size ,",
    "we run a simulation in the @xmath100 ensemble ( 300 k and 0 pa ) for 1000 steps and record the computation time @xmath101 ( in unit of second ) .",
    "the computational speed is then calculated as @xmath102 , which has a unit of atom @xmath1 step / second .",
    "for all the cases , the speed increases quickly with increasing @xmath2 and almost saturates when @xmath2 exceeds @xmath103 .",
    "the single - precision version for each potential is about 3 - 4 times as fast as the double - precision version .",
    "this is partly due to the faster single - precision floating point arithmetic , and partly due to the fact that the double - precision version uses more registers in the force - evaluation kernel and has lower gpu occupancy .",
    "detailed profiling shows that the occupancy is about 50% for the single - precision version and about 25% for the double - precision version . despite these relatively low occupancies , the computational speeds shown in fig .",
    "[ figure : speed ] are impressive .",
    "for example , the single - precision version of the stillinger - weber potential can achieve a speed of 25 ns per day ( with a time step of 2 fs ) for a system with @xmath104 atoms .            to better appreciate the high performance achieved by gpumd",
    ", we compare its performance against that obtained by lammps running on a single cpu core , which is intel xeon cpu x5670 @ 2.93 ghz .",
    "the speedup factors for gpumd running on a tesla k40 gpu are presented in fig .",
    "[ figure : speedup ] .",
    "for relatively large systems , the speedup factors range from a few tens to a few hundred , depending on the types of potential and floating point arithmetic .    to give a more realistic comparison between the performance of gpumd and the cpu version of lammps , we consider a system of 512 000 atoms and run the lammps code with varying number ( from 1 to 256 ) of cpu cores of the same specification as above using mpi parallelism .",
    "it can be seen from fig .",
    "[ figure : gpu_vs_many_cpus ] that the mpi version of lammps scales quite well up to about 100 cpu cores , but starts to scale less ideally afterwards . taking the tersoff potential as an example , the computational speeds for the double- and single - precision versions of gpumd are equivalent to those of lammps with about 100 and 500 cpu cores , respectively .",
    "finally , fig .",
    "[ figure : gpu_vs_gpu ] also gives a comparison between the performance of gpumd and that of the gpu version of lammps using the tersoff potential .",
    "for both double- and single - precision , gpumd is more than one order of magnitude faster .",
    "the optimized version by hhnerbach _",
    "@xcite is a few times faster than lammps , but is still a few times slower than gpumd .",
    "in summary , we analysed the difficulty in implementing many - body potentials in md simulations on graphics processing units and presented an efficient algorithm based on an explicit pairwise force expression for many - body potentials . in this algorithm ,",
    "the virial stress tensor and the heat current also have well - defined per - atom expressions .",
    "therefore , the force , virial stress , and heat current for a given atom can be accumulated within in a single thread and the algorithm is free of write conflict by construction .",
    "this crucial property allows for a simple , flexible , and efficient implementation of any many - body potential on the gpu .",
    "we have implemented the algorithm for the tersoff and stillinger - weber potentials in our gpumd code , which has excellent performance .",
    "gpumd running on a single tesla k40 gpu can be as fast as lammps running with tens to hundreds of cpu cores .",
    "our code is available upon request and will be made public later .",
    "this work was supported by the academy of finland through its centres of excellence programme ( 2015 - 2017 ) under project number 284621 and national natural science foundation of china under grants no .",
    "11404033 and 11504384 .",
    "we acknowledge the computational resources provided by aalto science - it project , finland s it center for science ( csc ) , and china scientific computing grid ( scgrid ) .",
    "a. w. gtz , m. j. williamson , d. xu , d. poole , s. le grand , r. c. walker , routine microsecond molecular dynamics simulations with amber on gpus .",
    "generalized born , j. chem .",
    "theory comput . 8",
    "( 2012 ) 1542 - 1555 .",
    "w. m. brown , a. kohlmeyer , s. j. plimpton , a. n. tharrington , implementing molecular dynamics on hybrid high performance computers - particle - particle particle - mesh .",
    "( 2012 ) 449 - 459 .",
    "j. glaser , t. d. nguyen , j. a. anderson , p. lui , f. spiga , j. a. millan , d. c. morse , s. c. glotzer , strong scaling of general - purpose molecular dynamics simulations on gpus , comput .",
    "( 2015 ) 97 - 107 .",
    "i. v. morozov , a. m. kazennov , r. g. bystryi , g. e. norman , v. v. pisarev , v. v. stegailov , molecular dynamics simulations of the relaxation processes in the condensed matter on gpus , comput .",
    "( 2011 ) 1974 - 1978 .",
    "w. m. brown , t. d. nguyen , m. fuentes - cabrera , j. d. fowlkes , p. d. rack , m. berger , a. s. bland , an evaluation of molecular dynamics performance on the hybrid cray xk6 supercomputer , procedia computer science 9 ( 2012 ) 186 - 195 .",
    "q. hou , m. li , y. zhou , j. cui , z. cui , j. wang , molecular dynamics simulations with many - body potentials on multiple gpus - the implementation , package and performance , comput .",
    "( 2013 ) 2091 - 2101 .                  z. fan , l. f. c. pereira , h .- q .",
    "wang , j .- c .",
    "zheng , d. donadio , a. harju , force and heat current formulas for many - body potentials in molecular dynamics simulation with applications to thermal conductivity calculations , phys .",
    "b 92 ( 2015 ) 094301 .",
    "m. p. howard , j. a. anderson , a. nikoubashman , s. c. glotzer , a. z. panagiotopoulos , efficient neighbour list calculation for molecular simulation of colloidal systems using graphics processing units , comput .",
    "( 2016 ) 45 - 52 .",
    "p. hirvonen , m. m. ervasti , z. fan , m. jalalvand , m. seymour , s. m. v. allaei , n. provatas , a. harju , k. r. elder , t. ala - nissila , multiscale modeling of polycrystalline graphene : a comparison of structure and defect energies of realistic samples from phase field crystal models , phys .",
    "b 94 ( 2016 ) 035414 ."
  ],
  "abstract_text": [
    "<S> graphics processing units have been extensively used to accelerate classical molecular dynamics simulations . </S>",
    "<S> however , there is much less progress on the acceleration of force evaluations for many - body potentials compared to pairwise ones . in the conventional force evaluation algorithm for many - body potentials , the force , virial stress , and heat current for a given atom </S>",
    "<S> are accumulated within different loops , which could result in write conflict between different threads in a cuda kernel . in this work , </S>",
    "<S> we provide a new force evaluation algorithm , which is based on an explicit pairwise force expression for many - body potentials derived recently [ phys . </S>",
    "<S> rev . </S>",
    "<S> b 92 ( 2015 ) 094301 ] . in our algorithm , </S>",
    "<S> the force , virial stress , and heat current for a given atom can be accumulated within a single thread and is free of write conflicts . </S>",
    "<S> we discuss the formulations and algorithms and evaluate their performance . </S>",
    "<S> a new open - source code , gpumd , is developed based on the proposed formulations . </S>",
    "<S> for the tersoff many - body potential , the double precision performance of gpumd using a tesla k40 card is equivalent to that of the lammps ( large - scale atomic / molecular massively parallel simulator ) molecular dynamics code running with about 100 cpu cores ( intel xeon cpu x5670 @ 2.93 ghz ) and the single - precision performance of gpumd reaches @xmath0 atom @xmath1 step / second .    molecular dynamics simulation , many - body potential , tersoff potential , stillinger - weber potential , graphics processing units , virial stress , heat current </S>"
  ]
}