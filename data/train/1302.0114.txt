{
  "article_text": [
    "in time series analysis , stationarity requires that dependence structure be sustained over time , and thus we can borrow information from one time period to study model dynamics over another period ; see fan and yao  @xcite for nonparametric treatments and lahiri  @xcite for various resampling and block bootstrap methods . in practice , however , many climatic , economic and financial time series are non - stationary and therefore challenging to analyze .",
    "first , since dependence structure varies over time , information is more localized .",
    "second , non - stationary processes often require extra parameters to account for time - varying structure .",
    "one way to overcome these issues is to impose certain local stationarity ; see , for example , dahlhaus  @xcite and adak  @xcite for spectral representation frameworks and dahlhaus and polonik  @xcite for a time domain approach .    in this article",
    "we study a class of modulated stationary processes ( see adak  @xcite ) @xmath0 where @xmath1 are stationary time series with zero mean , and @xmath2 are unknown constants adjusting for time - dependent variances . then",
    "@xmath3 oscillates around the constant mean @xmath4 , whereas its variance changes over time in an unknown manner . in the special case of @xmath5 , ( [ eq : xinons ] ) reduces to stationary case .",
    "if @xmath6 for a lipschitz continuous function @xmath7 on @xmath8 $ ] , then ( [ eq : xinons ] ) is locally stationary . for the general non - stationary case ( [ eq : xinons ] ) , the number of unknown parameters is larger than the number of observations , and it is infeasible to estimate @xmath9 . due to non - stationarity and the large number of unknown parameters , existing methods that are developed for ( locally ) stationary processes are not applicable , and our main purpose is to develop new statistical inference techniques.=1    first , we establish a uniform strong approximation result which can be used to derive a self - normalized central limit theorem ( clt ) for the sample mean @xmath10 of ( [ eq : xinons ] ) . for stationary case @xmath5 , by fan and yao  @xcite , under mild mixing conditions , @xmath11 for the modulated stationary case ( [ eq : xinons ] ) , it is non - trivial whether @xmath12 has a clt without imposing further assumptions on @xmath9 and the dependence structure of @xmath1 . moreover , even when the latter clt exists , it is difficult to estimate the limiting variance due to the large number of unknown parameters ; see de jong and davidson  @xcite for related work assuming a near - epoch dependent mixing framework .",
    "zhao  @xcite studied confidence interval construction for @xmath4 in  ( [ eq : xinons ] ) by assuming a block - wise asymptotically equal cumulative variance assumption .",
    "the latter assumption is rather restrictive and essentially requires that block averages be asymptotically independent and identically distributed ( i.i.d . ) . in this article",
    ", we deal with the more general setting  ( [ eq : xinons ] ) . under a strong invariance principle assumption , we establish a self - normalized clt with the self - normalizing constant adjusting for time - dependent non - stationarity .",
    "the obtained clt is an extension of the classical clt for i.i.d .",
    "data or stationary time series to modulated stationary processes .",
    "furthermore , we extend the idea to linear combinations of means over different time periods , which allows us to address inference regarding mean levels over multiple time periods .",
    "second , we study the wild bootstrap for modulated stationary processes . since the seminal work of efron  @xcite , a great deal of research has been done on the bootstrap under various settings , ranging from bootstrapping for i.i.d .",
    "data in efron  @xcite , wild bootstrapping for independent observations with possibly non - constant variances in wu  @xcite and liu  @xcite , to various block bootstrapping and resampling methods for stationary time series in knsch  @xcite , politis and romano  @xcite , bhlmann  @xcite and the monograph lahiri  @xcite . with the established self - normalized clt",
    ", we propose a wild bootstrap procedure that is tailored to deal with modulated stationary processes : the dependence is removed through a scaling factor , and the non - constant variance structure of the original data is preserved in the wild bootstrap data - generating mechanism .",
    "our simulation study shows that the wild bootstrap method outperforms the widely used stationarity - based block bootstrap .",
    "third , we address change - point analysis .",
    "the change - point problem has been an active area of research ; see pettitt  @xcite for proportion changes in binary data , horvth  @xcite for mean and variance changes in gaussian observations , bai and perron  @xcite for coefficient changes in linear models , aue _ et al . _",
    "@xcite for coefficient changes in polynomial regression with uncorrelated errors , aue _ et al . _",
    "@xcite for mean change in time series with stationary errors , shao and zhang  @xcite for change - points for stationary time series and the monograph by csrg and horvth  @xcite for more discussion .",
    "most of these works deal with stationary and/or independent data .",
    "hansen  @xcite studied tests for constancy of parameters in linear regression models with non - stationary regressors and conditionally homoscedastic martingale difference errors . here",
    "we consider @xmath13 where @xmath14 is an unknown change point .",
    "the aforementioned works mainly focused on detecting changes in mean while the error variance is constant . on the other hand",
    ", researchers have also realized the importance of the variance / covariance structure in change point analysis .",
    "for example , incln and tiao  @xcite studied change in variance for independent data , and aue _ et al . _",
    "@xcite and berkes , gombay and horvth  @xcite considered change in covariance for time series data . to our knowledge , there has been almost no attempt to advance change point analysis under the non - constant variances framework in ( [ eq : null ] ) .",
    "andrews  @xcite studied change point problem under near - epoch dependence structure that allows for non - stationary processes , but his assumption  1(c ) on page 830 therein essentially implies that the process has constant variance . the popular cumulative sum ( cusum ) test is developed for stationary time series and does not take into account the time - dependent variances . using the self - normalization idea , we propose a self - normalized cusum test and a wild bootstrap method to obtain its critical value .",
    "our empirical studies show that the usual cusum tests tend to over - reject the null hypothesis in the presence of non - constant variances .",
    "by contrast , the self - normalized cusum test yields size close to the nominal level .",
    "fourth , we estimate the long - run variance @xmath15 in ( [ eq : fanyao ] ) .",
    "long - run variance plays an essential role in statistical inferences involving time series .",
    "most works in the literature deal with stationary processes through various block bootstrap and subsampling approaches ; see carlstein  @xcite , knsch  @xcite , politis and romano  @xcite , gtze and knsch  @xcite and the monograph lahiri  @xcite . de jong and davidson  @xcite established the consistency of kernel estimators of covariance matrices under a near epoch dependent mixing condition .",
    "recently , mller  @xcite studied robust long - run variance estimation for locally stationary process . for model ( [ eq : xinons ] ) , the error process @xmath16 is contaminated with unknown standard deviations @xmath17 , and we apply blockwise self - normalization to remove non - stationarity , resulting in asymptotically stationary blocks .",
    "fifth , the proposed methods can be extended to deal with the linear regression model @xmath18 where @xmath19 are deterministic covariates , and @xmath20 is the unknown column vector of parameters . for @xmath21 ,",
    "hansen  @xcite established the asymptotic normality of the least - squares estimate of the slope parameter under a fairly general framework of non - stationary errors . while hansen  @xcite assumed that the errors form a martingale difference array so that they are uncorrelated , the framework in ( [ eq : lr ] ) is more general in that it allows for correlations . on the other hand ,",
    "hansen  @xcite allowed the conditional volatilities to follow an autoregressive model , hence introducing stochastic volatilities .",
    "phillips , sun and jin  @xcite considered ( [ eq : lr ] ) for stationary errors , and their approach is not applicable here due to the unknown non - constant variances @xmath22 . in section",
    "[ sec : ext ] we consider self - normalized clt for the least - squares estimator of @xmath23 in ( [ eq : lr ] ) . in the polynomial regression case @xmath24 , aue _",
    "et al . _",
    "@xcite studied a likelihood - based test for constancy of @xmath23 in ( [ eq : lr ] ) for uncorrelated errors with constant variance .",
    "due to the presence of correlation and time - varying variances , it is more challenging to study the change point problem for ( [ eq : lr ] ) and this is beyond the scope of this article.=1    the rest of this article is organized as follows .",
    "we present theoretical results in section  [ sec : main ] .",
    "sections [ sec : simu][sec : app ] contain monte carlo studies and applications to two real data sets .",
    "for sequences @xmath25 and @xmath26 , write @xmath27 , @xmath28 and @xmath29 , respectively , if @xmath30 , @xmath31 and @xmath32 , for some constants @xmath33 . for @xmath34 and a random variable @xmath35 ,",
    "write @xmath36 if @xmath37 .      in ( [ eq : xinons ] ) , assume without loss of generality that @xmath38 and @xmath39 so that @xmath16 and @xmath40 are centered stationary processes . with the convention @xmath41 ,",
    "define @xmath42    [ assump:1 ] there exist standard brownian motions @xmath43 and @xmath44 such that @xmath45 where @xmath46 is the approximation error , @xmath15 and @xmath47 are the long - run variances of @xmath16 and @xmath40 , respectively .",
    "further assume @xmath48 to avoid the degenerate case @xmath49 .",
    "the uniform approximations in ( [ eq : sip ] ) are generally called strong invariance principle . the two brownian motions @xmath43 and @xmath44 may be defined on different probability spaces and hence are not jointly distributed , which is not an issue because our argument does not depend on their joint distribution . to see how to use ( [ eq : sip ] ) , under @xmath50 in ( [ eq : null ] ) , consider @xmath51 theorem  [ thm:0 ] below presents uniform approximations for @xmath52 and @xmath53 .",
    "define @xmath54 \\label{eq : omegan } \\sigma^2_j&=&\\sum^j_{i=1 } \\sigma^2_i    \\quad\\mbox{and}\\quad \\sigma^{*2}_j=\\biggl ( \\sum^j_{i=1}\\sigma^4_i \\biggr)^{1/2}.\\vspace*{-2pt}\\end{aligned}\\ ] ]    [ thm:0 ] let ( [ eq : sip ] ) hold .",
    "for any @xmath55 $ ] , the following uniform approximations hold : @xmath56 & & \\max_{cn \\le j\\le n } |\\underline{v}^2_j - \\sigma^2_j |= \\mathrm{o}_\\mathrm { p}\\{(r^2_n\\delta^2_n + \\sigma^2_n)/n + \\sigma^{*2}_n+r^*_n \\delta _ n\\}.\\label{eq : thm0b}\\vspace*{-2pt}\\end{aligned}\\ ] ]    theorem  [ thm:0 ] provides quite general results under ( [ eq : sip ] ) .",
    "we now discuss sufficient conditions for ( [ eq : sip ] ) .",
    "shao  @xcite obtained sufficient mixing conditions for ( [ eq : sip ] ) . in this article , we briefly introduce the framework in wu  @xcite .",
    "assume that @xmath1 has the causal representation @xmath57 , where @xmath58 are i.i.d .",
    "innovations , and @xmath59 is a measurable function such that @xmath1 is well defined .",
    "let @xmath60 be an independent copy of @xmath61 .",
    "assume @xmath62 proposition  [ pro:1 ] below follows from corollary 4 in wu  @xcite .",
    "[ pro:1 ] assume that ( [ eq : pro1con ] ) holds .",
    "then ( [ eq : sip ] ) holds with @xmath63 , the optimal rate up to a logarithm factor .    for linear process",
    "@xmath64 with @xmath65 and @xmath66 , @xmath67 . if @xmath68 , then ( [ eq : sip ] ) holds with @xmath63 . for many nonlinear time series",
    ", @xmath69 decays exponentially fast and hence ( [ eq : pro1con ] ) holds ; see section 3.1 of wu  @xcite . from now on we assume ( [ eq : sip ] ) holds with @xmath63 .",
    "[ rmk : moment ] if @xmath1 are i.i.d . with @xmath38 and @xmath70 for some @xmath71 , the celebrated `` hungarian embedding '' asserts that @xmath72 satisfies a strong invariance principle with the optimal rate @xmath73 .",
    "thus , it is necessary to have the moment assumption @xmath74 in proposition  [ pro:1 ] in order to ensure strong invariance principles for both @xmath75 and @xmath76 in ( [ eq : snsn ] ) with approximation rate @xmath77 . on the other hand",
    ", one can relax the moment assumption by loosening the approximation rate .",
    "for example , by corollary 4 in wu  @xcite , assume @xmath78 for some @xmath79 and @xmath80 , then ( [ eq : sip ] ) holds with @xmath81 .",
    "as shown in examples  [ exmp:3][exmp:4 ] below , @xmath82 and @xmath83 in ( [ eq : volwei ] ) often have tractable bounds .    [ exmp:3 ]",
    "if @xmath9 is non - decreasing in @xmath84 , then @xmath85 and @xmath86 .",
    "if @xmath9 is non - increasing in @xmath84 , then @xmath87 and @xmath88 .",
    "if @xmath9 are piecewise constants with finitely many pieces , then @xmath89 .    [ exmp:6 ]",
    "let @xmath90 for @xmath91 $ ] and a lipschitz continuous function @xmath92 . then @xmath93 .",
    "if @xmath94 , we obtain a locally stationary case with the time window @xmath95 $ ] ; if @xmath96 , we have the infinite time window @xmath97 as @xmath98 , which may be more reasonable for data with a long time horizon .",
    "[ exmp:4 ] if @xmath99 for a slowly varying function @xmath100 such that @xmath101 as @xmath102 for all @xmath103",
    ". then we can show @xmath104 or @xmath105 and @xmath106 or @xmath105 , depending on whether @xmath107 or @xmath108 .",
    "for the boundary case @xmath109 , assume @xmath110 uniformly , then @xmath111 .",
    "similarly , @xmath112 .      in this section we establish a self - normalized clt for the sample average @xmath10 . to understand how non - stationarity makes this problem difficult",
    ", elementary calculation shows @xmath113 where @xmath114 . in the stationary case @xmath115 , under condition @xmath116 , @xmath117 , the long - run variance in ( [ eq : fanyao ] ) . for non - constant variances , it is difficult to deal with @xmath118 directly , due to the large number of unknown parameters and complicated structure .",
    "see de jong and davidson  @xcite for a kernel estimator of @xmath118 under a near - epoch dependent mixing framework .    to attenuate the aforementioned issue",
    ", we apply the uniform approximations in theorem  [ thm:0 ] .",
    "assume that ( [ eq : thmtestcon ] ) below holds .",
    "note that the increments @xmath119 of standard brownian motions are i.i.d .",
    "standard normal random variables . by ( [ eq : thm0a ] ) , @xmath120 is equivalent to @xmath121 in distribution . by ( [ eq : thm0b ] ) ,",
    "@xmath122 in probability . by slutsky",
    "s theorem , we have proposition  [ cor:1 ] .",
    "[ cor:1 ] let ( [ eq : sip ] ) hold with @xmath123 . for @xmath124 in ( [ eq : volwei])([eq : omegan ] ) , assume @xmath125 recall @xmath126 in ( [ eq : fj ] ) .",
    "then as @xmath127 , @xmath128 . consequently , a @xmath129 asymptotic confidence interval for @xmath4 is @xmath130 , where @xmath131 is a consistent estimate of @xmath132 ( section  [ sec : lrv ] below ) , and @xmath133 is @xmath134 standard normal quantile .",
    "proposition  [ cor:1 ] is an extension of the classical clt for i.i.d .",
    "data or stationary processes to modulated stationary processes . if @xmath3 are i.i.d . , then @xmath135 . in proposition",
    "[ cor:1 ] , @xmath15 can be viewed as the variance inflation factor due to the dependence of @xmath16 . for stationary data ,",
    "the sample variance @xmath136 is a consistent estimate of the population variance . here , for non - constant variances case ( [ eq :",
    "xinons ] ) , by ( [ eq : thm0b ] ) in theorem  [ thm:0 ] , @xmath136 can be viewed as an estimate of the time - average `` population variance ''",
    "so , we can interpret the clt in proposition  [ cor:1 ] as a self - normalized clt for modulated stationary processes with the self - normalizing term @xmath138 , adjusting for non - stationarity due to @xmath139 and @xmath15 , accounting for dependence of @xmath16 .",
    "clearly , parameters @xmath139 are canceled out through self - normalization . finally , condition ( [ eq : thmtestcon ] ) is satisfied in example  [ exmp:6 ] with @xmath140 and example  [ exmp:4 ] with @xmath141 .    in classical statistics ,",
    "the width of confidence intervals usually shrinks as sample size increases . by proposition  [ cor:1 ] and theorem  [ thm:0 ] , the width of the constructed confidence interval for @xmath4 is proportional to @xmath142 or , equivalently , @xmath143 .",
    "thus , a necessary and sufficient condition for shrinking confidence interval is @xmath144 , which is satisfied if @xmath145 .",
    "an intuitive explanation is as follows .",
    "for i.i.d .",
    "data , sample mean converges at a rate of @xmath146 . in ( [ eq : xinons ] ) ,",
    "if @xmath9 grows faster than @xmath147 , the contribution of a new observation is negligible relative to its noise level .",
    "[ exmp : ci ] if @xmath148 with @xmath149 , the length of confidence interval is proportional to @xmath150 .",
    "in particular , if @xmath151 for some positive constants @xmath152 and @xmath153 , then @xmath143 achieves the optimal rate @xmath154 .",
    "if @xmath155 , then @xmath156 .",
    "the same idea can be extended to linear combinations of means over multiple time periods .",
    "suppose we have observations from @xmath157 consecutive time periods @xmath158 , each of the form  ( [ eq : xinons ] ) with different means , denoted by @xmath159 , and each having time - dependent variances .",
    "let @xmath160 for given coefficients @xmath161 .",
    "for example , if we are interested in mean change from @xmath162 to @xmath163 , we can take @xmath164 ; if we are interested in whether the increase from @xmath165 to @xmath166 is larger than that from @xmath162 to @xmath163 , we can let @xmath167 .",
    "proposition  [ thm : lcm ] below extends proposition  [ cor:1 ] to multiple means .",
    "[ thm : lcm ]",
    "let @xmath160 . for @xmath168 ,",
    "denote its sample size @xmath169 and its sample average @xmath170 .",
    "assume that ( [ eq : thmtestcon ] ) holds for each individual time period @xmath168 and , for simplicity , that @xmath171 are of the same order .",
    "then @xmath172 ^ 2 \\biggr\\}.\\ ] ]      recall @xmath173 in ( [ eq : xinons ] ) .",
    "suppose we are interested in the self - normalized statistic @xmath174 for problems with small sample sizes , it is natural to use bootstrap distribution instead of the convergence @xmath175 in proposition  [ cor:1 ] .",
    "wu  @xcite and liu  @xcite have pioneered the work on the wild bootstrap for independent data with non - identical distributions .",
    "we shall extend their wild bootstrap procedure to the modulated stationary process ( [ eq : xinons ] ) .",
    "let @xmath176 be i.i.d .",
    "random variables independent of @xmath16 satisfying @xmath177 .",
    "define the self - normalized statistic based on the following new data : @xmath178 clearly , @xmath179 inherits the non - stationarity structure of @xmath173 by writing @xmath180 with @xmath181 . on the other hand , for the new error process @xmath182 , @xmath183 and @xmath184 for @xmath185 .",
    "thus , @xmath182 is a white noise sequence with long - run variance one . by proposition [ cor:1 ] ,",
    "the scaled version @xmath186 is robust against the dependence structure of @xmath16 , so we expect that @xmath187 should be close to @xmath188 in distribution .",
    "[ thm : bootstrap ] let the conditions in proposition  [ cor:1 ] hold .",
    "further assume @xmath189 let @xmath131 be a consistent estimate of @xmath132 .",
    "denote by @xmath190 the conditional law given @xmath16 .",
    "then @xmath191    theorem  [ thm : bootstrap ] asserts that , @xmath192 behaves like the scaled version @xmath193 , with the scaling factor @xmath131 coming from the dependence of @xmath16 . here",
    "we use the sample mean @xmath10 in ( [ eq : xinons ] ) to illustrate a wild bootstrap procedure to obtain the distribution of @xmath194 in proposition  [ cor:1 ] .",
    "a.   apply the method in section  [ sec : lrv ] to @xmath195 to obtain a consistent estimate @xmath131 of @xmath132 .",
    "b.   subtract the sample mean @xmath10 from data to obtain @xmath196 .",
    "c.   generate i.i.d .",
    "random variables @xmath197 satisfying @xmath198 .",
    "d.   based on @xmath199 in ( ii ) and @xmath200 in ( iii ) , generate bootstrap data @xmath201 , and compute @xmath202 where @xmath203 is a long - run variance estimate ( see section  [ sec : lrv ] ) for bootstrap data @xmath204 .",
    "e.   repeat ( iii)(iv ) many times and use the empirical distribution of those realizations of @xmath205 as the distribution of @xmath206 .",
    "the proposed wild bootstrap is an extension of that in liu  @xcite for independent data to modulated stationary case , and it has two appealing features .",
    "first , the scaling factor @xmath131 makes the statistic independent of the dependence structure .",
    "second , the bootstrap data - generating mechanism is adaptive to unknown time - dependent variances @xmath207 . for the distribution of @xmath200 in step  ( iii ) ,",
    "we use @xmath208 , which has some desirable properties .",
    "for example , it preserves the magnitude and range of the data .",
    "as shown by davidson and flachaire  @xcite , for certain hypothesis testing problems in linear regression models with symmetrically distributed errors , the bootstrap distribution is exactly equal to that of the test statistic ; see theorem 1 therein .    for the purpose of comparison",
    ", we briefly introduce the widely used block bootstrap for a stationary time series @xmath209 with mean @xmath4 . by ( [ eq : fanyao ] ) ,",
    "suppose that we want to bootstrap the distribution of @xmath12 .",
    "let @xmath211 be defined as in section  [ sec : lrv ] below .",
    "the non - overlapping block bootstrap works in the following way :    a.   take a simple random sample of size @xmath212 with replacement from the blocks @xmath213 , and form the bootstrap data @xmath214 by pooling together @xmath3s for which the index @xmath84 is within those selected blocks .",
    "b.   let @xmath215 be the sample average of @xmath216 .",
    "compute @xmath217 , where @xmath218 is the conditional expectation of @xmath215 given @xmath209 . c.   repeat ( i)(ii ) many times and use the empirical distribution of @xmath219 s as the distribution of @xmath12 .    in step ( ii )",
    ", another choice is the studentized version @xmath220 , where @xmath203 is a consistent estimate of @xmath132 based on bootstrap data . assuming stationarity and @xmath221 , the blocks are asymptotically independent and share the same model dynamics as the whole data , which validates the above block bootstrap .",
    "we refer the reader to lahiri  @xcite for detailed discussions . for a non - stationary process , block bootstrap is no longer valid , because individual blocks are not representative of the whole data .",
    "by contrast , the proposed wild bootstrap is adaptive to unknown dependence and the non - constant variances structure .      to test a change point in the mean of a process @xmath209 , two popular cusum - type tests ( see section  3 of robbins _ et al . _",
    "@xcite for a review and related references ) are @xmath222 where @xmath223 is a consistent estimate of the long - run variance @xmath15 of @xmath209 , and @xmath224 here @xmath103 ( @xmath225 in our simulation studies ) is a small number to avoid the boundary issue . for i.i.d .",
    "data , @xmath226 is proportional to the variance of @xmath227 , so @xmath228 is a studentized version of @xmath229 . for i.i.d .",
    "gaussian data , @xmath228 is equivalent to likelihood ratio test ; see csrg and horvth  @xcite .",
    "assume that , under null hypothesis , @xmath230 \\biggr\\ } _",
    "{ 0\\le t\\le1 } \\rightarrow\\tau\\{b_t\\}_{0\\le t\\le1},\\qquad   \\mbox{in the skorohod space}\\ ] ] for a standard brownian motion @xmath231 .",
    "the above convergence requires finite - dimensional convergence and tightness ; see billingsley  @xcite . by the continuous mapping theorem , @xmath232 and @xmath233 .",
    "for the modulated stationary case ( [ eq : null ] ) , ( [ eq : cusuma ] ) is no longer valid .",
    "moreover , since @xmath228 and @xmath229 do not take into account the time - dependent variances @xmath22 , an abrupt change in variances may lead to a false rejection of @xmath50 when the mean remains constant .",
    "for example , our simulation study in section  [ sec : power ] shows that the empirical false rejection probability for @xmath228 and @xmath229 is about @xmath234 for nominal level @xmath235 . to alleviate the issue of non - constant variances",
    ", we adopt the self - normalization approach as in previous sections .",
    "recall @xmath52 and @xmath236 in ( [ eq : fj ] ) . for each fixed @xmath237 , by theorem [ thm:0 ] and slutsky s theorem , @xmath238 in distribution ,",
    "assuming the negligibility of the approximation errors .",
    "therefore , the self - normalization term @xmath236 can remove the time - dependent variances . in light of this",
    ", we can simultaneously self - normalize the two terms @xmath239 and @xmath240 in ( [ eq : sxj ] ) and propose the self - normalized test statistic @xmath241 here , @xmath242 is defined as in ( [ eq : fj ] ) , @xmath243 with @xmath244 .",
    "[ thm : test ] assume ( [ eq : sip ] ) holds .",
    "let @xmath245 be as in ( [ eq : thmtestcon ] ) . under @xmath50",
    ", we have @xmath246 where @xmath247    by theorem  [ thm : test ] , under @xmath50 , @xmath248 is asymptotically equivalent to @xmath249 . due to the self - normalization , for each @xmath250 , the time - dependent variances are removed and @xmath251 has a standard normal distribution .",
    "however , @xmath252 and @xmath253 are correlated for @xmath254 .",
    "therefore , @xmath255 is a non - stationary gaussian process with a standard normal marginal density . due to the large number of unknown parameters @xmath9",
    ", it is infeasible to obtain the null distribution directly . on the other hand ,",
    "theorem [ thm : test ] establishes the fact that , asymptotically , the distribution of @xmath248 in ( [ eq : tstar ] ) depends only on @xmath139 and is robust against the dependence structure of @xmath16 , which motivates us to use the wild bootstrap method in section  [ sec : wild ] to find the critical value of @xmath248 .",
    "a.   compute @xmath256 and find @xmath257 .",
    "b.   divide the data into two blocks @xmath258 and @xmath259 . within each block ,",
    "subtract the sample mean from the observations therein to obtain centered data .",
    "pool all centered data together and denote them by @xmath260 . c.   based on @xmath260 ,",
    "obtain an estimate @xmath131 of @xmath132 .",
    "see section [ sec : lrv ] below .",
    "d.   compute the test statistic @xmath248 in ( [ eq : tstar ] ) .",
    "e.   based on @xmath199 in ( ii ) , use the wild bootstrap method in section  [ sec : wild ] to generate synthetic data @xmath261 , and use ( i)(iv ) to compute the bootstrap test statistic @xmath262 based on the bootstrap data @xmath263 .",
    "f.   repeat ( v ) many times and find @xmath129 quantile of those @xmath262s .    as argued in section  [ sec : wild ] , the synthetic data - generating scheme ( v ) inherits the time - varying non - stationarity structure of the original data . also , the statistic @xmath248 is robust against the dependence structure , which justifies the proposed bootstrap method .",
    "if @xmath50 is rejected , the change point is then estimated by @xmath264 .",
    "if there is no evidence to reject @xmath50 , we briefly discuss how to apply the same methodology to test @xmath265 , that is , whether there is a change point in the variances @xmath22 . by ( [ eq : xinons ] )",
    ", we have @xmath266 , where @xmath267 has mean zero .",
    "therefore , testing a change point in the variances @xmath22 of @xmath3 is equivalent to testing a change point in the mean of the new data @xmath268 .      to apply the results in sections [ sec : cltx][sec : cusum ] , we need a consistent estimate of the long - run variance  @xmath15 .",
    "most existing works deal with stationary time series through various block bootstrap and subsampling approaches ; see lahiri  @xcite and references therein . assuming a near - epoch dependent mixing condition , de jong and davidson  @xcite established the consistency of a kernel estimator of @xmath269 , and their result can be used to estimate @xmath118 in ( [ eq : lrvnon ] ) for the clt of @xmath270 .",
    "however , for the change point problem in section  [ sec : cusum ] , we need an estimator of the long - run variance @xmath15 of the unobservable process @xmath16 , so the method in de jong and davidson  @xcite is not directly applicable .",
    "to attenuate the non - stationarity issue , we extend the idea in section  [ sec : cltx ] to blockwise self - normalization .",
    "let @xmath271 be the block length .",
    "denote by @xmath272 the largest integer not exceeding @xmath273 .",
    "ignore the boundary and divide @xmath274 into @xmath212 blocks @xmath275 recall the overall sample mean @xmath10 .",
    "for each block @xmath250 , define the self - normalized statistic @xmath276}{v(j ) } ,   \\qquad\\mbox{where } \\bar{x}(j)=\\frac{1}{k_n } \\sum_{i\\in\\mathcal{i}_j } x_i , v^2(j ) = \\sum_{i\\in\\mathcal{i}_j } [ x_i-\\bar{x}(j)]^2.\\ ] ] by proposition  [ cor:1 ] , the self - normalized statistics @xmath277 are asymptotically i.i.d .",
    "thus , we propose estimating @xmath15 by @xmath278 as in ( [ eq : volwei])([eq : omegan ] ) , we define the quantities on block @xmath250 @xmath279    [ thm:3 ] let ( [ eq : sip ] ) hold with @xmath123 .",
    "recall @xmath280 in ( [ eq : volwei])([eq : omegan ] ) .",
    "define @xmath281 assume that @xmath282 and @xmath283 then @xmath284 .",
    "consequently , @xmath131 is a consistent estimate of @xmath132 .",
    "consider example  [ exmp:6 ] with @xmath285 .",
    "then @xmath286 . for @xmath287",
    ", it can be shown that the optimal rate is @xmath288 when @xmath289 .",
    "in example [ exmp:4 ] with @xmath290 for some @xmath291 , elementary but tedious calculations show that the optimal rate is @xmath292,\\vspace*{5pt}\\cr n^{{(\\beta-1)}/{(5 - 4\\beta)}}\\{\\log(n)\\}^{{(8(1-\\beta))}/{(5 - 4\\beta ) } } , \\qquad k_n\\asymp n^{{(4.5 - 4\\beta)}/{(5 - 4\\beta)}}\\{\\log(n)\\}^{{4}/{(5 - 4\\beta ) } } , \\vspace*{2pt}\\cr \\quad \\beta\\in(3/4,1).}\\ ] ]      the self - normalization approaches in sections [ sec : cltx][sec : lrv ] can be extended to linear regression model ( [ eq : lr ] ) with modulated stationary time series errors .",
    "the approach in phillips , sun and jin  @xcite is not applicable here due to non - stationarity . for simplicity",
    ", we consider the simple case that @xmath293 and @xmath294 .",
    "hansen  @xcite studied a similar setting for martingale difference errors .",
    "denote by @xmath295 and @xmath296 the simple linear regression estimates of @xmath297 and @xmath298 given by @xmath299 then simple algebra shows that @xmath300 the latter expressions are linear combinations of @xmath16 .",
    "thus , by the same argument in proposition  [ cor:1 ] and theorem  [ thm:0 ] , we have self - normalized clts for @xmath295 and @xmath296 .",
    "[ thm : lr ] let @xmath301 and @xmath302 .",
    "assume that @xmath303 and @xmath304 satisfy condition ( [ eq : thmtestcon ] ) .",
    "then as @xmath127 , @xmath305   \\frac{n^2(\\hat\\beta_1-\\beta_1)}{6 v_{n,1 } } & \\rightarrow & n(0,\\tau^2 ) ,   \\qquad\\mbox{where }    v_{n,1}^2 = \\sum^n_{i=1 } ( 2i - n-1)^2 ( x_i-\\hat\\beta_0-\\hat\\beta_1 i / n)^2.\\end{aligned}\\ ] ]    the long - run variance @xmath15 can be estimated using the idea of blockwise self - normalization in section  [ sec : lrv ] .",
    "let @xmath306 and @xmath307 be defined as in section [ sec : lrv ] .",
    "then we propose @xmath308 here , @xmath309 are asymptotically i.i.d .",
    "normal random variables with mean zero and variance  @xmath15 .",
    "consistency can be established under similar conditions as in theorem  [ thm:3 ] .    for the general linear regression model ( [ eq : lr ] ) , the linearly weighted average structure of linear regression estimates allows us to obtain self - normalized clts as in theorem  [ thm : lr ] under more complicated conditions . also , it is possible to extend the proposed method to the nonparametric regression model with time - varying variances @xmath310 where @xmath311 is a nonparametric time trend of interest .",
    "nonparametric estimates , for example , the nadaraya ",
    "watson estimate , are usually based on locally weighted observations .",
    "the latter feature allows us to derive similar self - normalized  clt .",
    "however , the change point problem for  ( [ eq : lr ] ) and ( [ eq : npm ] ) will be more challenging , and aue _ et al . _",
    "@xcite studied  ( [ eq : lr ] ) for uncorrelated errors with constant variance . also , it is more difficult to address the bandwidth selection issues ; see altman  @xcite for related contribution when @xmath5 .",
    "it remains a direction of future research to investigate ( [ eq : lr ] ) and ( [ eq : npm ] ) .",
    "recall that @xmath309 in ( [ eq : lrvest ] ) are asymptotically i.i.d .",
    "normal random variables . to get a sensible choice of the block length parameter @xmath271",
    ", we propose a simulation - based method by minimizing the empirical mean squared error ( mse ) :    a.   simulate @xmath312 i.i.d .",
    "standard normal random variables @xmath313 .",
    "b.   based on @xmath313 , obtain @xmath131 with block length @xmath157 .",
    "c.   repeat ( i)(ii ) many times , compute empirical @xmath314 as the average of realizations of @xmath315 , and find the optimal @xmath157 by minimizing @xmath314 .",
    "we find that the optimal block length @xmath157 is about 12 for @xmath316 , about 15 for @xmath317 , about 20 for @xmath318 and about 25 for @xmath319 .",
    "let sample size @xmath316 .",
    "recall @xmath1 and @xmath9 in ( [ eq : xinons ] ) . for @xmath9 , consider four choices : @xmath320 where @xmath321 is the standard normal density , and @xmath322 is the indicator function .",
    "the sequences a1a4 exhibit different patterns , with a piecewise constancy for a1 , a cosine shape for a2 , a sharp change around time @xmath323 for a3 and a gradual downtrend for a4 .",
    "let @xmath58 be i.i.d .",
    "n(0 , 1 ) . for @xmath1 ,",
    "we consider both linear and nonlinear processes .",
    "@xmath324 for b1 , by wu  @xcite , ( [ eq : pro1con ] ) holds . by andel , netuka and svara  @xcite , @xmath325 and @xmath326 . to examine how the strength of dependence affects the performance , we consider @xmath327 , representing independence , intermediate and strong dependence , respectively . for b2 with @xmath328 , ( [ eq : sip ] ) holds with @xmath63 , and we consider three cases @xmath329 . to assess the effect of block length @xmath271 , three choices @xmath330 are used",
    "thus , we consider all 72 combinations of @xmath331 .    without loss of generality we examine coverage probabilities",
    "based on @xmath332 realized confidence intervals for @xmath333 in ( [ eq : xinons ] ) .",
    "we compare our self - normalization - based confidence intervals to some stationarity - based methods . for ( [ eq : xinons ] ) , if we pretend that the error process @xmath334 is stationary , then we can use ( [ eq : fanyao ] ) to construct an asymptotic confidence interval for @xmath4 . under stationarity",
    ", the long - run variance @xmath15 of @xmath335 can be similarly estimated through the block method in section  [ sec : lrv ] by using the non - normalized version @xmath336 $ ] in ( [ eq : lrvest ] ) ; see lahiri  @xcite .",
    "thus , we compare two self - normalization - based methods and three stationarity - based alternatives : self - normalization - based confidence intervals through the asymptotic theory in proposition [ cor:1 ] ( sn ) and the wild bootstrap ( wb ) in section [ sec : wild ] ; stationarity - based confidence intervals through the asymptotic theory ( [ eq : fanyao ] ) ( st ) , non - overlapping block bootstrap ( bb ) and studentized non - overlapping block bootstrap ( sbb ) in section  [ sec : wild ] . from the results in table  [ tab1 ]",
    ", we see that the coverage probabilities of the proposed self - normalization - based methods ( columns sn and wb ) are close to the nominal level @xmath337 for almost all cases considered .",
    "by contrast , the stationarity - based methods ( columns st , bb and sbb ) suffer from substantial undercoverage , especially when dependence is strong ( @xmath338 in table  [ tab1](a ) and @xmath339 in table  [ tab1](b ) ) .",
    "for the two self - normalization - based methods , wb slightly outperforms sn .",
    "@llld2.1d2.1lld2.1d2.1@ & & + & & + & & & & & & & & + a1&0.0 & 4.9 & 9.1 & 8.4 & 2.1 & 7.3 & 12.2 & 13.4 + & 0.4 & 4.7 & 9.4 & 9.6 & 3.0 & 4.7 & 8.6 & 9.2 + & 0.8 & 6.0 & 15.1 & 14.7 & 4.0 & 5.6 & 9.9 & 7.7 + a2&0.0 & 5.7 & 8.2 & 6.1 & 2.1 & 5.8 & 9.5 & 8.6 + & 0.4 & 6.1 & 8.9 & 6.8 & 3.0 & 5.3 & 9.6 & 6.8 + & 0.8 & 7.3 & 12.6 & 9.3 & 4.0 & 4.2 & 7.5 & 4.2 + a3&0.0 & 5.0 & 5.7 & 4.8 & 2.1 & 5.5 & 7.7 & 6.7 + & 0.4 & 5.3 & 6.9 & 5.4 & 3.0 & 5.8 & 6.1 & 4.9 + & 0.8 & 7.0 & 9.8 & 10.0 & 4.0 & 5.0 & 6.5 & 4.2 + a4&0.0 & 5.4 & 8.4 & 6.0 & 2.1 & 6.9 & 8.8 & 7.1 + & 0.4 & 5.7 & 7.9 & 5.2 & 3.0 & 4.8 & 6.6 & 6.3 + & 0.8 & 7.2 & 11.1 & 9.2 & 4.0 & 5.3 & 6.2 & 5.8 +      in ( [ eq : null ] ) , we use the same setting for @xmath9 and @xmath1 as in section  [ sec : ecp ] . for mean",
    "@xmath340 , we consider @xmath341 , and compare the test statistics @xmath342 in ( [ eq : cusum ] ) and @xmath248 in ( [ eq : tstar ] ) .",
    "first , we compare their size under the null with @xmath343 .",
    "the critical value of @xmath248 is obtained using the wild bootstrap in section [ sec : cusum ] ; for @xmath228 and @xmath229 , their critical values are based on the block bootstrap in section  [ sec : wild ] . in each case , we use @xmath332 bootstrap samples , nominal level @xmath235 , and block length @xmath344 , and summarize the empirical sizes ( under the null @xmath343 ) in table  [ tab:3 ] based on @xmath332 realizations . while @xmath248 has size close to @xmath235 , @xmath228 and @xmath229 tend to over - reject the null , and the false rejection probabilities can be three times the nominal level of @xmath235 .",
    "next , we compare the size - adjusted power .",
    "instead of using the bootstrap methods to obtain critical values , we use @xmath337 quantiles of @xmath345 realizations of the test statistics when data are simulated directly from the null model so that the empirical size is exactly @xmath235 . figure [ fig : power ]",
    "presents the power curves for combinations \\{a1a4 } @xmath346 \\{b1 with @xmath347 ; b2 with @xmath348 } with @xmath332 realizations each . for  a1",
    ", @xmath349 outperforms @xmath228 and @xmath229 ; for a2a4 , there is a moderate loss of power for @xmath350 .",
    "overall , @xmath349 has power comparable to other two tests . in practice , however , the null model is unknown , and when one turns to the bootstrap method to obtain the critical values , the usual cusum tests @xmath228 and @xmath229 will likely over - reject the null as shown in table  [ tab:3 ] . in summary , with such small sample size and complicated time - varying variances structure , @xmath248 along with the wild bootstrap method delivers reasonably good power and the size is close to nominal level .",
    "( dashed curve ) and @xmath229 ( dotdash curve ) in ( [ eq : cusum ] ) and @xmath248 ( solid curve ) in ( [ eq : tstar ] ) as functions of change size @xmath351 ( horizontal axis ) with sample size @xmath316 and block length @xmath344 . for ( a1 , b1)(a4 , b1 ) ,",
    "the error process @xmath16 is from b1 with @xmath347 ; for ( a1 , b2)(a4 , b2 ) , the error process @xmath16 is from b2 with @xmath348 . ]    finally , we point out that the proposed self - normalization - based methods are not robust to models with time - varying correlation structures . for example , consider the model @xmath352 for @xmath353 and @xmath354 for @xmath355 , where @xmath58 are i.i.d .",
    "n(0 , 1 ) . with @xmath344 , the sizes ( nominal level @xmath235 ) for the three tests @xmath248 , @xmath356 , @xmath229 are 0.154 , 0.196 , 0.223 for a1 .",
    "future research directions include ( i ) developing tests for change in the variance or covariance structure for ( [ eq : xinons ] ) ( see incln and tiao  @xcite , aue _ et al . _",
    "@xcite and berkes , gombay and horvth  @xcite for related contributions ) ; and ( ii ) developing methods that are robust to changes in correlations .",
    "the data set consists of annual mean precipitation rates in seoul during 17712000 ; see figure  [ fig : seoul ] for a plot .",
    "the mean levels seem to be different for the two time periods 17711880 and 18812000 .",
    "ha and ha  @xcite assumed the observations are i.i.d . under the null hypothesis .",
    "as shown in figure  [ fig : seoul ] , the variations change over time .",
    "also , the auto - correlation function plot ( not reported here ) indicates strong dependence up to lag 18 .",
    "therefore , it is more reasonable to apply our self - normalization - based test that is tailored to deal with modulated stationary processes . with sample size @xmath357 , by the method in section  [ sec : kn ] , the optimal block length is about 15 .",
    "based on @xmath358 bootstrap samples as described in section  [ sec : cusum ] , we obtain the corresponding p - values 0.016 , 0.005 , 0.045 , 0.007 , with block length @xmath359 , respectively . for all choices of @xmath271 ,",
    "there is compelling evidence that a change point occurred at year 1880 .",
    "while our result is consistent with that of ha and ha  @xcite , our modulated stationary time series framework seems to be more reasonable .",
    "denote by @xmath360 and @xmath361 the mean levels over pre - change and post - change time periods 17711880 and 18812000 .",
    "for the two sub - periods with sample sizes 110 and 120 , the optimal block length is about 12 . with @xmath362 , applying the wild bootstrap in section  [ sec : wild ] with @xmath358 bootstrap samples , we obtain @xmath337 confidence intervals @xmath363 $ ] for @xmath360 , @xmath364 $ ] for @xmath361 . for the difference @xmath365 , with optimal block length @xmath366 , the @xmath337 wild bootstrap confidence interval is @xmath367 $ ] .",
    "note that the latter confidence interval for @xmath365 does not cover zero , which provides further evidence for @xmath368 and the existence of a change point at year 1880 .",
    "the data set consists of quarterly u.s . gross national product ( gnp ) growth rates from the first quarter of 1947 to the third quarter of 2002 ; see section 3.8 in shumway and stoffer  @xcite for a stationary autoregressive model approach .",
    "however , the plot in figure  [ fig : gnp ] suggests a non - stationary pattern : the variation becomes smaller after year 1985 whereas the mean level remains constant .",
    "moreover , the stationarity test in kwiatkowski _",
    "et al . _",
    "@xcite provides fairly strong evidence for non - stationarity with a p - value of 0.088 . with the block length @xmath369",
    ", we obtain the corresponding p - values @xmath370 , and hence there is no evidence to reject the null hypothesis of a constant mean @xmath4 . based on @xmath366 ,",
    "the @xmath337 wild bootstrap confidence interval for @xmath4 is @xmath371 $ ] . to test whether there is a change point in the variance , by the discussion in the last paragraph of section  [ sec : cusum ] , we consider @xmath372 . with @xmath369 ,",
    "the corresponding p - values are @xmath373 , indicating strong evidence for a change point in the variance at year 1984 . in summary , we conclude that there is no change point in the mean level , but there is a change point in the variance at year 1984 .",
    "proof of theorem [ thm:0 ] let @xmath374 . by the triangle inequality , we have @xmath375 .",
    "recall @xmath75 in ( [ eq : sip ] ) . by the summation by parts formula , ( [ eq : thm0a ] ) follows via @xmath376 by kolmogorov s maximal inequality for independent random variables , for @xmath377 , @xmath378=\\delta^{-2}.\\quad\\ ] ] thus , by ( [ eq : tnsip ] ) , @xmath379 .",
    "observe that @xmath380 by ( [ eq : sip ] ) , the same argument in ( [ eq : tnsip ] ) and ( [ eq : maxin ] ) shows @xmath381 , uniformly .",
    "the desired result then follows via ( [ eq : p1a ] ) .",
    "proof of theorem [ thm : bootstrap ] denote by @xmath382 the standard normal distribution function . by proposition  [ cor:1 ] and slutsky s theorem , @xmath383 for each fixed @xmath384 . since @xmath382 is a continuous distribution , @xmath385 .",
    "it remains to prove @xmath386 , in probability .",
    "notice that , conditioning on @xmath16 , @xmath387 are independent random variables with zero mean . by the berry  essen bound in bentkus , bloznelis and gtze  @xcite",
    ", there exists a finite constant @xmath388 such that @xmath389 where @xmath390 denotes conditional expectations given @xmath16 .",
    "clearly , @xmath391 and @xmath392 .",
    "thus , under the assumption @xmath393 , we have @xmath394 .",
    "meanwhile , by the proof of theorem  [ thm:0 ] , @xmath395 .",
    "therefore , the desired result follows from ( [ eq : pfbta ] ) in view of ( [ eq : bootcon ] ) .",
    "proof of theorem [ thm : test ] for @xmath237 , @xmath396 . for @xmath227 in ( [ eq : sxj ] ) , by ( [ eq : thm0a ] ) , we have @xmath397 , where @xmath398 by ( [ eq : thm0b ] ) , @xmath399 , where @xmath400 for @xmath237 , @xmath401 .",
    "thus , condition ( [ eq : thmtestcon ] ) implies @xmath402 and @xmath403 .",
    "therefore , uniformly over @xmath404 , @xmath405 by ( [ eq : maxin ] ) , @xmath406 .",
    "thus , the result follows in view of @xmath407 .",
    "proof of theorem [ thm:3 ] condition @xmath408 implies @xmath409 . by ( [ eq : thm0b ] ) ,",
    "@xmath410 define @xmath411 .",
    "clearly , @xmath412 are independent standard normal random variables .",
    "thus , @xmath413 . by ( [ eq : thm0a ] ) ,",
    "recall the definition of @xmath415 in ( [ eq : dj ] ) . by the same argument in ( [ eq : thm0a ] ) , using @xmath416 as @xmath417 , we have @xmath418 \\{1+\\mathrm{o}(\\omega_j)\\ } + \\mathrm{o}_\\mathrm { p}\\biggl\\{\\frac{k_n\\sigma_n}{n\\sigma(j ) } \\biggr\\}\\\\ & = & \\tau u_j + \\mathrm{o}_\\mathrm { p}\\biggl\\ { \\sqrt{\\log(n ) } m_n + \\frac{\\sigma",
    "_ n}{\\ell_n \\sigma(j ) } \\biggr\\}.\\end{aligned}\\ ] ] by the latter expression and @xmath419 , we can easily verify @xmath420 .",
    "we are grateful to the associate editor and three anonymous referees for their insightful comments that have significantly improved this paper .",
    "we also thank amanda applegate for help on improving the presentation and kyung - ja ha for providing us the seoul precipitation data .",
    "zhao s research was partially supported by nida grant p50-da10075 - 15 .",
    "the content is solely the responsibility of the authors and does not necessarily represent the official views of the nida or the nih ."
  ],
  "abstract_text": [
    "<S> we study statistical inferences for a class of modulated stationary processes with time - dependent variances . due to non - stationarity and the large number of unknown parameters , existing methods for stationary , or locally stationary , time series </S>",
    "<S> are not applicable . based on a self - normalization technique , </S>",
    "<S> we address several inference problems , including a self - normalized central limit theorem , a self - normalized cumulative sum test for the change - point problem , a long - run variance estimation through blockwise self - normalization , and a self - normalization - based wild bootstrap . </S>",
    "<S> monte carlo simulation studies show that the proposed self - normalization - based methods outperform stationarity - based alternatives . </S>",
    "<S> we demonstrate the proposed methodology using two real data sets : annual mean precipitation rates in seoul from 17712000 , and quarterly u.s . </S>",
    "<S> gross national product growth rates from 19472002 . </S>"
  ]
}