{
  "article_text": [
    "deep neural networks have recently been applied to various tasks @xcite , including image processing @xcite , speech recognition @xcite , natural language processing @xcite , and bioinformatics @xcite .",
    "although they have simple layered structures of units and connections , they outperform other conventional models by their ability to learn complex nonlinear relationships between input and output data . in each layer ,",
    "inputs are transformed into more abstract representations under a given set of the model parameters .",
    "these parameters are automatically optimized through training so that they extract the important features of the input data . in other words",
    ", it does not require either careful feature engineering by hand , or expert knowledge of the data .",
    "this advantage has made deep neural networks successful in a wide range of tasks , as mentioned above .    however , the inference provided by a deep neural network consists of a large number of nonlinear and complex parameters , which makes it difficult for human beings to understand it .",
    "more complex relationships between input and output can be represented as the network becomes deeper or the number of units in each hidden layer increases , however interpretation becomes more difficult .",
    "the large number of parameters also causes problems in terms of computational time , memory and over - fitting , so it is important to reduce the parameters appropriately . since it is difficult to read the underlying structure of a neural network and to identify the parameters that are important to keep , we must perform experimental trials to find the appropriate values of the hyperparameters and the random initial parameters that achieve the best trained result .    in this paper , to overcome such difficulties , we propose a new method for extracting a global and simplified structure from a layered neural network ( for example , figure [ fig : exp1 m ] and [ fig : exp3 m ] ) . based on network analysis",
    ", the proposed method defines a modular representation of the original trained neural network by detecting communities or clusters of units with similar connection patterns .",
    "although the modular neural network proposed by @xcite has a similar name , it takes the opposite approach to ours .",
    "in fact , it constructs the model structure before training with multiple split neural networks inside it .",
    "then , each small neural network works as an expert of a subset task .",
    "our proposed method is based on the community detection algorithm . to date , various methods have been proposed to express the characteristics of diverse complex networks without layered structures @xcite , however , no method has been developed for detecting the community structures of trained layered neural networks .",
    "the difficulty of conventional community detection from a layered neural network arises from the fact that an assumption commonly used in almost all conventional methods does not hold for layered neural networks : to detect the community structure of network , previous approaches assume that there are more intra - community edges that connect vertices inside a community than inter - community edges that connect vertices in mutually different communities .",
    "a network with such a characteristic is called assortative .",
    "this seems to be a natural assumption , for instance , for a network of relationships between friends . in layered neural networks , however , units in the same layer do not connect to each other and they only connect via units in their parent or child layers .",
    "this characteristic is similar to that of a bipartite graph , and such networks are called disassortative .",
    "it is not appropriate to apply conventional methods based on the assumption of an assortative network to a layered neural network .",
    "a basic community detection method that can be applied to either assortative or disassortative networks has been proposed by newman et al @xcite . in this paper , we propose an extension of this method for extracting modular representations of layered neural networks .    the proposed method can be employed for various purposes . in this paper , we show its effectiveness with the following three applications .    1",
    ".   * decomposition of layered neural network into independent networks * : the proposed method decomposes a trained neural network into multiple small independent neural networks .",
    "in such a case , the output estimation by the original neural network can be regarded as a set of independent estimations made by the decomposed neural networks . in other words",
    ", it divides the problem and reduces the overall computation time . in section [ sec : decomposition ] , we show that our method can properly decompose a neural network into multiple independent networks , where the data consist of multiple independent vectors .",
    "* generalization error estimation from community structure * : modularity @xcite is defined as a measure of the validity of a community detection result .",
    "section [ sec : gee ] reveals that there is a correlation between modularity and the generalization error of a layered neural network .",
    "it is shown that the appropriateness of the trained result can be estimated from the community structure of the network .",
    "* knowledge discovery from modular representation * : the modular representation extracted by the proposed method serves as a clue for understanding the trained result of a layered neural network .",
    "it extracts the community structure in the input , hidden , and output layer . in section [ sec : kd ] , we introduce the result of applying the proposed method to practical data .",
    "the remaining part of this paper is composed as follows : we first describe a layered neural network model in section [ sec : lnn ] .",
    "then , we explain our proposed method for extracting a modular representation of a neural network in section [ sec : communitydetect ] .",
    "the experimental results are reported in section [ sec : experiment ] , which show the effectiveness of the proposed method in the above three applications . in section [ sec : discussion ] , we discuss the experimental results . section [ sec : conclusion ] concludes this paper .",
    "we start by defining @xmath0 and a probability density function @xmath1 on @xmath2 .",
    "a training data set @xmath3 with a sample size @xmath4 is assumed to be generated independently from @xmath1 .",
    "let @xmath5 be a function from @xmath6 to @xmath7 of a layered neural network that estimates an output @xmath8 from an input @xmath9 and a parameter @xmath10 .    for a layered neural network , @xmath11 , where @xmath12 is the weight of connection between the @xmath13-th unit in the depth @xmath14 layer and the @xmath15-th unit in the depth @xmath16 layer , and @xmath17 is the bias of the @xmath13-th unit in the depth @xmath14 layer .",
    "a layered neural network with @xmath18 layers is represented by the following function : @xmath19 where a sigmoid function is defined by @xmath20    the training error @xmath21 and the generalization error @xmath22 are respectively defined by @xmath23 where @xmath24 is the euclidean norm of @xmath7 .",
    "the generalization error is approximated by @xmath25 where @xmath26 is a test data set that is independent of the training data set .    to construct a sparse neural network",
    ", we adopt the lasso method @xcite in which the minimized function is defined by @xmath27 where @xmath28 is a hyperparameter .",
    "the parameters are trained by the stochastic steepest descent method , @xmath29 where @xmath30 is the training error computed only from the @xmath13-th sample @xmath31 . here , @xmath32 is defined for training time @xmath33 such that @xmath34 which is sufficient for convergence of the stochastic steepest descent .",
    "( [ eq : dw ] ) is numerically calculated by the following procedure , which is called error back propagation @xcite : for the @xmath18-th layer , @xmath35 for @xmath36 , @xmath37    algorithm [ alg_bp ] is used for training a layered neural network based on error back propagation . with this algorithm",
    ", we obtain a neural network whose redundant weight parameters are close to zero .",
    "randomly sample @xmath38 from uniform distribution on @xmath39 .",
    "@xmath40 , @xmath41 , where @xmath42 and @xmath43 is the @xmath15-th element of @xmath38-th sample .",
    "( 1 ) output calculation of all layers : let @xmath45 be an output of the @xmath15-th unit in the depth @xmath14 layer . @xmath46 .",
    "( 2 ) update weight @xmath12 and bias @xmath48 based on back propagation , where @xmath49 is a small constant . @xmath50 .",
    "here we propose a new community detection method , which is applied to any layered neural networks ( figure [ fig : com ] ( a ) ) .",
    "the proposed method is an extension of the basic approach proposed by newman et al @xcite .",
    "it detects communities of assortative or disassortative networks . the key idea behind our method is that the community assignment of the units in each layer is estimated by using connection with adjacent layers .",
    "as shown in figure [ fig : com ] ( b ) , a partial network consisting of the connections between every layer and its adjacent layers is represented in the form of two matrices : @xmath58 and @xmath59 .",
    "the matrix @xmath60 and @xmath61 represent the connections between two layers of depth @xmath62 and @xmath14 , and two layers of depth @xmath14 and @xmath16 , respectively .",
    "an element @xmath63 is defined as @xmath64 if the absolute value of the connection weight between the @xmath13-th unit in the depth @xmath62 layer and the @xmath15-th unit in the depth @xmath14 layer is larger than @xmath65 , otherwise @xmath66 , where @xmath65 is called a weight removing hyperparameter . in a similar way , @xmath67 is defined from the connection weight between the @xmath13-th unit in the depth @xmath14 layer and @xmath15-th unit in the depth @xmath16 layer . for simplicity",
    ", we denote @xmath60 and @xmath61 as @xmath68 and @xmath69 , respectively , in the following explanation .",
    "our method is based on the assumption that units in the same community have a similar probability of connection from / to other units .",
    "this assumption is almost the same as that in the previous method @xcite , except that our method utilizes both incoming and outgoing connections of each community , and it detects communities in individual layers .",
    "therefore , the community detection result is derived in a similar way to the previous method @xcite , as explained in the rest of this section .",
    "as shown on the right in figure [ fig : com ] ( b ) , the statistical model for community detection has three kinds of parameters .",
    "the first parameter @xmath70 represents the prior probability of a unit in the depth @xmath14 layer that belongs to the community @xmath71 .",
    "the conditional probability of connections for a given community @xmath71 is represented by the second and third parameters @xmath72 and @xmath73 , where @xmath74 represents the probability that a connection to a unit in the community @xmath71 is attached from the @xmath13-th unit in the depth @xmath62 layer .",
    "similarly , @xmath75 represents the probability that a connection from a unit in the community @xmath71 is attached to the @xmath15-th unit in the depth @xmath16 layer .",
    "these parameters are normalized so that they satisfy the following condition : @xmath76    our purpose is to find the parameters @xmath77 that maximize the likelihood of given matrices @xmath78 . to solve this problem",
    ", we introduce the community assignment @xmath79 , where @xmath80 is the community of the @xmath38-th unit in the depth @xmath14 layer .",
    "the parameters are optimized so that they maximize the likelihood of @xmath78 and @xmath81 :    @xmath82    where @xmath83 then , the log likelihood of @xmath78 and @xmath81 is given by @xmath84    here , the community assignment @xmath81 is a latent variable and is unknown in advance , so we can not directly calculate the above @xmath85 .",
    "therefore , we calculate the expected log likelihood @xmath86 over @xmath81 instead .",
    "@xmath87 where @xmath88 is the number of units in the depth @xmath14 layer . by defining @xmath89",
    "the above equation can be rewritten as follows : @xmath90    the parameter @xmath91 represents the probability that the @xmath38-th unit is assigned to the community @xmath71 . in other words ,",
    "the community detection result is given by the estimated @xmath92 .",
    "the optimal parameters for maximizing @xmath86 of eq .",
    "( [ eq : exploglh ] ) are found with the em algorithm .",
    "the parameters @xmath77 with given @xmath92 are recursively optimized .",
    "if @xmath93 maximizes @xmath86 , then they satisfy @xmath94 \\left [ \\prod_j { \\tau'_{c , j}}^{b_{k , j } } \\right]}{\\sum_s \\pi_s \\left [ \\prod_i { \\tau_{s , i}}^{a_{i , k } } \\right ] \\left [ \\prod_j { \\tau'_{s , j}}^{b_{k , j } } \\right ] } ,    \\label{eq : q}\\end{aligned}\\ ] ] and @xmath95    the denominator and numerator in the last term of eq .",
    "( [ eq : qdefine ] ) are given by @xmath96 \\left [ \\prod_j { \\tau'_{g_l , j}}^{b_{l , j } } \\right ] \\right\\}\\\\    & = & \\left\\ { \\pi_c \\left [ \\prod_i { \\tau_{c , i}}^{a_{i , k } } \\right ] \\left [ \\prod_j { \\tau'_{c , j}}^{b_{k , j } } \\right ] \\right\\ } \\left\\ { \\prod_{l\\neq k } \\sum_s \\pi_s \\left [ \\prod_i { \\tau_{s , i}}^{a_{i , l } } \\right ] \\left [ \\prod_j { \\tau'_{s , j}}^{b_{l , j } } \\right ] \\right\\},\\end{aligned}\\ ] ] and @xmath97 \\left [ \\prod_j { \\tau'_{s , j}}^{b_{k , j } } \\right],\\end{aligned}\\ ] ] where @xmath98 is the kronecker delta .",
    "therefore , @xmath91 is given by eq .",
    "( [ eq : q ] ) .",
    "the problem is to maximize @xmath86 of eq .",
    "( [ eq : exploglh ] ) with a given @xmath92 under the condition of eq .",
    "( [ eq : normalization ] ) .",
    "this is solved with the lagrangian undetermined multiplier method , which employs @xmath99 and @xmath100 from eq .",
    "( [ eq : lag1 ] ) , the following equations are derived : @xmath101 using eq .",
    "( [ eq : exploglh ] ) and eq .",
    "( [ eq : lag2 ] ) , we obtain @xmath102 from eq .",
    "( [ eq : lag3 ] ) and the condition of eq .",
    "( [ eq : normalization ] ) , lagrange s undetermined multipliers @xmath103 are determined , and eq .",
    "( [ eq : lag3 ] ) is rewritten as eq .",
    "( [ eq : pitau ] ) .    from the above theorem ,",
    "the optimal parameters @xmath104 and the probability of community assignment @xmath105 for the optimized parameters are recursively estimated based on eq .",
    "( [ eq : q ] ) and ( [ eq : pitau ] ) . in this paper ,",
    "the community assigned to the @xmath38-th unit is determined by the @xmath71 that maximizes @xmath91 ( figure [ fig : com ] ( c ) ) .    finally , we use the following methods to determine a modular representation of a layered neural network that summarizes multiple connections between the pairs of communities ( figure [ fig : com ] ( d ) ) .",
    "+   + * four algorithms for determining bundled connections * :    * method @xmath64 : community @xmath106 and @xmath107 have a bundled connection iff there exists at least one connection between the pairs of units @xmath108 .",
    "* method @xmath109 : let the number of units in communities @xmath106 and @xmath107 be @xmath110 and @xmath111 , respectively , and let the number of connections between the pairs of units @xmath108 be @xmath112 . communities @xmath106 and @xmath107 have a bundled connection iff @xmath113 holds , where @xmath114 is a threshold .",
    "* method @xmath115 : among the bundled connections defined by method @xmath109 , only those that satisfy the following ( 1 ) or ( 2 ) are kept and the others are removed .",
    "( 1 ) for any community @xmath116 in the same layer as community @xmath106 , @xmath117 . ( 2 ) for any community @xmath118 in the same layer as community @xmath107 , @xmath119 .",
    "* method @xmath120 : among the bundled connections defined by method @xmath109 , only those that satisfy the above ( 1 ) and ( 2 ) are kept and the others are removed .    by these procedures , we obtain the modular representation of a layered neural network .",
    "in this section , we show three applications of the proposed method : ( 1 ) the decomposition of a layered neural network into independent networks , ( 2 ) generalization error estimation from a community structure , and ( 3 ) knowledge discovery from a modular representation . here",
    "we verify the effectiveness of the proposed method in the above three applications .",
    "the following processing was performed in all the experiments :    1 .",
    "the input data were normalized so that the minimum and maximum values were @xmath121 and @xmath122 , respectively .",
    "the output data were normalized so that the minimum and maximum values were @xmath123 and @xmath124 , respectively .",
    "the initial parameters were independently generated as follows : @xmath125 .",
    "the connection matrix @xmath127 if the absolute value of the connection weight between the @xmath13-th unit in the depth @xmath62 layer and the @xmath15-th unit in the depth @xmath14 layer is larger than a threshold @xmath65 , otherwise @xmath128 .",
    "note that @xmath124 and @xmath123 are used instead of @xmath64 and @xmath129 for stable computation .",
    "similarly , @xmath67 is defined from the connection weight between the @xmath13-th unit in the depth @xmath14 layer and the @xmath15-th unit in the depth @xmath16 layer .",
    "all units were removed that had no connections to other units .",
    "5 .   for each layer in a trained neural network",
    ", @xmath130 community detection trials were performed .",
    "we defined the community detection result as one that achieved the largest expected log likelihood in the last of @xmath131 iterations of the em algorithm .",
    "6 .   in each community detection trial , the initial values of the parameters @xmath77 were independently generated from a uniform distribution on @xmath132 , and then normalized so that eq .",
    "[ eq : normalization ] held .",
    "7 .   in visualization of modular representation ,",
    "all communities with no output bundled connections from them were regarded as unnecessary communities . in the output layer ,",
    "the communities with no input bundled connections were regarded in the same way as above .",
    "the bundled connections with such unnecessary communities were also removed .",
    "these unnecessary communities and bundled connections were detected from depth @xmath18 to @xmath64 , since the unnecessary communities in the shallower layers depend on the removal of unnecessary bundled connections in the deeper layers .",
    "we show that the proposed method can properly decompose a neural network into a set of small independent neural networks , where the data set consists of multiple independent dimensions . for validation , we made synthetic data of three independent parts , merged them , and applied the proposed method to decompose them into the three independent parts .",
    "the method we used to generate the synthetic data is shown in figure [ fig : exp1 ] . in the following , we explain the experimental settings in detail .",
    "first , three sets of input data were independently generated .",
    "all the sets contained input data with five dimensions , and their values followed : @xmath133 .",
    "then , three neural networks were defined , each of which has independent weights and biases . in each neural network , all the layers consisted of five units , and the number of hidden layers was set at one .",
    "the sets of weights and biases for the first , second and third neural networks are denoted as @xmath134 , @xmath135 , and @xmath136 , respectively .",
    "these parameters were randomly generated as follows : @xmath137 for the weights @xmath138 and @xmath139 , the connections with smaller absolute values than one were replaced by @xmath129 .",
    "finally , three sets of output data were generated by using the above input data and neural networks by adding independent noise following @xmath140 .",
    "the three generated sets of input and output data were merged into one set of data , as shown in figure [ fig : exp1 ] .",
    "we trained another neural network with @xmath141 dimensions for input , hidden and output layer using the merged data .",
    "then , a modular representation of the trained neural network was made with the proposed method .",
    "the results of the trained neural network , its community structure , and its modular representation are shown in figures [ fig : exp1o ] , [ fig : exp1c ] , and [ fig : exp1 m ] , respectively .",
    "the numbers above the input layer and below the output layer are the indices of the three sets of data .",
    "these results showed that the proposed method could decompose the trained neural network into three independent networks .",
    "[ fig : exp1o ]     [ fig : exp1c ]       in general , a trained result of a layered neural network is affected by different hyperparameters and initial parameter values . here , we show that the appropriateness of a trained result can be estimated from the extracted community structure by checking the correlation between the generalization error and the modularity @xcite",
    ".    the modularity is defined as a measure of the validity of the community detection result , and it becomes higher with more intra - community connections and fewer inter - community connections .",
    "let the number of communities in the network be @xmath142 , and @xmath143 be a @xmath144 matrix whose element @xmath145 is the number of connections between communities @xmath13 and @xmath15 , divided by the total number of connections in the network .",
    "the modularity @xmath146 of the network is defined by @xmath147 this is a measure for verifying the community structure of assortative networks , so it can not be applied directly to layered neural networks . in this paper , we define a modified adjacency matrix based on the original adjacency matrix of a layered neural network , and use it for measuring modularity . in the modified adjacency matrix , an element indexed by row @xmath13 and column @xmath15 represents the number of common units that connect with both the @xmath13-th and @xmath15-th units ( figure [ fig : modularitycalc ] ) .",
    "we set the diagonal elements of modified adjacency matrix at @xmath129 , resulting that there are no self - loops .",
    "if the data consist of multiple independent sets of dimensions like the synthetic data used in the experiment in section [ sec : decomposition ] , the generalization error is expected to be smaller when the weights of the connections between independent sets of input and output are trained to be smaller .",
    "therefore , a higher modularity indicates a smaller generalization error .",
    "+     +        +   +   + captypetable    c|c||c|c|c & + & & & + & @xmath148 & @xmath149 & @xmath150 & @xmath151 + @xmath65 & @xmath152 & @xmath153 & @xmath154 & @xmath155 + & @xmath156 & @xmath157 & @xmath158 & @xmath159 +    captypetable    [ tab : cc ]    in the experiment , we iterated the neural network training and community detection from the trained network @xmath160 times using the same data as in the experiment described in section [ sec : decomposition ]",
    ". the generalization error and modularity results for nine pairs of hyperparameters @xmath161 are shown in figure [ fig : exp2 ] , where @xmath28 is the lasso hyperparameter and @xmath65 is the weight removing hyperparameter .",
    "for the smaller @xmath28 and @xmath65 , the overall modularities were lower , which indicates that there were more connections between mutually independent neural networks .",
    "it was experimentally shown for some hyperparameters that better trained results were obtained ( with smaller generalization errors ) when the trained neural networks had clearer community divisions ( with higher modularity ) .",
    "table [ tab : cc ] shows the correlations for given @xmath161 .      in order to show that the modular representation extracts the global structure of a trained neural network",
    ", we applied the proposed method to a neural network trained with practical data .",
    "we used data that represent the characteristics of each municipality in japan @xcite .",
    "the characteristics shown in table [ tab : notationsdata ] were used as the input and output data , and the data of municipalities that had any missing value were removed .",
    "there were @xmath162 training data .",
    "before the neural network was trained , all the dimensions for all sets of data were converted through the function of @xmath163 , because the original data are highly biased .",
    "the results are shown in figure [ fig : exp3data ] .        the trained neural network and the modular representation using the above data are shown in figures [ fig : exp3o ] and [ fig : exp3 m ] , respectively .",
    "figure [ fig : exp3 m ] shows , for example , that the number of births , marriages , divorces , and people who engage in tertiary industry work ( a1 ) were inferred from the population aged 65 and older , the number of out - migrants , establishments , employees with employment and so on ( a4 ) .        [ fig : exp3o ]     from the extracted modular representation , we found not only the grouping of the input and output units , but also the relational structure between the communities of the input , output and hidden layers .",
    "for instance , the third community from the right in the depth @xmath109 layer ( b4 ) and the second community from the right in the depth @xmath115 layer ( b3 ) only connected to partial input and output units : they were used only for inferring the number of primary industrial employees and employed workers in their municipalities ( b1 , b2 ) , from the total population , the population under 15 years of age , the population of transference , and the numbers of family workers and executives ( b5 , b6 ) .",
    "in this section , we discuss the proposed algorithm from three viewpoints , the community detection method , the validation of the extracted result , and the scalability of our method .",
    "firstly , to extract a modular representation from a trained neural network , we employed a basic iterative community detection method for each layer .",
    "it is possible to modify this method , for example , by using the weights of connections or the connections in further layers .",
    "utilizing the output of each unit might also improve preciseness of the community detection result .",
    "the optimization of community detection methods and hyperparameters according to the task is future work .",
    "secondly , knowledge discovered from a modular representation depends on both the data and the analyst who utilizes the proposed method .",
    "there are both sensitive and robust communities which do and do not depend on them .",
    "therefore , it becomes important to separate the essential results from fluctuations .",
    "we anticipate that our method will form the basic analytic procedure of such a study .",
    "and lastly , in this paper , we experimentally evaluated the relationship between modularity and generalization error .",
    "it is well known that a community detection technique can be employed for large size networks .",
    "the analysis of larger datasets with higher dimensions would provide further information on layered neural networks .",
    "for such large datasets , it would also be important to evaluate the effectiveness of parallel computation , using the independent neural networks extracted with our proposed method .",
    "deep neural networks have achieved a significant improvement in terms of classification or regression accuracy over a wide range of applications by their ability to capture the complex hidden structure between input and output data . however , the discovery or interpretation of knowledge using deep neural networks has been difficult , since its internal representation consists of many nonlinear and complex parameters .    in this paper",
    ", we proposed a new method for extracting a modular representation of a trained layered neural network .",
    "the proposed method detects communities of units with similar connection patterns , and determines the relational structure between such communities .",
    "we demonstrated the effectiveness of the proposed method experimentally in three applications .",
    "( 1 ) it can decompose a layered neural network into a set of small independent networks , which divides the problem and reduces the computation time .",
    "( 2 ) the trained result can be estimated by using a modularity index , which measures the validity of a community detection result . and",
    "( 3 ) providing the global relational structure of the network would be a clue to discover knowledge from a trained neural network .",
    "table [ tab : notationsdata ] shows the notations of the data @xcite used in the experiment described in section [ sec : kd ] .",
    "the experimental settings of the parameters are shown in table [ tab : notations ] .",
    "c|p5cm||c|p5 cm name & & name & + i1 & & i22 & + i2 & & i23 & + i3 & & i24 & + i4 & & o1 & + i5 & & o2 & + i6 & & o3 & + i7 & & o4 & + i8 & & o5 & + i9 & & o6 & + i10 & & o7 & + i11 & & o8 & + i12 & & o9 & + i13 & & o10 & + i14 & & o11 & + i15 & & o12 & + i16 & & o13 & + i17 & & o14 & + i18 & & o15 & + i19 & & o16 & + i20 & & o17 & + i21 & & o18 & +    [ tab : notationsdata ]    c|p6cm|c|c|c name & meaning & exp.1 & exp.2 & exp.3 + @xmath164 & & + @xmath4 & number of training data sets & @xmath165 & @xmath166 & @xmath162 + @xmath167 & number of test data sets & @xmath129 & @xmath166 & @xmath129 + @xmath168 & & & @xmath169 + @xmath18 & & & @xmath120 + @xmath28 & hyperparameter of lasso & @xmath170 & * & @xmath171 + @xmath172 & & + @xmath65 & weight removing hyperparameter & @xmath152 & * & @xmath173 + @xmath142 & & & @xmath174 + method & & & @xmath115 + @xmath114 & & + @xmath121 & & & @xmath175 + @xmath122 & & & @xmath64 +    * : the nine parameters shown in the caption of figure [ fig : exp2 ] are used . [ tab : notations ]",
    "we would like to thank akisato kimura for his helpful comments on this paper ."
  ],
  "abstract_text": [
    "<S> deep neural networks have greatly improved the performance of various applications including image processing , speech recognition , natural language processing , and bioinformatics . </S>",
    "<S> however , it is still difficult to discover or interpret knowledge from the inference provided by a deep neural network , since its internal representation has many nonlinear and complex parameters embedded in hierarchical layers . </S>",
    "<S> therefore , it becomes important to establish a new methodology by which deep neural networks can be understood .    in this paper </S>",
    "<S> , we propose a new method for extracting a global and simplified structure from a layered neural network . based on network analysis </S>",
    "<S> , the proposed method detects communities or clusters of units with similar connection patterns . </S>",
    "<S> we show its effectiveness by applying it to three use cases . </S>",
    "<S> ( 1 ) network decomposition : it can decompose a trained neural network into multiple small independent networks thus dividing the problem and reducing the computation time . </S>",
    "<S> ( 2 ) training assessment : the appropriateness of a trained result with a given hyperparameter or randomly chosen initial parameters can be evaluated by using a modularity index . and ( 3 ) data analysis : in practical data it reveals the community structure in the input , hidden , and output layers , which serves as a clue for discovering knowledge from a trained neural network . </S>",
    "<S> + _ keywords _ : layered neural networks , network analysis , community detection </S>"
  ]
}