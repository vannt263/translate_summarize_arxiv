{
  "article_text": [
    "[ [ section ] ]    five books have so far been published in george r. r. martin s popular _ song of ice and fire _ series @xcite , @xcite , @xcite , @xcite , @xcite .",
    "it is widely anticipated that two more will be published , the first of which ( * ? ? ? * foreword ) will be entitled _ the winds of winter _",
    "each chapter of the existing books is told from the point of view of one of the characters .",
    "so far , ignoring prologue and epilogue chapters in some of the books , @xmath0 characters have had chapters told from their point of view . a chapter told from the point of view of a particular character @xmath1",
    "will be called a _",
    "pov chapter for _ @xmath1 .",
    "a character who has at least one pov chapter in the series will be called a _ pov character . _    [ [ section-1 ] ]    the goal is to predict how many pov chapters each of the existing pov characters will have in the remaining two books ( especially @xcite . )",
    "this varies from character to character , since some major characters have been killed off and are unlikely to appear in future novels , whereas other characters are of minor importance and may or may not have chapters told from their point of view .",
    "[ [ section-2 ] ]    no attempt is made to deal with characters who have not yet appeared as pov characters .",
    "this issue is discussed further in section [ newcharacters ] .",
    "the data consist of a @xmath2 matrix @xmath3 obtained from http://www.lagardedenuit.com/ , a french fansite .",
    "the rows of @xmath3 correspond to pov characters and the columns to the existing books in order of publication .",
    "the @xmath4entry of @xmath3 is the number @xmath5 of pov chapters for character @xmath6 in book @xmath7 .",
    "the data are displayed in table [ m ] .",
    ".the data .",
    "novel titles are abbreviated using their initials , so for example agot = _ a game of thrones _ etc .",
    "obtained from http://www.lagardedenuit.com/wiki/index.php?title=personnages_pov .",
    "a similar table appears at http://en.wikipedia.org/wiki/a_song_of_ice_and_fire . [ cols=\"<,^,^,^,^,^\",options=\"header \" , ]     2ex      it is easy to predict how many pov chapters certain characters will have in the next book . for example , if character @xmath1 was beheaded in book @xmath8 and has not appeared in subsequent books , most readers would predict that @xmath1 will have @xmath9 pov chapters in book @xmath10 .",
    "if we denote by @xmath11 the number of pov chapters for @xmath1 in book @xmath10 , we predict @xmath12 .",
    "the prediction @xmath12 is a _ point prediction .",
    "on the other hand , suppose it is believed that a certain character @xmath13 will have @xmath14 pov chapters in book @xmath10 .",
    "it is quite plausible that @xmath13 might have @xmath15 or @xmath16 pov chapters instead . on the other hand",
    ", it may be thought unlikely that @xmath13 will appear in @xmath9 ( too few ) or @xmath17 ( too many ) pov chapters . instead of the point prediction @xmath18",
    ", it would be better to give a range of likely values for @xmath19 this is an _ interval prediction . _ for example",
    ", to say that the interval @xmath20 $ ] is an @xmath21 credible interval for @xmath22 is to say that there is an @xmath21 probability that @xmath23 .",
    "even more refined than an interval of likely values is a probability distribution over all possible values of the number of pov chapters .",
    "for example , using @xmath24 to denote probability , we could say @xmath25 , @xmath26 , @xmath27 , and @xmath28 .",
    "this probability distribution completely describes our belief about @xmath22 .",
    "conceptually , it can be thought of as a guide for betting .",
    "for example , if our beliefs are correct then it is an even money bet that @xmath18 and also an even money bet that @xmath29 .",
    "our aim is to give such a probability distribution for each pov character @xmath13 .",
    "how can we get a probabilistic prediction as described in section [ probabilisticprediction ] ?",
    "one way would be to assign probabilities based on our gut feelings about the probability of different outcomes , as a traditional bookmaker might do .",
    "( an uncharitable reader might even argue that this would be better than the approach taken below . )",
    "an alternative approach is to choose a statistical _ model _ , that is , a process which could plausibly have generated the observed data .",
    "once the model has been chosen , it can be used to predict future data . in section [ model ] , we describe a family of possible models which depend on six parameters .",
    "values of these parameters which are likely to have produced the observed data are found , and these parameters are used to generate predictions for future books .",
    "this step is repeated many times to build up probability distributions for the predictions .",
    "the whole process of finding values of the parameters is called _ inference _ or _ fitting the model . _    [ [ section-3 ] ]    in general ,",
    "the best predictions are obtained by a combination of modelling and common sense . here",
    "we focus entirely on the modelling side and leave common sense behind .",
    "the question to be answered could be expressed as :  what could be predicted about future books if we knew nothing about the existing books except for table [ m ] ? \"",
    "denote the number of pov chapters for character @xmath6 in book @xmath30 by @xmath31 , @xmath32 .",
    "we assume that there are times @xmath33 and @xmath34 such that the character is ` on - stage ' between @xmath33 and @xmath34 and ` off - stage ' at other times .",
    "for example , the character might be killed off at time @xmath34 . in other words , @xmath35 for @xmath36 and @xmath37 .",
    "for @xmath38 we assume that pov chapters follow a poisson distribution with parameter @xmath39 .",
    "[ [ section-4 ] ]    it is inconvenient to have @xmath33 and @xmath34 as model parameters , so instead we assume that there are @xmath40 and @xmath41 such that @xmath42 this is the same as putting @xmath43 and @xmath44 in section [ descriptionofmodel ] .",
    "note that we allow @xmath45 and @xmath41 to take real values , even though @xmath30 is constrained to be an integer .",
    "[ [ section-5 ] ]    it is undesirable for the @xmath45 , @xmath41 and @xmath39 for each character to be independent of the other characters as this would give a model with @xmath46 parameters where @xmath47 is the number of characters .",
    "it is unlikely that good predictions could be made from a model with too many parameters . to cut down the number of parameters , we assume that @xmath39 , @xmath41 and @xmath45 are _ random effects , _ which means that they are samples from some underlying probability distribution .",
    "one motivation behind this assumption is that the parameters for different characters are assumed to have something in common .",
    "for example , there might be a typical value for @xmath45 which reflects how long the average character is likely to last in _ a song of ice and fire .",
    "_    [ [ section-6 ] ]    the @xmath48 are assumed to be normally distributed and @xmath41 and @xmath45 are also assumed to be normally distributed . however , if there are no constraints on the values of @xmath41 and @xmath45 , the model becomes difficult to fit , because for example there would be no difference in data generated by @xmath49 and @xmath50 for a particular character @xmath6 , regardless of the value @xmath39 . because this makes inference problematic",
    ", we assume that the @xmath51 and @xmath52 distributions are truncated in the interval @xmath53 $ ] . the overall model is :      @xmath42    for @xmath54 , and @xmath32 , with    @xmath55\\\\ \\beta_i & \\sim n(\\mu_\\beta , \\sigma_\\beta^2 ) \\text { truncated to } [ 0,7]\\end{aligned}\\ ] ]    where @xmath56 and @xmath57 . for fixed @xmath6 , the @xmath31",
    "are assumed to be conditionally independent given @xmath58 and @xmath41 .",
    "for fixed @xmath30 and @xmath59 , the @xmath31 and @xmath60 are assumed to be conditionally independent given the values of @xmath61 and @xmath62 and @xmath63 .",
    "[ [ section-7 ] ]    to be explicit , let @xmath64 be the data . for @xmath54 define @xmath65 @xmath66 by @xmath67",
    "then the likelihood is proportional to @xmath68 where the integral is over the @xmath46 dimensions @xmath69 and the symbol @xmath70 stands for @xmath8 if @xmath71 is true and @xmath9 if @xmath71 is false .",
    "[ [ section-8 ] ]    a model like [ model ] is often called a _",
    "hierarchical _ model .",
    "the @xmath72 and @xmath73 are called _ hyperparameters _ to distinguish them from the individual @xmath39 , @xmath45 and @xmath41 .",
    "the model is fitted using bayesian inference with non - informative @xmath74 priors on the location parameters @xmath75 , @xmath76 and @xmath77 and inverse gamma @xmath78 priors on the scale parameters @xmath79 and @xmath73 . because intractable - looking integrals appear in the likelihood ( [ likelihood ] ) , the model is fitted using gibbs sampling . for the @xmath39 and @xmath41 ,",
    "samples are drawn from the marginal distribution using a histogram approximation .",
    "this is slow but easier to code than alternatives .",
    "[ [ algorithm ] ]    at each iteration of the algorithm , a value of @xmath80 is sampled using the theory of the normal distribution and then , for each character @xmath6 , the values of @xmath39 , @xmath45 and @xmath41 are sampled in that order",
    ". then predictions for @xmath81 and @xmath82 are sampled using the definition of model [ model ] .",
    "after all iterations are complete , a burn - in is discarded and the output is thinned to make the resulting samples as uncorrelated as possible .",
    "this is useful for some purposes , such as drawing figure [ prob0 ] .",
    "[ [ section-9 ] ]    the output of the algorithm is a collection of samples of @xmath80 and predictions @xmath83 and @xmath84 where @xmath85 .",
    "the algorithm is written in @xmath86 @xcite and uses the ` truncnorm ` package @xcite .",
    "model [ model ] does not fit the training data very well since there are so many zeroes in column affc in table [ m ] .",
    "it is known ( * ? ? ?",
    "* afterword ) , ( * ? ? ?",
    "* foreword ) that @xcite and @xcite were originally planned to be a single book , but it was later split into two volumes , each of which concentrates on a different subset of the characters .",
    "[ [ preprocess ] ]    this problem can be approached either by ignoring it , by modelling , or by pre - processing the data .",
    "the model already has a lot of parameters and making it more complex is unlikely to be a good idea .",
    "ignoring the problem and fitting the model to table [ m ] is not too bad , but it was decided to pre - process the data by replacing @xmath3 by @xmath87 where @xmath88 where @xmath89 and @xmath90 are the number of chapters in books @xmath91 and @xmath92 respectively .",
    "this preserves the total number of chapters in each book , which may be of interest ( see section [ newcharacters ] . )",
    "[ [ section-10 ] ]    another possible approach would be to treat books @xmath91 and @xmath92 as one giant book when fitting the model .",
    "the main disadvantage of this approach is that it decreases the amount of data available even further , although the resulting @xmath93 matrix would probably provide a better fit than @xmath87 to the chosen model .",
    "the gibbs sampler of section [ algorithm ] was run for @xmath94 iterations with a burn - in of @xmath95 and thinned by taking every @xmath96 sample , resulting in posterior samples of size @xmath97 .",
    "the algorithm was applied to the smoothed data @xmath87 of section [ datasmoothing ] .",
    "it was run several times with random starting points to check that the results are stable .",
    "only one run is recorded here .",
    "[ [ section-11 ] ]    table [ posteriors1 ] gives the posterior predictive distribution of pov chapters for each pov character in book @xmath10 .",
    "table [ posteriors2 ] gives a similar distribution for book @xmath98 .",
    "( the results for book @xmath98 are of less interest , as new predictions for book @xmath98 should be made following the appearance of book @xmath10 . )",
    "graphs of the posterior distributions for book @xmath10 are given in figures [ posteriors_1 ] and [ posteriors_2 ] .",
    "many of the distributions are bimodal and are not well - summarised by a single credible interval .",
    "the distribution for tyrion has the highest variance , followed by jon snow ( see figure [ posteriors_1 ] . )",
    "aeron , areo , jon connington , arianne@xmath99 , @xmath100quentyn , victarion , asha , barristan@xmath99 and @xmath100arys , melisandre@xmath99 the distributions are identical ; apparent differences are due to sampling variation . ]",
    "one of the most compelling aspects of the _ song of ice and fire _ series is that major characters are frequently and unexpectedly killed off .",
    "the probability of a character having zero pov chapters ( which is the first column of table [ posteriors1 ] divided by @xmath95 ) is therefore of interest .",
    "these values are plotted in figure [ prob0 ] . treating the posterior samples as independent",
    ", the error bars in figure [ prob0 ] indicate approximate @xmath101 confidence intervals of @xmath102 .",
    "note that although a character who has been killed off will have zero pov chapters , the converse is not necessarily true .",
    "the probabilities in figure [ prob0 ] are not based on events in the books , but solely reflect what the model can glean from table [ m ] .    .",
    "the characters are ordered on the @xmath1axis by the value of the posterior probability for book @xmath10 ( the black dots . )",
    "the blue circles are the posterior probabilities of having zero pov chapters in book @xmath98 .",
    "the error bars extend to @xmath102 and are intended to indicate when two posterior probabilities are roughly equal .",
    "the figure is discussed in sections [ zeroprobabilities ] to [ isjonsnowdead ] . ]",
    "[ [ section-12 ] ]    the characters in figure [ prob0 ] are arranged on the @xmath1axis in order of their probability of having zero pov chapters in book @xmath10 .",
    "eddard and catelyn were already killed off in @xcite and ( arguably ) @xcite respectively .",
    "arys , who has the third highest posterior probability of having zero pov chapters , was killed off in @xcite , but it is misleading to be impressed by this . the reason why arys has a high posterior probability of having zero pov chapters is that he only ever appeared in one pov chapter , so @xmath103 is small and so , even if @xmath104 is large , there is a high probability that @xmath105 .",
    "in fact , in the smoothed data , the rows for melisandre and arys are exactly the same .",
    "the difference between the posterior distributions for these two characters is due to sampling variation ( and varies from one model run to another . )",
    "[ [ section-13 ] ]    the characters between tyrion and aeron in figure [ prob0 ] all have roughly the same posterior probability of having zero pov chapters .",
    "they are mostly characters who have featured prominently in all the books since the beginning , together with the newer characters aeron , areo , arianne and jon connington whose posterior predictive distributions are identical and who have lower probabilities of non - appearance in book @xmath98 .",
    "[ [ section-14 ] ]    the next group of characters are those who have had relatively few pov chapters , including quentyn who , despite having been killed off in @xcite , is assigned a low posterior probability of @xmath106 of having zero pov chapters in book @xmath10 since he has the same posterior predictive distribution as asha , barristan and victarion .",
    "[ [ section-15 ] ]    finally , there are the characters cersei , brienne , jaime and samwell who have only recently become pov characters and have had a large number of pov chapters in the books in which they have appeared .",
    "the model suggests that the probability of jon snow _ not _ being dead is at least @xmath107 since this is less than the posterior probability of his having at least one pov chapter in book @xmath10 .",
    "given the events of @xcite , many readers would assess his probability of not being dead as being much lower than @xmath107 , but we must again point out that the model is unaware of the events in the books .",
    "the model can only say that , based on the number of pov chapters observed so far , he has about as much chance of survival as the other major characters .",
    "it is desirable to check that the model has been coded correctly .",
    "a way to check this is to generate a data set according to the model and then see whether the chosen method of inference can recover the parameters which were used to generate the data set .",
    "[ [ section-16 ] ]    if the model had been fitted by frequentist methods , it would be possible to generate a large number of data sets , fit the model to each one , calculate confidence intervals for the hyperparameters , and check that the confidence intervals have the correct coverage .",
    "since the model has been fitted by bayesian methods , it can only be used to produce credible intervals . however , as flat priors were used , the posterior distributions for the hyperparameters should be close to their likelihoods and so bayesian credible intervals should roughly coincide with frequentist confidence intervals when the posterior distributions of the hyperparameters are symmetric and unimodal , which they are .",
    "[ [ inferencetest ] ]    to test the method of inference , the model was fitted to @xmath108 data sets .",
    "each data set was generated from hyperparameters which were a perturbation of @xmath109 .",
    "these values were chosen because they were approximately the posterior medians for the hyperparameters obtained from one of the fits of model [ model ] to the smoothed data @xmath87 .",
    "the location parameters @xmath110 and @xmath77 were perturbed by adding @xmath111 noise and the scale parameters @xmath79 and @xmath73 were perturbed by multiplying by @xmath112 where @xmath113 . for each of the data sets ,",
    "@xmath114credible intervals were calculated by taking the middle @xmath115 of the posterior distributions for each of the @xmath10 hyperparameters , yielding @xmath116 credible intervals per @xmath114 .",
    "the results , plotted in figure [ inference_test ] ( left panel ) indicate that the credible intervals have roughly the correct coverage .",
    "[ inference_test ]   model fits as described in section [ inferencetest ] and section [ inferencetest_pred ] . on the left , a comparison of the nominal and actual coverage of credible intervals for the hyperparameters . on the right , a comparison of the nominal and actual coverage of credible intervals for the predicted number of pov chapters.,title=\"fig : \" ]    [ [ section-17 ] ]    note that there are choices of the hyperparameters which the chosen method of inference will not be able to recover .",
    "for example , data generated with @xmath117 will be practically indistinguishable from data generated with @xmath118 , so there is no hope of inferring the hyperparameters in this case .",
    "this is no great drawback as it should not affect the model s predictions , which are the topic of interest .",
    "[ [ inferencetest_pred ] ]    the procedure of section [ inferencetest ] was carried out for the one - step - ahead predictions for book @xmath10 .",
    "the result , shown in figure [ inference_test ] ( right panel ) shows that the credible intervals have greater coverage than they should .",
    "this is because the number of pov chapters can only take integer values and so the closed interval @xmath119 $ ] obtained by taking the @xmath120 and @xmath121 quantiles of the posterior distribution will in general cover more than @xmath115 of the posterior samples .",
    "[ [ section-18 ] ]    note once again that the purpose of these checks and experiments is to make sure that the model has been correctly coded .",
    "we now discuss how to evaluate its predictions .",
    "every predictive model should be applied to unseen test data to see how accurate its predictions really are .",
    "it will not be possible to test model [ model ] before the publication of @xcite but an attempt at validation can be made by fitting the model to earlier books and seeing what it would have predicted for the next book .",
    "[ [ validation_12 ] ]    the model was tested by fitting it to books @xmath8 and @xmath122 in the series .",
    "only @xmath14 pov characters appear in these books , so the data consist of the upper - left @xmath123 submatrix of table [ m ] .",
    "figure [ validation ] shows the result of fitting the model to this matrix and comparing with the true values from the third column of table [ m ] .",
    "the intervals displayed are central @xmath124 ( solid lines ) and @xmath21 ( dotted lines ) credible intervals .",
    "the coverage is satisfactory but the intervals are much too wide to be of interest .",
    "[ validation ]   ( plotted as dots ) from table [ m ] compared with central @xmath124 ( solid lines ) and @xmath21 ( broken lines ) credible intervals obtained from fitting the model to @xmath125 as described in section [ validation_12 ] .",
    "the characters have been sorted in increasing order of the posterior median.,title=\"fig : \" ]    [ [ section-19 ] ]    fitting the model to the @xmath126 upper - left submatrix of table [ m ] consisting of pov chapters from the first three books gives more interesting output , but it is not clear how to evaluate the results because of the splitting of books @xmath91 and @xmath92 discussed above in section [ datasmoothing ] .",
    "[ [ section-20 ] ]    we can also compare the model s predictions with preview chapters from @xcite which are said to have been released featuring the points of view of arya , arianne , victarion and barristan .",
    "given that there is at least one arya chapter , table [ posteriors1 ] indicates that there will probably be at least @xmath92 arya pov chapters and perhaps more .",
    "[ [ section-21 ] ]        the model can generate data containing a row of zeroes , but there are no zero rows in the data to which the model is fitted , because by definition this would correspond to a character who has never been a pov character in the books .",
    "this is a source of bias in the model but it is not obvious how it can be avoided .",
    "the effect of the bias can be tested by repeating the simulations of section [ inferencetest ] , but deleting zero rows before fitting the model . for @xmath127 ,",
    "the coverage of a @xmath115 credible interval for a hyperparameter tends to be roughly @xmath128 .",
    "there is little to support the choice of the poisson distribution in model [ model ] other than that it has the smallest possible number of parameters .",
    "it is more common to use the negative binomial distribution for count data , but this would introduce extra complexity into the model , which is undesirable .      with only @xmath0 values of @xmath39 , @xmath45 and @xmath41 available for finding the corresponding hyperparameters , it may not be possible to fit a ( truncated ) normal distribution in a meaningful way .",
    "consideration of the posterior samples of the @xmath39 , @xmath45 and @xmath41 suggest that they more - or - less follow the pattern which is evident in the data and that the shrinkage of these parameters towards a common mean , which is one of the benefits of using a hierarchical model , can not really be attained with so little data .",
    "for example , when the model is fitted to a data set containing a row in which the most recent entry is @xmath9 , the vast majority of posterior samples for the next entry in that row are always @xmath9 .",
    "this is one reason for smoothing the data in section [ datasmoothing ] before fitting the model , in preference to fitting the model directly to table [ m ] .",
    "if the number of pov characters was much larger , this might not be such a big problem .",
    "given @xmath129 , the model treats @xmath31 and @xmath60 as independent for @xmath59 .",
    "this is not a realistic assumption because if one character has more pov chapters , then the other characters will necessarily have fewer .",
    "again , addressing this would seem to over - complicate the model .",
    "the model ignores the introduction of new pov characters , although every book in the series has featured some new pov characters .",
    "we can , however , use the output from the fitted model to make guesses about new characters .",
    "the posterior distribution of the number of chapters in book @xmath10 told from the points of view of existing pov characters is unimodal with a mean of @xmath130 chapters , but typical books in the series so far have had about @xmath131 chapters .",
    "so we could estimate that there will be about @xmath132 chapters in @xcite told from the point of view of new pov characters . in the previous books , according to table [ m ] , the number of chapters told from the point of view of new pov characters has been @xmath133 and @xmath132 , so @xmath132 does not seem like an unreasonable guess .",
    "we could continue to make predictions in the hope of getting one right , but there is no merit in this .",
    "we hope that it will be possible to review the model s performance following the publication of @xcite ."
  ],
  "abstract_text": [
    "<S> predictions are made for the number of chapters told from the point of view of each character in the next two novels in george r. r. martin s _ a song of ice and fire _ series by fitting a random effects model to a matrix of point - of - view chapters in the earlier novels using bayesian methods . * </S>",
    "<S> spoiler warning : readers who have not read all five existing novels in the series should not read further , as major plot points will be spoiled , starting with table [ m ] . </S>"
  ]
}