{
  "article_text": [
    "standard machine learning algorithms require a huge number of training examples to uncover hidden features , while humans and other animals can learn new concepts from only a few examples without any supervision signal  @xcite .",
    "learning hidden features in unlabeled training examples is called unsupervised learning . understanding how the number of examples confines the learning process is of fundamental importance in both cognitive neuroscience and machine learning  @xcite .",
    "as already observed in training of deep neural networks , unsupervised pretraining can significantly enhance the final performance , because the unsupervised pretraining provides a good initial region in parameter space from which the final fine - tuning starts  @xcite . however , there are few theoretical works addressing how unsupervised learning extracts hidden features .",
    "one potential reason is that the unsupervised learning process in a deep neural network is typically very complicated .",
    "hence , understanding the mechanism of unsupervised learning in simple models is of significant importance .",
    "this topic is recently studied based on the bayesian inference framework  @xcite . in this recent work",
    ", the authors treated each example ( data ) as a constraint on the factor graph , and reformulated the learning of model parameters as a bayesian inference problem on graphical models , and derived the message passing equations to infer the hidden feature from a finite amount of data .",
    "they observed an entropy crisis in a simple restricted boltzmann machine ( rbm ) model , and predicted a discontinuous phase transition .",
    "however , in an approximate hopfield model obtained by a high - temperature expansion of the rbm model , the entropy crisis is absent , and instead , a continuous transition is observed .",
    "these properties observed in studies of single instances capture key characteristics of the unsupervised feature learning .    here",
    ", we further demonstrate that the message passing equation derived in the recent work  @xcite agrees with the statistical analysis of the system in the thermodynamic limit via replica theory , a standard theoretical tool in spin glass theory of disordered systems  @xcite .",
    "the replica computation predicts the location of phase transition separating an impossible - to - infer regime to inferable regime . this transition can be continuous depending on the embedded feature strength .",
    "a discontinuous phase transition always exists in a restricted boltzmann machine learning , but absent in an approximate hopfield model where only continuous phase transition is observed .",
    "we also develop an iterative equation to infer the hyper - parameter ( temperature ) hidden in the data , which in physics corresponds to iteratively imposing nishimori condition .",
    "this iterative scheme can even quantitatively predict how apparent features embedded in a real dataset are .",
    "our analysis gives a thorough understanding of novel properties of the restricted boltzmann machine within replica symmetric approximation .",
    "this paper is structured as follows . in sec .",
    "[ srbmm ] , we introduce a simple rbm model for unsupervised feature learning , and propose the bayesian inference framework to derive the message passing equation on factor graph representation of the learning process , and this equation is then statistically analyzed and compared with replica computation under the replica symmetric assumption",
    ". a more efficient approximate message passing equation is also derived .",
    "we also derive an iterative equation based on bayes rule to predict the unknown temperature ( feature strength ) in the data . in sec .",
    "[ ahopf ] , we approximate the rbm with the hopfield model where the stored pattern is interpreted as the feature vector . similar statistical analysis is carried out , and its physical implications are discussed .",
    "we end the paper with a summary in sec .",
    "restricted boltzmann machine is a basic unit widely used in building a deep belief network  @xcite .",
    "it consists of two layers of neurons .",
    "the visible layer receives the input examples while the other hidden layer builds an internal representation of the input .",
    "no lateral connections exist within each layer for computational efficiency .",
    "the symmetric connections ( synapses ) between visible and hidden neurons are considered as features the network tries to learn from a large number of training examples .",
    "it is a common strategy to use sampling - based gradient - decent method to learn features in the data  @xcite , however , the gradient - decent learning is complicated and not amenable for analytical studies .",
    "recent work showed that learning features can also be studied within a bayesian learning framework  @xcite , which has the advantage of accounting for the uncertainty ( about the features ) caused by noises in the data  @xcite , and furthermore can be analytically studied on probabilistic graphical models .",
    "we focus on an unsupervised learning of finite samplings generated by a simple rbm , where a single hidden neuron is considered .",
    "the task is to uncover an unknown rule embedded in the unlabeled data .",
    "the rule is represented by a binary feature vector defined as @xmath0 where @xmath1 goes from @xmath2 to @xmath3 , the number of neurons in the visible layer .",
    "we assume components of the true hidden feature vector connecting the visible neurons and the hidden neuron can take only two values , i.e. , @xmath4 or @xmath5 , with equal probabilities .",
    "then this feature vector is used to generate independent random samples according to the joint probability @xmath6 , where @xmath7 , and @xmath8 is the visible configuration and @xmath9 is the hidden neuron s state . both @xmath9 and components of @xmath8 take binary values ( @xmath10 ) as well .",
    "a rescaled feature factor by the system size is assumed .",
    "the feature vector is also multiplied by an inverse - temperature parameter @xmath11 to investigate effects of the feature strength on the unsupervised learning . for simplicity ,",
    "we consider the case of neurons without any external biases ( fields ) .",
    "generalization to the case of neurons with external fields is straightforward ( appendix  [ gener ] ) .",
    "suppose we have @xmath12 independent samples or examples @xmath13 to learn the true hidden feature vector @xmath14 , using the bayes formula , we have the posterior distribution of the feature vector as @xmath15 where @xmath16 is the partition function of the model , @xmath17 goes over all examples and @xmath18 denotes a vector transpose operation .",
    "a uniform prior probability for the feature vector is assumed for simplicity .",
    "a large @xmath11 indicates the feature in the data is strong , and expected to be revealed by a few examples , while a weak feature vector may not be revealed by a huge number of examples .",
    "each example serves as a constraint to the learning process .",
    "once @xmath19 , the model becomes non - trivial as the partition function could not be computed exactly for a large number of visible neurons .",
    "this @xmath12 can be finite or proportional to the system size , and in the latter case we define a data density as @xmath20 .",
    "hereafter , we omit the conditional dependence of @xmath21 on @xmath22 .     indicates the strength ( @xmath10 ) with which the feature component @xmath23 is related to @xmath17-th example .",
    "right panel : the top panel shows constraint @xmath17 collects information from its neighboring feature nodes other than @xmath1 and produces an output message to node @xmath1 . the bottom panel shows node @xmath1 collects information from its neighboring constraints other than @xmath24 and produces an output message to node @xmath24 . the figure is taken from ref  @xcite . ]",
    "we call optimizing the marginal posterior probability of feature vectors given the data as bayesian learning in the current unsupervised learning context",
    ". that is , we compute the maximizer of the posterior marginals ( mpm ) estimator @xmath25  @xcite .",
    "we define the overlap between the inferred feature vector and the true one as @xmath26 , where @xmath27 is the inferred feature vector .",
    "the mpm estimator maximizes the overlap .",
    "if @xmath28 , the examples do not give any information about the feature vector . if @xmath29 , the feature vector is perfectly determined .",
    "the statistical inference now is simplified to the computation of marginal probabilities , e.g. , @xmath30 , which is still a hard problem due to the interaction among example constraints . however , by mapping the model ( eq .",
    "( [ pobs ] ) ) onto a factor graph  @xcite , the marginal probability can be estimated by message passing ( fig .",
    "[ rbm ] ) . for simplicity , we give the final simplified message passing equations ( smp ) as follows ( detailed derivations are given in ref  @xcite , also in appendix  [ smp - rbm ] ) :    [ bp0 ] @xmath31    where @xmath32 .",
    "the cavity magnetization is defined as @xmath33 .",
    "@xmath34 can be interpreted as the message passing from feature @xmath1 to the data constraint @xmath17 , while @xmath35 can be interpreted as the message passing from data constraint @xmath24 to its feature @xmath1 .",
    "if the weak correlation assumption ( also named bethe approximation  @xcite ) is self - consistent , the smp would converge to a fixed point corresponding to a stationary point of the bethe free energy function with respect to the cavity messages @xmath36  @xcite . from this fixed point , one can extract useful information about the true feature vector of the data , by calculating the marginal probability as @xmath37 where @xmath38 . note that we perform the feature inference using the same inverse - temperature as used to generate the data , thus the inference is bayes - optimal and satisfies the nishimori condition  @xcite . in the simulation section",
    ", we also perform the bayesian inference using slightly different temperatures .    next , we compute the bethe free energy , which is obtained by @xmath39 .",
    "the free energy contribution of a feature node reads @xmath40+\\ln\\bigl(1+\\prod_{a\\in\\partial i}\\mathcal{g}_{a\\rightarrow i}\\bigr),\\ ] ] and the free energy contribution of a data node reads @xmath41 where we define @xmath42 , @xmath43 , @xmath44 , and @xmath45 .",
    "another important quantity is the number of feature vectors consistent with the presented random samplings , characterized by the entropy per neuron @xmath46 . in the presence of a larger dataset",
    ", the generative machine should have less uncertainty about the underlying feature , corresponding to small or vanishing entropy .",
    "the entropy can be derived by using the standard thermodynamic formula @xmath47 . under the bethe approximation ,",
    "@xmath48 is evaluated as summing up contributions from single feature nodes and example nodes : @xmath49 , where single feature node contribution is expressed as @xmath50+\\ln\\left(1+\\prod_{a\\in\\partial i}\\mathcal{g}_{a\\rightarrow i}\\right)\\\\ -\\left[\\sum_{a\\in\\partial i } \\mathcal{h}_{a\\rightarrow i}(+1)+\\prod_{a\\in\\partial i}\\mathcal{g}_{a\\rightarrow i}\\sum_{a\\in\\partial i}\\mathcal{h}_{a\\rightarrow i}(-1)\\right]/\\left(1+\\prod_{a\\in\\partial i}\\mathcal{g}_{a\\rightarrow i}\\right ) , \\end{split}\\ ] ] and single example contribution reads @xmath51 where we define @xmath52 .",
    "one iteration of the smp equation ( eq .  ( [ bp0 ] ) ) requires the time complexity of the order @xmath53 and memory of the order @xmath53 .",
    "the smp equation can be further simplified by reducing computational complexity .",
    "the final equation in physics is called thouless - anderson - palmer ( tap ) equation  @xcite , and in information theory is named approximate message passing ( amp ) equation  @xcite .",
    "one strategy is to use large-@xmath3 limit .",
    "we first get the cavity bias in this limit as @xmath54 . then by applying the same large-@xmath3 expansion ,",
    "we obtain @xmath55 .",
    "therefore , we get the first amp equation as follows : @xmath56 where @xmath57",
    ". then we define the local field @xmath58 , and note that @xmath59 , we can obtain an approximate @xmath60 in the large-@xmath3 expansion , @xmath61 the last term in the expression of @xmath60 serves as an onsager reaction term in a standard tap equation .",
    "finally we arrive at the second amp equation : @xmath62    now we have only @xmath63 equations to solve rather than @xmath64 equations in the smp equation ( eq .  ( [ bp0 ] ) ) . to make amp equations converge in a parallel iteration , the time indexes for the variables are important  @xcite .",
    "here we write down the closed form of amp equation with correct time indexes :    [ amprbm ] @xmath65    where @xmath66 denotes the time index for iteration .",
    "these time indexes just follow the temporal order when we derive the amp equation from the smp equation .",
    "next , we give a statistical analysis of the smp equation .",
    "we first define the cavity field @xmath67 . under the replica symmetric assumption",
    ", @xmath68 follows a gaussian distribution with mean zero and variance @xmath69 in the large-@xmath3 limit .",
    "we define @xmath70 .",
    "similarly , @xmath71 also follows a gaussian distribution with mean zero but variance @xmath72 .",
    "therefore , we arrive at the following thermodynamic equation :    [ rbmrm ] @xmath73    where @xmath74 .",
    "one can expect that when @xmath75 is small , only one solution of @xmath76 exists for the above equation , however , at some critical @xmath77 , there is a nontrivial solution of @xmath78 , which signals the fixed point of smp or amp starts to contain information about the underlying true feature vector .",
    "@xmath77 can be determined by expanding the above equation around @xmath76 .",
    "the expansion leads to @xmath79 , which implies that when @xmath80 , @xmath76 is the stable solution of the thermodynamic equation , but as long as @xmath81 , the @xmath76 is not the stable solution any more .",
    "however , as we compare this solution with smp result on single instances , @xmath78 solution does not match the numerical simulation very well .",
    "to explain this , a replica computation is required .",
    "now , we perform a replica computation of the free energy function . instead of calculating a disorder average of @xmath82 , the replica trick computes the disorder average of an integer power of @xmath16 , then the free energy density ( multiplied by @xmath83 ) can be obtained as  @xcite @xmath84 where the limit @xmath85 should be taken first since we can apply the saddle - point analysis  @xcite , and the disorder average is taken over all possible samplings ( data ) and the random realizations of true feature vector .",
    "the explicit form of @xmath86 reads @xmath87 where @xmath88 indicates the replica index .",
    "we leave the technical details to the appendix  [ replica - rbm ] , and give the final result here .",
    "the free energy function reads , @xmath89 and the associated saddle - point equations are expressed as    [ rbmreplica ] @xmath90    we make some remarks about the above saddle - point equations .",
    "@xmath91 indicates the typical value of the overlap between the true feature vector and the estimated one , while @xmath92 indicates the typical value of the overlap between two estimated feature vectors selected from the posterior probability ( eq .  ( [ pobs ] ) ) .",
    "according to the nishimori condition , @xmath93 , implying that the embedded true feature vector follows the same posterior distribution in bayesian inference .",
    "we verify this point later in numerical solution of the saddle point equations .",
    "assuming @xmath91 and @xmath92 are both small values close to zero , eq .",
    "( [ rbmreplica ] ) in this limit implies that a critical @xmath94 , above which @xmath28 is not a stable solution any more . by expanding eq .",
    "( [ rbmreplica ] ) around @xmath28 up to the second order @xmath95 , we find @xmath96 when @xmath75 approaches @xmath77 from above .    note that by assuming @xmath28 in eq .",
    "( [ rbmreplica ] ) , we obtain eq .",
    "( [ rbmrm ] ) ; this is the reason why eq .",
    "( [ rbmrm ] ) can predict the correct threshold for transition but could not describe the property of @xmath97 .",
    "the entropy can be derived from the free energy , and the result ( appendix  [ replica - rbm ] ) is given by @xmath98    when @xmath28 , @xmath99 , which coincides with that obtained with cavity method ( appendix  [ srbm ] ) .",
    "given @xmath28 , that is the data still do not contain information about the hidden feature , the entropy will become negative once @xmath100 .",
    "this suggests that the entropy crisis can even occur within @xmath28 regime .",
    "alternatively , at a fixed @xmath75 , the entropy crisis occurs at a temperature @xmath101 . setting @xmath102 ,",
    "one obtains @xmath103 , which distinguishes two cases : @xmath104 for @xmath105 , the transition of @xmath91 from zero to non - zero value takes place after the entropy crisis ; @xmath106 for @xmath107 , the transition takes place before the crisis .",
    "this has clear physical implications .",
    "if the transition occurs before the entropy crisis , the location of transition identified by the replica - symmetric theory is correct .",
    "however , the transition after the crisis is incorrect under the replica - symmetric assumption , because although the replica - symmetric solution is stable , the entropy is negative , violating the fact that for a system with discrete degrees of freedom , the entropy should be non - negative .",
    "according to arguments in refs .",
    "@xcite , there exists a discontinuous transition before the entropy crisis takes place , since the transition can not continuously emerge from a stable replica - symmetric solution . without further solving complex more - steps replica symmetric breaking equations",
    ", we adopt an alternative explanation of this entropy crisis . under the current context , if the data size is large enough , the data would shrink the feature space to a sub - exponential regime where the number of candidate features is not exponential with @xmath3 any more . in this case",
    ", we encounter the entropy crisis .",
    "therefore , the entropy crisis separates an exponential regime from a sub - exponential regime .",
    "the transition for @xmath91 occurs within the exponential regime if the feature strength is strong enough ( large @xmath11 ) .",
    "instances of size @xmath108 .",
    "( a ) entropy per neuron versus data density @xmath75 .",
    "the lines are replica result , while the symbols are results obtained on single instances .",
    "the error bars are smaller than the symbol size .",
    "( b ) order parameters ( @xmath109 ) versus @xmath75 . @xmath93 as expected .",
    "the numerical simulations on single instances approximately match the theoretical predictions of replica computation .",
    ", title=\"fig : \" ] .05 cm   instances of size @xmath108 .",
    "( a ) entropy per neuron versus data density @xmath75 .",
    "the lines are replica result , while the symbols are results obtained on single instances .",
    "the error bars are smaller than the symbol size .",
    "( b ) order parameters ( @xmath109 ) versus @xmath75 . @xmath93 as expected .",
    "the numerical simulations on single instances approximately match the theoretical predictions of replica computation .",
    ", title=\"fig : \" ] .05 cm      we use the above mean field theory to analyze single realizations ( instances ) of the unsupervised learning model .",
    "random samplings are first generated according to a rbm distribution @xmath6  @xcite , where the energy is rescaled by the system size and the inverse temperature , which tunes difficulty level of the learning task .",
    "these random samplings then serve as the quenched disorder specifying the interaction between the feature vector and the example constraint ( fig .  [ rbm ] ) . finally , by initializing the message on each link of the factor graph ( fig .  [ rbm ] )",
    ", we run the smp equation ( eq .",
    "( [ bp0 ] ) ) until it converges within a prefixed precision . from the fixed point , we compute the entropy of consistent feature vectors and the overlap between the inferred feature vector and the true one .    we first compare results of the message passing algorithm with those obtained by replica computation . in fig .",
    "[ rbm - mf ] ( a ) , we show the entropy density versus the data density @xmath75 .",
    "the entropy characterizes how the number of candidate feature vectors compatible with the given data changes with the network size @xmath3 .",
    "the replica result predicts an entropy crisis , i.e. , the entropy becomes negative at some data size , but the negativity of the entropy is not allowed in a system with discrete degrees of freedom ( @xmath110 ) .",
    "this implies that , a discontinuous phase transition should be present before the crisis  @xcite .",
    "the results obtained by running smp coincide perfectly with the replica result .",
    "the entropy density decreases more rapidly with @xmath75 at larger @xmath11 .",
    "this is expected , because large @xmath11 indicates strong feature , thus to shrink the feature space to the same size , less data is required compared to the case of detecting weak feature ( small @xmath11 ) .    in fig .",
    "[ rbm - mf ] ( b ) , we show how order parameters change with @xmath75 .",
    "the simulation results agree with the replica prediction , in spite of the deviation caused by finite size effects . for @xmath111 ,",
    "a first continuous transition occurs at @xmath112 , which should be correct since the replica computation is stable and the entropy is positive there . at @xmath113 ,",
    "the system starts to have information about the embedded feature , and therefore , the overlap @xmath91 starts to increase even for a finite - size system .",
    "the asymptotic behavior of @xmath91 at a slightly larger @xmath75 ( @xmath114 ) is captured by @xmath115 , as already derived in the theory section ( sec .",
    "[ replicasrbm ] ) .",
    "as predicted by replica computation , at a larger value of @xmath116 , the entropy becomes negative .",
    "equivalently , at this @xmath75 , the entropy vanishes at a critical temperature , and thus the equilibrium is dominated by a finite number of lowest energy states ( so - called condensation phenomenon  @xcite ) .",
    "but the smp is still stable , therefore a one - step replica symmetry breaking solution should grow discontinuously from the replica symmetric solution , and this second discontinuous glass transition is expected before or at the crisis @xmath75 to resolve the entropy crisis . intuitively , we expect that the increasing data will freeze the value of synapses , thus vanishing entropy indicates that the feature space develops isolated configurations : each configuration forms a single valley in the free energy profile , while the number of these valleys is not exponential any more ( but sub - exponential ) . to prove this picture",
    ", one needs to go beyond the replica symmetric assumption .    for @xmath117 , the continuous transition takes place after the entropy crisis , which is incorrect .",
    "therefore , according to the above argument , a discontinuous transition should be expected at or before the crisis data size .",
    "our simulation result also confirms the nishimori condition ( @xmath93 ) , that is , when the temperature used to generate data is equal to that used to infer the true feature , the true feature follows the posterior distribution as well . as a consequence ,",
    "the overlap between a typical feature configuration and the embedded one is equal to the overlap between two typical configurations .    , @xmath118 .",
    "@xmath119 random instances are considered . ]    secondly , the inference can also be carried out by using amp with less requirements of computer memory and time .",
    "the result is compared with that obtained by smp , which is shown in fig .",
    "[ amprbp0 ] .",
    "we also study the effects of temperature deviation .",
    "if the inference is carried out in a different temperature from that used to generate the data , is the performance degraded ?",
    "we address this question by considering two different temperatures : one is slightly larger than the data temperature ( @xmath120 ) ; the other is slightly below the data temperature ( @xmath121 ) , where @xmath122 denotes the inverse data temperature . as shown in fig .",
    "[ rbmnopt ] , when @xmath123 , the performance is optimal in the inferable regime , compared to other inference temperatures , as expected from the nishimori condition  @xcite .",
    "large fluctuations around the transition point may be caused by finite size effects .",
    "finally , we explore the effect of network size keeping the identical feature strength ( fig .",
    "[ netsize ] ) . at a given number of examples , larger network size yields better performance in terms of prediction overlap",
    ". however , the performance seems to get saturated when @xmath124 at a relatively large @xmath12 in the current context .",
    "using a larger network seems to make the unsupervised learning better , but further increasing the network size has a little effect on the performance .    , @xmath125 .",
    "@xmath119 random instances are considered . ]    .",
    "feature strength @xmath126 is kept constant ( @xmath127 ) . ]      in fig .",
    "[ rbmnopt ] , we have showed the inference performance with slightly different inference temperatures .",
    "is it possible to infer the true temperatures used to generate the data itself ?",
    "if we can learn the temperature parameters , we can know the typical properties of phase transitions intrinsic in the system .",
    "this is possible by applying the bayesian rule once again .",
    "the posterior probability of @xmath11 given the data @xmath13 is given by @xmath128 where we used the uniform prior probability @xmath129 for the hyper - parameters .",
    "note that @xmath130 is the same partition function as in eq .",
    "( [ pobs ] ) .",
    "we maximize the posterior probability with respect to @xmath11 , and obtain the self - consistent equation @xmath11 should satisfy : @xmath131 the left hand side of the above equation is exactly the negative energy ( @xmath132 ) , which can be evaluated by smp equation ( eq .  ( [ bp0 ] ) ) . under the bethe approximation , the energy per neuron @xmath133 can be computed by @xmath134 , where @xmath135 and @xmath136 are given respectively by    [ energy ] @xmath137/\\left(\\beta+\\beta\\prod_{a\\in\\partial i}\\mathcal{g}_{a\\rightarrow i}\\right),\\\\ \\delta\\epsilon_a&=\\beta\\xi_{a}^2+g_a\\tanh(\\beta g_a).\\end{aligned}\\ ] ]    starting from some initial value of @xmath11 , one can iteratively update the value of @xmath11 until convergence within some precision .",
    "after one updating , the messages in smp equation are also updated . to avoid numerical instability , we used the damping technique , i.e. , @xmath138 , where @xmath66 denotes the iteration step and @xmath139 $ ] is a damping factor .",
    "it is not guaranteed that there exists unique maximum of the posterior ( eq .",
    "( [ pobsem ] ) )  @xcite , but if necessary , one can choose the hyper - parameter corresponding to the global maximum of the posterior by running the smp from different initial conditions .    in statistics , this iterative scheme is named expectation - maximization algorithm  @xcite , where the message updates are called e - step , and the temperature update is called m - step . in physics ,",
    "( [ betaem ] ) corresponds to the nishimori condition ( @xmath93 , see also appendix  [ replica - rbm ] for derivation of the energy function ) .",
    "this means that , in principle , the hyper - parameter can be learned by iteratively imposing the nishimori condition  @xcite . on the nishimori condition ,",
    "the state space of the model is simple  @xcite , and thus smp yields informative information about the dominant feature vector .",
    "we know that the temperature parameter is related to the feature strength embedded in the data .",
    "once we learn the temperature , we are able to know how apparent the hidden feature is in a dataset , and determine the critical data size for unsupervised feature learning .",
    "we first test our method in synthetic dataset as already studied in sec .",
    "[ simusrbm ] , where the true value of hyper - parameter is known .",
    "we then infer the embedded feature strength in the real dataset ( mnist handwritten digit dataset  @xcite ) , where we do not have any knowledge about the true feature strength .",
    "results are shown in fig .",
    "[ rbm - em ] . for the synthetic data at @xmath140 , as the data size grows , inferred value of @xmath11 gets closer to the true value as expected ( fig .",
    "[ rbm - em ] ( a ) ) .",
    "as shown in the inset , the time ( iteration steps ) dependent inferred value first drops to a lower value , and then gradually approaches the true value . with a larger data size ( e.g. , @xmath141 ) , @xmath11 increases more rapidly after a sudden drop .",
    "for the real dataset ( fig .",
    "[ rbm - em ] ( b ) ) , we observe that the final fixed point of @xmath11 is quite large , implying that the feature strength in the handwritten digits is very strong ( @xmath142 ) .     in synthetic ( rbm ) and real ( mnist , digits @xmath143 and @xmath2 ) dataset .",
    "( a ) deviation of inferred @xmath11 from the true value decreases with the data size . in simulations ,",
    "we consider @xmath144 instances of size @xmath145 , and use @xmath146 and initial value of @xmath147 .",
    "two examplar trajectories of @xmath148 are shown in the inset .",
    "( b ) examplar trajectories of @xmath148 are shown for the real dataset .",
    "we use @xmath149 and @xmath150 .",
    "the fixed point does not change when we use @xmath151 .",
    ", title=\"fig : \" ] .05 cm   in synthetic ( rbm ) and real ( mnist , digits @xmath143 and @xmath2 ) dataset .",
    "( a ) deviation of inferred @xmath11 from the true value decreases with the data size . in simulations , we consider @xmath144 instances of size @xmath145 , and use @xmath146 and initial value of @xmath147 .",
    "two examplar trajectories of @xmath148 are shown in the inset .",
    "( b ) examplar trajectories of @xmath148 are shown for the real dataset .",
    "we use @xmath149 and @xmath150 .",
    "the fixed point does not change when we use @xmath151 .",
    ", title=\"fig : \" ] .05 cm",
    "it is interesting to show that one can also perform the same unsupervised learning task by using an associative memory ( hopfield ) model defined by @xmath152 where @xmath153 .",
    "this posterior distribution of feature vectors given the input examples can be obtained by a small-@xmath11 expansion of eq .",
    "( [ pobs ] )  @xcite .",
    "this relationship implies that one can infer the feature vector of a rbm by an approximate hopfield model , and the feature vector is interpreted as the stored pattern in the hopfield model , encoding memory characteristics of the input data .",
    "note that , @xmath154 are still governed by a rbm distribution , whereas , by applying the associative memory framework , we show many similar interesting properties of the unsupervised learning model .    in analogous to the derivation of eq .",
    "( [ bp0 ] ) , we have the smp corresponding to the posterior probability ( eq .  ( [ hopf ] ) ) : @xmath155 where @xmath156 , @xmath157 in which @xmath158 .",
    "details to derive eq .",
    "( [ bp5 ] ) are given in appendix  [ smp - hopf ] .",
    "in the approximate hopfield model , the bethe free energy can be constructed similarly , i.e. , @xmath39 , where    [ hopfz ] @xmath159+\\ln2\\cosh\\hb h_i,\\\\ \\ln z_a&=\\frac{\\hb}{2}\\hg^2_{a}f_a-\\frac{1}{2}\\ln(1-\\hb c_a),\\end{aligned}\\ ] ]    where we define @xmath160 , @xmath161 , @xmath162 , and @xmath163",
    ".    similar to the case in rbm , the entropy for the approximate model can be evaluated as @xmath49 , where single feature node contribution reads @xmath164\\\\ +\\ln\\left(2\\cosh(\\hb",
    "h_i)\\right)-(\\hb h_i+\\hb h'_i)\\tanh(\\hb h_i ) , \\end{split}\\ ] ] and single example contribution reads @xmath165 where @xmath166 , @xmath167 , and @xmath168 .",
    "we also derive amp equations for the hopfield model .",
    "note that @xmath169 .",
    "therefore @xmath170 , we thus derive the first amp equation for the hopfield model as @xmath171 where @xmath57 . from the definition of the local field @xmath60",
    ", we have @xmath172 in the large-@xmath3 expansion .",
    "finally , we derive the second amp equation : @xmath173    taking the time index into account , the amp equations can be summarized in a parallel update scheme as follows :    [ amphopf ] @xmath174      in this section , we derive the thermodynamic equation .",
    "similarly , the local field defined as @xmath175 follows a gaussian distribution with mean zero and variance @xmath176 .",
    "the variance can be derived by noting that @xmath177 .",
    "therefore , we have the following thermodynamic equation for the hopfield model : @xmath178 @xmath76 is a solution of eq .",
    "( [ hopfrm ] ) , however , it is stable only when @xmath179 ^ 2 $ ] .",
    "this threshold can be derived by expanding eq .",
    "( [ hopfrm ] ) around @xmath76 to the first order .",
    "interestingly , this equation matches the mean - field equation without ferromagnetic part ( related to retrieval phase ) derived in standard hopfield model  @xcite .",
    "gaussian assumption for messages does not generally hold , particularly for those messages related to the memorized patterns  @xcite .",
    "we also perform replica computation for the approximate hopfield model .",
    "the free energy function is given in the appendix  [ replica - hopf ] .",
    "the associated saddle - point equations are given as follows :    [ hopfreplica ] @xmath180    note that , the data is generated by the rbm , but the inference is carried out in an approximate hopfield model .",
    "although we use the same temperature , the model mismatching leads to @xmath181 .",
    "the threshold for the transition of @xmath92 can be determined by studying the linear stability around @xmath182 , and the result is @xmath183 ^ 2 $ ] , consistent with the cavity prediction .",
    "when @xmath75 approaches @xmath184 from above , @xmath92 behaves like @xmath185 .",
    "the transition for @xmath91 can only be determined numerically , since @xmath92 could not be assumed a small value , and it follows @xmath186 where @xmath187 .",
    "the recursive equation for @xmath92 in the regime of @xmath28 is exactly the equation derived from the smp equation ( eq .  ( [ hopfrm ] ) ) .",
    "the entropy of the model can also be similarly computed , and reads as follows : @xmath188      the thermodynamic properties of the approximate hopfield model are shown in fig .",
    "first , we show that the entropy crisis is absent in the hopfield model , although the inference is carried out by the smp equation of hopfield model .",
    "this is quite interesting , because within the associative memory framework , the inference is improved smoothly and there does not exist condensation in the feature space . secondly , the replica computation predicts @xmath181 , as expected from the fact that by applying hopfield model approximation , the nishimori condition does not hold .",
    "thirdly , the simulation results obtained by running smp equation agree with the theoretical predictions , in spite of observed fluctuations caused by finite size effects .",
    "the asymptotic behavior of @xmath92 near @xmath189 can be analytically determined by small-@xmath92 expansion of the saddle - point equation ( eq .  ( [ hopfreplica ] ) ) . as already derived in the theory section ( sec .",
    "[ replicaahopf ] ) , @xmath185 when @xmath75 tends to @xmath189 from above .",
    "the transition for @xmath91 can only be determined by numerically solving the saddle - point equation .",
    "it seems that @xmath91 changes smoothly to a non - zero value at the same data size as that of the rbm .",
    "due to the model mismatching , transition for @xmath92 takes place much more earlier than that for @xmath91 .",
    "instances of size @xmath108 . the feature strength @xmath190 .",
    ", title=\"fig : \" ] .05 cm    . @xmath119",
    "random instances are considered . ]    finally , the inference can also be carried out by using amp with less requirements of memory storage and computer time .",
    "the result is compared with that obtained by smp , which is shown in fig .",
    "[ amprbp1 ] .",
    "we also study the effects of temperature deviation . as shown in fig .",
    "[ hopfnopt ] , even when @xmath123 , it is not guaranteed that the performance is optimal in the inferable regime , compared to other inference temperatures .    , @xmath125 .",
    "@xmath119 random instances are considered . ]",
    "in conclusion , we build a physical model of unsupervised learning from a finite number of examples in the framework of rbm . here , we consider binary features rather than real - valued ones ; this is because binary features are more robust and efficient in large - scale neuromorphic applications  @xcite , yet it remains open to figure out an efficient algorithm .",
    "we show that physics method can inspire an efficient ( fully - distributed ) message passing procedure not only to infer the hidden feature embedded in a noisy data , but also to estimate the entropy of candidate features .",
    "distinct from conventional slow sampling - based methods , each example in this work is treated as a constraint on the factor graph , and the message passing carries out a direct bayesian inference of the hidden feature , which marks an important step implementing unsupervised learning in neural networks . in particular , the approximate message passing equation has low requirements of computer space and time in practical applications .    we show that , the results obtained by the cavity method are consistent with the statistical analysis by replica theory .",
    "the replica theory describes the thermodynamic properties of the unsupervised learning system .",
    "it first predicts a discontinuous phase transition in a restricted boltzmann machine , signaled by the entropy crisis before the message passing equation loses its stability .",
    "however , if the feature strength is strong enough , there exists another phase transition which is continuous , i.e. , the order parameter ( the overlap between the true feature vector and the inferred one ) smoothly changes from zero to non - zero value .",
    "this continuous transition will be followed by an additional discontinuous transition at a larger data size .",
    "interestingly , in an approximate hopfield model , the entropy crisis is absent , and the entropy decreases much more slowly towards zero .",
    "therefore , there exists a continuous transition from impossible - to - infer to inferable regime . unlike the rbm ,",
    "inference in the hopfield model does not satisfy the nishimori condition , and thus the statistics of metastable states would be very interesting , and its relationship with the dynamics of inference deserves further investigation .",
    "our work not only derives in detail various kinds of message passing algorithms in a bayesian framework for practical applications , but also statistically characterizes the thermodynamic properties of the restricted boltzmann machine learning with binary synapses , and its connection with associative memory networks .",
    "many interesting properties related to phase transitions are also revealed .",
    "in addition , we derive an iterative equation to infer the unknown temperature in the data , providing a quantitative measure of how cold a dataset is .",
    "this method corresponds to expectation - maximization algorithm in statistics  @xcite , and in physics iteratively imposing nishimori condition  @xcite .",
    "therefore , our study forms a theoretical basis of unsupervised feature learning in a single simple rbm , and are expected to be helpful in constructing a deep architecture for hierarchical information processing , which is currently under way .",
    "we first assume feature components on the factor graph are weakly correlated , then by using the cavity method  @xcite , we define a cavity probability @xmath191 of @xmath23 on a modified factor graph with example node @xmath17 removed . due to the weak correlation assumption , @xmath191 satisfies a recursive equation ( namely belief propagation ( bp ) in computer science  @xcite ) :    [ bp00 ] @xmath192    where the symbol @xmath193 indicates a normalization constant , @xmath194 defines the neighbors of node @xmath1 except constraint @xmath17 , @xmath195 defines the neighbors of constraint @xmath24 except visible node @xmath1 , and the auxiliary quantity @xmath196 represents the contribution from constraint @xmath24 to visible node @xmath1 given the value of @xmath23  @xcite .",
    "( [ bp00 ] ) has been similarly derived to understand rbm in a recent paper  @xcite . here",
    ", we exchange the role of the observed data @xmath22 and that of synaptic interaction ( feature vector here ) , and predict feature vector given the data .",
    "therefore the data ( random samplings ) , rather than the synaptic interaction in the previous work  @xcite , becomes a quenched disorder .",
    "note that in eq .",
    "( [ bp1 ] ) , the sum inside the hyperbolic cosine function with the @xmath1-dependent term excluded is a random variable following a normal distribution with mean @xmath71 and variance @xmath197  @xcite , where @xmath32 and @xmath198 . the cavity magnetization is defined as @xmath33 .",
    "thus the intractable sum over all @xmath199 ( @xmath200 ) can be replaced by an integral over the normal distribution . using the magnetization representation  @xcite , the bp equation ( eq .  ( [ bp00 ] ) )",
    "could be reduced to the simplified message passing equations ( see eq .",
    "( [ bp0 ] ) in the main text ) .    in physics , the contribution from a single feature node to the partition function ,",
    "@xmath201 is obtained via cavity method as @xmath202 ; the contribution of a single data node reads @xmath203 , which can be further computed by applying the central - limit theorem as well .",
    "this calculation is exact only when the underlying factor graph is a tree .",
    "however , it is approximately correct when correlations among feature components are weak .",
    "it needs to be compared with numerical simulations and replica computations .",
    "first , we compute @xmath204 . by noting that @xmath205 , @xmath206 , and @xmath207 , we have @xmath208 where we have used the fact that @xmath209 and @xmath210 .",
    "analogously , @xmath211 is simplified to be @xmath212 collecting the above results , we arrive at the final simplified entropy as @xmath99 .",
    "for the approximate hopfield model , we similarly define the auxiliary quantity @xmath196 as @xmath213 ^ 2\\right)\\\\    & = \\frac{1}{\\sqrt{1-\\hb c_{b\\rightarrow i}}}\\exp\\left(\\frac{\\hb f_{b\\rightarrow i}}{2}\\bigl(\\frac{1}{n}+\\hg_{b\\rightarrow i}^{2}\\bigr)\\right)\\exp\\left(\\frac{\\hb\\hg_{b\\rightarrow i}\\xi_i\\sigma_i^{b}f_{b\\rightarrow i}}{\\sqrt{n}}\\right ) ,   \\end{split}\\ ] ] where @xmath214 .",
    "@xmath191 is the same as that in rbm .",
    "using eq .",
    "( [ bp0hopf ] ) , the cavity magnetization @xmath34 can thus be derived as eq .",
    "( [ bp5 ] ) .",
    "we first define @xmath215 , and @xmath216 . both @xmath217 and @xmath218 are random variables subject to the covariance structure : @xmath219 , @xmath220 , @xmath221 , @xmath222 , @xmath223 , @xmath224 , where we have dropped off the data index @xmath17 because of independence among data samples , and defined the overlap between true feature vector and the estimated one as @xmath225 , and the overlap between two estimated feature vectors as @xmath226 . under the replica symmetric assumption , @xmath227 and @xmath228 , after introducing the definition of @xmath229 ( and @xmath230 ) as a delta function , @xmath86 can be estimated as @xmath231\\\\ \\times\\exp\\left[\\alpha n\\ln \\left\\{e^{-\\beta^2/2}\\int dy\\int dt\\cosh\\beta t(\\cosh\\beta(qt+\\sqrt{r - q^2}y))^n\\right\\}\\right ] , \\end{split}\\ ] ] where we have written @xmath232 ( @xmath233 and @xmath234 are standard gaussian random variables ) .",
    "finally , we arrived at the following free energy function : @xmath235 the saddle - point equation for the order parameters @xmath236 can be derived from @xmath237,@xmath238 , @xmath239 , and @xmath240 .",
    "note that to derive the entropy formula , we used @xmath241 , where @xmath242 is the inverse temperature at which the data is generated , and @xmath11 the temperature at which the bayesian inference is carried out .",
    "for the approximate hopfield model , we replace @xmath243 with @xmath244 in eq .",
    "( [ zn ] ) .",
    "the subsequent calculation proceeds similarly to the appendix  [ replica - rbm ] .",
    "analogously , we have @xmath245\\\\ \\times\\exp\\left[\\alpha n\\ln \\left\\{e^{-\\beta^2/2}\\int dy\\int dt\\cosh\\beta t\\bigl(\\frac{1}{\\sqrt{1-\\hb(1-r)}}e^{\\frac{\\hb(qt+\\sqrt{r - q^2}y)^2}{2(1-\\hb(1-r))}}\\bigr)^n\\right\\}\\right ] . \\end{split}\\ ] ] using the replica trick defined in eq .",
    "( [ replica ] ) , we obtain the free energy function @xmath246 where we used the identity @xmath247",
    "^ 2\\cosh\\beta t = e^{\\hb/2}(\\hb q^2+r)$ ] .",
    "the saddle - point equations can be derived similarly .",
    "the smp for rbm can be easily generalized to take into account external fields of visible neurons and the hidden neuron . here , for simplicity , we consider only the case of hidden neuron with external field .",
    "the external field has binary values as well , defined by @xmath248 ( @xmath249 ) .",
    "the only modification to the factor graph in fig .",
    "[ rbm ] is to add one additional variable node named by @xmath250 for the unknown external field .",
    "the additional node @xmath250 is connected to all data nodes . following the similar procedure as in appendix  [ smp - rbm ] , we obtain the following four kinds of messages :      once @xmath252 , the original smp for the rbm in the main text is recovered .",
    "to derive smp for the case of visible neurons with external fields , an additional central limit theorem applies to the interacting external fields ( @xmath253 ) , which leads to introducing a joint cavity probability @xmath254 as well as @xmath255 where @xmath256 .",
    "i am very grateful to taro toyoizumi , lukasz kusmierz , alireza goudarzi and roberto legaspi for attending a series of lectures about this work and their useful feedback .",
    "i thank lukasz kusmierz for a careful reading of the manuscript and his useful feedback .",
    "this work was supported by the program for brain mapping by integrated neurotechnologies for disease studies ( brain / minds ) from japan agency for medical research and development , amed ."
  ],
  "abstract_text": [
    "<S> revealing hidden features in unlabeled data is called unsupervised feature learning , which plays an important role in pretraining a deep neural network . here </S>",
    "<S> we provide a statistical mechanics analysis of the unsupervised learning in a restricted boltzmann machine with binary synapses . </S>",
    "<S> a message passing equation to infer the hidden feature is derived , and furthermore , variants of this equation are analyzed . </S>",
    "<S> a statistical analysis by replica theory describes the thermodynamic properties of the model . </S>",
    "<S> our analysis confirms an entropy crisis preceding the non - convergence of the message passing equation , suggesting a discontinuous phase transition as a key characteristic of the restricted boltzmann machine . </S>",
    "<S> continuous phase transition is also confirmed depending on the embedded feature strength in the data . </S>",
    "<S> the mean - field result under the replica symmetric assumption agrees with that obtained by running message passing algorithms on single instances of finite sizes . </S>",
    "<S> interestingly , in an approximate hopfield model , the entropy crisis is absent , and a continuous phase transition is observed instead . </S>",
    "<S> we also develop an iterative equation to infer the hyper - parameter ( temperature ) hidden in the data , which in physics corresponds to iteratively imposing nishimori condition . </S>",
    "<S> our study provides insights towards understanding the thermodynamic properties of the restricted boltzmann machine learning , and moreover important theoretical basis to build simplified deep networks . </S>"
  ]
}