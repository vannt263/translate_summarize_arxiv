{
  "article_text": [
    "with the recent advancements in deep learning , many image processing tasks that were once thought impossible are now possible .",
    "one such task is that of generating completely random textures that visually look similar to a given sample texture .",
    "recently gatys et al .",
    "[ 1 ] experimentally showed that by utilizing a specific energy function that uses the first few layers of a pretrained cnn , natural textures can be effectively generated .",
    "later champandard [ 2 ] used a different energy function and random weights to generate textures .",
    "he et al . [ 3 ] used the same method as in gatys et al .",
    "[ 1 ] with random weights , to generate nice textures . around the same time , ustyuzhaninov et al . [ 4 ] experimentally showed that less variable natural textures can be generated using just one layer of a cnn with random filters along with the same energy function .",
    "although these methods work quite well there is no theoretical analysis why these methods ( in particular the choice of energy functions and also use of random weights ) would be helpful for texture generation even in the most basic settings such as in [ 4 ] .",
    "this leads us to ask a natural question . what theoretical properties lead to the fact that we can generate random textures just using a one layer cnn with random filters ?",
    "in section 3 with a slight adjustment to the one layer cnn architecture , we show rigorously why random weights in one layer cnns without a nonlinearity , can be used to generate random textures with the same performance as with pre - trained weights , while also drawing connections to previous work in texture generation . in section 4 ,",
    "we show how the behavior of generated images changes when one adds a relu non-linearity.we show that in the case of a relu non - linearity there are often cases where there is only one solution that gives the minimum energy ( the input itself ) , whereas in the case with no nonlinearity there are infinite inputs that give the minimum energy .",
    "we now review a subset of the models used by ustyuzhaninov et al .",
    "[ 4 ] that we later analyze theoretically .",
    "in particular the subset corresponds to the models which use random weights .",
    "the model employs a single - layer cnn with standard rectified linear units(relus ) and convolutions with stride one , no bias and padding ( f-1)/2 where f is the filter - size ( f is always an odd number ) .",
    "this choice of padding will ensure that the spatial dimension of the output is the same as the spatial dimension of the input .",
    "in addition , 363 filters of dimensions 11 @xmath0 11 @xmath0 3 ( filter width , filter height , number of input channels ) are randomly generated from a uniform distribution according to [ 5 ] .",
    "for an original texture image @xmath1 , a matrix @xmath2 is formed ,    @xmath3    where @xmath4 corresponds to the output of the ith filter ( after the nonlinearity ) at location j.    to generate new textures corresponding to an original image @xmath1 , an energy function @xmath5 is developed ,    @xmath6    an image y is initialized randomly to values between 0 and 1 .",
    "then @xmath7 is changed according to gradient descent until @xmath7 is a local minimum with respect to the energy function .",
    "note that the smallest this energy can be is zero .",
    "in this section we consider the same model as above with two modifications .",
    "we consider a model where there is no non - linearity .",
    "we also use circular convolution rather than valid convolution as done in saxe et al . [ 6 ] .",
    "we study the behavior of the algorithm without a relu non - linearity to gain insight into what is happening or not happening in the non - linear case .",
    "in addition , if the random weights in the model of ustyuzhaninov et al . [ 4 ] were to be positive , then having a relu nonlinearity would be equivalent to using no non - linearity .",
    "this is because an image signal comes in values between 0 and 255 .",
    "thus the convolution of positive filters and an image signal will be positive .",
    "we use circular convolution , as in [ 6 ] , rather than valid convolution because we can more easily get a theoretical analysis .",
    "intuitively doing circular convolution in a texture image is not too different than doing valid convolution because the statistics are usually uniform across texture images .",
    "an important fact to note is that the output of a circular convolution of an original image and filter @xmath8 can be written as a matrix multiplication where the matrix is a function of the filter @xmath8 .",
    "these matrices are called block circulant with circulant blocks ( bccb ) matrices .",
    "these matrices can be diagonalized in the form    @xmath9    where @xmath10 is the discrete fourier basis in two dimensions [ 7].thus the output of convolving an image @xmath1 with a filter @xmath8 is    @xmath11    where @xmath1 has been appropriately vectorized , and an entry of @xmath2 can be re - expressed in the following way ,    @xmath12    where @xmath13 is the is the kth fourier coeffecient corresponding to the kth basis vector in @xmath10 such that    @xmath14    ideally we would like to generate all the images @xmath7 such that @xmath15 .",
    "this would correspond to an energy of zero . from the equations above",
    "we can see that finding a @xmath7 such that @xmath16 would come down to solving a system of linear equations . to be clear",
    "the system of equations would be    @xmath17    in matrix form the linear system of equations would look like    @xmath18    or @xmath19 is unique up to the rows of m. this has some nice implications .",
    "let s define    @xmath20    since we have constraints of the form    @xmath21    it implies the possible solutions of @xmath22 must lie on the @xmath23 dimensional space where equation 11 holds.thus @xmath22 lies within an intersection of roughly @xmath24 , @xmath23 dimensional spaces that have the same projection on to the frequency domain of the filters as does @xmath25 .",
    "notice if this low rank system were due to random weights then the possible solutions would lie within a different intersection of @xmath23 dimensional spaces .",
    "thus using random weights would yield distinctly different images than would using pre - trained weights in the case where the number of filters is not large enough to create a full rank matrix @xmath26 .",
    "notice that even if this system of equations is full rank , the solution to the linear system only gives us the magnitude of each fourier coefficient . also notice that the number of equations is approximately @xmath24 . thus if we have a relatively small number random filters compared to the dimension of @xmath1 , the matrix m can become full rank .",
    "if m is full rank and m is created either by random weights or pretrained weights , the solutions @xmath7 that yield energy zero are characterized by images for which the fourier coefficient magnitudes are the same as the fourier coefficient magnitudes of the image @xmath1 .",
    "this is because we already know the fourier coefficients of the image @xmath1 already satisfy the equations from 7 and thus @xmath27 for any @xmath28 .",
    "thus in the case that there are enough random weights and pre - trained weights , the solution @xmath7 to the system of equations is no different than the solutions @xmath7 to the system of equations generated by pretrained weights .    in figure 1",
    "we show textures generated by simply randomizing phase of fourier coefficients .",
    "one can notice that the ridges happen in completely different places .",
    "one issue with this however is that one can see vertical and horizontal lines appearing in the generated textures .",
    "this is due to the fact that the fft assumes images are periodic and in fact may suggest the need for nonlinearities .",
    "in the case where there is no nonlinearity , we show that in all cases there are infinite solutions that will yield energy equal to zero ( the results above also hold for 1-d vector signals by simply letting @xmath10 be discrete fourier transform for one dimensional signals ) .",
    "since in [ 4 ] a relu is used , the results above beg the question , does the same behavior hold in the case where there is a relu non - linearity .",
    "we show below that for almost all 1-d vector signals(as opposed to 2-d image signals ) if one samples enough random filters that there is only one solution that yields energy zero .",
    "first we set some notation .",
    "for an @xmath29 , let @xmath30 denote the circulant matrix corresponding to x. in other words the first row of @xmath30 is x. the second row is x circularly shifted by one and so on . let the set @xmath31 be all the vectors @xmath32 such that in the matrix - vector multiplication @xmath33 has only the @xmath34 element greater than zero .",
    "let @xmath35 , @xmath36 , ... , @xmath37 be linearly independent vectors .",
    "then the intersection of the half planes decided by these n vectors has non - zero volume",
    ".    denote the half plane restrictions by @xmath38 where the rows of c correspond to the vectors setting the half planes .",
    "@xmath39 is full rank because the rows are linearly independent .",
    "now let @xmath40 where @xmath41 are the canonical basis vectors of a euclidean vector space . notice that as a result the @xmath42 will be linearly independent and thus form a basis for @xmath43 because there are @xmath44 such vectors .",
    "thus any @xmath1 of the form    @xmath45    where @xmath46 , satisfies the half plane restrictions . since the region of possible @xmath1 is a convex cone constructed from basis vectors that span the whole euclidean space , the region of vectors x that satisfy the half - plane restrictions has non - zero volume .",
    "corollary : as long as @xmath30 is full rank , @xmath31 is a set that has nonzero volume in @xmath43 for any @xmath47 . in other words , if you were to uniformly sample vectors from the unit ball , there is a finite probability that these vectors would be in the set @xmath31 for any @xmath47 .    for any @xmath2",
    "there are n vectors @xmath7 such that @xmath15 .",
    "in particular , any y that is just a circularly shifted version of x will have a gram matrix @xmath48 .",
    "this holds for any kind of non - linearity used in the network .",
    "consider @xmath49 to be the circulant matrix corresponding to i - th convolution filter .",
    "then , output feature map corresponding to i - th convolution filter for input vector x can be written as @xmath50 .",
    "we ll use h ( . ) to represent the non - linearity that can be applied to any scalar or element - wise to any vector / matrix . using this notation",
    ", we can say that @xmath51 . where @xmath52 is the @xmath53 row of @xmath54 .",
    "now consider an operator cs such that , cs(x ) circularly shifts x by one e.g. if @xmath55 $ ] , then @xmath56 $ ] . here ,",
    "n is the length of vector x , @xmath57 is the dot product between vectors a and b. then , @xmath49 being circulant implies that @xmath58 $ ] and @xmath59 . also , it is straightforward to see @xmath60",
    ". hence , we have @xmath61 \\end{split}\\ ] ] and @xmath62 this is true for all i in range [ 1,n ] where n is the number of filters . hence , from eq . [ gram_eq ] and eq .",
    "[ f1 ] , [ f2 ] , we have @xmath63 hence , @xmath64 .",
    "hence , we get at least as many solutions as the possible circulations of x.    using the same notation set before theorem 1 , suppose we have @xmath44 sets of filters .",
    "let each set @xmath65 contain @xmath44 filters ( or equivalently vectors ) each of which @xmath66 for a unique @xmath47 . in others",
    "words , if @xmath67 , then if @xmath68 for some @xmath47 then @xmath69 . denote the set @xmath70 as the set of vectors @xmath71 .",
    "let the vectors in each @xmath70 be linearly independent . under these conditions ,",
    "if @xmath15 for some vector @xmath7 , then @xmath7 is equal to @xmath1 or some circularly shifted version of @xmath1 .      given that we want @xmath91 , it implies the first diagonal @xmath77 block of @xmath92 must also be diagonal .",
    "this implies that for any @xmath93 , @xmath94 because if it were greater than 1 , then the @xmath77 block of @xmath92 will have non diagonal non - zero elements .",
    "if @xmath95 , then of course the @xmath77 block would be the zero matrix .",
    "this means that the only equations that characterize the solutions are those that correspond to the diagonal terms of the first block of @xmath78 .",
    "these equations are @xmath96 . since @xmath97 , there are @xmath44 possible equations",
    "@xmath98y = \\sqrt{g^{s1}_{qq}}$ ] that could generate consistent vectors @xmath7 , where @xmath28 corresponds to some row in the matrix ( filter ) @xmath99 .",
    "performing the same reasoning using the other sets of filters @xmath65 , we end up with @xmath44 possible equations for each filter that characterize a solution y. @xmath100 y = \\sqrt{g_{qq}^{s_{j}}}$ ] .",
    "note that the non - diagonal blocks of @xmath78 happen to be the inner product of the output of filters coming from @xmath101 for some @xmath102 and @xmath47 .",
    "these blocks contain also one non zero value per row for the same reason that make the diagonal blocks diagonal matrices .",
    "the value of knowing for which pairs of @xmath103 and @xmath104 produce a non - zero value is that it lets us know which matrices are aligned . in other words if the entry in @xmath2 corresponding to @xmath103 and @xmath105 is greater than zero , then if @xmath106 y = \\sqrt{g^{s_{j}}_{q2q2}}$ ] , then @xmath107 y= \\sqrt{g^{s_{i}}_{q1q1}}$ ] .",
    "notice we used the same index @xmath28 .",
    "suppose we have aligned the above 2 filters .",
    "then we can align those with @xmath108 other filters coming from other @xmath65 , by looking in the appropriate non diagonal blocks of @xmath2 .",
    "since we have aligned the filters appropriately , we now have n possible system of equations since we can choose n values of k. we note under the conditions specified in theorem 3 , that these system of equations is full rank , and can thus only have one solution .",
    "one can also see that each of these system of equations just gives a circularly shifted solution of @xmath1 .",
    "this phenomenon corresponds to theorem 2 .",
    "suppose we generate @xmath109 random filters from the infinity norm 1 ball and suppose @xmath110 is full rank .",
    "as @xmath28 approaches infinity , the probability that there is unique solution to @xmath15 ( up to a circular shift ) approaches one .",
    "let @xmath111 be the probability that a random vector lands in @xmath112 respectively .",
    "we know @xmath111 is greater than zero because of theorem 1 . let @xmath113 where @xmath114 is some rational number .",
    "let @xmath115 be the number of times we get filters in @xmath31 in the @xmath34 set of @xmath28 draws .",
    "@xmath116 this follows from classical chernoff bound results .",
    "notice the term @xmath117 approaches 1 as @xmath114 approaches infinity .",
    "thus    @xmath118    which approaches 1 as c approaches infinity . in addition , given that we have sampled more than @xmath44 vectors that are in @xmath31 for some @xmath47 , if we choose n of those vectors they are going to be linearly independent with probability one since the set of linearly dependent vectors in @xmath31 has measure zero.thus as @xmath114 approaches infinity we have the conditions noted in theorem 3 with probability one .",
    "one can find the sets @xmath119 through exhaustive search of the gram matrix and then execute the proof for theorem 3 and come to the conclusion that if @xmath15 for some vector @xmath7 , then @xmath7 is equal to @xmath1 or some circularly shifted version of @xmath1 .",
    "galerne et al . [ 8 ] show that an algorithm called random phase noise ( rpn ) is effective at generating realistic textures of a certain class . in the most basic version of their algorithm ,",
    "the authors take texture like images and generate new textures simply by randomizing the phase of the fourier coefficients.as long as there are enough filters to create a full rank system , the slightly modified model in section 3 is mathematically equivalent to randomizing phase .",
    "thus we expect that even our modified model(with no nonlinearity ) should produce good random textures , which is in fact what we observe in figure 1 .",
    "the authors note the same horizontal and vertical line artifacts with this algorithm as we do in our experiments and thus do some extra image processing to get textures that do not have line artifacts .",
    "we have shown rigorously that in a slightly simplified model compared to that of [ 4 ] ( with circular convolution and no relu nonlinearity ) , there is no need for pretrained weights . to be precise ,",
    "if the number of weights is small , then the images generated with pretrained weights and random weights will be different from each other .",
    "however , as the number of weights increases , random or pretrained weights make no difference in the images generated .",
    "we show that the images generated from a large number of weights are just modified versions of the original image .",
    "frequency coefficient magnitudes are preserved , whereas frequency coefficient phases are randomized .    using the lack of a nonlinearity as a starting point and the fact that in [ 4 ] a relu nonlinearity is used , we seek to see if having a relu would generate a set of signals with similar properties as in the no relu case .",
    "we show rigorously in the one - dimensional case that there are conditions under which the only signals @xmath120 that can be generated with the same gram matrix as the original signal @xmath121 are circularly shifted versions of @xmath121 .",
    "this is distinctly different from the linear case where there would be an infinite number of solutions .",
    "we view these results as giving a theoretical starting point to understand the contribution of nonlinearities , random weights , as well as the gram matrix energy function in texture generation .",
    "to be clear though , there is room for future work .",
    "since the energy function is non - convex with respect to input images , input images may be caught in a local minima .",
    "we have no way to get a handle on what type of images these local minima might produce .",
    "secondly we have not shown in full generality what happens in the relu case .",
    "we have shown just sufficient conditions under which the solutions to a relu model and linear model differ greatly .",
    "thirdly our analysis does not yet account for multiple layers . on the other hand",
    ", the analysis presented above may provide guiding intuition for multiple layers since the output of each layer is just a linear convolution and relu of the previous layer .",
    "glorot , xavier , and yoshua bengio .",
    "`` understanding the difficulty of training deep feedforward neural networks . '' in aistats , vol . 9 , pp . 249 - 256 .",
    "saxe , andrew , pang w. koh , zhenghao chen , maneesh bhand , bipin suresh , and andrew y. ng .",
    "`` on random weights and unsupervised feature learning . '' in proceedings of the 28th international conference on machine learning ( icml-11 ) , pp .",
    "1089 - 1096"
  ],
  "abstract_text": [
    "<S> `` recent work in the literature has shown experimentally that one can use the lower layers of a trained convolutional neural network ( cnn ) to model natural textures . </S>",
    "<S> more interestingly , it has also been experimentally shown that only one layer with random filters can also model textures although with less variability . in this paper </S>",
    "<S> we ask the question as to why one layer cnns with random filters are so effective in generating textures ? </S>",
    "<S> we theoretically show that one layer convolutional architectures ( without a non - linearity ) paired with the an energy function used in previous literature , can in fact preserve and modulate frequency coefficients in a manner so that random weights and pretrained weights will generate the same type of images . based on the results of this analysis we question whether similar properties hold in the case where one uses one convolution layer with a non - linearity . </S>",
    "<S> we show that in the case of relu non - linearity there are situations where only one input will give the minimum possible energy whereas in the case of no nonlinearity , there are always infinite solutions that will give the minimum possible energy . </S>",
    "<S> thus we can show that in certain situations adding a relu non - linearity generates less variable images . </S>",
    "<S> + key words - texture generation , cnn , random weights </S>"
  ]
}