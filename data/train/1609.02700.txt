{
  "article_text": [
    "since their beginnings about half a century ago @xcite , bayesian optimization algorithms have been increasingly used for derivative - free global minimization of expensive to evaluate functions . typically assuming a continuous objective function @xmath0 , single - objective bayesian optimization algorithms consist in sequentially evaluating @xmath1 at promising points under the assumption that @xmath1 is a sample realization ( _ path _ or _ trajectory _ ) of a random field @xmath2 .",
    "such algorithms are especially popular in the case where evaluating @xmath3 requires heavy high - fidelity numerical simulations ( or _ computer experiments _ , see notably @xcite ) , where @xmath4 stands for some design parameters to be optimized over .",
    "such expensive simulations are classically encountered in the resolution of partial differential equations from physical sciences , engineering and beyond @xcite . in recent years",
    ", bayesian optimization also has attracted a lot of interest from the machine learning community @xcite , be it to optimize simulation - based objective functions @xcite or even to estimate tuning parameters of machine learning algorithms themselves @xcite . in both communities , a gaussian random field ( or _ gaussian process _ , gp ) model is often used for @xmath5 , so that prior information on @xmath1 is taken into account through a trend function @xmath6 and a covariance kernel @xmath7 .",
    "once @xmath8 and @xmath9 are specified , possibly up to some parameters to be inferred based on data , the considered gp model can be used as an instrument to locate the next evaluation point(s ) via so - called infill sampling criteria , also referred to as _ acquisition functions _ or simply as _",
    "criteria_. while a number of bayesian optimization criteria have been proposed in the literature ( see , e.g. , @xcite and references therein ) , we concentrate here essentially on the _ expected improvement _ ( ei ) criterion @xcite and on variations thereof , with a focus on its use in synchronous batch - sequential optimization .",
    "denoting by @xmath10 points were @xmath1 is assumed to have already been evaluated and by @xmath11 a batch of candidate points where to evaluate @xmath1 next , the multipoint ei is defined as @xmath12 where @xmath13 refers to the conditional expectation knowing the event @xmath14 .",
    "one way of calculating such criterion is to rely on monte carlo simulations .",
    "figure  [ fig : explanation ] illustrates both what the criterion means and how to approach it by simulations , relying on three samples from the multivariate gaussian distribution underlying equation  .",
    "our main focus here , in contrast , is on deriving equation   in closed form , studying the criterion s differentiability , and ultimately calculating and efficiently approximating its gradient in order to perform efficient batch optimization using derivative - based deterministic search .     for @xmath15 , @xmath16 , @xmath17 .",
    "left : gaussian process prediction of a function @xmath1 from observations @xmath18 ( depicted by black crosses ) .",
    "the green horizontal line stands for @xmath19 , the smallest response value from @xmath18 .",
    "three conditional simulation draws are plotted in orange and various point symbols represent their respective values at two unobserved locations @xmath20 and @xmath21 .",
    "right : distribution of the random vector @xmath22 knowing @xmath18 ( black contours ) . for each point",
    "symbol , the length of the purple segment represents the improvement realized by the corresponding sample path .",
    "the multipoint ei is the expectation of this length , or in other words , it is the integral of the improvement ( grey - scale function ) with respect to the conditional distribution of @xmath22 knowing @xmath18 . ]",
    "now , for @xmath23 , it is well known that ei can be expressed in closed form as a function of @xmath24 and @xmath25 as follows @xmath26 where @xmath27 ( defined for @xmath28 ) and @xmath29 are the cumulative distribution function and probability density function of the standard gaussian distribution , respectively .",
    "when deriving equation  , equation   happens ( hence for @xmath23 ) to involve a first order moment of the truncated univariate gaussian distribution .",
    "as shown in @xcite and developed further here , it turns out that equation   can be expanded in a similar way in the multipoint case ( @xmath30 ) relying on moments of truncated gaussian vectors .",
    "this is essential for the open challenges tackled here of efficiently calculating and optimizing the multipoint criterion of equation  .",
    "the applied motivation for having batch - sequential ei algorithms is strong , as distributing evaluations of bayesian optimization algorithms over several computing units allows significantly reducing wall - clock time and with the fast popularization of clouds , clusters and gpus in recent years it is becoming always more commonplace to launch several calculations in parallel . even at a slightly inflated price and",
    "scripting effort , reducing the total time off is often a primary goal in order to deliver conclusions involving heavy experiments , be they numerical or laboratory experiments , in studies subject to hard time limitations .",
    "obviously , given its practical importance , the question of parallelizing ei algorithms and alike by selecting @xmath31 points per iteration has been already tackled in a number of works from various disciplinary horizons ( including notably @xcite ) .",
    "here we essentially focus on approaches relying on the maximization of equation   and related multipoint criteria .",
    "the multipoint ei of equation   has been defined in @xcite and first calculated in closed form for the case @xmath17 in @xcite . for the case",
    "@xmath32 , a monte carlo scheme and some sub - optimal batch selection strategies were proposed .",
    "further work on monte carlo simulations for multipoint ei estimation can be found in @xcite ; besides this , stochastic simulation ideas have been explored in @xcite for maximizing this multipoint ei criterion via a stochastic gradient algorithm , an approach recently investigated in @xcite . meanwhile , a closed - form formula for the multipoint ei relying on combinations of @xmath33-and @xmath34-dimensional gaussian cumulative distribution functions was obtained in @xcite , a formula which applicability in reasonable time is however restricted to moderate @xmath34 ( say @xmath35 ) in the current situation . building upon @xcite ,",
    "@xcite recently calculated the gradient of the multipoint ei criterion in closed form and obtained some first experimental results on ( non - stochastic ) gradient - based multipoint ei maximization .",
    "our aim in the present paper is to present a set of novel analytical and numerical results pertaining to the calculation , the computation , and the maximization of the multipoint ei criterion . as most of these novel results apply to a broader class of criteria , we first present in section  [ sec : framework ] a generalization of the multipoint ei that allows accounting for noise in conditioning observations and also exponentiating the improvement .",
    "this generalized criterion is calculated using moments of truncated gaussian vectors in the flavour of @xcite .",
    "the obtained formula is then revisited in the standard case ( noise - free with an exponent set to @xmath36 ) , leading to a numerical approximation of the multipoint ei with arbitrary precision and very significantly reduced computation time .",
    "next , the @xmath37-dimensional maximization of the multipoint ei criterion is discussed in section  [ sec : discussion ] , where the differentiability of the generalized criterion is studied , its analytical gradient is calculated , and further numerical approaches for fast gradient approximations with controllable accuracy are presented . finally , section  [ sec : application ] is dedicated to numerical experiments where , in particular , a multistart derivative - based multipoint ei maximization algorithm highlighting the benefits of the considered methodological principles and the proposed fast approximations is tested and compared to baseline strategies .",
    "throughout this section the objective function @xmath1 may be observed noise - free or in noise , meaning that at some arbitrary iteration @xmath38 the observed value may be @xmath39 or @xmath40 where @xmath41 is a realization of a zero mean gaussian random variable with known ( or estimated and plugged - in ) variance .",
    "@xmath1 is assumed to be one realization of a random field @xmath5 , where @xmath5 has a gaussian random field ( grf ) distribution conditionally to events of the form @xmath42 ( with conditioning on @xmath43 in the noisy case , see for instance @xcite ) .",
    "this setup naturally includes the case where @xmath5 is a grf , but also the so - called universal kriging settings where @xmath5 is the sum of a trend with an improper prior and a grf @xcite . note that in noisy cases the @xmath41 s are generally assumed to be independent ( although the case of @xmath41 s forming a gaussian vector is tractable ) , but more essentially they are assumed independent of @xmath5 .",
    "in batch - sequential bayesian optimization we are interested in computing sampling criteria @xmath44 depending on @xmath45 new points @xmath46 . at any step of corresponding synchronous parallel algorithms , the next batch of @xmath34 points @xmath47",
    "is then defined by globally maximizing @xmath44 over all possible batches : @xmath48 values of such criteria typically depend on @xmath49 through the conditional distribution @xmath50 , simplifying to @xmath51 in the noiseless context .",
    "conditional mean and covariance functions are analytically formulated via the so - called _ kriging equations _ , see e.g. @xcite . working out these criteria",
    "thus generally boils down to gaussian vector calculus , which may become intricate and quite cumbersome to implement as @xmath34 ( or @xmath52 , in noisy settings ) increases .",
    "our considered generalized version of the multipoint ei criterion , that allows accounting for a gaussian noise in the conditioning observations and also for an exponentiation in the definition of the improvement , is defined as : @xmath53 where @xmath54 , @xmath55 and @xmath56 .",
    "this form gathers several sampling criteria notably including @xmath57 , both in noiseless and noisy settings , and also a multipoint version of the generalized ei of @xcite .",
    "in addition , the obtained results apply to batch - sequential versions of the expected quantile improvement  @xcite ( eqi ) and variations thereof , by a simply change of process from @xmath5 to the quantile process",
    ". we will show in proposition [ proposition : analyticqei ] that such generalized multipoint ei criteria can be formulated as a sum of moments of truncated gaussian vectors . in the next subsection , in order to get a closed form for the generalized ei we first define these moments and derive some first analytical formulas , that might also be of relevance in further contexts .",
    "we fix @xmath58 and @xmath59 in noisy settings or @xmath60 in noiseless settings .",
    "+    let @xmath61 be a gaussian vector with mean @xmath62 and covariance matrix @xmath63 , where @xmath64 is the cone of positive definite matrices of @xmath65 .",
    "for all positive integer @xmath66 , we define the function @xmath67 on @xmath68 by @xmath69 where the inequality @xmath70 is to be interpreted component - wise .",
    "if @xmath61 is composed of values of a grf at a batch of @xmath34 locations @xmath49 , we use the notation @xmath71 .",
    "we obtain the moments @xmath72 of a truncated gaussian distribution by an extension of tallis technique @xcite to any order , presented in the following proposition :    [ proposition : analyticalmoment ] the function @xmath73 defined by @xmath74 where @xmath75 is the cumulative distribution function of the centered @xmath76-variate normal distribution , is infinitely differentiable , and the moments @xmath67 are given by : @xmath77    the proof of this proposition is given in appendix [ appendix : analyticalmoment ] and relies on calculating the moment generating function @xmath78 . even if an analytical formula can be obtained at any order of differentiation @xmath79 , the complexity of derivatives in equation increases rapidly .",
    "we give below the results for @xmath79 equals 1 and 2 .",
    "differentiating @xmath81 with respect to @xmath82 yields : @xmath83 where @xmath84 is the gradient of @xmath85 ( see appendix [ appendix : cdfgradient ] for an analytical derivation ) .",
    "taking @xmath86 in the previous equation gives @xmath87 where @xmath88 is the @xmath89 column of @xmath90 .",
    "it is shown in appendix [ appendix : cdfgradient ] that computing each of the @xmath76 components of @xmath84 requires to compute a multivariate cdf of the normal distribution in dimension @xmath91 .",
    "the number of calls to this function for computing the first moment of the truncated gaussian distribution is thus of @xmath92 .",
    "similarly , differentiating @xmath81 twice with respect to @xmath82 yields @xmath94 for readability , the detailed formula of @xmath95 , the hessian matrix of @xmath85 , is sent to appendix [ appendix : cdfhessian ] .",
    "the number of calls to the multivariate normal cdf is of @xmath96 .",
    "the previous results obtained for the moments of the truncated normal distribution turn out to be of interest for computing the generalized @xmath57 introduced in equation  , as shown by the following proposition .    [ proposition : analyticqei ] for @xmath97 , the criterion @xmath98 defined by exists for all @xmath79 and can be written as a sum of moments of truncated normal distributions @xmath99 with @xmath100 a vector of size @xmath101 defined , noting @xmath102 , by + @xmath103    [ cols= \" < , < \" , ]     indeed , the first @xmath104 components of @xmath105 reflect @xmath106 , and the last components reflect @xmath107 and @xmath108 .",
    "let @xmath110 be an event , @xmath5 be a mean - squared differentiable gaussian process and @xmath111 .",
    "then we have : @xmath112 by mean - squared differentiability of @xmath113 .",
    "the term @xmath114 comes from a symmetry occurring when summing terms with different index but actually equal . at fixed summation index @xmath115 in ,",
    "we denote @xmath116 the @xmath117 term in the scalar product in for each @xmath118 required for @xmath57 : @xmath119_i.\\ ] ] then the following symmetry between indices @xmath38 and @xmath9 occurs : @xmath120 indeed , using the formula of the derivative of cdf , ( appendix [ appendix : cdfgradient ] ) , leads to : @xmath121                  , _ a tutorial on bayesian optimization of expensive cost functions , with application to active user modeling and hierarchical reinforcement learning .",
    "report , dept . of computer science , university of british columbia , 2009 .    , _ _ learning and intelligent optimization - 7th international conference , lion 7 , catania , italy , january 7 - 11 , 2013 , revised selected papers _ , chapter fast computation of the multipoint expected improvement with applications in batch selection , pages 59 - 69 _ , springer , ( 2014 . ) .",
    ", _ expected improvements for the asynchronous parallel global optimization of expensive functions : potentials and challenges _ , in lion 6 conference ( learning and intelligent optimization ) , paris : france , 2012 .                , _ machine learning , optimization , and big data : first international workshop , mod 2015 , taormina , sicily , italy , july 21 - 23 , 2015 , revised selected papers _ , springer international publishing , cham , 2015 , ch .  differentiating the multipoint expected improvement for optimal batch design , pp ."
  ],
  "abstract_text": [
    "<S> we deal with the efficient parallelization of bayesian global optimization algorithms , and more specifically of those based on the expected improvement criterion and its variants . </S>",
    "<S> a closed form formula relying on multivariate gaussian cumulative distribution functions is established for a generalized version of the multipoint expected improvement criterion . in turn , the latter relies on intermediate results that could be of independent interest concerning moments of truncated gaussian vectors . </S>",
    "<S> the obtained expansion of the criterion enables studying its differentiability with respect to point batches and calculating the corresponding gradient in closed form . </S>",
    "<S> furthermore , we derive fast numerical approximations of this gradient and propose efficient batch optimization strategies . </S>",
    "<S> numerical experiments illustrate that the proposed approaches enable computational savings of between one and two order of magnitudes , hence enabling derivative - based batch - sequential acquisition function maximization to become a practically implementable and efficient standard . </S>",
    "<S> * keywords : * kriging , expected improvement , parallel optimization . </S>"
  ]
}