{
  "article_text": [
    "this paper is concerned with the solution of large systems of parameter - dependent equations of the form @xmath0 where @xmath1 takes values in some parameter set @xmath2 .",
    "such problems occur in several contexts such as parametric analyses , optimization , control or uncertainty quantification , where @xmath1 are random variables that parametrize model or data uncertainties .",
    "the efficient solution of equation generally requires the construction of preconditioners for the operator @xmath3 , either for improving the performance of iterative solvers or for improving the quality of residual - based projection methods .",
    "a basic preconditioner can be defined as the inverse ( or any preconditioner ) of the matrix @xmath4 at some nominal parameter value @xmath5 or as the inverse ( or any preconditioner ) of a mean value of @xmath3 over @xmath2 ( see e.g. @xcite ) .",
    "when the operator only slightly varies over the parameter set @xmath2 , these parameter - independent preconditioners behave relatively well .",
    "however , for large variabilities , they are not able to provide a good preconditioning over the whole parameter set @xmath2 . a first attempt to construct a parameter - dependent preconditioner",
    "can be found in @xcite , where the authors compute through quadrature a polynomial expansion of the parameter - dependent factors of a lu factorization of @xmath3 .",
    "more recently , a linear lagrangian interpolation of the matrix inverse has been proposed in @xcite .",
    "the generalization to any standard multivariate interpolation method is straightforward .",
    "however , standard approximation or interpolation methods require the evaluation of matrix inverses ( or factorizations ) for many instances of @xmath1 on a prescribed structured grid ( quadrature or interpolation ) , that becomes prohibitive for large matrices and high dimensional parametric problems .    in this paper",
    ", we propose an interpolation method for the inverse of matrix @xmath3 .",
    "the interpolation is obtained by a projection of the inverse matrix on a linear span of samples of @xmath6 and takes the form @xmath7 where @xmath8 are @xmath9 arbitrary interpolation points in @xmath2 .",
    "a natural interpolation could be obtained by minimizing the condition number of @xmath10 over the @xmath11 , which is a clarke regular strongly pseudoconvex optimization problem @xcite .",
    "however , the solution of this non standard optimization problem for many instances of @xmath1 is intractable and proposing an efficient solution method in a multi - query context remains a challenging issue .",
    "here , the projection is defined as the minimizer of the frobenius norm of @xmath12 , that is a quadratic optimization problem . approximations of the frobenius norm using random matrices are introduced in order to handle large matrices .",
    "these statistical estimations of the frobenius norm allow to obtain quasi - optimal projections that are controlled with high probability .",
    "since we are interested in large matrices , @xmath13 are here considered as implicit matrices for which only efficient matrix - vector multiplications are available .",
    "typically , a factorization ( e.g. lu ) of @xmath14 is computed and stored .",
    "note that when the storage of factorizations of several samples of the operator is unaffordable or when efficient preconditioners are readily available , one could similarly consider projections of the inverse operator on the linear span of preconditioners of samples of the operator .",
    "however , the resulting parameter - dependent preconditioner would be no more an interpolation of preconditioners .",
    "this straightforward extension of the proposed method is not analyzed in the present paper .",
    "the paper then presents several contributions in the context of projection - based model order reduction methods ( e.g. reduced basis , proper orthogonal decomposition ( pod ) , proper generalized decompositon ) that rely on the projection of the solution @xmath15 of on a low - dimensional approximation space .",
    "we first show how the proposed preconditioner can be used to define a galerkin projection - based on the preconditioned residual , which can be interpreted as a petrov - galerkin projection of the solution with a parameter - dependent test space .",
    "then , we propose adaptive construction of the preconditioner , based on an adaptive selection of interpolation points , for different objectives : ( i ) the improvement of error estimators based on preconditioned residuals , ( ii ) the improvement of the quality of projections on a given low - dimensional approximation space , or ( iii ) the re - use of computations for sample - based model order reduction methods . starting from a @xmath9-point interpolation ,",
    "these adaptive strategies consist in choosing a new interpolation point based on different criteria . in ( i ) , the new point is selected for minimizing the distance between the identity and the preconditioned operator . in ( ii ) , it is selected for improving the quasi - optimality constant of petrov - galerkin projections which measures how far the projection is from the best approximation on the reduced approximation space . in ( iii ) , the new interpolation point is selected as a new sample determined for the approximation of the solution and not of the operator .",
    "the interest of the latter approach is that when direct solvers are used to solve equation at some sample points , the corresponding factorizations of the matrix can be stored and the preconditioner can be computed with a negligible additional cost .",
    "the paper is organized as follows . in section [ sec : frob_interpolation ] we present the method for the interpolation of the inverse of a parameter - dependent matrix . in section [ sec :",
    "model_reduction ] , we show how the preconditioner can be used for the definition of a petrov - galerkin projection of the solution of on a given reduced approximation space , and we provide an analysis of the quasi - optimality constant of this projection . then , different strategies for the selection of interpolation points for the preconditioner are proposed in section [ sec : greedy_precond ] . finally , in section [ sec : num_res ] , numerical experiments will illustrate the efficiency of the proposed preconditioning strategies for different projection - based model order reduction methods .",
    "note that the proposed preconditioner could be also used ( a ) for improving the quality of galerkin projection methods where a projection of the solution @xmath15 is searched on a subspace of functions of the parameters ( e.g. polynomial or piecewise polynomial spaces ) @xcite , or ( b ) for preconditioning iterative solvers for , in particular solvers based on low - rank truncations that require a low - rank structure of the preconditioner @xcite .",
    "these two potential applications are not considered here .",
    "in this section , we propose a construction of an interpolation of the matrix - valued function @xmath16 for given interpolation points @xmath8 in @xmath2 .",
    "we let @xmath17 , @xmath18 . for large matrices ,",
    "the explicit computation of @xmath19 is usually not affordable .",
    "therefore , @xmath19 is here considered as an implicit matrix and we assume that the product of @xmath19 with a vector can be computed efficiently . in practice , factorizations of matrices @xmath14 are stored .",
    "we introduce the subspace @xmath20 of @xmath21 .",
    "an approximation @xmath22 of @xmath6 in @xmath23 is then defined by @xmath24 where @xmath25 denotes the identity matrix of size @xmath26 , and @xmath27 is the frobenius norm such that @xmath28 with @xmath29 . since @xmath30",
    ", we have the interpolation property @xmath31 , @xmath32 .",
    "the minimization of @xmath33 has been first proposed in @xcite for the construction of a preconditioner @xmath34 in a subspace of matrices with given sparsity pattern ( spai method ) .",
    "the following proposition gives some properties of the operator @xmath10 ( see lemma 2.6 and theorem 3.2 in @xcite ) .",
    "[ prop : res_gonzalez ] let @xmath22 be defined by .",
    "we have @xmath35 where @xmath36 is the lowest singular value of @xmath10 verifying @xmath37 , with @xmath38 if and only if @xmath39 .",
    "also , the following bound holds for the condition number of @xmath10 : @xmath40 under the condition @xmath41 , equations and imply that @xmath42    for all @xmath43 , we have @xmath44 where the matrix @xmath45 and the vector @xmath46 are given by @xmath47 therefore , the solution of problem is @xmath48 with @xmath49 the solution of @xmath50 .",
    "when considering a small number @xmath9 of interpolation points , the computation time for solving this system of equations is negligible .",
    "however , the computation of @xmath51 and @xmath52 requires the evaluation of traces of matrices @xmath53 and @xmath54 for all @xmath55 .",
    "since the @xmath19 are implicit matrices , the computation of such products of matrices is not affordable for large matrices .",
    "of course , since @xmath56 , the trace of an implicit matrix @xmath57 could be obtained by computing the product of @xmath57 with the canonical vectors @xmath58 , but this approach is clearly not affordable for large @xmath26 .",
    "hereafter , we propose an approximation of the above construction using an approximation of the frobenius norm which requires less computational efforts .      here",
    ", we define an approximation @xmath22 of @xmath6 in @xmath23 by @xmath59 where @xmath60 , with @xmath61 .",
    "@xmath62 defines a semi - norm on @xmath63 . here",
    ", we assume that the linear map @xmath64 is injective on @xmath23 so that the solution of is unique .",
    "this requires @xmath65 and is satisfied when @xmath66 and @xmath23 is the linear span of linearly independent invertible matrices .",
    "then , the solution @xmath67 of is such that the vector @xmath68 satisfies @xmath69 , with @xmath70    the procedure for the computation of @xmath71 and @xmath72 is given in algorithm [ alg : compute_mv_sv ] .",
    "note that only @xmath73 matrix - vector products involving the implicit matrices @xmath19 are required .",
    "@xmath74 @xmath75 and @xmath76 @xmath71 and @xmath72 compute the vectors @xmath77 , for @xmath78 and @xmath18 set @xmath79 , @xmath18 compute @xmath80 for @xmath81 compute @xmath82 for @xmath83    now the question is to choose a matrix @xmath84 such that @xmath85 provides a good approximation of @xmath86 for any @xmath87 and @xmath88 .",
    "let @xmath57 an implicit @xmath26-by-@xmath26 matrix ( consider @xmath89 , with @xmath87 and @xmath88 ) .",
    "following @xcite , we show how hadamard matrices can be used for the estimation of the frobenius norm of an implicit matrix .",
    "the goal is to find a matrix @xmath84 such that @xmath90 is a good approximation of @xmath91 .",
    "the relation @xmath92 suggests that @xmath84 should be such that @xmath93 is as close as possible to the identity matrix .",
    "for example , we would like @xmath84 to minimize @xmath94 which is the mean square magnitude of the off - diagonal entries of @xmath93 .",
    "the bound @xmath95 is known to hold for any @xmath96 whose rows have unit norm @xcite .",
    "hadamard matrices can be used to construct matrices @xmath84 such that the corresponding error @xmath97 is close to the bound , see @xcite .",
    "a hadamard matrix @xmath98 is a @xmath99-by-@xmath99 matrix whose entries are @xmath100 , and which satisfies @xmath101 where @xmath25 is the identity matrix of size @xmath99 .",
    "for example , @xmath102 is a hadamard matrix of size @xmath103 .",
    "the kronecker product ( denoted by @xmath104 ) of two hadamard matrices is again a hadamard matrix .",
    "then it is possible to build a hadamard matrix whose size @xmath99 is a power of 2 using a recursive procedure : @xmath105 .",
    "the @xmath106-entry of this matrix is @xmath107 , where @xmath108 and @xmath109 are the binary vectors such that @xmath110 and @xmath111 . for a sufficiently large @xmath112",
    ", we define the _ rescaled partial hadamard matrix _ @xmath96 as the first @xmath26 rows and the first @xmath113 columns of @xmath114 .      for the computation of the frobenius norm of @xmath57 , we can also use a statistical estimator as first proposed in @xcite .",
    "the idea is to define a random matrix @xmath96 with a suitable distribution law @xmath115 such that @xmath90 provides a controlled approximation of @xmath116 with high probability .",
    "[ def : jl_matrix ] a distribution @xmath115 over @xmath117 satisfies the @xmath118-concentration property if for all @xmath119 , @xmath120    two distributions @xmath115 will be considered here .",
    "\\(a ) the _ rescaled rademacher distribution_. here the entries of @xmath96 are independent and identically distributed with @xmath121 with probability @xmath122 .",
    "according to theorem 13 in @xcite , the rescaled rademacher distribution satisfies the @xmath118-concentration property for @xmath123    \\(b ) the _ subsampled randomized hadamard transform distribution _ ( srht ) , first introduced in @xcite .",
    "here we assume that @xmath26 is a power of @xmath124 .",
    "it is defined by @xmath125 where    * @xmath126 is a diagonal random matrix where @xmath127 are independent rademacher random variables ( i.e. @xmath128 with probability @xmath122 ) , * @xmath129 is a hadamard matrix of size @xmath26 ( see section [ sec : hadamard_trace ] ) , * @xmath130 is a subset of @xmath113 rows from the identity matrix of size @xmath26 chosen uniformly at random and without replacement .",
    "in other words , we randomly select @xmath113 rows of @xmath131 without replacement , and we multiply the columns by @xmath132 .",
    "we can find in @xcite an analysis of the srht matrix properties .",
    "in the case where @xmath26 is not a power of @xmath124 , we define the partial srht ( p - srht ) matrix @xmath133 as the first @xmath26 rows of a srht matrix of size @xmath134 , where @xmath135 is the smallest power of @xmath124 such that @xmath136 .",
    "the following proposition shows that the ( p - srht ) distribution satisfies the @xmath118-concentration property .",
    "[ prop : srht_conf_interval ] the ( p - srht ) distribution satisfies the @xmath118-concentration property for @xmath137    let @xmath119 .",
    "we define the square matrix @xmath138 of size @xmath139 , whose first @xmath140 diagonal block is @xmath57 , and @xmath141 elsewhere .",
    "then we have @xmath142 .",
    "the rest of the proof is similar to the one of lemma 4.10 in @xcite .",
    "we consider the events @xmath143 and @xmath144 , where @xmath145 is the @xmath146-th canonical vector of @xmath147 .",
    "the relation @xmath148 holds . thanks to lemma 4.6 in @xcite ( with @xmath149 ) we have @xmath150 . now , using the scalar chernoff bound ( theorem 2.2 in @xcite with @xmath151 ) we have @xmath152 the condition implies @xmath153 , and then @xmath154 , which ends the proof .    such statistical estimators are particularly interesting for that they provide approximations of the frobenius norm of large matrices , with a number of columns @xmath113 for @xmath84 which scales as the logarithm of @xmath26 , see and",
    ". however , the concentration property holds only for a given matrix @xmath57 . the following proposition [ prop : concentration_sev ] extends these concentration results for any matrix @xmath57 in a given subspace .",
    "the proof is inspired from the one of theorem 6 in @xcite .",
    "the essential ingredient is the existence of an @xmath155-net for the unit ball of a finite dimensional space ( see @xcite ) .",
    "[ prop : concentration_sev ] let @xmath96 be a random matrix whose distribution @xmath115 satisfies the @xmath118-concentration property , with @xmath156 . then , for any @xmath157-dimensional subspace of matrices @xmath158 and for any @xmath159 , we have @xmath160    we consider the unit ball @xmath161 of the subspace @xmath162 .",
    "it is shown in @xcite that for any @xmath163 , there exists a net @xmath164 of cardinality lower than @xmath165 such that @xmath166 in other words , any element of the unit ball @xmath167 can be approximated by an element of @xmath168 with an error less than @xmath169 . using the @xmath118-concentration property and a union bound ,",
    "we obtain @xmath170 with a probability at least @xmath171 .",
    "we now impose the relation @xmath172 , where @xmath159 .",
    "to prove , it remains to show that equation implies @xmath173    we define @xmath174 .",
    "let @xmath175 be such that @xmath176 , and @xmath177 .",
    "then we have @xmath178 and @xmath179 , where @xmath180 is the inner product associated to the frobenius norm @xmath27 .",
    "we have @xmath181 we now have to bound the three terms in the previous expression .",
    "firstly , since @xmath182 , the relation @xmath183 holds . secondly , gives @xmath184 .",
    "thirdly , by definition of @xmath185 , we can write @xmath186 and @xmath187 , so that we obtain @xmath188 . finally ,",
    "from , we obtain @xmath189 since @xmath190 , we have @xmath191 . then @xmath192 and @xmath193 , so that implies @xmath194 and then @xmath195 . by definition of @xmath185 , we can write @xmath196 for any @xmath197 , that implies .",
    "[ prop : quasioptimalite_seminorm ] let @xmath88 , and let @xmath198 be defined by where @xmath96 is a realization of a rescaled rademacher matrix with @xmath199 or a realization of a p - srht matrix with @xmath200 for some @xmath201 , @xmath190 and @xmath159 . assuming @xmath202 , @xmath203 holds with a probability higher than @xmath204 .",
    "let us introduce the subspace @xmath205 of dimension less than @xmath206 , such that @xmath207 .",
    "then , we note that with the conditions or , the distribution law @xmath115 of the random matrix @xmath84 satisfies the @xmath208-concentration property .",
    "thanks to proposition [ prop : concentration_sev ] , the probability that @xmath209 holds for any @xmath87 is higher than @xmath204",
    ". then , by definition of @xmath22 , we have with a probability at least @xmath204 that for any @xmath87 , it holds @xmath210 then , taking the minimum over @xmath87 , we obtain .",
    "similarly to proposition [ prop : res_gonzalez ] , we obtain the following properties for @xmath10 , with @xmath22 the solution of .    [ prop : boundcond_seminorm ] under the assumptions of proposition [ prop : quasioptimalite_seminorm ] , the inequalities @xmath211 and @xmath212 hold with probability @xmath204 , where @xmath213 is the lowest singular value of @xmath10 .    the optimality condition for @xmath22 yields @xmath214 . since @xmath215 ( where @xmath216 is the subspace introduced in the proof of proposition )",
    ", we have @xmath217 with a probability higher than @xmath204 . using @xmath218 ( which is satisfies for any realization of the rescaled rademacher or the p - srht distribution )",
    ", we obtain @xmath219 with a probability higher than @xmath204 .",
    "then , @xmath220 yields the right inequality of . following the proof of lemma 2.6 in @xcite",
    ", we have @xmath221 .",
    "together with , it yields the left inequality of .",
    "furthermore , with probability @xmath204 , we have @xmath222 .",
    "since the square of the frobenius norm of matrix @xmath223 is the sum of squares of its singular values , we deduce @xmath224 with a probability higher than @xmath204 , where @xmath225 is the largest singular value of @xmath10 . then follows from the definition of @xmath226",
    "we have presented different possibilities for the definition of @xmath84 .",
    "the rescaled partial hadamard matrices introduced in section [ sec : hadamard_trace ] have the advantage that the error @xmath97 is close to the theoretical bound @xmath227 , see figure [ fig : compare_hadamard_mc ] ( note that the rows of @xmath84 have unit norm ) .",
    "furthermore , an interesting property is that @xmath93 has a structured pattern ( see figure [ fig : hadamard_spy ] ) .",
    "as noticed in @xcite , when @xmath228 the matrix @xmath93 have non - zero entries only on the @xmath229-th upper and lower diagonals , with @xmath230 . as a consequence , the error on the estimation of @xmath91 will be induced only by the non - zero off - diagonal entries of @xmath57 that occupy the @xmath229-th upper and lower diagonals , with @xmath231 .",
    "if the entries of @xmath57 vanish away from the diagonal , the frobenius norm is expected to be accurately estimated .",
    "note that the p - srht matrices can be interpreted as a `` randomized version '' of the rescaled partial hadamard matrices , and figure [ fig : compare_hadamard_mc ] shows that the error @xmath97 associated to the p - srht matrix behaves almost like the rescaled partial hadamard matrix .",
    "also , p - srht matrices yield a structured pattern for @xmath93 , see figure [ fig : srht_spy ] .",
    "the rescaled rademacher matrices give higher errors @xmath97 and yield matrices @xmath93 with no specific patterns , see figure [ fig : mc_spy ] .",
    "+     the advantage of using rescaled rademacher matrices or p - srht matrices is that the quality of the resulting projection @xmath22 can be controlled with high probability , provided that @xmath84 has a sufficiently large number of rows @xmath113 ( see proposition [ prop : quasioptimalite_seminorm ] ) .",
    "table [ tab : compare_rade_srht ] shows the theoretical value for @xmath113 in order to obtain the quasi - optimality result with @xmath232 and @xmath233 .",
    "it can be observed that @xmath113 grows very slowly with the matrix size @xmath26 .",
    "also , @xmath113 depends on @xmath9 linearly for the rescaled rademacher matrices and quadratically for the p - srht matrices ( see equations and ) .",
    "however , these theoretical bounds for @xmath113 are very pessimistic , especially for the p - srht matrices . in practice",
    ", it can be observed that a very small value for @xmath113 may provide very good results ( see section [ sec : num_res ] ) .",
    "also , it is worth mentioning that our numerical experiments do not reveal significant differences between the rescaled partial hadamard , the rescaled rademacher and the p - srht matrices .      here , we propose a modification of the interpolation which ensures that @xmath22 is invertible when @xmath3 is positive definite .",
    "since @xmath14 is positive definite , @xmath234 is positive definite .",
    "we introduce the vectors @xmath235 and @xmath236 whose components @xmath237 correspond respectively to the lowest and highest eigenvalues of the symmetric part of @xmath19 .",
    "then , for any @xmath238 , @xmath239 where @xmath240 and @xmath241 are respectively the positive and negative parts of @xmath242 . as a consequence , if the right hand side of is strictly positive , then @xmath34 is invertible .",
    "furthermore , we have @xmath243 , where @xmath244 is the vector of component @xmath245 , where @xmath246 denotes the operator norm of @xmath19 .",
    "if we assume that @xmath247 , the condition number of @xmath34 satisfies @xmath248 it is then possible to bound @xmath249 by @xmath250 by imposing @xmath251 which is a linear inequality constraint on @xmath252 and @xmath253 .",
    "we introduce two convex subsets of @xmath23 defined by    @xmath254    from , we have that any nonzero element of @xmath255 is invertible , while any nonzero element of @xmath256 is invertible and has a condition number lower than @xmath250 . under the condition @xmath257",
    ", we have @xmath258 then definitions and for the approximation @xmath22 can be replaced respectively by    @xmath259    which are quadratic optimization problems with linear inequality constraints .",
    "furthermore , since @xmath260 for all @xmath146 , all the resulting projections @xmath22 interpolate @xmath6 at the points @xmath8 .    the following proposition shows that properties and still hold for the preconditioned operator .",
    "the solution @xmath22 of is such that @xmath10 satisfies and .",
    "also , under the assumptions of proposition [ prop : quasioptimalite_seminorm ] , the solution @xmath22 of is such that @xmath10 satisfies and .",
    "since @xmath255 ( or @xmath256 ) is a closed and convex positive cone , the solution @xmath22 of is such that @xmath261 for all @xmath262 ( or @xmath256 ) .",
    "taking @xmath263 and @xmath264 , we obtain that @xmath265 , which implies @xmath266 .",
    "we refer to the proof of lemma 2.6 and theorem 3.2 in @xcite to deduce and . using the same arguments",
    ", we prove that the solution @xmath22 of satisfies @xmath267 , and then that and hold with a probability higher than @xmath204 .      here , we detail how to efficiently compute @xmath71 and @xmath72 given in equation in a multi - query context , i.e. for several different values of @xmath1 .",
    "the same methodology can be applied for computing @xmath51 and @xmath52 .",
    "we assume that the operator @xmath3 has an affine expansion of the form @xmath268 where the @xmath269 are matrices in @xmath270 and the @xmath271 are real - valued functions",
    ". then @xmath71 and @xmath72 also have the affine expansions    [ eq : ms_affine ] @xmath272    respectively .",
    "computing the multiple terms of these expansions would require many computations of traces of implicit matrices and also , it would require the computation of the affine expansion of @xmath3 . here",
    ", we use the methodology introduced in @xcite for obtaining affine decompositions with a lower number of terms .",
    "these decompositions only require the knowledge of functions @xmath273 in the affine decomposition , and evaluations of @xmath274 and @xmath275 ( that means evaluations of @xmath3 ) at some selected points .",
    "we briefly recall this methodology .",
    "suppose that @xmath276 , with @xmath277 a vector space , has an affine decomposition @xmath278 , with @xmath279 and @xmath280 .",
    "we first compute an interpolation of @xmath281 under the form @xmath282 , with @xmath283 , where @xmath284 are interpolation points and @xmath285 the associated interpolation functions .",
    "such an interpolation can be computed with the empirical interpolation method @xcite described in algorithm [ alg : compute_eim ] .",
    "then , we obtain an affine decomposition @xmath286 which can be computed from evaluations of @xmath287 at interpolation points @xmath288 .",
    "@xmath289 @xmath290 and @xmath291 define @xmath292 for all @xmath293 initialize @xmath294 , @xmath295 @xmath296 find @xmath297 set the error to @xmath298 actualize @xmath299 for all @xmath293 fill in the @xmath300-by-@xmath300 matrix @xmath301 : @xmath302 for all @xmath303 compute @xmath304 for all @xmath1 and @xmath305    applying the above procedure to both @xmath71 and @xmath72 , we obtain @xmath306    the first ( so - called _ offline _ ) step consists in computing the interpolation functions @xmath307 and @xmath308 and associated interpolation points @xmath309 and @xmath310 using algorithm [ alg : compute_eim ] with input @xmath311 and @xmath312 respectively , and then in computing matrices @xmath313 and vectors @xmath314 using algorithm [ alg : compute_mv_sv ] . the second ( so - called _ online _ )",
    "step simply consists in computing the matrix @xmath71 and the vector @xmath72 for a given value of @xmath1 using .",
    "we consider a parameter - dependent linear equation @xmath315 with @xmath316 and @xmath317 .",
    "projection - based model reduction consists in projecting the solution @xmath15 onto a well chosen approximation space @xmath318 of low dimension @xmath319 .",
    "such projections are usually defined by imposing the residual of to be orthogonal to a so - called test space of dimension @xmath320 .",
    "the quality of the projection on @xmath321 depends on the choice of the test space .",
    "the latter can be defined as the approximation space itself @xmath321 , thus yielding the classical galerkin projection .",
    "however when the operator @xmath3 is ill - conditioned ( for example when @xmath3 corresponds to the discretization of non coercive or weakly coercive operators ) , this choice may lead to projections that are far from optimal . choosing the test space as @xmath322 , where @xmath323 is called the `` supremizer operator '' ( see e.g. @xcite ) , corresponds to a minimal residual approach , which may also results in projections that are far from optimal . in this section ,",
    "we show how the preconditioner @xmath22 can be used for the definition of the test space .",
    "we also show how it can improve the quality of residual - based error estimates , which is a key ingredient for the construction of suitable approximation space @xmath321 in the context of the reduced basis method .",
    "@xmath277 is endowed with the norm @xmath324 defined by @xmath325 , where @xmath326 is a symmetric positive definite matrix and @xmath180 is the canonical inner product of @xmath327 .",
    "we also introduce the dual norm @xmath328 such that for any @xmath329 we have @xmath330 .      here , we suppose that the approximation space @xmath321 has been computed by some model order reduction method .",
    "the best approximation of @xmath15 on @xmath321 is @xmath331 and is characterized by the orthogonality condition @xmath332 or equivalently by the petrov - galerkin orthogonality condition @xmath333    obviously the computation of test functions @xmath334 for basis functions @xmath335 of @xmath321 is prohibitive . by replacing @xmath6 by @xmath22",
    ", we obtain the feasible petrov - galerkin formulation @xmath336    denoting by @xmath337 a matrix whose range is @xmath321 , the solution of is @xmath338 where the vector @xmath339 is the solution of @xmath340    note that corresponds to the standard galerkin projection when replacing @xmath22 by @xmath341 .",
    "indeed , the orthogonality condition becomes @xmath342 for all @xmath343 .    here , the preconditioner @xmath22 is used for the definition of the parameter - dependent test space @xmath344 which defines the petrov - galerkin projection .",
    "however , @xmath22 could also be used to construct preconditioners for the solution of the linear system @xmath345 corresponding to the galerkin projection on @xmath321 .",
    "following the idea proposed in @xcite , such preconditoner can take the form @xmath346 , thus yielding the preconditioned reduced linear system @xmath347 such preconditioning strategy can be used to accelerate the solution of the reduced system of equations when using iterative methods .",
    "however , and contrarily to , this strategy does not change the definition of @xmath348 , which is the standard galerkin projection .",
    "we give now a quasi - optimality result for the approximation @xmath348 .",
    "this analysis relies on the notion of @xmath349-proximality introduced in @xcite .",
    "[ prop : delta_prox_pg ] let @xmath350 $ ] be defined by @xmath351    the solutions @xmath352 and @xmath353 of and satisfy @xmath354    moreover , if @xmath355 holds , then @xmath356    the orthogonality condition yields @xmath357 for all @xmath343 . using",
    ", we have that for any @xmath358 , @xmath359    taking the infimum over @xmath360 and by the definition of @xmath361 , we obtain @xmath362    then , noting that @xmath363 , we obtain @xmath364 that is . finally , using orthogonality condition , we have that @xmath365 from which we deduce when @xmath355 .",
    "an immediate consequence of proposition [ prop : delta_prox_pg ] is that when @xmath366 , the petrov - galerkin projection @xmath348 coincides with the orthogonal projection @xmath367 .",
    "following @xcite , we show in the following proposition that @xmath361 can be computed by solving an eigenvalue problem of size @xmath320 .",
    "[ prop : delta_computable ] we have @xmath368 , where @xmath369 is the lowest eigenvalue of the generalized eigenvalue problem @xmath370 , with @xmath371 where @xmath372 and where @xmath373 is a matrix whose range is @xmath321 .    since the range of @xmath374 is @xmath321 , we have @xmath375 for any @xmath376 , the minimizer @xmath377 of @xmath378 over @xmath379 is given by @xmath380 .",
    "therefore , we have @xmath381 , and @xmath382 which concludes the proof .",
    "following the idea of the reduced basis method @xcite , a sequence of nested approximation spaces @xmath383 in @xmath277 can be constructed by a greedy algorithm such that @xmath384 , where @xmath385 is a point where the error of approximation of @xmath15 in @xmath321 is maximal .",
    "an ideal greedy algorithm using the best approximation in @xmath321 and an exact evaluation of the projection error is such that    [ eq : ideal_greedy ] @xmath386    this ideal greedy algorithm is not feasible in practice since @xmath15 is not known .",
    "therefore , we rather rely on a feasible weak greedy algorithm such that    [ eq : greedy - weak ] @xmath387    assume that @xmath388 holds with @xmath389 and @xmath390 , where @xmath213 and @xmath225 are respectively the lowest and largest singular values of @xmath10 with respect to the norm @xmath391 , respectively defined by the infimum and supremum of @xmath392 over @xmath393 such that @xmath394 . then , we easily prove that algorithm is such that @xmath395 where @xmath396 measures how far the selection of the new point is from the ideal greedy selection . under condition",
    ", convergence results for this weak greedy algorithm can be found in @xcite .",
    "we give now sharper bounds for the preconditoned residual norm that exploits the fact that the approximation @xmath348 is the petrov - galerkin projection .",
    "let @xmath348 be the petrov - galerkin projection of @xmath15 on @xmath321 defined by .",
    "then we have @xmath397 with    @xmath398    for any @xmath399 and @xmath400 and according to , we have @xmath401 where @xmath402 . taking the infimum over @xmath358 , dividing by @xmath403 and taking the supremum over @xmath399 , we obtain @xmath404 which proves the first inequality .",
    "furthermore , for any @xmath405 and @xmath360 , we have @xmath406 taking the infimum over @xmath360 , dividing by @xmath407 and taking the supremum over @xmath399 , we obtain the second inequality .",
    "since @xmath408 , we have @xmath409 and @xmath410 .",
    "equation holds with @xmath411 replaced by the parameter @xmath412 .",
    "since @xmath413 increases with @xmath320 , a reasonable expectation is that the convergence properties of the weak greedy algorithm will improve when @xmath320 increases .    when replacing @xmath22 by @xmath341 , the preconditioned residual norm @xmath414 turns out to be the residual norm @xmath415 , which is a standard choice in the reduced basis method for the greedy selection of points ( with @xmath326 being associated with the natural norm on @xmath277 or with a norm associated with the operator at some nominal parameter value ) .",
    "this can be interpreted as a basic preconditioning method with a parameter - independent preconditioner .",
    "in this section , we propose strategies for the adaptive selection of the interpolation points . for a given set of interpolation points @xmath8 , three different methods are proposed for the selection of a new interpolation point @xmath416 .",
    "the first method aims at reducing uniformly the error between the inverse operator and its interpolation .",
    "the resulting interpolation of the inverse is pertinent for preconditioning iterative solvers or estimating errors based on preconditioned residuals .",
    "the second method aims at improving petrov - galerkin projections of the solution of a parameter - dependent equation on a given approximation space .",
    "the third method aims at reducing the cost for the computation of the preconditioner by reusing operators computed when solving samples of a parameter - dependent equation .",
    "a natural idea is to select a new interpolation point where the preconditioner @xmath22 is not a good approximation of @xmath6 .",
    "obviously , an ideal strategy for preconditioning would be to choose @xmath416 where the condition number of @xmath10 is maximal .",
    "the computation of the condition number for many values of @xmath1 being computationaly expensive , one could use upper bounds of this condition number , e.g. computed using scm @xcite .    here , we propose the following selection method : given an approximation @xmath22 associated with interpolation points @xmath8 , a new point @xmath417 is selected such that @xmath418 where the matrix @xmath84 is either the random rescaled rademacher matrix , or the p - srht matrix ( see section [ sec : semifro ] ) . this adaptive selection of the interpolation points yields the construction of an increasingsequence of subspaces @xmath419 in @xmath420 .",
    "this algorithm is detailed below .",
    "interpolation points @xmath422 and interpolation @xmath423 .",
    "initialize @xmath424 compute the new point @xmath416 according to compute a factorization of @xmath425 define @xmath426 as an implicit operator update the space @xmath427 compute @xmath428    the following lemma interprets the above construction as a weak greedy algorithm .",
    "[ lem : weak - greedy ] assume that @xmath3 satisfies @xmath429 for all @xmath430 , and let @xmath22 be defined by . under the assumption that there exists @xmath431 such that @xmath432 holds for all @xmath88 and @xmath87 , we have @xmath433 with @xmath434 , and with @xmath416 defined by .",
    "since @xmath435 holds for any matrices @xmath57 and @xmath436 , with @xmath437 the operator norm of @xmath436 , we have for all @xmath438 , @xmath439 then , thanks to we have @xmath440 which implies @xmath441 we easily deduce that @xmath416 is such that holds .    the assumption of lemma [ lem : weak - greedy ] can be proved to hold with high probability in two cases .",
    "a first case is when @xmath2 is a training set of finite cardinality , where the results of proposition [ prop : quasioptimalite_seminorm ] can be extended to any @xmath88 by using a union bound .",
    "we then obtain that holds with a probability higher than @xmath442 .",
    "a second case is when @xmath3 admits an affine decomposition with @xmath443 terms .",
    "then the space @xmath444 is of dimension @xmath445 and proposition [ prop : concentration_sev ] allows to prove that assumption holds with high probability .",
    "the quality of the resulting spaces @xmath23 have to be compared with the kolmogorov @xmath9-width of the set @xmath446 , defined by @xmath447 which evaluates how well the elements of @xmath448 can be approximated on a @xmath9-dimensional subspace of matrices .",
    "implies that the following results holds ( see corollary 3.3 in @xcite ) : @xmath449 where @xmath450 is a constant which depends on @xmath451 and @xmath109 .",
    "that means that if the kolmogorov @xmath9-width has an algebraic or exponential convergence rate , then the weak greedy algorithm yields an error @xmath452 which has the same type of convergence .",
    "therefore , the proposed interpolation method will present good convergence properties when @xmath453 rapidly decreases with @xmath9 .",
    "when the parameter set @xmath454 is @xmath455^d$ ] ( or a product of compact intervals ) , an exponential decay can be obtained when @xmath6 admits an holomorphic extension to a domain in @xmath456 containing @xmath2 ( see @xcite ) .",
    "note that here , there is no constraint on the minimization problem over @xmath23 ( either optimal subspaces or subspaces constructed by the greedy procedure ) , so that we have no guaranty that the resulting approximations @xmath23 are invertible ( see section [ sec : ensuring_invertibility ] ) .",
    "we here suppose that we want to find an approximation of the solution @xmath15 of a parameter - dependent equation onto a low - dimensional approximation space @xmath321 , using a petrov - galerkin orthogonality condition given by . the best approximation is considered as the orthogonal projection defined by .",
    "the quantity @xmath361 defined by controls the quality of the petrov - galerkin projection on @xmath321 ( see proposition [ prop : delta_prox_pg ] ) .",
    "as indicated in proposition [ prop : delta_computable ] , @xmath361 can be efficiently computed .",
    "thus , we propose the following selection strategy which aims at improving the quality of the petrov - galerkin projection : given a preconditioner @xmath22 associated with interpolation points @xmath8 , the next point @xmath416 is selected such that @xmath457    the resulting construction is described by algorithm [ alg : compute_greedy_precond ] with the above selection of @xmath416 . note that this strategy is closely related with @xcite , where the authors propose a greedy construction of a parameter - independent test space for petrov - galerkin projection , with a selection of basis functions based on an error indicator similar to @xmath361",
    "when using a sample - based approach for solving a parameter - dependent equation , the linear system is solved for many values of the parameter @xmath1 .",
    "when using a direct solver for solving a linear system for a given @xmath1 , a factorization of the operator is usually available and can be used for improving a preconditioner for the solution of subsequent linear systems .",
    "we here describe this idea in the particular context of greedy algorithms for reduced basis method , where the interpolation points @xmath458 for the interpolation of the inverse @xmath6 are taken as the evaluation points @xmath459 for the solution . at iteration @xmath320 , having a preconditioner @xmath460 and an approximation @xmath348 , a new interpolation point is defined such that @xmath461    algorithm [ alg : greedy_simultaneous ] describes this strategy .",
    "@xmath462 , @xmath84 , and @xmath463 .",
    "approximation @xmath464 .",
    "initialize @xmath465 , @xmath466 find @xmath467 compute a factorization of @xmath468 solve the linear system @xmath469 update the approximation subspace @xmath470 define the implicit operator @xmath471 update the space @xmath472 ( or @xmath473 ) compute the preconditioner : @xmath474 compute the petrov - galerkin approximation @xmath475 of @xmath15 on @xmath476 using equation",
    "in this section we compare the different interpolation methods on the following one parameter - dependent advection - diffusion - reaction equation : @xmath477 defined over a square domain @xmath478 ^ 2 $ ] with periodic boundary conditions .",
    "the advection vector field @xmath479 is spatially constant and depends on the parameter @xmath1 that takes values in @xmath480 $ ] : @xmath481 , with @xmath482 and @xmath483 the canonical basis of @xmath484 .",
    "@xmath2 denotes a uniform grid of 250 points on @xmath480 $ ] .",
    "the source term @xmath485 is represented in figure [ fig : add_rot_source ] .",
    "we introduce a finite element approximation space of dimension @xmath486 with piecewise linear approximations on a regular mesh of @xmath487 .",
    "the mesh pclet number takes moderate values ( lower than one ) , so that a standard galerkin projection without stabilization is here sufficient .",
    "the galerkin projection yields the linear system of equations @xmath488 , with @xmath489 where the matrices @xmath490 , @xmath491 , @xmath492 and the vector @xmath109 are given by @xmath493 where @xmath494 is the basis of the finite element space . figures [ fig : add_rot_sample1 ] , [ fig : add_rot_sample2 ] and [ fig : add_rot_sample3 ] show three samples of the solution .",
    "we first choose arbitrarily 3 interpolation points ( @xmath495 , @xmath496 and @xmath497 ) and show the benefits of using the frobenius norm projection for the definition of the preconditioner .",
    "for the comparison , we consider the shepard and the nearest neighbor interpolation strategies .",
    "let @xmath498 denote a norm on the parameter set @xmath2 .",
    "the shepard interpolation method is an inverse weighted distance interpolation : @xmath499 where @xmath500 is a parameter .",
    "here we take @xmath103 .",
    "the nearest neighbor interpolation method consists in choosing the value taken by the nearest interpolation point , that means @xmath501 for some @xmath502 , and @xmath503 for all @xmath504 .    concerning the frobenius norm projection on @xmath23 ( or @xmath255 ) ,",
    "we first construct the affine decomposition of @xmath51 and @xmath52 as explained in section [ sec : practical_projection ] .",
    "the interpolation points @xmath505 ( resp .",
    "@xmath506 ) given by the eim procedure for @xmath51 ( resp .",
    "@xmath52 ) are @xmath507 ( resp .",
    "@xmath508 ) .",
    "the number of terms @xmath509 in the resulting affine decomposition of @xmath51 ( see equation ) is less than the expected number @xmath510 ( see equation ) . considering the functions @xmath511 , @xmath512 , @xmath513 , and thanks to relation @xmath514 , the space @xmath515 is of dimension @xmath509 .",
    "the eim procedure automatically detects the redundancy in the set of functions and reduces the number of terms in the decomposition .",
    "then , since the dimension @xmath26 of the discretization space is reasonable , we compute the matrices @xmath516 and the vectors @xmath517 using equation .    the functions @xmath11 are plotted on figure [ fig : add_rot_interpolation_function ] for the proposed interpolation strategies .",
    "it is important to note that contrary to the shepard or the nearest neighbor method , the frobenius norm projection ( on @xmath23 or @xmath255 ) leads to periodic interpolation functions , _",
    "i.e. _ @xmath518 .",
    "this is consistent with the fact that the application @xmath519 is @xmath520-periodic .",
    "the frobenius norm projection automatically detects such a feature .",
    "+    figure [ fig : add_rot_condition_number ] shows the condition number @xmath521 of @xmath10 with respect to @xmath1 .",
    "we first note that for the constant preconditioner @xmath522 , the resulting condition number is higher than the one of the non preconditioned matrix @xmath3 for @xmath523 $ ] .",
    "we also note that the interpolation strategies based on the frobenius norm projection lead to better preconditioners than the shepard and nearest neighbor interpolation strategies . when considering the projection on @xmath255 and @xmath256 ( with @xmath524 such that holds ) , the resulting condition number is roughly the same , so as the interpolation functions of figures [ fig : add_rot_interpolation_function_d ] and [ fig : add_rot_interpolation_function_e ] . since the projection on @xmath256 requires the expensive computation of the constants @xmath525 , @xmath526 and @xmath436 ( see section [ sec : ensuring_invertibility ] ) , we prefer to simply use the projection on @xmath255 in order to ensure the preconditioner to be invertible .",
    "finally , for this example , it is not necessary to impose any constraint since the projection on @xmath23 leads to the best preconditioner and this preconditioner appears to be invertible for any @xmath430 .     for different interpolation strategies .",
    "the condition number of @xmath3 is given as a reference.,scaledwidth=85.0% ]      we analyze now the interpolation method defined by the frobenius semi - norm projection on @xmath23 for the different definitions of @xmath527 proposed in sections [ sec : stat_trace ] and [ sec : hadamard_trace ] . according to table [ tab : error_lambda ] ,",
    "the error on the interpolation functions decreases slowly with @xmath113 ( roughly as @xmath528 ) , and the use of the p - srht matrix leads to a slightly lower error .",
    "the interpolation functions are plotted on figure [ fig : add_rot_hadamard_mc_a ] in the case where @xmath529 .",
    "even if we have an error of @xmath530 to @xmath531 on the interpolation functions , the condition number given on figure [ fig : add_rot_hadamard_mc_b ] remains close to the one computed with the frobenius norm .",
    "also , an important remark is that with @xmath529 the computational effort for computing @xmath313 and @xmath314 is negligible compared to the one for @xmath516 and @xmath517 .",
    ".relative error @xmath532 : @xmath533 ( resp .",
    "@xmath49 ) are the interpolation functions associated to the frobenius semi - norm projection ( resp .",
    "the frobenius norm projection ) on @xmath23 , with @xmath84 either the rescaled partial hadamard matrix , the random rescaled rademacher matrix or the p - srht matrix ( 3 different samples for random matrices ) . [ cols=\"<,^,^,^,^,^,^,^,^\",options=\"header \" , ]      +      ( left ) and quantile of probability @xmath534 ( right ) of the relative error @xmath535 with respect to @xmath320 .",
    "comparison of preconditioned reduced basis algorithms with ideal and standard greedy algorithms.,title=\"fig : \" ]    ( left ) and quantile of probability @xmath534 ( right ) of the relative error @xmath535 with respect to @xmath320 . comparison of preconditioned reduced basis algorithms with ideal and standard greedy algorithms.,title=\"fig : \" ]    let us finally consider the effectivity index @xmath536 which evaluates the quality of the preconditioned residual norm for error estimation .",
    "we introduce the confidence interval @xmath537 defined as the smallest interval which satisfies @xmath538    on figure [ fig : opus_effectivity ] we see that the confidence intervals are shrinking around @xmath520 when @xmath320 increases , meaning that the preconditioned residual norm becomes a better and better error estimator when @xmath320 increases . again , the positivity constraint is needed for small values of @xmath320 , but we obtain a better error estimation without imposing this constraint for @xmath539 . on the contrary , the standard residual norm leads to effectivity indices that spread from @xmath540 to @xmath541 with no improvement as @xmath320 increases , meaning that we can have a factor @xmath542 between the error estimator @xmath543 and the true error @xmath544 .",
    "we have proposed a method for the interpolation of the inverse of a parameter - dependent matrix .",
    "the interpolation is defined by the projection of the identity in the sense of the frobenius norm .",
    "approximations of the frobenius norm have been introduced to make computationally feasible the projection in the case of large matrices .",
    "then , we have proposed preconditioned versions of projection - based model reduction methods .",
    "the preconditioner can be used to define petrov - galerkin projections on a given approximation space with better quasi - optimality constants by introducing a parameter - dependent test space depending on the preconditioner .",
    "also , the preconditioner can be used to improve residual - based error estimates that are used for assessing the quality of a given approximation , which is required in any adaptive approximation strategy .",
    "different strategies have been proposed for the selection of interpolation points depending on the objective : ( i ) the construction of an optimal approximation of the inverse operator for preconditioning iterative solvers or for improving error estimators based on preconditioned residuals , ( ii ) the improvement of the quality of petrov - galerkin projections of the solution of a parameter - dependent equation on a given reduced approximation space , or ( iii ) the re - use of operators factorizations when solving a parameter - dependent equation with a sample - based approach .",
    "the performance of the obtained parameter - dependent preconditioners has been illustrated in the context of projection - based model reduction techniques such as the proper orthogonal decomposition and the reduced basis method .",
    "the proposed preconditioner has been used to define petrov - galerkin projections with better stability constants . for the solution of pdes ,",
    "the petrov - galerkin projection has been defined at the discrete ( algebraic ) level for obtaining a better approximation ( in a reduced space ) of the finite element galerkin approximation of the pde .",
    "therefore , for convection - dominated problems , the proposed approach does not avoid using stabilized finite element formulations .",
    "similar observations can be found in @xcite .",
    "however , a petrov - galerkin method could be defined at the continuous level with a preconditioner being the interpolation of inverse partial differential operators . in this continuous framework",
    ", the preconditioner would improve the stability constant for the finite element galerkin projection and may avoid the use of stabilized finite element formulations .",
    "such petrov - galerkin methods have been proposed in @xcite for convection - dominated problems ( as an alternative to standard stabilization methods ) , which can be interpreted as an implicit preconditioning method defined at the continuous level .    in the present paper ,",
    "the parameter - dependent preconditioner is obtained by a projection onto the space generated by snapshots of the inverse operator .",
    "when the storage of many inverse operators ( even as implicit matrices ) is not feasible , a parameter - dependent preconditioner could be obtained by a projection into the linear span of preconditioners , such as incomplete factorizations , sparse approximate inverses , h - matrices or other preconditoners that are readily available for a considered application . also , we have restricted the presentation to the case of real matrices but the methodology can be naturally extended to the case of complex matrices .                    , _ streamline upwind / petrov - galerkin formulations for convection dominated flows with particular emphasis on the incompressible navier - stokes equations _ , comput",
    "appl . m. , 32 ( 1982 ) , pp .  199259 .                                                              ,",
    "_ a posteriori error bounds for reduced - basis approximation of parametrized noncoercive and nonlinear elliptic partial differential equations _ , proceedings of the 16th aiaa computational fluid dynamics conference , 03 ( 2003 ) , pp",
    ".  20033847 ."
  ],
  "abstract_text": [
    "<S> we propose a method for the construction of preconditioners of parameter - dependent matrices for the solution of large systems of parameter - dependent equations . </S>",
    "<S> the proposed method is an interpolation of the matrix inverse based on a projection of the identity matrix with respect to the frobenius norm . </S>",
    "<S> approximations of the frobenius norm using random matrices are introduced in order to handle large matrices . </S>",
    "<S> the resulting statistical estimators of the frobenius norm yield quasi - optimal projections that are controlled with high probability . </S>",
    "<S> strategies for the adaptive selection of interpolation points are then proposed for different objectives in the context of projection - based model order reduction methods : the improvement of residual - based error estimators , the improvement of the projection on a given reduced approximation space , or the re - use of computations for sampling based model order reduction methods . </S>"
  ]
}