{
  "article_text": [
    "fundamental neuroscience problem is to understand how neurons encode and process information @xcite . in general",
    "it is not easy to determine the neural code structure . since adrian s experiments @xcite which established that individual sensory neurons produce action potentials , or spikes it is assumed that a single neuron provides information just through spikes sequence , i.e. spike - trains .",
    "although it is now generally accepted that a spike sequence is the way the information is coded by a single neuron , the structure and the mechanisms of code formation are still mysteries . in 1976 burns and",
    "webb @xcite for the first time showed that the total number of emitted spikes arrives in a highly irregular manner .",
    "when the same stimulus is applied repeatedly the number of spikes varies substantially from trial to trial @xcite",
    ". this has inclined neuroscientists to formulate very different hypotheses about the nature of the neural code .",
    "two main ideas , not excluding each other , are of special interest .",
    "the first theory is based on the idea of `` temporal code '' @xcite and goes into the spike -trains structure while the second referred to as `` rate code '' theory @xcite assumes that the neural code is embedded in the spike frequency , defined as the number of spikes emitted per second .",
    "the temporal coding mechanism , which builds a relationship of temporal process between the output firing patterns and the inputs of the nervous system , has received much attention @xcite .",
    "on the other hand in the transfer of information the most expansive energetically is spiking process @xcite , thus in the first approximation the firing rate can be treated as the energy marker .",
    "inspired by the thermodynamics @xcite we also consider the derivative of entropy over energy which is the analog of temperature inverse .    in this article",
    "we give the theoretical insight into understanding the neural code nature , namely we study this problem for two types of binary information sources .",
    "assuming that the information transmitted by a neuron is governed by uncorrelated stochastic process or by process with a memory we study the relation between the information transmission rates itr carried by such spike - trains and their firing rate @xmath0 . to this",
    "end the information - firing - quotient @xmath1 , being the ratio of information and firing rate , is introduced in section [ info ] . for large @xmath1 transmission is more optimal in the sense of information amount transmitted at the cost of unit energy .",
    "we show that the crucial role in studying @xmath1 properties is played by the `` jumping '' parameter .",
    "this parameter is the sum of transition probabilities from no - spike - state to spike - state and vice versa .",
    "we show that for the low values of jumping parameter the quotient of information and firing rates is monotonically decreasing function of firing rate , thus there is straightforward , one - to - one , relation between temporal and rate codes . on the contrary",
    ", it turns out that for large enough jumping parameter this quotient is non - monotonic function of firing rate and it exhibits well pronounced global maximum .",
    "thus , in this case the optimal firing rate exists .",
    "moreover , there is no one - to - one relation between information and firing rate and the temporal and rate codes differ qualitatively .",
    "the behavior of the quotient of information and firing rates for large jumping parameter is especially important in the context of bursting phenomenon@xcite .",
    "the paper is organized as follows . in section [ info ]",
    "the basic concepts of information theory and formulas concerning bernoulli and markov processes are briefly recalled .",
    "the comparison of information transmission and firing rates for these processes is presented in section [ results ] .",
    "the last section contains discussion and conclusions .",
    "in neuroscience the information transfer is quantified by many authors in terms of information theory @xcite . in general , neuronal communication systems are represented by information source , communication channel and output signals @xcite .",
    "both messages coming from information source and output signals are represented by sequences of symbols @xcite .",
    "these sequences can be understood as trajectories of stationary stochastic processes . in this paper",
    "we study the information sources which are represented by bernoulli or markov processes @xcite .    [",
    "[ entropy ] ] entropy + + + + + + +    first , we briefly recall the fundamental concepts of information theory @xcite that are adapted to neuroscience issues .",
    "let @xmath2 be a set of all words ( i.e. blocks ) of length @xmath3 , built of symbols ( letters ) from some finite alphabet @xmath4 .",
    "each word @xmath5 can be treated as a message sent by information source @xmath6 being a stationary stochastic process .",
    "if @xmath7 denotes the probability the word @xmath8 occurs , then the information in shannon sense carried by this word is defined as @xmath9 in this sense , less probable events carry more information .",
    "we use the natural logarithm to get more compact form of the formulas . in case",
    "when logarithm to the base 2 is used , the factor of @xmath10 appears . expected or average information of @xmath2 , called shannon block entropy reads @xmath11 since the word length @xmath3 can be chosen arbitrary , the block entropy does not perfectly describe the information source @xcite . the more adequate characteristics of the information transmission rate is defined in the next subsection . for the special case of two - letter alphabet @xmath12 and the length of words @xmath13",
    "we introduce the following notation for the entropy @xmath14 where @xmath15 are the associated probabilities .",
    "this is , in fact , formula for the entropy of two - state system .",
    "[ [ information - transmission - rate ] ] information transmission rate + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the entropy of spike trains themselves evaluates how much information these spikes could provide .",
    "the adequate measure for estimation of efficiency of information source is the information transmitted in average by a single symbol .",
    "this measure , which characterizes information source @xmath16 , is called information transmission rate @xmath17 and is defined as @xcite @xmath18 information transmission rate exists if and only if the stochastic process @xmath16 is stationary @xcite .",
    "the information transmission rate is very important quantity especially due to the asymptotic equipartition theorem .",
    "this theorem states that information per symbol for most of the messages coming from a given source is close to @xmath17 @xcite .",
    "[ [ firing - rate ] ] firing rate + + + + + + + + + + +    since the experiment of adrian @xcite very important characterization of both neural network dynamics and neural computation is the firing rate @xmath0 of spike - trains .",
    "the first and most commonly used definition of the firing rate refers to temporal average @xcite and reads @xmath19 where @xmath20 denotes spike count and @xmath21 is time window length . in practice , in order to get sensible averages , some reasonable number of spikes should occur within the time window . since the messages are trajectories of stationary stochastic process the firing rate as defined by ( [ fr ] ) is specyfic for a given information source provided @xmath21 is large enough .",
    "thus , @xmath22 can be identified with the probability p of spike appearance , where @xmath23 is the time resolution or bin size .    [",
    "[ bernoulli - process ] ] bernoulli process + + + + + + + + + + + + + + + + +    assuming the size of bin spike trains can be encoded @xcite in such a way that 1 is generated with probability @xmath24 ( spike is arrived in the bin ) , 0 is generated with probability @xmath25 ( spike is not arrived ) . in the situation when consecutive bits in message are uncorrelated , we are in the regime of bernoulli process @xcite . following the entropy definition ( [ entropy ] ) the information transmission rate ( [ fr ] ) for bernoulli process reads @xmath26 further in the paper the bernoulli process will be considered as a benchmark for more complex processes like the markov ones .",
    "[ [ markov_process ] ] markov process + + + + + + + + + + + + + +    again assuming the size of bin we now consider as the information source the discrete - time , two - state markov processes",
    ". the conditional probabilities for such processes are completely defined in terms of two transition probabilities from state 0 to state 1 , @xmath27 and from state 1 to state 0 , @xmath28 .",
    "the markov transition probability matrix @xmath29 can by written as @xmath30 \\ .\\ ] ] we assume here that the process is homogeneous in time .",
    "the probability evolution is governed by master equation @xcite @xmath31= \\left[\\begin{array}{ccc } 1-p_{1|0 } & p_{0|1 } \\\\ & \\\\",
    "p_{1|0 } & 1-p_{0|1 } \\nonumber \\end{array }   \\right]\\             \\cdot \\left[\\begin{array}{ccc } p_{n}(0 ) \\\\ & \\\\ p_{n}(1 ) \\end{array } \\right]\\ , \\ ] ] where @xmath32 stands for the discrete time , and the stationary solution reads @xmath33= \\left[\\begin{array}{ccc } p_{0|1}/(p_{0|1}+p_{1|0 } ) \\\\ & \\\\ p_{1|0}/(p_{0|1}+p_{1|0 } ) \\end{array } \\right]\\ .\\ ] ] the information transmission rate ( [ itr_3 ] ) of such a markov source reads @xcite .",
    "@xmath34 or making use of notation ( [ h1 ] ) , in compact form @xmath35 for the later use the probability of state `` 1 '' is for short denoted by @xmath24 @xmath36 and in fact is interpreted as the firing rate .",
    "please , note that for the special case when @xmath37 the markov process becomes uncorrelated and reduces to bernoulli process with @xmath38 .",
    "it is well known that the `` temporal code '' approach requires the reliable estimation of information transmission rate which must take into account the patterns and temporal structures @xcite , while the `` rate code '' is determined just by the firing rate , which in turn is fully given by the probability @xmath24 . addressing the problem of relation between `` temporal code '' and `` rate code '' we introduce",
    "the information - firing - quotient @xmath1 defined as the ratio @xmath39 @xmath1 can be understood as the information cost in terms of the energy units .",
    "further we analyze the @xmath1 for messages coming from two qualitatively different information sources , namely bernoulli and markov processes .",
    "[ fig1 ]        [ cols=\"^ \" , ]      +      let us consider the information - firing - quotient formula for information source being a bernoulli process with probability parameter @xmath24 . for this source",
    "we denote the @xmath1 by @xmath40 .    using ( [ itr_3 ] ) and ( [ itr_2 ] )",
    "we get @xmath41 for @xmath42 .",
    "now , we evaluate the derivative of the quotient @xmath40 over the firing rate @xmath24 ( corresponding to energy ) , which has the form @xmath43 for @xmath42 . in order to find lower and upper bounds of these expressions",
    "we make use of the following inequalities @xmath44 which hold for @xmath45 and the inequality @xmath46 which is true for @xmath47 .    applying ( [ bound1 ] ) and ( [ 4ln ] ) to ( [ bp1 ] )",
    "we obtain the following bounds for @xmath40 @xmath48 and making use of ( [ dbdt ] ) and ( [ bound1 ] ) we get bound of the derivative of @xmath40 @xmath49 introducing the following notation @xmath50 we express ( [ 4ln ] ) and ( [ 4ln2 ] ) in the compact forms @xmath51 further on we show that these bounds can be interpreted as benchmarks for more complex processes such as markov processes .",
    "consider as an information source the two - state markov process . under the notation from section [ markov_process ]",
    "we introduce the `` jumping '' parameter @xmath52 , which in fact can be interpreted as the tendency of transition from one state to the other state @xmath53 as we show below this parameter plays a crucial role in qualitative behavior of the @xmath1 coefficient .",
    "observe that for markov case @xmath54    and for @xmath55 the firing frequency @xmath24 is in the full interval @xmath56 $ ] , while for @xmath57 it is limited to the smaller interval @xmath58 , i.e. @xmath59 thus , @xmath24 is localized symmetrically around @xmath60 .",
    "note that for @xmath61 the spike probability @xmath24 is well separated from zero .    for the markov source",
    "we denote the @xmath1 indicator by @xmath62 . using ( [ itr_markov_case ] ) and ( [ p_markov_case ] ) we have @xmath63 making use of ( [ p ] )",
    "we express @xmath62 in terms of @xmath24 and @xmath64 , @xmath65 where @xmath66 is given by ( [ h1 ] ) .",
    "observe that for @xmath67 there are the following limits @xmath68 and for @xmath61 @xmath69}{s-1}=\\frac{h_{1}(s-1)}{s-1 } \\ , \\ ] ] @xmath70=h_{1}(s-1 ) \\ .\\ ] ] next we evaluate the derivative of the quotient @xmath71 over the firing rate @xmath24 @xmath72}-s\\ln{(1-sp)}+s\\ln{(s(1-p))}+\\frac{\\ln{(1-ps)}}{p^{2}}\\ .\\ ] ] making use of ( [ bound1 ] ) and ( [ 4ln ] ) we obtain the following bounds @xmath73 and referring to the bernoulli bounds ( [ 3b1 ] ) and ( [ 3b2 ] ) we arrive to the following limits @xmath74 notice that the left bound is maximal for @xmath75 , i.e. for bernoulli case .    now applying again ( [ bound1 ] ) and ( [ 4ln ] ) to ( [ dms ] )",
    "we get @xmath76 \\le \\frac{\\mbox{d}}{\\mbox{d}p}m_{s}(p)\\le -s\\left[\\frac{1}{p}-\\frac{(s-1)}{[1-(1-p)s](1-sp)}\\right]\\ ] ] and referring again to the bernoulli bounds ( [ 3b2 ] ) and ( [ 3b3 ] ) we obtain the following limits on derivative of @xmath1 for markov sources @xmath77 \\le \\frac{\\mbox{d}}{\\mbox{d}p}m_{s}(p ) \\le -s\\left[u_{1}(p)-\\frac{(s-1)}{[1-(1-p)s](1-sp)}\\right ] \\ .\\ ] ] observe , that for @xmath75 , inequalities ( [ limits ] ) and ( [ last ] ) reduce to inequalities ( [ 4ln3 ] ) and ( [ 4ln4 ] ) respectively , i.e. just to bernoulli case .",
    "moreover , we see that for @xmath64 close to 1 the bounds of @xmath78 and @xmath79 rigorously approach the bernoulli bounds . for ( [ limits ] )",
    "this observation is clear while for ( [ last ] ) it follows from the fact that the deviations from bernoulli bounds , @xmath80 and @xmath81 , contain the factor ( @xmath64 - 1 ) .",
    "we see that for @xmath55 the derivative is negative , @xmath82 , thus @xmath71 is a decreasing function of @xmath24 ( fig .",
    "[ fig1 ] ) and clearly it is significantly larger for small @xmath24 . for @xmath57 case",
    "the behavior of function @xmath71 is qualitatively different ( fig . [ fig2 ] ) .",
    "it is non - monotonic and it has a global maximum .",
    "what means that in this case for each s the optimal firing rate exists .",
    "the increasing @xmath64 corresponds to the increasing @xmath27 so in this case the transition from the no - spike - state ( state 0 ) to the spike - state ( state 1 ) occurs more and more often .",
    "this means that for larger @xmath64 the neuron is more firing leading to bursting phenomena .",
    "in this paper we address the fundamental question in neuronal coding .",
    "we analyze the possible correspondence between `` temporal '' and `` firing rate '' coding for two qualitatively different types of information sources . for the first type of source it is assumed that consecutive spikes are uncorrelated , thus it is governed by the bernoulli process . in the second case we assume that there is a short time correlation ( memory ) between consecutive spikes , thus we model this source by the markov process .    for the quantitative study of the relation between temporal and rate coding",
    "we propose the information - firing - quotient being the ratio of information transmission rate and firing rate . since the energy used for transfer of information is proportional to the firing rate this quotient",
    "is understood as amount of information transmitted at the cost of unit energy .",
    "clearly , for larger @xmath1 the transmission is more efficient .",
    "the goal is to find the optimal parameters of transmission .",
    "we found that the crucial role in qualitative and quantitative behavior of @xmath1 is played by the parameter @xmath64 which , in fact , measures the ability of transition from non - spike to spike state and vice versa .",
    "taking into account that in the real biological systems the firing rate is limited from below by the spontaneous activity and very small values of @xmath24 and large values of @xmath1 , are non - realistic .",
    "this situation may happen for @xmath67 ( fig .",
    "[ fig1 ] ) , when @xmath1 is monotonically decreasing with @xmath24 , and then the realistic cut - off of @xmath24 separating it from zero should be assumed . on the other hand for @xmath61 ,",
    "i.e. for more active , say bursting , neurons we observe that the global maximum of @xmath1 exists , thus in this case there is the unique optimal firing rate ( fig .",
    "[ fig2 ] ) well separated from zero .",
    "this leads to the non - intuitive hypothesis that even for bursting phenomenon there may still exist the optimal regime of transmission .",
    "we gratefully acknowledge financial support from the polish national science centre under grant no .",
    "2012/05/b / st8/03010 .",
    "adrian ed . ( 1926 ) the impulses produced by sensory nerve endings : part i , j. physiol . , 61 , 4972 .",
    "amesiii a , ( 2000 ) cns energy metabolism as related to function , brain res .",
    "34 , 4268 .",
    "barlow hb , ( 1961 ) possible principles underlying the transformation of sensory messages , in : rosenblith w ( ed ) sensory communication . mit press , cambridge .",
    "bialek w , rieke f , de ruyter van sttevenick rr , warland d , ( 1991 ) , reading a neural code , science 252 , 18541857 .",
    "borst a , theunissen fe , ( 1999 ) information theory and neural coding , nat .",
    "2 , 947957 .",
    "burns bd , webb ac , ( 1976 ) the spontaneous activity of neurons in the cat s visual cortex , proc .",
    "194(1115 ) , 21123 . butts da , weng c , jin jz , yeh ci , lesica na , alonso jm and et al . , ( 2007 ) temporal precision in the neural code and the time scales of natural vision , nature 449 , 9295 .",
    "coop ad , reeke gn , ( 2001 ) , deciphering the neural code : neuronal discharge variability is preferentially controlled by the temporal distribution of afferent impulses , neurocomputing 3840 , 153157 .",
    "i maio v , ( 2008 ) , regulation of information passing by synaptic transmission : a short review , brain research 1225 , 2638 .",
    "duguid i , sjostrom pj , ( 2006 ) novel presynaptic mechanisms for coincidence detection in synaptic plasticity , cur",
    "16 , 312332 .",
    "gerstner w , kreiter ak , markram h , herz av , ( 1997 ) neuralcodes : firing rates and beyond , proc .",
    "u. s. a. 94 , 1274012741 .",
    "feller w , ( 1958 ) an introduction to probability theory and its applications , united states of america : a wiley publications in statistics , new york .",
    "goldberg dh , andreou ag , ( 2004 ) , spike communication of dynamic stimuli : rate decoding versus temporal decoding , neurocomputing 5860 , 101107 .",
    "hubel dh , wiesel tn , ( 1962 ) receptive fields , binocular interaction and functional architecture in the cat s visual cortex , j. physiol .",
    "( lond . ) 160 , 106154 .",
    "lansky p , sacerdote l , ( 2001 ) the ornstein uhlenbeck neuronal model with the signal dependent noise , phys .",
    "a 285 , 132140 .",
    "moujahid a , danjou a , ( 2012 ) metabolic efficiency with fast spiking in the squid axon , frontiers in computational neuroscience 6 , 18 .",
    "paprocki b. , szczepanski j. , ( 2013 ) how do the amplitude fluctuations affect the neuronal transmission efficiency , neurocomputing 104 , 5056 .",
    "pregowska a , szczepanski j , wajnryb e , ( 2015 ) mutual information against correlations in binary communication channel , bmc neuroscience 16(32 ) , 17 .",
    "ricciardi lm , sacerdoteb l , ( 1979 ) the ornstein uhlenbeck process as a model of neuronal activity , biol .",
    "richmond bj , optican lm spitzer h , ( 1990 ) two dimensional patterns are represented in temporally modulated activity of striate cortex cell , j. neurophysiol .",
    "64(2 ) , 35169 . rieke f , warland d , de ruyter van steveninck rr , bialek w , ( 1997 ) spikes .",
    "exploring de neural code , mit press , cambridge , ma .",
    "shannon ce , weaver w , ( 1963 ) the mathematical theory of communication , united states of america : university of illinois press , urbana .",
    "stein rb , ( 1965 ) a theoretical analysis of neuronal variability , biophys .",
    "j. 5 , 173195 .",
    "van hemmen jl , sejnowski t , ( 2006 ) 23 problems in systems neurosciences , oxford university press , oxford .",
    "yu q , tang h , tan kch , yu h , ( 2014 ) a brain - inspired spiking neural network model with temporal encoding and learning , neurocomputing 138 , 313 .",
    "ash rb , ( 1965 ) , the mathematical theory of communication , john wiley and sons , new york , london , sydney , united states of america .",
    "cover tm , thomas ja , ( 1991 ) elements of information theory , a wiley - interscience publication , new york , united states of america .",
    "arnold m , szczepanski j , montejo n , wajnryb e , sanchez - vives mv , ( 2013 ) information content in cortical spike trains during brain state transitions , journal of sleep research 2 , 1321 .",
    "strong sp , koberle r , de ruyter van steveninck rr , bialek w , ( 1998 ) entropy and information in neural spike trains , physical review letters 80(1 ) , 197200 .",
    "lempel a , ziv j , ( 1976 ) on the complexity of individual sequences , ieee transactions on information theory 22 , 7581 .",
    "kontoyiannis i , algoet ph , suhov ym , wyner aj , ( 1998 ) nonparametric entropy estimation for stationary processes and random fields , with applications to english text , ieee transactions on information theory 44 , 13191327 .",
    "amigo jm , keller k , unakafova va , ( 2015 ) ordinal symbolic analysis and its application to biomedical recordings , philos .",
    "trans . a math .",
    ", 373(2034 ) , pii : 2014009 .",
    "an kampen ng , ( 2007 ) stochastic processes in physics and chemistry , north - holland personal library .",
    "gerstner w , kistler wm , naud r , paninski l , ( 2014 ) neuronal dynamics , from single neurons to networks and models of cognition , cambridge university press , cambridge .",
    "mukherjee p , kaplan e , ( 1995 ) dynamics of neurons in the cat lateral geniculate nucleus : in vivo electrophysiology and computational modeling , j. neurophysiol .",
    "74(3 ) , 12221243 .",
    "reinagel p , godwin d , sherman sm , koch c , ( 1999 ) encoding of visual information by lgn bursts , j. neurophysiol .",
    "81(5 ) , 255869 ."
  ],
  "abstract_text": [
    "<S> neuroscientists formulate very different hypotheses about the nature of neural code . at one extreme , it has been argued that neurons encode information in relatively slow changes of individual spikes arriving rates ( rates codes ) and the irregularity in the spike trains reflects noise in the system , while in the other extreme this irregularity is the code itself ( temporal codes ) thus the precise timing of every spike carries additional information about the input . </S>",
    "<S> it is well known that in the estimation of shannon information transmission rate the patterns and temporal structures are taken into account , while the `` rate code '' is already determined by the firing rate , i.e. by spike frequency . in this paper </S>",
    "<S> we compare these two types of codes for binary information sources which model encoded spike - trains . assuming that the information transmitted by a neuron is governed by uncorrelated stochastic process or by process with a memory we compare the information transmission rates carried by such spike - trains with their firing rates . </S>",
    "<S> we showed that the crucial role in studying the relation between information and firing rates is played by a quantity which we call `` jumping '' parameter . </S>",
    "<S> it corresponds to the probabilities of transitions from no - spike - state to the spike - state and vice versa . for low values of jumping parameter the quotient of information and firing rates is monotonically decreasing function of firing rate , </S>",
    "<S> thus there is straightforward , one - to - one , relation between temporal and rate codes . on the contrary </S>",
    "<S> , it turns out that for large enough jumping parameter this quotient is non - monotonic function of firing rate and it exhibits a global maximum , i.e. in this case there exists the optimal firing rate . </S>",
    "<S> moreover , there is no one - to - one relation between information and firing rates , so the temporal and rate codes differ qualitatively . </S>",
    "<S> this leads to the observation that the behavior of the quotient of information and firing rates for large jumping parameter is especially important in the context of bursting phenomena .    </S>",
    "<S> information theory , information source , stochastic process , information transmission rate , firing rate </S>"
  ]
}