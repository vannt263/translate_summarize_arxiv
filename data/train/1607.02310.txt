{
  "article_text": [
    "multiple compositional distributional semantic models have been proposed in the past several years .",
    "most models are based on a vector representation for words and a separate function that performs composition on the vectors @xcite .",
    "another line of investigation represents atomic types ( mainly nouns and sentences ) as vectors but predicate types , for example adjectives and verbs , as functions that act upon the atomic types or other functions @xcite .    in the full form of the categorial framework @xcite , each lexical item and type pairing",
    "is represented by a tensor whose order is determined by the number of atomic - type arguments in its category . if we use combinatory categorial grammar ( ccg ) as the basis for further discussion in this paper , the noun phrase type is , which is represented by a first order tensor , i.e. , a vector .",
    "an adjective category is , and is modelled by a matrix ( a second order tensor ) , and a transitive verb is a third order tensor whose category is .",
    "the verb category is interpreted as looking for a noun phrase to the right ( object ) , a noun phrase to the left ( subject ) , and when these are found the function results in a complete sentence . since the subject and the object are represented by vectors and",
    "the verb is a tensor , the natural composition operation is tensor contraction , which is equivalent to matrix multiplication for second order tensors .",
    "the result of the composition between the tensor , the subject , and object vectors is a vector representing a composed sentence ( figure  [ svocomp ] ) .",
    "while the type - based approach has strong theoretical grounding @xcite and integrates well with categorial grammars @xcite , there are two major practical challenges .",
    "the first is that there are many type - lexeme combinations and some of the types lead to high - order tensors .",
    "this results in a large number of parameters that need to be estimated and stored .",
    "the second challenge is that some of the type - lexeme combinations are very rare , and , therefore , may never be observed in the training data , leading to difficulties in accurate parameter estimation for those types .    in this paper , we propose a method to address the second challenge and we evaluate it both on the adjective and transitive verb case .",
    "we build on previous empirical work on the lexical function ( lf ) model @xcite , which considers each adjective to be a matrix that transforms a noun vector into a vector encoding the adjective - noun phrase in the same vector space the original noun , and the contemporaneous categorial framework ( cf ) @xcite , which applies the idea of words as functions to all word classes other than nouns .",
    "the type - based models have a solid theoretical grounding @xcite neatly combining mathematical category theory and categorial grammars like ccg with linear algebra to represent the meaning of argument taking words as functions modelled in vector space . the tensors in cf",
    "provide a way of representing words like verbs as functions that can recognise the prototypical combinations of features that are expected from their arguments .",
    "so , for example , a function representing the verb _ eat _ would not only encode that it expects animate subjects that consume edible objects , but ultimately the types of edible objects that make sense with particular subjects .",
    "this is different from most other methods @xcite where all words are represented as vectors , and combined with operators of various complexity which themselves do not encode any semantics .",
    "due to the parameter explosion problems mentioned in section  1 , the cf implementations have mostly been tested on restricted constructions such as adjective - noun or subject - verb - object phrases , while the neural - network - based approaches have been optimised for and tested on full - sentence tasks @xcite . by addressing the problem of training data sparsity",
    ", we hope to bring the categorial framework a step closer to full implementation , so that it can be compared to other models on a variety of tasks .",
    "various approximations of cf which curb the number of parameters have previously been implemented @xcite , but most of those changed the shape of the tensors in some way that diminishes the spirit of the full model . of these ,",
    "the practical lexical function ( plf ) model of @xcite comes closest to full sentence implementation of a type - based semantic model .",
    "it extends the type - based approach to full sentences by representing argument taking types as a set of matrices , each of which interacts with one argument .",
    "while this has the effect of reducing the number of parameters , it also decouples the interactions between arguments , which is one of the main strengths of the tensor - based model .",
    "in contrast , @xcite provide a mathematically principled way of achieving parameter reduction while preserving the shape of tensors , and hence the interactions between the arguments , by employing tensor decomposition .",
    "the lack of training data has previously been partially addressed in @xcite by training third - order verb tensors in two steps in order to take advantage of more plentiful verb - object training data . on the other hand , @xcite",
    "show that it is possible to train full verb tensors with a single - step multi - linear regression method which is the basis of the approaches described in this paper .",
    "@xcite introduce the generalised lexical function ( glf ) , a method for overcoming the sparsity of training data for adjectives .",
    "we reimplement this method for comparison in this paper .",
    "the two methods that we develop in this paper were loosely inspired by multitask learning @xcite and retrofitting @xcite .",
    "in this section we first describe the methods used to train second and third - order tensors as function representations of adjectives and transitive verbs respectively .",
    "we also consider the low - rank representations @xcite , which were demonstrated to produce competitive performance with fewer parameters .",
    "we then describe the methods introduced in this paper , that is the full and low - rank training with parameter sharing . and",
    "finally , we describe the glf model @xcite , which is introduced as a competitive baseline for the adjective methods .",
    "all methods have been implemented using gradient descent , rather than analytic regression .",
    "we model each adjective as a linear function that maps a noun to an adjective - noun phrase .",
    "the @xmath0-dimensional noun vectors ( @xmath1 ) are transformed into @xmath0-dimensional noun phrase vectors via an @xmath2 matrix @xmath3 , just as in the lexical function ( lf ) model of @xcite .",
    "the loss function consists of minimising the error between the vector resulting from the adjective - noun multiplication and a distributional vector representing the adjective - noun phrase ( @xmath4 ) :    @xmath5    [ [ low - rank - adjectives ] ] low - rank adjectives + + + + + + + + + + + + + + + + + + +    we learn low - rank adjective matrices by fixing a maximal rank @xmath6 and maintaining each matrix in a rank - decomposed form , which is similar to the singular value decomposition ( svd ) @xcite .",
    "the low - rank representation for an adjective @xmath3 is @xmath7 where @xmath8 are parameter matrices , @xmath9 gives the @xmath10th row of matrix @xmath11 , and @xmath12 is the tensor product .",
    "the adjective matrix s action on vectors is then given by @xmath13      we model each transitive verb as a bilinear function mapping subject and object noun vectors , each of dimensionality @xmath0 , to a single sentence vector of dimensionality @xmath14 .",
    "each transitive verb @xmath15 is associated with a third - order tensor @xmath16 , which defines this bilinear function .",
    "if vectors @xmath17 , @xmath18 for subject and object nouns , respectively , then the loss function for each verb is : @xmath19    that is the error between the sentence vector produced by applying _ tensor contraction _ ( the higher - order analogue of matrix multiplication ) to the verb tensor and two noun vectors and the distributional representation for that sentence @xmath20 .",
    "@xcite examine several different distributional sentence spaces ; from these we chose the intra - sentential contextual sentence space consisting of content words that occur within the same sentences as the svo triple , disregarding the verb itself .",
    "[ [ low - rank - verbs ] ] low - rank verbs + + + + + + + + + + + + + +    following @xcite , we use _ canonical polyadic ( cp ) decomposition _ representation of verb tensors .",
    "cp decomposition factors a tensor into a sum of @xmath6 tensor products of vectors , reducing the number of parameters we have to learn .",
    "the low - rank representation for a verb @xmath21 is : @xmath22 where @xmath23 are parameter matrices . representing tensors in this form allows us to avoid explicitly generating the full tensor by formulating the verb tensor s action on noun vectors as matrix multiplication :    @xmath24    where @xmath25 is the elementwise vector product . as a result",
    "we are able to reduce the number of parameters needed to model each verb from @xmath26 to @xmath27 , which in our experiments where we have @xmath28 and @xmath29 means a reduction from @xmath30 to @xmath31 parameters per verb .                      while low - rank methods reduce the amount of memory required to store a full lexicon and the amount of time required to train the tensors , they do not address the problem of data sparsity . to train tensors we need high - quality examples , which potentially have to be extracted from parsed data",
    ". however , there are word - types for which there are few reliable training examples and others for which there are no training examples at all . in those cases we would still like to have a non - zero approximation for the particular function .",
    "we propose two approaches to address these instances of sparsity .",
    "both approaches are based on the existence of an external method that gives similarity between the words for which we are trying to build tensors .",
    "we define this as a function @xmath32 which gives us the similarity between words @xmath33 corresponding to tensors @xmath34 and @xmath35 , where @xmath36 refers to either adjective matrices or verb third - order tensors as the approaches are analogous across the types .",
    "in addition , we only use the @xmath37 top most similar tensors according to @xmath38 , where @xmath37 is currently a manually chosen parameter .",
    "the method which provides the similarities in @xmath38 could be manual or derived from an ontology or any distributional or distributed representation of these words .",
    "if the method relies on vectors , then these do not have to match the training data at all , as we only rely on a matrix of similarity values between all pairs of adjectives ( or separately verbs ) that we are training .",
    "the first approach shares parameters between tensors that we have declared to be similar by directly creating a weighted average of the target tensor with the sum of the tensors of the @xmath37 closest words ( weighted by the similarity values from @xmath38 ) .",
    "this approach is somewhat heavy handed and we expect that it produces large jumps around the error landscape which is being navigated by gradient descent .",
    "the second approach is more gentle and uses a regularisation component to push a tensor closest to its nearest neighbours ( according to @xmath38 ) by encouraging smaller distances between them . in the experimental sections we apply these two methods individually and together .",
    "we compare our methods to the glf model @xcite on the adjective - based datasets .",
    "glf is a third order tensor that is used to generate adjective matrices using adjective vectors .",
    "it is trained using lexical function adjective matrices .",
    "the advantage of our methods over glf is that we are not restricted by the source of our word - word similarities , where glf requires adjective vectors that are sourced from the same corpus as the lexical function training data .",
    "word vectors created from another corpus , even with the same method , may lie in a different vector space and thus be incompatible with the tensor and matrices trained in the original space .",
    "in addition we introduce the deterministic function @xmath38 and two tuneable scalar parameters each of which balances the contributions of one of the methods . on the other hand",
    ", glf requires construction of an additional @xmath39 sized tensor , which in the verb case would have to be extended to a fourth order tensor .      in this first method",
    "we share parameters between most similar tensors using the function @xmath38 .",
    "we adjust the appropriate loss function ( eq .  [ eqn : adj_loss ] or eq .",
    "[ eqn : verb_loss ] ) to incorporate parameter sharing ( ps ) during gradient descent : @xmath40    the parameter @xmath41 balances the amount of tensor we are replacing by the normalised sum of the nearby tensors . in case of the low - rank representations of tensors , we use the deconstructed versions of the tensors and share the parameters between corresponding decomposed matrix representations by aligning the @xmath11 , @xmath42 , and for verbs @xmath43 , for the word pairs without reconstructing the tensors .",
    "the second method is used in place of @xmath44-regularisation to push the parameters of the current tensor closer to the parameters of the tensors of the similar words .",
    "the regularisation component is @xmath45    and is integrated into the training function via the parameter @xmath46 : @xmath47 like with ps , in the low - rank representations we regularise each of the component matrices separately .",
    "the glf model @xcite addresses training data sparsity by introducing a third order tensor ( @xmath48 ) that acts as a function that takes in an adjective vector @xmath49 and generates a matrix @xmath50 for that adjective .",
    "we train the tensor using gradient descent to minimise the error between the adjective matrix produced by glf and the adjective matrices generated by the lexical function ( @xmath51 ) , as described in equation  [ eqn : adj_loss ] .",
    "@xmath52    we vary slightly from the loss described by @xcite in that we use the standard frobenius norm instead of the straight subtraction .",
    "the difference between the two methods we introduced and glf is that glf does require adjective vectors which are created by the same procedure as the adjective training data ; therefore , if an adjective is completely unavailable in the distributional training corpus , this method could not produce a matrix representation for that word .    for the sparse experiments we train the glf as above , but @xmath53",
    "are reconstituted low - rank matrices .",
    "therefore , this method will still have many more parameters than the true low - rank approximations .",
    "we use several datasets that test composition or directly compare the quality of the produced tensors :    * ml10 : * adjective - noun ( an ) pairs rated for similarity @xcite .",
    "+ * men : * word - word pairs rated for relatedness from which we extract the adjective - adjective pairs only @xcite . +",
    "* simlex : * word - word pairs rated for similarity from which we extracted the adjective - adjective and verb - verb pairs @xcite . +",
    "* gs11 : * a verb disambiguation dataset consisting of subject - verb - object ( svo ) triples arranged in pairs , where in each pair the subject and the object remain the same but the verb changes @xcite . + * ks14 : * a dataset subject - verb - object sentence pairs rated for similarity @xcite , which is an extension of the verb - object component of the ml10 dataset . +",
    "* anvan : * a verb disambiguation dataset containing pairs of adjective - noun - verb - adjective - noun sentences where only the verb varies @xcite .      in order to train the tensors for the adjectives and verbs occurring in the above test data we need to find examples of their usage in text .",
    "we use the october 2013 dump of wikipedia articles , which was tokenised using the stanford nlp tools , lemmatised with the morpha lemmatiser @xcite , and parsed with the c&c parser @xcite .",
    "we use the parser output to find adjective - noun and subject - verb - object combinations that involve our target words . from these",
    "we choose up to 500 tuples that contain nouns that occur at least 100 times and which themselves occur at least twice .",
    "some words are quite rare and do not have any training data , e.g. the adjective _ ashamed _ , or very little training data , e.g. adjectives _ glad _ and _ gritted _ , each of which has a single training example .",
    "the vectors for nouns and the holistic vectors for the an and svo phrases are generated using the paragraph vector @xcite model from the modified word2vec @xcite code .",
    "the lf model forms the basis of all the collaborative training models . the adjectives and verbs",
    "are trained up to 200 iterations using batched gradient descent with adadelta @xcite , at which point most of the tensors have finished training .",
    "the stopping criterion for adjectives is stagnation or an increase in training error . for verbs",
    "we also use a 10% validation dataset if there are at least 20 training points .",
    "full tensor training also uses @xmath44 regularisation with parameter 0.1 , while sparse tensors are trained without regularisation as this was observed to be more optimal in @xcite .",
    "we then use the pre - trained adjective matrices to train the glf model for 10,000 iterations .",
    "the original glf algorithm specifies training until convergence , but we found little improvement and sometimes observed overfitting with further training .",
    "the combined ps and ft models are trained for up to 200 iterations using the same stopping criteria as above .",
    "more training did not lead to significant improvements .",
    "we use ml10 as a development dataset and test a range of mixing parameters for ps and ft , and how they work together .",
    "there can be two types of sparsity that occur in data : inadequate amount of training data ( ab1 ) and complete lack of training data ( ab2 ) .",
    "we implement both of these test environments separately in order to observe the effects on the algorithms . in both cases",
    ", we assume that : we have some information about adjective - adjective similarities ; the non - zero adjectives are pre - trained for 200 epochs and then for 200 more epochs using a combined ps and ft loss function ; and the parameters which are being shared are updated after each epoch .",
    "method & 1% & 5% & 30% & 70% & 100% & 1% & 5% & 30% & 70% & 100% & 1% & 5% & 30% & 70% & 100% +    ps+ft@xmath54 & 0.24 & 0.40 & 0.43 & 0.46 & * 0.49 * & & * 0.38 * & 0.29 & 0.33&*0.61 * & & * 0.47&*0.43&*0.45&*0.55 + ps+ft@xmath55 & & 0.42 & 0.44 & 0.46 & * 0.49 * & & 0.36 & 0.31 & 0.33 & * 0.61 * & * 0.39&0.45&0.40&*0.45*&*0.55 * + glf & 0.19 & * 0.47 * & * 0.49 * & 0.46 & 0.47 & & 0.33 & * 0.58 * & * 0.57 * & 0.50 & & 0.22 & 0.34 & 0.33&0.35 + lf & - & - & & & 0.47 & - & - & & & 0.39 & -&- & & & + * * * * *    * low rank * & & & + ps+ft@xmath56 & 0.25 & 0.12 & 0.37 & 0.42 & 0.45 & 0.33 & & & 0.52 & 0.54 & * 0.61&*0.51&0.32&0.32&0.38 + ps+ft@xmath57 & & * 0.39 & 0.32 & 0.39 & 0.47 & 0.24 & * 0.40*&0.38&0.45 & * 0.62 * & 0.54 & 0.50&*0.43&*0.45&*0.49 + glf & 0.18&*0.39*&*0.49*&*0.44*&*0.48*&&0.25&*0.55 * & * 0.54 & 0.48 & & 0.21&0.40 & 0.38 & 0.40 + lf & -&- & & & 0.45&-&-&&&0.39 & - & - & & & + * * * * * * *    [ cols=\"<,^,^,^,^,^,^,^,^,^,^,^,^,^,^,^ \" , ]",
    "since the goal of training tensors for the categorial framework is good performance in composition tasks , we tuned our parameters on an adjective - noun composition dataset ( ml10 ) . in table",
    "[ table : exp1res ] we can see that when we have only 2 adjectives with training data ( 1% column ) we have acceptable performance on this dataset ; but , the performance on the datasets where we compare the adjective matrices directly ( men and simlex ) is statistically non - correlated and sometimes negative",
    ". nearest neighbours analysis of adjectives produced by the parameter settings @xmath58 and @xmath59 shows that similarities between all adjectives approach one and lead to nonsensical rankings .",
    "this is due to the fact that the numbers in the matrices themselves are very low and approach machine precision .",
    "using cosine similarity leads to elementwise multiplication between low numbers and hence near - zero values in the numerator . on the other hand",
    ", composition is performed using matrix multiplication between the adjective matrices and the noun vectors , which have much larger numbers , resulting in the more sensible performance in the composed datasets .",
    "an alternative way of evaluating the quality of the tensors would be to treat them as functions , and instead of cosine employ the function comparison within the type - driven framework introduced by @xcite .",
    "another interesting phenomenon we observed is that ps+ft works better than lf when all adjective training data is available .",
    "so if we compare the nearest neighbours for adjectives produced by @xmath58 setting to the ones produced with lf , we can notice qualitative improvement ( table  [ nnex ] ) . keeping in mind that our pool of nearest neighbours is limited to the 297 adjectives in our training data",
    ", we can see subtle differences in the adjective _ yellow _ , which is well represented with a large amount of training data and nearest neighbours .",
    "although all of the closest terms are chromatic , _ orange _ and _ red _ are the closest when collaborative training is involved . for the underrepresented adjective _",
    "outdoor _ we can see that ps+ft finds more semantically related and less general adjectives although true neighbours are not available .    for verbs we found that ps+ft often does worse than the straight - forward tensor method ( tables  [ table : exp1resverb ] and [ table : exp2resverb ] ) . in table",
    "[ nnex ] , we can see an example of easy to train verb _ to play _ where ps+ft does indeed rate a similar term _",
    "participate _ highly .",
    "in contrast , the verb _ to entangle _ is rarer , and hence would have less training data and poorer vector representation .",
    "the closest term is _ win - over _ , a verb which is artificially hyphenated in the anvan dataset and hence has no naturally occurring training data . together",
    "adjective and verb results indicate that a larger training pool from which we can choose related tensors may produce a better representations overall .",
    "in this paper we introduced two methods that address the lack of training data in the type - driven framework for compositional distributional semantics . in our experiments",
    "we use distributed vectors which have been found to achieve state - of - the - art results on some of the datasets we used here @xcite ; however , the goal here was to compare these methods to the standard regression approach where each tensor is trained separately .",
    "we find that for both full tensors and low - rank approximations collaborative training enables training of tensors under conditions where individual training would result in low - quality or null tensors .",
    "in addition these methods often outperform the individual training even with all of the available training data ."
  ],
  "abstract_text": [
    "<S> type - based compositional distributional semantic models present an interesting line of research into functional representations of linguistic meaning . </S>",
    "<S> one of the drawbacks of such models , however , is the lack of training data required to train each word - type combination . in this paper </S>",
    "<S> we address this by introducing training methods that share parameters between similar words . </S>",
    "<S> we show that these methods enable zero - shot learning for words that have no training data at all , as well as enabling construction of high - quality tensors from very few training examples per word . </S>"
  ]
}