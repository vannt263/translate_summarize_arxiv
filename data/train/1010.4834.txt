{
  "article_text": [
    "graphics processing units ( gpus ) have become important in providing processing power for high performance computing applications .",
    "cuda @xcite is a proprietary api and set of language extensions that works only on nvidia s gpus and call a piece of code that runs on the gpu , a kernel .    in 2007 , nvidia released cuda for gpu computing as a language extension to c. cuda makes the gpu programming and computing development easier and more efficient than the earlier attempts , using opengl and associated shader languages , in which it was necessary to translate the computation to a graphics language @xcite .",
    "the most successful theories that describe elementary particle physics are the so called gauge theories .",
    "su(2 ) is an interesting gauge group , either to simulate the electroweak theory , or to use as a simplified case of the su(3 ) gauge group of the strong interaction .",
    "gauge theories can be addressed by lattice field theory in a non - perturbative approximation scheme based on the path integral formalism in which space - time is discretized .",
    "quantities in the form of a path integral can be transformed to euclidean space - time , which can be evaluated numerically using monte carlo simulations allowing us to use statistical mechanics methods .    generating su(n ) lattice configurations is a highly demanding task computationally and requires advanced computer architectures such as cpu clusters or gpus .",
    "compared with cpu clusters , gpus are easier to access and maintain , as they can run on a local desktop computer",
    ". there are some groups @xcite using gpus to accelerate their lattice simulations , however this is for the full lagrangian description .    in this work ,",
    "we make use of the new gpu technologies to accelerate the calculations in pure gauge lattice su(2 ) .",
    "note that pure gauge lattice simulations do not include the full lagrangian description , i.e. , dynamical fermions .",
    "in particular , we are able to perform our computations integrally in the gpu , thus reaching a quite higher benchmarks when compared with previous computations partially done in the gpu and partially done in the cpu .    this paper is divided in 6 sections . in section 2",
    ", we present a brief description on how to generate lattice su(2 ) configurations and in section 3 we give an overview of the gpu hardware and the cuda programming model . in section 4",
    "we show how to generate lattice su(2 ) configurations and calculate the static quark - antiquark potential in one gpu or in multiple gpus . in section 5",
    "we present the gpu performance over one cpu core , as well as results for the mean average plaquette and polyakov loop for different @xmath4 and lattice sizes .",
    "we also present the static quark - antiquark potential with and without ape smearing . finally , in section 6 , we conclude .",
    "in this section , we describe the heat bath algorithm for generating su(2 ) configurations @xcite . in su(2 ) , any group element @xmath5 may be parametrized in the form , @xmath6 where the @xmath7 are the usual pauli matrices and where @xmath8 this condition defines the unitary hyper - sphere surface @xmath9 and @xmath10 the invariant haar group measure is given by @xmath11 where @xmath12 is a normalization factor .",
    ".,width=377 ]    in order to update a particular link , we need only to consider the contribution to the action from the six plaquettes containing that link , the staple @xmath13 .",
    "the plaquette is illustrated in fig .",
    "[ fig : plaq ] .",
    "notice that the pure gauge su(2 ) lattice action is composed by the sum of all possible plaquettes , but all the other plaquettes factor out from the expectation value of a particular link .",
    "the distribution to be generated for every single link is given by @xmath14\\ , \\ ] ] where @xmath15 , @xmath16 is the coupling constant .",
    "we apply a useful property of su(2 ) elements , that any sum of them is proportional to another su(2 ) element @xmath17 , @xmath18 using the invariance of the group measure , we obtain @xmath19du = \\exp\\left[\\beta k a_0\\right ] \\frac{1}{2\\pi^2}\\delta\\left(a^2 - 1\\right)d^4 a\\ .\\ ] ] thus , we need to generate @xmath20 $ ] with distribution , @xmath21 the components of @xmath22 are generated randomly on the 3d unit sphere in a four dimensional space with exponential weighting along the @xmath23 direction . once the @xmath23 and @xmath22 are obtained in this way , the new link is updated , @xmath24    in order to accelerate the decorrelation of subsequent lattice configurations",
    ", we can employ the over - relaxation algorithm , @xmath25 where @xmath26 is the staple , @xmath27 and @xmath28 .",
    "the simplest measurement that can be done in the lattice is the average plaquette .",
    "the average plaquette , @xmath29 , is given by , @xmath30 where @xmath13 is the lattice volume and @xmath31 , see fig .",
    "[ fig : plaq ] , is @xmath32 \\ .\\ ] ]        another interesting operator that can be calculated in the lattice is the expectation value of the polyakov loop , @xmath33 @xcite , @xmath34 where @xmath35 .",
    "the product of link variables on the temporal direction @xmath36 is depicted in fig .",
    "[ fig : ploop ] , some times this is called a wilson line , @xmath37 where @xmath38 is the link along the temporal direction .",
    "since we employ periodic boundary conditions in time direction , @xmath39 , and in space direction , this is equivalent to a closed loop .",
    "the expectation value of the polyakov loop is the order parameter for the deconfinement transition on an infinite lattice @xcite .",
    "the order parameter measures the free energy , @xmath40 of a single static ( infinite mass ) quark at temperature @xmath41 , @xmath42 where @xmath41 is connected to the lattice spacing @xmath43 by @xmath44 when @xmath45 , the free energy of the quark increases arbitrarily with the volume , and this is interpreted as a signal of quark confinement .",
    "when @xmath46 , the free energy of the quark tend to a constant for large volume , and this is interpreted as a signal of deconfinement .",
    "we can also extend the square of size @xmath47 , i.e. , the plaquette , to construct an operator with a larger size , the wilson loop .",
    "the wilson loop , depicted in fig .",
    "[ wl ] , is given by , @xmath48 \\",
    ",   \\end{aligned}\\ ] ] where @xmath49 is the spatial direction and @xmath41 is the temporal direction .",
    "note that the smallest non - trivial wilson loop on the lattice is the plaquette .",
    "the mean value of the wilson loop is utilized to compute the static quark - antiquark potential .        in order to improve the signal to noise ratio of the wilson loop",
    ", we can use the ape smearing .",
    "the ape smearing is a gauge equivariant prescription for averaging a link @xmath50 with its nearest neighbours , @xmath51 where @xmath52 is a projector back onto the su(2 ) group , @xmath53 and iterate this procedure 25 times in the spatial direction .",
    "empirically , it is seen that using a smeared operator helps to improve ground - state overlap dramatically .",
    "cuda @xcite is the hardware and software that enables nvidia gpus to execute programs written with languages such as c , c++ , fortran , opencl and directcompute .",
    "cuda programs call parallel kernels , each of which executes in parallel across a set of parallel threads .",
    "these threads are then organized , by the compiler or the programmer , in thread blocks and grids of thread blocks .",
    "the gpu instantiates a kernel program on a grid of parallel thread blocks . within the thread blocks ,",
    "an instance of the kernel will be executed by each thread , which has a thread i d within its thread block , program counter , registers , per - thread private memory , inputs , and output results .",
    "thread blocks are sets of concurrently executing threads , cooperating among themselves by barrier synchronization and shared memory .",
    "thread blocks also have block i d s within their grids .",
    "a grid is an array of thread blocks .",
    "this array executes the same kernel , reads inputs from global memory , writes results to global memory , and synchronizes between dependent kernel calls .    in the cuda parallel programming model",
    ", each thread has a per - thread private memory space used for register spills , function calls , and c automatic array variables .",
    "each thread block has a per - block shared memory space used for inter - thread communication , data sharing , and result sharing in parallel algorithms .",
    "grids of thread blocks share results in global memory space after kernel - wide global synchronization .        in the hardware execution view ,",
    "cuda s hierarchy of threads maps to a hierarchy of processors on the gpu ; a gpu executes one or more kernel grids ; a streaming multiprocessor ( sm ) executes one or more thread blocks ; and cuda cores and other execution units in the sm execute threads .",
    "the sm executes threads in groups of 32 threads called a warp .",
    "while programmers can generally ignore warp execution for functional correctness and think of programming one thread , they can greatly improve performance by having threads in a warp executing the same code path and accessing memory in nearby addresses .",
    "the first fermi based gpu implemented with 3.0 billion transistors , features up to 512 cuda cores , organized in 16 sms of 32 cores each .",
    "a cuda core executes a floating point or integer instruction per clock for a thread . in fig .",
    "[ fermi_architecture ] and table [ nvidia_arch_sum ] we present the details of the fermi architecture .",
    "the gpu has six 64-bit memory partitions , for a 384-bit memory interface , and supports up to a total of 6 gb of gddr5 dram memory .",
    "the connection of the gpu to the cpu is made by a host interface via pci - express .",
    "gigathread global scheduler distributes thread blocks to sm thread schedulers .        ' '' ''    ' '' ''",
    "gpu & g80 & gt200 & fermi    ' '' ''    ' '' ''    transistors & 681 million & 1.4 billion & 3.0 billion    ' '' ''    ' '' ''    cuda cores & 128 & 240 & 512    ' '' ''    ' '' ''    double precision & none & 30 fma & 256 fma    ' '' ''    ' '' ''    floating point capability & & ops / clock & ops / clock    ' '' ''    ' '' ''    single precision & 128 mad & 240 mad & 512 mad    ' '' ''    ' '' ''    floating point capability & ops / clock & ops / clock & ops / clock    ' '' ''    ' '' ''    warp schedulers ( per sm ) & 1 & 1 & 2    ' '' ''    ' '' ''    special function & 2 & 2 & 4    ' '' ''    ' '' ''    units ( sfus)/sm & & &    ' '' ''    ' '' ''    shared memory & 16 kb & 16 kb & configurable    ' '' ''    ' '' ''    ( per sm ) & & & 48 kb or 16 kb    ' '' ''    ' '' ''    l1 cache & none & none & configurable    ' '' ''    ' '' ''    ( per sm ) & & & 16 kb or 48 kb    ' '' ''    ' '' ''    l2 cache ( per sm ) & none & none & 768 kb    ' '' ''    ' '' ''    ecc memory support & no & no & yes    ' '' ''    ' '' ''    concurrent kernels & no & no & up to 16    ' '' ''    ' '' ''    load / store address width & 32-bit & 32-bit & 64-bit    the fermi architecture @xcite represents the most important improvement in gpu architecture since the original g80 , an early vision on unified graphics and computing parallel processor .",
    "gt200 extended its performance and functionality .",
    "table [ nvidia_arch_sum ] shows the details between the different architectures ( g80 , gt200 and fermi architectures ) . with fermi , nvidia used the knowledge from the two prior processors and all the applications that were written for them , and employed a completely new approach to design and to create the world s first computational gpu .",
    "the fermi team designed a processor , fig .",
    "[ fermi_architecture ] , that highly increases not only raw compute horsepower , but also , at the same time , the programmability and computational efficiency using architectural innovations .",
    "they made improvements in double precision performance , a true cache hierarchy since some algorithms can not take advantage of the shared memory resources ( nvidia parallel datacache hierarchy with configurable l1 and unified l2 caches ) , have more shared memory , faster context switching and faster atomic operations .    in all gpus architectures , it is necessary to take into account the following performance considerations : memory coalescing , shared memory bank conflicts , control - flow divergence , occupancy and kernel launch overheads .",
    "in this section , we discuss the parallelization scheme for generating pure gauge su(2 ) lattice configurations .    a cuda application works by spawning a very large number of threads on the gpu which are executed in parallel .",
    "the threads are grouped in thread blocks and the entire collection of blocks is called a grid .",
    "cuda provides primitives that allow the synchronization within a thread block .",
    "however , it is not possible to synchronize threads within different thread blocks . in order to avoid the penalty for high latency",
    ", we must ensure a high multiprocessor occupancy , i.e. , each multiprocessor should have many threads simultaneously loaded and waiting for execution . in this work ,",
    "we assign one thread to each lattice site and in all runs we maintain the thread block size fixed .",
    "since cuda only supports thread blocks up to 3d and grids up to 2d , and the lattice needs four indexes , we use 3d thread blocks , one for @xmath54 , one for @xmath55 and one for both @xmath56 and @xmath57 .",
    "we then reconstruct the other index inside the kernel .",
    "we place most of the constants needed by the gpu , like the number of points in the lattice , in the constant memory using , as in the following example    .... cudamemcpytosymbol ( \" nx \" , & nx , sizeof(int ) ) ; ....    the code to obtain the four indices of the 4d hypercube , when using a single gpu , inside the kernel is    .... int blockidxz = _ _ float2int_rd(blockidx.y * invblocky ) ; int blockidxy = blockidx.y - _ _ umul24(blockidxz , blocks_y ) ; int ij = _ _ mul24(blockidx.x , blockdim.x ) + threadidx.x ;    //index 's of 4d hyper - cube int i = mod(ij , nx ) ; int j = _ _ float2int_rd(ij / nx ) ; int k = _ _ mul24(blockidxy , blockdim.y ) + threadidx.y ; int t = _ _ mul24(blockidxz , blockdim.z ) + threadidx.z ; ....    and outside the kernel we define ,    ....",
    "threads_x = mineq(nx * ny , 16 ) ; threads_y = mineq(nz , 4 ) ; threads_z = mineq(nt , 4 ) ;    blocks_x = ( nx * ny + threads_x - 1 ) / threads_x ; blocks_y = ( nz + threads_y - 1 ) / threads_y ; blocks_z = ( nt + threads_z - 1 ) / threads_z ;       block = make_uint3(threads_x , threads_y , threads_z ) ; grid = make_uint3(blocks_x , blocks_y * blocks_z , 1 ) ; invblocky = 1.0f / ( t)blocks_y ; ....    where is a function that returns the minimum value .",
    "a kernel is then defined , for example , as    ....",
    "cold_start < t4 > < < < grid , block > > > ( lattice_d ) ; ....    note that in the polyakov loop kernel we only need three indexes and we can use the 3d thread blocks , i.e. , in the kernel , we use    .... int blockidxz = _ _ float2int_rd(blockidx.y * invblocky_3d ) ; int blockidxy = blockidx.y - _ _ umul24(blockidxz , blocky_3d ) ; int i      = _ _ mul24(blockidx.x , blockdim.x ) + threadidx.x ; int j      = _ _ mul24(blockidxy , blockdim.y ) + threadidx.y ; int k      = _ _ mul24(blockidxz , blockdim.z ) + threadidx.z ; ....    and each thread make the temporal link multiplication from @xmath58 to @xmath59 and the number of thread blocks and the number of block is defined as ,    ....",
    "threads_x = mineq(nx , 8) ; threads_y = mineq(ny , 8) ; threads_z = mineq(nz , 8) ;    blocks_x = ( nx + threads_x - 1 ) / threads_x ; blocks_y = ( ny + threads_y - 1 ) / threads_y ; blocks_z = ( nz + threads_z - 1 ) / threads_z ;    block_3d = make_uint3(threads_x , threads_y , threads_z ) ; grid_3d = make_uint3(blocks_x , blocks_y * blocks_z , 1 ) ; invblocky_3d = 1.0f/(t)blocks_y ; blocky_3d = blocks_y ; ....    since memory transfers between cpu and gpu are very slow comparing with other gpu memory and in order to maximize the gpu performance , we should only use this feature when it is extremely necessary .",
    "hence , we only use cpu / gpu memory transfers in three cases : in the initial array of seeds for the random number generator in the gpu , in the end of the kernel to perform the sum over all lattice sites ( copy the final result to cpu memory ) and when using multi - gpus ( exchange the border cells between gpus ) .",
    "the kernels developed for this work are :    * random number generator , rng ; * lattice initialization : * * cold start , @xmath60 ; * * hot start , random su(2 ) matrix ; * * read a configuration from input file .",
    "* heat bath algorithm ; * over - relaxation method ; * plaquette ( for each site ) ; * polyakov loop ( for each site ) ; * wilson loop ( for each site ) ; * ape smearing ; * parallel reduction .",
    "sum over all sites of an array .",
    "this kernel performs a sum over all sites after calculation of the plaquette , polyakov loop and wilson loop .    for the generation of the random numbers needed in the hot start lattice initialization and in the heat bath algorithm",
    ", we use a linear congruential random number generator ( lcrng ) @xcite , given by @xmath61 and @xmath62 with @xmath63 , @xmath64 , @xmath65 , @xmath66 and @xmath67 .",
    "we generate the first random numbers @xmath68 in the cpu and then copy the array to the gpu .",
    "therefore , we can generate a different random number in each gpu thread .",
    "the lcrng is used only in the performance tests since this type of random number generator is not suitable for production running .",
    "however , in the results we use the random number generator included with the nvidia toolkit 3.2 rc2 , curand library @xcite .    for the lattice array we can not use in cuda a four dimensional array to store the lattice .",
    "therefore we use a 1d array with size @xmath69 and a , in the case of single precision , or , for double precision , to store the generators of su(2 ) ( @xmath23 , @xmath70 , @xmath71 and @xmath72 ) .",
    "then , we need to construct all the cuda operators to make all the operations needed . in this way , we only need four floating point numbers per link instead of having a @xmath73 complex matrix . in order to select single or double precision",
    ", we use templates in the code .    in the heat bath and over - relaxation methods , since we need to calculate the staple at each link direction and given the gpu architecture , we use the chessboard method , calculating the new links separately by direction and by even and odd sites .",
    "the plaquette , polyakov loop and wilson loop kernels are used to calculate the plaquette , the polyakov loop and the wilson loop by lattice site . in the end",
    "we need to perform the sum over all lattice sites . to make this sum",
    ", we use the parallel reduction code ( kernel 6 ) in the nvidia gpu computing sdk package @xcite .",
    "although cuda neither supports explicitly double textures nor supports double4 textures , it is possible to bind a double4 array to a texture and then retrieve double4 values .",
    "this can be done by declaring the texture as and then using to cast it to double , as in the following code example :    .... texture < int4 , 1 , cudareadmodeelementtype > tex_lattice_double ; ....    .... _ _ device _ _ double4 fetch_lat(double4 * x , int i ) { # if _ _",
    "cuda_arch _ _ > = 130      //",
    "double requires compute capability 1.3 or greater      if ( usetex )      {            int4 v = tex1dfetch(tex_lattice_double , 2 * i ) ;            int4 u = tex1dfetch(tex_lattice_double , 2 * i + 1 ) ;            return make_double4(__hiloint2double(v.y , v.x ) ,                                _ _ hiloint2double(v.w , v.z ) ,                                _ _ hiloint2double(u.y , u.x ) ,                                _ _ hiloint2double(u.w , u.z ) ) ;      }      else          return x[i ] ; # else      return x[i ] ; # endif } ....    moreover , float textures are declared and accessed as ,    .... texture < float4 , 1 , cudareadmodeelementtype > tex_lattice ; ....    .... _ _ device _ _",
    "float4 fetch_lat(float4 * x , int i ) {      if ( usetex )          return tex1dfetch(tex_lattice , i ) ;      else          return x[i ] ; } ....    we now address the multi - gpu approach .",
    "the multi - gpu part was implemented using cuda and openmp , each cpu thread controls one gpu .",
    "each gpu computes @xmath74 .",
    "the total length of the array in each gpu is then @xmath75 , see fig .",
    "[ openmp_grid ] . at each iteration ,",
    "the links are calculated separately by even and odd lattice sites and by the direction @xmath76 . before calculating the next direction , the border cells in each gpu",
    "need to be exchanged between each gpu . on the border of each lattice ,",
    "at least one of the neighboring sites is located in the memory of another gpu , see fig .",
    "[ grid2 ] . for this reason ,",
    "the links at the borders of each lattice have to be transferred from one gpu to the gpu handling the adjacent lattice . in order to exchange the border cells between gpus",
    "it is necessary to copy these cells to cpu memory and then synchronize each cpu thread with the command before updating the gpu memory , ghost cells .",
    "here we present the benchmark results using two different gpu architectures ( gt200 and fermi ) in generating pure gauge lattice su(2 ) configurations .",
    "we also compare the performance with two fermi gpus working in parallel in the same mother - board , using cuda and openmp .",
    "results for the mean average plaquette and polyakov loop are also presented .",
    "finally , the static quark - antiquark potential is calculated on gpus using single and double precision .",
    "we also present results with smeared and unsmeared configurations , as well as the results obtained for the lattice spacing with @xmath77 . in these results",
    ", we did nt use any step of over - relaxation .",
    "our code can be downloaded from the portuguese lattice qcd collaboration homepage @xcite .      in this section ,",
    "we compare the performance between gpu s , see table [ nvidia_gpu_specs ] , ( two different architectures , nvidia gtx 295 , gt200 architecture , and nvidia gtx 480 , fermi architecture ) and a cpu ( intel core i7 cpu 920 , 2.67ghz , 8 mb l2 cache and 12 gb of ram ) .",
    "we compare the performance in generating pure gauge lattice su(2 ) configurations and measure the mean average plaquette for each iteration with @xmath78 , hot start initialization and 100 iterations in single and double precision .        ' '' ''    ' '' ''    nvidia geforce gtx & 295 & 480    ' '' ''    ' '' ''    number of gpus & 2 & 1    ' '' ''    ' '' ''    cuda capability & 1.3 & 2.0    ' '' ''    ' '' ''    number of cores & 2@xmath79240 & 480    ' '' ''    global memory & 1792 mb gddr3 & 1536 mb    ' '' ''    & ( 896 mb per gpu ) & gddr5    ' '' ''    ' '' ''    number of threads per block & 512 & 1024    ' '' ''    ' '' ''    registers per block & 16384 & 32768    ' '' ''    ' '' ''    shared memory ( per sm ) & 16 kb b & 48 kb or 16 kb    ' '' ''    ' '' ''    l1 cache ( per sm ) & none & 16 kb or 48 kb    ' '' ''    ' '' ''    l2 cache ( per sm ) & none & 768 kb    ' '' ''    ' '' ''    clock rate & 1.37 ghz & 1.40 ghz    in fig .",
    "[ fig : spec ] , we present the performance results using nvidia gpus , nvidia gtx 295 ( with 2 gpus per board ) and 2 nvidia gtx 480 ( with 1 gpu per board ) versus one cpu core .",
    "our cpu code to generate a random su(2 ) matrix and the heat bath algorithm is simply the same code we developed for the gpu , except that the memory and process transfers to the gpu are different , as well as the process to sum an array ( calculate the mean plaquette value ) .",
    "moreover , our cpu code is not implemented with sse instructions . in fig .",
    "[ fig : spec_flops ] , we show the gpu performance in gflops / s .",
    "we run the code for 100 iterations , starting with a random su(2 ) configuration . in the heat bath algorithm",
    ", we only perform one try to update the link . in this way",
    ", we measure the flops in all kernels used ( kernel to initialize the random su(2 ) configuration , heat bath kernel , plaquette kernel and the parallel reduction kernel ) .",
    "although the gpu peak performance is around one tflops / s in single performance , the performance achieved by our code , around 70 gflops / s using one fermi gpu , is significantly affected by the large memory transfers , i.e. , for each try to update one gauge link , we need to copy from global memory 19 links ( 19@xmath79float4(double4 ) ) plus one unsigned int in the random array and to calculate the plaquette at each lattice site we need to copy 24 links ( 24@xmath79float4(double4 ) ) .",
    "note that in the heat bath kernel we need to calculate new random numbers but this is not accounted in the number of flops , as well as we only count one instruction for log ( ) , cos ( ) , sin ( ) and sqrt ( ) functions .",
    "the cpu ( intel core i7 cpu 920 , 2.67ghz , 8 mb l2 cache and 12 gb of ram ) performance in one core is almost constant as the lattice size increases , 510 - 520 mflops / s .",
    "the memory access inside the gpus was done using two methods , one using textures and the other one using the global memory in the nvidia gtx 295 case and the cache memory in nvidia gtx 480 .",
    "we do nt use the shared memory because it is a resource too small to fit in our problem .",
    "we only show the performance tests for a maximum lattice array that can fit in our gpu memory .",
    "using only one fermi gpu , the maximum lattice array size in the gpu memory is @xmath80 and @xmath81 for single and double precision , respectively .    in the fermi architecture",
    "there is not much difference between using textures or accessing to global memory when using single precision .",
    "this is because of the new cache hierarchy ( l1 and l2 cache ) . in architectures",
    "prior to fermi , there is no cache hierarchy , therefore , when using textures on these architectures , we can achieve a higher performance in comparison to accessing to the global memory .",
    "however , when using textures there is a limitation of the array size , the maximum width for a 1d texture reference bound to linear memory is @xmath82 , independent of the gpu architecture .    splitting the lattice array in four , i.e.",
    ", one array for each link direction , we can achieve @xmath83 the speed over using only one single array to store all the lattice .",
    "however , using four arrays makes it harder to add new code , since it forces us to write the code more explicitly and the programming errors are more difficult to find .",
    "thus we prefer to use a single array .",
    "the measurement of the average plaquette is defined as the average trace of each plaquette , as defined in eq .",
    "( [ eq : plaq ] ) , in all configurations and is the simplest measurement that can be done in the lattice . in fig .",
    "[ fig : mean_avg_plaq ] , we present the results for the mean average plaquette , as well as the analytic predictions , for different @xmath4 , with @xmath84 configurations and @xmath85 lattice size .",
    "we are able to perform , at least 3 million monte carlo steps per day and calculate the mean average plaquette , in the case of a @xmath85 lattice using the two fermi gpus . for a @xmath86 lattice size , we perform @xmath87 iterations per day .     lattice size ( data points ) and analytic predictions ( denoted by dashed lines).,width=302 ]      we now test the gpu performance measuring the polyakov loop at each generated lattice su(2 ) , in the same conditions made in the performance tests of subsection 5.2 .",
    "the performance is almost the same , @xmath88 , compared with only measuring the average plaquette . fig .",
    "[ fig : polyakov_loop ] shows the expectation value of the polyakov loop as a function of @xmath15 ( @xmath16 is the coupling constant ) , for several lattice sizes and using @xmath84 configurations .",
    "the confinement is evident at high couplings , while the deconfinement occurs at small couplings , i.e. , the polyakov loop is zero at high couplings and then at certain critical coupling value it rises to a finite value . as can be seen , the shape of the curve depends on the temporal size , related to the temperature @xmath41 of the lattice , when the spatial size is kept fixed at @xmath89 .",
    "dependence of the mean average polyakov loop from monte carlo simulation.,width=302 ]      the static quark - antiquark potential , i. e. the potential between two infinitely heavy quarks , has the following long distance expansion , @xmath90 where @xmath91 is the static quark - antiquark potential , @xmath43 is the lattice spacing , @xmath92 is a constant term , @xmath93 is the coefficient to the coulomb term , @xmath94 is the string tension and @xmath49 is the distance in lattice units .",
    "the extraction of the signal of the static quark potential from thermalized lattice gauge configurations is given by , the effective mass plot , @xmath95 since @xmath96    in fig .",
    "[ fig : wl_2.5 ] , we show the fit results for the static quark - antiquark potential using two gpu architectures ( gt200 and fermi ) .",
    "results in single precision from both architectures are presented , as well as the results from double precison from fermi architecture .",
    "all these results agree within our error bars .",
    "configurations with @xmath97 and with ape smearing .",
    "comparison between two different architectures ( gt200 and fermi ) in single precision and in double precision for the fermi architecture.,width=340 ]    in fig .",
    "[ fig : wl_2.8 ] , we show the results for the static quark - antiquark potential with @xmath77 and @xmath98 lattice size , using the fermi gpu .",
    "importantly , we show our results obtained with ape smearing , and without no smearing at all . in table",
    "[ tab : wilson_loop ] , we show the values obtained for the lattice spacing @xmath43 as well as the number of configurations used . the lattice spacing , @xmath43 , was calculated using the relation @xmath99 , where @xmath100 is the value obtained from the linear part of the fit and @xmath94 the physical value for the string tension , @xmath101 , i.e. , @xmath102     and @xmath98.,width=340 ]    importantly , utilizing the computational power of the gpus , we can now afford to calculate the static quark - antiquark potential using thousands of configurations , to study whether the results obtained with and without smearing are in agreement .",
    "note that the quark - antiquark potential has already been extensively studied @xcite , either for small interquark distances or using different smearing techniques , like the ape smearing , but usually fail to pick up a significant signal for long distances with no smearing .",
    "the ape smearing , or other smearing method , have the property to enhance the ground state and therefore decouple it from excitations effectively , since the ground state wave function is always the smoothest wave function within any given channel .",
    "the use of ape smearing is an important tool in order to obtain a clear plateau in eq .",
    "( [ eq : pot_lat ] ) .    in table",
    "[ tab : wilson_loop ] for @xmath77 and in fig .",
    "[ fig : wl_2.8 ] we compare our results with and without smearing .",
    "although , the unsmeared configurations have larger contribution from the excited states , we can extract the static potential , noting that the number of configurations needed to obtain a good signal are indeed quite large .",
    "we confirm that smearing , or at least ape smearing , get a potential consistent within error bars to the one produced by unsmeared configurations .        ' '' ''    ' '' ''    @xmath4 & @xmath103 & @xmath43 ( fm ) & lattice size & ape smearing & # of config .    ' '' ''    ' '' ''    2.5 & @xmath104 & @xmath105 & @xmath106 & @xmath107 , @xmath108 & 1981    ' '' ''    ' '' ''    2.8 & @xmath109 & @xmath110 & @xmath111 & none & 52712    ' '' ''    ' '' ''    2.8 & @xmath112 & @xmath113 & @xmath111 & @xmath107 , @xmath108 & 1981",
    "the use of gpus can improve dramatically the speed of pure gauge su(2 ) lattice computations . using 2 nvidia geforce 480 gtx gpus in a desktop computer ,",
    "we achieve @xmath114 the computation speed over one cpu core , in single precision , around 110 gflops / s using two fermi gpus .",
    "we obtain excellent benchmarks because our computation is integrally performed in the gpu .",
    "our code can be downloaded from the site of the portuguese lattice qcd collaboration @xcite .",
    "the use of textures can increase the speed of memory access when memory access patterns are very complicated and the shared memory can not be used , although the maximum array size , when using textures , is limited . taking advantage of the cache hierarchy introduced in the last architecture , allowed to have similar performance results when accessing to the memory and without having limitations in the array size .",
    "when using multiple gpus we can improve the speed , making the overlap between computation and data transfers , however this was not yet implemented in the code . in the future",
    ", we will implement this using and streams .",
    "we have used to perform the data transfers .",
    "when this function is used , the control is returned to the host thread only after the data transfer is complete . with",
    ", the control is returned immediately to the host thread .",
    "the asynchronous transfer version requires pinned host memory and an additional argument , a stream i d .",
    "a stream is simply a sequence of sorted in time operations , performed in order on the gpu .",
    "therefore , operations in different streams can be interleaved and in some cases overlapped , a property that can be used to hide data transfers between the host ( cpu ) and the device ( gpu ) .",
    "we exploit our computational power to compute benchmarks for the monte carlo generation of su(2 ) lattice gauge configurations , for the plaquette and polyakov loop expectation values , and for the static quark - antiquark potential with wilson loops .",
    "we are able to verify , utilizing a very large number of configurations , that the ape smearing does not distort the static quark - antiquark potential .",
    "this work was financed by the fct contracts poci / fp/81933/2007 , cern / fp/83582/2008 , ptdc / fis/100968/2008 and cern / fp/109327/2009 .",
    "we thank marco cardoso and orlando oliveira for useful discussions .",
    "m.  a.  clark , r.  babich , k.  barros , r.  c.  brower and c.  rebbi , comput .",
    "commun .   * 181 * , 1517 ( 2010 ) [ arxiv:0911.3191 [ hep - lat ] ] .",
    "t.  w.  chiu , t.  h.  hsieh , y.  y.  mao and k.  ogawa [ twqcd collaboration and twqcd collaboration and twqcd collaboration an ] , pos * lattice2010 * , 030 ( 2010 ) [ arxiv:1101.0423 [ hep - lat ] ] . m.  hayakawa , k.  i.  ishikawa , y.  osaki , s.  takeda , s.  uno and n.  yamada , arxiv:1009.5169 [ hep - lat ] .",
    "j.  engels , http://www.sciencedirect.com / science / article / b6tvd-47gj1gk-2x/2/e13d97% c0f93d9ede9f4dda9b571d1e24[the polyakov loop near deconfinement in su(2 ) gauge theory ] , nuclear physics b - proceedings supplements 4 ( 1988 ) 289  293 .",
    "http://dx.doi.org/doi : 10.1016/0920 - 5632(88)90115 - 6 [ ] .",
    "http://www.sciencedirect.com / science / article / b6tvd-47gj% 1gk-2x/2/e13d97c0f93d9ede9f4dda9b571d1e24[http://www.sciencedirect.com / science / article / b6tvd-47gj% 1gk-2x/2/e13d97c0f93d9ede9f4dda9b571d1e24 ]                      a.  huntley , c.  michael , http://www.sciencedirect.com / science / article / b6tvc-473frxg-2t/2/4afbb1% 0609bcc0b73d10f96c7dbdd214[static potentials and scaling in su(2 ) lattice gauge theory ] , nuclear physics b 270 ( 1986 ) 123  134 . http://dx.doi.org/doi : 10.1016/0550 - 3213(86)90548 - 1 [ ] .",
    "http://www.sciencedirect.com / science / article / b6tvc-473f% rxg-2t/2/4afbb10609bcc0b73d10f96c7dbdd214[http://www.sciencedirect.com / science / article / b6tvc-473f% rxg-2t/2/4afbb10609bcc0b73d10f96c7dbdd214 ]"
  ],
  "abstract_text": [
    "<S> in this work we explore the performance of cuda in quenched lattice su(2 ) simulations . </S>",
    "<S> cuda , nvidia compute unified device architecture , is a hardware and software architecture developed by nvidia for computing on the gpu . </S>",
    "<S> we present an analysis and performance comparison between the gpu and cpu in single and double precision . </S>",
    "<S> analyses with multiple gpus and two different architectures ( g200 and fermi architectures ) are also presented . in order to obtain a high performance , </S>",
    "<S> the code must be optimized for the gpu architecture , i.e. , an implementation that exploits the memory hierarchy of the cuda programming model .    </S>",
    "<S> we produce codes for the monte carlo generation of su(2 ) lattice gauge configurations , for the mean plaquette , for the polyakov loop at finite t and for the wilson loop . </S>",
    "<S> we also present results for the potential using many configurations ( @xmath0 ) without smearing and almost @xmath1 configurations with ape smearing . with two fermi gpus </S>",
    "<S> we have achieved an excellent performance of @xmath2 the speed over one cpu , in single precision , around 110 gflops / s . </S>",
    "<S> we also find that , using the fermi architecture , double precision computations for the static quark - antiquark potential are not much slower ( less than @xmath3 slower ) than single precision computations .    </S>",
    "<S> cuda , gpu , fermi , su(2 ) lattice gauge theory 12.38.gc , 07.05.bx , 12.38.mh , 14.40.pq </S>"
  ]
}