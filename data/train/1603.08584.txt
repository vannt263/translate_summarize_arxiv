{
  "article_text": [
    "many important quantities in machine learning and statistics can be viewed as integral functionals of one of more continuous probability densities ; that is , quanitities of the form @xmath4 where @xmath5 are probability densities of random variables taking values in @xmath6 , respectively , and @xmath7 is some measurable function . for simplicity",
    ", we refer to such integral functionals of densities as ` density functionals ' . in this paper , we study the problem of estimating density functionals . in our framework , we assume that the underlying distributions are not given explicitly . only samples of @xmath8 independent and identically distributed ( i.i.d . )",
    "points from each of the unknown , continuous , nonparametric distributions @xmath5 are given .",
    "one density functional of interest is conditional mutual information ( cmi ) , a measure of conditional dependence of random variables , which comes in several varieties including rnyi-@xmath9 and tsallis-@xmath9 cmi ( of which shannon cmi is the @xmath10 limit case ) . estimating conditional dependence in a consistent manner is a crucial problem in machine learning and statistics ; for many applications , it is important to determine how the relationship between two variables changes when we observe additional variables .",
    "for example , upon observing a third variable , two correlated variables may become independent , and , similarly , two independent variables may become dependent . hence , cmi estimators can be used in many scientific areas to detect confounding variables and avoid infering causation from apparent correlation @xcite .",
    "conditional dependencies are also central to bayesian network learning @xcite , where cmi estimation can be used to verify compatibility of a particular bayes net with observed data under a local markov assumption .",
    "other important density functionals are divergences between probability distributions , including rnyi-@xmath9 @xcite and tsallis-@xmath9 @xcite divergences ( of which kullback - leibler ( ) divergence @xcite is the @xmath10 limit case ) , and @xmath11 divergence .",
    "divergence estimators can be used to extend machine learning algorithms for regression , classification , and clustering from the standard setting where inputs are finite - dimensional feature vectors to settings where inputs are sets or distributions @xcite .",
    "entropy and mutual information ( mi ) can be estimated as special cases of divergences .",
    "entropy estimators are used in goodness - of - fit testing @xcite , parameter estimation in semi - parametric models @xcite , and texture classification @xcite , and mi estimators are used in feature selection @xcite , clustering @xcite , optimal experimental design @xcite , and boosting and facial expression recognition @xcite . both entropy and mutual information estimators",
    "are used in independent component and subspace analysis @xcite and image registration @xcite .",
    "further applications of divergence estimation are in @xcite .    despite the practical utility of density functional estimators ,",
    "little is known about their statistical performance , especially for functionals of more than one density .",
    "in particular , few density functional estimators have known convergence rates , and , to the best of our knowledge , no finite sample exponential concentration bounds have been derived for general density functional estimators .",
    "one consequence of this exponential bound is that , using a union bound , we can guarantee accuracy of multiple estimates simultaneously .",
    "for example , @xcite shows how this can be applied to optimally analyze forest density estimation algorithms .",
    "because the cmi of variables @xmath12 and @xmath13 given a third variable @xmath14 is zero if and only @xmath12 and @xmath13 are conditionally independent given @xmath14 , by estimating cmi with a confidence interval , we can test for conditional independence with bounded type i error probabilty .    *",
    "our main contribution * is to derive convergence rates and an exponential concentration inequality for a particular , consistent , nonparametric estimator for large class of density functionals , including conditional density functionals .",
    "we also apply our concentration inequality to the important case of rnyi-@xmath9 cmi .",
    "although lower bounds are not known for estimation of general density functionals ( of arbitrarily many densities ) , @xcite lower bounded the convergence rate for estimators of functionals of a single density ( e.g. , entropy functionals ) by @xmath15 .",
    "@xcite extended this lower bound to the two - density cases of @xmath16 , rnyi-@xmath9 , and tsallis-@xmath9 divergences and gave plug - in estimators which achieve this rate .",
    "these estimators enjoy the parametric rate of @xmath17 when @xmath18 , and work by optimally estimating the density and then applying a correction to the plug - in estimate .",
    "in contrast , our estimator undersmooths the density , and converges at a slower rate of @xmath19 when @xmath20 ( and the parametric rate @xmath17 when @xmath21 ) , but obeys an exponential concentration inequality , which is not known for the estimators of @xcite .",
    "another exception for @xmath22-divergences is provided by @xcite , using empirical risk minimization .",
    "this approach involves solving an @xmath23-dimensional convex minimization problem which be reduced to an @xmath8-dimensional problem for certain function classes defined by reproducing kernel hilbert spaces ( @xmath8 is the sample size ) .",
    "when @xmath8 is large , these optimization problems can still be very demanding .",
    "they studied the estimator s convergence rate , but did not derive concentration bounds .",
    "a number of papers have studied @xmath24-nearest - neighbors estimators , primarily for rnyi@xmath9 density functionals including entropy @xcite , divergence @xcite and conditional divergence and mi @xcite .",
    "these estimators work directly , without the intermediate density estimation step , and generally have proofs of consistency , but their convergence rates and dependence on @xmath24 , @xmath9 , and the dimension are unknown .",
    "one recent exception is a @xmath24-nearest - neighbors based estimator that converges at the parametric rate when @xmath25 , using an optimally weighted ensemble of weak estimators @xcite .",
    "these estimators appear to perform well in higher dimensions , but rates for these estimators require that @xmath26 as @xmath27 , causing computational difficulties for large samples .",
    "although the literature on dependence measures is huge , few estimators have been generalized to the conditional case @xcite . there is some work on testing conditional dependence @xcite , but , unlike cmi estimation , these tests are intended to simply accept or reject the hypothesis that variables are conditionally independent , rather than to measure conditional dependence .",
    "our exponential concentration inequality also suggests a new test for conditional independence .",
    "this paper continues a line of work begun in @xcite and continued in @xcite .",
    "@xcite proved an exponential concentration inequality for an estimator of shannon entropy and mi in the @xmath28-dimensional case .",
    "@xcite used similar techniques to derive an exponential concentration inequality for an estimator of rnyi-@xmath9 divergence in @xmath0 dimensions , for a larger family of densities . both used plug - in estimators based on a mirrored kernel density estimator ( kde ) on @xmath1^d$ ] .",
    "our work generalizes these results to a much larger class of density functionals , as well as to conditional density functionals ( see section 6 ) . in particular , we use a plug - in estimator for general density functionals based on the same mirrored kde , and also use some lemmas regarding this kde proven in @xcite . by considering the more general density functional case ,",
    "we are also able to significantly simplify the proofs of the convergence rate and exponential concentration inequality .      in section 2",
    ", we establish the theoretical context of our work , including notation , the precise problem statement , and our estimator . in section 3",
    ", we outline our main theoretical results and state some consequences .",
    "sections 4 and 5 give precise statements and proofs of the results in section 3 .",
    "finally , in section 6 , we extend our results to conditional density functionals , and state the consequences in the particular case of rnyi-@xmath9 cmi .",
    "for an integer @xmath24 , @xmath29 = \\{1,\\cdots , k\\}$ ] denotes the set of positive integers at most @xmath24 . using the notation of multi - indices common in multivariate calculus",
    ", @xmath30 denotes the set of @xmath0-tuples of non - negative integers , which we denote with a vector symbol @xmath31 , and , for @xmath32 , @xmath33 for fixed @xmath34 , @xmath35 , and a positive integer @xmath0 , we will work with densities in the following bounded subset of a @xmath2-hlder space : @xmath36^d )      : = \\left\\ { p : [ 0,1]^d \\to { \\mathbb{r}}\\middle|              \\sup_{\\substack{x \\neq y \\in d\\\\|{{\\vec{i}}}| = \\ell } }              \\frac{|d^{{\\vec{i}}}p(x ) - d^{{\\vec{i}}}p(y)|}{\\|x - y\\|^{(\\beta - \\ell ) } }      \\right\\ } , \\label{eq : holder}\\end{aligned}\\ ] ] where @xmath37 is the greatest integer _",
    "strictly _ less than @xmath2 , and @xmath38 is the usual @xmath39-norm . to correct for boundary bias , we will require the densities to be nearly constant near the boundary of @xmath1^d$ ] , in that their derivatives vanish at the boundary .",
    "hence , we work with densities in @xmath40^d ) \\middle|              \\max_{1 \\leq |{{\\vec{i}}}| \\leq \\ell } |d^{{\\vec{i}}}p(x)| \\to 0              \\mbox { as }              { \\operatorname{dist}}(x,\\partial[0,1]^d ) \\to 0      \\right\\ } , \\label{eq : bdd_holder}\\end{aligned}\\ ] ] where @xmath41^d = \\{x \\in [ 0,1]^d : x_j \\in \\{0,1\\ }      \\mbox { for some } j \\in [ d]\\}$ ] .      for each @xmath42 $ ] let @xmath43 be a @xmath44-dimensional random vector taking values in @xmath45^{d_i}$ ] , distributed according to a density @xmath46 . for an appropriately smooth function @xmath7 , we are interested in a using random sample of @xmath8 i.i.d .",
    "points from the distribution of each @xmath43 to estimate @xmath47      for a fixed bandwidth @xmath48 , we first use the mirrored kernel density estimator ( kde ) @xmath49 described in @xcite to estimate each density @xmath50 .",
    "we then use a plug - in estimate of @xmath51 .",
    "@xmath52 our main results generalize those of @xcite to a broader class of density functionals .",
    "in this section , we outline our main theoretical results , proven in sections 4 and 5 , and also discuss some important corollaries .",
    "we decompose the estimatator s error into bias and a variance - like terms via the triangle inequality : @xmath53 we will prove the `` variance '' bound @xmath54 for all @xmath55 and the bias bound @xmath56 where @xmath57 , and @xmath58 and @xmath59 are constant in the sample size @xmath8 and bandwidth @xmath48 for exact values . to the best of our knowledge ,",
    "this is the first time an exponential inequality like ( [ ineq : var_bdd ] ) has been established for general density functional estimation .",
    "this variance bound does not depend on @xmath48 and the bias bound is minimized by @xmath60 , we have the convergence rate @xmath61 it is interesting to note that , in optimizing the bandwidth for our density functional estimate , we use a smaller bandwidth than is optimal for minimizing the bias of the kde .",
    "intuitively , this reflects the fact that the plug - in estimator , as an integral functional , performs some additional smoothing .",
    "we can use our exponential concentration bound to obtain a bound on the true variance of @xmath62 . if @xmath63 denotes the cumulative distribution function of the squared deviation of @xmath62 from its mean , then @xmath64 thus , @xmath65   &   = { \\mathbb{e}}\\left [ \\left ( f(\\hat p_1,\\dots,\\hat p_k )                          - { \\mathbb{e}}f(\\hat p_1,\\dots,\\hat p_k ) \\right)^2 \\right ]    \\\\   &   = \\int_0^\\infty 1 - g({\\varepsilon } ) \\ , d{\\varepsilon}\\leq 2\\int_0^\\infty \\exp \\left ( -\\frac{2{\\varepsilon}n}{c_v^2 } \\right )      = c_v^2n{^{-1}}.\\end{aligned}\\ ] ] we then have a mean squared error of @xmath66      \\in o \\left ( n{^{-1}}+ n^{-\\frac{2\\beta}{\\beta + d } } \\right),\\ ] ] which is in @xmath67 if @xmath21 and @xmath68 otherwise .",
    "it should be noted that the constants in both the bias bound and the variance bound depend exponentially on the dimension @xmath0 .",
    "lower bounds in terms of @xmath0 are unknown for estimating most density functionals of interest , and an important open problem is whether this dependence can be made asymptotically better than exponential .",
    "in this section , we precisely state and prove the bound on the bias of our density functional estimator , as introduced in section 3 .    assume each @xmath69 ( for @xmath42 $ ] ) ,",
    "assume @xmath7 is twice continuously differentiable , with first and second derivatives all bounded in magnitude by some @xmath70 , is known to lie within some cube @xmath71^k$ ] , then it suffices for @xmath22 to be twice continuously differentiable on @xmath71^k$ ] ( and the boundedness condition follows immediately ) .",
    "this will be important for our application to rnyi-@xmath9 conditional mutual information . ] and assume the kernel @xmath72 has bounded support @xmath73 $ ] and satisfies @xmath74 then , there exists a constant @xmath75 such that @xmath76      by taylor s theorem , @xmath77 , for some @xmath78 on the line segment between @xmath79 and @xmath80 , letting @xmath81 denote the hessian of @xmath22 @xmath82 ^ 2      \\right)\\end{aligned}\\ ] ] where we used that @xmath49 and @xmath83 are independent for @xmath84 .",
    "applying hlder s inequality , @xmath85 ^ 2 \\ , dx_i                      + \\hspace{-2mm}\\sum_{i < j \\leq k }                      \\int_{{\\mathcal{x}}_i } \\hspace{-1.5mm}|b_{p_i}(x_i)| \\ , dx_i                      \\int_{{\\mathcal{x}}_j } \\hspace{-1.5mm}|b_{p_j}(x_j)| \\ , dx_j              \\right ) \\\\   &   \\leq c_f              \\bigg ( \\sum_{i = 1}^k \\sqrt{\\int_{{\\mathcal{x}}_i } b_{p_i}^2(x_i ) \\ , dx_i }                          + \\int_{{\\mathcal{x}}_i } { \\mathbb{e}}[\\hat p_i(x_i ) - p_i(x_i)]^2 \\ , dx_i \\\\   & \\hspace{70 mm }     + \\sum_{i < j \\leq k }                      \\sqrt{\\int_{{\\mathcal{x}}_i } b_{p_i}^2(x_i ) \\ , dx_i                      \\int_{{\\mathcal{x}}_j } b_{p_j}^2(x_j ) \\ , dx_j }              \\bigg).\\end{aligned}\\ ] ]    we now make use of the so - called bias lemma proven by @xcite , which bounds the integrated squared bias of the mirrored kde @xmath86 on @xmath1^d$ ] for an arbitrary @xmath87 . writing the bias of @xmath86 at @xmath88^d$ ] as @xmath89 , @xcite showed that there exists @xmath90 constant in @xmath8 and @xmath48 such that @xmath91^d } b_p^2(x ) \\ , dx \\leq ch^{2\\beta}. \\label{ineq : bias_lemma}\\ ] ] applying the bias lemma and certain standard results in kernel density estimation ( see , for example , propositions 1.1 and 1.2 of @xcite ) gives @xmath92 where @xmath93 denotes the @xmath94-norm of the kernel .",
    "in this section , we precisely state and prove the exponential concentration inequality for our density functional estimator , as introduced in section 3 .",
    "assume that @xmath22 is lipschitz continuous with constant @xmath96 in the @xmath94-norm on @xmath97 ( i.e. , @xmath98    and assume the kernel @xmath99 ( i.e. , it has finite @xmath94-norm ) .",
    "then , there exists a constant @xmath100 such that @xmath101 , @xmath102 note that , while we require no assumptions on the densities here , in certain specific applications , such us for some rnyi-@xmath9 quantities , where @xmath103 , assumptions such as lower bounds on the density may be needed to ensure @xmath22 is lipschitz on its domain .",
    "consider i.i.d .",
    "samples @xmath104 drawn according to the product distribution @xmath105 . in anticipation of using mcdiarmid s inequality @xcite ,",
    "let @xmath106 denote the @xmath107 mirrored kde when the sample @xmath108 is replaced by new sample @xmath109 . then , applying the lipschitz condition ( [ ineq : lip_cond ] ) on @xmath22 , @xmath110 since most terms of the sum in ( [ ineq : lip_cond ] ) are zero .",
    "expanding the definition of the kernel density estimates @xmath83 and @xmath106 and noting that most terms of the mirrored kdes @xmath83 and @xmath106 are identical gives @xmath111 where @xmath112 denotes the @xmath113-dimensional mirrored product kernel based on @xmath114 . performing a change of variables to remove @xmath48 and applying the triangle inequality followed by the bound on the integral of the mirrored kernel proven in @xcite , @xmath115^{d_j } } \\hspace{-5mm}|k_{d_j}(x)| \\ , dx      \\leq \\frac{2c_f}{n } \\|k\\|_1^{d_j }      = \\frac{c_v}{n } , \\label{ineq : mcd_cond}\\end{aligned}\\ ] ] for @xmath116 .",
    "since @xmath62 depends on @xmath117 independent variables , mcdiarmid s inequality then gives , for any @xmath55 , @xmath118",
    "our convergence result and concentration bound can be fairly easily adapted to to kde - based plug - in estimators for many functionals of interest , including rnyi-@xmath9 and tsallis-@xmath9 entropy , divergence , and mi , and @xmath11 norms and distances , which have either the same or analytically similar forms as as the functional ( [ eq : df_form ] ) .",
    "as long as the density of the variable being conditioned on is lower bounded on its domain , our results also extend to conditional density functionals of the form to denote all of its marginal densities . ]",
    "@xmath119 including , for example , rnyi-@xmath9 conditional entropy , divergence , and mutual information , where @xmath22 is the function @xmath120 .",
    "the proof of this extension for general @xmath24 is essentially the same as for the case @xmath121 , and so , for notational simplicity , we demonstrate the latter .      for given dimensions @xmath122 , consider random vectors @xmath12 and @xmath14 distributed on unit cubes @xmath123^{d_x}$ ] and @xmath124^{d_z}$ ] according to a joint density @xmath125 .",
    "we use a random sample of @xmath126 i.i.d .",
    "points from @xmath127 to estimate a conditional density functional @xmath128 , where @xmath129 has the form ( [ eq : cond_df_form ] ) .",
    "suppose that @xmath127 is in the hlder class @xmath130 , noting that this implies an analogous condition on each marginal of @xmath127 , and suppose that @xmath127 bounded below and above , i.e. , @xmath131 and @xmath132 .",
    "suppose also that @xmath22 and @xmath133 are continuously differentiable , with @xmath134 } |f(x)|      \\quad \\mbox { and } \\quad      c_{f ' } : = \\sup_{x \\in [ c_g , c_g ] } |f'(x)| , \\label{ineq : f_bounds}\\ ] ] where @xmath135\\right )      \\quad \\mbox { and } \\quad      c_g : = \\sup g\\left(\\left[0 , \\frac{\\kappa_2}{\\kappa_1}\\right]\\right).\\ ] ] after estimating the densities @xmath136 and @xmath137 by their mirrored kdes , using @xmath8 independent data samples for each , we clip the estimates of @xmath137 and @xmath136 below by @xmath138 and above by @xmath139 and denote the resulting density estimates by @xmath140 .",
    "our estimate @xmath141 for @xmath128 is simply the result of plugging @xmath140 into equation ( [ eq : cond_df_form ] ) .",
    "suppose @xmath142 is either the true density @xmath127 or a plug - in estimate of @xmath127 computed as described above , and @xmath143 is a plug - in estimate of @xmath127 computed in the same manner but using a different data sample .",
    "applying the triangle inequality twice , @xmath144 applying the mean value theorem and the bounds in ( [ ineq : f_bounds ] ) gives @xmath145 where @xmath146 is the density functional @xmath147 note that , since the data are split to estimate @xmath136 and @xmath137 , @xmath148 depends on each data point through only one of these kdes . in the case",
    "that @xmath142 is the true density @xmath127 , taking the expectation and using fubini s theorem gives @xmath149 applying hlder s inequality and our bias bound ( [ ineq : bias_bdd ] ) , followed by the bias lemma ( [ ineq : bias_lemma ] ) .",
    "this extends our bias bound to conditional density functionals . for the variance bound , consider the case where @xmath142 and @xmath143 are each mirrored kde estimates of @xmath127 , but with one data point resampled ( as in the proof of the variance bound , setting up to use mcdiarmid s inequality ) . by the same sequence of steps used to show ( [ ineq : mcd_cond ] ) , @xmath150 and @xmath151 ( by casing on whether the resampled data point was used to estimate @xmath137 or @xmath136 ) , for an appropriate @xmath58 depending on @xmath152 } |g'(x)|$ ] . then , by mcdiarmid s inequality , @xmath153      as an example , we demonstrate our concentration inequality to the rnyi-@xmath9 conditional mutual information ( cmi ) .",
    "consider random vectors @xmath154 , and @xmath14 on @xmath155^{d_x}$ ] , @xmath156^{d_y}$ ] , @xmath157^{d_z}$ ] , respectively .",
    "@xmath158 , the rnyi-@xmath9 cmi of @xmath12 and @xmath13 given @xmath14 is @xmath159 in this case , the estimator which plugs mirrored kdes for @xmath160 , @xmath137 , @xmath161 , and @xmath136 into ( [ eq : r_cmi ] ) obeys the concentration inequality ( [ ineq : var_bdd ] ) with @xmath162 , where @xmath163 depends only on @xmath9 , @xmath138 , and @xmath139 ."
  ],
  "abstract_text": [
    "<S> we analyze a plug - in estimator for a large class of integral functionals of one or more continuous probability densities . </S>",
    "<S> this class includes important families of entropy , divergence , mutual information , and their conditional versions . for densities on the @xmath0-dimensional unit cube @xmath1^d$ ] that lie in a @xmath2-hlder smoothness class , </S>",
    "<S> we prove our estimator converges at the rate @xmath3 . </S>",
    "<S> furthermore , we prove the estimator is exponentially concentrated about its mean , whereas most previous related results have proven only expected error bounds on estimators . </S>"
  ]
}