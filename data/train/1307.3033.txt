{
  "article_text": [
    "sorting a sequence of @xmath2 elements remains one of the most frequent tasks carried out by computers .",
    "a lower bound for sorting by only pairwise comparisons is @xmath4 comparisons for the worst and average case ( logarithms referred to by @xmath5 are base 2 , the average case refers to a uniform distribution of all input permutations assuming all elements are different ) .    sorting algorithms that are optimal in the leading term",
    "are called _ constant - factor - optimal_. table  [ compare ] lists some milestones in the race for reducing the coefficient in the linear term .",
    "one of the most efficient ( in terms of number of comparisons ) constant - factor - optimal algorithms for solving the sorting problem is ford and johnson s mergeinsertion algorithm @xcite .",
    "it requires @xmath6 comparisons in the worst case  @xcite .",
    "mergeinsertion has a severe drawback that makes it uninteresting for practical issues : similar to insertionsort the number of element moves is quadratic in @xmath2 .",
    "with insertionsort we mean the algorithm that inserts all elements successively into the already ordered sequence finding the position for each element by binary search ( _ not _ by linear search as mostly done ) .",
    "however , mergeinsertion and insertionsort can be used to sort small subarrays such that the quadratic running time for these subarrays is small in comparison to the overall running time .",
    "reinhardt  @xcite used this technique to design an internal mergesort variant that needs in the worst case @xmath7 comparisons .",
    "unfortunately , implementations of this inplacemergesort algorithm have not been documented .",
    "katajainen et al.s  @xcite work inspired by reinhardt is practical , but the number of comparisons is larger .    throughout the text",
    "we avoid the terms _ in - place _ or _ in - situ _ and prefer the term _ internal _ ( opposed to _ external _ ) .",
    "we call an algorithm _ internal _ if it needs at most @xmath8 space in addition to the array to be sorted .",
    "that means we consider quicksort as an internal algorithm whereas standard mergesort is external because it needs a linear amount of extra space .",
    "based on quickheapsort @xcite , in this paper we develop the concept of quickxsort and apply it to other sorting algorithms as mergesort or weakheapsort .",
    "this yields efficient internal sorting algorithms .",
    "the idea is very simple : as in quicksort the array is partitioned into the elements greater and less than some pivot element .",
    "then one part of the array is sorted by some algorithm x and the other part is sorted recursively .",
    "the advantage of this procedure is that , if x is an external algorithm , then in quickxsort the part of the array which is not currently being sorted may be used as temporary space , what yields an internal variant of x. we show that under natural assumptions quickxsort performs up to @xmath9 terms on average the same number of comparisons as x.    .constant - factor - optimal sorting with @xmath10 comparisons . [",
    "cols=\"^,^,^,^,^,^\",options=\"header \" , ]     abbreviations : # in this paper , mi mergeinsertion ,  not analyzed , * for @xmath11 , @xmath12 : computer word width in bits ; we assume @xmath13 .    for quickxsort",
    "we assume inplacemergesort as a worst - case stopper ( without @xmath14 ) .",
    "the concept of quickxsort ( without calling it like that ) was first applied in ultimateheapsort by katajainen @xcite . in ultimateheapsort ,",
    "first the median of the array is determined , and then the array is partitioned into subarrays of equal size . finding",
    "the median means significant additional effort .",
    "cantone and cincotti @xcite weakened the requirement for the pivot and designed quickheapsort which uses only a sample of smaller size to select the pivot for partitioning .",
    "ultimateheapsort is inferior to quickheapsort in terms of average case running time , although , unlike quickheapsort , it allows an @xmath15 bound for the worst case number of comparisons .",
    "diekert and wei  @xcite analyzed quickheapsort more thoroughly and showed that it needs less than @xmath16 comparisons in the average case when implemented with approximately @xmath17 elements as sample for pivot selection and some other improvements .",
    "edelkamp and stiegeler @xcite applied the idea of quickxsort to weakheapsort ( which was first described by dutton @xcite ) introducing quickweakheapsort .",
    "the worst case number of comparisons of weakheapsort is @xmath18 , and , following edelkamp and wegener @xcite , this bound is tight . in",
    "@xcite an improved variant with @xmath19 comparisons in the worst case and requiring extra space is presented . with externalweakheapsort",
    "we propose a further refinement with the same worst case bound , but in average requiring approximately @xmath20 comparisons .",
    "using externalweakheapsort as x in quickxsort we obtain an improvement over quickweakheapsort of @xcite .",
    "as indicated above , mergesort is another good candidate to apply the quickxsort - construction .",
    "with quickmergesort we describe an internal variant of mergesort which not only in terms of number of comparisons is almost as good as mergesort , but also in terms of running time . as mentioned before ,",
    "mergeinsertion can be used to sort small subarrays .",
    "we study mergeinsertion and provide an implementation based on weak heaps .",
    "furthermore , we give an average case analysis . when sorting small subarrays with mergeinsertion",
    ", we can show that the average number of comparisons performed by mergesort is bounded by @xmath21 , and , therefore , quickmergesort uses at most @xmath22 comparisons in the average case .",
    "in this section we give a more precise description of quickxsort and derive some results concerning the number of comparisons performed in the average and worst case .",
    "let x be some sorting algorithm .",
    "quickxsort works as follows : first , choose some pivot element as median of some random sample .",
    "next , partition the array according to this pivot element , i.e. , rearrange the array such that all elements left of the pivot are less or equal and all elements on the right are greater or equal than the pivot element . then , choose one part of the array and sort it with algorithm x. ( in general , it does not matter whether the smaller or larger half of the array is chosen .",
    "however , for a specific sorting algorithm x like heapsort , there might be a better and a worse choice . ) after one part of the array has been sorted with x , move the pivot element to its correct position ( right after / before the already sorted part ) and sort the other part of the array recursively with quickxsort .",
    "the main advantage of this procedure is that the part of the array that is not being sorted currently can be used as temporary memory for the algorithm x. this yields fast _ internal _ variants for various _ external _ sorting algorithms ( such as mergesort ) .",
    "the idea is that whenever a data element should be moved to the external storage , instead it is swapped with some data element in the part of the array which is not currently being sorted . of course",
    ", this works only , if the algorithm needs additional storage only for data elements .",
    "furthermore , the algorithm has to be able to keep track of the positions of elements which have been swapped .",
    "as the specific method depends on the algorithm x , we give some more details when we describe the examples for quickxsort .",
    "for the number of comparisons we can derive some general results which hold for a wide class of algorithms x. under natural assumptions the average case number of comparisons of x and of quickxsort differs only by an @xmath23-term . for the rest of the paper ,",
    "we assume that the pivot is selected as the median of approximately @xmath17 randomly chosen elements . sample sizes of approximately @xmath17 are likely to be optimal as the results in @xcite suggest .",
    "[ quickxsort average - case][thm : quickxsort ] let x be some sorting algorithm requiring at most @xmath24 comparisons in the average case .",
    "then , quickxsort implemented with @xmath25 elements as sample for pivot selection is a sorting algorithm that also needs at most @xmath26 comparisons in the average case .    for the proofs",
    "we assume that the arrays are indexed starting with @xmath27 .",
    "the following lemma is crucial for our estimates .",
    "it can be derived by applying chernoff bounds or by direct elementary calculations .",
    "[ lm : prob_bound ] let @xmath28 . if we choose the pivot as median of @xmath29 elements such that @xmath30 , then we have @xmath31 } < ( 2\\gamma+1 ) \\alpha^\\gamma$ ] where @xmath32 .",
    "let @xmath33 denote the average number of comparisons performed by quickxsort on an input array of length @xmath2 and let @xmath34 with @xmath35 be an upper bound for the average number of comparisons performed by the algorithm  x on an input array of length @xmath2 .",
    "without loss of generality we may assume that @xmath36 is monotone .",
    "we are going to show by induction that @xmath37 for some monotonically increasing @xmath38 with @xmath39 which we will specify later .",
    "let @xmath40 with @xmath41 , i.e. , @xmath42 is some function tending slowly to zero for @xmath43 .",
    "because of @xmath44 , we see that @xmath45 tends to zero if @xmath46 .",
    "hence , by it follows that the probability that the pivot is more than @xmath47 off the median @xmath48}+{\\mathop{\\mathrm{pr}}\\left[\\,\\text{pivot } > n\\left(\\frac{1}{2 } + \\delta(n)\\right)\\,\\right]}$ ] tends to zero for @xmath43 . in the following we write @xmath49 \\cap { \\mathbb{n}}$ ] and @xmath50 .",
    "we obtain the following recurrence relation : @xmath51}\\cdot\\max\\left\\{\\ , t(k-1 ) + s(n - k ) , t(n - k)+s(k-1)\\,\\right\\}\\\\   & \\leq n - 1 + t_{\\mathrm{pivot}}(n ) \\\\   & \\qquad+ { \\mathop{\\mathrm{pr}}\\left[\\,\\!\\text{pivot } \\in m \\!\\,\\right]}\\cdot\\max_{k\\in m}\\left\\{\\ , t(k ) + s(n - k ) , t(n - k)+s(k)\\,\\right\\}\\\\    & \\qquad + { \\mathop{\\mathrm{pr}}\\left[\\,\\!\\text{pivot } \\in \\overline m \\!\\,\\right]}\\cdot\\max_{k\\in\\overline m}\\left\\{\\ , t(k ) + s(n - k ) , t(n - k)+s(k)\\,\\right\\}. \\ ] ] the function @xmath52 , @xmath53 has its only minimum in the interval @xmath54 $ ] at @xmath55 , i.e. , for @xmath56 it decreases monotonically and for @xmath57 it increases monotonically .",
    "we set @xmath58 .",
    "that means that we have @xmath59 for @xmath60 and @xmath61 for @xmath62 . using this observation , the induction hypothesis , and our assumptions , we conclude @xmath63 with @xmath64 as above",
    "we obtain : @xmath65 we subtract @xmath66 on both sides and then divide by @xmath67 .",
    "let @xmath68 be some constant such that @xmath69 for all @xmath2 ( which exists since @xmath70 for all @xmath2 and @xmath71 for @xmath43 ) .",
    "then , we obtain @xmath72 where the last inequality follows from @xmath73 for @xmath74 .",
    "we see that @xmath75 if @xmath76 satisfies the inequality @xmath77 we choose @xmath76 as small as possible .",
    "inductively , we can show that for every @xmath78 there is some @xmath79 such that @xmath80 .",
    "hence , the theorem follows .",
    "does quickxsort provide a good bound for the worst case ?",
    "the obvious answer is `` no '' .",
    "if always the @xmath17 smallest elements are chosen for pivot selection , a running time of @xmath81 is obtained .",
    "however , we can prove that such a worst case is very unlikely .",
    "in fact , let @xmath82 be the worst case number of comparisons of the algorithm x. states that the probability that quickxsort needs more than @xmath83 comparisons decreases exponentially in @xmath2 .",
    "( this bound is not tight , but since we do not aim for exact probabilities , is enough for us . )",
    "[ prop : worstunlikely ] let @xmath84 .",
    "the probability that quickxsort needs more than @xmath83 comparisons is less than @xmath85{n}}$ ] for @xmath2 large enough .",
    "let @xmath2 be the size of the input .",
    "we say that we are in a _ good _ case if an array of size @xmath86 is partitioned in the interval @xmath87 $ ] , i.e. , if the pivot is chosen in that interval .",
    "we can obtain a bound for the desired probability by estimating the probability that we always are in such a good case until the array contains only @xmath17 elements . for smaller arrays",
    ", we can assume an upper bound of @xmath88 comparisons for the worst case .",
    "for all partitioning steps that sums up to less than @xmath89 comparisons if we are always in a good case . we also have to consider the number of comparisons required to find the pivot element . at any stage",
    "the pivot is chosen as median of at most @xmath17 elements .",
    "since the median can be determined in linear time , for all stages together this sums up to less than @xmath2 comparisons if we are always in a good case and @xmath2 is large enough .",
    "finally , for all the sorting phases with x we need at most @xmath82 comparisons in total ( that is only a rough upper bound which can be improved as in the proof of ) .",
    "hence , we need at most @xmath83 comparisons if always a good case occurs .",
    "now , we only have to estimate the probability that always a good case occurs . by",
    ", the probability for a good case in the first partitioning step is at least @xmath90 for some constant @xmath91 .",
    "we have to choose @xmath92 times a pivot in the interval @xmath87 $ ] , then the array has size less than @xmath17 .",
    "we only have to consider partitioning steps where the array has size greater than @xmath17 ( if the size of the array is already less than @xmath17 we define the probability of a good case as @xmath27 ) .",
    "hence , for each of these partitioning steps we obtain that the probability for a good case is greater than @xmath93{n}\\cdot\\left(3/4\\right)^{\\sqrt[4]{n}}$ ] .",
    "therefore , we obtain @xmath94}&\\geq\\left(1-d\\cdot\\sqrt[4]{n}\\cdot\\left(3/4\\right)^{\\sqrt[4]{n}}\\right)^{1.21\\log(n)}\\\\ & \\geq 1 - 1.21\\log(n ) \\cdot d\\cdot\\sqrt[4]{n}\\cdot\\left(3/4\\right)^{\\sqrt[4]{n}}\\end{aligned}\\ ] ] by bernoulli s inequality . for @xmath2",
    "large enough we have @xmath95{n}\\cdot\\left(3/4\\right)^{\\sqrt[4]{n}}\\leq ( 3/4 + \\epsilon)^{\\sqrt[4 ] { n}}$ ] .    to obtain a provable bound for the worst case complexity",
    "we apply a simple trick .",
    "we fix some worst case efficient sorting algorithm y. this might be , e.g. , inplacemergesort .",
    "worst case efficient means that we have a @xmath15 bound for the worst case number of comparisons .",
    "we choose some slowly decreasing function @xmath96 , e.g. , @xmath97 .",
    "now , whenever the pivot is more than @xmath98 off the median , we switch to the algorithm y. we call this quickxysort . to achieve a good worst case bound , of course",
    ", we also need a good bound for algorithm x. w.l.o.g .",
    "we assume the same worst case bounds for x as for y. note that quickxysort only makes sense if one needs a provably good worst case bound . since quickxsort is always expected to make at most as many comparisons as quickxysort ( under the reasonable assumption that x on average is faster than y  otherwise one would use simply y ) , in every step of the recursion quickxsort is the better choice for the average case .    in order to obtain an efficient internal sorting algorithm , of course",
    ", y has to be internal and x using at most @xmath2 extra spaces for an array of size @xmath2 .",
    "[ thm : quickxysort ] let x be a sorting algorithm with at most @xmath99 comparisons in the average case and @xmath100 comparisons in the worst case ( @xmath101 ) .",
    "let y be a sorting algorithm with at most @xmath82 comparisons in the worst case .",
    "then , quickxysort is a sorting algorithm that performs at most @xmath102 comparisons in the average case and @xmath103 comparisons in the worst case .    since the proof is very similar to the proof of ,",
    "we provide only a sketch . by replacing @xmath33 by @xmath104 with @xmath105 in the right side of in the proof of we",
    "obtain for the average case : @xmath106 as in the statement for the average case follows .    for the worst case , there are two possibilities : either the algorithm already fails the condition @xmath107 $ ] in the first partitioning step or it does not . in the first case , it is immediate that we have a worst case bound of @xmath108 , which also is tight . note that we assume that we can choose the pivot element in time @xmath23 which is no real restriction , since the median of @xmath25 elements can be found in @xmath25 time . in the second case ,",
    "we assume by induction that @xmath109 for @xmath110 for some @xmath111 and obtain a recurrence relation similar to in the proof of : @xmath112 by the same arguments as above the result follows .",
    "in this section consider quickweakheapsort as a first example of quickxsort .",
    "we start by introducing weak heaps and then continue by describing weakheapsort and a novel external version of it .",
    "this external version is a good candidate for quickxsort and yields an efficient sorting algorithm that uses approximately @xmath113 comparisons ( this value is only a rough estimate and neither a bound from below nor above ) .",
    "a drawback of weakheapsort and its variants is that they require one extra bit per element .",
    "the exposition also serves as an intermediate step towards our implementation of mergeinsertion , where the weak - heap data structure will be used as a building block .",
    "conceptually , a _ weak heap _ ( see fig .  [",
    "fig : example ] ) is a binary tree satisfying the following conditions :    1 .",
    "the root of the entire tree has no left child .",
    "2 .   except for the root , the nodes that have at most one child are in the last two levels only .",
    "leaves at the last level can be scattered , i.e. , the last level is not necessarily filled from left to right .",
    "3 .   each node stores an element that is smaller than or equal to every element stored in its right subtree .    from the first two properties we deduce that the height of a weak heap that has @xmath2 elements is @xmath114 .",
    "the third property is called the _ weak - heap ordering _ or half - tree ordering . in particular",
    ", this property enforces no relation between an element in a node and those stored its left subtree . on the other hand",
    ", it implies that any node together with its right subtree forms a weak heap on its own . in an array - based implementation , besides the element array @xmath115 , an array @xmath116 of _ reverse bits _",
    "is used , i.e. , @xmath117 for @xmath118 .",
    "the root has index @xmath119 .",
    "the array index of the left child of @xmath120 is @xmath121 , the array index of the right child is @xmath122 , and the array index of the parent is @xmath123 ( assuming that @xmath124 ) . using the fact that the indices of the left and right children of @xmath120 are exchanged when flipping @xmath125 ,",
    "subtrees can be reversed in constant time by setting @xmath126 .",
    "the _ distinguished ancestor _ ( @xmath127 ) of @xmath128 for @xmath129 , is recursively defined as the parent of @xmath128 if @xmath128 is a right child , and the distinguished ancestor of the parent of @xmath128 if @xmath128 is a left child .",
    "the distinguished ancestor of @xmath128 is the first element on the path from @xmath128 to the root which is known to be smaller or equal than @xmath128 by ( 3 ) .",
    "moreover , any subtree rooted by @xmath128 , together with the distinguished ancestor @xmath120 of @xmath128 , forms again a weak heap with root @xmath120 by considering @xmath128 as right child of @xmath120 .",
    "the basic operation for creating a weak heap is the operation which combines two weak heaps into one .",
    "let @xmath130 and @xmath131 be two nodes in a weak heap such that @xmath120 is smaller than or equal to every element in the left subtree of @xmath128 .",
    "conceptually , @xmath128 and its right subtree form a weak heap , while @xmath120 and the left subtree of @xmath128 form another weak heap .",
    "( note that @xmath120 is not allowed be in the subtree with root @xmath128 . )",
    "the result of is a weak heap with root at position @xmath130 . if @xmath132 , the two elements are swapped and @xmath133 is flipped . as a result ,",
    "the new element @xmath128 will be smaller than or equal to every element in its right subtree , and the new element @xmath120 will be smaller than or equal to every element in the subtree rooted at @xmath128 . to sum up ,",
    "requires constant time and involves one element comparison and a possible element swap in order to combine two weak heaps to a new one .",
    "the construction of a weak heap consisting of @xmath2 elements requires @xmath134 comparisons . in the standard bottom - up construction of a weak heap",
    "the nodes are visited one by one . starting with the last node in the array and moving to the front , the two weak heaps rooted at a node and its distinguished ancestor are joined .",
    "the amortized cost to get from a node to its distinguished ancestor is @xmath135  @xcite .",
    "when using weak heaps for sorting , the minimum is removed and the weak heap condition restored until the weak heap becomes empty .",
    "after extracting an element from the root , first the _ special path _ from the root is traversed top - down , and then , in a bottom - up process the weak - heap property is restored using at most @xmath136 join operations .",
    "( the special path is established by going once to the right and then to the left as far as it is possible . ) hence , extracting the minimum requires at most @xmath136 comparisons .",
    "now , we introduce a modification to the standard procedure described by dutton @xcite , which has a slightly improved performance , but requires extra space .",
    "we call this modified algorithm externalweakheapsort .",
    "this is because it needs an extra output array , where the elements which are extracted from the weak heap are moved to . on average",
    "externalweakheapsort requires less comparisons than relaxedweakheapsort  @xcite .",
    "integrated in quickxsort we can implement it without extra space other than the extra bits @xmath116 and some other extra bits .",
    "we introduce an additional array _ active _ and weaken the requirements of a weak heap : we also allow nodes on other than the last two levels to have less than two children .",
    "nodes where the _ active _ bit is set to false are considered to have been removed .",
    "externalweakheapsort works as follows : first , a usual weak heap is constructed using @xmath134 comparisons . then , until the weak heap becomes empty , the root   which is the minimal element   is moved to the output array and the resulting hole has to be filled with the minimum of the remaining elements ( so far the only difference to normal weakheapsort is that there is a separate output area ) .",
    "the hole is filled by searching the special path from the root to a node @xmath137 which has no left child .",
    "note that the nodes on the special path are exactly the nodes having the root as distinguished ancestor .",
    "finding the special path does not need any comparisons , since one only has to follow the reverse bits .",
    "next , the element of the node @xmath137 is moved to the root leaving a hole .",
    "if @xmath137 has a right subtree ( i.e. , if @xmath137 is the root of a weak heap with more than one element ) , this hole is filled by applying the hole - filling algorithm recursively to the weak heap with root @xmath137 . otherwise , the _ active _ bit of @xmath137 is set to false .",
    "now , the root of the whole weak heap together with the subtree rooted by @xmath137 forms a weak heap .",
    "however , it remains to restore the weak heap condition for the whole weak heap .",
    "except for the root and @xmath137 , all nodes on the special path together with their right subtrees form weak heaps .",
    "following the special path upwards these weak heaps are joined with their distinguished ancestor as during the weak heap construction ( i.e. , successively they are joined with the weak heap consisting of the root and the already treated nodes on the special path together with their subtrees ) .",
    "once , all the weak heaps on the special path are joined , the whole array forms a weak heap again .",
    "[ thm : whsms ] for @xmath138 externalweakheapsort performs exactly the same comparisons as mergesort applied on a fixed permutation of the same input array .",
    "first , recall the mergesort algorithm : the left half and the right half of the array are sorted recursively and then the two subarrays are merged together by always comparing the smallest elements of both arrays and moving the smaller one to the separate output area .",
    "now , we move to weakheapsort .",
    "consider the tree as it is initialized with all reverse bits set to _",
    "false_. let @xmath116 be the root and @xmath139 its only child ( not the elements but the positions in the tree ) .",
    "we call @xmath116 together with the left subtree of @xmath139 the left part of the tree and we call @xmath139 together with its right subtree the right part of the tree .",
    "that means the left part and the right part form weak heaps on their own .",
    "the _ only _ time an element is moved from the right to the left part or vice - versa is when the data elements @xmath140 and @xmath141 are exchanged .",
    "however , always one of the data elements of @xmath116 and @xmath139 comes from the right part and one from the left part . after extracting the minimum @xmath140 , it is replaced by the smallest remaining element of the part @xmath140 came from .",
    "then , the new @xmath140 and @xmath141 are compared again and so on .",
    "hence , for extracting the elements in sorted order from the weak heap the following happens .",
    "first , the smallest elements of the left and right part are determined , then they are compared and finally the smaller one is moved to the output area .",
    "this procedure repeats until the weak heap is empty .",
    "this is exactly how the recursion of mergesort works : always the smallest elements of the left and right part are compared and the smaller one is moved to the output area . if @xmath11 , then the left and right parts for mergesort and weakheapsort have the same sizes .    by @xcite",
    "we obtain the following corollary .",
    "[ cor : ewhs ] for @xmath138 the algorithm externalweakheapsort uses approximately @xmath20 comparisons in the average case",
    ".    if @xmath2 is not a power of two , the sizes of left and right parts of weakheapsort are less balanced than the left and right parts of ordinary mergesort and one can expect a slightly higher number of comparisons . for quickweakheapsort , the half of the array which is not sorted by externalweakheapsort",
    "is used as output area . whenever the root is moved to the output area ,",
    "the element that occupied that place before is inserted as a dummy element at the position where the _ active _ bit is set to false .",
    "applying , we obtain the rough estimate of @xmath142 comparisons for the average case of quickweakheapsort .",
    "as another example for quickxsort we consider quickmergesort . for the mergesort",
    "part we use standard ( top - down ) mergesort which can be implemented using @xmath86 extra spaces to merge two arrays of length @xmath86 ( there are other methods like in @xcite which require less space   but for our purposes this is good enough ) .",
    "the procedure is depicted in .",
    "we sort the larger half of the partitioned array with mergesort as long as we have one third of the whole array as temporary memory left , otherwise we sort the smaller part with mergesort .    2    ( 0,0 ) ",
    "( 9,0 ) ; ( 0,1 )  ( 9,1 ) ; ( 0,0 ) ",
    "( 0,1 ) ;    ( 6,0 ) ",
    "( 6,1 ) ; ( st ) at ( 6,-0.3 ) pivot ;    ( 4.5,1 ) .. controls(4.5,2 ) and ( 7.5 , 2 ) .. ( 7.5,1 ) ;    ( 3,0 )  ( 3,1 ) ; ( 9,0 )  ( 9,1 ) ;    ( 0,0 )  ( 9,0 ) ; ( 0,1 )  ( 6,1 ) ; ( 6,0.3 )  ( 9,1 ) ; ( 0,0 )  ( 0,1 ) ;    ( 6,0 )  ( 6,1 ) ; ( st ) at ( 6,-0.3 ) pivot ;    ( 1.5,1 ) .. controls(1.5,2 ) and ( 4.5 , 2 ) .. ( 4.5,1 ) ;    ( 3,0 )  ( 3,1 ) ; ( 9,0 )  ( 9,1 ) ;    ( 0,0 )  ( 9,0 ) ; ( 0,1 )  ( 3,1 ) ; ( 3,0.3 )  ( 6,1 ) ; ( 6,0.3 )  ( 9,1 ) ; ( 0,0 )  ( 0,1 ) ;    ( 6,0 )  ( 6,1 ) ; ( st ) at ( 6,-0.3 ) pivot ;    ( 4.5,0.65 ) .. controls(4,1.7 ) and ( 1.5 , 1.7 ) .. ( 1,1 ) ; ( 7.5,0.65 ) .. controls(7,2 ) and ( 1.5 , 2 ) .. ( 1,1 ) ;    ( 3,0 ) ",
    "( 3,1 ) ; ( 9,0 ) ",
    "( 9,1 ) ;    hence , the part which is not sorted by mergesort always provides enough temporary space .",
    "when a data element should be moved to or from the temporary space , it is swapped with the element occupying the respective position . since mergesort moves through the data from left to right",
    ", it is always known which are the elements to be sorted and which are the dummy elements .",
    "depending on the implementation the extra space needed is @xmath8 words for the recursion stack of mergesort . by avoiding recursion this",
    "can even be reduced to @xmath135 . together with @xcite yields the next result .",
    "[ average case quickmergesort][thm : qms ] quickmergesort is an internal sorting algorithm that performs at most @xmath143 comparisons on average .",
    "we can do even better if we sort small subarrays with another algorithm z requiring less comparisons but extra space and more moves , e.g. , insertionsort or mergeinsertion .",
    "if we use @xmath8 elements for the base case of mergesort , we have to call z at most @xmath144 times . in this case",
    "we can allow additional operations of z like moves in the order of @xmath145 , given that @xmath146 .",
    "note that for the next theorem we only need that the size of the base cases grows as @xmath2 grows .",
    "nevertheless , @xmath8 is the largest growing value we can choose if we apply a base case algorithm with @xmath147 moves and want to achieve an @xmath148 overall running time .",
    "[ thm : qmsbase ] let @xmath149 be some sorting algorithm with @xmath150 comparisons on the average and other operations taking at most @xmath145 time . if base cases of size @xmath8 are sorted with z , quickmergesort uses at most @xmath150 comparisons and @xmath148 other instructions on the average .    by and the preceding remark , the only thing we have to prove is that mergesort with base case z requires on average at most @xmath151 comparisons , given that z needs @xmath152 comparisons on average .",
    "the latter means that for every @xmath153 we have @xmath154 for @xmath2 large enough",
    ".    let @xmath155 denote the average case number of comparisons of mergesort with base cases of size @xmath156 sorted with z and let @xmath84 .",
    "since @xmath157 grows as @xmath2 grows , we have that @xmath158 for @xmath2 large enough and @xmath159 .",
    "for @xmath160 we have @xmath161 and by induction we see that @xmath162 . hence , also @xmath163 for @xmath2 large enough .",
    "using insertionsort we obtain the following result . here , @xmath164 denotes the natural logarithm . as we did not find a result in literature",
    ", we also provide a proof . recall that insertionsort inserts the elements one by one into the already sorted sequence by binary search .",
    "[ prop : avgins ] the sorting algorithm insertionsort needs @xmath165 comparisons on the average where @xmath166 $ ] .",
    "if we use as base case insertionsort , quickmergesort uses at most @xmath167 comparisons and @xmath168 other instructions on the average .",
    "first , we take a look at the average number of comparisons @xmath169 to insert one element into a sorted array of @xmath170 elements by binary insertion .    to insert a new element into @xmath170 elements either needs @xmath171 or @xmath172 comparisons .",
    "there are @xmath156 positions where the element to be inserted can end up , each of which is equally likely . for @xmath173 of these positions",
    "@xmath174 comparisons are needed . for the other @xmath175 positions @xmath172 comparisons are needed .",
    "this means @xmath176 comparisons are needed on average . by",
    "* (3 ) ) , we obtain for the average case for sorting @xmath2 elements : @xmath177 we examine the last sum separately . in the following we write @xmath178 for the harmonic sum with @xmath179 the euler constant .",
    "@xmath180 hence , we have @xmath181 in order to obtain a numeric bound for @xmath182 , we compute @xmath183 and then replace @xmath184 by @xmath137 .",
    "this yields a function @xmath185 which oscillates between @xmath186 and @xmath187 for @xmath188 . for @xmath189 ,",
    "its value is @xmath190 .",
    "bases cases of growing size , always lead to a constant factor overhead in running time if an algorithm with a quadratic number of total operations is applied .",
    "therefore , in the experiments we will also consider constant size base cases which offer a slightly worse bound for the number of comparisons , but are faster in practice .",
    "we do not analyze them separately , since the preferred choice for the size depends on the type of data to be sorted and the system on which the algorithms run .",
    "mergeinsertion by ford and johnson @xcite is one of the best sorting algorithms in terms of number of comparisons .",
    "hence , it can be applied for sorting base cases of quickmergesort what yields even better results than insertionsort .",
    "therefore , we want to give a brief description of the algorithm and our implementation . while the description is simple , mergeinsertion is not easy to implement efficiently .",
    "our implementation is based on weak heaps and uses @xmath191 extra bits .",
    "algorithmically , mergeinsertion@xmath192 can be described as follows ( an intuitive example for @xmath193 can be found in  @xcite ) .    1 .",
    "arrange the input such that @xmath194 for @xmath195 with one comparison per pair .",
    "let @xmath196 and @xmath197 for @xmath195 , and @xmath198 if @xmath2 is odd .",
    "sort the values @xmath199 recursively with mergeinsertion .",
    "3 .   rename the solution as follows : @xmath200 and insert the elements @xmath201 via binary insertion , following the ordering @xmath202 , @xmath203 , @xmath204 , @xmath205 , @xmath206 , @xmath207 , @xmath208 , @xmath209 , @xmath210 into the main chain , where @xmath211 .    due to the different renamings , the recursion , and the change of link structure ,",
    "the design of an efficient implementation is not immediate .",
    "our proposed implementation of mergeinsertion is based on a tournament tree representation with weak heaps as in .",
    "the pseudo - code implementations for all the operations to construct a tournament tree with a weak heap and to access the partners in each round are shown in fig .",
    "[ tournament ] in the appendix .",
    "( note that for simplicity in the above formulation the indices and the order are reversed compared to our implementation . )",
    "one main subroutine of mergeinsertion is binary insertion .",
    "the call @xmath212 inserts the element at position @xmath213 between position @xmath214 and @xmath215 by binary insertion .",
    "( the pseudo - code implementations for the binary search routine is shown in in the appendix . ) in this routine we do not move the data elements themselves , but we use an additional index array @xmath216 to point to the elements contained in the weak heap tournament tree and move these indirect addresses .",
    "this approach has the advantage that the relations stored in the tournament tree are preserved .",
    "the most important procedure for mergeinsertion is the organization of the calls for . after adapting the addresses for the elements @xmath217 ( w.r.t .",
    "the above description ) in the second part of the array , the algorithm calls the binary insertion routine with appropriate indices .",
    "note that we always use @xmath156 comparisons for all elements of the @xmath156-th block ( i.e. , the elements @xmath218 ) even if there might be the chance to save one comparison . by introducing an additional array , which for each @xmath217 contains the current index of @xmath219",
    ", we can exploit the observation that not always @xmath156 comparisons are needed to insert an element of the @xmath156-th block . in the following we call this the _ improved _ variant .",
    "the pseudo - code of the basic variant is shown in fig .",
    "[ xxmerge ] .",
    "the last sequence is not complete and is thus tackled in a special case .",
    "@xmath220    [ thm : avgmergeins ] the sorting algorithm mergeinsertion needs @xmath221 comparisons on the average , where @xmath222 .    [ thm : qmsbasemi ] when using mergeinsertion as base case , quickmergesort needs at most @xmath223 comparisons and @xmath168 other instructions on the average .",
    "according to knuth @xcite , mergeinsertion requires at most @xmath224 comparisons in the worst case , where @xmath225 . in the following we want to analyze the average savings relative to the worst case .",
    "therefore , let @xmath226 denote the average number of comparisons of the insertion steps of mergeinsertion , i.e. , all comparisons minus the efforts for the weak heap construction , which always takes place . then , we obtain the recurrence relation @xmath227 with @xmath228 such that @xmath229 and some @xmath230 $ ] . as we do not analyze the improved version of the algorithm , the insertion of elements with index less or equal @xmath231 requires always the same number of comparisons .",
    "thus , the term @xmath232 is independent of the data .",
    "however , inserting an element after @xmath231 may either need @xmath228 or @xmath233 comparisons .",
    "this is where @xmath234 comes from .",
    "note that @xmath234 only depends on @xmath86 .",
    "we split @xmath226 into @xmath235 with @xmath236    for the average case analysis , we have that @xmath237 is independent of the data . for @xmath238 we have @xmath239 , and hence , @xmath240 . since otherwise @xmath241 is non - negative , this proves that exactly for @xmath238 the average case matches the worst case .",
    "now , we have to estimate @xmath242 for arbitrary @xmath2 .",
    "we have to consider the calls to binary insertion more closely . to insert a new element into an array of @xmath243 elements either needs @xmath244 or @xmath245 comparisons . for",
    "a moment assume that the element is inserted at every position with the same probability .",
    "under this assumption the analysis in the proof of is valid , which states that @xmath246 comparisons are needed on average .",
    "the problem is that in our case the probability at which position an element is inserted is not uniformly distributed .",
    "however , it is monotonically increasing with the index in the array ( indices as in our implementation ) . informally speaking ,",
    "this is because if an element is inserted further to the right , then for the following elements there are more possibilities to be inserted than if the element is inserted on the left .",
    "now , can be implemented such that for an odd number of positions the next comparison is made such that the larger half of the array is the one containing the positions with lower probabilities .",
    "( in our case , this is the part with the lower indices   see . )",
    "that means the less probable positions lie on rather longer paths in the search tree , and hence , the average path length is better than in the uniform case",
    ". therefore , we may assume a uniform distribution in the following as an upper bound .    in each of the recursion steps we have @xmath247 calls to binary insertion into sets of size @xmath248 elements each .",
    "hence , for inserting one element , the difference to the worst case is @xmath249 . summing up , we obtain for the average savings @xmath250 w.r.t .",
    "the worst case number @xmath251 the recurrence @xmath252for @xmath253 we write @xmath254 with @xmath255 and we set @xmath256 recall that we have @xmath211 .",
    "thus , @xmath228 and @xmath257 coincide for most @xmath86 and differ by at most 1 for a few values where @xmath86 is close to @xmath258 or @xmath231 . since in both cases @xmath259 is smaller than some constant , this implies that @xmath259 and @xmath260 differ by at most a constant",
    ". furthermore , @xmath259 and @xmath261 differ by at most a constant .",
    "hence , we have : @xmath262 since we have @xmath263 , this resolves to @xmath264 with @xmath265 this means up to @xmath266-terms @xmath267 writing @xmath268 we obtain with @xcite @xmath269 where @xmath270 , i.e. , @xmath271 for some @xmath272 . with @xmath273 it follows @xmath274 this function reaches its minimum in @xmath275 for @xmath276 .",
    "it is not difficult to observe that @xmath277 .",
    "for the factor @xmath278 in @xmath279 we have @xmath280 , where @xmath281 . we know that @xmath137 can be rewritten as @xmath282",
    "hence , we have @xmath283 . finally , we are interested in the value @xmath284 .",
    "our experiments consist of two parts .",
    "first , we compare the different algorithms we use as base cases , i.e. , mergeinsertion , its improved variant , and insertionsort .",
    "the results can be seen in .",
    "depending on the size of the arrays the displayed numbers are averages over 10 - 10000 runs .",
    "the data elements we sorted were randomly chosen 64-bit integers .",
    "the outcome in shows that our improved mergeinsertion implementation achieves results for the constant @xmath285 of the linear term in the range of @xmath286 $ ] ( for some values of @xmath2 are even smaller than @xmath287 ) .",
    "moreover , the standard implementation with slightly more comparisons is faster than insertionsort . by the @xmath145 work ,",
    "the resulting runtimes for all three implementations raises quickly , so that only moderate values of @xmath2 can be handled .",
    "the value of @xmath285 is displayed .",
    ", title=\"fig:\",width=245,height=245 ]   the value of @xmath285 is displayed .",
    ", title=\"fig:\",width=245,height=245 ]    the second part of our experiments ( shown in fig .",
    "[ fig : compare_qms ] ) consists of the comparison of quickmergesort ( with base cases of constant and growing size ) and quickweakheapsort with state - of - the - art algorithms as stl - introsort ( i.e. , quicksort ) , stl - stable - sort ( an implementation of mergesort ) and quicksort with median of @xmath17 elements for pivot selection . for quickmergesort with base cases , the improved variant of mergeinsertion is used to sort subarrays of size up to @xmath288 . for the normal quickmergesort we used base cases of size @xmath289 .",
    "we also implemented quickmergesort with median of three for pivot selection , which turns out to be practically efficient , although it needs slightly more comparisons than quickmergesort with median of @xmath17 .",
    "however , since also the larger half of the partitioned array can be sorted with mergesort , the difference to the median of @xmath17 version is not as big as in quickheapsort @xcite . as suggested by the theory , we see that our improved quickmergesort implementation with growing size base cases mergeinsertion yields a result for the constant in the linear term that is in the range of @xmath290 $ ]  close to the lower bound .",
    "however , for the running time , normal quickmergesort as well as the stl - variants introsort ( ` std::sort ` ) and bottomupmergesort ( ` std::stable_sort ` ) are slightly better . with about 15% the time gap ,",
    "however , is not overly big , and may be bridged with additional efforts like skewed pivots and refined partitioning . also ,",
    "if comparisons are more expensive , quickmergesort should perform significantly faster than introsort .",
    "the value of @xmath285 is displayed .",
    ", title=\"fig:\",width=245,height=245 ]   the value of @xmath285 is displayed .",
    ", title=\"fig:\",width=245,height=245 ]",
    "sorting @xmath2 elements remains a fascinating topic for computer scientists both from a theoretical and from a practical point of view .",
    "with quickxsort we have described a procedure how to convert an external sorting algorithm into an internal one introducing only @xmath23 additional comparisons on average .",
    "we presented quickweakheapsort and quickmergesort as two examples for this construction .",
    "quickmergesort is close to the lower bound for the average number of comparisons and at the same time is practically efficient , even when the comparisons are fast .",
    "using mergeinsertion to sort base cases of growing size for quickmergesort , we derive an an upper bound of @xmath3 comparisons for the average case . as far as we know a better result has not been published before .",
    "our experimental results validate the theoretical considerations and indicate that the factor @xmath287 can be beaten .",
    "of course , there is still room in closing the gap to the lower bound of @xmath291 comparisons .",
    "10    d.  cantone and g.  cinotti .",
    "quickheapsort , an efficient mix of classical sorting algorithms .",
    ", 285(1):2542 , 2002 .",
    "v.  diekert and a.  wei .",
    "quickheapsort : modifications and improved analysis . in a.",
    "a. bulatov and a.  m. shur , editors , _ csr _ , volume 7913 of _ lecture notes in computer science _ ,",
    "pages 2435 .",
    "springer , 2013 .",
    "r.  d. dutton .",
    "weak - heap sort .",
    ", 33(3):372381 , 1993 .",
    "s.  edelkamp and p.  stiegeler .",
    "implementing heapsort with @xmath292 and quicksort with @xmath293 comparisons . , 10(5 ) , 2002 .",
    "s.  edelkamp and i.  wegener . on the performance of weak - heapsort . in _",
    "17th annual symposium on theoretical aspects of computer science _ ,",
    "volume 1770 , pages 254266 .",
    "springer - verlag , 2000 .",
    "a.  elmasry , j.  katajainen , and m.  stenmark .",
    "branch mispredictions do nt affect mergesort . in _ sea _ ,",
    "pages 160171 , 2012 .",
    "j.  ford , lester  r. and s.  m. johnson .",
    "a tournament problem .",
    ", 66(5):pp . 387389 , 1959 .",
    "j.  katajainen .",
    "the ultimate heapsort . in",
    "_ cats _ , pages 8796 , 1998 .",
    "j.  katajainen , t.  pasanen , and j.  teuhola .",
    "practical in - place mergesort . , 3(1):2740 , 1996 .",
    "d.  e. knuth .",
    ", volume  3 of _ the art of computer programming_. addison wesley longman , 2nd edition , 1998 .    c.  martnez and s.  roura .",
    "optimal sampling strategies in quicksort and quickselect . , 31(3):683705 , 2001 .",
    "k.  reinhardt . sorting _ in - place _ with a _",
    "worst case _ complexity of @xmath294 comparisons and @xmath295 transports . in _",
    "isaac _ , pages 489498 , 1992 .",
    "i.  wegener .",
    "ottom - up - heapsort , a new variant of heapsort beating , on an average , quicksort ( if @xmath2 is not very small ) . , 118:8198 , 1993 ."
  ],
  "abstract_text": [
    "<S> in this paper we generalize the idea of quickheapsort leading to the notion of quickxsort . given some external sorting algorithm  x , quickxsort yields an internal sorting algorithm if  x satisfies certain natural conditions .    with quickweakheapsort and quickmergesort we present two examples for the quickxsort - construction . </S>",
    "<S> both are efficient algorithms that incur approximately @xmath0 comparisons on the average . </S>",
    "<S> a worst case of @xmath1 comparisons can be achieved without significantly affecting the average case .    </S>",
    "<S> furthermore , we describe an implementation of mergeinsertion for small @xmath2 . </S>",
    "<S> taking mergeinsertion as a base case for quickmergesort , we establish a worst - case efficient sorting algorithm calling for @xmath3 comparisons on average . </S>",
    "<S> quickmergesort with constant size base cases shows the best performance on practical inputs : when sorting integers it is slower by only 15% to stl - introsort . </S>"
  ]
}