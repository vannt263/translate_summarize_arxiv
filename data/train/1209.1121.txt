{
  "article_text": [
    "our study is broadly motivated by questions in high - dimensional learning . as is well known ,",
    "learning in high dimensions is feasible only if the data distribution satisfies suitable prior assumptions .",
    "one such assumption is that the data distribution lies on , or is close to , a low - dimensional set embedded in a high dimensional space , for instance a low dimensional manifold .",
    "this latter assumption has proved to be useful in practice , as well as amenable to theoretical analysis , and it has led to a significant amount of recent work . starting from @xcite ,",
    "this set of ideas , broadly referred to as _ manifold learning _",
    ", has been applied to a variety of problems from supervised @xcite and semi - supervised learning @xcite , to clustering @xcite and dimensionality reduction @xcite , to name a few .",
    "interestingly , the problem of learning the manifold itself has received less attention : given samples from a d - manifold @xmath0 embedded in some ambient space @xmath1 , the problem is to learn a set that approximates @xmath0 in a suitable sense .",
    "this problem has been considered in computational geometry , but in a setting in which typically the manifold is a hyper - surface in a low - dimensional space ( e.g.  @xmath2 ) , and the data are typically not sampled probabilistically , see for instance  @xcite .",
    "the problem of learning a manifold is also related to that of estimating the support of a distribution , ( see  @xcite for recent surveys . ) in this context , some of the distances considered to measure approximation quality are the hausforff distance , and the so - called _ excess mass _ distance .",
    "the reconstruction framework that we consider is related to the work of  @xcite , as well as to the framework proposed in  @xcite , in which a manifold is approximated by a set , with performance measured by an expected distance to this set .",
    "this setting is similar to the problem of dictionary learning ( see for instance  @xcite , and extensive references therein ) , in which a dictionary is found by minimizing a similar reconstruction error , perhaps with additional constraints on an associated encoding of the data . crucially , while the dictionary is learned on the empirical data , the quantity of interest is the expected reconstruction error , which is the focus of this work .",
    "we analyze this problem by focusing on two important , and widely - used algorithms , namely k - means and k - flats .",
    "the k - means algorithm can be seen to define a piecewise constant approximation of @xmath0 .",
    "indeed , it induces a voronoi decomposition on @xmath0 , in which each voronoi region is effectively approximated by a fixed mean . given this ,",
    "a natural extension is to consider higher order approximations , such as those induced by discrete collections of @xmath3 @xmath4-dimensional affine spaces ( k - flats ) , with possibly better resulting performance .",
    "since @xmath0 is a @xmath4-manifold , the k - flats approximation naturally resembles the way in which a manifold is locally approximated by its tangent bundle .",
    "our analysis extends previous results for k - means to the case in which the data - generating distribution is supported on a manifold , and provides analogous results for k - flats .",
    "we note that the k - means algorithm has been widely studied , and thus much of our analysis in this case involves the combination of known facts to obtain novel results .",
    "the analysis of k - flats , however , requires developing substantially new mathematical tools .",
    "the rest of the paper is organized as follows . in section  [ sec : algo ] , we describe the formal setting and the algorithms that we study .",
    "we begin our analysis by discussing the reconstruction properties of k - means in section  [ sec : disc ] . in section",
    "[ sec : results ] , we present and discuss our main results , whose proofs are postponed to the appendices .",
    "let @xmath1 by a hilbert space with inner product @xmath5 , endowed with a borel probability measure @xmath6 supported over a compact , smooth @xmath4-manifold @xmath0 .",
    "we assume the data to be given by a training set , in the form of samples @xmath7 drawn identically and independently with respect to @xmath6 . + our goal is to _ learn _ a set @xmath8 that approximates well the manifold .",
    "the approximation ( learning error ) is measured by the expected reconstruction error @xmath9 where the distance to a set @xmath10 is @xmath11 , with @xmath12 .",
    "this is the same reconstruction measure that has been the recent focus of  @xcite .",
    "it is easy to see that any set such that @xmath13 will have zero risk , with @xmath0 being the  smallest \" such set ( with respect to set containment . ) in other words , the above error measure does not introduce an explicit penalty on the `` size '' of @xmath8 : enlarging any _ given _ @xmath8 can never increase the learning error .",
    "+ with this observation in mind , we study specific learning algorithms that , given the data , produce a set belonging to some restricted hypothesis space @xmath14 ( e.g.  sets of size @xmath3 for k - means ) , which effectively introduces a constraint on the size of the sets . finally , note that the risk of equation  [ eerho ] is non - negative and , if the hypothesis space is sufficiently _ rich _",
    ", the risk of an unsupervised algorithm may converge to zero under suitable conditions .      in this work , we focus on two specific algorithms , namely k - means  @xcite and k - flats  @xcite .",
    "although typically discussed in the euclidean space case , their definition can be easily extended to a hilbert space setting .",
    "the study of manifolds embedded in a hilbert space is of special interest when considering non - linear ( kernel ) versions of the algorithms  @xcite .",
    "more generally , this setting can be seen as a limit case when dealing with high dimensional data .",
    "naturally , the more classical setting of an absolutely continuous distribution over @xmath4-dimensional euclidean space is simply a particular case , in which @xmath15 , and @xmath0 is a domain with positive lebesgue measure .    * k - means*. let @xmath16 be the class of sets of size @xmath3 in @xmath1 . given a training set @xmath17 and a choice of @xmath3 , k - means is defined by the minimization over @xmath18 of the empirical reconstruction error @xmath19 where , for any fixed set @xmath20",
    ", @xmath21 is an unbiased empirical estimate of @xmath22 , so that k - means can be seen to be performing a kind of empirical risk minimization @xcite .",
    "a minimizer of equation  [ eeemp ] on @xmath23 is a discrete set of @xmath3 _ means _",
    "@xmath24 , which induces a dirichlet - voronoi tiling of @xmath1 : a collection of @xmath3 regions , each closest to a common mean  @xcite ( in our notation , the subscript @xmath25 denotes the dependence of @xmath26 on the sample , while @xmath3 refers to its size . ) by virtue of @xmath26 being a minimizing set , each mean must occupy the center of mass of the samples in its voronoi region .",
    "these two facts imply that it is possible to compute a local minimum of the empirical risk by using a greedy coordinate - descent relaxation , namely lloyd s algorithm  @xcite .",
    "furthermore , given a finite sample @xmath17 , the number of locally - minimizing sets @xmath26 is also finite since ( by the center - of - mass condition ) there can not be more than the number of possible partitions of @xmath17 into @xmath3 groups , and therefore the global minimum must be attainable . even though lloyd s algorithm provides no guarantees of closeness to the global minimizer , in practice it is possible to use a randomized approximation algorithm , such as kmeans++  @xcite , which provides guarantees of approximation to the global minimum in expectation with respect to the randomization .",
    "* k - flats*. let @xmath27 be the class of collections of @xmath3 _ flats _",
    "( affine spaces ) of dimension @xmath4 . for any value of @xmath3 ,",
    "k - flats , analogously to k - means , aims at finding the set @xmath28 that minimizes the empirical reconstruction   over @xmath29 . by an argument similar to the one used for k - means , a global minimizer must be attainable , and a lloyd - type relaxation converges to a local minimum .",
    "note that , in this case , given a voronoi partition of @xmath0 into regions closest to each @xmath4-flat , new optimizing flats for that partition can be computed by a @xmath4-truncated pca solution on the samples falling in each region .",
    "in practice , k - means is often interpreted to be a clustering algorithm , with clusters defined by the voronoi diagram of the set of means @xmath30 . in this interpretation ,",
    "equation  [ eeemp ] is simply rewritten by summing over the voronoi regions , and adding all pairwise distances between samples in the region ( the intra - cluster distances . ) for instance , this point of view is considered in  @xcite where k - means is studied from an information theoretic persepective .",
    "k - means can also be interpreted to be performing vector quantization , where the goal is to minimize the encoding error associated to a nearest - neighbor quantizer  @xcite .",
    "interestingly , in the limit of increasing sample size , this problem coincides , in a precise sense  @xcite , with the problem of optimal quantization of probability distributions ( see for instance the excellent monograph of  @xcite . )    when the data - generating distribution is supported on a manifold @xmath0 , k - means can be seen to be approximating points on the manifold by a discrete set of means .",
    "analogously to the euclidean setting , this induces a voronoi decomposition of @xmath0 , in which each voronoi region is effectively approximated by a fixed mean ( in this sense k - means produces a piecewise constant approximation of @xmath0 . ) as in the euclidean setting , the limit of this problem with increasing sample size is precisely the problem of optimal quantization of distributions on manifolds , which is the subject of significant recent work in the field of optimal quantization  @xcite .",
    "in this paper , we take the above view of k - means as defining a ( piecewise constant ) approximation of the manifold @xmath0 supporting the data distribution .",
    "in particular , we are interested in the behavior of the expected reconstruction error @xmath31 , for varying @xmath3 and @xmath25 .",
    "this perspective has an interesting relation with dictionary learning , in which one is interested in finding a dictionary , and an associated representation , that allows to approximately reconstruct a finite set of data - points / signals . in this interpretation ,",
    "the set of means can be seen as a dictionary of size @xmath3 that produces a maximally sparse representation ( the k - means encoding ) , see for example  @xcite and references therein .",
    "crucially , while the dictionary is learned on the available empirical data , the quantity of interest is the expected reconstruction error , and the question of characterizing the performance with respect to this latter quantity naturally arises .",
    "since k - means produces a piecewise constant approximation of the data , a natural idea is to consider higher orders of approximation , such as approximation by discrete collections of @xmath3 @xmath4-dimensional affine spaces ( k - flats ) , with possibly better performance .",
    "since @xmath0 is a @xmath4-manifold , the approximation induced by k - flats may more naturally resemble the way in which a manifold is locally approximated by its tangent bundle .",
    "we provide in sec .",
    "[ sec : kflats_rates ] a partial answer to this question .",
    "since we are interested in the behavior of the expected reconstruction   of k - means and k - flats for _ varying @xmath3 and @xmath25 _ , before analyzing this behavior , we consider what is currently known about this problem , based on previous work . while k - flats is a relatively new algorithm",
    "whose behavior is not yet well understood , several properties of k - means are currently known .",
    "recall that k - means find an discrete set @xmath26 of size @xmath3 that best approximates the samples in the sense of  .",
    "clearly , as @xmath3 increases , the empirical reconstruction error @xmath32 can not increase , and typically decreases .",
    "however , we are ultimately interested in the expected reconstruction error , and therefore would like to understand the behavior of @xmath31 with varying @xmath33 .",
    "in the context of optimal quantization , the behavior of the expected reconstruction error @xmath34 has been considered for an approximating set @xmath35 obtained by minimizing the _ expected _ reconstruction error itself over the hypothesis space @xmath36 .",
    "the set @xmath35 can thus be interpreted as the output of a _ population _ , or infinite sample version of k - means . in this case , it is possible to show that @xmath37 is a non increasing function of @xmath3 and , in fact , to derive explicit rates .",
    "for example in the case @xmath38 , and under fairly general technical assumptions , it is possible to show that @xmath39 , where the constants depend on @xmath40 and @xmath4  @xcite . in machine learning ,",
    "the properties of k - means have been studied , _ for fixed @xmath3 _ , by considering the _ excess _ reconstruction error @xmath41 . in particular , this quantity has been studied for @xmath38 , and shown to be , with high probability , of order @xmath42 , up - to logarithmic factors  @xcite .",
    "the case where @xmath1 is a hilbert space has been considered in  @xcite , where an upper - bound of order @xmath43 is proven to hold with high probability . the more general setting where @xmath1 is a metric space has been studied in  @xcite .     dimensional sphere embedded in @xmath44 ( left ) .",
    "for each value of @xmath3 , k - means ( with k - means++ seeding ) is run @xmath45 times , and the best solution kept . the reconstruction performance on a ( large ) hold - out set",
    "is reported as a function of @xmath3 . the results for four different training set cardinalities",
    "are reported : for small number of points , the reconstruction error decreases sharply for small @xmath3 and then increases , while it is simply decreasing for larger data sets . a similar experiment , yielding similar results , is performed on subsets of the mnist ( ` http://yann.lecun.com/exdb/mnist ` ) database ( right ) . in this case the data might be thought to be concentrated around a low dimensional manifold .",
    "for example @xcite report an average intrinsic dimension @xmath4 for each digit to be between @xmath46 and @xmath47 . ]     dimensional sphere embedded in @xmath44 ( left ) . for each value of @xmath3 , k - means ( with k - means++ seeding ) is run @xmath45 times , and the best solution kept .",
    "the reconstruction performance on a ( large ) hold - out set is reported as a function of @xmath3 .",
    "the results for four different training set cardinalities are reported : for small number of points , the reconstruction error decreases sharply for small @xmath3 and then increases , while it is simply decreasing for larger data sets . a similar experiment , yielding similar results ,",
    "is performed on subsets of the mnist ( ` http://yann.lecun.com/exdb/mnist ` ) database ( right ) . in this case the data might be thought to be concentrated around a low dimensional manifold .",
    "for example @xcite report an average intrinsic dimension @xmath4 for each digit to be between @xmath46 and @xmath47 . ]    when analyzing the behavior of @xmath31 , and in the particular case that @xmath38 , the above results can be combined to obtain , with high probability , a bound of the form @xmath48 up to logarithmic factors , where the constant @xmath49 does not depend on @xmath3 or @xmath25 ( a complete derivation is given in the appendix . )",
    "the above inequality suggests a somewhat surprising effect : the expected reconstruction properties of k - means may be described by a _ trade - off _ between a statistical error ( of order @xmath50 ) and a geometric approximation error ( of order @xmath51 . )",
    "the existence of such a tradeoff between the approximation , and the statistical errors may itself not be entirely obvious , see the discussion in  @xcite .",
    "for instance , in the k - means problem , it is intuitive that , as more means are inserted , the expected distance from a random sample to the means should decrease , and one might expect a similar behavior for the expected reconstruction error .",
    "this observation naturally begs the question of whether and when this trade - off really exists or if it is simply a result of the looseness in the bounds .",
    "in particular , one could ask how tight the bound   is .    while the bound on @xmath37 is known to be tight for @xmath3 sufficiently large  @xcite , the remaining terms ( which are dominated by @xmath52 ) are derived by controlling the supremum of an empirical process @xmath53 and it is unknown whether available bounds for it are tight  @xcite .",
    "indeed , it is not clear how close the _ distortion redundancy _",
    "@xmath54 is to its known lower bound of order @xmath55 ( in expectation )  @xcite .",
    "more importantly , we are not aware of a lower bound for @xmath56 itself . indeed , as pointed out in  @xcite ,  the exact dependence of the minimax distortion redundancy on k and d is still a challenging open problem \" .",
    "finally , we note that , whenever a trade - off can be shown to hold , it may be used to justify a heuristic for choosing @xmath3 empirically as the value that minimizes the reconstruction error in a hold - out set .    in figure  [ fig:1 ]",
    "we perform some simple numerical simulations showing that the trade - off indeed occurs in certain regimes .",
    "the following example provides a situation where a trade - off can be easily shown to occur .",
    "consider a setup in which @xmath57 samples are drawn from a uniform distribution on the unit @xmath58-sphere , though the argument holds for other @xmath25 much smaller than @xmath4 . because @xmath59 , with high probability , the samples are nearly orthogonal : @xmath60 , while a third sample @xmath61 drawn uniformly on @xmath62 will also very likely be nearly orthogonal to both @xmath63  @xcite .",
    "the k - means solution on this dataset is clearly @xmath64 ( fig  [ fig : tradeoffa ] ) . indeed , since @xmath65 ( fig  [ fig : tradeoffb ] ) , it is @xmath66 with very high probability . in this case , it is better to place a single mean closer to the origin ( with @xmath67 ) , than to place two means at the sample locations .",
    "this example is sufficiently simple that the exact k - means solution is known , but the effect can be observed in more complex settings .",
    "* contributions*. our work extends previous results in two different directions :    a.   we provide an analysis of k - means for the case in which the data - generating distribution is supported on a manifold embedded in a hilbert space .",
    "in particular , in this setting : 1 ) we derive new results on the approximation error , and 2 ) new sample complexity results ( learning rates ) arising from the choice of @xmath3 by optimizing the resulting bound .",
    "we analyze the case in which a solution is obtained from an approximation algorithm , such as k - means++  @xcite , to include this computational error in the bounds .",
    "b.   we generalize the above results from k - means to k - flats , deriving learning rates obtained from new bounds on both the statistical and the approximation errors . to the best of our knowledge ,",
    "these results provide the first theoretical analysis of k - flats in either sense .",
    "we note that the k - means algorithm has been widely studied in the past , and much of our analysis in this case involves the combination of known facts to obtain novel results",
    ". however , in the case of k - flats , there is currently no known analysis , and we provide novel results as well as new performance bounds for each of the components in the bounds .    throughout this section",
    "we make the following technical assumption :    [ ass0 ] @xmath0 is a smooth d - manifold with metric of class @xmath68 , contained in the unit ball in @xmath1 , and with volume measure denoted by @xmath69 .",
    "the probability measure @xmath40 is absolutely continuous with respect to @xmath69 , with density @xmath70 .",
    "the first result considers the idealized case where we have access to an exact solution for k - means .",
    "[ thkm ] under assumption  [ ass0 ] , if @xmath26 is a solution of k - means then , for @xmath71 , there are constants @xmath49 and @xmath72 dependent only on @xmath4 , and sufficiently large @xmath73 such that , by setting @xmath74 and @xmath75 , it is @xmath76 \\ge 1-\\delta , \\ ] ] for all @xmath77 , where @xmath78 and @xmath72 grows sublinearly with @xmath4 .",
    "note that the distinction between distributions with density in @xmath0 , and singular distributions is important .",
    "the bound of equation   holds only when the absolutely continuous part of @xmath6 over @xmath0 is non - vanishing .",
    "the case in which the distribution is singular over @xmath0 requires a different analysis , and may result in faster convergence rates .",
    "the following result considers the case where the k - means++ algorithm is used to compute the estimator .",
    "[ thkmpp ] under assumption  [ ass0 ] , if @xmath26 is the solution of k - means++ , then for @xmath79 , there are constants @xmath49 and @xmath72 that depend only on @xmath4 , and a sufficiently large @xmath73 such that , by setting @xmath74 and @xmath75 , it is @xmath80 \\ge 1-\\delta , \\ ] ] for all @xmath77 , where the expectation is with respect to the random choice @xmath81 in the algorithm , and @xmath82 , @xmath78 , and @xmath72 grows sublinearly with @xmath4 .    in the particular case that @xmath15 and @xmath0 is contained in the unit ball",
    ", we may further bound the distribution - dependent part of equations  [ eqkm ] and  [ eqkmpp ] .",
    "using hlder s inequality , one obtains @xmath83^{d/(d+2 ) } \\cdot \\left[\\displaystyle{\\int_{{\\mathcal m } } d\\nu(x)}\\right]^{2/(d+2 ) } \\\\          & \\le \\text{vol}({\\mathcal m})^{2/(d+2 ) } \\le \\omega_d^{2/(d+2 ) } ,   \\end{split}\\ ] ] where @xmath84 is the lebesgue measure in @xmath85 , and @xmath86 is the volume of the @xmath4-dimensional unit ball .",
    "it is clear from the proof of theorem  [ thkm ] that , in this case , we may choose @xmath87 independently of the density @xmath70 , to obtain a bound @xmath88 with probability @xmath89 ( and similarly for theorem  [ thkmpp ] , except for an additional @xmath90 term ) , where the constant only depends on the dimension .",
    "note that according to the above theorems , choosing @xmath3 requires knowledge of properties of the distribution @xmath40 underlying the data , such as the intrinsic dimension of the support .",
    "in fact , following the ideas in @xcite section 6.3 - 5 , it is easy to prove that choosing @xmath3 to minimize the reconstruction error on a hold - out set , allows to achieve the same learning rates ( up to a logarithmic factor ) , adaptively in the sense that knowledge of properties of @xmath40 are not needed .      to study k - flats , we need to slightly strengthen assumption  [ ass0 ] by adding to it by the following :    [ asskf ] assume the manifold @xmath0 to have metric of class @xmath91 , and finite second fundamental form @xmath92  @xcite .",
    "one reason for the higher - smoothness assumption is that k - flats uses higher order approximation , whose analysis requires a higher order of differentiability .",
    "+ we begin by providing a result for k - flats on hypersurfaces ( codimension one ) , and next extend it to manifolds in more general spaces .",
    "[ thkf ] let , @xmath93 . under assumptions  [ ass0],[asskf ] , if @xmath94 is a solution of k - flats , then there is a constant @xmath49 that depends only on @xmath4 , and sufficiently large @xmath73 such that , by setting @xmath95 and @xmath96 , then for all @xmath77 it is @xmath97 \\ge 1-\\delta , \\ ] ] where @xmath98 is the total root curvature of @xmath0 , @xmath99 is the measure associated with the ( positive ) second fundamental form , and @xmath100 is the gaussian curvature on @xmath0 .    in the more general case of a @xmath4-manifold @xmath0 ( with metric in @xmath91 ) embedded in a separable hilbert space @xmath1",
    ", we can not make any assumption on the codimension of @xmath0 ( the dimension of the orthogonal complement to the tangent space at each point . ) in particular , the second fundamental form @xmath92 , which is an extrinsic quantity describing how the tangent spaces bend locally is , at every @xmath101 , a map @xmath102 ( in this case of class @xmath68 by assumption  [ asskf ] ) from the tangent space to its orthogonal complement ( @xmath103 in the notation of  @xcite . ) crucially , in this case , we may no longer assume the dimension of the orthogonal complement @xmath104 to be finite .",
    "+ denote by @xmath105 , the operator norm of @xmath106 .",
    "we have :    [ thkfg ] under assumptions  [ ass0],[asskf ] , if @xmath94 is a solution to the k - flats problem , then there is a constant @xmath49 that depends only on @xmath4 , and sufficiently large @xmath73 such that , by setting @xmath107 and @xmath96 , then for all @xmath77 it is @xmath108 \\ge 1-\\delta , \\ ] ] where @xmath109    note that the better k - flats bounds stem from the higher approximation power of @xmath4-flats over points .",
    "although this greatly complicates the setup and proofs , as well as the analysis of the constants , the resulting bounds are of order @xmath110 , compared with the slower order @xmath111 of k - means .      in all the results , the final performance does not depend on the dimensionality of the embedding space ( which in fact can be infinite ) , but only on the intrinsic dimension of the space on which the data - generating distribution is defined .",
    "the key to these results is an approximation construction in which the voronoi regions on the manifold ( points closest to a given mean or flat ) are guaranteed to have vanishing diameter in the limit of @xmath3 going to infinity . under our construction , a hypersurface",
    "is approximated efficiently by tracking the variation of its tangent spaces by using the second fundamental form .",
    "where this form vanishes , the voronoi regions of an approximation will not be ensured to have vanishing diameter with @xmath3 going to infinity , unless certain care is taken in the analysis .",
    "an important point of interest is that the approximations are controlled by averaged quantities , such as the total root curvature ( k - flats for surfaces of codimension one ) , total curvature ( k - flats in arbitrary codimensions ) , and @xmath112-norm of the probability density ( k - means ) , which are integrated over the domain where the distribution is defined .",
    "note that these types of quantities have been linked to provably tight approximations in certain cases , such as for convex manifolds  @xcite , in contrast with worst - case methods that place a constraint on a maximum curvature , or minimum injectivity radius ( for instance  @xcite . ) intuitively , it is easy to see that a constraint on an average quantity may be arbitrarily less restrictive than one on its maximum . a small difficult region ( e.g.  of very high curvature ) may cause the bounds of the latter to substantially degrade , while the results presented here would not be adversely affected so long as the region is small .",
    "additionally , care has been taken throughout to analyze the behavior of the constants .",
    "in particular , there are no constants in the analysis that grow exponentially with the dimension , and in fact , many have polynomial , or slower growth .",
    "we believe this to be an important point , since this ensures that the asymptotic bounds do not hide an additional exponential dependence on the dimension .",
    "although both k - means and k - flats optimize the same empirical risk , the performance measure we are interested in is that of equation  [ eerho ]",
    ". we may bound it from above as follows : @xmath113 where @xmath114 is the best attainable performance over @xmath115 , and @xmath116 is a set for which the best performance is attained .",
    "note that @xmath117 by the definition of @xmath26 .",
    "the same error decomposition can be considered for k - flats , by replacing @xmath26 by @xmath94 and @xmath115 by @xmath29 .",
    "equation  [ eqsplit ] decomposes the total learning error into two terms : a uniform ( over all sets in the class @xmath118 ) bound on the difference between the empirical , and true error measures , and an _ approximation error _ term .",
    "the uniform statistical error bound will depend on the samples , and thus may hold with a certain probability .    in this setting",
    ", the approximation error will typically tend to zero as the class @xmath118 becomes larger ( as @xmath3 increases . )",
    "note that this is true , for instance , if @xmath118 is the class of discrete sets of size @xmath3 , as in the k - means problem .",
    "the performance of equation  [ eqsplit ] is , through its dependence on the samples , a random variable .",
    "we will thus set out to find probabilistic bounds on its performance , as a function of the number @xmath25 of samples , and the size @xmath3 of the approximation . by choosing the approximation size parameter @xmath3 to minimize these bounds ,",
    "we obtain performance bounds as a function of the sample size .",
    "we use the above decomposition to derive sample complexity bounds for the performance of the k - means algorithm . to derive explicit bounds on the different error terms we have to combine in a novel way some previous results and some new observations .",
    "+ * approximation error*. the error @xmath119 is related to the problem of optimal quantization .",
    "the classical optimal quantization problem is quite well understood , going back to the fundamental work of  @xcite on optimal quantization for data transmission , and more recently by the work of  @xcite .",
    "in particular , it is known that , for distributions with finite moment of order @xmath120 , for some @xmath121 , it is  @xcite @xmath122 where @xmath84 is the lebesgue measure , @xmath123 is the density of the absolutely continuous part of the distribution ( according to its lebesgue decomposition ) , and @xmath49 is a constant that depends only on the dimension .",
    "therefore , the approximation error decays _ at least _ as fast as @xmath51 .",
    "we note that , by setting @xmath124 to be the uniform distribution over the unit cube @xmath125^d$ ] , it clearly is @xmath126 and thus , by making use of zador s asymptotic formula  @xcite , and combining it with a result of brczky ( see  @xcite , p.  491 ) , we observe that @xmath127 with @xmath128 , for the @xmath129-th order quantization problem .",
    "in particular , this shows that the constant @xmath49 only depends on the dimension , and , in our case ( @xmath130 ) , has only linear growth in @xmath4 , a fact that will be used in the sequel .",
    "the approximation error @xmath119 of k - means is related to the problem of optimal quantization on manifolds , for which some results are known  @xcite . by calling @xmath131 the approximation error only among sets of means contained in @xmath0 , theorem  [ thgruber ] in appendix  [ sec : kf ] ,",
    "implies in this case ( letting @xmath130 ) that @xmath132 where @xmath70 is absolutely continuous over @xmath0 and , by replacing @xmath0 with a @xmath4-dimensional domain in @xmath85 , it is clear that the constant @xmath49 is the same as above .",
    "since restricting the means to be on @xmath0 can not decrease the approximation error , it is @xmath133 , and therefore the right - hand side of equation  [ eq : aekm ] provides an ( asymptotic ) upper bound to @xmath134 .    for the statistical error we use available bounds .",
    "+ * statistical error*. the statistical error of equation  [ eqsplit ] , which uniformly bounds the difference between the empirical , and expected error , has been widely - studied in recent years in the literature  @xcite . in particular , it has been shown that , for a distribution @xmath70 over the unit ball in @xmath85 , it is @xmath135 with probability @xmath89  @xcite . clearly , this implies convergence @xmath136 almost surely , as @xmath137 ; although this latter result was proven earlier in  @xcite , under the less restrictive condition that @xmath138 have finite second moment . + by bringing together the above results , we obtain the bound in theorem  [ thkm ] on the performance of k - means , whose proof is postponed to appendix a. + further , we can consider the error incurred by the actual optimization algorithm used to compute the k - means solution . + * computational error*. in practice , the k - means problem is np - hard  @xcite , with the original lloyd relaxation algorithm providing no guarantees of closeness to the global minimum of equation  [ eeemp ] .",
    "however , practical approximations , such as the k - means++ algorithm  @xcite , exist .",
    "when using k - means++ , means are inserted one by one at samples selected with probability proportional to their squared distance to the set of previously - inserted means .",
    "this randomized seeding has been shown by  @xcite to output a set that is , in expectation , within a @xmath139-factor of the optimal . once again , by combining these results , we obtain theorem  [ thkmpp ] , whose proof is also in appendix a.    we use the results discussed in section  [ sec : derivation ] to obtain the proof of theorem  [ thkm ] as follows .    letting @xmath140 , then with probability @xmath89 , it is @xmath141 where the parameter @xmath142 has been chosen to balance the summands in the third line of equation  [ eq : derivkm ] .",
    "the proof of theorem  [ thkmpp ] follows a similar argument .    in the case of theorem  [ thkmpp ] ,",
    "the additional multiplicative term @xmath143 corresponding to the computational error incurred by the k - means++ algorithm does not affect the choice of parameter @xmath144 since both summands in the third line of equation  [ eq : derivkm ] are multiplied by @xmath145 in this case .",
    "therefore , we may simply use the same choice of @xmath144 as in equation  [ kmkn ] in this case to obtain @xmath146 \\end{split}\\ ] ] with probability @xmath89 , where the expectation is with respect to the random choice @xmath81 in the algorithm . from this",
    "the bound of theorem  [ thkmpp ] follows .",
    "here we state a series of lemma that we prove in the next section . for the k - flats problem , we begin by introducing a uniform bound on the difference between empirical ( equation  [ eeemp ] ) and expected risk ( equation  [ eerho ] . )      by combining the above result with approximation error bounds , we may produce performance bounds on the expected risk for the k - flats problem , with appropriate choice of parameter @xmath144 .",
    "we distinguish between the codimension one hypersurface case , and the more general case of a smooth manifold @xmath0 embedded in a hilbert space .",
    "we begin with an approximation error bound for hypersurfaces in euclidean space .",
    "[ kfae ] assume given @xmath0 smooth with metric of class @xmath91 in @xmath150 . if @xmath147 is the class of sets of @xmath3 @xmath4-dimensional affine spaces , and @xmath151 is the minimizer of equation  [ eerho ] over @xmath147 , then there is a constant @xmath49 that depends on @xmath4 only , such that @xmath152 where @xmath153 is the total root curvature of @xmath0 , and @xmath154 is the measure associated with the ( positive ) second fundamental form .",
    "the constant @xmath49 grows as @xmath155 with @xmath128 .    for the more general problem of approximation of a smooth manifold in a separable hilbert space , we begin by considering the definitions in section  [ sec : results ] the second fundamental form @xmath92 and its operator norm @xmath156 at a point @xmath157",
    "the we have :    [ kfaepp ] assume given a @xmath4-manifold @xmath0 with metric in @xmath91 embedded in a separable hilbert space @xmath1 .",
    "if @xmath147 is the class of sets of @xmath3 @xmath4-dimensional affine spaces , and @xmath151 is the minimizer of equation  [ eerho ] over @xmath147 , then there is a constant @xmath49 that depends on @xmath4 only , such that              we begin by finding uniform upper bounds on the difference between equations  [ eerho ] and  [ eeemp ] for the class @xmath147 of sets of @xmath3 @xmath4-dimensional affine spaces . to do this",
    ", we will first bound the rademacher complexity @xmath161 of the class @xmath147 .",
    "let @xmath162 and @xmath163 be gaussian processes indexed by @xmath147 , and defined by @xmath164 @xmath165 , @xmath166 is the union of @xmath3 @xmath4-subspaces : @xmath167 , where each @xmath168 is an orthogonal projection onto @xmath169 , and @xmath170 are independent gaussian sequences of zero mean and unit variance .",
    "noticing that @xmath171 for any orthogonal projection @xmath172 ( see for instance  @xcite , sec .",
    "2.1 ) , where @xmath173 is the hilbert - schmidt inner product , we may verify that : @xmath174 ^ 2 } \\\\",
    "& \\le   \\ds{\\sum_{i=1}^n   \\max_{j=1}^k \\left (    \\left < x_i x_i^t , \\pi'_j\\right>_{_f }   -   \\left < x_i x_i^t , \\pi''_j\\right>_{_f }   \\right)^2 } \\\\          & \\le   \\ds{\\sum_{i=1}^n   \\sum_{j=1}^k \\left (    \\left < x_i x_i^t , \\pi'_j\\right>_{_f }   -   \\left < x_i x_i^t , \\pi''_j\\right>_{_f }   \\right)^2 }   = \\operatorname{\\mathbb{e}}_\\gamma \\left(\\psi_{\\x ' } - \\psi_{\\x '' } \\right)^2 \\end{split}\\ ] ]    since it is , @xmath175 we may bound the gaussian complexity @xmath176 as follows : @xmath177 where the first inequality follows from equation  [ eq : slepian ] and slepian s lemma  @xcite , and the second from equation  [ eq : gc ] .          in order to prove approximation bounds for the k - flats problem , we will begin by first considering the simpler setting of a smooth @xmath4-manifold in @xmath150 space ( codimension @xmath180 ) , and later we will extend the analysis to the general case .",
    "assume that it is @xmath93 with the natural metric , and @xmath0 is a compact , smooth @xmath4-manifold with metric of class @xmath181 .",
    "since @xmath0 is of codimension one , the second fundamental form at each point is a map from the tangent space to the reals .",
    "assume given @xmath182 and @xmath183 .",
    "at every point @xmath101 , define the metric @xmath184 , where    * @xmath185 and @xmath92 are , respectively , the first and second fundamental forms on @xmath0  @xcite . * @xmath186 is the _ convexified _ second fundamental form , whose eigenvalues are those of @xmath92 but in absolute value . if the second fundamental form @xmath92 is written in coordinates ( with respect to an orthonormal basis of the tangent space ) as @xmath187 , with @xmath20 orthonormal , and @xmath188 diagonal , then @xmath186 is @xmath189 in coordinates .",
    "because @xmath186 is continuous and positive semi - definite , it has an associated measure @xmath154 ( with respect to the volume measure @xmath160 . )",
    "* @xmath190 is chosen such that @xmath191 .",
    "note that such @xmath190 always exists since : * * @xmath192 implies @xmath193 , and * * @xmath194 can be made arbitrarily large by increasing @xmath195 .",
    "+ and therefore there is some intermediate value of @xmath196 that satisfies the constraint .",
    "let @xmath69 and @xmath198 be the measures over @xmath0 , associated with @xmath185 and @xmath197 . since , by its definition , @xmath199 is absolutely continuous with respect to @xmath185 , then so must @xmath197 be .",
    "therefore , we may define @xmath200 to be the density of @xmath198 with respect to @xmath160 .",
    "consider the discrete set @xmath201 of size @xmath3 that minimizes the quantity @xmath202",
    "p_k } d^4_{_{_q}}(x , p )   } \\ ] ] among all sets of @xmath3 points _ on _ @xmath0 .",
    "@xmath203 is the ( fourth - order ) quantization error over @xmath0 , with metric @xmath197 , and with respect to a weight function @xmath204 .",
    "note that , in the definition of @xmath203 , it is crucial that the measure ( @xmath205 ) , and distance ( @xmath206 ) match , in the sense that @xmath207 is the geodesic distance with respect to the metric @xmath197 , whose associated measure is @xmath198 .      _",
    "[ @xcite]_[thgruber ] given a smooth compact riemannian @xmath4-manifold @xmath0 with metric @xmath197 of class @xmath68 , and a continuous function @xmath208 , then @xmath209 as @xmath210 , where the constant @xmath49 depends only on @xmath4 .",
    "furthermore , for each connected @xmath0 , there is a number @xmath211 such that each set @xmath212 that minimizes equation  [ gruberoq ] is a @xmath213-packing and @xmath214-cover of @xmath0 , with respect to @xmath207 .",
    "this last result , which shows that a minimizing set @xmath212 of size @xmath3 must be a @xmath214-cover , clearly implies , by the definition of voronoi diagram and the triangle inequality , the following key corollary .",
    "[ cor : diam ] given @xmath0 , there is @xmath215 such that each set @xmath212 that minimizes equation  [ gruberoq ] has voronoi regions of diameter no larger than @xmath216 , as measured by the distance @xmath207 .",
    "let each @xmath201 be a minimizer of equation  [ eqoq4 ] of size @xmath3 , then , for each @xmath3 , define @xmath217 to be the union of ( @xmath4-dimensional affine ) tangent spaces to @xmath0 at each @xmath218 , that is , @xmath219 .",
    "we may now use the definition of @xmath212 to bound the approximation error @xmath220 on this set .",
    "we begin by establishing some results that link distance to tangent spaces on manifolds to the geodesic distance @xmath206 associated with @xmath197 .",
    "the following lemma appears ( in a slightly different form ) as lemma 4.1 in  @xcite , and is borrowed from  @xcite .    _",
    "[ @xcite ,  @xcite]_[lem : lambda ] given @xmath0 as above , and @xmath183 then , for every @xmath221 there is an open neighborhood @xmath222 in @xmath0 such that , for all @xmath223 , it is @xmath224 where @xmath225 is the distance from @xmath61 to the tangent plane @xmath226 at @xmath227 , and @xmath228 is the geodesic distance associated with the convexified second fundamental form .      given our choice of @xmath183 , lemma  [ lem : lambda ] implies that there is a collection of @xmath3 neighborhoods , centered around the points @xmath231 , such that equation  [ eq : lambda ] holds inside each .",
    "however , these neighborhoods may be too small for our purposes . in order to apply lemma  [ lem : lambda ] to our problem",
    ", we will need to prove a stronger condition .",
    "we begin by considering the dirichlet - voronoi regions @xmath232 of points @xmath231 , with respect to the distance @xmath207 .",
    "that is , @xmath233 where , as before , @xmath212 is a set of size @xmath3 minimizing equation  [ eqoq4 ] .",
    "* remark * note that , if it were @xmath237 with @xmath238 ( if each @xmath239 were constructed by adding one point to @xmath212 ) , then lemma  [ lem : cover ] would follow automatically from lemma  [ lem : lambda ] and corollary  [ cor : diam ] .",
    "since , in general , this not the case , the following proof is needed .",
    "+    it suffices to show that every voronoi region @xmath240 , for sufficiently large @xmath3 , is contained in a neighborhood @xmath241 of the type described in lemma  [ lem : lambda ] , for some @xmath242 .",
    "clearly , by lemma  [ lem : lambda ] , the set @xmath243 is an open cover of @xmath0 .",
    "since @xmath0 is compact , @xmath49 admits a finite subcover @xmath244 . by the lebesgue number lemma",
    ", there is @xmath245 such that every set in @xmath0 of diameter less than @xmath246 is contained in some open set of @xmath244 .",
    "now let @xmath247 . by corollary  [ cor : diam ]",
    ", every voronoi region @xmath240 , with @xmath218 , @xmath235 , has diameter less than @xmath246 , and is therefore contained in some set of @xmath244 .",
    "since equation  [ eq : lambda ] holds inside every set of @xmath244 then , in particular , it holds inside @xmath240 .",
    "we now have all the tools needed to prove : + * lemma  [ kfae ] * _ if @xmath147 is the class of sets of @xmath3 @xmath4-dimensional affine spaces , and @xmath151 is the minimizer of equation  [ eerho ] over @xmath147 , then there is a constant @xmath49 that depends on @xmath4 only , such that @xmath152 where @xmath153 is the total root curvature of @xmath0 .",
    "the constant @xmath49 grows as @xmath155 with @xmath128 .",
    "_    pick @xmath182 and @xmath183 .",
    "given @xmath212 minimizing equation  [ eqoq4 ] , if @xmath217 is the union of tangent spaces at each @xmath231 , by lemmas  [ lem : lambda ] and  [ lem : cover ] , it is @xmath248^{d/(d+4 ) } } \\right\\}^{(d+4)/d } \\cdot k^{-4/d } \\end{split}\\ ] ] where the last line follows from the fact that @xmath212 has been chosen to minimize equation  [ eqoq4 ] , and where , in order to apply theorem  [ thgruber ] , we use the fact that @xmath70 is absolutely continuous in @xmath0 .    by the definition of @xmath249",
    ", it follows that @xmath250^{d/(d+4 ) } } \\right\\}^{(d+4)/d } & = \\left\\ { \\ds {   \\int_{\\mathcal m}d\\mu_{_\\text{i}}(x ) \\omega_q(x)^{4/(d+4 ) } { p}(x)^{d/(d+4 ) } }   \\right\\}^{(d+4)/d }     \\\\                  & \\le                       \\left\\ { \\ds {   \\int_{\\mathcal m}d\\mu_{_\\text{i}}(x )   \\omega_{_{q}}(x ) } \\right\\}^{4/d }                     \\end{split}\\ ] ] where the last line follows from hlder s inequality ( @xmath251 with @xmath252 , and @xmath253 . )    finally , by the definition of @xmath197 and @xmath254 , it is @xmath255 where @xmath256 is the total volume of @xmath0 , and @xmath257 is the total root curvature of @xmath0 . therefore @xmath258 since @xmath182 and @xmath183 are arbitrary , lemma  [ kfae ] follows .",
    "finally , we discuss an important technicality in the proof that we had nt mentioned before in the interest of clarity of exposition . because we are taking absolutely values in its definition ,",
    "@xmath197 is not necessarily of class @xmath68 , even if @xmath92 is .",
    "therefore , we may not apply theorem  [ thgruber ] directly .",
    "we may , however , use weierstrass approximation theorem ( see for example  @xcite p.  133 ) , to obtain a smooth @xmath259-approximation to @xmath197 , which can be enforced to be positive definite by relating the choice of @xmath259 to that of @xmath260 , and with @xmath261 as @xmath262 .",
    "since the @xmath259-approximation @xmath197 only affects the final performance ( equation  [ eq : perf ] ) by at most a constant times @xmath259 , then the fact that @xmath260 is arbitrarily small ( and thus so is @xmath259 ) implies the lemma .",
    "assume given a @xmath4-manifold @xmath0 with metric in @xmath91 embedded in a separable hilbert space @xmath1 .",
    "consider the definition in section  [ sec : results ] of the second fundamental form @xmath92 and its operator norm @xmath186 .",
    "we begin extending the results of lemma  [ lem : lambda ] to the general case , where the manifold is embedded in a possibly infinite - dimensional ambient space . in this case",
    ", the orthogonal complement @xmath263 to the tangent space at @xmath101 may be infinite - dimensional ( although , by the separability of @xmath1 , it has a countable basis . )    for each @xmath101 , consider the largest @xmath61-centered ball @xmath264 for which there is a smooth one - to - one monge patch @xmath265 . since @xmath0 is smooth , and @xmath92 bounded , by the inverse function theorem it holds @xmath266 . because @xmath267 , we can always choose @xmath268 to be continuous in @xmath0 , and thus by the compactness of @xmath0 there is a minimum @xmath269 such that @xmath270 with @xmath101 .",
    "let @xmath271 denote the geodesic neighborhood around @xmath101 of radius @xmath246 .",
    "we begin by proving the following technical lemma .",
    "the monge function @xmath275 is such that @xmath276 implies @xmath277 ( with the appropriate identification of vectors in @xmath1 and in @xmath278 ) , and therefore for all @xmath276 it holds @xmath279 therefore @xmath280 .",
    "let @xmath288 be a geodesic neighborhood of radius smaller than @xmath289 , so that lemma  [ lem : monge ] holds .",
    "define the extension @xmath290 of the second fundamental form to @xmath1 , where @xmath291 and @xmath292 is the unique decomposition of @xmath293 into tangent and orthogonal components .    by lemma  [ lem : monge ] ,",
    "given @xmath286 , @xmath61 is in the ( one - to - one ) monge patch @xmath294 of @xmath227 .",
    "let @xmath295 be the unique point such that @xmath296 , and let @xmath297 . since the domain of @xmath294 is convex , the curve @xmath298\\rightarrow{\\mathcal m}$ ] given by @xmath299 is well - defined , where the last equality follows from the smoothness of @xmath92 . clearly , @xmath300",
    ".    for @xmath301 the length of @xmath302)$ ] is @xmath303 ) ) = \\displaystyle{\\int_0^t d\\tau \\| \\dot{\\gamma_{y , r}}(\\tau)\\|_{_{\\mathcal{x } } } }               = \\displaystyle{\\int_0^t d\\tau   \\left(\\|r\\|_{\\mathcal{x}}+ o(t)\\right ) } = t \\cdot ( 1 + o(1 ) )     \\end{split}\\ ] ] ( where @xmath304 as @xmath305 . )",
    "this establishes the closeness of distances in @xmath226 to geodesic distance on @xmath0 .",
    "in particular , for any @xmath306 , @xmath307 , there is a sufficiently small geodesic neighborhood @xmath308 such that , for @xmath309 , it holds @xmath310    by the smoothness of @xmath92 , for @xmath307 and @xmath311 , with @xmath312 , it is @xmath313 and therefore for any @xmath182 , there is a sufficiently small @xmath314 such that , given any @xmath315 , it is @xmath316 by the smoothness of @xmath92 , and the same argument as in lemma  [ lem : monge ] , there is a continuous choice of @xmath317 , and therefore a minimum value @xmath318 , for @xmath307 .",
    "similarly , by the smoothness of @xmath319 , for any @xmath306 and @xmath307 , there is a sufficiently small @xmath320 such that , for all @xmath321 , it holds @xmath322 by the argument of lemma  [ lem : monge ] , there is a continuous choice of @xmath323 , and therefore a minimum value @xmath324 , for @xmath307 .    finally , let @xmath325 , and restrict @xmath326 ( larger @xmath327 are simply less restrictive . ) for each @xmath157 , let @xmath328 be a sufficiently small geodesic neighborhood such that , for all @xmath286 , eqs .",
    "[ lem : lambdag_cond1 ] and  [ lem : lambdag_cond2 ] hold .",
    "note that the same argument as that of lemma  [ lem : cover ] can be used here , with the goal of making sure that , for sufficiently large @xmath3 , every voronoi region of each @xmath231 in the approximation satisfies equation  [ eq : lambdag ] .",
    "we may now finish the proof by using a similar argument to that of the codimension - one case .",
    "let @xmath332 .",
    "consider a discrete set @xmath201 of size @xmath3 that minimizes @xmath333 note once again that the distance and measure in equation  [ eqoq4 g ] match and therefore , since @xmath334 is continuous , we can apply theorem  [ thgruber ] ( with @xmath335 ) in this case .",
    "let @xmath219 .",
    "by lemma  [ lem : lambdag ] and lemma  [ lem : cover ] , adapted to this case , there is @xmath234 such that for all @xmath235 it is @xmath336^{d/(d+4 ) } } \\right\\}^{(d+4)/d } \\cdot k^{-4/d } \\end{split}\\ ] ] where the last line follows from the fact that @xmath212 has been chosen to minimize equation  [ eqoq4 g ] .",
    "finally , by hlder s inequality , it is @xmath337^{d/(d+4 ) } } \\right\\}^{(d+4)/d } & \\le           \\left\\ { \\ds {   \\int_{\\mathcal m}d\\mu_{_\\text{i}}(x){p}(x ) } \\right\\ } \\left\\ { \\ds {   \\int_{\\mathcal m}d\\mu_{_\\text{i}}(x )   \\left(\\frac 1 4   |\\text{ii}_x\\|^2 \\right)^{d/4 } } \\right\\}^{4/d}\\\\          & = \\| \\frac 1 4   |\\text{ii}|^2 \\|_{d/4 } \\end{split}\\ ] ] and thus @xmath338 where the total curvature @xmath339 is the geometric invariant of the manifold ( aside from the dimension ) that controls the constant in the bound .",
    "since @xmath182 and @xmath183 are arbitrary , lemma  [ kfaepp ] follows .",
    "we use the results discussed in section  [ sec : derivation ] to obtain the proof of theorem  [ thkf ] as follows .",
    "the proof of theorem  [ thkfg ] follows from the derivation in section  [ sec : derivation ] , as well as the argument below , with @xmath340 substituted by @xmath341 , and is omitted in the interest of brevity ."
  ],
  "abstract_text": [
    "<S> we study the problem of estimating a manifold from random samples . in particular , we consider piecewise constant and piecewise linear estimators induced by k - means and k - flats , and analyze their performance . </S>",
    "<S> we extend previous results for k - means in two separate directions . </S>",
    "<S> first , we provide new results for k - means reconstruction on manifolds and , secondly , we prove reconstruction bounds for higher - order approximation ( k - flats ) , for which no known results were previously available . while the results for k - means are novel , some of the technical tools are well - established in the literature . in the case of k - flats , both the results and the mathematical tools are new . </S>"
  ]
}