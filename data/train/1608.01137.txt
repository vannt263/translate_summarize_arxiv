{
  "article_text": [
    "the detection of a sparse set of facial landmarks in still images has been a widely - studied problem within the computer vision community .",
    "interestingly , many face analysis methods either systematically rely on video sequences ( e.g. , facial expression recognition @xcite ) or can benefit from them ( e.g. , face recognition @xcite ) .",
    "it is thus surprising that facial landmark tracking has received much less attention in comparison .",
    "our focus in this paper is on one of the most important problems in model - specific tracking , namely that of updating the tracker using previously tracked frames , also known as incremental ( face ) tracking .",
    "the standard approach to face tracking is to use a facial landmark detection algorithm initialised on the landmarks detected at the previous frame .",
    "this exploits the fact that the face shape varies smoothly in videos of sufficiently high framerates : if the previous landmarks were detected with acceptable accuracy , then the initial shape will be close enough for the algorithm to converge to a `` good '' local optimum for the current frame too . hence ,",
    "tracking algorithms are more likely to produce highly accurate fitting results than detection algorithms that are initialised by the face detector bounding box .    however , in this setting the tracker still employs a generic deformable model of the face built offline using a generic set of annotated facial images , which does not include the subject being tracked .",
    "it is well known that person - specific models are far more constrained and easier to fit than generic ones @xcite .",
    "hence one important problem in tracking is how to improve the generic model used to track the first few frames into an increasingly person - specific one as more frames are tracked .",
    "learned offline is updated with each new frame . ]",
    "this problem can be addressed with incremental learning , which allows for the smart adaptation of pre - trained generic appearance models .",
    "incremental learning is a common resource for generic tracking , being used in some of the state - of - the - art trackers @xcite , and incremental learning for face tracking is by no means a new concept , please see ross et al .",
    "@xcite for early work on the topic .",
    "more recently , incremental learning within cascaded regression , the state - of - the - art approach for facial landmark localisation , was proposed by xiong & de la torre @xcite and independently by asthana et al .",
    "@xcite . however , in both @xcite and @xcite the model update is far from being sufficiently efficient to allow real - time tracking , with @xcite mentioning that the model update requires 4.7 seconds per frame .",
    "note that the actual tracking procedure ( without the incremental update ) is faster than 25 frames per second , clearly illustrating that the incremental update is the bottleneck impeding real - time tracking .",
    "if the model update can not be carried out in real time , then incremental learning might not be the best option for face tracking - once the real - time constraint is broken in practice one would be better off creating person - specific models in a post - processing step @xcite ( e.g. , re - train the models once the whole video is tracked and then track again ) .",
    "that is to say , without the need and capacity for real - time processing , incremental learning is sub - optimal and of little use .    our main contribution in this paper is to propose the first incremental learning framework for cascaded regression which allows real - time updating of the tracking model . to do this ,",
    "we build upon the concept of continuous regression @xcite as opposed to standard sampling - based regression used in almost all prior work , including @xcite and @xcite .",
    "we note that while we tackle the facial landmark tracking problem , cascaded regression has also been applied to a wider range of problems such as pose estimation @xcite , model - free tracking @xcite or object localisation @xcite , thus making our methodology of wider interest .",
    "we will release code for training and testing our algorithm for research purposes .",
    "our main contributions are as follows :    * we propose a complete * new formulation for continuous regression * , of which the original continuous regression formulation @xcite is a special case .",
    "crucially , our method is now formulated by means of a * full covariance matrix capturing real statistics * of how faces vary between consecutive frames rather than on the shape model eigenvalues .",
    "this makes our method particularly suitable for the task of tracking , something the original formulation can not deal with .",
    "* we incorporate continuous regression in the cascaded regression framework ( coined cascaded continuous regression , or * ccr * ) and demonstrate its performance is equivalent to sampling - based cascaded regression .",
    "* we derive the * incremental learning for continuous regression * , and show that it * is an order of magnitude faster * than its standard incremental sdm counterpart . *",
    "we evaluate the incremental cascaded continuous regression ( * iccr * ) on the 300vw data set @xcite and show the importance of incremental learning in achieving state - of - the - art performance , especially for the case of very challenging tracking sequences .",
    "facial landmark tracking methods have often been adaptations of facial landmark detection methods .",
    "for example , active appearance models ( aam ) @xcite , constrained local models ( clm ) @xcite or the supervised descent method ( sdm ) @xcite were all presented as detection algorithms .",
    "it is thus natural to group facial landmark tracking algorithms in the same way as the detection algorithms , i.e. splitting them into discriminative and generative methods @xcite .    on the generative side ,",
    "aams have often been used for tracking .",
    "since the model fitting relies on gradient descent , it suffices to start the fitting from the last solution .",
    "tracking is particularly useful to aams since they are considered to have frequent local minima and a small basin of attraction , making it important that the initial shape is close to the ground truth .",
    "aams have further been regarded as very reliable for person specific tracking , but not for generic tracking ( i.e. , tracking faces unseen during training ) @xcite .",
    "recently @xcite showed however that an improved optimisation procedure and the use of in - the - wild images for training can lead to well - behaving person independent aam . eliminating the piecewise - affine representation and adopting a part - based model led to the gauss - newton deformable part model ( gn - dpm ) @xcite , which is the aam state of the art .    historically , discriminative methods relied on the training of local classifier - based models of appearance , with the local responses being then constrained by a shape model @xcite .",
    "these algorithms can be grouped into what is called the constrained local models ( clm ) framework @xcite . however , the appearance of discriminative regression - based models quickly transformed the state - of - the - art for face alignment .",
    "discriminative regressors were initially used within the clm framework substituting classifiers , showing improved performance @xcite . however , the most important contributions came with the adoption of cascaded regression @xcite and direct estimation of the full face shape rather than first obtaining local estimates @xcite .",
    "successive works have further shown the impressive efficiency @xcite and reliable performance @xcite of face alignment algorithms using cascaded regression .",
    "however , how to best exploit discriminative cascaded regression for tracking and , in particular , how to best integrate incremental learning , is still an open problem .",
    "in this section we revise the preliminary concepts over which we build our method . in particular , we describe the methods most closely related to ours , to wit the incremental supervised descent method @xcite and the continuous regressor @xcite , and motivate our work by highlighting their limitations .      a face image",
    "is represented by @xmath0 , and a face shape is a @xmath1 matrix describing the location of the @xmath2 landmarks considered .",
    "a shape is parametrised through a point distribution model ( pdm ) @xcite . in a pdm",
    ", a shape @xmath3 is parametrised in terms of @xmath4 \\in \\real^{m}$ ] , where @xmath5 represents the rigid parameters and @xmath6 represents the flexible shape parameters , so that @xmath7 , where @xmath8 is a procrustes transformation parametrised by @xmath9 .",
    "@xmath10 and @xmath11 are learned during training and represent the linear subspace of flexible shape variations .",
    "we will sometimes use an abuse of notation by referring treating shape @xmath3 also as function @xmath12 .",
    "we also define @xmath13 as the feature vector representing shape @xmath12 .",
    "an asterisk represents the ground truth , e.g. , @xmath14 is the ground truth shape for image @xmath15 .    given a test image @xmath0 , and a current shape prediction @xmath16 , the goal of linear regression for face alignment is to find a mapping matrix @xmath17 able to infer @xmath18 , the increment taking directly to the ground truth , from @xmath19 . by using @xmath20 training images , and @xmath21 perturbations per image ,",
    "the mapping matrix @xmath22 is typically learned by minimising the following expression w.r.t .",
    "@xmath22 :    @xmath23    where the bias term is implicitly included by appending a 1 to the feature vector .    in order to produce @xmath21 perturbed shapes @xmath24 per image , it suffices to draw the perturbations from an adequate distribution , ideally capturing the statistics of the perturbations encountered at test time .",
    "for example , during detection , the distribution should capture the statistics of the errors made by using the face detection bounding box to provide a shape estimation .",
    "the minimisation in eq .",
    "[ eq : standard_linearreg_problem ] has a closed - form solution .",
    "given @xmath20 images and @xmath21 perturbed shapes per training image , let @xmath25 and @xmath26 represent the matrices containing in its columns the input feature vectors and the target output @xmath27 respectively .",
    "then , the optimal regressor @xmath22 can be computed as :    @xmath28    given a test shape @xmath12 , the predicted shape is computed as @xmath29 .",
    "continuous regression ( cr ) @xcite is an alternative solution to the problem of linear regression for face alignment .",
    "the main idea of continuous regression is to treat @xmath18 as a continuous variable and to use _ all samples _ within some finite limits , instead of sampling a handful of perturbations per image .",
    "that is to say , the problem is formulated in terms of finite integrals as :    @xmath30    where @xmath31 is the eigenvalue associated to the @xmath32-th flexible parameter of the pdm , @xmath33 represent the number of flexible parameters , and @xmath34 is a parameter determining the number of standard deviations considered in the integral .",
    "unfortunately , this formulation does not have a closed - form solution .",
    "however , it is possible to solve it approximately in a very efficient manner by using a first order taylor expansion of the loss function . following the derivations in @xcite , we denote @xmath35 as the jacobian of the image features with respect to the shape parameters evaluated at the ground truth @xmath36 , which can be calculated simply as @xmath37 . a solution to eq .",
    "[ eq : old_continuous ] can then be written as :    @xmath38    where @xmath39 is a diagonal matrix whose @xmath32-th entries are defined as @xmath40 .",
    "cr formulated in this manner has the following practical limitations :    1 .",
    "it does not account for correlations within the perturbations .",
    "this corresponds to using a fixed ( not data - driven ) diagonal covariance to model the space of shape perturbations , which is a harmful oversimplification .",
    "because of 1 , it is not possible to incorporate cr within the popular cascaded regression framework in an effective manner .",
    "derivatives are computed over image pixels , so more robust features , e.g. , hog or sift , are not used .",
    "4 .   the cr can only account for the flexible parameters , as the integral limits are defined in terms of the eigenvalues of the pdm s pca space .",
    "in section [ ssec : continuous_regression ] we will solve all of these shortcomings , showing that it is possible to formulate the cascaded continuous regression and that , in fact , its performance is equivalent to the sdm .",
    "the main limitation of using a single linear regressor to predict the ground truth shape is that the training needs to account for too much intra - class variation .",
    "that is , it is hard for a single regressor to be simultaneously accurate and robust . to solve this , @xcite successfully adapted the cascaded regression of framework of dollr et al .",
    "@xcite to face alignment .",
    "however , the most widely - used form of face alignment is the sdm @xcite , which is a cascaded linear regression algorithm .    at * test time * , the sdm takes an input @xmath41 , and then for a fixed number of iterations computes @xmath42 and @xmath43 .",
    "the key idea is to use a different regressor @xmath44 for each iteration .",
    "the input to the * training * algorithm is a set of images @xmath45 and corresponding perturbed shapes @xmath46 . the training set @xmath32 is defined as @xmath47 , with @xmath48 , and @xmath49 , with @xmath50",
    ". then regressor @xmath32 is computed using eq .",
    "[ eq : linear_reg ] on training set @xmath32 , and a new training set @xmath51 is created using the shape parameters @xmath52 .",
    "incremental versions of the sdm have been proposed by both xiong & de la torre @xcite and asthana et al . @xcite .",
    "the latter proposed the _",
    "parallel sdm _ , a modification of the original sdm which facilitates the incremental update of the regressors .",
    "more specifically , they proposed to alter the sdm training procedure by modelling @xmath53 as a normal distribution @xmath54 , allowing training shape parameters to be sampled for the next level of the cascade as :    @xmath55    once the parallel sdm is defined , its incremental extension is immediately found . without loss of generality , we assume that the regressors are updated in an on - line manner , i.e.",
    ", the information added is extracted from the fitting of the last frame .",
    "we thus define @xmath56 , arrange the matrices @xmath57 and @xmath58 accordingly , and define the shorthand @xmath59 , leading to the following update rules @xcite :    @xmath60    where @xmath61 is the @xmath21-dimensional identity matrix .",
    "the cost for these incremental updates is dominated by the multiplication @xmath62 , where both matrices have dimensionality @xmath63 , which has a computational complexity of @xmath64 .",
    "since @xmath65 is high - dimensional ( @xmath66 ) , the cost of updating the models becomes prohibitive for real - time performance .",
    "once real time is abandoned , offline techniques that do not analyse every frame in a sequential manner can be used for fitting , e.g. , @xcite .",
    "we provide a full analysis of the computational complexity in section  [ sec : computational_complexity ] .",
    "in this section we describe the proposed incremental cascaded continuous regression , which to the best of our knowledge is the first cascaded regression tracker with * real - time incremental learning * capabilities .",
    "to do so , we first extend the continuous regression framework into a fully fledged cascaded regression algorithm capable of performance on par with the sdm ( see sections  [ ssec : continuous_regression ] and [ ssec : cascaded_continuous_regression ] ) .",
    "then , we derive the incremental learning update rules within our cascaded continuous regression formulation ( see section  [ ssec : incremental_learning_update_rules ] )",
    ". we will show in section  [ sec : computational_complexity ] that our newly - derived formulas have complexity of one order of magnitude less than previous incremental update formulations .",
    "we first modify the original formulation of continuous regression .",
    "in particular , we add a `` data term '' , which is tasked with encoding the probability of a certain perturbed shape , allowing for the modelling of correlations in the shape dimensions . plainly speaking",
    ", the previous formulation assumed an i.i.d .",
    "uniform sampling distribution .",
    "we instead propose using a data - driven full covariance distribution , resulting in regressors that model the test - time scenario much better .",
    "in particular , we can see the loss function to be optimised as :    @xmath67    ]    it is interesting to note that this equation appears in @xcite , where the sdm equations are interpreted as a mcmc sampling - based approximation of this equation .",
    "contrariwise , the continuous regression proposes to use a different approximation based on a first - order taylor approximation of the _ ideal loss function _ defined in eq .",
    "[ eq : train_problem_def ] .",
    "however , the continuous regression proposed in @xcite extends the functional data analysis @xcite framework to the imaging domain , without considering any possible data correlation .",
    "instead , the `` data term '' in eq .",
    "[ eq : train_problem_def ] ( which defines how the data is sampled in the mcmc approach ) , will serve to correlate the different dimensions in the continuous regression .",
    "that is to say , the  data term \" does not play the role of how samples are taken , but rather helps to find an analytical solution in which dimensions can be correlated .",
    "these differences are illustrated in figure  [ fig : differences ] .",
    "the first - order approximation of the feature vector is given by :    @xmath68    where @xmath35 is the jacobian of the feature representation of image @xmath45 at @xmath36 .",
    "while @xcite used a pixel - based representation , the jacobian under an arbitrary representation can be computed empirically as :    @xmath69 ) - f ( \\i , [ \\s_x - \\delta x , \\s_y ] ) } { 2 \\delta x}\\ ] ]    where @xmath70 are the @xmath71 coordinates of shape @xmath3 , and @xmath72 indicates that @xmath73 is added to each element of @xmath70 ( in practice , @xmath73 is the smallest possible , 1 pixel ) .",
    "@xmath74 can be computed similarly . then @xmath75 \\frac{\\partial\\s } { \\partial \\p_j^*}$ ] .",
    "[ eq : train_problem_def ] has a closed form solution as :    @xmath76    where @xmath77 and @xmath78 are the mean and covariance of the data term , @xmath79 .",
    "finally , we can see that eq .",
    "[ eq : closed_form_cont ] can be expressed in a more compact form .",
    "let us first define the following shorthand notation : @xmath80 $ ] , @xmath81 , @xmath82 $ ] and @xmath83 $ ] . then :    @xmath84    where @xmath85 . through this arrangement , the parallels with the sampling - based regression formula are clear ( see eq .",
    "[ eq : linear_reg ] ) .",
    "it is interesting that , while the standard linear regression formulation needs to sample perturbed shapes from a distribution , the continuous regression training formulation only needs to extract the features and the jacobians on the ground - truth locations .",
    "this means that once these features are obtained , re - training a new model under a different distribution takes seconds , as it only requires the computation of eq .",
    "[ eq : compact_form_cont ] .",
    "now that we have introduced a new formulation with the continuous regression capable of incorporating a data term , it is straightforward to extend the cr into the cascade regression formulation : we take the distribution in equation  [ eq : parsdm_sampling ] as the _ data term _ in eq .",
    "[ eq : train_problem_def ] .",
    "one might argue that due to the first - order taylor approximation required to solve equation  [ eq : train_problem_def ] , ccr might not work as well as the sdm .",
    "one of the main experimental contributions of this paper is to show that in reality this is not the case : in fact ccr and sdm have equivalent performance ( see section  [ sec : experimental_results ] ) .",
    "this is important because , contrary to previous works on cascaded regression , incremental learning within ccr allows for real time performance .",
    "once frame @xmath15 is tracked , the incremental learning step updates the existing training set @xmath86 with @xmath87 , where @xmath88 denotes the predicted shape parameters for frame @xmath15 .",
    "note that in this case @xmath89 consists of only one example compared to @xmath21 examples in the incremental sdm case .",
    "the update process consists of computing matrix @xmath90 , which stores the feature vector and its jacobian at @xmath88 and then , using the shorthand notation @xmath91 , updating continuous regressor as :    @xmath92    in order to avoid the expensive re - computation of @xmath93 , it suffices to update its value using the woodbury identity @xcite :    @xmath94    note that @xmath95 , where @xmath96 accounts for the number of shape parameters .",
    "we can see that computing eq .",
    "[ eq : closed_form_icont ] requires computing first @xmath97 , which is @xmath98 .",
    "this is a central result of this paper , and reflects a property previously unknown .",
    "we will examine in section  [ sec : computational_complexity ] its practical implications in terms of real - time capabilities .",
    "in this section we first detail the computational complexity of the proposed iccr , and show that it is real - time capable . then",
    ", we compare its cost with that of incremental sdm , showing that our update rules are an order of magnitude faster .",
    "* iccr update complexity : * let us note the computational cost of the feature extraction as @xmath99 .",
    "the update only requires the computation of the feature vector at the ground truth , and in two adjacent locations to compute the jacobian , thus resulting in @xmath100 complexity .",
    "interestingly , this is independent from the number of cascade levels .",
    "then , the update equation ( eq .  [ eq : closed_form_icont ] ) , has a complexity dominated by the operation @xmath101 , which has a cost of @xmath102 .",
    "it is interesting to note that @xmath103 is a matrix of size @xmath104 and thus its inversion is extremely efficient .",
    "the detailed cost of the incremental update is :    @xmath105    * incremental sdm update complexity : * incremental learning for sdm requires sampling at each level of the cascade .",
    "the cost per cascade level is @xmath106 , where @xmath21 denotes the number of samples .",
    "thus , for @xmath107 cascade levels the total cost of sampling is @xmath108 .",
    "the cost of the incremental update equations ( eqs .",
    "( [ eq : incremental_discrete_eq1]-[eq : incremental_discrete_eq4 ] ) ) , is in this case dominated by the multiplication @xmath109 , which is @xmath64 .",
    "the detailed computational cost is :    @xmath110    * detailed comparison and timing : * one advantage of iccr comes from the much lower number of feature computations , being as low as 3 vs. the @xmath111 computations required for incremental sdm .",
    "however , the main difference is the @xmath112 complexity of the regressor update equation for the incremental sdm compared to @xmath113 for the iccr . in our case , @xmath114 , while @xmath115 . the feature dimensionality results from performing pca over the feature space , which is a standard procedure for sdm .",
    "note that if we avoided the use of pca , the complexity comparison would be even more in our favour .",
    "a detailed summary of the operations required by both algorithms , together with their computational complexity and the execution time on our computer are given in algorithm  [ algo_iccr ] .",
    "note that @xmath116 is the cost of projecting the output vector into the pca space .",
    "note as well that for incremental sdm , the `` sampling and feature extraction '' step is repeated @xmath107 times .",
    "_ iccr update ( total : 72 ms . ) : _",
    "+    ' '' ''    _ isdm @xcite update ( total : 705 ms . ) : _ +",
    "this section describes the experimental results .",
    "first , we empirically demonstrate the performance of ccr is equivalent to sdm . in order to do so ,",
    "we assess both methods under the same settings , avoiding artefacts to appear , such as face detection accuracy .",
    "we follow the vot challenge protocol @xcite . then , we develop a fully automated system , and we evaluate both the ccr and iccr in the same settings as the 300vw , and show that our fully automated system achieves state of the art results , illustrating the benefit of incremental learning to achieve it .",
    "* training data : * we use data from different datasets of static images to construct our training set . specifically , we use helen @xcite , lfpw @xcite , afw @xcite , ibug @xcite , and a subset of multipie @xcite .",
    "the training set comprises @xmath1177000 images .",
    "we have used the facial landmark annotations provided by the 300 faces in the wild challenge @xcite , as they offer consistency across datasets .",
    "the _ statistics _ are computed across the training sequences , by computing the differences of ground - truth shape parameters between consecutive frames .",
    "given the easiness of the training set with respect to the test set , we also included differences of several frames ahead . this way , higher displacements are also captured .",
    "* features : * we use the sift @xcite implementation provided by xiong & de la torre @xcite .",
    "we apply pca on the output , retaining 2000 dimensions .",
    "we apply the same pca to all of the methods , computed during our sdm training .",
    "* test data : * all the methods are evaluated on the test partition of the 300 videos in the wild challenge ( 300vw ) @xcite .",
    "the 300vw is the only publicly - available large - scale dataset for facial landmark tracking .",
    "its test partition has been divided into categories 1 , 2 and 3 , intended to represent increasingly unconstrained scenarios .",
    "in particular , category 3 contains videos captured in totally unconstrained scenarios .",
    "the ground truth has been created in a semi - supervised manner using two different methods @xcite .",
    "* error measure : * to compute the error for a specific frame , we use the error measure defined in the 300vw challenge @xcite .",
    "the error is computed by dividing the average point - to - point euclidean error by the inter - ocular distance , understood as the distance between the two outer eye corners .      in order to demonstrate the performance capability of our ccr method against sdm",
    ", we followed the protocol established by the visual object tracking ( vot ) challenge organisers for evaluating the submitted tracking methods @xcite . specifically ,",
    "if the tracker error exceeds a certain threshold ( 0.1 in our case , which is a common definition of alignment failure ) , we proceed by re - initialising the tracker . in this case",
    ", the starting point will be the ground truth of the previous frame .",
    "this protocol is adopted to avoid the pernicious influence on our comparison of some early large failure from which the tracker is not able to recover , which would mean that successive frames would yield a very large error .",
    "results are shown in fig .",
    "[ fig : ccrvssdm ] ( * left * ) .",
    "we show that the ccr and the sdm provide similar performance , thus ensuring that the ccr is a good starting point for developing an incremental learning algorithm .",
    "it is possible to see from the results shown in fig .",
    "[ fig : ccrvssdm ] that the ccr compares better and even sometimes surpasses the sdm on the lower levels of the error , while the sdm systematically provides a gain for larger errors with respect to the ccr .",
    "this is likely due to the use of first - order taylor approximation , which means that larger displacements are less accurately approximated .",
    "instead , the use of _ infinite _ shape perturbations rather than a handful of sampled perturbations compensates this problem for smaller errors , and even sometimes provides some performance improvement .",
    "we now show the benefit of incremental learning with respect to generic models .",
    "the incremental learning needs to filter frames to decide whether a fitting is suitable or harmful to update the models .",
    "that is , in practice , it is beneficial to filter out badly - tracked frames by avoiding performing incremental updates in these cases .",
    "we follow @xcite and use a linear svm trained to decide whether a particular fitting is `` correct '' , understood as being under a threshold error . despite its simplicity",
    ", this tactic provides a solid performance increase .",
    "results on the test set are shown in fig .",
    "[ fig : ccrvssdm ] ( * right * ) .    , title=\"fig : \" ] , title=\"fig : \" ]      we developed a fully automated system to compare against state of the art methods .",
    "our fully automated system is initialised with a standard sdm @xcite , and an svm is used to detect whether the tracker gets lost .",
    "we assessed both our ccr and iccr in the most challenging category of the 300vw , consisting of 14 videos recorded in unconstrained settings . for a fair comparison",
    ", we have reproduced the challenge settings ( a brief description of the challenge and submitted methods can be found in @xcite ) .",
    "we compare our method against the top two participants @xcite .",
    "results are shown in fig .",
    "[ fig : soacomp ] .",
    "the influence of the incremental learning to achieve state of the art results is clear .",
    "importantly , as shown in the paper , our iccr allows for real - time implementation .",
    "that is to say , our iccr reports state of the art results whilst working in near real - time , something that could not be achieved by previous works on cascaded regression .",
    "code for our fully automated system is available for download at www.cs.nott.ac.uk/~psxes1 .     ]",
    "in this article we have proposed a novel facial landmark tracking algorithm that is capable of performing on - line updates of the models through incremental learning . compared to previous incremental learning methodologies",
    ", it can produce much faster incremental updates without compromising on accuracy .",
    "this was achieved by firstly extending the continuous regression framework @xcite , and then incorporating it into the cascaded regression framework to lead to the ccr method , which we showed provides equivalent performance to the sdm .",
    "we then derived the incremental learning update formulas for the ccr , resulting in the iccr algorithm .",
    "we further show the computational complexity of the incremental sdm , demonstrating that iccr is an order of magnitude simpler computationally .",
    "this removes the bottleneck impeding real - time incremental cascaded regression methods , and thus results in the state of the art for real - time face tracking .",
    "the work of snchez - lozano , martinez and valstar was supported by the european union horizon 2020 research and innovation programme under grant agreement no 645378 , aria - valuspa .",
    "the work of snchez - lozano was also supported by the vice - chancellor s scholarship for research excellence provided by the university of nottingham .",
    "the work of tzimiropoulos was supported in part by the epsrc project ep / m02153x/1 facial deformable models of animals .",
    "we are also grateful for the given access to the university of nottingham high performance computing facility , and we would like to thank jie shen and grigoris chrysos for their insightful help in our tracking evaluation .",
    "as shown in the paper , linear regression aims at minimising the average expected training error , respect to @xmath22 .",
    "the average expected error is formulated as follows :    @xmath118    where @xmath119 represents the ground truth shape parameters , and @xmath18 the parameters displacement . due to the intractability of the integral ,",
    "this is typically solved by a mcmc ( sampling - based ) approximation , in which samples are taken from the distribution @xmath79 .",
    "the continuous regression framework avoids the need to sample by performing the first order taylor approximation of the function @xmath120 , defined as :    @xmath121    where @xmath122 , evaluated at @xmath123 , is the jacobian of the feature representation of image @xmath45 , respect to shape parameters @xmath124 , at @xmath36 . combining this approximation with the integral in eq .",
    "[ eq : train_problem_def_appx ] leads to :    @xmath125 d \\delta \\p \\hspace{-5pt } \\label{eq : long_deriv_ccr_appx}\\end{aligned}\\ ] ]    where recall @xmath126 is the shorthand of @xmath127 .",
    "if we group independent , linear , and quadratic terms , respect to @xmath18,we can express eq .",
    "[ eq : long_deriv_ccr_appx ] as : @xmath128 d \\delta \\p ,    \\label{eq : long_deriv_ccr2_appx}\\end{aligned}\\ ] ] where @xmath129 and @xmath130 .",
    "let us assume that @xmath79 is parametrised by its mean @xmath77 and covariance @xmath78 .",
    "then , it follows that : @xmath131 which means that the expected error , for the @xmath15-th training example , has a closed - form solution as follows : @xmath132 now , @xmath22 is obtained after minimising eq .",
    "[ eq : closed_form_appx ] , whose derivatives are obtained as follows :",
    "@xmath133    this leads to the solution presented in the paper : @xmath134",
    "we can see that our new formulation generalises that of @xcite . more specifically , if we solve eq .",
    "[ eq : train_problem_def ] for the non - rigid parameters only , we can define @xmath79 to be a uniform distribution defined within the limits @xmath135 , with @xmath31 the eigenvalue assciated to the @xmath32-th basis of the pdm , and @xmath34 the number of standard deviations considered for that eigenvalue . in such case , eq .",
    "[ eq : train_problem_def ] would be defined , for @xmath136 non - rigid parameters , as : @xmath137 which is the problem definition that appeared in @xcite .",
    "moreover , we can see that such a uniform distribution would be parametrised by a zero - mean vector and a diagonal covariance matrix whose entries are @xmath40 .",
    "in such case , eq .  [ eq : closed_form_cont ] would be reduced to the solution presented in @xcite .",
    "that is to say , the continuous regression presented in @xcite assumed a uniform distribution , without connection to tracking statistics , and no correlation between target dimensions were possible .",
    "instead , our formulation accepts a  data term \" , which correlates the target dimensions , and allows for its solution for rigid parameters as well .",
    "data term \" is crucial to the performance of the ccr ."
  ],
  "abstract_text": [
    "<S> this paper introduces a novel real - time algorithm for facial landmark tracking . </S>",
    "<S> compared to detection , tracking has both additional challenges and opportunities . </S>",
    "<S> arguably the most important aspect in this domain is updating a tracker s models as tracking progresses , also known as incremental ( face ) tracking . </S>",
    "<S> while this should result in more accurate localisation , how to do this online and in real time without causing a tracker to drift is still an important open research question . </S>",
    "<S> we address this question in the cascaded regression framework , the state - of - the - art approach for facial landmark localisation . because incremental learning for cascaded regression is costly </S>",
    "<S> , we propose a much more efficient yet equally accurate alternative using continuous regression . </S>",
    "<S> more specifically , we first propose cascaded continuous regression ( ccr ) and show its accuracy is equivalent to the supervised descent method . </S>",
    "<S> we then derive the incremental learning updates for ccr ( iccr ) and show that it is an order of magnitude faster than standard incremental learning for cascaded regression , bringing the time required for the update from seconds down to a fraction of a second , thus enabling real - time tracking . finally , we evaluate iccr and show the importance of incremental learning in achieving state - of - the - art performance . </S>",
    "<S> code for our iccr is available from http://www.cs.nott.ac.uk/~psxes1    16subnumber1763 </S>"
  ]
}