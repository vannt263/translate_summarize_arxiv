{
  "article_text": [
    "actual problems of science , engineering and society can be so complex , that their mathematical portrait requires more than three dimensions .",
    "quantum world gives us a perfect example of essentially high  dimensional systems , described by a joint wavefunction ( or density matrix ) of all particles .",
    "a simple system of @xmath0 spin-@xmath1 particles is an entanglement of @xmath2 possible states , and should be described by the same amount of numbers , which creates out - of - memory errors on a typical workstation for @xmath3 even with a brute force of modern supercomputers , standard numerical methods can not honestly simulate protein - size molecules ( @xmath4  @xmath5 ) , since the complexity and storage explode exponentially with @xmath6    to overcome this problem , known as the  _ curse of dimensionality _ , we use data - sparse representations for high - dimensional vectors and matrices , and develop special algorithms to work with them . proposed in 1992 , the _ density matrix renormalization group _ ( dmrg ) algorithm  @xcite and the _ matrix product states _ ( mps ) formalism  @xcite suggest to represent a wavefunction @xmath7 in the following tensor - product form @xmath8 in numerical linear algebra this format was re - discovered as the  _ tensor train _ ( tt ) decomposition  @xcite . a single tt core ( or _ site _ ) @xmath9 $ ] is described by @xmath10 numbers , where @xmath11 denotes the number of possible states for the @xmath12th particle ( the _ mode size _ ) , and @xmath13 is the tt rank ( or _ bond dimension _ ) .",
    "the total number of representation parameters scales as @xmath14 @xmath15 @xmath16 and is feasible for computations with @xmath17    to    the dmrg algorithm was originally proposed to find the _ ground state _ , i.e. the minimal eigenpair of a hermitian matrix @xmath18 this problem is equivalent to the minimization of the rayleigh quotient @xmath19 substituting @xmath20 with @xmath21 and applying the same algorithm , we can solve linear systems @xmath22 with hermitian positive definite matrix  @xcite .",
    "this framework can be extended to a broad class of problems .",
    "since @xmath7 is a huge high - dimensional vector , the solution is sought in the structured format   with some tt ranks @xmath23 defined _ a priori _ or chosen adaptively .",
    "the simultaneous optimization over all sites is a highly nonlinear and difficult problem .",
    "as it is usual in high - dimensional optimization , we substitute it by a sequence of partial optimizations , each over a particular ( small ) group of variables . for our problem , it is natural to group the variables according to the tensor format  , e.g. optimize over the components of a single site @xmath24 at a time .",
    "the tt format is linear in each site , i.e. @xmath25 where @xmath26 is the @xmath27 _ frame matrix _ , which linearly maps the elements of @xmath24 to the full vector @xmath28 this turns every partial optimization into a local problem of the same type , as the original one , @xmath29 where @xmath30 is the @xmath31 _ reduced matrix _ , which inherits the properties of @xmath32 i.e. is hermitian . since the frame matrix @xmath26 has a structured tt representation ( which is the same as   with @xmath24 substituted by the identity matrix ) , the reduced matrix @xmath33 can be assembled avoiding the exponential costs .",
    "finally , introducing simple orthogonality conditions for all sites but @xmath34 we can make the whole matrix @xmath26 orthogonal  @xcite . as a consequence , @xmath33 becomes better conditioned than @xmath32 and the reduced functional writes @xmath35 for the ground state problem , and @xmath36 for the linear system .",
    "each optimization   is now a classical problem of a tractable size , that can be solved by classical algorithms of numerical linear algebra .",
    "each local step   finds @xmath37 where the subspace @xmath38 is of dimension @xmath10 , see fig [ fig:3d ] ( left ) . here and later by @xmath39",
    "we denote the subspace of columns of a matrix @xmath40 if tt ranks are fixed , the local convergence of such scheme can be analysed using standard methods of multivariate analysis  @xcite .",
    "however , in numerical practice the tensor ranks of the solution are not known in advance , and fixed - rank optimization with wrong ranks would not be efficient .",
    "the dmrg scheme with variable tt ranks is more advantageous , but the theoretical analysis is even more difficult .",
    "when we allow tt ranks to grow , the dimensions of subspaces @xmath41 grow as well , and we can use different strategies to expand the subspaces . originally , the one - site dmrg scheme ( dmrg1 ) increased the rank @xmath13 by adding ( random ) orthogonal vectors to @xmath41 , but this algorithm often got stuck far from the ground state .",
    "the problem was solved using _ two sites _ instead of one in the optimization step  @xcite .",
    "the _ two - site _ dmrg algorithm ( dmrg2 ) merges blocks @xmath24 and @xmath42 and solves the local optimization problem in @xmath43 see fig .",
    "[ fig:3d ] ( right ) . here",
    "@xmath44 is the @xmath45 matrix , which has the same tt representation as   with blocks @xmath24 and @xmath46 replaced by the identities .",
    "the dmrg2 converges remarkably well ( and is in fact a method of choice ) for 1d systems with short  range interactions , but the cost is approximately @xmath47 times larger than in the dmrg1 . for systems with long - range interactions two neighboring sites do not provide sufficient information , and dmrg2 can stagnate as well .",
    "to simulate such systems faster and more accurately , better methods to choose search subspaces are required .",
    "the _ gradient direction _ is central in the theory of optimization methods , and many algorithms use the gradient or its approximate surrogates . in 2005 , s. white proposed the _ corrected one - site _ dmrg algorithm ( dmrg1c ) , which adds auxiliary direction to improve the convergence and reduce the computational cost , see @xcite and ( * ? ? ?",
    "* sec 6.3 ) for more details . in this paper",
    "we compare the dmrg1c with the _ alternating minimum energy _ ( amen )",
    "the amen algorithm was recently proposed in  @xcite for the solution of linear equations , and the version for the ground state problem appears immediately when we choose @xmath20 as a target function . in the next section",
    "we compare the ideas and implementation aspects of both methods and explain the motivation behind amen from numerical linear algebra perspective . in sec .",
    "[ sec : num ] we reproduce a numerical experiment of s. white from  @xcite , and demonstrate that amen can solve it better than dmrg1c .",
    "both dmrg1c and amen combine the local optimization   with the step that injects the auxiliary information .",
    "both algorithms are _ local _ , i.e. modify only one block @xmath24 at a time ( cf . the non - local `` als@xmath48 '' algorithm in @xcite ) .",
    "both methods sequentially cycle over tt blocks ( @xmath49 ) . in the following we assume that was just solved for @xmath34 and",
    "consider the step that corrects @xmath24 before the optimization passes to the next block @xmath50 this step does not change the vector @xmath51 ( for amen ) , or perturbs it slightly ( for dmrg1c ) , and therefore has a minor direct effect on @xmath52 however , it inserts additional directions to @xmath53 that improves the convergence of @xmath20 to its global minimum .",
    "it is crucial how exactly the block @xmath24 is modified , and which vectors end up in @xmath54 after that . in the following we discuss these details , which constitute the main difference between the dmrg1c and the amen .",
    "following the _ power iteration _ method , the dmrg1c algorithm of s. white targets in addition to the solution @xmath7 the first krylov vector @xmath57 the amen algorithm uses the gradient direction @xmath58 in exact arithmetics this makes no difference , since @xmath59 in practical computations both @xmath60 and @xmath61 are perturbed by inevitable machine rounding errors , perturbations associated with the use of tensor format , and additional errors which appear when a surrogate formula ( like  ( * ? ? ?",
    "* eq .  @xmath62 ) ) is used to speed up the computations .",
    "the dmrg1c algorithm is derived from perturbation arguments , valid in the vicinity of the minimum of @xmath52 when @xmath7 approaches the ground state , the angle between @xmath7 and @xmath63 vanishes , and any perturbation in @xmath64 yields a random new direction .",
    "this creates a certain gap between the theory supporting the dmrg1c , and the practice .    following the _",
    "steepest descent _",
    "algorithm , the amen uses orthogonal vectors @xmath61 and @xmath65 and @xmath66 is much more stable to perturbations of @xmath67 ( in general , the krylov vectors @xmath68 form an extremely unstable basis , and orthogonalization is crucial . ) the steepest descent algorithm with @xmath61 substituted by @xmath69 converges as long as @xmath70 for the linear systems this fact is elegantly proven in  @xcite , and the convergence rate of perturbed method is estimated .",
    "an eigenvalue counterpart follows similarly , and the rate of convergence in @xmath71 can be estimated from the spectral range of @xmath18 this makes the approach implemented in the amen algorithm preferable both theoretically and in practice .",
    "the computation of full vectors @xmath63 and @xmath73 is not possible due to their exponentially large size . since @xmath7 and @xmath74 are both in tt format , we can avoid the curse of dimension and represent @xmath63 and @xmath73 by the tt format",
    ". however , the tt ranks of @xmath64 can be as large as product of tt ranks of @xmath74 and @xmath65 which makes the calculations difficult .    to reduce these costs ,",
    "s. white suggests in the * dmrg1c * the following scheme .",
    "the tt format   is divided in two parts : left blocks ( number @xmath75 ) are referred to as _ system _ , and right blocks ( @xmath76 ) as  _ environment_. the tt format for the matrix @xmath74 is written accordingly , @xmath77 or shortly @xmath78 similarly , eq .   reduces to @xmath79 the targeting of @xmath63 is substituted by targeting of all @xmath80    although in general @xmath81 it can be argued that the set @xmath82 contains a sufficient subspace information . to show this , we write @xmath83 and consider vectors @xmath60 and @xmath84 as system - by - environment matrices @xmath85 and @xmath86 of size @xmath87 now @xmath88 where @xmath89 contains the coefficients of the required linear combination  in the exact arithmetics the _",
    "system_-related components of @xmath60 belong to @xmath90 each @xmath84 is easier to compute than @xmath91 because it does not depend on the environment part @xmath92    the total dimension of @xmath93 grows in each step , and to keep tt ranks and storage moderate , we have to truncate it .",
    "the approximation step in the dmrg1c replaces @xmath93 with a subspace @xmath94 of a smaller dimension , using a classical _ singular value decomposition _ ( svd ) , or schmidt decomposition technique .",
    "the _ dominant subspace _",
    "@xmath72 is spanned by the first singular vectors of the matrix @xmath95 where all target vectors are concatenated with empirically chosen weighting coefficients @xmath96 .",
    "the method assumes that the vector @xmath63 is likely to belong to @xmath97    this assumption makes perfect sense if @xmath60 is a random sample from @xmath93  for a random @xmath98 , @xmath99 is more likely to end up in the dominant subspace of @xmath40 however , the target vector @xmath63 does not belong to @xmath72 in general , for any choice of weights @xmath100 the reason is that @xmath60 depends crucially on @xmath101 whereas this information is dropped for the sake of faster computations in @xmath84 and hence @xmath93 and @xmath72 . selecting @xmath89 in  , we may come across any vector in @xmath93 , even the smallest singular vector .",
    "that is , for each choice of @xmath102 and @xmath7 there is a _ counterexample _ of a hamiltonian , for which the slightest truncation of @xmath93 loses the system - related part of the target vector @xmath57    the * amen * approximates @xmath56 into its own tt format using any compression tool .",
    "either the svd  based technique , which computes the approximation @xmath103 up to any prescribed tolerance @xmath104 , or a faster ( but heuristic ) _ alternating least squares _ ( als ) method may be used . in any case",
    ", we may generate an approximation @xmath69 with a desired accuracy , which guarantees the convergence of the steepest descent method with the imperfect direction @xmath105 this fact provides the theoretical bounds for the global convergence rate of the whole amen scheme , similarly to  @xcite .",
    "the last but not the least detail is how exactly the information about the auxiliary direction is injected in the algorithm . to show this in isolation from the other dissimilarities outlined above , we assume that in both methods we target in addition to @xmath7 _ only one _ vector @xmath106 to simplify the presentation we also consider the @xmath107 case , and write @xmath108 and @xmath109 where `` @xmath110 '' and `` @xmath111 '' denote the first and the second blocks , respectively .",
    "the dmrg1c  _ averages _ the subspaces @xmath112 and @xmath113 by computing the dominant subspace @xmath114 of the gram matrix as follows , @xmath115 where @xmath116 as shown in previous subsection , this procedure does not guarantee that @xmath7 or @xmath117 ends up in @xmath118 unless @xmath119 the tt core @xmath120 is replaced by the vectors of @xmath121 that introduces a @xmath122 perturbation to @xmath7 and probably worsen @xmath52 it is clear though that @xmath123 should vanish when we approach the exact solution , but the general recipe is not known .",
    "the amen avoids the outlined difficulties by  _ merging _ @xmath124 and zero - padding the second block .",
    "values of @xmath7 and @xmath20 are preserved , no rescaling is required , and both @xmath125 the downside is that we choose @xmath126 each time we expand the subspaces . however , when we use the approximate gradient direction @xmath127 the low - rank @xmath69 usually suffice , e.g. with @xmath128 we can also truncate the tt - ranks at the end of each iteration and control the perturbation to @xmath52",
    "following s. white  @xcite , we consider the spin-1 periodic heisenberg chain , @xmath129 where @xmath130 , are the @xmath131 pauli matrices for spin-@xmath132 particles .",
    "the number of spins @xmath0 is set to @xmath133 , i.e. the wavefunction belongs to the @xmath134-dimensional hilbert space .",
    "this example is particularly illustrating , since the mismatch between the linear tt model   and the cycle structure of   complicates the problem  the solution has large tt ranks , and both the one and two  site dmrg converge slowly .",
    "the way how the tt ranks are chosen during the algorithm is also very important .",
    "we first adopt the rank selection strategy from @xcite , and compare the dmrg2 , the dmrg1c and the amen algorithms .",
    "the results are shown in fig .",
    "[ fig : heisen ] ( top left ) , which overlays  ( * ? ? ?",
    "3 ) with the amen behavior . in fig .",
    "[ fig : heisen ] ( top right ) the convergence of @xmath135 to the reference value @xmath136 ( computed in @xcite by the dmrg1c with tt ranks @xmath137 ) is given w.r.t . the cumulative cpu time .",
    "we see that both dmrg methods correctly reproduce the experiment from @xcite : the two - site dmrg stagnates at a high error level , while the corrected dmrg converges significantly faster .",
    "the amen method manifests practically the same efficiency .",
    "since it searches in a larger subspace , it is even more accurate w.r.t .",
    "iterations , but becomes slightly slower during the optimization of inner tt blocks .",
    "however , letting it to increase the ranks ( each fourth iteration ) yields sharper error decays .    to free the algorithm from tuning parameter , we prefer to choose the ranks adaptively to the desired accuracy . with this",
    "we also avoid artificial rank limitation , which pollutes the convergence .",
    "therefore , in the second experiment we use the same algorithms but perform the truncation of tt blocks via the svd using the relative frobenius - norm accuracies @xmath138 and @xmath139 .",
    "the results are shown in fig .",
    "[ fig : heisen ] ( bottom ) .",
    "we see that when ranks are chosen adaptively , the amen rapidly becomes faster than the other algorithms .",
    "even the dmrg1c stagnates relatively early , since the correction @xmath140 contaminates the dominant basis of the ground state .",
    "moreover , since the @xmath141-truncation eliminates the correction if @xmath142 , it is worthless to decrease the scale @xmath123 ( cf .",
    "fig [ fig : heisen ] , top left ) . both the adaptivity and speed speak in favour of such truncation :",
    "the same accuracy levels are achieved several times faster than in the fixed - rank experiment ( e.g. 10 vs. 100 sec . for @xmath143 and @xmath138 ) .",
    "larger time spent by amen in the latter iterations is compensated by a significantly better accuracy , which is close to the optimal level @xmath144 .",
    "height 2pt depth -1.6pt width 23pt , _ alternating minimal energy methods for linear systems in higher dimensions .",
    "part ii : faster algorithm and application to nonsymmetric systems _",
    ", arxiv preprint 1304.1222 , 2013 ."
  ],
  "abstract_text": [
    "<S> given in the title are two algorithms to compute the extreme eigenstate of a high - dimensional hermitian matrix using the tensor train ( tt ) / matrix product states ( mps ) representation . </S>",
    "<S> both methods empower the traditional alternating direction scheme with the auxiliary ( e.g. gradient ) information , which substantially improves the convergence in many difficult cases . </S>",
    "<S> being conceptually close , these methods have different derivation , implementation , theoretical and practical properties . </S>",
    "<S> we emphasize the differences , and reproduce the numerical example to compare the performance of two algorithms .    _ </S>",
    "<S> keywords : _ high  dimensional problems , dmrg , mps , tensor train format , extreme eigenstate .    </S>",
    "<S> _ msc : _ </S>",
    "<S> 15a18 , 15a69 , 65f10 , 65f15 , 82b28 , 82b20    _ pacs : _ </S>",
    "<S> 02.10.xm , 02.60.dc , 75.10.pq , 05.10.cc </S>"
  ]
}