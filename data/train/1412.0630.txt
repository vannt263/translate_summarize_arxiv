{
  "article_text": [
    "probabilistic state estimation has been a core topic in mobile robotics since the 1980s @xcite , often as part of the _ simultaneous localization and mapping _ ( slam ) problem @xcite .",
    "early work in estimation theory focused on recursive ( as opposed to batch ) formulations @xcite , and this was mirrored in the formulation of slam as a filtering problem @xcite .",
    "however , despite the fact that continuous - time estimation techniques have been available since the 1960s @xcite , trajectory estimation for mobile robots has been formulated almost exclusively in discrete time .",
    "2pt @xcite showed how to formulate slam as a batch estimation problem incorporating both odometry measurements ( to smooth solutions ) as well as landmark measurements .",
    "this can be viewed as a generalization of _ bundle adjustment _",
    "@xcite , which did not incorporate odometry . today",
    ", batch approaches in mobile robotics are commonplace ( e.g. , graphslam by @xcite ) .",
    "@xcite show how batch solutions can be efficiently updated as new measurements are gathered and @xcite show that batch methods are able to achieve higher accuracy than their filtering counterparts , for the same computational cost .",
    "most of these results are formulated in discrete time .    .",
    ", title=\"fig : \" ] +    2pt discrete - time representations of robot trajectories are sufficient in many situations , but they do not work well when estimating motion from certain types of sensors ( e.g. , rolling - shutter cameras and scanning laser - rangefinders ) and sensor combinations ( e.g. , high datarate , asynchronous ) . in these cases ,",
    "a smooth , continuous - time representation of the trajectory is more suitable .",
    "for example , in the case of estimating motion from a scanning - while - moving sensor , a discrete - time approach ( with no motion prior ) can fail to find a unique solution ; something is needed to tie together the observations acquired at many unique timestamps .",
    "additional sensors ( e.g. , odometry or inertial measurements ) could be introduced to serve in this role , but this may not always be possible . in these cases ,",
    "a motion prior can be used instead ( or as well ) , which is most naturally expressed in continuous time .",
    "2pt one approach to continuous - time trajectory representation is to use interpolation ( e.g. , linear , spline ) directly between nearby discrete poses @xcite .",
    "instead , we choose to represent the trajectory nonparametrically as a one - dimensional gaussian process ( gp ) @xcite , with time as the independent variable ( see figure  [ fig : setup ] ) .",
    "@xcite show that querying the state of the robot at a time of interest can be viewed as a nonlinear , gp regression problem . while their approach is very general , allowing a variety of gp priors over robot trajectories , it is also quite expensive due to the need to invert a large , dense kernel matrix .",
    "while gps have been used in robotic state estimation to accomplish dimensionality reduction @xcite and to represent the measurement and motion models @xcite , these uses are quite different than representing the latent robot trajectory as a gp @xcite .",
    "2pt in this paper , we consider a particular class of gps , generated by nonlinear , time - varying ( ntv ) stochastic differential equations ( sde ) driven by white noise .",
    "we first show that gps based on linear , time - varying ( ltv ) sdes have an inverse kernel matrix that is _ exactly sparse _",
    "( block - tridiagonal ) and can be derived in closed form ; an approximation for gps based on a ntv sde is then shown that results in the same sparsity properties as the linear case .",
    "concentrating on this class of covariance functions results in only a minor loss of generality , because many commonly used covariance functions such the matrn class and the squared exponential covariance function can be exactly or approximately represented as linear sdes @xcite .",
    "we provide an example of this relationship at the end of this paper .",
    "the resulting sparsity allows the approach of @xcite to be implemented very efficiently .",
    "the intuition behind why this is possible is that the state we are estimating is _ markovian _ for this class of gps , which implies that the corresponding precision matrices are sparse @xcite .",
    "3002pt this sparsity property has been exploited in estimation theory to allow recursive methods ( both filtering and smoothing ) since the 1960s @xcite .",
    "@xcite offers an interesting discussion on nonlinear , continuous - time filtering ( using both a nonlinear dynamical plant and nonlinear observations ) .",
    "the tracking literature , in particular , has made heavy use of motion priors ( in both continuous and discrete time ) and has exploited the markov property for efficient solutions @xcite . for the nonlinear , discrete - time case , @xcite shows that kalman filtering and smoothing with iterated relinearization is equivalent to gauss - newton on the full - state trajectory .",
    "it is of no surprise that in vision and robotics , discrete - time batch methods commonly exploit this sparsity property as well @xcite . in this paper",
    ", we make the ( retrospectively obvious ) observation that this sparsity can also be exploited in a batch , continuous - time context .",
    "the result is that we derive a principled method to construct trajectory - smoothing terms for batch optimization ( or factors in a factor - graph representation ) based on a class of useful motion models ; this paves the way to incorporate vehicle dynamics models , including exogenous inputs , to help with trajectory estimation .",
    "2pt therefore , our main contribution is to emphasize the strong connection between classical estimation theory and machine learning via gp regression .",
    "we use the fact that the inverse kernel matrix is sparse for a class of useful gp priors @xcite in a new way to efficiently implement nonlinear , gp regression for batch , continuous - time trajectory estimation .",
    "we also show that this naturally leads to a subtle generalization of slam that we call _ simultaneous trajectory estimation and mapping _ ( steam ) , with the difference being that chains of discrete _ poses _ are replaced with _",
    "markovian trajectories _ in order to incorporate continuous - time motion priors in an efficient way . finally , by using this gp paradigm , we are able to exploit the classic gp interpolation approach to query the trajectory at any time of interest in an efficient manner .",
    "3004pt this ability to query the trajectory at any time of interest in a principled way could be useful in a variety of situations . for example , @xcite mapped a large urban area using a combination of stereo vision and laser rangefinders ; the motion was estimated using the camera and the laser data were subsequently placed into a three - dimensional map based on this estimated motion .",
    "our method could provide a seamless means to ( i ) estimate the camera trajectory and then ( ii ) query this trajectory at every laser acquisition time .",
    "3004pt this paper is a significant extension of our recent conference paper @xcite .",
    "we build upon the exactly - sparse gp - regression approach that used _ linear _ , time - varying sdes , and show how to use gps based on _ nonlinear _ , time - varying sdes , while maintaining the same level of sparsity as the linear case .",
    "the algorithmic differences are discussed in detail and results are provided using comparable linear and nonlinear priors .",
    "furthermore , this paper shows how the block - tridiagonal sparsity of the kernel matrix can be exploited to improve the computational performance of hyperparameter training .",
    "finally , discussion is provided on using the gp interpolation equation for further state reduction at the cost of accuracy .",
    "the paper is organized as follows .",
    "section  [ sec : gpr ] summarizes the general approach to batch state estimation via gp regression .",
    "section  [ sec : ltvsde ] describes the particular class of gps we use and elaborates on our main result concerning sparsity .",
    "section  [ sec : experiment ] demonstrates this main result on a mobile robot example using a ` constant - velocity ' prior and compares the computational cost to methods that do not exploit the sparsity .",
    "section  [ sec : discussion ] provides some discussion and section  [ sec : conclusion ] concludes the paper .",
    "we take a _",
    "gaussian - process - regression _ approach to state estimation .",
    "this allows us to ( i ) represent trajectories in continuous time ( and therefore query the solution at any time of interest ) , and ( ii ) optimize our solution by iterating over the entire trajectory ( recursive methods typically iterate at a single timestep ) .",
    "3004pt we will consider systems with a continuous - time , gp process model and a discrete - time , nonlinear measurement model : @xmath0 where @xmath1 is the state , @xmath2 is the mean function , @xmath3 is the covariance function , @xmath4 are measurements , @xmath5 is gaussian measurement noise , @xmath6 is a nonlinear measurement model , and @xmath7 is a sequence of measurement times . for the moment , we do not consider the steam problem ( i.e. , the state does not include landmarks ) , but we will return to this case in our example later .",
    "we follow the approach of @xcite to set up our batch , gp state estimation problem .",
    "we will first assume that we want to query the state at the measurement times , and will return to querying at other times later on .",
    "we start with an initial guess , @xmath8 , for the trajectory that will be improved iteratively . at each iteration , we solve for the optimal perturbation , @xmath9 , to our guess using gp regression , with our measurement model linearized about the current best guess .",
    "4pt the joint likelihood between the state and the measurements ( both at the measurement times ) is @xmath10 where    @xmath11    note , the measurement model is linearized about our best guess so far .",
    "we then have that the gaussian posterior is @xmath12 letting @xmath13 , and rearranging the posterior mean expression using the sherman - morrison - woodbury identity , we have @xmath14 which is a linear system for @xmath15 and can be viewed as the solution to the associated _ maximum a posteriori _ ( map ) problem .",
    "we know that the @xmath16 term in   is block - diagonal ( assuming each measurement depends on the state at a single time ) , but in general @xmath17 could be dense , depending on the choice of gp prior . at each iteration",
    ", we solve for @xmath15 and then update the guess according to @xmath18 ; upon convergence we set @xmath19 .",
    "this is effectively gauss - newton optimization over the whole trajectory .",
    "4pt we may want to also query the state at some other time(s ) of interest ( in addition to the measurement times ) .",
    "though we could jointly estimate the trajectory at the measurement and query times , a better idea is to use gp interpolation after the solution at the measurement times converges @xcite ( see section  [ sec : query ] for more details ) .",
    "gp interpolation automatically picks the correct interpolation scheme for a given prior ; it arrives at the same answer as the joint approach ( in the linear case ) , but at lower computational cost .    in general",
    ", this gp approach has complexity @xmath20 , where @xmath21 is the number of measurement times and @xmath22 is the number of query times ( the initial solve is @xmath23 and the queries are @xmath24 ) .",
    "this is quite expensive , and therefore we will seek to improve the cost by exploiting the structure of the matrices involved under a particular class of gp priors .",
    "4004pt we now show that the inverse kernel matrix is exactly sparse for a particular class of useful gp priors .",
    "we consider gps generated by linear , time - varying ( ltv ) stochastic differential equations ( sde ) of the form @xmath25 where @xmath1 is the state , @xmath26 is a ( known ) exogenous input , @xmath27 is white process noise , and @xmath28 , @xmath29 are time - varying system matrices .",
    "the process noise is given by @xmath30 a ( stationary ) zero - mean _ gaussian process _ ( gp ) with ( symmetric , positive - definite ) _ power - spectral density matrix _",
    ", @xmath31 , and @xmath32 is the _ dirac delta function_.    the general solution to this ltv sde @xcite is @xmath33 where @xmath34 is known as the _ transition matrix_. from this model , we seek the mean and covariance functions for @xmath1 .      for the mean function , we take the expected value of  : @xmath35 = { { \\boldsymbol{\\phi}}}(t , t_0 ) { { { \\check{{\\mbfa{x}}}}}}_0 + \\int_{t_0}^t { { \\boldsymbol{\\phi}}}(t , s ) { \\mbfa{v}}(s )   \\ , ds,\\ ] ] where @xmath36 is the initial value of the mean . if we now have a sequence of measurement times , @xmath37 , then we can write the mean at these times in _ lifted form _ as @xmath38 where @xmath39 note that @xmath40 , the _ lifted transition matrix _ , is lower - triangular .",
    "we arrive at this form by simply splitting up   into a sum of integrals between each pair of measurement times .      for the covariance function , we take the second moment of   to arrive at @xmath41 \\\\ & =   { { \\boldsymbol{\\phi}}}(t , t_0 ) { { { \\check{{\\mbfa{p}}}}}}_0 { { \\boldsymbol{\\phi}}}(t^\\prime , t_0)^t     \\\\   & + \\int_{t_0}^{\\min(t , t^\\prime ) } { { \\boldsymbol{\\phi}}}(t , s ) { \\mbfa{l}}(s){\\mbfa{q}}_c{\\mbfa{l}}(s)^t { { \\boldsymbol{\\phi}}}(t^\\prime , s)^t   \\ , ds ,   \\end{split}\\ ] ] where @xmath42 is the initial covariance at @xmath43 and we have assumed @xmath44 = { \\mbfa{0}}$ ] . using a sequence of measurement times , @xmath37 ,",
    "we can write the covariance between two times as    @xmath45    where @xmath46 for @xmath47 and @xmath48 ( to keep the notation simple ) . given this preparation , we are now ready to state the main sparsity result that we will exploit in the rest of the paper .",
    "[ lem1 ] let @xmath49 be a monotonically increasing sequence of time values .",
    "using  , we define the @xmath50 kernel matrix ( i.e. , the prior covariance matrix between all pairs of times ) , @xmath51_{ij}$ ] . then , we can factor @xmath52 according to a lower - diagonal - upper decomposition , @xmath53 where @xmath40 is the lower - triangular matrix given in   and @xmath54 with @xmath55 given in  .",
    "straightforward to verify by substitution .",
    "[ thm1 ] the inverse of the kernel matrix constructed in lemma  [ lem1 ] , @xmath17 , is exactly sparse ( block - tridiagonal ) .",
    "the decomposition of @xmath52 in lemma  [ lem1 ] provides @xmath56 where the inverse of the lifted transition matrix is @xmath57 and @xmath58 is block - diagonal .",
    "the block - tridiagonal property of @xmath17 follows by substitution and multiplication .",
    "while the block - tridiagonal property stated in theorem 1 has been exploited in vision and robotics for a long time @xcite , the usual route to this point is to begin by converting the continuous - time motion model to discrete time and then to directly formulate a maximum a posteriori optimization problem ; this bypasses writing out the full expression for @xmath52 and jumps to an expression for @xmath17 .",
    "however , we require expressions for both @xmath52 and @xmath17 to carry out our gp reinterpretation and facilitate querying the trajectory at an arbitrary time ( through interpolation ) . that said , it is also worth noting we have not needed to convert the motion model to discrete time and",
    "have made no approximations thus far .",
    "3004pt given the above results , the prior over the state ( at the measurement times ) can be written as @xmath59 more importantly , using the result of theorem  [ thm1 ] in   gives @xmath60 which can be solved in @xmath61 time ( at each iteration ) , using a sparse solver ( e.g. , sparse cholesky decomposition then forward - backward passes ) .",
    "in fact , in the case of a linear measurement model , one such solver is the classical , forward - backward smoother ( i.e. , kalman or rauch ",
    "striebel smoother ) @xcite .",
    "put another way , the forward - backward smoother is possible _ because _ of the sparse structure of  .",
    "for nonlinear measurement models , our scheme iterates over the whole trajectory ; it is therefore related to , but not the same as , the ` extended ' version of the forward - backward smoother @xcite .",
    "smoothing terms , @xmath62 , in the associated optimization problem , the solution of which is  . we can depict these graphically as factors ( black dots ) in a factor - graph representation of the prior @xcite .",
    "the triangles are _ trajectory states _ , the nature of which depends on the choice of prior . ]",
    "3004pt perhaps the most interesting outcome of theorem  [ thm1 ] is that , although we are using a continuous - time prior to smooth our trajectory , at implementation we require only @xmath63 smoothing terms in the associated map optimization problem : @xmath21 between consecutive pairs of measurement times plus @xmath64 at the initial time ( unless we are also estimating a map ) . as mentioned before , this is the same form that we would have arrived at had we started by converting our motion model to discrete time at the beginning @xcite .",
    "this equivalence has been noticed before for recursive solutions to estimation problems with a continuous - time state and discrete - time measurements @xcite , but not in the batch scenario .",
    "figure  [ fig : factors ] depicts the @xmath63 smoothing terms in a factor - graph representation of the prior @xcite .",
    "however , while the form of the smoothing terms / factors is similar to the original discrete - time form introduced by @xcite , our approach provides a principled method for their construction , starting from the continuous - time motion model .",
    "critically , we stress that the state being estimated must be _ markovian _ in order to obtain the desirable sparse structure . in the experiment section",
    ", we will investigate a common gp prior , namely the ` constant - velocity ' or white - noise - on - acceleration model : @xmath65 , where @xmath66 represents position . for this choice of model ,",
    "@xmath66 is not markovian , but @xmath67 is .",
    "this implies that , if we want to use the ` constant - velocity ' prior and enjoy the sparse structure without approximation , we must estimate a stacked state with both position and velocity .",
    "marginalizing out the velocity variables fills in the inverse kernel matrix , thereby destroying the sparsity .    if all we cared about was estimating the value of the state at the measurement times , our gp paradigm arguably offers little beyond a reinterpretation of the usual discrete - time approach to batch estimation .",
    "however , by taking the time to set up the problem in this manner , we can now query the trajectory at _ any _ time of interest using the classic interpolation scheme that is inherent to gp regression @xcite .",
    "as discussed in section  [ sec : gpr ] , after we solve for the trajectory at the measurement times , we may want to query it at other times of interest .",
    "this operation also benefits greatly from the sparse structure . to keep things simple ,",
    "we consider a single query time , @xmath68 ( see figure  [ fig : setup ] ) .",
    "the standard linear gp interpolation formulas @xcite are    [ eq : lininterp ] @xmath69    where @xmath70 .",
    "note , we write in a less common , but equivalent , form , as we intend to exploit the sparsity of the product @xmath71 .    for the mean function at the query time , we simply have @xmath72 which can be evaluated in @xmath73 time . for the covariance function at the query time , we have @xmath74 which is also @xmath73 to evaluate .",
    "the computational savings come from the sparsity of the product @xmath71 , which represents the burden of the cost in the interpolation formula . after some effort",
    ", it turns out we can write @xmath75 as @xmath76 where @xmath40 was defined before , @xmath77,\\end{aligned}\\ ] ] and @xmath78 returning to the desired product , we have @xmath79 since @xmath58 is block - diagonal , and @xmath80 has only the main diagonal and the one below it non - zero , we can evaluate the product very efficiently .",
    "note , there are exactly two non - zero block - columns : @xmath81,\\ ] ] where    @xmath82    inserting this into  , we have    @xmath83    which is a linear combination of just the terms from @xmath84 and @xmath85 .",
    "if the query time is beyond the last measurement time , @xmath86 , the expression will involve only the term at @xmath87 and represents extrapolation / prediction rather than interpolation / smoothing . in summary , to query the trajectory at a single time of interest is @xmath73 complexity .      thus far",
    ", we have shown that by storing the _ markovian _ , @xmath88 , state at every measurement time , @xmath89 , we are able to perform gaussian - process regression in @xmath61 time and then query for other times of interest @xmath90 , all without approximation . given the ability to interpolate , storing the state at every measurement time may be excessive , especially in a scenario where the measurement rate is high in comparison to the smoothness of the robot kinematics ( e.g. a 1000hz imu or individually timestamped lidar measurements , mounted on a slow indoor platform ) .",
    "@xcite discuss a scheme to remove some of the measurement times from the initial solve , which further reduces computational cost with some loss of accuracy . by estimating @xmath91 at only some _ keytimes _ , @xmath92 , which may or may not align with some subset of the measurement times , the interpolation equation can be used to modify our measurement model as follows , @xmath93 where @xmath94 .",
    "the matrices @xmath95 and @xmath96 are constructed according to , where the measurement time , @xmath84 , is now the query time and the bounding times ( previously @xmath84 and @xmath97 ) become the keytimes @xmath98 and @xmath99 . the effect on @xmath100",
    "is that each block row now has two adjacent non - zero block columns , rather than one .",
    "this causes the structure of @xmath101 to change from being block - diagonal to block - tridiagonal ; the structure of @xmath101 is of particular importance because it directly affects the complexity of solving .",
    "fortunately , the complexity is unaffected because our prior term is also block - tridiagonal .",
    "4pt an important intuition is that the interpolated state at some measurement time , @xmath84 , depends only on the state estimates , @xmath102 and @xmath103 , and the prior terms , @xmath2 and @xmath104 .",
    "therefore , the effect of estimating @xmath91 at some reduced number of times is that we obtain a smoothed solution ; any details provided by high frequency measurements between two times of interest , @xmath98 and @xmath105 , are lost .",
    "although this detail information is smoothed over , there are obvious computational savings in having a smaller state and a subtle benefit regarding the prevention of overfitting measurements .",
    "with respect to fitting , the glaring issue with this scheme is that there is not a principled method to determine an appropriate spacing for the _ keytimes _ , @xmath98 , such that we could guarantee a bound on the loss of accuracy .",
    "learning from the parametric continuous - time estimation schemes , which suffer from a similar issue , it is reasonable to start with some uniform spacing and add additional _ keytimes _ based on the results of a normalized - innovation - squared test between each pair of _ keytimes _ @xcite .",
    "some experimentation is necessary to better understand which approach to state discretization is best in which situation .",
    "4pt in reality , most systems are inherently nonlinear and can not be accurately described by a ltv sde in the form of . moving forward ,",
    "we show how our results concerning sparsity can be applied to nonlinear , time - varying ( ntv ) stochastic differential equations ( sde ) of the form @xmath106 where @xmath107 is a nonlinear function , @xmath1 is the state , @xmath108 is a ( known ) exogenous input , and @xmath27 is white process noise . to perform gp regression with a nonlinear process model ,",
    "we begin by linearizing the sde about a continuous - time operating point @xmath109 , @xmath110 where @xmath111 setting @xmath112 lets us rewrite in the familiar ltv sde form , @xmath113 where @xmath114 , @xmath26 , and @xmath29 are known functions of time , since @xmath109 is known . setting the operating point , @xmath109 , to our best guess of the underlying trajectory at each iteration of gp regression",
    ", we note a similarity in nature to the discrete - time , recursive method of @xcite ; in essence , our approach offers a continuous - discrete version of the gauss - newton estimator , using the gaussian - process - regressor type of approximate bridging between the measurement times .",
    "although the equations for calculating the mean , @xmath115 , and covariance , @xmath52 , remain the same as in and , there are a few algorithmic differences and issues that arise due to the new dependence on the continuous - time operating point , @xmath109 .",
    "an obvious difference in contrast to the gp regression using a ltv sde is that we now must recalculate @xmath115 and @xmath52 at each iteration of the optimization ( since the linearization point is updated ) .",
    "the main algorithmic issue that presents itself is that the calculation of @xmath115 and @xmath52 require integrations involving @xmath114 , @xmath26 , and @xmath29 , which in turn require the evaluation of @xmath109 over the time period @xmath116 $ ] . since an estimate of the posterior mean , @xmath8 ,",
    "is only stored at times of interest , we must make use of the efficient ( for our particular choice of process model ) gp interpolation equation derived in section  [ sec : query ] , @xmath117 the problem is that the above interpolation depends again on @xmath115 and @xmath52 , which are the prior variables for which we want to solve . to rectify this circular dependence , we take advantage of the iterative nature of gp regression and choose to evaluate using the values of @xmath118 , @xmath115 , and @xmath119 from the previous iteration .",
    "4pt another issue with nonlinear process models is that identifying an analytical expression for the state transition matrix , @xmath34 ( which is dependent on the form of @xmath114 ) , can be very challenging .",
    "fortunately , the transition matrix can also be calculated numerically via the integration of the normalized fundamental matrix , @xmath120 , where @xmath121 storing @xmath120 at times of interest , the transition matrix can then be computed using @xmath122 in contrast to the ltv sde system , using a nonlinear process model causes additional computational costs ( primarily due to numerical integrations ) , but complexity remains linear in the length of the trajectory ( at each iteration ) , and therefore continues to be computationally tractable .      in this section ,",
    "we discuss the algorithmic details of the gp interpolation procedure as it pertains to a ntv sde process model .",
    "recall the standard linear gp interpolation formulas presented in , for a single query time , @xmath68 :    @xmath123    the final iteration of gp regression ( where @xmath19 ) , provides values for @xmath115 , @xmath52 , @xmath124 , and @xmath125 ; however , obtaining values for @xmath118 , @xmath71 ( recall sparse structure in ) , and @xmath126 is not trivial .",
    "the suggestion made in section  [ sec : nonlinear_mean_and_cov ] was to use values from the previous ( or in this case , final ) iteration .",
    "thus far , the method of storing these continuous - time functions has been left ambiguous .    the naive way to store @xmath118 ,",
    "@xmath71 , and @xmath126 is to keep the values at all numerical integration timesteps ; the memory requirement of this is proportional to the length of the trajectory . during the optimization procedure",
    "this naive method may in fact be preferable as it reduces computation time in lieu of additional storage ( which is fairly cheap using current technology ) . for long - term storage ,",
    "a method that uses a smaller memory footprint ( at the cost of additional computation ) may be desirable .",
    "the remainder of this section will focus on identifying the minimal storage requirements , such that queries remain @xmath73 complexity .",
    "4pt we begin by examining the mean function ; substituting and , into the gp interpolation formula above : @xmath127 it is straightforward to see how @xmath128 can be simultaneously numerically integrated with the normalized fundamental matrix from , as long as we can evaluate the term @xmath71 .",
    "although we chose to examine the mean function , a similar conclusion can be drawn by examining the covariance function in .",
    "recalling the sparse structure of @xmath71 ( see ) for an ltv sde process model , where @xmath129 we are able to draw two conclusions .",
    "first , in the case that an analytical expression for @xmath34 is unavailable , we must store @xmath130 at the times of interest @xmath89 , since any numerical integration will involve ` future ' values of @xmath120 ( via the evaluation of @xmath131 , @xmath132 , and @xmath133 ) .",
    "second , in the case that @xmath29 is a time - varying matrix , we must store @xmath134 , since the evaluation of @xmath135 , requires @xmath136 ( evaluated at @xmath137 ) over the time period @xmath138 $ ] .",
    "the memory requirements of this alternative are still proportional to the length of the trajectory , but are greatly reduced in contrast to the naive method ; the added cost is that any new query requires numerical integration from the nearest time @xmath84 to the query time @xmath139 ( which is of order @xmath73 ) .",
    "2pt as with any gp regression , we have _ hyperparameters _ associated with our covariance function , namely @xmath31 , which affect the smoothness and length scale of the class of functions we are considering as motion priors .",
    "the covariances of the measurement noises can also be unknown or uncertain .",
    "the standard approach to selecting these parameters is to use a training dataset ( with ground - truth ) , and perform optimization using the log marginal likelihood ( log - evidence ) or its approximation as the objective function @xcite .",
    "we begin with the marginal likelihood equation , @xmath140 where @xmath141 is a stacked vector of state observations ( ground - truth measurements ) with additive noise @xmath142 , @xmath115 is a stacked vector of the mean equation evaluated at the observation times , @xmath143 , and @xmath52 is the covariance matrix associated with @xmath115 and generated using the hyperparameters @xmath31 . taking partial derivatives of the marginal likelihood with respect to the hyperparameters ,",
    "we get @xmath144where we have used that @xmath145 .",
    "2pt the typical complexity of hyperparameter training is bottlenecked at @xmath146 due to the inversion of @xmath147 ( which is typically dense ) .",
    "given @xmath148 , the complexity is then typically bottlenecked at @xmath149 due to the calculation of @xmath150 .",
    "fortunately , in the present case , the computation of the log marginal likelihood can also be done efficiently due to the sparseness of the inverse kernel matrix , @xmath17 . despite the addition of observation noise , @xmath151 , causing @xmath148 to be a dense matrix ,",
    "we are still able to take advantage of our sparsity using the sherman - morrison - woodbury identity : @xmath152 where we define @xmath153 although computing @xmath148 explicitly is of order @xmath149 , we note that the product with a vector , @xmath154 , can be computed in @xmath155 time ; this is easily observable given that @xmath58 is block - diagonal , @xmath156 is lower block - bidiagonal , and the product @xmath157 can be computed in @xmath155 time using cholesky decomposition , because @xmath158 is block - tridiagonal . while the two terms in can be combined into a single trace function , it is simpler to study their computational complexity separately .",
    "starting with the first term , we have @xmath159 where @xmath160 and @xmath161 denotes a projection matrix with a @xmath64 at the @xmath162^th^ row and @xmath163^th^ column . taking advantage of the sparse matrices and previously mentioned fast matrix - vector products ,",
    "it is clear that can be computed in @xmath155 time ( in contrast to the typical @xmath146 time ) .",
    "taking a look at the second term , we have that @xmath164 which can only be computed in @xmath149 , due to the form of @xmath165 . in general , the total complexity of training for this sparse class of prior is then bottlenecked at @xmath149 .",
    "a complexity of @xmath155 can only be achieved by ignoring the additive measurement noise , @xmath166 ; revisiting the second term of , and setting @xmath167 , we find that @xmath168 which can be computed in time @xmath155 .",
    "the effect of ignoring the measurement noise , @xmath166 , is that the trained hyperparameters will result in an underconfident prior ; the degree of this underconfidence depends on the magnitude of the noise we are choosing to ignore .",
    "if accurate ground - truth measurements are available and the size of the training dataset is very large , this approximation may be beneficial .",
    "2pt we conclude this section with a brief discussion of the time complexity of the overall algorithm when exploiting the sparse structure .",
    "if we have @xmath21 measurement times and want to query the trajectory at @xmath22 additional times of interest , the complexity of the resulting algorithm using gp regression with _ any _ linear ( or nonlinear ) , time - varying process model driven by white noise will be @xmath169 .",
    "this is broken into the two major steps as follows .",
    "the initial solution to find @xmath8 ( at the measurement times ) can be done in @xmath61 time ( per iteration ) owing to the block - tridiagonal structure discussed earlier .",
    "then , the cost of the queries at @xmath22 other times of interest is @xmath170 since each individual query is @xmath73 .",
    "clearly , @xmath169 is a big improvement over the @xmath171 cost when we did not exploit the sparse structure of the problem .",
    "2pt we will demonstrate the advantages of the sparse structure through an example employing the ` constant - velocity ' prior , @xmath65 .",
    "this can be expressed as a linear , time - invariant sde of the form in   with @xmath172 where @xmath173 is the pose and @xmath174 is the pose rate . in this case , the transition function is @xmath175 which can be used to construct @xmath40 ( or @xmath80 directly ) . as we will be doing a steam example , we will constrain the first trajectory state to be @xmath176 and so will have no need for @xmath177 and @xmath42 . for @xmath47 , we have @xmath178 and @xmath179 with @xmath180 . the inverse blocks are @xmath181 so we can build @xmath58 directly .",
    "we now have everything we need to represent the prior : @xmath80 , @xmath58 , and @xmath182 , which can be used to construct @xmath17 .",
    "we will also augment the trajectory with a set of @xmath183 landmarks , @xmath184 , into a combined state , @xmath185 , in order to consider the steam problem : @xmath186 while others have folded velocity estimation into discrete - time , filter - based slam @xcite and even discrete - time , batch slam @xcite , we are actually proposing something more general than this : the choice of prior tells us what to use for the trajectory states . and ,",
    "although we solve for the state at a discrete number of measurement times , our setup is based on an underlying continuous - time prior , meaning that we can query it at any time of interest in a principled way .",
    "2pt since the main source of acceleration in this example is robot - oriented ( the actuated wheels ) , we investigate the use of an alternate ` constant - velocity ' prior , with the white noise affecting acceleration in the robot body frame , @xmath187 .",
    "this _ nonlinear _ prior can be written as , @xmath188 where @xmath189 is a rotation matrix between the inertial and robot body frame , and the _ markovian _ state is @xmath190 , where @xmath191 is the robot - oriented velocity ( with longitudinal , latitudinal , and rotational components , respectively ) .",
    "linearizing about an arbitrary operating point , @xmath109 , the components @xmath114 , @xmath26 and @xmath29 from @xmath192 are straight - forward to derive .",
    "similarly , to the linear prior example described above , we will define the exogenous input @xmath193 ; however , we note that @xmath194 ( see ) .",
    "since expressions for @xmath34 and @xmath55 are not obvious , we will rely on numerical integration for their evaluation .",
    "we will use two types of measurements : range / bearing to landmarks ( using a laser rangefinder ) and wheel odometry ( in the form of robot - oriented velocity ) .",
    "the range / bearing measurement model takes the form @xmath195 the wheel odometry measurement model gives the longitudinal and rotational speeds of the robot , taking the form @xmath196 for the lti sde prior , and @xmath197 for the ntv sde prior . note that in both cases the velocity information is extracted easily from the state since we are estimating it directly",
    ". the jacobians with respect to the state , @xmath1 , are straightforward to derive .",
    "4pt figure  [ fig : steam ] shows an illustration of the steam problem we are considering . in terms of linear algebra , at each iteration we need to solve a linear system of the form @xmath198 which retains exploitable structure despite introducing landmarks to the state @xcite . in particular , @xmath199 is block - tridiagonal ( due to our gp prior ) and @xmath200 is block - diagonal ; the sparsity of the off - diagonal block , @xmath201 , depends on the specific landmark observations .",
    "we reiterate the fact that if we marginalize out the @xmath202 ( or @xmath203 ) variables and keep only the @xmath204 variables to represent the trajectory , the @xmath199 block becomes dense ( for these priors ) ; this is precisely the approach of @xcite .    to solve   efficiently",
    ", we can begin by either exploiting the sparsity of @xmath199 or of @xmath200 . since each trajectory variable represents a unique measurement time ( range / bearing or odometry ) ,",
    "there are potentially a lot more trajectory variables than landmark variables , @xmath205 , so we will exploit @xmath199 .",
    "we use a sparse ( lower - upper ) cholesky decomposition : @xmath206 we first decompose @xmath207 , which can be done in @xmath61 time owing to the block - tridiagonal sparsity .",
    "the resulting @xmath208 will have only the main block - diagonal and the one below it non - zero .",
    "we can then solve @xmath209 for @xmath210 in @xmath211 time .",
    "finally , we decompose @xmath212 , which we can do in @xmath213 time .",
    "this completes the decomposition in @xmath214 time .",
    "we then perform the standard forward - backward passes , ensuring to exploit the sparsity : first solve @xmath215 for @xmath216 , then @xmath217 for @xmath218 , both in @xmath219 time .",
    "note , this approach does not marginalize out any variables during the solve , as this can ruin the sparsity ( i.e. , we avoid inverting @xmath199 ) .",
    "the whole solve is @xmath214 .    at each iteration",
    ", we update the state , @xmath220 , and iterate to convergence . finally , we query the trajectory at @xmath22 other times of interest using the gp interpolation discussed earlier .",
    "the whole procedure is then @xmath221 , including the extra queries .    due to the addition of landmarks",
    ", the cost of a steam problem must be either be of order @xmath222 or @xmath223 , depending on the way we choose to exploit @xmath199 or @xmath200 .",
    "the state reduction scheme presented in section  [ sec : measinterp ] becomes very attractive when both the number of measurements and landmarks are very high ; estimating the state at @xmath224 _ keytimes _ , @xmath225 , and exploiting @xmath200 , the procedure becomes order @xmath226 , which is the same complexity as a traditional discrete - time slam problem ( with the addition of the @xmath22 query times ) .      for experimental validation , we employed the same mobile robot dataset as used by @xcite .",
    "this dataset consists of a mobile robot equipped with a laser rangefinder driving in an indoor , planar environment amongst a forest of @xmath227 plastic - tube landmarks .",
    "the odometry and landmark measurements are provided at a rate of 1hz , and additional trajectory queries are computed at a rate of 10hz after estimator convergence .",
    "ground - truth for the robot trajectory and landmark positions is provided by a vicon motion capture system .",
    "2pt we implemented three estimators for comparison .",
    "the first was the algorithm described by @xcite , _ gp - pose - dense _ , the second was a naive version of our estimator , _ gp - traj - dense _ , based on the lti sde prior described in section  [ sec : exp_ltipri ] , but did not exploit sparsity , and the third was a full implementations of our estimator , _ gp - traj - sparse _ , that exploited the sparsity structure as described in this paper .",
    "the final estimator has two variants , _ gp - traj - sparse - lti _ and _ gp - traj - sparse - ntv _ , based on the lti sde and ntv sde priors described in sections  [ sec : exp_ltipri ] and [ sec : exp_ntvpri ] , respectively ; as the first two estimators , _ gp - pose - dense _ and _ gp - traj - dense _ , are only of interest with regard to computational performance , they each implement only the lti ` constant - velocity ' prior .    for this experiment",
    ", we obtained @xmath31 for both the lti and ntv priors by modelling it as a diagonal matrix and taking the data - driven training approach using log marginal likelihood ( with ground - truth measurements ) described in section  [ sec : hyper_training ] .",
    "3004pt though the focus of the exactly sparse gaussian process priors is to demonstrate the significant reductions in computational cost , we provide figure  [ fig : trajectory_plot ] to illustrate the smooth trajectory estimates we obtained from the continuous - time formulation . while the algorithms differed in their number of degrees of freedom and types of their estimated states , their overall accuracies were similar for this dataset .",
    "the _ gp - traj - sparse - ntv _ algorithm differed slightly from the others ; qualitatively , we found that the trajectory estimated by the _ gp - traj - sparse - ntv _ algorithm more accurately matched the local shape of the ground - truth on many occasions , such as the ones highlighted by the insets in figure  [ fig : trajectory_plot ] . also , it is clear from the plotted @xmath228 covariance envelope that the estimate from the _ gp - traj - sparse - ntv _ algorithm tends to be more uncertain .",
    "2pt to evaluate the computational savings of exploiting an exactly sparse gp prior , we implemented all algorithms in matlab with a 2.4ghz i7 processor and 8 gb of 1333mhz ddr3 ram and timed the computation for segments of the dataset of varying lengths .",
    "these results are shown in figure  [ fig : timing_plots ] , where we provide the computation time for the individual operations that benefit most from the sparse structure , as well as the overall processing time .",
    "we see that the _ gp - traj - dense _ algorithm is much slower than the original _ gp - pose - dense _ algorithm of @xcite .",
    "this is because we have reintroduced the velocity part of the state , thereby doubling the number of variables associated with the trajectory .",
    "however , once we start exploiting the sparsity with the _ gp - traj - sparse _ methods , the increase in number of variables pays off .    for the _ gp - traj - sparse _ methods , we see in figure  [ fig : timing_kernel ] that the kernel matrix construction was linear in the number of estimated states .",
    "this can be attributed to the fact that we constructed the sparse @xmath17 directly . as predicted , the optimization time per iteration was also linear in figure  [ fig : timing_optimization ] , and the interpolation time per additional query was constant regardless of state size in figure  [ fig : timing_interpolation ] .",
    "finally , figure  [ fig : timing_total ] shows that the total compute time was also linear .",
    "the additional cost of the _ gp - traj - sparse - ntv _ algorithm over the _ gp - traj - sparse - lti _ algorithm in kernel construction time is due to the linearization and numerical integration of the prior mean and covariance .",
    "the optimization time of the _ gp - traj - sparse - ntv _ algorithm is also affected because the kernel matrix must be reconstructed from a new linearization of the prior during every optimization iteration .",
    "the _ gp - traj - sparse - ntv _ algorithm also incurs some numerical integration cost in interpolation time , but it is constant and very small .",
    "we also note that the number of iterations for optimization convergence varied for each algorithm .",
    "in particular , we found that the _ gp - traj - sparse _",
    "implementations converged in fewer iterations than the other implementations due to the fact that we constructed the inverse kernel matrix directly , which resulted in greater numerical stability .",
    "the _ gp - traj - sparse _ approaches clearly outperform the other algorithms in terms of computational cost .      in a problem with fairly accurate and high - rate measurements ,",
    "both the _ gp - traj - sparse - lti _ and _ gp - traj - sparse - ntv _ estimators provide similar accuracy . in order to expose the benefit of a nonlinear prior based on the expected motion of the vehicle",
    ", we increase the nonlinearity of the problem by reducing measurement frequency .",
    "4pt the result of varying range measurement frequency , with and without the use of odometry measurements , is shown in figure  [ fig : nonlinearity_errors ] . in general , it is clear that as the interval between range measurements is increased , the _ gp - traj - sparse - ntv _ estimator is able to produce a more accurate estimate of the continuous - time pose ( translation and rotation ) than the _ gp - traj - sparse - lti _ estimator .    in the case",
    "that the 1hz odometry measurements are available , as seen in figure  [ fig : error_with_odom ] , the difference in the rotation estimates is small , because the _ gp - traj - sparse - lti _ estimator has a good amount of information about its heading ; however , in the case that the odometry measurements are unavailable , as seen in figure  [ fig : error_without_odom ] , the advantage of the nonlinear prior implemented by the _ gp - traj - sparse - ntv _ estimator is prominent with respect to the rotational estimate .    in order to gain some qualitative intuition about how the continuous - time pose estimates are affected by the reduction of measurements , figure  [ fig : nonlinearity_trajplot ] shows the trajectories for the same small subsection as presented in figure  [ fig : trajectory_plot ] ; the estimates used an interval between range measurements of 7 seconds and are shown with and without the use of odometry measurements . in both plots , it is clear that the _ gp - traj - sparse - ntv _ estimator matches the ground - truth more closely , as previously indicated by the error plots in figure  [ fig : nonlinearity_errors ] .",
    "it is worth elaborating on a few issues .",
    "the main reason that the @xmath199 block is sparse in our approach , as compared to @xcite , is that we reintroduced velocity variables that had effectively been marginalized out .",
    "this idea of reintroducing variables to regain exact sparsity has been used before by @xcite in the delayed state filter and by @xcite in the extended information filter .",
    "this is a good lesson to heed : the underlying structure of a problem may be exactly sparse , but by marginalizing out variables it appears dense . for us",
    "this means we need to use a markovian trajectory state that is appropriate to our prior .    in much of mobile robotics ,",
    "odometry measurements are treated more like inputs to the mean of the prior than pure measurements .",
    "we believe this is a confusing thing to do as it conflates two sources of uncertainty : the prior over trajectories and the odometry measurement noise . in our framework , we have deliberately separated these two functions and believe this is easier to work with and understand .",
    "we can see these two functions directly in figure  [ fig : steam ] , where the prior is made up of binary factors joining consecutive trajectory states , and odometry measurements are unary factors attached to some of the trajectory states ( we could have used binary odometry factors but chose to set things up this way due to the fact that we were explicitly estimating velocity ) .",
    "while our analysis appears to be restricted to a small class of covariance functions , we have only framed our discussions in the context of robotics .",
    "recent developments from machine learning @xcite and signal processing @xcite have shown that it is possible to generate other well - known covariance functions using a ltv sde ( some exactly and some approximately ) .",
    "this means they can be used with our framework .",
    "one example is the matrn covariance family @xcite , @xmath229 where @xmath230 , @xmath231 , @xmath232 are magnitude , smoothness , and length - scale parameters , @xmath233 is the gamma function , and @xmath234 is the modified bessel function .",
    "for example , if we let @xmath67 with @xmath235 with @xmath236 and use the following sde : @xmath237 where @xmath238 and @xmath239 ( our usual white noise ) with power spectral density matrix , @xmath240 then we have that @xmath66 is distributed according to the matrn covariance family : @xmath241 with @xmath236 . another way to look at this",
    "is that passing white noise through ltv sdes produces particular coloured - noise priors ( i.e. , not flat across all frequencies ) .    in terms of future work",
    ", we plan to incorporate the dynamics ( i.e. , kinematics plus newtonian mechanics ) of a robot platform into the gp priors ; real sensors do not move arbitrarily through the world as they are usually attached to massive robots and this serves to constrain the motion .",
    "another idea is to incorporate _ latent force models _ into our gp priors ( e.g. , see @xcite or @xcite ) .",
    "we also plan to look further at the sparsity of steam and integrate our work with modern solvers to tackle large - scale problems ; this should allow us to exploit more than just the primary sparsity of the problem and do so in an online manner .",
    "4pt we have considered continuous - discrete estimation problems where a trajectory is viewed as a one - dimensional _ gaussian process _ ( gp ) , with time as the independent variable and measurements acquired at discrete times . querying the trajectory can be viewed as nonlinear , gp regression .",
    "our main contribution in this paper is to show that this querying can be accomplished very efficiently . to do this",
    ", we exploited the markov property of our gp priors ( generated by nonlinear , time - varying stochastic differential equations driven by white noise ) to construct an inverse kernel matrix that is sparse .",
    "this makes it fast to solve for the state at the measurement times ( as is commonly done in vision and robotics ) but also at any other time(s ) of interest through gp interpolation .",
    "other implications of this sparsity were discussed with respect to hyperparameter training , and including measurements at query times .",
    "we also considered a slight generalization of the slam problem , _ simultaneous trajectory estimation and mapping _ ( steam ) , which makes use of a continuous - time trajectory prior and allows us to query the state at any time of interest in an efficient manner .",
    "we hope this paper serves to deepen the connection between classical state estimation theory and recent machine learning methods by viewing batch estimation through the lens of gaussian process regression .",
    "thanks to dr .",
    "alastair harrison at oxford who asked the all - important question : _ how can the gp estimation approach @xcite be related to factor graphs ? _ this work was supported by the canada research chair program , the natural sciences and engineering research council of canada , and the academy of finland ."
  ],
  "abstract_text": [
    "<S> 2pt in this paper , we revisit batch state estimation through the lens of _ gaussian process _ ( gp ) regression . </S>",
    "<S> we consider continuous - discrete estimation problems wherein a trajectory is viewed as a one - dimensional gp , with time as the independent variable . </S>",
    "<S> our continuous - time prior can be defined by any nonlinear , time - varying stochastic differential equation driven by white noise ; this allows the possibility of smoothing our trajectory estimates using a variety of vehicle dynamics models ( e.g. , ` constant - velocity ' ) . </S>",
    "<S> we show that this class of prior results in an inverse kernel matrix ( i.e. , covariance matrix between all pairs of measurement times ) that is exactly sparse ( block - tridiagonal ) and that this can be exploited to carry out gp regression ( and interpolation ) very efficiently . when the prior is based on a linear , time - varying stochastic differential equation and the measurement model is also linear , this gp approach is equivalent to classical , discrete - time smoothing ( at the measurement times ) ; when a nonlinearity is present , we iterate over the whole trajectory to maximize accuracy . </S>",
    "<S> we test the approach experimentally on a simultaneous trajectory estimation and mapping problem using a mobile robot dataset . </S>"
  ]
}