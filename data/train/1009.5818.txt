{
  "article_text": [
    "support vector machines [ svm ;  @xcite ] are a popular tool for classification .",
    "two important aspects contributed a lot to this popularity .",
    "first , support vector machines handle high - dimensional , low sample size data very well , in terms of computational efficiency as well as prediction quality .",
    "therefore , they are well suited to tackle , for example , microarray data containing thousands of gene expression levels ( high dimensionality ) for a limited number of subjects ( low sample size ) ; see , for example , @xcite and  @xcite .",
    "second , support vector machines allow for incorporating kernel functions via the so - called kernel trick .",
    "this way nonlinearity in the data can be handled , for example , using a polynomial or a gaussian kernel . moreover ,",
    "nonnumerical data can be modeled by designing an appropriate kernel function using a priori biological information about the data at hand .",
    "this strategy is reported to perform very well , for instance , in protein homology detection , for example , fisher svm  [ @xcite ] , pairwise svm  [ @xcite ] , spectrum kernel  [ @xcite ] , mismatch kernel  [ @xcite ] and local alignment kernel [ @xcite ] .    for high - dimensional and complex data sets ,",
    "the assumption of clean , independent and identically distributed samples is not always appropriate . in @xcite and @xcite , for instance , several samples are  regarded as suspicious .",
    "a potential drawback of support vectormachines is the sensitivity to an even very small number of outliers .",
    "outlier detection is thus important and many approaches have been proposed in the literature .",
    "although often useful , these methods come with some important drawbacks as well :    * as discussed by @xcite , many techniques are limited to situations where the sample size exceeds the dimension , thus excluding modern high - dimensional data analysis .",
    "* several types of outliers exist .",
    "algorithms such as those proposed by @xcite , @xcite and @xcite focus on samples that are potentially mislabeled .",
    "however , not every outlier is a mislabeled observation and vice versa : a sample can be correctly labeled yet behave in a completely different way than its group members .",
    "such discrimination between several types of outliers is usually not provided . *",
    "most algorithms basically provide a ranking of the samples according to potential mislabeling .",
    "however , intuitively it is not always clear how many of the top ranked samples are serious outlier candidates .",
    "automatic cut - off procedures often turn out too conservative ( not detecting all outliers ) or too aggressive ( pointing out good samples as outliers ) . *",
    "the role of the kernel is highly undervalued .",
    "some methods [ @xcite , @xcite ] do not use support vector machines or kernels at all .",
    "@xcite use support vector machines , but restrict themselves to a linear kernel and even a constant regularization parameter , whereas optimization of hyperparameters through cross validation is preferred .    in order to avoid some of these difficulties",
    ", we propose an outlier map for  svm classification .",
    "outlier maps ( also called diagnostic plots ) are quite common in multivariate statistics , for example , for linear regressionand linear principal component analysis  [ @xcite , @xcite ] .",
    "the idea is to start from a robust method guaranteeing resistance to potential outliers .",
    "based on this robust fit , appropriate measures of interest ( e.g. , residuals in regression ) are computed and plotted .    in this paper a similar idea is developed for providing an outlier map which is easy to interpret , distinguishes different types of potential outliers , and works for any type of kernel .",
    "on the @xmath0-axis of this map we put the stahel  donoho outlyingness . in section  [",
    "sect : sd ] we explain how to compute this outlyingness measure in a general kernel induced feature space . on the @xmath1-axis of the outlier map we put the value of the classification function of a trimmed support vector machine .",
    "more details on this robustified svm are given in section  [ sect : sdsvm ] .",
    "the main part of the paper is section  [ sect : defom ] , where the outlier map is defined and illustrated in a simple two - dimensional example . in section  [ sect : examples ] the outlier map is discussed in @xmath2 high - dimensional real life examples .",
    "let @xmath3 be a data set of @xmath4-dimensional samples @xmath5 . in multivariate statistics the stahel ",
    "donoho outlyingness of sample @xmath6 [ @xcite , @xcite ] is defined by @xmath7 with @xmath8 a robust univariate estimator of location and @xmath9 a univariate estimator of spread .",
    "popular choices are , for instance , the median for @xmath8 and the median absolute deviation ( mad ) for @xmath9 .",
    "the set @xmath10 is a set of @xmath11 directions in @xmath12 . in practice , this set is often constructed by selecting directions orthogonal to subspaces containing @xmath4 observations if @xmath4 is sufficiently small .",
    "another possibility is taking @xmath11 times a direction through @xmath13 randomly chosen observations .",
    "this strategy works in any dimension @xmath4 and since we will extend the outlyingness to high - dimensional kernel spaces , this is the strategy of our choice .",
    "the stahel ",
    "donoho outlyingness plays a crucial role in several multivariate robust algorithms , for example , covariance estimation [ @xcite ] and pca [ @xcite ] .",
    "first we note that this outlyingness measure can be computed in an arbitrary kernel induced feature space .",
    "let @xmath14 be @xmath15 elements in a set @xmath16 .",
    "let @xmath17 be an appropriate kernel function @xmath18 with corresponding feature space  @xmath19 and feature map @xmath20 such that the inner product @xmath21 between feature vectors in  @xmath19 can be computed by @xmath17 : @xmath22 denote @xmath23 the matrix containing @xmath24 as entry @xmath25 .",
    "this matrix is called the kernel matrix . a typical kernel method such as svm consists of applying a linear method in the feature space @xmath19 such that the computations only depend on pairwise inner products and thus on the kernel matrix  [ @xcite ] .",
    "we now show that the stahel  donoho outlyingness ( [ eq : stdorig ] ) can be computed in such a manner .",
    "let @xmath26 be the direction in @xmath19 through @xmath13 feature vectors @xmath27 and @xmath28 : @xmath29 the projection of a feature vector @xmath30 onto the direction @xmath26 is then @xmath31 since the squared norm of an element equals the inner product of the element with itself , we have that @xmath32 the vector @xmath33 denotes the vector with entry @xmath34 equal to @xmath35 , entry @xmath36 equal to @xmath37 and all other entries equal to @xmath38 .",
    "then @xmath39 denote @xmath40 the vector containing the projections of all feature vectors onto the direction @xmath26 through feature vectors @xmath41 and @xmath28 : @xmath42 note that only the kernel matrix @xmath23 is needed and not the explicit feature vectors @xmath27 to compute the projections @xmath40 . from these projections",
    "the stahel  donoho outlyingness of a feature vector @xmath28 in @xmath19 can be calculated as follows : @xmath43 again @xmath8 and @xmath9 are univariate robust estimators of location and scale . from this point on we always take @xmath44 note that in  ( [ eq : stdkern ] ) we have to check @xmath45 directions to find the maximum , where @xmath15 denotes the number of observations in the data set",
    ". then all directions through @xmath13 observations are considered .",
    "if @xmath15 is too large , a random subset of directions can be taken .",
    "typically a few hundred is already enough to provide a good approximation [ @xcite ] . in our implementation",
    "we use the full set if @xmath46 .",
    "otherwise we select @xmath47 directions at random .",
    "let us now turn to the typical svm setup .",
    "let @xmath48 be a data set of @xmath49 training samples in some set @xmath50 and let @xmath17 be a kernel function @xmath51 .",
    "let @xmath52 be the corresponding labels : @xmath53 if sample @xmath34 belongs to the negative group , @xmath54 if sample @xmath34 belongs to the positive group",
    ". denote by @xmath55 the number of samples with label @xmath37 and @xmath56 the number of samples with label @xmath57 .",
    "the following algorithm basically trims a fraction of the data with largest outlyingness and trains a standard svm on the remaining samples",
    ". we will refer to this algorithm as sd - svm ( sd stands for stahel  donoho ) :    1 .",
    "set @xmath58 .",
    "denote @xmath59 and @xmath60 ( @xmath61 denotes the largest integer smaller than @xmath62 ) .",
    "trimming step : consider only the inputs with group label @xmath37 . compute the stahel ",
    "donoho outlyingness for every sample in this set using  ( [ eq : stdkern ] ) .",
    "retain the @xmath63 observations with smallest outlyingness .",
    "denote this set of size @xmath63 as @xmath64 .",
    "analoguously obtain the set @xmath65 containing the @xmath66 samples with group label @xmath57 with smallest outlyingness .",
    "training step : train a standard svm on the reduced training set @xmath67 .",
    "thus solve @xmath68\\\\[-8pt ] & & \\mbox{subject to}\\qquad 0\\leq\\alpha_i\\leq c \\quad\\mbox{and}\\quad   \\sum_{x_i\\in t}\\alpha_i y_i=0.\\nonumber\\end{aligned}\\ ] ] the classifying function is given by @xmath69 to predict the group membership of a sample @xmath70 , one takes @xmath71 .    note",
    "that the computations in the training step are exactly the same as for an ordinary svm .",
    "the only difference is that the reduced set @xmath72 containing the observations with smallest outlyingness is used , in order to avoid negative effects from possible outliers .",
    "the regularization parameter @xmath73 in  ( [ eq : sdsvmalpha ] ) is sometimes set to @xmath74 as a default value .",
    "however , it is preferable to optimize the value of @xmath73 .",
    "sd - svm is of course compatible with any type of model selection strategy : it suffices to add the model selection strategy to the training step ( step 3 ) of the algorithm outlined in section  [ sect : sdsvm ] . in all the examples of this paper ,",
    "10-fold cross - validation was used to optimize @xmath73 .      to illustrate sd - svm ,",
    "consider the following simple experiment : @xmath75 samples ( negative group ) are generated , each with @xmath76 independent standard normal components .",
    "another @xmath75 samples ( positive group ) are generated with @xmath77 independent normal components with mean @xmath78 . in a second setup",
    "the same data is used with additional outliers : @xmath2 samples are added to the negative group with @xmath77 independent normal components with mean @xmath79 . to the positive group , @xmath2 samples are added with @xmath77 independent normal components with mean @xmath80 . in both situations",
    "sd - svm with a linear kernel is applied for several values of @xmath81 .",
    "the fraction of misclassifications on @xmath82 newly generated test data is computed .",
    "figure  [ fig : sim ] shows boxplots over @xmath83 simulation runs .    .",
    "]    in the case without outliers the number of misclassifications increases as @xmath84 decreases .",
    "this is quite expected since a lower @xmath84 means more trimming , which is unnecessary in this case since all samples are nicely generated from two gaussian distributions .",
    "thus , it is no surprise that a classical svm ( @xmath85 ) performs best .",
    "however , a relatively small amount of outliers ( 8 out of 58 ) changes things completely ( right - hand side of figure  [ fig : sim ] ) .",
    "a classical svm ( @xmath85 ) is no better than guessing anymore ( more than 50@xmath86 misclassifications ) .",
    "sd - svm with @xmath87 is not good enough either , since the trimming percentage is still smaller than the percentage of outliers .",
    "only if @xmath84 is chosen small enough , good performance is obtained .",
    "thus , a small @xmath84 provides protection against outliers at the cost of a slightly worse classification performance at uncontaminated data . for the outlier map it is most important to avoid the huge effects of outliers , whereas the small effect of unnecessary trimming is practically invisible .",
    "therefore , a default choice of @xmath88 turns out to be a good choice for the construction of the outlier map , and we retain this choice throughout the remainder of the paper .",
    "the following visualization is proposed :    1 .",
    "make a scatterplot of the outlyingness and the value of the classifier  @xmath89 .",
    "thus , for @xmath90 , plot pairs @xmath91 where @xmath92 is the stahel  donoho outlyingness of sample @xmath36 computed in the trimming step of the algorithm and @xmath93 can be calculated from ( [ eq : sdsvmf ] ) .",
    "2 .   plot the inputs with group labels @xmath57 as circles and those with group labels @xmath37 as crosses .",
    "add a solid vertical line at horizontal coordinate  @xmath38 .",
    "consider a simple example in @xmath13 dimensions as follows : @xmath94 observations are generated from a bivariate gaussian distribution with mean @xmath95 and identity covariance matrix .",
    "they have group label @xmath37 .",
    "thirty observations are generated from a bivariate gaussian distribution with mean @xmath96 and identity covariance matrix .",
    "they receive group label @xmath57 .",
    "apart from these @xmath97 observations , @xmath98 more are added , representing several types of outliers : @xmath79 data points ( denoted 6163 ) are placed around position @xmath99 with label @xmath57 .",
    "two observations ( denoted 64 and 65 ) with label @xmath57 are placed around @xmath100 .",
    "one point ( denoted @xmath101 ) is placed at position @xmath95 with label @xmath57 . a two - dimensional view of the data is given in figure  [ fig : fig1](a ) .",
    "the solid line represents the sd - svm classification boundary with a linear kernel . despite the @xmath98 outliers in the data , sd - svm still manages to separate both groups quite nicely .    c    -dimensional classification problem .",
    "the solid line is the sd - svm classifying line .",
    "corresponding outlier map visualizing the two main groups and the different types of outliers.,title=\"fig : \" ] + ( a ) + -dimensional classification problem .",
    "the solid line is the sd - svm classifying line .",
    "corresponding outlier map visualizing the two main groups and the different types of outliers.,title=\"fig : \" ] + ( b ) +    figure  [ fig : fig1](b ) shows the corresponding outlier map . on the vertical axis one reads the stahel ",
    "donoho outlyingness .",
    "observations @xmath102 and @xmath103 are positioned in the center of their respective group .",
    "their outlyingness is indeed small .",
    "observations further away from the group center have a larger outlyingness , for example , @xmath104 , @xmath79 and @xmath105 . on the horizontal axis the value of the classifying function @xmath89 as in  ( [ eq : sdsvmf ] ) can be read .",
    "the sign of this function determines the predicted group labels .",
    "the vertical line at @xmath106 divides the plot in two parts : every point left of the line is classified into the negative group by sd - svm and every point on the right is classified into the positive group .",
    "we can now see , for instance , that observation @xmath101 is a misclassification : it belongs to the positive group , but receives group label @xmath37 since it lies on the left of the vertical line in figure  [ fig : fig1](a ) .",
    "the absolute value of the @xmath1-coordinate in the diagnostic plot represents a distance to the classification boundary . in figure",
    "[ fig : fig1](a ) it can be seen , for example , that observations @xmath79 and @xmath104 are almost equally distant from the negative group center , but observation @xmath79 is much closer to the classification line .",
    "this information can be found in the outlier map in figure  [ fig : fig1](b ) as well , since both have almost the same outlyingness ( vertical axis ) , but @xmath79 is much closer to the vertical line than sample @xmath104 ( horizontal axis ) .",
    "the outliers in the data can be detected and characterized too .",
    "observations 6163 are outlying with respect to the other data points in their group , which is clearly indicated by their large outlyingness",
    ". however , both samples still follow the classification rule .",
    "indeed , both are lying on the right side in figure  [ fig : fig1](b ) .",
    "samples 64 and 65 , on the other hand , are outlying with respect to the other observations in their group as well as with respect to the classification line : their outlyingness is large and the value of the classification function is negative , although it should have been positive to obtain a correct classification . finally consider observation @xmath101 .",
    "its not extremely outlying with respect to the other data points in the positive group .",
    "however , taking the negative group and the classification line into account , it seems to share more characteristics with the negative group than with its own positive group colleagues .",
    "in the outlier map this is revealed by a moderate outlyingness and by its position almost in the middle of the left side of the vertical line .",
    "the first example considers a data set by@xcite .",
    "the data consist of microarrays from 128 different individuals with acute lymphoblastic leukemia ( all ) , publicly available in the all package in the software environment r. the number of gene expressions at each individual equals @xmath107 .",
    "there are @xmath108 adult patients with t - cell all and @xmath109 with b - cell all .",
    "figure  [ fig : leuk ] presents the outlier map for svm with a linear kernel applied to this data set .",
    "it turns out that the data is well classified and that there are no samples with a very large outlyingness .",
    "both t - cell and b - cell form homogeneous groups as one would like when applying a linear svm .",
    "thus , the outlier map immediately shows that the data is clean and one can safely proceed analysis without worrying about outliers .",
    "the breast cancer data set from @xcite contains @xmath110 tumor samples that are either positive ( er@xmath111 ) or negative ( er@xmath112 ) to estrogen receptor .",
    "the expression levels of @xmath113 genes are given for each sample .",
    "for a linear kernel the corresponding outlier map is shown in figure  [ fig : breast](a ) .",
    "samples @xmath114 , @xmath115 and @xmath116 immediately catch the eye .",
    "their outlyingness is unusually large . in  @xcite samples @xmath114 and",
    "@xmath115 were already rejected and taken out of the analysis due to failed array hybridization .",
    "also sample @xmath116 was characterized as unusual .",
    "it was the only sample in the er@xmath111 group for which the out of sample prediction was highly unreliable in the analysis performed by  @xcite .",
    "the samples @xmath103 and @xmath108 attract attention as well .",
    "they have a large outlyingness and both are clearly misclassified .",
    "it turns out that for this data the group membership er@xmath111 or er@xmath112 was determined not only by immunohistochemistry at time of diagnosis , but also by later immunoblotting . for samples @xmath108 and @xmath103 both methods returned different results .",
    "@xcite show via statistical analysis that the initial labeling er@xmath111 for @xmath108 and er@xmath112 for @xmath103 is probably wrong and that the immunoblotting results are more appropriate .",
    "this is clearly confirmed by the outlier map .",
    "it is worth noting that the same data set was analyzed in@xcite , where a comparison was made between a proposed stability criterion , a simple leave - one - out criterion and the algorithm from  @xcite .",
    "however , none of these methods was able to detect the @xmath105 clear outliers discussed so far .",
    "five more suspicious samples were indicated in  @xcite : @xmath104 , @xmath117 , @xmath118 , @xmath119 and @xmath120 . in figure",
    "[ fig : breast](b ) these samples are shown on a zoom - in from the full outlier map into the region @xmath121 on the vertical axis . except for @xmath104 , these samples are suspicious in the sense that they are not confidently classified , since the value of the classifying function is close to @xmath38 .",
    "it is no surprise that these samples are found by the algorithms compared in  @xcite , since those methods are designed to detect potentially mislabeled samples .",
    "also note that some of these mislabeling detection algorithms pointed out samples @xmath122 and @xmath123 as suspicious , although these samples were not considered in  @xcite . from the outlier map",
    "it can be seen that @xmath122 and @xmath123 are indeed wrongly classified by sd - svm .",
    "c     are outlying but well classified .",
    "samples @xmath108 and @xmath103 are slightly outlying with respect to their groups , but are clearly wrongly classified .",
    "this suggests that they are mislabeled rather than erroneous , confirming the original analysis by west et al .",
    "same plot , but zoomed - in at the region @xmath121 on the vertical axis for better visibility .",
    "observations flagged by algorithms searching for mislabelings are shown @xmath124.,title=\"fig : \" ] + ( a ) +   are outlying but well classified . samples",
    "@xmath108 and @xmath103 are slightly outlying with respect to their groups , but are clearly wrongly classified .",
    "this suggests that they are mislabeled rather than erroneous , confirming the original analysis by west et al .",
    "same plot , but zoomed - in at the region @xmath121 on the vertical axis for better visibility .",
    "observations flagged by algorithms searching for mislabelings are shown @xmath124.,title=\"fig : \" ] + ( b ) +              the colon cancer data set from  @xcite contains @xmath47 gene expression levels for @xmath118 tumor samples and @xmath125 normal samples .",
    "the outlier map with a linear kernel is shown in figure  [ fig : alon ] . in the tumor group t2 ,",
    "t33 , t36 and t30 are misclassified .",
    "sample t37 is classified correctly , but with low confidence : it is very close to the classification boundary . in the normal group n8 and especially n34 and n36 are the suspicious cases that behave differently from the other normal samples .",
    "the 8 aforementioned samples plus sample n12 were identified as possible outliers in the original paper by  @xcite for biological reasons . thus , 8 out of 9 true outliers can be identified on the outlier map , only leaving n12 undetected . however , in  @xcite none of the methods that were compared could detect n12 . moreover , the stability criterion proposed by malossini et al . was unable to detect t37 and n8 too and incorrectly pointed at n2 and n28 as possibly suspicious samples .",
    "also note the interesting sample t6 . from the outlier map",
    "we see that this sample is classified correctly and with much confidence .",
    "nevertheless , its outlyingness with respect to the other tumor samples is rather large .",
    "this means that t6 behaves quite differently than the other tumor samples , but without distorting the classification . in malossini",
    "most of the methods analyzed did not detect t6 at all .",
    "again this is no surprise since methods such as the stability criterion of malossini et al .",
    "specifically focus on mislabeled observations , whereas t6 is certainly not mislabeled .",
    "only the outlier detection method of  @xcite is able to detect t6 , but does rather poorly on the other samples detecting only 5 out of 9 true outliers .",
    "the protein data set taken from  @xcite contains @xmath126 protein sequences of the essentially ubiquitous glycolytic enzyme 3-phosphoglycerate kinase ( 3-pgk ) in three domains : archaea , bacteria and eukaryota . the data set is available in the protein classification benchmark collection at http://net.icgeb.org ( accession number pcb00015 ) .",
    "we consider here classification task number 10 where the positive group consists of 35 eukaryota .",
    "the negative group consists of 4 archaea and 40 bacteria . to classify these two groups of protein sequences , we use svm with the local alignment kernel  [ @xcite ] .",
    "default parameter values were used : gap opening penalty @xmath127 11 , gap extension penalty @xmath127 1 , scaling parameter @xmath127 0.5 .",
    "the outlier map is shown in figure  [ fig : protein ] .",
    "one observes that the positive group of eukaryota is very heterogeneous as several clusters appear .",
    "these clusters all have a biological interpretation , as the group of eukaryota contains several subgroups of different phyla .",
    "for instance , observations 2931 are from the phylum of alveolata",
    ". samples 1317 are the euglenozoa . note that 18 ( named q8srz8 ) , which belongs to the fungi , was clustered in the group of euglenozoa by pollack et al . ;",
    "this is actually confirmed by the outlier map .",
    "finally , samples 33 and 34 are outlying with respect to the positive group .",
    "they form , together with 32 , the group of stramenopiles .",
    "note that the different behavior of sample 32 from its fellow stramenopiles is again a confirmation of the analysis by pollack et al .",
    ": their clustering method assigned 32 ( named q8h721 ) in the main group of eukaryota metazoa .",
    "also , in the outlier map 32 is situated in the main group , whereas 33 and 34 form a separate cluster . in the positive group",
    "the heterogeneity is less clear , although the 4 archaea ( 3639 ) do have the largest outlyingness compared to the other samples which are all bacteria .",
    "an outlier map is proposed for support vector machine classification .",
    "if the outlier map shows two homogeneous and well classified groups , one can safely proceed analysis without worrying about outliers .",
    "however , in some situations this may not be the case and the outlier map can be a simple and useful tool to detect this . moreover , the outlier map can be drawn for any choice of kernel , including rather exotic ones such as used in protein analysis .",
    "it can also be helpful to gain insight in the type of outliers , for example , whether outliers are mislabeled observations or not , or whether the outliers are isolated errors or rather a small subgroup of the group structure considered .",
    "this is important to know how to proceed analysis .",
    "if the outliers are truly erroneous observations , one should not take them into account to build a classifier , and one can manually discard them from the data set or apply a robust classifier . if the outliers are mislabeled observations , one probably should re - examine the labeling and change the label of the outlier if this seems indeed appropriate . if the outliers form a small subgroup of the data , one might reconsider the use of a binary classifier and turn to a more appropriate modeling technique . in any event",
    ", the outlier map can be helpful for practitioners of svm classification to make such decisions .",
    "alon , u. , barkai , n. , notterman , d. a. , gish , k. , ybarra , s. , mack , d. and levine , a. j. ( 1999 ) .",
    "broad patterns of gene expression revealed by clustering of tumor and normal colon tissues probed by oligonucleotide arrays .",
    "sci . _ * 96 * 64756750 .",
    "chiaretti , s. , li , x. , gentleman , r. , vitale , a. , vignetti , m. , mandelli , f. , ritz , j. and foa , r. ( 2004 ) . gene expression profile of adult t - cell acute lymphocytic leukemia identifies distinct subsets of patients with different response to therapy and survival . _ blood _ * 103 * 27712778 .",
    "furey , t. s. , cristianini , n. , duffy , d. , bednarski , w. , schummer , m. and haussler ,  d. ( 2000 ) .",
    "support vector machine classification and validation of cancer tissue samples using microarray expression data",
    ". _ bioinformatics _ * 16 * 906914 .",
    "kadota , k. , tominaga , d. , akiyama , y. and takahashi , k. ( 2003 ) . detecting outlying samples in microarray data : a critical assessment of the effect of outliers on sample classification .",
    "_ chem - bio .",
    "j. _ * 3 * 3045 .",
    "leslie , c. , eskin , e. and noble , w. s. ( 2002 ) .",
    "the spectrum kernel : a string kernel for svm protein classification . in _ proceedings of the pacific symposium on biocomputing 2002 _",
    "( r. b. altman , a. k. dunker , l. hunter , k. lauerdale and t. e. klein , eds . ) 564575 .",
    "world scientific , hackensack , nj .    leslie , c. , eskin , e. , weston , j. and noble , w. s. ( 2003 ) .",
    "mismatch string kernels for svm protein classification . in",
    "_ advances in neural information processing systems _ ( s. becker , s. thrun and k. obermayer , eds . ) * 15 * 14411448 . mit press , cambridge , ma .",
    "li , l. , darden , t. a. , weinberg , c. r. , levine , a. j. and pedersen , l. g. ( 2001 ) .",
    "gene assessment and sample classification for gene expression data using a genetic algorithm / k - nearest neighbor method . _",
    "high throughput screen . _ * 4 * 727739 .",
    "liao , l. and noble , w. s. ( 2002 ) . combining pairwise sequence similarity and support vector machines for remote protein homology detection . in",
    "_ proceedings of the sixth international conference on computational molecular biology _ ( t. lengauer , ed . ) 225232 .",
    "acm press , new york .",
    "pochet , n. , de smet , f. , suykens , j. a. k. and de moor , b. ( 2004 ) .",
    "systematic benchmarking of microarray data classification : assessing the role of nonlinearity and dimensionality reduction .",
    "_ bioinformatics _ * 20 * 31853195",
    ".    pollack , j. d. , li , q. and pearl , d. k. ( 2005 ) .",
    "taxonomic utility of a phylogenetic analysis of phosphoglycerate kinase proteins of archaea , bacteria and eukaryota : insights by bayesian analyses .",
    "evol . _ * 35 * 420430 .                west , m. , blanchette , c. , dressman , h. , huang , e. , ishida , s. , spang , r. , zuzan , h. , marks , j. r. and nevins , j. r. ( 2001 ) . predicting the clinical status of human breast cancer by using gene expression profiles .",
    "_ * 98 * 1146211467 ."
  ],
  "abstract_text": [
    "<S> support vector machines are a widely used classification technique . they are computationally efficient and provide excellent predictions even for high - dimensional data . moreover , support vector machines are very flexible due to the incorporation of kernel functions . </S>",
    "<S> the latter allow to model nonlinearity , but also to deal with nonnumerical data such as protein strings . </S>",
    "<S> however , support vector machines can suffer a lot from unclean data containing , for example , outliers or mislabeled observations . </S>",
    "<S> although several outlier detection schemes have been proposed in the literature , the selection of outliers versus nonoutliers is often rather ad hoc and does not provide much insight in the data . in robust multivariate statistics outlier maps are quite popular tools to assess the quality of data under consideration . </S>",
    "<S> they provide a visual representation of the data depicting several types of outliers . </S>",
    "<S> this paper proposes an outlier map designed for support vector machine classification . </S>",
    "<S> the stahel  donoho outlyingness measure from multivariate statistics is extended to an arbitrary kernel space . a trimmed version of support vector machines is defined trimming part of the samples with largest outlyingness . based on this classifier , </S>",
    "<S> an outlier map is constructed visualizing data in any type of high - dimensional kernel space . </S>",
    "<S> the outlier map is illustrated on 4 biological examples showing its use in exploratory data analysis .    . </S>"
  ]
}