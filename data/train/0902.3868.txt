{
  "article_text": [
    "among many different ways of testing models of cosmological sources , especially quasars , one is through the investigations of the distributions , ranges and more importantly the correlations among the relevant physical characteristics , such as luminosity , spectra , redshifts or cosmological distances .",
    "the impossibility of direct measurements to quasi stellar objects prevents one to validate any direct relationship between distance and redshift where , the measurable quantities are the apparent magnitude @xmath0 , redshift @xmath1 which , as related to luminosity function or , even the probability distribution of absolute magnitude @xmath2 , as predicted by a given cosmology and the angular diameter of the object .",
    "the recent increase of the computing power led the theorists to simulate the realistic physical situations in specific details and predictions which are beyond the present limit of experimental techniques .",
    "for example , in the case of n - body simulations , it is possible to answer _ given certain initial conditions at some time @xmath3 and some assumed laws of physics , what will be the state of the system at later time @xmath4 _ hockney(1988 ) ? however , it is harder to solve the _ inverse problem _ aster(2004 ) : given all of the data , what can be said about the laws of physics that have been operating brewer(2008 ) ?",
    "various cosmological models are considered to understand the formation and evolution of the universe . in cosmology , for a given set of data",
    ", there exists many possible explanations .",
    "a typical observation may rule out some theories but may be consistent with some others .",
    "again , many specific techniques have been constructed to tackle each inverse problem seperately .",
    "it is worth mentioning that efron(1992 ) , efron(1999 ) considered different types of statistical arguments and tests on the truncated data gathered by astronomers to extract important statistical characteristics .",
    "one of the present authors ( sr ) along with his collaborators roy(2007 ) used the non - parametric methods of efron(1992 ) and efron(1999 ) to study the bivariate distribution of two physically important parameters i.e. redshift and apparent magnitude observed in sdss quasar survey ( 2005 ) .",
    "the data is truncated in nature .",
    "however , the data in sdss quasar survey of 2007 is no longer truncated . here , we will discuss a general framework using concepts of bayesian mixture models and dirichlet process to study the existence of clusterings in the quasar sample for the whole range of redshifts and the changepoints associated to the non - linearity of the fitting curve .",
    "the existence of non - linearity can be associated with the certain physical factors like evolution and presence of different environments around quasars at high redshift .",
    "this will shed new light on the present cosmological debates regarding the concordant redshift , age of the universe and acceleration / deceleration parameters .    on the statistical methodological side",
    ", we adopt a very fast and efficient method for learning about posteriors associated with bayesian mixture models with unknown number of components . in particular , we adopt a semiparametric bayesian curve - fitting procedure based on our mixture model . we demonstrate that our methods are particularly suitable for application in massive data , as our present cosmological data .",
    "we also adopt a methodology for analysing the posterior distribution of the clusterings associated with our bayesian mixture model with unknown number of components .",
    "our methods are broadly based on the works of bhattacharya(2008 ) and bhattacharya et al(2008 ) ; however , important extensions to modeling multivariate data are described here .",
    "perhaps more importantly , we demonstrate in this paper that cutting - edge research works of great scientific importance are possible with our methodologies , despite the enormity of the size of the data sets . to our knowledge ,",
    "such advancement in the bayesian semiparametric / clustering paradigm has not been possible before .",
    "the rest of the paper is structured as follows . in section [ sec : overview ]",
    "we explain the data and in this connection , provide a brief overview of bayesian mixture models with unknown number of components .",
    "our bayesian semiparametric curve fitting method in ( massive ) data sets consisting of multivariate observations is introduced in section [ sec : mult_extension ] . a gibbs sampling algorithm to simulate from the associated posterior distributions",
    "is derived in section [ sec : gibbs_sampling ] . in section [ sec : simstudy_curve ] we illustrate our curve - fitting methodology with a simulated data set , and application to the real cosmological data is considered in section [ sec : real_application ] .",
    "discussion on summarization of the posterior distribution of clusterings is provided in section [ sec : clustering ] , and application of the clustering ideas to the real cosmological data set is considered in section [ sec : clustering_application ] .",
    "the implications of our analysis of the cosmological data set , and related future work , are enlisted in section [ sec : implications ] .",
    "our massive cosmological data set , consists of 96307 data points on logarithm of redshift ( @xmath5 ) and apparent magnitude ( @xmath6 ) for qsasars ( qsasi - stellar objects ) collected from sdss data .",
    "the data set does not reveal any clear - cut parametric relationship between the two variables of interest ; moreover , our exploratory analyses clearly indicated that the ( bivariate ) normality assumption does not hold for the data .",
    "indeed , our quantile - quantile plots of each of the two variables showed that the marginal distributions of both the variables are far from univariate normal .",
    "to resolve this problem we will use idea of mixture models , which are noted for their flexibility . indeed , as noted by dalal(1983 ) and diaconis(1985 ) , mixture models composed of standard densities can , in principle , approximate any underlying distribution . for more on mixture models , see titterington(1985 ) , mclachlan(1988 ) .",
    "however , a technical problem associated with classical analysis of mixture models is associated with the number of mixture components included in the model . in the classical statistical literature there",
    "does not seem to exist any rigorously procedure of selecting an adequate number of mixture components .",
    "on the other hand , the bayesian paradigm offers elegant solutions to this problem . among the contributions of bayesians in this topic ,",
    "notable are those of escobar(1995 ) ( henceforth , ew ) and richardson(1997 ) ( henceforth , rg ) . the former use dirichlet process to indirectly induce ( random ) variability in the number of components ,",
    "while the approach of rg directly acknowledges uncertainty about the number of components and puts a prior distribution on the same , thus rendering the problem variable - dimensional .",
    "the methodology of rg relies on reversible jump markov chain monte carlo ( rjmcmc ) green(1995 ) for drawing inference .    however , it is important to note that the rjmcmc method proposed by rg is quite complicated , and is error prone .",
    "but of more concern is the fact that their methodology is extremely sensitive to the  move types \" selected , and since there are no general guidelines for selecting optimum move types , the algorithm could be very inefficient .",
    "moreover , for variable - dimensional problems diagnosis of convergence of rjmcmc is a serious problem .",
    "the aforementioned problems asociated with the rjmcmc method are of course many times aggravated for multivariate observations .",
    "the methodology of ew is not a variable dimensional problem and straightforward gibbs sampling methods are available , however , the number of parameters increases with data size , making gibbs sampling ( or any other sampling methods ) infeasible for massive data sets . in response to this computational challenge wang(2008 ) have proposed the sequential updating and greedy search ( sugs ) algorithm which proceeds by cyling through the data points , sequentially allocating them to the cluster that maximizes the conditional posterior allocation probability .",
    "the conditional distribution of the unknown parameter , which admits a closed form expression given the maximizing cluster , is then updated . a complete sweep of the algorithm yields the conditional posterior distribution of all the parameters , given the seuqentially optimal clusterings .",
    "the advantage of the method of wang(2008 ) is that it is quite fast , since it does not rely upon mcmc methods .",
    "the disadvantages are that the method does not have a theoretical basis , in that the correct joint or marginal posterior distributions of the parameters or clusterings are not obtained . moreover , although the algorithm produces a sequentially optimal clustering , it does not yield a global maximum _ a posteriori _ ( map ) estimate .",
    "the algorithm depends upon the the order in which we consider the observations . in case of large data set this problem",
    "is tackled by considering a few random ordering of the observations and then using pseudo - marginal likelihood ( pml ) , which makes this method an ad hoc one .",
    "perhaps more critically , the algorithm does not assist in any way in obtaining and studying the probability distribution of the clusterings .",
    "we avoid all the difficulties noted above by adopting a model which may be viewed as a reconciliation of the methods of ew and rg .",
    "the details are outlined next .",
    "the cosmological data set of our interest is , as already noted above , consists of bivariate observations . as a result ,",
    "the model and the methodologies proposed by bhattacharya(2008 ) warrants extension to bivariate , in fact , more generally , to multivariate situations . for the sake of full generality",
    ", we extend the proposals of bhattacharya(2008 ) to the case of @xmath7-dimensional observations , where @xmath8 .",
    "we assume for @xmath9 , data set @xmath10 is available , where observation @xmath11 can be modeled as a mixture of @xmath7-variate normal distributions , having @xmath12 components .",
    "crucially , @xmath12 is assumed to be unknown . rather than assuming a prior distribution on @xmath12 like rg and treating the problem as variable dimensional , we assume the following form of mixture representation of the @xmath7-variate observation @xmath11 : @xmath13=\\frac{1}{m}\\sum_{j=1}^m\\frac{|{\\mbox{\\boldmath $ \\lambda$}}_j|^{\\frac{1}{2}}}{(2\\pi)^{\\frac{d}{2}}}\\exp\\left\\{-\\frac{1}{2}\\left({\\textbf{y}}_i-{\\mbox{\\boldmath $ \\mu$}}_j\\right)'{\\mbox{\\boldmath $ \\lambda$}}_j\\left({\\textbf{y}}_i-{\\mbox{\\boldmath $ \\mu$}}_j\\right)\\right\\ } \\label{eq : mult_normixture}\\ ] ] in the above",
    ", @xmath14 is the maximum number of components the mixture can possibly have , and is known ; @xmath15 , with @xmath16 .",
    "we further assume that @xmath17 are samples drawn from a dirichlet process ( see , for example , ferguson(1973 ) , ew ) +    @xmath18 is iid from * g * + * g * is from @xmath19    a crucial feature of our modelling style concerns the discreteness of the prior distribution @xmath20 , given the assumption of dirichlet process ; that is , under these assumptions , the parameters @xmath21 are coincident with positive probability .",
    "in fact , this is the property that can be exploited to show that ( [ eq : mult_normixture ] ) boils down to the form @xmath13=\\sum_{j=1}^p\\pi_j\\frac{|{\\mbox{\\boldmath $ \\lambda$}}^*_j|^{\\frac{1}{2}}}{(2\\pi)^{\\frac{d}{2}}}\\exp\\left\\{-\\frac{1}{2}\\left({\\textbf{y}}_i-{\\mbox{\\boldmath $ \\mu$}}^*_j\\right)'{\\mbox{\\boldmath $ \\lambda$}}^*_j\\left({\\textbf{y}}_i-{\\mbox{\\boldmath $ \\mu$}}^*_j\\right)\\right\\ } \\label{eq : mult_normixture2}\\ ] ] where @xmath22 are @xmath12 distinct components in @xmath17 with @xmath23 occuring @xmath24 times , and @xmath25 .",
    "hence , although our model is actually variable dimensional , this is induced through the dirichlet process prior , and does not involve complexities as in rjmcmc .",
    "in fact , we will derive an easily implementable gibbs sampling algorithm , even for highly multivariate observaions .",
    "observe that , in sharp contrast to the proposed model of ew , the number of parameters to be simulated remains fixed ( since the maximum number of mixture components is fixed ) , even though the number of observations , @xmath26 , could be extremely large .",
    "associated with the mixture model ( [ eq : mult_normixture ] ) is the idea of bayesian curve - fitting .",
    "this we illustrate in the next section .      in simplified notation",
    ", we write ( [ eq : mult_normixture ] ) as @xmath27=\\frac{1}{m}\\sum_{j=1}^mn_d\\left({\\textbf{y}}:{\\mbox{\\boldmath $ \\mu$}}_j,{\\mbox{\\boldmath $ \\lambda$}}^{-1}_j\\right ) \\label{eq : mult_normixture3}\\ ] ] it follows that the conditional distribution of @xmath28 given @xmath29 is given by @xmath30= \\frac{1}{m}\\sum_{j=1}^mn_{d-1}\\left({\\textbf{y}}_{-1}:{\\mbox{\\boldmath $ \\mu$}}_{-1j},{\\mbox{\\boldmath $ \\lambda$}}^{-1}_{-1j}\\right ) \\times n\\left(y_{1}:\\mu^{(j)}_{1|2,\\ldots , d},\\lambda^{(j)}_{1|2,\\ldots , d}\\right ) \\label{eq : mult_normixture4}\\ ] ] where @xmath31 and @xmath32 are , respectively , the univariate conditional mean @xmath33 and the inverse precision @xmath34 under the assumption that * y * is from @xmath35 . the @xmath36 dimensional parameters @xmath37 stand for @xmath38 but without the first component .    as a result , assuming @xmath39 distinct components @xmath40 in @xmath17 , and assuming further that each distinct component @xmath23 occurs @xmath24 times , we have , @xmath41= \\sum_{j=1}^k w^{(j)}({\\textbf{y}}_{-1})\\mu^{(j)}_{1|2,\\ldots , d } \\label{eq : regression_estimation}\\ ] ] is a weighted sum of the component regression functions @xmath31 , where the associated weight @xmath42 is given by +    @xmath42 is proportional to +    @xmath43    and the proportionality constant is chosen such that @xmath44 .",
    "note that the regression function estimator developed above is structurally quite different from that given by muller(1996 ) , who develop a regression estimator based on the model of ew .",
    "one clear advantage of our curve over that of muller(1996 ) is that for massive data sets the curve - fitting idea of muller(1996 ) can not be implemented due to extreme computational burden , while our curve ( [ eq : regression_estimation ] ) can be easily fitted to any data set , massive or not .",
    "assuming that a sample @xmath45 is available from the posterior distribution of @xmath17 ( typically by mcmc ) , the marginalized curve @xmath46 is estimated as @xmath47 pointwise variability of the curve is measured by @xmath48 the first component of the above variance is estimated by the sample variance of @xmath49 , and the second component is estimated by the sample mean of @xmath50 .",
    "approximate @xmath51 pointwise credible intervals of the curve are given by @xmath52 , where @xmath53 is the @xmath54-th quantile of a standard normal distribution .",
    "hence , once the mcmc realizations @xmath45 are available , it is an easy task to obtain a bayesian regression curve with all summaries readily available . in the next section",
    "we derive an extremely fast and easily implementable gibbs sampling algorithm .",
    "we assume that under @xmath55 , +    @xmath56 $ ] is from + @xmath57 @xmath58 $ ] is from + @xmath59    hence , the joint distribution of @xmath18 is given by @xmath60[{\\mbox{\\boldmath $ \\mu$}}_j\\mid{\\mbox{\\boldmath $ \\lambda$}}_j]&=&c|\\lambda_j|^{\\frac{s - d-1}{2}}\\exp\\left\\{-tr\\left(\\frac{{\\textbf{s}}\\lambda_j}{2}\\right)\\right\\}\\nonumber\\\\ & \\times&\\frac{|{\\mbox{\\boldmath $ \\lambda$}}_j|^{\\frac{1}{2}}}{(2\\pi\\psi)^{\\frac{d}{2}}}\\exp\\left\\{-\\frac{1}{2}({\\mbox{\\boldmath $ \\mu$}}_j-{\\mbox{\\boldmath $ \\mu$}}_0)'\\lambda_j({\\mbox{\\boldmath $ \\mu$}}_j-{\\mbox{\\boldmath $ \\mu$}}_0)\\right\\ } \\label{eq : niw}\\end{aligned}\\ ] ] where @xmath61      the distribution of @xmath62 $ ] given by ( [ eq : mult_normixture ] ) can be represented by introducing the allocation variables @xmath63 , as follows :    for @xmath9 and @xmath64 , @xmath65&=&\\frac{|{\\mbox{\\boldmath",
    "$ \\lambda$}}_j|^{\\frac{1}{2}}}{(2\\pi)^{\\frac{d}{2}}}\\exp\\left\\{-\\frac{1}{2}\\left({\\textbf{y}}_i-{\\mbox{\\boldmath $ \\mu$}}_j\\right)'{\\mbox{\\boldmath $ \\lambda$}}_j\\left({\\textbf{y}}_i-{\\mbox{\\boldmath $ \\mu$}}_j\\right)\\right\\ }   \\label{eq : y_given_z}\\\\   \\left[z_i = j\\right]&=&\\frac{1}{m }   \\label{eq : latent_z }   \\end{aligned}\\ ] ]    it follows that the full conditional distribution of the allocation variables @xmath63 @xmath66 given the rest is given by    @xmath67 $ ] is proportional to + @xmath68      defining @xmath69 and @xmath70 , we note that the full conditional distribution of @xmath18 given the rest is given by @xmath71=q_{0j}g_j({\\mbox{\\boldmath $ \\theta$}}_j)+\\sum_{\\ell=1,\\ell\\neq j}^mq_{\\ell j}\\delta_{{\\mbox{\\boldmath $ \\theta$}}_{\\ell}}({\\mbox{\\boldmath $ \\theta$}}_j ) \\label{eq : mult_full_cond_theta}\\ ] ] under @xmath72 the distribution of @xmath18 is given by : @xmath73 & from & wishart_d\\left(\\frac{s+n_j}{2},\\frac{1}{2}\\left\\{{\\textbf{s}}+\\frac{n_j(\\bar { \\textbf{y}}_j-{\\mbox{\\boldmath $ \\mu$}}_0)(\\bar{\\textbf{y}}_j-{\\mbox{\\boldmath $ \\mu$}}_0)'}{n_j\\psi+1}+\\sum_{i : z_i = j}({\\textbf{y}}_i-\\bar { \\textbf{y}}_j)({\\textbf{y}}_i-\\bar{\\textbf{y}}_j)'\\right\\}\\right)\\nonumber\\\\ \\label{eq : blambda_base_posterior}\\\\ \\left[{\\mbox{\\boldmath $ \\mu$}}_j\\mid{\\mbox{\\boldmath $ \\lambda$}}_j\\right ] & from & n_d\\left(\\frac{n_j\\bar { \\textbf{y}}_j\\psi+{\\mbox{\\boldmath $ \\mu$}}_0}{n_j\\psi+1},\\frac{\\psi{\\mbox{\\boldmath $ \\lambda$}}^{-1}_j}{(n_j\\psi+1)}\\right)\\label{eq : bmu_base_posterior}\\end{aligned}\\ ] ] in ( [ eq : mult_full_cond_theta ] ) @xmath74 and @xmath75 are given by the following :    @xmath74 is proportional to @xmath76 , where + @xmath77 and , @xmath75 is proportional to + @xmath78\\nonumber\\\\\\label{eq : mult_ql }        \\end{aligned}\\ ] ] the proportionality constant is chosen such that @xmath79 .",
    "it is useful to provide the intuition behind updating the allocation variables @xmath80 and the parameters @xmath17 .",
    "given @xmath81 distinct values of the parameter vector @xmath17 , the allocation vector @xmath80 clusters the @xmath26-dimensional observation vector @xmath82 into @xmath83 clusters of the form @xmath84 ; @xmath85 .",
    "these can be thought of as the _ initial clusters _ , since the dirichlet process prior acts upon @xmath86 , to yield @xmath87 distinct parameter values @xmath40 out of the possible @xmath88 distinct values to yield the final clustering , say , @xmath89 , of @xmath90 , with @xmath91 .",
    "the clusters @xmath92 are associated with the configuration vector @xmath93 . clearly , the clustering @xmath89 is coarser than @xmath90 in the sense that the former consists of lesser number of blocks with more elements in each block .",
    "we note a computational advantage our method over the rjmcmc algorithm of rg .",
    "note that empty components are naturally handled in our method ; indeed , if the @xmath94-th component is an empty component ( which can happen if the allocation variables do not allocate any observation to the @xmath94-th component ) , then the fact @xmath95 occurs naturally in our model and no special care is necessary for validation of this step .",
    "but this situation requires an extra , careful , and complicated step in the method of rg .",
    "this remains same as in the univariate case , which is given , for the prior @xmath97 on @xmath96 , given the number of distinct components @xmath39 , and another continuous random variable @xmath98 , by @xmath99 & from & \\pi_{\\eta}gamma(a_{\\alpha}+k , b_{\\alpha}-\\log(\\eta))\\nonumber\\\\        & + & ( 1-\\pi_{\\eta})gamma(a_{\\alpha}+k-1,b_{\\alpha}-\\log(\\eta ) )        \\label{eq : full_cond_alpha }        \\end{aligned}\\ ] ] where @xmath100    the full conditional distribution of @xmath98 given the rest is @xmath101 , that is , a @xmath102 distribution with mean @xmath103 .",
    "the distributions of the hyperparameters @xmath104 and @xmath105 are given by : the distribution of @xmath106 $ ] is @xmath7-variate normal , with mean vector and dispersion matrix given by : @xmath107&=&\\left(\\psi{\\bf i}+{\\bf a}\\sum_{j=1}^k{\\mbox{\\boldmath $ \\lambda$}}^*_j\\right)^{-1}\\left(\\psi { \\bf a}+{\\bf",
    "a}\\sum_{j=1}^k{\\mbox{\\boldmath $ \\lambda$}}^*_j{\\mbox{\\boldmath $ \\mu$}}^*_j\\right)\\label{eq : e_mult_full_cond_mu0}\\\\        v\\left[{\\mbox{\\boldmath $ \\mu$}}_0\\mid { \\textbf{y}},{\\textbf{z}},{\\mbox{\\boldmath $ \\theta$}}_m,\\psi\\right]&= & \\left(\\psi{\\bf i}+{\\bf a}\\sum_{j=1}^k{\\mbox{\\boldmath $ \\lambda$}}^*_j\\right)^{-1}{\\bf a}\\psi \\label{eq : v_mult_full_cond_mu0 }        \\end{aligned}\\ ] ] in the above , we have denoted the identity matrix by @xmath108 .",
    "the full conditional distribution of psi is given by    @xmath109 $ ] from + @xmath110 in ( [ eq : mult_full_cond_psi ] ) , @xmath111 .",
    "we have thus derived a simple gibbs sampling algorithm for our mixture model with unknown number of components , which is computationally highly suitable for massive data sets , thanks to the fixed maximum number of components .",
    "we assume a bivariate normal distribution of two random variables @xmath112 ( that is , @xmath113 in the general multivariate methodologies developed in sections [ sec : mult_extension ] and [ sec : gibbs_sampling ] ) , where the true regression function of @xmath114 on @xmath115 is @xmath116 , a highly non - linear curve . pretending that the true curve is unknown , and that all we have is a sample of 1000 observations @xmath117",
    ", we demonstrate that our curve - fitting idea can accurately estimate the ( unknown ) true curve .",
    "we obtain the data by actually simulating from the bivariate normal distribution .    to implement our curve - fitting procedure , we need to fit the data using the bayesian mixture model based on dirichlet process .",
    "some of the prior parameters are chosen such that fast convergence to the target posterior is ensured , and other choices ( and justifications thereof ) are motivated by those of ew , rg , and bhattacharya(2008 ) .",
    "for example , selecting @xmath104 to be the sample mean vector , and @xmath118 to be the sample dispersion matrix indicated good mixing properties of our gibbs sampler . however , it is important to select the prior parameters of @xmath96 carefully , since this can significantly affect the probability distribution of the number of components , and hence the fit of the curve . to select an appropriate prior for @xmath96",
    ", we first assume that it is a constant to be determined ( by a procedure to be described below ) .",
    "once it is determined , we select the prior parameters @xmath119 and @xmath120 such that the mode of the prior distribution of @xmath96 , @xmath97 is set equal to the determined value and the variance is as large as possible to reflect our vagueness about the prior .    to determine the mode of the prior of @xmath96 , we fit the bayesian curve with many fixed values of @xmath96 , and compute the maximum absolute difference at @xmath121 between the true curve and the fitted curve @xmath122 . we choose that value of @xmath96 as the prior mode for which the deviation is less than 0.4 and the fitted curve contains most of the features of the true curve .    .two - way table showing the deviations of the fitted curve from the true curve [ cols=\"^,^\",options=\"header \" , ]     the merged central clustering consisting of two components only ( which corresponds to the prefixed limit being 0.9 ) is shown in figure [ fig:2clusters ] .",
    "this is provided to make our analysis comparable to the clustering done by astro - physicists on the basis of radio loud and radio quiet .",
    "we have repeated the same analysis taking the maximum number of components , @xmath123 .",
    "no notable difference between the results of the two analyses were observed .",
    "indeed , even with @xmath124 , the posterior probability of 30 components turned out to be negligible ( about @xmath125 ) .",
    "our statistical analysis of the sdss data has a number of implications that may give answer to many interesting questions from view point of quasar astronomy and cosmological models .    *",
    "the curve is linear for the small values of @xmath5 and becomes nonlinear for high values .",
    "it is to be noted that for low redshift @xmath5 the curve is linear with gradient of 0.5136 ( 1.182608 when logarithm of @xmath5 is taken in log base 10 while fitting the curve ) and intercept 18.7840 . in case of standard cosmological model , the gradient of the hubble line",
    "is supposed to lie between @xmath126 and @xmath127 efron(92 ) .",
    "one of the present authors sr roy(07 ) analysed the quasar data using non - parametric methods developed by efron(92 ) both for veron cetty as well as for sdss dr-3 data and found also linearity for small @xmath5 and non - linearity for high @xmath5 .",
    "* the whole curve can be approximated by five line segments with four change points .",
    "* we performed a detail cluster analysis that gives rise to possible number of clusters as 29 . but from observing values of the components it is evident that the clusters can be merged further to fewer number of clusters .",
    "the degree of reduction depends upon the prefixed threshold for merging the clusters . *",
    "the merged clusters can be compared with the clusterings observed by astronomers porciani(2006 ) for example , redshift dependent clusters or luminosity dependent clusters .",
    "this will be discussed elaborately in future communications .",
    "* there exists two broad class of quasars like radio - loud and radio quiet quasars .",
    "the environments around these clusters of quasars are different .",
    "the width and other characteristics of the emission or absorption lines from these quasars will be affected depending on the nature of the environments . * in our framework ,",
    "we have merged the clusters into two broad clusterings under certain thresholds .",
    "the characteristics of these clusterings need to be investigated in details so as to compare with the radio loud or radio quiet quasars which will be done in subsequent publications .",
    "the non - linearity of the curve may be due to several factors like evolution of the quasars , acceleration or deceleration of the universe .",
    "our findings will shed new light not only on the validity of hubble law but also help us to estimate the acceleration / deceleration parameters .",
    "these issues are very much important from the point of view of cosmological debates and will be considered in details in subsequent publications .",
    "let @xmath128 denote the number of distinct values in @xmath129 , and let @xmath130 ; @xmath131 denote the distinct values .",
    "also suppose that @xmath130 occurs @xmath132 times .",
    "then a reparameterization of our model parameters can be devised as follows .    as in muller(96 ) , we introduce the configuration vector @xmath133 , where @xmath134 if and only if @xmath135 . the configuration vector @xmath93 thus provides a reparameterization of the original parameters , the latter being reparameterized into distinct components and the associated configuration vector .",
    "using this reparameterized version one can can avoid simulation of all the parameters corresponding to all @xmath6 components .",
    "in fact , once a configuration is simulated , only the distinct parameters may be simulated .",
    "moreover , the corresponding gibbs sampler may have superior convergence properties ( see maceachern(94 ) ) .",
    "[ [ subsec : mult_distinct_theta_fullcond ] ] full conditional distributions of the distinct values of @xmath136 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    the conditional posterior distribution of @xmath137 is given by + @xmath138 $ ] from + @xmath139 in the above , @xmath140 , @xmath141 , @xmath142 , @xmath143 , and @xmath144 .",
    "it is to be noted that the @xmath137 are conditionally independent .",
    "the conditional distributions of @xmath145 are given , in the multivariate case , by + @xmath146 $ ] is proportional to + @xmath147 where @xmath74 is the expression given by ( [ eq : mult_q0 ] ) , and + @xmath148 is proportional to + @xmath149\\nonumber\\\\\\label{eq : mult_ql_star }        \\end{aligned}\\ ] ] note that it is possible to replace @xmath74 in ( [ eq : mult_q0 ] ) with @xmath150\\nonumber\\\\        \\label{eq : mult_nonconjugate_form}\\ ] ] where @xmath151 .",
    "the latter formulation is most appropriate when @xmath55 is not conjugate to the likelihood , which may preclude integration of ( [ eq : mult_nonconjugate_form ] ) with respect to @xmath55 , making the explicit form of @xmath74 intractable . in our case , we can also integrate @xmath148 with respect to the conditional posterior distribution of @xmath130 given by ( [ eq : mult_theta_star_given_c ] ) to obtain @xmath152 in ( [ eq : mult_integrated_ql ] ) @xmath153 and @xmath154[(\\bar{\\textbf{y}}_j-{\\mbox{\\boldmath $ \\mu$}}_0)+\\psi n^*_{\\ell}(\\bar{\\textbf{y}}_j-\\bar{\\textbf{y}}^*_{\\ell})]'}{(\\psi n^*_{\\ell}+1)(\\psi n^*_{\\ell}+\\psi n_j+1 ) } \\bigg|^{\\frac{n^*_{\\ell}+n_j+s}{2 } }        \\label{eq : mult_s2_l }        \\end{aligned}\\ ] ]        bhattacharya , s. , samanta , t. , dihdar , k. , and ghosh , j.  k. ( 2009 ) .",
    "n bayesian central clustering : application to landscape classification of western ghats . technical report , indian statistical institute . submitted for pulication .",
    "diaconis , p. and ylvisaker , d. ( 1985 ) . quantifying prior opinion . in j.",
    "m. bernardo , j.  o. berger , a.  p. dawid , and a.  f.  m. smith , editors , _ bayesian statistics 2 _ , pages 183201 .",
    "oxford university press .",
    "roy , s. , datta , d. , ghosh , j. , roy , m. , and kafatos , m. ( 2007 ) . on -",
    "parametric tests for quasar data and hubble diagram . in v.",
    "de gesu , g.  l. bosco , and m.  maccarone , editors , _ modeling and simulation in science : 6th international workshop on data analysis in astronomy _ , pages 99106 .",
    "world scientific , nj ."
  ],
  "abstract_text": [
    "<S> recent technological advances have led to a flood of new data on cosmology rich in information about the formation and evolution of the universe , _ e.g. , the data collected in sloan digital sky survey ( sdss ) for more than 200 million objects_. the analyses of such data demand cutting edge statistical technologies . here , we have used the concept of mixture model within bayesian semiparametric methodology to fit the regression curve with the bivariate data for the apparent magnitude and redshift for quasars in sdss ( 2007 ) catalogue . associated with the mixture modeling </S>",
    "<S> is a highly efficient curve - fitting procedure , which is central to the application considered in this paper . </S>",
    "<S> moreover , we adopt a new method for analysing the posterior distribution of clusterings , also generated as a by - product of our methodology . </S>",
    "<S> the results of our analysis of the cosmological data clearly indicate the existence of four change points on the regression curve and posssibiltiy of _ clustering _ of quasars specially at high redshift . </S>",
    "<S> this sheds new light not only on the issue of evolution , existence of acceleration or decceleration and environment around quasars at high redshift but also help us to estimate the cosmological parameters related to acceleration or decceleration . </S>",
    "<S> + _ * keywords : * cluster analysis ; cosmology ; dirichlet process ; model validation ; markov chain monte carlo ; non - linear regression . </S>",
    "<S> _    * fast bayesian semiparametric curve - fitting and clustering in massive data with application to cosmology *     sabyasachi mukhopadhyay , sisir roy , and sourabh bhattacharya </S>"
  ]
}