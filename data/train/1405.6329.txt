{
  "article_text": [
    "parameter estimation is an integral part of physics .",
    "accurate estimates of physical parameters in quantum mechanical models allows for precision quantum control @xcite which enables practical goals such as quantum computation @xcite and quantum metrology @xcite which in turn can provide probes of fundamental physics such as gravity wave detection @xcite .",
    "quantum parameter estimation shares many similarities with its classical counterpart .",
    "but there are many subtle and peculiar differences . even in single parameter estimation",
    ", quantum metrology @xcite shows that we can obtain advantages from quantum resources such as squeezed states of light @xcite and entanglement @xcite . with such rich structure , many new subtleties @xcite , considerations @xcite and generalities",
    "@xcite can arise , including entirely new approaches to estimation @xcite and verification @xcite .    at the other end of the parameter spectrum",
    "the quantum state of a physical system is our most complete description of it .",
    "thus estimation of quantum states @xcite might be seen as the most ambiguous form of estimation .",
    "there are many approaches to the general problem @xcite , some which specialize for computational efficiency @xcite and those which go beyond to estimation of regions @xcite .",
    "all of the above mentioned results assume a model .",
    "that is , it was taken as given that a particular parametric distribution generated the data .",
    "but what if this assumption is not correct ?",
    "this question has garnered recent interest and classical approaches to _ model selection _ have been used in a variety of experimental @xcite and theoretical @xcite works . here",
    "we supplement these results with a new approach to parameter estimation : _ model averaging_. the technique shares many similarities with model selection  in fact , model selection is a crucial component of model averaging , but goes beyond it .",
    "although model selection adds an additional layer of security to overconfident estimation , selecting models can be itself a red herring , for the most probable model might only be slightly more probable than others .",
    "the approach considered here combines bayesian parameter estimation with bayesian model selection , such that the final estimate of the parameters is the best value of the parameters within each model , averaged over the probability assigned to each model .",
    "we will show that such an approach can reduce the error incurred by first selecting a model  which has some probability of being incorrect  then selecting parameters within that model .",
    "in fact , the numerical experiments presented here show that model average estimation always does better than the estimates from incorrect models and in some scenarios can perform better than even estimates from the correct model .",
    "this is due to the additional hedging afforded by considering multiple models , all of which carry _ some _ information .    the paper is organized as follows . in sec .",
    "[ sec : review ] we outline the problem and review the model selection techniques used so far . in sec .",
    "[ sec : bayes ] , we present the full bayesian approach to model selection and define the _ model average estimate _ of parameters . sec .",
    "[ sec : examples ] presents two distinct examples where the model average estimate provides an advantage for parameter estimation .",
    "we conclude with a discussion in sec .",
    "[ sec : conclusion ] .",
    "in sec . [ sec : a ] we overview the problem . in secs . [ sec : aic ] and [ sec : bic ] we review the akaike information criterion ( aic ) and the bayesian information criterion ( bic ) which are by far the most commonly used approaches to model selection ( and the only used thus far for quantum estimation ) . these sections are included for reference and completeness .",
    "let us begin with the base problem of _",
    "parameter estimation_. a physical model prescribes the probabilities for the outcomes of experiments : @xmath0 .",
    "here @xmath1 is some hypothetical or observed data , @xmath2 is a set of real numbers in @xmath3 , where @xmath4 is the _ dimension _ of the model , @xmath5 is the experimental context , here means any additional information necessary to provide a well defined function @xmath0 . ] and @xmath6 is our model .",
    "for example , we could have the model @xmath6 be that of qubit which is parameterized by a bloch vector @xmath7 .",
    "the experimental context could be a measurement basis , say that of @xmath8",
    ". then , the quantum mechanical model prescribes @xmath9 . in quantum mechanics this",
    "is called the _ born rule _ and in statistics , the _",
    "likelihood function_.    going from parameters to the probability of data is a deductive process ",
    "the model gives us numerical values of @xmath0 .",
    "the experiment , on the other hand , gives us a particular data set @xmath1but what we really want is @xmath2 .",
    "this is an example of an _",
    "inverse problem_. what the bayesian solution provides is @xmath10 , the _ distribution _ of @xmath2 given the data .",
    "although we lack certainty about @xmath2 , we can accurately ( read : quantitatively , mathematically ) describe our state of knowledge of @xmath2 given we have seen the data @xmath1 .",
    "the formal path forward is through bayes rule : @xmath11 from a chronological point of view relative to @xmath1 , we begin with the _ prior _ @xmath12 , which encodes the information we have about @xmath2 prior to learning about @xmath1 .",
    "we weight the prior by the likelihood function and normalize by the _",
    "_ marginal likelihood__$ ] means take the expectation of @xmath13 with respect to the distribution of @xmath14 ( the distribution itself is implicit ) . ] : @xmath15.\\ ] ] the distribution produced as result of bayes rule , @xmath10 , is called the _ posterior _ and represents our knowledge of @xmath2 after the data has been observed .    at this point",
    "we could call the problem solved .",
    "this , however , assumes the model is correct .",
    "if the model is suspect , then we have the meta  problem of determining the best model .",
    "the most commonly used model selection technique is the akaike information criterion ( aic ) , which arises as follows .",
    "first , we suppose there is some true model , @xmath16 , giving a distribution @xmath17 .",
    "we quantity the discrepancy between this model and our candidate model via the _ kullback - leibler divergence _ :",
    "@xmath18 = \\mathbb e_{d'|t;c}\\left[\\log{\\pr(d'|t , c)}\\right ] - \\mathbb e_{d'|t;c}\\left[\\log{\\pr(d'|\\vec{x};m , c)}\\right],\\ ] ] for some appropriate choice of parameters @xmath2 .",
    "the `` best '' model , then , is the one that minimizes @xmath19 , which is equivalent to maximizing the second term in since the first term only depends on the true model .",
    "however , we do not know the true model and we do not know the best set of parameters within our candidate model . the latter problem is naturally address by collecting data @xmath1 and producing an estimate of the parameters @xmath20 , then averaging over possible data sets such that the quantity of interest becomes @xmath21\\right].\\ ] ] akaike showed that , independent of the true model ( and under some regularity conditions ) , an unbiased estimator of this quantity is @xmath22 where @xmath4 is the number of dimension of the model ( the number of free parameters ) .",
    "the preferred model is the one with largest value of aic@xmath23 .",
    "the simple linear penalization with dimension makes it is clear how models with more parameters are penalized .",
    "the bayesian approach is more general .",
    "being so , it is less obvious how it might penalize complex models .",
    "here we show how an asymptotic approximation leads to a form similar to the aic .",
    "first , we write the marginal likelihood as the integral expectation @xmath24 we will approximate this integral using laplace s method . to this end , consider the taylor expansion of the log of the likelihood function about its peak ( note that for brevity , we have dropped the context @xmath5 and model @xmath6 from the conditionals ) : @xmath25 if we assume the number of measurements @xmath26 , the law of large numbers gives us @xmath27,\\\\ & = -\\sum_{j=1}^n i_j(\\vec{x}_0),\\\\ & = - n i(\\vec{x}_0),\\end{aligned}\\ ] ] where @xmath28 is the fisher information of the @xmath29th measurement and @xmath30 is the arithmetic average of these values .",
    "then , the integral becomes @xmath31 now we take the logarithm to obtain @xmath32 if we ignore the terms not changing with @xmath33 , we have a new quantity @xmath34 which is the well - known bayesian information criterion or bic . notice the striking similarity to the aic .",
    "being nearly equivalent , the bic is often considered in addition to the aic .",
    "next , we will consider the full solution , which will allow us to obtain more accurate estimates of parameters averaged over the competing models .",
    "recent proposals have used the aic / bic on both simulated @xcite and experimental data @xcite . in @xcite , however , the authors caution its unattended use .",
    "the argument against aic for quantum states , for example , is simple .",
    "the aic is derived from a metric which measures their closeness of model in their _ predictive _ probability  a certainly well - motivated measure .",
    "however , such a measure is only useful if all future measurements will be the same as those used to perform the data analysis .",
    "that is , one can measure copies of a quantum system in some fixed set of bases , estimate the state , then use that estimate to predict the outcome of a measurement in a new basis .",
    "thus , in quantum theory , one ought to consider a measure on predictive distribution as maximized over all possible future measurements .",
    "as suggested in @xcite , such a measure might well be the quantum relative entropy , for example . here",
    "we avoid these problems by considering the full bayesian solution , which we describe next .",
    "within the bayesian framework , the model selection approach is no different than for parameter estimation . rather than focus on the distribution @xmath10 , we first consider @xmath35 . using bayes rule we have @xmath36 often , practitioners of bayesian methods go one step further and compare two models  say , @xmath37 and @xmath38by taking the ratio of these posteriors , @xmath39 noticing the normalization factor cancels .",
    "this quantity is called the _",
    "posterior odds ratio _ and first considered by jeffreys @xcite .",
    "clearly , if the posterior odds ratio is larger than 1 , we favor @xmath37 .",
    "the last fraction is called the _",
    "prior odds ratio _ and the unbiased choice favoring neither model is set this term equal to 1 .",
    "this leaves us with @xmath40 which is called the _ bayes factor _ @xcite .",
    "each quantity in the ratio is marginal likelihood of its respective model .",
    "for a discrete number of hypothetical models @xmath41 , and assuming one model must be chosen , the optimal strategy is to compute the marginal likelihood if each model and select the one with the highest value .",
    "model selection of this type has been used in quantum mechanical problems of hamiltonian finding @xcite and estimating error channels from syndrome measurements @xcite .    here",
    "we will investigate the idea of using a meta - model , which is an average over those in @xmath41 .",
    "assume that we are interested in some subset of parameters @xmath42 common to all models .",
    "given we have taken data @xmath1 and computed each marginal likelihood @xmath43 , we define the model average estimate ( mae ) as @xmath44.\\ ] ] in words , this is the average ( over models ) of the average ( over parameters within models ) . variants of this approach are referred to as bayesian model averaging @xcite .    before moving to our examples ,",
    "a few comments are in order .",
    "first , it is not necessary that the models are `` nested '' in the sense that we can order them into supersets .",
    "the only requirement is that the parameters of interest are included in each model .",
    "the other parameters in each model we might call _ nuisance parameters_. part of the appeal of the bayesian approach is that these parameters are automatically dealt with and we can focus on those parameters which are of immediate interest . of course , the nuisance parameters can be inferred as well .",
    "let us wax philosophical for a moment .",
    "what is being proposed is to select a meta - model , an average over many different physical models .",
    "this may seem awkward for physical theories ",
    "after all , there is only one true model , right ?",
    "not in our view .",
    "models are human constructs , those platonic ideals which describe another world , a world we were clever enough to find through our mastery of mathematics and abstraction . here , we have dropped the idea that the point is to find the capital - t - truth .",
    "rather , we measure our understanding of nature through our ability to predict and control its behavior . by averaging physical models",
    ", we can show that this idea has merit .      in practice , the bayesian update rule and",
    "the expectations required in the equations above are analytically and computationally intractable since they involve complicated integrals over multidimensional parameter spaces which may include solutions to equations of motions which are themselves intractable . to perform the calculations we turn to monte carlo techniques .",
    "our numerical algorithm fits within the subclass of monte carlo methods called _ sequential monte carlo _ ( smc ) or _ particle filtering _ @xcite .",
    "the smc procedure prescribes that we approximate the probability distribution by a weighted sum of dirac delta - functions , @xmath45 where the weights at each step are iteratively calculated from the previous step via @xmath46 followed by a normalization step .",
    "the elements of the set @xmath47 are called _",
    "particles_. here , @xmath48 is the number of particles and controls the accuracy of the approximation . like all monte carlo algorithms ,",
    "the smc algorithm approximates expectation values , such that @xmath49 \\approx \\sum_{j=1}^n w_j f(\\vec{x}_j).\\ ] ] in other words , sequential monte carlo allows us to efficiently compute multidimensional integrals with respect to the measure defined by the probability distribution .",
    "the resultant posterior probability provides a full specification of our knowledge .",
    "however , in most applications , it is sufficient  and certainly more efficient  to summarize this distribution . in our context , the optimal single parameter vector to report is the mean of the posterior distribution @xmath50 = \\sum_{j=1}^{n}w_j \\vec{x}_j.\\ ] ] the smc approximation can also provide efficient calculation and description of _ regions _ @xcite . for our purpose",
    ", we also require the smc approximation to give an accurate and efficient esimate of the marginal likelihood eq .  , which we need to calculate bayes rule at the level of models eq .  .",
    "via the smc approximation , the integral expectation in the definition of the marginal likelihood , eq .  , is @xmath51 it is not immediate obvious but is easy to see in hindsight that this is exactly the normalization that must be computed already in the smc algorithm after the weight update , eq .  ,",
    "is applied . by storing this value",
    ", we can apply bayes rule at the meta - level of models eq .  .",
    "an iterative numerical algorithm such as smc requires care to ensure stability .",
    "conditions for stability of the algorithm and the specifications of an implementation have been detailed elsewhere @xcite .",
    "the smc algorithm has now been used in many quantum mechanical parameter estimation problems @xcite and a software implementation ( the one used here ) is available as a python package @xcite .",
    "we consider first an example similar to guta , kypraios and dryden @xcite : @xmath52 qubits subjected to random pauli measurements where the models under consideration are those of differing rank .",
    "each model will be denoted @xmath53 where @xmath54 is the rank of the unknown quantum state so that the dimension of model @xmath53 is @xmath55 .",
    "we generate unknown quantum states with fixed rank @xmath54 as follows @xcite .",
    "begin with a matrix @xmath56 where each component @xmath57 is chosen independently according to @xmath58 and @xmath59standard normal distributions . then define the rank @xmath54 density operator @xmath60 if @xmath61",
    ", this construction is equivalent to a hilbert - schmidt random density matrix .",
    "the model parameters will be the vectorization of the matrix @xmath62 : @xmath63 .",
    "we label the single qubit pauli operators @xmath64 and the multi - qubit paulis by @xmath65 where @xmath66 . since each pauli is idempotent , @xmath67 , each individual measurement has @xmath68 possible outcomes which we label @xmath69 for the @xmath70 and @xmath71 eigenvalues",
    ". then the likelihood function of a single measurement can be related to the expectation value via : @xmath72 . using the properties of the vec operation ,",
    "we can write this as an explicit function of @xmath2 : @xmath73    we label the parameter vector within the rank @xmath54 model @xmath53 by @xmath74 and the associated density matrix given by eq .",
    "@xmath75 . within each model",
    ", there is not a one - to - one correspondence between @xmath74 and @xmath75different vectors will yield the same density matrix . since we will be interested in obtaining accurate estimates of density matrices as quantified by some norm on the space of density matrices , we will average the models over their @xmath75 s rather than their @xmath74 s .    within each model ,",
    "then , we have the mean density matrix ( recall @xmath1 is data , @xmath5 is the context  that is , which paulis were measured ) @xmath76.\\ ] ] explicitly , the mae in eq .   is @xmath77 we assume there is a true density matrix @xmath78 and we judge each estimate of the true state @xmath79 by its spectral distance to @xmath78 : @xmath80 where @xmath81 denotes the largest singular value .",
    "this is the norm induced by the usual euclidean norm on vectors .     the performance of bayesian model selection and the model average estimate for 2 qubits .",
    "each box represents the data for its labeled rank as the `` true '' model  from left to right , ranks 1 through 4 .",
    "the lines represent the median of the data and , where present , the shaded areas are the interquartile ranges .",
    "each `` measurement '' on the horizontal axis corresponds to 100 experiments of a randomly chosen pauli measurement . in the smc algorithm ,",
    "@xmath82 particles were used in each model . for each true rank ,",
    "100 simulated states were generated and measured . ]    the data for 2 qubits is presented in fig .",
    "[ fig : rank ] .",
    "the important take - away is that the model average estimate does as well , or slightly better than the true model in every case .",
    "also , we see that it is quite easy to identify rank 1 models ( pure states ) as well as rank 4 models ( full rank states ) while it seems difficult to identify states of non - extreme rank .",
    "notice that it is extremely difficult to distinguish the rank 3 model from the full rank model when the former defines the underlying truth . however , both models perform well with respect to the error in the estimated parameters and the model average estimate does best , on average .    to further illustrate the difficulty in differentiating high rank states , the probabilities assigned to 3 qubit models is shown in fig .  [",
    "fig : rank3 ] .",
    "again , we see that pure states and full rank states are correctly identified , yet it is difficult to correctly distinguish between rank 7 and rank 8 states , in the same way as it was difficult to distinguish rank 3 and rank 4 states for 2 qubits .",
    "we conclude that for the models considered here , it is easiest to correctly identify low rank and full rank states , while it is difficult to correctly identify nearly high rank states .     the performance of bayesian model selection and the model average estimate for 3 qubits .",
    "each box represents the data for its labeled rank as the `` true '' model .",
    "the lines represent the median of the data and , where present , the shaded areas are the interquartile ranges .",
    "each `` measurement '' on the horizontal axis corresponds to 100 experiments of a randomly chosen pauli measurement . in the smc algorithm ,",
    "@xmath83 particles were used in each model . for each true rank ,",
    "10 simulated states were generated and measured . ]",
    "notice also that models which are far away  in the sense that the ranks differ by relatively large amounts  are quickly ruled out . in these cases , since the smc algorithm can be run online ( in parallel with the experiment ) , simulating such models can be stopped to mitigate the computational difficultly in simultaneously simulating many quantum models .",
    "still , tomography is at one extreme in the spectrum of methods for estimating quantum mechanical parameters ",
    "it is the _ most complete _ description of the physical system . at the other extreme",
    "is summarizing information from experiments into a single number , such as fidelity .",
    "in the second example , we consider the experimental protocol of randomized benchmarking @xcite which has been demonstrated in a variety of experimental settings @xcite to efficiently characterize noise and quantum channels .",
    "the protocol consists of stringing together potentially long sequences of gates which is then undone to determine if the initial state has survived . in @xcite",
    "the approach was shown to give , in expectation , an exponentially decay @xmath84 , where @xmath85 and @xmath86 encode the errors in preparation and measurement , @xmath87 is the bare survival probability and @xmath88 is the length of the sequence . in the models we consider , @xmath87 can be related to the average fidelity over a group of gates , but more specialized protocols exists @xcite .    typically , @xmath87 is the only parameter of interest since it is directly related to the average fidelity of the device which is then compared to some threshold .",
    "the other parameters are often consider nuisance parameters . in @xcite",
    "it was shown that the decay can be interpreted as probabilistic model where the binary outcome of each measurement sequence of length @xmath88 has probability of survival ( labeled @xmath89 ) @xmath90 where the subscripts refer to this as the _ zeroth order model_. in @xcite , a hierarchy of models was introduced because the zeroth order model assumes the errors in the gates within the sequence are independent . by dropping this assumption , a richer set of noise models",
    "can be studied @xcite . here",
    "we will study the zeroth order model and the _ first order model _",
    ", @xmath91 where @xmath92 and @xmath93 again encode the preparation and measurement errors , @xmath94 encodes the error on the final gate in the sequence and @xmath95 is a measure of the gate dependence in the errors .    for the zeroth order model ,",
    "we take the prior to be a normal distribution with a mean vector @xmath96 and equal diagonal covariances given by a deviation of @xmath97 .",
    "note that the first order model is equal to the zeroth order model when either @xmath98 or @xmath99 . in order to not make the model so different that it would be trivial to distinguish them",
    ", we look at two priors for the first order model which are close to the zeroth order model .",
    "the first is a normal distribution with a mean vector @xmath100 and equal diagonal covariances given by a deviation of @xmath97 and the second slightly closer with the same covariance matrix but mean vector @xmath101 .",
    "note that the difference between these two distribution in the relative entropy divergence is only 0.050 , and so we might expect them to behave the same . since the both models are close to the zeroth order model , we expect it to be difficult to distinguish them .     the performance of bayesian model selection and the model average estimate for the survival probability in randomized benchmarking experiments .",
    "each box represents the data for its label as the `` true '' model .",
    "the lines represent the median of the data and , where present , the shaded areas are the interquartile ranges .",
    "each `` measurement '' on the horizontal axis corresponds to a randomized benchmarking experiment with sequences lengths @xmath102 and @xmath103 repetitions per sequence length . in the smc algorithm ,",
    "@xmath104 particles were used in for each model . ]    in fig .",
    "[ fig : rbmodels ] we simulate the models noting again that , via the priors , they are very close .",
    "this intuition is quantified by the fact that the models are hard to distinguish , regardless of which is true .",
    "on the left in fig .",
    "[ fig : rbmodels ] , we see that parameters in the first order model are so close to those in the zeroth order model that it is irrelevant which is chosen , for the purpose of estimating @xmath87 .",
    "however , what is `` close '' can be deceiving as we see in the right of fig .",
    "[ fig : rbmodels ] . recall that relative entropy from @xmath105 to @xmath106 is only 0.050 ( which explains why they are so difficult to distinguish ) . in this case , the accuracy of the estimates of the parameter @xmath87 depend crucially on which model is actually correct . in such cases , the model average estimate can be seen as providing a more conservative estimate of the average gate fidelity by hedging what is at best a 50/50 guess on which model is correct .",
    "we have introduced a bayesian model averaging approach to estimating parameters in quantum mechanical models describing data . in the examples considered , the model average estimate performance as well as the unknown true model in most cases . in situations where models are difficult to distinguish , the model average estimate can slightly _",
    "outperform _ the true model .    for the quantum state estimation example ( sec .",
    "[ sub : rank ] ) we considered models of differing rank of the density matrix .",
    "ranks which differ by large amounts from the true rank are rapidly ruled out  that is , the probability assigned to them quickly approaches zero .",
    "thus , they contribute nothing to the model average estimate . on the other hand , ranks which are close to the true rank  especially when",
    "the true rank is high  are not so easily distinguished .",
    "this means that first selecting a rank and then performing estimation within that model can lead to overconfident estimates of the state . by averaging",
    ", we can allow those estimates to only contribute with the relative probability that we deem them to be true .",
    "the mechanism for why higher rank states are hard to distinguish is yet unclear .",
    "although the smc algorithm and the implementation used here have been extensively studied for a wide range of problems , it is possible that state tomography is an outlier .",
    "that is , some major modification to modeling or the algorithm itself may be required to obtain the best performance .",
    "this resolution would be less interesting than a physical explanation , such as pauli measurement tomography ( which we have used in the example here ) is not the optimal scheme to distinguish rank .",
    "these questions are left for future work .    in the example of randomized benchmarking ( sec .",
    "[ sub : rb ] ) , we explored a situation where the presence of higher order perturbations were difficult to detect .",
    "the bayesian model selection approach accurately predicts this by assigning roughly 50/50 probability assignments to the models .",
    "surprisingly , the `` closeness '' of the models as measured by our ability to distinguish them does not translate into our ability to accurately estimate parameters common to each .",
    "in some cases we can do no more than guess which is correct , yet guessing the wrong model may have disastrous consequences in our ability to accurately infer the parameters of interest .",
    "again , the model average estimate mitigates the risk of improperly `` guessing '' which model is correct .",
    "we have noted in the introduction the numerous approaches to estimation within quantum theory .",
    "this should urge one to ask , are all of these distinct approaches necessary  is there not some unified approach ?",
    "yes ! the bayesian framework outlined here is remarkably powerful in its generality .",
    "we note that bayesian ideas have already been put to good use in quantum information theoretic and foundational problems @xcite as well as for tomographic and parameter estimation problems @xcite and experimental design @xcite .",
    "importantly , the bayesian algorithm can provide solutions to these problems _ online _ , while the experiment is running , with the same software tools @xcite .",
    "the author thanks chris granade and robin blume - kohout for helpful discussions .",
    "this work was supported in part by national science foundation grant no .",
    "phy-1212445 and by the canadian government through the nserc pdf program .",
    "a. fujiwara and h. imai , _ a fibre bundle over manifolds of quantum channels and its application to quantum statistics _ , http://dx.doi.org/10.1088/1751-8113/41/25/255304[journal of physics a : mathematical and theorical * 41 * , 255304 ( 2008 ) ] .",
    "s. knysh , v.  n. smelyanskiy and g.  a. durkin , _ scaling laws for precision in quantum interferometry and the bifurcation landscape of the optimal state _ , http://dx.doi.org/10.1103/physreva.83.021804[physical review a * 83 * , 021804(r ) ( 2011 ) ] .",
    "b. m. escher , r. l. de matos filho and l. davidovich , _ general framework for estimating the ultimate precision limit in noisy quantum - enhanced metrology _ , http://dx.doi.org/10.1038/nphys1958[nature physics * 7 * , 406 ( 2011 ) ] .",
    "j. m. arrazola , o. gittsovich , j. m. donohue , j. lavoie , k. j. resch and n. ltkenhaus , _ reliable entanglement verification _ , http://dx.doi.org/10.1103/physreva.87.062331 [ physical review a * 87 * 062331 ( 2013 ) ] .",
    "s.  j. van enk and r. blume - kohout , _ when quantum tomography goes wrong : drift of quantum sources and other errors _ , http://dx.doi.org/10.1088/1367-2630/15/2/025024[new journal of physics * 15 * , 025024 ( 2013 ) ] .",
    "k. usami , y. nambu , y. tsuda , k. matsumoto and k. nakamura , _ accuracy of quantum - state estimation utilizing akaike s information criterion _ ,",
    "http://dx.doi.org/10.1103/physreva.68.022314[physical review a * 68 * , 022314 ( 2003 ) ] .",
    "j. combes , c. ferrie , c. cesare , m. tiersch , g.  j. milburn , h.  j. briegel and c.  m. caves , _ in - situ characterization of quantum devices with error correction _ , http://arxiv.org/abs/1405.5656[arxiv:1405.5656 ( 2014 ) ] .",
    "k. zyczkowski and h .- j .",
    "sommers , _ induced measures in the space of mixed quantum states _ , http://dx.doi.org/10.1088/0305-4470/34/35/335[journal of physics",
    "a : mathematical and general , * 34 * , 7111 ( 2001 ) ] .",
    "e.  magesan , j. m. gambetta , and j. emerson .",
    "_ scalable and robust randomized benchmarking of quantum processes _ , http://dx.doi.org/10.1103/physrevlett.106.180504 [ physical review letters * 106 * , 180504 ( 2011 ) ] .",
    "o. moussa , m.  p. da silva , c.  a. ryan and r. laflamme , _ practical experimental certification of computational quantum gates using a twirling procedure _ , http://dx.doi.org/10.1103/physrevlett.109.070504[physical review letters * 109 * , 070504 ( 2012 ) ] ."
  ],
  "abstract_text": [
    "<S> standard tomographic analyses ignore model uncertainty . it is assumed that a given model generated the data and the task is to estimate the quantum state , or a subset of parameters within that model . here </S>",
    "<S> we apply a model averaging technique to mitigate the risk of overconfident estimates of model parameters in two examples : ( 1 ) selecting the rank of the state in tomography and ( 2 ) selecting the model for the fidelity decay curve in randomized benchmarking . </S>"
  ]
}