{
  "article_text": [
    "in this article we consider two types of processes arisen from mean - shift algorithms @xcite . starting with @xmath0 points",
    "@xmath1 as initials , the nonblurring type process is given by @xmath2})df_n(x ) } { \\int w(x- x_{i , n}^{[t]})df_n(x ) }   = \\sum_{j=1}^n   \\displaystyle \\frac{w(x_{j , n } - x_{i , n}^\\tn ) } { \\sum_{\\ell=1}^n w(x_{\\ell , n } -x_{i , n}^\\tn)}\\ , x_{j , n},\\end{aligned}\\ ] ] where @xmath3}_{i , n}=x_{i , n}$ ] and @xmath4 is the empirical distribution function based on the initial points @xmath1 .",
    "this process  ( [ eq : updatnb ] ) consists of @xmath0 simultaneous updating paths , wherein each path starts from one initial .",
    "another type of updating process , called the blurring type , is considered by replacing @xmath4 with the iteratively updated empirical distribution @xmath5 based on updated points @xmath6 , in addition to the above idea of weighted scores for updating : @xmath7 where @xmath8 .",
    "same as the nonblurring process , the blurring process  ( [ eq : updatbl ] ) starts with @xmath0 initials @xmath1 , and then it goes through a simultaneous updating at each iteration .",
    "the key difference from the nonblurring process is that , this process  ( [ eq : updatbl ] ) takes weighted average according to the updated empirical distribution @xmath5 , while the nonblurring process takes weighted average with respect to the initial empirical distribution @xmath4 .",
    "that is , at each iteration in the blurring process , not just the weighted centers are updated from @xmath6 to @xmath9 , the empirical distribution is also updated from @xmath5 to @xmath10 .",
    "the blurring process was developed and named sup ( self - updating process in chen and shiu , 2007 ) and was recently applied to cryo - em image clustering ( chen et al . , 2014 ) .",
    "it is also known as the blurring type mean - shift algorithm ( cheng , 1995 ; comaniciu and meer , 2002 ) . blurring",
    "mean - shift can be viewed as a homogeneous self - updating process .",
    "algorithm convergence and location estimation consistency of the blurring and nonblurring processes were discussed in @xcite , @xcite , @xcite , @xcite , and @xcite . in this article , we study their weak convergence and associated central limit theorem . due to the complicated dependent structure of random variables in @xmath11 and @xmath6 ,",
    "the study of their asymptotic behavior becomes challenging .",
    "the convergence point of the blurring or the nonblurring process can be used for location estimation , which is one of the most basic and commonly used tasks in statistical analysis as well as in computer vision .",
    "it is well - known that the sample mean is not a robust location estimator and it is sensitive to outliers and data contamination . to reduce the influence from deviant data ,",
    "there is a wide class of robust m - estimators in statistics literature using weighted scores ( hampel et al . , 1986 ; huber , 2009 ; van de geer , 2000 ) .",
    "consider a weighted score equation for the mean @xmath12 : @xmath13 where @xmath14 is a symmetric weight function . the weighted mean that satisfies the estimating equation  ( [ est_eq ] ) can be shown to take the following form @xmath15 this estimator  ( [ mu_eq ] ) can be obtained by the fixed - point iteration algorithm at convergence , where the iterative update is given by @xmath16 with an appropriate starting initial @xmath17}$ ] . here , we consider a simple change of the updating process by starting with @xmath0 data points @xmath1 as initials and by replacing @xmath18 with @xmath19 in  ( [ eq : score2 ] ) .",
    "it then leads to the nonblurring process given in  ( [ eq : updatnb ] ) : @xmath20})df_n(x ) } { \\int w(x- x_{i , n}^{[t]})df_n(x ) }   = \\sum_{j=1}^n   \\displaystyle \\frac{w(x_{j , n } - x_{i , n}^\\tn ) } { \\sum_{\\ell=1}^n w(x_{\\ell , n } -x_{i , n}^\\tn)}\\ , x_{j , n}.\\ ] ] by replacing @xmath4 with @xmath5 , we have the blurring process given in  ( [ eq : updatbl ] ) : @xmath21 the iterative updating process based on either ( [ eq : updatnb ] ) or ( [ eq : score2 ] ) has been adopted for robust mean estimation ( field and smith , 1994 ; fujisawa and eguchi , 2008 ; maronna , 1976 ; windham , 1995 ; among others ) and robust clustering ( notsu et al . , 2014 ) .",
    "it is also known as the nonblurring type mean - shift algorithm . on the other hand ,",
    "robust estimation based on blurring approach is rather rare in the literature .",
    "here we strongly recommend it as an alternative choice . from our simulation studies in section  [ sec : simulation ] , the blurring type algorithm is often more robust with smaller mean square error .",
    "thus , the blurring type algorithm deserves more attention and further exploration .",
    "the contribution of this article is twofold .",
    "first , we derive theoretical properties of the blurring and nonblurring processes including their weak convergence to a brownian bridge - like process and associated central limit theorem .",
    "these theoretical results are presented in section  [ sec : main ] , with all technical proofs being placed in the appendix .",
    "second , we apply the derived central limit theorem to location estimation .",
    "simulation studies comparing location estimation based on using blurring and nonblurring processes are presented in section  [ sec : simulation ] .",
    "our simulation results suggest that the blurring type algorithm is often more robust than the existent nonblurring type algorithm for robust m - estimation .",
    "let @xmath22 , @xmath23 , be a triangular array of random variables .",
    "assume the following conditions .",
    "* the underlying distribution @xmath24 has a continuous probability density function @xmath25 , which is symmetric about its mean @xmath12 . *",
    "the weight function @xmath14 is a probability density function .",
    "it is log - concave and symmetric about 0 .",
    "( this condition implies that @xmath26 for all @xmath27 and that @xmath14 is unimodal and non - constant . ) * for simplicity but without loss of generality , assume @xmath28 .",
    "let @xmath29 and @xmath30 where @xmath31 is a blurring transformation given by @xmath32 where @xmath33 is the cumulative distribution functions of @xmath34 with @xmath35 .",
    "note that the blurring transformation @xmath31 shifts @xmath36 toward a mode by an amount depending on @xmath37 , @xmath38 let @xmath39 be the empirical blurring transformation based on @xmath5 , which is the empirical cumulative distribution of @xmath6 .",
    "in theorem  [ thm : main ] below , it is shown @xmath40 almost surely for each @xmath36 , as @xmath41 . the blurring process , in empirical level and in population level , can be expressed as @xmath42    at @xmath43 , it is known that @xmath44 almost surely for each @xmath36 , as @xmath45 .",
    "furthermore , by donsker s theorem , the sequence @xmath46 converges in distribution to a gaussian process with zero mean and covariance given by @xmath47 let this convergence in distribution be denoted by @xmath48 where @xmath49 is the standard brownian bridge . in this article",
    "we establish the weak convergence for the empirical process of cumulative distribution function for each iteration .",
    "we will show the weak convergence by mathematical induction .",
    "the weak convergence is true at @xmath43 by donsker s theorem .",
    "next , by assuming that @xmath50 almost surely and that @xmath51 for some brownian bridge like process @xmath52 , we show that claimed statements hold for @xmath53 . because of the complicated dependent structure in @xmath54 , the almost sure convergence for @xmath55 and the weak convergence for @xmath56 become difficult , where @xmath57 we first establish the connection between the empirical process of cumulative distribution functions of two consecutive iterations .",
    "then we prove that this connection is a continuous mapping under the skorokhod topology .    before establishing the main theorem",
    "we derive a few technical lemmas first .",
    "lemma  [ lemma : order ] , with the proof given in appendix  [ appendix : lemma1 ] , shows that @xmath58 is a one - to - one transformation , which implies that the data orders do not change during the blurring process at each update .",
    "this phenomenon is important when we calculate the empirical cumulative distribution function of the current iteration based on the process of previous iteration .",
    "[ lemma : order ] assume conditions c1-c3 .",
    "we have @xmath59 for @xmath60 .",
    "next in lemma  [ lemma2 ] , we derive the asymptotic behavior of @xmath39 , which immediately implies that @xmath61 almost surely .",
    "[ lemma2 ] for any @xmath36 , @xmath62 a.s . for @xmath63",
    "the proof is given in appendix  [ appendix : lemma2 ] .    to show the weak convergence of @xmath64 , we need a tighter estimate of @xmath65 , which is presented in the following lemma with the proof given in appendix  [ appendix : lemma3 ] . while lemma  [ lemma2 ] shows @xmath66 , lemma  [ lemma3 ] implies that @xmath67 .",
    "[ lemma3 ] for @xmath68 , @xmath69    let @xmath70 and @xmath71 be the inverse function of @xmath72 and @xmath73 , respectively .",
    "then , we have @xmath74 using this formula and the taylor expansion , we establish the connection between @xmath64 and @xmath56 . the result is presented in the following lemma with the proof given in appendix  [ appendix : lemma4 ] .    [ lemma4 ] the process @xmath64 can be expressed as a function of the previous process @xmath56 up to an additive @xmath75-term .",
    "precisely , @xmath76 where @xmath77    with the assumption of the weak convergence of @xmath56 to a brownian bridge - like process , the variance of @xmath78 will also be @xmath75 .",
    "lemma  [ lemma : sk_conv ] below states that the mapping ( [ eq : wc_it ] ) , ignoring @xmath75 , is continuous .",
    "the continuity is stated in lemma  [ lemma : sk_conv ] with the proof given in appendix  [ appendix : lemma5 ] .",
    "[ lemma : sk_conv ] let @xmath79 $ ] be the space of real - valued functions on @xmath80 $ ] that are right - continuous and have left - hand limits . define @xmath81 as a mapping from @xmath82 to @xmath82 , such that @xmath83 where @xmath84 denotes the inverse function of @xmath85 .",
    "then , @xmath81 is continuous under the skorokhod topology .",
    "let @xmath86 .",
    "then @xmath87 , and @xmath88 lemma [ lemma : sk_conv ] shows that the mapping is asymptotically continuous .",
    "therefore , @xmath89 has the same weak convergence property as @xmath90 .",
    "the result is summarized in theorem  [ thm : main ] .",
    "[ thm : main ] assume conditions c1-c3 .",
    "we have    1 .   for each @xmath36 ,",
    "@xmath40 almost surely , as @xmath41 ; 2 .",
    "@xmath91 , where @xmath92 is the standard brownian bridge on @xmath80 $ ] , @xmath93 , @xmath94 , and @xmath95    proof for theorem  [ thm : main ] is in appendix  [ appendix : thm1 ] .",
    "@xmath96 and @xmath97 .      in the nonblurring process",
    "we use almost the same notation as in the blurring process except for the superscript @xmath98 $ ] . a similar result for the nonblurring process",
    "is stated in the following theorem with its proof given in appendix [ appendix : thm2 ] .",
    "[ thm : main2 ] assume conditions c1-c3 .",
    "we have    1 .   for each @xmath36 ,",
    "@xmath99 almost surely , as @xmath41 ; 2 .",
    "@xmath100 , where @xmath92 is the standard brownian bridge on @xmath80 $ ] , @xmath101 with @xmath102}(u , v)~~\\ , & : = & k^{[1]}(f^{-1}(u),f^{-1}(v))+    1_{\\left\\{\\eta^{[1]}(f^{-1}(v ) ) \\leq f^{-1}(u)\\right\\}}\\ , , \\\\",
    "h^\\tnplus(u , v ) & : = & k^\\tnplus(f^{-1}(u)),f^{-1}(v ) )   +    h^\\tn \\left(f(\\xi^\\tnplus(f^{-1}(u))),f(\\xi^\\tnplus(f^{-1}(v)))\\right).\\end{aligned}\\ ] ]      apply the weak convergence , we can have the central limit theorem for the sample mean of updated data .",
    "the result on the blurring case is presented below with its proof given in appendix [ appendix : thm3 ]    [ thm : clt_bl ] let @xmath103 .",
    "we have @xmath104 where @xmath105 .    from theorem [ thm : main ] ,",
    "@xmath106 take the derivative , we have @xmath107 now substitute @xmath108 in the above equation .",
    "we have @xmath109 therefore @xmath110db^{(0)}(v)\\end{aligned}\\ ] ] this distribution has mean 0 , and variance @xmath111 ^ 2dv.\\ ] ]    the central limiting theorem for the nonblurring case is similar , and the proof is almost identical and thus is omitted . the update of the nonblurring process is a weighted average over the original data and the weights depend on the updated data at the previous iteration .",
    "let @xmath112 .",
    "we have @xmath113 where @xmath114 .",
    "the convergence point with either the blurring or the nonblurring process is a reasonable robust estimation of the location parameter .",
    "the consistency of the nonblurring process is proved in  @xcite , and that of the blurring process is proved in  @xcite . with the central limit theorem provided in the previous section ,",
    "it is of our interest to compare the efficiency of both processes .",
    "theoretical comparison of asymptotic variance is quite difficult even for the simplest case that both the sampling distribution and the weight function are normal . in below we",
    "will first explain the rationale behind the phenomenon that blurring is more robust and often more efficient than nonblurring .",
    "then , we will show by simulation studies the behavior of asymptotic normality and the mean square error comparison for both processes .    without the update of empirical distribution",
    "i.e. , by keeping the initial empirical distribution @xmath4 throughout all iterations in the nonblurring process , the effective weights in  ( [ eq : updatnb ] ) are approaching @xmath115})}{\\int w(x-\\mu^{[\\tau ] } ) df_n(x)},\\ ] ] where @xmath116 is the last iteration step at convergence . with the update of empirical distribution in the blurring process",
    ", the effective weights in  ( [ eq : updatbl ] ) are getting more and more close to uniform .",
    "since the weighted average is taken with respect to newly updated centers , each of which is a weighted average of previous updated centers , the contribution of each original data point to the final estimation is relatively uniform for the blurring process , while the contribution of each data point in the nonblurring process is governed by @xmath14 .",
    "it is known that the sample mean , which corresponds to a uniform weight , is the uniformly minimum variance unbiased estimator for many distributions including normal distribution .",
    "while both blurring and nonblurring estimators are robust by reducing the contribution of outliers or data points in heavy tails , the blurring estimator , which takes a relatively uniformly weight , is expected to be more efficient than the nonblurring estimator on processing the relatively reliable part of information .",
    "our simulation results presented in section  [ compare_var ] also support this thinking .      in this simulation study",
    ", we compare the asymptotic normality of blurring and nonblurring processes . in theory all points should converge to a common point @xcite if the weight function has unbounded support .",
    "however , in empirical data simulation ( in particular , the case of student-@xmath117 distribution presented below ) some points far away from the main data cloud may fail to move to the location where most points have converged to , due to the precision in computer and the stopping criterion in the data implementation .",
    "therefore , we take the median value at the stopping of the updating process . precisely , let @xmath118 and @xmath119 , where @xmath116 is the number of iteration steps at convergence for the @xmath120 replicate run for @xmath121 .",
    "here we take @xmath122 .",
    "data are generated from @xmath123 , uniform@xmath124 , and student-@xmath117 with 3 degrees of freedom , where the sample size @xmath0 is set to 400 .",
    "two kinds of weight functions , normal and double exponential , are used . in figures",
    "[ qqplot ] and  [ qqplot2 ] , qq - plots for @xmath125 and @xmath126 are presented for two kinds of weight functions .",
    "we also include qq - plots using sample means ( for normal and uniform distributions ) and sample medians ( for student-@xmath117 distribution ) as a reference asymptotic behavior .",
    "for the student-@xmath117 , it requires a much larger @xmath0 for the sample means to behave like a normal .",
    "thus , we used sample medians , which require less larger @xmath0 . it can be seen that both @xmath127 and @xmath128 well follow a normal distribution .",
    "furthermore , the slopes in nonblurring qq - plots are a bit steeper than those in the blurring ones .",
    "it indicates that the convergence point of nonblurring process tends to have a larger variance .",
    "( bottom row ) .",
    "weight function is normal.,title=\"fig:\",width=624,height=240 ] +   ( bottom row ) .",
    "weight function is normal.,title=\"fig:\",width=624,height=240 ] +   ( bottom row ) .",
    "weight function is normal.,title=\"fig:\",width=624,height=240 ]    @xmath129 ( bottom row ) .",
    "weight function is double exponential.,title=\"fig:\",width=552,height=240 ] + @xmath130   ( bottom row ) .",
    "weight function is double exponential.,title=\"fig:\",width=624,height=240 ] + @xmath129 ( bottom row ) .",
    "weight function is double exponential.,title=\"fig:\",width=552,height=240 ]      in this simulation study , we compare mean square errors ( mse ) of @xmath127 and @xmath128 , which are obtained from blurring and nonblurring processes , respectively .",
    "three types of data distributions are used , @xmath123 , @xmath131 and student-@xmath117 with 3 , 5 and 10 degrees of freedom .",
    "normal and double exponential weight functions are adopted .",
    "sample size is set to @xmath132 .",
    "the mean square errors are calculated based on @xmath133 multiple trials .",
    "plots of @xmath134mse against the square root of sample size @xmath135 are presented .",
    "we also include mse of sample means and sample medians for comparison .",
    "sample mean is the umvue for estimating the normal mean . from figure  [ mse1 ] ( a ) and ( b )",
    ", we can see that the sample mean has the smallest mses for data generated from normal .",
    "estimates by the nonblurring process have larger mses compared to those by blurring process .",
    "we have experimented with various parameter values of normal and double exponential as the weight functions .",
    "all the results show that @xmath136 similar phenomena can be observed for data generated from the uniform @xmath124 distribution , which are depicted in figure  [ mse1 ] ( c ) and ( d ) .    ; ( c ) and ( d ) : sampling distribution is uniform @xmath124,height=384 ]    for heavy - tailed distributions such as student-@xmath117 distributions , the performance of blurring estimate , nonblurring estimate , sample mean and sample median for estimating location parameter depends on the weight function .",
    "figure  [ mse2n ] presents the results by normal weight functions with various parameter values , when the sampling distribution is student-@xmath117 with 3 degrees of freedom . as expected , the sample median is better than the sample mean in estimating the location parameter of student-@xmath117 distribution .",
    "in general , blurring estimates have smaller or competitive mses than those by nonblurring estimates .",
    "they produce a bit larger mses than the nonblurring ones only when an effectively flat weight function , such as @xmath137 with @xmath138 and @xmath139 , is used .",
    "similar phenomena for double exponential weight functions on student-@xmath117 can be observed in figure  [ mse2dex ] .     with 3 degrees of freedom and normal weight function .",
    ", height=480 ]     with 3 degrees of freedom and double exponential weight function . , height=480 ]    to further compare mses of all estimates , in figure  [ mse2 ] we summarize the mse results with sample size @xmath140 on cases from figures  [ mse2n ] and [ mse2dex ] . from figure  [ mse2 ] ,",
    "the best - performance weight functions for blurring estimates are @xmath123 and double exponential with mean parameter equal to 1 .",
    "the best - performance weight functions for nonblurring estimates are @xmath141 and double exponential with mean 2 .",
    "all the mse values are very close in these 4 best - performance cases .",
    "while the nonblurring estimation produces much larger mses for peaked weight functions , the performance of blurring estimation is still competitive with nonblurring for relatively flat weight function .",
    "therefore , blurring is viewed as a more robust estimator for student-@xmath117 .",
    "we have also extended the experiment to student-@xmath117 distributions with 5 and 10 degrees of freedom .",
    "the results are presented in figures  [ mse5n]-[mse10 ] .",
    "they all support that the blurring estimates are more robust in the sense of smaller or competitive mses .",
    "the theoretic aspects behind these findings would be worthy of further exploration .     with 3 degrees of freedom .",
    "]     with 5 degrees of freedom and normal weight function . , height=480 ]     with 5 degrees of freedom and double exponential weight function . ]     with 5 degrees of freedom . ]     with 10 degrees of freedom and normal weight function . ]     with 10 degrees of freedom and double exponential weight function . ]     with 10 degrees of freedom . ]",
    "in this article , we have established the weak convergence and associated central limit theorem for the blurring and nonblurring processes .",
    "convergence points from both types of processes can be used for robust m - estimation of location . in our simulation study",
    ", it shows that the blurring type has a smaller mean square error than the nonblurring type if a weight function is reasonably chosen . the nonblurring type estimation is often adopted in robust statistics literature .",
    "our simulation results suggest that we shall consider the blurring type algorithm as an alternative choice to the existent nonblurring type algorithm for robust m - estimation .",
    "except for lemma  [ lemma3 ] with a direct proof , we will show the rest lemmas and theorem  [ thm : main ] by mathematical induction .",
    "for each individual lemma or theorem , we will first show that it is valid for @xmath43 .",
    "next by assuming the validity of lemmas  [ lemma : order]-[lemma : sk_conv ] as well as theorem  [ thm : main ] for @xmath117 and the validity of preceding lemmas for @xmath142 , we will establish the claim of the current target lemma .",
    "for instance , say , the current target lemma that we want to prove is lemma  [ lemma4 ] .",
    "we first show that lemma  [ lemma4 ] is valid for @xmath43 .",
    "next , by assuming that lemmas  [ lemma : order]-[lemma : sk_conv ] and theorem  [ thm : main ] hold for @xmath117 and further assuming that lemmas  [ lemma : order]-[lemma3 ] hold for @xmath142 , we will establish the claim of lemma  [ lemma4 ] for @xmath142 .      we will prove that for @xmath60 , @xmath143 for any symmetric probability density function @xmath144 .",
    "the proof for @xmath145 is identically the same .",
    "we first consider the case that @xmath146 .",
    "from @xmath147 we have @xmath148 since @xmath149 is log - concave , for any @xmath150 , @xmath151 is non - increasing for all @xmath36 .",
    "therefore @xmath152 and @xmath153 then @xmath154 similar , we have @xmath155 note that @xmath156 since @xmath157 . since @xmath149 is non - constant , there exists @xmath158 , such that @xmath159",
    ". therefore the inequality in ( [ eq : order2 ] ) is strictly less . combining ( [ eq : order1 ] ) and ( [ eq : order2 ] ) , we have @xmath160 therefore @xmath161 for the case that @xmath162 , the proof is almost the same .",
    "now @xmath163 then we have @xmath164 and @xmath165 combining both will again lead to @xmath166 .",
    "similar to lemma  [ lemma : order ] , we will prove this lemma for @xmath167 .",
    "the proof for @xmath39 is identically the same . by definition ,",
    "we have @xmath168 since @xmath149 is finite integrable , @xmath169 is bounded .",
    "then , there exists a constant @xmath170 such that @xmath171 from the weak convergence of @xmath4 , we have @xmath172 also note that , for a fixed @xmath36 @xmath173 is bounded away from @xmath174 .",
    "thus , from ( [ eq : g_n_splus ] ) , ( [ eq : num ] ) and ( [ eq : den ] ) , @xmath175            for the first term , we need to to calculate @xmath179 .",
    "since @xmath180 converges to @xmath181 almost surely , the inverse function @xmath182 also converges to @xmath183 almost surely .",
    "note that @xmath184 .",
    "expanding @xmath185 at @xmath186 , we have @xmath187 where @xmath188 is some number between @xmath36 and @xmath186 . since @xmath189 a.s . , @xmath190 a.s .",
    "therefore , @xmath191 a.s .",
    "then the first term in  ( [ ineq : cdf2term ] ) can be calculated as follows .",
    "@xmath192 & = & \\frac { d   f_n^\\t ( \\xi ) } { d   \\xi } \\big|_{\\xi = c_n } \\times \\sqrt{n}\\ ,     \\left(\\xi_n^\\tplus(x)-\\xi^\\tplus(x ) \\right)\\nonumber\\\\[1ex ] & = & \\left\\{\\frac { d   f^\\t ( \\xi ) } { d\\xi } \\big|_{\\xi=\\xi^\\tplus(x ) } + o_p(1)\\right\\}\\times     \\bigg\\{-\\left(\\frac { d   \\xi^\\tplus(u)}{d   u}\\big|_{u = x}+o_p(1)\\right ) \\nonumber\\\\ & & \\times \\sqrt{n}\\left(\\eta_n^\\tplus(\\xi^\\tplus(x ) )   -\\eta^\\tplus(\\xi^\\tplus(x))\\right)\\bigg\\}\\nonumber\\\\[1ex ] & = & -\\left(\\frac { d   f^\\tplus ( u ) } { d   u } \\big|_{u = x } + o_p(1)\\right )   \\times \\sqrt{n}\\left(\\eta_n^\\tplus(\\xi^\\tplus(x ) )   -\\eta^\\tplus(\\xi^\\tplus(x))\\right)\\nonumber\\\\[1ex ] & = & -f^\\tplus ( x)\\times \\sqrt{n}\\left(\\eta_n^\\tplus(\\xi^\\tplus(x ) )   -\\eta^\\tplus(\\xi^\\tplus(x))\\right ) + o_p(1 ) , \\label{eta_n_in_lem3}\\end{aligned}\\ ] ] where @xmath193 is some number between @xmath182 and @xmath183 .",
    "apply ( [ eq : wc_g ] ) in lemma  [ lemma3 ] to ( [ eta_n_in_lem3 ] ) , and then this lemma can be established .",
    "let @xmath194 denote the class of strictly increasing continuous mappings from @xmath80 $ ] onto itself .",
    "for @xmath195 , define @xmath196 the metric of @xmath197 is defined by ( see , e.g. , billingsley 1968 ) @xmath198 the topology generated by this metric is called the skorokhod topology . for @xmath199 , there exists @xmath200 such that @xmath201 this implies that @xmath202 then , we have @xmath203 plugging in defining expression for @xmath204 , we have @xmath205 & = & \\int_{y=-\\infty}^\\infty \\left|   -\\frac{f^\\tplus(x)\\cdot(y - x ) w(y-\\xi^\\tplus(x ) ) } { { \\int   w\\left(y-\\xi^\\tplus(x)\\right ) df^\\t(y ) } } + 1_{\\{y\\leq \\xi^\\tplus(x)\\ } } \\right| d f^\\t(y)\\\\[1ex ] & \\leq & \\int_{y=-\\infty}^\\infty \\left|   \\frac{f^\\tplus(x)\\cdot |y+x|\\cdot w(y-\\xi^\\tplus(x ) ) } { { \\int   w\\left(y-\\xi^\\tplus(x)\\right ) df^\\t(y ) } }   \\right| d f^\\t(y)+1.\\end{aligned}\\ ] ] note that @xmath206 and that @xmath207 where @xmath208 therefore , for @xmath199 and @xmath209 $ ] , we have @xmath210 hence , @xmath211 is continuous .",
    "we will prove this theorem by mathematical induction .",
    "for @xmath43 , statements ( i ) and ( ii ) are well - known results as almost sure convergence of empirical cdf and donsker s theorem , respectively .",
    "( see , e.g. , dudley , 1999 . )",
    "assume statements ( i ) and ( ii ) hold for @xmath212 .",
    "then , statement  ( i ) with @xmath53 is an immediate result of lemma  [ lemma2 ] .",
    "it is now left to show statement  ( ii ) with @xmath53 to complete the proof by mathematical induction .",
    "recall that @xmath213 . by lemma  [ lemma :",
    "sk_conv ] , @xmath214 by assumption @xmath215 , then @xmath216 therefore , @xmath217 where @xmath94 and @xmath218 by mathematical induction , we have shown the almost sure convergence of @xmath5 and the weak convergence of @xmath219 .",
    "we will use similar mathematical induction arguments to prove the weak convergence for the nonblurring case . for @xmath220 , nonblurring and blurring process",
    "are identically the same .",
    "therefore the statements hold for @xmath220 .",
    "assume that they hold for @xmath212 , we will prove that they hold for @xmath53 .",
    "since @xmath221 @xmath222 s are the same for all @xmath117 , i.e. @xmath223}(x ) = \\eta^{[2]}(x)=\\cdots$ ] . with the same arguments for blurring process , we have @xmath224 and @xmath225 then by similar arguments , @xmath226 & = & \\sqrt{n}\\left(f_n^\\sn(\\xi_n^\\snplus(x ) )     - f_n^\\sn(\\xi^\\snplus(x ) \\right)+\\sqrt{n}\\left(f_n^\\sn(\\xi^\\snplus(x ) )     - f^\\sn(\\xi^\\snplus(x ) \\right)\\nonumber\\\\[1ex ] & = & \\left(\\frac { d   f^\\sn ( \\xi ) } { d\\xi } \\big|_{\\xi=\\xi^\\snplus(x ) } + o_p(1)\\right)\\cdot   \\left(-\\frac { d   \\xi^\\snplus(u)}{d   u}\\big|_{u = x}+o_p(1)\\right ) \\nonumber\\\\ & & \\times \\sqrt{n}\\left(\\eta_n^\\snplus(\\xi^\\snplus(x))-\\eta^\\snplus(\\xi^\\snplus(x))\\right)\\nonumber\\\\ & & + \\sqrt{n}\\left(f_n^\\sn(\\xi^\\snplus(x ) )     - f^\\sn(\\xi^\\snplus(x ) \\right)\\nonumber\\\\[1ex ] & = & \\int_y ( k^\\snplus(x , y)+o_p(1 ) ) \\",
    ", d\\sqrt{n}\\left(f_n(y)-f(y)\\right)\\nonumber \\\\ & & + \\sqrt{n}\\left(f_n^\\sn(\\xi^\\snplus(x ) )     - f^\\sn(\\xi^\\snplus(x ) \\right),\\label{eq : nb_part}\\end{aligned}\\ ] ] where @xmath227 for @xmath220 , @xmath228}(x)-f^{[1]}(x)\\right)\\\\ & = & \\int_y ( k^{[1]}(x , y)+o_p(1 ) ) \\ , d\\sqrt{n}\\left(f_n(y)-f(y)\\right )    + \\sqrt{n } \\left(f_n^{[0]}(\\xi^{[1]}(x ) )     - f^{[0]}(\\xi^{[1]}(x ) \\right)\\\\ & = & \\int_y ( k^{[1]}(x , y)+1_{\\{y \\leq \\xi^{[1]}(x)\\}}+ o_p(1 ) ) \\ , d\\sqrt{n}\\left(f_n(y)-f(y)\\right)\\\\ & = & \\int_y ( k^{[1]}(x , y)+1_{\\{\\eta^{[1]}(y ) \\leq x\\ } } + o_p(1 ) ) \\ , d\\sqrt{n}\\left(f_n(y)-f(y)\\right).\\end{aligned}\\ ] ] let @xmath229}(u , v)=k^{[1]}(f^{-1}(u),f^{-1}(v ) ) + 1_{\\{\\eta^{[1]}(f^{-1}(v ) ) \\leq f^{-1}(u)\\}}$ ] . by mathematical induction assumption of the statement at @xmath212",
    ", we have @xmath230 where @xmath231 , @xmath232 , is defined iteratively via  ( [ eq : h_iter ] ) .",
    "take this into ( [ eq : nb_part ] ) , it becomes @xmath233 where @xmath234 then by similar arguments as in the blurring case , we have @xmath235 where @xmath236}(v).\\ ] ]      from @xmath237 then @xmath238 since @xmath239 is bounded .",
    "therefore @xmath240 next , we have @xmath241 by fubini theorem , since the integral of the absolute value is finite , we can change the order .",
    "@xmath242 since @xmath243 then @xmath244            chen , t.l . , hsieh , d.n . , hung , h. , tu , i.p .",
    ", wu , p.s .",
    ", wu , y.m . , chang , w. and huang , s.y .",
    "@xmath245-sup : a clustering algorithm for cryo - electron microscopy images of asymmetric particles . _",
    "annals of applied statistics _",
    ", 8(1):259285 ."
  ],
  "abstract_text": [
    "<S> this article studies the weak convergence and associated central limit theorem for blurring and nonblurring processes . </S>",
    "<S> then , they are applied to the estimation of location parameter . </S>",
    "<S> simulation studies show that the location estimation based on the convergence point of blurring process is more robust and often more efficient than that of nonblurring process .    </S>",
    "<S> * keywords * : weak convergence , central limit theorem , blurring process , robust estimation .    _ </S>",
    "<S> -(t)itn-[t ] isn-[s ] _ </S>"
  ]
}