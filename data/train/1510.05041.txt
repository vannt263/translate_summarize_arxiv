{
  "article_text": [
    "matrix scaling , additions and multiplications are basic operations in linear algebra libraries , scientific simulations and deep learning . offloading these operations to the blas library , which is the case in general practices ,",
    "can substantially improve the application performance due to the architecture specific optimizations inside blas kernels .",
    "this leads blas to become the standard building blocks for performing low level matrix operations in applications .",
    "hence , the blas library directly affects the application performance . in the last few years , the evolving gpu architectures , nvidia s kepler and maxwell in particular , feature thousands of stream processors , proven to be extremely efficient in computing level 3 blas .    while a multi - gpu system offers appealing high performance , using it entails nontrivial effort .",
    "a multi - gpu system typically consists of at least one cpus connected with peripheral gpus on the pci - e .",
    "gpu operates on its private onboard ram while cpu operates on the host ram . for gpus sharing the same i / o hub",
    ", they can directly communicate via pci - e switch referred to as gpu p2p access .",
    "the following factors need to be considered to fully utilize such architecture : ( 1 ) nowadays gpus of different architectures represent divergent computing capabilities ; and even the realtime performance of a gpu varies with respect to factors such as kernel saturation and gpu occupancy , all of which pose a great challenges to load balancing .",
    "( 2 ) minimizing and overlapping communication is key to achieve high performance .",
    "( 3 ) reducing the cpu - gpu communication to the gpu - gpu communication further improves the communication and energy efficiency .",
    "unfortunately , our study indicates the existing multi - gpu level-3 blas fail to optimize toward these factors , thereby delivering sub - optimal performance .    in this paper",
    ", we present the blasx : a high - performance level-3 blas library for heterogeneous multi - gpu systems . we address the load balancing with a dynamic scheduling runtime , which handles task level workload variations , single gpu realtime performance variations and speed discrepancies among heterogeneous multi - gpus .",
    "blasx adopts a novel two level hierarchical tile caches to explore the tile temporal locality , in which we consider the gpu onboard ram as the l1 tile cache and the combined multi - gpu rams as the l2 tile cache .",
    "the l1 tile cache minimizes global communication ; and the l2 tile cache successfully reduces the cpu - gpu communication to the gpu - gpu communication . in implementing this hierarchical tile caches , we propose a new lru algorithm to accommodate the asynchronous task progression and a new cache coherence protocol to ensure the data consistency on multi - gpus .",
    "blasx also optimizes the communication / computation overlapping on gpu streams so that the communication cost is negligible",
    ". finally , blasx offers backward compatibility to the vast existing cpu blas based applications ; thereby all the details , such as workload balancing , data caching , communication overlapping and memory management , can be ignored by library users .",
    "we evaluate blasx on two multi - gpu systems , everest and makalu ( table [ machines ] ) , against the related leading academic and industrial projects including cublas - xt , supermatrix , magma and parsec .",
    "blasx consistently outperforms the academic implementations on everest with 3 nvidia kepler k40c .",
    "in contrast to the highly optimized nvidia cublas - xt , blasx demonstrates on average 25% performance gain and 200% less communication volume .",
    "makalu features 2 kelper k40 and 2 maxwell titan x. blasx successfully tackles the heterogeneity and demonstrates linear speedup ; whereas other libraries such as cublas - xt , magma and supermatrix suffers from poor scalability .",
    "we organize the remaining paper as follows .",
    "section ii analyzes the background and related works ; and section iii briefly reviews the l3 blas tile algorithms . in section iv",
    ", we elaborate the detailed design and implementations of blasx including two level hierarchical tile caches and the scheduling runtime .",
    "we also present solutions to specific questions such as amortizing high frequency memory allocation and deallocation , communication and computation overlapping .",
    "the comprehensive evaluations of blasx against existing state - of - art implementations are presented in the section v. finally , we conclude at section vi .",
    "there are three levels of blas , divided with respect to the complexity of operations .",
    "level-1 ( l1 ) blas targets vector operations in @xmath0 such as vector dot products and vector norms . level-2 ( l2 ) blas targets matrix - vector operations in @xmath1 such as matrix - vector multiplication .",
    "level-3 ( l3 ) blas @xcite targets matrix operations in @xmath2 time such as matrix - matrix multiplications .",
    "the focus of this research is on l3 blas , which uses general matrix multiplication ( gemm ) as the primary building block for the routines within the category .",
    "therefore , the task of improving the performance of l3 blas can be reduced to the gemm speed .",
    "the massive but economic tflops brought forth by the evolving gpu architectures drives interests in the various implementations of multi - gpu l3 blas .",
    "supermatrix @xcite is one of the pioneers of matrix operation parallelization on smp multicores , however it provides limited support on gpus .",
    "the key insight of supermatrix is that a matrix can be factorized into a set of tiles .",
    "the tomasolu algorithm @xcite subsequently schedules these tiles in the out - of - order fashion .",
    "fig.[supermatrix ] demonstrates that supermatrix suffers from costly nonoverlapped cpu - gpu data transfers .",
    "starpu provides a centralized interface to various accelerator technologies @xcite .",
    "in contrast to supermatrix , starpu supports versatile scheduling algorithms such as work stealing @xcite and priority scheduling @xcite while requiring manual annotations to optimize under a specific problem .",
    "the insufficient communication / computation overlapping and the low gpu saturation in fig.[starpu ] demonstrate the suboptimal dgemm implementation in starpu .",
    "magma @xcite is another multi - gpu linear algebra library with incomplete lapack and blas support .",
    "it is a heavily hand tuned library relying on a static load balancer , which degrades magma s performance on heterogeneous multi - gpu systems .",
    "direct acyclic graph ( dag ) scheduling has seen a revival in the recent years as it can naturally integrate with tile algorithms .",
    "parsec @xcite is a leading dag scheduling runtime for dense linear algebraic operations .",
    "building dags at runtime and scheduling tasks within dags , however , can be a huge cost for the small scale l3 blas operations .",
    "parsec also assumes constant workload on each fine - grained task and constant speed on each gpu .",
    "it is possible to have workload variations and the gpu kernel saturation also affects the actual execution speed . in addition , parsec only exploits tile reusing within a single gpu ; caching on multigpu memory spaces by taking advantages of gpu - gpu p2p communication still remains unexplored .",
    "none of the aforementioned libraries are backward compatible to legacy cpu blas . as an reaction to the market",
    ", nvidia released a commercial multigpu l3 blas , cublas - xt @xcite , declaring it to be backward compatible when using the nvblas wrapper .",
    "cublas - xt consistently moves tiles on demand into gpu ram so that it can compute a large scale problem with a few mb of gpu ram . although major communication is overlapped , it does not address tile caching ; and this aggressive on demand communication pattern extremely overloads the pci - e as shown by the contiguous yellow blocks in fig.[cublas - xt ] .    in summary , these libraries can not deliver the optimal performance due to following issues : 1 ) insufficient communication / computation overlapping subjects supermatrix and starpu to suboptimal performance .",
    "2 ) excessive communication in cublas - xt overloads the pci - e dragging down the overall performance .",
    "3 ) low gpu occupancies in supermatrix and starpu indicate partial gpu utilization .",
    "4 ) efficient gpu - gpu p2p communication remains unexplored .",
    "we observe that the average throughput of cpu - gpu communication is 6.54 gb / s while the gpu - gpu is 7.80 gb / s .",
    "5 ) static scheduling in the cublas - xt and magma can not tackle the hardware heterogeneity .",
    "blasx successfully resolves these issues as demonstrated in fig.[blasx ] ; please refer to _ performance evaluation _ for detailed discussion .",
    "in this section , we give an overview of l3 blas tile algorithms .",
    "l3 blas is intended for @xmath3 matrix operations including general matrix multiplication ( gemm ) , symmetric rank - k update ( syrk ) , symmetric rank-2k update ( syr2k ) , triangular matrix multiplication ( trmm ) , symmetric matrix multiply ( symm ) , and triangular solve with multiple right hand side ( trsm ) .",
    "since the hermitian matrix multiplication ( hemm ) , hermitian rank - k update ( herk ) , and hermitian rank-2k update ( her2k ) are the complex counterparts of gemm , syrk and syr2k respectively , we omit their discussion in this paper .",
    "the tile algorithm logically partitions a matrix into its tiled representation .",
    "given tile size @xmath4 in a matrix of size @xmath5 , it creates @xmath6 square tiles of size @xmath7 and @xmath8 non - square tiles .",
    "furthermore , the algorithm treats tiles , uniquely indexed by row and column , as the basic elements in a matrix in lieu of scalars .",
    "operations on matrices are subsequently reduced to operations on tiles . as our focus is the l3 blas",
    ", we assume the tile indices of the output matrix are @xmath9 $ ] , the tile indices of the matrix to the left side of multiply operator are @xmath10 $ ] and the tile indices of matrix to the right of multiply operator are @xmath11 $ ] . hence tile indices @xmath12 and @xmath13 uniquely identify a tile , @xmath14 , in the output matrix while the upper bound of @xmath15 represents the computational intensity to solve the @xmath14 .",
    "[ gemm_percentages ]    .gemm percentages at 3 different matrix sizes n. [ cols=\"^,^,^,^\",options=\"header \" , ]",
    "existing l3 blas libraries such as parsec , magma , supermatrix , cublas - xt suffers from issues such as backward compatibility , insufficient communication / computation overlapping , inefficient communication and poor scalability . in this paper , we design and implement blasx , a suite of l3 blas , that delivers the best l3 blas performance on heterogeneous multigpu systems .",
    "we introduce a novel two level hierarchical tile cache to reduce the global communication ; our locality aware scheduling runtime perfectly balances the load across heterogeneous gpus and cpus .",
    "we optimized communication / computation overlapping on streams to renders trivial communication cost ; thereby blasx computes in a out - of - core fashion that insures input and output data always remains on the host ram .",
    "extensive benchmarks demonstrate that blasx consistently outperforms the leading industrial and academia related projects in terms of performance , scalability , and communication efficiency .",
    "more importantly , blasx requires the least effort from users to integrate with the vast amount of legacy blas based applications .",
    "dongarra , jack j. , et al .",
    "`` a set of level 3 basic linear algebra subprograms . ''",
    "acm transactions on mathematical software ( toms ) 16.1 ( 1990 ) : 1 - 17 .",
    "chan , ernie , et al .",
    "`` supermatrix : a multithreaded runtime scheduling system for algorithms - by - blocks . ''",
    "proceedings of the 13th acm sigplan symposium on principles and practice of parallel programming .",
    "acm , 2008 .",
    "tomasulo , robert m. `` an efficient algorithm for exploiting multiple arithmetic units . ''",
    "ibm j. res .",
    "dev ( 1995 ) : 13 - 21 .",
    "augonnet , cdric , et al .",
    "`` starpu : a unified platform for task scheduling on heterogeneous multicore architectures . '' concurrency and computation : practice and experience 23.2 ( 2011 ) : 187 - 198 .",
    "blumofe , robert d. , and charles e. leiserson . ``",
    "scheduling multithreaded computations by work stealing . ''",
    "journal of the acm ( jacm ) 46.5 ( 1999 ) : 720 - 748 .",
    "leung , joseph y - t . , and jennifer whitehead .",
    "`` on the complexity of fixed - priority scheduling of periodic , real - time tasks . ''",
    "performance evaluation 2.4 ( 1982 ) : 237 - 250 .",
    "nath , rajib , et al .",
    "`` optimizing symmetric dense matrix - vector multiplication on gpus . ''",
    "proceedings of 2011 international conference for high performance computing , networking , storage and analysis ( sc ) .",
    "acm , 2011 .",
    "wu , wei , et al .",
    "`` hierarchical dag scheduling for hybrid distributed systems . ''",
    "29th ieee international parallel and distributed processing symposium .",
    "developer.nvidia.com/cublasxt goto , kazushige , and robert van de geijn .",
    "`` high - performance implementation of the level-3 blas . ''",
    "acm transactions on mathematical software ( toms ) 35.1 ( 2008 ) : 4 .",
    "schroeder , tim c. `` peer - to - peer and unified virtual addressing . ''",
    "gpu technology conference , nvidia .",
    "kdzierski , kamil , et al .",
    "`` adapting cache partitioning algorithms to pseudo - lru replacement policies . ''",
    "parallel & distributed processing ( ipdps ) , 2010 ieee international symposium on .",
    "ieee , 2010 .",
    "sweazey , paul , and alan jay smith .",
    "`` a class of compatible cache consistency protocols and their support by the ieee futurebus . ''",
    "acm sigarch computer architecture news .",
    "2 . ieee computer society press , 1986 .",
    "song , fengguang , stanimire tomov , and jack dongarra .",
    "`` enabling and scaling matrix computations on heterogeneous multi - core and multi - gpu systems . ''",
    "proceedings of the 26th acm international conference on supercomputing .",
    "acm , 2012 .",
    "michael , maged m. , and michael l. scott .",
    "`` simple , fast , and practical non - blocking and blocking concurrent queue algorithms .",
    "'' proceedings of the fifteenth annual acm symposium on principles of distributed computing .",
    "acm , 1996 .",
    "jia , yangqing , et al .",
    "`` caffe : convolutional architecture for fast feature embedding . ''",
    "proceedings of the acm international conference on multimedia .",
    "acm , 2014 .",
    "chellapilla , kumar , sidd puri , and patrice simard .",
    "`` high performance convolutional neural networks for document processing . ''",
    "tenth international workshop on frontiers in handwriting recognition .",
    "suvisoft , 2006 .",
    "rumelhart , david e. , geoffrey e. hinton , and ronald j. williams .",
    "`` learning representations by back - propagating errors . ''",
    "cognitive modeling 5 ( 1988 ) : 3 .",
    "krizhevsky , alex , and geoffrey hinton .",
    "`` learning multiple layers of features from tiny images . ''",
    "wang , linnan , et al .",
    "`` large scale artificial neural network training using multigpus ''",
    "sc poster ( 2015 ) .",
    "seidel , raimund .",
    "`` on the all - pairs - shortest - path problem in unweighted undirected graphs . '' journal of computer and system sciences 51.3 ( 1995 ) : 400 - 403 .",
    "bendse , martin p. , and ole sigmund .",
    "`` topology optimization . ''",
    "( 2009 ) : 3928 - 3929 .",
    "szabo , barna aladar , and ivo babuka .",
    "finite element analysis .",
    "john wiley & sons , 1991 ."
  ],
  "abstract_text": [
    "<S> basic linear algebra subprograms ( blas ) are a set of low level linear algebra kernels widely adopted by applications involved with the deep learning and scientific computing . </S>",
    "<S> the massive and economic computing power brought forth by the emerging gpu architectures drives interest in implementation of compute - intensive level 3 blas on multi - gpu systems . in this paper , we investigate existing multi - gpu level 3 blas and present that 1 ) issues , such as the improper load balancing , inefficient communication , insufficient gpu stream level concurrency and data caching , impede current implementations from fully harnessing heterogeneous computing resources ; 2 ) and the inter - gpu peer - to - peer(p2p ) communication remains unexplored . </S>",
    "<S> we then present blasx : a highly optimized multi - gpu level-3 blas . </S>",
    "<S> we adopt the concepts of algorithms - by - tiles treating a matrix tile as the basic data unit and operations on tiles as the basic task . </S>",
    "<S> tasks are guided with a dynamic asynchronous runtime , which is cache and locality aware . </S>",
    "<S> the communication cost under blasx becomes trivial as it perfectly overlaps communication and computation across multiple streams during asynchronous task progression . </S>",
    "<S> it also takes the current tile cache scheme one step further by proposing an innovative 2-level hierarchical tile cache , taking advantage of inter - gpu p2p communication . as a result </S>",
    "<S> , linear speedup is observable with blasx under multi - gpu configurations ; and the extensive benchmarks demonstrate that blasx consistently outperforms the related leading industrial and academic projects such as cublas - xt , supermatrix , magma and parsec .    </S>",
    "<S> blas , scheduling runtime , tile algorithms , multigpus , hierarchical tile caches </S>"
  ]
}