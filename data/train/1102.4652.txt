{
  "article_text": [
    "by exploiting signal sparsity and smart reconstruction schemes , compressive sensing ( cs )  @xcite can enable signal acquisition with fewer measurements than traditional sampling . in cs ,",
    "an @xmath0-dimensional signal @xmath1 is measured through @xmath2 random linear measurements .",
    "although the signal may be undersampled ( @xmath3 ) , it may be possible to recover @xmath1 assuming some sparsity structure .",
    "so far , most of the cs literature has considered signal recovery directly from linear measurements . however , in many practical applications , measurements have to be discretized to a finite number of bits .",
    "the effect of such quantized measurements on the performance of the cs reconstruction has been studied in  @xcite . in  @xcite",
    "the authors adapt cs reconstruction algorithms to mitigate quantization effects . in  @xcite ,",
    "high - resolution functional scalar quantization theory was used to design quantizers for lasso reconstruction  @xcite .",
    "the contribution of this paper to the quantized cs problem is twofold : first , for quantized measurements , we propose reconstruction algorithms based on gaussian approximations of belief propagation ( bp ) .",
    "bp is a graphical model - based estimation algorithm widely used in machine learning and channel coding  @xcite that has also received significant recent attention in compressed sensing  @xcite .",
    "although exact implementation of bp for dense measurement matrices is generally computationally difficult , gaussian approximations of bp have been effective in a range of applications  @xcite .",
    "we consider a recently developed gaussian - approximated bp algorithm , called _ relaxed belief propagation _",
    "@xcite , that extends earlier methods  @xcite to nonlinear output channels .",
    "we show that the relaxed bp method is computationally simple and , with quantized measurements , provides significantly improved performance over traditional cs reconstruction based on convex relaxations .",
    "our second contribution concerns the quantizer design . with linear reconstruction and mean - squared error distortion",
    ", the optimal quantizer simply minimizes the mean squared error ( mse ) of the transform outputs .",
    "thus , the quantizer can be optimized independently of the reconstruction method .",
    "however , when the quantizer outputs are used as an input to a nonlinear estimation algorithm , minimizing the mse between quantizer input and output is not necessarily equivalent to minimizing the mse of the final reconstruction . to optimize the quantizer for the relaxed bp algorithm , we use the fact that the mse under large random transforms can be predicted accurately from a set of simple state evolution ( se ) equations  @xcite",
    "then , by modeling the quantizer as a part of the measurement channel , we use the se formalism to optimize the quantizer to asymptotically minimize distortions after the reconstruction by relaxed bp .",
    "in a noiseless cs setting the signal @xmath4 is acquired via @xmath3 linear measurements of the type @xmath5 where @xmath6 is the _ measurement matrix_. the objective is to recover @xmath1 from @xmath7 . although the system of equations formed is underdetermined , the signal is still recoverable if some favorable assumptions about the structure of @xmath1 and @xmath8 are made .",
    "generally , in cs the common assumption is that the signal is exactly or approximately sparse in some orthonormal basis @xmath9 , i.e. , there is a vector @xmath10 with most of its elements equal or close to zero .",
    "additionally , for certain guarantees on the recoverability of the signal to hold , the matrix @xmath8 must satisfy the _ restricted isometry property _ ( rip )  @xcite .",
    "some families of random matrices , like appropriately - dimensioned matrices with i.i.d .",
    "gaussian elements , have been demonstrated to satisfy the rip with high probability .    a common method for recovering the signal from the measurements is basis pursuit .",
    "this involves solving the following optimization problem : @xmath11 where @xmath12 is the @xmath13-norm of the signal .",
    "although it is possible to solve basis pursuit in polynomial time by casting it as a linear program ( lp )  @xcite , its complexity has motivated researchers to look for even cheaper alternatives like numerous recently - proposed iterative methods  @xcite . moreover , in real applications cs reconstruction scheme must be able to mitigate imperfect measurements , due to noise or limited precision  @xcite .      a quantizer is a function that discretizes its input by performing a mapping from a continuous set to some discrete set . more specifically , consider @xmath14-point regular scalar quantizer @xmath15 , defined by its output levels @xmath16 , decision boundaries @xmath17 , and a mapping @xmath18 when @xmath19  @xcite .",
    "additionally define the inverse image of the output level @xmath20 under @xmath15 as a _ cell _",
    "for @xmath22 , if @xmath23 we replace the closed interval @xmath24 by an open interval @xmath25 .",
    "typically quantizers are optimized by selecting decision boundaries and output levels in order to minimize the distortion between the random vector @xmath26 and its quantized representation @xmath27 .",
    "for example , for a given vector @xmath28 and the mse distortion metric , optimization is performed by solving @xmath29 where minimization is done over all @xmath14-level regular scalar quantizers .",
    "one standard way of optimizing @xmath15 is via the _",
    "lloyd algorithm _ , which iteratively updates the decision boundaries and output levels by applying necessary conditions for quantizer optimality  @xcite .",
    "however , for the cs framework finding the quantizer that minimizes mse between @xmath28 and @xmath30 is not necessarily equivalent to minimizing mse between the sparse vector @xmath31 and its cs reconstruction from quantized measurements @xmath32  @xcite .",
    "this is due to the nonlinear effect added by any particular cs reconstruction function .",
    "hence , instead of solving ( [ equ : quantization : optimal ] ) , it is more interesting to solve @xmath33 where minimization is performed over all @xmath14-level regular scalar quantizers and @xmath32 is obtained through a cs reconstruction method like relaxed bp or amp .",
    "this is the approach taken in this work .",
    "consider the problem of estimating a random vector @xmath34 from noisy measurements @xmath35 , where the noise is described by a measurement channel @xmath36 , which acts identically on each measurement @xmath37 of the vector @xmath38 obtained via ( [ equ : cs : measurement ] ) .",
    "moreover suppose that elements in the vector @xmath31 are distributed i.i.d .  according to @xmath39 .",
    "then we can construct the following conditional probability distribution over random vector @xmath31 given the measurements @xmath40 : @xmath41 where @xmath42 is the normalization constant and @xmath43 . by marginalizing this distribution",
    "it is possible to estimate each @xmath44 .",
    "although direct marginalization of @xmath45 is computationally intractable in practice , we approximate marginals through bp  @xcite .",
    "bp is an iterative algorithm commonly used for decoding of ldpc codes  @xcite .",
    "we apply bp by constructing a bipartite factor graph @xmath46 from ( [ equ : rbp : marginalization ] ) and passing the following messages along the edges @xmath47 of the graph : @xmath48 where @xmath49 means that the distribution is to be normalized so that it has unit integral and integration is over all the elements of @xmath1 except @xmath44 .",
    "we refer to messages @xmath50 as variable updates and to messages @xmath51 as measurement updates .",
    "we initialize bp by setting @xmath52 .",
    "earlier works on bp reconstruction have shown that it is asymptotically mse optimal under certain verifiable conditions .",
    "these conditions involve simple single - dimensional recursive equations called _ state evolution ( se ) _ , which predicts that bp is optimal when the corresponding se admits a unique fixed point  @xcite .",
    "nonetheless , direct implementation of bp is still impractical due to the dense structure of @xmath8 , which implies that the algorithm must compute the marginal of a high - dimensional distribution at each measurement node .",
    "however , as mentioned in section  [ sec : intro ] , bp can be simplified through various gaussian approximations , including the _ relaxed bp _ method  @xcite and _ approximate message passing ( amp )",
    "_  @xcite .",
    "recent theoretical work and extensive numerical experiments have demonstrated that , in the case of certain large random measurement matrices , the error performance of both relaxed bp and amp can also be accurately predicted by se . hence the optimal quantizers can be obtained in parallel for both of the methods , however in this paper we concentrate on design for relaxed bp , while keeping in mind that identical work can be done for amp as well .",
    "due to space limitations , in this paper we will limit our presentation of relaxed bp and se equations to the setting in figure  [ fig : probmod ] .",
    "see  @xcite for more general and detailed analysis .",
    "the vector @xmath53 denotes noiseless random measurements.,width=307 ]    consider the cs setting in figure  [ fig : probmod ] , where without loss of generality we assumed that @xmath54 .",
    "the vector @xmath34 is measured through the random matrix @xmath55 to result in @xmath56 , which is further perturbed by some additive white gaussian noise ( awgn ) .",
    "the resulting vector @xmath28 can be written as @xmath57 where @xmath58 are i.i.d .",
    "random variables distributed as @xmath59 .",
    "these noisy measurements are then quantized by the @xmath14-level scalar quantizer @xmath15 to give the cs measurements @xmath35 .",
    "the relaxed bp algorithm is used to estimate the signal @xmath31 from the corrupted measurements @xmath40 , given the matrix @xmath55 , noise variance @xmath60 , and the quantizer mapping @xmath15 . note that under this model each quantized measurement @xmath61 indicates that @xmath62",
    ", hence our measurement channel can be characterized as @xmath63 for @xmath64 and where @xmath65 is gaussian function @xmath66    relaxed bp can be implemented by replacing probability densities in ( [ equ : rbp : varupdatebp ] ) and ( [ equ : rbp : mesupdatebp ] ) by two scalar parameters each , which can be computed according to the following rules : @xmath67 where @xmath68 is the variance of the components @xmath69 .",
    "additionally , at each iteration we estimate the signal via @xmath70 for each @xmath71 .",
    "we refer to messages @xmath72 as variable updates and to messages @xmath73 as measurement updates .",
    "the algorithm is initialized by setting @xmath74 and @xmath75 where @xmath76 and @xmath77 are the mean and variance of the prior @xmath78 .",
    "the nonlinear functions @xmath79 and @xmath80 are the conditional mean and variance @xmath81 where @xmath82 , @xmath83 , and @xmath84 .",
    "note that these functions admit closed - form expressions and can easily be evaluated for the given values of @xmath85 and @xmath86 .",
    "similarly , the functions @xmath87 and @xmath88 can be computed via @xmath89 where the functions @xmath90 and @xmath91 are the conditional mean and variance @xmath92 of the random variable @xmath93 .",
    "these functions admit closed - form expressions in terms of @xmath94 .",
    "the equations ( [ equ : rbp : varupdaterbpmean])([equ : rbp : varestimationrbp ] ) are easy to implement , however they provide us no insight into the performance of the algorithm .",
    "the goal of se equations is to describe the asymptotic behavior of relaxed bp under large measurement matrices .",
    "the se for our setting in figure  [ fig : probmod ] is given by the recursion @xmath95 where @xmath96 is the iteration number , @xmath97 is a fixed number denoting the measurement ratio , and @xmath68 is the variance of the awgn components which is also fixed .",
    "we initialize the recursion by setting @xmath98 , where @xmath77 is the variance of @xmath44 according to the prior @xmath78 .",
    "we define the function @xmath99 as @xmath100 where the expectation is taken over the scalar random variable @xmath82 , with @xmath101 , and @xmath102 .",
    "similarly , the function @xmath103 is defined as @xmath104 where @xmath88 is given by ( [ equ : rbp : d2 ] ) and the expectation is taken over @xmath36 and @xmath105 , with the covariance matrix @xmath106    one of the main results of  @xcite , which we present below for completeness , was to demonstrate the convergence of the error performance of the relaxed bp algorithm to the se equations under large sparse measurement matrices .",
    "denote by @xmath107 the number of nonzero elements per column of @xmath8 . in the large sparse limit analysis , first let @xmath108 with @xmath109 and keeping @xmath110 fixed .",
    "this enables the local - tree properties of the factor graph @xmath111 .",
    "then let @xmath112 , which will enable the use of a central limit theorem approximation .",
    "[ thm : rbptoseconvergence ] consider the relaxed bp algorithm under the large sparse limit model above with transform matrix @xmath8 and index @xmath113 satisfying the assumption 1 of  @xcite for some fixed iteration number @xmath114 .",
    "then the error variances satisfy the limit @xmath115 where @xmath116 is the output of the se equation ( [ equ : se : serecursion ] ) .",
    "see  @xcite .",
    "another important result regarding se recursion in ( [ equ : se : serecursion ] ) is that it admits at least one fixed point .",
    "it has been showed that as @xmath117 the recursion decreases monotonically to its largest fixed point and if the se admits a unique fixed point , then relaxed bp is asymptotically mean - square optimal  @xcite .",
    "although in practice measurement matrices are rarely sparse , simulations show that se predicts well the behavior of relaxed bp .",
    "moreover , recently more sophisticated techniques were used to demonstrate the convergence of approximate message passing algorithms to se under large i.i.d .",
    "gaussian matrices  @xcite .",
    "we now return to the problem of designing mse - optimal quantizers under relaxed bp presented in ( [ equ : quantization : csoptimal ] ) . by modeling the quantizer as part of the channel and working out the resulting equations for relaxed bp and se",
    ", we can make use of the convergence results to recast our optimization problem to @xmath118 where minimization is done over all @xmath14-level regular scalar quantizers . in practice ,",
    "about 10 to 20 iterations are sufficient to reach the fixed point of @xmath116 . then by applying theorem  [ thm : rbptoseconvergence ] , we know that the asymptotic performance of @xmath119 will be identical to that of @xmath120 .",
    "it is important to note that the se recursion behaves well under quantizer optimization .",
    "this is due to the fact that se is independent of actual output levels and small changes in the quantizer boundaries result in only minor change in the recursion ( see ( [ equ : rbp : eout ] ) ) .",
    "although closed - form expressions for the derivatives of @xmath116 for large @xmath114 s are difficult to obtain , we can approximate them by using finite difference methods .",
    "finally , the recursion itself is fast to evaluate , which makes the scheme in ( [ equ : quantization : seoptimal ] ) practically realizable under standard optimization methods like sequential quadratic programming ( sqp ) .",
    "we now present experimental validation for our results .",
    "assume that the signal @xmath31 is generated with i.i.d .",
    "elements from the gauss - bernoulli distribution @xmath121 where @xmath122 is the sparsity ratio that represents the average fraction of nonzero components of @xmath31 . in the following experiments we assume @xmath123 .",
    "we form the measurement matrix @xmath8 from i.i.d .",
    "gaussian random variables , i.e. , @xmath124 ; and we assume that awgn with variance @xmath125 perturbs measurements before quantization .",
    "now , we can formulate the se equation ( [ equ : se : serecursion ] ) and perform optimization ( [ equ : quantization : seoptimal ] ) .",
    "we compare two cs - optimized quantizers : _ uniform _ and _ optimal_. we fix boundary points @xmath23 and @xmath126 , and compute the former quantizer through optimization of type ( [ equ : quantization : optimal ] ) . in particular , by applying the central limit theorem we approximate elements @xmath127 of @xmath28 to be gaussian and determine the _ uniform _ quantizer by solving ( [ equ : quantization : optimal ] ) , but with an additional constraint of equally - spaced output levels . to determine _ optimal _ quantizer , we perform ( [ equ : quantization : seoptimal ] ) by using a standard sqp optimization algorithm for nonlinear continuous optimization .     bits / component of @xmath31 .",
    "optimal quantizer is found by optimizing quantizer boundaries for each @xmath128 and then picking the result with smallest distortion , width=288 ]    figure  [ fig : boundaries ] presents an example of quantization boundaries .",
    "for the given bit rate @xmath129 over the components of the input vector @xmath31 , we can express the rate over the measurements @xmath28 as @xmath130 , where @xmath131 is the measurement ratio . to determine the optimal quantizer for the given rate @xmath129 we perform optimization for all @xmath128s and return the quantizer with the least mse .",
    "as we can see , in comparison with the uniform quantizer obtained by merely minimizing the distortion between the quantizer input and output , the one obtained via se minimization is very different ; in fact , it looks more concentrated around zero .",
    "this is due to the fact that by minimizing se we are in fact searching for quantizers that asymptotically minimize the mse of the relaxed bp reconstruction by taking into consideration the nonlinear effects due to the method .",
    "the trend of having more quantizer points near zero is opposite to the trend shown in  @xcite for quantizers optimized for lasso reconstruction .",
    "figure  [ fig : expdist ] presents a comparison of reconstruction distortions for our two quantizers and confirms the advantage of using quantizers optimized via ( [ equ : se : serecursion ] ) . to obtain the results we vary the quantization rate from @xmath132 to @xmath133 bits per component of @xmath31 , and for each quantization rate , we optimize quantizers using the methods discussed above . for comparison",
    ", the figure also plots the mse performance for two other reconstruction methods : linear mmse estimation and the widely - used lasso method  @xcite , both assuming a bounded uniform quantizer .",
    "the lasso performance was predicted by state evolution equations in  @xcite , with the thresholding parameter optimized by the iterative approach in  @xcite .",
    "it can be seen that the proposed relaxed bp algorithm offers dramatically better performance  more that @xmath134 db improvement at low rates . at higher rates ,",
    "the gap is slightly smaller since relaxed bp performance saturates due to the awgn at the quantizer input .",
    "similarly we can see that the mse of the quantizer optimized for the relaxed bp reconstruction is much smaller than the mse of the standard one , with more than 4 db difference for many rates .",
    "we present relaxed belief propagation as an efficient algorithm for compressive sensing reconstruction from the quantized measurements .",
    "we integrate ideas from recent generalization of the algorithm for arbitrary measurement channels to design a method for determining optimal quantizers under relaxed bp reconstruction .",
    "although computationally simpler , experimental results show that under quantized measurements relaxed bp offers significantly improved performance over traditional reconstruction schemes . additionally , performance of the algorithm is further improved by using the state evolution framework to optimize the quantizers .",
    "e.  j. cands , j.  romberg , and t.  tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ ieee trans .",
    "inform . theory _",
    "52 , pp .  489509 , feb ."
  ],
  "abstract_text": [
    "<S> we consider the optimal quantization of compressive sensing measurements following the work on generalization of relaxed belief propagation ( bp ) for arbitrary measurement channels . </S>",
    "<S> relaxed bp is an iterative reconstruction scheme inspired by message passing algorithms on bipartite graphs . its asymptotic error performance can be accurately predicted and tracked through the state evolution formalism . </S>",
    "<S> we utilize these results to design mean - square optimal scalar quantizers for relaxed bp signal reconstruction and empirically demonstrate the superior error performance of the resulting quantizers . </S>"
  ]
}