{
  "article_text": [
    "the additive white gaussian noise channel is basic to shannon theory and underlies practical communication models .",
    "we introduce classes of superposition codes for this channel and analyze their properties .",
    "we link theory and practice by showing superposition codes from polynomial size dictionaries with least squares decoding achieve exponentially small error probability for any communication rate less than the shannon capacity .",
    "a companion paper @xcite,@xcite provides a fast decoding method and its analysis .",
    "the developments involve a merging of modern perspectives on statistical linear model selection and information theory .",
    "the familiar communication problem is as follows .",
    "an encoder is required to map input bit strings @xmath0 of length @xmath1 into codewords which are length @xmath2 strings of real numbers @xmath3 , with norm expressed via the power @xmath4 .",
    "we constrain the average of the power across the @xmath5 codewords to be not more than @xmath6 .",
    "the channel adds independent @xmath7 noise to the selected codeword yielding a received length @xmath2 string @xmath8 .",
    "a decoder is required to map it into an estimate @xmath9 which we want to be a correct decoding of @xmath10 .",
    "block error is the event @xmath11 , bit error at position @xmath12 is the event @xmath13 , and the bit error rate is @xmath14 .",
    "an analogous section error rate for our code is defined below .",
    "the reliability requirement is that , with sufficiently large @xmath2 , the bit error rate or section error rate is small with high probability or , more stringently , the block error probability is small , averaged over input strings @xmath10 as well as the distribution of @xmath8 .",
    "the communication rate @xmath15 is the ratio of the input length to the codelength for communication across the channel .",
    "the supremum of reliable rates is the channel capacity @xmath16 , by traditional information theory as in @xcite , @xcite , @xcite .",
    "standard communication models , even in continuous - time , have been reduced to the above discrete - time white gaussian noise setting , as in @xcite,@xcite .",
    "this problem is also of interest in mathematics because of relationship to versions of the sphere packing problem as described in conway and sloane @xcite . for practical coding",
    "the challenge is to achieve rates arbitrarily close to capacity with a codebook of moderate size , while guaranteeing reliable decoding in manageable computation time .",
    "we introduce a new coding scheme based on sparse superpositions with a moderate size dictionary and analyze its performance .",
    "least squares is the optimal decoder .",
    "accordingly , we analyze the reliability of least squares and approximate least squares decoders .",
    "the analysis here is without concern for computational feasibility . in similar settings",
    "computational feasibility is addressed in the companion paper @xcite,@xcite , though the closeness to capacity at given reliability levels is not as good as developed here .",
    "we introduce sparse superposition codes and discuss the reliability of least squares in subsection [ sub : spar ] of this introduction .",
    "subsection [ sub : decod ] contrasts the performance of least squares with what is achieved by other methods of decoding . in subsection",
    "[ sub : pracd ] , we mention relations with work on sparse signal recovery in the high dimensional regression setting .",
    "subsection [ sub : awgncode ] discusses other codes and subsection [ sub : forneycover ] discusses some important forerunners to our developments here .",
    "our reliability bounds are developed in subsequent sections .",
    "we develop the framework for code construction by linear combinations .",
    "the story begins with a list ( or book ) @xmath17 of vectors , each with @xmath2 coordinates , for which the codeword vectors take the form of superpositions @xmath18 .",
    "the vectors @xmath19 which are linearly combined provide the terms or components of the codewords and the @xmath20 are the coefficients .",
    "the received vector is in accordance with the statistical linear model @xmath21 where @xmath22 is the matrix whose columns are the vectors @xmath17 and @xmath23 is the noise vector distributed normal(@xmath24 ) . in keeping with the terminology of that statistical setting , the book @xmath22 may be called the design matrix consisting of @xmath25 variables , each with @xmath2 observations , and this list of variables is also called the dictionary of candidate terms .",
    "the coefficient vectors @xmath26 are arranged to be of a specified form . for _ subset superposition coding _ we arrange for a number @xmath27 of the coordinates to be non - zero , with a specified positive value , and the message is conveyed by the choice of subset .",
    "denote @xmath28 .",
    "if @xmath29 is large , it is a _ sparse superposition code_. in this case , the number of terms sent is a small fraction of dictionary size . with somewhat greater freedom",
    ", one may arrange the non - zero coefficients to be @xmath30 or @xmath31 times a specified value , in which case the superposition code is said to be _",
    "signed_. then the message is conveyed by the sequence of signs as well as the choice of subset .    to allow such forms of @xmath26",
    ", we do not in general take the set of permitted coefficient vectors to be closed under a field of linear operations , and hence our linear statistical model does not correspond to a linear code in the sense of traditional algebraic coding theory . in a specialization",
    "we call a _ partitioned superposition code _ , the book @xmath22 is split into @xmath27 sections of size @xmath29 , with one term selected from each , yielding @xmath27 terms in each codeword out of a dictionary of size @xmath32 .",
    "likewise , the coefficient vector @xmath26 is split into sections , with one coordinate non - zero in each section to indicate the selected term .",
    "optionally , we have the additional freedom of choice of sign of this coefficient , for a signed partitioned code .",
    "it is desirable that the section sizes be not larger than a moderate order polynomial in @xmath27 or @xmath2 , for then the dictionary is arranged to be of manageable size .",
    "most convenient is the case that the sizes of these sections are powers of two",
    ". then an input bit string of length @xmath33 splits into @xmath27 substrings of size @xmath34 .",
    "the encoder mapping from @xmath10 to @xmath26 is then obtained by interpreting each substring of @xmath10 as simply giving the index of which coordinate of @xmath26 is non - zero in the corresponding section .",
    "that is , each substring is the binary representation of the corresponding index .",
    "as we have said , the rate of the code is @xmath35 input bits per channel uses and we arrange for @xmath36 arbitrarily close to @xmath37 . for the partitioned superposition code , this rate is @xmath38 . for specified rate @xmath36 , the codelength @xmath39 .",
    "thus , the length @xmath2 and the number of terms @xmath27 agree to within a log factor . with one term from each section , the number of possible codewords @xmath5 is equal to @xmath40 . alternatively ,",
    "if we allow for all subsets of size @xmath27 , the number of possible codewords would be @xmath41 , which is of order @xmath42 , for @xmath27 small compared to @xmath43 . to match the number of codewords",
    ", it would correspond to reducing @xmath43 by a factor of @xmath44 . though there would be the factor @xmath44 savings in dictionary size from allowing all subsets of the specified size ,",
    "the additional simplicity of implementation and simplicity of analysis with partitioned coding is such that we take advantage of it wherever appropriate .    with signed partitioned coding",
    "the story is similar , now with @xmath45 possible codewords using the dictionary of size @xmath46 .",
    "the input string of length @xmath47 , splits into @xmath27 sections with @xmath34 bits to specify the non - zero term and @xmath48 bit to specify its sign . for a rate @xmath36 code",
    "this entails a codelength of @xmath49 .",
    "control of the dictionary size is critical to computationally advantageous coding and decoding .",
    "possible dictionary sizes are between the extremes @xmath1 and @xmath5 dictated by the number and size of the sections , where @xmath1 is the number of input bits . at one extreme , with @xmath48 section of size @xmath50 ,",
    "one has @xmath22 as the whole codebook with its columns as the codewords , but the exponential size makes its direct use impractical .",
    "at the other extreme we have @xmath51 sections , each with two candidate terms in subset coding or two signs of a single term in sign coding with @xmath52 ; in which case @xmath22 is the generator matrix of a linear code .    between these extremes , we construct reliable , high - rate codes with codewords corresponding to linear combinations of subsets of terms in moderate size dictionaries .",
    "design of the dictionary is guided by what is known from information theory concerning the distribution of symbols in the codewords . by analysis of the converse to the channel coding theorem ( as in @xcite ) , for a reliable code at rate near capacity , with a uniform distribution on the sequence of input bits , the induced empirical distribution on coordinates of the codeword must be close to independent gaussian , in the sense",
    "that the resulting mutual information must be close to its maximum subject to the power constraint .",
    "we draw entries of @xmath22 independently from a normal distribution with mean zero and a variance we specify , yielding the properties we want with high probability .",
    "other distributions , such as independent equiprobable @xmath53 , might also suffice , with a near gaussian shape for the codeword distribution obtained by the convolutions associated with sums of terms in subsets of size @xmath27 .    for the vectors",
    "@xmath26 , the non - zero coefficients may be assigned to have magnitude @xmath54 , which with @xmath22 having independent entries of variance @xmath48 , yields codewords @xmath55 of average power near @xmath6 .",
    "there is a freedom of scale that allows us to simplify the coefficient representation .",
    "henceforth , we arrange the coordinates of @xmath19 to have variance @xmath56 and set the non - zero coefficients to have magnitude @xmath48 .",
    "optimal decoding for minimal average probability of error consists of finding the codeword @xmath55 with coefficient vector @xmath26 of the assumed form that maximizes the posterior probability , conditioning on @xmath22 and @xmath8 .",
    "this coincides , in the case of equal prior probabilities , with the maximum likelihood rule of seeking such a codeword to minimize the sum of squared errors in fit to @xmath8 .",
    "this is a least squares regression problem @xmath57 , with constraints on the coefficient vector .",
    "we show for all @xmath58 , that the least squares solution , as well as approximate least squares solutions such as may arise computationally , will have , with high probability , at most a negligible fraction of terms that are not correctly identified , producing a low bit error rate .",
    "the heart of the analysis shows that competing codewords that differ in a fraction of at least @xmath59 terms are exponentially unlikely to have smaller distance from @xmath8 than the true codeword , provided that the section size @xmath60 is polynomially large in the number of sections @xmath27 , where a sufficient value of @xmath61 is determined . for the partitioned superposition code",
    "there is a positive constant @xmath62 such that for rates @xmath36 less than the capacity @xmath37 , with a positive gap @xmath63 not too large , the probability of a fraction of mistakes at least @xmath59 is not more than @xmath64 consequently , for a target fraction of mistakes @xmath59 and target probability @xmath65 , the required number of sections @xmath27 or equivalently the codelength @xmath66 depends only polynomially on the reciprocal of the gap @xmath67 and on the reciprocal of @xmath59 .",
    "indeed @xmath2 of order",
    "@xmath68\\log ( 1/\\epsilon)$ ] suffices for the probability of the undesirable event to be less than @xmath65 .",
    "moreover , an approach is discussed which completes the task of identifying the terms by arranging sufficient distance between the subsets , using composition with an outer reed - solomon ( rs ) code of rate near one .",
    "the reed - solomon code is arranged to have an alphabet of size @xmath29 equal to a power of @xmath69 .",
    "it is tailored to the partitioned code by having the rs code symbols specify the terms selected from the sections .",
    "the outer rs code corrects the small fraction of remaining mistakes so that we end up not only with small section error rate but also with small block error probability . if @xmath70 is the rate of an rs code , with @xmath71 , then section error rate less than @xmath59 can be corrected , provided @xmath72 .",
    "further , if @xmath73 ( or simply @xmath36 ) is the rate associated with our inner ( superposition ) code , then the total rate after correcting for the remaining mistakes is given by @xmath74 .",
    "the end result , using our theory for the distribution of the fraction of mistakes of the superposition code , is that the block error probability is exponentially small .",
    "one may regard the composite code as a superposition code in which the subsets are forced to maintain at least a certain minimal separation , so that decoding to within a certain distance from the true subset implies exact decoding .",
    "particular interest is given to the case that the rate @xmath36 is made to approach the capacity @xmath37 .",
    "arrange @xmath75 and @xmath76 .",
    "one may let the rate gap @xmath77 tend to zero ( e.g. at a @xmath78 rate or any polynomial rate not faster than @xmath79 ) , then the overall rate @xmath80 continues to have drop from capacity of order @xmath77 , with the composite code having block error probability of order @xmath81 the exponent above , of order @xmath82 for @xmath36 near @xmath37 , is in agreement with the form of the optimal reliability bounds as in @xcite , @xcite , though here our constant @xmath62 is not demonstrated to be optimal .    in figure [",
    "fig : achrate ] we plot curves of achievable rates using our scheme for block error probability fixed at @xmath83 and signal to noise ratios of @xmath84 and @xmath85 . we also compare this to a rate curve given in polyanskiy , poor and verdu @xcite ( the ppv curve ) , where it is demonstrated that for a gaussian channel with signal to noise ratio @xmath86 , the block error probability @xmath65 , codelength @xmath2 and rate @xmath36 with an optimal code can be well approximated by the following relation , @xmath87 where @xmath88 is the channel dispersion and @xmath89 is the complementary gaussian cumulative distribution function .    for the superposition code curve ,",
    "the y - axis gives the highest @xmath90 for which the error probability stays below @xmath83 .",
    "these curves are based on the minimum of the bounds obtained by our lemma in section [ sec : rells ] .",
    "we see for the given @xmath86 and block error probability values , the achievable rates using our scheme are reasonably close to the theoretically best scheme .",
    "note that the ppv curve was computed with an approach that uses a codebook of size that is exponential in blocklength , whereas our dictionary , of size @xmath91 , is of considerably smaller size .      as we have said",
    "the least squares decoder minimizes @xmath92 with constraint on the form of coefficient vector @xmath26 .",
    "it is unknown whether approximate least squares decoding with rate @xmath36 near the capacity @xmath37 is practical in the equal power case studied here .",
    "alternative methods include an iterative decoder that we discuss briefly here and convex optimization methods discussed here and in subsection [ sub : pracd ] .",
    "the practical iterative decoder , for the partitioned superposition code , proposed and analyzed in @xcite,@xcite is called an _ adaptive successive decoder_. decoding is broken into multiple steps , with the identification of terms in a step achieved when the magnitude of the inner product between the corresponding @xmath93 s and a computed residual vector is above a specified threshold .",
    "the residual vector for each step being obtained as the difference of @xmath8 and the contribution from columns decoded in previous steps .    with a rate that is of order @xmath94 below capacity ,",
    "the error probability attained there is exponentially small in @xmath95 , to within a @xmath96 factor .",
    "this error exponent is slightly smaller than the optimal @xmath97 , obtained here by the least squares scheme .",
    "moreover , as we saw above , the least squares decoder achieves the optimal exponent for other orders @xmath77 of drop from capacity .",
    "the sparse superposition codes achieving these performance levels at rates near capacity , by least squares and by adaptive successive decoding are different in an important aspect . for the present paper ,",
    "we use a constant power allocation , with the same power @xmath56 for each term .",
    "however in @xcite , to yield rates near capacity we needed a variable power allocation , achieved by a specific schedule of the non - zero @xmath20 s .",
    "in contrast , if one were to use equal power allocation for the decoding scheme in @xcite , then reliable decoding holds only up to a threshold rate @xmath98 , which is less than the capacity @xmath37 , with the rate and capacity expressed in nats .",
    "the least squares optimization @xmath99 is made challenging by the non - convex constraint that there be a specified number of non - zero coefficients , one in each section .",
    "nevertheless , one can consider decoders based on projection to the convex hull .",
    "this convex hull consists of the @xmath26 vectors which have sum in each section equal to 1 .",
    "( with signed coding it becomes the constraint that the @xmath100 norm in each section is bounded by 1 . )",
    "geometrically , it provides a convex set of linear combinations in which the codewords are the vertices .",
    "decoding is completed with convex projection by moving to a vertex , e.g. with the largest coefficient value in each section .",
    "this is a setting in which we initiated investigations , however , in that preliminary analysis , we found that such @xmath100 constrained quadratic optimization allows for successful decoding only for rates up to @xmath101 for the equal power case .",
    "it is as yet unclear what its reliability properties would be at rates up to capacity @xmath37 with variable power .",
    "the conclusions regarding communication rate may be also expressed in the language of _ sparse signal recovery _ and _ compressed sensing_. a number of terms selected from a dictionary is linearly combined and subject to noise in accordance with the linear model framework @xmath102 .",
    "let @xmath43 be the number of variables and @xmath27 the number of non - zero terms .",
    "an issue dealt with by these fields , is the minimal number of observations @xmath2 sufficient to reliably recover the terms . in our setting ,",
    "the non - zero values of the coefficients are known and @xmath2 satisfies the relationship @xmath103 for general subsets and @xmath104 for the partitioned case .",
    "we show that reliable recovery is possible provided @xmath58 .",
    "the conclusions here complement recent work on sparse signal recovery @xcite,@xcite , @xcite in the sparse noise case and @xcite,@xcite,@xcite,@xcite,@xcite,@xcite in the gaussian noise case .",
    "connections between signal recovery and channel coding are also highlighted in @xcite .",
    "a hallmark of work in signal recovery is allowance for greater generality of signal coefficient values . in the regime",
    "as treated here , where @xmath105 and where there is a control on the sum of squares of the coefficients as well as a control on the minimum coefficient value , conclusions from this literature take the form that the best @xmath2 is of the order @xmath106 , with upper and lower bounds on the constants derived .",
    "it is natural to call ( the reciprocal of ) the best constant , for a given set of allowed signals and given noise distribution , the _ compressed sensing capacity _ or _ signal recovery capacity_.    for the converse results in @xcite,@xcite , fano s inequality is used to establish constants related to the channel capacity .",
    "refinements of this work can be found in @xcite .",
    "convex projection methods with @xmath100 constraints as in @xcite,@xcite,@xcite , have been used for achievability results . the same order of performance",
    "is achieved by a maximum correlation estimator @xcite .",
    "analysis of constants achieved by least squares is in @xcite , @xcite .",
    "the above analysis , when interpreted in our setting , correspond to saying that these schemes have communication rate that is positive , though at least a fixed amount below the channel capacity . for our setting , a consequence of the result here is that the signal recovery capacity is equal to the _ channel capacity_.      the development here is specific to the discrete - time channel for which @xmath107 for @xmath108 with real - valued inputs and outputs and with independent gaussian noise .",
    "standard communication models , even in continuous - time , have been reduced to this discrete - time white gaussian noise setting , or to parallel uses of such , when there is a frequency band constraint for signal modulation and when there is a specified spectrum of noise over that frequency band , as in @xcite , @xcite .",
    "standard approaches , as discussed in @xcite , entail a decomposition of the problem into separate problems of coding and of shaping of a multivariate signal constellation . for the low signal - to - noise regime , binary codes suffice for communication near capacity and there is no need for shaping .",
    "there is prior work concerning reliable communications near capacity for certain discrete input channels .",
    "iterative decoding algorithms based on statistical belief propagation in loopy networks have been _ empirically _ shown in various works to provide reliable and moderately fast decoding at rates near the capacity for such channels , and mathematically proven to provide such properties in certain special cases , such as the binary erasure channel in @xcite .",
    "these include codes based on low density parity check codes @xcite and turbo codes @xcite .",
    "see @xcite,@xcite for some aspects of the state of the art with such techniques .",
    "a different approach to reliable and computationally feasible decoding to achieve the rates possible with restriction to discrete alphabet signaling , is in the work on _ channel polarization _ of arikan and telatar @xcite .",
    "they achieve rates up to the mutual information @xmath109 between a uniform input distribution and the output of the channel .",
    "error probability is demonstrated there at a level exponentially small in @xmath110 for any fixed @xmath111 .",
    "in contrast for our codes the error probability is exponentially small in @xmath112 for the least squared decoder and within a log factor of being exponentially small in @xmath2 for the practical decoder in @xcite .",
    "moreover , communication is permitted at higher rates beyond that associated with a uniform input distribution .",
    "we are aware from personal conversation with imre telatar and emanuel abbe that they are investigating the extent to which channel polarization can be adapted to gaussian signaling .    in the high signal - to - noise regime , one needs a greater signal alphabet size .",
    "as explained in @xcite , along with coding schemes on such alphabets , additional shaping is required in order to be able to achieve rates up to capacity . here",
    "shaping refers to making the codewords vectors approximate a good packing of points on the @xmath2 dimensional sphere of square radius dictated by the power .",
    "an implication is that , marginally and jointly for any subset of codeword coordinates , the set of codewords should have empirical distribution not far from gaussian . notice that we build shaping directly into the coding scheme by the superposition strategy yielding codewords following a gaussian distribution .",
    "our ideas of sparse superposition coding are adapted to gaussian vector quantization in , kontoyiannis , gitzenis and rad @xcite .",
    "applicability to vector quantization is natural because of the above - mentioned connection between packing and coding .",
    "the analysis of concatenated codes in forney @xcite is an important forerunner to the development we give here .",
    "he identified benefits of an outer reed - solomon code paired in theory with an optimal inner code of shannon - gallager type and in practice with binary inner codes based on linear combinations of orthogonal terms ( for target rates @xmath113 less than @xmath48 such a basis is available ) .",
    "the challenge concerning theoretically good inner codes is that the number of messages searched is exponentially large in the inner codelength .",
    "forney made the inner codelength of logarithmic size compared to the outer codelength as a step toward practical solution .",
    "however , caution is required with such a strategy .",
    "suppose the rate of the inner code has only a small drop from capacity , @xmath114 . for small inner code error probability ,",
    "the inner codelength must be of order at least @xmath115 .",
    "so with that scheme one has the undesirable consequence that the required outer codelength becomes exponential in @xmath115 .    for the gaussian noise channel , our tactic to overcome that difficulty uses a superposition inner code with a polynomial size dictionary .",
    "we use inner and outer codelengths that are comparable , with the outer code used to correct errors in a small fraction of the sections of the inner code . the overall codelength to achieve error probability @xmath65 remains of the order @xmath116 .",
    "another point of relationship of this work with other ideas is the problem of multiple comparisons in hypothesis tests .",
    "false discovery rate @xcite for a given significance level , rather than exclusively overall error probability is a recent focus in statistical development , appropriate when considering very large numbers of hypotheses as arise with many variables in regression .",
    "our theory for the distribution of the fraction of incorrectly determined terms ( associated with bit error rate rather than block error rate ) provides an additional glimpse of what is possible in a regression setting with a large number of subset hypotheses .",
    "the work of @xcite is a recent example where subset selection within groups ( sections ) of variables is addressed by extension of false discovery methods .",
    "the idea of superposition coding for gaussian noise channels began with cover @xcite in the context of multiple - user channels . in that setting what is sent",
    "is a sum of codewords , one for each message .",
    "here we are putting that idea to use for the original shannon single - user problem .",
    "the purpose here of computational feasibility is different from the original multi - user purpose which was identification of the set of achievable rates .",
    "another connection with that broadcast channel work by cover is that for such gaussian channels , the power allocation can be arranged such that messages can be peeled off one at a time by successive decoding .",
    "related rate splitting and successive decoding for superposition codes are developed for gaussian multiple - access problems in @xcite and @xcite , where in some cases to establish such reductions , rate splitting is applied to individual users .",
    "however , feasibility has been lacking in part due to the absence of demonstration of reliability at high rate with superpositions from polynomial size code designs .",
    "it is an attractive feature of our solution for the single - user channel that it should be amenable to extension to practical solution of the corresponding multi - user channels , namely , the gaussian multiple access and gaussian broadcast channel .",
    "section [ sec : prelim ] contains brief preliminaries .",
    "section [ sec : rells ] provides core lemmas on the reliability of least squares for our superposition codes .",
    "section [ sec : sufsec ] analyzes the matter of section size sufficient for reliability .",
    "section [ sec : conprob ] confirms that the probability of more than a small fraction of mistakes is exponentially small .",
    "section [ sec : reed ] discusses properties of the composition of our code with a binary outer code for correction of any remaining small fraction of mistakes .",
    "the appendix collects some auxiliary matters .",
    "for vectors @xmath117 of length @xmath2 , let @xmath118 be the sum of squares of coordinates , let @xmath119 be the average square and let @xmath120 be the associated inner product .",
    "it is a matter of taste , but we find it slightly more convenient to work henceforth with the norm @xmath121 rather than @xmath122 .",
    "concerning the base of the logarithm ( @xmath123 ) and associated exponential ( @xmath124 ) , base @xmath69 is most suitable for interpretation and base @xmath125 most suitable for the calculus .",
    "for instance , the rate @xmath126 is measured in bits if the log is base @xmath69 and nats if the log is base @xmath125 .",
    "typically , conclusions are stated in a manner that can be interpreted to be invariant to the choice of base , and base @xmath125 is used for convenience in the derivations .",
    "we make repeated use of the following moment generating function and its associated large deviation exponent in constructing bounds on error probabilities .",
    "if @xmath127 and @xmath128 are normal with means equal to @xmath129 , variances equal to @xmath48 , and correlation coefficient @xmath130 then @xmath131 takes the value @xmath132^{1/2}\\ ] ] when @xmath133 and infinity otherwise .",
    "so the associated cumulant generating function of @xmath134 is @xmath135 , with the understanding that the minus log is replaced by infinity when @xmath136 is at least @xmath137 . for positive @xmath67",
    "we define the quantity @xmath138 given by @xmath139 this @xmath140 matches the relative entropy @xmath141 between bivariate normal densities , where @xmath142 is the joint density of @xmath143 of correlation @xmath130 and where @xmath144 is the joint normal obtained by tilting that density by @xmath145 , chosen to make @xmath146 have mean @xmath67 , when there is such a @xmath147 .",
    "let s give @xmath148 explicitly as an increasing function of the ratio @xmath149 .",
    "working with logarithm base @xmath125 , the derivative with respect to @xmath147 of the expression being maximized yields a quadratic equation which can be solved for the optimal @xmath150 let @xmath151 and @xmath152 , which is near @xmath153 when @xmath154 is small and approximately @xmath155 when @xmath154 is large .",
    "plug the optimized @xmath147 into the above expression and simplify to obtain @xmath156 , which is at least @xmath157 .",
    "thus @xmath140 is the composition of strictly increasing non - negative functions @xmath158 and @xmath159 evaluated at @xmath160 . for small values of this ratio , we see that @xmath140 is near @xmath161 .",
    "the expression corresponding to @xmath140 but with the maximum restricted to @xmath162 is denoted @xmath163 , that is , @xmath164 the corresponding optimal value of @xmath147 is @xmath165 . when the optimal @xmath147 is less than @xmath48 , the value of @xmath166 matches @xmath140 as given above .",
    "the @xmath167 case occurs when @xmath168 , or equivalently @xmath169 .",
    "then the exponent is @xmath170 , which is as least @xmath171 .",
    "consequently , in this regime @xmath166 is between @xmath172 and @xmath67 .",
    "the special case @xmath173 is included with @xmath174 .",
    "as we have said , least squares provides optimal decoding of superposition codes . in this section",
    "we examine the performance of this least squares choice in terms of rate and reliability .",
    "we focus on partitioned superposition codes in which the codewords are superpositions with one term from each section .",
    "let @xmath175 be an allowed subset of terms .",
    "we examine first subset coding in which to each such @xmath175 there is a corresponding coefficient vector @xmath26 in which the non - zero coefficients take a specified positive value as discussed above .",
    "we may denote the corresponding codeword @xmath176 . among such codewords , least squares provides a choice for which @xmath177 is minimal .",
    "for a subset @xmath175 of size @xmath27 we measure how different it is from @xmath178 , the subset that was sent .",
    "let @xmath179 be the number of entries of @xmath175 not in @xmath178 .",
    "equivalently , since @xmath175 and @xmath178 are of the same size , it is the number of entries of @xmath178 not in @xmath175 .",
    "let @xmath180 be the least squares solution , or an approximate least squares solution , achieving @xmath181 with @xmath182 .",
    "we call @xmath183 the number of mistakes .",
    "indeed , for a partitioned superposition code it is the number of sections incorrectly decoded .",
    "there is a role for the function @xmath184 for @xmath185 , where @xmath186 is the signal - to - noise ratio and @xmath187 is the channel capacity .",
    "we note that @xmath188 is a non - negative concave function equal to @xmath129 when @xmath189 is @xmath129 or @xmath48 and strictly positive in between .",
    "the quantity @xmath190 is larger by the additional amount @xmath191 , positive when the rate @xmath36 is less than the shannon capacity @xmath37 .",
    "the function @xmath192 $ ] with @xmath162 is the cumulant generating function of a test statistic in our analysis .    our first result on the distribution of the number of mistakes is the following . * lemma 1 : * set @xmath193 for an @xmath194 . for approximate least squares with @xmath195 ,",
    "the probability of a fraction @xmath196 mistakes is upper bounded by @xmath197 or equivalently , @xmath198 where @xmath199 and @xmath86 is the signal - to - noise ratio .",
    "* remark 1 : * we find this lemma 1 to be especially useful for @xmath189 in the lower range of the interval from @xmath129 to @xmath48 .",
    "lemma 2 below will refine the analysis to provide an exponent more useful in the upper range of the interval .",
    "* proof of lemma 1 : * to incur @xmath200 mistakes , there must be an allowed subset @xmath175 of size @xmath27 which differs from the subset @xmath178 sent in an amount @xmath201 which undesirably has squared distance @xmath202 less than or equal to the value @xmath203 achieved by @xmath178 .",
    "the analysis proceeds by considering an arbitrary such @xmath175 , bounding the probability that @xmath204 , and then using an appropriately designed union bound to put such probabilities together .",
    "consider the statistic @xmath205 given by @xmath206.\\ ] ] we set a threshold for this statistic equal to @xmath207 .",
    "the event of interest is that @xmath208 .",
    "the subsets @xmath175 and @xmath178 have an intersection @xmath209 of size @xmath210 and difference @xmath211 of size @xmath212 .",
    "given @xmath213 the actual density of @xmath8 is normal with mean @xmath214 and variance @xmath215 and we denote this density @xmath216 . in particular",
    ", there is conditional independence of @xmath8 and @xmath217 given @xmath218 .",
    "consider the alternative hypothesis of a conditional distribution for @xmath8 given @xmath219 and @xmath217 which is normal(@xmath220 ) .",
    "it is the distribution which would have governed @xmath8 if @xmath175 were sent .",
    "let @xmath221 be the associated conditional density .",
    "with respect to this alternative hypothesis , the conditional distribution for @xmath8 given @xmath218 remains normal(@xmath222 ) .",
    "that is , @xmath223 .",
    "we decompose the above test statistic as @xmath224\\ ] ] @xmath225.\\ ] ] let s call the two parts of this decomposition @xmath226 and @xmath227 , respectively .",
    "note that @xmath228 depends only on terms in @xmath178 , whereas @xmath229 depends also on the part of @xmath175 not in @xmath178 .",
    "concerning @xmath227 , note that we may express it as @xmath230 where @xmath231 is the adjustment by the logarithm of the ratio of the normalizing constants of these densities .",
    "thus @xmath227 is equivalent to a likelihood ratio test statistic between the actual conditional density and the constructed alternative hypothesis for the conditional density of @xmath8 given @xmath218 and @xmath217 .",
    "it is helpful to use bayes rule to provide @xmath232 via the equality of @xmath233 and @xmath234 and to interpret this equality as providing an alternative representation of the likelihood ratio in terms of the reverse conditionals for @xmath217 given @xmath218 and @xmath8 .",
    "we are examining the event @xmath235 that there is an allowed subset @xmath236 ( with @xmath237 of size @xmath238 and @xmath239 of size @xmath200 ) such that that @xmath240 is less than @xmath241 . for positive @xmath147",
    "the indicator of this event satisfies    @xmath242 because , if there is such an @xmath175 with @xmath243 negative , then indeed that contributes a term on the right side of value at least @xmath48 . here",
    "the outer sum is over @xmath244 of size @xmath238 . for each such @xmath245 , for the inner sum , we have @xmath200 sections in each of which , to comprise @xmath246 , there is a term selected from among @xmath247 choices other than the one prescribed by @xmath178 .    to bound the probability of @xmath235 , take the expectation of both sides , bring the expectation on the right inside the outer sum , and write it as the iterated expectation , where on the inside condition on @xmath8 , @xmath218 and @xmath248 to pull out the factor involving @xmath226 , to obtain that @xmath249 $ ] is not more than @xmath250 a simplification here is that the true density for @xmath217 is independent of the conditioning variables @xmath8 , @xmath218 and @xmath248 .",
    "we arrange for @xmath147 to be not more than @xmath48 .",
    "then by jensen s inequality , the conditional expectation may be brought inside the @xmath147 power and inside the inner sum , yielding @xmath251 \\le \\sum_{s_1 } { \\mathbb{e}}e^{-n\\lambda(t_1(s_1)-t ) } \\left(\\sum_{s_2 } { \\mathbb{e}}_{x_{s_2}|y , x_{s_1 } } e^{-nt_2(s ) } \\right)^\\lambda .\\ ] ] recall that @xmath252 and that the true density for @xmath217 is independent of the conditioning variables in accordance with the @xmath253 in denominator .",
    "so when we take the expectation of this ratio we cancel the denominator leaving the numerator density which integrates to @xmath48 .",
    "consequently , the resulting expectation of @xmath254 is not more than @xmath255 .",
    "the sum over @xmath246 entails less than @xmath256 choices so the bound is @xmath251 \\le \\sum_{s_1 } { \\mathbb{e}}e^{-n\\lambda t_1(s_1 ) } e^{-n\\lambda[c_\\alpha - \\alpha r - t]}.\\ ] ] now @xmath257 is a sum of @xmath2 independent mean - zero random variables each of which is the difference of squares of normals for which the squared correlation is @xmath258 .",
    "so the expectation @xmath259 is found to be equal to @xmath260^{n/2}$ ] . when plugged in above it yields the claimed",
    "bound optimized over @xmath147 in @xmath261 $ ] .",
    "we recognize that the exponent takes the form @xmath262 with @xmath263 as discussed in the preliminaries .",
    "this completes the proof of lemma 1 .    * some additional remarks : * the exponent @xmath166 in lemma 1 ( and its refinement in lemma 2 to follow ) depends on the fraction of mistakes @xmath189 and the signal - to - noise ratio @xmath86 only through @xmath264 and @xmath265 .",
    "as we have seen , the @xmath266 case occurs when @xmath267 and then @xmath140 is near @xmath268 when it is small ; whereas , the @xmath167 case occurs when @xmath269 and then the exponent is as least @xmath270 .",
    "this behavior of the exponent is similar to the usual order @xmath271 for @xmath36 close to @xmath37 and order @xmath272 for @xmath36 farther from @xmath37 associated with the theory in gallager @xcite .",
    "a difficulty with the lemma 1 bound is that for @xmath189 near @xmath48 and for @xmath36 correspondingly close to @xmath37 , in the key quantity @xmath273 , the order of @xmath274 is @xmath275 , which is too close to zero to cancel the effect of the combinatorial coefficient .",
    "the following lemma refines the analysis of lemma 1 , obtaining the same exponent with an improved correlation coefficient .",
    "the denominator @xmath276 is improved by the presence of the factor @xmath277 allowing the conclusion to be useful also for @xmath189 near @xmath48 .",
    "the price we pay is the presence of an additional term in the bound .    for the statement of lemma 2 we again use the test statistic @xmath240 as defined in the proof of lemma 1 .",
    "for interpretation of what follows with arbitrary base of logarithm , in that definition of @xmath240 multiply by @xmath278 and likewise take the threshold to be @xmath279 .    * lemma 2 : * let a positive integer @xmath280 be given and let @xmath196 .",
    "suppose @xmath281 .",
    "as above let @xmath235 be the event that there is an allowed @xmath27 term subset @xmath175 with @xmath282 of size @xmath200 such that @xmath240 is less than @xmath241 . then @xmath249 $ ]",
    "is bounded by the minimum for @xmath283 in the interval between @xmath241 and @xmath190 of the following    @xmath284 @xmath285\\big\\}.\\ ] ] where @xmath286 * proof of lemma 2 : * split the test statistic @xmath287 where @xmath288}\\ ] ] and @xmath289}\\ ] ] likewise we split the threshold @xmath290 where @xmath291 is negative and @xmath292 is positive .",
    "the event that there is an @xmath175 with @xmath293 is contained in the union of the two events @xmath294 , that there is an @xmath175 with @xmath295 , and the event @xmath296 , that @xmath297 .",
    "the part @xmath298 has no dependence on @xmath175 so it can be treated more simply .",
    "it is a mean zero average of differences of squared normal random variables , with squared correlation @xmath299 .",
    "so using its moment generating function , @xmath300 $ ] is exponentially small , bounded by the second of the two expressions above .",
    "concerning @xmath301 $ ] , its analysis is much the same as for lemma 1 .",
    "we again decompose @xmath302 as the sum @xmath303 , where @xmath304 is the same as before .",
    "the difference is that in forming @xmath305 we subtract @xmath306 rather than @xmath307 .",
    "consequently , @xmath308,\\ ] ] which again involves a difference of squares of standardized normals .",
    "but here the coefficient @xmath277 multiplying @xmath248 is such that we have maximized the correlations between the @xmath309 and @xmath310 .",
    "consequently , we have reduced the spread of the distribution of the differences of squares of their standardizations as quantified by the cumulant generating function .",
    "one finds that the squared correlation coefficient is @xmath311 for which @xmath312 .",
    "accordingly we have that the moment generating function is @xmath313\\}$ ] which gives rise to the bound appearing as the first of the two expressions above .",
    "this completes the proof of lemma 2 .     using exact least squares , i.e. ,",
    "@xmath314 , with @xmath315,@xmath316 , signal - to - noise ratio @xmath317 , and rate @xmath318 of capacity .",
    "the red and blue curves are the @xmath319 $ ] and @xmath320 $ ] bounds , using the natural logarithm , from the two terms in lemma 2 with optimized @xmath283 .",
    "the dotted green curve is @xmath321 explained below . with @xmath322 , the total probability of at least that fraction of mistakes",
    "is bounded by @xmath323.,height=336 ]    the method of analysis also allows consideration of subset coding without partitioning . for , in this case",
    "all @xmath324 subsets of size @xmath27 correspond to codewords , so with the rate in nats we have @xmath325 .",
    "the analysis proceeds in the same manner , with the same number @xmath326 of choices of sets @xmath327 where @xmath175 and @xmath178 agree on @xmath238 terms , but now with @xmath328 choices of sets @xmath329 of size @xmath200 where they disagree .",
    "we obtain the same bounds as above except that where we have @xmath330 with the exponent @xmath331 it is replaced by @xmath332 with the exponent @xmath333 defined by @xmath334 .",
    "thus we have the following conclusion .",
    "* corollary 3 : * for subset superposition coding , the probability of the event @xmath335 that there is a @xmath26 that is incorrect in @xmath200 sections and has @xmath336 is bounded by the minimum of the same expressions given in lemma 1 and lemma 2 except that the term @xmath331 appearing in these expression be replaced by the quantity @xmath333 defined above .",
    "we come to the matter of sufficient conditions on the section size @xmath29 for our exponential bounds to swamp the combinatorial coefficient , for partitioned superposition codes .",
    "we call @xmath337 the _ section size rate _ ,",
    "that is , the bits required to describe the member of a section relative to the bits required to describe which section .",
    "it is invariant to the base of the log .",
    "equivalently we have @xmath29 and @xmath27 related by @xmath60 .",
    "note that the size of @xmath61 controls the polynomial size of the dictionary @xmath338 .    in both cases",
    "the codelength may be written as @xmath339    we do not want a requirement on the section sizes with @xmath61 of order @xmath340 for then the complexity would grow exponentially with this inverse of the gap from capacity .",
    "so instead let s decompose @xmath341 where @xmath342 .",
    "we investigate in this section the use of @xmath343 to swamp the combinatorial coefficient . in the next section excess in @xmath343 , beyond that needed to cancel the combinatorial coefficient , plus @xmath344 are used to produce exponentially small error probability .",
    "define @xmath345 and @xmath346 .",
    "now @xmath262 is increasing as a function of @xmath67 , so @xmath347 is greater than @xmath348 whenever @xmath349 .",
    "accordingly , we decompose the exponent @xmath347 as the sum of two components , namely , @xmath348 and the difference @xmath350 .",
    "we then ask whether the first part of the exponent denoted @xmath348 is sufficient to wash out the affect of the log combinatorial coefficient @xmath351 .",
    "that is , we want to arrange for the nonnegativity of the difference @xmath352 this difference is small for @xmath189 near @xmath129 and @xmath48 .",
    "furthermore , its constituent quantities have a shape comparable to multiples of @xmath353 .",
    "consider first @xmath342 and take the log to be base @xmath125 .",
    "it has second derivative @xmath354 .",
    "it follows that @xmath355 , since the difference of the two sides has negative second derivative , so it is concave and equals @xmath129 at @xmath356 and @xmath357 .",
    "likewise @xmath358 so the ratio @xmath359 is at least @xmath360 .",
    "consequently , whether the optimal @xmath147 is equal to @xmath48 or is less than @xmath48 , we find that @xmath348 is of order @xmath353 .",
    "similarly , there is the matter of @xmath361 , with @xmath362 restricted to have integer values .",
    "it enjoys the upper bounds @xmath363 and @xmath364 so that it is not more than @xmath365 where @xmath366 .",
    "consequently , using @xmath66 , one finds that for sufficiently large @xmath61 depending on @xmath86 , the difference @xmath321 is nonnegative uniformly for the permitted @xmath189 in @xmath261 $ ] .",
    "the smallest such section size rate is @xmath367 where the maximum is for @xmath189 in @xmath368 .",
    "this definition has the required invariance to the choice of base of the logarithm , assuming that the same base is used for the communication rate @xmath36 and for the @xmath369 that arises in the definition of @xmath348 .    in the above ratio the numerator and denominator are both @xmath129 at @xmath356 and @xmath357 ( yielding @xmath370 at the ends ) .",
    "accordingly , we have excluded @xmath129 and @xmath48 from the definition of @xmath371 for finite @xmath27 . nevertheless , limiting ratios arise at these ends .",
    "we show that the value of @xmath371 is fairly insensitive to the value of @xmath27 , with the maximum over the whole range being close to a limit @xmath372 which is characterized by values in the vicinity of @xmath373 .",
    "let @xmath374 near @xmath375 be the solution to @xmath376 * lemma 4 : * the section size rate @xmath371 has a continuous limit @xmath377 which is given , for @xmath378 , by @xmath379 ^ 2 /[8v(1\\!+\\!v)\\log e]}\\ ] ] and for @xmath380 by @xmath381/[2(1\\!+\\!v)]}\\ ] ] where @xmath86 is the signal - to - noise ratio . with @xmath36 replaced by @xmath382 and using log base e , in the case @xmath383 , it is @xmath384 ^ 2 } \\ ] ] which is approximately @xmath385 for small positive @xmath86",
    "; whereas , in the case @xmath380 it is @xmath386 which asymptotes to the value @xmath48 for large @xmath86 .",
    "* proof of lemma 4 : * for @xmath189 in @xmath387 we use @xmath388 and the strict positivity of @xmath348 to see that the ratio in the definition of @xmath371 tends to zero uniformly within compact sets interior to @xmath387 .",
    "so the limit @xmath372 is determined by the maximum of the limits of the ratios at the two ends . in the vicinity of the left and right",
    "ends we replace @xmath361 by the continuous upper bounds @xmath389 and @xmath390 , respectively , which are tight at @xmath391 and @xmath392 , respectively .",
    "then in accordance with lhopital s rule , the limit of the ratios equals the ratios of the derivatives at @xmath393 and @xmath394 , respectively .",
    "accordingly , @xmath395 where @xmath396 and @xmath397 are the derivatives of @xmath348 with respect to @xmath189 evaluated at @xmath356 and @xmath357 , respectively .    to determine the behavior of @xmath398 in the vicinity of @xmath129 and @xmath48 we first need to determine whether the optimal @xmath147 in its definition is strictly less than @xmath48 or equal to @xmath48 . according to our earlier developments that is determined by whether @xmath399 .",
    "the right side of this is @xmath400 .",
    "so it is equivalent to determine whether the ratio @xmath401 is less than @xmath48 for @xmath189 in the vicinity of @xmath129 and @xmath48 .",
    "using lhopital s rule it suffices to determine whether the ratio of derivatives is less than @xmath48 when evaluated at @xmath129 and @xmath48 .",
    "at @xmath402 it is @xmath403/v$ ] which is not more than @xmath404 ( certainly less than @xmath48 ) for all positive @xmath86 ; whereas , at @xmath373 the ratio of derivatives is @xmath405/v$ ] which is less than @xmath48 if and only if @xmath406 .    for the cases in which the optimal @xmath407 , we need to determine the derivative of @xmath408 at @xmath356 and @xmath357 .",
    "recall that @xmath409 is the composition of the functions @xmath410 and @xmath411 and @xmath412 .",
    "we use the chain rule taking the products of the associated derivatives .",
    "the first of these functions has derivative @xmath413 which is @xmath414 at @xmath415 , the second of these has derivative @xmath416 which is @xmath404 at @xmath417 , and the third of these functions is @xmath418 which has derivative that evaluates to @xmath419 at @xmath420 and evaluates to @xmath421 ^ 2/[v(1\\!+\\!v)]$ ] at @xmath357 .",
    "the first of these gives what is needed for the left end for all positive @xmath86 and the second what is needed for the right end for all @xmath406 .",
    "the magnitude of the derivative at @xmath48 is smaller than at @xmath129 .",
    "indeed , taking square roots this is the same as the claim that @xmath422 . replacing @xmath423 and rearranging",
    ", it reduces to @xmath424 , which is true for @xmath425 since the two sides match at @xmath426 and have derivatives @xmath427 .",
    "thus the limiting value for @xmath189 near @xmath48 is what matters for the maximum .",
    "this produces the claimed form of @xmath372 for @xmath406 .",
    "in contrast for @xmath428 , the optimal @xmath167 for @xmath189 in the vicinity of @xmath48 . in this case",
    "we use @xmath429 which has derivative equal to @xmath430/(1\\!+\\!v)$ ] at @xmath357 , which is again smaller in magnitude than the derivative at @xmath356 , producing the claimed form for @xmath372 for @xmath428 .    at @xmath431",
    "we equate @xmath432 and see that both of the expressions for the magnitude of the derivative at @xmath48 agree with each other ( both reducing to @xmath433 ) so the argument extends to this case , and the expression for @xmath372 is continuous in @xmath86 .",
    "this completes the proof of lemma 3 .",
    "while @xmath372 is undesirably large for small @xmath86 , we have reasonable values for moderately large @xmath86 .",
    "in particular , @xmath372 equals @xmath434 and @xmath435 , respectively , at @xmath436 and @xmath437 , and it is near @xmath48 for large @xmath86 .     as a function of the signal - to - noise ratio @xmath86 .",
    "the dashed curve shows @xmath371 at @xmath438 . just below it",
    "the thin solid curve is the limit for large @xmath27 . for section size",
    "@xmath439 the error probabilities are exponentially small for all @xmath58 and any @xmath440 .",
    "the bottom curve shows the minimal section size rate for the bound on the error probability contributions to be less than @xmath441 , with @xmath442 and @xmath443 at @xmath438 .",
    ", height=336 ]    numerically is of interest to ascertain the minimal section size rate @xmath444 , for a specified @xmath27 such as @xmath438 , for @xmath36 chosen to be a proscribed high fraction of @xmath37 , say @xmath445 , for @xmath59 a proscribed small target fraction of mistakes , say @xmath443 , and for @xmath65 to be a small target probability , so as to obtain @xmath446,p[\\tilde e_\\ell]+p[e_\\ell^*]\\}\\le \\epsilon$ ] , taking the minimum over allowed values of @xmath283 , for every @xmath193 at least @xmath59 . for this calculation",
    "the bound from lemma 1 is used for @xmath447 $ ] and the bound from lemma 2 is used for @xmath448+p[e_\\ell^*]$ ] .",
    "this is illustrated in figure [ fig : secsizerate ] plotting the minimal section size rate as a function of @xmath86 for @xmath449 . with such @xmath36",
    "moderately less than @xmath37 we observe substantial reduction in the required section size rate .    * extra @xmath450 beyond the minimum : * via the above analysis we determine the minimum value of @xmath67 for which the combinatorial term is canceled , and we characterize the amount beyond that minimum which makes the error probability exponentially small .",
    "arrange @xmath451 to be the solution to the equation @xmath452 to see its characteristics , let @xmath453 at @xmath454 using log base @xmath125 . here",
    "@xmath455 is the inverse of the function @xmath456 which is the composition of the increasing functions @xmath457 $ ] and @xmath458 previously discussed , beginning in section 2 .",
    "this @xmath455 is near @xmath459 for small @xmath460 .",
    "when @xmath461 the condition @xmath462 is satisfied and @xmath463 indeed solves the above equation ; otherwise @xmath464 provides the solution .",
    "now @xmath465 . with @xmath466",
    "restricted to integers between @xmath129 and @xmath27 , it is not more than @xmath467 and @xmath468 , with equality at particular @xmath189 near @xmath129 and @xmath48 , respectively .",
    "it remains small , with @xmath469 , for @xmath185 .",
    "also we have @xmath470 from lemma 2 .",
    "consequently , @xmath451 is small for large @xmath27 ; moreover , for @xmath189 near @xmath129 and @xmath48 , it is of order @xmath189 and @xmath471 , respectively , and via the indicated bounds , derivatives at @xmath129 and @xmath48 can be explicitly determined .",
    "the analysis in lemma 4 may be interpreted as determining section size rates @xmath61 such that the differentiable upper bounds on @xmath451 are less than or equal to @xmath472 for @xmath185 , where , noting that these quantities are @xmath129 at the endpoints of the interval , the critical section size rate is determined by matching the slopes at @xmath473 . at the other end of the interval ,",
    "the bound on the difference @xmath474 has a strictly positive slope at @xmath402 , given by @xmath475 - [ 2vr / a]^{1/2}$ ] .",
    "recall that @xmath476 . for a sensible probability bound in lemma 2 , less than @xmath48",
    ", we need to arrange @xmath450 greater than @xmath451 .",
    "this we can do if the threshold @xmath241 is less than @xmath477 and @xmath283 is strictly between .",
    "express @xmath450 as the sum of @xmath451 , needed to cancel the combinatorial coefficient , and @xmath478 which is positive .",
    "this @xmath479 arises in establishing that the main term in the probability bound is exponentially small .",
    "it decomposes as @xmath480 , which reveals different regimes in the behavior of the exponent . for high @xmath189 what matters is the @xmath481 term , positive with @xmath482 , and that @xmath283 stays less than the gap @xmath481 . for small @xmath189 , we approximate @xmath479 by @xmath483 - t_\\alpha.$ ]    for moderate and small @xmath189 , having @xmath58 is not so important to the exponent , as the positivity of @xmath474 produces a positive exponent even if @xmath36 matches or is slightly greater than @xmath37 . in this regime ,",
    "the lemma 1 bound is preferred , where we set @xmath264 without need for @xmath283 .",
    "in this section we put the above conclusions together to demonstrate the reliability of approximate least squares .",
    "the probability of the event of more than any small positive fraction of mistakes @xmath484 is shown to be exponentially small .",
    "recall the setting that we have a random dictionary @xmath22 of @xmath27 sections , each of size @xmath29 .",
    "the mapping from @xmath1-bit input strings @xmath10 to coefficient vectors @xmath485 is as previously described .",
    "the set @xmath486 of such vectors @xmath26 are those that have one non - zero coefficient in each section ( with possible freedom for the choice of sign ) and magnitude of the non - zero coefficient equal to @xmath48 .",
    "let @xmath487 be the coefficient vector for an arbitrary input @xmath488 .",
    "we treat both the case of a fixed input , and the case that the input is drawn at random from the set of possible inputs .",
    "the codeword sent @xmath489 is the superposition of a subset of terms with one from each section .",
    "the received string is @xmath490 with @xmath23 distributed normal @xmath491 .",
    "the columns of @xmath22 are independent @xmath492 and @xmath22 and @xmath8 are known to the receiver , but not @xmath493 . the section size rate @xmath61 is such that @xmath60 . in fashion with shannon theory , the expectations in the following theorem are taken with respect to the distribution of the design @xmath22 as well as with respect to the distribution of the noise ; implications for random individual dictionaries @xmath22 are discussed after the proof .",
    "the estimator @xmath494 is assumed to be an ( approximate ) least squares estimator , taking values in @xmath486 and satisfying @xmath495 @xmath496 , with @xmath182 .",
    "let @xmath497 denote the number of mistakes , that is , the number of sections in which the non - zero term in @xmath494 is different from the term in @xmath493 .",
    "suppose the threshold @xmath498 is not more than @xmath499 .",
    "some natural choices for the threshold include @xmath314 , @xmath500 , and @xmath501 . for positive @xmath502",
    "let @xmath503 .",
    "* theorem 5 : * suppose the section size rate @xmath61 is at least @xmath371 , that the communication rate @xmath36 is less than the capacity @xmath37 with codeword length @xmath504 , and that we have an approximate least squares estimator .",
    "for @xmath505 between @xmath48 and @xmath27 , the probability @xmath506 $ ] is bounded by the sum over integers @xmath200 from @xmath505 to @xmath27 of @xmath249 $ ] using the minimum of the bounds from lemmas 1 and 2 .",
    "it follows that there is a positive constant @xmath62 , such that for all @xmath59 between @xmath129 and @xmath48 , @xmath507 \\le 2l \\exp\\{-nc\\min\\{\\alpha_0,g(c\\!-\\!r)\\}\\}.\\ ] ] consequently , asymptotically , taking @xmath59 of the order of a constant times @xmath508 , the fraction of mistakes is of order @xmath508 in probability , provided @xmath272 is at least a constant multiple of @xmath509 .",
    "moreover , for any fixed @xmath59 , @xmath61 , and @xmath36 , not depending on @xmath27 , satisfying @xmath510 , @xmath511 and @xmath482 , we conclude that this probability is exponentially small .",
    "* proof : * consider the exponent @xmath512 as given at the start of the preceding section .",
    "we take a reference @xmath513 for which @xmath514 and for which @xmath513 is at least @xmath451 and at least a multiple of @xmath515 .",
    "the simplest choice is @xmath516 , which may be used when @xmath241 is less than a fixed fraction of @xmath517 .",
    "then @xmath518 exceeds @xmath515 , taking @xmath283 to be between @xmath241 and @xmath481 .",
    "small precision @xmath241 makes for a greater computational challenge .",
    "allowance is made for a more relaxed requirement that @xmath241 be less than @xmath519 and less than a fixed fraction of @xmath520 .",
    "both of these conditions are satisfied when @xmath241 is less than the value @xmath499 stated for the theorem .",
    "accordingly , set @xmath521 $ ] to be half way between @xmath451 and @xmath450 . with @xmath241",
    "less than both @xmath522 $ ] and @xmath523 $ ] , arrange @xmath524 to be less than both of these as well .",
    "for then @xmath513 exceeds both @xmath451 and @xmath525 as required .    now @xmath526 has a nondecreasing derivative with respect to @xmath67 .",
    "so @xmath527 is greater than @xmath528 .",
    "consequently , it lies above the tangent line ( the first order taylor expansion ) at @xmath513 , that is , @xmath529 where @xmath530 is the derivative of @xmath531 with respect to @xmath67 , which is here evaluated at @xmath532 . in detail , the derivative @xmath533 is seen to equal @xmath534 when @xmath535 , and this derivative is equal to @xmath48 otherwise .",
    "[ the latter case with derivative equal to @xmath48 includes the situations @xmath356 and @xmath357 where @xmath536 with @xmath537 ; all other @xmath189 have @xmath538 . ]",
    "now lower bound the components of this tangent line .",
    "first lower bound the derivative @xmath539 evaluated at @xmath540 . since this",
    "derivative is non - decreasing it is at least as large as the value at @xmath541 . as in our developments in previous sections",
    "@xmath542 is a bounded function of @xmath189 .",
    "moreover , @xmath515 and @xmath543 are positive functions of order @xmath353 in the unit interval , with ratio tending to positive values as @xmath189 tends to @xmath129 and @xmath48 , so their ratio is uniformly bounded away from @xmath129 . consequently @xmath544 is strictly positive .",
    "[ this is where we have taken advantage of @xmath513 being at least a multiple of @xmath515 ; if instead we used @xmath451 as the reference , then for some @xmath189 we would find the @xmath545 being of order @xmath546 , producing a slightly inferior order in the exponent of the probability bound . ]    next examine @xmath547 .",
    "since @xmath513 is at least @xmath451 , it follows that @xmath547 is at least @xmath548 .",
    "now we are in position to apply lemma 2 and lemma 4 . if the section size rate @xmath61 is at least @xmath371 we have that @xmath549 cancels the combinatorial coefficient and hence the first term in the @xmath249 $ ] bound ( the part controlling @xmath550 $ ] ) is not more than @xmath551 \\ , d^\\prime\\},\\ ] ] where @xmath193 . in the first case , with @xmath552 and @xmath516 ,",
    "this yields @xmath249 $ ] not more than the sum of @xmath553 \\ , d^\\prime \\}\\ ] ] and @xmath554 for any choice of @xmath283 between @xmath241 and @xmath481 .",
    "for instance one may choose @xmath283 to be half way between @xmath241 and @xmath555 .    now if @xmath241 is less than a fixed fraction of @xmath517 , we have arranged for both @xmath344 and @xmath556 to be of order @xmath481 uniformly for @xmath557 .",
    "accordingly , the first of the two parts in the bound has exponent exceeding a quantity of order @xmath517 .",
    "the second of the two parts has exponent related to a function of the ratio @xmath558 $ ] as explained in section ii , where the function is of order @xmath10 for small @xmath10 and order @xmath559 for large @xmath10 . here",
    "@xmath10 is of order @xmath560 uniformly in @xmath189 .",
    "it follows that there is a constant @xmath62 ( depending on @xmath86 ) such that @xmath561 \\le 2 \\exp \\{-nc \\min\\{\\alpha_0 ( c\\!-\\!r),g(c\\!-\\!r)\\}\\}.\\ ] ]    an improved bound is obtained , along with allowance of a larger threshold @xmath241 , using @xmath513 half way between @xmath562 and @xmath450 .",
    "then the first part of the bound becomes @xmath563 \\ , d^\\prime \\}\\ ] ] provided @xmath283 is chosen between @xmath241 and @xmath564 , e.g. half way between works for our purposes .",
    "this bound is superior to the previous one , when @xmath36 closely matches @xmath37 , because of the addition of the non - negative @xmath565 term . for @xmath189 less than , say , @xmath404",
    ", we use that the exponent exceeds a fixed multiple of @xmath566 ; whereas for @xmath567 we use that the exponent exceeds a fixed multiple of @xmath568 . for @xmath569",
    ", it yields the desired bounds on @xmath249 $ ] , uniformly exponentially small for @xmath557 , with the stated conditions on @xmath241 .    with optimized @xmath283 ,",
    "let @xmath570 be the minimum of the two exponents from the two terms in the bound on @xmath249 $ ] at @xmath193 .",
    "likewise , let @xmath571 be the minimum of these exponents for @xmath572 .",
    "we have established that @xmath573 exceeds a quantity of order @xmath574 . then for @xmath572 , @xmath251 \\",
    ", \\le \\ , 2e^{-nd_{\\min}}\\ ] ] and accordingly @xmath507 \\ , \\le \\ , 2l e^{-n d_{\\min}}.\\ ] ]    using the form of the constants identified above , we see that even for @xmath59 of order @xmath508 , that is , for @xmath575 constant , the probability @xmath576 $ ] goes to zero polynomially in @xmath508 .",
    "indeed , for @xmath272 at least a multiple of @xmath509 , and sufficiently small @xmath241 , the bound becomes @xmath577 which with @xmath578 becomes , @xmath579",
    "\\le 2(1/l)^{(1/2)(a / r)\\tau_v w_v \\ell_0 - 1}.\\ ] ] it is assured to go to zero with @xmath27 for @xmath505 at least @xmath580 $ ] .",
    "this completes the proof of theorem 5 .",
    "* remarks : * for a range of values of @xmath505 , up to the point where a multiple of @xmath581 hits @xmath582 , the upper tail of the distribution of the number of mistakes past a minimal value is shown to be less than that of a geometric random variable . using the geometric sum ,",
    "an alternative to the factor @xmath27 outside the exponent can be arranged .",
    "the form given for the exponential bound is meant only to reveal the general character of what is available .",
    "in particular , via appeal to the section size analysis , we ensure to have canceled the combinatorial coefficient and yet , for @xmath583 , to have enough additional exponent that the probability of a fraction of at least @xmath59 mistakes is exponentially small .",
    "a compromise was made , by introduction of an inequality ( the tangent bound on the exponent ) to proceed most simply to this demonstration .",
    "now understanding that it is exponentially small , our best evaluation avoids this compromise and proceeds directly , using for each @xmath189 the best of the bounds from lemma 1 and lemma 2 , as it provides substantial numerical improvement .",
    "the polynomial bound on more than a constant number of mistakes is here extracted as an aside to the exponential bound with exponent proportional to @xmath200 .",
    "one can conclude , for sufficient section size rate @xmath61 , using @xmath584 , that the probability of even @xmath48 or more mistake is polynomially small .",
    "polynomially small block error probability is not as impressive when by a simple device it is made considerably better .",
    "indeed , we have established smaller probability bounds with larger mistake thresholds @xmath505 . with certain such thresholds ,",
    "fewer mistakes than that are guaranteed correctable by suitable outer codes ; thereby yielding smaller overall block error probability .",
    "the probability of the error event @xmath585 has been computed averaging over random generation of the dictionary @xmath22 as well as the distribution of the received sequence @xmath8 . in this case",
    "the bounds apply equally to an individual input @xmath10 as well as with the uniform distribution on the ensemble of possible inputs .",
    "implications of the bounds for a randomly generated dictionary @xmath22 are discussed further in appendix a.    in the next section we review basic properties of reed solomon codes and discusses its role in correcting any existing section errors .",
    "we employ reed - solomon ( rs ) codes ( @xcite , @xcite ) as an outer code for correcting any remaining section mistakes .",
    "the symbols for the rs code come from a galois field consisting of @xmath154 elements denoted by @xmath586 , with @xmath154 typically taken to be of the form @xmath587 .",
    "if @xmath588 represent message and codeword lengths respectively , then an rs code with symbols in @xmath589 and minimum distance between codewords given by @xmath590 can have the following parameters : @xmath591 here @xmath592 gives the number of parity check symbols added to the message to form the codeword . in what follows",
    "we find it convenient to take @xmath29 to be equal to @xmath587 so that can view each symbol in @xmath589 as giving a number between 1 and @xmath29 .",
    "we now demonstrate how the rs code can be used as an outer code in conjunction with our inner superposition code , to achieve low block error probability . for simplicity",
    "assume that @xmath29 is a power of 2 .",
    "first consider the case when @xmath27 equals @xmath29 .",
    "taking @xmath593 , we have that since @xmath27 is equal to @xmath29 , the rs codelength becomes @xmath27 .",
    "thus , one can view each symbol as representing an index in each of the @xmath27 sections .",
    "the number of input symbols is then @xmath594 , so setting @xmath595 , one sees that the outer rate @xmath596 , equals @xmath597 which is at least @xmath598 .    for code composition",
    "@xmath599 message bits become the @xmath600 input symbols to the outer code .",
    "the symbols of the outer codeword , having length @xmath27 , gives the labels of terms sent from each section using our inner superposition with codelength @xmath601 . from the received @xmath8 the estimated labels @xmath602 using our least squares decoder",
    "can be again thought of as output symbols for our rs codes .",
    "if @xmath603 denotes the section mistake rate , it follows from the distance property of the outer code that if @xmath604 then these errors can be corrected .",
    "the overall rate @xmath90 is seen to be equal to the product of rates @xmath605 which is at least @xmath606 .",
    "since we arrange for @xmath603 to be smaller than some @xmath59 with exponentially small probability , it follows from the above that composition with an outer code allows us to communicate with the same reliability , albeit with a slightly smaller rate given by @xmath607 .",
    "the case when @xmath608 can be dealt with by observing ( @xcite , page 240 ) that an @xmath609 rs code as above , can be shortened by length @xmath610 , where @xmath611 , to form an @xmath612 code with the same minimum distance @xmath590 as before .",
    "this is easily seen by viewing each codeword as being created by appending @xmath592 parity check symbols to the end of the corresponding message string .",
    "then the code formed by considering the set of codewords with the @xmath610 leading symbols identical to zero has precisely the properties stated above . with @xmath29",
    "equal to @xmath587 as before , we have @xmath613 equals @xmath29 so taking @xmath610 to be @xmath614 we get an @xmath615 code , with @xmath616 , @xmath617 and minimum distance @xmath590 .",
    "now since the codelength is @xmath27 and symbols of this code are in @xmath618 the code composition can be carried out as before .",
    "we summarize the above in the following .",
    "* proposition 6 : * to obtain a code with small block error probability it is enough to have demonstrated a partitioned superposition code for which the section error rate is small with high probability .",
    "in particular , for any given positive @xmath65 and @xmath59 , let @xmath36 be a rate for which the partitioned superposition code with @xmath27 sections has @xmath619 then through concatenation of such a code with an outer reed - solomon code , one obtains a composite code for which the rate is @xmath620 and the block error probability is less than or equal to @xmath65 .",
    "here we provide discussion of the implications of our error probability bound of section v for randomly generated dictionaries @xmath22 .",
    "the probability of the error event @xmath621 has been computed averaging over random generation of the dictionary @xmath22 as well as the distribution of the received sequence @xmath8 .",
    "let s denote the given bound @xmath622 .",
    "the theorem asserts that this bound is exponentially small .",
    "for instance , it is less than @xmath623 .",
    "the same bound holds for any given @xmath1 bit input sequence @xmath10 .",
    "indeed , the probability of @xmath624 given that @xmath10 is sent , which we may write as @xmath625 $ ] is the same for all @xmath10 by exchangeability of the distribution of the columns of @xmath22 .",
    "accordingly , it also matches the average probability @xmath626 = \\frac{1}{2^k } \\sum_u { \\mathbb{p}}[e|u]$ ] , averaging over all possible inputs , so this average probability will have the same bound .    reversing the order of the average over @xmath10 and the average over the choice of dictionary @xmath22 , the average probability",
    "may be written @xmath627\\big]$ ] , where @xmath628 $ ] denotes the probability of the error event @xmath624 , conditioning on the event that the input is @xmath10 and that the dictionary is @xmath22 ( the only remaining average in @xmath628 $ ] is over the distribution of the noise ) .",
    "this @xmath628 $ ] will vary with @xmath10 as well as with @xmath22 .",
    "an appropriate target performance measure is @xmath629=\\frac{1}{2^k } \\sum_u { \\mathbb{p}}[e|u , x],\\ ] ] the probability of the error event , averaged with respect to the input , conditional on the random dictionary @xmath22 . since the expectation @xmath630={\\mathbb{e}}\\big [ { \\mathbb{p}}[e|x ] \\big]$ ] satisfies the indicated bound , random @xmath22 are likely to behave similarly .",
    "indeed , by markov s inequality @xmath631 \\ge \\tau p_e^{b}\\big ] < 1/\\tau$ ] .",
    "so with a single draw of the dictionary @xmath22 , it will satisfy @xmath632 \\le \\tau p_{e}^{b}$ ] , with probability at least @xmath633 .",
    "the manageable size of the dictionary facilitates computational verification by simulation that the bound holds for that @xmath22 . with @xmath634",
    "one may independently repeat the generation of @xmath22 a geometric(@xmath404 ) number of times until success .",
    "the mean number of draws of the dictionary required for one with the desired performance level is @xmath69 .",
    "even with only one draw of @xmath22 , one has with @xmath635 , that @xmath629 \\le 2l e^{-(n/2)d_{min}},\\ ] ] except for @xmath22 in an event of probability not more than @xmath636 .",
    "now @xmath632 $ ] exponentially small implies that @xmath628 $ ] is exponentially small for most @xmath10 ( again by markov s inequality ) . in theory one could expurgate the codebook , leaving only good performing @xmath26 and reassigning the mapping from @xmath10 to @xmath26 , to remove the minority of cases in which @xmath628 > 4l e^{-(n/2)d_{min}}$ ] .",
    "thereby one would have uniformly exponentially small error probability .    in principle",
    ", simulations can be used to evaluate @xmath628 $ ] for a specific @xmath26 and @xmath22 , to decide whether that @xmath26 should be used .",
    "however , it is not practical to do so in advance for all @xmath26 , and it is not apparent how to perform such expurgations efficiently on - line during communications .",
    "thus we maintain our focus in this paper on average case error probability , averaging over the possible inputs , rather than maximal error probability .",
    "as we have said , for the average case analysis , armed with a suitable decoder , one can check , for a dictionary @xmath22 , whether it satisfies an exponential bound on @xmath632 $ ] empirically by simulating a number of draws of the input and of the noise .",
    "nevertheless , it would be nice to have a more direct , non - sampling check that a dictionary @xmath22 satisfies requirement for such a bound on @xmath632 $ ] .",
    "our current method of proof does not facilitate providing such a direct check .",
    "the reason is that our analysis does not exclusively use the distribution of @xmath8 given @xmath10 and @xmath22 ; rather it makes critical use of properties of the joint distribution of @xmath8 and @xmath22 given @xmath10 .",
    "likewise , averaging over the random generation of the dictionary , permits a simple look at the satisfaction of the average power constraints . with a randomly drawn @xmath10 , and associated coefficient vector @xmath637 , consider the behavior of the power @xmath638 and whether it stays less than @xmath639 .",
    "the event @xmath640 , when conditioning on the input @xmath10 , has exponentially small probability @xmath641 $ ] , in accordance with the normal distribution of the codeword obtained via the distribution of the dictionary @xmath22 . again",
    "@xmath641 $ ] is the same for all @xmath10 and hence matches the average @xmath642 $ ] with expectation taken with respect to random input @xmath10 as well as with respect to the distribution of @xmath22 . so reversing the order of the expectation we have that @xmath643\\big]$ ] enjoys the exponential bound , from which , again by applications of markov s inequality , except for @xmath22 in an event of exponentially small probability , @xmath644 for all but an exponentially small fraction of coefficient vectors @xmath26 in @xmath486 .",
    "control of the average power is a case in which we can formulate a direct check of what is required of the dictionary @xmath22 , as is examined in appendix b.",
    "here we examine the average and maximal power of the codewords . the maximal power has a role in our analysis of decoding .",
    "the power of a codeword @xmath62 is its squared norm @xmath645 , consisting of the average square of the codeword values across its @xmath2 coordinates .",
    "the terminology _ power _ arises from settings in which codeword values are voltages on a communication wire or a transmission antenna in the wireless case , recalling that power equals average squared voltage divided by resistance .    *",
    "average power for the signed subset code : * consider first our signed , subset superposition code .",
    "each input correspond to a coefficient vector @xmath646 , where for each of the @xmath27 sections there is only one @xmath647 for which @xmath20 is nonzero , and , having absorbed the size of the terms into the @xmath93 , the nonzero coefficients are taken to be @xmath53 .",
    "these are the coefficient vectors @xmath26 of our codewords @xmath648 , for which the power is @xmath649 .    with a uniform distribution on the binary input sequence of length @xmath650 ,",
    "the induced distribution on the sequence of indices @xmath651 is independent uniform on the @xmath29 choices in section @xmath12 , and likewise the signs are independent uniform @xmath53 valued , for @xmath652 . fix a dictionary @xmath22 , and consider the average of the codeword powers with this uniform distribution on inputs , @xmath653 by independence across sections , this average simplifies to @xmath654 now we consider the size of this average power , using the distribution of the dictionary @xmath22 , with each entry independent normal(@xmath655 ) .",
    "this average power @xmath656 has mean @xmath657 equal to @xmath6 , standard deviation @xmath658 , and distribution equal to @xmath659 { { \\mathcal{x}}}_{nn}^2 $ ] , where @xmath660 is a chi - square random variable with @xmath661 degrees of freedom .",
    "accordingly @xmath662 is very close to @xmath6 .    indeed , in a random draw of the dictionary @xmath22 , the chance that @xmath662 exceeds @xmath663 is approximately less than @xmath65 , as can be seen via the chernoff - cramer bound @xmath664 , for positive @xmath61 , where the exponent @xmath665 $ ] is near @xmath666 for small positive @xmath667 , so that the bound is near @xmath668 , which is @xmath65 for @xmath669 .",
    "or we may appeal to the normal approximation for fixed @xmath61 when @xmath661 is large ; the probability is not more than @xmath670 that the dictionary has average power @xmath662 outside the interval formed by the mean plus or minus two standard deviations @xmath671    for instance , suppose @xmath672 and the rate is near the capacity @xmath673 , so that @xmath674 is near @xmath675 , and pick @xmath438 and @xmath676 .",
    "then with high probability @xmath662 is not more than @xmath677 times @xmath6 .",
    "if the average power constraint is held stringently , with average power to be precisely not more than @xmath6 , then in the design of the code proceed by generating the entries of @xmath22 with power @xmath678 , where @xmath679 is less than @xmath6 .",
    "the analysis of the preceding sections then carries through to show exponentially small probability of more than a small fraction of mistakes when @xmath680 as long as @xmath679 is sufficiently close to @xmath6 .    * average power for the subset code : * likewise , let s consider the case of subset superposition coding without use of the signs . once again fix @xmath22 and consider a uniform distribution on inputs ; it again makes the term selections @xmath651 independent and uniformly distributed over the @xmath29 choices in each section .",
    "now there is a small , but non - zero , average @xmath681 of the terms in each section @xmath12 , and likewise a very small , but non - zero , overall average @xmath682 .",
    "we need to make adjustments by these averages when invoking the section independence to compute the average power . indeed , as in the rule that an expected square is the square of the expectation plus a variance , the average power is the squared norm of the average of the codewords plus the average norm squared difference between codewords and their mean .",
    "the mean of the codewords , with the uniform distribution on inputs , is @xmath683 , which is a normal(@xmath684 ) random vector of length @xmath2 .    by independence of the term selections ,",
    "the codeword variance is @xmath685 .",
    "accordingly , in this subset coding setting , @xmath686 using the independence of @xmath687 and @xmath688 and standard distribution theory for sample variances , with a randomly drawn dictionary @xmath22 , we have that @xmath656 is @xmath689 times a chi - square random variable with @xmath690 degrees of freedom , plus @xmath691 times an independent chi - square random variable with @xmath2 degrees of freedom .",
    "so it has mean equal to @xmath6 and a standard deviation of @xmath692 , which is slightly greater than before .",
    "it again yields only a small departure from the target average power @xmath6 , as long as @xmath2 and @xmath29 are large .",
    "* worst case power : * next we consider the matter of the size of the maximum power @xmath693 among codewords for a given design @xmath22 .",
    "the simplest distribution bound is to note that for each @xmath26 , the codeword @xmath55 is distributed as a random vector with independent normal(@xmath694 ) coordinates , for which @xmath638 is @xmath695 times a chi - square @xmath2 random vector .",
    "there are @xmath696 such codewords , with the rate written in nats .",
    "we recall the probability bound @xmath697 .",
    "accordingly , by the union bound , @xmath698 is not more than @xmath699 except in an event of probability which we bound by @xmath700 , where @xmath701 is the inverse of the function @xmath702 $ ] .",
    "this @xmath703 is seen to be of order @xmath704 for small positive @xmath460 and of order @xmath705 for large @xmath460 .",
    "consequently , the bound on the maximum power is near @xmath706 rather than @xmath6 .    according to this characterization , for positive rate communication , with subset superpositions , one can not rely , either in encoding or in decoding , on the norms @xmath638 being uniformly close to their expectation .    *",
    "individual codeword power : * we return to signed subset coding and provide explicitly verifiable conditions on @xmath22 such that for every subset , the power @xmath638 is near @xmath6 for most choices of signs .",
    "the uniform distribution on choices of signs ameliorates between - section interference to produce simplified analysis of codeword power .",
    "the input specifies the term @xmath651 in each sections along with the choice of its sign given by @xmath707 in @xmath708 , leading to coefficient vectors @xmath26 equal to @xmath707 at position @xmath651 in section @xmath12 , for @xmath709 .",
    "the uniform distribution on the choices of signs leads to them being independently , equiprobable @xmath30 and @xmath31 .",
    "now the codeword is given by @xmath710 .",
    "it has the property that conditional on @xmath22 and the subset @xmath711 , the contributions @xmath712 for distinct sections are made to be mean zero uncorrelated vectors by the random choice of signs . in particular , again conditioning on the dictionary @xmath22 and the subset @xmath175 , we have that the power @xmath638 has conditional mean @xmath713 which we shall see is close to @xmath6 . the deviation from the conditional mean @xmath714 equals @xmath715 .",
    "the presence of the random signs approximately symmetrizes the conditional distribution and leads to conditional variance @xmath716 .",
    "now concerning the columns of the dictionary , the squared norms @xmath717 are uniformly close to @xmath56 , since the number of such @xmath46 is not exponentially large .",
    "indeed , by the union bound the maximum over the @xmath43 columns , satisfies @xmath718 except in an event of probability bounded by @xmath65 .    whence the conditional mean power @xmath719 is not more than @xmath720 uniformly over all allowed selections of @xmath27 term subsets .",
    "note here that the polynomial size of @xmath46 makes the @xmath721 small ; this is in contrast to the worst case analysis above were the log cardinality divided by @xmath2 is the fixed rate @xmath36 .",
    "next to show that the conditional mean captures the typical power , we show that the conditional variance is small . toward that end",
    "we examine the inner products @xmath722 and their maximum absolute value @xmath723 .",
    "consider products of independent standard normals @xmath724 .",
    "these have moment generating function @xmath725 equal to @xmath726 .",
    "[ this matches the moment generating function for half the difference in squares of independent normals found in section 2 ; to see why note that @xmath727 equals half the difference in squares of @xmath728 and @xmath729 . ]",
    "accordingly @xmath730 , for positive @xmath67 , where @xmath731 . as previously discussed , this @xmath732 is near @xmath733 for small @xmath67 and accordingly its inverse function @xmath455 is near @xmath459 for small @xmath460 .",
    "the corresponding two - sided bound is @xmath734 . by the union bound",
    ", we have that @xmath735 except for dictionaries @xmath22 in an event of probability not more than @xmath65 .    recall that the conditional variance of @xmath638 equals @xmath716 . in the likely event that the above bound holds , we have that this conditional variance is not more than @xmath736 .",
    "consequently , the conditional distribution of the power @xmath638 given @xmath22 and @xmath175 is indeed concentrated near @xmath6 .",
    "accordingly , for each subset , most choices of sign produce a codeword with power @xmath638 near @xmath6 .",
    "moreover , for this codeword power property , it is enough that the individual columns of the dictionary have @xmath717 near @xmath56 and @xmath722 near @xmath129 , uniformly over @xmath737 .",
    "we thank john hartigan , cong huang , yiannis kontiyiannis , mokshay madiman , xi luo , dan spielman , edmund yeh , john hartigan , mokshay madiman , dan spielman , imre teletar , harrison zhou , david smalling and creighton heaukulani for helpful conversations .",
    "barron , a.  joseph , `` least squares superposition codes of moderate dictionary size , reliable at rates up to capacity , '' _ proc .",
    "symp information theory _",
    ", austin , texas , jun 13 - 18 , 2010 .",
    "benjamini , y. and hochberg , y. `` controlling the false discovery rate : a practical and powerful approach to multiple testing , '' _",
    ", 57 , 1995 . g. berrou , a. glavieux , and p. thitimajshima , `` near shannon limit error - correcting coding : turbo codes , '' _ proc .",
    "commun _ , geneva , switzerland , may 1993 , pp .",
    "1064 - 1070 .",
    "r. j. mceliece , d. j. c. mackay , and j - f .",
    "cheng , `` turbo decoding as an instance of pearl s belief propagation algorithm , '' _ ieee journal on selected areas in commun _ ,",
    "16 , 2 , pp .",
    "140 - 152 , feb . 1998 .",
    "d. donoho , `` for most large underdetermined systems of linear equations , the minimal l1-norm solution is also the sparsest solution , '' commun .",
    "pure and appl .",
    "59 , no . 6 , pp .",
    "797 - 829 , jun . 2006 .",
    "donoho , j. tanner , `` exponential bounds implying construction of compressed sensing matrices , error - correcting codes , and neighborly polytopes by random sampling , '' _ ieee trans .",
    "inform . theory _",
    "alyson k. fletcher , sundeep rangan , vivek k. goyal , kannan ramchandran , `` denoising by sparse approximation : error bounds based on rate - distortion theory,''__j .",
    "signal process _",
    "10 , 2006 .",
    "w.  hoeffding , `` probability inequalities for sums of bounded random variables , '' _ j. american statist .",
    "_ , pp.13 - 30 , march , 1963 .",
    "hu , h. zhao and h.h .",
    "zhou , `` multiple hypothesis testing with groups , '' manuscript .",
    "l.  jones , `` a simple lemma for optimization in a hilbert space , with application to projection pursuit and neural net training , '' _ annals of statistics _ , vol.20 , pp.608 - 613 , 1992 .",
    "wainwright , `` sharp thresholds for high - dimensional and noisy sparsity recovery using @xmath738-constrained quadratic programming ( lasso ) . ''",
    "_ ieee trans .",
    "inform . theory _ , vol.55 , no.5 , pp.2183 - 2202 , may 2009 .",
    "w. wang , m. j. wainwright , and k. ramchandran ,  information - theoretic limits on sparse signal recovery : dense versus sparse measurement matrices ,  _ ieee trans . inform . theory _",
    "6 , jun 2010 ."
  ],
  "abstract_text": [
    "<S> for the additive white gaussian noise channel with average codeword power constraint , new coding methods are devised in which the codewords are sparse superpositions , that is , linear combinations of subsets of vectors from a given design , with the possible messages indexed by the choice of subset . </S>",
    "<S> decoding is by least squares , tailored to the assumed form of linear combination . </S>",
    "<S> communication is shown to be reliable with error probability exponentially small for all rates up to the shannon capacity . </S>"
  ]
}