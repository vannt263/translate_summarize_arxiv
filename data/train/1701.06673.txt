{
  "article_text": [
    "caching the most popular contents close to user terminals is a promising solution to deal with the tremendous growth of the mobile traffic in the fifth generation ( 5 g ) wireless networks .",
    "a caching system comprises of two separate phases .",
    "the first phase is the placement phase that occurs during the off - peak hours when the resources are under - utilized . during this phase ,",
    "the network nodes fill their caches with the popular contents .",
    "the second phase is the delivery phase that occurs during the peak hours when the network is congested , and hence , the caches at the network nodes can be exploited to partly serve the user requests . in  @xcite , maddah - ali and niesen",
    "have proposed a novel centralized coded caching scheme for an error - free broadcast channel .",
    "the authors showed that the network congestion can be significantly reduced by jointly designing the placement and delivery to generate coded multicast transmissions in the delivery phase .",
    "however , the centralized scheme in  @xcite relies on the knowledge of the number of active users in the delivery phase to design the placement phase .",
    "this limits the applicability of the centralized scheme , since the placement and the delivery phases might take place at different times .",
    "moreover , the users in the wireless networks are characterized by their mobility in which the number of users varies in the network over time . to cope with this problem ,",
    "the work in  @xcite is extended in  @xcite to present the decentralized coded caching .",
    "recently , the concept of coded caching is studied in interference networks  @xcite .",
    "it is shown in  @xcite that caches at the transmitters can improve the sum degrees of freedom ( dof ) of the interference channel by allowing cooperation between transmitters for interference mitigation . in  @xcite",
    ", the authors studied the benefits of caching at both the transmitters and receivers in the interference channel while focusing on the case of three receivers .",
    "the extension for an arbitrary number of receivers was studied in  @xcite .",
    "the authors in  @xcite proposed a centralized coded caching scheme for a fog radio access network ( f - ran ) with caches equipped at edge - nodes ( ens ) to minimize a novel metric named normalized delivery time ( ndt ) which measures the worst case delivery latency with respect to an interference - free baseline system in the high signal - to - noise ratio ( snr ) regime .",
    "in contrast to prior works  @xcite that discussed the centralized content placement , we study the decentralized caching problem for an f - ran architecture with caches equipped at both ens and users .",
    "we first formulate the problem for arbitrary number of ens and users .",
    "then , we propose the decentralized content placement in which each node stores a fraction of each file randomly and independently from other nodes . based on the decentralized content placement , we design a coded delivery scheme that exploits the network topology to minimize the ndt for the two ens and arbitrary number of users case .",
    "we show that the performance of the decentralized scheme is within a constant factor of the information - theoretic optimum for the case in which only ens have caches .",
    "the performance of the decentralized scheme is evaluated numerically for the general scenario .",
    "we consider a fog radio access network ( f - ran ) as depicted in fig  [ fig1 ] comprising a cloud server that has a library of @xmath1 files , @xmath2 , each of size @xmath3 bits .",
    "a set of @xmath4 edge - nodes , @xmath5 , is ready to serve the requests of @xmath0 users through a @xmath6 gaussian wireless interference channel with channel coefficient @xmath7 between @xmath8 and the @xmath9th user .",
    "the channels @xmath10 $ ] , @xmath11 $ ] , @xmath12 $ ] , are independent and identically distributed according to a continuous distribution .",
    "the cloud is connected to each en via a fronthaul link of capacity @xmath13 bits per symbol , where the symbol refers to a channel use of the downlink wireless channel .",
    "each @xmath8 , @xmath11 $ ] , is equipped with a cache memory @xmath14 of size @xmath15 bits , while each user , @xmath12 $ ] , is equipped with a cache memory @xmath16 of size @xmath17 bits .",
    "we refer to @xmath18 and @xmath19 as the normalized cache size at each en and at each user , respectively , where @xmath20 $ ] . to denote the bitwise xor operation .",
    "@xmath21 $ ] refers to the real numbers between @xmath22 and @xmath23 .",
    "@xmath24 $ ] denotes the set of integers @xmath25 , and @xmath26 $ ] denotes the set of integers @xmath27 for @xmath28 . ]",
    "the system operates in two phases , namely _ placement phase _ and _ delivery phase_. the placement phase takes place when the traffic load is low , and hence , the network resources can be utilized to fill the cache memories of each network node as a function of the content library @xmath29 without any prior knowledge of the number of active users in the next phase , the future user demands , and channel coefficients . in the delivery phase , the @xmath9th user requests a file @xmath30 among the @xmath1 files available at the cloud , where we consider @xmath31\\in \\left[n\\right]^{k_r}$ ] as the demand vector .",
    "the cloud replies to user demands by sending a message @xmath32 of block length @xmath33 to @xmath8 via the fronthaul link , where each fronthaul message @xmath34 can not exceed @xmath35 bits due to the fronthaul capacity limitations .",
    "each @xmath8 , @xmath11 $ ] , has an encoding function mapping the cache contents @xmath14 , the fronthaul message @xmath36 , the demand vector @xmath37 , and the channels @xmath38 to a message @xmath39 of block length @xmath40 .",
    "@xmath8 transmits message @xmath41 over the interference channel to @xmath0 users under an average transmit power constraint @xmath42 .",
    "each user @xmath9 implements a decoding function to estimate the requested file @xmath43 from the cache contents @xmath44 and the received message @xmath45 .",
    "@xmath46\\ ] ] where @xmath47 is the received signal of the @xmath9th user at time @xmath48 , and @xmath49 denotes the complex gaussian noise at the @xmath9th user . for a given coding scheme ( caching , cloud encoding , en encoding , and user decoding functions ) , the transmission rate of the system is defined as @xmath50 , where @xmath51 is the end - to - end latency .",
    "we say that a coding scheme is feasible , if and only if , we have @xmath52^{k_r } } \\max_{k\\in\\left[k_r\\right]}\\mathbb{p}\\left(\\hat{w}_{d_k}\\neq w_{d_k}\\right)\\rightarrow 0\\ \\text{as}\\ f\\rightarrow \\infty\\ ] ] which is the worst - case probability of error over all possible demands @xmath37 and over all users .",
    "our goal in this paper is to design a decentralized coding scheme to minimize the end - to - end latency @xmath51 for delivering user demands @xmath53^{k_r}$ ] in the delivery phase . in the following ,",
    "we define the normalized delivery time ( ndt ) first discussed in  @xcite as a performance metric for any coding scheme .    the normalized delivery time ( ndt ) for any feasible coding scheme with a given normalized cache size @xmath54 , @xmath55 , and fronthaul capacity @xmath56 , is defined as @xmath57 where @xmath58 denotes the expectation with respect to the channel matrix @xmath38 , and @xmath59 measures the multiplexing gain of the fronthaul links .",
    "moreover , we define the minimum ndt for a given tuple @xmath60 , as @xmath61    the ndt @xmath62 represents the worst case delay to serve any possible user demands @xmath53^{k_r}$ ] normalized with respect to an interference - free baseline system of transmission rate @xmath63 in the high snr regime .",
    "[ t ]   edge - nodes and @xmath64 users.,title=\"fig : \" ]    we point out that the end - to - end latency for an f - ran depends on two transmission delays , _ fronthaul delay _",
    "@xmath33 , and _ edge delay _ @xmath40 .",
    "the fronthaul delay @xmath33 represents the transmission time to deliver the cloud messages to ens , while the edge delay @xmath40 represents the transmission time to deliver the messages of ens to end users . in order to obtain the fronthaul - ndt @xmath65 , we substitute in   @xmath66 .",
    "similarly , we get the edge - ndt @xmath67 by substituting @xmath68 in  . throughout the paper , we consider two types of transmissions .",
    "\\1 ) * serial transmission : * fronthaul and edge transmissions occur consecutively .",
    "the cloud first transmits its messages @xmath69 to ens that begin delivering their messages @xmath41 to the users only after receiving the fronthaul message . as a result , the overall latency is the sum of the fronthaul and edge delays .",
    "thus , the ndt of system with serial transmission is defined as @xmath70    \\2 ) * pipelined transmission : * in  @xcite , the authors introduced the concept of pipelined transmission , where the fronthaul and edge transmissions take place simultaneously .",
    "in other words , ens do not wait for the fronthaul transmission to be completed in order to start transmitting their messages to the users .",
    "accordingly , the ndt of the system for the pipelined transmission is given by @xmath71 which is the maximum between the fronthaul - ndt and the edge - ndt . in the following ,",
    "we characterize the trade - off between the normalized cache size @xmath54 , @xmath55 and the ndt of the system for the decentralized caching scheme with both serial transmission @xmath72 and pipelined transmission @xmath73 .",
    "in this section , we introduce the decentralized coded caching for the @xmath74 f - ran caching problem .",
    "first , we present the decentralized content placement . then , we design the delivery scheme to serve any user demands in the delivery phase .      in the placement phase each user @xmath12",
    "$ ] independently stores @xmath75 bits from each file @xmath76 uniformly at random . in a similar way ,",
    "each @xmath8 , @xmath11 $ ] , independently stores @xmath77 bits from each file @xmath76 uniformly at random .",
    "therefore , the total number of bits stored at each user , and at each en is @xmath78 bits and @xmath79 bits , respectively .",
    "thus , the cache memory size constraint is not violated at each node . moreover ,",
    "each bit of a file has probability @xmath55 to be cached at a specific user and probability @xmath54 to be cached at a specific en .",
    "accordingly , each file @xmath76 is partitioned into fragments @xmath80 for @xmath81 $ ] a subset of ens , and @xmath82 $ ] a subset of users , in which the fragment @xmath80 denotes the bits of file @xmath83 stored exclusively in the cache memory of each en @xmath84 and each user @xmath85 , but these bits do not exist in the caches of other nodes . for @xmath86 , @xmath87 , each file @xmath83",
    "is split into @xmath88 fragments .",
    "[ ex1 ] consider an f - ran with @xmath89 ens , @xmath64 users , and @xmath90 files denoted by @xmath91 . according to the decentralized content placement",
    ", we can represent the file @xmath92 as follows as a shorthand for @xmath93 , and @xmath94 as a shorthand for @xmath95 @xmath96 @xmath97 such that @xmath98 consists of bits of file @xmath92 that are not stored at any node , and @xmath99 consists of bits of file @xmath92 in the cache memory of all nodes except @xmath100 .",
    "the same follows for file @xmath101 . by strong law of large numbers ,",
    "the size of fragment @xmath80 is approximately equal to @xmath102 with probability approaching one for large file size @xmath3 . in our example , @xmath103 .",
    "consider any demand vector @xmath53^{k_r}$ ] .",
    "in example  [ ex1 ] , let the first user request file @xmath92 and the second user request file @xmath101 . therefore , the first user needs fragments @xmath104 to decode file @xmath92 , and the second user needs fragments @xmath105 to decode file @xmath101 . in the following , we explain the delivery scheme to send these fragments .",
    "the proposed delivery scheme is divided into five stages . in each stage ,",
    "the cloud and ens exploit the wireless channel to deliver a group of fragments of the requested files depending on the cache contents at the nodes .",
    "\\1 ) _ delivery of fragments @xmath106 _ : since these @xmath0 fragments are not available at any nodes , the cloud sends @xmath107 to each en via the fronthaul links .",
    "hence , each en can perform cooperative zero - forcing ( zf ) beamforming on the interference channel , where the system becomes a multiple - input - single - output ( miso ) broadcast channel with @xmath89 transmit antennas and @xmath0 receivers . in general",
    ", the sum dof for a broadcast channel with @xmath4 transmit antennas and @xmath0 receivers is @xmath108  @xcite . in our case @xmath109 .",
    "thus , the edge transmission latency to deliver these fragments in the high snr regime is @xmath110 . by using the _ soft - transfer mode _ on fronthaul links as proposed in  @xcite , the cloud implements zf - beamforming and quantizes the encoded signal to be transmitted to each en .",
    "therefore , the fronthaul transmission latency equals to the edge transmission latency multiplied by the ratio between the transmission rates @xmath111 , yielding a fronthaul transmission latency @xmath112 .",
    "thus , fronthaul - ndt and edge - ndt for this stage are given by @xmath113 in our example , the first user receives fragment @xmath98 and the second user receives fragment @xmath114 at the end of this stage .",
    "\\2 ) _ delivery of fragments @xmath115 , @xmath116 _ : these fragments are available at least at one user and at none of ens . for each subset",
    "@xmath117 $ ] of cardinality @xmath118 $ ] , the cloud sends a multicast message @xmath119 to each en , where @xmath120 refers to the bits of file @xmath30 requested by user @xmath9 which are available exclusively at users @xmath121 except user @xmath9 .",
    "then ens deliver the fronthaul message to @xmath0 users .",
    "since the @xmath9th user has the bits @xmath122 for @xmath123 , the @xmath9th user can recover its needed bits @xmath120 from the multicast transmission .",
    "there are @xmath124 subsets of cardinality @xmath125 , so the rate of this transmission is given by    l r^(2)=_i=2^k_r k_ri ( 1-_t)^2 _ r^i-1(1-_r)^k_r - i+1f + = + = f    where the expected size of @xmath119 with @xmath126 is @xmath127 . as a result ,",
    "fronthaul and edge transmission delays are @xmath128 , @xmath129 , respectively .",
    "moreover , the fronthaul - ndt and edge - ndt are given by @xmath130 in our example , the transmission in this stage is @xmath131 , so that the first user can recover fragment @xmath132 from the received signal and cached fragment @xmath133 .",
    "similarly , the second user can recover fragment @xmath133 from the received signal and cached fragment @xmath132 .",
    "\\3 ) _ delivery of fragments @xmath134 , @xmath116 _ : in this stage , each subset @xmath81 $ ] of ens , @xmath135 , transmits a multicast signal @xmath136 from their cache contents to a subset @xmath117 $ ] of users for @xmath137 , so that each user @xmath138 can recover the needed fragment @xmath120 from the received signal and its cache contents .",
    "thus , the rate of this transmission is given by    l r^(3)=_t=1 ^ 2_i=2^k_r 2 t k_ri _ t^t(1-_t)^2-t _ r^i-1(1-_r)^k_r - i+1f + = f.    as a result , the edge transmission latency is @xmath139 , yielding the the fronthaul - ndt and edge - ndt @xmath140 in our example , @xmath100 first transmits a message @xmath141 , then , @xmath142 transmits a message @xmath143 . finally , both of ens cooperatively transmit a message @xmath144 .",
    "hence , the first user obtains fragments @xmath145 , and the second user obtains fragments @xmath146 at the end of this stage .",
    "0.49     and the lower bound in theorem  [ th2 ] for @xmath147 and @xmath148 . ]",
    "0.49     and the lower bound in theorem  [ th2 ] for @xmath147 and @xmath148 . ]",
    "\\4 ) _ delivery of fragments @xmath149_:each of the two ens has these @xmath0 fragments .",
    "hence , each en can implement zf beamforming on the interference channel to transmit these fragments with @xmath109 .",
    "thus , the edge transmission latency is @xmath150 , yielding the fronthaul - ndt and edge - ndt @xmath151 in our example , the first user receives fragment @xmath152 , and the second user receives fragment @xmath153 at the end of this stage .",
    "\\5 ) _ delivery of fragments @xmath154 , @xmath155 _ : in this stage , we propose two methods for transmitting these fragments , where the ens choose the method that minimizes the overall ndt as we will elaborate in theorem  [ th1 ] .",
    "+ a ) each en has a dedicated fragment to each user .",
    "hence , the system can be treated as `` @xmath6 x - channel '' , where the sum dof is @xmath156 by applying the interference alignment ( ia ) scheme  @xcite . in our case",
    "the sum dof is @xmath157 .",
    "thus , the edge transmission latency is @xmath158 leading to the fronthaul - ndt and edge - ndt @xmath159 b ) another achievable transmission scheme for this stage is that the cloud transmits fragments @xmath160 to @xmath142 and fragments @xmath161 to @xmath100 via the fronthaul links .",
    "thus , both ens have @xmath162 fragments and the system becomes a broadcast channel with dof @xmath163 by applying zf beamforming on the interference channel .",
    "hence the fronthaul - ndt and edge - ndt are given by @xmath164    [ t ! ]",
    ".,title=\"fig : \" ]    the following theorem gives the ndt of the proposed decentralized scheme for the serial and pipelined transmissions .",
    "[ th1 ] for the @xmath74 f - ran with decentralized caching placement , each en having a normalized cache size @xmath165 $ ] , each user having a normalized cache size @xmath166 $ ] , and the fronthaul gain @xmath167 , the proposed coded delivery scheme achieves ndt for serial transmission @xmath168 and achieves ndt for pipelined transmission @xmath169 @xmath170\\\\ \\delta_f^{\\left(b\\right)}=\\frac{\\left(1-\\mu_t\\right)^2\\left(1-\\mu_r\\right)}{r\\mu_r}\\big[1-\\left(1-\\mu_r\\right)^{k_r}\\\\ \\qquad\\qquad\\qquad-\\frac{k_r}{2}\\mu_r\\left(1-\\mu_r\\right)^{k_r-1}\\left(\\frac{1 - 3\\mu_t}{1-\\mu_t}\\right)\\big]\\\\ \\delta_e^{\\left(a\\right)}=\\frac{\\left(1-\\mu_r\\right)}{\\mu_r}\\big[1-\\left(1-\\mu_r\\right)^{k_r}\\\\ \\qquad\\qquad\\qquad-\\left(\\frac{k_r}{2}-\\mu_t\\left(1-\\mu_t\\right)\\right)\\mu_r\\left(1-\\mu_r\\right)^{k_r-1}\\big]\\\\ \\delta_e^{\\left(b\\right)}=\\frac{\\left(1-\\mu_r\\right)}{\\mu_r}\\left[1-\\left(1-\\mu_r\\right)^{k_r}-\\frac{k_r}{2}\\mu_r\\left(1-\\mu_r\\right)^{k_r-1}\\right ] \\end{array}\\ ] ]    see appendix  [ app1 ]    * lower bound : * to evaluate the performance of the decentralized scheme , we derive a generic lower bound on the optimum ndt for the @xmath6 f - ran caching problem for serial and pipelined transmissions .",
    "[ th2 ] for an f - ran with fronthaul capacity @xmath56 , @xmath4 ens , each with a normalized cache size @xmath54 , and @xmath0 users , each with a normalized cache size @xmath55 , the optimum ndt @xmath171 for serial transmission is lower bounded by @xmath172 where @xmath173 is the solution of the following linear programming @xmath174 for pipelined transmission , the optimum ndt @xmath175 is lower bounded by    @xmath176}\\left\\{\\frac{f\\left(s\\right)}{s+\\left(k_t - s\\right)r}\\right\\},\\left(1-\\mu_r\\right)\\right\\}\\]]in   and  , @xmath177 $ ] , and @xmath178 $ ] .",
    "the proof of theorem  [ th2 ] uses the cut - set lower bound on any achievable ndt .",
    "the proof can be found in appendix  [ app2 ]",
    "in this section , we evaluate the performance of the decentralized scheme introduced in section  [ decentralized ] .",
    "we first evaluate numerically the performance of the decentralized scheme for the general case .",
    "then , we discuss two special cases .    fig .",
    "[ fig2 ] shows the gap between the achievable decentralized ndt in theorem  [ th1 ] and the lower bound in theorem  [ th2 ] for serial and pipelined transmissions with fronthaul gain @xmath147 and a number of users @xmath148 .",
    "it is observed that the maximum gap is about @xmath179 for serial transmission and @xmath180 for pipelined transmission .",
    "moreover , we observe that the gap decreases rapidly as @xmath181 , however , there is only a subtle decrease as @xmath54 increases .    * for @xmath182 or / and @xmath183 : * in this case , each en has all the library files , and hence , the system becomes a miso broadcast channel with two transmit antennas . from theorem  [ th1 ] , the decentralized scheme achieves ndt as    @xmath184\\ ] ] comparing with the performance of the broadcast channel with a single transmit antenna in  @xcite , ndt in   is minimum with term @xmath185 , since @xmath0 fragments cached at none of the users are transmitted with @xmath109 in our case as compared to @xmath186 in  @xcite . in fig  [ fig3 ]",
    ", we see that this reduction is significant when @xmath55 is low ; however , the two systems achieve the same performance when @xmath55 approaches to one .    * for @xmath187 : * the network becomes an f - ran with caches at ens only . in  @xcite ,",
    "the authors provide a centralized coded caching scheme for this network within a constant factor @xmath188 from the lower bound . from theorem  [ th1 ] , the decentralized scheme for @xmath189 achieves    l _ s^dec=\\ {    ll + _ t(1-_t ) & 0<rk_r + & r > k_r    .  [ eqn5 ] + _ p^dec=\\ {    ll & rr_1 + ( 1-_t^2 ) & r_2r < r_1 + ( 1-_t)^2 & 0 < r",
    "< r_2    .[eqn4 ]    where @xmath190 @xmath191 , @xmath192 .",
    "[ th3 ] for pipelined transmission , the decentralized scheme   is optimum for @xmath165 $ ] , @xmath193 and @xmath194 , @xmath195 .",
    "let @xmath171 be the optimal centralized caching scheme for serial transmission .",
    "we have @xmath196 for @xmath165 $ ] , @xmath197 and @xmath198 $ ] , @xmath195 .",
    "the proof is presented in appendix  [ app3 ]    theorem  [ th3 ] shows that the proposed decentralized scheme is approximately optimal for pipelined and serial transmissions .",
    "however , the optimality is not maintained in case @xmath199 $ ] , @xmath200 for serial transmission and @xmath201 $ ] , @xmath202 for pipelined transmission .",
    "the reason is that when the fronthaul gain is low and the aggregate size of ens caches is high enough to store all the library files , the best strategy is to store different contents in ens so that the fronthaul link would not be used .",
    "however , this strategy is not valid in the decentralized scheme due to the random content placement .",
    "in this paper , we have studied the problem of decentralized caching in fog radio access networks . for two edge - nodes and arbitrary number of users",
    ", a coded delivery scheme has been developed to minimize the latency for delivering user demands .",
    "we have shown that even with the decentralized placement , the proposed delivery scheme achieves a significant performance as compared with the derived lower bound .",
    "in subsection  [ coded ] , we proposed two delivery schemes that have the same first fourth stages of transmission and differ only at the last stage . in order to obtain the fronthaul - ndt and edge - ndt presented in   for each scheme",
    ", we sum the over all fronthaul - ndt and edge - ndt of the five stages : @xmath203 the same analysis holds for the second delivery scheme @xmath204 . for serial transmission ,",
    "the ndt of the system is the sum of the fronthaul - ndt and edge - ndt ( see  ) , so that ens choose the delivery scheme that minimizes the summation of the fronthaul - ndt and edge - ndt .",
    "it is obvious that the first scheme @xmath205 has lower ndt than the second scheme @xmath204 as @xmath206 .",
    "thus , the ndt for serial transmission is given by  .",
    "for pipelined transmission , the ndt of the system is the maximum between fronthaul - ndt and edge - ndt ( see  ) .",
    "therefore , we first obtain the performance of each delivery scheme by taking the maximum between fronthaul - ndt and edge - ndt , then , we choose the delivery scheme that has the minimum ndt between them as in  .",
    "the lower bound is similar to the one presented in proposition @xmath23 in  @xcite which is based on the cut - set argument . consider the user demands @xmath37 , where each user @xmath12 $ ] requests a different file @xmath30 . to obtain the constraint  [ cons1 ] ,",
    "let us consider @xmath207 users . from the the received signals of @xmath208 users which are functions of the transmitted messages @xmath209}$ ] , the cache contents of @xmath210 ens , and the fronthaul messages received by the @xmath210 ens , we can reconstruct the transmitted messages @xmath209}$ ] at high snr regime by solving @xmath4 equations in @xmath4 unknowns .",
    "therefore , we can argue that the @xmath0 user demands can be decodable by observing the received signals of @xmath208 users @xmath211}$ ] , the cache contents of @xmath210 ens @xmath212}$ ] , the fronthaul messages of @xmath210 ens @xmath213}$ ] , and the cache contents of @xmath0 users @xmath214}$ ] .",
    "l  [ lower1 ] k_rf = h(w_)h(w_|w _ ) + i(w_;_,z_,v_,_|w _ ) + + h(w_|w_,_,z_,v _ , _ ) + i(w_;_|w_)+ + i(w_;z_,v_,_|w_,_)+_f f + i(_;_)+i(w_;z_|w_,_)+ + i(w_;v_,_|w_,_,z_)+_f f + st_e(p)+h(z_|w_,_)+ + i(w_;v_,_|w_,_,z_)+_f f + = st_e(p)+h(z_|w _ , _ ) + + i(w_;v_|w_,_,z_)+ + i(w_;_|w_,_,z_,v_)+_f f + = st_e(p)+h(z_|w _ , _ ) + + h(v_|w_,_,z _ ) + + h(_|w_,_,z_,v_)+_f f + st_e(p)+h(z_|w_,_)+ + ( k_t - s)(k_r - s)_tf + h(_)+_f f + st_e(p)+h(z_|w_,_)+ + ( k_t - s)(k_r - s)_tf + ( k_t - s)t_fr(p)+_f f    where @xmath205 follows from the fact that the files are independent .",
    "step @xmath204 follows from the chain rule .",
    "step @xmath215 follows from fano s inequality , where    @xmath216}|w_{\\left[k_r+1:n\\right]},\\mathbf{y}_{\\left[1:s\\right]},\\mathbf{u}_{\\left[1:k_t - s\\right]},v_{\\left[1:k_t - s\\right]},z_{\\left[1:k_r\\right]}\\right)\\leq \\varepsilon_f f\\ ] ] since the users are able to decode their requested files from their cache contents and the transmitted messages of ens .",
    "@xmath217 is a function of @xmath3 that vanishes as @xmath218 .",
    "step @xmath219 follows from the data processing inequality , where @xmath209}$ ] are functions of @xmath220}$ ] , and the fact that the transmitted and received signals are independent on the unrequested files .",
    "step @xmath221 follows from the capacity bound of @xmath222 broadcast channel over @xmath40 time slots in the high snr regime , where @xmath223 , in addition to @xmath224,\\mathbf{y}_{\\left[1:s\\right]}}\\mid w_{\\left[1:n\\right]}\\right)=0 $ ] .",
    "step @xmath225 follows from the fact that conditioning reduces entropy , and the fact that caches of @xmath210 ens have @xmath226 bits of @xmath227 files .",
    "step @xmath228 follows from the capacity bound on @xmath210 fronthaul links over @xmath33 times slots",
    ". the second term in  [ lower1 ] can be upper bounded as follows @xmath229}|w_{\\left[k_r+1:n\\right]},\\mathbf{y}_{\\left[1:s\\right]}\\right)\\stackrel{\\left(a\\right)}{=}h\\left(z_{\\left[1:s\\right]}|w_{\\left[k_r+1:n\\right]},\\mathbf{y}_{\\left[1:s\\right]}\\right)\\\\ & + h\\left(z_{\\left[s+1:k_r\\right]}|w_{\\left[k_r+1:n\\right]},\\mathbf{y}_{\\left[1:s\\right]},z_{\\left[1:s\\right]}\\right)\\\\ & \\stackrel{\\left(b\\right)}{\\leq } h\\left(z_{\\left[1:s\\right]}|w_{\\left[k_r+1:n\\right]}\\right)+h\\left(z_{\\left[s+1:k_r\\right]}|w_{\\left[1:s\\right]\\cup\\left[k_r+1:n\\right]}\\right)\\\\ & \\stackrel{\\left(c\\right)}{=}sk_r\\mu_rf+\\left(k_r - s\\right)^2\\mu_rf \\end{aligned}\\ ] ] where @xmath205 follows from the chain rule .",
    "step @xmath204 follows from that conditioning reduces entropy , and @xmath208 users are able to decode files @xmath230}$ ] from their cache contents and the received signals .",
    "step @xmath215 follows from that caches of @xmath208 users have @xmath231 bits of @xmath0 files , and caches of @xmath227 users have @xmath232 bits of @xmath227 files . substituting from  [ lower2 ] into  [ lower1 ] , we get @xmath233f+\\varepsilon_f f \\end{aligned}\\ ] ] by taking the limit @xmath234 and the limit @xmath235",
    ", we reach to the inequality  [ cons1 ] as @xmath236 \\end{aligned}\\ ] ]    to obtain the constraint @xmath237 , consider the cut - set at the first user @xmath238}\\right)\\\\ & \\stackrel{\\left(a\\right)}{=}i\\left(w_1;y_1,z_1|w_{\\left[2:n\\right]}\\right)+h\\left(w_1|w_{\\left[2:n\\right]},y_1,z_1\\right)\\\\ & \\stackrel{\\left(b\\right)}{\\leq}i\\left(w_1;y_1|w_{\\left[2:n\\right]}\\right)+i\\left(w_1;z_1|w_{\\left[2:n\\right]},y_1\\right)+\\varepsilon_f f\\\\ & \\stackrel{\\left(c\\right)}{\\leq } t_e \\log\\left(p\\right)+h\\left(z_1|w_{\\left[2:n\\right]},y_1\\right)+\\varepsilon_f f\\\\ & \\stackrel{\\left(d\\right)}{\\leq } t_e \\log\\left(p\\right)+\\mu_r+\\varepsilon_f f\\\\ \\end{aligned}\\ ] ] where @xmath205 follows from the chain rule .",
    "step @xmath204 follows from the fano s inequality .",
    "step @xmath215 follows from the capacity bound of the broadcast channel .",
    "step @xmath219 follows from the fact that the cache of each user has @xmath75 bits of each file . by taking limits @xmath234 , @xmath239 ,",
    "we get @xmath237 .",
    "this complete the proof of lower bound on the ndt for serial transmission .",
    "note that fronthaul messages @xmath240}$ ] , ens messages @xmath209}$ ] , and received messages @xmath211}$ ] can not exceed @xmath241 time slots for pipelined transmission , where @xmath241 denotes end - to - end latency in case of pipelined transmission .",
    "thus , the lower bound on the ndt for pipelined transmission can be obtained in a similar proof for serial transmission by substituting @xmath242 in  . by taking limits @xmath234 , @xmath239 and combing with the fact that @xmath243",
    ", we obtain the lower bound presented in  .",
    "for pipelined transmission , we have three inequalities on the minimum ndt @xmath244 from theorem  [ th2 ] by substituting @xmath89 and @xmath187 in  , as follows @xmath245 we consider two main regimes of fronthaul gain @xmath59 :      \\2 ) for @xmath202 and @xmath194 : using inequality  , we have @xmath246 for @xmath247 and @xmath194 .",
    "similarly , we have @xmath248 for @xmath249 and @xmath194 . combining with the fact that @xmath250 , this proves the optimality of the decentralized scheme for pipelined transmission when @xmath165 $ ] , @xmath193 and @xmath194 , @xmath195 .    from theorem  [ th2 ] by substituting with @xmath187 and @xmath89 in  , we have @xmath251 inequality constraints for serial transmission as follows @xmath252 we consider three different regimes of fronthaul gain @xmath59 :        \\3 ) for @xmath200 , @xmath265 $ ] : from the inequalities  [ inq1 ] ,  [ inq2 ] , we have @xmath266 using   and the lower bound  , we obtain @xmath267+\\mu_t\\left(1-\\mu_t\\right)}{\\frac{k_r}{2r}\\left(1 - 2\\mu_t\\right)+\\frac{k_r}{2}+\\mu_t}\\\\ & = 1+\\frac{\\mu_t^2\\frac{k_r}{2r}-\\mu_t^2}{\\frac{k_r}{2r}\\left(1 - 2\\mu_t\\right)+\\frac{k_r}{2}+\\mu_t}\\leq 2 \\end{aligned}\\ ] ] where @xmath268 for @xmath269 $ ] .",
    "this completes the proof of theorem  [ th3 ] .",
    "o.  simeone , o.  somekh , h.  v. poor , and s.  shamai , `` downlink multicell processing with limited - backhaul capacity , '' _ eurasip journal on advances in signal processing _ , vol .",
    "2009 , no .  1 , pp . 110 , 2009 ."
  ],
  "abstract_text": [
    "<S> this paper studies the decentralized coded caching for a fog radio access network ( f - ran ) , whereby two edge - nodes ( ens ) connected to a cloud server via fronthaul links with limited capacity are serving the requests of @xmath0 users . </S>",
    "<S> we consider all ens and users are equipped with caches . </S>",
    "<S> a decentralized content placement is proposed to independently store contents at each network node during the off - peak hours . </S>",
    "<S> after that , we design a coded delivery scheme in order to deliver the user demands during the peak - hours under the objective of minimizing the _ normalized delivery time _ ( ndt ) , which refers to the worst case delivery latency . </S>",
    "<S> an information - theoretic lower bound on the minimum ndt is derived for arbitrary number of ens and users . </S>",
    "<S> we evaluate numerically the performance of the decentralized scheme . </S>",
    "<S> additionally , we prove the approximate optimality of the decentralized scheme for a special case when the caches are only available at the ens . </S>"
  ]
}