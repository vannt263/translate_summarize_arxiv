{
  "article_text": [
    "diffusion as a physical phenomenon has been an essential part of the history and development of magnetic resonance imaging . @xcite observed the effect of diffusion to spin - echoes , @xcite studied the effects of diffusion on free precession , and @xcite modified the bloch equations to include diffusion term with spatially varying magnetic field .",
    "@xcite , in their seminal paper , introduced the pulsed gradient spin echo sequence and showed the potential of diffusion related signal attenuation to probe the motion of molecules and to define the diffusion coefficient . in 1973",
    "p. lauterbur ( who shared the nobel prize with sir peter mansfield in 2003 ) made history publishing his groundbreaking paper entitled `` image formation by induced local interactions : examples employing nuclear magnetic resonance '' in his experiment lauterbur superimposed a magnetic field gradient on the static uniform magnetic field . because of the larmor principle , different parts of the sample would have different resonance frequencies and so a given resonance frequency could be associated with a given position .",
    "he also pointed out that it is possible to measure molecular diffusion from the decay of the mr - signal .",
    "diffusion weighted magnetic resonance imaging was introduced by @xcite measuring the displacement of protons .",
    "@xcite observed that diffusion in the white matter was anisotropic .",
    "in anisotropic media the mobility of the molecules is orientation dependent and can not be represented by one single diffusion coefficient .",
    "the three dimensional process of diffusion modeled by diffusion tensors was introduced by @xcite . without going into the physics of dmri , we sketch the idea from the statistical point of view . after applying two consecutive and opposite gradient pulses with amplitude @xmath1 in the direction @xmath2 ,",
    "denotes the unit sphere . ] with time delay @xmath3 , mr produces at every spatial location @xmath4 a signal @xmath5 where @xmath6 is the concentration of water molecules at @xmath4 , and @xmath7 is the 3-dimensional pulse gradient , @xmath8 . in eq .",
    "( [ eq : signal1 ] ) appears the characteristic function of a centered gaussian random vector @xmath9 with covariance matrix @xmath10 and @xmath11 .",
    "] , which is interpreted as the displacement of a water molecule with initial position @xmath4 in the time interval @xmath12 $ ] between the two pulses .",
    "the symmetric and positive definite matrix - valued field @xmath13 describes the geometry of the media and it is the object of interest .",
    "note that for an eigenvector @xmath7 with eigenvalue @xmath14 satisfying @xmath15 , the mr signal @xmath16 is highest when @xmath7 belongs to the eigenspace of the smallest eigenvalue of @xmath17 , and lowest in the principal direction . in neuroimaging , we measure restricted diffusion within neuron cells , and the principal diffusion eigenvector corresponds to the direction of a nervous fiber .    it is well known that the noise in an mr measurement has a rice distribution instead of gaussian @xcite .",
    "several authors ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) , add the noise - induced bias into the measurement so that a simple gaussian noise model can be fitted to the data . but none of them can easily gain the potential important information ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) from the high - frequency data , because in the high @xmath0-value range the corrected data does not fit the gaussian distribution . also the rice noise model is used ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , but in all cases the methods dealing with rice noise are computationally intensive .",
    "our work also deals directly with the rice noise distribution . by using data augmentation ,",
    "we reduce the non - standard regression problem to a standard poisson regression .",
    "this novel strategy can obtain diffusion information also from high amplitudes in the low snr regime , including the zero measurements which fall below the detection threshold .",
    "bayesian regularization is introduced in order to reduce the noise and obtain estimates also when the data is locally corrupted and contains artefacts .",
    "in addition , our method applies directly to high - order tensor models and spherical harmonics expansions of the diffusivity function see @xcite , @xcite , @xcite , which can capture more complex brain structures as fiber crossings and branchings . in order to regularize the 4th order tensor field we use a recent result by @xcite on invariants of 4th order tensors to derive the general form of an isotropic gaussian distribution for the tensor coefficients .",
    "this generalizes the probabilistic models proposed in the literature ( see * ? ? ?",
    "* ; * ? ? ?",
    "the paper is structured as follows : the nonlinear regression problem with rician noise model is described in section [ rice : section ] .",
    "the main contribution of the paper , data - augmentation by poissonization is introduced in section [ data : augmentation : section ] . in section [ bayes : modeling ] , after a general discussion on mcmc methods , we construct the bayesian hierarchical model for a single tensor ( section [ sub : hierarchical ] ) , and the gibbs - metropolis algorithm for sampling posterior distribution ( section [ mcmc : update ] ) . in sections [ dependence_prior ] and [ regularization ] , we continue with the isotropic gaussian markov field prior the gibbs - metropolis updates for the tensor field together with the bayesian estimation of the regularization parameters . in sections [ sub:4thorder_model],[sub :",
    "update : regularization_4 ] , [ sh : subsection ] we extend the method to higher order tensor models and explain the correspondences between tensors and the spherical harmonic expansion of the diffusivity .",
    "the implementation of these methods is illustrated in section [ results ] with an analysis of human brain data .",
    "we follow the discussion in @xcite .",
    "let us fix a position @xmath4 and omit the indexing .",
    "the signal is expressed conveniently as @xmath18 with parameter @xmath19 and the design matrix @xmath20 has rows @xmath21 in the mr experiment the signal is corrupted by rice noise .",
    "we measure @xmath22 where @xmath23 are independent with gaussian distribution @xmath24 , and @xmath25 is a complex gaussian noise .    from the statistical point of view , the estimation of @xmath26 from diffusion - mr data is a non - linear regression problem with the positivity constraint @xmath27 for all @xmath28 .",
    "it follows that the rice likelihood function is given by @xmath29 where @xmath30 is the modified bessel function of first kind .",
    "diffusion - mr data @xmath31 is collected for a series of pulses @xmath32 .",
    "direct maximum likelihood estimation of the parameters @xmath33 from the sampling density of eq .",
    "@xmath34 is problematic , involving the numerical evaluation of modified bessel functions .",
    "a simplified popular approach is to approximate the rice likelihood of eq .",
    "@xmath34 by a log - normal model for @xmath35 , where @xmath36 is gaussian with mean @xmath37 and variance @xmath38 .",
    "the model parameters are then estimated by using iterated weighted least squares ( wls ) ( see * ? ? ?",
    "* ; * ? ? ?",
    "however this approximation works well only within a certain narrow range of amplitudes . in clinical studies and research papers ,",
    "most often the maximal @xmath0-value is in the range of @xmath39 @xcite,@xcite . within this range",
    "the log - normal approximation to the rice noise is adequate .",
    "however , for large @xmath0-values , the snr is low , the data does not fit the log - normal approximation , and the wls - algorithm may fail to converge .",
    "reports ( e.g. * ? ? ?",
    "* ) address more than half underestimation of the true noise based the gaussian model .",
    "moreover , since the data is digitalized , at high @xmath0-values one may get measurements @xmath40 which are coded as zeros . in order to use the log - normal approximation ,",
    "these zero values have to be discarded , inducing sampling bias .",
    "when the estimation concerns only of 2nd - order tensors , under the assumption of gaussian diffusion , it is enough to use low @xmath0-value measurements .",
    "however , to estimate higher order characteristics and finer details of the diffusion distribution using higher order tensor models , expansions of spherical functions or mixture models , and ideally , to invert the characteristic function in eq .",
    "( [ eq : signal1 ] ) in the non - gaussian case , the high @xmath0-value measurements are also needed .      from a statistician s point of view , a non - linear regression problem is most conveniently framed in the context of _ generalized linear models _ ( glm ) , where the measurements have probability density of the form @xmath41 see @xcite .",
    "the function @xmath42 in eq .",
    "( [ eq : glm ] ) specifies an exponential family of distributions for the response @xmath35 , and @xmath43 is determined implicitly by the relation @xmath44 , where @xmath45 is the expectation and @xmath46 is the _ link _ function .",
    "unfortunately , this assumption is not satisfied by the rice likelihood in eq .",
    "( [ rice : density ] ) . in order to reduce the non - linear regression problem to the framework of generalized linear models , we propose a novel data augmentation strategy for parameter estimation under the exact rice likelihood . for each data point",
    "@xmath35 we introduce an unobservable variable @xmath47 which follows a generalized linear model with poisson response corresponding to @xmath48 , @xmath49 , and link function @xmath50 . in a bayesian framework",
    ", we then use markov chain monte carlo to integrate , conditionally on the observations @xmath35 , the variables @xmath51 and @xmath47 .",
    "[ representation : lemma]consider random variables @xmath52 , where @xmath47 is poisson distributed with mean @xmath53 , and given @xmath47 , @xmath54 has conditional distribution @xmath55 , that is @xmath56 then    1 .",
    "[ wikipedia ] @xmath57 has marginal density @xmath58 2 .",
    "the conditional distribution of @xmath47 given @xmath35 is @xmath59 in particular @xmath60 .",
    "[ wikipedia ] is well known . after a change of variable sum over @xmath61 by using the representation @xmath62 @xcite , where @xmath63 is a gaussian hypergeometric function .",
    "( [ n|y ] ) is a consequence of the bayes formula .    for @xmath64 ,",
    "consider two i.i.d .",
    "random variables @xmath65 with poisson(@xmath43 ) distribution , and define the probability distribution @xmath66 we call @xmath67 the _ reinforced poisson distribution _ with parameter @xmath43 .    in appendix ( [ reiforced : sampling ] ) we discuss random sampling from this distribution .    [ representation : coro ] in the settings of lemma [ representation : lemma ] , with @xmath68 ,    * the marginal distribution of @xmath35 has rice density of eq . @xmath34 . *",
    "the conditional distribution @xmath69 is a reinforced poisson distribution @xmath70 with parameter @xmath71",
    "the metropolis - hastings algorithm @xcite is a general method to explore a probability distribution in high - dimensional space .",
    "the idea is to construct a markov chain @xmath72 which is reversible with respect to the target probability @xmath73 , i.e. the transition probability @xmath74 satisfies the _ detailed balance condition _ @xmath75",
    "it follows that @xmath76 is the _ equilibrium distribution _ of the markov chain , meaning that the markov chain starting from the equilibrium distribution remains in equilibrium , i.e. @xmath77 .",
    "let @xmath78 be the target probability density for a configuration @xmath79 , where @xmath80 is a possibly unknown normalizing constant .",
    "starting from a configuration @xmath81 , sample a proposal value @xmath82 from a proposal density @xmath83 .",
    "with probability @xmath84 we accept the proposed value and set @xmath85 , otherwise the proposed move is rejected and we set @xmath86 .",
    "the ratio of densities in the right hand side of eq .",
    "( [ mhacceptance ] ) is referred as hastings ratio .",
    "it is straightforward to check that the resulting transition probability @xmath87 satisfies detailed balance and the markov chain @xmath72 is reversible with respect to the target distribution @xmath76 . in order to implement the algorithm ,",
    "it is enough to know the target density up to a proportionality constant .",
    "note that when we apply consecutively different metropolis - hastings transitions , the equilibrium distribution is preserved . under some irreducibility assumptions ,",
    "the markov chain covers the support of the target distribution ( see * ? ? ?",
    "* ) , and the ergodic theorem @xmath88 holds with probability 1 , for any initial state @xmath89 with @xmath90 .    how we choose the proposal distribution @xmath91 ?",
    "in fact we have almost complete freedom , the only requirement is the mutual absolute continuity of the 1-step forward and backward measures : @xmath92 in high dimension , to construct mcmc proposals with good mixing properties can be very challenging and it is an art by itself .",
    "a reference text is @xcite .",
    "a general idea is to update a subset of coordinates ( block ) , keeping the rest fixed ( gibbs - metropolis update ) .",
    "a gibbs update is a special case , where a subset of coordinates is updated by sampling a block from its conditional distribution given the remaining coordinates .",
    "a gibbs update is always accepted .    in bayesian inference , all the unknown parameters and variables of the problem",
    "are thought as random variables with a given prior probability distribution .",
    "then the target distribution of the metropolis - hastings algorithm is the posterior distribution of the unobserved variables conditionally on the observed ones .",
    "bayes formula gives @xmath93 where only the right hand side need to be specified and the normalizing constant may remain unknown .",
    "the 2nd - order tensor model in eq .",
    "( [ eq : signal1 ] ) describes the decay of the signal @xmath94 in each direction @xmath95 as @xmath96 increases . in order to have physical meaning",
    ", the diffusivity function @xmath97 should be non - negative , hence the matrix @xmath17 must have non - negative eigenvalues .    in general , there are two simple ways to include a constraint @xmath98 in a mcmc algorithm . in order to approximate the constrained expectation",
    "@xmath99 one has to choose :    * include the constraint into the target distribution obtaining a new target density proportional to @xmath100 . in practice",
    "this means starting from a state @xmath101 , and rejecting every proposed state which does not satisfy the constraint .",
    "the resulting markov chain takes values in the constraint set @xmath102 .",
    "* alternatively , include the constraint in the test function and sample from the unconstrained metropolis - algorithm . by the law of large numbers , with probability 1 @xmath103 this second method has the advantage of simplicity , it is not even required to start the markov chain from @xmath101 , and the unconstrained markov chain may have better mixing properties than the constrained one .",
    "the drawback is that the samples not satisfying the constraint are lost .",
    "we assign non - informative priors to the parameters of the likelihood function in corollary [ representation : coro ] :    * @xmath104 has a flat shift - invariant improper prior @xmath105 , * @xmath106 has scale invariant improper prior , with density @xmath107 .",
    "given the parameters @xmath108 , the random pairs @xmath109 are conditionally independent with conditional distribution    * @xmath110   \\sim \\mbox{poisson}\\biggl (    \\exp ( 2 \\theta \\cdot z_i ) / ( 2\\sigma^2 ) \\biggr)$ ] , * @xmath111 \\sim$ ] gamma@xmath112 , @xmath113 .",
    "we combine sequentially several block updates , where in turn a subset of paramaters is updated keeping the remaining ones fixed .",
    "when it is feasible , we sample the parameters from their full conditional distribution ( gibbs update ) . for the regression parameter @xmath26 , we construct a gaussian proposal distribution which approximates the full conditional .    *",
    "* updating @xmath106 : * the variance parameter is updated in a gibbs step . conditionally on the augmented data @xmath114 and the parameter @xmath26 , the conditional density of @xmath106 up to a multiplicative constant is given by @xmath115 which corresponds to the inverse gamma distribution , with shape and rate parameters @xmath116 + note that the noise variance @xmath106 appears in both augmented likelihood factors @xmath117 which makes the pair @xmath118 identifiable .",
    "* * updating @xmath47 : * [ n : update ] the auxiliary random variables @xmath119 are updated by sampling from the full conditional distribution . conditionally on @xmath120 and the measurements @xmath121 , the r.vs @xmath119 are conditionally independent with reinforced poisson distributions , with parameters @xmath122 respectively . in appendix",
    "a we discuss monte carlo sampling from the reinforced poisson distribution .",
    "+ the augmented data @xmath47 is generated `` on the fly '' from the full conditional distribution above when needed .",
    "it is not necessary to store @xmath47 into the computer memory . *",
    "* updating @xmath26 : * [ theta : update ] conditionally on @xmath123 and @xmath106 , the parameter @xmath26 is independent of the observations @xmath40 , the full conditional distribution being proportional to @xmath124 having assumed a flat prior @xmath125 , we choose a gibbs - metropolis update with gaussian proposal distribution @xmath126 where have employed the laplace approximation of eq .",
    "( [ theta : logconditional ] ) around the mode @xmath127 . here",
    "@xmath106 and @xmath47 are fixed and the precision matrix is the fisher information @xmath128 to find the mode @xmath127 , we use the iterative fisher scoring algorithm ( see * ? ? ?",
    "* ) , @xcitechapter 10 . the hastings ratio ( hr ) for @xmath129 sampled from the proposal distribution @xmath130",
    "is given by @xmath131 + computing the laplace approximation ( eq .",
    "[ gaussian : proposal ] ) to the full conditional density ( eq . [ theta : logconditional ] ) , is crucial in order to get high acceptance rates in the mcmc . without data augmentation ,",
    "the glm - likelihood in eq .",
    "( [ theta : logconditional ] ) should be replaced by a product of rice likelihoods .",
    "it is also possible to compute by fisher scoring the laplace approximation of the full conditional under such rice likelihood .",
    "however , for large sample size @xmath132 , it could be not computationally affordable to do that at every mcmc update of every single tensor .",
    "+ the algorithm is based on the assumption that the fisher scoring algorithm converges to same global maximum @xmath133 for all initial values @xmath26 .",
    "however , with a finite number of iterations , the approximate mode @xmath134 obtained by starting the fisher scoring algorithm from the proposal value @xmath135 will be slightly different than the approximate mode @xmath133 obtained starting with initial value @xmath26 . in order to correct for this discrepancy",
    "we have to run the fisher scoring algorithm a second time starting from the proposed value @xmath129 and reaching another approximate maximum @xmath134 . in this case",
    "we redefine the hastings ratio as @xmath136 + [ rmk : fixings_0 ] denote @xmath137 . by fixing @xmath138 to the current value",
    ", we can also update the tensor parameters @xmath139 conditionally on @xmath140 .",
    "this is useful in situations where data almost determine @xmath141 and the fisher information @xmath142 for @xmath143 is numerically close to be singular . in such cases",
    "fisher scoring algorithm is unstable and may fail to converge .",
    "we take @xmath144 as known , and use instead the fisher information for @xmath145 . *",
    "* separate update for @xmath144 : * we consider also updating @xmath146 and the tensor @xmath145 separately .",
    "we see that @xmath147 where @xmath148 since @xmath149 has improper flat prior , @xmath150 is the improper prior of @xmath151 .",
    "it follows that conditional on @xmath152 and @xmath153 , @xmath151 is gamma@xmath154-distributed .",
    "we sample @xmath155 from this gamma distribution and set @xmath156 .",
    "bayesian regularization is an image - denoising technique , introduced by @xcite , which has been already applied in dti studies @xcite .",
    "it is assumed that under the prior distribution that the spatial parameters of the model are not independent but form a correlated random field . this is a reasonable assumption in our context :",
    "even when a priori we do not have any information about the main tensor direction at a given voxel , we know that often tensors from neighbour voxels are similar , just because a nervous fiber possibly continues from one voxel to the next .",
    "the prior dependence is taken into account according to bayes formula and it has a smoothing and denoising effect on the posterior estimates .",
    "an alternative , is to estimate first the parameters independently at each voxel , and then interpolate the preliminary tensor estimators to obtain a smoothed estimator .",
    "the advantage of bayesian regularization is that estimation and regularization are performed in a single procedure , by using all the available information .",
    "consider a zero mean @xmath157 symmetric gaussian random matrix @xmath158 . in @xcite,@xcite",
    ", it is shown that the distribution of @xmath159 is isotropic if and only if it has density of the form @xmath160 with @xmath161 and @xmath162 . in section [ sh : subsection ] , we will see that ( [ isotropic_prior:2 ] ) follows from the isotropic gaussian random field characterization in terms of the law of its spherical harmonic coefficients .    for the vector @xmath163",
    ", this corresponds to a gaussian distribution with zero mean and precision matrix @xmath164 we construct an ( improper ) pairwise - difference gaussian prior for a markov random field of @xmath165 symmetric matrices @xmath166 where @xmath167 is the set of voxels , provided with the neighbourhood relation @xmath168 in the @xmath169 lattice .",
    "this bayesian approach is equivalent to least - squares tikhonov regularization in the framework of penalized maximum likelihood @xcite .",
    "define the improper prior density @xmath170 which is shift - invariant in @xmath171 and invariant under rotations in @xmath172 .",
    "the increments @xmath173 have a proper rotation invariant distribution , but the marginal prior of @xmath174 does not integrate to a probability distribution . for each voxel @xmath175 introduce the regression parameter vector @xmath176 for the log - intensity parameters @xmath177 we could either assume prior independence and assign a flat prior , or use a pairwise difference improper contextuality prior with density @xmath178 called instrinsic prior @xcite .",
    "the hyperparameters @xmath179 , @xmath180 , are tuning the correlations of the difference @xmath181 . as in section [ data : augmentation : section ] , for each voxel @xmath4 we introduce :    * a noise - parameter @xmath182 with scale - invariant improper prior @xmath183 , * a random vector @xmath184 which follows the generalized linear model of corollary [ representation : coro ] with poisson response distribution and logarithmic link function , covariate matrix @xmath185 and parameter @xmath186 .    here",
    "@xmath187 are independent and @xmath188 are conditionally independent given @xmath189 .    as before ,",
    "we compute the laplace approximation for the log - likelihood at each voxel @xmath4 . when we combine this gaussian log - likelihood approximation with the pairwise - difference gaussian prior by using bayes formula , we obtain an approximating gaussian posterior for @xmath186 , which we will use as proposal distribution in the gibbs - metropolis update",
    ". we may consider the single site update , where @xmath186 is updated voxelwise conditionally on @xmath190 and the values @xmath191 at neighbour voxels @xmath168 .",
    "alternatively we can construct a gaussian approximation to the full conditional as a joint proposal in a simultaneous update for a block @xmath192 , where @xmath193 is a connected subset of voxels .",
    "the size of a block can vary from a single site to the whole brain .",
    "for example we may define a block as a ball with given center and radius under the graph distance , which is the length of the shortest path between two voxels .",
    "we denote the exterior boundary of @xmath194 by @xmath195 and set @xmath196 , @xmath197 denotes the neighbourhood of @xmath4 , and @xmath198 stands for its cardinality .",
    "we update the variable @xmath199 conditional on the observations @xmath200 and @xmath201 .",
    "the prior of @xmath202 is gaussian and the likelihood of @xmath191 with respect to the augmented data @xmath203 is approximated by the gaussian density @xmath204 , where @xmath205 and @xmath206 are functions of @xmath207 and the design matrix @xmath20 , computed by using fisher scoring under the poisson glm as in section [ theta : update ] .",
    "the corresponding gaussian posterior distribution @xmath208 will be used as proposal in the metropolis block update , and satisfies @xmath209 where the constant term does not depend on @xmath210 and may change from line to line , @xmath211 is a @xmath212 precision matrix , and after completing the squares we have defined @xmath213 is a band diagonal precision matrix with @xmath214 blocks and @xmath215 .",
    "this corresponds to a gaussian proposal distribution @xmath216 with mean @xmath217 and covariance @xmath218 .",
    "[ [ prior - contribution ] ] prior contribution + + + + + + + + + + + + + + + + + +    the prior contribution is derived as the proposal contribution by conditioning on the values @xmath219 without including data .",
    "we obtain @xmath220 these expressions determine the hastings ratio for this gibbs - metropolis update ( here omitted ) .",
    "the precision matrix of the gaussian random field @xmath221 is the kronecker product @xmath222 , where @xmath223 is the adjacency matrix of the graph @xmath167 , and @xmath224 was given in ( [ omega_d : matrix:2nd ] ) . since @xmath225 the likelihood for @xmath226 based on @xmath227 is proportional to @xmath228 with constraints @xmath161 and @xmath229 .    in order to factorize the likelihood we reparametrize with @xmath230 , obtaining @xmath231 assuming scale invariant independent priors for @xmath232 , @xmath233 we obtain the full conditional distribution of @xmath234 as the product of two gamma densities , @xmath235 in the mcmc",
    ", we update the regularization parameters by sampling @xmath236 independently from these full conditional distribution and setting @xmath237 .",
    "several authors , ( @xcite ) , argue that the 2nd - order tensors fail to capture complex tissue structures such as fibers crossing and branching in a single voxel .",
    "in such voxels most often anisotropy is underestimated and fiber tracking algortihms based on 2nd - order tensors estimates terminate .",
    "in fact , while at every spatial location we have a diffusion matrix , in the time scales we are considering , the scale of water diffusion is of smaller order than the size of a voxel .",
    "the 2nd - order tensor model assumes that the diffusion tensor is constant at all points inside one voxel . in reality",
    "a voxel contains a whole population of cellular structures , corresponding to a population of diffusion tensors .",
    "equation @xmath238 should be replaced by @xmath239 which is the characteristic function of the random displacement @xmath9 of a water molecule randomly selected within the voxel . here",
    "@xmath240 is a probability distribution on the space @xmath241 of positive definite matrices for the population of diffusion tensors .",
    "instead of measuring the characteristic function of centered gaussian random vector , the mr - experiment measures the characteristic function of a gaussian mixture .",
    "we see from ( [ mixed : gaussian ] ) that the signal @xmath242 is a decreasing function of @xmath1 . in @xmath243-th",
    "order tensor modeling it is assumed that the signals are given by @xmath244 where @xmath8 is the @xmath0-value , @xmath245 is the gradient direction , and the _ diffusivity function _",
    "@xmath246 is an homogenous polynomial of degree @xmath243 . here",
    "the 4-th order tensor @xmath247 is totally symmetric . in ( [ diffusivity:4 ] )",
    "we have introduced the parameter @xmath248 as @xmath249 and the design matrix @xmath250 with rows@xmath251 because the diffusivity function models signal decay , the @xmath243-th order tensor must satisfy the positivity constraint @xmath252    when we analyze the data at each voxel separately , under the high order tensor diffusivity model , only the dimensions of the parameter @xmath26 and the design matrix @xmath20 are changed , and the data augmentation of section [ data : augmentation : section ] and the bayesian procedures of section [ bayes : modeling ] apply directly .    in what follows , in order to perform bayesian regularization of the tensor field , we first give the general form of an isotropic gaussian distribution for the @xmath243-th order tensor , in analogy with ( [ isotropic_prior:2 ] ) .",
    "then , by taking pairwise differences , we obtain an isotropic gaussian random field of @xmath243-th order tensors which replaces the prior ( [ prior:2 ] ) in the bayesian regularization method of section [ dependence_prior ] .",
    "+ in @xcite , the 4th - order tensor in dimension @xmath253 is shown to be isomorphic to a 2nd - order tensor in dimension @xmath254 under the isomorphism @xmath255 the six eigenvalues and eigentensors of the @xmath243-th order tensor @xmath159 , correspond to the eigenvalues and eigenvectors of the matrix @xmath256 .",
    "furthermore , it is shown in @xcite , that @xmath257 and the polynomial @xmath258 are invariant under 3d - rotations and span the space of isotropic homogeneous polynomials of degree 2 in the variables @xmath159 . here",
    "we give the general form of a zero - mean isotropic gaussian distribution for the 4th - order tensor , with density @xmath259 again ( [ rotation : invariant4th ] ) follows from the characterization of isotropic gaussian random fields in terms of the law of their spherical harmonic coefficients , which we discuss in section [ sh : subsection ] .    under ( [ rotation : invariant4th ] ) ,",
    "the random coefficients @xmath260 have precision matrix @xmath261 and are independent from @xmath262 , which have precision matrix @xmath263 the covariance matrix of @xmath159 is positive definite under the constraints @xmath264 the construction and block - updates described in section [ dependence_prior ] extends directly to a @xmath243-th order tensor valued random field @xmath265 , with the improper rotation - invariant pairwise - difference gaussian prior @xmath266 inside the exponential , appears a generalization of the regularization term used in @xcite . in order to proceed as in section [ dependence_prior ]",
    ", we just need to replace the precision matrix in ( [ omega : matrix ] ) by the @xmath267 block - diagonal matrix @xmath268    [ [ positivity - constraint - for-4th - order - tensors . ] ] positivity constraint for 4th order tensors .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    it follows that the diffusivity function @xmath269 in ( [ diffusivity ] ) is positive when the @xmath270 matrix @xmath256 in ( [ dhat ] ) has positive eigenvalues .",
    "this is a sufficient but not a necessary condition , because it is enough to have positivity on the algebraic variety @xmath271 when @xmath256 is negative definite , we should check the sign of the @xmath20-eigenvalue of the diffusivity , which was introduced by @xcite as the solution of the constrained optimization problem @xmath272      the likelihood for @xmath273 based on @xmath227 is proportional to @xmath274 where the polynomial @xmath275 was given in ( [ g : polynomial ] ) . in order to factorize the likelihood we reparametrize with @xmath276 with @xmath277 .",
    "the linear system has solution @xmath278 and the corresponding likelihood is proportional to @xmath279 we assume scale invariant priors for @xmath280 , @xmath281 and obtain the full conditional distribution of @xmath282 as the product of these gamma densities : @xmath283 in the mcmc , @xmath282 are updated independently by sampling from these full conditionals .",
    "the corresponding @xmath284 are then obtained from equation ( [ linear solution:4th order ] ) .",
    "in general , the diffusivity function @xmath285 can be expanded as @xmath286 where @xmath287 and the _ real spherical harmonics _",
    "@xmath288 are homogeneous polynomials of respective degrees @xmath289 forming an orthonormal basis of @xmath290 equipped with the haar measure @xmath291 ( see @xcite , paragraph 3.4 ) . because of the symmetry @xmath292 @xmath293 , only the spherical harmonics of even degree contribute to ( [ diffu : expansion ] ) . by truncating the expansion ( [ diffu : expansion ] ) up to polynomials of degree @xmath294",
    ", we obtain a finite dimensional parametrization , which corresponds to the tensor model of order @xmath294 @xmath295 where the tensor @xmath159 is totally symmetric and the coefficients @xmath296 have multiplicities @xmath297 by comparing the representations ( [ tensor : representation ] ) and ( [ diffu : expansion ] ) as in @xcite , it follows that the coefficients of the tensor of order @xmath294 and the spherical harmonic coefficients of degrees @xmath298 are related by a linear bijection @xmath299 . for the 2nd - order tensor model this holds for @xmath300 and for the 4th order tensor model it holds with @xmath301    @xmath302     + next",
    "we discuss the prior distribution for the spherical harmonic coefficients . when these are independent gaussian random variables with @xmath303 it follows from theorem 6.11 in ( @xcite ) that @xmath304 is an isotropic , centered and symmetric gaussian random field .",
    "moreover all the random fields in this class are obtained in such a way , and are characterized by their _ angular power spectrum _ @xmath305 .",
    "consequently the tensor coefficients @xmath306 are also centered gaussian random variables with covariance @xmath307 where the diagonal matrix @xmath308 is the covariance of the spherical harmonic coefficients @xmath309 after inverting the covariance and comparing with the precision matrices @xmath310 in ( [ omega_d : matrix:2nd ] ) and ( [ omega_d : matrix:4th ] ) , we find the following linear correspondances between precision parameters : for the 2nd - order tensor model @xmath311 and for the 4th - order tensor model @xmath312 when the diffusivity function is assigned voxelwise as @xmath313 with common truncation level @xmath61 , we define the ( improper ) regularization prior for the random field by assigning a gaussian prior to the coefficients pairwise differences as follows @xmath314 the bayesian computations of sections [ mcmc : update],[dependence_prior ] , apply directly with parameter @xmath315 design matrix @xmath316 with rows @xmath317 and diagonal precision matrix @xmath318 with diagonal entries @xmath319    assuming an improper and scale invariant prior for the angular power spectrum , given as @xmath320 we obtain the full conditional distribution for the precision coefficients as @xmath321 in the mcmc the angular power spectrum is then updated by sampling independently from these full conditionals and taking the inverse .",
    "in the follow - up , we illustrate the performance of our method with a real data example .",
    "the data consists of @xmath322 diffusion mr - images of the brain of an healthy human volunteer , taken from four @xmath323-thick consecutive axial slices , and measured with a philips achieva @xmath324 tesla mr - scanner .",
    "the image resolution is @xmath325 pixels with size @xmath326 @xmath327 . after masking out the skull and the ventricles , we remain with a region of interest ( roi ) @xmath167 containing @xmath328 voxels . in the protocol we used all the combinations of the @xmath329 gradient directions listed in table [ gradient : directions ] , with the @xmath0-values in table [ bvalues ] , varying in the range @xmath330 , with @xmath331 repetitions , for a total of @xmath332 data points .",
    "the data is analyzed under 2nd and 4th - order tensor models , with and without bayesian regularization , estimating the regularization parameters in the first case . in the markov chain monte carlo",
    "we do not impose positivity constraints on the tensors as we discussed in section [ sub : posi ] , since we want to count the voxels where the posterior expectation of the tensor is non - positive . to begin with , we compute independently at each voxel @xmath4 a preliminary estimator for the tensor and noise parameters @xmath333 , obtaining the initial state of the gibbs - metropolis markov chain .",
    "this is done under the log - gaussian approximation discussed in section [ rice : section ] , by the method of weighted least - squares , and using only observations in the low @xmath0-value range @xmath334 . for the regularized model , at each mcmc - cycle we divide @xmath167 into blocks , where each block is the intersection of @xmath167 with a ball of radius @xmath335 under the graph distance , and can contain up to 342 voxels .",
    "since blocks are separated by at least one voxel , the parameters from different blocks are conditionally independent given the exterior boundary values , and it is possible to update the blocks in parallel .",
    "the centers of the blocks are then cyclically shifted at each mcmc cycle , and at the end of each cycle we also update the regularization parameters .",
    "the markov chain was running for @xmath336 and @xmath337 cycles respectively , under 2nd and 4th - order tensor models , which took @xmath338 and @xmath339 cpu hours on a 15-core intel xeon e5 - 2670 processor .      before computing empirical averages",
    ", we waited for the markov chain to reach stationarity . the burnin - period ( @xmath340 and @xmath341 cycles under the 2nd and 4th - order tensor models , respectively )",
    "was selected by monitoring the logposterior and the regularization parameters of the samples shown in fig .",
    "[ fig : cov ] , which deserves an explanation .",
    "we see that the rice - loglikelihood increases first very rapidly , and then decreases before stabililizing .",
    "such phenomena is not uncommon in high dimensional models , when a maximum likelihood estimator is used to construct the initial configuration ( see for example fig . 3 in @xcite ) .",
    "to see this effect in a toy model , just consider a gaussian vector @xmath342 with i.i.d .",
    "coordinates @xmath343 , which satisfies @xmath344 in high dimension , under the posterior distribution the typical configuration and the maximum a posteriori ( map ) configuration can be very different , with a set of typical configurations containing most of the probability mass , while the probability mass concentrated around the map - configuration is negligible . since we start the markov chain from the maximum likelihood estimator under the approximating log - normal model , at the beginning the orientation of all tensors ( but not their eigenvalues )",
    "are close to optimal also under the exact rice likelihood model .",
    "then the tensor eigenvalues and noise parameters move rapidily towards configurations with highest posterior probability . after this phase",
    ", it takes a while for the tensor orientations to mix - up . since the acceptance probabilities are not uniform betwteen blocks , and we are estimating simultaneously the regularization parameters , the total logposterior density shows a slow decay before reaching stationarity .    for comparison ,",
    "we plot in fig .",
    "[ fig : indep ] the mcmc trace of the rician loglikelihood for a single voxel under 2nd and 4th tensor models , without bayesian regularization , which converges rapidly to stationarity .            in fig .",
    "[ fig : cov ] , we can see that all the chains got convergence after certain periods . in 2nd order dte",
    ", the logarithmic rician posterior is getting to stabilize roughly after 15,000 iterations ( iters . ) . while , the converging situation of the smoothing parameters @xmath347 and @xmath348 happened only after 5,000 iters . in 4th order scenario",
    ", all the chains are getting to convergence after 10,000 iters .    in order to get better estimation of the tensor parameters , based on mcmc a recognized burnin period should be ignore .",
    "for 2nd order case , we chose the first 15,600 iters .",
    "as the burnin , then compute the average values of tensor parameters from the last 9,400 iters . as the tensor estimates .",
    "the estimates of smoothing parameters ( @xmath347 and @xmath348 ) can be calculate after e.g. 5,000 iter .",
    ", since they converged much faster than the others . for 4th order case , all the parameters of interest go to stabilize after roughly 10,000 as the burnin .",
    "so we compute the empirical means after the first 10,450 iters ..    in the histories of rice densities , we got some strange curves ( also see * ? ? ?",
    "* ) at the beginning where the traces go up rapidly to a maximum , then go down a little bit to stabilize .",
    "one main reason leads such situation is the effects of regularization .",
    "the other reason is that some voxels have very low acceptance rates which effect the monitors of whole voxels .",
    "fig.[fig : singletensor ] show the mcmc monitors of two random picked up but good ( not from artefacts ) voxels without regularization , where we run 10,000 and 6000 independent updated with 1000 burnin for 2nd and 4th order tensor model , respectively .      in fig .",
    "[ fig : accept_regularized ] we show the acceptance probabilities for the gibbs - metropolis block update of the tensor parameters , estimated for each voxel under the regularized 2nd and 4th order tensor models .",
    "note that , although we use large block updates with more than 300 voxels in each block , the acceptance probabilities are remarkably high in most of the voxels ( see the histograms ) .",
    "it means that in most cases the our gaussian approximation is very close to the exact full conditional distribution of the tensor parameters in a block .",
    "note also that in fig .",
    "[ fig : avacpt2 ] ( which corresponds to 2nd order tensor model ) there are some regions with lower acceptance probability . in such areas",
    "one should use update blocks of smaller size .",
    "these regions of lower acceptance probability are either artefacts , where the data are corrupted , or contain complex structures where the 2nd order tensor model does not fit well the data , and a higher order model would be more appropriate .",
    "we see two low acceptance probability regions situated symmetrically on the left and right sides of the ventricles .",
    "anatomically this corresponds to the corona radiata where fiber bundles from multiple directions are crossing . by comparing with fig .",
    "[ fig : avacpt4 ] we see that in these regions the acceptance probability improves under the ( regularized ) 4th order tensor model .",
    "for the diffusion model without regularization , the independent tensor updates have high acceptance probabilities at all voxels , under both 2nd and 4th - order tensor models ( in [ fig : hist_acc_indep ] ) .",
    "the deviance information criterion ( dic ) , introduced by @xcite , is a measure of model fitting used in bayesian model selection as an alternative to bayes factors . unlike bayes factors",
    ", dic is well defined also when improper priors are assumed , as it is the case in our settings .",
    "it is defined as @xmath349 where @xmath350 is the deviance , and we take conditional expectations with respect to the posterior distribution of the parameters @xmath26 . defined in analogy with the toy example of eq .",
    "( [ toy : example ] ) , the _ effective number of parameters _ @xmath351 appears as penalization term in the expression @xmath352 this allows for model comparisons , lower dic meaning a better fit to the data relatively to the effective number of parameters . in fig .",
    "[ dic : independent ] the dic is computed independently at each voxel under the 2nd and 4th - order tensor models ( without regularization ) .",
    "note that the voxels with the highest dic corresponds to artefacts where the data is corrupted , and the area of high dic correspond to complex white matter structures .",
    "we also calculated the overall dic for all voxel under the model 2nd and 4th - order tensor models with regularization .",
    "the respective values dic@xmath353 and dic@xmath354 , indicate that when we penalize the model by the effective number of parameters , overall the 2th - order tensor model fits our data better than the 4th - order model . in fig .",
    "[ fig : posterior_noise ] the posterior expectation of the noise parameters @xmath355 , are shown . when these are interpreted as residual variances in model fitting",
    ", we see that they are consistent with the dic .",
    "is a statistical criteria which is widely used in bayesian model selection , and particularly for those posteriors obtaining by markov chain monte carlo ( mcmc ) simulation .",
    "it is defined as @xmath356 where the deviance is @xmath357                we also calculated the dic of all voxels with regularization . for 2nd and 4th order tensor model with @xmath359",
    "we have @xmath360 and @xmath361 , respectively .",
    "therefore , the dic criteria shows that the 4th tensor order model is better than the 2nd one .",
    "[ fig:2ja4dtis1 ] shows the diffusivity profiles based on the posterior estimates of the tensors at all voxels in a region of interest . for each direction",
    "@xmath362 and spatial location @xmath363 , we plot the point @xmath364 , where @xmath365 is the posterior expectation of the diffusivity . in order to observe the differences between 2nd and 4th order tensor models , in fig .",
    "[ fig:2ja4rank ] we zoom into the roi ( a ) and ( b ) , and see that the 4th order tensor model captures the fiber - crossings which the 2nd order model can not capture . at the fiber - crossing locations , under the 2nd - order model the two largest eigenvalues of the estimated tensor have similar sizes , with a donut - shaped diffusivity profile .    1   estimated diffusivity profiles from a roi , under 2nd and 4th - order tensor model .",
    "the color - code represents the main direction of the principal eigenvalue of the 2nd - order tensor : red , left - right ; green , anterior - posterior ; blue , superior - inferior .",
    "these figures are drawn with the matlab package fandtasia written by barmpoutis @xcite .",
    ", title=\"fig : \" ]   estimated diffusivity profiles from a roi , under 2nd and 4th - order tensor model .",
    "the color - code represents the main direction of the principal eigenvalue of the 2nd - order tensor : red , left - right ; green , anterior - posterior ; blue , superior - inferior .",
    "these figures are drawn with the matlab package fandtasia written by barmpoutis @xcite .",
    ", title=\"fig : \" ]    1   estimated diffusivity profiles from a roi , under 2nd and 4th - order tensor model .",
    "the color - code represents the main direction of the principal eigenvalue of the 2nd - order tensor : red , left - right ; green , anterior - posterior ; blue , superior - inferior .",
    "these figures are drawn with the matlab package fandtasia written by barmpoutis @xcite .",
    ", title=\"fig : \" ]   estimated diffusivity profiles from a roi , under 2nd and 4th - order tensor model .",
    "the color - code represents the main direction of the principal eigenvalue of the 2nd - order tensor : red , left - right ; green , anterior - posterior ; blue , superior - inferior .",
    "these figures are drawn with the matlab package fandtasia written by barmpoutis @xcite .",
    ", title=\"fig : \" ]    furthermore , roi ( a ) shows crossing fibers between the corticospinal tract and superior longitudinal fibers , and roi ( b ) shows fiber crossing near the corpus callosum .",
    "it can be seen that the 4th order angular resolution of tensors provide more detailed information of the water diffusion , especially in those complicated fiber - tissue areas .",
    ".43 estimated diffusivity profiles under 2nd and 4th - order tensor models in roi ( a ) , showing crossing fibers between the corticospinal tract and superior longitudinal fibers , and roi ( b ) , showing fiber crossing near the corpus callosum , both selected from fig .",
    "[ fig:2ja4dtis1 ] , title=\"fig : \" ]    .45 estimated diffusivity profiles under 2nd and 4th - order tensor models in roi ( a ) , showing crossing fibers between the corticospinal tract and superior longitudinal fibers , and roi ( b ) , showing fiber crossing near the corpus callosum , both selected from fig .",
    "[ fig:2ja4dtis1 ] , title=\"fig : \" ]      in fig .",
    "[ fig:4rank ] we compare diffusivity profiles from a region of interest without and with regularization , under the 4th order tensor model . with regularization , the differences in shape and direction between neighbouring tensors get smoothed .",
    "this also implies noise reduction : the tensor information from data corrupted by artefacts is corrected by the information from the neighbours . for the 2nd - order tensor model ,",
    "the regularization effect in the same region was not that evident .",
    "since the regularization parameters are not fixed but estimated from the data , we can not always expect an increase from the smoothness level determined by the data . in order to achieve a prespecified level of smoothness we should either fix the regularization parameters or assign them a strongly informative prior .",
    "the posterior mean and standard deviation of the regularization parameters is given in table [ table : post ] ."
  ],
  "abstract_text": [
    "<S> mapping white matter tracts is an essential step towards understanding brain function . </S>",
    "<S> diffusion magnetic resonance imaging ( dmri ) is the only noninvasive technique which can detect in vivo anisotropies in the 3-dimensional diffusion of water molecules , which correspond to nervous fibers in the living brain . in this process </S>",
    "<S> , spectral data from the displacement distribution of water molecules is collected by a magnetic resonance scanner . from the statistical point of view </S>",
    "<S> , inverting the fourier transform from such sparse and noisy spectral measurements leads to a non - linear regression problem . </S>",
    "<S> diffusion tensor imaging ( dti ) is the simplest modeling approach postulating a gaussian displacement distribution at each volume element ( voxel ) . </S>",
    "<S> typically the inference is based on a linearized log - normal regression model that can fit the spectral data at low frequencies . </S>",
    "<S> however such approximation fails to fit the high frequency measurements which contain information about the details of the displacement distribution but have a low signal to noise ratio . in this paper , we directly work with the rice noise model and cover the full range of @xmath0-values . using data augmentation to represent the likelihood </S>",
    "<S> , we reduce the non - linear regression problem to the framework of generalized linear models . </S>",
    "<S> then we construct a bayesian hierarchical model in order to perform simultaneously estimation and regularization of the tensor field . finally the bayesian paradigm is implemented by using markov chain monte carlo . </S>",
    "<S> this work is motivated by the need of diagnostics for the lewy bodies disease .    * </S>",
    "<S> key words and phrases : * markov chain monte carlo , poissonization , tensor - valued gaussian random field , isotropy , generalized linear model , statistical inverse problem . </S>"
  ]
}