{
  "article_text": [
    "in recent years bayesian networks have been successfully applied in several different disciplines , including medicine , biology and epidemiology ( see for example @xcite and @xcite ) .",
    "this has been made possible by the rapid evolution of structure learning algorithms , from constraint - based ones ( such as pc @xcite , grow - shrink @xcite , iamb @xcite and its variants @xcite ) to score - based ( such as tabu search @xcite , greedy equivalent search @xcite and genetic algorithms @xcite ) and hybrid ones ( such as max - min hill climbing @xcite ) .",
    "the main goal in the development of these algorithms has been the reduction of the number of either independence tests or score comparisons needed to learn the structure of the bayesian network .",
    "their correctness has been proved assuming either very large sample sizes in relation to the number of variables ( in the case of greed equivalent search ) or the absence of both false positives and false negatives ( in the case of grow - shrink and iamb ) . in most cases",
    "the characteristics of the learned networks were studied using a small number of reference data sets @xcite as benchmarks , and differences from the true structure measured with purely descriptive measures such as hamming distance @xcite .",
    "this approach to model evaluation is not possible for real world data sets , as the true structure of their probability distribution is not known .",
    "an alternative is provided by the use of either parametric or nonparametric bootstrap @xcite . by applying a learning algorithm to a sufficiently large number of bootstrap samples it is possible to obtain the empirical probability of any feature of the resulting network @xcite , such as the structure of the markov blanket of a particular node .",
    "the fundamental limit in the interpretation of the results is that the `` reasonable '' level of confidence for thresholding depends on the data .    in this paper",
    "we propose a modified bootstrap - based approach for the inference on the structure of a bayesian network .",
    "the undirected graph underlying the network structure is modeled as a multivariate bernoulli random variable in which each component is associated with an arc .",
    "this assumption allows the derivation of both exact and asymptotic measures of the variability of the network structure or any of its parts .",
    "bayesian networks are graphical models where nodes represent random variables ( the two terms are used interchangeably in this article ) and arcs represent probabilistic dependencies between them @xcite .",
    "the graphical structure @xmath0 of a bayesian network is a _ directed acyclic graph _",
    "( dag ) which defines a factorization of the joint probability distribution of @xmath1 , often called the _ global probability distribution _ , into a set of _ local probability distributions _ , one for each variable .",
    "the form of the factorization is given by the _",
    "markov property _ of bayesian networks , which states that every random variable @xmath2 directly depends only on its parents @xmath3 .",
    "therefore it is important to define confidence and variability measures for specific features in the network structure , such as the presence of specific configurations of arcs . in particular",
    "a measure of variability for the network structure as a whole has many applications both as an indicator of goodness of fit for a particular bayesian network and as a criterion to evaluate the performance of a learning algorithm .",
    "confidence measures have been developed by @xcite using bootstrap simulation , and later modified by @xcite to estimate the marginal confidence in the presence of an arc ( called _ edge intensity _ , and also known as _ arc strength _ ) and its direction .",
    "this approach can be summarized as follows :    1 .   for @xmath4 1 .",
    "re - sample a new data set @xmath5 from the original data @xmath6 using either parametric or nonparametric bootstrap . 2 .",
    "learn a bayesian network @xmath7 from @xmath5 .",
    "2 .   estimate the confidence in each feature @xmath8 of interest as @xmath9",
    ".    however , the empirical probabilities @xmath10 are difficult to evaluate , because the distribution of @xmath11 in the space of dags is unknown and because the confidence threshold value depends on the data .",
    "let @xmath12 , @xmath13 be bernoulli random variables with marginal probability of success @xmath14 , that is @xmath15 , @xmath16 . then the distribution of the random vector @xmath17^t$ ] over the joint probability space of @xmath12 is a _ multivariate bernoulli random variable _",
    "@xcite , denoted as @xmath18 .",
    "its probability function is uniquely identified by the parameter collection @xmath19 , which represents the _ dependence structure _ among the marginal distributions in terms of simultaneous successes for every non - empty subset @xmath20 of elements of @xmath21 .",
    "however , several useful results depend only on the first and second order moments of @xmath21 @xmath22 and the reduced parameter collection @xmath23 , which can be used as an approximation of @xmath24 in the generation random multivariate bernoulli vectors in @xcite .",
    "we will first consider a simple result that links covariance and independence of two univariate bernoulli variables .    [ thm : univindep ] let @xmath25 and @xmath26 be two bernoulli random variables .",
    "then @xmath25 and @xmath26 are independent if and only if they are uncorrelated .    if @xmath25 and @xmath26 are independent , then by definition @xmath27 .",
    "if on the other hand we have that @xmath27 , then @xmath28 which completes the proof",
    ".    this theorem can be extended to multivariate bernoulli random variables as follows .",
    "let @xmath17^t$ ] and @xmath29^t$ ] , @xmath30 be two multivariate bernoulli random variables .",
    "then @xmath21 and @xmath31 are independent if and only if @xmath32 , where @xmath33 is the zero matrix .",
    "if @xmath21 is independent from @xmath31 , then by definition every pair @xmath34 , @xmath16 , @xmath35 is independent . therefore @xmath36",
    "= \\mathbf{o}$ ] .",
    "if conversely @xmath32 , every pair @xmath34 is independent as @xmath37 implies @xmath28 .",
    "this in turn implies the independence of the random vectors @xmath21 and @xmath31 , as their sigma - algebras @xmath38 and @xmath39 are functions of the sigma - algebras induced by the two sets of independent random variables @xmath12 and @xmath40 .",
    "the correspondence between uncorrelation and independence is identical to the analogous property of the multivariate gaussian distribution @xcite , and is closely related to the strong normality defined for orthogonal second order random variables in @xcite .",
    "it can also be applied to disjoint subsets of components of a single multivariate bernoulli variable , which are also distributed as multivariate bernoulli random variables .",
    "let @xmath17^t$ ] be a multivariate bernoulli random variable ; then every random vector @xmath41^t$ ] , @xmath42 is a multivariate bernoulli random variable .",
    "the marginal components of @xmath43 are bernoulli random variables , because @xmath21 is multivariate bernoulli .",
    "the new dependency structure is defined as @xmath44 , and uniquely identifies the probability distribution of @xmath43 .",
    "the covariance matrix @xmath45 $ ] , @xmath46 associated with a multivariate bernoulli random vector has several interesting numerical properties . due to the form of the central second order moments defined in formulas [ eq : var ] and [ eq : cov ] , the diagonal elements @xmath47 are bounded in the interval @xmath48 $ ] .",
    "the maximum is attained for @xmath49 , and the minimum for both @xmath50 and @xmath51 . for",
    "the cauchy - schwarz theorem @xcite then @xmath52 $ ] .",
    "the eigenvalues @xmath53 of @xmath54 are similarly bounded , as shown in the following theorem .",
    "let @xmath17^t$ ] be a multivariate bernoulli random variable , and let @xmath45 $ ] , @xmath46 be its covariance matrix .",
    "let @xmath55 , @xmath16 be the eigenvalues of @xmath54",
    ". then @xmath56 and @xmath57 .    since @xmath54 is a real , symmetric , non - negative definite matrix , the eigenvalues @xmath55 are non - negative real numbers @xcite",
    "; this proves the lower bound in both inequalities .",
    "the upper bound in the first inequality holds because @xmath58 and this in turn implies @xmath59 , which completes the proof .",
    "these bounds define a convex set in @xmath60 , defined by the family @xmath61\\right\\}\\ ] ] where @xmath62 is the non - standard @xmath63 simplex @xmath64      consider now a sequence of independent and identically distributed multivariate bernoulli variables @xmath65 .",
    "the sum @xmath66 is distributed as a _ multivariate binomial random variable _",
    "@xcite , thus preserving one of the fundamental properties of the univariate bernoulli distribution .",
    "a similar result holds for the _ law of small numbers _ ,",
    "whose multivariate version states that a @xmath67-variate binomial distribution @xmath68 converges to a _ multivariate poisson distribution _",
    "@xmath69 : @xmath70    both these distributions probability functions , while tractable , are not very useful as a basis for closed - form inference procedures .",
    "an alternative is given by the asymptotic _ multivariate gaussian distribution _",
    "defined by the _",
    "multivariate central limit theorem _",
    "@xcite : @xmath71 the limiting distribution is guaranteed to exist for all possible values of @xmath24 , as the first two moments are bounded and therefore are always finite .",
    "let @xmath72 be the undirected graph underlying a dag @xmath0 , defined as its unique biorientation @xcite .",
    "each edge @xmath73 of @xmath74 corresponds to the directed arcs in @xmath75 with the same incident nodes , and has only two possible states ( it s either present in or absent from the graph ) .",
    "then each possible edge @xmath76 , @xmath77 is naturally distributed as a bernoulli random variable @xmath78 and every set @xmath79 ( including @xmath80 ) is distributed as a multivariate bernoulli random variable @xmath81 and identified by the parameter collection @xmath82 .",
    "the elements of @xmath83 can be estimated via parametric or nonparametric bootstrap as in @xcite , because they are functions of the dags @xmath7 , @xmath84 through the underlying undirected graphs @xmath85 .",
    "the resulting empirical probabilities @xmath86 in particular @xmath87 can be used to obtain several descriptive measures and test statistics for the variability of the structure of a bayesian network .      considering the undirected graphs @xmath88 instead of the corresponding directed graphs @xmath89",
    "greatly simplifies the interpretation of bootstrap s results .",
    "in particular the variability of the graphical structure can be summarized in three cases according to the entropy @xcite of the set of the bootstrapped networks :    * _ minimum entropy _ : all the networks learned from the bootstrap samples have the same structure , that is @xmath90 .",
    "this is the best possible outcome of the simulation , because there is no variability in the estimated network . in this case",
    "the first two moments of the multivariate bernoulli distribution are equal to @xmath91 * _ intermediate entropy _ : several network structures are observed with different frequencies @xmath92 , @xmath93 .",
    "the first two sample moments of the multivariate bernoulli distribution are equal to @xmath94 * _ maximum entropy _ : all @xmath95 possible network structures appear with the same frequency , that is @xmath96 this is the worst possible outcome because edges vary independently of each other and each one is present in only half of the networks ( proof provided in [ app : maxent ] ) : @xmath97 this is also the only case in which all eigenvalues reach their maximum , that is @xmath98 .",
    "several functions have been proposed in literature as univariate measures of spread of a multivariate distribution , usually under the assumption of multivariate normality ( see for example @xcite and @xcite ) .",
    "three of them in particular can be used as descriptive statistics for the multivariate bernoulli distribution :    * the _ generalized variance _ , @xmath99 . * the _ total variance _ , @xmath100 , also called _ total variation _ in @xcite . * the squared _ frobenius matrix norm _ , @xmath101 .",
    "both generalized and total variance associate high values of the statistic to unstable network structures , and are bounded due to the properties of the covariance matrix @xmath54 . for the total variance it s easy to show that @xmath102 the generalized variance is similarly bounded due to hadamard s theorem on the determinant of a non - negative definite matrix @xcite : @xmath103 they reach the respective maxima in the _ maximum entropy _ case and are equal to zero only in the _ minimum entropy _ case .",
    "the generalized variance is also strictly convex ( the maximum is reached only for @xmath104 ) , but it is equal to zero if @xmath54 is rank deficient .",
    "for this reason it s convenient to reduce @xmath54 to a smaller , full rank matrix ( let s say @xmath105 ) and compute @xmath106 instead of @xmath107 .    the squared difference in frobenius norm between @xmath54 and @xmath67 times the _ maximum entropy _ covariance matrix associates high values of the statistic to stable network structures .",
    "it can be rewritten in terms of the eigenvalues @xmath108 of @xmath54 as @xmath109 it has a unique maximum ( in the _ minimum entropy _ case ) , which can be computed as the solution of the constrained minimization problem in @xmath110^t$ ] @xmath111 using lagrange multipliers @xcite .",
    "it also has a single minimum in @xmath112 $ ] , which is the projection of @xmath113 $ ] onto the set @xmath114 and coincides with the _ maximum entropy _ case .",
    "the proof for these boundaries and the rationale behind the use of @xmath115 instead of @xmath116 are reported in [ app : frob ] .",
    "the corresponding normalized statistics are : @xmath117 all of them vary in the @xmath118 $ ] interval and associate high values of the statistic to networks whose structure display a high variability across the bootstrap samples .",
    "equivalently we can define their complements @xmath119 , @xmath120 and @xmath121 , which associate high values of the statistic to networks with little variability and can be used as measures of distance from the _ maximum entropy _ case .",
    "the limiting distribution of the descriptive statistics defined above can be derived by replacing the covariance matrix @xmath54 with its unbiased estimator @xmath122 and by considering the multivariate gaussian distribution from equation [ eqn : mclt ] .",
    "the hypothesis we are interested in is @xmath123 which relates the sample covariance matrix with the one from the _ maximum entropy _ case .    for the total variance we have that @xmath124 @xcite , and",
    "since the maximum value of @xmath125 is achieved in the _ maximum entropy _ case , the hypothesis in equation [ eqn : null ] assumes the form @xmath126 then the observed significance value is @xmath127 , and can be improved with the finite sample correction @xmath128\\right ) =     \\frac{{\\mathsf{p}}(t_t \\leqslant t_t^{oss})}{{\\mathsf{p}}(t_t \\leqslant mk)}\\ ] ] which accounts for the bounds on @xmath129 from inequality [ eq : vart ] .    for the generalized variance",
    "there are several possible asymptotic and approximate distributions :    * the gaussian distribution defined in @xcite @xmath130 * the gamma distribution defined in @xcite @xmath131{\\frac{\\det(\\hat\\sigma)}{\\det(\\frac{1}{4}i_k ) } } \\stackrel{.}{\\sim } ga\\left(\\frac{k(m+1-k)}{2 } , 1\\right).\\ ] ] * the saddlepoint approximation defined in @xcite .",
    "as before the hypothesis in equation [ eqn : null ] assumes the form @xmath132 the observed significance values for the gaussian and gamma distributions are @xmath133 and @xmath134 , and the respective finite sample corrections for the bounds on @xmath135 are @xmath136\\right )       = \\frac{{\\mathsf{p}}(t_{g_1 } \\leqslant t_{g_1}^{oss } ) - { \\mathsf{p}}(t_{g_1 } \\leqslant -\\sqrt{m})}{{\\mathsf{p}}(t_{g_1 } \\leqslant 0 ) -   { \\mathsf{p}}(t_{g_1 } \\leqslant -\\sqrt{m})}\\\\     \\tilde\\alpha_{g_2 } & = { \\mathsf{p}}\\left(t_{g_2 } \\leqslant t_{g_2}^{oss } \\,|\\ , t_{g_2 } \\in \\left[0 , \\frac{mk}{2}\\right]\\right )       =   \\frac{{\\mathsf{p}}(t_{g_2 } \\leqslant t_{g_2}^{oss})}{{\\mathsf{p}}(t_{g_2 } \\leqslant \\frac{mk}{2})}.\\end{aligned}\\ ] ]    the test statistic associated with the squared frobenius norm is the test for the equality of two covariance matrices defined in @xcite , @xmath137 ^ 2\\right )      = \\frac{m}{2 } { \\operatorname{tr}}\\left(\\left[4\\hat\\sigma   - i_k\\right]^2\\right ) \\stackrel{.}{\\sim } \\chi^2_{\\frac{1}{2}k(k+1)},\\ ] ] because @xmath138 ^ 2\\right )       = 16 \\sum_{i=1}^k \\left(\\lambda_i - \\frac{1}{4}\\right)^2 = 16 ||| \\hat\\sigma - \\frac{1}{4}i_k|||_f^2\\ ] ] see [ app : frob ] for an explanation of the use of @xmath116 instead of @xmath115 .",
    "the significance value for @xmath139 is @xmath140 as the hypothesis in equation [ eqn : null ] becomes @xmath141 unlike the previous statistics , nagao s test displays a good convergence speed , to the point that the finite sample correction for the bounds on the squared frobenius matrix norm @xmath142\\right )       = \\frac{{\\mathsf{p}}(t_n \\geqslant t_n^{oss } ) - { \\mathsf{p}}(t_n > t_n^{max})}{{\\mathsf{p}}(t_n \\leqslant t_n^{max})}\\ ] ] is not appreciably better than the raw significance value ( see table [ tab : asym ] for a simple example ) .",
    "another approach to compute the significance values associated with @xmath129 , @xmath107 and @xmath143 is applying again parametric bootstrap .",
    "the multivariate bernoulli distribution @xmath144 specified by the hypothesis in [ eqn : null ] has a diagonal covariance matrix , so its components @xmath145 , @xmath16 are uncorrelated .",
    "according to theorem [ thm : univindep ] they are also independent , so the joint distribution of @xmath144 is completely specified by the marginal distributions @xmath146 .",
    "therefore it s possible ( and indeed quite easy ) to generate observations from the null distribution and use them to estimate the significance value of the normalized statistics @xmath119 , @xmath120 and @xmath121 defined in section [ sec : descriptive ] :    1 .   compute the value of test statistic @xmath147 on the original covariance matrix @xmath54 .",
    "2 .   for @xmath148 .",
    "1 .   generate @xmath149 sets of @xmath67 random samples from a @xmath150 distribution .",
    "2 .   compute their covariance matrix @xmath151 .",
    "3 .   compute @xmath152 from @xmath151 3 .",
    "compute the monte carlo significance value as @xmath153 .",
    "this approach has two important advantages over the parametric tests defined in section [ sec : aymptotic ] :    * the test statistic is evaluated against its true null distribution instead of its asymptotic approximation , thus removing any distortion caused by lack of convergence ( which can be quite slow and problematic in high dimensions ) . *",
    "each simulation @xmath154 has a lower computational cost than the equivalent application of the structure learning algorithm to a bootstrap sample @xmath155 .",
    "therefore the monte carlo test can achieve a good precision with a smaller number of bootstrapped networks , allowing its application to larger problems .",
    ", @xmath156 and @xmath157 represented as functions of their eigenvalues in @xmath114 ( grey ) .",
    "the points @xmath158 and @xmath159 correspond to the _ minimum entropy _ and _ maximum entropy _",
    "cases.,width=377 ]    consider the multivariate bernoulli distributions @xmath160 , @xmath161 and @xmath162 with second order moments @xmath163 associated with two ( increasingly correlated ) arcs from networks .",
    "the eigenvalues of @xmath164 , @xmath156 and @xmath157 are @xmath165 the values of the generalized variance , total variance and squared frobenius matrix norm ( both normalized and in the original scale ) for the three covariance matrices are reported int table [ tab : num ] .",
    ".original and normalized values of @xmath166 , @xmath167 and @xmath168 for @xmath164 , @xmath156 and @xmath157 .",
    "[ cols= \" < , < , < , < , < , < , < \" , ]     the corresponding asymptotic and monte carlo significance values are reported in table [ tab : asym ] and [ tab : boot ] respectively .",
    "each one has been computed for various hypothetical sample sizes ( @xmath169 ) .",
    "parametric bootstrap has been performed on @xmath170 covariance matrices generated from the null distribution for each configuration of test statistic and sample size .",
    "& @xmath171 & @xmath172 & @xmath173 & @xmath174 & @xmath175 + & @xmath176 & @xmath177 & @xmath178 & @xmath179 & @xmath180 + & @xmath181 & @xmath182 & @xmath183 & @xmath184 & @xmath185 + & @xmath186 & @xmath187 & @xmath188 & @xmath189 & @xmath190 + & @xmath191 & @xmath192 & @xmath193 & @xmath194 & @xmath195 + & @xmath186 & @xmath187 & @xmath188 & @xmath189 & @xmath190 + & @xmath191 & @xmath192 & @xmath193 & @xmath194 & @xmath195 +   + & @xmath196 & @xmath197 & @xmath198 & @xmath199 & @xmath200 + & @xmath201 & @xmath202 & @xmath203 & @xmath204 & @xmath205 + & @xmath206 & @xmath207 & @xmath208 & @xmath190 & @xmath190 + & @xmath209 & @xmath210 & @xmath211 & @xmath195 & @xmath195 + & @xmath190 & @xmath190 & @xmath190 & @xmath190 & @xmath190 + & @xmath195 & @xmath212 & @xmath213 & @xmath195 & @xmath212 +   + & @xmath214 & @xmath215 & @xmath216 & @xmath217 & @xmath218 + & @xmath219 & @xmath220 & @xmath221 & @xmath222 & @xmath223 + & @xmath224 & @xmath225 & @xmath226 & @xmath227 & @xmath190 + & @xmath228 & @xmath229 & @xmath230 & @xmath231 & @xmath195 + & @xmath232 & @xmath233 & @xmath234 & @xmath190 & @xmath190 + & @xmath235 & @xmath236 & @xmath237 & @xmath195 & @xmath195 +      + & @xmath171 & @xmath172 & @xmath173 & @xmath174 & @xmath175 + @xmath164 & @xmath238 & @xmath239 & @xmath240 & @xmath241 & @xmath242 + @xmath156 & @xmath243 & @xmath244 & @xmath245 & @xmath245 & @xmath245 + @xmath157 & @xmath243 & @xmath244 & @xmath245 & @xmath245 & @xmath245 +   +   + @xmath164 & @xmath246 & @xmath247 & @xmath248 & @xmath249 & @xmath250 + @xmath156 & @xmath251 & @xmath252 & @xmath245 & @xmath245 & @xmath245 + @xmath157 & @xmath253 & @xmath234 & @xmath245 & @xmath245 & @xmath245 +   +   + @xmath164 & @xmath254 & @xmath255 & @xmath256 & @xmath257 & @xmath258 + @xmath156 & @xmath259 & @xmath260 & @xmath261 & @xmath262 & @xmath245 + @xmath157 & @xmath263 & @xmath264 & @xmath245 & @xmath245 & @xmath245 +",
    "we will now illustrate how these tests can be used to compare different structure learning strategies , i.e. different combinations of structure learning algorithms , conditional independence tests and network scores .",
    "the impact of different choices for each component on the variability of the model can easily be assessed while keeping the other ones fixed .    )",
    "used with the same structure learning algorithm ( grow - shrink ) . ]    first we will compare the performance of the grow - shrink algorithm for three different conditional independence tests .",
    "the learning algorithm has been applied to samples of size @xmath265 , @xmath266 , @xmath267 , @xmath268 , @xmath269 , @xmath270 and @xmath271 ( 20 for each size ) generated from the alarm reference network @xcite , which is composed by 37 discrete nodes and 46 arcs for a total of 509 parameters .",
    "both the data and the software implementation of the algorithm are included in the bnlearn package @xcite for r @xcite .",
    "the following tests have been considered :    * the asymptotic @xmath272 test based on mutual information @xcite , which is in fact a log - likelihood ratio test and is also called the @xmath273 test @xcite . * the shrinkage estimator for the mutual information , which is a james - stein regularized estimator developed by @xcite . *",
    "pearson s @xmath272 asymptotic test for independence @xcite .",
    "the same threshold @xmath274 for type i error has been used in three cases , and network variability has been assessed with the monte carlo test for the squared frobenius norm .",
    "results are shown in figure [ fig : smalltest ] .",
    "all the tests considered in the analysis start producing relatively stable network structures ",
    "i.e. the null hypothesis corresponding to the maximum entropy case is rejected  at sample sizes @xmath268 and @xmath269 .",
    "pearson s @xmath272 test performs slightly better than mutual information , as documented in @xcite when dealing with sparse contingency tables .",
    "this is also true for the shrinkage estimator .",
    "however , the difference among the three sets of significance values is very small .    on the other hand we will now compare three different learning algorithms :    * tabu search ( which is a score - based algorithm ) , combined with a bayesian information criterion ( bic ) score . *",
    "grow - shrink ( which is a constraint - based algorithm ) combined with the asymptotic @xmath272 test based on mutual information described above and @xmath274 . *",
    "max - min hill climbing ( which is hybrid algorithm ) , combined with a bic score and the asymptotic mutual information test .",
    "as can be seen in figure [ fig : smallalgo ] in this case differences are more pronounced .",
    "the max - min hill climbing algorithm , which is one of the top performers up to date for large networks , displays less variability than tabu search and grow - shrink at the same sample size .",
    "in particular the difference between max - min hill climbing and grow - shrink confirms the analysis made in @xcite and the well - documented @xcite instability displayed by constraint - based algorithms at small sample sizes .",
    "in this paper we derived the properties of several measures of variability for the structure of a bayesian network through its underlying undirected graph , which is assumed to have a multivariate bernoulli distribution .",
    "descriptive statistics , asymptotic and monte carlo tests were developed along with their fundamental properties .",
    "they can be used to compare the performance of different learning algorithms and to measure the strength of arbitrary subsets of arcs .",
    "many thanks to prof .",
    "adriana brogini , my supervisor at the ph.d .",
    "school in statistical sciences ( university of padova ) , for proofreading this article and giving many useful comments and suggestions .",
    "i would also like to thank giovanni andreatta and luigi salce ( full professors at the department of pure and applied mathematics , university of padova ) for their help in the development of the constrained optimization and matrix norm applications respectively .",
    "the squared frobenius matrix norm of the difference between the covariance matrix @xmath54 and the _ maximum entropy _",
    "matrix @xmath116 is @xmath275 its unique global minimum is zero for @xmath276 but it has a varying number of global maxima depending on the dimension @xmath67 of @xmath54 .",
    "they are the solutions of the constrained minimization problem @xmath277 this configuration of stationary points is not a problem for asymptotic and monte carlo tests , but prevents any direct interpretation of the values of descriptive statistics .",
    "( on the left ) and @xmath115 ( on the right ) in @xmath114 for @xmath278 .",
    "the green area is the set @xmath114 of the possible eigenvalues of @xmath54 and the red lines are level curves . ]    on the other hand , the difference in squared frobenius norm @xmath279 has both a unique global minimum ( because it s a strictly convex function ) @xmath280 and a unique global maximum @xmath281 which correspond to the _ minimum entropy _",
    "( @xmath282 $ ] ) and the _ maximum entropy _ ( @xmath283 $ ] ) covariance matrices respectively ( see figure [ fig : frobenius ] ) .",
    "however since @xmath115 is not a valid covariance matrix for a multivariate bernoulli distribution , @xmath143 can not be used to derive any probabilistic result .",
    "the values of @xmath284 and @xmath54 in the _ maximum entropy _ case are a direct consequence from the following theorem .",
    "let @xmath285 , @xmath286 , @xmath287 be all possible undirected graphs with vertex set @xmath288 and let @xmath289 , @xmath290 .",
    "let @xmath76 and @xmath291 , @xmath292 be two edges",
    ". then @xmath293 and @xmath294 .",
    "the number of possible configurations of an undirected graph is given by the cartesian product of the possible states of its @xmath149 edges , resulting in @xmath295 possible undirected graphs .",
    "then edge @xmath76 is present in @xmath296 graphs and @xmath76 and @xmath291 are simultaneously present in @xmath297 graphs .",
    "therefore @xmath298    the fact that @xmath299 for every @xmath292 also proves that the edges are independent according to theorem [ thm : univindep ] ."
  ],
  "abstract_text": [
    "<S> the structure of a bayesian network includes a great deal of information about the probability distribution of the data , which is uniquely identified given some general distributional assumptions . </S>",
    "<S> therefore it s important to study its variability , which can be used to compare the performance of different learning algorithms and to measure the strength of any arbitrary subset of arcs .    </S>",
    "<S> in this paper we will introduce some descriptive statistics and the corresponding parametric and monte carlo tests on the undirected graph underlying the structure of a bayesian network , modeled as a multivariate bernoulli random variable . a simple numeric example and the comparison of the performance of some structure learning algorithm on small samples </S>",
    "<S> will then illustrate their use .    </S>",
    "<S> bayesian network , bootstrap , multivariate bernoulli distribution , structure learning algorithm . </S>"
  ]
}