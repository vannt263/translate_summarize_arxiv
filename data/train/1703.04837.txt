{
  "article_text": [
    "representation learning @xcite , which aims to learn the features automatically based on various deep learning models @xcite , has been extensively studied in recent years . traditionally , supervised learning tasks require hand - designed features as inputs .",
    "deep learning models have shown great success in automatically learning the semantic representations for different types of data , like image , text and speech @xcite . in this paper , we focus on representation learning of networks , in particular , signed networks .",
    "several representation learning methods of unsigned networks have been developed recently @xcite .",
    "they represent each node as a low - dimensional vector which captures the structure information of the network .",
    "signed networks are ubiquitous in real - world social systems , which have both positive and negative relationships .",
    "for example , epinions   allows users to mark their trust or distrust to other users on product reviews and slashdot   allows users to specify other users as friends or foes .",
    "most unsigned network embedding models @xcite are based on skip - gram @xcite , a classic approach for training word embeddings .",
    "the objective functions used in unsigned network embedding approaches do not incorporate the sign information of edges .",
    "thus , they can not simply migrate to signed networks because the negative links change the theories or assumptions on which unsigned network embedding models rely @xcite .    in this paper",
    ", we develop a signed network embedding model called sne . to the best of our knowledge ,",
    "this is the first research on signed network embedding .",
    "our sne model adopts the log - bilinear model @xcite , uses node representations of all nodes along a given path , and further incorporates two signed - type vectors to capture the positive or negative relationship of each edge along the path .",
    "our sne significantly outperforms existing unsigned network embedding models which assume all edges are from the same type of relationship and only use the representations of nodes in the target s neighborhood .",
    "we conduct two experiments to evaluate our model , node classification and link prediction , on both an undirected signed network and a directed signed network built from real - world data .",
    "we compare with four baselines including a matrix factorization method and three state - of - the - art network embedding models designed for unsigned networks .",
    "the experimental results demonstrate the effectiveness of our signed network embedding .",
    "in this section , we first introduce the skip - gram model , one of commonly used neural language models to train word embeddings @xcite .",
    "we then give a brief overview of several state - of - the - art unsigned network embedding models based on the skip - gram model .",
    "* skip - gram model * the skip - gram is to model the co - occurrence probability @xmath0 that word @xmath1 co - occurs with word @xmath2 in a context window .",
    "the co - occurrence probability is calculated based on softmax function : @xmath3 where @xmath4 is the set of all words and @xmath5 and @xmath6 are word embeddings for @xmath1 and @xmath2 , respectively .",
    "the parameters @xmath7 , i.e. , @xmath8 , @xmath5 , are trained by maximizing the log likelihood of predicting context words in a corpus : @xmath9 where @xmath10 is the set of context words of @xmath2 .",
    "* network embedding * network embedding aims to map the network @xmath11 into a low dimensional space where each vertex is represented as a low dimensional real vector .",
    "the network embedding treats the graph s vertex set @xmath12 as the vocabulary @xmath4 and treats each vertex @xmath13 as a word @xmath2 in the skip - gram approach .",
    "the corpus used for training is composed by the edge set @xmath14 , e.g. , in @xcite , or a set of truncated random walks from the graph , e.g. , in @xcite and @xcite .    to train the node vectors , the objective of previous network embedding models is to predict the neighbor nodes @xmath15 of a given source node @xmath13 .",
    "however , predicting a number of neighbor nodes requires modeling the joint probability of nodes , which is hard to compute .",
    "the conditional independence assumption , i.e. , the likelihood of observing a neighbor node is independent of observing other neighbor nodes given the source node , is often assumed @xcite .",
    "thus , the objective function is defined as : @xmath16 where @xmath17 is softmax function similar to equation [ eq : softmax ] except that the word vectors are replaced with node vectors .",
    "we present our network embedding model for signed networks . for each node s embedding",
    ", we introduce the use of both source embedding and target embedding to capture the two potential roles of each node .",
    "formally , a signed network is defined as @xmath18 , where @xmath12 is the set of vertices and @xmath19 ( @xmath20 ) is the set of positive ( negative ) edges .",
    "each edge @xmath21 is represented as @xmath22 , where @xmath23 and @xmath24 indicates the sign value of edge @xmath25 , i.e. , @xmath26 if @xmath27 and @xmath28 if @xmath29 . in the scenario of signed directed graphs , @xmath30 is a directed edge where node node @xmath31 denotes the source and @xmath32 denotes the target .",
    "our goal is to learn node embedding for each vertex in a signed network while capturing as much topological information as possible . for each vertex @xmath13",
    ", its node representation is defined as @xmath33 $ ] where @xmath34 denotes its source embedding and @xmath35 denotes its target embedding .",
    "we develop our signed network embedding by adapting the log - bilinear model such that the trained node embedding can capture node s path and sign information .",
    "recall that existing unsigned network embedding models are based on the skip - gram which only captures node s neighbour information and can not deal with the edge sign .",
    "* log - bilinear model * given a sequence of context words @xmath36 , the log - bilinear model firstly computes the predicted representation for the target word by linearly combining the feature vectors of words in the context with the position weight vectors : @xmath37 where @xmath38 indicates the element - wise multiplication and @xmath39 denotes the position weight vector of the context word @xmath2 .",
    "a score function is defined to measure the similarity between the predicted target word vector and its actual target word vector : @xmath40 where @xmath41 is the bias term .",
    "the log - bilinear model then trains word embeddings @xmath42 and position weight vectors @xmath43 by optimizing the objective function similar to the skip - gram .",
    "* sne algorithm * in our signed network embedding , we adopt the log - bilinear model to predict the target node based on its paths . the objective of the log - bilinear model is to predict a target node given its predecessors along a path .",
    "thus , the signed network embedding is defined as a maximum likelihood optimization problem .",
    "one key idea of our signed network embedding is to use signed - type vector @xmath44 ( @xmath45 ) to represent the positive ( negative ) edges .",
    "formally , for a target node @xmath32 and a path @xmath46 $ ] , the model computes the predicted target embedding of node @xmath32 by linearly combining source embeddings ( @xmath47 ) of all source nodes along the path @xmath48 with the corresponding signed - type vectors ( @xmath49 ) : @xmath50 where @xmath51 if @xmath52 , or @xmath53 if @xmath54 and @xmath38 denotes element - wise multiplication . the score function is to evaluate the similarity between the predicted representation @xmath55 and the actual representation @xmath56 of target node @xmath32 : @xmath57 where @xmath58 is a bias term .    to train the node representations",
    ", we define the conditional likelihood of target node @xmath32 generated by a path of nodes @xmath48 and their edge types @xmath59 based on softmax function : @xmath60 where @xmath12 is the set of vertices , and @xmath61 $ ] .",
    "the objective function is to maximize the log likelihood of equation [ eq : score_softmax ] : @xmath62    initialization : randomly initialize the source and target node embeddings @xmath63 and @xmath64of each node @xmath12    generate the corpus based on uniform random walk    algorithm [ alg : sne ] shows the pseudo - code of our signed network embedding .",
    "we first randomly initialize node embeddings ( line 1 ) and then use random walk to generate the corpus ( line 2 ) .",
    "lines 4 - 11 show how we specify @xmath49 based on the sign of edge @xmath65 .",
    "we calculate the predicted representation of the target node by combining source embeddings of nodes along the path with the edge type vectors ( line 12 ) .",
    "we calculate the score function to measure the similarity between the predicted representation and the actual representation of the target node ( line 13 ) and compute the conditional likelihood of target node given the path ( line 14 ) .",
    "finally , we apply the adagrad method @xcite to optimize the objective function ( line 15 ) .",
    "the procedures in lines 4 - 15 repeat over each path in the corpus .    for a large network ,",
    "the softmax function is expensive to compute because of the normalization term in equation [ eq : score_softmax ] .",
    "we adopt the sampled softmax approach @xcite to reduce the computing complexity . during training , the source and target embeddings of each node",
    "are updated simultaneously . once the model is well - trained , we get node embeddings of a signed network .",
    "we also adopt the approach in @xcite to generate paths efficiently .",
    "given each starting node @xmath31 , we uniformly sample the next node from the neighbors of the last node in the path until it reaches the maximum length @xmath66 .",
    "we then use a sliding window with size @xmath67 to slide over the sequence of nodes generated by random walk .",
    "the first @xmath68 nodes in each sliding window are treated as the sequence of path and the last node as the target node . for each node @xmath31",
    ", we repeat this process @xmath69 times .",
    "to compare the performance of different network embedding approaches , we focus on the quality of their output , i.e. , node embeddings .",
    "we use the generated node embeddings as input of two data mining tasks , node classification and link prediction . for node classification , we assume each node in the network is associated with a known class label and use node embeddings to build classifiers . in link prediction , we use node embeddings to predict whether there is a positive , negative , or no edge between two nodes . in our signed network embedding , we use the whole node representation @xmath33 $ ] that contains both source embedding @xmath70 and target embedding @xmath71 .",
    "this approach is denoted as @xmath72 .",
    "we also use only the source node vector @xmath70 as the node representation .",
    "this approach is denoted as @xmath73 . comparing the performance of @xmath72 and @xmath73 on both directed and undirected networks expects to help better understand the performance and applicability of our signed network embedding .    * baseline algorithms * we compare our sne with the following baseline algorithms .",
    "* signedlaplacian @xcite .",
    "it calculates eigenvectors of the @xmath74 smallest eigenvalues of the signed laplacian matrix and treats each row vector as node embedding . *",
    "deepwalk @xcite .",
    "it uses uniform random walk ( i.e. , depth - first strategy ) to sample the inputs and trains the network embedding based on skip - gram .",
    "* line @xcite .",
    "it uses the breadth - first strategy to sample the inputs based on node neighbors and preserves both the first order and second order proximities in node embeddings . *",
    "node2vec @xcite .",
    "it is based on skip - gram and adopts the biased random walk strategy to generate inputs . with the biased random walk",
    ", it can explore diverse neighborhoods by balancing the depth - first sampling and breath - first sampling .",
    "* datasets * we conduct our evaluation on two signed networks .",
    "( 1 ) the first signed network , _ wikieditor _ , is extracted from the umd wikipedia dataset @xcite .",
    "the dataset is composed by 17015 vandals and 17015 benign users who edited the wikipedia pages from jan 2013 to july 2014 .",
    "different from benign users , vandals edit articles in a deliberate attempt to damage wikipedia .",
    "one edit may be reverted by bots or editors .",
    "hence , each edit can belong to either _ revert _ or _ no - revert _ category .",
    "the wikieditor is built based on the co - edit relations .",
    "in particular , a positive ( negative ) edge between users @xmath75 and @xmath76 is added if the majority of their co - edits are from the same category ( different categories ) .",
    "we remove from our signed network those users who do not have any co - edit relations with others .",
    "note that in wikieditor , each user is clearly labeled as either benign or vandal .",
    "hence , we can run node classification task on wikieditor in addition to link prediction .",
    "( 2 ) the second signed network is based on the slashdot zoo dataset  .",
    "the slashdot network is signed and directed .",
    "unfortunately , it does not contain node label information .",
    "thus we only conduct link prediction .",
    "table [ tb : wiki ] shows the statistics of these two signed networks .",
    "max width=0.8    .statistics of wikieditor and slashdot [ cols=\"^,^,^\",options=\"header \" , ]      * vector dimension * we evaluate how the dimension size of node vectors affects the accuracy of two tasks on both wikieditor and slashdot . for link prediction , we use @xmath72 with hadamard operation as it can achieve the best performance as shown in the last section .",
    "figure [ fig : dim ] shows how the accuracy of link prediction varies with different dimension values of node vectors used in @xmath72 for both datasets .",
    "we can observe that the accuracy increases correspondingly for both datasets when the dimension of node vectors increases .",
    "meanwhile , once the accuracy reaches the top , increasing the dimensions further does not have much impact on accuracy any more .",
    "* sample size * figure [ fig : samples ] shows how the accuracy of link prediction varies with the sample size used in @xmath72 for both datasets . for wikieditor",
    ", we tune the sample size by changing the number of random walks starting at each node ( @xmath69 ) . in our experiment , we set @xmath77 respectively and calculate the corresponding sample sizes . for slashdot",
    ", we directly use the number of sampled edges in our training as the path length is one . for both datasets ,",
    "the overall trend is similar",
    ". the accuracy increases with more samples .",
    "however , the accuracy becomes stable when the sample size reaches some value .",
    "adding more samples further does not improve the accuracy significantly .    0.4     0.4     * path length * we use wikieditor to evaluate how the path length @xmath68 affects the accuracy of both node classification and link prediction . from figure",
    "[ fig : node ] , we observe that slightly increasing the path length in our @xmath78 can improve the accuracy of node classification .",
    "this indicates that the use of long paths in our sne training can generally capture more network structure information , which is useful for node classification .",
    "however , the performance of the @xmath73 and @xmath72 decreases when the path length becomes too large .",
    "one potential reason is that sne uses only two signed - type vectors for all nodes along paths and nodes in the beginning of a long path may not convey much information about the target node . in figure",
    "[ fig : link ] , we also observe that the accuracy of link prediction decreases when the path length increases .",
    "for link prediction , the performance depends more on local information of nodes .",
    "hence the inclusion of one source node in the path can make our sne learn the sufficient local information .",
    "0.4 ) , title=\"fig:\",height=153 ]    0.4 ) , title=\"fig:\",height=153 ]",
    "* signed network analysis * mining signed network attracts increasing attention @xcite .",
    "the balance theory @xcite and the status theory @xcite have been proposed and many algorithms have been developed for tasks such as community detection , link prediction , and spectral graph analysis of signed networks @xcite .",
    "spectral graph analysis is mainly based on matrix decomposition which is often expensive and hard to scale to large networks .",
    "it is difficult to capture the non - linear structure information as well as local neighborhood information because it simply projects a global matrix to a low dimension space formed by leading eigenvectors .",
    "* network embedding * several network embedding methods including deepwalk @xcite , line @xcite , node2vec @xcite , deep graph kernels @xcite and ddrw @xcite have been proposed .",
    "these models are based on the neural language model .",
    "several network embedding models are based on other neural network model .",
    "for example , dnr @xcite uses the deep auto - encoder , dngr @xcite is based on a stacked denoising auto - encoder , and the work @xcite adopts the convolutional neural network to learn the network feature representations .",
    "meanwhile , some works learn the network embedding by considering the node attribute information . in @xcite",
    "the authors consider the node label information and present semi - supervised models to learn the network embedding .",
    "the heterogeneous network embedding models are studied in @xcite .",
    "hope @xcite focuses on preserving the asymmetric transitivity of a directed network by approximating high - order proximity of a network .",
    "unlike all the works described above , in this paper , we explore the signed network embedding .",
    "in this paper , we have presented sne for signed network embedding .",
    "our sne adopts the log - bilinear model to combine the edge sign information and node representations of all nodes along a given path .",
    "thus , the learned node embeddings capture the information of positive and negative links in signed networks .",
    "experimental results on node classification and link prediction showed the effectiveness of sne . our sne expects to keep the same scalability as deepwalk or node2vec because sne adopts vectors to represent the sign information and uses linear operation to combine node representation and signed vectors . in our future work",
    ", we plan to examine how other structural information ( e.g. , triangles or motifs ) can be preserved in signed network embedding .",
    "the authors acknowledge the support from the national natural science foundation of china ( 71571136 ) , the 973 program of china ( 2014cb340404 ) , and the research project of science and technology commission of shanghai municipality ( 16jc1403000 , 14511108002 ) to shuhan yuan and yang xiang , and from national science foundation ( 1564250 ) to xintao wu .",
    "this research was conducted while shuhan yuan visited university of arkansas .",
    "yang xiang is the corresponding author of the paper ."
  ],
  "abstract_text": [
    "<S> several network embedding models have been developed for unsigned networks . however </S>",
    "<S> , these models based on skip - gram can not be applied to signed networks because they can only deal with one type of link . in this paper , we present our signed network embedding model called sne . </S>",
    "<S> our sne adopts the log - bilinear model , uses node representations of all nodes along a given path , and further incorporates two signed - type vectors to capture the positive or negative relationship of each edge along the path . </S>",
    "<S> we conduct two experiments , node classification and link prediction , on both directed and undirected signed networks and compare with four baselines including a matrix factorization method and three state - of - the - art unsigned network embedding models . </S>",
    "<S> the experimental results demonstrate the effectiveness of our signed network embedding . </S>"
  ]
}