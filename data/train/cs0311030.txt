{
  "article_text": [
    "we study the problem of designing an efficient and distributed algorithm that partitions the sensors in a wsn into @xmath5 covers such that as many areas are monitored as frequently as possible .",
    "the problem of choosing a cover for each sensor is abstracted into a variant of the set k - cover problem , in which we are given a finite set @xmath6 of elements , corresponding to the areas to be monitored , a collection @xmath7 of subsets of @xmath6 , where each @xmath8 represents a sensor and contains the areas that sensor monitors from @xmath6 , and a positive integer @xmath9 .",
    "the goal is to find a partition of the subsets into @xmath5 covers @xmath10 where each cover is a set of subsets , such that @xmath11 is maximized .",
    "informally , we are maximizing the number of times the areas are covered by the partition .",
    "the set k - cover problem can be used to increase the energy efficiency of wsns .",
    "a single area in a wsn may be covered by multiple sensors due to the ad hoc nature of sensor placement , topological constraints , or perhaps to compensate for the short lifetime of a sensor by placing multiple sensors close together .",
    "therefore , in an effort to increase the longevity of the network and conserve battery power , it can be beneficial to activate groups of sensors in rounds , so that the battery life of a sensor is not wasted on areas that are already monitored by other sensors .",
    "in addition , certain batteries last up to twice as long when used in short bursts as opposed to continuously @xcite .",
    "therefore , activating a sensor only once every @xmath5 time units can extend the lifetime of its battery .",
    "previous results on this problem  @xcite solve a fair version where the objective is to maximize @xmath5 such that every cover contains _ all _ the elements .",
    "in many environments , requiring that a cover contain all the elements may be too strict .",
    "consider , for instance , that there is a single area that is monitored by only one sensor but all other areas are monitored by hundreds of sensors . except for that single area ,",
    "all other areas could be covered for much longer by dividing the sensors into covers .",
    "but in the fair version , we can not partition the sensors at all because only one partition would be able to monitor that one area .",
    "therefore , we relax the requirement that every cover contain all the elements .",
    "we explore three algorithms that solve the set k - cover problem : randomized , distributed greedy , and centralized greedy . in the randomized algorithm ,",
    "each sensor simply assigns itself to a cover chosen uniformly at random from the set of all possible covers . in the distributed greedy algorithm ,",
    "each sensor assigns itself , in turn , to the cover with the minimum intersection between the areas the sensor monitors and the areas monitored by the cover thus far .",
    "the centralized greedy algorithm is similar to the distributed greedy , except that an area in the intersection is weighted based on how likely it is to be covered by some other sensor later on in the assignment process .",
    "the performance of our three algorithms are summarized in table [ f : sum ] .",
    "one metric for the performance of our algorithms is the worst case ratio between the number of times the areas are covered , according to the algorithm s partition , and the optimum number of times the areas can be covered by any partition .",
    "this ratio is referred to interchangeably as the performance guarantee and the approximation ratio .",
    "our simulations show that for high density networks , the set k - cover partition can simultaneously achieve high @xmath5 and high coverage at each time instant .",
    "simulation results indicate that the increase in longevity is a constant function of the density of the network . in table",
    "[ f : sum ] , @xmath12 is the number of sensor - area pairs such that the given sensor covers the given area , and @xmath13 is a scaling factor(perhaps dependent on other problem parameters ) .",
    "the running time of an algorithm is the number of time units the sensor network needs to create the partition ( within the distributed or centralized setting in which the algorithm is presented ) .",
    "@xmath14 is the cardinality of the largest subset .",
    "there is no worst case guarantee on fairness for the distributed and centralized greedy algorithms .",
    "however , in simulations , calculations of the area that is covered by the least number of covers , relative to the number of sensors that are capable of covering it , suggest the algorithms are fair in practice .",
    "we find , in accordance with `` no free lunch theorems ''  @xcite that there is a trade - off between the complexity ( both in terms of running time and simplicity ) and the performance guarantee .",
    "the randomized algorithm is remarkably simple , robust , easy to use , and easy to code .",
    "it is also fair in two respects .    1 .   in expectation ,",
    "an area is covered within @xmath0 of the maximum number of times possible .",
    "2 .   with high probability , the least covered area is covered within @xmath15 of the maximum number of times possible    the randomized algorithm does bears some risk since its approximation ratio is an expectation .",
    "the distributed greedy algorithm has a deterministic approximation ratio , but the ratio is smaller than the ratio for the randomized algorithm , and both the running time and the requirements of the network are slightly higher .",
    "finally , the centralized greedy algorithm gives a best possible guarantee for some variants of the problem , but it may not always possible to design a distributed implementation .",
    "we show that it is np - complete to guarantee better than @xmath4 of the optimal coverage , indicating that all three algorithms perform well with respect to the best approximation algorithm possible .",
    "the hardness of approximation is obtained by a reduction from the e4-set splitting problem .",
    "simulations show that in practice , the algorithms perform well above the worst case bounds proved in the theoretical analysis .",
    "many simulations show the algorithms covering more than 99% of the maximum possible .",
    "simulations also suggest that using the sensors in rounds has the potential to significantly increase the longevity of sensor networks . in simulation results ,",
    "the energy savings using the set k - cover algorithm are directly proportional to the density of the network .",
    "significant increases in the longevity of the network are observed when the overlap between sensors is high .",
    "in addition , there is time gained by extended battery lifetimes due to operation in short bursts .",
    "the paper is organized as follows . in sections ii , iii , and iv respectively , a randomized , distributed greedy , and centralized greedy algorithm are presented and analyzed .",
    "section v shows the hardness of approximation for the set k - cover problem .",
    "section vi contains the results of various simulations .",
    "we conclude with open problems and areas of further exploration .",
    "the randomized algorithm assigns each sensor to a cover chosen uniformly at random .",
    "it requires no preprocessing and makes extremely few assumptions about the network .",
    "its simplicity facilitates implementation , use , and maintenance .",
    "it is also robust to sensor failure , and can easily accommodate the entry of new sensors into the system .",
    "in addition , the expected coverage is high , at least @xmath3 of the best coverage possible .",
    "this is also true per individual area , so that the expected amount an area is covered is proportional to how many sensors are capable of monitoring that area .",
    "we can attain close to the expected performance in practice because the algorithm is simple enough that it can be run many times during the lifetime of the sensor network .",
    "this reduces the risk that the overall performance is far from the average .",
    "assumptions : + 1 .",
    "it is assumed that all sensors have clocks with a unified start time @xmath16 , so that operations can be synchronized .",
    "each sensor has a random number generator .",
    "+ the following algorithm partitions the sensors into covers and is executed in parallel at each sensor starting from initialization at time @xmath17 .    at the end of the algorithm",
    ", sensor @xmath18 belongs to cover @xmath19 . during the round - robin covering of the areas ,",
    "sensor @xmath18 will activate itself when cover @xmath19 is active .",
    "[ ra ] the expected number of times elements are covered by the randomized algorithm is a @xmath0 approximation to opt , where opt is the best coverage possible .    for a single area @xmath20",
    ", we will calculate @xmath21 $ ] , the expected number of covers that cover @xmath20 in our solution .",
    "we use @xmath22 to denote the number of subsets that contain @xmath20 .",
    "a cover will _ not _ contain @xmath20 with probability @xmath23 because there are @xmath22 sets to be assigned and each has probability @xmath24 of being assigned to a particular cover .",
    "the expected number of covers containing @xmath20 is @xmath25 .",
    "so the total expected number of times areas are covered by the partition is @xmath26 = \\sum_v ( k - k(1 - \\frac{1}{k})^{n_v})$ ] .",
    "let @xmath27 be the number of times @xmath20 is covered in the optimum solution .",
    "then , @xmath28 because an area can not be covered by more than @xmath5 covers or by more than the number of subsets containing it .",
    "the expected number of times areas are covered by the algorithm is at least @xmath26 $ ] and the total covered by opt is at most @xmath29 . to show the overall fraction @xmath30}{\\sum_v \\min(k , n_v ) } \\geq ( 1 - \\frac{1}{e})$ ]",
    "we will show that @xmath31 , @xmath32}{\\min(k , n_v ) } \\geq ( 1 - \\frac{1}{e})$ ] .",
    "there are two cases .    1 .",
    "@xmath33 then , @xmath32}{\\min(k , n_v ) } = 1 - ( 1-\\frac{1}{k})^{n_v }   \\geq 1 - ( 1-\\frac{1}{k})^k   \\geq 1 - \\frac{1}{e}$ ] . the last inequality is due to the power series expansion of @xmath34 , which shows that @xmath35 .",
    "2 .   @xmath36 + we will show that the derivative of the ratio @xmath32}{\\min(k , n_v)}$ ] with respect to @xmath22 is negative , implying that the ratio is smallest when @xmath37 .",
    "+ @xmath38}{n^2_v}$ ] . +",
    "this is negative iff @xmath39 which is again true due to the power series expansion ( @xmath40  @xcite ) .",
    "another attractive property of the randomized algorithm is that the element that is covered least is not covered too much less than the maximum number of times that it could possibly be covered . from case",
    "i above , in expectation , an area is covered within @xmath0 of the maximum number of times possible .",
    "the tails of the distribution over @xmath41 can also be bounded .",
    "more precisely , consider our objective is to find a partition of the subsets of @xmath6 into @xmath5 covers such that @xmath41 is maximized , where @xmath41 satisfies @xmath42 .",
    "let @xmath43 be the optimum value of @xmath41 .    with high probability ( greater than @xmath44 )",
    ", the randomized algorithm gives a solution with @xmath45 .",
    "let @xmath46 be the number of covers area @xmath20 belongs to after the randomized rounding and @xmath22 be the number of sets containing @xmath20 .",
    "@xmath47 $ ] + @xmath48 .",
    "each @xmath20 falls into one of two cases .    1 .   @xmath49",
    ". then @xmath50 .",
    "2 .   @xmath51 . using chernoff bounds , @xmath52 because @xmath53 , @xmath54    the probability that a single @xmath46 is less than @xmath55 is less than @xmath56 , so the probability that any @xmath46 is less than @xmath55 is less than @xmath57 due to the bool - bonferroni inequalities  @xcite .",
    "therefore , the probability that the result does not have all @xmath46 within constant is significantly small , less than @xmath58 .",
    "the distributed greedy algorithm , in contrast with the randomized algorithm , gives a _",
    "deterministic _ guarantee that the produced partition covers at least half as many areas as the best possible partition .",
    "the algorithm makes some assumptions about what the network is able to do and also requires some preprocessing steps .",
    "assumptions : + 1 . a clock with a unified start time @xmath16 ,",
    "so that operations can be synchronized .",
    "+ 2 . a unique i",
    "d number taken from the set of integers @xmath59 .",
    "knowledge of the parameter @xmath5 and memory for storing a matrix of size @xmath60 , all entries initialized to 1 .",
    "+ 4 . some way to recognize an area of interest ( for instance , geographic coordinates or a mapping from unique sensor information to an area identification number ) .",
    "+ 5 . some way to communicate with other sensors that cover a common area ( preferably in a local manner through direct broadcasting ) .",
    "+      several preprocessing steps must take place before the partition can be created .",
    "first , each sensor determines which areas of interest it will be capable of monitoring once it is in an activated on state .",
    "this can be done using gps or sensor localization which is itself an area of active research , and algorithms to achieve this task are described in  @xcite ,  @xcite , and  @xcite , among others .",
    "next , each sensor must determine a method of communication with other sensors covering the areas that it covers , which we will refer to as the sensor s neighbors .",
    "it may be necessary to communicate this information using a broadcasting tree  @xcite or other forms of message routing .",
    "we will give a two step distributed algorithm for stationary sensors in euclidean space with no obstacles .",
    "however , the specific implementation of this task will vary between applications .    1",
    ".   every sensor broadcasts its unique sensor i d number , the areas it monitors , and the distances to these areas , to twice the distance of the furthest area that it monitors .",
    "+ 2 .   based on information a sensor receives in step 1 , from the set of sensors with which it knows it shares an area in common",
    ", it records the distance from the area to the sensor that is furthest away as its @xmath61 parameter .",
    "if this distance is less than the distance of its broadcast in step 1 , it instead sets its @xmath61 parameter to the distance that was used for broadcasting in step 1 .",
    "this process ensures that every sensor node knows the broadcast distance necessary so that the other nodes covering a common area can be notified by the sensor .",
    "the @xmath61 distance will be used by the sensor to inform other sensors of its decisions during the partition phase .      in this phase , the sensors are partitioned into covers .",
    "the algorithm is initiated at time @xmath62 .",
    "the above distributed greedy algorithm is simple and requires only @xmath63 time .",
    "in addition , it is guaranteed to cover more than half of what the optimum partition is capable of covering .",
    "the distributed greedy algorithm is a @xmath2 approximation for the set k - cover problem .",
    "proof by construction .",
    "we will iterate back through the @xmath64 subsets , creating a copy of @xmath8 , called @xmath65 at its location @xmath66 in opt .",
    "the number of newly covered elements by @xmath65 is @xmath67 and the number of elements covered by @xmath8 at the moment it was assigned to @xmath19 at time @xmath68 will be called @xmath69 . because @xmath8 was assigned to @xmath19 , and because @xmath67 only decreases by the addition of more covers as we are iterating backward , @xmath70 .",
    "in addition , @xmath71 since this assignment subsumes the sets assigned to their optimal positions . combining equations , @xmath72",
    "the centralized greedy algorithm has a better approximation ratio than the distributed greedy algorithm , and this ratio is tight for some instances of the problem",
    ". however , the communication and storage requirements for deploying this algorithm in a distributed setting are more involved than the above algorithms and may vary greatly between applications .",
    "we do not propose this as a distributed algorithm but instead show that in a centralized setting , the performance of the randomized algorithm can be made into a deterministic guarantee .",
    "we leave as an open problem the implementation of this algorithm in a distributed setting .",
    "this algorithm is the same as the distributed greedy algorithm except that each area is assigned a weight of @xmath73 where @xmath74 is the number of subsets containing area @xmath20 , in the given time step , that have not yet been assigned to a cover .",
    "now , instead of summing entries in the rows of the matrix , the matrix is multiplied with a @xmath75 vector corresponding to the weights of the areas covered by the sensor .",
    "the sensor is then assigned to the column which is largest in the @xmath76 vector resulting from the matrix multiplication . through this process",
    ", the algorithm chooses a cover @xmath19 , for a given subset @xmath8 , that maximizes the weighted sum of uncovered elements , @xmath77 , instead of simply @xmath78 as in the distributed greedy algorithm .",
    "this is an intuitive algorithm in that each subset is assigned to the cover where it covers the largest possible number of uncovered elements , weighted according to how likely it is that the element will be covered in future iterations .",
    "initializec = \\{c_1 : = , ... , c_k : = } ; + for j:=1 until n + find i = argmax_i _ v : v s_j v _",
    "s_j c_i s_j ( 1 - ) ^y_v - 1 ; + c_i : = c_i s_j ( assign s_j to the cover c_i ) ; +    we will prove that this algorithm gives a @xmath3 approximation ratio by showing that the above greedy algorithm is the derandomization of random assignment using the method of conditional expectation .",
    "the centralized greedy algorithm is a @xmath0 approximation for the set k - cover problem .",
    "we would like to show that at each decision , the conditional expectation , given that decision , is greater than the expectation before being conditioned on that decision .",
    "suppose we are at the step where we are assigning subset @xmath8 .",
    "we want to assign @xmath8 to a cover such that the expected number of areas covered , conditioned on having assigned to cover @xmath19 , is maximized .",
    "more precisely , if we denote by @xmath79 the assignment of subset @xmath18 ( in iteration @xmath18 ) to cover @xmath19 and by @xmath80 all subset - cover assignments from previous rounds , we want to choose @xmath81 that maximizes @xmath82 $ ] . because we maximize at every step , by linearity of expectation , the conditional expectation can not decrease .",
    "therefore , at the end of the algorithm , we have an assignment for which the objective function is at least expected initial value  @xcite .",
    "the subset @xmath8 will only effect @xmath83 $ ] if it contains area @xmath20 so we will ignore vertices not in @xmath8 in our decision .",
    "suppose an area @xmath20 that is in subset @xmath8 is covered in exactly @xmath84 covers before the assignment of @xmath8 .",
    "then the expected number of times @xmath20 will be covered is @xmath21 = k - ( k - x)(1 - \\frac{1}{k})^{y_v}$ ] . regardless of where @xmath8 is placed , @xmath74 will decrease by @xmath85 .",
    "if @xmath20 is newly covered in some cover , @xmath84 will increase by @xmath85 , otherwise @xmath84 will remained unchanged .",
    "let us consider both scenarios :    1 .",
    "element @xmath20 is not newly covered by @xmath8 in the assignment @xmath79 .",
    "then , @xmath86 = k - ( k - x)(1 - \\frac{1}{k})^{y_v-1}\\ ] ] 2 .",
    "element @xmath20 _ is _ newly covered by @xmath8 in the assignment @xmath79 .",
    "then , + @xmath87{ll}\\\\ e[l_v|p_a \\wedge a_{ji}]&= k - ( k - x-1)(1 - \\frac{1}{k})^{y_v-1 } \\\\ & = k - ( k - x)(1 - \\frac{1}{k})^{y_v-1 } + ( 1 - \\frac{1}{k})^{y_v-1}\\\\ \\end{array } } } \\ ] ]    the component of the conditional expectation that our choice of assignment affects is whether or not an element falls into scenario i or ii . if it is in scenario ii , the profit is the last term of the above equation , @xmath88 .",
    "so we want to maximize @xmath89 .",
    "this results in the above greedy algorithm .",
    "we now have an algorithm that deterministically performs as well as the expected performance of the randomized algorithm .",
    "for specific cases , our algorithm is tight .",
    "in particular , set k - cover is a generalization of the e4-set splitting problem , and it is np - hard to design an approximation algorithm for e4-set splitting that performs better than our algorithm . we will first show a weaker statement , that the general case can not be approximated to better than @xmath4",
    ". we will begin with some necessary definitions .    in the e4-set splitting problem",
    "we are given a ground set @xmath90 and a number of sets @xmath91 each of size exactly @xmath92 .",
    "find a partition @xmath93 of @xmath90 to maximize the number of @xmath81 with both @xmath94 and @xmath95 nonempty .",
    "the hardness of approximation for e4-set splitting has been well studied , leading to the following result using pcp  @xcite .",
    "it is np - hard to distinguish between instances of max e4-set splitting where all the sets can be split by some partition and those where any partition splits at most a fraction @xmath96 of the sets , for any @xmath97 .",
    "we use the above definitions to show the hardness of set k - cover .",
    "it is np - complete to @xmath98-approximate the set k - cover problem with @xmath99 for any @xmath97 .    given an approximation algorithm @xmath100 for the set k - cover problem , we could use it to approximate e4-set splitting .",
    "suppose we would like to approximate an instance @xmath101 of the e4-set splitting problem .",
    "we can create an instance @xmath102 of the set 2-cover problem .",
    "for every variable of the ground set @xmath90 in @xmath101 , there is a subset in @xmath102 . for every set @xmath91 in @xmath101",
    ", there is an element in the set @xmath6 of @xmath102 .",
    "a subset in problem @xmath102 contains an element of @xmath6 iff the corresponding variable from @xmath90 belonged to the corresponding set @xmath103 .",
    "the proof is by contradiction .",
    "assume @xmath104 for some @xmath97 .",
    "case 1 : all the sets can be split in @xmath101 .",
    "then the optimum in @xmath102 is @xmath105 and we run algorithm @xmath100 on @xmath102 and are guaranteed to cover at least @xmath106 elements .",
    "case 2 : only a fraction @xmath96 of the sets can be split in @xmath101 .",
    "then the optimum in @xmath102 is less than @xmath107 , and any solution to @xmath102 will be less than this value .",
    "therefore , we could use @xmath100 to distinguish between instances of @xmath101 that can be split completely and instances where only a fraction @xmath96 of the sets can be split , which would contradict @xmath108 .",
    "in fact , after more precise analysis of the centralized greedy algorithm in the context of e4-set splitting , we see that the algorithm achieves an approximation ratio of exactly @xmath4 and is therefore tight .",
    "the centralized greedy algorithm is the best approximation algorithm possible for specific instances of the set k - cover problem .",
    "consider instances where the number of covers is @xmath109 and every area is contained in exactly @xmath92 subsets , implying @xmath110 @xmath31 . from the proof of @xmath111 , the approximation ratio @xmath31 is @xmath32}{\\min(k , n_v ) } = \\frac{(k - k(1 - \\frac{1}{k})^{n_v})}{k } = \\frac{15}{16}$ ] .",
    "the centralized greedy algorithm is therefore the best approximation possible when we constrain the parameters @xmath5 and @xmath22 .",
    "we performed simulations using all three algorithms .",
    "problem instances were generated by setting parameters @xmath112 ( number of areas ) , @xmath64 ( number of subsets ) , and @xmath12 ( number of edges ) .",
    "then a bipartite graph is created , where the edges are chosen uniformly at random from all possible subset - area pairs . a subset is then considered to contain an area if it has an edge connecting it with that area . for each set of parameters ,",
    "ten problem instances were generated and the numbers in the tables below are the average result over all ten instances .",
    "we chose this approach as opposed to an approach where areas are points in euclidean space and sensors sense within a radius of their location ( as in  @xcite ) because the latter limits the variety of applications .",
    "for instance , consider the sensors are embedded in vehicles , animals , or robots that are moving around in some physical space , then the set of problem instances are much richer and our test scenarios capture this richness of possible applications .",
    "simulations show that in practice , when compared to the optimum , our algorithms perform better than their worst case bounds .",
    "we bound the optimum by noting that the objective function of the optimum partition can not be larger than @xmath113 , since we can cover at most all the areas in all covers .",
    "we can also not hope to achieve more coverage than there are edges .",
    "thus we have two possible upper bounds for the optimum objective function that are listed in the column labeled opt bound .    [ cols= \" < , < , <",
    ", < , < , < \" , ]     simulations indicate that the deterministic greedy algorithm achieves performance that is on the order of 10 - 20% better than the randomized algorithm .",
    "the performance of the deterministic and centralized greedy solutions are strikingly close , differing by less than 1% in every instance of the problem that was tested .",
    "we see the randomized algorithm is consistent with theoretical analysis , with the worst performance achieving 63% coverage , which is quite close to the analysis of @xmath0 .",
    "both deterministic algorithms perform significantly above their worst case bounds , with the lowest ratio covering more than 72% of the maximum possible .",
    "many instances perform even higher , with four instances acheiving higher than 99% of the maximum possible .",
    "our simulations used the set k - cover algorithms to partition the sensors into @xmath5 covers such that when we rotate among the @xmath5 covers , more than @xmath114 of the areas are covered within the sliding window of k previous time steps .",
    "specifically , we maximize @xmath5 such that the total coverage is more than @xmath115 .",
    "since every set belongs to some cover , every area is covered at least once every @xmath5 time steps .",
    "the lifetime of our solution is compared with the straightforward approach of activating all the sensors every time step until the percent covereage over the previous @xmath5 time steps drops below @xmath114 .",
    "we assume that all sensors have the same amount of power initially , that their energy depletes at the same rate , and that they are all capable of lasting for several time steps . therefore ,",
    "if the set k - cover can achieve the specified goal of @xmath114 , then this signifies the lifetime of the network is more than @xmath116 times longer than the lifetime when the straightforward approach is used .",
    "because we only require information from @xmath114 of the nodes on average , this approach is most valuable for wsns where it is not necessary to collect information from all the data in every time step . in a wsn where network longevity is of primary importance",
    "this approach uses @xmath5 times less energy to collect the required information .",
    "our simulations try several values of @xmath5 , which is difficult to do in a distributed setting .",
    "however , it is possible to find a good value for @xmath5 in advance through simulations or mathematical properties of @xmath5 .",
    "wsn designers can choose @xmath5 such that , in expectation , the solution has the desired properties .",
    "alternatively , running simulations in advance allows designers to make a good choice for the value of @xmath5 ahead of time .",
    "= 280.0pt    our simulations show a significant increase in the lifetime of a network that uses the set k - cover solution . in figure",
    "[ f : nlr ] , the value of @xmath5 is plotted for problem instances with varying density .",
    "for all three algorithms , the increase in longevity is proportional to the amount of connectivity .",
    "this is expected , since a highly connected graph has more overlap and therefore more redundancy that the set k - cover approach can utilize to increase the lifetime of the network .",
    "this relationship between connectivity and energy savings is reflected in the simulation results .",
    "= 280.0pt    in addition to the clear benefits in energy savings , the covers produced by our algorithms have the useful property that they result in coverage of an area that is positively correlated with the number of sensors covering that area .",
    "this means that if there is a particular area in need of more frequent monitoring , then multiple sensors could be deployed close together to bolster the monitoring capabilities in that area .",
    "for example , if we are monitoring traffic , we might want frequent coverage of a busy highway intersection , and have less need for vigilant sensor information about an empty country road .",
    "figure [ f : fair ] charts 200 elements for a single problem instance , with @xmath22 plotted along the domain and @xmath117 plotted along the range .",
    "the randomized algorithm was applied 100 times on the same problem instance and the results in figure [ f : fair ] are the average over all of these runs .",
    "the optimum equals @xmath118 because an area can not belong to more covers than the number of subsets containing it . in the distributed and centralized greedy algorithms",
    ", no @xmath46 has a value that is less than 50% of the optimum @xmath46 it could possibly obtain . in the randomized algorithm , the worst ratio occurs when k = 10 and the ratio is @xmath119 in accord with theoretical analysis .",
    "we see that on average , the @xmath46 values are within 70% to 80% of the optimum .",
    "these simulations suggest that the algorithms are fair in that every area receives coverage relative to the number of sets that cover the area .",
    "= 280.0pt    another convenient property of the greedy algorithms is that for all covers in a given solution , the number of areas covered by each cover lies within a small range .",
    "thus , we could use the set k - cover partitions if we had the requirement that _ every _ cover monitor at least 80% of the areas . in figure",
    "[ f : coverrange ] we graph the size of the minimum cover divided by the size of the maximum cover , over several problem parameters .",
    "we see that for the distributed and centralized greedy algorithms , the smallest cover is always at least 60% of the largest cover .",
    "when there are many covers ( as in the problems with @xmath120 ) , the ratio decreases slightly since it is more likely to have outliers when the group is larger .",
    "when there are many covers in the randomized algorithm , however , there are a few covers with no areas at all . as the number of covers increases , the probability",
    "there will be a cover with little or no areas becomes larger , leading to the fast dropoff we observe in figure [ f : coverrange ] .",
    "therefore , the distributed and centralized greedy algorithms are better suited for applications that require covers that lie within a close range of coverage .",
    "it is an interesting area of further research to determine whether the centralized greedy algorithm can be efficiently implemented in a distributed fashion .",
    "the main challenge in making this algorithm distributed is that it is not clear where the @xmath74 values that determine the weights should be stored and how their values are to be updated in every round .",
    "one possible solution is to run the preprocessing phase between every sensor assignment , but this significantly increases the communication overhead .    from a theoretical perspective",
    ", this work raises the question of whether the centralized greedy algorithm is tight for the general case when @xmath22 are non - uniform and @xmath121 .",
    "perhaps the recent breakthroughs in lower bound results using pcp  @xcite  @xcite can be applied to the set k - cover problem .",
    "another open area of further study is the design of approximation algorithms for fair versions of the problem .",
    "the approach in  @xcite is to design an algorithm that maximizes @xmath5 , such that all areas are included in every cover .",
    "we examined a flipped variant of the problem in section ii , where , given a value of @xmath5 , the fewest number of times any element is covered is maximized . in the optimum , the second problem subsumes the first since , by doing a binary search on @xmath5 and choosing the largest @xmath5 for which @xmath122 , we have found the solution to the first problem .",
    "however , in a distributed sensor network environment , it is very difficult to try many possible values of @xmath5 .",
    "more work needs to be done to give a deeper understanding of the implications of using either method .",
    "a. howard , m. mataric , and g. sukhatme .",
    "relocation on a mesh : a formalism for generalized localization .",
    "_ ieee / rsj international conference on intelligent robots and systems ( iros ) _ , wailea , hawaii , oct . 2001 .",
    "v. guruswami .",
    "inapproximability results for set splitting and satisfiability problems with no mixed clauses . in _ proceedings of the 3rd international workshop on approximation algorithms for combinatorial optimization problems ( approx ) _",
    ", september 2000 .",
    "f. bian , a. goel , c. raghavendra , and x. li .",
    "energy - efficient broadcast in wireless ad hoc networks : lower bounds and algorithms . in _ journal of interconnection networks _",
    ", 3(3 - 4 ) , pp 149 - 166 , september 2002 .",
    "a. cerpa et al .. habitat monitoring : application driver for wireless communications technology . in _",
    "2001 acm sigcomm workshop on data communications in latin america and the caribbean _ , costa rica , april 2001 ."
  ],
  "abstract_text": [
    "<S> wireless sensor networks ( wsns ) are emerging as an effective means for environment monitoring . </S>",
    "<S> this paper investigates a strategy for energy efficient monitoring in wsns that partitions the sensors into covers , and then activates the covers iteratively in a round - robin fashion . </S>",
    "<S> this approach takes advantage of the overlap created when many sensors monitor a single area . </S>",
    "<S> our work builds upon previous work in @xcite , where the model is first formulated . </S>",
    "<S> we have designed three approximation algorithms for a variation of the set k - cover problem , where the objective is to partition the sensors into covers such that the number of covers that include an area , summed over all areas , is maximized . </S>",
    "<S> the first algorithm is randomized and partitions the sensors , in expectation , within a fraction @xmath0 ( @xmath1.63 ) of the optimum . </S>",
    "<S> we present two other deterministic approximation algorithms . </S>",
    "<S> one is a distributed greedy algorithm with a @xmath2 approximation ratio and the other is a centralized greedy algorithm with a @xmath3 approximation ratio . </S>",
    "<S> we show that it is np - complete to guarantee better than @xmath4 of the optimal coverage , indicating that all three algorithms perform well with respect to the best approximation algorithm possible . </S>",
    "<S> simulations indicate that in practice , the deterministic algorithms perform far above their worst case bounds , consistently covering more than 72% of what is covered by an optimum solution . </S>",
    "<S> simulations also indicate that the increase in longevity is proportional to the amount of overlap amongst the sensors . </S>",
    "<S> the algorithms are fast , easy to use , and according to simulations , significantly increase the longevity of sensor networks . </S>",
    "<S> the randomized algorithm in particular seems quite practical .    </S>"
  ]
}