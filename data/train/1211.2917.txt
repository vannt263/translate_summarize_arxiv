{
  "article_text": [
    "many statistical estimation problems are now formulated , implicitly or explicitly , as solutions of certain optimization problems .",
    "naturally , the parameters of these problems tend to be estimated from data and it is therefore important that we understand the relationship between the solutions of two types of optimization problems : those which use the population parameters and those which use the estimated parameters .",
    "this question is particularly relevant in high - dimensional inference where one suspects that the differences between the two solutions might be considerable .",
    "the aim of this paper is to contribute to this understanding by focusing on quadratic programs with linear constraints .",
    "an important example of such a program where our questions are very natural is the celebrated markowitz optimization problem in finance which will serve as a supporting example throughout the paper .",
    "the markowitz problem @xcite is a classic portfolio optimization problem in finance , where investors choose to invest according to the following framework : one picks assets in such a way that the portfolio guarantees a certain level of expected returns but minimizes the `` risk '' associated with them . in the standard framework ,",
    "this risk is measured the variance of the portfolio .",
    "markowitz s paper was highly influential and much work has followed .",
    "it is now part of the standard textbook literature on these issues [ @xcite , @xcite ] .",
    "let us recall the setup of the markowitz problem .",
    "* we have the opportunity to invest in @xmath0 assets , @xmath5 . * in the ideal situation , the mean returns are known and represented by a @xmath0-dimensional vector , @xmath6 . * also , the covariance between the returns is known ; we denote it by @xmath7 . *",
    "we want to create a portfolio , with guaranteed mean return @xmath8 , and minimize its risk , as measured by variance . *",
    "the question is how should items be weighted in portfolio ?",
    "what are weights  @xmath9 ?",
    "we note that @xmath7 is positive semi - definite and hence is in particular symmetric . in the ideal ( or population ) solution , the covariance and the mean are known .",
    "the mathematical formulation is then the following simple quadratic program .",
    "we wish to find the weights @xmath9 that solve the following problem : @xmath10 here @xmath11 is a @xmath0-dimensional vector with 1 in every entry .",
    "if @xmath7 is invertible , the solution is known explicitly ( see section [ sec : generalqp ] ) .",
    "if we call @xmath12 the solution of this problem , the curve @xmath13 , seen as a function of @xmath14 , is called the _ efficient frontier_.    of course , in practice , we do not know @xmath6 and @xmath7 and we need to estimate them .",
    "an interesting question is therefore to know what happens in the markowitz problem when we replace population quantities by corresponding estimators .",
    "naturally , we can ask a similar question for general quadratic programs with linear constraints [ see below or @xcite for a definition ] , the markowitz problem being a particular instance of such a problem .",
    "this paper provides an answer to these questions under certain distributional assumptions on the data .",
    "hence our paper is really about the impact of estimation error on certain high - dimensional @xmath15-estimation problems .",
    "it has been observed by many that there are problems in practice when replacing population quantities by standard estimators [ see @xcite , section  3.5 ] , and alternatives have been proposed .",
    "a famous one is the black  litterman model [ @xcite , @xcite and , e.g. , @xcite ] .",
    "adjustments to the standard estimators have also been proposed : @xcite , partly motivated by portfolio optimization problems , proposed to `` shrink '' the sample covariance matrix toward another positive definite matrix ( often the identity matrix properly scaled ) , while @xcite proposed to use the bootstrap and to average bootstrap weights to find better - behaved weights for the portfolio .",
    "as noted in @xcite , there is a dearth of theoretical studies regarding , in particular , the behavior of bootstrap estimators .",
    "an aspect of the problem that is of particular interest to us is the study of large - dimensional portfolios ( or quadratic programs with linear constraints ) . to make matters clear",
    ", we focus on a portfolio with @xmath16 assets .",
    "if we use a year of daily data to estimate @xmath17 , the covariance between the daily returns of the assets , we have @xmath18 observations at our disposal . in modern statistical parlance , we are therefore in a `` large @xmath1 , large @xmath0 '' setting , and we know from random matrix theory that @xmath19 the sample covariance matrix is a poor estimator of @xmath7 , especially when it comes to spectral properties of  @xmath7 .",
    "there is now a developing statistical literature on properties of sample covariance matrices when @xmath1 and @xmath0 are both large , and it is now understood that , though @xmath19 is unbiased for @xmath7 , the eigenvalues and eigenvectors of @xmath19 behave very differently from those of @xmath7 .",
    "we refer the interested reader to @xcite , @xcite ( @xcite ) , @xcite , @xcite for a partial introduction to these problems .",
    "we wish with this study to make clear that the `` large @xmath1 , large  @xmath0 '' character of the problem has an important impact of the empirical solution of the problem .",
    "by contrast , standard but thorough discussions of these problems @xcite give only a cursory treatment of dimensionality issues ( e.g. , one page out of a whole book ) .",
    "another interesting aspect of this problem is that the high - dimensional setting does not allow , by contrast to the classical `` small @xmath0 , large @xmath1 '' setting , a perturbative approach to go through . in the `` small @xmath0 , large @xmath1 '' setting ,",
    "the paper @xcite is concerned , in the gaussian case , with issues similar to the ones we will be investigating .",
    "the `` large @xmath1 , large @xmath0 '' setting is the one with which random matrix theory is concerned  and the high - dimensional markowitz problem has therefore been of interest to random matrix theorists for some time now .",
    "we note in particular the paper @xcite , where a random matrix - inspired ( shrinkage ) approach to improved estimation of the sample covariance matrix is proposed in the context of the markowitz problem .",
    "let us now remind the reader of some basic facts of random matrix theory that suggest that serious problems may arise if one solves naively the high - dimensional markowitz problem or other quadratic programs with linear equality constraints .",
    "a key result in random matrix theory is the marenko ",
    "pastur equation @xcite which characterizes the limiting distribution of the eigenvalues of the sample covariance matrix and relates it to the spectral distribution of the population covariance matrix .",
    "we give only in this introduction its simplest form and refer the reader to @xcite , @xcite , @xcite , @xcite and , for example , @xcite for a more thorough introduction and very recent developments , as well as potential geometric and statistical limitations of the models usually considered in random matrix theory .    in the simplest setting",
    ", we consider data @xmath20 , which are @xmath0-dimensional . in a financial context",
    ", these vectors would be vectors of ( log)-returns of assets , the portfolio consisting of @xmath0 assets .",
    "to simplify the exposition , let us assume that the @xmath21 s are i.i.d . with distribution @xmath22 .",
    "we call @xmath23 the @xmath24 matrix whose @xmath25th row is the vector @xmath21 .",
    "let us consider the sample covariance matrix @xmath26 where @xmath27 is a matrix whose rows are all equal to the column mean of @xmath23 .",
    "now let us call @xmath28 the spectral distribution of @xmath19 , that is , the probability distribution that puts mass @xmath29 at each of the @xmath0 eigenvalues of @xmath19 .",
    "a graphical representation of this probability distribution is naturally the histogram of eigenvalues of @xmath19 .",
    "a  consequence of the main result of the very profound paper @xcite is that @xmath28 , though a random measure , is asymptotically nonrandom , and its limit , in the sense of weak convergence of distributions , @xmath30 has a density ( when @xmath4 ) that can be computed .",
    "@xmath30  depends on @xmath31 in the following manner : if @xmath4 , the density of @xmath30 is @xmath32 where @xmath33 and @xmath34 .",
    "figure [ fig : illustrationmapalaw ] presents a graphical illustration of this result .",
    ", @xmath35 .",
    "the red curve is the density of the marenko  pastur - law for @xmath36 .",
    "the simulation was done with i.i.d .",
    "gaussian data .",
    "the histogram is the histogram of eigenvalues of @xmath37 . ]",
    "what is striking about this result is that it implies that the largest eigenvalue of @xmath7 , @xmath38 , will be overestimated by @xmath39 the largest eigenvalue of @xmath19 .",
    "also , the smallest eigenvalue of @xmath7 , @xmath40 , will be underestimated by the smallest eigenvalue of @xmath19 , @xmath41 . as a matter of fact , in the model described above , @xmath7 has all its eigenvalues equal to 1 , so @xmath42 , while @xmath39 will asymptotically be larger or equal to @xmath43 and @xmath41 smaller or equal to @xmath44 ( in the gaussian case and several others , @xmath39 and @xmath41 converge to those limits ) .",
    "we note that the result of @xcite is not limited to the case where @xmath7 is identity , as presented here , but holds for general covariance  @xmath7 ( @xmath28 has of course a different limit then ) .",
    "perhaps more concretely , let us consider a projection of the data along a vector  @xmath45 , with @xmath46 , where @xmath47 is the euclidian norm of @xmath45 .",
    "here it is clear that , if @xmath48 , @xmath49 , for all @xmath45 , since @xmath50 .",
    "however , if we do not know @xmath7 and estimate it by @xmath19 , a naive ( and wrong ) reasoning suggests that we can find direction of lower variance than 1 , namely those corresponding to eigenvectors of @xmath19 associated with eigenvalues that are less than 1 .",
    "in particular , if @xmath51 is the eigenvector associated with @xmath41 , the smallest eigenvalue of @xmath19 , by naively estimating , for @xmath23 independent of @xmath20 , the variance in the direction of @xmath51 , @xmath52 , by the empirical version @xmath53 , one would commit a severe mistake : the variance in any direction is 1 , but it would be estimated by something roughly equal to @xmath54 in the direction of @xmath51 .    in a portfolio optimization context , this suggests that by using standard estimators , such as the sample covariance matrix , when solving the high - dimensional markowitz problem , one might underestimate the variance of certain portfolios ( or `` optimal '' vectors of weights ) . as a matter of fact , in the previous toy example , thinking ( wrongly ) that there is low variance in the direction @xmath51 , one might ( numerically ) `` load '' this direction more than warranted , given that the true variance is the same in all directions .",
    "this simple argument suggests that severe problems might arise in the high - dimensional markowitz problem and other quadratic programs with linear constraints , and in particular , risk might be underestimated .",
    "while this heuristic argument is probably clear to specialists of random matrix theory , the problem had not been investigated at a mathematical level of rigor in that literature before this paper was submitted [ the paper @xcite has appeared while this paper was being refereed .",
    "it is concerned with different models than the ones we will be investigating and our results do not overlap ] .",
    "it has received some attention at a physical level of rigor [ see , e.g. , @xcite , where the authors treat only the gaussian case , and do not investigate the effect of the mean , which as we show below creates problems of its own ] . in this paper , we propose a theoretical analysis of the problem in a gaussian and elliptical framework for general quadratic programs with linear constraints , one of them involving the parameter  @xmath6 .",
    "our results and contributions are several - fold .",
    "we relate the empirical efficient frontier to the theoretical efficient frontier that is key to the markowitz theory , in a variety of theoretical settings .",
    "we show that the empirical frontier generally yields an underestimation of the risk of the portfolio and that gaussian analysis gives an over - optimistic view of this problem .",
    "we show that the expected returns of the naive `` optimal '' portfolio are poorly estimated by @xmath14 .",
    "we argue that the bootstrap will not solve the problems we are pointing out here . beside new formulas",
    ", we also provide robust estimators of the various quantities we are interested in .",
    "the paper is divided into four main parts and a conclusion . in section  [ sec : generalqp ] , to make the paper self - contained , we discuss the solution of quadratic problems with linear equality constraints  a focus of this paper . in section  [ sec : gaussiancase ] , we study the impact of parameter estimation on the solution of these problems when the observed data is i.i.d . gaussian and obtain some exact distributional results for fixed @xmath0 and @xmath1 . in section  [ sec :",
    "ellipticalcase ] , we obtain results in the case where the data is elliptically distributed .",
    "this allows us also to understand the impact of correlation between observations in the gaussian case and to get information about the behavior of the nonparametric bootstrap . in section  [ sec : comparisongaussianelliptical ] , we apply the results of section  [ sec : ellipticalcase ] to the quadratic programs at hand and compare the elliptical and the gaussian cases .",
    "we show , among other things , that the gaussian results are not robust in the class of elliptical distribution .",
    "in particular , two models may yield the same @xmath6 and @xmath7 but can have very different empirical behavior . in section  [ sec : comparisongaussianelliptical ] , we also propose various schemes to correct the problems we highlight ( see pages , and for pictures ) and study more general problems with linear constraints ( see section  [ subsec : inequalityconstraints ] ) .",
    "the conclusion summarizes our findings and the contains various facts and proofs that did not naturally flow in the main text or were better highlighted by being stated separately .    several times in the paper @xmath55 and @xmath56 will appear .",
    "unless otherwise noted , when taking the inverse of a population matrix , we implicitly assume that it exists .",
    "the question of existence of inverse of sample covariance matrices is well understood in the statistics literature .",
    "because our models will have a component with a continuous distribution , there are essentially no existence problems ( unless we explicitly mention and treat them ) as proofs similar to standard ones found in textbooks [ e.g. , @xcite ] would show . hence , we do not belabor this point any further in the rest of the paper as our focus is on things other than rather well - understood technical details , and the paper is already a bit long .    finally , let us mention that while the finance motivation for our study is important to us , we treat the problem in this paper as a high - dimensional @xmath15-estimation question ( which we think has practical relevance ) .",
    "we will not introduce particular modelization assumptions which might be relevant for practitioners of finance but might make the paper less relevant in other fields . a companion paper @xcite deals with more `` financial '' issues and the important question of the realized risk of portfolios that are `` plug - in '' solutions of the markowitz problem .",
    "we discuss here the properties of the solution of quadratic programs with linear equality constraints as they lay the foundations for our analysis of similar problems involving estimated parameters ( and of problems with inequality constraints ) .",
    "we included this section for the convenience of the reader to make the paper as self - contained as possible .",
    "the problem we want to solve is the following :    @xmath57    here @xmath7 is a positive definite matrix of size @xmath58 , @xmath59 and @xmath60 .",
    "we have the following theorem :    [ thm : solngeneralqp ] let us call @xmath61 the @xmath62 matrix whose @xmath25th column is @xmath63 , @xmath64 the  @xmath65 dimensional vector whose @xmath25th entry is @xmath66 and @xmath15 the @xmath67 matrix @xmath68 we assume that the @xmath63 s are such that @xmath15 is invertible .",
    "the solution of the quadratic program with linear equality constraints is achieved for @xmath69 and we have @xmath70    let us call @xmath71 a @xmath65 dimensional vector of lagrange multipliers .",
    "the lagrangian function is , in matrix notation , @xmath72 this is clearly a ( strictly ) convex function in @xmath9 , since @xmath7 is positive definite by assumption .",
    "we have @xmath73 so @xmath74 .",
    "now we know that @xmath75 .",
    "so @xmath76 .",
    "therefore , @xmath77 we deduce immediately that @xmath70    we now turn to another result which will prove to be useful later .",
    "it gives a compact representation of linear combinations of the weights of the optimal solution , and we will rely heavily on it in particular in the case of gaussian data .",
    "[ lemma : replinearcomboptimalweights ] let us consider @xmath12 the solution of the optimization problem .",
    "let @xmath78 be a vector in @xmath79 .",
    "let us call @xmath80 the @xmath81 matrix that is written in block form @xmath82    assume that @xmath80 is invertible .",
    "then @xmath83    the proof is a consequence of the results discussed in the concerning inverses of partitioned matrices [ see section [ subsec : classicallinearalgebra ] and equation there ] .",
    "let us write @xmath84 where @xmath85 is @xmath67 , @xmath86 is naturally @xmath87 and @xmath88 is a scalar . with the same block notation ,",
    "we have @xmath89 then , we know [ see equation ] that @xmath90 but since @xmath91 is a scalar , equal to @xmath92 , we have @xmath93 now @xmath94 , so @xmath95 .",
    "hence , @xmath96    we note that here @xmath97 , as an application of equation clearly shows .",
    "from now on , we will assume that we are in the high - dimensional setting where @xmath0 and @xmath1 go to infinity .",
    "our study will be divided into two .",
    "we will first consider the gaussian setting ( in this section ) and then study an elliptical distribution setting ( in section [ sec : ellipticalcase ] ) .",
    "( we note that for the markowitz problem , the assumption of gaussianity would be satisfied if we worked under black ",
    "scholes diffusion assumptions for our assets and were considering log - returns as our observations . )",
    "interestingly , we will show that the results are not robust against the assumption of gaussianity , which is not ( so ) surprising in light of recent random matrix results [ see @xcite ] .",
    "we will also show that understanding the elliptical setting allows us to understand the impact of correlation between observations and to discuss bootstrap - related ideas .",
    "in particular , we will see that various problems arise with the bootstrap in high - dimension and that the results change when one deals with observations that are correlated ( in time ) or not .",
    "we also address similar questions concerning inequality constrained problems in section  [ subsec : inequalityconstraints ] .",
    "before we proceed , we need to set up some notations : we call @xmath11 the @xmath0-dimensional vector whose entries are all equal to 1 .",
    "we call @xmath61 , as above , the matrix containing all of our constraint vectors , which we may have to estimate ( for instance , if @xmath98 for a certain @xmath25 ) .",
    "we call @xmath99 the matrix of estimated constraint vectors",
    ".    the template question for all our investigations will be the following ( markowitz ) question : what can be said of the statistical properties of the solution of @xmath100 compared to the solution of the population version @xmath101    we will solve the problem at a much greater degree of generality , by considering first quadratic programs with linear equality constraints ( see section  [ subsec : inequalityconstraints ] for inequality constraints ) and comparing the solutions of    @xmath102    and    @xmath103    here @xmath19 and @xmath104 will be estimated from the data .",
    "we call @xmath105 the vector that yields a solution of problem and @xmath106 the vector that yields a solution of problem .",
    "we call @xmath107 the @xmath62 matrix containing @xmath108 and @xmath104 , and @xmath61 its population counterpart , which contains @xmath109 and @xmath6 .",
    "we assume that @xmath108 are deterministic and known ( just like the vector @xmath11 in the markowitz problem ) . in our analysis , @xmath65 will be held fixed .",
    "( the @xmath65th column of @xmath107 will contain @xmath104 in general or our estimator of @xmath6 . )",
    "as should be clear from theorem  [ thm : solngeneralqp ] , the properties of the entries of the matrix @xmath110 as compared to those of the matrix @xmath111 will be key to our understanding of this question . in what follows ,",
    "we assume that the vectors @xmath112 are either deterministic or equal to @xmath104 .",
    "the extension to linear combinations of a deterministic vector and @xmath104 is straightforward .",
    "we also note that in the gaussian case , we could just assume that the @xmath112 are ( deterministic ) functions of @xmath104 ( because @xmath104 and @xmath19 are independent in this case ) . on the other hand ,",
    "the vector @xmath64 is assumed to be deterministic .",
    "before we proceed , let us mention that after our study was completed , we learned of similar results ( restricted to the markowitz case and not dealing with general quadratic programs with linear equality constraints ) by @xcite .",
    "we stress the fact that our work was independent of theirs and is more general which is why it is included in the paper .",
    "we first study questions concerning the efficient frontier and then turn to information we can get about linear functionals of the empirical weights .",
    "[ thm : galqpgaussiancase ] let us assume that we observe data @xmath113 , for @xmath114 . here @xmath7 is @xmath58 and @xmath4 .",
    "suppose we estimate @xmath7 with the sample covariance matrix @xmath19 , and @xmath6 with the sample mean @xmath104 .",
    "suppose we wish to solve the problem    @xmath115    where @xmath116 are deterministic , @xmath117 are deterministic and given for @xmath118 and @xmath119 .",
    "assume that we use as a proxy for the previous problem the empirical version with plugged - in parameters .",
    "let us consider the solution of the problem    @xmath120    now @xmath121 for @xmath118 and @xmath122 , for a given deterministic function @xmath123 .",
    "let us call @xmath105 the corresponding `` weight '' vector .",
    "the plug - in estimate of @xmath124 is @xmath125 .",
    "let us call @xmath126 the optimal solution of the quadratic program obtained under the assumption that @xmath7 is given , but @xmath6 is not and is estimated by @xmath127 .",
    "finally , we assume that @xmath128",
    ".    then we have    @xmath129 where @xmath130 is random ( because @xmath104 is ) but is statistically independent of @xmath131 .",
    "also , @xmath132    the previous theorem means that the cost of not knowing the covariance matrix and estimating it is the apparition of the @xmath133 . in the high - dimensional setting when @xmath0 and @xmath1 are of the same order of magnitude and @xmath3 is large , this terms is approximately @xmath134 .",
    "hence , the theorem quantifies the random matrix intuition that having to estimate the high - dimensional covariance matrix at stake here leads to risk _ underestimation _",
    ", by the factor @xmath134 .",
    "in other words , using plug - in procedures leads to over - optimistic conclusions in this situation .",
    "we also note that the previous theorem shows that , in the gaussian setting under study here , the effect of estimating the mean and the covariance on the solution of the quadratic program are `` separable '' : the effect of the mean estimation is in the oracle term , while the effect of estimating the covariance is in the @xmath135 term . to show risk underestimation",
    ", it will therefore be necessary to relate @xmath130 to @xmath136 .",
    "we do it in proposition [ prop : riskunderestimationbymeanestimation ] but first give a proof of theorem  [ thm : galqpgaussiancase ] .",
    "proof of theorem  [ thm : galqpgaussiancase ] the crux of the proof is the following result , which is well known by statisticians , concerning ( essentially ) blocks of the inverse of a wishart matrix : if @xmath137 , that is , @xmath138 is a @xmath58 wishart matrix with @xmath139 degree of freedoms and covariance @xmath7 , and @xmath140 is @xmath62 , deterministic matrix , then , when @xmath141 , @xmath142 we refer to @xcite [ ( @xcite ) , proposition 8.9 , page 312 ] for a proof , and to @xcite [ ( @xcite ) , pages 7073 ] for related results .",
    "another important remark is the well - known fact that , in the situation we are considering , @xmath104 is @xmath143 and independent of @xmath19 .",
    "finally , it is also well known that if @xmath137 and @xmath64 is a @xmath0-dimensional deterministic vector , then @xmath144 .    now @xmath145 .",
    "therefore , since @xmath99 is a function of @xmath104 , we have , by independence of @xmath104 and @xmath19 , @xmath146 therefore , @xmath147 because the right - hand side does not depend on @xmath104 , we have established the independence of @xmath148 hence , we conclude that @xmath149 and the two terms are independent . now",
    "the term @xmath150 is the estimate we would get for the solution of problem , if @xmath7 were known and @xmath6 were estimated by @xmath127 . in other words , it is the `` oracle '' solution described above .",
    "theorem  [ thm : galqpgaussiancase ] sheds light on the separate effects of mean and covariance estimation on the problem considered above . to understand further the problem of risk estimation",
    ", we need to better understand the role the estimation of the mean might play .",
    "this is what we do now .",
    "[ prop : riskunderestimationbymeanestimation ] suppose that the last column of @xmath107 is @xmath104 .",
    "let us call @xmath151 the @xmath152 dimensional matrix whose @xmath153th column is @xmath117 , which are known deterministic vectors .",
    "suppose that @xmath154 .",
    "suppose further that @xmath155 , where @xmath156 is the smallest eigenvalue of the @xmath67 matrix  @xmath138 .",
    "further , call @xmath157 and call @xmath158 the canonical basis vectors in @xmath159 . finally , call @xmath160 .    then , when @xmath161 , asymptotically , @xmath162    let us discuss a little bit this result before we provide a proof .",
    "in the asymptotics we have in mind and are considering , @xmath163 and therefore @xmath164 .",
    "so if @xmath165 , when the above analysis applies , the impact of the estimation of @xmath6 by @xmath104 will be risk underestimation , just as is the case for the case of the covariance matrix . here , we can also quantify the impact of this estimation of @xmath6 by @xmath104 : it leads to risk underestimation by the amount @xmath166 .",
    "proof of proposition  [ prop : riskunderestimationbymeanestimation ] let us write @xmath167 , where @xmath168 . clearly , @xmath169 , where @xmath170 is @xmath22 .",
    "we have , using block notations , @xmath171 replacing @xmath172 by its value , we have @xmath173 . by the same token",
    ", we can also get that @xmath174 our assumption that @xmath175 implies that @xmath176 and @xmath177 .",
    "therefore , @xmath178 hence , since @xmath179 , @xmath180 our assumptions guarantee that @xmath181 , and therefore@xmath182 . in other respects ,",
    "let @xmath140 be a matrix such that @xmath183 and @xmath184 be a matrix such that @xmath185 .",
    "recall that for symmetric matrices , @xmath186 [ see , e.g. , weyl s theorem , @xcite , page 185 ] .",
    "so in this situation , @xmath187 .",
    "let us now consider the implications of this remark on the difference of @xmath188 and @xmath189 .",
    "we claim that @xmath190 . by the first resolvent identity , @xmath191 ; our previous remark implies that @xmath192={\\mathrm{o}}(1)$ ] and the result follows .",
    "applying the results of this discussion to @xmath193 and @xmath194 , we have @xmath195 we can now use well - known results concerning inverses of rank-1 perturbation of matrices , namely @xmath196 this allows us to conclude that @xmath197 this is the result announced in the theorem and the proof is complete .    we can now combine the results of theorem  [ thm : galqpgaussiancase ] and proposition  [ prop : riskunderestimationbymeanestimation ] to obtain the following corollary .    [",
    "coro : gaussiancasefinalapproxefffrontier ] we assume that the assumptions of theorem  [ thm : galqpgaussiancase ] and proposition  [ prop : riskunderestimationbymeanestimation ] hold and that @xmath198 has a finite nonzero limit , as @xmath199 , and @xmath3 tends to infinity .",
    "then we have    @xmath200 \\\\[-8pt ] & & { } + { \\mathrm{o}}_p ( { w_{\\mathrm{theo}}}{'}\\sigma{w_{\\mathrm{theo}}}\\vee n^{-1/2 } ) , \\nonumber\\end{aligned}\\ ] ] where @xmath15 is the population quantity @xmath201 .",
    "the corollary shows that the effects of both covariance and mean estimation are to underestimate the risk , and the empirical frontier is asymptotically deterministic .",
    "our matrix characterization of the empirical optimal weights ( lemma [ lemma : replinearcomboptimalweights ] ) allows us to give a precise characterization of the statistical properties of linear functionals of these weights . we give here some exact results , concerning distributions and expectations of those functionals .",
    "a longer discussion , including robustness and more detailed bias issues can be found in section  [ sec : comparisongaussianelliptical ] .",
    "[ prop : exactresultsweightsgaussiancase ] assume that the assumptions of theorem  [ thm : galqpgaussiancase ] hold and in particular @xmath21 are i.i.d .",
    "@xmath202 . let @xmath78 be a fixed @xmath1-dimensional vector .",
    "let us call @xmath203 the @xmath204 matrix whose first @xmath65 columns are those of @xmath107 .",
    "let @xmath205 and @xmath206 be a @xmath81 matrix with distribution @xmath207 ( conditional on @xmath104 ) .",
    "then , @xmath208 in particular , @xmath209    we note , somewhat heuristically , that when @xmath6 is estimated by @xmath104 , since @xmath210 , @xmath211 , when @xmath0 , @xmath1 and @xmath3 are all large ( we refer again to section [ sec : comparisongaussianelliptical ] for a more precise statement )",
    ". hence @xmath212 is a not a consistent estimator of @xmath213 . as we will see in section  [ sec : gaussellip : subsec : weightsportfolio ] and as can be expected from the previous proposition , this will also imply bias for linear combinations of empirical optimal weights",
    ". we will show in particular that returns are overestimated when using @xmath104 as an estimator for  @xmath6 .",
    "another interesting aspect of the previous proposition is that it allows us to understand the fluctuation behavior of @xmath214 when @xmath215 is large : as a matter of fact , the limiting fluctuation behavior of the entries of a ( fixed - dimensional ) wishart matrix with large number of degrees of freedom is well known [ see , e.g. , @xcite , theorem 3.4.4 , page 87 ] and the @xmath216-method can be applied to get the information ",
    "conditional on @xmath104 .",
    "for instance , if we assume that , conditional on @xmath104 , the matrix @xmath212 converges to a matrix @xmath217 , which possibly depends on @xmath104 , we see that calling @xmath218 the last column @xmath219 , @xmath218 is asymptotically normal ( all statements are conditional on @xmath104 ) , if @xmath215 goes to infinity when @xmath0 and @xmath1 go to infinity .",
    "furthermore we know the limiting covariance of @xmath218 ( after scaling by @xmath220 ) , using theorem 3.4.4 in @xcite .",
    "let us call it @xmath221 and let us call @xmath222 the limit of @xmath218which we assume exists .",
    "if we assume that @xmath223 is not 0 , slutsky s lemma and the @xmath216-method give us through simple computations that @xmath224 where @xmath225 .",
    "we know the distribution of @xmath104 , so we could get ( limiting ) unconditional results for @xmath214 .",
    "this is not hard but a bit tedious if we want explicit expressions , and because our focus is mostly on first - order properties in this paper , we do not state the result .",
    "proof of proposition [ prop : exactresultsweightsgaussiancase ] the proof follows from the representation we gave in lemma [ lemma : replinearcomboptimalweights ] , that is , @xmath226 and the fact that , by the same arguments as before , conditional on @xmath104 , @xmath227 we conclude that @xmath228 this shows the fist part of the proposition .",
    "the second part follows from the following observation .",
    "suppose the matrix @xmath229 is @xmath230 .",
    "if @xmath231 and @xmath232 are @xmath1-dimensional , orthogonal vectors , let us consider @xmath233 we can , of course , write @xmath234 , where @xmath235 are i.i.d .",
    "@xmath22 . in other respects , @xmath236 and @xmath237 are clearly independent normal random variables , since their covariance is @xmath238 , and they are normal .",
    "so @xmath239 because the quantity whose expectation we are taking is a linear combination of mean 0 independent normal random variables .",
    "hence , also , @xmath240 now , when @xmath231 is not orthogonal to @xmath232 , we write @xmath241 , where @xmath242 is orthogonal to @xmath232 .",
    "we immediately deduce that in general , @xmath243 furthermore , when @xmath229 is @xmath244 , because we can write @xmath245 , where @xmath246 , we finally have @xmath247    in the case of interest to us , we have @xmath248 , @xmath249 and @xmath250 .",
    "applying the previous formula gives us the second part of the proposition .",
    "we now turn to the question of understanding the robustness properties of the gaussian results we just obtained .",
    "we will do so by studying the same problems under more general distributional assumptions , and specifically we will now assume that the observations are elliptically distributed .",
    "in section  [ sec : gaussiancase ] , we studied the properties of the `` plug - in '' solution of problem under the assumption that the data was normally distributed .",
    "while this allowed us to shed light on the statistical properties of the solution of problem , it is naturally extremely important to understand how robust the results are to our normality assumptions .    in this section",
    ", we will consider elliptical models , that is , models such that the data can be expressed as @xmath251 where @xmath252 is a random variable and @xmath235 are i.i.d . @xmath22 entries . @xmath252 and",
    "@xmath235 are assumed to be independent , and to lift the indeterminacy between @xmath7 and @xmath71 , we assume that @xmath253 . under this assumption , we clearly have @xmath254 .",
    "we note that this is not the standard definition of elliptical models , which generally replaces @xmath235 with a vector uniformly distributed on the sphere in @xmath79 , but it captures the essence of the problem .",
    "we refer the interested reader to @xcite and @xcite for extensive discussions of elliptical distributions .",
    "our motivation for undertaking this study comes also from the fact that for certain types of data , such as financial data , it is sometimes argued that elliptical models are more reasonable than gaussian ones , for instance , because they can capture nontrivial tail dependence [ see @xcite where such models are advocated for high - dimensional modelization of financial returns , @xcite for a discussion of their relevance for certain financial markets , @xcite for modelization considerations quite similar to @xcite and @xcite for a thorough discussion of tail dependence ] . from a theoretical standpoint , considering elliptical models will also help in several other ways : the results will yield alternative proofs to some of the results we obtained in the gaussian case , they will allow us to deal with some situations where the data @xmath21 are not independent and they will also allow us to understand the properties of the bootstrap .",
    "we also want to point out that elliptical distributions allow us to not fall into the geometric `` trap '' of standard random matrix models highlighted in @xcite : the fact that data vectors drawn from standard random matrix models are essentially assumed to be almost orthogonal to one another and that their norm ( after renormalization by @xmath255 ) is almost constant . in a sense , studying elliptical models will allow us to understand what is the impact of the implicit geometric assumptions made about the data when assuming normality .",
    "( we purposely do so not under minimal assumptions but under assumptions that capture the essence of the problem while allowing us to show in the proofs the key stochastic phenomena at play . )",
    "this part of the article can therefore be viewed as a continuation of the investigation we started in @xcite where we showed a lack of robustness of random matrix models ( contradicting claims of `` universality '' ) by thoroughly investigating limiting spectral distribution properties of high - dimensional covariance matrices when the data is drawn according to elliptical models and generalizations .",
    "we show here that the theoretical problems we highlighted in @xcite have important practical consequences .",
    "[ for more references on elliptical models in a random matrix context , we refer the reader to @xcite where an extended bibliography can be found . ]",
    "we now turn to the problem of understanding the solution of problem in the setting where the data is elliptically distributed .",
    "we will limit ourselves to the case where the matrix @xmath107 is full of known and deterministic vectors , except possibly for the sample mean . in this section",
    "we restrict ourselves to convergence in probability results .",
    "it is clear from section [ sec : generalqp ] that to tackle the problems we are considering we need to understand at least three types of quantities : @xmath256 for a deterministic @xmath45 with unit norm , @xmath257 and @xmath258 .",
    "here is a brief overview of our findings .",
    "when we consider elliptical models , our results say that roughly speaking , under certain assumptions given precisely later :    1 .",
    "@xmath259 , where @xmath260 satisfies , if @xmath261 is the limit law of the empirical distribution of the @xmath262 and @xmath161 , @xmath263 .",
    "2 .   if @xmath264 , @xmath265 .",
    "3 .   if @xmath264 , @xmath266 .",
    "all these convergence results are to be understood in probability . they naturally allow us  under certain conditions on the population parameters  to conclude about the convergence in probability of the matrix @xmath267 .",
    "the results mentioned above are stated in all details in theorems [ thm : quadformsinverseellipticalcase ] and [ thm : summaryquadformsmuhatandsigmahat ] .",
    "in the situation where @xmath252 are i.i.d . , the results above hold when @xmath252 have a second moment and they do not put too much mass near 0 . this is interesting in practice because it tells us that our results hold for heavy - tailed data , which are of particular interest in some financial applications .",
    "the bootstrap situation corresponds basically to @xmath261 being poisson(1 ) , which we denote by @xmath268 .",
    "also in the statement above for @xmath269 , one should replace @xmath270 by @xmath271 in the bootstrap case .",
    "this is explained in theorem [ thm : bootquadformsinverseellipticalcasenormalcase ] and section [ subsubsec : bootresquadformsmuhatsigmahat ] .",
    "finally , in the case of gaussian data with `` temporal '' correlation , that is , when the data can be written in matrix form @xmath272 , where @xmath273 is not diagonal ( and @xmath274 is an @xmath1-dimensional vector with only 1 s in its entries ) , one should replace @xmath261 by the limiting spectral distribution of @xmath275 .",
    "the question of convergence of @xmath258 is then more involved .",
    "we refer to proposition [ proposition : quadformsmuhatsigmainversecorrcase ] for details about this situation .",
    "though we are taking a fundamentally random matrix theoretic approach , our presentation purposely avoids borrowing too many techniques from random matrix theory in the hope of making clear(er ) the phenomena that yield the results we will obtain .",
    "a more general but considerably more technically complicated ( for non - specialists of random matrix theory ) approach is being developed in our study of a connected problem and will appear in another paper .",
    "this section is divided into four subsections .",
    "the first two are devoted to the main technical issues arising in the study of the problem when the data is elliptically distributed .",
    "the third discusses the impact of correlation between observations when the data is gaussian , as it can be recast as a variant of elliptical problems .",
    "the last subsection discusses questions related to the ( nonparametric ) bootstrap .",
    "the focus of this subsection is on understanding statistics of the type @xmath256 , where @xmath45 is a deterministic vector .",
    "we will prove the following important theorem .",
    "[ thm : quadformsinverseellipticalcase ] suppose we observe @xmath1 observations @xmath21 , where @xmath21 has the form @xmath276 , with @xmath277 and @xmath278 is independent of @xmath279 .",
    "@xmath280 is deterministic and @xmath253 .",
    "we call @xmath281 and assume that @xmath282 .",
    "we use the notation @xmath283 and assume that the empirical distribution , @xmath284 , of @xmath285 converges weakly in probability to a deterministic limit @xmath261 . we also assume that @xmath286 for all @xmath25 .",
    "if @xmath287 is the @xmath25th largest @xmath288 , we assume that we can find a random variable @xmath289 and positive real numbers @xmath290 and @xmath291 such that    @xmath292    under these assumptions , if @xmath45 is a ( sequence of ) deterministic vector , @xmath293 where @xmath260 satisfies    @xmath294    a few comments are in order before we turn to the proof .",
    "first , the assumption that @xmath295 for all @xmath25 could be dispensed of , as long as all assumptions stated above hold when @xmath1 is understood to denote the number of nonzero @xmath252 s .",
    "second , concerning @xmath296 and @xmath297 will generally hold as soon as @xmath261 does not put too much mass at 0 , the only problem - specific question remaining being how much mass is put at 0 by @xmath261 compared to @xmath298 , the limit of @xmath198 .    in particular , in the case where the @xmath285 s are i.i.d .",
    ", if there exists @xmath299 and @xmath300 such that @xmath301 , and if @xmath284 is the empirical distribution of the @xmath285 s , if @xmath302 , we see , using , for example , lemma 2.2 in @xcite , that @xmath303 so picking @xmath304 will guarantee that we have , if @xmath302 in probability , @xmath305 and , of course , @xmath306 . hence ,",
    "in checking whether the theorem applies , we just need to see whether @xmath307 stays bounded away from 1 .    in the simpler case",
    "when all the @xmath308 are bounded away from 0 , the conditions on @xmath296 and @xmath297 apply directly by taking @xmath309 .",
    "finally , let us say that is needed in the proof to guarantee that the smallest eigenvalues of @xmath19 stay bounded away from 0 with high - probability .",
    "we now briefly compare the gaussian and elliptical cases . a simple convexity argument",
    "[ relying on the fact that @xmath310 is a convex function of @xmath311 for @xmath312 and jensen s inequality ] shows that , if @xmath313 is the mean of @xmath261 , @xmath314 in the case of gaussian data , @xmath315 , that is , it is a point mass at 1 and we have @xmath316 . in other respects , for @xmath21 to have covariance @xmath7 , we need @xmath317 . when the @xmath252 s are i.i.d . , with @xmath262 having distribution @xmath261 , @xmath318 ,",
    "and we know that @xmath302 in probability",
    ". therefore , in the class of elliptical distributions considered here , risk underestimation , which is essentially measured by @xmath319 ( see theorem [ thm : solngeneralqp ] and section [ sec : comparisongaussianelliptical ] ) will be least severe in the gaussian case . in other words , the gaussian results lead to over - optimistic conclusions ( in terms of proximity between sample and population solutions of the quadratic programs we are considering ) within the class of elliptical distributions .",
    "we go back to these questions in more detail in section [ sec : comparisongaussianelliptical ] and now turn to the proof of theorem [ thm : quadformsinverseellipticalcase ] . the proof could be carried out in at least two ways .",
    "we take one that is not standard but we feel best explains the phenomenon that is occurring .",
    "proof of theorem [ thm : quadformsinverseellipticalcase ] the proof is easier to carry out when we write the problem in matrix form .",
    "because we focus on @xmath19 , we can assume without loss of generality ( wlog ) that @xmath264 .",
    "let us consider the @xmath24 data matrix @xmath23 whose @xmath25th row is @xmath21 .",
    "similarly , we denote by @xmath320 the @xmath24 data matrix whose @xmath25th row is @xmath235 .",
    "let us call @xmath321 the diagonal matrix with @xmath25th diagonal entry @xmath252 and @xmath322 , where @xmath11 is an @xmath1-dimensional vector whose entries are all equal to 1 .",
    "note that @xmath323 . with these notations",
    ", we have , since we assume that @xmath264 , @xmath324 therefore , @xmath325 , and @xmath326 let us call @xmath327 the matrix @xmath328 .",
    "note that @xmath329 is a rank @xmath0 matrix with probability 1 , if we assume that @xmath330 ( recall that all the entries of @xmath273 are nonzero ) .",
    "hence , @xmath331 is invertible with probability 1",
    ". therefore , @xmath332 finally , we have @xmath333 where @xmath334 is a vector of @xmath335 norm 1 .",
    "we now make all of our statements conditional on @xmath273 . because of the independence of @xmath320 and @xmath273",
    ", we can therefore treat the @xmath252 s as if they were constant and the @xmath336 s as i.i.d .",
    "@xmath337 random variables .",
    "@xmath273 is now assumed to be in the set of matrices @xmath338 , defined just below , for which we have control of the smallest eigenvalue of @xmath339 .",
    "in the steps that follow that are conditional on @xmath273 , we therefore consider that we control the smallest eigenvalue of @xmath340 .",
    "we note that if @xmath273 is in @xmath338 , @xmath296 is lower bounded . because @xmath296 is a function of the @xmath252 s and hence of @xmath273 , we write all the results conditionally on @xmath321 , but the reader should keep in mind that this conditioning constrains also the possible values of @xmath296 .",
    "@xmath341 _ the set @xmath338_.    in lemma  [ lemma : lowerboundonsmallesteig ] in the , we prove the following result : when @xmath273 is such that @xmath342 , if @xmath343 [ see and lemma  [ lemma : lowerboundonsmallesteig ] for definitions ] and @xmath344 is the smallest eigenvalue of @xmath345 , we have , if @xmath346 denotes probability conditional on @xmath273 , @xmath347 \\bigr)\\leq\\exp\\bigl(-(n-1)t^2 \\bigr ) .\\ ] ]    let us call @xmath338 the set of matrices @xmath273 such that @xmath342 and @xmath348 .",
    "under , for a @xmath216 bounded away from 0 ( e.g. , @xmath349 , since we need a bound on @xmath350 that holds with probability going to 1 ) , @xmath351 . in other respects , if @xmath352 , @xmath353 \\bigr)\\leq\\exp\\bigl(-(n-1)\\delta t^2/c_0",
    "\\bigr ) .\\ ] ]    @xmath341 _ getting results conditionally on @xmath354_.    if @xmath355 is an orthogonal matrix , @xmath356 , because @xmath320 is full of i.i.d .",
    "@xmath337 random variables and is therefore invariant ( in law ) by left and right rotation . therefore the eigenvalues and eigenvectors of @xmath331 are independent and its matrix of eigenvectors is uniformly ( i.e. , haar ) distributed on the orthogonal group [ see also @xcite , page 40 , equation ( 2.4.4 ) ] .",
    "let us write a spectral decomposition of @xmath331 @xmath357 we know that a.s .",
    "@xmath358 for all @xmath25 , so @xmath359 we claim that @xmath360 to see this , note that @xmath361 because @xmath362 is uniformly distributed on the unit sphere when @xmath363 ( the matrix containing the @xmath362 ) is haar distributed on the orthogonal group . hence , given the independence between @xmath364 and @xmath362 , @xmath365 now let us call @xmath9 the vector with @xmath366 , and @xmath367 the vector with @xmath25th entry @xmath368 . clearly , since @xmath369 , @xmath370 . by symmetry",
    "it is clear that @xmath371 and @xmath372 if @xmath373 .",
    "further , since the matrix @xmath363 containing the vectors @xmath362 is haar distributed on the orthogonal group , we can assume without loss of generality that @xmath374 for all the computations at stake .",
    "as a matter of fact , if @xmath375 is an orthogonal matrix such that @xmath376 , then @xmath377 where the matrix @xmath378 is again haar distributed on the orthogonal group .",
    "so from now on , we assume ( without loss of generality ) that @xmath374 , and we therefore simply need to understand the correlation between @xmath379 and @xmath380 .",
    "now , the first row of an orthogonal matrix uniformly distributed on the orthogonal group is a unit vector uniformly distributed on the unit sphere , because if @xmath355 is haar distributed , so is @xmath381 .",
    "we now recall the fact that a vector uniformly distributed on the unit sphere , @xmath382 can be generated by drawing at random a @xmath22 random vector and normalizing it . in other words , if @xmath383 , @xmath384 .",
    "so our task has now been considerably simplified , and it consists in understanding the covariance between 2 random variables , @xmath385 and @xmath386 such that , if @xmath387 are i.i.d .",
    "@xmath337 , @xmath388 now , by symmetry , @xmath389 for all @xmath373 and @xmath390 .",
    "in other words , @xmath391 we can therefore conclude that @xmath392 hence , @xmath393 . on the other hand , @xmath394 since @xmath395 , and @xmath396 , for @xmath397 [ see , e.g. , @xcite , page 487 ] . applying these results with @xmath398 yields the above result as soon as @xmath399 , by using the fact that @xmath400 .",
    "we therefore have @xmath401 since , for instance by symmetry , @xmath402 , and @xmath403 , we conclude that @xmath404 we have therefore established the fact that @xmath405    on the other hand , since @xmath406 , we have @xmath407 now using the ( standard ) fact that , for symmetric matrices @xmath15 , if @xmath408 is the largest singular value of @xmath15 , @xmath409 [ it can easily be proved using , for instance , theorems 5.6.6 and 5.6.9 in @xcite , or gergorin s theorem ( theorem 6.1.1 in the same reference ) ] we have @xmath410 the first term in the previous bound comes from the contribution of the diagonal and the second term is the sum over the @xmath411 off - diagonal elements on a given row of the upper - bound we had on each such element , that is , @xmath412 for some @xmath297 .",
    "let us now return to our initial question which was to show that the conditional variance of interest to us was going to zero .",
    "recall that @xmath367 is a vector whose @xmath25th entry is @xmath413 . since @xmath414 and @xmath415",
    ", we have , for @xmath297 a constant , and if @xmath416 denotes the operator norm ( or largest singular value ) of the matrix @xmath140 , @xmath417 now given the assumptions we made on @xmath273 , according to the arguments given at the beginning of this proof and lemma [ lemma : lowerboundonsmallesteig ] in the , @xmath418 , where @xmath419 , with high ( @xmath279)-probability .",
    "so we conclude that all the @xmath364 s are bounded away [ uniformly for @xmath273 in @xmath338 and with high ( @xmath420)-probability ] from 0 , and when this is the case , @xmath421 therefore , @xmath422 let us now show that this implies convergence in probability to 0 ( conditional on @xmath273 only ) of @xmath423 .",
    "let us call @xmath424 .",
    "for @xmath425 to be determined later , we have @xmath426 on the other hand , @xmath427 because @xmath428 is a function of the @xmath429 s and @xmath430 , @xmath431 but when @xmath352 , under our assumptions and their consequences on the @xmath432 s mentioned above [ i.e. , @xmath433 with high @xmath434 probability ] , we have @xmath435 , so taking @xmath436 , we have @xmath437 and of course , @xmath438 .",
    "hence , for any @xmath439 , @xmath440    let us now turn to the question of identifying the limit .",
    "@xmath341 _ about @xmath441_.    the stieltjes transform of the spectral distribution of @xmath442 is @xmath443 the quantity @xmath444 is therefore @xmath445 and we are interested in its limit , if it exists , which would correspond to @xmath260 .    recall the marenko ",
    "pastur equation , from @xcite , @xcite and @xcite : if @xmath320 is @xmath24 has i.i.d .",
    "entries with mean 0 and variance 1 and @xmath327 is positive semidefinite , has limiting spectral distribution @xmath261 and is independent of @xmath320 , if @xmath446 , and if @xmath447 is the stieltjes transform of the spectral distribution of @xmath448 , then @xmath449 tends ( in probability ) to @xmath450 for all @xmath451 in @xmath452 and @xmath139 satisfies    @xmath453 note that , if @xmath454 , we have @xmath455    therefore , according to @xcite , @xcite and @xcite , we know that @xmath456 converges for @xmath457 to a nonrandom quantity @xmath458 , in probability . note that @xmath459 satisfies , in light of equation , @xmath460    here , because we know using our assumptions ( see the end of the proof ) that @xmath364 are bounded away from 0 with probability going to 1 , we can also conclude that @xmath461 with probability going to 1 , because of the weak convergence ( in probability ) of spectral distributions that pointwise convergence of stieltjes transforms implies ( as a test function , we can use a function that coincides with @xmath462 except in a interval near 0 where we are guaranteed that there are no eigenvalues asymptotically ) .",
    "we also know that @xmath459 is continuous ( and actually analytic ) at @xmath463 in this situation since the @xmath459 is the stieltjes transform of a measure who has support bounded away from 0 .",
    "so the previous equation holds for @xmath464 , and we have @xmath465 multiplying both sides by @xmath466 , we get , after we recall that @xmath261 is a probability measure , @xmath467 calling @xmath468 , we have the result we announced , conditionally on @xmath273 . now , here @xmath261 is the limiting spectral distribution of @xmath469 , but because this matrix is a rank one perturbation of @xmath470 , these two matrices have the same limiting spectral distribution .",
    "this concludes this part of the proof .",
    "@xmath341 _ getting results unconditionally on @xmath354_.    all the statements above were made conditional on @xmath273 .",
    "if we can show that our probability bounds and our characterization of the limit hold uniformly in @xmath273 , we will have an unconditional statement , as we seek .",
    "the fact that the limit does not depend on @xmath273 is essentially obvious from its description : all that matters is the limiting spectral distribution , which is the same for all @xmath273 .",
    "let us consider the question of uniform probability bounds .",
    "all we need to do is show that we control @xmath471 uniformly in @xmath273 . at this point",
    ", it is helpful to recall that @xmath296 can be viewed as a function of @xmath273.=-1    recall also that if @xmath352 , @xmath353 \\bigr)\\leq\\exp\\bigl(-(n-1)\\delta t^2/c_0 \\bigr ) .\\ ] ] hence , when @xmath352 , if @xmath472 , @xmath473 , where @xmath474 tends to 0 as @xmath1 tends to infinity . in other words",
    ", we have now established that if @xmath352 , and @xmath475 , for any @xmath476 , @xmath477 using the fact that @xmath478 , we conclude that @xmath479 as @xmath1 tends to infinity for any @xmath476 and the proof is complete .",
    "as a consequence of theorem  [ thm : quadformsinverseellipticalcase ] , we have the following practically useful result .",
    "[ lemma : dotproductsinvolvingsigmainverse ] we assume that the assumptions of theorem [ thm : quadformsinverseellipticalcase ] hold and that @xmath261 is such that @xmath260 is not @xmath480 .",
    "suppose that @xmath481 and @xmath482 are deterministic vectors such that @xmath483 are bounded away from 0 .",
    "then under the assumptions of theorem  [ thm : quadformsinverseellipticalcase ] , @xmath484    in other respects , suppose that @xmath485 , while @xmath486 and @xmath487 stay bounded away from @xmath480 .",
    "then , under the assumptions of theorem [ thm : quadformsinverseellipticalcase ] , @xmath488    the proof of the first part of the lemma is an immediate consequence of theorem  [ thm : quadformsinverseellipticalcase ] , after writing @xmath489 for the proof of the second part , we note that theorem [ thm : quadformsinverseellipticalcase ] implies that @xmath490 note that since for @xmath491 , @xmath492 is assumed to stay bounded , the same is true of @xmath493 , where @xmath494 .",
    "now we write @xmath495 our previous remark and the assumption of boundedness of @xmath492 implies that , when @xmath485 , @xmath496",
    "as is clear from the solutions of problems and , when @xmath104 appears in the matrix @xmath107 , its influence on the solution of our quadratic program will manifest itself in the form of quantities of the type @xmath269 and @xmath497 .",
    "it is therefore important that we get a good understanding of those quantities .",
    "compared to the gaussian case , in the elliptical case , @xmath104 is not independent of @xmath19 anymore , which generates some complications .",
    "they are fully addressed in theorem [ thm : summaryquadformsmuhatandsigmahat ] , but as a stepping stone to that result ( the main of this subsection ) , we need the following theorem , which essentially takes care of the problem of understanding @xmath269 for the class of elliptical distributions we consider when the population mean is 0 .",
    "[ thm : simplequadformmuhatandsigmahat ] suppose @xmath320 is an @xmath24 matrix whose rows are the vectors @xmath235 , which are i.i.d .",
    "@xmath22 .",
    "suppose @xmath273 is a diagonal matrix whose @xmath25th entry is @xmath498 , which is possibly random and is independent of @xmath320 .",
    "call @xmath499 .",
    "we assume that @xmath500 for all @xmath25 and    @xmath501    if @xmath287 is the @xmath25th largest @xmath288 , we assume that we can find a random variable @xmath289 and positive real numbers @xmath290 and @xmath291 such that    @xmath502    let us call @xmath281 and @xmath503 .",
    "we assume that @xmath504 .",
    "we call @xmath505    then we have @xmath506 if the @xmath24 data matrix @xmath507 is written @xmath508 , and if @xmath509 is the vector of column means of @xmath507 , and if @xmath19 is the sample covariance matrix computed from @xmath507 , we have @xmath510    some comments on this theorem are in order .",
    "first , @xmath511 is unchanged if we rescale all the @xmath252 s by the same constant .",
    "so it appears we could assume that they are all less than 1 , for instance , and dispense entirely with .",
    "however , that would potentially violate the conditions of which appear to guarantee that @xmath511 has variance going to zero .",
    "we also note that because the @xmath235 s have a continuous distribution and we know that all the @xmath252 s are different from 0 , the existence of @xmath511 is guaranteed with probability 1 .",
    "some practical clarifications are also in order concerning the condition @xmath512 when the @xmath252 s are i.i.d .",
    ", this condition is satisfied ( almost surely and hence in probability ) if for , instance , the @xmath252 s have finite second moment according to the marcinkiewicz  zygmund law of large numbers [ see @xcite , page  125 ] .",
    "this is very interesting from a practical standpoint as it basically means that we only require our random variables @xmath21 to have a second moment for the theorem to hold .",
    "we note that if there were no variance , the premises of the problem would be essentially flawed ( after all the quadratic form we are optimizing involves a proxy for the population covariance , and , in the absence of a second moment for the @xmath252 s , the population covariance would not exist ) , and hence we require minimal conditions from the point of view of the practical problem at stake .",
    "finally , and remarkably , the limit of @xmath511 does not depend on the empirical distribution of the @xmath252 s .",
    "in particular , in the class of elliptical distributions ( satisfying the assumptions of theorem  [ thm : simplequadformmuhatandsigmahat ] ) , the limit of @xmath513 is always the same : @xmath514 .",
    "we now turn to proving theorem [ thm : simplequadformmuhatandsigmahat ] . the proof will be facilitated by the following lemma , which essentially gives us @xmath515 .",
    "[ lemma : meanrandomprojection ] let @xmath320 be an @xmath24 random matrix , with @xmath516 with , for instance , independent rows , @xmath235 . assume that @xmath235 have symmetric distributions , that is , @xmath517 .",
    "let @xmath273 be an @xmath518 diagonal matrix with possibly random entries .",
    "let @xmath519 be a random projection matrix .",
    "@xmath320 is assumed to be independent of @xmath273 and @xmath320 and @xmath273 are assumed to be such that @xmath229 exists with probability  1 .",
    "then , @xmath520    in particular , the result applies when @xmath235 are normally distributed , and @xmath273 is such that holds , and @xmath229 is defined with probability one .",
    "proof of lemma  [ lemma : meanrandomprojection ] let us note that @xmath521 . now",
    ", conditional on @xmath273 , @xmath522 .",
    "however , @xmath523 , if @xmath524 . as a matter of fact , @xmath525 hence , conditional on @xmath273 , @xmath526 .",
    "now @xmath229 is an orthogonal projection matrix , @xmath527 , so all its entries are less than 1 in absolute value , the operator norm of @xmath229 . in particular , all the entries have an expectation . since , if @xmath524 , @xmath528 has a symmetric distribution ( conditional on @xmath273 ) , we conclude that @xmath529 note that the same arguments would apply if @xmath530 were replaced by @xmath25 , so we really have @xmath531 therefore , @xmath532 since @xmath229 has rank @xmath0 and is a projection matrix .",
    "the same results hold when we take expectations over @xmath273 by similar arguments .    to prove theorem  [ thm : simplequadformmuhatandsigmahat ] , all we have to do ( in light of lemma  [ lemma : meanrandomprojection ] ) is to show that we control the variance of @xmath533 we are going to do this now by using rank 1 perturbation arguments , in connection with the efron ",
    "stein inequality .",
    "proof of theorem [ thm : simplequadformmuhatandsigmahat ] as before , we first work conditionally on @xmath273 .",
    "we assume until further notice that @xmath534 , a set of matrices which is defined at the end of the proof , will have measure going to 1 asymptotically , and is such that all the technical issues appearing in the proof can be taken care of .",
    "( the arguments are not circular . )",
    "we will use the notation @xmath535 note that @xmath536 is symmetric and positive semi - definite .",
    "naturally , in matrix form we can write @xmath537 and @xmath538 , where @xmath539 is the same matrix as @xmath273 , except that @xmath540 .",
    "our aim is to approximate @xmath541 by a random variable involving only @xmath542 , that is , not involving @xmath235 . using classic matrix perturbation results",
    "[ see @xcite , page 19 ] , we have @xmath543 of course ,",
    "if @xmath158 is the @xmath25th canonical basis vector in @xmath544 , @xmath545 let us now call @xmath546 and @xmath547 .",
    "we have    @xmath548 similarly , @xmath549 \\\\[-8pt ] & & { } + \\lambda_i r_i e_i{'}-\\lambda_i^3 q_i \\frac{r_ie_i{'}}{1+\\lambda_i^2 q_i}+\\lambda_i^2",
    "n q_i e_i e_i{'}-\\lambda_i^4 n q_i^2 \\frac{e_ie_i{'}}{1+\\lambda_i^2 q_i}.\\nonumber\\end{aligned}\\ ] ] this is , in some sense , the key expansion in this proof .",
    "now let us call @xmath550 and @xmath551 .",
    "we have @xmath552 now let us call @xmath553 .",
    "clearly , @xmath387 does not depend on @xmath235 .",
    "now , it is easily verified that @xmath554 we finally conclude that @xmath555 we now recall the efron ",
    "stein inequality , as formulated in theorem 9 of @xcite : if @xmath556 , where the @xmath21 s are independent , and @xmath557 is a measurable function of @xmath558 , then @xmath559 in particular , for us , it means that @xmath560 if we now use equation and the fact that @xmath561 , we have @xmath562 moreover , conditional on @xmath563 ( and @xmath273 since all our arguments at this point are made conditional on @xmath273 ) , @xmath564 is @xmath565 when the @xmath320 s are @xmath22 , because @xmath566 .",
    "therefore , @xmath567 almost by definition , we have @xmath568 , since the vector @xmath569 has norm 1 and @xmath570 is a projection matrix ( recall that @xmath571 and @xmath550 ) .",
    "so we would be done if we had uniform control on @xmath572 .",
    "let us now go around this difficulty .",
    "@xmath341 _ regularization interlude .",
    "_    let us consider , for @xmath476 , @xmath573 , where @xmath574 . clearly , @xmath575 , because @xmath576 in the positive - semidefinite ordering .",
    "in other respects , the decomposition in equation is still valid if we replace @xmath387 by @xmath577 and @xmath536 by @xmath578 everywhere .",
    "however , @xmath579 .",
    "we therefore have @xmath580 so applying the previous analysis and using the fact that @xmath581 , we conclude that @xmath582",
    "so under our assumptions , @xmath583 can be approximated , in probability , at least conditionally on @xmath273 , by @xmath584 .",
    "if we write the singular value decomposition of @xmath585 , where @xmath586 , we have @xmath587 , @xmath588 , and therefore @xmath589 to get the inequality above , we used the fact that the @xmath590 are orthonormal in @xmath544 , and can therefore be completed to form an orthonormal basis of this vector space .",
    "the quantities @xmath591 are naturally the coefficients of @xmath11 in this basis , and we know that their sum of squares should be the squared norm of @xmath11 , which is @xmath1 .",
    "let us now call @xmath592 the set of matrices @xmath273 such that @xmath593 and @xmath348",
    ". under our assumptions , for a @xmath594 bounded away from 0 ( e.g. , @xmath595 ) , @xmath596 .",
    "let us pick such a @xmath594 .",
    "if @xmath534 , according to lemma [ lemma : lowerboundonsmallesteig ] and the proof of theorem [ thm : quadformsinverseellipticalcase ] , if @xmath346 denotes probability conditional on @xmath273 , @xmath597 \\bigr)\\leq\\exp\\bigl(-(n-1)\\delta_0 t^2/c_0 \\bigr ) .\\ ] ] hence , when @xmath534 , we can find , for any @xmath598 , an @xmath599 , @xmath600 where , @xmath601 as @xmath199 , for fixed @xmath602 .    on the other hand , our conditional variance computations have established that , for any @xmath603 , @xmath604 converges in probability ( conditional on @xmath273 ) to 0 if @xmath605 tends to 0 .",
    "we note that @xmath606 and that the same is true for @xmath607 .",
    "therefore , @xmath608 and @xmath609 goes to zero , since @xmath610 in other words , we also have , if @xmath611 , for any @xmath598 , @xmath612    hence , if @xmath534 and @xmath613 , @xmath614 goes to zero as @xmath1 goes to infinity , and we conclude that , since @xmath615 , @xmath616 @xmath341 _ deconditioning on @xmath273 .",
    "_    let us call @xmath617 the set of matrices such that @xmath618 .",
    "our previous computations clearly show that we can find a function @xmath619 , with @xmath620 as @xmath199 , such that , for any @xmath598 , when @xmath621 , @xmath622 , and hence we have the `` uniform bound , '' if @xmath623 , @xmath624 now under our assumptions , @xmath625 goes to 1 for any given @xmath602 , so we conclude , using the fact that @xmath626\\\\ & & { } + p[\\lambda\\notin{{\\mathcaligr}l}^2(u ) ] , \\end{aligned}\\ ] ] that @xmath627 this last statement is now understood of course unconditionally on @xmath273 and this proves the first part of the theorem .    @xmath341 _ proof of the second part of the theorem .",
    "_    we now focus on the @xmath628 part of the theorem .",
    "let us call @xmath629 .",
    "then , @xmath630 .",
    "therefore , @xmath631 hence , @xmath632 since @xmath633 in probability with @xmath504 , we have the result announced in the theorem .",
    "now that we have proved theorem [ thm : simplequadformmuhatandsigmahat ] , we need to turn to results that will allow us to handle the case of nonzero population mean , as well as questions such as the convergence of @xmath257 , for deterministic @xmath45 .",
    "this is the situation where the results are most different from that of the uncorrelated case . once again , here we will be content to just state the results ; a detailed justification of our claims is in the , section  [ app : subsec : generalquadformsprojmat ] .    as before ,",
    "the most complicated aspect of the problem is to understand quantities of the type @xmath269 , in the situation where @xmath264 . in this setting",
    ", we have the following result .",
    "[ proposition : quadformsmuhatsigmainversecorrcase ] suppose the @xmath24 data matrix @xmath507 is such that , for @xmath320 an @xmath24 matrix with i.i.d .",
    "@xmath337 entries , and @xmath273 a deterministic matrix , @xmath731 we assume that holds for the eigenvalues of @xmath275 , for a deterministic sequence @xmath732 .",
    "we write the singular value decomposition of @xmath273 as @xmath733 .",
    "we call @xmath734 and @xmath735 , that is , the sample mean of the columns of @xmath507 .",
    "we denote by @xmath736 the diagonal elements of @xmath724 , and @xmath737 .",
    "we also call @xmath738 if we call @xmath739 , and @xmath740 , we have , if @xmath741 and , @xmath742 where @xmath743 further , @xmath744    furthermore , under the above assumptions , if the spectral distribution of @xmath745 converges to @xmath261 and @xmath746 remains bounded , a result similar to theorem  [ thm : summaryquadformsmuhatandsigmahat ] holds , with @xmath260 being computed by solving equation with the corresponding @xmath261 and @xmath747 playing the role of @xmath748 .",
    "essentially the previous proposition tells us that when dealing with correlated variables , the new @xmath747 replaces the old @xmath749 .",
    "we note that there are no inconsistencies with our previous results as @xmath750 and in the `` elliptical '' case ( i.e. , @xmath273 diagonal ) , @xmath751 , so the previous proposition is consistent with the results we have obtained in the elliptical case .",
    "we also remark that @xmath752 , since @xmath140 is orthogonal .",
    "finally , in the case where the @xmath736 s have a limiting spectral distribution and satisfy , further computations show that @xmath753 .",
    "however , this does not help ( in general ) in getting a simpler expression for @xmath747 .",
    "an interesting aspect of the analysis of elliptical models is that it also shed lights on the properties of the bootstrap in this context . as a matter of fact , the nonparametric bootstrap yields covariance matrices that have a structure similar to those computed from elliptical distributions : if we call @xmath724 the diagonal matrix whose @xmath25th diagonal entry is the number of times observation @xmath21 appears in our bootstrap sample , we have , if @xmath754 is the bootstrapped covariance matrix , @xmath755 where @xmath23 is our original data matrix , and @xmath756 is the sample mean of our bootstrap sample , which can also be written @xmath757 .",
    "unless otherwise noted , we assume in the discussion that follows that the population mean @xmath6 is 0 .",
    "since the covariance matrix is shift - invariant , we can make this assumption without loss of generality .",
    "we call @xmath758 as we will see shortly , understanding the properties of @xmath754 boils down to understanding those of @xmath759 so we will focus on this slightly more convenient object in this short discussion .",
    "we note that if @xmath23 is gaussian , @xmath760 can be thought of as a `` covariance matrix '' computed from the elliptical data @xmath761 .",
    "the same remark applies when @xmath23 is elliptical , that is , for us , @xmath762 : all we need to do is change the `` ellipticity parameter '' @xmath252 to @xmath763 .",
    "the same remark is also applicable to the case of correlated observations , that is , @xmath764 , where @xmath273 is not diagonal anymore . studying the bootstrap properties of such a model",
    "is the same as studying that of the model where we replace @xmath273 by @xmath765 .",
    "we therefore would like to apply directly all the results we have obtained above in our study of elliptical models to better understand the bootstrap . for quantities of the form @xmath766",
    ", we will see that we can essentially do it , but differences will appear when dealing with @xmath767 , which yields statistics that are not exactly analogous to corresponding statistics appearing in the elliptical case",
    ".    our focus will be on bias properties of bootstrapped replications , so we will aim for convergence in probability results and not fluctuation behavior .",
    "our overall strategy here is to show convergence in probability of the quantities we are interested in as functions of both the @xmath736 s and @xmath21 s .",
    "we will derive the convergence properties of our bootstrapped statistics by then conditioning on the data and arguing that with high probability ( over the @xmath21 s ) , this does not change the results much .",
    "we first give some needed background on the bootstrap in sections  [ subsubsec : neededconvpropertiesboot ] and  [ subsubsec : propertiesbootweights ] , then turn to properties of quantities like @xmath768 ( in section [ subsubsec : invcovmatbootdata ] ) and finally study @xmath769 ( in section [ subsubsec : bootresquadformsmuhatsigmahat ] ) , where we will see ( in proposition  [ proposition : bootstrappinggaussiandata ] ) some key differences with the elliptical case .",
    "we conclude this subsection with a brief discussion of the parametric bootstrap and the conclusions that can be reached about it through our results .",
    "making statements about bootstrapped statistics requires us to make statements that are conditional on the observed data .",
    "this is not a trivial matter for the statistics we deal with since they can not be easily described in terms of simple formulas involving the original observations .",
    "however , we can take a roundabout way : by showing joint convergence in probability ( joint here refers to the `` new '' data being the vectors of bootstrapped weights and observations ) , we can obtain interesting conclusions conditional on the data . though this is not difficult to show , we give full arguments here for the sake of completeness .",
    "we will look at our statistics as functions of the number of times an observation appears in the sample and also , of course , of our observations .",
    "in other words , the original statistic , @xmath770 can be written @xmath771 and , the bootstrapped version @xmath772 is , if observation @xmath21 appears @xmath773 times in the bootstrap sample , @xmath774 the following simple proposition is used repeatedly in our bootstrap work .    [",
    "prop : joinconvinprobaimpliesbootstrapconvergence ] let us consider a statistic @xmath775 , where @xmath564 is the number of times @xmath21 appears in our sample .",
    "suppose that the vector of weights , @xmath9 , is independent of the data matrix @xmath23 .",
    "denote by @xmath776 the joint probability distribution of the @xmath564 s , @xmath777 the joint probability distribution of the @xmath21 s and @xmath778 the probability distribution of @xmath779 .",
    "suppose we have established that @xmath770 tends in @xmath780-probability to @xmath781 , a deterministic object , as @xmath199 .",
    "then we have , with @xmath777-probability going to 1 as @xmath199 , @xmath782 in other words , calling @xmath783 , for all @xmath784 , if @xmath785 , @xmath786 as @xmath1 tends to infinity .    in the case",
    "where the weights @xmath564 are obtained by standard bootstrapping , @xmath787 is multinomial(@xmath788 ) .",
    "then , @xmath789 has the distribution of the usual bootstrap quantity @xmath772 .",
    "we will focus on this case more specifically later .",
    "proof of proposition  [ prop : joinconvinprobaimpliesbootstrapconvergence ] the proof and the statement are almost obvious but we include them for the sake of completeness .",
    "let us call @xmath790 and @xmath791 . by assumption , @xmath792 in @xmath780 probability .",
    "@xmath793 ) { \\rightarrow}0 .\\ ] ] let us call @xmath785 .",
    "clearly , @xmath794 and@xmath795 , so for any @xmath603 , @xmath796    we now investigate the case of the classical bootstrap , that is , the situation in which @xmath776 is multinomial@xmath797 .",
    "as we saw in theorem  [ thm : quadformsinverseellipticalcase ] , the empirical distribution of the ellipticity parameters affect crucially statistics of the type @xmath798 , so to understand the effect of bootstrapping , we need to understand the empirical distribution of the bootstrap weights",
    ". this question has surely been investigated , but we did not find a good reference , so we provide the result and a simple proof for the convenience of the reader .",
    "[ prop : empdistbootweights ] let the vector @xmath9 be distributed according to a multinomial@xmath799 distribution .",
    "call @xmath800 the empirical distribution of the vector @xmath9 .",
    "then @xmath801 where @xmath268 is the poisson distribution with parameter 1 .",
    "let us first start by an elementary remark : suppose @xmath802 are i.i.d . with distribution @xmath268 .",
    ". then @xmath804 this result is a simple application of bayes s rule and the fact that @xmath805 .",
    "let us now show that if @xmath123 is bounded and continuous , and if @xmath806 , @xmath807 to do so , we note that @xmath808 and therefore its marginal distribution is asymptotically @xmath268 .",
    "therefore , @xmath809 now all we need to do is therefore to show that @xmath810 goes to zero . clearly , by independence of the @xmath811 s , @xmath812 because @xmath123 is bounded .",
    "but our first remark implies that @xmath813 now , @xmath814 since @xmath815 has @xmath816 distribution , @xmath817 .",
    "hence , @xmath818 and the result is established .",
    "we will also need later to use on the following ( coarse ) fact :    [ fact : boundmaximumbootstrapsample ] let the vector @xmath9 be distributed according to a multinomial@xmath819 distribution .",
    "then @xmath820 in particular , this probability goes to 0 faster than any @xmath821 , @xmath822 .",
    "the proof of the fact is elementary , and relies on the representation used above for the vector @xmath9 , a simple union bound , the fact that @xmath823 and the fact that @xmath824 which is easy to see by writing explicitly the probability we are trying to compute .    with these preliminaries behind us , we are now ready to tackle the question of understanding the ( first - order ) bootstrap properties of the statistics appearing in the study of quadratic programs with linear equality constraints .      our aim in this subsubsection and the next is to find analogs to theorems  [ thm : quadformsinverseellipticalcase ] and theorems [ thm : summaryquadformsmuhatandsigmahat ] .",
    "our first result along these lines is an analog of theorem  [ thm : quadformsinverseellipticalcase ] .",
    "we present the result in the case of gaussian data , where we can get a somewhat explicit expression for the quantity we care about , and discuss possible extensions below .",
    "[ thm : bootquadformsinverseellipticalcasenormalcase ] suppose we observe @xmath1 i.i.d .",
    "observations @xmath21 , where @xmath21 are i.i.d . in @xmath79 with distribution @xmath202 .",
    "call @xmath281 and assume that @xmath825 . call @xmath754 the covariance matrix computed after bootstrapping the @xmath21 s . call @xmath777 the joint distribution of the @xmath21 s .",
    "if @xmath45 is a ( sequence of ) deterministic vectors , then conditional on @xmath826 , with high @xmath777 probability , @xmath827 where @xmath260 satisfies , if @xmath261 is a @xmath268 distribution @xmath828    as before , we call @xmath776 the law of the bootstrap weights [ i.e. , multinomial@xmath829 and @xmath830 . without loss of generality , we can assume that @xmath264 .",
    "let us call @xmath724 the diagonal matrix containing the bootstrap weights .",
    "we have @xmath757 . also , it is true that @xmath831 since @xmath832 , we also have @xmath833 because @xmath23 is of the form @xmath834 under our assumptions , we see that @xmath835 if we call @xmath836 , we have @xmath837 because the sum of the bootstrap weights is @xmath1",
    ". therefore , @xmath838 . also , @xmath839 ( like @xmath840 ) is a projection matrix and a rank 1 perturbation of @xmath841 .",
    "the situation is therefore very similar to the question we studied in theorem  [ thm : quadformsinverseellipticalcase ] , except that @xmath322 is replaced by @xmath842 .",
    "all the arguments given there hold provided we can show that is satisfied for the bootstrap weights in the situation we have here .",
    "now let us call @xmath296 the number of nonzero bootstrap weights . in the notation of theorem  [ thm :",
    "quadformsinverseellipticalcase ] , @xmath843 and @xmath844 .",
    "so clearly , @xmath845 . so @xmath846 is a possibility .",
    "also , @xmath847 in probability , so @xmath307 has a limit in probability and this limit is bounded away from 1 because of our assumption that @xmath848 . finally , we can pick @xmath849 .",
    "so the proof of theorem  [ thm : quadformsinverseellipticalcase ] applies [ it is easy to see here that the assumption that @xmath850 can be dispensed of , because we know that the nonzero @xmath285 s are large enough for our arguments to go through , and there are enough of them that we do not have problems ( at least in probability ) with @xmath55 not being defined ] , and we have the announced result .",
    "the previous theorems settled the question of understanding the impact of the nonparametric bootstrap on statistics of the form @xmath798 in the situation where the original data were gaussian .",
    "a similar analysis could be carried out in the case of elliptical data , when we assume that the `` ellipticity '' parameters , @xmath252 , are such ( [ eq : assumptionsmallestsingularvalueboundedbelow ] ) is satisfied for the `` new weights '' @xmath851 .",
    "the result would then depend on the limiting distribution of @xmath852 ( if it exists ) , where @xmath564 is the bootstrap weight given to observation @xmath25 .      an important piece of our analysis of quadratic programs with linear equality constraints when the data are elliptically distributed was the study of quadratic forms of the type @xmath269 .",
    "it is natural to ask what happens to them when we bootstrap the data . in the elliptical case",
    ", we saw that the key statistic was of the form , when @xmath264 and @xmath853 , @xmath854 however , in the bootstrap case , if @xmath273 is the diagonal matrix containing the bootstrap weights , we have @xmath855 , but @xmath856 , so the key statistic is going to be of the form @xmath857 this creates complications because the matrix @xmath858 is not a projection matrix , and hence some of our previous analysis can not be applied directly .",
    "however , this statistic can be rewritten , if we denote @xmath859 , as @xmath860 where @xmath861 is now a projection matrix .",
    "as before its off - diagonal elements have mean 0",
    "( conditional on @xmath273 ) , but now we also need to understand@xmath862 and not only @xmath863 . a detailed analysis of the former quantity is done in appendix  [ app : subsec : bootspecresults ] .",
    "we naturally now assumes that @xmath198 has a finite limit , @xmath298 in @xmath864 .",
    "as explained in appendix [ app : subsec : bootspecresults ] , @xmath865 in @xmath776-probability , with @xmath777 probability going to 1 , where @xmath260 is computed by solving equation   [ i.e. , using @xmath268 for @xmath261 in that equation ] .",
    "similarly , it is explained there , that with @xmath777 probability going to 1 , when @xmath21 have mean 0 , @xmath866    finally , an analog of theorem  [ thm : quadformsmuhatsigmahatinvv ] holds , so we have an analog of theorem  [ thm : summaryquadformsmuhatandsigmahat ] , where @xmath260 is as defined above , and @xmath867 needs to be replaced by @xmath271 .    in summary , we have the following proposition .",
    "[ proposition : bootstrappinggaussiandata ] call @xmath260 the quantity defined by equation .",
    "suppose the data @xmath868 is i.i.d .",
    "@xmath869 , and call @xmath777 the corresponding probability distribution .",
    "suppose @xmath45 is a given deterministic sequence of vectors . under the assumptions of theorem [ thm : bootquadformsinverseellipticalcasenormalcase ]",
    ", we have , when bootstrapping the data , with @xmath777 probability going to 1 @xmath870    we note that our techniques could yield generalizations of the previous fact for the case where the data is elliptically distributed .",
    "however , in the case where @xmath21 have mean 0 , the quantity @xmath871 does not seem to have a limiting value that is writable in compact form , so we do not dwell on this question further .",
    "naturally , the motivation behind the previous proposition is practical and the results are interesting from that standpoint .",
    "they show that the bootstrap yields inconsistent estimators of the population quantities , something that is not completely unexpected when we understand the random matrix aspects of these questions .",
    "perhaps even more interesting is that bootstrap estimates of bias are themselves inconsistent : as a matter of fact , the key quantity that measures bias in the gaussian case is @xmath872 ; when bootstrapping it is replaced by @xmath260 , as defined in equation .",
    "these results therefore cast some doubts on the practical relevance of the bootstrap for the high - dimensional problems we are considering , at least when the bootstrap is used in `` classical '' ways .      in the settings considered here , it is also natural to ask how the parametric bootstrap would behave .",
    "for instance , if we assumed gaussianity of the data , we could just estimate @xmath7 and @xmath6 ( e.g. , naively , by @xmath19 and @xmath104 ) and use a parametric bootstrap to get at the quantities we are interested in .",
    "naturally , the analysis of such a scheme is similar to the analysis of the gaussian case carried out in section  [ sec : gaussiancase ] , where the population parameters @xmath7 and @xmath6 need to be replaced by the estimators we use in our parametric bootstrap .",
    "the same would be true if we were to do a parametric bootstrap for elliptical data , but we would have to use the results of section  [ sec : ellipticalcase ] instead .",
    "our computations show that the parametric bootstrap could be used in the problems under study to estimate the bias of various plug - in estimators : we would for instance recover the correct @xmath260 by considering @xmath873 .",
    "we note , however , that our analyses , and the estimation work we carry out in section [ sec : comparisongaussianelliptical ] could do this too , at a cheaper numerical cost .",
    "finally and very interestingly , we see that a naive use of the parametric bootstrap to estimate the bias in the empirical efficient frontier  a perhaps reasonable idea at first glance  would yield inconsistent estimates of bias .",
    "recall that the key quantity in the solution of problem , the problem of main interest in this paper , is of the form @xmath267 .",
    "therefore , it is important for us to understand quantities of the type @xmath634 for a fixed vector @xmath45 . at this point",
    ", we focus on the particular case where @xmath635 .",
    "to do so , we will need to study , if @xmath636 , @xmath637 for a fixed vector @xmath45 .",
    "as it turns out , this random variable goes to zero in probability when for instance @xmath46 .",
    "[ thm : quadformsmuhatsigmahatinvv ] suppose @xmath45 is a deterministic vector , with @xmath46 .",
    "suppose the assumptions stated in theorem [ thm : simplequadformmuhatandsigmahat ] hold and also that    @xmath638    consider @xmath637 where @xmath639 .",
    "then @xmath640    before giving the proof , we note that if the @xmath252 s are i.i.d . and have a second moment , the `` extra '' condition on @xmath641 introduced in this theorem ( as compared to theorem [ thm : simplequadformmuhatandsigmahat ] ) is clearly satisfied by the law of large numbers .    proof of theorem  [ thm : quadformsmuhatsigmahatinvv ] the proof is quite similar to the proof of theorem  [ thm : simplequadformmuhatandsigmahat ] above .",
    "we start by conditioning on @xmath273 .",
    "let us call @xmath642 the quantity obtained when we replace @xmath340 by @xmath643 in the definition of @xmath644 .",
    "note that since @xmath320 is symmetric , @xmath645 , conditionally on @xmath273 , by arguments similar to those given in the proof of lemma  [ lemma : meanrandomprojection ] .",
    "now @xmath642 clearly has an expectation ( conditional on @xmath273 ) , because @xmath646 , for @xmath476 , so @xmath647 .",
    "now recall equation : with the notations used there , @xmath648 let us now call @xmath649 , @xmath650 and @xmath651 . clearly , if @xmath652 is the random variable obtained by excluding @xmath235 from the computation of @xmath642 ( e.g. , by replacing @xmath498 by 0 ) , we have @xmath653 we remark that @xmath654 and recall that @xmath655 . using the fact that @xmath46 , @xmath656 and the remarks we made in the proof of theorem [ thm : simplequadformmuhatandsigmahat ]",
    ", we get that @xmath657^{2k}|(y_{(-i)},\\break\\lambda))\\leq c_k t^{-2k}$ ] , @xmath658^{2k}|(y_{(-i)},\\lambda))\\leq c_k t^{-k}$ ] , where @xmath659 and @xmath660 .",
    "we also have @xmath661 ^ 2\\leq 2 [ \\lambda_i^2\\theta_i^2(t)+\\lambda_i^4 \\theta _ i^2(t)w_i^2(t ) ] .\\ ] ] hence , simply using the fact that @xmath662 , we get @xmath663    we conclude by the efron ",
    "stein inequality that , when @xmath273 is such that @xmath664 , for any @xmath476 , @xmath665    as before , let us call @xmath592 the set of matrices @xmath273 such that @xmath593 and @xmath348 .",
    "recall that under our assumptions , for @xmath594 bounded away from 0 ( e.g. , @xmath666 ) , @xmath596 .    as we saw before , when @xmath534 , @xmath667 is bounded with high - probability ( conditional on @xmath273 ) ,",
    "so we conclude that , for any @xmath603 , we can find a @xmath668 such that @xmath669 we also notice that conditionally on @xmath273 , @xmath670 and hence , @xmath671 . we recall that @xmath46 , and since @xmath672 we conclude that with high - probability ( conditional on @xmath273 ) , for any @xmath603 , @xmath673 and finally , @xmath674 now along the same lines as what was done in the proof of theorem [ thm : simplequadformmuhatandsigmahat ] , we can make all these probability bounds uniform in @xmath273 when @xmath273 is in a set of matrices such as @xmath675 and when we also have bounds on @xmath676 and @xmath641 . under our assumptions , the set of @xmath273 for which these conditions hold has measure going to 1 , so we can finally conclude  along the same lines ( omitted here ) as in the proof of theorem  [ thm : simplequadformmuhatandsigmahat]that , unconditionally on @xmath273,=-1 @xmath640=0    after these preliminaries , we can finally state the theorem of main interest . recall that under the assumptions of theorem [ thm : quadformsinverseellipticalcase ] , if @xmath45 is deterministic , @xmath677 where @xmath260 is defined in equation .",
    "[ thm : summaryquadformsmuhatandsigmahat ] suppose that @xmath678 , where @xmath235 are i.i.d.@xmath679 and @xmath278 are random variables , independent of @xmath279 .",
    "let @xmath45 be a deterministic vector .",
    "suppose that @xmath281 has a finite nonzero limit , @xmath298 and that @xmath504 .",
    "we call @xmath283 .",
    "we assume that @xmath500 for all @xmath25 as well as    @xmath680$\\frac{1}{n}\\sum_{i=1}^n \\lambda_i^2 \\mbox { remains bounded in probability.}$\\hspace*{-50pt } \\end{tabular}\\end{aligned}\\ ] ]    if @xmath287 is the @xmath25th largest @xmath288 , we assume that we can find a random variable @xmath289 and positive real numbers @xmath290 and @xmath291 such that    @xmath681    we also assume that the empirical distribution of @xmath285 s converges weakly in probability to a deterministic limit @xmath261 .",
    "we call @xmath273 the @xmath518 diagonal matrix with @xmath682 , @xmath320 the @xmath24 matrix whose @xmath25th row is @xmath235 , @xmath683 and @xmath684 .",
    "finally , we use the notation @xmath685 , @xmath686 .",
    "then , we have , for @xmath260 defined as in equation ,    @xmath687 the second statement holding if , for instance , @xmath6 and @xmath45 are such that the first set of conditions in lemma [ lemma : dotproductsinvolvingsigmainverse ] are met .    also , @xmath688 and",
    "we recall that @xmath689 and @xmath690 .",
    "to be able to exploit equation in practice , we make the following remarks .",
    "we can consider three cases , having to do with the size of @xmath691 :    1 .   if @xmath692 , then , @xmath693 .",
    "2 .   if @xmath694 , then @xmath695 .",
    "3 .   finally , if @xmath696 stays bounded away from 0 and infinity , @xmath697    a noticeable feature of these results is that the `` extra bias '' @xmath698 , which comes essentially from mis - estimation of @xmath6 , is constant within the class of elliptical distributions considered here .",
    "this should be contrasted with the `` scaling , '' @xmath260 , which strongly depends on the empirical distribution of the @xmath262 s .",
    "we now give a brief proof of theorem [ thm : summaryquadformsmuhatandsigmahat ] .",
    "proof of theorem [ thm : summaryquadformsmuhatandsigmahat ] we first note that @xmath699 in the notation of theorem  [ thm : simplequadformmuhatandsigmahat ] .",
    "also , @xmath700 . finally , @xmath701    _ proof of equation _ . by writing @xmath702 ,",
    "we clearly have @xmath703 we have already seen in theorem [ thm : simplequadformmuhatandsigmahat ] that the third term tends to @xmath704 . on the other hand ,",
    "half of the middle term is equal to @xmath705 since @xmath706 , we have @xmath707 and we deduce the result of equation .",
    "we now remark that @xmath708 is equal to the quantity @xmath511 in theorem [ thm : simplequadformmuhatandsigmahat ] .",
    "the fact that @xmath689 follows from applying theorem  [ thm : quadformsmuhatsigmahatinvv ] with @xmath709 .",
    "_ proof of equation _ .",
    "the proof of this result follows from a decomposition similar to the one we just made .",
    "clearly the only question is whether @xmath710 goes to 0 .",
    "as we just saw , @xmath711 the results of theorem  [ thm : quadformsmuhatsigmahatinvv ] guarantee that @xmath712 since @xmath708 tends to @xmath713 and @xmath714 , we have shown the result stated in equation .",
    "it is clear that in financial practice and other applied settings , the assumption that the returns ( or observed data vectors ) are independent is often questionable .",
    "so for quadratic programs with linear equality constraints ( including the markowitz problem but also going beyond it ) , it is natural to ask what is the impact of correlation in our observations on the empirical solution of the problem . in our notation",
    ", this means that the vectors @xmath21 and @xmath715 are correlated ; we refer to this situation as the correlated case or as the case of temporal correlation .",
    "our work on the elliptical case comes in handy here and allows us to also draw conclusions concerning the correlated case .",
    "we consider a particular model , namely we assume that the @xmath716 data matrix @xmath23 is given by @xmath717 where @xmath273 is a deterministic but _ not _ necessarily a diagonal matrix , and @xmath320 is a matrix with i.i.d .",
    "@xmath337 entries .",
    "we assume throughout that @xmath273 is full rank .",
    "the model we consider now is more general than the one we looked at before , since if @xmath718 , we get the i.i.d .",
    "gaussian case , and if @xmath273 is diagonal we are back in an `` elliptical '' case ( where the ellipticity parameters are assumed to be deterministic , which amounts to doing computations conditional on @xmath273 ) .",
    "but when @xmath273 is not diagonal , @xmath21 and @xmath715 might be correlated .",
    "[ in all the situations where @xmath273 is deterministic , the marginal distribution of @xmath21 is @xmath719 , where @xmath720 is the norm of the @xmath25th row of @xmath273 . ]",
    "because we want to focus here on robustness questions arising when going from independent gaussian random variables to correlated ones , we will assume throughout that @xmath273 is deterministic .",
    "( allowing @xmath273 to be random simply requires some minor technical modifications but would make the exposition a bit less clear . ) our main results in this subsection can be interpreted as saying that that the gaussian analysis of section  [ sec : gaussiancase ] , carried out in the setting of independent observations , is not robust against these independence assumptions . the results change quite significantly when the vectors of observations are correlated .    in general",
    ", we write the singular value decomposition of the @xmath721 matrix @xmath273 as @xmath722 [ see @xcite , page 414 ] , where @xmath140 and @xmath723 are orthogonal , and @xmath724 is diagonal .",
    "therefore , @xmath725 , and @xmath726",
    "so we are almost back in the elliptical case .",
    "the key difference now is that what will matter in our analysis are not the diagonal entries of @xmath275 , but rather its eigenvalues ( see proposition [ prop : quadformsinverseellipticalcasecorrelated ] ) .",
    "also , we will see ( in proposition  [ proposition : quadformsmuhatsigmainversecorrcase ] ) that the results change quite significantly when we look at quantities like @xmath269 .",
    "as a counterpart to theorem  [ thm : quadformsinverseellipticalcase ] , we have the following proposition .    [ prop : quadformsinverseellipticalcasecorrelated ] suppose the @xmath24 data matrix @xmath23 ( whose @xmath25th row is the @xmath25th vector of observations ) can be written as @xmath727 where @xmath273 is a deterministic but _ not _ necessarily diagonal matrix .",
    "suppose that the eigenvalues of @xmath275 satisfy with a deterministic @xmath296 and that the spectral distribution of @xmath275 converges weakly to a probability distribution @xmath261 .",
    "suppose also that @xmath161 .",
    "call @xmath19 the classical sample covariance matrix , that is , @xmath728 then , if @xmath45 is a deterministic vector , we have @xmath729 where @xmath260 satisfies , if @xmath261 is the limiting spectral distribution @xmath275 @xmath730    the proposition shows that theorem [ thm : quadformsinverseellipticalcase ] essentially applies again ; however , now what matters , unsurprisingly , are the singular values of @xmath273 and not its diagonal entries .",
    "the proof of proposition  [ prop : quadformsinverseellipticalcasecorrelated ] , or rather the adjustments needed to make the proof of theorem [ thm : quadformsinverseellipticalcase ] go through , are given in the , section  [ app : sec : generalizations : subsec : quadformsv ] .",
    "we now go back to our original problem , which was to understand the relationship between the solution of problem and the solution of problem ( see page for definitions ) .",
    "it is naturally important to understand the effect of making the assumption that the data is normally distributed as compared to , say , an assumption of elliptical distribution for the data .",
    "the following discussion fleshes out some of our theoretical results and what their significance is when solving quadratic programs with linear constraints .",
    "the discussion is an application of the work done in sections  [ sec : generalqp][sec : ellipticalcase ] .",
    "it might appear to be mainly heuristic , but precise statements can be easily deduced from the precise statements of the theorems given in the corresponding technical sections .",
    "we discuss here only the case of i.i.d . data .",
    "as we have shown above , the bootstrap case and the case of correlated observations are more complicated to handle , and the formulas are not as explicit in those cases as they are in the case of i.i.d . data .",
    "but for certain cases , one could plug - in our earlier results for those situations to obtain explicit results about efficient frontiers and weight vectors in those cases too .    as a matter of notation ,",
    "all of our approximation statements hold with high - probability asymptotically , unless otherwise noted .",
    "we will carry out our work under the model put forward in theorem [ thm : quadformsinverseellipticalcase ] , assuming that the @xmath252 s are i.i.d .",
    "and the following assumptions :    1 .   for all @xmath874",
    ", @xmath875 stays bounded away from 0 .",
    "@xmath876 is assumed to be equal to @xmath6 .",
    "2 .   the smallest eigenvalue of @xmath877 stays bounded away from 0 and the condition number of @xmath15 remains bounded .",
    "3 .   if @xmath494 , @xmath878 stays bounded away from infinity .",
    "4 .   and hold .",
    "( see theorem  [ thm : summaryquadformsmuhatandsigmahat ] for definitions . )",
    "we have , for some @xmath439 , if @xmath879 , @xmath880 , where @xmath881 is the largest eigenvalue of @xmath7 .",
    "these assumptions guarantee that the noise terms involving @xmath104 do not overwhelm the signal terms involving @xmath6 , and also that we can safely take inverses of our approximations to get approximations of their inverses .",
    "also , all the key results we obtained in sections [ sec : gaussiancase ] and  [ sec : ellipticalcase ] are applicable , and our conclusions will of course heavily rely on them",
    ".    we will use the notation @xmath281 .",
    "we recall that in the gaussian case , the quantity @xmath260 appearing below is approximately equal to @xmath882 and in the elliptical case , it is always greater than @xmath882 , as we explained after the proof of theorem  [ thm : quadformsinverseellipticalcase ] .",
    "we start by investigating the case of equality constraints .",
    "we discuss inequality constraints in section  [ subsec : inequalityconstraints ] .",
    "when assumptions ( a1a4 ) hold , it is clear that @xmath883 now recall that in the elliptical case , @xmath884 , that is , the `` @xmath260 '' corresponding to the gaussian case .",
    "calling @xmath885 the empirical estimator of @xmath15 we get in the elliptical case and @xmath886 its analog in the gaussian case , we have , when a1a4 are satisfied , with high - probability , @xmath887 at least asymptotically .",
    "we now call @xmath888 and @xmath889 the `` efficient frontiers '' obtained by solving problem when the data is respectively elliptical and gaussian . recall that under our assumptions , @xmath6 and @xmath7 are the same for the two problems ,",
    "so the population version corresponding to the two problems is the same .",
    "we call the population solution , that is , the efficient frontier computed with the population parameters , @xmath890 . naturally , this is the quantity we are fundamentally interested in estimating .",
    "using the fact that @xmath891 , the following important results .",
    "[ thm : positionfrontierellipticalandgaussiancase ] when assumptions are satisfied , we have with high - probability and asymptotically , @xmath892 in other words , risk underestimation in the empirical quadratic program with linear equality constraints is least severe ( within the class of elliptical models ) in the gaussian case .    in other respects , we have , asymptotically , with high - probability , if @xmath893 , @xmath894    another way of phrasing this result is the fact that the gaussian analysis gives the most optimistic view of risk underestimation within the class of elliptical models considered here .",
    "practically , it means that users of markowitz - type optimization should be wary of the empirical solution they get , and even of the correction that gaussian results suggest .",
    "if the data is elliptical , they will underestimate the risk of their portfolio even more than the gaussian results suggest .",
    "let us now give a proof of theorem [ thm : positionfrontierellipticalandgaussiancase ] .",
    "proof of theorem  [ thm : positionfrontierellipticalandgaussiancase ] under the assumptions of the theorem , we can use the approximation in equation .",
    "the first part of the theorem has been argued before , so we do not need to do anything else to obtain it .",
    "the second part follows directly from a rank one perturbation argument .",
    "we have @xmath895 using the classic result @xmath896 , we conclude that @xmath897 we now recall from section  [ sec : generalqp ] that @xmath898 , and we have the announced result .    equation naturally suggests better ways of estimating @xmath890 than using @xmath899 .",
    "we postpone a discussion of this issue to section  [ subsec : estimationefffrontier ] because it requires somewhat lengthy preliminaries .",
    "besides problems in the location of the efficient frontiers , our analysis reveals another very interesting phenomenon : problems with estimating @xmath106 , the optimal vector of weights .",
    "in particular , one can show that the mean return of the portfolio is poorly estimated and the weight given to each asset is biased .",
    "[ thm : biasinweights ] suppose assumptions hold .",
    "we have , asymptotically and with high - probability , @xmath900 where @xmath901 this approximation is valid when looking at linear combinations of the vector of weights : if @xmath902 is deterministic and assumption extended to include this vector holds , @xmath903    we note that the last assertion of the theorem does not necessarily immediately follow from equation in high - dimension , but it is true in the setting we consider .",
    "a particularly interesting corollary is the following statement concerning inconsistent estimation of the returns .",
    "recall that with our notations , @xmath904 . in practical terms",
    ", @xmath14 corresponds to the desired expected returns we wish to have for our `` portfolio . ''",
    "under the same assumptions as that of theorem  [ thm : biasinweights ] , we have @xmath905    the previous corollary is a statement about poor estimation of returns for the following reason : @xmath906 by construction , so one might naively hope that , for a new observation @xmath907 , independent of @xmath868 and with the same distribution as them , @xmath908 .",
    "however , as the previous corollary shows , this is not satisfied .",
    "we note that the factor affecting @xmath14 is a shrinkage factor , always smaller than 1 because @xmath15 is positive semi - definite .",
    "the other term could have either sign , so its effect on return estimation is less interpretable . for large @xmath14 , it is nonetheless clear that the previous corollary shows that the returns are overestimated : the realized returns are ( asymptotically and with high - probability ) less than @xmath14 .",
    "hence , our result can be seen as a generalization of the overestimation of returns result first found in @xcite , in the low - dimensional gaussian case .",
    "we now prove these two results .",
    "the proof of the corollary is at the end of the proof of the theorem .",
    "proof of theorem  [ thm : biasinweights ] under the assumptions of the theorem we have @xmath909 and our assumptions guarantee that we can take inverses and still have valid approximations . hence , using the classic formula for inversion of a rank one perturbation of a matrix [ see @xcite , page 19 ] , we have @xmath910 now recall that @xmath911 and @xmath912 . for a deterministic @xmath78 , our work in section  [ sec : ellipticalcase ] indicates that @xmath913 .",
    "so we conclude that @xmath914 in other words , we have @xmath915 or , as announced , @xmath916 it seems difficult to say more , because @xmath917 and @xmath644 are population parameters and their properties and values may vary from problem to problem .",
    "@xmath341 _ proof of the corollary .",
    "_    we now assume that @xmath918 .",
    "we remark that @xmath919 , by construction of  @xmath61 .",
    "therefore , @xmath920 further , @xmath921 these two remarks and the result of theorem  [ thm : biasinweights ] give the conclusion of the corollary .      an important question",
    "now that we have identified possible problems with the empirical weights is to try and correct them .",
    "we propose such a scheme , suggested by our computations .",
    "our investigations will rely on the following asymptotic result , discussed in theorem  [ thm : biasinweights ] : in the notations of this theorem , @xmath916    our efforts will focus on trying to estimate @xmath922 and @xmath923 , as @xmath893 is known and computable from the data .",
    "recall that we assumed that @xmath119 and let us call @xmath924 under the assumptions underlying the previous computations , we have @xmath925 in practice , we wish @xmath926 to be a positive semi - definite matrix  something that is guaranteed asymptotically , but might require checking and potentially corrections in practice .",
    "we propose to use :    1 .   as an estimator of @xmath917 ,",
    "@xmath927 2 .   as an estimator of @xmath928 , @xmath929    for any deterministic @xmath78 ( such that the assumptions of theorem [ thm : biasinweights ] hold ) , @xmath930 ,",
    "because @xmath931 and @xmath932 .",
    "also,@xmath933 , and @xmath934 , so @xmath935 .",
    "hence , @xmath936 in other words , we have found an asymptotically consistent way of estimating the quantities of interest .",
    "hence , the estimator we propose to use is @xmath937 interestingly , this proposal does not require us to estimate @xmath260 . furthermore , because we have consistency of the estimator in the whole class of elliptical distributions , this estimator is fairly robust to distributional assumptions about the data .",
    "finally , the estimator is consistent in the sense that all ( deterministic and given ) linear combinations of @xmath938 are consistent for the corresponding linear combinations of @xmath106 ( provided these linear combinations are such that the assumptions of theorem  [ thm : biasinweights ] apply to them ) .",
    "( naturally , we can not take a supremum over too large a class of @xmath78 s . )",
    "it is nonetheless natural to raise the following question : does the proposed estimator satisfy the constraints of the problem ? if not , our proposal would be problematic , but it is indeed the case that our estimator satisfies the constraints @xmath939 for all @xmath940 . naturally , the last constraint ( i.e. , @xmath941 ) is difficult to satisfy exactly because @xmath6 is unknown , so it is also less of a concern .",
    "let us now briefly justify our claim concerning the satisfaction of the equality constraints . by construction",
    ", @xmath105 satisfies the constraints @xmath942 , @xmath943 , so all we have to show is that the @xmath87 vector @xmath944 is proportional to @xmath945 .",
    "we recall that @xmath946 , so @xmath947 using the standard formula for the inverse of a rank-1 perturbation of a matrix , we therefore get @xmath948 once we recall that @xmath949 , we immediately get the equality @xmath950 which shows that @xmath951 for @xmath943 , as announced .    finally , from a practical point of view , one might be worried that the estimator proposed in equation `` puts too much weight on the theory and not enough on the data '' and that better practical performance might be achieved by tuning more finely our corrections to the data .",
    "for instance , one might propose , we think reasonably , to use , instead of @xmath926 the matrix @xmath952 , where @xmath38 would be picked by some form of cross - validation based on the new estimator @xmath953 .",
    "we do not discuss this issue any further in this paper as we plan to address it in another , more applied , article .",
    "we do , however , show the performance of our estimator in limited simulations in section  [ subsec : numericalresults ] .",
    "we now discuss the question of improved estimation of the efficient frontier .",
    "this is naturally an important quantity in the problem , and , as we hope to have shown , a difficult one to estimate by naive methods .",
    "one aspect of its importance is that it gives us a benchmark of performance for optimal portfolios .",
    "we therefore think that in a financial context , it might be of great interest in particular to regulators .      though we have seen that we could devise a scheme to improve the estimation of the weights without having to estimate  @xmath260",
    ", this latter quantity is still an important one to estimate if we want to better understand the pitfalls we might be facing .    in the elliptical case , where @xmath276 , we wish to estimate @xmath262 , as we have seen that @xmath260 is `` driven '' by this quantity .",
    "we now describe heuristics that suggest how to estimate @xmath260 ; more detailed consistency arguments follow in proposition  [ prop : consistencyestimatelimitscaling ] .",
    "to estimate @xmath260 , we recall that standard concentration of measure results ( see below ) say that with very high probability , if the largest eigenvalue of @xmath7 does not grow too fast , @xmath955 hence , in this setting , the concentration of measure phenomenon can be used for practical purposes .",
    "now , note that @xmath956 , because under our assumptions a1a4 and the assumption of independence of the @xmath252 s , @xmath957 and a5 implies that the previous approximation holds .",
    "hence , @xmath958 we now propose the following estimator for @xmath262 : @xmath959 if we denote @xmath281 , we then propose to estimate @xmath260 using the positive solution of @xmath960 we note that this is just the discretized version of the equation characterizing  @xmath260 .",
    "( @xmath961 is clearly a continuous convex decreasing function of @xmath311 on @xmath962 , so the existence and uniqueness of a solution to @xmath963 is clear . )",
    "we recall an important result from theorem [ thm : positionfrontierellipticalandgaussiancase ] : under the assumptions made in this section , @xmath964 now recall that we have a consistent estimator of @xmath965 , that is , @xmath966 , and we just discussed how to estimate @xmath260 .    as an estimator of the efficient frontier we therefore propose @xmath967    we also note that @xmath926 could be replaced by @xmath968 described above with a similar cross - validation scheme .",
    "let us now show that our proposed estimator of @xmath260 is consistent .",
    "we place ourselves in the setting where @xmath252 s are i.i.d . with a second moment and @xmath253 .",
    "recall also that the @xmath235 s that appear below are such that @xmath969 .",
    "we have the following proposition .",
    "[ prop : consistencyestimatelimitscaling ] let us call @xmath970 and @xmath881 the largest eigenvalue of @xmath7",
    ". then we have , with probability going to 1 , @xmath971 is the solution of @xmath972 , @xmath973 as soon as , for some @xmath439 , @xmath974 as @xmath199 .",
    "let us consider the function @xmath975 .",
    "clearly this function is @xmath976-lipschitz with respect to euclidian norm in @xmath79 .",
    "now suppose that @xmath977 .",
    "let us call @xmath978 a median of @xmath979 .",
    "using standard results on the concentration properties of gaussian random variables [ see @xcite , chapter 1 and theorem 2.6 ] , we have @xmath980 hence , using a simple union bound argument , we have , after some algebra , if @xmath981 , @xmath982 so with large probability , @xmath983 now , if we call @xmath984 , we have , using proposition 1.9 in @xcite , @xmath985 now , using the fact that @xmath986 , and the fact that @xmath987 , we have @xmath988 our previous results imply that with large probability , @xmath989 and therefore , with large probability , @xmath990 as announced in the proposition . as a consequence , we have , if we call @xmath991 hence , when for some @xmath439 , @xmath992 goes to zero , which implies that @xmath993 , we have with high probability , @xmath994    @xmath341 _ consistency of @xmath995 .",
    "_    we now assume that @xmath992 goes to zero for some @xmath439 and turn to showing the consistency of @xmath996 . first , let us note that @xmath997 hence , by the same concentration arguments we just used , and using the fact that the @xmath252 s are i.i.d . with @xmath317",
    ", we have @xmath998 now , @xmath999 also , the law of large numbers ( for triangular arrays ) imply that with probability 1 @xmath1000 so we can write @xmath1001 and we have now all the terms on the right - hand side under control .    in particular , it is clear that when @xmath1002 , @xmath1003    with all these preliminaries behind us , let us now turn to the final part of the proof .",
    "let us call @xmath1004 with a slight abuse of notation , we note that @xmath260 is the solution of @xmath1005 .",
    "if @xmath1006 is the solution of @xmath1007 , it is clear that @xmath1006 is consistent for @xmath260 : we can just use the fact that @xmath1008 is decreasing and evaluate it at @xmath1009 and @xmath1010 which are on either sides of @xmath260 .",
    "clearly @xmath1011 is consistent for @xmath1012 , and similarly for @xmath1010 , so with high probability , @xmath1006 needs to be in @xmath1013 $ ] asymptotically .",
    "recall that the roots we are looking for are positive .",
    "so we have @xmath1014 and therefore , for @xmath1015 , @xmath1016 by noting that @xmath1017 , we have @xmath1018 since @xmath1006 is bounded above .",
    "now since @xmath1019 is decreasing and is pointwise consistent for @xmath1020 , it is clear that we can find @xmath1021 , deterministic and bounded away from @xmath480 , such that asymptotically , @xmath1022 , with high probability .",
    "also , @xmath1019 is convex , so this guarantees that @xmath1023 can be bounded below ( uniformly in @xmath1 with high probability ) on @xmath1024 $ ] by a quantity that is strictly greater than 0 with high probability .",
    "note that this latter interval contains both @xmath1006 and @xmath996 asymptotically . using the mean value theorem , the fact that we have a lower bound ( different from 0 ) on @xmath1025 on @xmath1026 $ ] , and the equation in the previous display",
    ", we can finally conclude that @xmath1027 with high probability , and since @xmath1006 is consistent for @xmath260 , so is @xmath996 .",
    "we just saw that we could take advantage of the high - dimensionality of the problem to essentially estimate @xmath262 , by using concentration of measure arguments .",
    "this also allows us to propose estimates of scatter that are tailored for high - dimensional problems .    in low - dimension , estimation of individual @xmath262",
    "is not possible and a classic proposal for estimating the scatter matrix @xmath7 is tyler s estimator [ see @xcite ] , which is the solution @xmath1028 ( defined up to scaling ) , of the equation @xmath1029 it has been observed in a random matrix context [ see @xcite and @xcite ] that when using tyler s estimator in connection with elliptically distributed data , one seemed to recover a spectrum that looked similar to predictions of the marenko ",
    "pastur law , at least in the case of @xmath1030 scatter . at this point , the evidence is mostly based on simulations though a rigorous proof seems feasible with a little bit of effort ( the argument given in @xcite is interesting though it falls short of a `` full proof , '' which is acknowledged in that paper ) .",
    "we do not try to give a proof here because this is quite far from being the topic of this paper .    as a high - dimensional alternative to tyler s estimator",
    ", we could use @xmath1031 one potential advantage of this proposal over tyler s estimator is that tyler s estimator is a priori not - defined when @xmath1032 , because it becomes impossible to invert @xmath1028 .",
    "also , this estimator is rather quick to compute and does not require multiple inversions of @xmath1033 matrices , where @xmath0 is large [ tyler s estimator is generally found through an iterating procedure ",
    "see @xcite and references therein ] .",
    "the spectral properties of @xmath1034 are also quite easy to analyze in light of the detailed work we carried out concerning consistency of our estimator of @xmath260 .",
    "for instance in the simple case where @xmath6 is known , it is easy to see that under some conditions on @xmath7 and the @xmath252 s , the limiting spectral distribution of @xmath1034 will satisfy a marenko  pastur - type equation .",
    "( because this is really tangential to our main points in the paper , we do not give further details . )",
    "note that these estimates of scatter essentially make the influence of the @xmath252 s on the problem disappear , at least as far as covariance ( or really scatter ) is concerned .",
    "so to answer a question asked by an insightful referee , it is reasonable to think that another approach might be to turn the problem back to an essentially gaussian problem by using an estimate of scatter instead of an estimate of covariance  if we ignore problems due to mean estimation .",
    "since in the gaussian case , @xmath316 , corrections are relatively easy then .",
    "however , the impact of mean estimation needs to be investigated and furthermore , at this point there are no rigorous results that we know of ( only very limited simulations ) concerning the spectral properties of tyler s estimator in high - dimension .",
    "so we leave further investigations of the properties of these estimates of scatter to future work , as they are not a primary concern in this already long paper ( after all we have a provably consistent estimator that takes care of all the problems and is fast to compute ) .",
    "let us however note that using estimates of scatter ( instead of covariance ) would likely yield a serious improvement in terms of the realized risk of portfolios which is discussed in the paper @xcite .",
    "however , these questions touch more on the issue of allocation , whereas we are concerned in this paper with estimating the efficient frontier and have shown that we can do this well ( at least asymptotically and theoretically ) independently of allocation issues , a fact that is potentially useful for , for instance , creating benchmarks .",
    "this subsection gives some numerical results to assess the quality of the proposed estimators for both weights and `` efficient frontier . ''",
    "the simulation analysis is done in an a priori quite favorable case  the question being whether even then the theory could be useful in practice .",
    "our aim was to investigate among other things the improvement in the quality of our approximations as @xmath1 and @xmath0 grew to infinity .",
    "hence , we present the results of two simulation setups : one where @xmath1035 and one where @xmath1036 .",
    "we chose to work with simulations where we picked both @xmath7 and @xmath6 so that we could guarantee , for instance , that the efficient frontier was basically the same for both simulations .",
    "more specifically , we chose @xmath7 to be a @xmath58 toeplitz matrix , with @xmath1037 , where @xmath1038 . in the smaller",
    "dimensional simulation , that is , @xmath16 , we picked @xmath481 to be the eigenvector associated with the 90th smallest eigenvalue of @xmath17 . calling @xmath1039 the eigenvector associated with the 15th smallest eigenvalue of @xmath7 , we picked @xmath1040 to be @xmath1041 . in the larger dimensional simulation , we used for @xmath481 the eigenvector associated with the 900th smallest eigenvalue of @xmath7 , while @xmath1039 was now associated with the 150th smallest eigenvalue of @xmath7 .",
    "@xmath1042 was computed in the same fashion in both simulations .",
    "the simulations are here to illustrate `` how large is large , '' that is , when the asymptotics kick - in and our theoretical predictions become accurate .",
    "the parameters were chosen so that we would be close to satisfying assumptions a1a5 .",
    "also , the choice of @xmath481 and @xmath482 guarantees that the off - diagonal elements of @xmath15 are not zero , which we thought might make the problem easier and lead to overoptimistic pictures .",
    "( this choice of parameters is not motivated by a particular problem in finance .",
    "we also note that if we knew that the covariance matrix were toeplitz , we could resort to regularization methods to better solve the problem",
    ". however , if we applied the same random rotation to @xmath7 , @xmath481 and @xmath482 , it becomes less clear how one could use other approaches than the ones presented here for estimation . )",
    "we did simulations both in the gaussian case and in the case of an elliptical distribution as described above , that is , @xmath1043 , where @xmath252 was proportional to a @xmath668-distributed random variables with 6 degrees of freedom and scaled to have variance 1 .",
    "we picked 6 degrees of freedom to have simulations with relatively heavy tails and capture visually the corresponding effects .",
    "it was also naturally a way to investigate the practical robustness of our estimators and compare with the gaussian case .",
    "we call below the set of simulations involving the @xmath668-distribution the `` @xmath1044 '' case because of its similarity with multivariate @xmath668-distributions .",
    "we repeated 1000 times the simulations in all the cases considered .",
    "we chose @xmath1045 and @xmath1046 ( the `` target returns '' in a financial context ) ranging from 0.1 to 5 .",
    "we note that our estimators require taking inverses of matrices which naturally raises the question of how well conditioned those matrices are .",
    "this is particularly the case when we deal with @xmath15 and @xmath926 : if @xmath15 is poorly conditioned , even though @xmath926 is a good estimator of @xmath1047 , it can turn out that @xmath1048 is a relatively poor estimator of @xmath1049 . in our simulations , both @xmath15 and @xmath7 were well conditioned but in practice , one should be aware of potential difficulties that may arise if , for instance , @xmath926 indicates that @xmath15 may be ill conditioned .",
    "when this is the case , it is actually quite easy to make the estimators perform poorly ( but of course this violates assumptions a1a5 ) .    ''",
    "( top picture ) and gaussian returns . here",
    "@xmath1050 and the number of simulations is 1000 .",
    "the dashed lines represent 95% confidence bands .",
    "the @xmath311-axis represents the returns an investor expects .",
    "the @xmath1051-axis represents what she would actually get on average ( i.e. , @xmath1052 ) .",
    "the plots show both the bias in the naive solution ( blue solid lines ) and the fact that our estimator is nearly unbiased ( red solid lines ) .",
    "they also illustrate the robustness of our corrections .",
    "the black line is very close to the red line , showing a very good correction ( on average ) in this setting where assumptions are satisfied . ]    '' ( left picture ) and gaussian returns . here",
    "@xmath1053 and the number of simulations is 1000 .",
    "the dashed lines represent 95% confidence bands .",
    "the @xmath311-axis represents the returns an investor expects .",
    "the @xmath1051-axis represents what she would actually get on average ( i.e. , @xmath1054 ) .",
    "the plots show both the bias in the naive solution ( blue solid lines ) and the fact that our estimator is nearly unbiased ( red solid lines ) .",
    "they also illustrate the robustness of our corrections .",
    "note the narrower confidence bands as compared to figure [ fig : returnscorrectionsmall ] .",
    "the black line is essentially hidden under the red line , showing a near perfect correction ( on average ) in this setting where assumptions are satisfied . ]",
    "@cc@    '' [ and ] and gaussian returns [ and ] . here , in the left column @xmath1055 and @xmath16 .",
    "in the right column , @xmath1053 . the number of simulations is 1000 in all pictures .",
    "the dashed lines represent ( empirical ) 95% confidence bands . ( the confidence bands corresponds are computed for a fixed @xmath1051 . )",
    "the @xmath311-axis represents our estimate of variance of the optimal portfolio .",
    "the @xmath1051-axis represents the target returns for the portfolio .",
    "the plots show both the bias in the naive solution ( blue solid curves ) and the fact that our estimator is nearly unbiased ( red solid curves near , or covering the black curve , the population solution ) .",
    "they also illustrate the robustness of our corrections .",
    "another striking feature is the lack of robustness of gaussian computations , since the `` efficient frontiers '' computed with `` @xmath1044 '' returns are different from the gaussian ones .",
    "the fact that , as our theoretical work predicts , gaussian computations underestimate risk - underestimation in the class of elliptical distributions considered in the paper is illustrated by the fact that the `` @xmath1044 '' curves are to the left of the gaussian curves .",
    "note the narrower confidence bands in the larger dimensional simulations [ and ] .",
    "the black line is essentially hidden under the red line in and , showing a near perfect correction ( on average ) in this setting where assumptions are satisfied.,title=\"fig : \" ] & '' [ and ] and gaussian returns [ and ] . here , in the left column @xmath1055 and @xmath16 .",
    "in the right column , @xmath1053 . the number of simulations is 1000 in all pictures .",
    "the dashed lines represent ( empirical ) 95% confidence bands .",
    "( the confidence bands corresponds are computed for a fixed @xmath1051 . )",
    "the @xmath311-axis represents our estimate of variance of the optimal portfolio .",
    "the @xmath1051-axis represents the target returns for the portfolio .",
    "the plots show both the bias in the naive solution ( blue solid curves ) and the fact that our estimator is nearly unbiased ( red solid curves near , or covering the black curve , the population solution ) .",
    "they also illustrate the robustness of our corrections .",
    "another striking feature is the lack of robustness of gaussian computations , since the `` efficient frontiers '' computed with `` @xmath1044 '' returns are different from the gaussian ones .",
    "the fact that , as our theoretical work predicts , gaussian computations underestimate risk - underestimation in the class of elliptical distributions considered in the paper is illustrated by the fact that the `` @xmath1044 '' curves are to the left of the gaussian curves .",
    "note the narrower confidence bands in the larger dimensional simulations [ and ] .",
    "the black line is essentially hidden under the red line in and , showing a near perfect correction ( on average ) in this setting where assumptions are satisfied.,title=\"fig : \" ] + & + '' [ and ] and gaussian returns [ and ] . here , in the left column @xmath1055 and @xmath16 .",
    "in the right column , @xmath1053 . the number of simulations is 1000 in all pictures .",
    "the dashed lines represent ( empirical ) 95% confidence bands .",
    "( the confidence bands corresponds are computed for a fixed @xmath1051 . )",
    "the @xmath311-axis represents our estimate of variance of the optimal portfolio .",
    "the @xmath1051-axis represents the target returns for the portfolio .",
    "the plots show both the bias in the naive solution ( blue solid curves ) and the fact that our estimator is nearly unbiased ( red solid curves near , or covering the black curve , the population solution ) .",
    "they also illustrate the robustness of our corrections .",
    "another striking feature is the lack of robustness of gaussian computations , since the `` efficient frontiers '' computed with `` @xmath1044 '' returns are different from the gaussian ones .",
    "the fact that , as our theoretical work predicts , gaussian computations underestimate risk - underestimation in the class of elliptical distributions considered in the paper is illustrated by the fact that the `` @xmath1044 '' curves are to the left of the gaussian curves .",
    "note the narrower confidence bands in the larger dimensional simulations [ and ] .",
    "the black line is essentially hidden under the red line in and , showing a near perfect correction ( on average ) in this setting where assumptions are satisfied.,title=\"fig : \" ] & '' [ and ] and gaussian returns [ and ] . here , in the left column @xmath1055 and @xmath16 .",
    "in the right column , @xmath1053 . the number of simulations is 1000 in all pictures .",
    "the dashed lines represent ( empirical ) 95% confidence bands .",
    "( the confidence bands corresponds are computed for a fixed @xmath1051 . )",
    "the @xmath311-axis represents our estimate of variance of the optimal portfolio .",
    "the @xmath1051-axis represents the target returns for the portfolio .",
    "the plots show both the bias in the naive solution ( blue solid curves ) and the fact that our estimator is nearly unbiased ( red solid curves near , or covering the black curve , the population solution ) .",
    "they also illustrate the robustness of our corrections .",
    "another striking feature is the lack of robustness of gaussian computations , since the `` efficient frontiers '' computed with `` @xmath1044 '' returns are different from the gaussian ones .",
    "the fact that , as our theoretical work predicts , gaussian computations underestimate risk - underestimation in the class of elliptical distributions considered in the paper is illustrated by the fact that the `` @xmath1044 '' curves are to the left of the gaussian curves .",
    "note the narrower confidence bands in the larger dimensional simulations [ and ] .",
    "the black line is essentially hidden under the red line in and , showing a near perfect correction ( on average ) in this setting where assumptions are satisfied.,title=\"fig : \" ] + &      as we have seen earlier , the `` naive '' weights obtained by plugging - in the sample mean and the sample covariance matrix in our quadratic program with linear equality constraints are biased , in the sense that their projection in any given direction will generally be biased .    here",
    "we show the performance of our estimator as measured by its projection on @xmath119 .",
    "it is a natural direction to consider since , for instance , in a financial context and under our modeling assumptions , it gives us the expected returns of our portfolio ( conditional on @xmath868 ) .",
    "as our limited simulations indicate , our estimator appears to be practically unbiased here ( even in the `` lower - dimensional '' case ) , which means in a financial context that the corresponding investment strategy will yield the returns that the investor expected .",
    "( we note that from a mean  variance point of view , we do not claim that our estimator is optimal .",
    "work is under way to find better performing portfolios  but it requires a new set of theoretical investigations whose results are postponed to another paper . in limited simulations , it appeared that our `` debiased '' portfolio performed similarly to the naive one from a mean  variance point of view , its main advantage being that it delivers the returns that the investor expects . )    we present two pictures , figure  [ fig : returnscorrectionsmall ] , page and figure  [ fig : returnscorrection ] , page to give a sense to the reader of the impact of the size of @xmath1 and @xmath0 on the estimators we proposed [ the `` larger - dimensional '' case gives quite significantly better results , with narrower confidence bands , though ( empirical ) near - unbiasedness is present in both cases ] .",
    "we now turn to the issue of estimating the `` efficient frontier , '' that is , the curve that represents the minima of our convex optimization problem ( [ eq : generalqp ] ) , on page .",
    "the pictures we present on figure  [ fig : returnscorrection ] ( see page ) were obtained from the simulations we described above .",
    "we chose to plot the variance ( i.e. , @xmath1056 ) on the @xmath311-axis and the `` target returns '' [ i.e. , the @xmath1057 s in the notation of equation ] on the @xmath1051-axis as this is the convention in financial applications .",
    "as the reader can see , our estimator turns out to be essentially unbiased , even in the `` lower - dimensional '' case .",
    "we note too that the variance can be quite large but that the confidence bands obtained from our corrections were always to the right of the confidence bands obtained from the naive estimator , meaning that if one is concerned with risk estimation that in ( essentially ) the worst case for our estimator , we still obtained a better performing estimator than in ( essentially ) the best case for the naive estimator .",
    "( we do not claim that this is always the case and it might be an artifact of the simulation setup chosen here . )    finally , for graphical purposes and to help comparisons , we chose to put all the graphs on the same scale .",
    "some of the information on our original graphs ( for the `` lower - dimensional '' case ) was therefore left out but can be inferred by `` naturally '' extrapolating the curves shown on our graphs which are essentially parabolas .",
    "our work has mostly been concerned with obtaining results for the case of a quadratic program with linear equality constraints .",
    "we now explain that our results can also be used to obtain approximation results concerning the case of a quadratic program with linear inequality constraints .    in this subsection",
    "we therefore consider the problem    @xmath1058    here @xmath1059 is a subset of @xmath159 , and @xmath61 is a @xmath62 matrix .",
    "we naturally want to relate the solution of the above problem to that of the empirical version of the problem :    @xmath1060    when @xmath1059 is a product of intervals , we obtain a quadratic program with linear inequality constraints .",
    "but our formulation allows us to deal with even more complicated constraint structures .",
    "we note that if @xmath1061 is the solution of problem with @xmath1062 ( i.e. , a singleton ) , where @xmath64 is a vector in @xmath159 , we are back in the case of the equality constrained problem that we worked with for most of this paper .",
    "let us call @xmath1063 the solution of problem with @xmath1062 .",
    "we now make the simple following observation : note that @xmath1064    the main idea here is that we can find a deterministic equivalent to @xmath1065 and we can relate this deterministic equivalent to @xmath1066",
    ".    recall from section  [ sec : generalqp ] that @xmath1067 and @xmath1068 .",
    "recall also that under the assumptions a1a5 made at the beginning of this section , we have found a deterministic equivalent to @xmath1069 : we have shown that @xmath1070 in probability .",
    "the previous result is valid entry - wise , and since we assume that @xmath65 stays bounded in the asymptotics we are considering , it is also valid in operator norm .",
    "now , @xmath1071 is invertible with probability one under our assumptions , so we have , using the first resolvent identity , that is , @xmath1072 , @xmath1073 hence , since @xmath1074 remains bounded under our assumptions , @xmath1075    under our assumptions , we also know that the smallest eigenvalue of @xmath1071 and @xmath1076 stay bounded away from 0 .",
    "therefore , for any @xmath1077 , we know that asymptotically , and with probability 1 , @xmath1078 furthermore , let us note that assumption a2 guarantees that @xmath1079 remains bounded and hence so do @xmath1080 and @xmath1081 .",
    "we have the following theorem .",
    "[ thm : uniformapproxconvexmaps ] suppose @xmath1082 and @xmath1083 are maps from @xmath159 to @xmath1084 such that :    1 .",
    "@xmath1082 is deterministic and @xmath1083 is possibly random .",
    "2 .   @xmath1085 .",
    "@xmath1086 such that , @xmath1087 , @xmath1088 .",
    "similarly , @xmath1089 such that , @xmath1087 , @xmath1090 .",
    "furthermore , @xmath1091 with probability 1 .",
    "@xmath1092 such that @xmath1093 in probability and @xmath1087 , latexmath:[$ |\\widehat{g}_n(u)-g_0(u )     assume that @xmath65 is fixed as @xmath199 .",
    "suppose @xmath1059 is a ( nonempty ) subset of @xmath159 and that we can find @xmath1095 such that @xmath1096 and @xmath1097 .",
    "then , @xmath1098    we have the following corollary :    when assumptions are satisfied @xmath1099    hence , we have found a deterministic equivalent to @xmath1065 .",
    "it should also be noted that because @xmath1100 , we also have @xmath1101 hence , our results on risk underestimation remain valid , even with these more general ( nonequality ) linear constraints .",
    "the comparison theorems between gaussian and elliptical assumptions remain also valid , because of similar comparison theorems for their deterministic equivalents .",
    "[ note also that when @xmath1102 ( i.e. , when the sample mean does not appear in @xmath107 ) , the previous inequalities become equalities . ] finally , our corrections also give a way to get a consistent estimator of @xmath1066 : one can simply solve the optimization problem over @xmath1059 with @xmath1071 replaced by @xmath1103 in the definition of @xmath1104 .",
    "note that the corollary follows immediately from theorem [ thm : uniformapproxconvexmaps ] because of our remarks on the operator norm of @xmath1071 and @xmath1069 and their deterministic equivalents .",
    "let us now prove theorem  [ thm : uniformapproxconvexmaps ] .",
    "proof of theorem  [ thm : uniformapproxconvexmaps ] let us pick @xmath1105 in @xmath1059 .",
    "we can do so because @xmath1059 is nonempty .",
    "we assume without loss of generality that @xmath1106 , for otherwise the problem is trivial , since @xmath463 is the global minimizer of both ( deterministic and stochastic ) problems .",
    "let us pick @xmath1107 , with @xmath1097 .",
    "suppose that @xmath1108 , where @xmath1109 is the closed ball of radius @xmath1110 with center @xmath463 .",
    "then , our assumptions on @xmath1082 guarantee that @xmath1111 so @xmath1112 . also , if we call @xmath1113 , @xmath1114 is nonempty and @xmath1115 because if @xmath64 is outside of @xmath1109 , @xmath1116 . now",
    ", suppose that @xmath1117 and @xmath1118 are two sets of real numbers .",
    "we have @xmath1119 as a matter of fact , for any @xmath153 , @xmath1120 now @xmath1121=(\\inf\\alpha_k)-(\\inf\\beta _ k)$ ] . and the previous display guarantees that @xmath1122\\leq\\sup_k |\\alpha_k-\\beta_k|$ ] .",
    "by symmetry of the role of @xmath231 and @xmath232 , we therefore have @xmath1119 hence , we can conclude that @xmath1123 by our assumptions , and the fact that @xmath1124 in @xmath1114 .",
    "hence , since @xmath1110 stays fixed as @xmath199 , @xmath1125 if we can show that with high - probability , @xmath1126 the result will be shown .",
    "first we note that if @xmath1127 , @xmath1128 for some @xmath1077 with high probability under our assumptions .",
    "let us call @xmath1129 the event @xmath1130 .",
    "of course , @xmath1131 under our assumptions , since @xmath1132 in probability and @xmath1133 in probability .",
    "when @xmath1129 is true , we have @xmath1134 so when @xmath1129 is true , and hence with high - probability , @xmath1135 we can finally conclude that @xmath1136 and the theorem is proved .",
    "this study of quadratic programs with linear constraints whose parameters are estimated from data has highlighted the difficulties created by the high - dimensionality of the data . in particular , we have shown that the fact that @xmath1 ( the number of observations used to estimate the parameters ) and @xmath0 both grew to infinity lead to a systematic underestimation of the minimal `` risk '' one exposed oneself to when approaching the optimization problem by solving its naive proxy .",
    "our study produced exact distributional results in the gaussian case ( section  [ sec : gaussiancase ] ) and convergence results in probability in the elliptical case ( section  [ sec : ellipticalcase ] ) , which also allowed us to reach conclusions for the bootstrap and the case of nonindependent data ( in particular , it covers the case of gaussian data correlated in time ) . as explained in section  [ sec : comparisongaussianelliptical ] , the study of the gaussian case gives an over - optimistic assessment of risk underestimation in the context we study : in the class of elliptical distributions we consider , risk is minimally underestimated in the gaussian case , and the situation is more dire for other elliptical distributions .",
    "our study also highlights the fact that standard bootstrap estimates of bias will be inconsistent .",
    "it also suggests that in the case of correlated gaussian observations , risk underestimation is likely to be more severe than in the i.i.d . case .",
    "another benefit of our analysis is that it sheds light on what is creating those difficulties and allows us to propose robust corrections to these problems .",
    "as shown in the theoretical part of the paper and illustrated in our limited simulation work , they are robust in the class of elliptical distributions we consider",
    ". they also appear to work reasonably well in practice ( when the underlying assumptions hold ) , as our ( somewhat limited ) simulation work seems to indicate .    perhaps surprisingly",
    ", we did not need to make very strong assumptions about the covariance matrix at stake or the mean , whereas recent statistical work focused on estimation of covariance matrices [ see @xcite or @xcite ] tends to do so .",
    "this is in part because our theoretical analysis clearly showed what functionals of these two parameters one needed to estimate , and hence we were able to bypass stronger requirements by focusing on those particular functionals and correcting the first order errors that appeared . in other words , even though our aim was to estimate a complicated function of the population covariance matrix and of the population mean , for which we do not have good estimators in high - dimension in general , we were able to use poor estimators of both ( and our theoretical analysis ) to get an accurate estimator of the functional of interest .",
    "this is an interesting result in the context of high - dimensional statistics more generally , as it suggests that we might be able to estimate certain functions of high - dimensional parameters without having to accurately estimate the parameters themselves [ and hence we might be able to bypass in some situations sparsity ( or other similar requirements ) for the population quantities ] .    beside the interesting statistical and mathematical questions this study raised ,",
    "we hope that it might also be helpful to , for instance , financial regulators by perhaps providing them with more realistic benchmarks for the performance of optimal portfolios and that it sheds light on how the high - dimensionality of the data affects the proper assessment of risk of large portfolios obtained by solving high - dimensional optimization problems .",
    "in our study of the gaussian case , and in particular in connection with properties of wishart matrices , we relied several times on properties of the inverse of a partitioned matrix .",
    "here is a detailed statement of what we needed .",
    "let @xmath140 be a generic matrix , and let us decompose it by blocks @xmath1137    let us call @xmath189 the inverse of @xmath140 .",
    "we assume that all inverses we take are well defined .",
    "let us write @xmath1138 then , it is well known that [ see , e.g. , @xcite , pages 458459 , or @xcite , page 650 ] @xmath1139",
    "in many proofs in the course of the paper we needed to have quantitative bounds on the behavior of the smallest eigenvalue of a number of matrices and made repeated use of the following lemma .",
    "[ lemma : lowerboundonsmallesteig ] suppose @xmath320 is a @xmath24 matrix , with i.i.d .",
    "@xmath337 entries , with @xmath1140 , and @xmath1141 .",
    "suppose @xmath273 is an @xmath518 diagonal and deterministic matrix and that we can find @xmath296 , @xmath1142 and @xmath439 such that , if @xmath285 is the @xmath25th largest eigenvalue of @xmath275 , @xmath1143 , for some fixed @xmath1142 .",
    "@xmath296 is such that , for @xmath0 and @xmath1 large , @xmath342 and @xmath1144 stays bounded away from 0 .",
    "finally , we assume that all the diagonal entries of @xmath273 are different from 0 .",
    "call @xmath1145 , where @xmath837",
    ". then @xmath40 , the smallest eigenvalue of @xmath1146 , is bounded away from 0 with high - probability .    in particular , when @xmath342 , if @xmath1147 , @xmath1148 \\bigr)\\leq\\exp\\bigl(-(n-1)t^2 \\bigr ) .\\ ] ]    the following proof makes clear that the result holds also when some of the diagonal entries of @xmath273 are equal to zero if we make the following modification : @xmath1 should now denote the number of nonzero entries on the diagonal of @xmath273 , and the corresponding assumptions about @xmath0 and @xmath296",
    "should then hold .",
    "we also point out that under our assumptions @xmath840 is an orthogonal projection matrix .",
    "proof of lemma  [ lemma : lowerboundonsmallesteig ] before we start the proof per se , we need some notations : we call @xmath1149 the @xmath65th largest eigenvalue of a symmetric matrix . in other words ,",
    "the eigenvalues are decreasingly ordered and @xmath1150    the result is known if @xmath1151 , since @xmath1152 using @xcite , theorem ii.13 , we have the following result : the smallest eigenvalue of a matrix with distribution @xmath1153 is strongly concentrated around @xmath1154 when @xmath1155 , and @xmath1156    this gives our result in the case where @xmath718 .",
    "let us now investigate what happens when @xmath273 is not @xmath841 .    the matrix @xmath1157 is a rank-1 perturbation of @xmath275 and is positive semi - definite , because @xmath840 is .",
    "therefore , for any @xmath1158 , @xmath1159 , by the interlacing theorem 4.3.4 in @xcite .",
    "@xmath15 has rank @xmath1160 matrix since , @xmath1161 and @xmath1162 .",
    "we can diagonalize @xmath1163 , where @xmath724 has @xmath1164 nonzero coefficients , and because @xmath1165 , we have @xmath1166 where @xmath736 are the nonzero diagonal entries of @xmath724 . because @xmath15 is positive semi - definite , we have @xmath1167 for all @xmath25 . in other respects , because for all @xmath1168 , @xmath1169 by our remark on interlacing inequalities .",
    "hence , we have , if @xmath1170 denotes positive - semidefinite ordering , @xmath1171 therefore , we have in law , @xmath1172 as we recalled above , the smallest eigenvalue of @xmath1173 remains bounded away from 0 with high - probability in our setting because @xmath307 remains bounded away from 1 by assumption .",
    "we also assumed that @xmath1144 and @xmath297 were bounded away from 0 . if we call @xmath1174 , we have @xmath1175 , and , for any @xmath603 , according to the result of @xcite we have , for @xmath1176 and @xmath1177 ,",
    "@xmath1178 \\bigr)\\leq\\exp\\bigl(-(n-1)t^2 \\bigr ) .\\ ] ] in particular , when @xmath307 is such that @xmath1179 , @xmath1148 \\bigr)\\leq\\exp\\bigl(-(n-1)t^2 \\bigr ) .\\ ] ]    interestingly , this bound is `` quite uniform '' in @xmath273 , in the sense that the only characteristics of @xmath273 that matter are @xmath1147 and @xmath296 .",
    "this part of the appendix explains how to appropriately modify the proofs of theorems  [ thm : quadformsinverseellipticalcase ] and  [ thm : summaryquadformsmuhatandsigmahat ] to obtain the results we need in the case of correlated observations ( section  [ sec : elliptical : subsec : correlobs ] ) and the bootstrap .",
    "we explain in this subsection how to modify the proof of theorem [ thm : quadformsinverseellipticalcase ] in the case where the vectors of observations @xmath21 and @xmath715 are potentially correlated .",
    "the data was assumed to have the following representation , in matrix form : @xmath1180 where @xmath273 is @xmath518 , deterministic but not necessarily diagonal and @xmath320 has i.i.d .",
    "@xmath337 entries .",
    "we also wrote the svd of @xmath273 as @xmath722 , where @xmath140 and @xmath723 are orthogonal .",
    "if we call @xmath322 , we have , of course , @xmath1181 the orthogonality of @xmath723 implies @xmath1182 , and we have @xmath1183 if we now call @xmath1184 , we see that @xmath1185 , because @xmath140 is orthogonal .",
    "it can also easily be seen that @xmath1186 .",
    "because of the remark we just made on the norm of @xmath216 , @xmath839 is clearly an orthogonal projection matrix .",
    "so we have to understand @xmath1187 which is extremely close to the situation of theorem [ thm : quadformsinverseellipticalcase ] , where we had to work with @xmath1188 @xmath724 now plays the role @xmath273 played in theorem [ thm : quadformsinverseellipticalcase ] and the main modification is that @xmath1189 is now replaced by @xmath839 .",
    "an examination of the proof of theorem [ thm : quadformsinverseellipticalcase ] shows that we never relied on the fact that we used specifically @xmath1190 ( instead of @xmath839 ) in that proof .",
    "all we used was the fact that our @xmath840 there was a rank-1 perturbation of @xmath841 and an orthogonal projection matrix .",
    "similarly , lemma  [ lemma : lowerboundonsmallesteig ] , on which we relied in the course of the proof of theorem [ thm : quadformsinverseellipticalcase ] , handles @xmath839 for general @xmath216 with squared norm @xmath1 without any problems , so it is still usable in the course of the current study .    because we know that the squared singular values of @xmath273 ( and hence the eigenvalues of @xmath724 ) satisfy , the proof of theorem [ thm : quadformsinverseellipticalcase ] goes through without further modifications and proposition [ prop : quadformsinverseellipticalcasecorrelated ] holds .",
    "a recurrent issue in the questions we addressed was the understanding of statistics of the form @xmath1191 where @xmath229 is a random projection matrix and @xmath602 a ( generally deterministic ) vector of dimension @xmath1 .",
    "in particular , the projection matrices we dealt with were of the form @xmath1192 for @xmath273 a ( possibly random ) @xmath518 diagonal matrix and @xmath320 an @xmath24 matrix with i.i.d . @xmath337 entries .",
    "we also assume that @xmath273 and @xmath602 are independent of  @xmath320 .",
    "finally , we assume that @xmath1193 .",
    "in the course of the text , we carried out successfully computations when @xmath1194 , but relied to do so on properties of @xmath1195 .",
    "the case of general @xmath602 is more involved and is treated here .",
    "[ lemma : generalquadformsrandomproj ] assume that @xmath273 and u ( which is deterministic ) are such that @xmath1196 and that holds for @xmath273 for a certain sequence @xmath732 .    under the preceding assumptions , we have , if @xmath1197 , @xmath1198 conditionally on @xmath273 .",
    "we simply sketch the modifications to the proof given after the statement of theorem  [ thm : simplequadformmuhatandsigmahat ] .",
    "as noted in lemma  [ lemma : meanrandomprojection ] , the off - diagonal elements of @xmath229 have mean 0 conditionally on @xmath273 .",
    "now , using the same notations as in theorem  [ thm : simplequadformmuhatandsigmahat ] , we have , using equation there , if @xmath1199 is the quantity obtained by replacing @xmath252 by @xmath463 in @xmath170 , @xmath1200 , @xmath1201 and @xmath66 is the @xmath25th coordinate of @xmath602 , @xmath1202 the expression between the parentheses is easily seen to be equal to @xmath1203 .",
    "we get an analog of equation @xmath1204 clearly , from the definition of @xmath564 , @xmath1205 since by assumption @xmath1206 , we have @xmath1207 because @xmath570 is an orthogonal projection matrix ( hence its eigenvalues are only 0 and 1 ) and @xmath1208 .",
    "so we are exactly in the situation we were in during the proof of theorem  [ thm : simplequadformmuhatandsigmahat ] , except for a term in @xmath1209 that now appears in our bound on the variance .",
    "hence , with our extra assumption on @xmath1210 , we conclude similarly ( after a regularization step ) that @xmath1211 converges in probability , conditional on @xmath273 to its conditional mean which is simply @xmath1212    we remark that to get an analog of theorem [ thm : quadformsmuhatsigmahatinvv ] , where now @xmath1213 one just needs to go through the proof and replace the @xmath564 appearing there by the `` new '' @xmath1214 .",
    "exactly the same arguments go through when @xmath1215 remains bounded .",
    "so under this condition , @xmath644 tends to zero in probability .    with the help of the previous lemma",
    ", we can now prove the gist of proposition  [ proposition : quadformsmuhatsigmainversecorrcase ] .",
    "[ fact : propcorrcaseholds ] proposition  [ proposition : quadformsmuhatsigmainversecorrcase ] holds .",
    "we note that proposition [ proposition : quadformsmuhatsigmainversecorrcase ] is essentially an application of the previous lemma , with appropriate change of notation .",
    "recall the notations from the proposition .",
    "we have @xmath1216 and @xmath273 , which is @xmath518 , has singular value decomposition @xmath1217 . also , @xmath734 , @xmath1218 , @xmath1219 .",
    "hence , in the language of the proposition , @xmath1220 where @xmath1221 and @xmath739 .",
    "when the assumptions of the proposition are in force , @xmath273 is deterministic and lemma [ lemma : generalquadformsrandomproj ] applies ; from which we conclude @xmath1222 this gives us the analog of theorem  [ thm : simplequadformmuhatandsigmahat ] .    to get the analog of theorem  [ thm : quadformsmuhatsigmahatinvv ] , we just need @xmath1223 to remain bounded , which is an assumption stated in proposition [ proposition : quadformsmuhatsigmainversecorrcase ] .        our analysis of the bootstrap problem requires an analysis similar to the one we performed in the previous subsection . in particular",
    ", there we have @xmath1224 , where @xmath273 contains the bootstrap weights . since those add - up to @xmath1 , the assumption @xmath1225 was clearly satisfied .",
    "also , in the situation where @xmath1226 , we are guaranteed that @xmath1227 is well defined with high - probability .",
    "when conditioning on @xmath273 , we see that we can work only with the submatrix @xmath1228 ( of size @xmath1229 ) whose diagonal entries are nonzero .",
    "this submatrix has its diagonal entries bounded away from 0 as they are at least equal to 1 .",
    "also , using arguments similar to those given in the proof of lemma  [ lemma : lowerboundonsmallesteig ] , we see that we can get a uniform ( in @xmath273 ) lower bound on the smallest singular value of @xmath1230 , which holds with probability exponentially [ in @xmath1231 close to 1 .",
    "so now we assume that we are dealing with @xmath273 such that @xmath1232 tends to @xmath480 , the empirical distribution of @xmath273 goes to @xmath268 and @xmath1233 .",
    "we also assume that are satisfied for this @xmath273 .",
    "finally , we assume that @xmath1234 .",
    "we call the corresponding set of matrices @xmath1235 .",
    "when the diagonal entries of @xmath273 are drawn from a multinomial@xmath1236 it is clear that these conditions are satisfied with probability going to 1 .",
    "the only thing that might require an explanation is why the condition @xmath1237 holds with probability going to 1 .",
    "the mean of @xmath1238 clearly goes to 2 , using the marginal distribution of @xmath252 . on the other hand ,",
    "the arguments we gave in proposition  [ prop : empdistbootweights ] show that its variance goes to 0 , so this quantity goes to 2 in probability and therefore is less than 10 with probability going to 1 .",
    "the main question that we still have to address is that of the behavior of @xmath1239 when @xmath1240 . by definition , @xmath1241 where @xmath1242 .",
    "now concentration arguments ( see , e.g. , section  [ subsec : inequalityconstraints ] ) show that , if @xmath1243 is the smallest singular value of @xmath536 , @xmath1244 we also know that with overwhelming probability ( measured over @xmath1245 ) , @xmath1243 is bounded away from 0 , conditionally on @xmath273 , when @xmath273 is such that holds .",
    "( note for instance that @xmath1246 and use lemma  [ lemma : lowerboundonsmallesteig ] . )",
    "hence , we conclude that @xmath1247 hence , conditionally on @xmath273 , @xmath1248 with very high - probability , that is , the probability that the difference between the two is greater than @xmath1249 is @xmath1250 for a fixed @xmath297 ( by arguments similar to those given in lemma  [ lemma : lowerboundonsmallesteig ] ) . in other respects , we note that rank-1 perturbation arguments give , if @xmath1251 , @xmath1252 in particular , when @xmath273 is such that holds , by using a union bound argument , @xmath1253 we also note that @xmath1254 conditionally on @xmath273 , if @xmath273 is such that its empirical distribution goes to @xmath268 , @xmath1255 and @xmath1140 .",
    "therefore , we also have by a simple union bound argument , conditional on @xmath273 , and assuming that @xmath273 is such that its empirical distribution goes to @xmath268 , @xmath1255 and hence @xmath1256 is less than 10 , @xmath1257 now when @xmath1258 , which we write @xmath261 , and @xmath1259 , @xmath1260 but in light of the marenko  pastur equation , we have , under these circumstances , @xmath1261    we finally conclude that conditional on @xmath273 being in @xmath1235 ( whose probability goes to 1 ) , @xmath1262 since we know that @xmath1263 when @xmath261 is @xmath268 , since its mean is 1 .",
    "similar arguments as the ones used in the proofs in the main body of the paper show that the same convergence in probability result holds unconditionally on @xmath273 , the problem being to get bounds that are uniform in @xmath273 , when @xmath1255 .",
    "hence , an analog of theorem  [ thm : simplequadformmuhatandsigmahat ] follows ( with @xmath777 probability going to 1 ) , where the ratio @xmath1264 is replaced by @xmath271 .",
    "the analog of theorem  [ thm : quadformsmuhatsigmahatinvv ] follows from the arguments given in appendix  [ app : subsec : generalquadformsprojmat ] , if we can show , in the notation used there that @xmath1265 remains bounded with probability going to 1 .",
    "note that @xmath1266 here , where @xmath252 are the bootstrap weights , so we just need to show that @xmath1238 remains bounded .",
    "but we did this when describing @xmath1235 .",
    "we therefore have an analog of theorem [ thm : quadformsmuhatsigmahatinvv ] and also of theorem [ thm : summaryquadformsmuhatandsigmahat ] when bootstrapping gaussian data .",
    "finally , let us say a few words about what would happen if we replaced the normality assumption for the @xmath21 s by an elliptical distribution assumption .",
    "we focus on the case where @xmath1267 , that is , the mean of the @xmath21 s is 0 .",
    "the previous analyses make clear that the key questions concern @xmath766 and @xmath871 .    the questions concerning @xmath766 fall pretty much directly under the study we have made of elliptical distributions , since we know , according to the proof of theorem [ thm : bootquadformsinverseellipticalcasenormalcase ] , that @xmath1268 where @xmath724 is the diagonal matrix containing the bootstrap weights and @xmath1269 .",
    "so , as long as @xmath1270 satisfies , results similar to theorem [ thm : bootquadformsinverseellipticalcasenormalcase ] will hold .    the questions dealing with @xmath871 are more involved .",
    "analyses similar to the ones performed above show that the key quantity to understand is now @xmath1271 where @xmath1272 and @xmath1273 .",
    "the analysis of this quadratic form can be carried out just like we did above in the gaussian case , that is , @xmath718 . however , the remarks we made to get simplified expressions for the limit do not seem to apply anymore : quantities of the type @xmath1274 appear , where @xmath260 is the solution of equation with @xmath261 being the limit ( if it exists ) of the empirical distribution of the random variables @xmath1275 .",
    "these quantities do not appear to simplify any further to yield a clearer and more exploitable expression .",
    "i am very grateful to nizar touzi and nicole el karoui for several very interesting discussions at the beginning of this project and for their interest in it .",
    "i would also like to thank two anonymous referees for their constructive comments and insightful questions .",
    "bai , z. d. ( 1999 ) .",
    "methodologies in spectral analysis of large - dimensional random matrices , a review . _ statist .",
    "* 9 * 611677 . with comments by g. j. rodgers and jack w. silverstein ; and a rejoinder by the author ."
  ],
  "abstract_text": [
    "<S> we first study the properties of solutions of quadratic programs with linear equality constraints whose parameters are estimated from data in the high - dimensional setting where @xmath0 , the number of variables in the problem , is of the same order of magnitude as @xmath1 , the number of observations used to estimate the parameters . </S>",
    "<S> the markowitz problem in finance is a subcase of our study . assuming normality and independence of the observations we relate the efficient frontier computed empirically to the `` true '' efficient frontier . </S>",
    "<S> our computations show that there is a separation of the errors induced by estimating the mean of the observations and estimating the covariance matrix . in particular , the price paid for estimating the covariance matrix is an underestimation of the variance by a factor roughly equal to @xmath2 . </S>",
    "<S> therefore the risk of the optimal population solution is underestimated when we estimate it by solving a similar quadratic program with estimated parameters .    </S>",
    "<S> we also characterize the statistical behavior of linear functionals of the empirical optimal vector and show that they are biased estimators of the corresponding population quantities .    </S>",
    "<S> we investigate the robustness of our gaussian results by extending the study to certain elliptical models and models where our @xmath1 observations are correlated ( in `` time '' ) . </S>",
    "<S> we show a lack of robustness of the gaussian results , but are still able to get results concerning first order properties of the quantities of interest , even in the case of relatively heavy - tailed data ( we require two moments ) . </S>",
    "<S> risk underestimation is still present in the elliptical case and more pronounced than in the gaussian case .    </S>",
    "<S> we discuss properties of the nonparametric and parametric bootstrap in this context . </S>",
    "<S> we show several results , including the interesting fact that standard applications of the bootstrap generally yield inconsistent estimates of bias .    </S>",
    "<S> we propose some strategies to correct these problems and practically validate them in some simulations . throughout this paper </S>",
    "<S> , we will assume that @xmath0 , @xmath1 and @xmath3 tend to infinity , and @xmath4 .    </S>",
    "<S> finally , we extend our study to the case of problems with more general linear constraints , including , in particular , inequality constraints .    .    </S>"
  ]
}