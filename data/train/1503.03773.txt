{
  "article_text": [
    "signal estimation has been a fundamental problem in a number of scenarios , such as wireless sensor networks ( wsn ) and cognitive radio ( cr ) .",
    "wsn has received a lot of attention and is found to be useful in diverse disciplines such as environmental monitoring , smart grid , and wireless communications @xcite .",
    "cr appears as an enabling technique for flexible and efficient use of the radio spectrum @xcite , since it allows the unlicensed secondary users ( sus ) to access the spectrum provided that the licensed primary users ( pus ) are idle , and/or the interference generated by the sus to the pus is below a certain level that is tolerable for the pus @xcite .",
    "one in cr systems is the ability to obtain a precise estimate of the pus power distribution map so that the sus can avoid the areas in which the pus are actively transmitting .",
    "this is usually realized through the estimation of the position , transmit status , and/or transmit power of pus @xcite , and such an estimation is typically obtained based on the minimum mean - square - error ( mmse ) criterion @xcite .",
    "the mmse approach involves the calculation of the expectation of a squared @xmath1-norm function that depends on the so - called regression vector and measurement output , both of which are random variables .",
    "this is essentially a stochastic optimization problem , but when the statistics of these random variables are unknown , it is impossible to calculate the expectation analytically .",
    "an alternative is to use the sample average function , constructed from the sequentially available measurements , as an approximation of the expectation , and this leads to the well - known recursive least - square ( rls ) algorithm @xcite .",
    "as the measurements are available sequentially , at each time instance of the rls algorithm , an ls problem has to be solved , which furthermore admits a closed - form solution and thus can efficiently be computed .",
    "more details can be found in standard textbooks such as @xcite .    in practice",
    ", the signal to be estimated may be sparse in nature @xcite .",
    "in a recent attempt to apply the rls approach to estimate a sparse signal , a regularization function in terms of @xmath0-norm was incorporated into the ls function to encourage sparse estimates @xcite , leading to an @xmath0-regularized ls problem which has the form of the least - absolute shrinkage and selection operator ( lasso ) @xcite .",
    "then in the recursive estimation of a sparse signal , the only difference from standard rls is that at each time instance , instead of solving an ls problem as in rls , an @xmath0-regularized ls problem in the form of lasso is solved @xcite .",
    "however , a closed - form solution to the @xmath0-regularized ls problem no longer exists because of the @xmath0-norm regularization function and the problem can only be solved iteratively . as a matter of fact , iterative algorithms to solve",
    "the @xmath0-regularized ls problems have been the center of extensive research in recent years and a number of solvers have been developed , e.g. , @xmath2 @xcite , @xmath3 @xcite , @xmath4 @xcite , @xmath5 @xcite , and @xmath6 @xcite . since the measurements are sequentially available , and with each measurement , a new @xmath0-regularized ls problem is formed and solved , the overall complexity of using solvers for the whole sequence of @xmath0-regularized ls problems is no longer affordable",
    ". if the environment is furthermore fast changing , this method is not even real - time applicable because new samples may have already arrived before the old @xmath0-regularized ls problem is solved .    to make the estimation scheme suitable for online ( real - time ) implementation , a _ _",
    "sequential algorithm was proposed in @xcite , in which the @xmath0-regularized ls problem at each time instance is solved only approximately . in particular , at each time instance , the @xmath0-regularized ls problem is solved with respect to ( w.r.t . ) only a single element of the unknown vector variable ( instead of _ all _ elements as in a solver ) while remaining elements are fixed , and the element is updated in closed - form based on the so - called soft - thresholding operator @xcite .",
    "after a new sample arrives , a new @xmath0-regularized ls problem is formed and solved w.r.t . the next element while remaining elements are fixed .",
    "this sequential update rule is known in literature as block coordinate descent method @xcite . to our best knowledge , @xcite is the only work on online algorithms for recursive estimation of sparse signals .",
    "intuitively , since only a single element is updated at each time instance , the online algorithm proposed in @xcite sometimes suffers from slow convergence , especially when the signal has a large dimension while large dimension of sparse signals is universal in practice .",
    "it is tempting to use the parallel algorithm proposed in @xcite , but it works for deterministic optimization problems only and may not converge for the stochastic optimization problem at hand . besides , its convergence speed heavily depends on the stepsize .",
    "typical stepsizes are armijo - like successive line search , constant stepsize , and diminishing stepsize .",
    "the former two suffer from high complexity and slow convergence ( * ? ? ? * remark 4 ) , while the decay rate of the diminishing stepsize is very difficult to choose : on the one hand , a slowly decaying stepsize is preferable to make notable progress and to achieve satisfactory convergence speed ; on the other hand , theoretical convergence is guaranteed only when the stepsizes decays fast enough .",
    "it is a difficult task on its own to find the decay rate that gives a good trade - off .    a recent work on parallel algorithms for stochastic optimization is @xcite .",
    "however , the algorithms proposed in @xcite are not applicable for the recursive estimation of sparse signals .",
    "this is because the regularization function in @xcite must be strongly convex and differentiable while the regularization gain must be lower bounded by some positive constant so that convergence can be achieved , but the regularization function in terms of @xmath0-norm in this paper is convex ( but not strongly convex ) and nonsmooth while the regularization gain is decreasing to 0 .    in this paper",
    ", we propose an online parallel algorithm with provable _ _ convergence for recursive estimation of sparse signals .",
    "in particular , our contributions are as follows :    \\1 ) at each time instance , the @xmath0-regularized ls problem is solved approximately and all elements are updated in parallel , so the convergence speed is greatly enhanced compared with @xcite . as a nontrivial extension of @xcite from sequential update to parallel update and @xcite from deterministic optimization problems to stochastic optimization problems ,",
    "the convergence of the proposed algorithm is established .",
    "\\2 ) the proposed stepsize is based on the so - called minimization rule ( also known as exact line search ) and its benefits are twofold : firstly , it guarantees the convergence of the proposed algorithm , which may however diverge under other stepsize rules ; secondly , notable progress is achieved after each variable update and the trouble of parameter tuning in @xcite is saved .",
    "besides , both the update direction and stepsize of each element have a simple closed - form expression , so the algorithm is fast to converge and suitable for online implementation .",
    "\\3 ) when implemented in a distributed manner , for example in cr networks , the proposed algorithm has a much smaller signaling overhead than in state - of - the - art techniques @xcite .",
    "besides , the estimates of different sus are always the same and they are based on the global information of all sus . compared with consensus - based distributed implementations @xcite where each su makes individual estimate decision mainly based on his own local information and all sus converge to the same estimate only asymptotically , the proposed approach can better protect the quality - of - service ( qos ) of pus because it eliminates the possibility that the estimates maintained by different sus lead to conflicting interests of the pus ( i.e. , some may correctly detect the presence of pus but some may not in consensus - based algorithms ) .",
    "the rest of the paper is organized as follows . in section [ sec : system - model ]",
    "we introduce the system model and formulate the recursive estimation problem .",
    "the online parallel algorithm is proposed in section [ sec : algorithm ] , and its implementations and extensions are discussed in section [ sec : implementation - and - extensions ] . the performance of the proposed algorithm is evaluated numerically in section [ sec : numerical - results ] and finally concluding remarks are drawn in section [ sec : concluding - remarks ] .    _",
    "notation : _ we use @xmath7 , @xmath8 and @xmath9 to denote scalar , vector and matrix , respectively .",
    "@xmath10 is the @xmath11-th element of @xmath9 ; @xmath12 and @xmath13 is the @xmath14-th element of @xmath8 and @xmath15 , respectively , and @xmath16 and @xmath17 .",
    "@xmath18 is a vector that consists of the diagonal elements of @xmath9 .",
    "@xmath19 denotes the hadamard product between @xmath8 and @xmath20 .",
    "@xmath21_{\\mathbf{a}}^{\\mathbf{b}}$ ] denotes the element - wise projection of @xmath8 onto @xmath22 $ ] : @xmath21_{\\mathbf{a}}^{\\mathbf{b}}\\triangleq\\max(\\min(\\mathbf{x},\\mathbf{b}),\\mathbf{a})$ ] , and @xmath23^{+}$ ] denotes the element - wise projection of @xmath8 onto the nonnegative orthant : @xmath23^{+}\\triangleq\\max(\\mathbf{x},\\mathbf{0})$ ] .",
    "@xmath24 denotes the moore - penrose inverse of @xmath9 .",
    "suppose @xmath25 is a deterministic sparse signal to be estimated based on the the measurement @xmath26 , and they are connected through a linear regression model : @xmath27 where @xmath28 is the number of measurements at any time instance .",
    "the regression vector @xmath29 is assumed to be known , and @xmath30 is the additive estimation noise . throughout the paper , we make the following assumptions on @xmath31 and @xmath32 for @xmath33 :    1 .",
    "@xmath31 are independently and identically distributed ( i.i.d . )",
    "random variables with a bounded positive definite covariance matrix ; 2 .   @xmath32 are i.i.d .",
    "random variables with zero mean and bounded variance , and are uncorrelated with @xmath31 .",
    "given the linear model in ( [ eq : linear - model ] ) , the problem is to estimate @xmath34 from the set of regression vectors and measurements @xmath35 . since both the regression vector @xmath31 and estimation noise @xmath32 are random variables , the measurement @xmath36 is also random .",
    "a fundamental approach to estimate @xmath34 is based on the mmse criterion , which has a solid root in adaptive filter theory @xcite . to improve the estimation precision , all available measurements @xmath35",
    "are exploited to form a cooperative estimation problem which consists in finding the variable that minimizes the mean - square - error @xcite : @xmath37\\label{eq : mmse}\\\\   & = \\underset{\\mathbf{x}}{\\arg\\min}\\;\\frac{1}{2}\\mathbf{x}^{t}\\mathbf{g}\\mathbf{x}-\\mathbf{b}^{t}\\mathbf{x},\\nonumber\\end{aligned}\\ ] ] where @xmath38 $ ] and @xmath39 $ ] , and the expectation is taken over @xmath40 .    in practice ,",
    "the statistics of @xmath40 are often not available to compute @xmath41 and @xmath42 analytically .",
    "in fact , the absence of statistical information is a general rule rather than an exception .",
    "it is a common approach to approximate the expectation in ( [ eq : mmse ] ) by the sample average constructed from the samples @xmath43 sequentially available up to time @xmath44 @xcite :    [ eq : rls ] @xmath45    where @xmath46 and @xmath47 is the sample average of @xmath41 and @xmath42 , respectively : @xmath48 and @xmath49 is the moore - penrose pseudo - inverse of @xmath50 . in literature , ( [ eq : rls ] ) is known as recursive least square ( rls ) , as indicated by the subscript `` rls '' , and @xmath51 can be computed efficiently in closed - form , cf .",
    "( [ eq : rls-2 ] ) .    in many practical applications ,",
    "the unknown signal @xmath34 is sparse by nature or by design , but @xmath51 given by ( [ eq : rls ] ) is not necessarily sparse when @xmath44 is finite @xcite . to overcome this shortcoming ,",
    "a sparsity encouraging function in terms of @xmath0-norm is incorporated into the sample average function in ( [ eq : rls ] ) , leading to the following @xmath0-regularized sample average function at any time instance @xmath52 @xcite : @xmath53 where @xmath54 .",
    "define @xmath55 as the minimizing variable of @xmath56 : @xmath57 in literature , problem ( [ eq : l1-rls ] ) for any fixed @xmath44 is known as the _ least - absolute shrinkage and selection operator _ ( lasso ) @xcite ( as indicated by the subscript `` lasso '' in ( [ eq : l1-rls ] ) ) .",
    "note that in batch processing @xcite , problem ( [ eq : l1-rls ] ) is solved only once when a certain number of measurements are collected ( so @xmath44 is equal to the number of measurements ) , while in the recursive estimation of @xmath34 , the measurements are sequentially available ( so @xmath44 is increasing ) and ( [ eq : l1-rls ] ) is solved repeatedly at each time instance @xmath52    the advantage of ( [ eq : l1-rls ] ) over ( [ eq : mmse ] ) , whose objective function is stochastic and whose calculation depends on unknown parameters @xmath41 and @xmath42 , is that ( [ eq : l1-rls ] ) is a sequence of deterministic optimization problems whose theoretical and algorithmic properties have been extensively investigated and widely understood . a natural question arises in this context : is ( [ eq : l1-rls ] ) equivalent to ( [ eq : mmse ] ) in the sense that @xmath55 is a strongly consistent estimator of @xmath34 , i.e. , @xmath58 with probability 1 ?",
    "the connection between @xmath55 in ( [ eq : l1-rls ] ) and the unknown variable @xmath34 is given in the following lemma @xcite .",
    "[ lem : connection]suppose assumptions ( a1)-(a2 ) as well as the following assumption are satisfied for ( [ eq : l1-rls ] ) :    1 .",
    "@xmath59 is a positive sequence converging to @xmath60 , i.e. , @xmath54 and @xmath61 .",
    "then @xmath62 with probability 1 .",
    "an example of @xmath63 satisfying assumption ( a3 ) is @xmath64 with @xmath65 and @xmath66 .",
    "typical choices of @xmath67 are @xmath68 and @xmath69 @xcite .",
    "lemma [ lem : connection ] not only states the connection between @xmath55 and @xmath34 from a theoretical perspective , but also suggests a simple algorithmic solution for problem ( [ eq : mmse ] ) : @xmath34 can be estimated by solving a sequence of deterministic optimization problems ( [ eq : l1-rls ] ) , one for each time instance @xmath52 .",
    "however , different from rls in which each update has a closed - form expression , cf .",
    "( [ eq : rls-2 ] ) , problem ( [ eq : l1-rls ] ) does not have a closed - form solution and it can only be solved numerically by iterative algorithm such as @xmath2 @xcite , @xmath3 @xcite , @xmath4 @xcite , @xmath5 @xcite , and @xmath6 @xcite . as a result , solving ( [ eq : l1-rls ] ) repeatedly at each time instance",
    "@xmath52 is neither computationally practical nor real - time applicable .",
    "the aim of the following sections is to develop an algorithm that enjoys easy implementation and fast convergence .",
    "the lasso problem in ( [ eq : l1-rls ] ) is convex , but the objective function is nondifferentiable and it can not be minimized in closed - form , so solving ( [ eq : l1-rls ] ) completely w.r.t . all elements of @xmath8 by a solver at each time instance @xmath52 is neither computationally practical nor suitable for online implementation . to reduce the complexity of the variable update , an algorithm based on inexact optimization",
    "is proposed in @xcite : at time instance @xmath44 , only a single element @xmath12 with @xmath70 is updated by its so - called best response , i.e. , @xmath56 is minimized w.r.t .",
    "@xmath12 only : @xmath71 with @xmath72 , which can be solved in closed - form , while the remaining elements @xmath73 remain unchanged , i.e. , @xmath74 . at the next instance @xmath75 ,",
    "a new sample average function @xmath76 is formed with newly arriving samples , and the @xmath77-th element , @xmath78 , is updated by minimizing @xmath76 w.r.t .",
    "@xmath78 only , while the remaining elements again are fixed .",
    "although easy to implement , sequential updating schemes update only a single element at each time instance and they sometimes suffer from slow convergence when the number of elements @xmath79 is large .    to overcome the slow convergence of the sequential update , we propose an online parallel update scheme , with provable convergence , in which ( [ eq : l1-rls ] ) is solved _ approximately _ by simultaneously updating all elements only once based on their individual best response .",
    "given the current estimate @xmath80 which is available before the @xmath44-th sample arrives could be arbitrarily chosen , e.g. , @xmath81 . ] , the next estimate @xmath82 is determined based on all the samples collected up to instance @xmath44 in a three - step procedure as described next .",
    "@xmath83^{1}_{0}\\ ] ]    * step 1 ( update direction ) : * in this step , all elements of @xmath8 are updated _ in parallel _ and the update direction of @xmath8 at @xmath84 , denoted as @xmath85 , is determined based on the best - response @xmath86 . for each element of @xmath8 ,",
    "say @xmath12 , its best response at @xmath84 is given by : @xmath87 where @xmath88 and it is fixed to their values of the preceding time instance @xmath89 .",
    "an additional quadratic proximal term with @xmath90 is included in ( [ eq : x - hat - definition ] ) for numerical simplicity and stability @xcite , because it plays an important role in the convergence analysis of the proposed algorithm ; conceptually it is a penalty ( with variable weight @xmath91 ) for moving away from the current estimate @xmath92 .    after substituting ( [ eq : l^t(x ) ] ) into ( [ eq : x - hat - definition ] ) , the best - response in ( [ eq : x - hat - definition ] ) can be expressed in closed - form : @xmath93 where @xmath94 and @xmath95 is the well - known soft - thresholding operator @xcite . from the definition of @xmath46 in ( [ eq : a - and - b ] ) , @xmath96 and @xmath97 for all @xmath14 , so the division in ( [ eq : x - hat - scalar ] ) is well - defined .",
    "given the update direction @xmath85 , an intermediate update vector @xmath98 is defined : @xmath99 where @xmath100 and @xmath101 $ ] is the stepsize . the update direction @xmath85 is a descent direction of @xmath56 in the sense specified by the following proposition .    [",
    "* descent direction][prop : descent - direction]for @xmath100 given in ( [ eq : x - hat - scalar ] ) and the update direction @xmath85 , the following holds for any @xmath101 $ ] : @xmath102 where @xmath103 . *    the proof follows the general line of arguments in ( * ? ? ?",
    "8(c ) ) and is thus omitted here .",
    "* step 2 ( stepsize ) : * in this step , the stepsize @xmath104 in ( [ eq : x_tilde ] ) is determined so that fast convergence is observed .",
    "it is easy to see from ( [ eq : descent ] ) that for sufficiently small @xmath104 , the right hand side of ( [ eq : descent ] ) becomes negative and @xmath56 decreases as compared to @xmath84 .",
    "thus , to minimize @xmath56 , a natural choice of the stepsize rule is the so - called `` minimization rule '' ( * ? ? ?",
    "2.2.1 ) ( also known as the `` exact line search '' ( * ? ?",
    "9.2 ) ) , which is the stepsize , denoted as @xmath105 , that decreases @xmath56 to the largest extent along the direction @xmath85 at @xmath84 : @xmath106 therefore by definition of @xmath105 we have for any @xmath101 $ ] : @xmath107 the difficulty with the standard minimization rule ( [ eq : minimization - rule-1 ] ) is the complexity of solving the optimization problem in ( [ eq : minimization - rule-1 ] ) , since the presence of the @xmath0-norm makes it impossible to find a closed - form solution and the problem in ( [ eq : minimization - rule-1 ] ) can only be solved numerically by a solver such as @xmath108 @xcite .    to obtain a stepsize with a good trade off between convergence speed and computational complexity",
    ", we propose a _ simplified _",
    "minimization rule which yields fast convergence but can be computed at a low complexity .",
    "firstly it follows from the convexity of norm functions that for any @xmath101 $ ] :    [ eq : triangle - inequality ] @xmath109    the right hand side of ( [ eq : triangle - inequality-2 ] ) is linear in @xmath104 , and equality is achieved in ( [ eq : triangle - inequality-1 ] ) either when @xmath110 or @xmath111 .    in the proposed simplified minimization rule , instead of directly minimizing @xmath112 over @xmath104 , its upper bound based on ( [ eq : triangle - inequality ] ) is minimized and @xmath113 is given by @xmath114 the scalar problem in ( [ eq : stepsize-1 ] ) consists in a convex quadratic objective function along with a bound constraint and it has a closed - form solution , given by ( [ eq : stepsize-2 ] ) at the top of the next page , where @xmath115_{0}^{1}\\triangleq\\min(\\max(x,0),1)$ ] denotes the projection of @xmath7 onto @xmath116 $ ] , and obtained by projecting the unconstrained optimal variable of the convex quadratic scalar problem in ( [ eq : stepsize-1 ] ) onto the interval @xmath116 $ ] .",
    "the advantage of minimizing the upper bound function of @xmath117 in ( [ eq : stepsize-1 ] ) is that the optimal @xmath104 , denoted as @xmath113 , always has a closed - form expression , cf .",
    "( [ eq : stepsize-2 ] ) . at the same time",
    ", it also yields a decrease in @xmath56 at @xmath84 as the standard minimization rule @xmath105 ( [ eq : minimization - rule-1 ] ) does in ( [ eq : minimization - rule-2 ] ) , and this decreasing property is stated in the following proposition .    given @xmath98 and @xmath113 defined in ( [ eq : x_tilde ] ) and ( [ eq : stepsize-1 ] ) ,",
    "respectively , the following holds : @xmath118 and equality is achieved if and only if @xmath119 .    denote the objective function in ( [ eq : stepsize-1 ] ) as @xmath120 .",
    "it follows from ( [ eq : triangle - inequality ] ) that    @xmath121    and equality in ( [ eq : inequality-1 ] ) is achieved when @xmath119 and @xmath122 .",
    "besides , it follows from the definition of @xmath113 that @xmath123 since the optimization problem in ( [ eq : stepsize-1 ] ) has a unique optimal solution @xmath113 given by ( [ eq : stepsize-2 ] ) , equality in ( [ eq : inequality-2 ] ) is achieved if and only if @xmath119 . finally , combining ( [ eq : inequality-1 ] ) and ( [ eq : inequality-2 ] ) yields the conclusion stated in the proposition .    the signaling required to perform ( [ eq : stepsize-2 ] ) ( and also ( [ eq : x - hat - scalar ] ) ) will be discussed in section [ sec : implementation - and - extensions ] .",
    "* step 3 ( dynamic reset ) : * in this step , the next estimate @xmath82 is defined based on @xmath124 given in ( [ eq : x_tilde ] ) and ( [ eq : stepsize-2 ] ) .",
    "we first remark that @xmath124 is not necessarily the solution of the optimization problem in ( [ eq : l1-rls ] ) , i.e. , @xmath125 this is because @xmath8 is updated only once from @xmath126 to @xmath127 , which in general can be further improved unless @xmath124 already minimizes @xmath56 , i.e. , @xmath128 .",
    "the definitions of @xmath56 and @xmath55 in ( [ eq : l^t(x)])-([eq : l1-rls ] ) reveal that @xmath129 however , @xmath130 may be larger than 0 and @xmath124 is not necessarily better than the point @xmath131",
    ". therefore we define the next estimate @xmath82 to be the best between the two points @xmath124 and @xmath131 : @xmath132 and it is straightforward to infer the following relationship among @xmath80 , @xmath124 , @xmath82 and @xmath55 : @xmath133 moreover , the dynamic reset ( [ eq : x^(t+1 ) ] ) guarantees that @xmath134 since @xmath135 and @xmath47 converges from assumptions ( a1)-(a2 ) , ( [ eq : low - level - set ] ) guarantees that @xmath136 is a bounded sequence .",
    "* initialization : * @xmath81 , @xmath137 .    at each time instance @xmath52 :    * step 1 : * calculate @xmath86 according to ( [ eq : x - hat - scalar ] ) .",
    "* step 2 : * calculate @xmath113 according to ( [ eq : stepsize-2 ] ) .    *",
    "step 3 - 1 : * calculate @xmath124 according to ( [ eq : x_tilde ] ) .",
    "* step 3 - 2 : * update @xmath82 according to ( [ eq : x^(t+1 ) ] ) .    to summarize the above development",
    ", the proposed online parallel algorithm is formally described in algorithm [ alg : iterative - algorithm ] , and its convergence properties are given in the following theorem .",
    "[ * strong consistency][thm : convergence]suppose assumptions ( a1)-(a3 ) as well as the following assumptions are satisfied : *    1 .",
    "both @xmath31 and @xmath32 have bounded moments ; 2 .",
    "@xmath138 for some @xmath139 ; 3 .",
    "the sequence @xmath140 is nonincreasing , i.e. , @xmath141 .    then @xmath80 is a strongly consistent estimator of @xmath34 , i.e. , @xmath142 with probability 1 .",
    "see appendix [ sec : proof - of - theorem - convergence ] .",
    "assumption ( a4 ) is standard on random variables and is usually satisfied in practice .",
    "we can see from assumption ( a5 ) that if there already exists some @xmath139 such that @xmath143 for all @xmath44 , the quadratic proximal term in ( [ eq : x - hat - definition ] ) is no longer needed , i.e. , we can set @xmath144 without affecting convergence .",
    "this is the case when @xmath44 is sufficiently large because @xmath135 . in practice",
    "it may be difficult to decide if @xmath44 is large enough , so we can just assign a small value to @xmath91 for all @xmath44 in order to guarantee the convergence . as for assumption ( a6 ) ,",
    "it is satisfied by the previously mentioned choices of @xmath63 , e.g. , @xmath64 with @xmath65 and @xmath145 .",
    "theorem [ thm : convergence ] establishes that there is no loss of strong consistency if at each time instance , ( [ eq : l1-rls ] ) is solved only approximately by updating all elements simultaneously based on best - response only once . in what follows , we comment on some of the desirable features of algorithm [ alg : iterative - algorithm ] that make it appealing in practice :    \\i ) algorithm [ alg : iterative - algorithm ] belongs to the class of parallel algorithms where all elements are updated simultaneously each time .",
    "compared with sequential algorithms where only one element is updated at each time instance @xcite , the improvement in convergence speed is notable , especially when the signal dimension is large .",
    "\\ii ) algorithm [ alg : iterative - algorithm ] is easy to implement and suitable for online implementation , since both the computations of the best - response and the stepsize have closed - form expressions . with the simplified minimization stepsize rule , notable decrease in objective function value",
    "is achieved after each variable update , and the trouble of tuning the decay rate of the diminishing stepsize as required in @xcite is also saved .",
    "most importantly , the algorithm may not converge under decreasing stepsizes .",
    "\\iii ) algorithm [ alg : iterative - algorithm ] converges under milder assumptions than state - of - the - art algorithms .",
    "the regression vector @xmath31 and noise @xmath32 do not need to be uniformly bounded , which is required in @xcite and which is not satisfied in case of unbounded distribution , e.g. , the gaussian distribution .",
    "the proposed algorithm [ alg : iterative - algorithm ] can be further simplified if @xmath34 , the signal to be estimated , has additional properties .",
    "for example , in the context of cr studied in @xcite , @xmath34 represents the power vector and it is by definition always nonnegative . in this case , a nonnegative constraint on @xmath12 in ( [ eq : x - hat - definition ] ) is needed : @xmath147 and the best - response @xmath148 in ( [ eq : x - hat - scalar ] ) is simplified to @xmath149^{+}}{g_{kk}^{(t)}+c_{k}^{(t)}},\\ ; k=1,\\ldots , k.\\ ] ] furthermore , since both @xmath80 and @xmath86 are nonnegative , we have @xmath150 and @xmath151 therefore the _ standard _ minimization rule ( [ eq : minimization - rule-1 ] ) can be adopted directly and the stepsize is accordingly given as @xmath152_{0}^{1},\\ ] ] where @xmath153 is a vector with all elements equal to 1 .",
    "algorithm [ alg : iterative - algorithm ] can be implemented in both a centralized and a distributed network architecture . to ease the exposition",
    ", we discuss the implementation issues in the context of wsn with a total number of @xmath28 sensors .",
    "the discussion for cr is similar and thus not duplicated here .",
    "_ network with a fusion center : _ the fusion center performs the computation of ( [ eq : x - hat - scalar ] ) and ( [ eq : stepsize-2 ] ) . to do this ,",
    "the signaling from sensors to the fusion center is required : at each time instance @xmath44 , each sensor @xmath154 sends @xmath155 to the fusion center . note that @xmath46 and @xmath47 defined in ( [ eq : a - and - b ] )",
    "can be updated recursively :    [ eq : recursive - update ] @xmath156    after updating @xmath8 according to ( [ eq : x - hat - scalar ] ) and ( [ eq : stepsize-2 ] ) , the fusion center broadcasts @xmath157 to all sensors .",
    "we discuss next the computational complexity of algorithm [ alg : iterative - algorithm ] .",
    "note that in ( [ eq : recursive - update ] ) , the normalization by @xmath44 is not really computationally necessary because they appear in both the numerator and denominator and thus cancel each other in the division in ( [ eq : x - hat - scalar ] ) and ( [ eq : stepsize-2 ] ) . computing ( [ eq : recursive - update-1 ] ) requires @xmath158 multiplications and @xmath159 additions . to perform ( [ eq : recursive - update-2 ] ) ,",
    "@xmath160 multiplications and @xmath161 additions are needed .",
    "associated with the computation of @xmath162 in ( [ eq : x - hat - r_t ] ) are @xmath163 multiplications and @xmath164 additions .",
    "then in ( [ eq : x - hat - scalar ] ) , @xmath165 additions , @xmath79 multiplications and @xmath79 divisions are required .",
    "the projection in ( [ eq : x - hat - scalar ] ) also requires @xmath164 number comparisons . to compute ( [ eq : stepsize-2 ] ) , first note that @xmath166 can be recovered from @xmath162 because @xmath167 and computing @xmath168 requires @xmath79 number comparisons and @xmath169 additions , so what are requested in total are @xmath170 multiplications , @xmath171 additions , 1 addition , and @xmath172 number comparisons ( the projection needs at most 2 number comparisons ) .",
    "the above analysis is summarized in table [ tab : complexity ] , and one can see that the complexity is at the order of @xmath173 , which is as same as traditional rls ( * ? ? ?",
    ".[tab : complexity]computational complexity of algorithm [ alg : iterative - algorithm ] [ cols=\"^,^,^,^,^\",options=\"header \" , ]     _ network without a fusion center : _ in this case , the computational tasks are evenly distributed among the sensors and the computation of ( [ eq : x - hat - scalar ] ) and ( [ eq : stepsize-2 ] ) is performed locally by each sensor at the price of ( limited ) signaling exchange among different sensors .",
    "we first define the following sensor - specific variables @xmath174 and @xmath175 for sensor @xmath154 as follows : @xmath176 so that @xmath177 and @xmath178 .",
    "note that @xmath174 and @xmath179 can be computed _ locally _ by sensor @xmath154 and no signaling exchange is required .",
    "it is also easy to verify that , similar to ( [ eq : recursive - update ] ) , @xmath174 and @xmath179 can be updated recursively by sensor @xmath154 , so the sensors do not have to store all past data .",
    "the message passing among sensors in carried out in two phases .",
    "firstly , for sensor @xmath154 , to perform ( [ eq : x - hat - scalar ] ) , @xmath180 and @xmath162 are required , and they can be decomposed as follows :    @xmath181    furthermore , to determine the stepsize as in ( [ eq : stepsize-2 ] ) and to compare @xmath130 with 0 , the following variables are required at sensor @xmath154 : @xmath182 @xmath183 and @xmath184    but computing ( [ eq : information-5 ] ) does not require any additional signaling since @xmath185 is already available from ( [ eq : information-2 ] ) .",
    "note that @xmath130 can be computed from ( [ eq : information-2])-([eq : information-4 ] ) because @xmath186 where @xmath166 comes from ( [ eq : information-2 ] ) , @xmath187 comes from ( [ eq : information-3 ] ) , and @xmath188 comes from ( [ eq : information-3])-([eq : information-4 ] ) .    to summarize , in the first phase , each node needs to exchange @xmath189 , while in the second phase , the sensors need to exchange @xmath190 ; thus the total signaling at each time instance is a vector of the size @xmath191 . the signaling exchange can be implemented in a distributed manner by , for example , consensus algorithms , which converge if the graph representing the links among the sensors is connected .",
    "a detailed discussion , however , is beyond the scope of this paper , and interested readers are referred to @xcite for a more comprehensive introduction .",
    "now we compare algorithm [ alg : iterative - algorithm ] with state - of - the - art distributed algorithms in terms of signaling exchange .",
    "\\1 ) the signaling exchange of algorithm [ alg : iterative - algorithm ] is much less than that in @xcite . in (",
    "a.5 ) , problem ( [ eq : l1-rls ] ) is solved completely for each time instance , so it is essentially a double layer algorithm : in the inner layer , an iterative algorithm is used to solve ( [ eq : l1-rls ] ) while in the outer layer @xmath44 is increased to @xmath75 and ( [ eq : l1-rls ] ) is solved again . in each iteration of the inner layer , a vector of the size @xmath164 is exchanged among the sensors , and this is repeated until the termination of the inner layer ( which typically takes many iterations ) , leading to a much heavier signaling burden than the proposed algorithm .",
    "\\2 ) a distributed implementation of the online sequential algorithm in @xcite is also proposed in @xcite .",
    "it is a double layer algorithm , and in each iteration of the inner layer , a vector with the same order of size as algorithm [ alg : iterative - algorithm ] is exchanged among the sensors .",
    "similar to @xcite , this has to be repeated until the convergence of the inner layer .",
    "we also remark that in consensus - based distributed algorithms @xcite , the estimate decision of each sensor depends mainly on its own local information and those local estimates maintained by different sensors are usually different , which may lead to conflicting interests of the pus ( i.e. , some may correctly detect the presence of pus but some may not , so the pus may still be interfered ) ; an agreement ( i.e. , convergence ) is reached only when @xmath44 goes to infinity @xcite . by comparison , in the proposed algorithm",
    ", all sensors update the estimate according to the same expression ( [ eq : x - hat - scalar ] ) and ( [ eq : stepsize-2 ] ) based on the information jointly collected by all sensors , so they have the same estimate of the unknown variable all the time and the qos of pus are better protected .",
    "for a given vector @xmath8 , its support @xmath192 is defined as the set of indices of nonzero elements : @xmath193 suppose without loss of generality @xmath194 , where @xmath195 is the number of nonzero elements of @xmath8 .",
    "it is shown in @xcite that the time - weighted sparsity regularization ( [ eq : l1-rls ] ) does not make @xmath55 satisfy the so - called `` oracle properties '' , which consist of support consistency , i.e. , @xmath196=1,\\ ] ] and @xmath197-estimation consistency , i.e. , @xmath198 where @xmath199 means convergence in distribution and @xmath200 is the upper left block of @xmath41 .    to make the estimation satisfy the oracle properties ,",
    "it was suggested in @xcite that a time- and norm - weighted lasso be used , and the loss function @xmath56 in ( [ eq : l^t(x ) ] ) be modified as follows : @xmath201 where :    * @xmath51 is given in ( [ eq : rls ] ) ; * @xmath61 and @xmath202 , so @xmath63 must decrease slower than @xmath203 ; * the weight factor @xmath204 is defined as @xmath205 where @xmath206 is a given constant .",
    "therefore , the value of the weight function @xmath207 in ( [ eq : l1-rls - weighted ] ) depends on the relative magnitude of @xmath63 and @xmath208 .    after replacing the universal sparsity regularization gain @xmath63 for element @xmath12 in ( [ eq : x - hat - scalar ] ) and ( [ eq : stepsize-2 ] ) by @xmath209 , algorithm [ alg : iterative - algorithm ] can readily be applied to estimate @xmath34 based on the time- and norm - weighted loss function ( [ eq : l1-rls - weighted ] ) and the strong consistency holds as well . to see this",
    ", we only need to verify the nonincreasing property of the weight function @xmath207 .",
    "we remark that when @xmath44 is sufficiently large , it is either @xmath210 or @xmath211 .",
    "this is because @xmath212 under the conditions of lemma [ lem : connection ] . if @xmath213 , since @xmath61 , we have for any arbitrarily small @xmath214 some @xmath215 that @xmath216 for all @xmath217 ; the weight factor in this case is 0 for all @xmath217 , and the nonincreasing property is automatically satisfied .",
    "if , on the other hand , @xmath218 , then @xmath219 converges to @xmath218 at a speed of @xmath203 @xcite .",
    "since @xmath63 decreases slower than @xmath203 , we have for some @xmath215 such that @xmath220 for all @xmath217 ; in this case , @xmath221 is equal to 1 and the weight factor is simply @xmath63 for all @xmath217 , which is nonincreasing .",
    "if the signal to be estimated is time - varying , the loss function ( [ eq : l^t(x ) ] ) needs to be modified in a way such that the new measurement samples are given more weight than the old ones .",
    "defining the so - called `` forgetting factor '' @xmath67 , where @xmath222 , the new loss function is given as follows @xcite :    @xmath223    and as expected , when @xmath68 , ( [ eq : time - varying - estimation ] ) is as same as ( [ eq : l^t(x ) ] ) . in this case , the only modification to algorithm [ alg : iterative - algorithm ] is that @xmath46 and @xmath47 are updated according to the following recursive rule : @xmath224    for problem ( [ eq : time - varying - estimation ] ) , since the signal to be estimated is time - varying , the convergence analysis in theorem [ thm : convergence ] does not hold any more . however , simulation results show there is little loss of optimality when optimizing ( [ eq : time - varying - estimation ] ) only approximately by algorithm [ alg : iterative - algorithm ] .",
    "this establishes the superiority of the proposed algorithm over the distributed algorithm in @xcite which solves ( [ eq : time - varying - estimation ] ) exactly at the price of a large delay and a large signaling burden . besides , despite the lack of theoretical analysis , algorithm [ alg : iterative - algorithm ] performs better than the online sequential algorithm @xcite numerically , cf . figure [ fig : se - varying ] in section [ sec : numerical - results ] .",
    "convergence behavior in terms of objective function value . ]    in this section , the desirable features of the proposed algorithm are illustrated numerically .",
    "we first test the convergence behavior of algorithm [ alg : iterative - algorithm ] with the online sequential algorithm proposed in @xcite . in this example , the parameters are selected as follows :    * @xmath225 , so the subscript @xmath154 is omitted .",
    "* the dimension of @xmath34 : @xmath226 ; * the density of @xmath34 : 0.1 ; * both @xmath227 and @xmath228 are generated by i.i.d .",
    "standard normal distributions : @xmath229 and @xmath230 ; * the sparsity regularization gain @xmath231 ; * unless otherwise stated , the simulations results are averaged over 100 realizations .",
    "we plot in figure [ fig : convergence - value ] the relative error in objective value @xmath232 versus the time instance @xmath44 , where 1 ) @xmath55 is defined in ( [ eq : l1-rls ] ) and calculated by @xmath233 @xcite ; 2 ) @xmath80 is returned by algorithm [ alg : iterative - algorithm ] in the proposed online parallel algorithm ; 3 ) @xmath80 is returned by ( * ? ? ?",
    "* algorithm 1 ) in online sequential algorithm ; and 4 ) @xmath81 for both parallel and sequential algorithms .",
    "note that @xmath234 is by definition the lower bound of @xmath56 and @xmath235 for all @xmath44 . from figure",
    "[ fig : convergence - value ] it is clear that the proposed algorithm ( black curve ) converges to a precision of @xmath236 in less than 200 instances while the sequential algorithm ( blue curve ) needs more than 800 instances . the improvement in convergence speed is thus notable .",
    "if the precision is set as @xmath237 , the sequential algorithm does not even converge in a reasonable number of instances .",
    "therefore , the proposed online parallel algorithm outperforms in both convergence speed and solution quality .",
    "we also evaluate in figure [ fig : convergence - value ] the performance loss incurred by the simplified minimization rule ( [ eq : stepsize-1 ] ) ( indicated by the black curve ) compared with the standard minimization rule ( [ eq : minimization - rule-1 ] ) ( indicated by the red curve ) .",
    "it is easy to see from figure [ fig : convergence - value ] that these two curves almost coincide with each other , so the extent to which the simplified minimization rule decreases the objective function is nearly as same as standard minimization rule and the performance loss is negligible .",
    "comparison of original signal and estimated signal . ]",
    "comparison of original signal and estimated signal . ]",
    "then we consider in figure [ fig : convergence - mse ] the relative square error @xmath238 versus the time instance @xmath44 , where the benchmark is @xmath239 , i.e. , the recursive lasso ( [ eq : l1-rls ] ) . to compare the estimation approaches with and without sparsity regularization , rls in ( [ eq : l1-rls ] )",
    "is also implemented , where a @xmath1 regularization term @xmath240 is included into ( [ eq : l1-rls ] ) when @xmath46 is singular .",
    "we see that the relative square error of the proposed online parallel algorithm quickly reaches the benchmark ( recursive lasso ) in about 100 instances , while the sequential algorithm needs about 800 instances .",
    "the improvement in convergence speed is consolidated again .",
    "another notable feature is that , the relative square error of the proposed algorithm is always decreasing , even in beginning instances , while the relative square error of the sequential algorithm is not : in the first 100 instances , the relative square error is actually increasing . recall the signal dimension ( @xmath226 ) , we infer that the relative square error starts to decrease only after each element has been updated once .",
    "what is more , estimation with sparsity regularization performs better than the classic rls approach because they exploit the a prior sparsity of the to - be - estimated signal @xmath34 .",
    "the precision of the estimated signal by the proposed online parallel algorithm ( after 1000 time instances ) is also shown element - wise in figure [ fig : comparison_signal ] , from which we observe that both the zeros and the nonzero elements of the original signal @xmath34 are estimated accurately .    weight factor in time- and norm - weighted sparsity regularization . ]",
    "weight factor in time- and norm - weighted sparsity regularization . ]",
    "we now compare the proposed simplified minimization rule ( cf .",
    "( [ eq : stepsize-1])-([eq : stepsize-2 ] ) ) , coined as stepsize_simplified , with the standard minimization rule ( cf .",
    "( [ eq : minimization - rule-1 ] ) ) , coined as stepsize_optimal , in terms of the stepsize error defined as follows : @xmath241 in addition to the above parameter where @xmath242 ( in the lower subplot ) , we also simulate the case when @xmath243 ( in the upper subplot ) .",
    "we see from figure [ fig : stepsize - error ] that the stepsize error is reasonably small , namely , mainly in the interval [ -5%,5% ] , while only a few of beginning instances are outside this region , so the simplified minimization rule achieves a good trade - off between performance and complexity .",
    "comparing the two subplots , we find that , as expected , the stepsize error depends on the value of @xmath244 .",
    "we can also infer that the simplified minimization rule tends to overestimate the optimal stepsize .",
    "relative square error for recursive estimation of time - varying signals ]    in figure [ fig : weight - factor ] we simulate the weight factor @xmath245 versus the time instance @xmath44 in time- and norm - weighted sparsity regularization , where @xmath246 in the upper plot and @xmath247 in the lower plot .",
    "the parameters are as same as the above examples , except that @xmath248 and @xmath34 are generated such that the first @xmath249 elements ( where 0.1 is the density of @xmath34 ) are nonzero while all other elements are zero .",
    "the weight factors of other elements are omitted because they exhibit similar behavior as the ones plotted in figure [ fig : weight - factor ] . as analyzed , @xmath250 , the weight factor of the first element , where @xmath251 , quickly converges to 0 , while @xmath252 , the weight factor of the eleventh element , where @xmath253 , quickly converges to @xmath254 , making the overall weight factor monotonically decreasing , cf .",
    "( [ eq : l1-rls - weighted ] ) .",
    "therefore the proposed algorithm can readily be applied to the recursive estimation of sparse signals with time- and norm - weighted regularization .",
    "when the signal to be estimated is time - varying , the theoretical analysis of the proposed algorithm is not valid anymore , but we can test numerically how the proposed algorithm performs compared with the online sequential algorithm .",
    "the time - varying unknown signal is denoted as @xmath255 , and it is changing according to the following law : @xmath256 where @xmath257 for any @xmath14 such that @xmath258 , with @xmath259 and @xmath260 . in figure [ fig : se - varying ] , the relative square error @xmath261 is plotted versus the time instance . despite the lack of theoretical consolidation , we observe the online parallel algorithm is almost as same as the pseudo - online algorithm , so the inexact optimization is not an impeding factor for the estimation accuracy .",
    "this also consolidates the superiority of the proposed algorithm over @xcite where a distributed iterative algorithm is employed to solve ( [ eq : time - varying - estimation ] ) exactly , which inevitably incurs a large delay and extensive signaling .",
    "in this paper , we have considered the recursive estimation of sparse signals and proposed an online parallel algorithm with provable convergence .",
    "the algorithm is based on inexact optimization with an increasing accuracy and parallel update of all elements at each time instance , where both the update direction and the stepsize can be calculated in closed - form expressions .",
    "the proposed simplified minimization stepsize rule is well motivated and easily implementable , achieving a good trade - off between complexity and convergence speed , and avoiding the common drawbacks of the decreasing stepsizes used in literature .",
    "simulation results consolidate the notable improvement in convergence speed over state - of - the - art techniques , and they also show that the loss in convergence speed compared with the full version ( where the lasso problem is solved exactly at each time instance ) is negligible .",
    "we have also considered numerically the recursive estimation of time - varying signals where theoretical convergence do not necessarily hold , and the proposed algorithm works better than state - of - the - art algorithms .",
    "it is easy to see that @xmath262 can be divided into a smooth part @xmath263 and a nonsmooth part @xmath264 :    [ rem : decompose - of - l_t(x ) ] @xmath265    we also use @xmath266 to denote the smooth part of the objective function in ( [ eq : x - hat - scalar ] ) : @xmath267 functions @xmath266 and @xmath263 are related according to the following equation : @xmath268 from which it is easy to infer that @xmath269 . then from the first - order optimality condition , @xmath270 has a subgradient @xmath271 at @xmath272 such that for any @xmath12 : @xmath273    now consider the following equation : @xmath274 the rest of the proof consists of three parts .",
    "firstly we prove that there exists a constant @xmath275 such that @xmath276 .",
    "then we show that the sequence @xmath277 converges .",
    "finally we prove that any limit point of the sequence @xmath278 is a solution of ( [ eq : mmse ] ) .    *",
    "part 1 ) * since @xmath279 for all @xmath44 ( @xmath280 is defined in proposition [ prop : descent - direction ] ) from assumption ( a5 ) , it is easy to see from ( [ eq : descent ] ) that the following is true : @xmath281 since @xmath282 is a continuous function @xcite and @xmath46 converges to a positive definite matrix by assumption ( a1 ) , there exists a @xmath283 such that @xmath284 for all @xmath44 .",
    "we thus conclude from the preceding inequality that for all @xmath285 : @xmath286    it follows from ( [ eq : triangle - inequality ] ) , ( [ eq : stepsize-1 ] ) and ( [ eq : descent - lemma-4 ] ) that @xmath287 since the inequalities in ( [ eq : descent - lemma ] ) are true for any @xmath288 , we set @xmath289",
    ". then it is possible to show that there is a constant @xmath275 such that @xmath290 besides , because of step 3 in algorithm [ alg : iterative - algorithm ] , @xmath82 is in the following lower level set of @xmath56 : @xmath291 because @xmath292 for any @xmath8 , ( [ eq : lower - level - set ] ) is a subset of @xmath293 which is a subset of @xmath294 since @xmath46 and @xmath47 converges and @xmath135 , there exists a bounded set , denoted as @xmath295 , such that @xmath296 for all @xmath44 ; thus the sequence @xmath297 is bounded and we denote its upper bound as @xmath298 .    *",
    "part 2 ) * combining ( [ eq : basic - relation ] ) and ( [ eq : descent - property-1 ] ) , we have the following : @xmath299 where the last inequality comes from the decreasing property of @xmath63 by assumption ( a6 ) .",
    "recalling the definition of @xmath263 in ( [ rem : decompose - of - l_t(x ) ] ) , it is easy to see that @xmath300 where @xmath301    taking the expectation of the preceding equation with respect to @xmath302 , conditioned on the natural history up to time @xmath75 , denoted as @xmath303 : @xmath304 we have @xmath305\\nonumber \\\\ = \\ ; & \\mathbb{e}\\left[l^{(t+1)}(\\mathbf{x}^{(t+1)})|\\mathcal{f}^{(t+1)}\\right]-\\frac{1}{t}\\sum_{\\tau=1}^{t}\\mathbb{e}\\left[l^{(\\tau)}(\\mathbf{x}^{(t+1)})|\\mathcal{f}^{(t+1)}\\right]\\nonumber \\\\ = \\ ; & \\mathbb{e}\\left[l^{(t+1)}(\\mathbf{x}^{(t+1)})|\\mathcal{f}^{(t+1)}\\right]-\\frac{1}{t}\\sum_{\\tau=1}^{t}l^{(\\tau)}(\\mathbf{x}^{(t+1)}),\\label{eq : expectation - conditional}\\end{aligned}\\ ] ] where the second equality comes from the observation that @xmath306 is deterministic as long as @xmath303 is given .",
    "this together with ( [ eq : step2 - 0 ] ) indicates that @xmath307\\\\ \\leq\\ ; & \\mathbb{e}\\left[f^{(t+1)}(\\mathbf{x}^{(t+1)})-f^{(t)}(\\mathbf{x}^{(t+1)})|\\mathcal{f}^{(t+1)}\\right]\\\\ \\leq\\ ; & \\frac{1}{t+1}\\left(\\mathbb{e}\\left[l^{(t+1)}(\\mathbf{x}^{(t+1)})|\\mathcal{f}^{(t+1)}\\right]-\\frac{1}{t}\\sum_{\\tau=1}^{t}l^{(\\tau)}(\\mathbf{x}^{(t+1)})\\right)\\\\ \\leq\\ ; & \\frac{1}{t+1}\\left|\\mathbb{e}\\left[l^{(t+1)}(\\mathbf{x}^{(t+1)})|\\mathcal{f}^{(t+1)}\\right]-\\frac{1}{t}\\sum_{\\tau=1}^{t}l^{(\\tau)}(\\mathbf{x}^{(t+1)})\\right|,\\end{aligned}\\ ] ] and @xmath308\\right]_{0}\\nonumber \\\\ \\leq\\ ; & \\frac{1}{t+1}\\left|\\mathbb{e}\\left[l^{(t+1)}(\\mathbf{x}^{(t+1)})|\\mathcal{f}^{(t+1)}\\right]-\\frac{1}{t}\\sum_{\\tau=1}^{t}l^{(\\tau)}(\\mathbf{x}^{(t+1)})\\right|\\nonumber \\\\ \\leq\\ ; & \\frac{1}{t+1}\\sup_{\\mathbf{x}\\in\\mathcal{x}}\\left|\\mathbb{e}\\left[l^{(t+1)}(\\mathbf{x})|\\mathcal{f}^{(t+1)}\\right]-\\frac{1}{t}\\sum_{\\tau=1}^{t}l^{(\\tau)}(\\mathbf{x})\\right|,\\label{eq : step2 - 1 - 1}\\end{aligned}\\ ] ] where @xmath115_{0}=\\max(x,0)$ ] , and @xmath309 in ( [ eq : step2 - 1 - 1 ] ) with @xmath310 is the complete path of @xmath8 .",
    "now we derive an upper bound on the expected value of the right hand side of ( [ eq : step2 - 1 - 1 ] ) : @xmath311-\\frac{1}{t}\\sum_{\\tau=1}^{t}l^{(\\tau)}(\\mathbf{x})\\right|\\right]\\nonumber \\\\ = \\ ; & \\mathbb{e}\\left[\\sup_{\\mathbf{x}\\in\\mathcal{x}}\\bigl|\\breve{y}^{(t)}-(\\mathbf{r}_{2}^{(t)})^{t}\\mathbf{x}+\\mathbf{x}^{t}\\mathbf{r}_{3}^{(t)}\\mathbf{x}\\bigr|\\right]\\nonumber \\\\ \\leq\\ ; & \\mathbb{e}\\left[\\sup_{\\mathbf{x}\\in\\mathcal{x}}\\bigl|\\breve{y}^{(t)}\\bigr|+\\sup_{\\mathbf{x}\\in\\mathcal{x}}\\bigl|(\\breve{\\mathbf{b}}^{(t)})^{t}\\mathbf{x}\\bigr|+\\sup_{\\mathbf{x}\\in\\mathcal{x}}\\bigl|\\mathbf{x}^{t}\\breve{\\mathbf{g}}^{(t)}\\mathbf{x}\\bigr|\\right]\\nonumber \\\\ = \\ ; & \\mathbb{e}\\left[\\sup_{\\mathbf{x}\\in\\mathcal{x}}\\bigl|\\breve{y}^{(t)}\\bigr|\\right]+\\mathbb{e}\\left[\\sup_{\\mathbf{x}\\in\\mathcal{x}}\\bigl|(\\breve{\\mathbf{b}}^{(t)})^{t}\\mathbf{x}\\bigr|\\right]+\\mathbb{e}\\left[\\sup_{\\mathbf{x}\\in\\mathcal{x}}\\bigl|\\mathbf{x}^{t}\\breve{\\mathbf{g}}^{(t)}\\mathbf{x}\\bigr|\\right],\\label{eq : bound-1}\\end{aligned}\\ ] ] where @xmath312-(y_{n}^{(\\tau)})^{2}\\right),\\\\ \\breve{\\mathbf{b}}^{(t ) } & \\triangleq\\frac{1}{t}\\sum_{\\tau=1}^{t}\\sum_{n=1}^{n}2\\left(\\mathbb{e}_{\\{y_{n},\\mathbf{g}_{n}\\}}\\left[y_{n}\\mathbf{g}_{n}\\right]-y_{n}^{(\\tau)}\\mathbf{g}_{n}^{(\\tau)}\\right),\\\\ \\breve{\\mathbf{g}}^{(t ) } & \\triangleq\\frac{1}{t}\\sum_{\\tau=1}^{t}\\sum_{n=1}^{n}\\left(\\mathbb{e}_{\\mathbf{g}_{n}}\\left[\\mathbf{g}_{n}\\mathbf{g}_{n}\\right]-\\mathbf{g}_{n}^{(t)}\\mathbf{g}_{n}^{(\\tau)t}\\right).\\end{aligned}\\ ] ] then we bound each term in ( [ eq : bound-1 ] ) individually .",
    "for the first term , since @xmath313 is independent of @xmath80 , @xmath314=\\mathbb{e}\\left[\\bigl|\\breve{y}^{(t)}\\bigr|\\right ] & = \\mathbb{e}\\left[\\sqrt{(\\breve{y}^{(t)})^{2}}\\right]\\nonumber \\\\   & \\leq\\sqrt{\\mathbb{e}\\left[(\\breve{y}^{(t)})^{2}\\right]}\\leq\\sqrt{\\frac{\\sigma_{1}^{2}}{t}}\\label{eq : bound-2}\\end{aligned}\\ ] ] for some @xmath315 , where the second equality comes from jensen s inequality .",
    "because of assumptions ( a1 ) , ( a2 ) and ( a4 ) , @xmath313 has bounded moments and the existence of @xmath316 is then justified by the central limit theorem @xcite .    for the second term of ( [ eq : bound-1 ] ) , we have @xmath317 & \\negmedspace\\leq\\negthinspace\\mathbb{e}\\left[\\sup_{\\mathbf{x}}(\\bigl|\\breve{\\mathbf{b}}^{(t)}\\bigr|)^{t}\\left|\\mathbf{x}\\right|\\right]\\negmedspace\\leq\\negmedspace\\left(\\mathbb{e}\\left[\\bigl|\\breve{\\mathbf{b}}^{(t)}\\bigr|\\right]\\right)^{\\negmedspace t}\\negmedspace\\left|\\bar{\\mathbf{x}}\\right|.\\end{aligned}\\ ] ] similar to the line of analysis of ( [ eq : bound-2 ] ) , there exists a @xmath318 such that @xmath319\\leq\\left(\\mathbb{e}\\left[\\bigl|\\breve{\\mathbf{b}}^{(t)}\\bigr|\\right]\\right)^{t}\\left|\\bar{\\mathbf{x}}\\right|\\leq\\sqrt{\\frac{\\sigma_{2}^{2}}{t}}.\\label{eq :",
    "bound-3}\\ ] ]    for the third term of ( [ eq : bound-1 ] ) , we have @xmath320\\nonumber \\\\ = \\ ; & \\mathbb{e}\\left[\\max_{1\\leq k\\leq k}\\bigl|\\lambda_{k}(\\breve{\\mathbf{g}}^{(t)})\\bigr|\\cdot\\left\\vert \\bar{\\mathbf{x}}\\right\\vert _ { 2}^{2}\\right]\\nonumber \\\\ = \\ ; & \\left\\vert \\bar{\\mathbf{x}}\\right\\vert _ { 2}^{2}\\cdot\\mathbb{e}\\left[\\sqrt{\\max\\{\\lambda_{\\max}^{2}(\\breve{\\mathbf{g}}^{(t)}),\\lambda_{\\min}^{2}(\\breve{\\mathbf{g}}^{(t)})\\}}\\right]\\nonumber \\\\ \\leq\\ ; & \\left\\vert \\bar{\\mathbf{x}}\\right\\vert _ { 2}^{2}\\cdot\\sqrt{\\mathbb{e}\\left[\\max\\{\\lambda_{\\max}^{2}(\\breve{\\mathbf{g}}^{(t)}),\\lambda_{\\min}^{2}(\\breve{\\mathbf{g}}^{(t)})\\}\\right]}\\nonumber \\\\ \\leq\\ ; & \\left\\vert \\bar{\\mathbf{x}}\\right\\vert _ { 2}^{2}\\cdot\\sqrt{\\mathbb{e}\\left[\\sum_{k=1}^{k}\\lambda_{k}^{2}(\\breve{\\mathbf{g}}^{(t)})\\right]}\\nonumber \\\\ = \\ ; & \\left\\vert \\bar{\\mathbf{x}}\\right\\vert _ { 2}^{2}\\cdot\\sqrt{\\mathbb{e}\\left[\\textrm{tr}\\left(\\breve{\\mathbf{g}}^{(t)}(\\breve{\\mathbf{g}}^{(t)})^{t}\\right)\\right]}\\leq\\sqrt{\\frac{\\sigma_{3}^{2}}{t}}\\label{eq : bound-4}\\end{aligned}\\ ] ] for some @xmath321 , where the first equality comes from the observation that @xmath8 should align with the eigenvector associated with the eigenvalue with largest absolute value . then combing ( [ eq : bound-2])-([eq : bound-4 ] ) , we can claim that there exists @xmath322 such that @xmath323-\\frac{1}{t}\\sum_{\\tau=1}^{t}l^{(\\tau)}(\\mathbf{x})\\right|\\right]\\leq\\frac{\\sigma}{\\sqrt{t}}.\\ ] ] in view of ( [ eq : step2 - 1 - 1 ] ) , we have @xmath324\\right]_{0}\\right]\\leq\\frac{\\sigma}{t^{3/2}}.\\label{eq : step2 - 1}\\ ] ] summing ( [ eq : step2 - 1 ] ) over @xmath44 , we obtain @xmath325\\right]_{0}\\right]<\\infty.\\ ] ] then it follows from the quasi - martingale convergence theorem ( cf .",
    "* th . 6 ) ) that @xmath326 converges almost surely .    *",
    "part 3 ) * combining ( [ eq : basic - relation ] ) and ( [ eq : descent - property-1 ] ) , we have @xmath327 besides , it follows from the convergence of @xmath277 @xmath328 and the strong law of large numbers that @xmath329 taking the limit inferior of both sides of ( [ eq : step3 - 2 ] ) , we have @xmath330 so we can infer that @xmath331 .",
    "since @xmath332 , we can infer that @xmath333 and thus @xmath334 .",
    "consider any limit point of the sequence @xmath278 , denoted as @xmath335 .",
    "since @xmath336 is a continuous function of @xmath8 in view of ( [ eq : x - hat - scalar ] ) and @xmath337 , it must be @xmath338 , and the minimum principle in ( [ eq : minimum - principle ] ) can be simplified as @xmath339 whose summation over @xmath340 leads to @xmath341 therefore @xmath335 minimizes @xmath342 and @xmath343 almost surely by lemma [ lem : connection ] .",
    "since @xmath34 is unique in view of assumptions ( a1)-(a3 ) , the whole sequence @xmath297 has a unique limit point and it thus converges to @xmath34 . the proof is thus completed .",
    "the authors would like to thank prof .",
    "gesualdo scutari for the helpful discussion .",
    "s.  barbarossa , s.  sardellitti , and p.  di lorenzo , `` distributed detection and estimation in wireless sensor networks , '' in _ academic press library in signal processing _",
    ", r.  chellappa and s.  theodoridis , eds . , 2014 , vol .  2 , pp . 329408 .",
    "y.  yang , g.  scutari , p.  song , and d.  p. palomar , `` robust mimo cognitive radio systems under interference temperature constraints , '' _ ieee journal on selected areas in communications _ , vol .",
    "31 , no .  11 , pp . 24652482 , nov .",
    "2013 .",
    "kim , e.  dallanese , and g.  b. giannakis , `` cooperative spectrum sensing for cognitive radios using kriged kalman filtering , '' _ ieee journal of selected topics in signal processing _ , vol .  5 , no .  1 ,",
    "2436 , feb .",
    "f.  zeng , c.  li , and z.  tian , `` distributed compressive spectrum sensing in cooperative multihop cognitive networks , '' _ ieee journal of selected topics in signal processing _ , vol .  5 , no .  1 ,",
    "3748 , feb .",
    "2011 .",
    "g.  mateos , i.  schizas , and g.  giannakis , `` distributed recursive least - squares for consensus - based in - network adaptive estimation , '' _ ieee transactions on signal processing _ , vol .",
    "57 , no .  11 , pp .",
    "45834588 , nov .",
    "2009 .",
    "d.  angelosante , j.  a. bazerque , and g.  b. giannakis , `` online adaptive estimation of sparse signals : where rls meets the @xmath344-norm , '' _ ieee transactions on signal processing _ ,",
    "58 , no .  7 , pp . 34363447 , jul .",
    "y.  kopsinis , k.  slavakis , and s.  theodoridis , `` online sparse system identification and signal reconstruction using projections onto weighted @xmath344 balls , '' _ ieee transactions on signal processing _ , vol .",
    "59 , no .  3 , pp .",
    "936952 , mar .",
    "r.  tibshirani , `` regression shrinkage and selection via the lasso : a retrospective , '' _ journal of the royal statistical society : series b ( statistical methodology ) _ , vol .",
    "58 , no .  1 ,",
    "267288 , jun . 1996 .",
    "m.  a.  t. figueiredo , r.  d. nowak , and s.  j. wright , `` gradient projection for sparse reconstruction : application to compressed sensing and other inverse problems , '' _ ieee journal of selected topics in signal processing _ ,",
    "vol .  1 , no .  4 , pp .",
    "586597 , dec . 2007 .",
    "kim , k.  koh , m.  lustig , s.  boyd , and d.  gorinevsky , `` an interior - point method for large - scale @xmath345-regularized least squares , '' _ ieee journal of selected topics in signal processing _ , vol .  1 , no .  4 , pp .",
    "606617 , dec . 2007 .",
    "f.  facchinei , g.  scutari , and s.  sagratella , `` parallel selective algorithms for big data optimizatio , '' dec .",
    "2013 , accepted in _",
    "ieee trans . on signal process .",
    "_ [ online ] .",
    "available : http://arxiv.org/abs/1402.5521      g.  scutari , f.  facchinei , p.  song , d.  p. palomar , and j .- s .",
    "pang , `` decomposition by partial linearization : parallel optimization of multi - agent systems , '' _ ieee transactions on signal processing _ ,",
    "62 , no .  3 , pp .",
    "641656 , feb . 2014 .",
    "y.  yang , g.  scutari , d.  p. palomar , and m.  pesavento , `` a parallel stochastic approximation method for nonconvex multi - agent optimization problems , '' oct .",
    "2014 , submitted to _ ieee transactions on signal processing_. [ online ] .",
    "available : http://arxiv.org/abs/1410.5076      s.  boyd , n.  parikh , e.  chu , b.  peleato , and j.  eckstein , `` distributed optimization and statistical learning via the alternating direction method of multipliers , '' _ foundations and trends in machine learning _ , vol .  3 , no .  1 , 2010 ."
  ],
  "abstract_text": [
    "<S> in this paper , we consider a recursive estimation problem for linear regression where the signal to be estimated admits a sparse representation and measurement samples are only sequentially available . </S>",
    "<S> we propose a convergent parallel estimation scheme that consists in solving a sequence of @xmath0-regularized least - square problems approximately . </S>",
    "<S> the proposed scheme is novel in three aspects : i ) all elements of the unknown vector variable are updated in parallel at each time instance , and convergence speed is much faster than state - of - the - art schemes which update the elements sequentially ; ii ) both the update direction and stepsize of each element have simple closed - form expressions , so the algorithm is suitable for online ( real - time ) implementation ; and iii ) the stepsize is designed to accelerate the convergence but it does not suffer from the common trouble of parameter tuning in literature . both centralized and distributed implementation schemes are discussed . </S>",
    "<S> the attractive features of the proposed algorithm are also numerically consolidated . </S>"
  ]
}