{
  "article_text": [
    "let @xmath0 be an @xmath2 large , sparse , hermitian matrix . in many applications in science and engineering",
    ", one would like to find eigenvalues and eigenfunctions of @xmath0 near a given real number @xmath1 . as a motivating problem",
    ", we consider @xmath0 to be obtained from a certain discretization ( e.g. finite difference or finite element discretization ) of a second order partial differential operator of the form @xmath3 , where @xmath4 is the laplacian operator , and @xmath5 is a potential function . depending on the context and the choice of @xmath6 , this type of problems can arise from quantum mechanics , wave propagation , electromagnetism etc .",
    "when @xmath1 locates inside the spectrum of @xmath0 , the eigenvalues to be computed are called interior eigenvalues . these interior eigenvalues and corresponding eigenfunctions",
    "are in general difficult to compute .",
    "since @xmath7 is large and @xmath0 is sparse , iterative methods such as inverse power method  @xcite , preconditioned conjugate gradient type of methods  @xcite , and shift - inverse lanczos type of methods  @xcite are desirable .",
    "the effectiveness of such methods often depends on the availability of a good preconditioner that can approximately apply @xmath8 to vectors , and such preconditioner can be difficult to construct .",
    "another type of methods that recently receives increasing amount of attention is based on the construction of a matrix function @xmath9 , where the corresponding scalar function @xmath10 only takes significant values on a small interval near @xmath1 on the real line .",
    "such a matrix function @xmath9 can be called a _",
    "spectrum slicing _ operator , since for any vector @xmath11 , @xmath12 is approximately only spanned by eigenvectors of @xmath0 corresponding to eigenvalues near @xmath1 , and the vector @xmath12 is said to be _ spectrally localized_. the spectrum slicing operator can be simultaneously applied to a set of random vectors @xmath13 $ ] .",
    "when @xmath14 is large enough but is still small compared to @xmath7 , the subspace spanned by @xmath15 will approximately contain the subspace of all eigenvectors corresponding to eigenvalues near @xmath1 .",
    "let @xmath16 then the desired eigenvalues and eigenvectors can be computed via the solution of a generalized eigenvalue problem @xmath17 in practice @xmath9 can be constructed through relatively high order chebyshev polynomials  @xcite , or contour integral based methods  @xcite .",
    "it should be noted that contour integral based methods still require solving equations of the form @xmath18 where @xmath19 is close to @xmath1 in the complex plane , either through direct methods or iterative methods .    in general the spectrum slicing operator",
    "@xmath9 is a dense matrix .",
    "therefore the matrix @xmath20 is in general a dense matrix , regardless of how the initial matrix @xmath21 is chosen .",
    "the matrices @xmath22 are in general dense matrices that do not reveal much structure to be further exploited , and the solution of the projected problem   may still be expensive when @xmath14 is large .      in this paper , we consider the use of a simple choice of gaussian function with a positive number @xmath23 @xmath24 and the corresponding matrix gaussian function @xmath25 is spectrally localized near @xmath1 with width proportional to @xmath23 . we demonstrate that under a proper choice of @xmath23 , @xmath25 can have many entries that are small in magnitude , so that after truncating these small entries the resulting matrix is close to be a spectrum slicing operator but is also sparse . in this sense",
    ", @xmath25 is called a _ localized spectrum slicing ( lss ) operator_.",
    "we demonstrate that the lss operator @xmath25 can be constructed in a divide - and - conquer method with controllable error using only a sequence of submatrices of @xmath0 with @xmath26 cost , under certain assumptions of the behavior of the sparsity , spectral radius , and sizes of submatrices of @xmath0 as @xmath7 increases .",
    "the column space of the lss operator is spanned by a sparse matrix @xmath27 , and the subspace spanned by @xmath28 will approximately contain the subspace of eigenvectors to be computed . as a result ,",
    "the projected matrices @xmath29 are sparse matrices . in this aspect",
    ", the matrix @xmath28 can be regarded as a specially tailored basis set for representing the subspace approximately spanned by eigenvectors of @xmath0 near @xmath1 , and each column of @xmath28 is localized both spectrally and spatially . in the following text @xmath28",
    "is called a _ localized spectrum slicing ( lss ) basis set_. the lss basis set can be constructed without explicitly constructing the lss operator . the generalized eigenvalue problem for the sparse projected matrices @xmath30 may be solved both by direct methods , but also by methods using sparse linear algebra techniques . during the construction of the lss operator and/or the lss basis set , a good global preconditioner for @xmath8 is not needed .",
    "we demonstrate the construction of the lss basis set and its use for solving interior eigenvalues problems for matrices obtained from discretizing second order partial differential operators , and find that the use of the lss basis set can be more efficient than solving the global problem directly for matrices of large sizes .",
    "we also apply the lss method to a general matrix from the university of florida matrix collection  @xcite .",
    "the spectral locality of the lss operator is valid by construction .",
    "comparatively the spatial locality of the lss operator is less obvious , and is given more precisely by the _ decay properties _ of matrix functions that are analytic in a certain region in the complex plane ( see e.g.  @xcite ) .",
    "the decay properties of matrix functions were first realized for matrix inverse @xmath31 ( i.e. @xmath32 ) , where @xmath0 is a banded , positive definite matrix  @xcite .",
    "the method for showing decay properties relies on whether @xmath33 can be well approximated by a low order chebyshev polynomial evaluated at the eigenvalues of @xmath0 , and this method is therefore generalizable to any analytic function @xmath33 for banded matrices @xmath0 .",
    "in order to generalize from banded matrices to general sparse matrices , decay properties should be defined using geodesic distances of the graph induced by @xmath0 .",
    "these techniques have been shown in  @xcite and references therein , for demonstrating the decay properties of e.g. fermi - dirac operators in electronic structure theory .",
    "these techniques are directly used for showing the decay properties of the lss operator in this work , which then allows the construction of the divide - and - conquer method . in physics literature",
    ", such decay property is dubbed `` near - sightedness property '' and is vastly studied using various models ( see e.g.  @xcite ) .",
    "the decay property is also used for constructing linear scaling algorithms  @xcite for density functional theory calculations .",
    "the rest of this paper is organized as follows .",
    "we introduce the decay properties of matrix functions and in particular the localized spectrum slicing operator in section  [ sec : prelim ] .",
    "based on the decay properties , section  [ sec : lss ] describes a divide - and - conquer algorithm for constructing the lss operator and the lss basis set , and provides the error bound and computational complexity . the computation of interior eigenvalues and a domain partitioning strategy for general sparse matrices are also discussed .",
    "we demonstrate numerical results using the lss basis set for solving interior eigenvalue problems in section  [ sec : numer ] , and discuss the conclusion and future work in section  [ sec : conclusion ] .",
    "the @xmath34-th element of a matrix @xmath35 is denoted by @xmath36 .",
    "the submatrix of @xmath0 corresponding to a set of row indices @xmath37 and a set of column indices @xmath38 is denoted by @xmath39 . using matlab notation , all elements in the @xmath40-th row of @xmath0 are denoted by @xmath41 , and all elements in a set of rows @xmath37 are denoted by @xmath42 .",
    "similarly , all elements in the @xmath43-th column of @xmath0 are denoted by @xmath44 , and all elements for a set of columns @xmath38 are denoted by @xmath45 .",
    "the @xmath46-th power of @xmath0 is denoted by @xmath47 .",
    "the matrix @xmath14-norm of @xmath0 is denoted by @xmath48 , and the vector @xmath14-norm of a vector @xmath49 is denoted by @xmath50 ( @xmath51 ) .",
    "the max norm of a matrix is denoted by @xmath52 , which is the same as the @xmath53-norm of a vector of length @xmath54 , formed by all the elements of @xmath0 .",
    "the hermitian conjugate of @xmath0 is denoted by @xmath55 .",
    "depending on the context , we may also refer to a matrix as an _",
    "operator_.    a hermitian matrix @xmath0 induces an undirected graph @xmath56 with @xmath57 , and @xmath58 .",
    "each element in @xmath59 is called a vertex , and each element in @xmath60 is called an edge .",
    "the cardinality of a set of indices @xmath37 is denoted by @xmath61 .",
    "a hermitian matrix @xmath35 has the eigen - decomposition @xmath62 here @xmath63 $ ] is a diagonal matrix containing the ( real ) eigenvalues of @xmath0 and we assume @xmath64 are ordered non - decreasingly .",
    "$ ] and @xmath66 is the eigenvector corresponding to the eigenvalue @xmath67 .",
    "if all eigenvalues ( and corresponding eigenvectors ) to be computed are with in a small interval @xmath68 on the real line with @xmath69 , then this problem is called an interior eigenvalue problem .      in this section ,",
    "we provide a short but self - contained description of the decay properties of @xmath25 .",
    "more details on the description of the decay properties of general matrix functions can be found in  @xcite and references therein .",
    "let @xmath46 be a non - negative integer , and @xmath70 be the set of all polynomials of degrees less than or equal to @xmath46 with real coefficients .",
    "without loss of generality we assume the eigenvalues of @xmath0 are within the interval @xmath71 . for a real continuous function @xmath72 on",
    "@xmath73 $ ] , the best approximation error is defined as @xmath74 consider an ellipse in the complex plane @xmath75 with foci in @xmath76 and @xmath77 , and @xmath78 be the half axes so that the vertices of the ellipse are @xmath79 , respectively .",
    "let the sum of the half axes be @xmath80 , then using the identity @xmath81 we have @xmath82 thus the ellipse is determined only by @xmath83 , and such ellipse is denoted by @xmath84 . then bernstein s theorem  @xcite is stated as follows .",
    "let @xmath33 be analytic in @xmath84 with @xmath85 , and @xmath33 is a real valued function for real @xmath19 .",
    "then @xmath86 where @xmath87 [ thm : bernstein ]    using theorem  [ thm : bernstein ] , a more quantitative description of the approximation properties for @xmath88 in eq .",
    "is given in theorem  [ thm : approxgaussian ] .",
    "let @xmath88 be a gaussian function defined in eq .",
    ", then for any @xmath89 , @xmath90 [ thm : approxgaussian ]    for any @xmath91 , the gaussian function @xmath92 is analytic in any ellipse @xmath84 with @xmath85 , then @xmath93 for any @xmath89 , let @xmath94 then @xmath95 , and @xmath96 using theorem  [ thm : bernstein ] , eq .",
    "is the direct consequence of eq .   and the choice of @xmath83 in eq .  .    for the graph @xmath56 associated with the matrix @xmath0 and vertices @xmath97 , a path linking @xmath98",
    "is given by a sequence of edges @xmath99 where @xmath100 , and each element in @xmath14 is an edge in @xmath60 .",
    "the length of the path @xmath14 is defined to be @xmath101 . if @xmath102 then the length of @xmath14 is @xmath77 .",
    "the _ geodesic distance _",
    "@xmath103 between vertices @xmath40 and @xmath43 is defined as the length of the shortest path between @xmath40 and @xmath43 .",
    "it should be noted that for structurally symmetric matrices , i.e. @xmath104 implies @xmath105 for all indices @xmath98 , the geodesic distance is symmetric , i.e. @xmath106 . in particular , hermitian matrices are structurally symmetric . if @xmath107 , then @xmath108 . if @xmath109 then there is no path connecting @xmath40 and @xmath43 . more generally , for any positive integer @xmath46 , if @xmath110 then @xmath111 , where @xmath47 is the @xmath46-th power of the matrix @xmath0 .    the precise statement of the spatial locality of the matrix function @xmath25 is given by the decay properties along the off - diagonal direction in theorem  [ thm : decaygauss ] . for a given column @xmath43",
    ", the magnitude of each element @xmath112 decays exponentially with respect to the geodesic distance @xmath103 .",
    "let @xmath0 be a sparse and hermitian matrix with all eigenvalues contained in the interval @xmath71 .",
    "for any @xmath113 , let @xmath114 then for all @xmath115 , @xmath116 where @xmath103 is the geodesic distance between vertices @xmath40 and @xmath43 .",
    "[ thm : decaygauss ]    for any integer @xmath117 , there exists a polynomial @xmath118 such that @xmath119 the last inequality follows from theorem  [ thm : approxgaussian ]",
    ". now consider all edges @xmath34 such that the geodesic distance @xmath120 , and then @xmath121 .",
    "therefore @xmath122    as suggested in eq .  ,",
    "@xmath123 only depend on @xmath23 but not on @xmath1",
    ". therefore the decay properties of the matrix function @xmath124 seem to be independent of the shift @xmath1 .",
    "this is because an upper bound for @xmath125 is given in theorem  [ thm : approxgaussian ] that is valid for all @xmath1 .",
    "numerical results in section  [ sec : numer ] indicate that the preconstant of the exponential decay may have a strong dependency on @xmath1 , and such dependency can be specific to the matrix under study .    in theorem  [ thm : decaygauss ] there is an arbitrary positive constant @xmath126 . for",
    "any given @xmath89 , the off - diagonal entries of @xmath127 should decay exponentially with respect to the geodesic distance . by optimizing @xmath126 together with the degree of the chebyshev polynomial @xmath46",
    ", the actual decay rate can be slightly faster than exponential .",
    "[ fig : superexpdecay ] gives an example of the magnitude of the first column @xmath128 where @xmath0 is a discretized laplacian operator in 1d with periodic boundary conditions , with @xmath129 and @xmath130 respectively . although the discretized 1d laplacian matrix is a banded matrix , all its eigenfunctions are plane waves which are fully delocalized in the global domain .",
    "nonetheless the upper bound of the decay rate of the lss operator is clearly exponential as shown in fig .",
    "[ fig : superexpdecay ] .    .",
    "@xmath0 is a discretized laplacian operator in 1d with periodic boundary conditions , with @xmath131 and @xmath132 , respectively.,scaledwidth=40.0% ]    in order to limit the numerical rank of @xmath25 in practice , it is desirable to use a small @xmath23 . with fixed @xmath126 and",
    "assume @xmath133 , we have @xmath134 here @xmath23 reflects the spectral locality , and @xmath103 reflects the spatial locality , which reveals the balance between the spectral and spatial locality , tuned by one parameter @xmath23 .",
    "using the decay properties of the lss operator @xmath25 in theorem  [ thm : decaygauss ] , a set of basis functions called the lss basis set can be constructed in a divide - and - conquer fashion .",
    "below we demonstrate that if the smearing parameter @xmath23 is large enough , then the localized spectrum slicing operator @xmath135 can be approximately computed using submatrices of @xmath0 .",
    "the size of each submatrix is independent of the size of @xmath0 .",
    "this is important for reducing the computational complexity and for parallel computation .",
    "let @xmath0 , @xmath136 be @xmath2 hermitian matrices .",
    "the graph induced by @xmath136 is a spanning subgraph of the graph @xmath137 induced by @xmath0 , and the geodesic distance @xmath103 is defined using the graph @xmath137 .",
    "we assume for a given integer @xmath138 ( @xmath139 , @xmath140 then for any integer @xmath46 ( @xmath141 ) , @xmath142 [ prop : product ]    the statement is apparently correct for @xmath143 .",
    "assume the statement for @xmath144 is proved , and we prove the statement is true for @xmath46 .",
    "first @xmath145 in the summation above , @xmath146 is nonzero only if @xmath147",
    ". similarly @xmath148 is nonzero only if @xmath149 .",
    "since the graph induced by @xmath136 is a subgraph of the graph induced by @xmath0 , @xmath149 implies @xmath147 , and therefore we only need to consider @xmath14 such that @xmath147 , i.e. @xmath150 . consider @xmath40 such that @xmath151 , then @xmath14 satisfies @xmath152 also for any @xmath153 such that @xmath154 , by the assumption that the statement for @xmath144 is proved , @xmath155 . together with @xmath156",
    ", we have @xmath157 .",
    "therefore @xmath158 is valid for all @xmath159 such that @xmath160 .    using proposition  [ prop : product ]",
    ", theorem  [ thm : truncate ] shows that the @xmath43-th column of @xmath25 can be accurately computed from @xmath161 , as long as @xmath0 and @xmath136 are sufficiently close in the vicinity of @xmath43 in the sense of geodesic distance .",
    "let @xmath0 , @xmath136 be @xmath2 hermitian matrices with eigenvalues in @xmath71 . for a given @xmath43 and an even integer @xmath162 ( @xmath163 ) , @xmath164 then @xmath165 for all @xmath40 such that @xmath166 , where the constants @xmath167 are given in eq .  .",
    "[ thm : truncate ]    for any @xmath98 and @xmath117 we have @xmath168 take @xmath169 .",
    "for any @xmath40 such @xmath170 , by proposition  [ prop : product ] , @xmath171 . also from theorem",
    "[ thm : approxgaussian ] , we have @xmath172 @xmath173 and hence the result .",
    "theorem  [ thm : truncate ] shows that in order to compute any column @xmath43 of the matrix @xmath25 up to certain accuracy , it is only necessary to have a matrix that is the same as @xmath0 up to a certain distance away from @xmath43 .",
    "together with the decay property of each column of @xmath25 , this allows the @xmath43-th column of @xmath25 to be constructed in a _ divide and conquer _ manner .",
    "for instance , for a given integer @xmath162 we can define @xmath174 which is simply a submatrix of @xmath0 . as a submatrix ,",
    "@xmath175 and the assumption of the spectral radius in theorem  [ thm : truncate ] is satisfied .    in practice",
    "it would be very time consuming to construct an approximate matrix for each column of @xmath43 , since the rank of the lss operator @xmath25 is often much less compared to @xmath7 .",
    "for structured matrices such as matrices obtained from finite difference or finite element discretization of pde operators , it is often possible to partition the domain into well structured disjoint columns sets , and apply the truncated matrix to each column set .",
    "the cost for generating such partition can be very small if the structure of the matrix is known _ a priori_. for the discussion below , we assume that the partition @xmath176 into @xmath177 simply connected disjoint sets @xmath178 is given , i.e. @xmath179 for general sparse matrices , such partition may not be readily available .",
    "we discuss the choice of domain partitioning strategy in section  [ subsec : lssgeneral ] .    for each @xmath180 and an integer @xmath162 , we define an associated set @xmath181 theorem  [ thm : truncate ] implies that the submatrix @xmath182 can be constructed by a submatrix of @xmath0 defined as @xmath183 in the following discussion , we refer to @xmath180 as an _ element _ , and to @xmath184 as an _ extended element _ associated with @xmath180 .",
    "it should be noted that the zero entries of @xmath185 outside the index set @xmath184 do not need to be explicitly stored .    the choice in eq",
    ".   takes a submatrix of @xmath0 to compute the localized spectrum slicing operator . from the point of view of partial differential operators ,",
    "this is similar to imposing zero dirichlet boundary condition on some local domains .    since @xmath0 is hermitian and sparse , and so is @xmath185 , and the latter has the eigen - decomposition @xmath186 here @xmath187 is a diagonal matrix .",
    "note that @xmath185 only takes nonzero values on the extended element @xmath184 .",
    "the entries of each column of @xmath188 outside the index set @xmath184 can be set to zero , and such zero entries do not need to be explicitly stored .",
    "this is equivalent to solving an eigenvalue problem of size @xmath189 .",
    "define @xmath190 using theorem  [ thm : truncate ] , @xmath191 can be approximated by @xmath192 , in the sense that @xmath193 since @xmath92 is spectrally localized , in practice not all eigenvalues and eigenvectors of @xmath185 as in   are needed .",
    "instead only a _ partial eigen - decomposition _ is needed to compute all eigenvalues of @xmath185 in the interval @xmath194 . due to the fast decay properties of gaussian functions , in practice @xmath195",
    "can be chosen to be @xmath196 to be sufficiently accurate .",
    "we denote by @xmath197 the column dimension of @xmath188 in the partial eigen - decomposition of @xmath185 .",
    "the factorized representation in eq .   also allows the computation of a set of vectors approximately spanning the column space of @xmath192 , through a local singular value decomposition ( svd ) procedure , i.e. @xmath198 here @xmath199 is svd truncation criterion .",
    "the size of the matrix for the svd decomposition is @xmath200 . in practice @xmath199",
    "may also be chosen using a relative criterion as @xmath201 in used in our numerical experiment , where we assume @xmath202 is the largest singular value in eq .  .",
    "in practice this can be performed by only keeping the singular values in the diagonal matrix @xmath203 that are larger than @xmath199 .",
    "then we can define @xmath204 we combine all @xmath205 together @xmath206 ,    \\label{eqn : lssbasis}\\ ] ] and @xmath28 is the lss basis set that is both spectrally localized and spatially localized .",
    "we denote by @xmath207 the total number of columns of @xmath28 , which is also referred to as the size of the lss basis set . using the lss basis set , an approximation to the lss operator",
    "is defined as @xmath208 @xmath209 is an @xmath2 sparse matrix , and the error in the max norm for approximating the lss operator @xmath25 is given in theorem  [ thm : lss ] .",
    "let @xmath0 be an @xmath2 hermitian matrix with eigenvalues in @xmath71 , and the induced graph is partitioned into @xmath177 elements @xmath210 .",
    "for each element @xmath180 , there is an extended element @xmath184 given in  , a submatrix @xmath185 given in  , and matrices @xmath211 satisfying and .",
    "let @xmath209 be an @xmath2 matrix defined in eq .",
    ", then @xmath212 [ thm : lss ]    for each element @xmath213 , from eq .",
    "we have @xmath214 using theorem  [ thm : truncate ] and the definition of the extended element   @xmath215 for vertices @xmath216 , @xmath217 .",
    "then from theorem  [ thm : decaygauss ] and use @xmath218 @xmath219 combining eqs .",
    ", , , we have @xmath220    theorem  [ thm : lss ] indicates that in order to accurately approximate the lss operator , the svd truncation criterion @xmath199 must be small enough .",
    "however , this may not necessarily be the case for approximating interior eigenvalues .",
    "this will be discussed in section  [ sec : numer ] .",
    "finally , we summarize the algorithm for finding the divide - and - conquer method for constructing the lss basis set in algorithm  [ alg : lss ] .      in order to simplify the analysis of the complexity of the algorithm  [ alg : lss ] for finding the lss basis set",
    ", we make the assumption that the set of @xmath7 vertices is equally divided into @xmath177 elements , so that @xmath221",
    ". as @xmath7 increases we assume @xmath222 can be kept as a constant , i.e. the number of elements @xmath177 increases proportionally with respect to @xmath7 .",
    "@xmath223 , where @xmath224 is a small number denoting the ratio between the size of the extended element and the size of the element . for instance , for the discretized 1d and 2d laplacian operators in the numerical examples , @xmath224",
    "is set to be @xmath225 and @xmath226 , respectively .",
    "denote by @xmath197 the column dimension of @xmath188 in the partial eigen - decomposition of @xmath185 , and by @xmath227 the column dimension of @xmath205 with @xmath228 .",
    "for simplicity we assume @xmath229 are uniform i.e. @xmath230 . if @xmath185 is treated as a dense matrix for the computation of the local eigen - decomposition of @xmath185 , then the cost is @xmath231 .",
    "the cost of the svd decomposition is @xmath232 .",
    "the cost of matrix multiplication to obtain @xmath205 is @xmath233 .",
    "so the total cost for finding the lss basis set is proportional to @xmath234 if we assume that as @xmath7 increases , the spectral radius of @xmath0 does not increase , then all constants in the parenthesis in the right hand side of eq .",
    "are independent of @xmath7 , and the overall computational complexity for finding the lss basis set is @xmath26 .    in practice",
    "the constant for the finding the local eigen - decomposition can be large due to the term @xmath235 in eq .  .",
    "since @xmath185 is still a sparse matrix on @xmath184 , iterative methods can be used to reduce the computational cost to @xmath236 .",
    "this modifies the overall complexity to be @xmath237 however , it should be noted that the preconstant @xmath238 might be larger than @xmath239 . whether direct or iterative method should be used to solve the local eigenvalue problem may depend on a number of practical factors such as the size of the local problem , and the availability of efficient preconditioner on the local domain etc .      using the lss basis set in",
    ", one may compute the interior eigenvalues near @xmath1 together with its associated eigenvectors .",
    "this can be done by using the projected matrices @xmath30 according to eq .  .",
    "due to the spatial sparsity of @xmath28 , @xmath30 are also sparse matrices , and can be assembled efficiently with local computation .",
    "first , the matrix multiplication @xmath240 can be performed locally .",
    "this is because each column of @xmath205 is localized in @xmath184 , then @xmath241 second , denote by @xmath242 then for each @xmath213 it is sufficient to loop over elements @xmath243 so that @xmath244 is non - empty .",
    "the details for constructing the projected matrices are given in algorithm  [ alg : assembly ] .",
    "let @xmath30 be zero matrices of size @xmath245 .",
    "symmetrize @xmath246 .",
    "after @xmath30 are assembled , the eigenvalues and corresponding eigenvectors near @xmath1 can be solved in various ways .",
    "when the size of the lss basis set @xmath207 is small , one can treat @xmath30 as dense matrices and solve the generalized eigenvalue problem @xmath247 and only keep the ritz values @xmath248 $ ] and corresponding ritz vectors @xmath249 near @xmath1 .",
    "each column of the ritz vector @xmath250 can be partitioned according to the element partition @xmath251 as @xmath252^{t}.\\ ] ] then an approximate eigenvector for @xmath0 can be computed as @xmath253    we remark that in the computation of interior eigenvalues , spurious eigenvalues may appear . a spurious eigenvalue is a ritz value @xmath254 near the vicinity of @xmath1 as obtained from eq .  , but the corresponding vector @xmath255 as given in eq .",
    "is not an approximate eigenvector .",
    "the appearance of spurious eigenvalue is also referred to as spectral pollution  @xcite , and could be identified by computing the residual @xmath256 a ritz value @xmath254 corresponding to large residual norm @xmath257 should be removed .",
    "note that the residual can also be computed with local computation @xmath258 where @xmath259 is given in  .",
    "our numerical experience indicates that the use of residual is an effective way for identifying spurious eigenvalues when the lss basis set is accurate enough for approximating the subspace spanned by the eigenvectors to be computed .",
    "in such case the norm of the residual for most ritz values is small and the norm of the residual for the spurious eigenvalue stands out .",
    "when the basis set can not accurately capture all the eigenvalues in the prescribed interval especially for those clustered near the boundary of the interval , it becomes more difficult to identify all the spurious eigenvalues .",
    "for a general sparse matrix @xmath0 , we discuss here the strategy to partition the associated undirected graph @xmath56 into @xmath177 elements @xmath260 .",
    "intuitively we would like to choose a partition that keeps all @xmath180 to have similar sizes , while minimizing the number of edges that is being cut by the partition , i.e. @xmath261 . here @xmath262 if @xmath104 and @xmath263 otherwise .",
    "this is called a minimal @xmath177-cut problem .",
    "it is known that the minimal @xmath177-cut problem is np - hard .",
    "various heuristic methods have been developed .",
    "here we use the nested dissection approach  @xcite as implemented in the metis  @xcite package .",
    "the nested dissection approach can find an approximate minimal @xmath264-cut of the graph , and then recursively partitions each part of the graph , with iterative adjustment of the size of @xmath180 .",
    "for each @xmath213 we define a neighbor list @xmath265 , which consists of @xmath213 itself , as well as other element indices @xmath266 such that there exists at least one pair of indices @xmath267 and @xmath104 .",
    "then the extended element @xmath184 is defined as the collection of all indices in @xmath243 such that @xmath268 .",
    "algorithm  [ alg : partition ] gives a pseudo - code for generating the elements @xmath251 , the neighbor lists @xmath269 , and the extended elements @xmath270 . in terms of implementation , the partition of the graph is given by a _ graph partition map _",
    "@xmath271 such that @xmath272 , and @xmath271 can be directly returned from a graph partitioning package such as metis .",
    "@xmath273 .",
    "@xmath274 .",
    "@xmath275 .",
    "in this section we demonstrate the accuracy and efficiency of the divide - and - conquer procedure for computing the lss operator and the lss basis set , and for computing interior eigenvalues .",
    "all the computation is performed on a single computational thread of an intel i7 cpu processor with @xmath277 gigabytes ( gb ) of memory using matlab .",
    "the matrix @xmath0 is obtained from a discretized second order partial differential operator @xmath278 in one - dimension ( 1d ) and in two - dimension ( 2d ) with periodic boundary conditions , and a general matrix from the university of florida matrix collection .      in the 1d case , the global domain is @xmath279 $ ] .",
    "the laplacian operator is discretized using a 3-point finite difference stencil .",
    "the domain is uniformly discretized into @xmath280 grid points so that @xmath281 , with the grid spacing @xmath282 .",
    "all the @xmath7 grid points ( vertices ) are uniformly and contiguously partitioned into @xmath177 elements @xmath178 . for simplicity",
    "let @xmath184 be the union of @xmath180 and its two neighbors taking into account the periodic boundary condition , i.e. @xmath283 the potential @xmath5 is given by the sum of @xmath284 exponential functions as @xmath285 here @xmath286 are a set of equally spaced points .",
    "the distance between two points @xmath287 and @xmath288 is defined to be the minimal distance between @xmath287 and all the periodic images of @xmath288 , i.e. @xmath289 in order to study the performance of the algorithm for systems of increasing sizes , we set @xmath290 so that the length of the computational domain is proportional to the number of potential wells @xmath284 . to show that we do not take advantage of the periodicity of the potential , we introduce some randomness in each exponential function .",
    "we choose @xmath291 , which is a gaussian random variable with a mean value @xmath292 and a standard deviation @xmath293 .",
    "similarly the width of the exponential function @xmath294 .",
    "one realization of the potential with @xmath295 is given in fig .",
    "[ fig : vx1d ] ( a ) , with the partition of elements indicated by black dashed lines . for the choice of parameter @xmath296 and @xmath131 , fig .",
    "[ fig : vx1d ] ( b ) shows the function @xmath297 evaluated on the eigenvalues of @xmath0 plotted in log - scale in the interval @xmath298 , and the lss operator @xmath25 is spectrally localized .",
    "[ fig : vx1d ] ( c ) demonstrates the histogram of the eigenvalues ( unnormalized spectral density ) for all eigenvalues of @xmath0 .",
    "[ fig : fmuerror ] ( a)-(c ) demonstrates the behavior of the exact lss operator @xmath25 with @xmath131 and increasing value of @xmath1 . in fig .",
    "[ fig : fmuerror ] , @xmath299(x , y)$ ] should be interpreted using its discretized matrix element @xmath299_{ij}$ ] for @xmath300 .",
    "we find that as @xmath1 increases , the off - diagonal elements of @xmath72 decays rapidly and remains to be well approximated by a banded ( and therefore sparse ) matrix with increasing bandwidth .",
    "[ fig : fmuerror ] ( d)-(f ) demonstrates the quality of the divide - and - conquer approximation @xmath209 to the lss operator .",
    "here we first demonstrate the accuracy of @xmath209 without the truncation using svd decomposition ( i.e. the svd truncation criterion @xmath301 as in eq .  ) .",
    "when @xmath302 , the approximation is nearly exact , while when @xmath1 increases to @xmath303 the relative error is around @xmath304 since the support size of each column of @xmath72 already extends beyond each extended element @xmath184 .    a more complete picture of the @xmath1-dependence for approximating the lss operator is given in fig .",
    "[ fig : fmuerror2 ] . fig .",
    "[ fig : fmuerror2 ] ( a ) shows the max norm error of the divide - and - conquer approximation to the lss operator for @xmath1 traversing the entire spectrum of @xmath0 from @xmath305 to @xmath306 .",
    "the error increases rapidly as @xmath1 initially increases , achieves its maximum at @xmath307 and then starts to decrease .",
    "[ fig : fmuerror2 ] ( b ) shows the same picture but zooms into the interval near @xmath308 .",
    "as @xmath1 increases above @xmath309 , the vectors spanning columns of @xmath25 are approximately linear combination of high frequency fourier modes , and fig .",
    "[ fig : fmuerror2 ] ( a ) shows that the fourier modes are increasingly more difficult to localize as the frequency increases . fig .",
    "[ fig : fmuerror2 ] ( c)-(d ) shows similar behavior for @xmath310 .",
    "the profile of the error with respect to @xmath1 closely resembles a gaussian function . compared to the case with @xmath131 the error significantly reduces for all @xmath1 , indicating the balance between spatial locality and spectral locality with varying @xmath23 .",
    "[ fig : ferrorsigma ] ( a ) demonstrates the max norm error of the lss operator for @xmath296 with increasing value of @xmath23 . when @xmath23 is less than @xmath311 the lss operator is very localized spectrally , but the matrix is almost dense .",
    "therefore the divide - and - conquer approximation leads to large error .",
    "as @xmath23 increases above @xmath311 , the max norm error decreases exponentially with the increase of @xmath23 .",
    "we observe that the choice of @xmath23 is crucial : by varying @xmath23 from @xmath312 to @xmath313 , the error is reduced by over @xmath314 orders of magnitude from @xmath315 to below @xmath316 .",
    "next we study the effect of grid refinement by varying the grid size from @xmath317 to @xmath318 . for 3-point finite difference stencil",
    "the spectral radius of @xmath0 , denoted by @xmath319 is proportional to @xmath320 , and in practice @xmath319 increases from @xmath321 to @xmath322 .",
    "we note that theorem  [ thm : truncate ] indicates that the error should be determined by the ratio @xmath323 , and therefore the size of the extended element as characterized by the geodesic distance @xmath162 should increase proportionally to @xmath319 to preserve accuracy . here",
    "instead we fix the number of elements to be @xmath324 as the grid refines .",
    "therefore @xmath325 , and we should expect that the error increases as the grid refines .",
    "[ fig : ferrorsigma ] ( b ) shows that max norm error of the lss operator for @xmath296,@xmath131 , with increasing @xmath319 .",
    "as the ratio @xmath323 decreases over one order of magnitude , the max norm error does not increase , but rather decreases by more than a factor of @xmath264 .",
    "we note that this numerical result does not contradict the theoretical prediction , since theorem  [ thm : approxgaussian ] only provide an _ upper bound _ of the decay rate , and the _ actual _ decay rate can be faster . note that as the grid refines , the change towards the high end of the spectrum is often larger than the change at the low end of the spectrum .",
    "[ fig : ferrorsigma ] indicates that the accuracy of the lss operator is relatively insensitive to the change in the high end of the spectrum , and it may be possible to construct the lss operator with improved discretization scheme , without sacrificing too much in terms of the spatial locality .",
    "so far the numerical results are obtained for the divide - and - conquer approximation to the lss operator with @xmath301 .",
    "next we apply the svd truncation to obtain the lss basis set @xmath326 for varying svd relative truncation criterion . in our numerical experiments ,",
    "we use @xmath327 as the _ relative _ svd truncation criterion with respect to the largest singular value of @xmath203 .",
    "[ fig : ftolerror ] shows the error of the approximation to the lss operator with @xmath327 being @xmath328 , respectively . as indicated in eq .",
    ", the max norm error of the approximation of the lss operator is approximately proportional to @xmath327 , as @xmath327 becomes dominant in eq .  .",
    "the lss basis set comes from the svd decomposition of @xmath209 on each element .",
    "[ fig : basis1d ] ( a ) shows the @xmath77-st lss basis function on two elements @xmath329 and @xmath330 , respectively , and fig .  [ fig : basis1d ] ( b ) shows the @xmath331-th lss basis function on the same two elements for @xmath332 .",
    "it is clear that each lss basis function is well localized in each extended element @xmath184 and its center is in @xmath180 .",
    "[ fig : ftolerror ] seems to suggest that in order to accurately compute the interior eigenvalues , a very tight svd criterion @xmath327 is needed .",
    "however , we note that many of the lss basis functions associated with the small singular values actually corresponds to the tail of the gaussian function in   which are away from @xmath1 . therefore in order to compute the interior eigenvalues near @xmath1 accurately , it is possible to use a much larger value of @xmath327 .",
    "[ fig : erroreigtau ] ( a ) shows the difference between the @xmath333 eigenvalues of @xmath0 within the interval @xmath334 and the corresponding ritz values of @xmath0 with @xmath335 .",
    "the computed ritz values are highly accurate and the maximum error is under @xmath336 even though a large svd truncation criterion @xmath327 is used . section  [ sec : interior ] discusses the identification of spurious eigenvalues using the residual for each computed ritz value . indeed within the interval @xmath334 we find @xmath337 ritz values , and the @xmath77 additional ritz value should be a spurious eigenvalue .",
    "[ fig : erroreigtau ] ( b ) shows @xmath257 for each ritz value , and we identify that the @xmath338-th ritz value has a much larger residual than the rest and should be removed . after removing this spurious eigenvalue ,",
    "the remaining ritz values become accurate approximation to the eigenvalues as indicated in fig .",
    "[ fig : erroreigtau ] ( a ) .",
    "while the accuracy of the divide - and - conquer approximation to the lss operator improves as the svd truncation criterion @xmath327 decreases , using a very small value of @xmath327 may result in ill - conditioned projection matrices @xmath339 and @xmath340 , i.e. some of the lss basis functions can be approximately represented as the linear combination of other lss basis functions .",
    "[ fig : cond_nb_tau ] ( a ) shows the condition number of @xmath339 , @xmath340 with respect to @xmath327 .",
    "the condition numbers are below @xmath341 when @xmath342 , and increase very rapidly to @xmath343 for @xmath344 . in the latter case ,",
    "numerical results obtained from the generalized eigenvalue solver can not be trusted .",
    "decreasing @xmath327 also leads to increase of the size of the lss basis set .",
    "as @xmath327 decreases from @xmath345 to @xmath346 , the number of lss basis functions increase from @xmath347 to @xmath348 .",
    "the accuracy of the lss basis set for different values of @xmath327 is given in table  [ tab : errortau ] .",
    "when @xmath327 is too small , the number of computed ritz values is less than @xmath333 due to the very large condition number of the generalized eigenvalue problem , and the difference between the eigenvalues and the ritz values is not a meaningful quantity to report and is reported as n / a .",
    "the error of the ritz values reaches its minimum near @xmath349 at only @xmath350 , and then starts to increase as @xmath327 increases .",
    "we observe that even if @xmath351 , the absolute ( and relative ) error of the ritz values is still within @xmath352 .",
    "for this case the dimension of the projected generalized eigenvalue problem is @xmath353 , which is much smaller compared to the dimension of @xmath0 which is @xmath354 .",
    ".the number of computed ritz values in the interval @xmath334 with @xmath129 ( spurious eigenvalues removed ) .",
    "if the number of ritz values match the number of eigenvalues in the interval ( @xmath333 ) , then the third column gives the maximum difference between the eigenvalues and the ritz values .",
    "otherwise the third column gives n / a . [ cols=\"^,^,^\",options=\"header \" , ]     even for the 1d simple example , the lss basis set can be an efficient way to compute interior eigenvalue problems compared to the solution of the eigenvalue problem directly . for comparison of efficiency and accuracy , matlab s sparse eigenvalue solver",
    "is used for the matrix @xmath0 .",
    "we acknowledge that may not be the best eigensolver to use for large interior eigenvalue problems , and other choices such as preconditioned conjugate gradient type of solvers , or jacobi - davidson type of solvers may give better results .",
    "we also remark that the current implementation of the lss solver is only for proof of principle , and many of its components can be further optimized before a more thorough performance study is to be performed . here",
    "we consider systems of increasing size by changing @xmath284 in the potential function in eq .   from @xmath324 to @xmath355 .",
    "correspondingly the number of grid points @xmath7 increases from @xmath354 to @xmath356 , and the number of elements increases proportionally from @xmath324 to @xmath355 .",
    "@xmath357 is used for all systems to compute the eigenvalues within the interval @xmath334 .",
    "[ fig : eiglarge1d ] shows the time for computing the interior eigenvalues near @xmath1 using matlab s sparse eigenvalue solver ( `` global total '' ) , and the time using the lss basis set ( `` lss total '' ) .",
    "the tolerance for is set to @xmath358 .",
    "the breakdown of the time cost for the lss solver includes the time for constructing the lss basis set ( `` lss basis '' ) , the time for assembling the projected matrix ( `` assembly '' ) , and the time for solving the projected eigenvalue problem ( `` lss solve '' ) . fig .",
    "[ fig : nnzau ] shows the sparsity pattern for @xmath339 for @xmath359 , and the sparsity pattern for @xmath340 is by definition the same .",
    "the number of nonzero elements is @xmath360 of the total number of elements in @xmath339 .",
    "the sparsity of the projected matrices is not used in our example here , but can be exploited using alternative methods .",
    "since the size of the local problem is small , the local eigenvalue problem on each @xmath184 is performed using matlab s dense eigenvalue solver , and so is the solution of the generalized eigenvalue problem for the projected matrix .",
    "the time for the global solver scales cubically with respect to @xmath7 , and the constructing the lss basis and the assembly of the projected matrix increases linearly with respect to @xmath7 .",
    "the solution of the generalized eigenvalue problem also scales cubically with respect to @xmath7 , and therefore does not dominate in the lss solver until @xmath361 .",
    "the cross - over time between the lss solver and the global solver is around @xmath362 . for @xmath361 ,",
    "the lss solver costs @xmath363 sec , which is @xmath364 times faster than the global solver which costs @xmath365 sec .",
    "[ fig : eiglarge1d ] ( b ) shows the accuracy of the lss solver .",
    "the ritz values remain as accurate approximation to the eigenvalues as the number of eigenvalues in the interval increases from @xmath333 to @xmath366 .     for @xmath359.,scaledwidth=33.0% ]",
    "the setup of the 2d example is similar to that in 1d .",
    "the global domain is @xmath279\\times [ 0,l]$ ] , and the laplacian operator is discretized using a 5-point finite difference stencil . the grid spacing is chosen to be @xmath367",
    ". the potential function @xmath368 is given by sum of periodized exponential functions with random perturbation in terms of heights , widths and positions of the exponential functions .",
    "this can be viewed as a model potential for a crystal under thermal noise .",
    "one realization of this potential is given in fig .",
    "[ fig : v2d ] .",
    "let the number of elements @xmath177 is a square number and the number of grid points @xmath7 is divisible by @xmath177 .",
    "then all @xmath7 grid points ( vertices ) are uniformly partitioned into @xmath369 elements .",
    "we also assume each extended element @xmath184 contains @xmath180 and its @xmath324 nearest neighbor elements .",
    "[ fig : v2d ] shows the partition of the 2d domain into @xmath370 elements separated by black dashed lines .",
    "elements separated by black dashed lines.,scaledwidth=35.0% ]    we compare the accuracy of the lss basis set by comparing the eigenvalues within the interval @xmath371 with @xmath372 .",
    "the svd relative truncation criterion @xmath327 is set to be @xmath345 .",
    "[ fig : accuracy2d ] ( a ) shows the error of ritz values compared to all the @xmath373 eigenvalues within the interval , and the error of all ritz values is very small , within @xmath374 .",
    "[ fig : accuracy2d ] ( b ) shows the residual of the ritz values .",
    "for all the ritz values the residual are below @xmath375 and no spurious eigenvalue is identified for this case .",
    "finally we demonstrate the performance of the lss solver for a 2d problem with increasing size .",
    "the number of grid points @xmath7 increases from @xmath354 to @xmath376 , and the number of elements increases proportionally from @xmath377 to @xmath355 . fig .",
    "[ fig : eiglarge2d ] shows the time for computing the interior eigenvalues near @xmath1 using matlab s sparse eigenvalue solver ( `` global total '' ) , and the time using the lss basis set ( `` lss total '' ) .",
    "the tolerance for is set to @xmath358 .",
    "the breakdown of the lss solver includes the time for constructing the lss basis set ( `` lss basis '' ) , the time for assembling the projected matrix ( `` assembly '' ) , and the time for solving the projected eigenvalue problem ( `` lss solve '' ) . again the local eigenvalue problem on each @xmath184 is performed using matlab s dense eigenvalue solver , and so is the solution of the generalized eigenvalue problem for the projected matrix .",
    "the crossover point between the global solver and the lss solver is around @xmath378 . for @xmath379 ,",
    "the lss solver costs @xmath380 sec , which is @xmath381 times faster than the global solver which costs @xmath382 sec .",
    "[ fig : eiglarge2d ] ( b ) shows the accuracy of the lss solver .",
    "the ritz values remain as accurate approximation to the eigenvalues as the number of eigenvalues in the interval increases with respect to the system size and no spurious eigenvalue is observed for all cases .      for a general sparse matrix ,",
    "we take the ` turon - m ` matrix from the university of florida matrix collection  @xcite .",
    "the dimension of the matrix is 189924 , with 1690876 number of nonzeros .",
    "the lu factorization procedure for this matrix is relatively expensive . using the approximate minimum degree ( amd ) ordering strategy provided through the `",
    "symamd ` command in matlab  @xcite . the number of nonzeros in @xmath383 and @xmath28 are @xmath384 with a fill - in ratio ( i.e. the ratio between the number of nonzeros in @xmath385 and the number of nonzeros in @xmath0 ) is @xmath386 .",
    "the lu factorization takes @xmath387 sec , and each triangular solve @xmath388 for a random right hand side vector @xmath389 takes @xmath390 sec , compared to each matrix vector multiplication @xmath391 which takes @xmath392 sec .",
    "the spectral radius of this matrix is @xmath393 .",
    "the sparsity pattern of this matrix , together with the histogram of the eigenvalues ( unnormalized spectral density ) in the interval @xmath394 is given in fig .",
    "[ fig : turon_m ] ( a ) ( b ) , respectively .    in order to apply the lss method to this unstructured matrix",
    ", we use the strategy in section  [ subsec : lssgeneral ] and use the metis  @xcite package interfaced by the ` metismex ` program with matlab for generating the graph partitioning map @xmath271 .",
    "we set @xmath395 . as in fig .",
    "[ fig : turon_m ] ( b ) , @xmath396 indeed corresponds to interior eigenvalues .",
    "we select this region mainly because the spectral density is relatively low so that the computation can be treated on a single computational core .",
    "the matrix is partitioned into @xmath377 elements using metis .",
    "the matrix partition routine is efficient and only takes @xmath397 sec . due to the large size of the submatrix on a single extended element",
    ", we use to solve @xmath398 eigenvalues on each extended element with tolerance set to @xmath358 , and set the svd relative truncation criterion @xmath327 to be @xmath399 .",
    "the size of the projected matrix is @xmath400 , which is much reduced compared to the dimension of @xmath0 .",
    "the projected generalized eigenvalue problem is solved with the dense eigenvalue solver ` eig ` .",
    "we compare the accuracy of the lss basis set by comparing the eigenvalues within the interval @xmath401 .",
    "there are @xmath402 eigenvalues in this interval , and ` eigs ` takes @xmath403 sec to converge to tolerance with @xmath358 . for lss , the time for computing the basis functions for all @xmath377 elements is @xmath404 sec .",
    "the time for constructing the projected matrix is @xmath405 sec , and the time for solving the projected matrix is @xmath406 sec . for the projected matrix ,",
    "we find @xmath407 eigenvalues in total , and identified @xmath331 spurious spurious eigenvalues . after removing the spurious eigenvalues with the largest residual ,",
    "the accuracy of the ritz values compared to the true eigenvalues are given in fig .",
    "[ fig : turon_m ] ( c ) . in this case , the lss method is more expensive .",
    "this is mainly due to the cost for constructing the lss basis functions .",
    "however , this part can be potentially performed independently for each element and without inter - element communication on parallel computers .",
    "in this paper , we present a method for constructing a novel basis set called the localized spectrum slicing ( lss ) basis set .",
    "each function in the lss basis set is localized both spectrally and spatially , and therefore can be used as an efficient way for representing eigenvectors of a general sparse hermitian matrix corresponding to a relatively narrow range of eigenvalues .",
    "the lss basis set uses the decay properties of analytic matrix functions , and can be constructed in a divide - and - conquer method .",
    "we show that by carefully tuning one parameter @xmath23 , spatial locality and spectral locality of the basis functions can be balanced .",
    "the projected matrices are both sparse and have reduced sizes . in terms of the future work ,",
    "the gaussian function used in the lss operator is a smooth approximation to the @xmath408 function .",
    "the same concept of locality can be used to approximate other matrix functions , such as matrix sign functions .",
    "this aspect is , e.g. closely related to the recently developed adaptive local basis functions  @xcite and element orbitals  @xcite for constructing efficient basis functions for solving the kohn - sham density functional theory .",
    "the lss basis set can also be used to efficiently characterize the eigenvectors close to the null space of @xmath0 , which could potentially be used to construct preconditioners to accelerate linear solves for indefinite problems .    from efficiency point of view , in the current implementation , the local eigenvalue problem is solved mostly using a dense eigenvalue solver .",
    "this is still feasible for the 1d and 2d model problems presented in the numerical section in this paper , but for 3d problems this is going to be too expensive .",
    "efficient iterative solvers , or local chebyshev expansion based schemes should be used instead .",
    "another practical issue is to control the condition number of the lss basis set when the svd truncation criterion is small . an efficient way to identify a subset of well conditioned lss basis functions",
    "is needed to be more robust .",
    "the balance between spatial and spectral locality is an important topic in fourier analysis and multi - resolution analysis .",
    "because the construction of the lss basis set is completely algebraic and can be applied to any sparse hermitian matrix , it is possible to extend the current work to construct multi - resolution basis functions tailored for given matrices , or multi - resolution basis functions for operators on graphs .",
    "this work was supported by laboratory directed research and development ( ldrd ) funding from berkeley lab , provided by the director , office of science , of the u.s .",
    "department of energy under contract no .",
    "de - ac02 - 05ch11231 , the doe scientific discovery through the advanced computing ( scidac ) program and the doe center for applied mathematics for energy research applications ( camera ) program .                                          , _ implicitly restarted lanczos method ( section 4.5 ) _ , in templates for the solution of algebraic eigenvalue problems : a practical guide , z.  bai , j.  demmel , j.  dongarra , a.  ruhe , and h.  van  der vorst , eds . ,",
    "philadelphia , 2000 , siam , pp ."
  ],
  "abstract_text": [
    "<S> given a sparse hermitian matrix @xmath0 and a real number @xmath1 , we construct a set of sparse vectors , each approximately spanned only by eigenvectors of @xmath0 corresponding to eigenvalues near @xmath1 . </S>",
    "<S> this set of vectors spans the column space of a localized spectrum slicing ( lss ) operator , and is called an lss basis set . </S>",
    "<S> the sparsity of the lss basis set is related to the decay properties of matrix gaussian functions . </S>",
    "<S> we present a divide - and - conquer strategy with controllable error to construct the lss basis set . </S>",
    "<S> this is a purely algebraic process using only submatrices of @xmath0 , and can therefore be applied to general sparse hermitian matrices . </S>",
    "<S> the lss basis set leads to sparse projected matrices with reduced sizes , which allows the projected problems to be solved efficiently with techniques using sparse linear algebra . as an example </S>",
    "<S> , we demonstrate that the lss basis set can be used to solve interior eigenvalue problems for a discretized second order partial differential operator in one - dimensional and two - dimensional domains , as well as for a matrix of general sparsity pattern .    </S>",
    "<S> spectrum slicing ; localization ; decay properties ; basis set ; interior eigenvalue problem    65f60 , 65f50 , 65f15 , 65n22 </S>"
  ]
}