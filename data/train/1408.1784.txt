{
  "article_text": [
    "to provide an analytic explanation for general phenomena using simple theoretical concept is the most interesting part of physics .",
    "statistical physics methods in spin glass theory provide new tools and ideas to study many hard constraint satisfaction problems  @xcite , especially the relation between detailed organization of solutions in the solution space and the algorithmic hardness  @xcite .",
    "a prototypical example is the binary perceptron problem , where @xmath0 input neurons ( units ) are connected to a single output unit by synapses of binary value ( @xmath1 ) synaptic weights .",
    "these weights have to be inferred from a set of examples ( input patterns ) with desired classification labels ( supervised learning ) .",
    "an assignment of these weights is referred to as a solution if the perceptron manages to classify all the input patterns by this assignment .",
    "the ratio between the number of patterns and the number of synapses is called the constraint density .",
    "each example acts as a constraint on the solution space , since increasing the number of examples causes the shrinkage of the space .",
    "the critical constraint density was reported to be about @xmath2  @xcite , below which the solution space is typically nonempty .",
    "the binary perceptron serves as an elementary building block of complex neural networks and is also one of the basic structures for learning and memory  @xcite .",
    "memory in neuronal systems is stored in the synaptic weights , and a binary synaptic weight is robust against noise and also suitable for simple hardware implementation in applications .",
    "the binary perceptron has thus a wide variety of applications ranging from rule inference or structure mining in machine learning  @xcite to error correcting codes or data compression in information theory  @xcite , and even high - dimensional data analysis in biology  @xcite . however , a learning task in the binary perceptron is known to be an np(nondeterministic polynomial time)-complete problem in the worst case  @xcite .",
    "many efforts have been devoted to design low - complexity algorithms to find a solution for a typical case of this difficult problem  @xcite . however",
    ", for many local search heuristics , the search process slows down as the constraint density grows , and the learning threshold decreases as the number of synapses increases  @xcite .",
    "this typical glassy behavior of stochastic local search algorithms remains to be explained and was conjectured to be related to the geometrical organization of the solution space  @xcite .",
    "the statistical properties of this problem were intensively studied by the statistical physics community in the past decades  @xcite . however , an analytic computation of a conclusive picture of the solution space structure is still lacking so far , although this is an important topic both in computer science ( machine learning or computational neuroscience ) and in statistical physics .    a recent study  @xcite carried out an entropy landscape analysis by focusing on the solution - pairs separated by certain hamming distance ( the number of elements in different states in two solutions ) , which motivated us to propose a suitable and solid framework to provide a comprehensive description of the solution space .",
    "the basic idea is to select an equilibrium solution sampled from the boltzmann measure , and then explore the solution space around this selected equilibrium solution by analyzing the entropy landscape in the vicinity of the reference equilibrium solution .",
    "this framework was originally introduced as the name of franz - parisi potential to study the metastable state structure for discontinuous mean - field spin glasses ( e.g. , @xmath3-spin spherical spin glass )  @xcite , where the potential has the physical meaning of the free energy cost to keep a system at a temperature with a fixed overlap from an equilibrium configuration at a different temperature . in this work",
    ", the franz - parisi potential is interpreted in terms of the entropy function to describe the solution space , and we show that a quenched computation ( average over the choice of the reference equilibrium solution ) of the potential in the zero temperature limit is possible and provides important physical insights towards understanding the geometrical organization of the solution ( weight ) space .",
    "our computation demonstrates that the weight space of the binary perceptron problem is indeed made of isolated solutions for any finite constraint density , with the minimal hamming distance separating two solutions growing with the constraint density .",
    "this study reveals the origin of the computational hardness in the binary perceptron problem , explaining the known fact that when the number of synapses becomes sufficiently large , an exponential scaling in computational time is required to maintain a fixed finite constraint density for a learning task  @xcite .    in sec .",
    "[ model ] , we define in detail the binary perceptron problem . in sec .",
    "[ compt ] , we introduce the franz - parisi potential framework and derive the explicit form of the potential under the replica symmetric approximation .",
    "results are presented and discussed in sec .",
    "concluding remarks and future perspectives are given in sec .",
    "the binary perceptron is a single - layered feed - forward neural network , i.e. , @xmath0 input neurons are connected to a single output neuron by @xmath0 synapses of weight @xmath4 @xmath5 .",
    "the perceptron tries to learn @xmath6 associations @xmath7 @xmath8 , where @xmath9 is an input pattern with @xmath10 , and @xmath11 is the desired classification of the input pattern @xmath12 . for a random classification task , both @xmath13 and the desired output @xmath14 are generated randomly independently with @xmath15 and @xmath16 being @xmath17 with probability @xmath18 .",
    "given the input pattern @xmath19 , the actual output @xmath20 of the perceptron is @xmath21 . if @xmath22 , we say that the synaptic weight vector @xmath23 has learned the @xmath12-th pattern .",
    "each input pattern imposes a constraint on all synaptic weights , therefore @xmath24 denotes the constraint density .",
    "the solution space of the binary perceptron is composed of all the weight configurations @xmath25 that satisfy @xmath26 for @xmath27 .",
    "the energy cost is thus defined as the number of patterns mapped incorrectly  @xcite , i.e. , @xmath28 where @xmath29 is a step function with the convention that @xmath30 if @xmath31 and @xmath32 otherwise .",
    "the prefactor @xmath33 is introduced to ensure that the argument of the step function remains at the order of unity , for the sake of the following statistical mechanical analysis in the thermodynamic limit . without loss of generality , we assume @xmath34 for any input pattern in the remaining part of this paper , since one can perform a gauge transformation @xmath35 to each input pattern without affecting the result .    from a theoretical perspective , the perceptron is typically able to learn an extensive number of random input patterns with the storage capacity @xmath36  @xcite .",
    "however , to find such a solution configuration @xmath23 in practice , is quite a nontrivial task . here ,",
    "to reveal the origin of this computational hardness , we apply the replica method from the theory of disordered systems  @xcite to derive an analytic expression of the franz - parisi potential , which characterizes the entropy landscape of the problem .",
    "the binary perceptron problem is a densely - connected graphical model  @xcite in that a proper assignment of all synaptic weights is needed to satisfy each constraint ( learn each pattern ) .",
    "its equilibrium property can thus be described by mean - field computation in terms of the franz - parisi potential .",
    "the basic idea is to first select an equilibrium configuration @xmath23 at a temperature @xmath37 , then constrain its overlap with another equilibrium configuration @xmath38 at a different temperature @xmath39 , which yields a constrained free energy  @xcite : @xmath40 after taking the quenched disorder average ( over the pattern distribution @xmath41 , denoted by the angular bracket ) and the average over the distribution of @xmath23 , which is @xmath42 .",
    "@xmath43 is the partition function for the original measure and @xmath44 is the inverse temperature .",
    "the constrained free energy @xmath45 is a self - averaging quantity with respect to both the quenched disorder and the probability distribution of the reference configuration @xmath23  @xcite .",
    "its value doesnot depend on the particular realization and coincides with the typical value , which can be calculated via the replica method .    in our current setting , we are interested in the ground states of the problem , thus we set @xmath46 , arriving at the following formula : @xmath47 e^{x\\sum_{\\gamma , i}j_{i}^{1}w_{i}^{\\gamma}}\\right>,\\ ] ] where @xmath48 and @xmath49 . in eq .",
    "( [ replica ] ) , we have @xmath50 replicas @xmath51 and @xmath52 replicas @xmath53 , with the coupling field ( @xmath54 ) term being an interaction of all the constrained replicas @xmath55 with one privileged replica @xmath56 . the replica method to compute the typical value of the constrained free energy",
    "is based on two mathematical identities : @xmath57 and @xmath58 .",
    "to evaluate the average in eq .",
    "( [ replica ] ) , we need to define the overlap matrixes @xmath59 , @xmath60 and @xmath61 , which characterize the following disorder averages @xmath62 , @xmath63 and @xmath64 . under the replica symmetric ( rs ) ansatz , we have @xmath65 , @xmath66 and @xmath67 , where @xmath68 if @xmath69 and @xmath70 otherwise .    after some algebraic manipulations , we finally get the constrained free energy density @xmath71 as :    @xmath72 ,      \\end{split}\\ ] ]    where @xmath73 , @xmath74 , and @xmath75 with the gaussian measure @xmath76 in which @xmath77 .",
    "@xmath78 where @xmath79 . @xmath80 and @xmath81 . the associated self - consistent ( saddle - point )",
    "equations for the order parameters @xmath82 are derived in the appendix  [ app : cfe ] .",
    "the franz - parisi potential @xmath83 is obtained through a legendre transform of @xmath71 , i.e. , @xmath84 and @xmath85 .",
    "@xmath83 has the meaning of the entropy characterizing the growth rate of the number of solutions ( @xmath86 ) lying apart at a normalized distance @xmath87 ( hamming distance divided by @xmath0 ) from the fixed equilibrium solution .",
    "detailed information about the solution space structure can be extracted from the behavior of this potential at different values of @xmath3 , especially those values close to one .",
    "since the potential curve may lose its concavity , one has to solve numerically the saddle - point equations ( see appendix  [ app : deriv ] ) by fixing @xmath3 and searching for compatible coupling field @xmath54 ( by using the secant method  @xcite ) .    , for which an observed maximum implies the change of the concavity of the entropy curve ( this also holds for other finite values of @xmath24 ) .",
    "( b ) minimal distance versus the constraint density . within the minimal distance , there are no solutions satisfying the distance constraint from the reference equilibrium solution .",
    "( c ) schematic illustration of the weight space based on results of ( a ) and ( b ) .",
    "the points indicate the equilibrium solutions of weights .",
    "@xmath36 is the storage capacity after which the solution space is typically empty .",
    "@xmath88 is the actual hamming distance without normalization.,title=\"fig : \" ] .1 cm , for which an observed maximum implies the change of the concavity of the entropy curve ( this also holds for other finite values of @xmath24 ) .",
    "( b ) minimal distance versus the constraint density . within the minimal distance",
    ", there are no solutions satisfying the distance constraint from the reference equilibrium solution .",
    "( c ) schematic illustration of the weight space based on results of ( a ) and ( b ) .",
    "the points indicate the equilibrium solutions of weights .",
    "@xmath36 is the storage capacity after which the solution space is typically empty .",
    "@xmath88 is the actual hamming distance without normalization.,title=\"fig : \" ] .1 cm , for which an observed maximum implies the change of the concavity of the entropy curve ( this also holds for other finite values of @xmath24 ) .",
    "( b ) minimal distance versus the constraint density . within the minimal distance",
    ", there are no solutions satisfying the distance constraint from the reference equilibrium solution .",
    "( c ) schematic illustration of the weight space based on results of ( a ) and ( b ) .",
    "the points indicate the equilibrium solutions of weights .",
    "@xmath36 is the storage capacity after which the solution space is typically empty .",
    "@xmath88 is the actual hamming distance without normalization.,title=\"fig : \" ] .1 cm",
    "the franz - parisi potential versus the predefined normalized hamming distance ( @xmath89 ) is shown in fig .",
    "[ v - fp ] ( a ) . at the maximum corresponding to @xmath90 ( @xmath91 ) ,",
    "@xmath83 gives back the entropy of the original system .",
    "as the distance gets close to zero , one finds that there exists a value of distance at which the entropy curve loses its concavity and turns to a convex part ( see the inset of fig .",
    "[ v - fp ] ( a ) and note that the sign of the slope changes at the maximum point ) .",
    "this behavior leads to an important result that there exists a minimal distance of @xmath92 below which no solutions are separated from the reference equilibrium solution .",
    "note that the reference solution is distributed according to the boltzmann measure ( a uniform measure over all solutions ) .",
    "the minimal distance grows with the constraint density , as shown in fig .",
    "[ v - fp ] ( b ) .",
    "this can be understood by the following argument . due to the hard nature of the pattern constraint in the binary perceptron problem",
    " all synapses are involved in classifying each input pattern , flipping one synaptic weight should force the rearrangement of many weight values to memorize the learned patterns .",
    "similar phenomena were also observed in gallager s type error correcting code  @xcite and locked constraint satisfaction problem  @xcite .    for small @xmath24 , it is not easy to show the convex part numerically .",
    "however , one can prove that when @xmath93 , the franz - parisi potential vanishes as expected for all @xmath24 ( see appendix  [ app : proof ] ) .",
    "in addition , at @xmath93 ( @xmath94 ) , we have @xmath95 ( see appendix  [ app : deriv ] ) where @xmath96 is a finite constant and @xmath97 is a positive constant .",
    "the first term dominates the divergent behavior in the limit @xmath98 .",
    "this means that , for any finite @xmath99 , the entropy curve in fig .",
    "[ v - fp ] ( a ) has a negative infinite slope ( @xmath100 ) at @xmath101 , supporting the existence of the convex part and the minimal distance . as expected from the tendency shown in fig .",
    "[ v - fp ] ( b ) , the value of the minimal distance becomes very small for the less constrained case ( small constraint density ) .",
    "this explains why a simple local search algorithm can find a solution when either @xmath0 or @xmath24 is small  @xcite .",
    "as @xmath24 increases , the minimal distance grows rapidly , as a consequence , any algorithms working by local move ( each time a few weights are flipped ) should find increasing difficulty to identify a solution ( especially at a very large @xmath0 ) , which holds even for reinforced message passing algorithms  @xcite .",
    "in other words , an extensive energy or entropic barrier should be overcome .",
    "the energy landscape is always valleys dominated ( valleys are metastable states with positive energy cost ) .",
    "these metastable states are much more numerous than the frozen ground states  @xcite",
    ". local algorithms will get trapped by these metastable states with high probability .",
    "we thus conclude that , at variance with random @xmath102-sat or @xmath103-coloring problems  @xcite , the solution space of the binary perceptron problem is simple in the sense that it is made of isolated solutions instead of well separated clusters of exponentially many close - by solutions .",
    "this picture is consistent with evidences reported in previous studies  @xcite .",
    "moreover , non - convergence of the iteration of the saddle - point equations was not observed , which may be related to the simple structure of the solution space .",
    "in fact , below the storage capacity , the replica symmetric solution is stable without any need to introduce replica symmetry breaking scenario for this problem  @xcite .",
    "our quenched computation of the franz - parisi potential reveals that , synaptic weights to realize the random classification task are organized into point - like clusters ( zero internal entropy ) far apart from each other ( see fig .",
    "[ v - fp ] ( c ) ) , with the result that in the thermodynamic limit , an exponential computation time is required to reach a finite fixed @xmath24  @xcite .",
    "we give an analytic expression of the franz - parisi potential for the binary perceptron problem .",
    "this potential describes the entropy landscape of solutions in the vicinity of a reference equilibrium solution , and its shape is independent of the choice of the reference point . solving the saddle - point equations ,",
    "we find that the concavity of the curve changes at some distance , leading to a minimal distance below which there doesnot exist solutions satisfying the distance constraint . furthermore ,",
    "this minimal distance increases with the constraint density , implying that the problem is extremely hard because the solution space is composed of isolated solutions ( point - like clusters ) with the property that to go from one solution to another solution , one should flip an extensive number ( proportional to @xmath0 ) of synaptic weights .",
    "our analysis establishes a refined picture of the organization structure of the solution space for the binary perceptron problem , which is helpful for understanding the glassy behavior of local search heuristics  @xcite , which may have some connections with recent studies of constrained glasses  @xcite , and furthermore , is expected to shed light on design of efficient algorithms for large - scale neuromorphic devices .",
    "the analytic analysis presented in this paper also offers a basis for possible rigorous mathematical ( probabilistic ) analysis of the entropy landscape  @xcite , and has potentially applications for studying the solution space structure of other hard problems in information processing , e.g. , spike time - based neural classifiers  @xcite .",
    "we thank lenka zdeborov for helpful discussions and haijun zhou for helpful comments on the manuscript .",
    "this work was partially supported by the jsps fellowship for foreign researchers ( grant no .",
    "@xmath104 ) ( h.h . ) and jsps / mext kakenhi grant no .",
    "@xmath105 ( y.k . ) . support from the jsps core - to - core program non - equilibrium dynamics of soft matter and informationis also acknowledged .",
    "in the current context , for a reference equilibrium configuration @xmath23 at temperature @xmath37 , one is interested in the free energy of a perturbed system ( with the constraint that the configuration @xmath38 at temperature @xmath39 should satisfy a prefixed overlap with @xmath23 ) , leading to the constrained free energy  @xcite : @xmath106 where @xmath107 and @xmath54 is the coupling field to control the overlap ( or distance ) between two configurations , i.e. , @xmath108 .",
    "we are interested in the ground state , then we set both inverse temperatures equal and make them tend to infinity . substituting the definition of energy cost of the problem , and using @xmath109 in the zero temperature limit , we have @xmath110    to evaluate the typical value of @xmath111 , we resort to the replica method  @xcite , by using two mathematical identities : @xmath57 and @xmath58 . introducing @xmath50 unconstrained replicas @xmath51 and @xmath52 constrained replicas @xmath53 , we rewrite @xmath111 as : @xmath47 e^{x\\sum_{\\gamma , i}j_{i}^{1}w_{i}^{\\gamma}}\\right>_{\\boldsymbol{\\xi}},\\ ] ] where @xmath48 and @xmath49 .",
    "to proceed , we define the following overlap matrixes : @xmath59 , @xmath60 and @xmath61 , which characterize the following disorder averages @xmath62 , @xmath63 and @xmath64 . by inserting delta functions for these definitions and using their integral representations",
    ", we obtain the disorder average @xmath112 in eq .",
    "( [ replica ] ) as : @xmath113\\right>_{\\boldsymbol{\\xi}}e^{x\\sum_{i,\\gamma}j_{i}^{1}w_{i}^{\\gamma}}.      \\end{split}\\ ] ]    now we re - scale the variable @xmath114 ( this also applies for other conjugated variables ) .",
    "we apply the replica symmetric approximation  @xcite , which assumes the permutation symmetry of the overlap matrix . to be more precise , @xmath65 , @xmath66 and @xmath67 , where @xmath68 if @xmath69 and @xmath70 otherwise .",
    "we first simplify @xmath115 as : @xmath116     + ( \\hat{p}-\\widehat{p'})\\sum_{\\gamma}j^{1}w^{\\gamma } ,      \\end{split}\\ ] ] where the site index @xmath117 is dropped off since each @xmath117 shares the same formula .",
    "then we compute the disorder average as : @xmath118\\right>_{\\boldsymbol{\\xi}}=\\left[\\int d\\omega\\int dt\\int_{\\tilde{t}}^{\\infty }      dyh^{m}(h(\\omega , t , y))h^{n-1}(\\tilde{t})\\right]^{\\alpha n},\\ ] ] where @xmath74 , and @xmath119 with the gaussian measure @xmath120 in which @xmath121 .",
    "@xmath78 where @xmath79 . in deriving eq .",
    "( [ replica05 ] ) , we have parameterized @xmath122 and @xmath123 , by using independent standard gaussian random variables @xmath124 of zero mean and unit variance .",
    "the parameterization retains the covariance structure of @xmath125 .",
    "the pattern index ( @xmath12 ) is also dropped off for the same reason .",
    "after a few algebraic manipulations , we obtain @xmath126\\\\      & \\times\\exp\\left[n\\ln\\int dz_{1}\\int dz_{2}\\int dz_{3}\\mathcal      { a}(\\hat{q},\\hat{r},\\hat{p},\\widehat{p'},m , n)\\right]\\\\      & \\times\\exp\\left[\\alpha n\\ln\\int d\\omega\\int dt\\int_{\\tilde{t}}^{\\infty }      dyh^{m}(h(\\omega , t , y))h^{n-1}(\\tilde{t})\\right ] ,      \\end{split}\\ ] ] after approximating the integral in eq .",
    "( [ replica02 ] ) by its dominant part ( a saddle point analysis in the large @xmath0 limit ) . to derive eq .",
    "( [ replica03 ] ) , the hubbard - stratonovich transformation was used . in eq .",
    "( [ replica03 ] ) , @xmath127 $ ] , in which @xmath80 and @xmath128 .    the saddle point analysis ( also called laplace method ) implies that @xmath129 should take its maximal value so that @xmath130 should be extremized with respect to the order parameters @xmath82 . keeping up to the first order in @xmath50 , the extremization with respect to @xmath131 and @xmath132 gives the self - consistent equations for @xmath131 and @xmath132 ( see eqs .",
    "( [ sdea ] ) and  ( [ sdeb ] ) ) .",
    "as expected , their values do not rely on other order parameters characterizing the property of the constrained replicas .",
    "these two equations describe the @xmath23 system at equilibrium , and it should not be affected by the @xmath38 system which follows a perturbed distribution depending on the reference solution @xmath23 . finally , one can readily get the constrained free energy density following the definition given in eq .",
    "( [ replica ] ) : @xmath72 ,      \\end{split}\\ ] ] together with the associated saddle - point equations :    [ sde ] @xmath133,\\label{sdec}\\\\      \\hat{p}&=x+\\frac{\\alpha}{\\sqrt{(1-q)(1-r)}}\\int d\\omega\\int dt \\mathcal      { r}(\\tilde{t})\\mathcal { r}(h(\\omega , t , y=\\tilde{t})),\\\\      r&=\\int      d\\mathbf{z}(2\\cosh\\hat{a})^{-1}\\left[e^{\\hat{a}}\\tanh^{2}(\\hat{a}'+\\hat{p}-\\widehat{p'})+e^{-\\hat{a}}\\tanh^{2}(\\hat{a}'-\\hat{p}+\\widehat{p'})\\right],\\label{sdee}\\\\      \\hat{r}&=\\frac{\\alpha}{1-r}\\int d\\omega\\int dth^{-1}(\\tilde{t})\\int_{\\tilde{t}}^{\\infty}dy\\mathcal      { r}^{2}(h(\\omega , t , y)),\\label{sdef}\\\\      p'&=\\int      d\\mathbf{z}(2\\cosh\\hat{a})^{-1}\\left[e^{\\hat{a}}\\tanh\\hat{a}\\tanh(\\hat{a}'+\\hat{p}-\\widehat{p'})+e^{-\\hat{a}}\\tanh\\hat{a}\\tanh(\\hat{a}'-\\hat{p}+\\widehat{p'})\\right],\\\\      \\widehat{p'}&=\\frac{\\alpha}{\\sqrt{(1-q)(1-r)}}\\int d\\omega\\int dth^{-1}(\\tilde{t})\\mathcal      { r}(\\tilde{t})\\int_{\\tilde{t}}^{\\infty}dy\\mathcal      { r}(h(\\omega , t , y)),\\end{aligned}\\ ] ]    where @xmath73 , and @xmath134 . in deriving these equations , we have used a useful property of the gaussian measure @xmath135 where @xmath136 is the derivative of the function @xmath137 with respect to @xmath138 .    to solve these saddle - point equations , for example , eq .",
    "( [ sdef ] ) , one efficient way is to generate a random number @xmath139 according to the conditional distribution @xmath140 each time when using monte - carlo method to perform the integral .",
    "in some cases , one may reexpress @xmath141 and @xmath142 to retain their covariances @xmath143 ( their means are both zero , and variances @xmath144,@xmath145 ) according to their definition , this is because , @xmath146 or @xmath147 may get negative .",
    "the franz - parisi potential @xmath83 is obtained through a legendre transform of @xmath71 , i.e. , @xmath84 .",
    "the overlap @xmath108 is related to the coupling field by @xmath85 .",
    "since the potential curve may lose its concavity , one has to solve numerically the saddle - point equations by fixing @xmath3 and searching for compatible coupling field @xmath54 ( by using the secant method ) .",
    "if a solution of @xmath54 is found for a given @xmath3 , then we have @xmath149 at this value of @xmath3 . because @xmath89 , @xmath54 is also equal to @xmath150 .",
    "the derivative of the franz - parisi potential with respect to the overlap @xmath3 is given by : @xmath151 note that when @xmath152 , @xmath153 will get close to @xmath3 but smaller than @xmath3 , and @xmath154 , which is observed in numerical simulations and can be understood from the definition of these order parameters .",
    "therefore , in the limit @xmath155 , the second term in the right - hand side of eq .",
    "( [ slope ] ) is @xmath156 with @xmath157 .",
    "the expression of @xmath158 as a function of @xmath159 can be deduced from eq .",
    "( [ sdec ] ) . using the fact that @xmath93 implies that @xmath160 , and the identity @xmath161 ( @xmath162 ) , one finally gets @xmath163 . in the above derivations",
    ", we have used the fact that @xmath164 in the limit @xmath152 based on eqs .",
    "( [ sdec ] ) and  ( [ sdee ] ) . taken together",
    ", one arrives at the slope of @xmath83 at @xmath101 : @xmath165",
    "at @xmath101 , the franz - parisi potential can be expressed as : @xmath167 .",
    "\\end{split}\\ ] ] note that @xmath168 when @xmath169 .",
    "hence the @xmath24-dependent term disappears .",
    "the last term becomes @xmath170\\\\      & = \\int dz_1\\int dz_3      ( 2\\cosh\\hat{a})^{-1}\\left[e^{\\hat{a}}(\\sqrt{\\widehat{p'}}z_3+\\hat{p}-\\widehat{p'})+e^{-\\hat{a}}(-\\sqrt{\\widehat{p'}}z_3+\\hat{p}-\\widehat{p'})\\right]\\\\ & = \\hat{p}-\\widehat{p'}+\\widehat{p'}\\left[1-\\int dz_1\\int dz_3\\tanh^{2}\\bigl(\\sqrt{\\hat{q}-\\widehat{p'}}z_1+\\sqrt{\\widehat{p'}}z_3\\bigr)\\right]\\\\      & = \\hat{p}-q\\widehat{p'}.      \\end{split}\\ ] ] collecting the above results , one arrives at @xmath171 ."
  ],
  "abstract_text": [
    "<S> supervised learning in a binary perceptron is able to classify an extensive number of random patterns by a proper assignment of binary synaptic weights . however , to find such assignments in practice , is quite a nontrivial task . </S>",
    "<S> the relation between the weight space structure and the algorithmic hardness has not yet been fully understood . to this end </S>",
    "<S> , we analytically derive the franz - parisi potential for the binary preceptron problem , by starting from an equilibrium solution of weights and exploring the weight space structure around it . </S>",
    "<S> our result reveals the geometrical organization of the weight spacethe weight space is composed of isolated solutions , rather than clusters of exponentially many close - by solutions . </S>",
    "<S> the point - like clusters far apart from each other in the weight space explain the previously observed glassy behavior of stochastic local search heuristics . </S>"
  ]
}