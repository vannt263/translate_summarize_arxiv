{
  "article_text": [
    "the histogram is a fundamental tool in physics , providing a powerful yet accessible means of non - parameteric inference",
    ". often , however , analyses are concerned not with the underlying distribution itself but rather the consistency between multiple histograms . comparing different analysis techniques , for example ,",
    "requires the comparison of each respective analysis output given the same input data .",
    "perhaps the most ubiquitous example is the validation of monte carlo simulations against data .",
    "although numerous approaches to the problem exist , none are without their limitations .",
    "when comparing the consistency of two one - dimensional samples , @xmath0 and @xmath1 , standard frequentist methodologies turn to significance testing , in particular the kolmogorov - smirnov ( ks ) test @xcite . here the null hypothesis that the two samples are consistent with draws from the same distribution is accepted or rejected based on the sampling distribution of the test statistic    @xmath2    where @xmath3 is the empirical distribution of the sample @xmath4 ,    @xmath5    with @xmath6 the indicator function    @xmath7    in practical applications , the sampling distribution for @xmath8 is usually approximated with a large @xmath9 limit such that the ks test becomes extremely conservative ( in more technical terms , the type ii error of the test is small only when the samples are large ) .",
    "moreover , the sampling distribution of the weighted samples from importance sampling can not be approximated so easily and the proper application of the ks test to simulated events is infeasible .",
    "given histograms instead of the individual samples , the empirical distributions must be approximated by the bin contents themselves and the test statistic @xmath8 reduces to the maximum difference between bins .",
    "information in all other bins is ignored , no matter now relevant it might be .",
    "considering also the usual faults of frequentist significance tests @xcite , the application of the ks test to the problem of histogram comparison leaves much to be desired .",
    "various procedures have also been developed in various communities .",
    "one approach subtracts one histogram from the other before performing a significance test on the hypothesis of a constant residual across all bins .",
    "the sampling distribution of the residuals , however , does not admit familiar tests such as @xmath10 outside of the limit of large bin contents .",
    "still , the limit is often assumed and the resulting @xmath10 taken as a measure of consistency between the two histograms .    a similar approach utilizing the quotient of the two histograms fares even worse .",
    "the usual linearized gaussian approximations to the resulting bin quotients are not very accurate , and the subsequent @xmath10 test is even more misleading than for the bin residuals .",
    "more accurate considerations @xcite become considerably more involved , and significance testing much more challenging .",
    "both approaches are further limited by the dependence on frequentist significance testing .",
    "from the bayesian perspective , the problem of histogram comparison becomes one of model comparison between @xmath11 , the model where the two histograms are drawn from the same distribution , and @xmath12 , the model where they are drawn from distinct distributions .",
    "the probability of @xmath11 follows from an application of bayes theorem on the model evidences , @xmath13 and @xmath14 ,    @xmath15    where @xmath16 and @xmath17 are the bin populations of the two histograms . in practice ,",
    "a uniform prior is taken and the probability reduces to    @xmath18    subsequently , @xmath11 best models the data when @xmath19 while @xmath20 is superior when @xmath21 .",
    "when @xmath22 , however , the interpretation is more subtle . the intermediate case can arise not only when each bin is independently ignorant , but also when individual bins are best described by different models and the discrepancies negate the respective contributions to the model evidences .    in order to discriminate between ignorance and disagreement , first note that the underlying multinomial distributions modeling the histogram bin contents are well described by independent poisson distributions .",
    "the resulting evidences conveniently factor ,    @xmath23    which enables an explicit mixture model ,    @xmath24 , \\\\ % \\end{aligned}\\ ] ]    with the mixture posterior    @xmath25    in the case of a single bin and a uniform prior , the mixture posterior is linear in @xmath26 and the possible posteriors fall into three classes . those with a negative slope are best modeled by @xmath20 ( fig [ fig : single]a ) , those uniform across @xmath26 are indifferent between the two models ( fig [ fig : single]b ) , and those with a positive slope favor @xmath11 ( fig [ fig : single]c ) .",
    "in the general case , each bin is modeled independently .",
    "if all bins favor @xmath20 then the combined product quickly concentrates towards small @xmath26 with a mode exactly at @xmath27 ( fig [ fig : mult]a ) .",
    "conversely , full support of @xmath11 produces a posterior concentrated towards @xmath28 with a mode exactly at the boundary ( fig [ fig : mult]c ) .",
    "any bins indifferent between the models contribute little to the final shape , unless all the bins are ignorant in which case the posterior remains uniform ( fig [ fig : mult]b ) . when bins are best modeled by different models , however , the posterior becomes peaked away from the boundaries with the exact location of the mode indicating the overall superior model ( fig [ fig : multmixed ] ) .",
    ", width=288 ]    diagnosing the best model is then straightforward .",
    "when a mode arises at the boundaries , the bins are consistently described by a single model : @xmath20 for @xmath27 and @xmath11 for @xmath28 .",
    "a mode away from the boundaries indicates mixed agreement , implying that the two histograms are not entirely consistent .",
    "note that the non - mixture posterior can be recovered by comparing the boundaries of the mixture posterior ,    @xmath29    the additional information contained in the shape of mixture posterior underlies the benefit of the extension to a mixture model .",
    "not only does the mixture model break the the degeneracy between ignorance and balancing disagreement , but it can also reveal more subtle disagreements that might otherwise be obscured in the model posterior .      actually computing the mixture posterior requires the bin by bin evidences , @xmath30 and @xmath31 .",
    "when both bin populations are drawn from the same distribution , the joint likelihood is given by    @xmath32    assigning a uniform prior ,    @xmath33    yields the posterior    @xmath34 } , % \\end{aligned}\\ ] ]    where @xmath35 is the normalized lower incomplete gamma function ,    @xmath36    the evidence follows ,    @xmath37    where @xmath38 is the beta function ,    @xmath39    in practice the prior support , @xmath40 , is chosen to encompass the bulk of the likelihood . making the support as small as possible while keeping the posterior normalization practically constant implements a crude form of model selection that improves the model without limiting its power .",
    "given two distinct sources , the joint likelihood factors into    @xmath41    the two sources require two independent priors ,    @xmath42    and the joint posterior becomes    @xmath43 } \\frac { \\mu^{n_{i } } e^{- \\mu } } { \\gamma \\left(n_{i } + 1 \\right ) \\left [ \\bar { \\gamma } \\left ( n_{i } + 1 , \\zeta \\right ) - \\bar { \\gamma } \\left ( n_{i } + 1 , \\epsilon \\right ) \\right ] } .",
    "% \\end{aligned}\\ ] ]    the evidence is then    @xmath44 } {   \\gamma \\left ( m_{i } + 1 \\right ) \\left ( \\delta - \\gamma \\right )   } \\frac { \\gamma \\left(n_{i } + 1 \\right ) \\left [ \\bar { \\gamma } \\left ( n_{i } + 1 , \\zeta \\right ) - \\bar { \\gamma } \\left ( n_{i } + 1 , \\epsilon \\right ) \\right ] } {   \\gamma \\left ( n_{i } + 1\\right ) \\left ( \\zeta - \\epsilon \\right )   } \\\\ %   & = \\frac { \\left [ \\bar { \\gamma } \\left ( m_{i } + 1 , \\delta \\right ) - \\bar { \\gamma } \\left ( m_{i } + 1 , \\gamma \\right ) \\right ] } { \\left ( \\delta - \\gamma \\right )   } \\frac { \\left [ \\bar { \\gamma } \\left ( n_{i } + 1 , \\zeta \\right ) - \\bar { \\gamma } \\left ( n_{i } + 1 , \\epsilon \\right ) \\right ] } { \\left ( \\zeta - \\epsilon \\right )   } . % \\end{aligned}\\ ] ]        cross sections defining the physics from which the data are generated are equivalent to the probability distribution    @xmath45    where @xmath46 defines the phase space of the given physics .    given an integrated luminosity @xmath47 ,",
    "the expected number of events falling into a particular histogram bin is then    @xmath48    where    @xmath49    now consider a monte carlo simulation generating @xmath50 events from the differential cross section @xmath51 .",
    "the samples admit the monte carlo estimate of the expected counts ,    @xmath52    which is just the number of events falling into the bin @xmath53 .    the estimate is unbiased ,    @xmath54 = \\lambda,\\ ] ]    with the variance    @xmath55 & = \\frac{1}{n } \\mathrm{var}_{p } \\left [ n \\left ( x \\right ) \\right ] \\\\ & = \\frac{1}{n } \\left ( \\mathrm{e}_{p } \\left [ n^{2 } \\left(x\\right ) \\right ] - \\mathrm{e}_{p } \\left [ n \\left(x\\right ) \\right]^{2 } \\right ) \\\\ & = \\frac{1}{n } \\left ( \\int_{-\\infty}^{\\infty } i^{2}_{ab } \\left(x\\right ) \\left ( \\bar{\\mathcal{l } } \\sigma\\right)^{2 } \\ , p \\left(x\\right ) - \\lambda^{2 } \\right ) \\\\ & = \\frac{1}{n } \\left ( \\bar{\\mathcal{l } } \\sigma \\int_{-\\infty}^{\\infty } i_{ab } \\left(x\\right ) \\bar{\\mathcal{l } } \\sigma \\ , p \\left(x\\right ) - \\lambda^{2 } \\right ) \\\\ & = \\frac{1}{n } \\left ( n \\lambda - \\lambda^{2 } \\right ) \\\\ & = \\lambda - \\lambda^{2 } / n .\\end{aligned}\\ ] ]    in the large @xmath9 limit , the second term becomes negligible and the sampling distribution of the monte carlo estimate converges to a poisson distribution ,    @xmath56    note that this limit does not require large bin contents .",
    "in fact , the large @xmath9 limit is exactly the same rare - events limit in which the multinomial distribution reduces to independent poisson distributions .",
    "the monte carlo samples , then , are entirely analogous to the data and inferring the underlying rate @xmath57 is the same in both cases .",
    "importance sampling is slightly more subtle . here",
    "events are not drawn from the physics distribution @xmath58 but instead an auxiliary distribution @xmath59 .",
    "the expected number of events becomes    @xmath60    with the resulting monte carlo estimate    @xmath61    note that this estimate is just the sum of the weights of events falling into the chosen bin .",
    "as above , the importance sampling estimate is an unbiased estimator ,    @xmath62 = \\lambda,\\ ] ]    but with the variance    @xmath63 & = \\frac{1}{n } \\mathrm{var}_{q } \\left [ n \\left(x\\right ) w \\left(x\\right ) \\right ] \\\\ & = \\frac{1}{n } \\left ( \\mathrm{e}_{q } \\left [ n^{2 } \\left(x\\right ) w^{2 } \\left(x\\right ) \\right ] - \\mathrm{e}_{q } \\left [ n \\left(x\\right ) w \\left(x\\right ) \\right]^{2 } \\right ) \\\\ & = \\frac{1}{n } \\left ( \\int_{-\\infty}^{\\infty } i^{2}_{ab } \\left(x\\right ) \\left ( \\bar{\\mathcal{l } } \\sigma\\right)^{2 } w^{2 } \\left ( x\\right ) \\ , q \\left(x\\right ) - \\lambda^{2 } \\right ) \\\\ & = \\frac{1}{n } \\left ( \\bar{\\mathcal{l } } \\sigma \\int_{-\\infty}^{\\infty } i_{ab } \\left(x\\right ) \\bar{\\mathcal{l } } \\sigma \\ ,   w \\left ( x\\right ) p \\left(x\\right ) - \\lambda^{2 } \\right ) \\\\ & = \\frac{1}{n } \\left ( n \\int_{-\\infty}^{\\infty } i_{ab } \\left(x\\right ) \\bar{\\mathcal{l } } \\sigma \\ ,   w \\left ( x\\right ) p \\left(x\\right ) - \\lambda^{2 } \\right ) \\\\ & = \\int_{-\\infty}^{\\infty } i_{ab } \\left(x\\right ) \\bar{\\mathcal{l } } \\sigma \\ ,   w \\left ( x\\right ) p \\left(x\\right ) - \\lambda^{2 } / n .\\end{aligned}\\ ] ]    if the weighting function @xmath64 is relatively constant across the bin @xmath53 then @xmath64 can be approximated by its ensemble average ,    @xmath65    and the remaining integral becomes    @xmath66    the variance is then    @xmath63 & \\approx \\bar{w } \\lambda - \\lambda^{2 } / n , \\end{aligned}\\ ] ]    and in the large @xmath9 limit reduces to    @xmath67 \\approx \\bar{w } \\lambda .\\ ] ]    note that if the weights are unity , in which case the importance sampling reduces to standard monte carlo sampling , then the variance reduces to the previous result as consistency would demand .",
    "the resulting sampling distribution is approximately gaussian ,    @xmath68.\\ ] ]    the above sampling distribution admits a bayesian approach to simulated histograms and consequently the comparison of simulated histograms .",
    "computation of the evidence ratio follows as in the case of data comparison .",
    "the importance sampling distribution for the simulated histogram bins requires not only the sum of the weights falling into that bin ,    @xmath69    but also the ensemble average of the weights ,    @xmath70    as before , the sampling distribution of the data histograms bins @xmath71 is taken to be poisson .",
    "here the joint likelihood becomes    @xmath72 .\\end{aligned}\\ ] ]    a uniform prior ,    @xmath73    gives the posterior    @xmath74 } { \\int_{\\alpha}^{\\beta } \\mathrm{d } \\lambda \\ , \\lambda^{m_{i } - \\frac{1}{2 } } \\exp \\left [ \\lambda - \\frac{1}{2 } \\frac { \\left ( n_{i } - \\lambda \\right)^{2 } } { \\bar{w}_{i } \\lambda } \\right ] } , \\end{aligned}\\ ] ]    resulting in the evidence    @xmath75.\\ ] ]    note that limiting the prior support not only improves the model but also eases the computational demands of the required numerical integration .",
    "given two distinct sources , the joint likelihood factors into    @xmath76 .",
    "% \\end{aligned}\\ ] ]    the two sources require two independent priors ,    @xmath77    and the joint posterior becomes    @xmath78 } { \\int_{\\epsilon}^{\\zeta }   \\mathrm{d } \\mu \\ , \\mu^{-\\frac{1}{2 } } \\exp \\left [ - \\frac{1}{2 } \\frac { \\left ( n_{i } - \\mu \\right)^{2 } } { \\bar{w}_{i } \\mu } \\right ] } \\\\ %   & = \\frac { \\lambda^{m_{i } } e^{- \\lambda } } {   \\gamma \\left(m_{i } + 1 \\right ) \\left [ \\bar{\\gamma } \\left ( m_{i } + 1 , \\delta \\right ) - \\bar{\\gamma } \\left ( m_{i } + 1 , \\gamma \\right ) \\right ] } \\frac { \\mu^{-\\frac{1}{2 } } \\exp \\left",
    "[ - \\frac{1}{2 } \\frac { \\left ( n_{i } - \\mu \\right)^{2 } } { \\bar{w}_{i } \\mu } \\right ] } { \\int_{\\epsilon}^{\\zeta } \\mathrm{d } \\mu \\ , \\mu^{-\\frac{1}{2 } } \\exp \\left",
    "[ - \\frac{1}{2 } \\frac { \\left ( n_{i } - \\mu \\right)^{2 } } { \\bar{w}_{i } \\mu } \\right ] } , % \\end{aligned}\\ ] ]    yielding the evidence    @xmath79\\cdot \\int_{\\epsilon}^{\\zeta } \\mathrm{d } \\mu \\ , \\mu^{-\\frac{1}{2 } } \\exp \\left",
    "[ - \\frac{1}{2 } \\frac { \\left ( n_{i } - \\mu \\right)^{2 } } { \\bar{w}_{i } \\mu } \\right ] \\end{aligned}\\ ] ]      comparing two simulated histogram bins requires the sum of the weights and ensemble average for both the first bin ,    @xmath80    @xmath81    and the second bin ,    @xmath82    @xmath83      in this case the joint likelihood becomes    @xmath84 \\frac{1}{\\sqrt { 2 \\pi \\bar{v}_{i } \\lambda } } \\exp \\left",
    "[ - \\frac{1}{2 } \\frac { \\left ( n_{i } - \\lambda \\right)^{2 } } { \\bar{v}_{i } \\lambda } \\right ] \\\\ & = \\frac{1 } { 2 \\pi \\lambda \\sqrt { \\bar{w}_{i } \\bar{v}_{i } } } \\exp \\left [ - \\frac{1}{2 \\lambda } \\left ( \\frac { \\left ( m_{i } - \\lambda \\right)^{2 } } { \\bar{w}_{i } } - \\frac { \\left ( n_{i } - \\lambda \\right)^{2 } } { \\bar{v}}_{i } \\right ) \\right]\\end{aligned}\\ ] ]    a uniform prior ,    @xmath73    gives the posterior    @xmath85 } { \\int_{\\alpha}^{\\beta } \\mathrm{d } \\lambda \\ ,   \\lambda^{-1 } \\exp \\left [ - \\frac{1}{2 \\lambda } \\left ( \\frac { \\left ( m_{i } - \\lambda \\right)^{2 } } { \\bar{w}_{i } } - \\frac { \\left ( n_{i } - \\lambda \\right)^{2 } } { \\bar{v}_{i } } \\right ) \\right ] } , \\end{aligned}\\ ] ]    with the evidence    @xmath86\\ ] ]      with two distinct sources the joint likelihood factors into    @xmath87 \\cdot \\frac{1}{\\sqrt { 2 \\pi \\bar{v}_{i } \\mu } } \\exp \\left [ - \\frac{1}{2 } \\frac { \\left ( n_{i } - \\mu \\right)^{2 } } { \\bar{v}_{i } \\mu } \\right ] \\\\ %   & = \\frac{1}{2 \\pi \\sqrt { \\bar{w}_{i } \\bar{v}_{i } } } \\ , \\lambda^{-\\frac{1}{2 } } \\exp \\left",
    "[ - \\frac{1}{2 } \\frac { \\left ( m_{i } - \\lambda \\right)^{2 } } { \\bar{w}_{i } \\lambda } \\right ] \\cdot \\mu^{-\\frac{1}{2 } } \\exp \\left [ - \\frac{1}{2 } \\frac { \\left ( n_{i } - \\mu \\right)^{2 } } { \\bar{v}_{i } \\mu } \\right ] .",
    "% \\end{aligned}\\ ] ]    taking two independent priors ,    @xmath88    gives the posterior    @xmath89 } { \\int_{\\gamma}^{\\delta } \\mathrm{d } \\lambda \\ , \\lambda^{-\\frac{1}{2 } } \\exp \\left [ - \\frac{1}{2 } \\frac { \\left ( m_{i } - \\lambda \\right)^{2 } } { \\bar{w}_{i } \\lambda } \\right ] } \\frac { \\mu^{-\\frac{1}{2 } } \\exp \\left",
    "[ - \\frac{1}{2 }",
    "\\frac { \\left ( n_{i } - \\mu \\right)^{2 } } { \\bar{v}_{i } \\mu } \\right ] } { \\int_{\\epsilon}^{\\zeta } \\mathrm{d } \\mu \\ , \\mu^{-\\frac{1}{2 } } \\exp \\left [ - \\frac{1}{2 } \\frac { \\left ( n_{i } - \\mu \\right)^{2 } } { \\bar{v}_{i } \\mu } \\right ] } , % \\end{aligned}\\ ] ]    along with the evidence    @xmath90 \\cdot \\int_{\\epsilon}^{\\zeta } \\mathrm{d } \\mu \\ , \\mu^{-\\frac{1}{2 } } \\exp \\left [ - \\frac{1}{2 } \\frac { \\left ( n_{i } - \\mu \\right)^{2 } } { \\bar{v}_{i } \\mu } \\right]\\end{aligned}\\ ] ]",
    ", width=384 ]    , width=384 ]    , width=384 ]",
    "the capability of the bayesian approach is evident when considering an ensemble of histogram comparisons models .    at each iteration in the ensemble a true model is randomly selected between @xmath11 and @xmath20 . in the former case ,",
    "the two histograms to be compared are drawn from a randomly generated multinomial distribution with 50 bins ,    @xmath91    @xmath92    the latter case samples one histogram from the randomly generated multinomial distribution , but draws the second from a perturbed distribution with    @xmath93    @xmath94    @xmath95    the two distributions are kept similar in order to assess the performance of each algorithm when comparison is highly nontrivial .",
    "the two histograms generated for each model are then evaluated with three tests :    [ cols= \" < , < \" , ]     false acceptances and false rejections were tabulated for four ensembles , each with 2000 independent models but different total bin contents @xmath9 .",
    "the respective rate posteriors were calculated assuming a binomial likelihood and beta prior , @xmath96 ; the posterior modes and 68.3% confidence intervals for each algorithm are plotted in figure [ fig : modelensembletest ] .",
    "note that in the two cases with the smallest statistics , @xmath97 and @xmath98 the variance of the bin contents equals or surpasses the expected difference between the two underlying distributions and the large false accept rates are to be expected .    in all cases",
    "the bayesian mixture test , @xmath99 , is superior to ks .",
    "the non - mixture test outperforms ks in almost all cases as well , failing only in the small statistics ensembles where small discrepancies tend to be washed out by the bin content variances .",
    "a bayesian mixture model has been developed to test whether two histograms are consistent , in that they are more likely to have been drawn from a single distribution rather than two distinct distributions .",
    "the model is extended to handle histograms generated from importance sampling , resulting in a robust and powerful approach to the comparison of histograms populated by both data and simulation .",
    "said power is evident with studies of a large model ensemble .",
    "a c++ implementation of the bayesian mixture model utilizing the root @xcite data analysis framework is available at http://web.mit.edu/~betan/www/code.html .",
    "i thank chris jones , steve voinea , and matt walker for helpful discussion and comments ."
  ],
  "abstract_text": [
    "<S> determining if two histograms are consistent , whether they have been drawn from the same underlying distribution or not , is a common problem in physics . </S>",
    "<S> existing approaches are not only limited in power but also inapplicable to histograms filled with importance weights , a common feature of monte carlo simulations . from a bayesian perspective , the comparison between a single underlying distribution and two underlying distributions is readily solved within the context of model comparison . </S>",
    "<S> i introduce an implementation of bayesian model comparison to the problem , including the extension to importance sampling .    a bayesian approach to histogram comparison    michael betancourt    _ massachusetts institute of technology , cambridge , ma 02139 _ </S>"
  ]
}