{
  "article_text": [
    "gaussian process models are widely used in spatial statistics and machine learning . in most applications ,",
    "the covariance structure of the process is at least partially unknown and must be estimated from the available data .",
    "likelihood - based methods , including bayesian methods , are natural choices for carrying out the inferences on the unknown covariance structure . for large data sets",
    ", however , calculating the likelihood function exactly may be difficult or impossible in many cases .",
    "assuming we are willing to specify the covariance structure up to some parameter @xmath5 , the generic problem we are faced with is computing the loglikelihood for @xmath6 for some random vector @xmath7 and @xmath8 an @xmath9 positive definite matrix indexed by the unknown @xmath10 . in many applications , there would be a mean vector that also depends on unknown parameters , but since unknown mean parameters generally cause fewer computational difficulties , for simplicity we will assume the mean is known to be 0 throughout this work .",
    "for the application to ozone data in section [ sec6 ] , we avoid modeling the mean by removing the monthly mean for each pixel . the simulations in section [ sec5 ] all first preprocess the data by taking a discrete laplacian , which filters out any mean function that is linear in the coordinates , so that the results in those sections would be unchanged for such mean functions .",
    "the loglikelihood is then , up to an additive constant , given by @xmath11 if @xmath8 has no exploitable structure , the standard direct way of calculating @xmath12 is to compute the cholesky decompositon of @xmath13 , which then allows @xmath14 and @xmath15 to be computed quickly .",
    "however , the cholesky decomposition generally requires @xmath16 storage and @xmath17 computations , either of which can be prohibitive for sufficiently large @xmath1 .",
    "therefore , it is worthwhile to develop methods that do not require the calculation of the cholesky decomposition or other matrix decompositions of @xmath8 .",
    "if our goal is just to find the maximum likelihood estimate ( mle ) and the corresponding fisher information matrix , we may be able to avoid the computation of the log determinants by considering the score equations , which are obtained by setting the gradient of the loglikelihood equal to 0 . specifically , defining @xmath18 , the score equations for @xmath10 are given by ( suppressing the dependence of @xmath8 on @xmath10 ) @xmath19 for @xmath20 .",
    "if these equations have a unique solution for @xmath21 , this solution will generally be the mle .",
    "iterative methods often provide an efficient ( in terms of both storage and computation ) way of computing solves in @xmath8 ( expressions of the form @xmath22 for vectors  @xmath23 ) and are based on being able to multiply arbitrary vectors by @xmath8 rapidly . in particular , assuming the elements of @xmath8 can be calculated as needed , iterative methods require only @xmath0 storage , unlike matrix decompositions such as the cholesky , which generally require @xmath16 storage . in terms of computations ,",
    "two factors drive the speed of iterative methods : the speed of matrix  vector multiplications and the number of iterations . exact matrix ",
    "vector multiplication generally requires @xmath16 operations , but if the data form a partial grid , then it can be done in @xmath24 operations using circulant embedding and the fast fourier transform . for irregular observations ,",
    "fast multipole approximations can be used [ @xcite ] .",
    "the number of iterations required is related to the condition number of @xmath8 ( the ratio of the largest to smallest singular value ) , so that preconditioning [ @xcite ] is often essential ; see @xcite for some circumstances under which one can prove that preconditioning works well .",
    "computing the first term in ( [ score ] ) requires only one solve in @xmath8 , but the trace term requires @xmath1 solves ( one for each column of @xmath25 ) for @xmath20 , which may be prohibitive in some circumstances .",
    "recently , @xcite analyzed and demonstrated a stochastic approximation of the trace term based on the hutchinson trace estimator [ @xcite ] . to define it ,",
    "let @xmath26 be i.i.d .",
    "random vectors in @xmath27 with i.i.d .",
    "symmetric bernoulli components , that is , taking on values 1 and @xmath28 each with probability @xmath29 . define a set of estimating equations for @xmath10 by @xmath30 for @xmath20 . throughout this work",
    ", @xmath31 means to take expectations over @xmath6 and over the @xmath32 s as well . since @xmath33 , @xmath34 and ( [ ascore ] ) provides a set of unbiased estimating equations for @xmath10 .",
    "therefore , we may hope that a solution to ( [ ascore ] ) will provide a good approximation to the mle .",
    "the unbiasedness of the estimating equations ( [ ascore ] ) requires only that the components of the @xmath32 s have mean 0 and variance 1 ; but , subject to this constraint , @xcite shows that , assuming the components of the @xmath32 s are independent , taking them to be symmetric bernoulli minimizes the variance of @xmath35 for any @xmath9 matrix @xmath36 .",
    "the hutchinson trace estimator has also been used to approximate the gcv ( generalized cross - validation ) statistic in nonparametric regression [ @xcite ] . in particular",
    ", @xcite shows that @xmath37 does not need to be large to obtain a randomized gcv that yields results nearly identical to those obtained using exact gcv .",
    "suppose for now that it is possible to take @xmath37 much smaller than @xmath1 and obtain an estimate of @xmath10 that is nearly as efficient statistically as the exact mle .",
    "from here on , assume that any solves in @xmath8 will be done using iterative methods .",
    "in this case , the computational effort to computing ( [ score ] ) or ( [ ascore ] ) is roughly linear in the number of solves required ( although see section [ sec4 ] for methods that make @xmath37 solves for a common matrix @xmath8 somewhat less than @xmath37 times the effort of one solve ) , so that ( [ ascore ] ) is much easier to compute than ( [ score ] ) when @xmath38 is small . an attractive feature of the approximation ( [ ascore ] ) is that if at any point one wants to obtain a better approximation to the score function , it suffices to consider additional @xmath32 s in ( [ ascore ] ) .",
    "however , how exactly to do this if using the dependent sampling scheme for the @xmath32 s in section [ sec4 ] is not so obvious .",
    "since this stochastic approach provides only an approximation to the mle , one must compare it with other possible approximations to the mle .",
    "many such approaches exist , including spectral methods , low - rank approximations , covariance tapering and those based on some form of composite likelihood .",
    "all these methods involve computing the likelihood itself and not just its gradient , and thus all share this advantage over solving ( [ ascore ] ) .",
    "note that one can use randomized algorithms to approximate @xmath39 and thus approximate the loglikelihood directly [ @xcite ] .",
    "however , this approximation requires first taking a power series expansion of @xmath8 and then applying the randomization trick to each term in the truncated power series ; the examples presented by @xcite show that the approach does not generally provide a good approximation to the loglikelihood .",
    "since the accuracy of the power series approximation to @xmath39 depends on the condition number of @xmath8 , some of the filtering ideas described by @xcite and used to good effect in section [ sec4 ] here could perhaps be of value for approximating @xmath39 , but we do not explore that possibility .",
    "see @xcite for some recent developments on stochastic approximation of log determinants of positive definite matrices .",
    "let us consider the four approaches of spectral methods , low - rank approximations , covariance tapering and composite likelihood in turn .",
    "spectral approximations to the likelihood can be fast and accurate for gridded data [ @xcite ] , although even for gridded data they may require some prefiltering to work well [ @xcite ] .",
    "in addition , the approximations tend to work less well as the number of dimensions increase [ @xcite ] and thus may be problematic for space  time data , especially if the number of spatial dimensions is three .",
    "spectral approximations have been proposed for ungridded data [ @xcite ] , but they do not work as well as they do for gridded data from either a statistical or computational perspective , especially if large subsets of observations do not form a regular grid .",
    "furthermore , in contrast to the approach we propose here , there appears to be no easy way of improving the approximations by doing further calculations , nor is it clear how to assess the loss of efficiency by using spectral approximations without a large extra computational burden .",
    "low - rank approximations , in which the covariance matrix is approximated by a low - rank matrix plus a diagonal matrix , can greatly reduce the burden of memory and computation relative to the exact likelihood [ @xcite ] .",
    "however , for the kinds of applications we have in mind , in which the diagonal component of the covariance matrix does not dominate the small - scale variation of the process , these low - rank approximations tend to work poorly and are not a viable option [ @xcite ] .",
    "covariance tapering replaces the covariance matrix of interest by a sparse covariance matrix with similar local behavior [ @xcite ] .",
    "there is theoretical support for this approach [ @xcite ] , but the tapered covariance matrix must be very sparse to help a great deal with calculating the log determinant of the covariance matrix , in which case @xcite finds that composite likelihood approaches will often be preferable .",
    "there is scope for combining covariance tapering with the approach presented here in that sparse matrices lead to efficient matrix  vector multiplication , which is also essential for our implementation of computing ( [ ascore ] ) based on iterative methods to do",
    "the matrix solves .",
    "@xcite show that covariance tapering and low - rank approximations can also sometimes be profitably combined to approximate likelihoods .",
    "we consider methods based on composite likelihoods to be the main competitor to solving ( [ ascore ] ) .",
    "the approximate loglikelihoods described by @xcite can all be written in the following form : for some sequence of pairs of matrices @xmath40 , @xmath41 , all with @xmath1 columns , at most @xmath1 rows and full rank , @xmath42 where @xmath43 is the conditional gaussian density of @xmath44 given @xmath45 .",
    "as proposed by @xcite and @xcite , the rank of @xmath46 will generally be larger than that of @xmath47 , in which case the main computation in obtaining ( [ composite ] ) is finding cholesky decompositions of the covariance matrices of @xmath48 .",
    "for example , @xcite just lets @xmath44 be the @xmath49th component of @xmath50 and @xmath45 some subset of @xmath51 . if @xmath52 is the largest of these subsets , then the storage requirements for this computation are @xmath53 rather than @xmath16 .",
    "comparable to increasing the number of @xmath32 s in the randomized algorithm used here , this approach can be updated to obtain a better approximation of the likelihood by increasing the size of the subset of @xmath51 to condition on when computing the conditional density of @xmath54 .",
    "however , for this approach to be efficient from the perspective of flops , one needs to store the cholesky decompositions of the covariance matrices of @xmath55 , which would greatly increase the memory requirements of the algorithm . for dealing with truly massive data sets ,",
    "our long - term plan is to combine the randomized approach studied here with a composite likelihood by using the randomized algorithms to compute the gradient of ( [ composite ] ) , thus making it possible to consider @xmath47 s and @xmath46 s of larger rank than would be feasible if one had to do exact calculations .",
    "section [ sec2 ] provides a bound on the efficiency of the estimating equations based on the approximate likelihood relative to the fisher information matrix .",
    "the bound is in terms of the condition number of the true covariance matrix of the observations and shows that if the covariance matrix is well conditioned , @xmath37 does not need to be very large to obtain nearly optimal estimating equations .",
    "section [ sec3 ] shows how one can get improved estimating equations by choosing the @xmath32 s in ( [ ascore ] ) based on a design related to @xmath2 factorial designs .",
    "section [ sec4 ] describes details of the algorithms , including methods for solving the approximate score equations and the role of preconditioning .",
    "section [ sec5 ] provides results of numerical experiments on simulated data .",
    "these results show that the basic method can work well for moderate values of @xmath37 , even sometimes when the condition numbers of the covariance matrices do not stay bounded as the number of observations increases .",
    "furthermore , the algorithm with the @xmath32 s chosen as in section [ sec3 ] can lead to substantially more accurate approximations for a given @xmath37 .",
    "a large - scale numerical experiment shows that for observations on a partially occluded grid , the algorithm scales nearly linearly in the sample size .",
    "section [ sec6 ] applies the methods to omi ( ozone monitoring instrument ) level 3 ( gridded ) total column ozone measurements for april 2012 in the latitude band @xmath3@xmath4n .",
    "the data are given on a @xmath56 grid , so if the data were complete , there would be a total of @xmath57 observations .",
    "however , as figure [ figozone ] shows , there are missing observations , mostly due to a lack of overlap in data from different orbits taken by omi , but also due to nearly a full day of missing data on april 2930 , so that there are 84,942 observations . by acting as if all observations are taken at noon local time and assuming the process is stationary in longitude and time , the covariance matrix for the observations can be embedded in a block circulant matrix , greatly reducing the computational effort needed for multiplying the covariance matrix by a vector . using ( [ ascore ] ) and a factorized sparse inverse preconditioner [ @xcite ] , we are able to compute an accurate approximation to the mle for a simple model that captures some of the main features in the omi data , including the obvious movement of ozone from day to day visible in figure [ figozone ] that coincides with the prevailing westerly winds in this latitude band .",
    "this section gives a bound relating the covariance matrices of the approximate and exact score functions .",
    "let us first introduce some general notation for unbiased estimating equations .",
    "suppose @xmath10 has @xmath58 components and @xmath59 is a set of unbiased estimating equations for @xmath10 so that @xmath60 for all @xmath10 .",
    "write @xmath61 for the @xmath62 matrix whose @xmath63th element is @xmath64 and @xmath65 for the covariance matrix of @xmath66 .",
    "the godambe information matrix [ @xcite ] , @xmath67^{-1 } e_\\theta\\bigl\\{\\dot{g}(\\theta)\\bigr\\}\\ ] ] is a natural measure of the informativeness of the estimating equations [ @xcite , definition 2.1 ] . for positive semidefinite matrices @xmath68 and @xmath69 , write @xmath70 if @xmath71 is positive semidefinite .",
    "for unbiased estimating equations @xmath72 and @xmath73 , then we can say @xmath74 dominates @xmath75 if @xmath76 . under sufficient regularity conditions on the model and the estimating equations ,",
    "the score equations are the optimal estimating equations [ @xcite ] .",
    "specifically , for the score equations , the godambe information matrix equals the fisher information matrix , @xmath77 , so this optimality condition means @xmath78 for all unbiased estimating equations @xmath72 .",
    "writing @xmath79 for the @xmath63th element of the matrix @xmath36 , for the score equations in ( [ score ] ) , @xmath80 [ @xcite , page  179 ] .",
    "for the approximate score equations ( [ ascore ] ) , it is not difficult to show that @xmath81 . furthermore ,",
    "writing @xmath82 for @xmath83 and defining the matrix @xmath84 by @xmath85 , we have @xmath86 so that @xmath87 , which , as @xmath88 , tends to @xmath77 .    in fact , as also demonstrated empirically by @xcite , one may often not need @xmath37 to be that large to get estimating equations that are nearly as efficient as the exact score equations . writing @xmath89 for the @xmath49th component of @xmath90 , we have @xmath91\\\\[-8pt ] & = & \\sum_{k\\ne\\ell}\\bigl(w_{k\\ell}^i w_{k\\ell}^j + w_{k\\ell}^i w_{\\ell k}^j\\bigr ) \\nonumber \\\\ & = & \\operatorname{tr}\\bigl(w^i w^j\\bigr ) + \\operatorname{tr } \\bigl\\{w^i \\bigl(w^j\\bigr ) ' \\bigr\\ } - 2\\sum _ { k=1}^n w_{kk}^iw_{kk}^j.\\nonumber\\end{aligned}\\ ] ] as noted by @xcite , the terms with @xmath92 drop out in the second step because @xmath93 with probability 1 .",
    "when @xmath13 is diagonal for all @xmath10 , then @xmath94 gives the exact score equations , although in this case computing @xmath95 directly would be trivial .",
    "writing @xmath96 for the condition number of a matrix , we can bound@xmath97 in terms of @xmath77 and @xmath98 .",
    "the proof of the following result is given in the .",
    "[ tmain ] @xmath99    it follows from ( [ bbound ] ) that @xmath100 in practice , if @xmath101 , so that the loss of information in using ( [ ascore ] ) rather than ( [ score ] ) was at most 1% , we would generally be satisfied with using the approximate score equations and a loss of information of even 10% or larger might be acceptable when one has a massive amount of data . for example , if @xmath102 , a  bound of 0.01 is obtained with @xmath103 and a bound of 0.1 with @xmath104 .",
    "it is possible to obtain unbiased estimating equations similar to ( [ ascore ] ) whose statistical efficiency does not depend on @xmath98 .",
    "specifically , if we write @xmath95 as @xmath105 , where @xmath106 is any matrix satisfying @xmath107 , we then have that @xmath108 for @xmath20 are also unbiased estimating equations for @xmath10 . in this case ,",
    "@xmath109 , whose proof is similar to that of theorem [ tmain ] but exploits the symmetry of @xmath110 . this bound is less than or equal to the bound in ( [ bbound ] ) on @xmath97 .",
    "whether it is preferable to use ( [ symscore ] ) rather than ( [ ascore ] ) depends on a number of factors , including the sharpness of the bound in ( [ bbound ] ) and how much more work it takes to compute @xmath111 than to compute @xmath112 .",
    "an example of how the action of such a matrix square root can be approximated efficiently using only @xmath0 storage is presented by @xcite .",
    "[ secdependent ] choosing the @xmath32 s independently is simple and convenient , but one can reduce the variation in the stochastic approximation by using a more sophisticated design for the @xmath32 s ; this section describes such a design .",
    "suppose that @xmath113 for some nonnegative integer @xmath52 and that @xmath114 are fixed vectors of length @xmath37 with all entries @xmath115 for which @xmath116 .",
    "for example , if @xmath117 for a positive integer @xmath118 , then the @xmath119 s can be chosen to be the design matrix for a saturated model of a @xmath120 factorial design in which the levels of the factors are set at @xmath115 [ @xcite , chapter 5 ] .",
    "in addition , assume that @xmath121 are random diagonal matrices of size @xmath37 and @xmath122 , @xmath123 are random variables such that all the diagonal elements of the @xmath124 s and all the @xmath122 s are i.i.d .",
    "symmetric bernoulli random variables .",
    "then define @xmath125 one can easily show that for any @xmath126 matrix @xmath36 , @xmath127 .",
    "thus , we can use this definition of the @xmath32 s in ( [ ascore ] ) , and the resulting estimating equations are still unbiased .",
    "this design is closely related to a class of designs introduced by @xcite , who propose selecting the @xmath32 s as follows .",
    "suppose @xmath128 is a hadamard matrix , that is , an @xmath9 orthogonal matrix with elements @xmath115 .",
    "@xcite actually consider @xmath128 a multiple of a unitary matrix , but the special case @xmath128 hadamard makes their proposal most similar to ours .",
    "then , using simple random sampling ( with replacement ) , they choose @xmath37 columns from this matrix and multiply this @xmath129 matrix by an @xmath130 diagonal matrix with diagonal entries made up of independent symmetric bernoulli random variables .",
    "the columns of this resulting matrix are the @xmath32 s .",
    "we are also multiplying a subset of the columns of a hadamard matrix by a random diagonal matrix , but we do not select the columns by simple random sampling from some arbitrary hadamard matrix .    the extra structure we impose yields beneficial results in terms of the variance of the randomized trace approximation , as the following calculations show .",
    "partitioning @xmath36 into an @xmath131 array of @xmath132 matrices with @xmath133th block @xmath134 , we obtain the following : @xmath135 using @xmath136 and @xmath137 , we have @xmath138 which is not random .",
    "thus , if @xmath36 is block diagonal ( i.e. , @xmath134 is a matrix of zeroes for all @xmath139 ) , ( [ ujmuj ] ) yields @xmath140 without error .",
    "this result is an extension of the result that independent @xmath32 s give @xmath140 exactly for diagonal @xmath36 .",
    "furthermore , it turns out that , at least in terms of the variance of @xmath141 , for the elements of @xmath36 off the block diagonal , we do exactly the same as we do when the @xmath32 s are independent .",
    "write @xmath142 for @xmath143 with @xmath144 defined as in ( [ ascore ] ) with independent @xmath32 s .",
    "define @xmath145 for the unbiased estimating equations defined by ( [ ascore ] ) with dependent @xmath32 s defined by ( [ uj ] ) and @xmath146 to be the covariance matrix of @xmath147 . take @xmath148 to be the set of pairs of positive integers @xmath149 with @xmath150 for which @xmath151 .",
    "we have the following result , whose proof is given in the .",
    "[ tdependent ] for any vector @xmath152 , @xmath153    thus , @xmath154 . since @xmath155",
    ", it follows that @xmath156 .",
    "how much of an improvement will result from using dependent @xmath32 s depends on the size of the @xmath157 s within each block . for spatial data",
    ", one would typically group spatially contiguous observations within blocks . how to block for space ",
    "time data is less clear .",
    "the results here focus on the variance of the randomized trace approximation .",
    "@xcite obtain bounds on the probability that the approximation error is less than some quantity and note that these results sometimes give rankings for various randomized trace approximations different from those obtained by comparing variances .",
    "finding @xmath10 that solves the estimating equations ( [ ascore ] ) requires a nonlinear equation solver in addition to computing linear solves in @xmath8 .",
    "the nonlinear solver starts at an initial guess @xmath158 and iteratively updates it to approach a ( hopefully unique ) zero of ( [ ascore ] ) . in each iteration , at @xmath159 , the nonlinear solver typically requires an evaluation of @xmath160 in order to find the next iterate  @xmath161 . in turn",
    ", the evaluation of @xmath74 requires employing a linear solver to compute the set of vectors @xmath162 and @xmath112 , @xmath163 .",
    "the fisher information matrix @xmath164 and the matrix @xmath165 contain terms involving matrix traces and diagonals .",
    "write @xmath166 for a column vector containing the diagonal elements of a matrix and @xmath167 for the hadamard ( elementwise ) product of matrices . for any real matrix @xmath68 , @xmath168 where the expectation @xmath169 is taken over @xmath170 , a random vector with i.i.d .",
    "symmetric bernoulli components .",
    "one can unbiasedly estimate @xmath164 and @xmath165 by @xmath171 and @xmath172\\\\[-8pt ] & & { } -2\\sum_{\\ell=1}^n \\biggl [ \\frac{1}{n_2}\\sum_{k=1}^{n_2 } \\bigl(u_k\\circ w^iu_k\\bigr ) \\biggr]_{\\ell } \\biggl[\\frac{1}{n_2}\\sum_{k=1}^{n_2 } \\bigl(u_k\\circ w^ju_k\\bigr ) \\biggr]_{\\ell}. \\nonumber\\end{aligned}\\ ] ] note that here the set of vectors @xmath173 need not be the same as that in ( [ ascore ] ) and that @xmath174 may not be the same as @xmath37 , the number of @xmath32 s used to compute the estimate of  @xmath10 . evaluating @xmath175 and @xmath176 requires linear solves since @xmath177 and @xmath178 .",
    "note that one can also unbiasedly estimate @xmath179 as the sample covariance of @xmath180 and @xmath181 for @xmath182 , but ( [ jij2 ] ) directly exploits properties of symmetric bernoulli variables ( e.g. , @xmath183 ) .",
    "further study would be needed to see when each approach is preferred .",
    "we consider an iterative solver for solving a set of linear equations @xmath184 for a symmetric positive definite matrix @xmath185 , given a right - hand vector @xmath186 .",
    "since the matrix @xmath68 ( in our case the covariance matrix ) is symmetric positive definite , the conjugate gradient algorithm is naturally used .",
    "let @xmath187 be the current approximate solution , and let @xmath188 be the residual .",
    "the algorithm finds a search direction @xmath189 and a step size @xmath190 to update the approximate solution , that is , @xmath191 , such that the search directions @xmath192 are mutually @xmath68-conjugate [ i.e. , @xmath193 for @xmath194 and the new residual @xmath195 is orthogonal to all the previous ones , @xmath196 .",
    "one can show that the search direction is a linear combination of the current residual and the past search direction , yielding the following recurrence formulas : @xmath197 where @xmath198 and @xmath199 , and @xmath200 denotes the vector inner product .",
    "letting @xmath201 be the exact solution , that is , @xmath202 , then @xmath187 enjoys a linear convergence to @xmath201 : @xmath203 where @xmath204 is the @xmath68-norm of a vector .",
    "asymptotically , the time cost of one iteration is upper bounded by that of multiplying @xmath68 by @xmath189 , which typically dominates other vector operations when @xmath68 is not sparse .",
    "properties of the covariance matrix can be exploited to efficiently compute the matrix  vector products .",
    "for example , when the observations are on a lattice ( regular grid ) , one can use the fast fourier transform ( fft ) , which takes time @xmath205 [ @xcite ] .",
    "even when the grid is partial ( with occluded observations ) , this idea can still be applied . on the other hand , for nongridded observations",
    ", exact multiplication generally requires @xmath16 operations .",
    "however , one can use a combination of direct summations for close - by points and multipole expansions of the covariance kernel for faraway points to compute the matrix ",
    "vector products in @xmath24 , even @xmath0 , time [ @xcite ] . in the case of matrn - type gaussian processes and in the context of solving the stochastic approximation ( [ ascore ] ) , such fast multipole approximations were presented by @xcite .",
    "note that the total computational cost of the solver is the cost of each iteration times the number of iterations , the latter being usually much less than @xmath1 .",
    "the number of iterations to achieve a desired accuracy depends on how fast @xmath187 approaches @xmath201 , which , from ( [ eqncgconverge ] ) , is in turn affected by the condition number @xmath206 of  @xmath68 .",
    "two techniques can be used to improve convergence .",
    "one is to perform preconditioning in order to reduce @xmath206 ; this technique will be discussed in the next section .",
    "the other is to adopt a block version of the conjugate gradient algorithm .",
    "this technique is useful for solving the linear system for the same matrix with multiple right - hand sides .",
    "specifically , denote by @xmath207 the linear system one wants to solve , where @xmath69 is a matrix with @xmath208 columns , and the same for the unknown  @xmath209 .",
    "conventionally , matrices such as @xmath69 are called _ block vectors _ , honoring the fact that the columns of @xmath69 are handled simultaneously .",
    "the block conjugate gradient algorithm is similar to the single - vector version except that the iterates @xmath187 , @xmath210 and @xmath189 now become block iterates @xmath211 , @xmath212 and @xmath213 and the coefficients @xmath190 and @xmath214 become @xmath215 matrices .",
    "the detailed algorithm is not shown here ; interested readers are referred to @xcite .",
    "if @xmath216 is the exact solution , then @xmath211 approaches @xmath216 at least as fast as linearly : @xmath217 where @xmath218 and @xmath219 are the @xmath49th column of @xmath211 and @xmath216 , respectively ; @xmath220 is some constant dependent on @xmath49 but not @xmath221 ; and @xmath222 is the ratio between @xmath223 and @xmath224 with the eigenvalues @xmath225 sorted increasingly .",
    "comparing ( [ eqncgconverge ] ) with ( [ eqnbcgconverge ] ) , we see that the modified condition number @xmath226 is less than @xmath206 , which means that the block version of the conjugate gradient algorithm has a faster convergence than the standard version does . in practice , since there are many right - hand sides ( i.e. , the vectors @xmath50 , @xmath32 s and @xmath227 s ) , we always use the block version .",
    "preconditioning is a technique for reducing the condition number of the matrix . here",
    ", the benefit of preconditioning is twofold : it encourages the rapid convergence of an iterative linear solver and , if the effective condition number is small , it strongly bounds the uncertainty in using the estimating equations ( [ ascore ] ) instead of the exact score equations ( [ score ] ) for estimating parameters ( see theorem [ tmain ] ) . in numerical linear algebra",
    ", preconditioning refers to applying a matrix @xmath36 , which approximates the inverse of @xmath68 in some sense , to both sides of the linear system of equations . in the simple case of left preconditioning",
    ", this amounts to solving @xmath228 for @xmath229 better conditioned than @xmath68 . with certain algebraic manipulations , the matrix @xmath36 enters into the conjugate gradient algorithm in the form of multiplication with vectors . for the detailed algorithm ,",
    "see @xcite .",
    "this technique does not explicitly compute the matrix @xmath229 , but it requires that the matrix  vector multiplications with @xmath36 can be efficiently carried out.=-1    for covariance matrices , certain filtering operations are known to reduce the condition number , and some can even achieve an optimal preconditioning in the sense that the condition number is bounded by a constant independent of the size of the matrix [ @xcite ] .",
    "note that these filtering operations may or may not preserve the rank / size of the matrix .",
    "when the rank is reduced , then some loss of statistical information results when filtering , although similar filtering is also likely needed to apply spectral methods for strongly correlated spatial data on a grid [ @xcite ] .",
    "therefore , we consider applying the same filter to all the vectors and matrices in the estimating equations , in which case ( [ ascore ] ) becomes the stochastic approximation to the score equations of the _ filtered _ process .",
    "evaluating the filtered version of @xmath144 becomes easier because the linear solves with the filtered covariance matrix converge faster .",
    "[ secnonlinearsolver ] the choice of the nonlinear solver is problem dependent .",
    "the purpose of solving the score equations ( [ score ] ) or the estimating equations ( [ ascore ] ) is to maximize the loglikelihood function @xmath230 .",
    "therefore , investigation into the shape of the loglikelihood surface helps identify an appropriate solver .    in section [ sec5 ]",
    ", we consider the power law generalized covariance model ( @xmath231 ) : @xmath232 where @xmath233\\in{{\\mathbb r}}^d$ ] denotes coordinates , @xmath10 is the set of parameters containing @xmath231 , @xmath234\\in{{\\mathbb r}}^d$ ] , and @xmath235 is the elliptical radius @xmath236 allowing a different scaling in different directions may be appropriate when , for example , variations in a vertical direction may be different from those in a horizontal direction .",
    "the function @xmath106 is conditionally positive definite ; therefore , only the covariances of authorized linear combinations of the process are defined [ @xcite , section 4.3 ] . in fact , @xmath106 is @xmath58-conditionally positive definite if and only if @xmath237 [ see @xcite , section 4.5 ] , so that applying the discrete laplace filter ( which gives second - order differences ) @xmath238 times to the observations yields a set of authorized linear combinations when @xmath239 .",
    "@xcite show that if @xmath240 , then the covariance matrix has a bounded condition number independent of the matrix size .",
    "consider the grid @xmath241 for some fixed spacing @xmath242 and @xmath243 a vector whose components take integer values between @xmath244 and @xmath52 .",
    "applying the filter @xmath238 times , we obtain the covariance matrix @xmath245 where @xmath246 denotes the discrete laplace operator @xmath247 with @xmath248 meaning the unit vector along the @xmath58th coordinate .",
    "if @xmath249 , the resulting @xmath8 is both positive definite and reasonably well conditioned .",
    "figure [ figloglik ] shows a sample loglikelihood surface for @xmath250 based on an observation vector @xmath50 simulated from a 1d partial regular grid spanning the range @xmath251 $ ] , using parameters @xmath252 and @xmath253 .",
    "( a similar 2d grid is shown later in figure  [ figgrid ] . )",
    "the peak of the surface is denoted by the solid white dot , which is not far away from the truth @xmath254 .",
    "the white dashed curve ( profile of the surface ) indicates the maximum loglikelihoods @xmath255 given @xmath256 .",
    "the curve is also projected on the @xmath257 plane and the @xmath258 plane .",
    "one sees that the loglikelihood value has small variation ( ranges from @xmath259 to @xmath260 ) along this curve compared with the rest of the surface , whereas , for example , varying just the parameter @xmath261 changes the loglikelihood substantially .",
    "a newton - type nonlinear solver starts at some initial point @xmath158 and tries to approach the optimal point ( one that solves the score equations ) . ) .",
    "conceptually it is similar to that for solving the estimating equations ( [ ascore ] ) .",
    "] let the current point be @xmath159 .",
    "the solver finds a direction @xmath189 and a step size @xmath190 in some way to move the point to @xmath262 , so that the value of @xmath255 is increased .",
    "typically , the search direction @xmath189 is the inverse of the jacobian multiplied by @xmath263 , that is , @xmath264 . this way , @xmath161 is closer to a solution of the score equations .",
    "figure [ figloglik ] shows a loglikelihood surface when @xmath250 .",
    "the solver starts somewhere on the surface and quickly climbs to a point along the profile curve .",
    "however , this point might be far away from the peak .",
    "it turns out that along this curve a newton - type solver is usually unable to find a direction with an appropriate step size to numerically increase @xmath255 , in part because of the narrow ridge indicated in the figure .",
    "the variation of @xmath255 along the normal direction of the curve is much larger than that along the tangent direction .",
    "thus , the iterate @xmath159 is trapped and can not advance to the peak .",
    "in such a case , even though the estimated maximized likelihood could be fairly close to the true maximum , the estimated parameters could be quite distant from the mle of @xmath265 .    to successfully solve the estimating equations",
    ", we consider each component of @xmath261 an implicit function of @xmath256 .",
    "denote by @xmath266 the estimating equations , ignoring the fixed variable @xmath37 .",
    "the implicit function theorem indicates that a set of functions @xmath267 exists around an isolated zero of ( [ ascore2 ] ) in a neighborhood where ( [ ascore2 ] ) is continuously differentiable , such that @xmath268 therefore , we need only to solve the equation @xmath269 with a single variable @xmath256 . numerically , a much more robust method than a newton - type method exists for finding a root of a one - variable function .",
    "we use the standard method of forsythe , malcolm and moler [ ( @xcite ) , see the fortran code ] for solving ( [ ascore3 ] ) . this method in turn",
    "requires the evaluation of the left - hand side of ( [ ascore3 ] ) . then , the @xmath270 s are evaluated by solving @xmath271 fixing @xmath256 , whereby a newton - type algorithm is empirically proven to be an efficient method .",
    "[ secexp ] in this section we show a few experimental results based on a partially occluded regular grid .",
    "the rationale for using such a partial grid is to illustrate a setting where spectral techniques do not work so well but efficient matrix  vector multiplications are available .",
    "a partially occluded grid can occur , for example , when observations of some surface characteristics are taken by a satellite - based instrument and it is not possible to obtain observations over regions with sufficiently dense cloud cover .",
    "the ozone example in section [ sec6 ] provides another example in which data on a partial grid occurs .",
    "this section considers a grid with physical range @xmath251\\times[0,100]$ ] and a hole in a disc shape of radius @xmath272 centered at @xmath273 .",
    "an illustration of the grid , with size @xmath274 , is shown in figure  [ figgrid ] . the matrix ",
    "vector multiplication is performed by first doing the multiplication using the full grid via circulant embedding and fft , followed by removing the entries corresponding to the hole of the grid .",
    "recall that the covariance model is defined in section [ secnonlinearsolver ] , along with the explanation of the filtering step .",
    "grid with a region of missing observations in a disc shape .",
    "internal grid points are grouped to work with the dependent design in section [ secdependent ] . ]",
    "when working with dependent samples , it is advantageous to group nearby grid points such that the resulting blocks have a plump shape and that there are as many blocks with size exactly @xmath37 as possible . for an occluded grid",
    ", this is a nontrivial task .",
    "here we use a simple heuristic to effectively group the points .",
    "we divide the grid into horizontal stripes of width @xmath275 ( in case @xmath275 does not divide the grid size along the vertical direction , some stripes have a width @xmath276 ) .",
    "the stripes are ordered from bottom to top , and the grid points inside the odd - numbered stripes are ordered lexicographically in their coordinates , that is , @xmath277 . in order to obtain as many contiguous blocks as possible ,",
    "the grid points inside the even - numbered stripes are ordered lexicographically according to @xmath278 .",
    "this ordering gives a zigzag flow of the points starting from the bottom - left corner of the grid .",
    "every @xmath37 points are grouped in a block .",
    "the coloring of the grid points in figure [ figgrid ] shows an example of the grouping .",
    "note that because of filtering , observations on either an external or internal boundary are not part of any block .",
    "one of the most important factors that affect the efficacy of approximating the score equations is the value @xmath37 .",
    "theorem [ tmain ] indicates that @xmath37 should increase at least like @xmath98 in order to guarantee the additional uncertainty introduced by approximating the score equations be comparable with that caused by the randomness of the sample @xmath50 . in the ideal case , when the condition number of the matrix ( possibly with filtering ) is bounded independent of the matrix size  @xmath1 , then even taking @xmath94 is sufficient to obtain estimates with the same rate of convergence as the exact score equations . when @xmath206 grows with @xmath1 , however , a better guideline for selecting @xmath37 is to consider the growth of @xmath279 .",
    "figure [ figpowerbounds ] plots the condition number of @xmath8 and the spectral norm of @xmath279 for varying sizes of the matrix and preconditioning using the laplacian filter .",
    "although performing a laplacian filtering will yield provably bounded condition numbers only for the case @xmath280 , one sees that the filtering is also effective for the cases @xmath281 and @xmath282 .",
    "moreover , the norm of @xmath283 is significantly smaller than @xmath206 when @xmath1 is large and , in fact , it does not seem to grow with @xmath1 .",
    "this result indicates the bound in theorem 1 is sometimes far too conservative and that using a fixed @xmath37 can be effective even when @xmath206 grows with @xmath1 .",
    "compared with that of @xmath284 , for power law kernel in 2d .",
    "left : @xmath281 ; right : @xmath252 . ]",
    "of course , the norm of @xmath279 is not always bounded . in figure",
    "[ figmaternbounds ] we show two examples using the matrn covariance kernel with smoothness parameter @xmath285 and 1.5 ( essentially @xmath280 and 3 ) . without filtering , both @xmath98 and @xmath286 grow with @xmath1 , although the plots show that the growth of the latter is significantly slower than that of the former .",
    "compared with that of @xmath284 , for matrn kernel in 1d , without filtering .",
    "left : @xmath285 ; right : @xmath287 . ]",
    "if the occluded observations are more scattered , then the fast matrix  vector multiplication based on circulant",
    "embedding still works fine .",
    "however , if the occluded pixels are randomly located and the fraction of occluded pixels is substantial , then using a filtered data set only including laplacians centered at those observations whose four nearest neighbors are also available might lead to an unacceptable loss of information . in this case",
    ", one might instead use a preconditioner based on a sparse approximation to the inverse cholesky decomposition as described in section [ sec6 ] .      here",
    ", we show the details of solving the estimating equations ( [ ascore ] ) using a @xmath274 grid as an example . setting the truth @xmath288 and @xmath289",
    "[ i.e. , @xmath290 , consider exact and approximate maximum likelihood estimation based on the data obtained by applying the laplacian filter once to the observations .",
    "writing @xmath291 for @xmath292 , one way to evaluate the approximate mles is to compute the ratios of the square roots of the diagonal elements of @xmath293 to the square roots of the diagonal elements of @xmath294 .",
    "we know these ratios must be at least 1 , and that the closer they are to 1 , the more nearly optimal the resulting estimating equations based on the approximate score function are . for @xmath295 and independent sampling ,",
    "we get 1.0156 , 1.0125 and 1.0135 for the three ratios , all of which are very close to 1 .",
    "since one generally can not calculate @xmath293 exactly , it is also worthwhile to compare a stochastic approximation of the diagonal values of @xmath293 to their exact values .",
    "when this approximation was done once for @xmath295 and by using @xmath296 in ( [ iij ] ) and ( [ jij2 ] ) , the three ratios obtained were 0.9821 , 0.9817 and 0.9833 , which are all close to 1 .",
    "( 1 , 2 , 4 , 8 , 16 , 32 , 64 ) . in each plot",
    ", the curve with the plus sign corresponds to the independent design , whereas that with the circle sign corresponds to the dependent design .",
    "the horizontal axis represents @xmath37 . in plots , and , the vertical axis represents the mean squared differences between the approximate and exact mles divided by the mean squared errors for the exact mles , for the components @xmath256 , @xmath297 and @xmath298 , respectively . in plot , the vertical axis represents the mean squared difference between the loglikelihood values at the exact and approximate mles . ]",
    "figure [ fign ] shows the performance of the resulting estimates ( to be compared with the exact mles obtained by solving the standard score equations ) . for @xmath94 , @xmath299 , @xmath300 , @xmath301 , @xmath302 , @xmath303 and @xmath304",
    ", we simulated 100 realizations of the process on the @xmath274 occluded grid , applied the discrete laplacian once , and then computed exact mles and approximations using both independent and dependent ( as described in the beginning of section [ secexp ] ) sampling . when @xmath94 , the independent and dependent sampling schemes are identical , so only results for independent sampling are given .",
    "figure [ fign ] plots , for each component of @xmath10 , the mean squared differences between the approximate and exact mles divided by the mean squared errors for the exact mles",
    ". as expected , these ratios decrease with @xmath37 , particularly for dependent sampling .",
    "indeed , dependent sampling is much more efficient than independent sampling for larger @xmath37 ; for example , the results in figure [ fign ] show that dependent sampling with @xmath305 yields better estimates for all three parameters than does independent sampling with @xmath295 .",
    "we experimented with larger grids ( in the same physical range ) .",
    "we show the results in table [ tablargescale ] and figure [ figlargescale ] for @xmath295 . when the matrix becomes large , we are unable to compute @xmath306 and @xmath291 exactly . based on the preceding experiment , it seems reasonable to use @xmath296 in approximating @xmath306 and @xmath291 .",
    "therefore , the elements of @xmath306 and @xmath291 in table [ tablargescale ] were computed only approximately .",
    "@lcd2.4d2.4d2.4cc@ * grid size * & & & & & & + @xmath307 & 1.5355 & 1.5084 & 1.4919 & 1.4975 & 1.5011 & 1.5012 + & 6.8507 & 6.9974 & 7.1221 & 7.0663 & 6.9841 & 6.9677 + & 9.2923 & 10.062 & 10.091 & 10.063 & 9.9818 & 9.9600 + [ 6pt ] @xmath308 & 0.0882 & 0.0406 & 0.0196 & 0.0096 & 0.0048 & 0.0024",
    "+ & 0.5406 & 0.3673 & 0.2371 & 0.1464 & 0.0877 & 0.0512 + & 0.8515 & 0.5674 & 0.3605 & 0.2202 & 0.1309 & 0.0760 + [ 6pt ] @xmath309 & 1.0077 & 1.0077 & 1.0077 & 1.0077 & 1.0077 & 1.0077 + & 1.0062 & 1.0070 & 1.0073 & 1.0074 & 1.0075 & 1.0076 + & 1.0064 & 1.0071 & 1.0073 & 1.0075 & 1.0075 & 1.0076 +     times a constant . ]",
    "one sees that as the grid becomes larger ( denser ) , the variance of the estimates decreases as expected .",
    "the matrices @xmath294 and @xmath293 are comparable in all cases and , in fact , the ratios stay roughly the same across different sizes of the data .",
    "the experiments were run for data size up to around one million , and the scaling of the running time versus data size is favorable .",
    "the results show a strong agreement of the recorded times with the scaling @xmath24 .",
    "ozone in the stratosphere blocks ultraviolet radiation from the sun and is thus essential to all land - based life on earth .",
    "satellite - based instruments run by nasa have been measuring total column ozone in the atmosphere daily on a near global scale since 1978 ( although with a significant gap in 19941996 ) and the present instrument is the omi . here , we consider level 3 gridded data for the month april 2012 in the latitude band @xmath3@xmath4n [ aura omi ozone level-3 global gridded ( @xmath310 deg ) data product - omto3d ( v003 ) ] . because total column ozone shows persistent patterns of variation with location , we demeaned the data by , for each pixel , subtracting off the mean of the available observations during april 2012 .",
    "figure [ figozone ] displays the resulting demeaned data .",
    "there are potentially @xmath311 observations on each day in this latitude strip .",
    "however , figure [ figozone ] shows 14 or 15 strips of missing observations each day , which is due to a lack of overlap in omi observations between orbits in this latitude band ( the orbital frequency of the satellite is approximately 14.6 orbits per day ) .",
    "furthermore , there is nearly a full day of missing observations toward the end of the record . for the 30-day period",
    ", a complete record would have 108,000 observations , of which 84,942 are available .",
    "the local time of the level 2 data on which the level 3 data are based is generally near noon due to the sun - synchronous orbit of the satellite , but there is some variation in local time of level 2 data because omi simultaneously measures ozone over a swath of roughly 3000 km , so that the actual local times of the level 2 data vary up to about 50 minutes from local noon in the latitude band we are considering .",
    "nevertheless , @xcite showed that , for level 3 total column ozone levels ( as measured by a predecessor instrument to the omi ) , as long as one stays away from the equator , little distortion is caused by assuming all observations are taken at exactly local noon and we will make this assumption here . as a consequence , within a given day , time ( absolute as opposed to local ) and longitude are completely confounded , which makes distinguishing longitudinal and temporal dependencies difficult .",
    "indeed , if one analyzed the data a day at a time , there would be essentially no information for distinguishing longitude from time , but by considering multiple days in a single analysis , it is possible to distinguish their influences on the dependence structure .",
    "fitting various matrn models to subsets of the data within a day , we found that the local spatial variation in the data is described quite well by the whittle model ( the matrn model with smoothness parameter 1 ) without a nugget effect .",
    "results in @xcite suggest some evidence for spatial anisotropy in total column ozone at midlatitudes , but the anisotropy is not severe in the band @xmath3@xmath4n and we will ignore it here .",
    "the most striking feature displayed in figure [ figozone ] is the obvious westerly flow of ozone across days .",
    "based on these considerations , we propose the following simple model for the demeaned data @xmath312 . denoting by @xmath235 the radius of the earth , @xmath313 the latitude , @xmath314 the longitude , and @xmath315 the time , we assume @xmath50 is a 0 mean gaussian process with covariance function ( parameterized by @xmath316 , @xmath317 , @xmath318 and @xmath319 ) : @xmath320 where @xmath321 is the temporal difference , @xmath322 is the ( adjusted for drift ) spatial difference and @xmath323 maps a spherical coordinate to @xmath324 . here , @xmath325 is the matrn correlation function @xmath326 with @xmath327 the modified bessel function of the second kind of order @xmath328 .",
    "we used the following unit system : @xmath313 and @xmath329 are in degrees , @xmath315 is in days , and @xmath330 . in contravention of standard notation , we take longitude to increase as one heads westward in order to make longitude increase with time within a day . although the use of euclidean distance in @xmath331 might be viewed as problematic [ @xcite ] , it is not clear that great circle distances are any more appropriate in the present circumstance in which there is strong zonal flow .",
    "the model ( [ spacetime ] ) has the virtues of simplicity and of validity : it defines a valid covariance function on the sphere@xmath332time whenever @xmath333 and @xmath318 are positive .",
    "a more complex model would clearly be needed if one wanted to consider the process on the entire globe rather than in a narrow latitude band .    because the covariance matrix @xmath334 can be written as @xmath335 , where the entries of @xmath36 are generated by the matrn function , the estimating equations ( [ ascore ] ) give @xmath336 as the mle of @xmath316 given values for the other parameters .",
    "therefore , we only need to solve ( [ ascore ] ) with respect to @xmath317 , @xmath318 and  @xmath319 . initial values for the parameters were obtained by applying a simplified fitting procedure to a subset of the data .",
    "we first fit the model using observations from one latitude at a time .",
    "since there are about 8500 observations per latitude band , it is possible , although challenging , to compute the exact mles for the observations within a single band using the cholesky decomposition .",
    "however , we chose to solve ( [ ascore ] ) with the number @xmath37 of i.i.d .",
    "symmetric bernoulli vectors @xmath32 fixed at 64 . a first order finite difference filtering [ @xcite ]",
    "was observed to be the most effective in encouraging the convergence of the linear solve .",
    "differences across gaps in the data record were included , so the resulting sizes of the filtered data sets were just one less than the number of observations available in each longitude . under our model , the covariance matrix of the observations within a latitude can be embedded in a circulant matrix of dimension 21,600 , greatly speeding up the necessary matrix  vector multiplications .",
    "table [ tabozoneonelat ] summarizes the resulting estimates and the fisher information for each latitude band .",
    "the estimates are consistent across latitudes and do not show any obvious trends with latitude except perhaps at the two most northerly latitudes .",
    "the estimates of @xmath319 are all near @xmath337 , which qualitatively matches the westerly flow seen in figure  [ figozone ] .",
    "the differences between @xmath338 and @xmath339 were all less than @xmath340 , indicating that the choice of @xmath37 is sufficient .",
    "@lccd2.3ccccc@ & & & & & + & & & & & + * latitude * & @xmath341 @xmath342**10**@xmath343 & @xmath344 & & @xmath345 & @xmath342**10**@xmath343 & & & + @xmath346n & 1.076 & 2.110 & 11.466 & @xmath347 & 0.106 & 0.127 & 0.586 & 0.244 + @xmath348n & 1.182 & 2.172 & 11.857 & @xmath349 & 0.123 & 0.136 & 0.634 & 0.251 + @xmath350n & 1.320 & 2.219 & 12.437 & @xmath351 & 0.144 & 0.145 & 0.698 & 0.266 + @xmath352n & 1.370 & 2.107 & 12.104 & @xmath353 & 0.145 & 0.136 & 0.660 & 0.285 + @xmath354n & 1.412 & 2.059 & 11.845 & @xmath355 & 0.145 & 0.130 & 0.628 & 0.294 + @xmath356n & 1.416 & 2.010 & 11.814 & @xmath357 & 0.147 & 0.128 & 0.632 & 0.313 + @xmath358n & 1.526 & 2.075 & 12.254 & @xmath359 & 0.166 & 0.138 & 0.686 & 0.320 + @xmath360n & 1.511 & 2.074 & 11.939 & @xmath361 & 0.161 & 0.135 & 0.654 & 0.319 + @xmath362n & 1.325 & 1.887 & 10.134 & @xmath355 & 0.128 & 0.114 & 0.505 & 0.303 + @xmath363n & 1.246 & 1.846 & 9.743 & @xmath364 & 0.117 & 0.110 & 0.473 & 0.305",
    "+    the following is an instance of the asymptotic correlation matrix , obtained by normalizing each entry of @xmath294 ( at @xmath363n ) with respect to the diagonal : @xmath365.\\ ] ] we see that @xmath366 and @xmath367 are all strongly correlated .",
    "the high correlation of the estimated range parameters @xmath368 and @xmath367 with the estimated scale @xmath369 is not unexpected considering the general difficulty of distinguishing scale and range parameters for strongly correlated spatial data [ @xcite ] .",
    "the strong correlation of the two range parameters is presumably due to the near confounding of time and longitude for these data .",
    "next , we used the data at all latitudes and progressively increased the number of days .",
    "in this setting , the covariance matrix of the observations can be embedded in a block circulant matrix with blocks of size @xmath370 corresponding to the 10 latitudes .",
    "therefore , multiplication of the covariance matrix times a vector can be accomplished with a discrete fourier transform for each pair of latitudes , or @xmath371 discrete fourier transforms . because we are using the whittle covariance function as the basis of our model",
    ", we had hoped filtering the data using the laplacian would be an effective preconditioner . indeed , it does well at speeding the convergence of the linear solves , but it unfortunately appears to lose most of the information in the data for distinguishing spatial from temporal influences , and thus is unsuitable for these data .",
    "instead , we used a banded approximate inverse cholesky factorization [ @xcite , ( 2.5 ) , ( 2.6 ) ] to precondition the linear solve . specifically , we ordered the observations by time and then , since observations at the same longitude and day are simultaneous , by latitude south to north .",
    "we then obtained an approximate inverse by subtracting off the conditional mean of each observation given the previous 20 observations , so the approximate cholesky factor has bandwidth 21 .",
    "we tried values besides 20 for the number of previous observations on which to condition , but 20 seemed to offer about the best combination of fast computing and effective preconditioning .",
    "the number @xmath37 of i.i.d .",
    "symmetric bernoulli vectors @xmath32 was increased to 128 , in order that the differences between @xmath372 and @xmath339 were around @xmath373 .",
    "the results are summarized in table [ tabozonealllat ] .",
    "one sees that the estimates are reasonably consistent with those shown in table [ tabozoneonelat ] . nevertheless , there are some minor discrepancies such as estimates of @xmath319 that are modestly larger ( in magnitude ) than found in table  [ tabozonealllat ] , suggesting that taking account of correlations across latitudes changes what we think about the advection of ozone from day to day .",
    "@lcccccccc@ & & & & & + & & & & & + * days * & @xmath341 @xmath342**10**@xmath343 & @xmath344 & @xmath374 & @xmath345 & @xmath342**10**@xmath343 & & & +   + 13 & @xmath375 & @xmath376 & @xmath377 & @xmath378 & @xmath379 & @xmath380 & @xmath381 & @xmath382 + 110 & @xmath383 & @xmath384 & @xmath385 & @xmath386 & @xmath387 & @xmath388 & @xmath389 & @xmath390 + 120 & @xmath391 & @xmath392 & @xmath393 & @xmath394 & @xmath395 & @xmath396 & @xmath397 & @xmath398 + 130 & @xmath399 & @xmath400 & @xmath401 & @xmath402 & @xmath403 & @xmath404 & @xmath405 & @xmath406 + [ 6pt ] + 130 & @xmath407 & @xmath408 & @xmath409 & @xmath410 & @xmath411 & @xmath412 & @xmath413 & @xmath414 +    note that the approximate inverse cholesky decomposition , although not as computationally efficient as applying the discrete laplacian , is a full rank transformation and thus does not throw out any statistical information .",
    "the method does require ordering the observations , which is convenient in the present case in which there are at most 10 observations per time point .",
    "nevertheless , we believe this approach may be attractive more generally , especially for data that are not on a grid .",
    "we also estimated the parameters using the dependent sampling scheme described in section [ sec3 ] with @xmath415 and obtained estimates given in the last row of table [ tabozonealllat ] .",
    "it is not as easy to estimate @xmath416 as defined in theorem [ tdependent ] as it is to estimate @xmath69 with independent @xmath32 s .",
    "we have carried out limited numerical calculations by repeatedly calculating @xmath417 for @xmath418 fixed at the estimates for dependent samples of size @xmath415 and have found that the advantages of using the dependent sampling are negligible in this case .",
    "we suspect that the reason the gains are not as great as those shown in figure  [ fign ] is due to the substantial correlations of observations that are at similar locations a day apart .",
    "we have demonstrated how derivatives of the loglikelihood function for a gaussian process model can be accurately and efficiently calculated in situations for which direct calculation of the loglikelihood itself would be much more difficult .",
    "being able to calculate these derivatives enables us to find solutions to the score equations and to verify that these solutions are at least local maximizers of the likelihood .",
    "however , if the score equations had multiple solutions , then , assuming all the solutions could be found , it might not be so easy to determine which was the global maximizer .",
    "furthermore , it is not straightforward to obtain likelihood ratio statistics when only derivatives of the loglikelihood are available .",
    "perhaps a more critical drawback of having only derivatives of the loglikelihood occurs when using a bayesian approach to parameter estimation .",
    "the likelihood needs to be known only up to a multiplicative constant , so , in principle , knowing the gradient of the loglikelihood throughout the parameter space is sufficient for calculating the posterior distribution .",
    "however , it is not so clear how one might calculate an approximate posterior based on just gradient and perhaps hessian values of the loglikelihood at some discrete set of parameter values .",
    "it is even less clear how one could implement an mcmc scheme based on just derivatives of the loglikelihood .    despite this substantial drawback",
    ", we consider the development of likelihood methods for fitting gaussian process models that are nearly @xmath0 in time and , perhaps more importantly , @xmath0 in memory , to be essential for expanding the scope of application of these models .",
    "calling our approach nearly @xmath0 in time admittedly glosses over a number of substantial challenges .",
    "first , we need to have an effective preconditioner for the covariance matrix @xmath8 .",
    "this allows us to treat @xmath37 , the number of random vectors in the stochastic trace estimator , as a fixed quantity as @xmath1 increases and still obtain estimates that are nearly as efficient as full maximum likelihood .",
    "the availability of an effective preconditioner also means that the number of iterations of the iterative solve can remain bounded as @xmath1 increases .",
    "we have found that @xmath419 is often sufficient and that the number of iterations needed for the iterative solver to converge to a tight tolerance can be several dozen , so writing @xmath0 can hide a factor of several thousand .",
    "second , we are assuming that matrix ",
    "vector multiplications can be done in nearly @xmath0 time .",
    "this is clearly achievable when the number of nonzero entries in @xmath8 is @xmath0 or when observations form a partial grid and a stationary model is assumed so that circulant embedding applies . for dense , unstructured matrices",
    ", fast multipole methods can achieve this rate , but the method is only approximate and the overhead in the computations is substantial so that @xmath1 may need to be very large for the method to be faster than direct multiplication .",
    "however , even when using exact multiplication , which requires @xmath16 time , despite the need for @xmath37 iterative solves , our approach may still be faster than computing the cholesky decomposition , which requires @xmath17 computations .",
    "furthermore , even when @xmath8 is dense and unstructured , the iterative algorithm is @xmath0 in memory , assuming that elements of @xmath8 can be calculated as needed , whereas the cholesky decomposition requires @xmath16 memory .",
    "thus , for example , for @xmath1 in the range 10,000100,000 , even if @xmath8 has no exploitable structure , our approach to approximate maximum likelihood estimation may be much easier to implement on the current generation of desktop computers than an approach that requires calculating the cholesky decomposition of @xmath8 .",
    "the fact that the condition number of @xmath8 affects both the statistical efficiency of the stochastic trace approximation and the number of iterations needed by the iterative solver indicates the importance of having good preconditioners to make our approach effective .",
    "we have suggested a few possible preconditioners , but it is clear that we have only scratched the surface of this problem .",
    "statistical problems often yield covariance matrices with special structures that do not correspond to standard problems arising in numerical analysis .",
    "for example , the ozone data in section [ sec6 ] has a partial confounding of time with longitude that made laplacian filtering ineffective as a preconditioner .",
    "further development of preconditioners , especially for unstructured covariance matrices , will be essential to making our approach broadly effective .",
    "proof of theorem [ tmain ] since @xmath8 is positive definite , it can be written in the form @xmath420 with @xmath331 orthogonal and @xmath421 diagonal with elements @xmath422 .",
    "then @xmath423 is symmetric , @xmath424 and , similarly , @xmath425 for real @xmath426 , @xmath427 furthermore , by ( [ firstterm ] ) , @xmath428 and , by ( [ secondterm ] ) , @xmath429 write @xmath430 for @xmath431 and note that @xmath432 .",
    "consider finding an upper bound to @xmath433 think of maximizing this ratio as a function of the @xmath434 s for fixed @xmath225 s .",
    "we then have a ratio of two positively weighted sums of the same positive scalars ( the @xmath434 s for @xmath435 ) , so this ratio will be maximized if the only positive @xmath434 values correspond to cases for which the ratio of the weights , here @xmath436 is maximized .",
    "since we are considering only @xmath435 , @xmath437 and @xmath438 is increasing on @xmath439 , so ( [ ratioweights ] ) is maximized when @xmath440 and @xmath441 , yielding @xmath442 the theorem follows by putting this result together with ( [ b ] ) , ( [ jij ] ) and  ( [ lastterm ] ) .",
    "proof of theorem [ tdependent ] define @xmath443 to be the @xmath444th element of @xmath445 and @xmath446 the @xmath444th diagonal element of @xmath447 . then note that for @xmath139 and @xmath448 and @xmath449 , @xmath450 have the same joint distribution as for independent @xmath32 s . specifically , the two components are independent symmetric bernoulli random variables unless @xmath451 and @xmath452 or @xmath453 and @xmath454 , in which case they are the same symmetric bernoulli random variable .",
    "straightforward calculations yield ( [ improve ] ) .",
    "the data used in this effort were acquired as part of the activities of nasas science mission directorate , and are archived and distributed by the goddard earth sciences ( ges ) data and information services center ( disc ) ."
  ],
  "abstract_text": [
    "<S> we discuss the statistical properties of a recently introduced unbiased stochastic approximation to the score equations for maximum likelihood calculation for gaussian processes . under certain conditions , including bounded condition number of the covariance matrix , the approach achieves @xmath0 storage and nearly @xmath0 computational effort per optimization step , where @xmath1 is the number of data sites . here </S>",
    "<S> , we prove that if the condition number of the covariance matrix is bounded , then the approximate score equations are nearly optimal in a well - defined sense . therefore , not only is the approximation efficient to compute , but it also has comparable statistical properties to the exact maximum likelihood estimates . </S>",
    "<S> we discuss a modification of the stochastic approximation in which design elements of the stochastic terms mimic patterns from a @xmath2 factorial design . </S>",
    "<S> we prove these designs are always at least as good as the unstructured design , and we demonstrate through simulation that they can produce a substantial improvement over random designs . </S>",
    "<S> our findings are validated by numerical experiments on simulated data sets of up to 1 million observations . </S>",
    "<S> we apply the approach to fit a space  time model to over 80,000 observations of total column ozone contained in the latitude band @xmath3@xmath4n during april 2012 .    , </S>"
  ]
}