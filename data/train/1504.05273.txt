{
  "article_text": [
    "we have seen the success of the matrix svd for several decades . however in the advent of modern and massive datasets , even svd has its limitation . since tensors have been known to be a natural representation of higher - order and hierarchical dimensional datasets , we focus on the extension of low rank matrix approximation to tensors .",
    "tensors have received much attention in recent years in the areas of signal processing @xcite , computer vision @xcite , neuroscience @xcite , data science and machine learning @xcite .",
    "most of these applications rely on decomposing a tensor data into its low - rank form to be able to perform efficient computing as well as to reduce memory requirements .",
    "this type of tensor decomposition into a sum of rank - one tensor terms is called the canonical polyadic ( cp ) decomposition ; thus , it is viewed as a generalization of the matrix svd .",
    "the generalization of matrix svd to tensors is not unique .",
    "another tensor decomposition is called the higher - order svd @xcite , which is a product of orthogonal matrices with a dense core tensor .",
    "higher - order svd is considered another extension of the matrix svd .    unlike the matrix case where the low - rank matrix approximation is afforded by truncating away _ small _ rank - one matrix terms @xcite , discarding negligible rank - one tensor terms",
    "does not necessarily provide the best low - rank tensor approximation @xcite .",
    "moreover , most low rank tensor algorithms do not provide an estimation on the tensor rank ; an a priori tensor rank is often required to find the decomposition .",
    "several theoretical results @xcite on tensor rank can help , but they are limited to low - multidimensional and low order tensors so they are inapplicable to tensors in real - life applications .",
    "in fact , for real dataset , tensor rank is important . in a source apportionment data problem",
    "@xcite , the tensor rank of the data provides the number of pollution source profiles to be identified . in this work ,",
    "the focus is on finding an estimation of the tensor rank and its rank - one tensor decomposition ( cp ) of a given tensor .",
    "there are several numerical techniques @xcite for approximating a @xmath1th rank tensor into its cp decomposition , but they do not give an approximation of the minimum rank .",
    "there are algorithms @xcite which give tensor rank , but they are specific to symmetric tensor decomposition over the complex field using algebraic geometry tools .    our proposed algorithm addresses two difficult problems for the cp decomposition : ( a ) one is that finding the rank of tensors is a np - hard problem @xcite , and ( b ) the other is that tensors can be ill - posed @xcite and failed to have their best low - rank approximations .    the tensor rank problem is formulated as an @xmath2 minimization problem ; i.e. @xmath3_r\\end{aligned}\\ ] ] where @xmath4_r$ ] represents a sum of the outer products of the vectors @xmath5 and @xmath6 for @xmath7 . here",
    "@xmath8 corresponds the number of nonzero coefficients in the sum .",
    "however , this problem formulation ( [ nphard ] ) is np - hard .",
    "inspired by the techniques in compressive sensing @xcite , we then consider an @xmath0-regularization formulation for tensor rank , @xmath9_r.\\end{aligned}\\ ] ] here we denote @xmath10 .",
    "it is well known in the compressed sensing community that minimizing the @xmath11 norm of the vector @xmath12 recovers the sparse solution of the linear system . in the presence of noise , the constraint , @xmath13_r$ ]",
    "is replaced with @xmath14_r \\vert_f \\leq\\varepsilon$ ] where @xmath15 is the frobenius norm with @xmath16 . moreover , to achieve a tensor decomposition as well as tensor rank , we minimize over the factor matrices , @xmath17 and @xmath18 , thus this minimization problem is considered : @xmath19_r\\|_f^2+\\lambda\\|\\bm{\\alpha}\\|_1.\\end{aligned}\\ ] ] the @xmath11-regularization achieves a good approximation of tensor rank due to the sparsity structure and its tractability .",
    "in addition , the @xmath0-regularization term provides a restriction on the boundedness of the variables thereby ameliorating the ill - posedness of the best low - rank approximation of tensors . for more tractable computing , an alternative multi - block constraint optimization @xcite",
    "is implemented , which is similar to the technique discussed in @xcite . since",
    "( [ l1regnoise ] ) is a minimization of a sum of a smooth term and a nonsmooth term , we consider the following optimization problem with smooth and non smooth terms : @xmath20 where @xmath21 and @xmath22 are the smooth and nonsmooth functions , respectively . here",
    "@xmath21 is approximated at a given point @xmath23 .",
    "here we list our contributions in this paper :    1 .",
    "we develop an iterative technique for tensor rank approximation given that the main objective function contains a nonsmooth @xmath0-regularization term . the proximal alternating minimization technique @xcite has been adapted and rescaled for our tensor rank minimization problem .",
    "we provide some theoretical results on the convergence of our algorithm .",
    "we show the objective function satisfies a descent property in lemma [ descent ] and a subdifferential lower bound @xcite . a monotonically decreasing objective function",
    "is ensured on the sequence generated by the algorithm .",
    "furthermore , we point out that the sequence generated by algorithm converges to a critical point of the objective function with indicator functions on the normalization constraint that all the columns of the factor matrices have length one .",
    "3 .   for practical implementation",
    ", we provide a technique ( as well as theoretical results ) to find a suitable choice on the regularization parameter directly from the data .",
    "the regularization parameter choice has remained a very challenging problem @xcite in applied inverse problems .",
    "our technique is based on the probabilistic consistency of the sparsity in the classical model found in @xcite : @xmath24 where @xmath25 is a sparse signal , @xmath26 is a design matrix and @xmath27 is a vector of independent subgaussian entries with mean zero and parameter @xmath28 .",
    "we show that to find the true sparsity structure with a high probability , the regularization parameter relies on two intrinsic parameters @xmath28 and @xmath29 of models , where @xmath28 represents the variance of noise , and @xmath29 is the incoherence parameter @xcite on design matrix @xmath26 .",
    "the relationship between the regularization and intrinsic parameters actually provides us a suggestion on how to choose a reasonable regularization parameter for practical computation . to illustrate the performance of this low - rank approximation method ,",
    "our experiment consists of four parts . in the first part",
    ", we show the relationship between the regularization parameter and the estimated rank . in the second part",
    ", we estimate the number of rank - one components for given tensors by adaptively selecting the regularization parameter @xmath30 . in the third one",
    ", we compare our algorithm with a modified alternating least - squares algorithm . in the last one",
    ", we handle the real surveillance video data .",
    "our paper is organized as follows . in section 2 , we provide some notations and terminologies used throughout this paper . in section 3 ,",
    "we formulate an @xmath0-regularization optimization to the low - rank approximation of tensors . in section 4",
    ", we propose an algorithm to solve this @xmath0-regularization optimization by using a rescaling version of the proximal alternating minimization technique . in section 5",
    "we discuss the probabilistic consistency of the sparse optimal solution and give a suggestion on how to choose the regularization parameter .",
    "the numerical experiments in section 6 consist of simulated and real datasets .",
    "finally , our conclusion and future work are given in section 7 .",
    "we denote a vector by a bold lower - case letter @xmath31 . the bold upper - case letter @xmath32 represents a matrix and",
    "the symbol of tensor is a calligraphic letter @xmath33 . throughout this paper ,",
    "we focus on third - order tensors @xmath34 of three indices @xmath35 and @xmath36 , but all the methods proposed here can be also applied to tensors of arbitrary high order .",
    "a third - order tensor @xmath33 has column , row and tube fibers , which are defined by fixing every index but one and denoted by @xmath37 , @xmath38 and @xmath39 respectively .",
    "correspondingly , we can obtain three kinds @xmath40 and @xmath41 of matricization of @xmath33 according to respectively arranging the column , row , and tube fibers to be columns of matrices .",
    "we can also consider the vectorization for @xmath33 to obtain a row vector @xmath31 such the elements of @xmath33 are arranged according to @xmath1 varying faster than @xmath42 and @xmath42 varying faster than @xmath43 , i.e. , @xmath44 .",
    "the outer product @xmath45 of three nonzero vectors @xmath46 and @xmath47 is a rank - one tensor with elements @xmath48 for all the indices . a canonical polyadic ( cp ) decomposition of @xmath49 expresses @xmath33 as a sum of rank - one outer products : @xmath50 where @xmath51 for @xmath52 .",
    "every outer product @xmath53 is called as a rank - one component and the integer @xmath54 is the number of rank - one components in tensor @xmath33 .",
    "the minimal number @xmath54 such that the decomposition ( [ cpd ] ) holds is the rank of tensor @xmath33 , which is denoted by @xmath55 . for any tensor @xmath49",
    ", @xmath55 has an upper bound @xmath56 @xcite .",
    "the cp decomposition ( [ cpd ] ) can be also written as : @xmath57 where @xmath58 is a rescaling coefficient of rank - one tensor @xmath53 for @xmath59 . for convenience ,",
    "we let @xmath60 and @xmath61_r = \\sum_{r=1}^{r } \\alpha_r\\mathbf{x}_r\\circ\\mathbf{y}_r\\circ\\mathbf{z}_r$ ] in ( [ cpd2 ] ) where @xmath62 and @xmath63 are called the factor matrices of tensor @xmath33 .",
    "we impose a normalization constraint on factor matrices such that each column is normalized to length one @xcite which is denoted by @xmath64 . for most alternating optimization algorithms for tensors , _ flattening _ the tensor ( matricization ) is necessary to be able to break down the problem into several subproblems . here",
    "we describe a standard approach for a matricizing of a tensor .",
    "the khatri - rao product of two matrices @xmath65 and @xmath66 is defined as @xmath67 where the symbol ",
    "@xmath68 \" denotes the kronecker product : @xmath69 using the khatri - rao product , the decomposition ( [ cpd2 ] ) can be written in three different matrix forms of tensor @xmath33 @xcite : @xmath70 where the matrix @xmath71 is diagonal with elements of @xmath12 .",
    "the main goal of this study is to find a tensor of low - rank of the original tensor efficiently and accurately .",
    "we first formulate a tensor rank optimization problem : @xmath72 for any given error @xmath73 , the minimal rank of @xmath74 such that @xmath75 is no larger than @xmath55 .",
    "the optimal solution @xmath76 is a low - rank approximation of @xmath33 with error @xmath73 .",
    "we represent the tensor @xmath74 as @xmath4_r$ ] where @xmath54 is a upper bound of the rank of @xmath33 and columns of @xmath77 satisfy the normalization constraint @xmath64 . rescaling the columns of the matrices",
    "@xmath77 is a standard technique @xcite .",
    "it is implemented in practice for canonical polyadic tensor decomposition to prevent the norm of the approximated matrices blowing up to infinity while another factor matrix tend to zero while keeping the residual small .",
    "the tensor rank minimization is equivalent to the following constraint optimization problem with @xmath2-norm : @xmath78_r\\|_f^2\\leq\\varepsilon,\\mathbf{n(x , y , z)}=1\\ ] ] the problem ( [ problem2 ] ) is equivalent to that of finding the rank of tensors when @xmath79 , whose decision version is np - hard @xcite .",
    "to make it more tractable , we turn to an optimization problem with @xmath0-norm : @xmath80_r\\|_f^2\\leq\\varepsilon,\\mathbf{n(x , y , z)}=1\\ ] ] furthermore , we then solve : @xmath81_r\\|_f^2+\\lambda\\|\\bm{\\alpha}\\|_1 \\quad\\mbox{s.t.}\\ \\mathbf{n(x , y , z)}=1,\\ ] ] an @xmath0-regularization optimization problem in which it includes the factor matrices as primal variables .",
    "these optimization formulations are common in compressed sensing @xcite . by introducing the indicator function",
    ", we switch the constrained optimization problem ( [ problem4 ] ) into the following unconstrained form : @xmath82_r\\|_f^2+\\lambda\\|\\bm{\\alpha}\\|_1 + \\delta_{s_1}(\\mathbf{x})+\\delta_{s_2}(\\mathbf{y})+\\delta_{s_3}(\\mathbf{z})\\ ] ] where @xmath83 and @xmath84 .",
    "here there is no simple manner to compute the relationship between @xmath73 and @xmath30 without already knowing the optimal solutions of formulations ( [ problem3 ] ) and ( [ problem4 ] ) . in the matrix versions of basis pursuit : @xmath85 and @xmath86 it is possible to create a mapping between the two parameters through a pareto curve to estimate the relationship from the support of few solutions @xcite .",
    "our algorithm is tailored for solving the problem ( [ problem5 ] ) .",
    "let the objective function in ( [ problem5 ] ) as @xmath87 where @xmath88 with the approximation term @xmath89_r\\|_f^2 $ ] , the regularized penalty term @xmath90 and three indicator functions @xmath91 , @xmath92 , @xmath93 .",
    "the function @xmath94 is a real polynomial function on @xmath95 and the function @xmath96 is a non - differential continuous function on @xmath12 . since @xmath97 are closed sets , indicator functions @xmath91 , @xmath92 and @xmath93 are proper and lower semicontinuous .",
    "moreover , since @xmath91 , @xmath92 and @xmath93 are three semi - algebraic functions , thus the objective function is also a semi - algebraic function .",
    "so it is a kurdyka - ojasiewicz ( kl ) function @xcite . for a point @xmath98 ,",
    "if its ( limiting ) subdifferential @xcite , denoted by @xmath99 , contains @xmath100 , we call it a critical point of @xmath101 . the set of critical points of @xmath101 is denoted by @xmath102 . due to ill - posedness @xcite of the best low - rank approximation of tensors , it is known that the problem of finding a best rank - r approximation for tensors of order 3 or higher has no solution in general .",
    "however , after introducing the @xmath0 penalty term @xmath103 to the low - rank approximation term @xmath94 , it is always attainable for the minimization of the objective function in ( [ problem5 ] )",
    ". thus we have the following theorem to show the existence of the global optimal solution of problem ( [ problem5 ] ) .",
    "[ theorem2 ] the global optimal solution of problem ( [ problem5 ] ) exists .    for any tensor @xmath49 , the objective function @xmath104_r\\|^2_f+\\lambda\\|\\bm{\\alpha}\\|_1+\\delta_{s_1}(\\mathbf{x})+\\delta_{s_2}(\\mathbf{y})+\\delta_{s_3}(\\mathbf{z})$ ]",
    "is denoted as @xmath105 .",
    "notice that all the columns of @xmath77 in problem ( [ problem5 ] ) are constrained to have length one .",
    "we define the @xmath106dimensional unit sphere as @xmath107 , and a set @xmath108 .",
    "since this function @xmath101 is continuous on @xmath109 , we only need to show that there is a point @xmath110 such that @xmath111 , i.e. , the minimization of low - rank approximation with @xmath0 penalty is attainable .    for a scalar @xmath112",
    ", we will show that the level set @xmath113 is compact . since @xmath101 is continuous on @xmath109 , the set @xmath114 is closed and we only need to prove that @xmath114 is bounded . actually , it is guaranteed by the @xmath0 penalty term @xmath103 of @xmath101 .",
    "otherwise , unbounded points will take the penalty term go to infinity contrary to the boundedness of @xmath101 on @xmath114 . from the compactness of the level set @xmath114 ,",
    "the infimum @xmath115 is attainable because @xmath101 is continuous on @xmath114 .",
    "furthermore , since @xmath116 , there exists a point @xmath110 such that @xmath111 .",
    "in this section , we first describe an algorithm ( lrat ) of low - rank approximation of tensor for computing the solution of problem ( [ problem5 ] ) , and then show some theoretical guarantees on the convergence of lrat : ( 1 ) the sequence @xmath117 generated by lrat converges to a critical point of @xmath101 .",
    "( 2 ) the limit point of @xmath117 is a kkt point of problem ( [ problem4 ] ) .",
    "a third order tensor @xmath33 , an upper bound @xmath54 of @xmath55 , a penalty parameter @xmath30 and a scale @xmath118 ; + an approximated tensor @xmath119 with an estimated rank @xmath120 ; + give an initial tensor @xmath121_r$ ] .",
    "update step : + b. update matrices @xmath77 : + compute @xmath122 by ( [ update ] ) and let @xmath123 . + compute @xmath124 and @xmath125 by @xmath126 where @xmath127 is the @xmath43-th column of @xmath124 for @xmath128 .",
    "+ compute @xmath129 by ( [ update ] ) and let @xmath130 . + compute @xmath131 and @xmath132 by @xmath133 where @xmath134 is the @xmath43-th column of @xmath131 for @xmath128 . + compute @xmath135 by ( [ update ] ) and let @xmath136 . + compute @xmath137 and @xmath138 by @xmath139 where @xmath140 is the @xmath43-th column of @xmath137 for @xmath128 .",
    "+ c. update the row vector @xmath12 : + compute @xmath141 by ( [ update2 ] ) and let @xmath142 .",
    "+ compute @xmath143 by ( [ updatea ] ) and use the soft thresholding : @xmath144 denote the limitations by @xmath145 , compute @xmath146_r$ ] and count the number @xmath120 of nonzero entries in @xmath147 .",
    "+ the tensor @xmath119 with the estimated rank @xmath120 .    as in ( [ khatri1 ] ) , the matricizations of tensor @xmath148_r$ ] via khatri - rao products are @xmath149 where @xmath150 .",
    "we introduce the following three matrices for updating in the algorithm [ alg : framwork ] : @xmath151 it follows that @xmath152 , @xmath153 and @xmath154 .",
    "thus the function @xmath155 can be written in three equivalent forms : @xmath156 .",
    "furthermore , we have the gradients of @xmath94 on @xmath77 : @xmath157    using the vectorization of tensors @xcite , we can vectorize every rank - one tensor of outer product @xmath53 into a row vector @xmath158 for @xmath52 .",
    "we denote a matrix consisting of all @xmath158 for @xmath52 by @xmath159 thus the function @xmath155 can be also written as @xmath160 , where @xmath31 is a vectorization for tensor @xmath33 .",
    "furthermore , the gradient of @xmath94 on @xmath12 is @xmath161    our algorithm starts from @xmath162 and iteratively update variables @xmath77 and then @xmath12 in each loop .",
    "inspired by the equation ( [ updatetech ] ) , the update of @xmath163 is based on the following constraint optimization problem : @xmath164 where @xmath165 , @xmath123 and @xmath122 is computed from @xmath166 by ( [ update ] ) . this problem is equivalent to : @xmath167 where @xmath168 .",
    "so we obtain the update of @xmath169 : @xmath170 where @xmath171 and @xmath127 are the @xmath43-th columns of @xmath125 and @xmath124 .",
    "similarly , the update of @xmath172 is based on the following optimization problem : @xmath173 where @xmath174 , @xmath130 and @xmath129 is computed from @xmath175 by ( [ update ] ) .",
    "so we obtain the update of @xmath176 : @xmath177 where @xmath178 and @xmath134 are the @xmath43-th columns of @xmath132 and @xmath179 .",
    "the update of @xmath18 is based on the following constraint optimization problem : @xmath180 where @xmath63 , @xmath136 and @xmath135 is computed from @xmath181 by ( [ update ] ) .",
    "the update of @xmath182 is : @xmath183 where @xmath184 and @xmath185 are the @xmath43-th columns of @xmath138 and @xmath186 .",
    "finally , we consider to update @xmath12 by using the equation ( [ updatetech ] ) : @xmath187 where @xmath142 and @xmath141 can be computed from @xmath188 by ( [ update2 ] ) .",
    "this optimization problem is equivalent to : @xmath189    so we can obtain the update form for @xmath12 in algorithm [ alg : framwork ] by using the separate soft thresholding : @xmath190 where @xmath191    it should be noted that if we set @xmath192 , the lrat algorithm turns into a modified alternative least square method ( modals ) .",
    "this modified als algorithm uses linearized iterative technique @xcite to update variables in each step .",
    "although the regularization parameter @xmath30 is fixed in algorithm [ alg : framwork ] , we can adaptively choose it for practical computation , which will be shown in the section 5 .    in our algorithm",
    ", the computational complexity mainly comes from matrix multiplications .",
    "the update step ( 2b ) for updating @xmath12 in lrat algorithm require more cpu time than the update step ( 2a ) because of the large matrix dimension of @xmath193 .",
    "the complexity of our algorithm is @xmath194 , where @xmath195 is the total number of iteration .      in this subsection",
    ", we illustrate the convergence mechanism of the lrat algorithm , which is a rescaling version of the proximal alternating linear minimization algorithm @xcite .",
    "the following lemma [ gradcon ] points out that for the function @xmath196_r\\|^2_f$ ] , the gradient @xmath197 of @xmath198 is lipschitz continuous on bounded subsets and all the partial gradients of @xmath198 are globally lipschitz with modulus .",
    "[ gradcon ] let @xmath198 be the approximation term @xmath104_r\\|^2_f$ ] where @xmath199 .",
    "we have that the gradient function @xmath200 is lipschitz continuous on bounded subsets of @xmath201 , i.e. , for any bounded subset @xmath202 , there exists @xmath203 such that for any @xmath204 , @xmath205 moreover , for any fixed @xmath206 , there exist four constants @xmath207 such that : @xmath208 where @xmath209 .",
    "+    the proof has not been included since it relies on standard techniques . in our lrat algorithm , those lipschitz constants rely on the iterative number @xmath210 and have a lower bound @xmath211 .",
    "specifically , @xmath212 , @xmath213 , @xmath214 , @xmath142 .",
    "[ sufdec](sufficient decrease property @xcite ) let @xmath215 be a continuously differentiable with gradient @xmath200 assumed @xmath216-lipschitz continuous and let @xmath217 $ ] be a proper and lower semicontinuous function with @xmath218 . for any @xmath219 and @xmath220 ,",
    "define @xmath221 then we have that @xmath222    [ descent ] let @xmath101 be the objective function in problem ( [ problem5 ] ) .",
    "if @xmath223 and @xmath224 are generated by our lrat algorithm , we have that for any @xmath118 @xmath225    these four inequalities can be obtained by using lemma [ sufdec ] .",
    "the following lemma shows that the value of @xmath101 monotonically decreases on the sequence @xmath226 , which is generated by our algorithm .",
    "[ keylem ] let @xmath227 be the objective function @xmath228_r\\|^2_f+\\lambda\\|\\bm{\\alpha}\\|_1+\\delta_{s_1}(\\mathbf{x})+\\delta_{s_2}(\\mathbf{y})+\\delta_{s_3}(\\mathbf{z})\\ ] ] where @xmath199 , then + @xmath229 the sequence @xmath230 is nonincreasing and for any @xmath231 , there is a scalar @xmath232 such that @xmath233 .",
    "+ @xmath234@xmath235,@xmath236,@xmath237 and @xmath238 .",
    "+ @xmath239 the sequence @xmath240 is bounded . +    in our algorithm , all the lipschitz constants @xmath241 .",
    "so by lemma [ descent ] , @xmath242 where @xmath243 .",
    "we can obtain the first conclusion @xmath229 .",
    "the second conclusion @xmath234 holds from the first one because the sum @xmath244 is finite .",
    "if the sequence @xmath240 is unbounded , it means that @xmath245 is unbounded since columns of @xmath246 are constrained to have length one .",
    "so the sequence @xmath230 is unbounded since @xmath247 . from the conclusion @xmath229 ,",
    "@xmath248 is nonincreasing . since @xmath101 has a lower bound , the sequence @xmath249 is not unbounded .",
    "it is a contradiction .",
    "so the sequence @xmath240 must be bounded .",
    "furthermore , from lemma [ gradcon ] and the boundness shown in lemma [ keylem ] , we can obtain the following lipschitz upper bounds for subdifferentials .",
    "[ gradbound]let @xmath250 be the sequence generated by our lrat algorithm .",
    "there exist four positive scales @xmath251 and @xmath252 such that the following inequalities hold for any @xmath231 . + there is some @xmath253 such that @xmath254 . + there is some @xmath255 such that @xmath256 .",
    "+ there is some @xmath257 such that @xmath258 .",
    "+ there is some @xmath259 such that @xmath260 .    by the update of @xmath163 ,",
    "@xmath261 so we have that @xmath262 where @xmath263 .",
    "hence @xmath264 since @xmath265 , we have that @xmath266 by lemma [ gradcon ] and the boundness of @xmath240 , we have that there exists a constant @xmath267 such that @xmath254 .    similarly , we can choose @xmath268 and @xmath269 so @xmath270 and @xmath271 .",
    "furthermore , there exist constants @xmath272 and @xmath273 such that @xmath256 and @xmath258 .    by the update of @xmath12 , @xmath274 where @xmath275 and @xmath276 .",
    "denote @xmath277 as @xmath278 .",
    "thus we have that @xmath279 and @xmath280 we also get the last inequality by using lemma [ gradcon ] and the boundness of @xmath240 .",
    "the following theorem shows that the sequence of the lrat algorithm is convergent to a critical point of @xmath101 .",
    "[ limitpoint ] let @xmath240 be a sequence generated by the lrat algorithm from a starting point @xmath281 .",
    "then the sequence @xmath240 converges to a critical point @xmath282 of @xmath227 .    by lemma",
    "[ descent ] , the sufficient decrease property is satisfied that there is a constant @xmath232 such that for any @xmath231 @xmath283    by lemma [ gradbound ] , the iterates gap has a lower bound by the length of a vector in the subdifferential of @xmath284 .",
    "there is a constant @xmath285 and @xmath286 such that for any @xmath231 , @xmath287 where @xmath288 .",
    "furthermore , since @xmath101 is a kl function , we complete the proof by using theorem 3.1 in @xcite .",
    "a point @xmath199 is called as a kkt point of problem ( [ problem4 ] ) if there are three diagonal matrices @xmath289 and a vector @xmath290 such that @xmath291 in the following , we show that the limit point @xmath292 of the sequence @xmath240 is a kkt point of problem ( [ problem4 ] ) .",
    "let @xmath293 be the limit point of the sequence @xmath240 generated by the lrat algorithm .",
    "if @xmath294 and @xmath295 has full column rank , the limit point @xmath282 is a kkt point of problem ( [ problem4 ] ) .",
    "@xmath296 is obvious since @xmath297 and the convergency of @xmath240 . from ( [ gradona ] )",
    ", there exists a vector @xmath298 such that @xmath299 by the update of @xmath163 , there is a diagonal matrix @xmath300 such that @xmath301 by the convergency of @xmath240 , we have that @xmath300 is convergent to some diagonal matrix @xmath302 since @xmath303 has full column rank .",
    "furthermore , we can obtain @xmath304 similarly , we have @xmath305 this completes the proof of this corollary .",
    "in this section , we will discuss the probabilistic consistency of the sparsity of the optimal solution to problem ( [ problem4 ] ) .",
    "we will see that under a suitable choice on the regularization parameter , the optimal solution can recover the true sparsity in a statistical model with a high probability .    for a given regularization parameter @xmath306 , an optimal solution to problem ( [ problem4 ] )",
    "is denoted by @xmath307_r\\|_f^2+\\lambda\\|\\bm{\\alpha}\\|_1 \\quad\\mbox{s.t.}\\ \\mathbf{n(x , y , z)}=1.\\ ] ] as shown in section 4.1 , we can construct a @xmath308 matrix @xmath309 from ( [ update2 ] ) , and vectorize tensor @xmath33 into a row vector @xmath31 .    for convenience ,",
    "we introduce new variables : @xmath310 for @xmath311 respectively .",
    "thus @xmath312 and @xmath313 are column vectors with dimension @xmath314 and @xmath54 , and @xmath26 is a @xmath315 matrix .",
    "furthermore , we have the following equality @xmath316_r\\|_f^2+\\|\\bm{\\alpha}\\|_1 = \\frac{1}{2}\\|\\mathbf{b}-\\mathbf{b}\\bm{\\theta}\\|_2 ^ 2+\\lambda\\|\\bm{\\theta}\\|_1.\\ ] ] the optimal solution @xmath317 for tensor approximation problem ( [ problem4 ] ) is also an optimal solution @xmath318 of a standard @xmath0-regularized least square problem @xmath319 assume that @xmath312 and @xmath26 have a sparse representation structure as @xmath320 where all the columns of @xmath26 are normalized to one . the variable @xmath25 is a sparse signal with @xmath1 non - zero entries @xmath321 , and @xmath27 is a vector with independent subgaussian entries of mean zero and parameter @xmath28 .",
    "denote a subgradient vector in @xmath322 as @xmath323 .",
    "the entries of @xmath324 satisfy that for any @xmath325 , @xmath326 if @xmath327 and @xmath328 $ ] if @xmath329 . as shown in the lemma 1 of @xcite , @xmath318 is an optimal solution to problem ( [ objectarg ] ) if and only if there exists a subgradient vector @xmath330 such that @xmath331 if and only if there exists a subgradient vector @xmath330 such that @xmath332    assume that @xmath26 is a full column rank matrix .",
    "then the objective function in problem ( [ objectarg ] ) is strictly convex , and the optimal solution @xmath318 to problem ( [ objectarg ] ) is unique and exact @xmath317 . denote @xmath109 and @xmath333 as the index sets of non - zero entries in @xmath25 and @xmath318 respectively .",
    "so the sparse signal @xmath25 can be rewritten as @xmath334 and the cardinality of @xmath109 is @xmath1 .",
    "we will show in theorem [ probcon ] that the optimal solution @xmath318 , which is also the @xmath317 , of problem ( [ objectarg ] ) may become a suitable approximation for the real sparse signal @xmath25 .",
    "similar results shown in @xcite consider the case @xmath335 as @xmath336 or @xmath337 where @xmath210 is the number of rows in @xmath26 , while in this paper all the @xmath338 are normalized to one .",
    "we can further obtain a specific probability bound shown in theorem [ probcon ] , which relies only on two intrinsic parameters of model .    according to the unknown set @xmath109",
    ", we can separate columns of the design matrix @xmath26 as two parts @xmath339 , where @xmath340 is the complement of @xmath109 . moreover , since @xmath341 also have full column rank , there exists a unique solution @xmath342 by solving the restricted lasso problem : + @xmath343 if furthermore @xmath344 satisfies the equation ( [ lassocon ] ) , thus @xmath344 is the unique optimal solution @xmath318 to problem ( [ objectarg ] ) since @xmath26 has full column rank .",
    "moreover , we also obtain that the index set @xmath345 . from ( [ lassocon ] ) , if @xmath342 satisfies two equations : @xmath346 and @xmath347 where @xmath348 and @xmath349 , we have that @xmath350 satisfies the equation ( [ lassocon ] ) and @xmath351 .",
    "actually , since @xmath342 minimizes the problem ( [ locop ] ) , there exists @xmath348 such that the equation ( [ req1 ] ) holds .",
    "so if it happens with a high probability that the equation ( [ req2 ] ) holds and @xmath352 , thus the event @xmath353 happens with a high probability .",
    "furthermore , the event @xmath354 also happens with a high probability .",
    "we are going to show these in the the following part of this section .    from equations ( [ req1 ] ) and ( [ req2 ] )",
    ", we have that : @xmath355 @xmath356 for any @xmath357 , we have that @xmath358    we assume that there exists an incoherence parameter @xmath359 $ ] such that @xmath360 , where matrix norm @xmath361 . it is easy to obtain @xmath362 . and let us consider @xmath363 , where @xmath364 .",
    "thus @xmath365 is a subgaussian distribution with zero mean and parameter @xmath366 . since @xmath367 ,",
    "this parameter is no more than @xmath368 .",
    "so @xmath369 , where @xmath1 is the cardinality of @xmath109 . by choosing @xmath370",
    ", we have that @xmath371 .",
    "thus we have that @xmath372    now let us consider about the upper bound of @xmath373 : @xmath374 . since @xmath375 has a fixed value , we only need to consider the first term . for any @xmath376",
    ", we have that @xmath377 , where @xmath378 .",
    "if we assume that @xmath379 , thus @xmath380 is a subgaussian distribution with zero mean and parameter @xmath381 .",
    "thus @xmath382 . by choosing @xmath383",
    ", we have that + @xmath384    by combining ( [ inequ1 ] ) and ( [ inequ2 ] ) , we have the probability inequality @xmath385 .",
    "thus the probability inequality on the complementary set is that @xmath386 furthermore , we have that @xmath387 where @xmath388 @xmath389 .    from the above discussion , we obtain the following theorem [ probcon ] , which illustrates the probabilistic consistency of the optimal solution @xmath318 to problem ( [ objectarg ] ) .",
    "[ probcon ] suppose that the sparse structure ( [ sparsity ] ) exists , the sparse signal @xmath390 and @xmath26 has full column rank .",
    "if there exist some parameters @xmath29 and @xmath391 where @xmath392 and @xmath393 such that @xmath360 and @xmath379 , we have that @xmath394 where @xmath333 is the index set of non - zero entries in @xmath318 , and @xmath395 and @xmath342 is the optimal solution of ( [ locop ] ) . furthermore",
    ", if the lower bound of the absolute values of elements in @xmath396 is larger than @xmath397 , we have that @xmath398    in terms of ( [ orgconsist ] ) , the first inequality ( [ consist ] ) follows from @xmath399 , where @xmath400 .",
    "if @xmath401 and the lower bound of the absolute values of elements in @xmath396 is larger than @xmath402 , it can be checked that the entries in @xmath342 and @xmath396 of the same index have the same sign . from ( [ orgconsist ] ) , we can obtain the second inequality ( [ consist2 ] ) .",
    "theorem [ probcon ] tells us that if we want to recover the sparsity in ( [ sparsity ] ) with a probability @xmath403 , we should choose a @xmath30 such that @xmath404 when we know the intrinsic parameters @xmath29 and @xmath28 .",
    "so to adaptively give a regularization parameter @xmath30 based on the data @xmath33 , we need to give two guesses on the intrinsic parameters @xmath29 and @xmath28 .",
    "we set @xmath30 to zero in the algorithm [ alg : framwork ] , and compute a estimated tensor @xmath146_r$ ] from the tensor data @xmath33 .",
    "the parameter @xmath28 is estimated by using the variance @xmath405 of all the entries in the difference @xmath406 , and the parameter @xmath29 is set as @xmath407 , where @xmath408 is the @xmath43-th column in @xmath409 .",
    "with regularization parameter @xmath410 , the result of our algorithm is shown by using the simulated and real data in the next section .",
    "in this section , we have four types of numerical experiments for testing the performance of our algorithm . the codes of the first three experiments are written in matlab with simulated data . in all the simulations ,",
    "the initial guesses are randomly generated .",
    "the stopping criterion used in the all experiments depends on two parameters : one is the upper bound of the number of iteration iteration number ( eg .",
    "iter_max@xmath411 ) , and the other is a tolerance to decide whether convergence has been reached ( eg .",
    "conv_tol@xmath412 ) . the fourth numerical experiment is executed in c++ with opencv for surveillance video data .",
    "these experiments ran on a laptop computer with intel i5 cpu 3.3ghz and 8 g memory .",
    "we randomly create a tensor @xmath413 with @xmath414 rank - one components , and then use lrat to estimate the rank of @xmath33 along with the increment of the regularization parameter .",
    "the upper bound @xmath54 of @xmath55 is fixed to @xmath415 in the algorithm while the regularization parameter @xmath30 varies from @xmath416 to @xmath417 by step @xmath418 . as shown in figure [ fig.ex1 ] , the estimated rank @xmath120 has a decreasing trend as the parameter @xmath30 increases for these particular random tensor examples .",
    "heuristically , the reason for this trend lies is in the minimization the objective function in ( [ problem4 ] ) , an increase in @xmath30 reduces the value of @xmath419 and thus the estimated rank @xmath120 .",
    "re10.eps ( 95,7)@xmath30 ( 10,72)@xmath120      we randomly generate three kinds of tensors with various dimensions and various rank - one component numbers ( cn ) .",
    "the estimated rank @xmath120 is calculated with the regularization parameter @xmath410 , where @xmath405 and @xmath420 are computed as discussed in section 5 .",
    "table [ tab:1 ] shows the mean and standard deviation of the estimated rank .",
    ".mean and standard deviation of the estimated rank @xmath120 .",
    "[ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]     for each component number @xmath421 , we randomly generate @xmath422 tensors in @xmath423 with @xmath424 , and then use the lrat with the upper bound @xmath425 to compute the estimated rank @xmath120 . as shown in table",
    "[ tab:1 ] , when the rank - one component number @xmath426 , the average estimation difference of @xmath427 is @xmath428 and the standard deviation of @xmath120 is @xmath429 .",
    "similarly , for each component number @xmath430 , we randomly generate @xmath422 tensors in @xmath431 and for @xmath432 , we randomly generate @xmath422 tensors in @xmath433 .",
    "the upper bound @xmath54 is set to @xmath434 .",
    "the mean and standard deviation of @xmath120 are shown in the last two columns of table [ tab:1 ] .      in this subsection",
    ", we show the comparison between lrat and modals @xcite on a toy model .",
    "a tensor @xmath33 in @xmath423 is randomly generated with 3 rank - one components .",
    "based on the residual function , the modals algorithm approximates tensor @xmath33 by a tensor of five rank - one components , while the lrat algorithm solves ( [ problem4 ] ) and obtain an estimate on the rank of tensor @xmath33 .",
    "figure [ fig : comparison ] ( a ) demonstrates the residual function @xmath435_r\\|_f^2 $ ] for the modals and lrat algorithms . compared to the lrat , the modals executed by using five rank - one components decreases monotonically and has a low misfit on the residual function .",
    "figure [ fig : comparison ] ( b ) shows the objective function @xmath436_r\\|_f^2+\\hat{\\lambda}\\|\\hat{\\bm{\\alpha}}\\|_1 $ ] in ( [ problem4 ] ) for the lrat algorithm .",
    "it decreases monotonically and provides an estimate on the number of rank - one components as shown in section 6.2 , though the lrat gave one less digit accuracy in the residual .",
    "grayscale video data is a natural candidate for third - order tensors . due to the correlation between subsequent frames of video",
    ", there exists some potential low - rank mechanism in the data . in this subsection",
    ", we apply the lrat and the modals to two surveillance videos on fountain and lobby . for each video of @xmath437 consecutive frames , we choose a region of interest with a resolution @xmath438 .",
    "figure [ fig : modals ] demonstrates simulation results on the lrat and the modals . here",
    "the upper bound @xmath54 is fixed to @xmath439 .",
    "figure [ fig : modals ] shows 20 frames in the original video data @xmath33 and those frames estimated by the lrat and the modals .",
    "the modals provides an approximation with three factor matrices of @xmath440 elements . for the lrat algorithm ,",
    "the regularization parameter @xmath441 is set to @xmath442 , where @xmath405 and @xmath420 are computed as discussed in section 5 .",
    "the estimated number of rank - one components in @xmath76 is @xmath443 for the fountain video .",
    "the representation of @xmath76 with three factor matrices only needs @xmath444 elements .",
    "the estimated number of rank - one components is @xmath445 for the lobby video , and the representation with three factor matrices needs @xmath446 elements .",
    "compared to the modals algorithm , the lrat has a smaller estimated rank but it sacrifices more cpu time , because the lrat algorithm requires an instructive ( a starter ) choice on @xmath441 . for this case , we used the modals algorithm to obtain a starter choice @xmath441 for lrat .",
    "we propose an algorithm based on the proximal alternating minimization to detect the rank of tensors .",
    "this algorithm comes from the understanding of the low - rank approximation of tensors from sparse optimization .",
    "we also provide some theoretical guarantees on the convergence of this algorithm and a probabilistic consistency of the approximation result .",
    "moreover , we suggest a way to choose a regularization parameter for practical computation .",
    "the simulation studies suggested that our algorithm can be used to detect the number of rank - one components in tensors .",
    "the work presented in this paper have potential applications and extensions , especially in video processing and latent component number estimation .",
    "the ongoing work is to apply this low - rank approximation method to moving object detection and video data compression .",
    "this work is in part supported by the national natural foundation of china 11401092 ."
  ],
  "abstract_text": [
    "<S> the goal of this paper is to find a low - rank approximation for a given tensor . </S>",
    "<S> specifically , we give a computable strategy on calculating the rank of a given tensor , based on approximating the solution to an np - hard problem . in this paper </S>",
    "<S> , we formulate a sparse optimization problem via an @xmath0-regularization to find a low - rank approximation of tensors . to solve this sparse optimization problem </S>",
    "<S> , we propose a rescaling algorithm of the proximal alternating minimization and study the theoretical convergence of this algorithm . </S>",
    "<S> furthermore , we discuss the probabilistic consistency of the sparsity result and suggest a way to choose the regularization parameter for practical computation . in the simulation experiments , </S>",
    "<S> the performance of our algorithm supports that our method provides an efficient estimate on the number of rank - one tensor components in a given tensor . </S>",
    "<S> moreover , this algorithm is also applied to surveillance videos for low - rank approximation .    </S>",
    "<S> @xmath0-regularization , low - rank approximation , proximal alternating minimization , sparsity    15a69 , 65f30 </S>"
  ]
}