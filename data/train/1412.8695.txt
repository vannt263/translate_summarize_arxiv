{
  "article_text": [
    "state - space models , also known as hidden markov models , are a very popular class of time series models that have found numerous of applications in fields as diverse as statistics , ecology , econometrics , engineering and environmental sciences ; see @xcite , @xcite , @xcite , @xcite .",
    "formally a state - space model is defined by two stochastic processes @xmath0 and @xmath1 .",
    "the process @xmath0 is a @xmath2-valued latent markov process of initial density @xmath3 and markov transition density @xmath4 , that is @xmath5 whereas the @xmath6-valued observations @xmath1 satisfy @xmath7 where @xmath8 denotes the conditional marginal density ,",
    "@xmath9 the parameter of the model and @xmath10 denotes components @xmath11 of a sequence @xmath12 .",
    "the spaces @xmath2 and @xmath6 can be euclidean but what follows applies to more general state spaces as well .",
    "the popularity of state - space models stems from the fact that they are flexible and easily interpretable .",
    "applications of state - space models include stochastic volatility models where @xmath13 is the volatility of an asset and @xmath14 its observed log - return @xcite , biochemical network models where @xmath13 corresponds to the population of various biochemical species and @xmath14 are imprecise measurements of the size of a subset of these species @xcite , neuroscience models where @xmath13 is a state vector determining the neuron s stimulus - response function and @xmath14 some spike train data @xcite . however , nonlinear non - gaussian state - space models are also notoriously difficult to fit to data and it is only recently , thanks to the advent of powerful simulation techniques , that it has been possible to fully realize their potential .    to illustrate the complexity of inference in state - space models , consider first the scenario where the parameter @xmath15 is _",
    "known_. on - line and off - line inference about the state process @xmath16 given the observations @xmath17 is only feasible analytically for simple models such as the linear gaussian state - space model .",
    "in nonlinear non - gaussian scenarios , numerous approximation schemes such as the extended kalman filter or the gaussian sum filter @xcite have been proposed over the past fifty years to solve these so - called optimal filtering and smoothing problems , but these methods lack rigor and can be unreliable in practice in terms of accuracy , while deterministic integration methods are difficult to implement .",
    "markov chain monte carlo ( mcmc ) methods can obviously be used but they are impractical for on - line inference ; and even for off - line inference it can be difficult to build efficient high - dimensional proposal distributions for such algorithms . for nonlinear non - gaussian state space models _ particle algorithms _ have emerged as the most successful .",
    "their widespread popularity is due to the fact that they are easy to implement , suitable for parallel implementation @xcite and , more importantly , have been demonstrated in numerous settings to yield more accurate estimates than the standard alternatives ; e.g. see @xcite , @xcite , @xcite , @xcite",
    ".    in most practical situations , the model ( [ equ : statespacesystemlitrev1])-([equ : statespacesystemlitrev-2 ] ) depends on an _ _  unknown _ _",
    "parameter vector @xmath15 that needs to be inferred from the data either in an on - line or off - line manner . in fact inferring the parameter @xmath15 is often the primary problem of interest ; e.g. for biochemical networks , we are not interested in the population of the species per se , but we want to infer some chemical rate constants , which are parameters of the transition prior @xmath4 . although it is possible to define an extended state that includes the original state @xmath13 and the parameter @xmath15 and then apply standard particle methods to perform parameter inference , it was recognized very early on that this naive approach is problematic @xcite due to the parameter space not being explored adequately .",
    "this has motivated over the past fifteen years the development of many particle methods for the parameter estimation problem , but numerically robust methods have only been proposed recently . the main objective of this paper is to provide a comprehensive overview of this literature .",
    "this paper thus differs from recent survey papers on particle methods which all primarily focus on estimating the state sequence @xmath18 or discuss a much wider range of topics , e.g. @xcite , @xcite , @xcite , @xcite .",
    "we will present the main features of each method and comment on their pros and cons .",
    "no attempt however is made to discuss the intricacies of the specific implementations .",
    "for this we refer the reader to the original references .",
    "we have chosen to broadly classify the methods as follows : bayesian or maximum likelihood ( ml ) and whether they are implemented off - line or on - line . in the bayesian approach ,",
    "the unknown parameter is assigned a prior distribution and the posterior density of this parameter given the observations is to be characterized . in the ml  approach ,",
    "the parameter estimate is the maximizing argument of the likelihood of @xmath15 given the data .",
    "both these inference procedures can be carried out off - line or on - line .",
    "specifically , in an off - line framework we infer @xmath15 using a fixed observation record @xmath19 .",
    "in contrast , on - line methods update the parameter estimate sequentially as observations @xmath20 become available .",
    "the rest of the paper is organized as follows . in section [ sec : computationalissues ] , we present the main computational challenges associated to parameter inference in state - space models . in section  [ sec : filteringandparticle ] , we review particle  methods for filtering when the model does not include any unknown parameters whereas section [ sec : smoothingandparticle ] is dedicated to smoothing . these filtering and smoothing techniques are at the core of the off - line and on - line ml parameter procedures described in section [ sec : mlestimation ] . in section [ sec : bayesianestimation ] , we discuss particle methods for off - line and on - line bayesian parameter inference .",
    "the performance of some of these algorithms are illustrated on simple examples in section [ sec : experimentalresults ] .",
    "finally , we summarize the main advantages and drawbacks of the methods presented and discuss some open problems in section [ sec : conclusion ] .",
    "a key ingredient of ml and bayesian parameter inference is the likelihood function @xmath21 of @xmath15 which satisfies @xmath22 where @xmath23 denotes the joint density of @xmath24 which is given from equations ( [ equ : statespacesystemlitrev1])-([equ : statespacesystemlitrev-2 ] ) by @xmath25 the likelihood function is also the normalizing constant of the posterior density @xmath26 of the latent states @xmath18 given data @xmath27 @xmath28 this posterior density is itself useful for computing the score vector @xmath29 associated to the log - likelihood @xmath30 , as fisher s identity yields @xmath31    the main practical issue associated to parameter inference in nonlinear non - gaussian state - space models is that the likelihood function is intractable . as performing ml",
    "parameter inference requires maximizing this intractable function , it means practically that it is necessary to obtain reasonably low variance monte carlo estimates of it , or of the associated score vector if this maximisation is carried out using gradient - based methods .",
    "both tasks involve approximating high dimensional integrals , and , whenever @xmath32 is large . on - line",
    "inference requires additionally these integrals to be approximated on the fly , ruling out the applications of standard computational tools such as mcmc .",
    "bayesian parameter inference is even more challenging , as it requires approximating the posterior density @xmath33 where @xmath34 is the prior density . here",
    "not only @xmath35 but also @xmath36 are intractable and , once more , these integrals must be approximated on - line if one wants to update the posterior density sequentially .",
    "we will show in this review that particle methods are particularly well - suited to these integration tasks .",
    "in this section , the parameter @xmath15 is assumed known and we focus on the problem of estimating the latent process @xmath0 sequentially given the observations .",
    "an important byproduct of this so - called filtering task from a parameter estimation viewpoint is that it provides us with an on - line scheme to compute @xmath37 . as outlined in section [ sec : computationalissues ] , the particle approximation of these likelihood terms is a key ingredient of numerous particle - based parameter inference techniques discussed further on .",
    "filtering denotes usually the task of estimating recursively in time the sequence of marginal posteriors @xmath38 , known as the filtering densities .",
    "however we will adopt here a more general definition and will refer to filtering as the task of estimating the sequence of joint posteriors @xmath39 recursively in time but we will still refer to the marginals @xmath38 as the filtering densities .",
    "it is easy to verify from and that the posterior @xmath26 and the likelihood @xmath21 satisfy the following fundamental recursions : for @xmath40 , @xmath41 and @xmath42 where @xmath43    there are essentially two classes of models for which @xmath26 and @xmath21 can be computed exactly : the class of linear gaussian models , for which the above recursions may be implemented using kalman techniques , and when @xmath2 is a finite state - space ; see for example @xcite . for other models",
    "these quantities are typically intractable , i.e. the densities in - can not be computed exactly .",
    "particle filtering methods are a set of simulation - based techniques which approximate numerically the recursions to .",
    "we focus here on the apf ( auxiliary particle filter @xcite ) for two reasons : first , this is a popular approach , in particular in the context of parameter estimation ( see e.g. section [ sub : using - mcmcsteps - within - smc ] ) ; second , the apf covers as special cases a large class of particle algorithms , such as the bootstrap filter @xcite and sisr ( sequential importance sampling resampling @xcite , @xcite ) .",
    "let @xmath44 where @xmath45 is a probability density function which is easy to sample from , and @xmath46 is not necessarily required to be a probability density function but just an non - negative function of @xmath47 one can evaluate .",
    "( for @xmath48 , remove the dependency on @xmath49 , i.e. @xmath50 . )    the algorithm relies on the following importance weights @xmath51 in order to alleviate the notational burden we omit the dependence of the importance weights on @xmath15 ; we will do so in the remainder of the paper when no confusion is possible . the auxiliary particle filter can be summarized as follows .    * _ at time _",
    "@xmath48 , for all @xmath52 : 1 .",
    "@xmath53 2 .",
    "@xmath54 @xmath55 3 .   @xmath56 . *",
    "_ at time _",
    "@xmath40 , for all @xmath52 : 1 .",
    "@xmath57 @xmath58 2 .",
    "@xmath59 @xmath60 3 .",
    "@xmath61    one recovers the sisr algorithm as a special case of algorithm [ alg : apf ] by taking @xmath62 ( or more generally , by taking @xmath63 , some arbitrary positive function ) .",
    "further , one recovers the bootstrap filter by taking @xmath64 .",
    "this is an important special case , as some complex models are such that one may sample from @xmath65 , but not compute the corresponding density ; in such a case the bootstrap filter is the only implementable algorithm . for models such that the density @xmath65 is tractable , @xcite recommend selecting @xmath66 and @xmath67 when these quantities are tractable , and using approximations of these quantities in scenarios when they are not .",
    "the intuition for these recommendations is that this should make the weight function nearly constant .",
    "the computational complexity of algorithm [ alg : apf ] is @xmath68 per time step ; in particular see e.g. @xcite for a @xmath68 implementation of the resampling step . at time @xmath32 , the approximations of @xmath69 and @xmath70 presented earlier in and respectively are given by @xmath71 where @xmath72 and @xmath73 in practice , one uses mostly to obtain approximations of posterior moments @xmath74\\ ] ] but expressing particle filtering as a method for approximating distributions ( rather than moments ) turns out to be a more convenient formalization .",
    "the likelihood is then estimated through @xmath75 the resampling procedure is introduced to replicate particles with high weights and discard particles with low weights . it serves to focus the computational efforts on the  promising  regions of the state - space .",
    "we have presented above the simplest resampling scheme .",
    "lower variance resampling schemes have been proposed in @xcite , @xcite , as well as more advanced particle algorithms with better overall performance , e.g. the resample - move algorithm @xcite . for the sake of simplicity ,",
    "we have also presented a version of the algorithm that operates resampling at every iteration @xmath32 .",
    "it may be more efficient to trigger resampling only when a certain criterion regarding the degeneracy of the weights is met ; see @xcite , ( * ? ? ?",
    "* and 74 ) .",
    "many sharp convergence results are available for particle methods @xcite .",
    "a selection of these results that gives useful insights on the difficulties of estimating static parameters with particle methods is presented below .    under minor regularity assumptions",
    ", one can show that for any @xmath76 , @xmath77 and any bounded test function @xmath78 $ ] , there exist constants @xmath79 such that for any @xmath80 @xmath81\\leq\\frac{a_{\\theta , n , p}}{n^{p/2}}\\label{eq : weakresult1}\\ ] ] where the expectation is with respect to the law of the particle filter .",
    "in addition , for more general classes of functions , we can obtain for any fixed @xmath32 a central limit theorem ( clt ) as @xmath82 @xcite , ( * ? ? ?",
    "* proposition 9.4.2 ) .",
    "such results are reassuring but weak as they reveal nothing regarding long time behavior .",
    "for instance , without further restrictions on the class of functions @xmath83 and the state - space model , @xmath84 typically grows exponentially with @xmath32 .",
    "this is intuitively not surprising as the dimension of the target density @xmath69 is increasing with @xmath32 .",
    "moreover the successive resampling steps lead to a depletion of the particle population ; @xmath85 will eventually be approximated by a single unique particle as @xmath86 increases .",
    "this is referred to as the _ degeneracy _ problem in the literature ( * ? ? ?",
    "* figure 8.4 , p. 282 ) .",
    "this is a fundamental weakness of particle methods : given a fixed number of particles @xmath87 , it is impossible to approximate @xmath26 accurately when @xmath32 is large enough .",
    "fortunately , it is also possible to establish much more positive results .",
    "many state - space models possess the so - called _ exponential forgetting _ property ( * ? ? ?",
    "* chapter 4 ) .",
    "this property states that for any @xmath88 and data @xmath27 , there exist constants @xmath89 and @xmath90 such that @xmath91 where @xmath92 is the total variation distance ; that is the optimal filter forgets exponentially fast its initial condition .",
    "this property is typically satisfied when the signal process @xmath93 is a uniformly ergodic markov chain and the observations @xmath94 are not too informative ( * ? ? ? * chapter 4 ) or when @xmath94 are informative enough that it effectively restricts the hidden state to a bounded region around it @xcite .",
    "weaker conditions can be found in @xcite .",
    "when exponential forgetting holds , it is possible to establish much stronger uniform - in - time convergence results _ for functions @xmath83 that depend only on recent states_. specifically , for an integer @xmath95 and any bounded test function @xmath96 $ ] , there exist constants @xmath97 such that for any @xmath80 , @xmath98 , @xmath99\\leq\\frac{c_{\\theta , l , p}}{n^{p/2}},\\label{eq : lp}\\ ] ] where @xmath100 this result explains why particle filtering is an effective computational tool in many applications such as tracking , where one is only interested in @xmath101 , as the approximation error is uniformly bounded over time .",
    "similar positive results holds for @xmath102 .",
    "this estimate is unbiased for any @xmath103 ( * ? ? ?",
    "* theorem 7.4.2 , page 239 ) and , under assumption , the _ relative _ variance of the likelihood estimate @xmath102 , that is the variance of the ratio @xmath104 , is bounded above by @xmath105 @xcite , @xcite .",
    "this is a great improvement over the exponential increase with @xmath32 that holds for standard importance sampling techniques , see for instance @xcite .",
    "however , the constants @xmath106 and @xmath107 are typically exponential in @xmath108 , the dimension of the state vector @xmath13 . we note that non - standard particle methods designed to minimize the variance of the estimate of @xmath21 have recently been proposed @xcite .",
    "finally we recall the theoretical properties of particles estimates of the following so - called smoothed additive functional ( * ? ? ?",
    "* section 8.3 ) , @xcite @xmath109 such quantities are critical when implementing ml  parameter estimation procedures ; see section [ sec : mlestimation ] .",
    "if we substitute @xmath110 to @xmath111 to approximate @xmath112 , then we obtain an estimate @xmath113 which can be computed recursively in time , see e.g. ( * ? ? ?",
    "* section 8.3 ) . for the remainder of this paper",
    "we will refer to this approximation as the _ path space _ approximation .",
    "even when holds , there exists @xmath114 such that the asymptotic bias @xcite and variance @xcite satisfy @xmath115 for @xmath116 $ ] where the variance is w.r.t the law of the particle filter . the fact that the variance grows at least quadratically in time follows from the degeneracy problem and makes @xmath113 unsuitable for some on - line likelihood based parameter estimation schemes discussed in section [ sec : mlestimation ] .",
    "in this section , the parameter @xmath15 is still assumed known and we focus on smoothing , that is the problem of estimating the latent variables @xmath117 given a fixed batch of observations @xmath19 . smoothing for a fixed parameter @xmath15 is at the core of the two main particle ml parameter inference techniques described in section [ sec : mlestimation ] as these procedures require computing smoothed additive functionals of the form .",
    "clearly one could unfold the recursion ( [ eq : jointupdate ] ) from @xmath48 to @xmath118 to obtain @xmath119 .",
    "however , as pointed out in the previous section , the path space approximation ( [ eq : smcfullposterior ] ) suffers from the degeneracy problem and yields potentially high variance estimates of ( [ eq : expectationadditivefunctionals ] ) as ( [ eq : sufficientstatsdegrade ] ) holds .",
    "this has motivated the development of alternative particle approaches to approximate @xmath119 and its marginals .      for state - space models with",
    " good forgetting properties ( e.g. ( [ eq : ergod ] ) ) , we have @xmath120 for @xmath121 large enough ; that is observations collected at times @xmath122 do not bring any significant additional information about @xmath18 . in particular , when having to evaluate @xmath123 of the form we can approximate the expectation of @xmath124 w.r.t @xmath125 by its expectation w.r.t @xmath126 .",
    "algorithmically , a particle implementation of means not resampling the components @xmath127 of the particles @xmath128 obtained by particle filtering at times @xmath122 .",
    "this was first suggested in @xcite and used in @xcite , @xcite .",
    "this algorithm is simple to implement but the main practical problem is the choice of @xmath121 .",
    "if taken too small , then @xmath129 is a poor approximation of @xmath130 .",
    "if taken too large , the degeneracy remains substantial .",
    "moreover , even as @xmath131 , this particle  approximation will have a non - vanishing bias since @xmath132 .",
    "the joint smoothing density @xmath119 can be expressed as a function of the filtering densities @xmath133 using the following key decomposition @xmath134 where @xmath135 is a backward ( in time ) markov transition density given by @xmath136 a backward in time recursion for @xmath137 follows by integrating out @xmath138 and @xmath139 in while applying @xmath140 this is referred to as forward - backward smoothing as a forward pass yields @xmath133 which can be used in a backward pass to obtain @xmath137 .",
    "combined to @xmath141 , this allows us to obtain @xmath123 .",
    "an alternative to these forward - backward procedures is the generalized two - filter formula @xcite .",
    "the decomposition suggests that it is possible to sample approximately from @xmath119 by running a particle filter from time @xmath48 to @xmath142 storing the approximate filtering distributions @xmath143 , i.e. the marginals of , then sampling @xmath144 and for @xmath145 sampling @xmath146 where this distribution is obtained by substituting @xmath147 for @xmath148 in : @xmath149 this forward filtering backward sampling ( ffbsa ) procedure was proposed in @xcite .",
    "it requires @xmath150 operations to generate a single path @xmath117 as sampling from costs @xmath151 operations .",
    "however , as noted in @xcite , it is possible to sample using rejection from an alternative approximation of @xmath152 in @xmath153 operations if we use an unweighted particle approximation of @xmath154 in and if the transition prior satisfies @xmath155 . hence , with this approach , sampling a path @xmath117 costs only on average @xmath156 operations .",
    "a related rejection technique was proposed in @xcite . in practice",
    ", one may generate @xmath87 such trajectories to compute monte carlo averages that approximates smoothing expectations @xmath157 $ ] . in that scenario ,",
    "the first approach costs @xmath158 , while the second approach costs @xmath150 on average . in some applications ,",
    "the rejection sampling procedure can be computationally costly as the acceptance probability can be very small for some particles ; see for example section 4.3 in @xcite for empirical results .",
    "this has motivated the development of hybrid procedures combining ffbsa and rejection sampling @xcite .",
    "we can also directly approximate the marginals @xmath137 .",
    "assuming we have an approximation @xmath159 where @xmath160 then by using and , we obtain the approximation @xmath161 with @xmath162 this forward filtering backward smoothing ( ffbsm , where ` m ' stands for ` marginal ' ) procedure requires @xmath158 operations to approximate @xmath137 instead of @xmath150 for the path space and fixed - lag methods .",
    "however this high computational complexity of forward - backward estimates can be reduced using fast computational methods @xcite .",
    "particle approximations of generalized two - filter smoothing procedures have also been proposed in @xcite , @xcite .",
    "whenever we are interested in computing the sequence @xmath163 recursively in time , the forward - backward procedure described above is cumbersome as it requires performing a new backward pass with @xmath164 steps at time @xmath165 an important but not well - known result is that it is possible to implement exactly the forward - backward procedure using only a forward procedure .",
    "this result is at the core of @xcite but its exposition relies on tools which are non - standard for statisticians .",
    "we follow here the simpler derivation proposed in @xcite which simply consists of rewriting as @xmath166 where @xmath167 it can be easily checked using that @xmath168 satisfies the following forward recursion for @xmath76 @xmath169 with @xmath170 and where @xmath135 is given by . in practice , we shall approximate the function @xmath171 on a certain grid of values @xmath172 , as explained in the next section .",
    "we can easily provide a particle approximation of the forward smoothing recursion .",
    "assume you have access to approximations @xmath173 of @xmath174 at time @xmath32 , where @xmath175 then when updating our particle filter to obtain @xmath176 we can directly compute the particle approximations @xmath177 by plugging and @xmath147 in ( [ eq : additivesmoothfunctionalsasfunctionoft])-([eq : recursionadditivefunctionalvn ] ) to obtain @xmath178 this approach requires @xmath179 operations to compute @xmath113 at iteration @xmath32 .",
    "a variation over this idea recently proposed in @xcite and @xcite consists of approximating @xmath180 by sampling @xmath181 for @xmath182 to obtain @xmath183 when it is possible to sample from @xmath184 in @xmath153 operations using rejection sampling , provides a monte carlo approximation to of overall complexity @xmath185 .",
    "empirically , for a fixed number of particles , these smoothing procedures perform significantly much better than the naive path space approach to smoothing ( that is , simply propagating forward the complete state trajectory within a particle filtering algorithm ) .",
    "many theoretical results validating these empirical findings have been established under assumption ( [ eq : ergod ] ) and additional regularity assumptions .",
    "the particle  estimate of @xmath112 based on the fixed - lag approximation ( [ eq : forgetting ] ) has an asymptotic variance in @xmath186 with a non - vanishing ( as @xmath131 ) bias proportional to @xmath32 and a constant decreasing exponentially fast with @xmath121 @xcite . in @xcite , @xcite , @xcite",
    ", it is shown that when holds , there exists @xmath187 such that the asymptotic bias and variance of the particle  estimate of @xmath112 computed using the forward - backward procedures satisfy @xmath188 the bias for the path space and forward - backward estimators of @xmath112 are actually equal @xcite .",
    "recently , it has also been established in @xcite that , under similar regularity assumptions , the estimate also admits an asymptotic variance in @xmath186 whenever @xmath189 .",
    "we describe in this section how the particle filtering and smoothing techniques introduced in sections [ sec : filteringandparticle ] and [ sec : smoothingandparticle ] can be used to implement maximum likelihood parameter estimation techniques .",
    "we recall that @xmath190 denote the log - likelihood function associated to data @xmath19 introduced in section [ sec : computationalissues ] .",
    "so as to maximize @xmath191 one can rely on standard nonlinear optimization methods , e.g. using quasi - newton or gradient - ascent techniques .",
    "we will limit ourselves to these approaches even if they are sensitive to initialization and might get trapped in a local maximum .",
    "we have seen in section [ sec : filteringandparticle ] that @xmath190 can be approximated using particle methods , for any fixed @xmath9 .",
    "one may wish then to treat ml estimation as an optimization problem using monte carlo evaluations of @xmath190 . when optimizing a function calculated with a monte carlo error , a popular strategy is to make the evaluated function continuous by using common random numbers over different evaluations to ease the optimization .",
    "unfortunately , this strategy is not helpful in the particle context .",
    "indeed , in the resampling stage , particles @xmath192 are resampled according to the distribution @xmath193 which admits a piecewise constant and hence discontinuous cumulative distribution function ( cdf ) .",
    "a small change in @xmath15 will cause a small change in the importance weights @xmath194 and this will potentially generate a different set of resampled particles . as a result , the log - likelihood function estimate will not be continuous in @xmath15 even if @xmath190 is continuous .    to bypass this problem ,",
    "an importance sampling method was introduced in @xcite but it has computational complexity @xmath158 and only provides low variance estimates in the neighborhood of a suitably preselected parameter value . in the restricted scenario where @xmath195 , an elegant solution to the discontinuity problem was proposed in @xcite .",
    "the method uses common random numbers and introduces a  continuous version of the resampling step by finding a permutation @xmath196 such that @xmath197 and defining a piecewise linear approximation of the resulting cdf from which particles are resampled , i.e. @xmath198 this method requires @xmath199 operations due to the sorting of the particles but the resulting continuous estimate of @xmath190 can be maximized using standard optimization techniques .",
    "extensions to the multivariate case where @xmath200 ( with @xmath201 ) have been proposed in @xcite and @xcite",
    ". however , the scheme @xcite does not guarantee continuity of the likelihood function estimate and only provides log - likelihood estimates which are positively correlated for neighboring values in the parameter space whereas the scheme in @xcite has @xmath202 computational complexity and relies on a non - standard particle filtering scheme .",
    "when @xmath15 is high - dimensional , the optimization over the parameter space may be made more efficient if provided with estimates of the gradient .",
    "this is exploited by the algorithms described in the forthcoming sections .",
    "the log - likelihood @xmath190 may be maximized with the following steepest ascent algorithm : at iteration @xmath203 @xmath204 where @xmath205 is the gradient of @xmath206 w.r.t @xmath15 evaluated at @xmath207 and @xmath208 is a sequence of positive real numbers , called the step - size sequence .",
    "typically , @xmath209 is determined adaptively at iteration @xmath210 using a line search or the popular barzilai - borwein alternative .",
    "both schemes guarantee convergence to a local maximum under weak regularity assumptions ; see @xcite for a survey .",
    "the _ score _ vector @xmath211 can be computed by using fisher s identity given in .",
    "given , it is easy to check that the score is of the form .",
    "an alternative to fisher s identity to compute the score is presented in @xcite but this also requires computing an expectation of the form .",
    "these score estimation methods are not applicable in complex scenarios where it is possible to sample from @xmath4 but the analytical expression of this transition kernel is unavailable @xcite . for those models ,",
    "a naive approach is to use a finite difference estimate of the gradient ; however this might generate too high a variance estimate .",
    "an interesting alternative presented in @xcite , under the name of iterated filtering , consists of deriving an approximation of @xmath205 based on the posterior moments @xmath212 of an artificial state - space model with latent markov process @xmath213 , @xmath214 and observed process @xmath215 . here",
    "@xmath216 is a zero - mean white noise sequence with variance @xmath217 , @xmath218 , @xmath219 @xmath220 .",
    "it is shown in @xcite that this approximation improves as @xmath221 and @xmath222 .",
    "clearly as the variance @xmath223 of the artificial dynamic noise @xmath224 on the @xmath225component decreases , it will be necessary to use more particles to approximate @xmath226 as the mixing properties of the artificial dynamic model deteriorates",
    ".      gradient ascent algorithms can be numerically unstable as they require to scale carefully the components of the score vector .",
    "the expectation maximization ( em ) algorithm is a very popular alternative procedure for maximizing @xmath206  @xcite . at iteration @xmath203 , we set @xmath227 where @xmath228",
    "the sequence @xmath229 generated by this algorithm is non - decreasing .",
    "the em is usually favored by practitioners whenever it is applicable as it is numerically more stable than gradient techniques .    in terms of implementation ,   the em consists of computing a @xmath230-dimensional summary statistic of the form ( [ eq : expectationadditivefunctionals ] ) when @xmath231 belongs to the exponential family , and the maximizing argument of @xmath232  can be characterized explicitly through a suitable function @xmath233 , i.e. @xmath234      the path space approximation ( [ eq : smcfullposterior ] ) can be used to approximate the score and the summary statistics of the em algorithm at the computational cost of @xmath235 ; see @xcite , @xcite , @xcite .",
    "experimentally the variance of the associated estimates increases typically quadratically with @xmath236 @xcite . to obtain estimates whose variance increases only typically linearly with @xmath236 with similar computational cost",
    ", one can use the fixed - lag approximation presented in section [ sub : fixedlagapproxparticle ] or a more recent alternative where the path space method is used but the additive functional of interest , which is a sum of terms over @xmath237 , is approximated by a sum of similar terms which are now exponentially weighted w.r.t @xmath32 @xcite .",
    "these methods introduce a non - vanishing asymptotic bias difficult to quantify but appear to perform well in practice .    to improve over the path space method without introducing any such asymptotic bias , the ffbsm and forward smoothing discussed in sections [ sub : forward - backwardsmoothing ] and [ sub : forward - only - smoothing ] as well as the generalized two - filter smoother",
    "have been used @xcite , @xcite , @xcite , @xcite , @xcite .",
    "experimentally the variance of the associated estimates increases typically linearly with @xmath236 @xcite in agreement with the theoretical results in @xcite , @xcite , @xcite .",
    "however the computational complexity of these techniques is @xmath238 . for a fixed computational complexity of order @xmath239 ,",
    "an informal comparison of the performance of the path space estimate using @xmath240 particles and the forward - backward estimate using @xmath87 particles suggest that both estimates admit a mean square error ( mse ) of order @xmath241 , but the mse of the path space estimate is variance dominated whereas the forward - backward estimates are bias dominated .",
    "this can be understood by decomposing the mse as the sum of the squared bias and the variance and then substituting appropriately for @xmath240 particles in for the path space method and for @xmath87 particles in for the forward - backward estimates .",
    "we confirm experimentally this fact in section [ sub : maximum - likelihood - methods ] .",
    "these experimental results suggest that these particle smoothing estimates might thus of limited interest compared to the path based estimates for ml parameter inference when accounting for computational complexity .",
    "however , this comparison ignores that the @xmath242 computational complexity of these particle smoothing estimates can be reduced to @xmath151 by sampling approximately from @xmath243 with the ffbsa procedure in section [ sub : forward - backwardsmoothing ] or by using fast computational methods @xcite .",
    "related @xmath151 approaches have been developed for generalized two - filter smoothing @xcite , @xcite .",
    "when applicable , these fast computational methods should be favored .      for a long observation sequence",
    "the computation of the gradient of @xmath206 can be prohibitive , moreover we might have real - time constraints . an alternative would be a recursive procedure in which the data is run through once sequentially . if @xmath244 is the estimate of the model parameter after the first @xmath32 observations , a recursive method would update the estimate to @xmath245  after receiving the new data @xmath246 . several on - line variants of the ml procedures described earlier are now presented . for these methods to be justified , it is crucial for the observation process to be ergodic for the limiting averaged likelihood function @xmath247 to have a well - defined limit @xmath248 as @xmath249      an alternative to gradient ascent is the following parameter update scheme at time @xmath76 @xmath250 where the positive non - increasing step - size sequence @xmath251 satisfies @xmath252 and @xmath253 @xcite , @xcite ; e.g. @xmath254 for @xmath255 . upon receiving @xmath246 ,",
    "the parameter estimate is updated in the direction of ascent of the conditional density of this new observation . in other words",
    ", one recognizes in the update of the gradient ascent algorithm , except that the partial ( up to time @xmath32 ) likelihood is used .",
    "the algorithm in the present form is however not suitable for on - line implementation , because evaluating the gradient of @xmath256 at the current parameter estimate requires computing the filter from time @xmath257 to time @xmath32 using the current parameter value @xmath244 .",
    "an algorithm bypassing this problem has been proposed in the literature for a finite state - space latent process in @xcite .",
    "it relies on the following update scheme @xmath258 where @xmath259 is defined as @xmath260 with the notation @xmath261 corresponding to a ` time - varying ' score which is computed with a filter using the parameter @xmath262 at time @xmath263 .",
    "the update rule ( [ eq : rml ] ) can be thought of as an approximation to the update rule ( [ eq : update_theta_partial_gradient ] ) .",
    "if we use fisher s identity to compute this ` time - varying ' score , then we have for @xmath264 @xmath265 the asymptotic properties of the recursion ( [ eq : rml ] ) ( i.e. the behavior of @xmath244  in the limit as @xmath32  goes to infinity ) has been studied in @xcite for a finite state - space hmm . it is shown that under regularity conditions this algorithm converges towards a local maximum of the average log - likelihood @xmath248 , @xmath248 being maximized at the ` true ' parameter value under identifiability assumptions .",
    "similar results hold for the recursion ( [ eq : update_theta_partial_gradient ] ) .",
    "it is also possible to propose an on - line version of the em algorithm .",
    "this was originally proposed for finite state - space and linear gaussian models in @xcite , @xcite ; see @xcite for a detailed presentation in the finite state - space case .",
    "assume that @xmath266 is in the exponential family .",
    "in the on - line implementation of em , running averages of the sufficient statistics @xmath267 are computed @xcite , @xcite .",
    "let @xmath268 be the sequence of parameter estimates of the on - line em  algorithm computed sequentially based on @xmath269 .",
    "when @xmath246 is received , we compute @xmath270 where @xmath251 needs to satisfy @xmath252 and @xmath253",
    ". then the standard maximization step ( [ eq : maximiem ] ) is used as in the batch version @xmath271 the recursive calculation of @xmath272 is achieved by setting @xmath273 , then computing @xmath274 and finally @xmath275 again , the subscript @xmath276 on @xmath277 indicates that the posterior density is being computed sequentially using the parameter @xmath262 at time @xmath278 .",
    "the filtering density then is advanced from time @xmath279 to time @xmath32 by using @xmath280 , @xmath281 and @xmath282 in the fraction of the rhs of . whereas the convergence of the em algorithm towards a local maximum of the average log - likelihood @xmath248 has been established for i.i.d .",
    "data @xcite , its convergence for state - space models remains an open problem despites empirical evidence it does @xcite , @xcite , @xcite .",
    "this has motivated the development of modified versions of the on - line em algorithm for which convergence results are easier to establish @xcite , @xcite . however , the on - line em presented here performs empirically usually better @xcite .",
    "both the on - line gradient and em procedures require approximating terms ( [ eq : timevaryingscore ] ) and ( [ eq : suffstatonline ] ) of the form ( [ eq : expectationadditivefunctionals ] ) , except that the expectation is now w.r.t the posterior density @xmath277 which is updated using the parameter @xmath262 at time @xmath278 . in this on - line",
    "framework , only the path space , fixed lag smoothing and forward smoothing estimates are applicable , the fixed lag approximation is also applicable but introduces a non - vanishing bias .",
    "for the on - line em algorithm , similarly to the batch case discussed in section [ sub : discussion - of - particle - batch ] , the benefits of using the forward smoothing estimate @xcite compared to the path space estimate @xcite with @xmath240 particles are rather limited , as experimentally demonstrated in section [ sub : maximum - likelihood - methods ] .",
    "however for the on - line gradient ascent algorithm , the gradient term @xmath259 in ( [ eq : rml ] ) is a difference between two score - like vectors ( [ eq : timevaryingscore ] ) and the behavior of its particle estimates differs significantly from its em counterpart .",
    "indeed the variance of the particle path estimate of @xmath259 increases linearly with @xmath32 , yielding an unreliable gradient ascent procedure , whereas the particle forward smoothing estimate has a variance uniformly bounded in time under appropriate regularity assumptions and yields a stable gradient ascent procedure @xcite . hence the use of a procedure of computational complexity @xmath202",
    "is clearly justified in this context .",
    "the very recent paper @xcite reports that the computationally cheaper estimate ( [ eq : olssonmcapproximation ] ) appears to exhibit similar properties whenever @xmath189 and might prove an attractive alternative .",
    "in the bayesian setting , we assign a suitable prior density @xmath283 for @xmath15 and inference is based on the joint posterior density @xmath284 in the off - line case , or the sequence of posterior densities @xmath285 in the on - line case .",
    "using mcmc is a standard approach to approximate @xmath284 . unfortunately designing efficient mcmc  sampling algorithms for non - linear non - gaussian state - space models",
    "is a difficult task : one - variable - at - a - time gibbs sampling typically mixes very poorly for such models , whereas blocking strategies that have been proposed in the literature are typically very model - dependent ; see for instance @xcite .",
    "particle mcmc are a class of mcmc  techniques which rely on particle  methods to build efficient high dimensional proposal distributions in a generic manner @xcite .",
    "we limit ourselves here to the presentation of the particle marginal metropolis  hastings ( pmmh )  sampler , which is an approximation of an ideal mmh  sampler for sampling from @xmath284 which would utilize the following proposal density @xmath286 where @xmath287 is a proposal density to obtain a candidate @xmath288 when we are at location @xmath15 .",
    "the acceptance probability of this sampler is @xmath289 unfortunately this ideal algorithm can not be implemented as we can not sample exactly from @xmath290 and we can not compute the likelihood terms @xmath291 and @xmath292 appearing in the acceptance probability .",
    "the pmmh  sampler is an approximation of this ideal mmh  sampler which relies on the particle approximations of these unknown terms .",
    "given @xmath15 and a particle approximation @xmath293 of @xmath291 , we sample @xmath294 then run a particle filter to obtain approximations @xmath295 and @xmath296 of @xmath297 and @xmath292 .",
    "we then sample @xmath298 , that is we choose randomly one of @xmath87 particles generated by the particle filter , with probability @xmath299 for particle @xmath300 , and accept @xmath301 ( and @xmath296 ) with probability @xmath302 the acceptance probability ( [ eq : acceptprobapmmh ] ) is a simple approximation of the  ideal  acceptance probability ( [ eq : acceptprobamh ] ) .",
    "this algorithm was first proposed as an heuristic to sample from @xmath303 in @xcite .",
    "its remarkable feature established in @xcite is that it does admit @xmath284 as invariant distribution whatever being the number of particles @xmath87 used in the particle  approximation @xcite .",
    "however the choice of @xmath87 has an impact on the performance of the algorithm . using large values of @xmath87",
    "usually results in pmmh averages with variances lower than the corresponding averages using fewer samples but the computational cost of constructing @xmath293 increases with @xmath87 .",
    "a simplified analysis of this algorithm suggests that @xmath87 should be selected such that the standard deviation of the logarithm of the particle likelihood estimate should be around @xmath304 if the ideal mmh sampler was using the perfect proposal @xmath305 @xcite and around @xmath306 if one uses an isotropic normal random walk proposal , the target is a product of @xmath307 i.i.d .",
    "components and @xmath308 @xcite . for general proposal and target densities , a recent theoretical analysis and empirical results suggest that this standard deviation should be selected around @xmath309 @xcite .",
    "as the variance of this estimate typically increases linearly with @xmath236 , this means that the computational complexity is of order @xmath310 by iteration .",
    "a particle version of the gibbs sampler is also available @xcite which mimicks the two - component gibbs sampler sampling iteratively from @xmath311 and @xmath119 .",
    "these algorithms rely on a non - standard version of the particle filter where @xmath312 particles are generated conditional upon a `` fixed '' particle .",
    "recent improvements over this particle gibbs sampler introduce mechanisms to rejuvenate the fixed particle , using forward or backward sampling procedures @xcite , @xcite , @xcite .",
    "these methods perform empirically extremely well but , contrary to the pmmh , it is still unclear how one should scale @xmath87 with @xmath236 .      in this context , we are interested in approximating on - line the sequence of posterior densities @xmath285 .",
    "we emphasize that , contrary to the on - line ml  parameter estimation procedures , none of the methods presented in this section bypass the particle degeneracy problem .",
    "this should come as no surprise .",
    "as discussed in section [ sec : convergenceparticle ] , even for a _ fixed _",
    "@xmath15 , the particle estimate of @xmath21 has a relative variance that increases linearly with @xmath32 under favorable mixing assumptions . the methods in this section attempt to approximate @xmath313 .",
    "this is a harder problem as it implicitly requires having to approximate @xmath314 for all the particles @xmath315 approximating @xmath316 .      at first sight",
    ", it seems that estimating the sequence of posterior densities @xmath285 can be easily achieved using standard particle methods , by merely introducing the extended state @xmath317 , with initial density @xmath318 and transition density @xmath319 i.e. @xmath320 .",
    "however , this extended process @xmath321 clearly does not possess any _ forgetting _ property ( as discussed in section [ sec : filteringandparticle ] ) , so the algorithm is bound to degenerate .",
    "specifically , the parameter space is explored only in the initial step of the algorithm .",
    "then , each successive resampling step reduces the diversity of the sample of @xmath15 values ; after a certain time @xmath32 , the approximation @xmath322 contains a single unique value for @xmath15 .",
    "this is clearly a poor approach . even in the  much simpler case",
    "when there is no latent variable @xmath18 , it is shown in ( * ? ? ?",
    "* theorem 4 ) that the asymptotic variance of the corresponding particle estimates diverges at least at a polynomial rate , which grows with the dimension of @xmath15",
    ".    a pragmatic approach that has proven useful in some applications is to introduce artificial dynamics for the parameter @xmath15",
    "@xcite @xmath323 where @xmath324 is an artificial dynamic noise with decreasing variance .",
    "standard particle methods can now be applied to approximate @xmath325 .",
    "a related kernel density estimation method also appeared in @xcite , which proposes to use a kernel density estimate @xmath316 from which one samples from .",
    "as before the static parameter is transformed to a slowly time - varying one , whose dynamics is related to the kernel bandwidth . to mitigate the artificial variance inflation , a shrinkage correction is introduced .",
    "an improved version of this method has been recently proposed in @xcite .",
    "it is difficult to quantify how much bias is introduced in the resulting estimates by the introduction of this artificial dynamics .",
    "additionally , these methods require a significant amount of tuning , e.g. choosing the variance of the artificial dynamic noise or the kernel width .",
    "however they can perform satisfactorily in practice @xcite , @xcite .",
    "the practical filtering approach proposed in @xcite relies on the following fixed - lag approximation @xmath326 for @xmath121 large enough ; that is observations coming after @xmath279 presumably brings little information on @xmath327 . to sample",
    "approximately from @xmath328 , one uses the following iterative process : at time @xmath32 , several mcmc  chains are run in parallel to sample from @xmath329 where the @xmath330 have been obtained at the previous iteration , and are such that ( approximately ) @xmath331 .",
    "then one collects the first component @xmath332 of the simulated sample @xmath333 , increments the time index and runs several new mcmc  chains in parallel to sample from @xmath334 and so on .",
    "the algorithm is started at time @xmath335 , with mcmc chains that target @xmath336 .",
    "like all methods based on fixed - lag approximation , the choice of the lag @xmath121 is difficult and this introduces a non - vanishing bias which is difficult to quantify .",
    "however , the method performs well on the examples presented in @xcite .      to avoid the introduction of an artificial dynamic model or of a fixed - lag approximation , an approach originally proposed independently in @xcite and @xcite",
    "consists of adding mcmc steps to re - introduce  diversity  among the particles .",
    "assume we use an auxiliary particle filter to approximate @xmath285 then the particles @xmath337 obtained after the sampling step at time @xmath32 are approximately distributed according to @xmath338 we have @xmath339 if @xmath66 and @xmath67 . to add diversity in this population of particles ,",
    "we introduce an mcmc  kernel @xmath340 with invariant density @xmath341 , and replace , at the end of each iteration , the set of resampled particles , @xmath342 with @xmath87 `` mutated '' particles @xmath343 simulated from , for @xmath344 @xmath345 if we use the sisr algorithm , then we can alternatively use an mcmc step of invariant density @xmath346 after the resampling step at time @xmath32 .",
    "contrary to standard applications of mcmc , the kernel does not have to be ergodic .",
    "ensuring ergodicity would indeed require one to sample an increasing number of variables as @xmath32 increases  this algorithm would have an increasing cost per iteration , which would prevents its use in on - line scenarios but it can be an interesting alternative to standard mcmc and was suggested in  @xcite . in practice one",
    "therefore sets @xmath347 and only sample @xmath348 and @xmath349 , where @xmath121 is a small integer ; often @xmath350 ( only @xmath15 is updated ) .",
    "note that the memory requirements for this method do not increase over time if @xmath351 is in the exponential family and thus can be summarized by a set of fixed dimensional sufficient statistics @xmath352 .",
    "this type of methods was first used in to perform on - line bayesian parameter estimation in a context where @xmath351 is in the exponential family @xcite , @xcite .",
    "similar strategies were adopted in @xcite and @xcite . in the particular scenario where @xmath66 and @xmath67 , this method was mentioned in @xcite , @xcite and is discussed at length in @xcite who named it particle learning .",
    "extensions of this strategy to parameter estimation in conditionally linear gaussian models , where a part of the state is integrated out using kalman techniques @xcite , @xcite , is proposed in @xcite .",
    "as opposed to the methods relying on kernel or artificial dynamics , these mcmc - based approaches have the advantage of adding diversity to the particles approximating @xmath316 without perturbing the target distribution . unfortunately , these algorithms rely implicitly on the particle approximation of the density @xmath353 even if algorithmically it is only necessary to store some fixed - dimensional sufficient statistics @xmath354 . hence in this respect",
    "they suffer from the degeneracy problem .",
    "this was noticed as early as in @xcite ; see also the word of caution in the conclusion of @xcite , @xcite and @xcite .",
    "the practical implications are that one observes empirically that the resulting monte carlo estimates can display quite a lot of variability over multiple runs as demonstrated in section [ sub : bayesianmethodsexperiments ] .",
    "this should not come as a surprise as the sequence of posterior distributions does not have exponential forgetting properties , hence there is an accumulation of monte carlo errors over time .",
    "the smc@xmath355 algorithm introduced simultaneously in @xcite and @xcite may be considered as the particle equivalent of particle mcmc .",
    "it mimics an `` ideal '' particle algorithm proposed in @xcite approximating sequentially @xmath356 where @xmath357 particles ( in the @xmath15-space ) are used to explore these distributions .",
    "the @xmath357 particles at time @xmath32 are reweighted according to @xmath358 at time @xmath164 .",
    "as these likelihood terms are unknown , we substitute to them @xmath359 where @xmath360 is a particle approximation of the partial likelihood @xmath35 , obtained by a running a particle filter of @xmath361 particles in the @xmath362dimension , up to time @xmath32 , for each of the @xmath357 @xmath225particles . when particle degeneracy ( in the @xmath225dimension ) reaches a certain threshold , @xmath225particles are refreshed through the succession of a resampling step , and an mcmc step , which in these particular settings takes the form of a pmcmc update .",
    "the cost per iteration of this algorithm is not constant and , additionally , it is advised to increase @xmath361 with @xmath32 for the relative variance of @xmath360 not to increase , therefore it can not be used in truly on - line scenarios .",
    "yet there are practical situations where it may be useful to approximate jointly all the posteriors @xmath363 , for @xmath364 , for instance to assess the predictive power of the model .",
    "we focus on illustrating numerically a few algorithms and the impact of the degeneracy problem on parameter inference .",
    "this last point is motivated by the fact that particle degeneracy seems to have been overlooked by many practitioners . in this way",
    "numerical results may provide valuable insights .",
    "we will consider the following simple scalar linear gaussian state space model : @xmath365 where @xmath366 are independent zero - mean and unit - variance gaussians and @xmath367 $ ] .",
    "the main reason for choosing this model is that kalman filter recursions can be implemented to provide the exact values of the summary statistics @xmath112 used for ml estimation through the em algorithm , and to compute the exact likelihood @xmath21 .",
    "hence , using a fine discretization of the low - dimensional parameter space , we can compute a very good approximation of the true posterior density @xmath328 . in this model",
    "it is straightforward to present numerical evidence of some effects of degeneracy for parameter estimation and show how it can be overcome by choosing an appropriate particle method .",
    "( top panel ) , empirical variance ( middle panel ) and mse ( bottom panel ) for the estimate of @xmath368 . left column : @xmath369 method using @xmath370 particles .",
    "right column : @xmath242 method using @xmath371 particles . in every subplot",
    ", the top line corresponds to using @xmath372 the middle for @xmath373 and the lower for @xmath374,scaledwidth=100.0% ]      as ml methods require approximating smoothed additive functionals @xmath112 of the form ( [ eq : expectationadditivefunctionals ] ) , we begin by investigating the empirical bias , variance and mse of two standard particle estimates of @xmath112 , where we set @xmath375 for the model described in .",
    "the first estimate relies on the path space method with computational cost @xmath369 per time , which uses @xmath110 in ( [ eq : smcfullposterior ] ) to approximate @xmath112 as @xmath113 ; see ( * ? ? ?",
    "* section 8.3 ) for more details .",
    "the second estimate relies on the forward implementation of ffbsm presented in section [ sub : forward - only - smoothing ] using ( [ eq : additivesmoothfunctionalsasfunctionoft])-([eq : smcapproxadditivefunctionals-1 ] ) ; see @xcite . recall that this procedure has a computational cost that is @xmath242 per time for @xmath87 particles and provides the same estimates as the standard forward - backward implementation of ffbsm . for the sake of brevity",
    "we will not consider the remaining smoothing methods of section [ sec : smoothingandparticle ] ; for the fixed - lag and the exponentially weighted approximations we refer the reader to @xcite respectively @xcite for numerical experiments .",
    "we use a simulated dataset of size @xmath376 obtained using @xmath377 and then generate 300 independent replications of each method in order to compute the empirical bias and variance of @xmath378 when @xmath15 is fixed to @xmath379 . in order to make a comparison that takes into account the computational cost , we use @xmath240 particles for the @xmath369 method and @xmath87 for the @xmath242 one .",
    "we look separately at the behavior of the bias of @xmath380 and the variance and mse of the rescaled estimates @xmath381 .",
    "the results are presented in figure [ fig : smoothing ] for @xmath371 .    for both methods the bias grows linearly with time , this growth being higher for the @xmath242 method . for the variance of @xmath381 ,",
    "we observe a linear growth with time for the @xmath369 method with @xmath240 particles whereas this variance appears roughly constant for the @xmath242 method .",
    "finally , the mse of @xmath381 grows for both methods linearly as expected . in this particular scenario ,",
    "the constants of proportionality are such that the mse is lower for the @xmath369 method than for the @xmath242 method . in general",
    ", we can expect that the @xmath369 method be superior in terms of the bias and the @xmath242 method superior in terms of the variance .",
    "these results are in agreement with the theoretical results in the literature @xcite , @xcite , @xcite but additionally show that the lower bound on the variance growth of @xmath380 for the @xmath369 method of @xcite appears sharp .     for various @xmath236 using 25 iterations of off - line em and 150 realizations of the algorithms .",
    "top panels : @xmath369 method using @xmath382 particles .",
    "bottom panels : @xmath242 with @xmath383 .",
    "the dotted horizontal lines are the ml estimate for each time @xmath236 obtained using kalman filtering on a grid.,scaledwidth=105.0% ]    we proceed to see how the bias and variance of the estimates of @xmath112 affect the ml estimates , when the former are used within both an off - line and an on - line em algorithm ; see figures [ fig : offlineem ] and [ fig : onlineem ] respectively .",
    "for the model in ( [ eq : dlm1 ] ) the e - step corresponds to computing @xmath112 where @xmath384 and the m - step update function is given by @xmath385 we compare the estimates of @xmath379 when the e - step is computed using the @xmath369 and the @xmath242 methods described in the previous section with @xmath386 and @xmath387 particles respectively .",
    "a simulated dataset for @xmath388 will be used . in every case",
    "we will initialize the algorithm using @xmath389 and assume @xmath390 is known . in figures [ fig : offlineem ] and [ fig : onlineem ] we present the results obtained using 150 independent replications of the algorithm . for the off - line em , we use @xmath391 iterations for @xmath392 . for the on - line em",
    ", we use @xmath393 with the step size set as @xmath394 and for the first @xmath395 iterations no m - step update is performed .",
    "this `` freezing '' phase is required to allow for a reasonable estimation of the summary statistic ; see @xcite , @xcite for more details .",
    "note that in figure [ fig : onlineem ] we plot only the results after the algorithm has converged , i.e. for @xmath396 . in each case , both the @xmath369 and the @xmath242 methods yield fairly accurate results given the low number of particles used .",
    "however we note , as observed previously in the literature , that the on - line em as well as the on - line gradient ascent method requires a substantial number of observations , i.e. over 10000 , before achieving convergence @xcite , @xcite , @xcite , @xcite . for smaller datasets ,",
    "these algorithms can also be used by going through the data say @xmath397 times .",
    "typically this method is cheaper than iterating ( [ eq : batchgradient ] ) or ( [ eq : qfunction])-([eq : maximiem ] ) @xmath397 times the off - line algorithms and can yield comparable parameter estimates @xcite .",
    "experimentally , the properties of the estimates of @xmath112 discussed earlier appear to translate into properties of the resulting parameter estimates : the @xmath369 method provides estimates with less bias but more variance than the @xmath242 method .     for @xmath398 using 150 realizations of the algorithms .",
    "we also plot the ml estimate at time @xmath32 obtained using kalman filtering on a grid ( black).,scaledwidth=105.0% ]    for more numerical examples regarding the remaining methods discussed in section [ sec : mlestimation ] , we refer the reader to @xcite , @xcite for iterated filtering , to @xcite , @xcite , @xcite for comparisons of the @xmath369 and @xmath242 methods for em and gradient ascent , to @xcite for the @xmath369 on - line em , to @xcite and ( * ? ? ?",
    "10 ) for smooth likelihood function methods and to ( * ? ? ?",
    "10 - 11 ) for a detailed exposition of off - line em methods .",
    "we still consider the model in ( [ eq : dlm1 ] ) but simplify it further by fixing either @xmath399 or @xmath400 .",
    "this is done in order to keep the computations of the benchmarks that use kalman computations on a grid relatively inexpensive .",
    "for those parameters that are not fixed , we shall use the following independent priors : a uniform on @xmath401 $ ] for @xmath399 , and inverse gamma for @xmath402 with the shape and scale parameter pair being @xmath403 and @xmath404 respectively with @xmath405 .",
    "in all the subsequent examples , we will initialize the algorithms by sampling @xmath15 from the prior .     at @xmath406 .",
    "top right : relative variance , i.e. empirical variance ( over independent runs ) for the estimator of the mean of @xmath407 using particle method with mcmc normalized with the true posterior variance computed using kalman filtering on a grid .",
    "bottom left : average ( over independent runs ) of the estimated variance of @xmath407 using particle method with mcmc normalized with the true posterior variance .",
    "bottom right : variance of the @xmath408 ; all plots are computed using @xmath409 and over 100 different independent runs.,title=\"fig:\",scaledwidth=50.0% ] at @xmath406 .",
    "top right : relative variance , i.e. empirical variance ( over independent runs ) for the estimator of the mean of @xmath407 using particle method with mcmc normalized with the true posterior variance computed using kalman filtering on a grid .",
    "bottom left : average ( over independent runs ) of the estimated variance of @xmath407 using particle method with mcmc normalized with the true posterior variance .",
    "bottom right : variance of the @xmath408 ; all plots are computed using @xmath409 and over 100 different independent runs.,title=\"fig:\",scaledwidth=50.0% ] +   at @xmath406 .",
    "top right : relative variance , i.e. empirical variance ( over independent runs ) for the estimator of the mean of @xmath407 using particle method with mcmc normalized with the true posterior variance computed using kalman filtering on a grid .",
    "bottom left : average ( over independent runs ) of the estimated variance of @xmath407 using particle method with mcmc normalized with the true posterior variance .",
    "bottom right : variance of the @xmath408 ; all plots are computed using @xmath409 and over 100 different independent runs.,title=\"fig:\",scaledwidth=50.0% ] at @xmath406 .",
    "top right : relative variance , i.e. empirical variance ( over independent runs ) for the estimator of the mean of @xmath407 using particle method with mcmc normalized with the true posterior variance computed using kalman filtering on a grid .",
    "bottom left : average ( over independent runs ) of the estimated variance of @xmath407 using particle method with mcmc normalized with the true posterior variance .",
    "bottom right : variance of the @xmath408 ; all plots are computed using @xmath409 and over 100 different independent runs.,title=\"fig:\",scaledwidth=50.0% ]    we proceed to examine the combination of particle method with mcmc methods described in section [ sub : using - mcmcsteps - within - smc ] .",
    "we focus on an efficient implementation of this idea discussed in @xcite which can be put in practice for the simple model under consideration .",
    "we investigate the effect of the degeneracy problem in this context .",
    "the numerical results obtained in this section have been produced in matlab ( code available from the first author ) , and double - checked using the r program available on the personal web page of the first author of @xcite .",
    "we first focus of the estimate of the posterior of @xmath410 given a long sequence of simulated observations with @xmath411 . in this scenario , @xmath266 admits the following two - dimensional sufficient statistics , @xmath412 , and @xmath15 can be updated using gibbs steps .",
    "we use @xmath413 and @xmath409 .",
    "we ran the algorithm over 100 independent runs over the same dataset .",
    "we present the results only for @xmath414 and omit the ones for @xmath223 as these were very similar .",
    "the top left panel of figure [ flo : relvar ] shows the box plots for the estimates of the posterior mean , and the top right panel shows how the corresponding relative variance of the estimator for the posterior mean evolves with time . here",
    "the relative variance is defined as the ratio of the empirical variance ( over different independent runs ) of the posterior mean estimates at time @xmath32 over the true posterior variance at time @xmath32 , which in this case is approximated using a kalman filter on a fine grid .",
    "this quantity exhibits a steep increasing trend when @xmath415 and confirms the aforementioned variability of the estimates of the posterior mean . in the bottom left panel of figure [ flo : relvar ]",
    "we plot the average ( over different runs ) of the estimators of the variance of @xmath416 .",
    "this average variance is also scaled / normalized by the actual posterior variance .",
    "the latter is again computed using kalman filtering on a grid .",
    "this ratio between the average estimated variance of the posterior over the true one decreases with time @xmath32 and it shows that the supports of the approximate posterior densities provided by this method cover on average only a small portion of the support of the true posterior .",
    "these experiments confirm that in this example the particle method with mcmc steps fails to adequately explore the space of @xmath15 .",
    "although the box plots provide some false sense of security , the relative and scaled average variance clearly indicate that any posterior estimates obtained from a single run of particle method with mcmc steps should be used with caution .",
    "furthermore , in the the bottom right panel of figure [ flo : relvar ] we also investigate experimentally the empirical variance of the marginal likelihood estimates @xmath408 .",
    "this variance appears to increase quadratically with @xmath32 for the particle method with mcmc moves instead of linearly as it does for state - space models with good mixing properties .",
    "this suggests that to one should increase the number of particles quadratically with the time index to obtain an estimate of the marginal likelihood whose relative variance remains uniformly bounded with respect to the time index .",
    "although we attribute this quadratic variance growth to the degeneracy problem , the estimate @xmath417 is not the particle approximation of a smoothed additive functional , thus there is not yet any theoretical convergence result explaining rigorously this phenomenon .    ; estimated marginal posterior densities for @xmath418 over 50 runs ( red ) versus ground truth ( blue ) .",
    ", title=\"fig:\",scaledwidth=95.0% ]    one might argue that these particle methods with mcmc moves are meant to be used with larger @xmath87 and/or shorter data sets @xmath236 .",
    "we shall consider this time a slightly different example where @xmath419 is known and we are interested in estimating the posterior of @xmath420 given a sequence of observations obtained using @xmath421 and @xmath422 . in that case , the sufficient statistics are @xmath423 , and the parameters can be rejuvenated through a single gibbs update .",
    "in addition , we let @xmath424 and use @xmath425 particles . in figures [ flo : dlm_drift_snapshot ] we display the estimated marginal posteriors @xmath426 and @xmath427 obtained from @xmath395 independent replications of the particle method . on this simple problem ,",
    "the estimated posteriors seem consistently rather inaccurate for @xmath399 , whereas they perform better for @xmath223 but with some non - negligible variability over runs which increases as @xmath236 increases .",
    "similar observations have been reported in @xcite and remain unexplained : for some parameters this methodology appears to provide reasonable results despite the degeneracy problem and for others it provides very unreliable results .",
    "we investigate further the performance of this method in this simple example by considering the same example for @xmath428 but now consider two larger numbers of particles , @xmath429 and @xmath430 , over 50 different runs .",
    "additionally we compare the resulting estimates with estimates provided by the particle gibbs sampler of @xcite using the same computational cost , that is @xmath431 particles with @xmath432 and @xmath433 iterations respectively .",
    "the results are displayed in figure [ flo : compare_low ] and [ flo : compare_high ] .",
    "as expected , we improve the performance of the particle with mcmc moves when @xmath87 increases for a fixed time horizon @xmath236 . for a fixed computational complexity ,",
    "the particle gibbs sampler estimates appear to display less variability . for a higher dimensional parameter @xmath15 and/or very vague priors",
    ", this comparison would be more favorable to the particle gibbs sampler as illustrated in @xcite .     with @xmath434 over 50 runs ( black - dotted ) versus ground truth ( green ) .",
    "top : particle method with mcmc , @xmath429 .",
    "bottom : particle gibbs with @xmath432 iterations and @xmath431.,title=\"fig:\",scaledwidth=90.0% ]     with @xmath434 over 50 runs ( black - dotted ) versus ground truth ( green ) .",
    "top : particle method with mcmc , @xmath430 .",
    "bottom : particle gibbs with @xmath433 iterations and @xmath431.,title=\"fig:\",scaledwidth=90.0% ]",
    "most particle methods proposed originally in the literature to perform inference about static parameters in general state - space models were computationally inefficient as they suffered from the degeneracy problem .",
    "several approaches have been proposed to deal with this problem by either adding an artificial dynamic on the static parameter @xcite , @xcite , @xcite or introducing a fixed - lag approximation @xcite , @xcite , @xcite .",
    "these methods can work very well in practice but it remains unfortunately difficult / impossible to quantify the bias introduced in most realistic applications .",
    "various asymptotically bias - free methods with good statistical properties and a reasonable computational cost have recently appeared in the literature .    to perform batch ml estimation , the forward filter backward sampler / smoother and generalized two filter procedures",
    "are recommended whenever the @xmath435 computational complexity per iteration of their direct implementations can be lowered to @xmath436 using , for example , the methods described in @xcite , @xcite , @xcite , @xcite . otherwise , besides a lowering of memory requirements , not much can be gained from these techniques compared to simply using a standard particle filter with @xmath240 particles . in an on - line ml context ,",
    "the situation is markedly different .",
    "whereas for the on - line em algorithm , the forward smoothing approach in @xcite , @xcite of complexity @xmath242 per time step will be similarly of limited interest compared to a standard particle filter using @xmath240 particles , it is crucial to use this approach when performing on - line gradient ascent as demonstrated empirically and established theoretically in @xcite . in on - line scenarios where one can admit a random computational complexity at each time step ,",
    "the method presented in @xcite is an interesting alternative when it is applicable .",
    "empirically these on - line ml methods converge rather slowly and will be primarily be useful for large datasets .    in a bayesian framework",
    ", batch inference can be conducted using particle mcmc methods @xcite , @xcite .",
    "however these methods are computationally expensive as , for example , an efficient implementation of the pmmh has a computational complexity of order @xmath310 per iteration @xcite . on - line",
    "bayesian inference remains a challenging open problem as all methods currently available , including particle methods with mcmc moves @xcite , @xcite , @xcite suffer from the degeneracy problem .",
    "these methods should not be ruled out but should be used cautiously as they can provide unreliable results even in simple scenarios as demonstrated in our experiments .",
    "very recent papers in this dynamic research area have proposed to combine individual parameter estimation techniques so as to design more efficient inference algorithms .",
    "for example , @xcite suggests to use the score estimation techniques developed for ml parameter estimation to design better proposal distributions for the pmmh algorithm whereas @xcite demonstrates that particle methods with mcmc moves might be fruitfully used in batch scenarios when plugged into a particle mcmc scheme .",
    "n. kantas was supported by the engineering and physical sciences research council ( epsrc ) under grant ep / j01365x/1 and programme grant on control for energy and sustainability ( ep / g066477/1 ) .",
    "singh s research is partly funded by epsrc under the first grant scheme ( ep / g037590/1 ) .",
    "a. doucet s research is partly funded by epsrc ( ep / k000276/1 and ep / k009850/1 ) .",
    "n. chopin s research is partly by the anr as part of the investissements davenir program ( anr-11-labex-0047 ) .",
    "del moral , p. and doucet , a. and singh , s.s .",
    "uniform stability of a particle approximation of the optimal filter derivative .",
    "preprint arxiv:1106.2525 .",
    "_ siam j. control optimization _ , to appear .",
    "doucet , a. , pitt , m.k . ,",
    "deligiannidis , g. and kohn , r. ( 2012 ) .",
    "efficient implementation of markov chain monte carlo when using an unbiased likelihood estimator .",
    "preprint arxiv:1210.1871 .",
    "_ biometrika _ , to appear .",
    "ford , j.j .",
    "adaptive hidden markov model estimation and applications .",
    "phd thesis , department of systems engineering , australian national university .",
    "available at http://infoeng.rsise.anu.edu.au/files/jason_ford_thesis.pdf                                      lopes , h.f .",
    ", carvalho , c. , johannes , m. and polson , n. ( 2010 ) .",
    "particle learning for sequential bayesian computation . in _ bayesian statistics",
    "_ 9 ( bernardo et al .",
    "eds ) , oxford university press , to appear .",
    "nemeth , c. , fearnhead , p. and mihaylova , l. ( 2013 ) .",
    "particle approximations of the score and observed information matrix for parameter estimation in state space models with linear computational cost .",
    "preprint arxiv:1306.0735 .",
    "paninski , l. , ahmadian , y. , ferreira , d.g . ,",
    "koyama , s. , rad .",
    "s.r , vidne , m. , vogelstein , j. and wu , w. ( 2010 ) . a new look at state - space models for neural data . _",
    "j. computational neuroscience _ , 29 , 107126 .",
    "poyiadjis , g. , doucet , a. and singh , s.s .",
    "particle approximations of the score and observed information matrix in state - space models with application to parameter estimation . _",
    "biometrika _ , 98 , 6580 .",
    "vercauteren , t. , toledo , a. and wang , x. ( 2005 ) .",
    "online bayesian estimation of hidden markov models with unknown transition matrix and applications to ieee 802.11 networks . _ proc .",
    "ieee icassp _ ,",
    "iv , 1316 ."
  ],
  "abstract_text": [
    "<S> nonlinear non - gaussian state - space models are ubiquitous in statistics , econometrics , information engineering and signal processing . </S>",
    "<S> particle methods , also known as sequential monte carlo ( smc ) methods , provide reliable numerical approximations to the associated state inference problems . </S>",
    "<S> however , in most applications , the state - space model of interest also depends on unknown static parameters that need to be estimated from the data . in this context , </S>",
    "<S> standard particle methods fail and it is necessary to rely on more sophisticated algorithms . the aim of this paper is to present a comprehensive review of particle methods that have been proposed to perform static parameter estimation in state - space models . </S>",
    "<S> we discuss the advantages and limitations of these methods and illustrate their performance on simple models .    ,    ,    , </S>"
  ]
}