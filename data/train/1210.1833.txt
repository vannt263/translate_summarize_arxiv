{
  "article_text": [
    "the last decades have seen an increasing interest in galaxy surveys as a means of studying the late - time evolution of the universe .",
    "forthcoming galaxy surveys , such as des @xcite , bigboss @xcite or euclid @xcite , will map large regions of the sky ( @xmath0 sq - deg ) to redshifts @xmath1 yielding catalogs containing hundreds of millions of objects .",
    "the spatial distribution of these objects on different scales contains invaluable information that could help clarify many open problems in cosmology and astrophysics , such as the nature of dark matter and dark energy or the presence of primordial non - gaussianities in the density field .",
    "one of the simplest observables that can be estimated to quantify the clustering of matter on different scales is the two - point correlation function ( 2pcf hereon , see section [ sec:2pcf ] ) .",
    "its estimation is based on counting pairs of objects separated by a given distance measure , and therefore its computational time grows with the square of the number of objects in the catalog .",
    "hence , when @xmath2 pairs must be considered , a simplistic serial approach is too slow for the full - scale problem , and , besides using some simplifying approximation , the only viable solution becomes parallelising the calculation . in this sense modern graphical processing units ( gpus )",
    "provide the means to perform many operations in parallel on a large number ( hundreds ) of cores with a moderate clock frequency for a comparatively cheap price .",
    "another approach is using a relatively smaller number of high - frequency cpu cores both in shared or distributed memory machines .    here",
    "we present a cute ( correlation utilities and two - point estimation ) , a free open - source code that estimates different kinds of two - point correlations from discrete cosmological catalogs using various speed - up techniques .",
    "the three - dimensional 2pcf @xmath3 of a set of discrete points in @xmath4 represents the excess probability of finding two of them inside two small volumes @xmath5 and @xmath6 separated by @xmath7 @xcite : @xmath8\\,dv_1\\,dv_2.\\ ] ] when this point distribution comes from a poisson process based on an underlying random density field @xmath9 , the field s 2pcf @xmath10 is directly related to that of the point distribution @xcite .",
    "note that even though , in principle , the two - point correlation should depend on the positions of both points , @xmath11 and @xmath12 , for homogeneous fields the only dependence is on the separation between them @xmath13 .",
    "* * the 3-d correlation function @xmath14 and @xmath15 . * different observational effects , such as redshift - space distortions or errors in the observed redshifts , transform what would otherwise be an isotropic 2pcf into a function that behaves differently along the line of sight and in the transverse direction .",
    "two coordinate systems are widely used in the literature : the @xmath16 and @xmath17 schemes ( see fig . [",
    "fig : coords ] ) , the relation between both being @xmath18 the @xmath17 scheme has the advantage that the usual multipole expansion is directly written in terms of these variables : @xmath19 where @xmath20 are the legendre polynomials . note that at the linear level and in the plane - parallel approximation ( i.e. the kaiser formula @xcite ) only the first three even multipoles ( @xmath21 ) contribute . *",
    "* the monopole @xmath22 . *",
    "the first element ( @xmath23 ) in the expansion above is the angle - averaged correlation function or `` monopole '' : @xmath24 this is the only non - zero contribution in the absence of redshift distortions . * * the radial correlation function @xmath25 . *",
    "correlating only pairs of galaxies aligned with the line of sight , one computes the so - called radial correlation function , which can be made to depend locally only on the redshift difference @xmath26 between each pair of galaxies .",
    "this quantity is related to the three - dimensional 2pcf through @xmath27 whith @xmath28 where @xmath29 is the radial comoving distance to redshift @xmath30 . * * the angular correlation function @xmath31 . *",
    "the angular correlation function is the 2pcf of the density contrast field projected on the sphere @xmath32 where @xmath33 is the redshift selection function .",
    "the angular correlation function is related to @xmath14 by @xmath34      as we have said , the two - point correlation function can be understood as the excess probability of finding two objects separated by a given distance with respect to a random distribution , and therefore : @xmath35 where @xmath36 is the number of pairs separated by @xmath37 in the data , and @xmath38 is the number of pairs that one would expect for a random distribution .",
    "the numerator can be easily calculated as @xmath39 where @xmath40 is the total number of objects , @xmath41 is 1 whenever its argument is true and 0 otherwise , and we have explicitly avoided counting self - pairs . if the catalog had no boundaries , the number of random pairs could easily be estimated as @xmath42 where @xmath43 is the volume of a spherical shell of radius @xmath44 and thickness @xmath45 .",
    "as we have said , this is can only be done if the catalog has no boundaries . effectively this true in the case of an n - body simulation , where a sphere that lies partly ouside the simulation box can be `` wrapped around '' due to the periodic boundary conditions .",
    "thus , in this case a possible estimator is @xmath46    however , when calculating the correlation function from a point distribution with complicated boundaries , as is usually the case in a galaxy survey , several observational dificulties arise : e.g. different parts of the sky may have been mapped to different depths and the radial distribution of objects ( selection function ) is never uniform .",
    "the most usual technique to deal with these issues is to compare the data catalog with catalogs made of randomly distributed objects that also contain these artificial effects . in this case",
    "the 2pcf can be naively estimated as @xmath47 where @xmath48 and @xmath49 are the number of points in the data and random catalogs respectively and @xmath50 and @xmath51 are histograms containing the counts of pairs of objects found separated by a given distance in each catalog .",
    "it has been shown @xcite that the variance of this estimator can be minimized , and its ability to cope with boundary conditions can be enhanced , by making use of the cross - correlation of random and data objects , @xmath52 .",
    "the most widely used estimator is the one proposed by landy & szalay @xcite : @xmath53 see @xcite for a thorough comparison of different estimators .",
    "the most delicate part of the estimation is in fact being able to generate the random catalogs correctly : the background spatial distribution , both in angles and redshift ( i.e. the one - point function ) , of random objects must be exactly the same as in the data .",
    "hence , all observational effects that affect the spatial distribution must be correctly reproduced by the random catalogs .",
    "also , in order to minimize poisson errors in @xmath52 and @xmath51 , random catalogs should be generated with more particles than the data .",
    "cute ( correlation utilities and two - point estimation ) is a free and open - source code for cosmological 2pcf estimation .",
    "cute is written in c and , in the current public version , comes with two implementations : one parallelized for shared - memory machines using openmp and one ( cu_cute ) that performs the correlations in a gpu using nvidia s cuda architecture .",
    "cute calculates 4 different correlation functions ( 3-d , monopole , angular and radial ) with different binning schemes and speed - up techniques .",
    "here we will explain the parallelization strategies followed by cute and some details specific to each type of 2pcf .",
    "we refer the reader to the readme file accompanying the latest public version of cute for the operational options and compilation instructions of the code . in this section",
    "we assume some basic knowledge of parallel computing with openmp and cuda by the reader .",
    "it must be noted that there exist two other codes @xcite , recently made public , designed to compute angular correlation functions with gpus .",
    "as in the case of cute the speed - up factor ( about @xmath54 ) gained by these codes through the use of graphical devices for parallelization clearly makes it worth the effort of adapting cpu algorithms to run on gpus .",
    "once the random catalog has been produced , @xmath50 , @xmath52 and @xmath51 are computed by autocorrelating or crosscorrelating each pair of catalogs . in a serial code",
    "this algorithm is extremely simple , involving one loop over each catalog and performing 3 operations in each iteration : calculating the distance between each pair of objects , determining the bin corresponding to that distance and increasing the histogram count on that bin",
    ". the corresponding c - code would be :    .... int histogram[nbins ] ; for(i=0;i < np1;i++ ) {    for(j=0;j < np2;j++ ) {      //calculate distance between two objects      double dist = get_dist(x1[i],y1[i],z1[i ] ,                           x2[j],y2[j],z2[j ] ) ;      //calculate bin number      int ibin = bin_dist(dist ) ;      //increase histogram count      histogram[ibin]++ ;    } }     ....    as we said before , these two nested loops make this an @xmath55 problem ( to be precise , an n1*n2 problem ) , whose computational time will grow very fast as we increase the size of the catalogs . at this point parallelization or / and some kind of fast approximate method are desirable , if not compulsory . in section [ ssec : paral ]",
    "we will describe the parallelization strategies used by cute , which complicate this simple algorithm .",
    "other speed - up techniques used by the code are explained in section [ ssec : neigh ] , and some especific details of each type of 2pcf are given in section [ ssec:2pcfs ] .",
    "openmp is an api that gives support for parallel programming in shared - memory platforms .",
    "once a parallel execution block is opened , the programmer can define private ( one independent copy per core ) or shared ( common ) variables and easily divide for loops between all available cores . for a thorough review of the different features of openmp see @xcite .",
    "the serial code above takes the following form when parallelized with openmp :    .... int histogram[nbins ] ; int histo_thread[nbins ] ; # pragma omp parallel default(none ) \\",
    "private(hthread ) shared ( ... ) {    //initialize private histograms    for(i=0;i <",
    "nbins;i++ )      histo_thread[nbins]=0 ; # pragma omp for     //parallelize",
    "loop    for(i=0;i <",
    "np1;i++ ) {      for(j=0;j < np2;j++ ) {        //calculate distance between two objects        double dist = get_dist(x1[i],y1[i],z1[i ] ,                             x2[j],y2[j],z2[j ] ) ;        //calculate bin number        int ibin = bin_dist(dist ) ;        //increase histogram count        histo_thread[ibin]++ ;      }    } # pragma omp critical {    //add private histograms    for(i=0;i <",
    "nbins;i++ )      histogram[i]+=histo_thread[i ] } }     ....    the strategy in this case to is to declare one private histogram per execution thread that will store that thread s pair counts .",
    "the first loop is then divided between all available threads and finally all partial histograms are added , avoiding read / write collisions , into the final shared one .",
    "as can be seen , parallelization with openmp is effortless , only requiring a few extra lines of code .",
    "a gpu ( graphical processing unit ) is a specialized piece of hardware designed for fast massively parallel manipulation of memory addresses .",
    "as their name suggests , gpus are mainly intended for image building and processing , however their highly parallel structure makes them ideal for intensive numerical computation , providing a relatively cheap flop / s ( floating - point operations per second ) .",
    "hence in the last years gpus have found their way into different branches of scientific research , computational cosmology being one of them @xcite initially the main difficulty when trying to use gpus for scientific computing was the programming of the numerical algorithm , since using the standard apis meant that data had to be disguised as pixel colors and some mathematical operations had to be encoded as graphics rendering . however lately a few programming models for gpus have seen the light of day that make general purpose computing on gpus ( gpgpu ) a lot easier . of these we have chosen nvidia s",
    "cuda @xcite for its syntactic simplicity .",
    "two main complications arise when one tries to adapt a code to execute on a gpu .",
    "first , in a massively parallel environment one must take great care to avoid race conditions due to simultaneous memory read / write processes by different threads .",
    "second , unlike in a multi - cpu machine , the amount of memory `` per thread '' available in a gpu is very limited , presently of the order of a few gb for hundreds of processors .",
    "besides these , there are other more subtle concerns , such as intra - warp communication or the presence of different types of cached and uncached memory in the gpu , the correct use of which may enhance dramatically the code s performance . in summary , correctly parallelising a code with cuda is not as straightforward as it is with openmp , and in some cases it may not be worth the effort . for an introduction to cuda and its many features see @xcite .",
    "implementing the serial algorithm above in cuda would involve executing the following _ _ device _ _ function in every thread in parallel :    .... _ _ shared _ _ int histo_thread[nbins ] ; int stride = blockdim.x*griddim.x ; //initialize shared histogram histo_thread[threadid.x]=0 ; _ _ syncthreads ( ) ; //correlate",
    "for(i=0;i < np1;i++ ) {    int j = threadidx.x+blockidx.x*blockdim.x ;    while(j < np2 ) {      //calculate distance between two objects      double dist = get_dist(x1[i],y1[i],z1[i ] ,                           x2[j],y2[j],z2[j ] ) ;      //calculate bin number      int ibin = bin_dist(dist ) ;      //increase histogram count      atomicadd(&(histo_thread[ibin]),1 ) ;      //increase second index by stride      j+=stride ;    } } //add block histograms _",
    "_ syncthreads ( ) ; atomicadd(&(histogram[threadidx.x ] ) ,            histo_thread[threadidx.x ] ) ;     ....    as before , we have divided one of the loops ( this time the second one ) among all the execution threads . the first difference with respect to openmp that we can see inmediately is that , due to the limited amount of memory of the gpu , we can only declare one partial histogram per block , and not per thread . to do this",
    "we declare it as a variable in shared memory , which also has the advantage of having a lower latency than global memory .",
    "this introduces a new complication , since now all threads in a block will try to add their pair counts to the same histogram .",
    "this has to be done avoiding race conditions by using the cuda atomicadd ( ) function .",
    "this is in fact the bottleneck of any algorithm involving histograms in cuda ( especially if the distribution under study is very degenerate ) , since many threads may have to remain idle while waiting for other threads to update their histogram entries .",
    "the best way to palliate this problem is to use at most as many threads per block as histogram bins ( in fact , note that the algorithm above will only work when using as many threads as histogram bins , however it can be easily extended to more general cases ) .",
    "finally all the partial block histograms are summed up into the global histogram in an ordered manner using again atomicadd ( ) .",
    "the fact that cute uses cuda atomic functions such as atomicadd ( ) means that it will only run on gpus that support atomic operations ( namely compute capability 2.0 or higher ) .",
    "there exist general algorithms for histograms that work on any cuda - enabled device @xcite , however no performance improvement was observed with respect to using atomicadd ( ) .",
    "furthermore , the method above reduces the use of shared memory for histograms to a minimum , allowing its use for other purposes .",
    "( blue ) , a larger cube is drawn ( gray ) , that safely contains all spheres of radius @xmath56 centered within @xmath57 ( red ) .",
    "neighbors of the objects within @xmath57 are only searched for in the gray region .",
    "the bottom panel shows the similar neighbor - searching regions used on the sphere for the calculation of the angular 2pcf . in this case",
    "the shape of the region is different depending on the position of the central pixel.,title=\"fig:\",scaledwidth=45.0% ]   ( blue ) , a larger cube is drawn ( gray ) , that safely contains all spheres of radius @xmath56 centered within @xmath57 ( red ) .",
    "neighbors of the objects within @xmath57 are only searched for in the gray region .",
    "the bottom panel shows the similar neighbor - searching regions used on the sphere for the calculation of the angular 2pcf . in this case",
    "the shape of the region is different depending on the position of the central pixel.,title=\"fig:\",scaledwidth=45.0% ]    often the maximum scale to which we want to calculate the 2pcf is significantly smaller than the size of our data . in this case , calculating the relative distance between particles that are further away than this maximum scale is useless , and therefore should be avoided .",
    "however , how can we determine which pairs to avoid without actually calculating their distances ?",
    "cute makes use of different approaches to minimize the amount of useless pair counts in an efficient way .",
    "the main strategy described here is very similar in the three - dimensional case ( for the 3-d and monopole 2pcfs ) and on the sphere ( for the angular correlation ) , however they differ slightly in the details .    in the three - dimensional case ,",
    "a box encompassing the whole catalog is first determined and divided into cubical cells .",
    "to each cell we associate the positions of all the objects that fall inside it . assuming that the maximum distance we are interested in is @xmath56 and that the cell size is @xmath58 , we draw a cube of @xmath59 cells per side around each cell @xmath57 ( here @xmath60 denotes the integer part of @xmath61 ) .",
    "this guarantees that we can draw spheres of radius @xmath56 around any point in @xmath57 and that these spheres will all lie inside the cube ( see the top panel in figure [ fig : neighbors ] ) .",
    "thus we can correlate all the objects inside @xmath57 with the objects in all the other cells inside the cube and safely ignore all other objects .",
    "the efficiency of this method depends largely on the number density of the catalog , the range of scales of interest , and the number of cubical cells used .    in the spherical case",
    "a similar approach is used .",
    "let us define a spherical cube as a region of the sphere with constant limits in spherical coordinates , i.e. a region with @xmath62 and @xmath63 .",
    "it is easy to prove that the spherical cube containing a spherical cap of radius @xmath64 centered at the point @xmath65 in spherical coordinates has sides of length : @xmath66 now , in the spherical case we can use pixels defined as small spherical cubes instead of the cubical cells of the three - dimensional case .",
    "then the result we have just quoted can be used to define a spherical cube of pixels centered at a given one that safely contains all particles within an angular distance @xmath64 of any particle in the central pixel ( see the bottom panel in figure [ fig : neighbors ] ) .",
    "once this is done , the same procedure is followed as in the three - dimensional case .",
    "another more sophisticated and very popular technique to discard unnecessary correlations is the so - called @xmath67-tree method . for a thorough description of this method ,",
    "see @xcite .       bin histogram can not be fit inside the device s shared memory , it is split into smaller ones , which are filled separately .",
    "although the catalogs have to be correlated more than once , the bottleneck caused by atomic operations is largely mitigated by the higher number of histogram bins.,scaledwidth=45.0% ]    in the previous section we have described the general strategy followed to parallelize the calculation of any 2pcf with openmp and cuda . however",
    "each of the 2pcfs detailed in section [ sec:2pcf ] requires a different treatment of the data and maybe allows for different , more optimal , approaches .",
    "we give the details specific to each of these types here .      as was said in section [ sec:2pcf ] the radial 2pcf",
    "is calculated by correlating pairs of aligned objects and binning them according to their relative redshift difference @xmath26 .",
    "spherical cubes are used by cute to quickly find pairs of galaxies subtending an angle smaller than some maximum aperture , which defines aligned pairs . for reasonable apertures ( @xmath68 )",
    "the number of pairs to correlate is relatively small .",
    "hence , since the computational time in this case is not an issue , there is no need for massive parallelization , and radial correlation functions are only supported by cute in its openmp version .      for the calculation of angular 2pcfs cute projects",
    "all objects in the catalog into the unit sphere and correlates pairs of objects according to their angular separation @xmath69 ( see figure [ fig : coords ] ) , which is used as a distance measure .",
    "two complementary speed - up techniques can be used by cute in this case : if one is not interested in extremely small angular scales one can create a pixel map from the catalog and then correlate the pixels ( weighting each of them by the number of objects that fall inside it ) .",
    "this may effectively reduce the number of objects that must be correlated by an order of magnitude and therefore reduce the computational time by a factor of 100 .",
    "also , in the calculation of the angular separation , the arc - cosine of the scalar product of two position vectors must be estimated .",
    "calculating the arc - cosine is a very time - consuming operation , and , if one is not interested in very large angular scales , the following approximation can be used , @xmath70 which is precise to 1 part in @xmath71 for angles below 40@xmath72 and reduces the computational time by a factor @xmath73 .",
    "both these time - saving techniques can be swiched on or off in cute by the user .",
    "the main difference in the calculation of the 3-d correlation function with respect to the other 2pcfs is that pairs are binned in 2-dimensional histograms , according to their @xmath74 or @xmath75 separations .",
    "this does not introduce any relevant changes in the openmp implementation , as long as the the amount of shared memory is large enough to accomodate one private 2-d histogram per thread , however it does matter when adapting the code to cuda .",
    "the reason is that currently the amount of shared memory per block in gpus is limited to 48 kb , which is too little to allocate , for example , a @xmath76 array of long integers .",
    "the solution to this problem chosen for cute is explained in fig .",
    "[ fig:3d_method ] : the catalogs are correlated several times , each time binning only pairs whose separation in one of the two coordinates is within a given range , until the whole histogram is filled .",
    "thus we can declare smaller 2-d histograms in shared memory , and even though the catalogs must be correlated several times , the histogram - filling bottleneck mentioned in section [ sec : cuda ] is largely alleviated by the higher number of histogram entries , thus conserving a reasonable computational time ( see section [ sec : speed ] ) .      in its current public version ,",
    "cute has a companion program , cute_box that calculates the correlation function from data inside a cubical box with periodic boundary conditions . in this case",
    "only the calculation of the isotropic 2pcf ( the monopole ) is supported , using two different types of algorithms :    * * particle - based algorithms*. in this case cute calculates the 2pcf from the pair counts using the estimator in equation [ eq : estim_box ] . as we have discussed , no random catalog is needed because of the periodic boundary conditions .",
    "two different types of neighbor - searching algorithms are supported : cubical cells and @xmath67-trees . *",
    "* density grid*. this algorithm is similar to the use of pixels to accelerate the calculation of the angular correlation function . in this case",
    "the particle content of the catalog is first interpolated to a grid and the overdensity field @xmath77 is estimated at every grid point using a tsc algorithm .",
    "then pairs of grid points are correlated , and a weight @xmath78 is given to each pair .",
    "the correlation function is estimated as @xmath79 where the average is taken over all pairs of grid points separated by a distance @xmath44 . due to the simplicity of a regular grid ,",
    "it is trivial to search for the neighboring grid points , and since the relative distances between neighboring points are the same everywhere , these relative distances only have to be calculated once . as a result of this , this method is usually the fastest , however it will only yield reliable results down to the scale of the grid .",
    "[ cols=\"^,^,^,^\",options=\"header \" , ]      can be gained by using a high - end gpu with respect to a sequential approach on a high - end cpu .",
    "even with a regular gaming gpu the increase in speed is substantial ( @xmath80 ) .",
    "the different devices are described in table [ tab : devices].,scaledwidth=45.0% ]    we have tested cute s performance in terms of computational time by running it on platforms with different capabilities , listed in table [ tab : devices ] .",
    "the serial version was tested by running cute on a single cpu core .",
    "we also tested the openmp version on a dual - core laptop and on a large shared - memory machine with 80 cores .",
    "the cuda version has been tried on a regular graphics card in a laptop and on a high - end gpu .",
    "all the computational times quoted in this section correspond to tests performed without any of the neighbor - searching techniques described in section [ ssec : neigh ] in order to provide a clearer comparison between platforms , and they should therefore be understood as the worst - case scenario .",
    "as we have noted , the use of these strategies may improve the computational time significantly with respect to a more naive approach ( even by orders of magnitude ) .",
    "however , this improvement depends largely on the number density of the data and the scales of interest .    for this test",
    "the monopole 2pcf was calculated for catalogs of different sizes in the range @xmath81 .",
    "the computational times for one single correlation ( i.e. just calculating , for example , @xmath52 ) in the 5 different platforms are plotted in figure [ fig : timings ] . as expected , using gpus or parallelising the computation on several cpu cores",
    "improves the code s speed by a factor 10 - 100 , even using a regular video - game graphics card .",
    "the ellapsed times were measured using openmp and cuda timing functions , since these give the most accurate estimate of the time spent doing the actual correlation .    for completeness",
    "we have also listed in table [ tab : times ] the computational times taken by the 5 different devices to calculate different correlation functions .",
    "the dataset used for this exercise is a subset of one of the mock catalogs provided by the mice project @xcite , with @xmath82 , @xmath83 , @xmath84 , containing @xmath85 particles .",
    "the 5 different correlation functions are :    * monopole correlation function : linear binning for @xmath86 and 256 bins .",
    "* monopole correlation function : logarithmic binning for @xmath86 using 256 bins and 50 bins per decade . *",
    "angular correlation function : linear binning for @xmath87 and 256 bins .",
    "calculated by brute - force .",
    "* angular correlation function : linear binning for @xmath87 and 256 bins . calculated using pixels with resolution @xmath88 sq - deg . *",
    "3-d correlation function : binning in @xmath75 on a @xmath89-bin histogram .",
    "figures [ fig : corrs1 ] and [ fig : corrs2 ] show the output produced by cute for different kinds of correlation functions .",
    "we have presented cute , a parallel code for computing two - point correlation functions from cosmological catalogs .",
    "cute has been optimized to run on shared - memory machines as well as graphical processing units .",
    "it can estimate the 3-d , monopole , radial and angular correlation functions from a set of data using different speed - up techniques and binning schemes .",
    "we have shown that great benefits in terms of computational speed can be gained by parallelising the algorithm on gpus .",
    "the code is publicly available through our website .",
    "cute is released under the gnu public license ( gpl ) .",
    "the author would like to thank ignacio sevilla , miguel crdenas and rafael ponce for their invaluable input and alexander knebe for useful suggestions and beta - testing .",
    "cute was initially tested on mock data kindly provided by the mice collaboration .",
    "da acknowledges support from a jae - predoc contract ."
  ],
  "abstract_text": [
    "<S> in the advent of new large galaxy surveys , which will produce enormous datasets with hundreds of millions of objects , new computational techniques are necessary in order to extract from them any two - point statistic , the computational time of which grows with the square of the number of objects to be correlated . </S>",
    "<S> fortunately technology now provides multiple means to massively parallelize this problem . here </S>",
    "<S> we present a free - source code specifically designed for this kind of calculations . </S>",
    "<S> two implementations are provided : one for execution on shared - memory machines using openmp and one that runs on graphical processing units ( gpus ) using cuda . </S>",
    "<S> the code is available at http://members.ift.uam-csic.es/dmonge/cute.html . </S>"
  ]
}